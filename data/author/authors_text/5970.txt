Proceedings of NAACL HLT 2007, pages 396?403,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Incremental Non-Projective Dependency Parsing
Joakim Nivre
Va?xjo? University, School of Mathematics and Systems Engineering
Uppsala University, Department of Linguistics and Philology
joakim.nivre@{msi.vxu.se,lingfil.uu.se}
Abstract
An open issue in data-driven dependency
parsing is how to handle non-projective
dependencies, which seem to be required
by linguistically adequate representations,
but which pose problems in parsing with
respect to both accuracy and efficiency.
Using data from five different languages,
we evaluate an incremental deterministic
parser that derives non-projective depen-
dency structures in O(n2) time, supported
by SVM classifiers for predicting the next
parser action. The experiments show that
unrestricted non-projective parsing gives
a significant improvement in accuracy,
compared to a strictly projective baseline,
with up to 35% error reduction, leading
to state-of-the-art results for the given
data sets. Moreover, by restricting the
class of permissible structures to limited
degrees of non-projectivity, the parsing
time can be reduced by up to 50% without
a significant decrease in accuracy.
1 Introduction
Data-driven dependency parsing has been shown to
give accurate and efficient parsing for a wide range
of languages, such as Japanese (Kudo and Mat-
sumoto, 2002), English (Yamada and Matsumoto,
2003), Swedish (Nivre et al, 2004), Chinese (Cheng
et al, 2004), and Czech (McDonald et al, 2005).
Whereas most of the early approaches were limited
to strictly projective dependency structures, where
the projection of a syntactic head must be contin-
uous, attention has recently shifted to the analysis
of non-projective structures, which are required for
linguistically adequate representations, especially in
languages with free or flexible word order.
The most popular strategy for capturing non-
projective structures in data-driven dependency
parsing is to apply some kind of post-processing to
the output of a strictly projective dependency parser,
as in pseudo-projective parsing (Nivre and Nilsson,
2005), corrective modeling (Hall and Nova?k, 2005),
or approximate non-projective parsing (McDonald
and Pereira, 2006). And it is rare to find parsers
that derive non-projective structures directly, the no-
table exception being the non-projective spanning
tree parser proposed by McDonald et al (2005).
There are essentially two arguments that have
been advanced against using parsing algorithms
that derive non-projective dependency structures di-
rectly. The first is that the added expressivity com-
promises efficiency, since the parsing problem for a
grammar that allows arbitrary non-projective depen-
dency structures has been shown to beNP complete
(Neuhaus and Bro?ker, 1997). On the other hand,
most data-driven approaches do not rely on gram-
mars, and with a suitable factorization of depen-
dency structures, it is possible to achieve parsing of
unrestricted non-projective structures inO(n2) time,
as shown by McDonald et al (2005).
The second argument against non-projective de-
pendency parsing comes from the observation that,
even in languages with free or flexible word order,
396
most dependency structures are either projective or
very nearly projective. This can be seen by con-
sidering data from treebanks, such as the Prague
Dependency Treebank of Czech (Bo?hmova? et al,
2003), the TIGER Treebank of German (Brants et
al., 2002), or the Slovene Dependency Treebank
(Dz?eroski et al, 2006), where the overall proportion
of non-projective dependencies is only about 2%
even though the proportion of sentences that con-
tain some non-projective dependency is as high as
25%. This means that an approach that starts by de-
riving the best projective approximation of the cor-
rect dependency structure is likely to achieve high
accuracy, while an approach that instead attempts
to search the complete space of non-projective de-
pendency structures runs the risk of finding struc-
tures that depart too much from the near-projective
norm. Again, however, the results of McDonald et
al. (2005) suggest that the latter risk is minimized if
inductive learning is used to guide the search.
One way of improving efficiency, and potentially
also accuracy, in non-projective dependency parsing
is to restrict the search to a subclass of ?mildly non-
projective? structures. Nivre (2006) defines degrees
of non-projectivity in terms of the maximum number
of intervening constituents in the projection of a syn-
tactic head and shows that limited degrees of non-
projectivity give a much better fit with the linguistic
data than strict projectivity, but also enables more ef-
ficient processing than unrestricted non-projectivity.
However, the results presented by Nivre (2006) are
all based on oracle parsing, which means that they
only provide upper bounds on the accuracy that can
be achieved.
In this paper, we investigate to what extent con-
straints on non-projective structures can improve
accuracy and efficiency in practical parsing, using
treebank-induced classifiers to predict the actions of
a deterministic incremental parser. The parsing al-
gorithm used belongs to the family of algorithms de-
scribed by Covington (2001), and the classifiers are
trained using support vector machines (SVM) (Vap-
nik, 1995). The system is evaluated using treebank
data from five languages: Danish, Dutch, German,
Portuguese, and Slovene.
The paper is structured as follows. Section 2
defines syntactic representations as labeled depen-
dency graphs and introduces the notion of degree
used to constrain the search. Section 3 describes the
parsing algorithm, including modifications neces-
sary to handle degrees of non-projectivity, and sec-
tion 4 describes the data-driven prediction of parser
actions, using history-based models and SVM clas-
sifiers. Section 5 presents the experimental setup,
section 6 discusses the experimental results, and sec-
tion 7 contains our conclusions.
2 Dependency Graphs
A dependency graph is a labeled directed graph, the
nodes of which are indices corresponding to the to-
kens of a sentence. Formally:
Definition 1 Given a set R of dependency types
(arc labels), a dependency graph for a sentence
x = (w1, . . . , wn) is a labeled directed graph G =
(V,E, L), where:
1. V = {0, 1, 2, . . . , n}
2. E ? V ? V
3. L : E ? R
The set V of nodes (or vertices) is the set of non-
negative integers up to and including n. This means
that every token index i of the sentence is a node
(1 ? i ? n) and that there is a special node 0, which
will always be a root of the dependency graph. The
set E of arcs (or edges) is a set of ordered pairs
(i, j), where i and j are nodes. Since arcs are used
to represent dependency relations, we will say that i
is the head and j is the dependent of the arc (i, j).
The function L assigns a dependency type (label)
r ? R to every arc e ? E. We use the notation
i ? j to mean that there is an arc connecting i and
j (i.e., (i, j) ? E); we use the notation i r? j if
this arc is labeled r (i.e., ((i, j), r) ? L); and we
use the notation i ?? j and i ?? j for the reflexive
and transitive closure of the arc relation E and the
corresponding undirected relation, respectively.
Definition 2 A dependency graph G is well-formed
if and only if:
1. The node 0 is a root, i.e., there is no node i such
that i ? 0 (ROOT).
2. G is weakly connected, i.e., i ?? j for every
pair of nodes i, j (CONNECTEDNESS).
3. Every node has at most one head, i.e., if i? j
then there is no node k such that k 6= i and
k ? j (SINGLE-HEAD).
397
(?Only one of them concerns quality.?)
0 1
R
Z
(Out-of
 
?
AuxP
2
P
nich
them
 
?
Atr
3
VB
je
is
 
?
Pred
4
T
jen
only
 
?
AuxZ
5
C
jedna
one-FEM-SG
 
?
Sb
6
R
na
to
 
?
AuxP
7
N4
kvalitu
quality
?
 
Adv
8
Z:
.
.)
 
?
AuxZ
Figure 1: Dependency graph for Czech sentence from the Prague Dependency Treebank
The well-formedness conditions are independent in
that none of them is entailed by any (combination)
of the others, but they jointly entail that the graph
is a tree rooted at the node 0. By way of example,
figure 1 shows a Czech sentence from the Prague
Dependency Treebank (Bo?hmova? et al, 2003) with
a well-formed dependency graph according to Defi-
nitions 1 and 2.
The constraints imposed on dependency graphs in
Definition 2 are assumed in almost all versions of
dependency grammar, especially in computational
systems, and are sometimes complemented by a
fourth constraint:
4. The graph G is projective, i.e., if i ? j then
i ?? k, for every node k such that i < k < j
or j < k < i (PROJECTIVITY).
Most theoretical formulations of dependency gram-
mar regard projectivity as the norm but recognize
the need for non-projective representations to cap-
ture non-local dependencies (Mel?c?uk, 1988; Hud-
son, 1990). Finding a way of incorporating a suit-
ably restricted notion of non-projectivity into prac-
tical parsing systems is therefore an important step
towards a more adequate syntactic analysis, as dis-
cussed in the introduction of this paper.
In order to distinguish classes of dependency
graphs that fall in between arbitrary non-projective
and projective, Nivre (2006) introduces a notion
of degree of non-projectivity, such that projective
graphs have degree 0 while arbitrary non-projective
graphs have unbounded degree.
Definition 3 Let G = (V,E, L) be a well-formed
dependency graph, let G(i,j) be the subgraph of G
defined by V(i,j) = {i, i+1, . . . , j?1, j}, and let
min(e) be the smallest and max(e) the largest ele-
ment of an arc e in the linear order <:
1. The degree of an arc e ? E is the number of
connected components (i.e., weakly connected
subgraphs) in G(min(e)+1,max(e)?1) that are not
dominated by the head of e in G(min(e),max(e)).
2. The degree of G is the maximum degree of any
arc e ? E.
To exemplify the notion of degree, we note that the
dependency graph in figure 1 has degree 1. The only
non-projective arc in the graph is (5, 1) and G(2,4)
contains three connected components, each consist-
ing of a single root node (2, 3, 4). Since exactly one
of these, 3, is not dominated by 5 in G(1,5), the arc
(5, 1) has degree 1.
Nivre (2006) presents an empirical study, based
on data from the Prague Dependency Treebank of
Czech (Bo?hmova? et al, 2003) and the Danish De-
pendency Treebank (Kromann, 2003), showing that
more than 99.5% of all sentences occurring in the
two treebanks have a dependency graph with a max-
imum degree of 2; about 98% have a maximum de-
gree of 1; but only 77% in the Czech data and 85% in
the Danish data have degree 0 (which is equivalent to
assuming PROJECTIVITY). This suggests that lim-
ited degrees of non-projectivity may allow a parser
to capture a larger class of naturally occurring syn-
tactic structures, while still constraining the search
to a proper subclass of all possible structures.1
1Alternative notions of mildly non-projective dependency
structures are explored in Kuhlmann and Nivre (2006).
398
3 Parsing Algorithm
Covington (2001) describes a parsing strategy for
dependency representations that has been known
since the 1960s but not presented in the literature.
The left-to-right (or incremental) version of this
strategy can be formulated in the following way:
PARSE(x = (w1, . . . , wn))
1 for j = 1 up to n
2 for i = j ? 1 down to 0
3 LINK(i, j)
LINK(i, j) is a nondeterministic operation that adds
the arc i ? j (with some label), adds the arc j ? i
(with some label), or does nothing at all. In this
way, the algorithm builds a graph by systematically
trying to link every pair of nodes (i, j) (i < j).
We assume that LINK(i,j) respects the ROOT and
SINGLE-HEAD constraints and that it does not in-
troduce cycles into the graph, i.e., it adds an arc
i ? j only if j 6= 0, there is no k 6= i such that
k ? j, and it is not the case that j ?? i. Given
these constraints, the graph G given at termination
can always be turned into a well-formed dependency
graph by adding arcs from the root 0 to any root node
in {1, . . . , n}.
Assuming that LINK(i, j) can be performed in
some constant time c, the running time of the al-
gorithm is
?n
i=1 c(i ? 1) = c(
n2
2 ?
n
2 ), which in
terms of asymptotic complexity is O(n2). Checking
ROOT and SINGLE-HEAD in constant time is easy,
but in order to prevent cycles we need to be able
to find, for any node k, the root of the connected
component to which k belongs in the partially built
graph. This problem can be solved efficiently us-
ing standard techniques for disjoint sets, including
path compression and union by rank, which guaran-
tee that the necessary checks can be performed in
average constant time (Cormen et al, 1990).
In the experiments reported in this paper, we mod-
ify the basic algorithm by making the performance
of LINK(i, j) conditional on the arcs (i, j) and (j, i)
being permissible under different degree constraints:
PARSE(x = (w1, . . . , wn), d)
1 for j = 1 up to n
2 for i = j ? 1 down to 0
3 if PERMISSIBLE(i, j, d)
4 LINK(i, j)
The function PERMISSIBLE(i, j, d) returns true if
and only if i ? j and j ? i have a degree less
than or equal to d given the partially built graph G.
Setting d = 0 gives strictly projective parsing, while
d = ? corresponds to unrestricted non-projective
parsing. With low values of d, we will reduce the
number of calls to LINK(i, j), which will reduce
the overall parsing time provided that the time re-
quired to compute PERMISSIBLE(i, j, d) is insignif-
icant compared to the time needed for LINK(i, j).
This is typically the case in data-driven systems,
where LINK(i, j) requires a call to a trained classi-
fier, while PERMISSIBLE(i, j, d) only needs access
to the partially built graph G.2
4 History-Based Parsing
History-based parsing uses features of the parsing
history to predict the next parser action (Black et al,
1992). In the current setup, this involves using fea-
tures of the partially built dependency graph G and
the input x = (w1, . . . , wn) to predict the outcome
of the nondeterministic LINK(i, j) operation. Given
that we use a deterministic parsing strategy, this re-
duces to a pure classification problem.
Let ?(i, j, G) = (?1,. . . ,?m) be a feature vec-
tor representation of the parser history at the time
of performing LINK(i, j). The task of the history-
based classifier is then to map ?(i, j, G) to one of
the following actions:
1. Add the arc i r? j (for some r ? R).
2. Add the arc j r? i (for some r ? R).
3. Do nothing.
Training data for the classifier can be generated by
running the parser on a sample of treebank data, us-
ing the gold standard dependency graph as an ora-
cle to predict LINK(i, j) and constructing one train-
ing instance (?(i, j, G), a) for each performance of
LINK(i, j) with outcome a.
The features in ?(i, j, G) = (?1, . . . , ?m) can
be arbitrary features of the input x and the partially
built graph G but will in the experiments below be
restricted to linguistic attributes of input tokens, in-
cluding their dependency types according to G.
2Checking PERMISSIBLE(i, j, d), again requires finding the
roots of connected components and can therefore be done in
average constant time.
399
Language Tok Sen T/S Lem CPoS PoS MSF Dep NPT NPS
Danish 94 5.2 18.2 no 10 24 47 52 1.0 15.6
Dutch 195 13.3 14.6 yes 13 302 81 26 5.4 36.4
German 700 39.2 17.8 no 52 52 0 46 2.3 27.8
Portuguese 207 9.1 22.8 yes 15 21 146 55 1.3 18.9
Slovene 29 1.5 18.7 yes 11 28 51 25 1.9 22.2
Table 1: Data sets; Tok = number of tokens (*1000); Sen = number of sentences (*1000); T/S = tokens
per sentence (mean); Lem = lemmatization present; CPoS = number of coarse-grained part-of-speech tags;
PoS = number of (fine-grained) part-of-speech tags; MSF = number of morphosyntactic features (split into
atoms); Dep = number of dependency types; NPT = proportion of non-projective dependencies/tokens (%);
NPS = proportion of non-projective dependency graphs/sentences (%)
The history-based classifier can be trained with
any of the available supervised methods for func-
tion approximation, but in the experiments below we
will rely on SVM, which has previously shown good
performance for this kind of task (Kudo and Mat-
sumoto, 2002; Yamada and Matsumoto, 2003).
5 Experimental Setup
The purpose of the experiments is twofold. First, we
want to investigate whether allowing non-projective
structures to be derived incrementally can improve
parsing accuracy compared to a strictly projective
baseline. Secondly, we want to examine whether
restricting the degree of non-projectivity can im-
prove efficiency compared to an unrestricted non-
projective baseline. In order to investigate both these
issues, we have trained one non-projective parser
for each language, allowing arbitrary non-projective
structures as found in the treebanks during training,
but applying different constraints during parsing:
1. Non-projective (d = ?)
2. Max degree 2 (d = 2)
3. Max degree 1 (d = 1)
These three versions of the non-projective parser are
compared to a strictly projective parser (d = 0),
which uses the same parsing algorithm but only con-
siders projective arcs in both training and testing.3
The experiments are based on treebank data from
five languages: the Danish Dependency Treebank
3An alternative would have been to train all parsers on non-
projective data, or restrict the training data for each parser
according to its parsing restriction. Preliminary experiments
showed that the setup used here gave the best performance for
all parsers involved.
(Kromann, 2003), the Alpino Treebank of Dutch
(van der Beek et al, 2002), the TIGER Treebank of
German (Brants et al, 2002), the Floresta Sinta?ctica
of Portuguese (Afonso et al, 2002), and the Slovene
Dependency Treebank (Dz?eroski et al, 2006).4 The
data sets used are the training sets from the CoNLL-
X Shared Task on multilingual dependency parsing
(Buchholz and Marsi, 2006), with 20% of the data
reserved for testing using a pseudo-random split. Ta-
ble 1 gives an overview of the five data sets, showing
the number of tokens and sentences, the presence
of different kinds of linguistic annotation, and the
amount of non-projectivity.
The features used in the history-based model for
all languages include the following core set of 20
features, where i and j are the tokens about to be
linked and the context stack is a stack of root nodes
k in G(i+1,j?1), added from right to left (i.e., with
the top node being closest to i):
1. Word form: i, j, j+1, h(i).
2. Lemma (if available): i.
3. Part-of-speech: i?1, i, j, j+1, j+2, k, k?1.
4. Coarse part-of-speech (if available): i, j, k.
5. Morphosyntactic features (if available): i, j.
6. Dependency type: i, j, l(i), l(j), r(i).
In the specification of features, we use k and k?1 to
refer to the two topmost tokens on the context stack,
and we use h(?), l(?) and r(?) to refer to the head,
4This set does not include the Prague Dependency Treebank
of Czech (Bo?hmova? et al, 2003), one of the most widely used
treebanks in studies of non-projective parsing. The reason is
that the sheer size of this data set makes extensive experiments
using SVM learning extremely time consuming. The work on
Czech was therefore initially postponed but is now ongoing.
400
Danish Dutch German Portuguese Slovene
Constraint AS ER AS ER AS ER AS ER AS ER
Non-projective 88.13 8.34 86.79 36.18 89.78 21.51 90.59 11.39 76.52 6.83
Max degree 2 88.08 7.95 86.15 33.09 89.74 21.20 90.58 11.30 76.48 6.67
Max degree 1 88.00 7.33 85.12 28.12 89.49 19.28 90.48 10.36 76.40 6.35
Projective 87.05 ? 79.30 ? 86.98 ? 89.38 ? 74.80 ?
Table 2: Parsing accuracy; AS = attachment score; ER = error reduction w.r.t. projective baseline (%)
the leftmost dependent and the rightmost dependent
of a token ? in the partially built dependency graph.5
In addition to the core set of features, the model
for each language has been augmented with a small
number of additional features, which have proven
useful in previous experiments with the same data
set. The maximum number of features used is 28
(Danish); the minimum number is 23 (German).
The history-based classifiers have been trained
using SVM learning, which combines a maximum
margin strategy with the use of kernel functions
to map the original feature space to a higher-
dimensional space. More specifically, we use LIB-
SVM (Chang and Lin, 2001) with a quadratic kernel
K(xi, xj) = (?xTi xj +r)
2. We use the built-in one-
versus-one strategy for multi-class classification and
convert symbolic features to numerical features us-
ing the standard technique of binarization.
Parsing accuracy is measured by the unlabeled at-
tachment score (AS), i.e., the proportion of words
that are assigned the correct head (not counting
punctuation). Although the parsers do derive labeled
dependency graphs, we concentrate on the graph
structure here, since this is what is concerned in the
distinction between projective and non-projective
dependency graphs. Efficiency is evaluated by re-
porting the parsing time (PT), i.e., the time required
to parse the respective test sets. Since both training
sets and test sets vary considerably in size between
languages, we are primarily interested in the rela-
tive differences for parsers applied to the same lan-
guage. Experiments have been performed on a Sun-
Blade 2000 with one 1.2GHz UltraSPARC-III pro-
cessor and 2GB of memory.
5The lack of symmetry in the feature set reflects the asym-
metry in the partially built graph G, where, e.g., only i can have
dependents to the right at decision time. This explains why there
are more features defined in terms of graph structure for i and
more features defined in terms of string context for j.
6 Results and Discussion
Table 2 shows the parsing accuracy of the non-
projective parser with different maximum degrees,
both the raw attachment scores and the amount of
error reduction with respect to the baseline parser.
Our first observation is that the non-projective parser
invariably achieves higher accuracy than the pro-
jective baseline, with differences that are statisti-
cally significant across the board (using McNemar?s
test). The amount of error reduction varies be-
tween languages and seems to depend primarily on
the frequency of non-projective structures, which is
not surprising. Thus, for Dutch and German, the
two languages with the highest proportion of non-
projective structures, the best error reduction is over
35% and over 20%, respectively. However, there
seems to be a sparse data effect in that Slovene,
which has the smallest training data set, has the
smallest error reduction despite having more non-
projective structures than Danish and Portuguese.
Our second observation is that the highest score is
always obtained with an unbounded degree of non-
projectivity during parsing. This seems to corrobo-
rate the results obtained by McDonald et al (2005)
with a different parsing method, showing that the
use of inductive learning to guide the search dur-
ing parsing eliminates the potentially harmful ef-
fect of increasing the size of the search space. Al-
though the differences between different degrees of
non-projectivity are not statistically significant for
the current data sets,6 the remarkable consistency
across languages suggests that they are nevertheless
genuine. In either case, however, they must be con-
sidered marginal, except possibly for Dutch, which
leads to our third and final observation about accu-
6The only exception is the difference between a maximum
degree of 1 and unrestricted non-projective for Dutch, which is
significant according to McNemar?s test with ?= .05.
401
Danish Dutch German Portuguese Slovene
Constraint PT TR PT TR PT TR PT TR PT TR
Non-projective 426 ? 3791 ? 24454 ? 3708 ? 204 ?
Max degree 2 395 7.29 2068 45.46 17903 26.79 3004 18.99 130 36.39
Max degree 1 346 18.72 1695 55.28 13079 46.52 2446 34.04 108 47.05
Projective 211 50.53 784 79.32 7362 69.90 1389 62.55 429 79.00
Table 3: Parsing time; PT = parsing time (s); TR = time reduction w.r.t. non-projective baseline (%)
System Danish Dutch German Portuguese Slovene
CoNLL-X McDonald et al 84.79 79.19 87.34 86.82 73.44
CoNLL-X Nivre et al 84.77 78.59 85.82 87.60 70.30
Incremental non-projective 84.85 77.91 85.90 87.12 70.86
Table 4: Related work (labeled attachment score)
racy, namely that restricting the maximum degree of
non-projectivity to 2 or 1 has a very marginal effect
on accuracy and is always significantly better than
the projective baseline.
Turning next to efficiency, table 3 shows the pars-
ing time for the different parsers across the five lan-
guages. Our first observation here is that the pars-
ing time can be reduced by restricting the degree
of non-projectivity during parsing, thus corroborat-
ing the claim that the running time of the history-
based classifier dominates the overall parsing time.
As expected, the largest reduction is obtained with
the strictly projective parser, but here we must also
take into account that the training data set is smaller
(because of the restriction to projective potential
links), which improves the average running time of
the history-based classifier in itself. Our second ob-
servation is that the amount of reduction in parsing
time seems to be roughly related to the amount of
non-projectivity, with a reduction of about 50% at
a max degree of 1 for the languages where more
than 20% of all sentences are non-projective (Dutch,
German, Slovene) but significantly smaller for Por-
tuguese and especially for Danish. On the whole,
however, the reduction in parsing time with limited
degrees of non-projectivity is substantial, especially
considering the very marginal drop in accuracy.
In order to compare the performance to the state
of the art in dependency parsing, we have retrained
the non-projective parser on the entire training data
set for each language and evaluated it on the final
test set from the CoNLL-X shared task (Buchholz
and Marsi, 2006). Thus, table 4 shows labeled at-
tachment scores, the main evaluation metric used in
the shared task, in comparison to the two highest
scoring systems from the original evaluation (Mc-
Donald et al, 2006; Nivre et al, 2006). The incre-
mental non-projective parser has the best reported
score for Danish and outperforms at least one of the
other two systems for four languages out of five,
although most of the differences are probably too
small to be statistically significant. But whereas the
spanning tree parser of McDonald et al (2006) and
the pseudo-projective parser of Nivre et al (2006)
achieve this performance only with special pre- or
post-processing,7 the approach presented here de-
rives a labeled non-projective graph in a single incre-
mental process and hence at least has the advantage
of simplicity. Moreover, it has better time complex-
ity than the approximate second-order spanning tree
parsing of McDonald et al (2006), which has expo-
nential complexity in the worst case (although this
does not appear to be a problem in practice).
7 Conclusion
In this paper, we have investigated a data-driven ap-
proach to dependency parsing that combines a deter-
ministic incremental parsing algorithm with history-
based SVM classifiers for predicting the next parser
action. We have shown that, for languages with a
7McDonald et al (2006) use post-processing for non-
projective dependencies and for labeling. Nivre et al (2006) use
pre-processing of training data and post-processing of parser
output to recover non-projective dependencies.
402
non-negligible proportion of non-projective struc-
tures, parsing accuracy can be improved signifi-
cantly by allowing non-projective structures to be
derived. We have also shown that the parsing time
can be reduced substantially, with only a marginal
loss in accuracy, by limiting the degree of non-
projectivity allowed during parsing. A comparison
with results from the CoNLL-X shared task shows
that the parsing accuracy is comparable to that of the
best available systems, which means that incremen-
tal non-projective dependency parsing is a viable al-
ternative to approaches based on post-processing of
projective approximations.
Acknowledgments
The research presented in this paper was partially
supported by a grant from the Swedish Research
Council. I want to thank Johan Hall and Jens Nils-
son for their contributions to MaltParser, which was
used to perform the experiments. I am also grateful
to three anonymous reviewers for finding important
errors in the preliminary version and for suggesting
several other improvements for the final version.
References
S. Afonso, E. Bick, R. Haber, and D. Santos. 2002. ?Floresta
sinta?(c)tica?: a treebank for Portuguese. In Proc. of LREC,
1698?1703.
E. Black, F. Jelinek, J. D. Lafferty, D. M. Magerman, R. L. Mer-
cer, and S. Roukos. 1992. Towards history-based grammars:
Using richer models for probabilistic parsing. In Proc. of the
DARPA Speech and Natural Language Workshop, 31?37.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003.
The PDT: a 3-level annotation scenario. In A. Abeille?, ed.,
Treebanks: Building and Using Parsed Corpora, chapter 7.
Kluwer, Dordrecht.
S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith. 2002.
The TIGER treebank. In Proc. of TLT.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on
multilingual dependency parsing. In Proc. of CoNLL, 149?
164.
C.-C. Chang and C.-J. Lin, 2001. LIBSVM: A Library
for Support Vector Machines. Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
Y. Cheng, M. Asahara, and Y. Matsumoto. 2004. Determinis-
tic dependency structure analyzer for Chinese. In Proc. of
IJCNLP, 500?508.
T. H. Cormen, C. E. Leiserson, and R. L. Rivest. 1990. Intro-
duction to Algorithms. MIT Press.
M. A. Covington. 2001. A fundamental algorithm for depen-
dency parsing. In Proc. of the Annual ACM Southeast Con-
ference, 95?102.
S. Dz?eroski, T. Erjavec, N. Ledinek, P. Pajas, Z. Z?abokrtsky, and
A. Z?ele. 2006. Towards a Slovene dependency treebank. In
Proc. of LREC.
Keith Hall and Vaclav Nova?k. 2005. Corrective modeling for
non-projective dependency parsing. In Proc. of IWPT, 42?
52
R. A. Hudson. 1990. English Word Grammar. Blackwell.
M. T. Kromann. 2003. The Danish dependency treebank and
the underlying linguistic theory. In Proc. of TLT.
T. Kudo and Y. Matsumoto. 2002. Japanese dependency analy-
sis using cascaded chunking. In Proc. of CoNLL, 63?69.
M. Kuhlmann and J. Nivre. 2006. Mildly non-projective de-
pendency structures. In Proc. of COLING-ACL, Posters,
507?514.
R. McDonald and F-. Pereira. 2006. Online learning of approx-
imate dependency parsing algorithms. In Proc. of EACL,
81?88.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005.
Non-projective dependency parsing using spanning tree al-
gorithms. In Proc. of HLT-EMNLP, 523?530.
R. McDonald, K. Lerman, and F. Pereira. 2006. Multilingual
dependency analysis with a two-stage discriminative parser.
In Proc. of CoNLL, 216?220.
I. Mel?c?uk. 1988. Dependency Syntax: Theory and Practice.
State University of New York Press.
P. Neuhaus and N. Bro?ker. 1997. The complexity of recog-
nition of linguistically adequate dependency grammars. In
Proc. of ACL-EACL, 337?343.
J. Nivre and J. Nilsson. 2005. Pseudo-projective dependency
parsing. In Proc. of ACL, 99?106.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based depen-
dency parsing. In Proc. of CoNLL, 49?56.
J. Nivre, J. Hall, J. Nilsson, G. Eryig?it, and S. Marinov. 2006.
Labeled pseudo-projective dependency parsing with support
vector machines. In Proc. of CoNLL, 221?225.
J. Nivre. 2006. Constraints on non-projective dependency
graphs. In Proc. of EACL, 73?80.
L. van der Beek, G. Bouma, R. Malouf, and G. van Noord.
2002. The Alpino dependency treebank. In Computational
Linguistics in the Netherlands (CLIN).
V. N. Vapnik. 1995. The Nature of Statistical Learning Theory.
Springer.
H. Yamada and Y. Matsumoto. 2003. Statistical dependency
analysis with support vector machines. In Proc. of IWPT,
195?206.
403
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 507?514,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Mildly Non-Projective Dependency Structures
Marco Kuhlmann
Programming Systems Lab
Saarland University
Germany
kuhlmann@ps.uni-sb.de
Joakim Nivre
V?xj? University and
Uppsala University
Sweden
nivre@msi.vxu.se
Abstract
Syntactic parsing requires a fine balance
between expressivity and complexity, so
that naturally occurring structures can be
accurately parsed without compromising
efficiency. In dependency-based parsing,
several constraints have been proposed that
restrict the class of permissible structures,
such as projectivity, planarity, multi-pla-
narity, well-nestedness, gap degree, and
edge degree. While projectivity is gener-
ally taken to be too restrictive for natural
language syntax, it is not clear which of the
other proposals strikes the best balance be-
tween expressivity and complexity. In this
paper, we review and compare the different
constraints theoretically, and provide an ex-
perimental evaluation using data from two
treebanks, investigating how large a propor-
tion of the structures found in the treebanks
are permitted under different constraints.
The results indicate that a combination of
the well-nestedness constraint and a para-
metric constraint on discontinuity gives a
very good fit with the linguistic data.
1 Introduction
Dependency-based representations have become in-
creasingly popular in syntactic parsing, especially
for languages that exhibit free or flexible word or-
der, such as Czech (Collins et al, 1999), Bulgarian
(Marinov and Nivre, 2005), and Turkish (Eryig?it
and Oflazer, 2006). Many practical implementa-
tions of dependency parsing are restricted to pro-
jective structures, where the projection of a head
word has to form a continuous substring of the
sentence. While this constraint guarantees good
parsing complexity, it is well-known that certain
syntactic constructions can only be adequately rep-
resented by non-projective dependency structures,
where the projection of a head can be discontinu-
ous. This is especially relevant for languages with
free or flexible word order.
However, recent results in non-projective depen-
dency parsing, especially using data-driven meth-
ods, indicate that most non-projective structures
required for the analysis of natural language are
very nearly projective, differing only minimally
from the best projective approximation (Nivre and
Nilsson, 2005; Hall and Nov?k, 2005; McDon-
ald and Pereira, 2006). This raises the question
of whether it is possible to characterize a class of
mildly non-projective dependency structures that is
rich enough to account for naturally occurring syn-
tactic constructions, yet restricted enough to enable
efficient parsing.
In this paper, we review a number of propos-
als for classes of dependency structures that lie
between strictly projective and completely unre-
stricted non-projective structures. These classes
have in common that they can be characterized in
terms of properties of the dependency structures
themselves, rather than in terms of grammar for-
malisms that generate the structures. We compare
the proposals from a theoretical point of view, and
evaluate a subset of them empirically by testing
their representational adequacy with respect to two
dependency treebanks: the Prague Dependency
Treebank (PDT) (Hajic? et al, 2001), and the Danish
Dependency Treebank (DDT) (Kromann, 2003).
The rest of the paper is structured as follows.
In section 2, we provide a formal definition of de-
pendency structures as a special kind of directed
graphs, and characterize the notion of projectivity.
In section 3, we define and compare five different
constraints on mildly non-projective dependency
structures that can be found in the literature: pla-
narity, multiplanarity, well-nestedness, gap degree,
and edge degree. In section 4, we provide an ex-
perimental evaluation of the notions of planarity,
well-nestedness, gap degree, and edge degree, by
507
investigating how large a proportion of the depen-
dency structures found in PDT and DDT are al-
lowed under the different constraints. In section 5,
we present our conclusions and suggestions for fur-
ther research.
2 Dependency graphs
For the purposes of this paper, a dependency graph
is a directed graph on the set of indices correspond-
ing to the tokens of a sentence. We write ?n? to refer
to the set of positive integers up to and including n.
Definition 1 A dependency graph for a sentence
x D w1; : : : ; wn is a directed graph1
G D .V I E/; where V D ?n? and E  V  V .
Throughout this paper, we use standard terminol-
ogy and notation from graph theory to talk about
dependency graphs. In particular, we refer to the
elements of the set V as nodes, and to the elements
of the set E as edges. We write i ! j to mean that
there is an edge from the node i to the node j (i.e.,
.i; j / 2 E), and i ! j to mean that the node i
dominates the node j , i.e., that there is a (possibly
empty) path from i to j . For a given node i , the set
of nodes dominated by i is the yield of i . We use
the notation .i/ to refer to the projection of i : the
yield of i , arranged in ascending order.
2.1 Dependency forests
Most of the literature on dependency grammar and
dependency parsing does not allow arbitrary de-
pendency graphs, but imposes certain structural
constraints on them. In this paper, we restrict our-
selves to dependency graphs that form forests.
Definition 2 A dependency forest is a dependency
graph with two additional properties:
1. it is acyclic (i.e., if i ! j , then not j ! i);
2. each of its nodes has at most one incoming
edge (i.e., if i ! j , then there is no node k
such that k ? i and k ! j ).
Nodes in a forest without an incoming edge are
called roots. A dependency forest with exactly one
root is a dependency tree.
Figure 1 shows a dependency forest taken from
PDT. It has two roots: node 2 (corresponding to the
complementizer proto) and node 8 (corresponding
to the final punctuation mark).
1We only consider unlabelled dependency graphs.
1 2 3 5 64 7 8
Nen? proto zapot?eb? uzav?rat nov? n?jemn? smlouvy .
contractsleasenewsignneededis-not therefore .
?It is therefore not needed to sign new lease contracts.?
Figure 1: Dependency forest for a Czech sentence
from the Prague Dependency Treebank
Some authors extend dependency forests by a
special root node with position 0, and add an edge
.0; i/ for every root node i of the remaining graph
(McDonald et al, 2005). This ensures that the ex-
tended graph always is a tree. Although such a
definition can be useful, we do not follow it here,
since it obscures the distinction between projectiv-
ity and planarity to be discussed in section 3.
2.2 Projectivity
In contrast to acyclicity and the indegree constraint,
both of which impose restrictions on the depen-
dency relation as such, the projectivity constraint
concerns the interaction between the dependency
relation and the positions of the nodes in the sen-
tence: it says that the nodes in a subtree of a de-
pendency graph must form an interval, where an
interval (with endpoints i and j ) is the set
?i; j ? WD f k 2 V j i  k and k  j g :
Definition 3 A dependency graph is projective, if
the yields of its nodes are intervals.
Since projectivity requires each node to dominate a
continuous substring of the sentence, it corresponds
to a ban on discontinuous constituents in phrase
structure representations.
Projectivity is an interesting constraint on de-
pendency structures both from a theoretical and
a practical perspective. Dependency grammars
that only allow projective structures are closely
related to context-free grammars (Gaifman, 1965;
Obre?bski and Gralin?ski, 2004); among other things,
they have the same (weak) expressivity. The pro-
jectivity constraint also leads to favourable pars-
ing complexities: chart-based parsing of projective
dependency grammars can be done in cubic time
(Eisner, 1996); hard-wiring projectivity into a de-
terministic dependency parser leads to linear-time
parsing in the worst case (Nivre, 2003).
508
3 Relaxations of projectivity
While the restriction to projective analyses has a
number of advantages, there is clear evidence that
it cannot be maintained for real-world data (Zeman,
2004; Nivre, 2006). For example, the graph in
Figure 1 is non-projective: the yield of the node 1
(marked by the dashed rectangles) does not form
an interval?the node 2 is ?missing?. In this sec-
tion, we present several proposals for structural
constraints that relax projectivity, and relate them
to each other.
3.1 Planarity and multiplanarity
The notion of planarity appears in work on Link
Grammar (Sleator and Temperley, 1993), where
it is traced back to Mel?c?uk (1988). Informally,
a dependency graph is planar, if its edges can be
drawn above the sentence without crossing. We
emphasize the word above, because planarity as
it is understood here does not coincide with the
standard graph-theoretic concept of the same name,
where one would be allowed to also use the area
below the sentence to disentangle the edges.
Figure 2a shows a dependency graph that is pla-
nar but not projective: while there are no crossing
edges, the yield of the node 1 (the set f1; 3g) does
not form an interval.
Using the notation linked.i; j / as an abbrevia-
tion for the statement ?there is an edge from i to j ,
or vice versa?, we formalize planarity as follows:
Definition 4 A dependency graph is planar, if it
does not contain nodes a; b; c; d such that
linked.a; c/ ^ linked.b; d/ ^ a < b < c < d :
Yli-Jyr? (2003) proposes multiplanarity as a gen-
eralization of planarity suitable for modelling de-
pendency analyses, and evaluates it experimentally
using data from DDT.
Definition 5 A dependency graph G D .V I E/ is
m-planar, if it can be split into m planar graphs
G1 D .V I E1/; : : : ;Gm D .V I Em/
such that E D E1]  ]Em. The planar graphs Gi
are called planes.
As an example of a dependency forest that is 2-
planar but not planar, consider the graph depicted in
Figure 2b. In this graph, the edges .1; 4/ and .3; 5/
are crossing. Moving either edge to a separate
graph partitions the original graph into two planes.
1 2 3
(a) 1-planar
1 2 3 4 5
(b) 2-planar
Figure 2: Planarity and multi-planarity
3.2 Gap degree and well-nestedness
Bodirsky et al (2005) present two structural con-
straints on dependency graphs that characterize
analyses corresponding to derivations in Tree Ad-
joining Grammar: the gap degree restriction and
the well-nestedness constraint.
A gap is a discontinuity in the projection of a
node in a dependency graph (Pl?tek et al, 2001).
More precisely, let i be the projection of the
node i . Then a gap is a pair .jk ; jkC1/ of nodes
adjacent in i such that jkC1   jk > 1.
Definition 6 The gap degree of a node i in a de-
pendency graph, gd.i/, is the number of gaps in i .
As an example, consider the node labelled i in the
dependency graphs in Figure 3. In Graph 3a, the
projection of i is an interval (.2; 3; 4/), so i has gap
degree 0. In Graph 3b, i D .2; 3; 6/ contains a
single gap (.3; 6/), so the gap degree of i is 1. In
the rightmost graph, the gap degree of i is 2, since
i D .2; 4; 6/ contains two gaps (.2; 4/ and .4; 6/).
Definition 7 The gap degree of a dependency
graph G, gd.G/, is the maximum among the gap
degrees of its nodes.
Thus, the gap degree of the graphs in Figure 3
is 0, 1 and 2, respectively, since the node i has the
maximum gap degree in all three cases.
The well-nestedness constraint restricts the posi-
tioning of disjoint subtrees in a dependency forest.
Two subtrees are called disjoint, if neither of their
roots dominates the other.
Definition 8 Two subtrees T1;T2 interleave, if
there are nodes l1; r1 2 T1 and l2; r2 2 T2 such
that l1 < l2 < r1 < r2. A dependency graph is
well-nested, if no two of its disjoint subtrees inter-
leave.
Both Graph 3a and Graph 3b are well-nested.
Graph 3c is not well-nested. To see this, let T1
be the subtree rooted at the node labelled i , and
let T2 be the subtree rooted at j . These subtrees
interleave, as T1 contains the nodes 2 and 4, and T2
contains the nodes 3 and 5.
509
ji
1 2 3 5 64
(a) gd D 0, ed D 0, wnC
j
i
1 2 3 5 64
(b) gd D 1, ed D 1, wnC
j
i
1 2 3 5 64
(c) gd D 2, ed D 1, wn 
Figure 3: Gap degree, edge degree, and well-nestedness
3.3 Edge degree
The notion of edge degree was introduced by Nivre
(2006) in order to allow mildly non-projective struc-
tures while maintaining good parsing efficiency in
data-driven dependency parsing.2
Define the span of an edge .i; j / as the interval
S..i; j // WD ?min.i; j /;max.i; j /? :
Definition 9 Let G D .V I E/ be a dependency
forest, let e D .i; j / be an edge in E, and let Ge
be the subgraph of G that is induced by the nodes
contained in the span of e.
 The degree of an edge e 2 E, ed.e/, is the
number of connected components c in Ge
such that the root of c is not dominated by
the head of e.
 The edge degree of G, ed.G/, is the maximum
among the degrees of the edges in G.
To illustrate the notion of edge degree, we return
to Figure 3. Graph 3a has edge degree 0: the only
edge that spans more nodes than its head and its de-
pendent is .1; 5/, but the root of the connected com-
ponent f2; 3; 4g is dominated by 1. Both Graph 3b
and 3c have edge degree 1: the edge .3; 6/ in
Graph 3b and the edges .2; 4/, .3; 5/ and .4; 6/ in
Graph 3c each span a single connected component
that is not dominated by the respective head.
3.4 Related work
Apart from proposals for structural constraints re-
laxing projectivity, there are dependency frame-
works that in principle allow unrestricted graphs,
but provide mechanisms to control the actually per-
mitted forms of non-projectivity in the grammar.
The non-projective dependency grammar of Ka-
hane et al (1998) is based on an operation on de-
pendency trees called lifting: a ?lift? of a tree T is
the new tree that is obtained when one replaces one
2We use the term edge degree instead of the original simple
term degree from Nivre (2006) to mark the distinction from
the notion of gap degree.
or more edges .i; k/ in T by edges .j ; k/, where
j ! i . The exact conditions under which a cer-
tain lifting may take place are specified in the rules
of the grammar. A dependency tree is acceptable,
if it can be lifted to form a projective graph.3
A similar design is pursued in Topological De-
pendency Grammar (Duchier and Debusmann,
2001), where a dependency analysis consists of
two, mutually constraining graphs: the ID graph
represents information about immediate domi-
nance, the LP graph models the topological struc-
ture of a sentence. As a principle of the grammar,
the LP graph is required to be a lift of the ID graph;
this lifting can be constrained in the lexicon.
3.5 Discussion
The structural conditions we have presented here
naturally fall into two groups: multiplanarity, gap
degree and edge degree are parametric constraints
with an infinite scale of possible values; planarity
and well-nestedness come as binary constraints.
We discuss these two groups in turn.
Parametric constraints With respect to the
graded constraints, we find that multiplanarity is
different from both gap degree and edge degree
in that it involves a notion of optimization: since
every dependency graph is m-planar for some suf-
ficiently large m (put each edge onto a separate
plane), the interesting question in the context of
multiplanarity is about the minimal values for m
that occur in real-world data. But then, one not
only needs to show that a dependency graph can be
decomposed into m planar graphs, but also that this
decomposition is the one with the smallest number
of planes among all possible decompositions. Up
to now, no tractable algorithm to find the minimal
decomposition has been given, so it is not clear how
to evaluate the significance of the concept as such.
The evaluation presented by Yli-Jyr? (2003) makes
use of additional constraints that are sufficient to
make the decomposition unique.
3We remark that, without restrictions on the lifting, every
non-projective tree has a projective lift.
510
1 2 3 5 64
(a) gd D 2, ed D 1
1 2 3 54
(b) gd D 1, ed D 2
Figure 4: Comparing gap degree and edge degree
The fundamental difference between gap degree
and edge degree is that the gap degree measures the
number of discontinuities within a subtree, while
the edge degree measures the number of interven-
ing constituents spanned by a single edge. This
difference is illustrated by the graphs displayed in
Figure 4. Graph 4a has gap degree 2 but edge de-
gree 1: the subtree rooted at node 2 (marked by
the solid edges) has two gaps, but each of its edges
only spans one connected component not domi-
nated by 2 (marked by the squares). In contrast,
Graph 4b has gap degree 1 but edge degree 2: the
subtree rooted at node 2 has one gap, but this gap
contains two components not dominated by 2.
Nivre (2006) shows experimentally that limiting
the permissible edge degree to 1 or 2 can reduce the
average parsing time for a deterministic algorithm
from quadratic to linear, while omitting less than
1% of the structures found in DDT and PDT. It
can be expected that constraints on the gap degree
would have very similar effects.
Binary constraints For the two binary con-
straints, we find that well-nestedness subsumes
planarity: a graph that contains interleaving sub-
trees cannot be drawn without crossing edges, so
every planar graph must also be well-nested. To see
that the converse does not hold, consider Graph 3b,
which is well-nested, but not planar.
Since both planarity and well-nestedness are
proper extensions of projectivity, we get the fol-
lowing hierarchy for sets of dependency graphs:
projective Proceedings of the 10th Conference on Parsing Technologies, pages 168?170,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Data-Driven Dependency Parsing across Languages and Domains:
Perspectives from the CoNLL 2007 Shared Task
Joakim Nivre
Va?xjo? University, School of Mathematics and Systems Engineering
Uppsala University, Department of Linguistics and Philology
E-mail: nivre@msi.vxu.se
Abstract
The Conference on Computational Natural
Language Learning features a shared task, in
which participants train and test their learn-
ing systems on the same data sets. In 2007,
as in 2006, the shared task has been devoted
to dependency parsing, this year with both a
multilingual track and a domain adaptation
track. In this paper, I summarize the main
findings from the 2007 shared task and try
to identify major challenges for the parsing
community based on these findings.
1 Introduction
The annual Conference on Computational Natural
Language Learning (CoNLL) has for the past nine
years organized a shared task, where participants
train and test their learning systems on the same
data sets. In 2006, the shared task was multilin-
gual dependency parsing, where participants had to
train and test a parser on data from thirteen differ-
ent languages (Buchholz and Marsi, 2006). In 2007,
the task was extended by adding a second track for
(monolingual) domain adaptation.
The CoNLL 2007 shared task on dependency
parsing featured two tracks:
? In the multilingual track, the task was to train a
parser using labeled data from Arabic, Basque,
Catalan, Chinese, Czech, English, Greek, Hun-
garian, Italian, and Turkish.
? In the domain adaptation track, the task was
to adapt a parser for English news text to other
domains using unlabeled data from the target
domains: biomedical and chemical abstracts,
parent-child dialogues.1 In the closed class, the
base parser had to be trained using the English
training set for the multilingual track and no
external resources were allowed. In the open
class, any base parser could be used and any
external resources were allowed.
Both tracks used the same column-based format for
labeled data with six input columns and two output
columns for each word of a sentence:
? Input: word-id, word form, lemma, coarse part
of speech, fine part-of-speech, morphosyntactic
features.
? Output: head (word-id), dependency label.
The main evaluation metric for both tracks was the
labeled attachment score (LAS), i.e., the percentage
of words that have been assigned the correct head
and dependency label. For more information about
the setup, see Nivre et al (2007)
In this paper, I will summarize the main findings
from the CoNLL 2007 shared task, starting with
a characterization of the different approaches used
(section 2), and moving on to the most interesting
results in the multilingual track (section 3) and the
domain adaptation track (section 4). Finally, based
on these findings, I will try to identify some im-
portant challenges for the wider parsing community
(section 5).
1The biomedical domain was the development domain,
which means that a small labeled development set was available
for this domain. The final testing was only done on chemical
abstracts and (optionally) parent-child dialogues.
168
2 Approaches
In total, test runs were submitted for twenty-three
systems in the multilingual track, and ten systems in
the domain adaptation track (six of which also par-
ticipated in the multilingual track). The majority of
these systems used models belonging to one of the
two dominant approaches in data-driven dependency
parsing in recent years (McDonald and Nivre, 2007):
? In graph-based models, every possible depen-
dency graph for a given input sentence is given
a score that decomposes into scores for the arcs
of the graph. The optimal parse can be found
using a spanning tree algorithm (Eisner, 1996;
McDonald et al, 2005).
? In transition-based models, dependency graphs
are modeled by sequences of parsing actions
(or transitions) for building them. The search
for an optimal parse is often deterministic and
guided by classifiers (Yamada and Matsumoto,
2003; Nivre, 2003).
The majority of graph-based parsers in the shared
task were based on what McDonald and Pereira
(2006) call the first-order model, where the score
of each arc is independent of every other arc, but
there were also attempts at exploring higher-order
models, either with exact inference limited to pro-
jective dependency graphs (Carreras, 2007), or with
approximate inference (Nakagawa, 2007). Another
innovation was the use of k-best spanning tree algo-
rithms for inference with a non-projective first-order
model (Hall et al, 2007b).
For transition-based parsers, the trend was clearly
to move away from deterministic parsing by adding
a probability model for scoring a set of candidate
parses typically derived using a heuristic search
strategy. The probability model may be either con-
ditional (Duan et al, 2007) or generative (Titov and
Henderson, 2007).
An interesting way of combining the two main
approaches is to use a graph-based model to build
an ensemble of transition-based parsers. This tech-
nique, first proposed by Sagae and Lavie (2006), was
used in the highest scoring system in both the mul-
tilingual track (Hall et al, 2007a) and the domain
adaptation track (Sagae and Tsujii, 2007).
3 Multilingual Parsing
The ten languages involved in the multilingual track
can be grouped into three classes with respect to the
best parsing accuracy achieved:
? Low (LAS = 76.3?76.9):
Arabic, Basque, Greek
? Medium (LAS = 79.2?80.2):
Czech, Hungarian, Turkish
? High (LAS = 84.4?89.6):
Catalan, Chinese, English, Italian
To a large extent, these classes appear to be definable
from typological properties. The class with the high-
est top scores contains languages with a rather im-
poverished morphology. Medium scores are reached
by the two agglutinative languages, Hungarian and
Turkish, as well as by Czech. The most difficult lan-
guages are those that combine a relatively free word
order with a high degree of inflection. Based on
these characteristics, one would expect to find Czech
in the last class. However, the Czech training set
is four times the size of the training set for Arabic,
which is the language with the largest training set
of the difficult languages. On the whole, however,
training set size alone is a poor predictor of parsing
accuracy, which can be seen from the fact that the
Italian training set is only about half the size of the
Arabic one and only one sixth of Czech one. Thus,
there seems to be a need for parsing methods that
can cope better with richly inflected languages.
4 Domain Adaptation
One result from the domain adaptation track that
may seem surprising at first was the fact that the
best closed class systems outperformed the best
open class systems on the official test set containing
chemical abstracts. To some extent, this may be ex-
plained by the greater number of participants in the
closed class (eight vs. four). However, it also seems
that the major problem in adapting existing, often
grammar-based, parsers to the new domain was not
the domain as such but the mapping from the native
output of the parser to the kind of annotation pro-
vided in the shared task data sets. In this respect,
the closed class systems had an advantage by having
been trained on exactly this kind of annotation. This
169
result serves to highlight the fact that domain adapta-
tion, as well as the integration of grammar-based and
data-driven methods, often involves transformations
between different kinds of linguistic representations.
The best performing (closed class) system in the
domain adaptation track used a combination of co-
learning and active learning by training two different
parsers on the labeled training data, parsing the un-
labeled domain data with both parsers, and adding
parsed sentences to the training data only if the two
parsers agreed on their analysis (Sagae and Tsujii,
2007). This resulted in a LAS of 81.1 on the test set
of chemical abstracts, to be compared with 89.0 for
the English test set in the multilingual track.
5 Conclusion
Based on the results from the CoNLL 2007 shared
task, it is clear that we need to improve our methods
for parsing richly inflected languages. We also need
to find better ways of integrating parsers developed
within different frameworks, so that they can be
reused effectively for, among other things, domain
adaptation. More generally, we need to increase our
knowledge of the multi-causal relationship between
language characteristics, syntactic representations,
and parsing and learning methods. In order to do
this, perhaps we also need a shared task at the Inter-
national Conference on Parsing Technologies.
Acknowledgments
I want to thank my fellow organizers of the shared
task, Johan Hall, Sandra Ku?bler, Ryan McDonald,
Jens Nilsson, Sebastian Riedel, and Deniz Yuret,
who are also co-authors of the longer paper on which
this paper is partly based (Nivre et al, 2007). I am
also indebted to all the people who have contributed
to the shared task by providing data or participating.
References
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
CoNLL, 149?164.
X. Carreras. 2007. Experiments with a high-order pro-
jective dependency parser. In Proc. of EMNLP-CoNLL
(Shared Task).
X. Duan, J. Zhao, and B. Xu. 2007. Probabilistic parsing
action models for multi-lingual dependency parsing.
In Proc. of EMNLP-CoNLL (Shared Task).
J. M. Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proc. of COL-
ING, 340?345.
J. Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi,
M. Nilsson, and M. Saers. 2007a. Single malt or
blended? A study in multilingual parser optimization.
In Proc. of EMNLP-CoNLL (Shared Task).
K. Hall, J. Havelka, and D. Smith. 2007b. Log-linear
models of non-projective trees, k-best MST parsing
and tree-ranking. In Proc. of EMNLP-CoNLL (Shared
Task).
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proc. of EMNLP-CoNLL.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Proc.
of EACL, 81?88.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. of HLT/EMNLP, 523?530.
T. Nakagawa. 2007. Multilingual dependency parsing
using gibbs sampling. In Proc. of EMNLP-CoNLL
(Shared Task).
J. Nivre and J. Nilsson. 2005. Pseudo-projective depen-
dency parsing. In Proc. of ACL, 99?106.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc. of
EMNLP-CoNLL (Shared Task).
J. Nivre. 2003. An efficient algorithm for projective de-
pendency parsing. In Proc. of IWPT, 149?160.
K. Sagae and A. Lavie. 2006. Parser combination by
reparsing. In Proc. of HLT-NAACL (Short Papers),
129?132.
K. Sagae and J. Tsujii. 2007. Dependency parsing and
domain adaptation with LR models and parser ensem-
bles. In Proc. of EMNLP-CoNLL (Shared Task).
I. Titov and J. Henderson. 2007. Fast and robust mul-
tilingual dependency parsing with a generative latent
variable model. In Proc. of EMNLP-CoNLL (Shared
Task).
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
of IWPT, 195?206.
170
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 93?101,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Learning Where to Look: Modeling Eye Movements in Reading
Mattias Nilsson
Department of Linguistics and Philology
Uppsala University
mattias.nilsson@lingfil.uu.se
Joakim Nivre
Department of Linguistics and Philology
Uppsala University
joakim.nivre@lingfil.uu.se
Abstract
We propose a novel machine learning task that
consists in learning to predict which words in
a text are fixated by a reader. In a first pilot
experiment, we show that it is possible to out-
perform a majority baseline using a transition-
based model with a logistic regression classi-
fier and a very limited set of features. We also
show that the model is capable of capturing
frequency effects on eye movements observed
in human readers.
1 Introduction
Any person engaged in normal skilled reading pro-
duces an alternating series of rapid eye movements
and brief fixations that forms a rich and detailed be-
havioral record of the reading process. In the last
few decades a great deal of experimental evidence
has accumulated to suggest that the eye movements
of readers are reflective of ongoing language pro-
cessing and thus provide a useful source of infor-
mation for making inferences about the linguistic
processes involved in reading (Clifton et al, 2007).
In psycholinguistic research, eye movement data is
now commonly used to study how experimental ma-
nipulations of linguistic stimuli manifest themselves
in the eye movement record.
Another related strand of research primarily at-
tempts to understand what determines when and
where the eyes move during reading. This line of
research has led to mathematically well specified ac-
counts of eye movement control in reading being
instantiated as computational models (Legge et al,
1997; Reichle et al, 1998; Salvucci, 2001; Engbert
et al, 2002; McDonald et al, 2005; Feng, 2006;
Reilly and Radach, 2006; Yang, 2006). (For a re-
cent overview, see (Reichle, 2006).) These models
receive text as input and produce predictions for the
location and duration of eye fixations, in approxima-
tion to human reading behavior. Although there are
substantial differences between the various models,
they typically combine both mechanisms of visuo-
motor control and linguistic processing. Two impor-
tant points of divergence concern the extent to which
language processing influences eye movements and
whether readers process information from more than
one word at a time (Starr and Rayner, 2001). More
generally, the models that have emerged to date are
based on different sets of assumptions about the un-
derlying perceptual and cognitive mechanisms that
control eye movements. The most influential model
so far, the E-Z Reader model (Reichle et al, 1998;
Reichle et al, 2003; Pollatsek et al, 2006), rests on
the assumptions that cognitive / lexical processing is
the engine that drives the eyes through the text and
that words are identified serially, one at a time.
Although eye movement models typically have
parameters that are fitted to empirical data sets, they
are not based on machine learning in the standard
sense and their predictions are hardly ever tested on
unseen data. Moreover, their predictions are nor-
mally averaged over a whole group of readers or
words belonging to a given frequency class. In this
study, however, we investigate whether saccadic eye
movements during reading can be modeled using
machine learning. The task we propose is to learn
to predict the eye movements of an individual reader
reading a specific text, using as training data the eye
93
movements recorded for the same person reading
other texts.
Predicting the eye movements of an individual
reader on new texts is arguably a hard problem, and
we therefore restrict the task to predicting word-
based fixations (but not the duration of these fixa-
tions) and focus on a first pilot experiment inves-
tigating whether we can outperform a reasonable
baseline on this task. More precisely, we present ex-
perimental results for a transition-based model, us-
ing a log-linear classifier, and show that the model
significantly outperforms the baseline of always pre-
dicting the most frequent saccade. In addition, we
show that even this simple model is able to capture
frequency effects on eye movements observed in hu-
man readers.
We want to emphasize that the motivation for this
modeling experiment is not to advance the state of
the art in computational modeling of eye movements
during reading. For this our model is far too crude
and limited in scope. The goal is rather to propose a
novel approach to the construction and evaluation of
such models, based on machine learning and model
assessment on unseen data. In doing this, we want
to establish a reasonable baseline for future research
by evaluating a simple model with a restricted set
of features. In future studies, we intend to inves-
tigate how results can be improved by introducing
more complex models as well as a richer feature
space. More generally, the machine learning ap-
proach explored here places emphasis on modeling
eye movement behavior with few a priori assump-
tions about underlying cognitive and physiological
mechanisms.
The rest of the paper is structured as follows. Sec-
tion 2 provides a brief background on basic charac-
teristics of eye movements in reading. The emphasis
is on saccadic eye movements rather than on tempo-
ral aspects of fixations. Section 3 defines the novel
task of learning to predict fixations during reading
and discusses different evaluation metrics for this
task. Section 4 presents a transition-based model
for solving this task, using a log-linear classifier to
predict the most probable transition after each fixa-
tion. Section 5 presents experimental results for the
model using data from the Dundee corpus (Kennedy
and Pynte, 2005), and Section 6 contains conclu-
sions and suggestions for future research.
2 Eye Movements in Reading
Perhaps contrary to intuition, the eyes of readers do
not move smoothly across a line or page of text. It is
a salient fact in reading research that the eyes make
a series of very rapid ballistic movements (called
saccades) from one location to another. In between
saccades, the eyes remain relatively stationary for
brief periods of time (fixations). Most fixations last
about 200-300 ms but there is considerable variabil-
ity, both between and within readers. Thus, some
fixations last under 100 ms while others last over
500 ms (Rayner, 1998). Much of the variability in
fixation durations appears associated to processing
ease or difficulty.
The number of characters that is within the re-
gion of effective vision on any fixation is known as
the perceptual span. For English readers, the per-
ceptual span extends approximately four characters
to the left and fifteen characters to the right of the
fixation. Although readers fixate most words in a
text, many words are also skipped. Approximately
85% of the content words are fixated and 35% of
the function words (Carpenter and Just, 1983). Vari-
ables known to influence the likelihood of skipping
a word are word length, frequency and predictabil-
ity. Thus, more frequent words in the language are
skipped more often than less frequent words. This is
true also when word length is controlled for. Simi-
larly, words that occur in constrained contexts (and
are thus more predictable) are skipped more often
than words in less constrained contexts.
Although the majority of saccades in reading is
relatively local, i.e., target nearby words, more dis-
tant saccades also occur. Most saccades move the
eyes forward approximately 7?9 character spaces.
Approximately 15% of the saccades, however, are
regressions, in which the eyes move back to earlier
parts of the text (Rayner, 1998). It has long been
established that the length of saccades is influenced
by both the length of the fixated word and the word
to the right of the fixation (O?Regan, 1979). Re-
gressions often go back one or two words, but occa-
sionally they stretch further back. Such backward
movements are often thought to reflect linguistic
processing difficulty, e.g., because of syntactic pars-
ing problems. Readers, however, are often unaware
of making regressions, especially shorter ones.
94
3 The Learning Task
We define a text T as a sequence of word tokens
(w1, . . . , wn), and we define a fixation sequence
F for T as a sequence of token positions in T
(i1, . . . , im) (1 < ik < n). The fixation set S(F )
corresponding to F is the set of token positions that
occur in F . For example, the text Mary had a lit-
tle lamb is represented by T = (Mary, had, a, little,
lamb); a reading of this text where the sequence of
fixations is Mary ? little ? Mary ? lamb is repre-
sented by F = (1, 4, 1, 5); and the corresponding
fixation set is S(F ) = {1, 4, 5}.
The task we now want to consider is the one
of predicting the fixation sequence F for a spe-
cific reading event E involving person P reading
text T . The training data consist of fixation se-
quences F1, . . . , Fk for reading events distinct from
E involving the same person P but different texts
T1, . . . , Tk. The performance of a model M is eval-
uated by comparing the predicted fixation sequence
FM to the fixation sequence FO observed in a read-
ing experiment involving P and T . Here are some
of the conceivable metrics for this evaluation:
1. Fixation sequence similarity: How similar
are the sequences FM and FO, as measured, for
example, by some string similarity metric?
2. Fixation accuracy: How large is the agree-
ment between the sets S(FM ) and S(FO), as
measured by 0-1-loss over the entire text, i.e.,
how large is the proportion of positions that are
either in both S(FM ) and S(FO) (fixated to-
kens) or in neither (skipped tokens). This can
also be broken down into precision and recall
for fixated and skipped tokens, respectively.
3. Fixation distributions: Does the model pre-
dict the correct proportion of fixated and
skipped tokens, as measured by the difference
between |S(FM )|/|T | and |S(FO)|/|T |? This
can also be broken down by frequency classes
of words, to see if the model captures frequency
effects reported in the literature.
These evaluation metrics are ordered by an implica-
tional scale from hardest to easiest. Thus, a model
that correctly predicts the exact fixation sequence
also makes correct predictions with respect to the
set of words fixated and the number of words fixated
(but not vice versa). In the same fashion, a model
that correctly predicts which words are fixated (but
not the exact sequence) also correctly predicts the
number of words fixated.
In the experiments reported in Section 5, we will
use variants of the latter two metrics and compare
the performance of our model to the baseline of al-
ways predicting the most frequent type of saccade
for the reader in question. We will report results
both for individual readers and mean scores over all
readers in the test set. The evaluation of fixation se-
quence similarity (the first type of metric) will be
left for future work.
4 A Transition-Based Model
When exploring a new task, we first have to decide
what kind of model to use. As stated in the introduc-
tion, we regard this as a pilot experiment to establish
the feasibility of the task and have therefore chosen
to start with one of the simplest models possible and
see whether we can beat the baseline of always pre-
dicting the most frequent saccade. Since the task
consists in predicting a sequence of different actions,
it is very natural to use a transition-based model,
with configurations representing fixation states and
transitions representing saccadic movements. Given
such a system, we can train a classifier to predict the
next transition given the information in the current
configuration. In order to derive a complete tran-
sition sequence, we start in an initial configuration,
representing the reader?s state before the first fixa-
tion, and repeatedly apply the transition predicted by
the classifier until we reach a terminal state, repre-
senting the reader?s state after having read the entire
text. At an abstract level, this is essentially the same
idea as in transition-based dependency parsing (Ya-
mada and Matsumoto, 2003; Nivre, 2006; Attardi,
2006). In the following subsections, we discuss the
different components of the model in turn, including
the transition system, the classifier used, the features
used to represent data, and the search algorithm used
to derive complete transition sequences.
4.1 Transition System
A transition system is an abstract machine consist-
ing of a set of configurations and transitions between
95
configurations. A configuration in the current sys-
tem is a triple C = (L,R, F ), where
1. L is a list of tokens representing the left con-
text, including the currently fixated token and
all preceding tokens in the text.
2. R is a list of tokens representing the right con-
text, including all tokens following the cur-
rently fixated token in the text.
3. F is a list of token positions, representing the
fixation sequence so far, including the currently
fixated token.
For example, if the text to be read is Mary had a
little lamb, then the configuration
([Mary,had,a,little], [lamb], [1,4])
represents the state of a reader fixating the word little
after first having fixated the word Mary.
For any text T = w1 . . . wn, we define initial and
terminal configurations as follows:
1. Initial: C = ([ ], [w1, . . . , wn], [ ])
2. Terminal: C = ([w1, . . . , wn], [ ], F )
(for any F )
We then define the following transitions:1
1. Progress(n):
([?|wi], [wi+1, . . . , wi+n|?], [?|i])?
([?|wi, wi+1, . . . , wi+n], ?, [?|i, i+n])
2. Regress(n):
([?|wi?n, . . . , wi?1, wi], ?, [?|i])?
([?|wi?n], [wi?n+1, . . . , wi|?], [?|i, i?n])
3. Refixate:
([?|wi], ?, [?|i])? ([?|wi], ?, [?|i, i])
The transition Progress(n) models progressive sac-
cades of length n, which means that the next fixated
word is n positions forward with respect to the cur-
rently fixated word (i.e., n?1 words are skipped).
In a similar fashion, the transition Regress(n) mod-
els regressive saccades of length n. If the parameter
1We use the variables ?, ? and ? for arbitrary sublists of L,
R and F , respectively, and we write the L and F lists with their
tails to the right, to maintain the natural order of words.
n of either Progress(n) or Regress(n) is greater than
the number of words remaining in the relevant di-
rection, then the longest possible movement is made
instead, in which case Regress(n) leads to a terminal
configuration while Progress(n) leads to a configu-
ration that is similar to the initial configuration in
that it has an empty L list. The transition Refixate,
finally, models refixations, that is, cases where the
next word fixated is the same as the current.
To illustrate how this system works, we may con-
sider the transition sequence corresponding to the
reading of the text Mary had a little lamb used as
an example in Section 3:2
Init ? ([ ], [Mary, . . . , lamb], [ ])
P(1) ? ([Mary], [had, . . . , lamb], [1])
P(3) ? ([Mary, . . . , little], [lamb], [1,4])
R(3) ? ([Mary], [had, . . . , lamb], [1,4,1])
P(4) ? ([Mary, . . . , lamb], [ ], [1,4,1,5])
4.2 Learning Transitions
The transition system defined in the previous section
specifies the set of possible saccade transitions that
can be executed during the reading of a text, but it
does not say anything about the probability of dif-
ferent transitions in a given configuration, nor does
it guarantee that a terminal configuration will ever
be reached. The question is now whether we can
learn to predict the most probable transition in such
a way that the generated transition sequences model
the behavior of a given reader. To do this we need
to train a classifier that predicts the next transition
for any configuration, using as training data the ob-
served fixation sequences of a given reader. Before
that, however, we need to decide on a feature repre-
sentation for configurations.
Features used in this study are listed in Table 1.
We use the notation L[i] to refer to the ith token
in the list L and similarly for R and F . The first
two features refer to properties of the currently fix-
ated token. Length is simply the character length
of the word, while frequency class is an index of
the word?s frequency of occurrence in representative
text. Word frequencies are based on occurrences in
the Bristish National Corpus (BNC) and divided into
2We abbreviate Progress(n) and Regress(n) to P(n) and
R(n), respectively.
96
Feature Description
CURRENT.LENGTH The length of the token L[1]
CURRENT.FREQUENCYCLASS The frequency class of the token L[1]
NEXT.LENGTH The length of the token R[1]
NEXT.FREQUENCYCLASS The frequency class of the token R[1]
NEXTPLUSONE.LENGTH The length of the token R[2]
NEXTPLUSTWO.LENGTH The length of the token R[3]
DISTANCE.ONETOTWO The distance, in tokens, between F [1] and F [2]
DISTANCE.TWOTOTHREE The distance, in tokens, between F [2] and F [3]
Table 1: Features defined over fixation configurations. The notation L[i] is used to denote the ith element of list L.
five classes. Frequencies were computed per million
words in the ranges 1?10, 11?100, 101?1000, 1001?
10000, and more than 10000.
The next four features define features of tokens
to the right of the current fixation. For the to-
ken immediately to the right, both length and fre-
quency are recorded whereas only length is con-
sidered for the two following tokens. The last
two features are defined over tokens in the fixa-
tion sequence built thus far and record the history
of the two most recent saccade actions. The first
of these (DISTANCE.ONETOTWO) defines the sac-
cade distance, in number of tokens, that led up
to the token currently being fixated. The second
(DISTANCE.TWOTOTHREE), defines the next most
recent saccade distance, that led up to the previous
fixation. For these two features the following holds.
If the distance is positive, the saccade is progressive,
if the distance is negative, the saccade is regressive,
and if the distance amounts to zero, the saccade is a
refixation.
The small set of features used in the current model
were chosen to reflect experimental evidence on eye
movements in reading. Thus, for example, as noted
in section 2, it is a well-documented fact that short,
frequent and predictable words tend to be skipped.
The last two features are included in the hope of
capturing some of the dynamics in eye movement
behavior, for example, if regressions are more likely
to occur after longer progressive saccades, or if the
next word is skipped more often if the current word
is refixated. Still, it is clear that this is only a tiny
subset of the feature space that might be considered,
and it remains an important topic for future research
to further explore this space and to study the impact
of different features.
Given our feature representation, and given some
training data derived from reading experiments, it
is straightforward to train a classifier for predicting
the most probable transition out of any configura-
tion. There are many learning algorithms that could
be used for this purpose, but in the pilot experiments
we only make use of logistic regression.
4.3 Search Algorithm
Once we have trained a classifier f that predicts the
next transition f(C) out of any configuration C, we
can simulate the eye movement behavior of a person
reading the text T = (w1, . . . , wn) using the follow-
ing simple search algorithm:
1. Initialize C to ([ ], [w1, . . . , wn], [ ]).
2. While C is not terminal, apply f(C) to C.
3. Return F of C.
It is worth noting that search will always terminate
once a terminal configuration has been reached, even
though there is nothing in the transition system that
forbids transitions out of terminal configurations. In
other words, while the model itself allows regres-
sions and refixations after the last word of the text
has been fixated, the search algorithm does not. This
seems like a reasonable approximation for this pilot
study.
5 Experiments
5.1 Experimental Setup
The experiments we report are based on data from
the English section of the Dundee corpus. This sec-
97
Fixation Accuracy Fixations Skips
Reader # sentences Baseline Model Prec Rec F1 Prec Rec F1
a 136 53.3 70.0 69.9 73.8 71.8 69.0 65.8 67.4
b 156 55.7 66.5 65.2 85.8 74.1 70.3 80.4 75.0
c 151 59.9 70.9 72.5 82.8 77.3 67.4 53.1 59.4
d 162 69.0 78.9 84.7 84.8 84.7 66.0 65.8 65.9
e 182 51.7 71.8 69.1 78.4 73.5 75.3 65.2 69.9
f 157 63.5 67.9 70.9 83.7 76.8 58.7 40.2 47.7
g 129 43.3 56.6 49.9 80.8 61.7 72.2 38.1 49.9
h 143 57.6 66.9 69.4 76.3 72.7 62.8 54.3 58.2
i 196 56.4 69.1 69.6 80.3 74.6 68.2 54.7 60.7
j 166 66.1 76.3 82.2 81.9 82.0 65.0 65.4 65.2
Average 157.8 57.7 69.5 70.3 80.9 75.2 67.5 58.3 62.6
Table 2: Fixation and skipping accuracy on test data; Prec = precision, Rec = recall, F1 = balanced F measure.
tion contains the eye tracking record of ten partici-
pants reading editorial texts from The Independent
newspaper. The corpus contains 20 texts, each of
which were read by all participants. Participants also
answered a set of multiple-choice comprehension
questions after having finished reading each text.
The corpus consists of 2379 sentences, 56212 tokens
and 9776 types. The data was recorded using a Dr.
Bouis Oculometer Eyetracker, sampling the position
of the right eye every millisecond (see Kennedy and
Pynte, 2005, for further details).
For the experiments reported here, the corpus was
divided into three data sets: texts 1-16 for training
(1911 sentences), texts 17-18 for development and
validation (237 sentences), and the last two texts 19-
20 for testing (231 sentences).
Since we want to learn to predict the observed
saccade transition for any fixation configuration,
where configurations are represented as feature vec-
tors, it is not possible to use the eye tracking data
directly as training and test data. Instead, we simu-
late the search algorithm on the corpus data of each
reader in order to derive, for each sentence, the fea-
ture vectors over the configurations and the tran-
sitions corresponding to the observed fixation se-
quence. The instances to be classified then consist of
feature representations of configurations while the
classes are the possible transitions.
To somewhat simplify the learning task in this
first study, we removed all instances of non-local
saccades prior to training. Progressions stretching
further than five words ahead of the current fixation
were removed, as were regressions stretching further
back than two words. Refixations were not removed.
Thus we reduced the number of prediction classes to
eight. Removal of the non-local saccade instances
resulted in a 1.72% loss over the total number of in-
stances in the training data for all readers.
We trained one classifier for each reader using lo-
gistic regression, as implemented in Weka (Witten
and Eibe, 2005) and default options. In addition, we
trained majority baseline classifiers for all readers.
These models always predict the most frequent sac-
cadic eye movement for a given reader.
The classifiers were evaluated with respect to the
accuracy achieved when reading previously unseen
text using the search algorithm in 4.3. To ensure
that test data were consistent with training data, sen-
tences including any saccade outside of the local
range were removed prior to test. This resulted
in removal of 18.9% of the total number of sen-
tences in the test data for all readers. Accuracy was
measured in three different ways. First, we com-
puted the fixation accuracy, that is, the proportion
of words that were correctly fixated or skipped by
the model, which we also broke down into precision
and recall for fixations and skips separately.3 Sec-
ondly, we compared the predicted fixation distribu-
3Fixation/skip precision is the proportion of tokens fix-
ated/skipped by the model that were also fixated/skipped by
the reader; fixation/skip recall is the proportion of tokens fix-
ated/skipped by the reader that were also fixated/skipped by the
model.
98
tions to the observed fixation distributions, both over
all words and broken down into the same five fre-
quency classes that were used as features (see Sec-
tion 4). The latter statistics, averaged over all read-
ers, allow us to see whether the model correctly pre-
dicts the frequency effect discussed in section 2.
5.2 Results and Discussion
Table 2 shows the fixation accuracy, and precision,
recall and F1 for fixations and skips, for each of the
ten different models and the average across all mod-
els (bottom row). Fixation accuracy is compared to
the baseline of always predicting the most frequent
saccade type (Progress(2) for readers a and e, and
Progress(1) for the rest).
If we consider the fixation accuracy, we see that
all models improve substantially on the baseline
models. The mean difference between models and
baselines is highly significant (p < .001, paired t-
test). The relative improvement ranges from 4.4 per-
centage points in the worst case (model of reader f )
to 20.1 percentage points in the best case (model of
reader e). The highest scoring model, the model of
reader d, has an accuracy of 78.9%. The lowest scor-
ing model, the model of reader g, has an accuracy
of 56.6%. This is also the reader for whom there
is the smallest number of sentences in the test data
(129), which means that a large number of sentences
were removed prior to testing because of the greater
number of non-local saccades made by this reader.
Thus, this reader has an unusually varied saccadic
behaviour which is particularly hard to model.
Comparing the precision and recall for fixation
and skips, we see that while precision tends to be
about the same for both categories (with a few no-
table exceptions), recall is consistently higher for
fixations than for skips. We believe that this is due
to a tendency of the model to overpredict fixations,
especially for low-frequency words. This has a great
impact on the F1 measure (unweighted harmonic
mean of precision and recall), which is considerably
higher for fixations than for skips.
Figure 1 shows the distributions of fixations
grouped by reader and model. The models appear
reasonably good at adapting to the empirical fixa-
tion distribution of individual readers. However, the
models typically tend to look at more words than the
readers, as noted above. This suggests that the mod-
els lack sufficient information to learn to skip words
more often. This might be overcome by introducing
features that further encourage skipping of words. In
addition to word length and word frequency, that are
already accounted for, n-gram probability could be
included as a measure of predictability, for example.
We also note that there is a strong linear relation
between the capability of fitting the empirical dis-
tribution well and achieving high fixation accuracy
(Pearson?s r: -0.91, as measured by taking the dif-
ferences of each pair of distributions and correlating
them with the fixation accuracy of the models).
Figure 2 shows the mean observed and predicted
fixation and skipping probability as a function of
word frequency class, averaged over all readers. As
seen here, model prediction is responsive to fre-
quency class in a fashion comparable to the read-
ers, although the predictions typically tend to exag-
gerate the observed frequency effect. In the lower
to medium classes (1?3), almost every word is fix-
ated. Then there is a clear drop in fixation proba-
bility for words in frequency class 4 which fits well
with the observed fixation probability. Finally there
is another drop in fixation probability for the most
frequent words (5). The skipping probabilities for
the different classes show the corresponding reverse
trend.
6 Conclusion
In this paper we have defined a new machine learn-
ing task where the goal is to learn the saccadic eye
movement behavior of individual readers in order
to predict the sequence of word fixations for novel
reading events. We have discussed different evalua-
tion metrics for this task, and we have established a
first benchmark by training and evaluating a simple
transition-based model using a log-linear classifier
to predict the next transition. The evaluation shows
that even this simple model, with features limited to
a few relevant properties in a small context window,
outperforms a majority baseline and captures some
of the word frequency effects on eye movements ob-
served in human readers.
This pilot study opens up a number of direc-
tions for future research. With respect to mod-
eling, we need to explore more complex models,
richer feature spaces, and alternative learning algo-
99
a b c d e f g h i j
ReaderModel
Pro
por
tion
0.0
0.2
0.4
0.6
0.8
Figure 1: Proportion of fixated tokens grouped by reader and model
F F
F
F
F
1 2 3 4 5
0.0
0.2
0.4
0.6
0.8
1.0
F F F
F
F
S S
S
S
S
S S
S
S
S
Fix
atio
n p
rob
abi
lity
Frequency class
F
F
S
S
Fixation ? Observed
Fixation ? Predicted
Skipping ? Observed
Skipping ? Predicted
Figure 2: Mean observed and predicted fixation and skipping probability for five frequency classes of words
rithms. For example, given the sequential nature
of the task, it seems natural to explore probabilistic
sequence models such as HMMs (see for example
Feng (2006)). With respect to evaluation, we need
to develop metrics that are sensitive to the sequential
behavior of models, such as the fixation sequence
similarity measure discussed in Section 3, and in-
vestigate to what extent results can be generalized
across readers. With respect to the task itself, we
need to introduce additional aspects of the reading
process, in particular the duration of fixations. By
pursuing these lines of research, we should be able
to gain a better understanding of how machine learn-
ing methods in eye movement modeling can inform
and advance current theories and models in reading
and psycholinguistic research.
100
References
Giuseppe Attardi. 2006. Experiments with a multilan-
guage non-projective dependency parser. In Proceed-
ings of the 10th Conference on Computational Natural
Language Learning (CoNLL), pages 166?170.
Patricia A. Carpenter and Marcel A. Just. 1983. What
your eyes do while your mind is reading. In Keith
Rayner, editor, Eye movements in reading: Perceptual
and language processes, pages 275?307. New York:
Academic Press.
Charles Clifton, Adrian Staub, and Keith Rayner. 2007.
Eye movements in reading words and sentences. In
Roger van Gompel, editor, Eye movements: A window
on mind and brain, pages 341?372. Amsterdam: Else-
vier.
Ralf Engbert, Andr? Longtin, and Reinhold Kliegl. 2002.
A dynamical model of saccade generation in reading
based on spatially distributed lexical processing. Vi-
sion Research, 42:621?636.
Gary Feng. 2006. Eye movements as time-series random
variables: A stochastic model of eye movement con-
trol in reading. Cognitive Systems Research, 7:70?95.
Alan Kennedy and Jo?l Pynte. 2005. Parafoveal-on-
foveal effects in normal reading. Vision research,
45:153?168.
Gordon E. Legge, Timothy S. Klitz, and Bosco S. Tjan.
1997. Mr. Chips: An ideal-observer model of reading.
Psychological Review, 104:524?553.
Scott A. McDonald, R.H.S. Carpenter, and Richard C.
Schillcock. 2005. An anatomically-constrained,
stochastic model of eye movement control in reading.
Psychological Review, 112:814?840.
Joakim Nivre. 2006. Inductive Dependency Parsing.
Springer.
J. Kevin O?Regan. 1979. Eye guidance in reading: Evi-
dence for the linguistic control hypothesis. Perception
& Psychophysics, 25:501?509.
Alexander Pollatsek, Erik Reichle, and Keith Rayner.
2006. Tests of the E-Z Reader model: Exploring the
interface between cognition and eye movements.
Keith Rayner. 1998. Eye movements in reading and in-
formation processing: 20 years of research. Psycho-
logical Bulletin, 124:372?422.
Erik Reichle, Alexander Pollatsek, Donald Fisher, and
Keith Rayner. 1998. Toward a model of eye
movement control in reading. Psychological Review,
105:125?157.
Erik Reichle, Keith Rayner, and Alexander Pollatsek.
2003. The E-Z Reader model of eye-movement con-
trol in reading: Comparisons to other models. Behav-
ioral and Brain Sciences, 26:445?476.
Eric Reichle, editor. 2006. Cognitive Systems Research.
7:1?96. Special issue on models of eye-movement
control in reading.
Ronan Reilly and Ralph Radach. 2006. Some empirical
tests of an interactive activation model of eye move-
ment control in reading. Cognitive Systems Research,
7:34?55.
Dario D. Salvucci. 2001. An integrated model of eye
movements and visual encoding. Cognitive Systems
Research, 1:201?220.
Matthew Starr and Keith Rayner. 2001. Eye movements
during reading: some current controversies. Trends in
Cognitive Sciences, 5:156?163.
Ian H. Witten and Frank Eibe. 2005. Data Mining: Prac-
tical machine learning tools and techniques. Morgan
Kaufmann.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statisti-
cal dependency analysis with support vector machines.
In Proceedings of the 8th International Workshop on
Parsing Technologies (IWPT), pages 195?206.
Shun-nan Yang. 2006. A oculomotor-based model of
eye movements in reading: The competition/activation
model. Cognitive Systems Research, 7:56?69.
101
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 29?32,
Paris, October 2009. c?2009 Association for Computational Linguistics
Learning Stochastic Bracketing Inversion Transduction Grammars
with a Cubic Time Biparsing Algorithm
Markus SAERS Joakim NIVRE
Dept. of Linguistics and Philology
Uppsala University
Sweden
first.last@lingfil.uu.se
Dekai WU
Human Language Technology Center
Dept. of Computer Science and Engineering
HKUST
Hong Kong
dekai@cs.ust.hk
Abstract
We present a biparsing algorithm for
Stochastic Bracketing Inversion Transduc-
tion Grammars that runs in O(bn3) time
instead of O(n6). Transduction gram-
mars learned via an EM estimation proce-
dure based on this biparsing algorithm are
evaluated directly on the translation task,
by building a phrase-based statistical MT
system on top of the alignments dictated
by Viterbi parses under the induced bi-
grammars. Translation quality at different
levels of pruning are compared, showing
improvements over a conventional word
aligner even at heavy pruning levels.
1 Introduction
As demonstrated by Saers & Wu (2009) there
is something to be gained by applying structural
models such as Inversion Transduction Grammars
(ITG) to the problem of word alignment. One is-
sue is that na??ve methods for inducing ITGs from
parallel data can be very time consuming. We in-
troduce a parsing algorithm for inducing Stochas-
tic Bracketing ITGs from parallel data in O(bn3)
time instead ofO(n6), where b is a pruning param-
eter (lower = tighter pruning). We try out different
values for b, and evaluate the results on a transla-
tion tasks.
In section 2 we summarize the ITG framework;
in section 3 we present our algorithm, whose time
complexity is analyzed in section 4. In section 5
we describe how the algorithm is evaluated, and in
section 6, the empirical results are given.
2 Inversion Transduction Grammars
Inversion transductions are a theoretically inter-
esting and empirically useful equivalence class of
transductions, with expressiveness and computa-
tional complexity characteristics lying intermedi-
ate between finite-state transductions and syntax-
directed transductions. An Inversion Transduc-
tion Grammar (ITG) can be used to synchronously
generate sentence pairs, synchronously parse sen-
tence pairs, or transduce from a sentence in one
language to a sentence in another.1
The equivalence class of inversion transduc-
tions can be described by restricting Syntax-
Directed Transduction Grammars (SDTG)2 in var-
ious equivalent ways to the special cases of (a) bi-
nary SDTGs, (b) ternary SDTGs, or (c) SDTGs
whose transduction rules are restricted to straight
and inverted permutations only.
Thus on one hand, any binary or ternary SDTG
is an ITG. Conversely, any ITG can be stated in
binary two-normal form (Wu, 1997). Only three
kinds of rules are present in the normal form:
A? [BC]
A? ?BC?
A? e/f
On the other hand, under characterization (c),
what distinguishes ITGs is that the permutation of
constituents is restricted in such a way that all chil-
dren of a node must be read either left-to-right, or
right-to-left. The movement only applies to one of
the languages, the other is fixed. Formally, an ITG
is a tuple ?N,V,?, S?, where N is a set of nonter-
minal symbols, ? is a set of rewrite rules, S ? N
is the start symbol and V ? VE ? VF is a set of
biterminal symbols, where VE is the vocabulary of
E and VF is the vocabulary of F . We will write a
biterminal as e/f , where e ? VE and f ? VF . A
sentence pair will be written as e/f , and a bispan
as es..t/fu..v.
Each rule ? ? ? is a tuple ?X, ?, ?? where
X ? N is the right hand side of the rule, ? ?
1All transduction grammars (a.k.a. synchronous gram-
mars, or simply bigrammars) can be interpreted as models
for generation, recognition, or transduction.
2SDTGs (Lewis & Stearns (1968); Aho & Ullman (1969),
(1972)) are also recently called synchronous CFGs.
29
{N ? V }? is a series of nonterminal and biter-
minal symbols representing the production of the
rule and ? ? {?, [], ??} denotes the orientation (ax-
iomatic, straight or inverted) of the rule. Straight
rules are read left-to-right in both languages, while
inverted rules are read left-to-right in E and right-
to-left in F . The direction of the axiomatic rules is
undefined, as they must be completely made up of
terminals. For notational convenience, the orien-
tation of the rule is written as surrounding the pro-
duction, like so: X ? ?, X ? [?] and X ? ???.
The vocabularies of the languages may both in-
clude the empty token , allowing for deletions
and insertions. The empty biterminal, / is not
allowed.
2.1 Stochastic ITGs
In a Stochastic ITG (SITG), each rule is also asso-
ciated with a probability, such that
?
?
Pr(X ? ?) = 1
for all X ? N . The probability of a deriva-
tion S ?? e/f is defined as the production of
the probabilities of all rules used. As shown by
Wu (1995), it is possible to fit the parameters of
a SITG to a parallel corpus via EM (expectation-
maximization) estimation.
2.2 Bracketing ITGs
An ITG where there is only one nonterminal (other
than the start symbol) is called a bracketing ITG
(BITG). Since the one nonterminal is devoid of
information, it can only be used to group its chil-
dren together, imposing a bracketing on the sen-
tence pairs.
3 Parsing SBITGs
In this section we present a biparsing algorithm
for Stochastic Bracketing Inversion Transduction
Grammars (SBITGs) in normal form which incor-
porates a pruning parameter b. The algorithm is
basically an agenda-based bottom-up chart parser,
where the pruning parameter controls the number
of active items of a given length.
To parse a sentence pair e/f , the parser needs
a chart C and a series of T + V agendas
A1, A2, . . . , AT+V , where T = |e| and V = |f |.
An item is defined as a nonterminal symbol (we
use X to denote the anonymous nonterminal sym-
bol of the bracketing ITG) and one span in each
language, written as Xstuv where 0 ? s ? t ? T
corresponds to the span es..t and 0 ? u ? v ? V
corresponds to the span fu..v. The length of an
item is defined as |Xstuv| = (t?s)+(v?u). Since
items are grouped by their length, highly skewed
links (eg. 6:1) will be competing with very even
links (eg. 4:3). Skewed links are generally bad
(and should be pruned), or have a high probability
(which means they are likely to survive pruning).
An item may be active or passive, the active items
are present in the agendas and the chart, whereas
the passive items are only present in the chart.
The parser starts by asserting items from all lex-
ical rules (X ? e/f ), and placing them on their
respective agendas. After the initial seeding, the
agendas are processed in order. When an agenda
is processed, it is first pruned, so that only the b
best items are kept active. After pruning, the re-
maining active items are allowed to be extended.
When extended, the item combines with an adja-
cent item in the chart to form a larger item. The
newly created item is considered active, and added
to both the chart and the appropriate agenda. Once
an item has been processed it goes from being ac-
tive to being passive. The process is halted when
the goal item S0T0V is reached, or when no active
items remain. To build the forest corresponding to
the parse process, back-pointers are used.
3.1 Initialization
In the initial step, the set of lexical items L is built.
All lexical items i ? L are then activated by plac-
ing them on their corresponding agenda A|i|.
L =
?
?
?Xstuv
??????
0?s? t?T,
0?u?v?V,
X ? es..t/fu..v ? ?
?
?
?
By limiting the length of phrasal terminals to some
threshold ?, the variables t and v can be limited to
s+? and u+? respectively, limiting the complexity
of the initialization step from O(n4) to O(n2).
3.2 Recursion
In the recursive step we build a set of extensions
E(i) for all active items i. All items in E(i)
are then activated by placing them on their cor-
responding agenda (i ? A|i|).
E(Xstuv) =
{XStUv|0?S?s, 0?U?u,XSsUu ? C} ?
{XsSuU |t?S?T, v?U?V,XtSvU ? C} ?
{XsSUv|t?S?T, 0?U?u,XtSUu ? C} ?
{XStuU |0?S?s, v?U?V,XSsvU ? C}
30
Since we are processing the agendas in order, any
item in the chart will be as long as or shorter than
the item being extended. This fact can be exploited
to limit the number of possible siblings explored,
but has no impact on time complexity.
3.3 Viterbi parsing
When doing Viterbi parsing, all derivations but
the most probable are discarded. This gives an
unambiguous parse, which dictates exactly one
alignment between e and f . The alignment of
the Viterbi parse can be used to substitute that of
other word aligners (Saers and Wu, 2009) such as
GIZA++ (Och and Ney, 2003).
4 Analysis
Looking at the algorithm, it is clear that there will
be a total of T + V = O(n) agendas, each con-
taining items of a certain length. The items in an
agenda can start anywhere in the alignment space:
O(n2) possible starting points, but once the end
point in one language is set, the end point in the
other follows from that, adding a factor O(n).
This means that each agenda contains O(n3) ac-
tive items. Each active item has to go through all
possible siblings in the recursive step. Since the
start point of the sibling is determined by the item
itself (it has to be adjacent), only the O(n2) pos-
sible end points have to be explored. This means
that each active item takes O(n2) time to process.
The total time is thus O(n6): O(n) agendas,
containing O(n3) active items, requiring O(n2)
time to process. This is also the time complex-
ity reported for ITGs in previous work (Wu, 1995;
Wu, 1997).
The pruning works by limiting the number of
active items in an agenda to a constant b, meaning
that there are O(n) agendas, containing O(b) ac-
tive items, requiring O(n2) time to process. This
gives a total time complexity of O(bn3).
5 Evaluation
We evaluate the parser on a translation task
(WMT?08 shared task3). In order to evaluate on
a translation task, a translation system has to be
built. We use the alignments from the Viterbi
parses of the training corpus to substitute the
alignments of GIZA++. This is the same approach
as taken in Saers & Wu (2009). We will evalu-
ate the resulting translations with two automatic
3http://www.statmt.org/wmt08/
metrics: BLEU (Papineni et al, 2002) and NIST
(Doddington, 2002).
6 Empirical results
In this section we describe the experimental setup
as well as the outcomes.
6.1 Setup
We use the Moses Toolkit (Koehn et al, 2007) to
train our phrase-based SMT models. The toolkit
also includes scripts for applying GIZA++ (Och
and Ney, 2003) as a word aligner. We have
trained several systems, one using GIZA++ (our
baseline system), one with no pruning at all, and
6 different values of b (1, 10, 25, 50, 75 and
100). We used the grow-diag-final-and
method to extract phrases from the word align-
ment, and MERT (Och, 2003) to optimize the re-
sulting model. We trained a 5-gram SRI language
model (Stolcke, 2002) using the corpus supplied
for this purpose by the shared task organizers. All
of the above is consistent with the guidelines for
building a baseline system for the WMT?08 shared
task.
The translation tasks we applied the above
procedure to are all taken from the Europarl
corpus (Koehn, 2005). We selected the tasks
German-English, French-English and Spanish-
English. Furthermore, we restricted the training
sentence pairs so that none of the sentences ex-
ceeded length 10. This was necessary to be able to
carry out exhaustive search. The total amount of
training data was roughly 100,000 sentence pairs
in each language pair, which is a relatively small
corpus, but by no means a toy example.
6.2 Grammar induction
It is possible to set the parameters of a SBITG
by applying EM to an initial guess (Wu, 1995).
As our initial guess, we used word co-occurrence
counts, assuming that there was one empty token
in each sentence. This gave an estimate of the lex-
ical rules. The probability mass was divided so
that the lexical rules could share half of it, while
the other half was shared equally by the two struc-
tural rules (X ? [XX] and X ? ?XX?).
Several training runs were made with different
pruning parameters. The EM process was halted
when a relative improvement in log-likelihood of
10?3 was no longer achieved over the previous it-
eration.
31
Baseline Different values of b for SBITGs
Metric (GIZA++) ? 100 75 50 25 10 1
Spanish-English
BLEU 0.2597 0.2663 0.2671 0.2661 0.2653 0.2655 0.2608 0.1234
NIST 6.6352 6.7407 6.7445 6.7329 6.7101 6.7312 6.6439 3.9705
time 03:20:00 02:40:00 02:00:00 01:20:00 00:38:00 00:17:00 00:03:10
German-English
BLEU 0.2059 0.2113 0.2094 0.2091 0.2090 0.2091 0.2050 0.0926
NIST 5.8668 5.9380 5.9086 5.8955 5.8947 5.9292 5.8743 3.4297
time 03:40:00 02:45:00 02:10:00 01:25:00 00:41:00 00:17:00 00:03:20
French-English
BLEU 0.2603 0.2663 0.2655 0.2668 0.2669 0.2654 0.2632 0.1268
NIST 6.6907 6.8151 6.8068 6.8068 6.8065 6.7013 6.7136 4.0849
time 03:10:00 02:45:00 02:10:00 01:25:00 00:42:00 00:17:00 00:03:25
Table 1: Results. Time measures are approximate time per iteration.
Once the EM process terminated, Viterbi parses
were calculated for the training corpus, and the
alignments from them outputted in the same for-
mat produced by GIZA++.
6.3 Results
The results are presented in Table 1. GIZA++
generally terminates within minutes (6?7) on the
training corpora used, making it faster than any
of the SBITGs (they generally required 4?6 iter-
ations to terminate, making even the fastest ones
slower than GIZA++). To put the times in per-
spective, about 6 iterations were needed to get
the ITGs to converge, making the longest training
time about 16?17 hours. The time it takes to ex-
tract the phrases and tune the model using MERT
is about 14 hours for these data sets.
Looking at translation quality, we see a sharp
initial rise as b grows to 10. At this point the
SBITG system is on par with GIZA++. It con-
tinues to rise up to b = 25, but after that is more or
less levels out. From this we conclude that the pos-
itive results reported in Saers & Wu (2009) hold
under harsh pruning.
7 Conclusions
We have presented a SBITG biparsing algorithm
that uses a novel form of pruning to cut the com-
plexity of EM-estimation from O(n6) to O(bn3).
Translation quality using the resulting learned
SBITG models is improved over using conven-
tional word alignments, even under harsh levels of
pruning.
Acknowledgments
The authors are grateful for the comments made by the two anonymous review-
ers. This work was funded by the Swedish National Graduate School of Lan-
guage Technology, the Defense Advanced Research Projects Agency (DARPA)
under GALE Contract No. HR0011-06-C-0023, and the Hong Kong Research
Grants Council (RGC) under research grants GRF621008, DAG03/04.EG09,
RGC6256/00E, and RGC6083/99E. Any opinions, findings and conclusions or
recommendations expressed in this material are those of the authors and do
not necessarily reflect the views of the Defense Advanced Research Projects
Agency.
References
Alfred V. Aho and Jeffrey D. Ullman. 1969. Syntax-directed translations
and the pushdown assembler. Journal of Computer and System Sciences,
3(1):37?56.
Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory of Parsing, Transla-
tion, and Compiling (Volumes 1 and 2). Prentice-Halll, Englewood Cliffs,
NJ.
George Doddington. 2002. Automatic evaluation of machine translation qual-
ity using n-gram co-occurrence statistics. In Human Language Technology
conference (HLT-2002), San Diego, CA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello
Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for statistical machine trans-
lation. In ACL-2007 Demo and Poster Sessions, pages 177?180, Prague,
Jun.
Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine trans-
lation. In Machine Translation Summit X, Phuket, Thailand, September.
Philip M. Lewis and Richard E. Stearns. 1968. Syntax-directed transduction.
Journal of the Association for Computing Machinery, 15(3):465?488.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various
statistical alignment models. Computational Linguistics, 29(1):19?52.
Franz Josef Och. 2003. Minimum error rate training in statistical machine
translation. In 41st Annual Meeting of the Association for Computational
Linguistics, pages 160?167, Sapporo, Japan, Jul.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU:
A method for automatic evaluation of machine translations. In 40th Annual
Meeting of the Association for Computational Linguistics (ACL-2002),
pages 311?318, Philadelphia, Jul.
Markus Saers and Dekai Wu. 2009. Improving phrase-based translation via
word alignments from Stochastic Inversion Transduction Grammars. In
Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statis-
tical Translation (at NAACL HLT 2009), pages 28?36, Boulder, CO, Jun.
Andreas Stolcke. 2002. SRILM ? an extensible language modeling toolkit.
In International Conference on Spoken Language Processing, Denver, CO,
Sep.
Dekai Wu. 1995. Trainable coarse bilingual grammars for parallel text brack-
eting. In Third Annual Workshop on Very Large Corpora (WVLC-3), pages
69?81, Cambridge, MA, Jun.
Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and bilingual
parsing of parallel corpora. Computational Linguistics, 23(3):377?404,
Sep.
32
Deterministic Dependency Parsing of English Text
Joakim Nivre and Mario Scholz
School of Mathematics and Systems Engineering
Va?xjo? University
SE-35195 Va?xjo?
Sweden
joakim.nivre@msi.vxu.se
Abstract
This paper presents a deterministic dependency
parser based on memory-based learning, which
parses English text in linear time. When trained
and evaluated on the Wall Street Journal sec-
tion of the Penn Treebank, the parser achieves
a maximum attachment score of 87.1%. Unlike
most previous systems, the parser produces la-
beled dependency graphs, using as arc labels a
combination of bracket labels and grammatical
role labels taken from the Penn Treebank II an-
notation scheme. The best overall accuracy ob-
tained for identifying both the correct head and
the correct arc label is 86.0%, when restricted
to grammatical role labels (7 labels), and 84.4%
for the maximum set (50 labels).
1 Introduction
There has been a steadily increasing interest in syn-
tactic parsing based on dependency analysis in re-
cent years. One important reason seems to be that
dependency parsing offers a good compromise be-
tween the conflicting demands of analysis depth,
on the one hand, and robustness and efficiency, on
the other. Thus, whereas a complete dependency
structure provides a fully disambiguated analysis
of a sentence, this analysis is typically less com-
plex than in frameworks based on constituent anal-
ysis and can therefore often be computed determin-
istically with reasonable accuracy. Deterministic
methods for dependency parsing have now been ap-
plied to a variety of languages, including Japanese
(Kudo and Matsumoto, 2000), English (Yamada and
Matsumoto, 2003), Turkish (Oflazer, 2003), and
Swedish (Nivre et al, 2004).
For English, the interest in dependency parsing
has been weaker than for other languages. To some
extent, this can probably be explained by the strong
tradition of constituent analysis in Anglo-American
linguistics, but this trend has been reinforced by the
fact that the major treebank of American English,
the Penn Treebank (Marcus et al, 1993), is anno-
tated primarily with constituent analysis. On the
other hand, the best available parsers trained on the
Penn Treebank, those of Collins (1997) and Char-
niak (2000), use statistical models for disambigua-
tion that make crucial use of dependency relations.
Moreover, the deterministic dependency parser of
Yamada and Matsumoto (2003), when trained on
the Penn Treebank, gives a dependency accuracy
that is almost as good as that of Collins (1997) and
Charniak (2000).
The parser described in this paper is similar to
that of Yamada and Matsumoto (2003) in that it uses
a deterministic parsing algorithm in combination
with a classifier induced from a treebank. However,
there are also important differences between the two
approaches. First of all, whereas Yamada and Mat-
sumoto employs a strict bottom-up algorithm (es-
sentially shift-reduce parsing) with multiple passes
over the input, the present parser uses the algorithm
proposed in Nivre (2003), which combines bottom-
up and top-down processing in a single pass in order
to achieve incrementality. This also means that the
time complexity of the algorithm used here is linear
in the size of the input, while the algorithm of Ya-
mada and Matsumoto is quadratic in the worst case.
Another difference is that Yamada and Matsumoto
use support vector machines (Vapnik, 1995), while
we instead rely on memory-based learning (Daele-
mans, 1999).
Most importantly, however, the parser presented
in this paper constructs labeled dependency graphs,
i.e. dependency graphs where arcs are labeled with
dependency types. As far as we know, this makes
it different from all previous systems for depen-
dency parsing applied to the Penn Treebank (Eis-
ner, 1996; Yamada and Matsumoto, 2003), although
there are systems that extract labeled grammati-
cal relations based on shallow parsing, e.g. Buch-
holz (2002). The fact that we are working with la-
beled dependency graphs is also one of the motiva-
tions for choosing memory-based learning over sup-
port vector machines, since we require a multi-class
classifier. Even though it is possible to use SVM
for multi-class classification, this can get cumber-
some when the number of classes is large. (For the
The
 
?
DEP
finger-pointing
 
?
NP-SBJ
has already
 
?
ADVP
begun
 
?
VP
.
?
 
DEP
Figure 1: Dependency graph for English sentence
unlabeled dependency parser of Yamada and Mat-
sumoto (2003) the classification problem only in-
volves three classes.)
The parsing methodology investigated here has
previously been applied to Swedish, where promis-
ing results were obtained with a relatively small
treebank (approximately 5000 sentences for train-
ing), resulting in an attachment score of 84.7% and
a labeled accuracy of 80.6% (Nivre et al, 2004).1
However, since there are no comparable results
available for Swedish, it is difficult to assess the sig-
nificance of these findings, which is one of the rea-
sons why we want to apply the method to a bench-
mark corpus such as the the Penn Treebank, even
though the annotation in this corpus is not ideal for
labeled dependency parsing.
The paper is structured as follows. Section 2 de-
scribes the parsing algorithm, while section 3 ex-
plains how memory-based learning is used to guide
the parser. Experimental results are reported in sec-
tion 4, and conclusions are stated in section 5.
2 Deterministic Dependency Parsing
In dependency parsing the goal of the parsing pro-
cess is to construct a labeled dependency graph of
the kind depicted in Figure 1. In formal terms, we
define dependency graphs as follows:
1. Let R = {r1, . . . , rm} be the set of permissible
dependency types (arc labels).
2. A dependency graph for a string of words
W = w1? ? ?wn is a labeled directed graph
D = (W,A), where
(a) W is the set of nodes, i.e. word tokens in
the input string,
(b) A is a set of labeled arcs (wi, r, wj)
(wi, wj ? W , r ? R),
(c) for every wj ? W , there is at most one
arc (wi, r, wj) ? A.
1The attachment score only considers whether a word is as-
signed the correct head; the labeled accuracy score in addition
requires that it is assigned the correct dependency type; cf. sec-
tion 4.
3. A graph D = (W,A) is well-formed iff it is
acyclic, projective and connected.
For a more detailed discussion of dependency
graphs and well-formedness conditions, the reader
is referred to Nivre (2003).
The parsing algorithm used here was first de-
fined for unlabeled dependency parsing in Nivre
(2003) and subsequently extended to labeled graphs
in Nivre et al (2004). Parser configurations are rep-
resented by triples ?S, I,A?, where S is the stack
(represented as a list), I is the list of (remaining)
input tokens, and A is the (current) arc relation
for the dependency graph. (Since in a dependency
graph the set of nodes is given by the input to-
kens, only the arcs need to be represented explic-
itly.) Given an input string W , the parser is initial-
ized to ?nil,W, ??2 and terminates when it reaches
a configuration ?S,nil, A? (for any list S and set of
arcs A). The input string W is accepted if the de-
pendency graph D = (W,A) given at termination
is well-formed; otherwise W is rejected. Given an
arbitrary configuration of the parser, there are four
possible transitions to the next configuration (where
t is the token on top of the stack, n is the next input
token, w is any word, and r, r? ? R):
1. Left-Arc: In a configuration ?t|S,n|I,A?, if
there is no arc (w, r, t) ? A, extend A with
(n, r?, t) and pop the stack, giving the configu-
ration ?S,n|I,A?{(n, r?, t)}?.
2. Right-Arc: In a configuration ?t|S,n|I,A?, if
there is no arc (w, r, n) ? A, extend A with
(t, r?, n) and push n onto the stack, giving the
configuration ?n|t|S,I,A?{(t, r?, n)}?.
3. Reduce: In a configuration ?t|S,I,A?, if there
is an arc (w, r, t)?A, pop the stack, giving the
configuration ?S,I,A?.
4. Shift: In a configuration ?S,n|I,A?, push
n onto the stack, giving the configuration
?n|S,I,A?.
2We use nil to denote the empty list and a|A to denote a list
with head a and tail A.
TH.POS
 
?
T.DEP
. . . TL.POS
 
?
TL.DEP
. . . T.POS
T.LEX
 
?
TR.DEP
. . . TR.POS . . . NL.POS
 
?
NL.DEP
. . . N.POS
N.LEX
L1.POS L2.POS L3.POS
T = Top of the stack
N = Next input token
TL = Leftmost dependent of T
TR = Rightmost dependent of T
NL = Leftmost dependent of N
Li = Next plus i input token
X.LEX = Word form of X
X.POS = Part-of-speech of X
X.DEP = Dependency type of X
Figure 2: Parser state features
After initialization, the parser is guaranteed to ter-
minate after at most 2n transitions, given an input
string of length n (Nivre, 2003). Moreover, the
parser always constructs a dependency graph that is
acyclic and projective. This means that the depen-
dency graph given at termination is well-formed if
and only if it is connected (Nivre, 2003). Otherwise,
it is a set of connected components, each of which
is a well-formed dependency graph for a substring
of the original input.
The transition system defined above is nondeter-
ministic in itself, since several transitions can of-
ten be applied in a given configuration. To con-
struct deterministic parsers based on this system,
we use classifiers trained on treebank data in or-
der to predict the next transition (and dependency
type) given the current configuration of the parser.
In this way, our approach can be seen as a form of
history-based parsing (Black et al, 1992; Mager-
man, 1995). In the experiments reported here, we
use memory-based learning to train our classifiers.
3 Memory-Based Learning
Memory-based learning and problem solving is
based on two fundamental principles: learning is the
simple storage of experiences in memory, and solv-
ing a new problem is achieved by reusing solutions
from similar previously solved problems (Daele-
mans, 1999). It is inspired by the nearest neighbor
approach in statistical pattern recognition and arti-
ficial intelligence (Fix and Hodges, 1952), as well
as the analogical modeling approach in linguistics
(Skousen, 1989; Skousen, 1992). In machine learn-
ing terms, it can be characterized as a lazy learn-
ing method, since it defers processing of input un-
til needed and processes input by combining stored
data (Aha, 1997).
Memory-based learning has been successfully
applied to a number of problems in natural language
processing, such as grapheme-to-phoneme conver-
sion, part-of-speech tagging, prepositional-phrase
attachment, and base noun phrase chunking (Daele-
mans et al, 2002). Previous work on memory-based
learning for deterministic parsing includes Veenstra
and Daelemans (2000) and Nivre et al (2004).
For the experiments reported in this paper, we
have used the software package TiMBL (Tilburg
Memory Based Learner), which provides a vari-
ety of metrics, algorithms, and extra functions on
top of the classical k nearest neighbor classification
kernel, such as value distance metrics and distance
weighted class voting (Daelemans et al, 2003).
The function we want to approximate is a map-
ping f from configurations to parser actions, where
each action consists of a transition and (except for
Shift and Reduce) a dependency type:
f : Config ? {LA,RA,RE,SH} ? (R ? {nil})
Here Config is the set of all configurations and R
is the set of dependency types. In order to make the
problem tractable, we approximate f with a func-
tion f? whose domain is a finite space of parser
states, which are abstractions over configurations.
For this purpose we define a number of features
that can be used to define different models of parser
state.
Figure 2 illustrates the features that are used to
define parser states in the present study. The two
central elements in any configuration are the token
on top of the stack (T) and the next input token
(N), the tokens which may be connected by a de-
pendency arc in the next configuration. For these
tokens, we consider both the word form (T.LEX,
N.LEX) and the part-of-speech (T.POS, N.POS), as
assigned by an automatic part-of-speech tagger in
a preprocessing phase. Next, we consider a selec-
tion of dependencies that may be present in the cur-
rent arc relation, namely those linking T to its head
(TH) and its leftmost and rightmost dependent (TL,
TR), and that linking N to its leftmost dependent
(NL),3 considering both the dependency type (arc
label) and the part-of-speech of the head or depen-
dent. Finally, we use a lookahead of three tokens,
considering only their parts-of-speech.
We have experimented with two different state
models, one that incorporates all the features de-
picted in Figure 2 (Model 1), and one that ex-
cludes the parts-of-speech of TH, TL, TR, NL (Model
2). Models similar to model 2 have been found to
work well for datasets with a rich annotation of de-
pendency types, such as the Swedish dependency
treebank derived from Einarsson (1976), where the
extra part-of-speech features are largely redundant
(Nivre et al, 2004). Model 1 can be expected to
work better for datasets with less informative de-
pendency annotation, such as dependency trees ex-
tracted from the Penn Treebank, where the extra
part-of-speech features may compensate for the lack
of information in arc labels.
The learning algorithm used is the IB1 algorithm
(Aha et al, 1991) with k = 5, i.e. classification based
on 5 nearest neighbors.4 Distances are measured us-
ing the modified value difference metric (MVDM)
(Stanfill and Waltz, 1986; Cost and Salzberg, 1993)
for instances with a frequency of at least 3 (and
the simple overlap metric otherwise), and classifica-
tion is based on distance weighted class voting with
inverse distance weighting (Dudani, 1976). These
settings are the result of extensive experiments par-
tially reported in Nivre et al (2004). For more infor-
mation about the different parameters and settings,
see Daelemans et al (2003).
4 Experiments
The data set used for experimental evaluation is
the standard data set from the Wall Street Journal
section of the Penn Treebank, with sections 2?21
3Given the parsing algorithm, N can never have a head or a
right dependent in the current configuration.
4In TiMBL, the value of k in fact refers to k nearest dis-
tances rather than k nearest neighbors, which means that, even
with k = 1, the nearest neighbor set can contain several in-
stances that are equally distant to the test instance. This is dif-
ferent from the original IB1 algorithm, as described in Aha et
al. (1991).
used for training and section 23 for testing (Collins,
1999; Charniak, 2000). The data has been con-
verted to dependency trees using head rules (Mager-
man, 1995; Collins, 1996). We are grateful to Ya-
mada and Matsumoto for letting us use their rule set,
which is a slight modification of the rules used by
Collins (1999). This permits us to make exact com-
parisons with the parser of Yamada and Matsumoto
(2003), but also the parsers of Collins (1997) and
Charniak (2000), which are evaluated on the same
data set in Yamada and Matsumoto (2003).
One problem that we had to face is that the stan-
dard conversion of phrase structure trees to de-
pendency trees gives unlabeled dependency trees,
whereas our parser requires labeled trees. Since the
annotation scheme of the Penn Treebank does not
include dependency types, there is no straightfor-
ward way to derive such labels. We have therefore
experimented with two different sets of labels, none
of which corresponds to dependency types in a strict
sense. The first set consists of the function tags for
grammatical roles according to the Penn II annota-
tion guidelines (Bies et al, 1995); we call this set G.
The second set consists of the ordinary bracket la-
bels (S, NP, VP, etc.), combined with function tags
for grammatical roles, giving composite labels such
as NP-SBJ; we call this set B. We assign labels to
arcs by letting each (non-root) word that heads a
phrase P in the original phrase structure have its in-
coming edge labeled with the label of P (modulo
the set of labels used). In both sets, we also include
a default label DEP for arcs that would not other-
wise get a label. This gives a total of 7 labels in the
G set and 50 labels in the B set. Figure 1 shows a
converted dependency tree using the B labels; in the
corresponding tree with G labels NP-SBJ would be
replaced by SBJ, ADVP and VP by DEP.
We use the following metrics for evaluation:
1. Unlabeled attachment score (UAS): The pro-
portion of words that are assigned the correct
head (or no head if the word is a root) (Eisner,
1996; Collins et al, 1999).
2. Labeled attachment score (LAS): The pro-
portion of words that are assigned the correct
head and dependency type (or no head if the
word is a root) (Nivre et al, 2004).
3. Dependency accuracy (DA): The proportion
of non-root words that are assigned the correct
head (Yamada and Matsumoto, 2003).
4. Root accuracy (RA): The proportion of root
words that are analyzed as such (Yamada and
Matsumoto, 2003).
5. Complete match (CM): The proportion of
sentences whose unlabeled dependency struc-
ture is completely correct (Yamada and Mat-
sumoto, 2003).
All metrics except CM are calculated as mean
scores per word, and punctuation tokens are con-
sistently excluded.
Table 1 shows the attachment score, both unla-
beled and labeled, for the two different state models
with the two different label sets. First of all, we
see that Model 1 gives better accuracy than Model
2 with the smaller label set G, which confirms our
expectations that the added part-of-speech features
are helpful when the dependency labels are less in-
formative. Conversely, we see that Model 2 outper-
forms Model 1 with the larger label set B, which
is consistent with the hypothesis that part-of-speech
features become redundant as dependency labels get
more informative. It is interesting to note that this
effect holds even in the case where the dependency
labels are mostly derived from phrase structure cate-
gories.
We can also see that the unlabeled attachment
score improves, for both models, when the set of
dependency labels is extended. On the other hand,
the labeled attachment score drops, but it must be
remembered that these scores are not really com-
parable, since the number of classes in the classifi-
cation problem increases from 7 to 50 as we move
from the G set to the B set. Therefore, we have also
included the labeled attachment score restricted to
the G set for the parser using the B set (BG), and we
see then that the attachment score improves, espe-
cially for Model 2. (All differences are significant
beyond the .01 level; McNemar?s test.)
Table 2 shows the dependency accuracy, root
accuracy and complete match scores for our best
parser (Model 2 with label set B) in comparison
with Collins (1997) (Model 3), Charniak (2000),
and Yamada and Matsumoto (2003).5 It is clear that,
with respect to unlabeled accuracy, our parser does
not quite reach state-of-the-art performance, even
if we limit the competition to deterministic meth-
ods such as that of Yamada and Matsumoto (2003).
We believe that there are mainly three reasons for
this. First of all, the part-of-speech tagger used
for preprocessing in our experiments has a lower
accuracy than the one used by Yamada and Mat-
sumoto (2003) (96.1% vs. 97.1%). Although this
is not a very interesting explanation, it undoubtedly
accounts for part of the difference. Secondly, since
5The information in the first three rows is taken directly
from Yamada and Matsumoto (2003).
our parser makes crucial use of dependency type in-
formation in predicting the next action of the parser,
it is very likely that it suffers from the lack of real
dependency labels in the converted treebank. Indi-
rect support for this assumption can be gained from
previous experiments with Swedish data, where al-
most the same accuracy (85% unlabeled attachment
score) has been achieved with a treebank which
is much smaller but which contains proper depen-
dency annotation (Nivre et al, 2004).
A third important factor is the relatively low root
accuracy of our parser, which may reflect a weak-
ness in the one-pass parsing strategy with respect to
the global structure of complex sentences. It is note-
worthy that our parser has lower root accuracy than
dependency accuracy, whereas the inverse holds for
all the other parsers. The problem becomes even
more visible when we consider the dependency and
root accuracy for sentences of different lengths, as
shown in Table 3. Here we see that for really short
sentences (up to 10 words) root accuracy is indeed
higher than dependency accuracy, but while depen-
dency accuracy degrades gracefully with sentence
length, the root accuracy drops more drastically
(which also very clearly affects the complete match
score). This may be taken to suggest that some kind
of preprocessing in the form of clausing may help
to improve overall accuracy.
Turning finally to the assessment of labeled de-
pendency accuracy, we are not aware of any strictly
comparable results for the given data set, but Buch-
holz (2002) reports a labeled accuracy of 72.6%
for the assignment of grammatical relations using
a cascade of memory-based processors. This can be
compared with a labeled attachment score of 84.4%
for Model 2 with our B set, which is of about the
same size as the set used by Buchholz, although the
labels are not the same. In another study, Blaheta
and Charniak (2000) report an F-measure of 98.9%
for the assignment of Penn Treebank grammatical
role labels (our G set) to phrases that were correctly
parsed by the parser described in Charniak (2000).
If null labels (corresponding to our DEP labels) are
excluded, the F-score drops to 95.7%. The corre-
sponding F-measures for our best parser (Model 2,
BG) are 99.0% and 94.7%. For the larger B set,
our best parser achieves an F-measure of 96.9%
(DEP labels included), which can be compared with
97.0% for a similar (but larger) set of labels in
Collins (1999).6 Although none of the previous re-
sults on labeling accuracy is strictly comparable to
ours, it nevertheless seems fair to conclude that the
6This F-measure is based on the recall and precision figures
reported in Figure 7.15 in Collins (1999).
Model 1 Model 2
G B BG G B BG
UAS 86.4 86.7 85.8 87.1
LAS 85.3 84.0 85.5 84.6 84.4 86.0
Table 1: Parsing accuracy: Attachment score (BG = evaluation of B restricted to G labels)
DA RA CM
Charniak 92.1 95.2 45.2
Collins 91.5 95.2 43.3
Yamada & Matsumoto 90.3 91.6 38.4
Nivre & Scholz 87.3 84.3 30.4
Table 2: Comparison with related work (Yamada and Matsumoto, 2003)
labeling accuracy of the present parser is close to the
state of the art, even if its capacity to derive correct
structures is not.
5 Conclusion
This paper has explored the application of a data-
driven dependency parser to English text, using data
from the Penn Treebank. The parser is deterministic
and uses a linear-time parsing algorithm, guided by
memory-based classifiers, to construct labeled de-
pendency structures incrementally in one pass over
the input. Given the difficulty of extracting labeled
dependencies from a phrase structure treebank with
limited functional annotation, the accuracy attained
is fairly respectable. And although the structural ac-
curacy falls short of the best available parsers, the
labeling accuracy appears to be competitive.
The most important weakness is the limited ac-
curacy in identifying the root node of a sentence,
especially for longer sentences. We conjecture that
an improvement in this area could lead to a boost
in overall performance. Another important issue
to investigate further is the influence of different
kinds of arc labels, and in particular labels that are
based on a proper dependency grammar. In the
future, we therefore want to perform more experi-
ments with genuine dependency treebanks like the
Prague Dependency Treebank (Hajic, 1998) and the
Danish Dependency Treebank (Kromann, 2003).
We also want to apply dependency-based evaluation
schemes such as the ones proposed by Lin (1998)
and Carroll et al (1998).
Acknowledgements
The work presented in this paper has been supported
by a grant from the Swedish Research Council (621-
2002-4207). The memory-based classifiers used in
the experiments have been constructed using the
Tilburg Memory-Based Learner (TiMBL) (Daele-
mans et al, 2003). The conversion of the Penn Tree-
bank to dependency trees has been performed using
head rules kindly provided by Hiroyasu Yamada and
Yuji Matsumoto.
References
D. W. Aha, D. Kibler, and M. Albert. 1991.
Instance-based learning algorithms. Machine
Learning, 6:37?66.
D. Aha, editor. 1997. Lazy Learning. Kluwer.
A. Bies, M. Ferguson, K. Katz, and R. MacIn-
tyre. 1995. Bracketing guidelines for Treebank II
style, Penn Treebank project. University of Penn-
sylvania, Philadelphia.
E. Black, F. Jelinek, J. Lafferty, D. Magerman,
R. Mercer, and S. Roukos. 1992. Towards
history-based grammars: Using richer models for
probabilistic parsing. In Proceedings of the 5th
DARPA Speech and Natural Language Workshop.
D. Blaheta and E. Charniak. 2000. Assigning
function tags to parsed text. In Proceedings of
NAACL, pages 234?240.
S. Buchholz. 2002. Memory-Based Grammatical
Relation Finding. Ph.D. thesis, University of
Tilburg.
J. Carroll, E. Briscoe, and A. Sanfilippo. 1998.
Parser evaluation: A survey and a new pro-
posal. In Proceedings of LREC, pages 447?454,
Granada, Spain.
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL.
M. Collins, J. Hajic?, E. Brill, L. Ramshaw, and
C. Tillmann. 1999. A Statistical Parser of Czech.
In Proceedings of ACL, pages 505?512, Univer-
sity of Maryland, College Park, USA.
DA RA CM
? 10 93.7 96.6 83.6
11?20 88.8 86.4 39.5
21?30 87.4 83.4 20.8
31?40 86.8 78.1 9.9
? 41 84.6 74.9 1.8
Table 3: Accuracy in relation to sentence length (number of words)
M. Collins. 1996. A new statistical parser based on
bigram lexical dependencies. In Proceedings of
ACL, pages 184?191, Santa Cruz, CA.
M. Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of
ACL, pages 16?23, Madrid, Spain.
M. Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Uni-
versity of Pennsylvania.
S. Cost and S. Salzberg. 1993. A weighted near-
est neighbor algorithm for learning with symbolic
features. Machine Learning, 10:57?78.
W. Daelemans, A. van den Bosch, and J. Zavrel.
2002. Forgetting exceptions is harmful in lan-
guage learning. Machine Learning, 34:11?43.
W. Daelemans, J. Zavrel, K. van der Sloot, and
A. van den Bosch. 2003. Timbl: Tilburg mem-
ory based learner, version 5.0, reference guide.
Technical Report ILK 03-10, Tilburg University,
ILK.
W. Daelemans. 1999. Memory-based language
processing. Introduction to the special issue.
Journal of Experimental and Theoretical Artifi-
cial Intelligence, 11:287?292.
S. A. Dudani. 1976. The distance-weighted k-
nearest neighbor rule. IEEE Transactions on Sys-
tems, Man, and Cybernetics, SMC-6:325?327.
J. Einarsson. 1976. Talbankens skriftspra?kskonkor-
dans. Lund University.
J. M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Pro-
ceedings of COLING, Copenhagen, Denmark.
E. Fix and J. Hodges. 1952. Discriminatory anal-
ysis: Nonparametric discrimination: Consistency
properties. Technical Report 11, USAF School of
Aviation Medicine, Randolph Field, Texas.
J. Hajic. 1998. Building a syntactically annotated
corpus: The prague dependency treebank. In Is-
sues of Valency and Meaning, pages 106?132.
Karolinum.
M. T. Kromann. 2003. The Danish dependency
treebank and the DTAG treebank tool. In Pro-
ceedings of the Second Workshop on Treebanks
and Linguistic Theories, pages 217?220, Va?xjo?,
Sweden.
T. Kudo and Y. Matsumoto. 2000. Japanese depen-
dency structure analysis based on support vec-
tor machines. In Proceedings of EMNLP/VLC,
Hongkong.
D. Lin. 1998. Dependency-based evaluation of
MINIPAR. In Proceedings of LREC.
D. M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of ACL,
pages 276?283, Boston, MA.
M. P. Marcus, B. Santorini, and M. A.
Marcinkiewicz. 1993. Building a large an-
notated corpus of English: The Penn Treebank.
Computational Linguistics, 19:313?330.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-
based dependency parsing. In Proceedings of
CoNLL, pages 49?56.
J. Nivre. 2003. An efficient algorithm for projective
dependency parsing. In Proceedings of IWPT,
pages 149?160, Nancy, France.
K. Oflazer. 2003. Dependency parsing with an ex-
tended finite-state approach. Computational Lin-
guistics, 29:515?544.
R. Skousen. 1989. Analogical Modeling of Lan-
guage. Kluwer.
R. Skousen. 1992. Analogy and Structure. Kluwer.
C. Stanfill and D. Waltz. 1986. Toward memory-
based reasoning. Communications of the ACM,
29:1213?1228.
V. N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag.
J. Veenstra and W. Daelemans. 2000. A memory-
based alternative for connectionist shift-reduce
parsing. Technical Report ILK-0012, University
of Tilburg.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines.
In Proceedings of IWPT, pages 195?206, Nancy,
France.
Proceedings of the 43rd Annual Meeting of the ACL, pages 99?106,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Pseudo-Projective Dependency Parsing
Joakim Nivre and Jens Nilsson
School of Mathematics and Systems Engineering
Va?xjo? University
SE-35195 Va?xjo?, Sweden
{nivre,jni}@msi.vxu.se
Abstract
In order to realize the full potential of
dependency-based syntactic parsing, it is
desirable to allow non-projective depen-
dency structures. We show how a data-
driven deterministic dependency parser,
in itself restricted to projective structures,
can be combined with graph transforma-
tion techniques to produce non-projective
structures. Experiments using data from
the Prague Dependency Treebank show
that the combined system can handle non-
projective constructions with a precision
sufficient to yield a significant improve-
ment in overall parsing accuracy. This
leads to the best reported performance for
robust non-projective parsing of Czech.
1 Introduction
It is sometimes claimed that one of the advantages
of dependency grammar over approaches based on
constituency is that it allows a more adequate treat-
ment of languages with variable word order, where
discontinuous syntactic constructions are more com-
mon than in languages like English (Mel?c?uk,
1988; Covington, 1990). However, this argument
is only plausible if the formal framework allows
non-projective dependency structures, i.e. structures
where a head and its dependents may correspond
to a discontinuous constituent. From the point of
view of computational implementation this can be
problematic, since the inclusion of non-projective
structures makes the parsing problem more com-
plex and therefore compromises efficiency and in
practice also accuracy and robustness. Thus, most
broad-coverage parsers based on dependency gram-
mar have been restricted to projective structures.
This is true of the widely used link grammar parser
for English (Sleator and Temperley, 1993), which
uses a dependency grammar of sorts, the probabilis-
tic dependency parser of Eisner (1996), and more
recently proposed deterministic dependency parsers
(Yamada and Matsumoto, 2003; Nivre et al, 2004).
It is also true of the adaptation of the Collins parser
for Czech (Collins et al, 1999) and the finite-state
dependency parser for Turkish by Oflazer (2003).
This is in contrast to dependency treebanks, e.g.
Prague Dependency Treebank (Hajic? et al, 2001b),
Danish Dependency Treebank (Kromann, 2003),
and the METU Treebank of Turkish (Oflazer et al,
2003), which generally allow annotations with non-
projective dependency structures. The fact that pro-
jective dependency parsers can never exactly repro-
duce the analyses found in non-projective treebanks
is often neglected because of the relative scarcity of
problematic constructions. While the proportion of
sentences containing non-projective dependencies is
often 15?25%, the total proportion of non-projective
arcs is normally only 1?2%. As long as the main
evaluation metric is dependency accuracy per word,
with state-of-the-art accuracy mostly below 90%,
the penalty for not handling non-projective construc-
tions is almost negligible. Still, from a theoretical
point of view, projective parsing of non-projective
structures has the drawback that it rules out perfect
accuracy even as an asymptotic goal.
99
(?Only one of them concerns quality.?)
R
Z
(Out-of
 
?
AuxP
P
nich
them
 
?
Atr
VB
je
is
T
jen
only
 
?
AuxZ
C
jedna
one-FEM-SG
 
?
Sb
R
na
to
 
?
AuxP
N4
kvalitu
quality
?
 
Adv
Z:
.
.)
 
?
AuxZ
Figure 1: Dependency graph for Czech sentence from the Prague Dependency Treebank1
There exist a few robust broad-coverage parsers
that produce non-projective dependency structures,
notably Tapanainen and Ja?rvinen (1997) and Wang
and Harper (2004) for English, Foth et al (2004)
for German, and Holan (2004) for Czech. In addi-
tion, there are several approaches to non-projective
dependency parsing that are still to be evaluated in
the large (Covington, 1990; Kahane et al, 1998;
Duchier and Debusmann, 2001; Holan et al, 2001;
Hellwig, 2003). Finally, since non-projective con-
structions often involve long-distance dependencies,
the problem is closely related to the recovery of
empty categories and non-local dependencies in
constituency-based parsing (Johnson, 2002; Dienes
and Dubey, 2003; Jijkoun and de Rijke, 2004; Cahill
et al, 2004; Levy and Manning, 2004; Campbell,
2004).
In this paper, we show how non-projective depen-
dency parsing can be achieved by combining a data-
driven projective parser with special graph transfor-
mation techniques. First, the training data for the
parser is projectivized by applying a minimal num-
ber of lifting operations (Kahane et al, 1998) and
encoding information about these lifts in arc labels.
When the parser is trained on the transformed data,
it will ideally learn not only to construct projective
dependency structures but also to assign arc labels
that encode information about lifts. By applying an
inverse transformation to the output of the parser,
arcs with non-standard labels can be lowered to their
proper place in the dependency graph, giving rise
1The dependency graph has been modified to make the final
period a dependent of the main verb instead of being a depen-
dent of a special root node for the sentence.
to non-projective structures. We call this pseudo-
projective dependency parsing, since it is based on a
notion of pseudo-projectivity (Kahane et al, 1998).
The rest of the paper is structured as follows.
In section 2 we introduce the graph transformation
techniques used to projectivize and deprojectivize
dependency graphs, and in section 3 we describe the
data-driven dependency parser that is the core of our
system. We then evaluate the approach in two steps.
First, in section 4, we evaluate the graph transfor-
mation techniques in themselves, with data from the
Prague Dependency Treebank and the Danish De-
pendency Treebank. In section 5, we then evaluate
the entire parsing system by training and evaluating
on data from the Prague Dependency Treebank.
2 Dependency Graph Transformations
We assume that the goal in dependency parsing is to
construct a labeled dependency graph of the kind de-
picted in Figure 1. Formally, we define dependency
graphs as follows:
1. Let R = {r1, . . . , rm} be the set of permissible
dependency types (arc labels).
2. A dependency graph for a string of words
W = w1? ? ?wn is a labeled directed graph
D = (W,A), where
(a) W is the set of nodes, i.e. word tokens in
the input string, ordered by a linear prece-
dence relation <,
(b) A is a set of labeled arcs (wi, r, wj), where
wi, wj ?W , r ? R,
(c) for every wj ?W , there is at most one arc
(wi, r, wj) ? A.
100
(?Only one of them concerns quality.?)
R
Z
(Out-of
 
?
AuxP
P
nich
them
 
?
Atr
VB
je
is
T
jen
only
 
?
AuxZ
C
jedna
one-FEM-SG
 
?
Sb
R
na
to
 
?
AuxP
N4
kvalitu
quality
?
 
Adv
Z:
.
.)
 
?
AuxZ
Figure 2: Projectivized dependency graph for Czech sentence
3. A graph D = (W,A) is well-formed iff it is
acyclic and connected.
If (wi, r, wj) ? A, we say that wi is the head of wj
and wj a dependent of wi. In the following, we use
the notation wi
r? wj to mean that (wi, r, wj) ? A;
we also use wi ? wj to denote an arc with unspeci-
fied label and wi ?? wj for the reflexive and transi-
tive closure of the (unlabeled) arc relation.
The dependency graph in Figure 1 satisfies all the
defining conditions above, but it fails to satisfy the
condition of projectivity (Kahane et al, 1998):
1. An arc wi?wk is projective iff, for every word
wj occurring between wi and wk in the string
(wi<wj<wk or wi>wj>wk), wi ?? wj .
2. A dependency graph D = (W,A) is projective
iff every arc in A is projective.
The arc connecting the head jedna (one) to the de-
pendent Z (out-of) spans the token je (is), which is
not dominated by jedna.
As observed by Kahane et al (1998), any (non-
projective) dependency graph can be transformed
into a projective one by a lifting operation, which
replaces each non-projective arc wj ? wk by a pro-
jective arc wi ? wk such that wi ?? wj holds in
the original graph. Here we use a slightly different
notion of lift, applying to individual arcs and moving
their head upwards one step at a time:
LIFT(wj ? wk) =
{
wi ? wk if wi ? wj
undefined otherwise
Intuitively, lifting an arc makes the word wk depen-
dent on the head wi of its original head wj (which is
unique in a well-formed dependency graph), unless
wj is a root in which case the operation is undefined
(but then wj ? wk is necessarily projective if the
dependency graph is well-formed).
Projectivizing a dependency graph by lifting non-
projective arcs is a nondeterministic operation in the
general case. However, since we want to preserve
as much of the original structure as possible, we
are interested in finding a transformation that in-
volves a minimal number of lifts. Even this may
be nondeterministic, in case the graph contains sev-
eral non-projective arcs whose lifts interact, but we
use the following algorithm to construct a minimal
projective transformation D? = (W,A?) of a (non-
projective) dependency graph D = (W,A):
PROJECTIVIZE(W , A)
1 A? ? A
2 while (W,A?) is non-projective
3 a? SMALLEST-NONP-ARC(A?)
4 A? ? (A? ? {a}) ? {LIFT(a)}
5 return (W,A?)
The function SMALLEST-NONP-ARC returns the
non-projective arc with the shortest distance from
head to dependent (breaking ties from left to right).
Applying the function PROJECTIVIZE to the graph
in Figure 1 yields the graph in Figure 2, where the
problematic arc pointing to Z has been lifted from
the original head jedna to the ancestor je. Using
the terminology of Kahane et al (1998), we say that
jedna is the syntactic head of Z, while je is its linear
head in the projectivized representation.
Unlike Kahane et al (1998), we do not regard a
projectivized representation as the final target of the
parsing process. Instead, we want to apply an in-
101
Lifted arc label Path labels Number of labels
Baseline d p n
Head d?h p n(n+ 1)
Head+Path d?h p? 2n(n+ 1)
Path d? p? 4n
Table 1: Encoding schemes (d = dependent, h = syntactic head, p = path; n = number of dependency types)
verse transformation to recover the underlying (non-
projective) dependency graph. In order to facilitate
this task, we extend the set of arc labels to encode
information about lifting operations. In principle, it
would be possible to encode the exact position of the
syntactic head in the label of the arc from the linear
head, but this would give a potentially infinite set of
arc labels and would make the training of the parser
very hard. In practice, we can therefore expect a
trade-off such that increasing the amount of infor-
mation encoded in arc labels will cause an increase
in the accuracy of the inverse transformation but a
decrease in the accuracy with which the parser can
construct the labeled representations. To explore this
tradeoff, we have performed experiments with three
different encoding schemes (plus a baseline), which
are described schematically in Table 1.
The baseline simply retains the original labels for
all arcs, regardless of whether they have been lifted
or not, and the number of distinct labels is therefore
simply the number n of distinct dependency types.2
In the first encoding scheme, called Head, we use
a new label d?h for each lifted arc, where d is the
dependency relation between the syntactic head and
the dependent in the non-projective representation,
and h is the dependency relation that the syntactic
head has to its own head in the underlying structure.
Using this encoding scheme, the arc from je to Z
in Figure 2 would be assigned the label AuxP?Sb
(signifying an AuxP that has been lifted from a Sb).
In the second scheme, Head+Path, we in addition
modify the label of every arc along the lifting path
from the syntactic to the linear head so that if the
original label is p the new label is p?. Thus, the arc
from je to jedna will be labeled Sb? (to indicate that
there is a syntactic head below it). In the third and
final scheme, denoted Path, we keep the extra infor-
2Note that this is a baseline for the parsing experiment only
(Experiment 2). For Experiment 1 it is meaningless as a base-
line, since it would result in 0% accuracy.
mation on path labels but drop the information about
the syntactic head of the lifted arc, using the label d?
instead of d?h (AuxP? instead of AuxP?Sb).
As can be seen from the last column in Table 1,
both Head and Head+Path may theoretically lead
to a quadratic increase in the number of distinct arc
labels (Head+Path being worse than Head only by
a constant factor), while the increase is only linear in
the case of Path. On the other hand, we can expect
Head+Path to be the most useful representation for
reconstructing the underlying non-projective depen-
dency graph. In approaching this problem, a vari-
ety of different methods are conceivable, including
a more or less sophisticated use of machine learn-
ing. In the present study, we limit ourselves to an
algorithmic approach, using a deterministic breadth-
first search. The details of the transformation proce-
dure are slightly different depending on the encod-
ing schemes:
? Head: For every arc of the form wi
d?h
?? wn,
we search the graph top-down, left-to-right,
breadth-first starting at the head node wi. If we
find an arc wl
h?? wm, called a target arc, we
replace wi
d?h
?? wn by wm
d?? wn; otherwise
we replace wi
d?h
?? wn by wi
d?? wn (i.e. we
let the linear head be the syntactic head).
? Head+Path: Same as Head, but the search
only follows arcs of the form wj
p?
?? wk and a
target arc must have the form wl
h?
?? wm; if no
target arc is found, Head is used as backoff.
? Path: Same as Head+Path, but a target arc
must have the form wl
p?
?? wm and no out-
going arcs of the form wm
p??
?? wo; no backoff.
In section 4 we evaluate these transformations with
respect to projectivized dependency treebanks, and
in section 5 they are applied to parser output. Before
102
Feature type Top?1 Top Next Next+1 Next+2 Next+3
Word form + + + +
Part-of-speech + + + + + +
Dep type of head +
leftmost dep + +
rightmost dep +
Table 2: Features used in predicting the next parser action
we turn to the evaluation, however, we need to intro-
duce the data-driven dependency parser used in the
latter experiments.
3 Memory-Based Dependency Parsing
In the experiments below, we employ a data-driven
deterministic dependency parser producing labeled
projective dependency graphs,3 previously tested on
Swedish (Nivre et al, 2004) and English (Nivre and
Scholz, 2004). The parser builds dependency graphs
by traversing the input from left to right, using a
stack to store tokens that are not yet complete with
respect to their dependents. At each point during the
derivation, the parser has a choice between pushing
the next input token onto the stack ? with or with-
out adding an arc from the token on top of the stack
to the token pushed ? and popping a token from the
stack ? with or without adding an arc from the next
input token to the token popped. More details on the
parsing algorithm can be found in Nivre (2003).
The choice between different actions is in general
nondeterministic, and the parser relies on a memory-
based classifier, trained on treebank data, to pre-
dict the next action based on features of the cur-
rent parser configuration. Table 2 shows the features
used in the current version of the parser. At each
point during the derivation, the prediction is based
on six word tokens, the two topmost tokens on the
stack, and the next four input tokens. For each to-
ken, three types of features may be taken into ac-
count: the word form; the part-of-speech assigned
by an automatic tagger; and labels on previously as-
signed dependency arcs involving the token ? the arc
from its head and the arcs to its leftmost and right-
most dependent, respectively. Except for the left-
3The graphs satisfy all the well-formedness conditions given
in section 2 except (possibly) connectedness. For robustness
reasons, the parser may output a set of dependency trees instead
of a single tree.
most dependent of the next input token, dependency
type features are limited to tokens on the stack.
The prediction based on these features is a k-
nearest neighbor classification, using the IB1 algo-
rithm and k = 5, the modified value difference met-
ric (MVDM) and class voting with inverse distance
weighting, as implemented in the TiMBL software
package (Daelemans et al, 2003). More details on
the memory-based prediction can be found in Nivre
et al (2004) and Nivre and Scholz (2004).
4 Experiment 1: Treebank Transformation
The first experiment uses data from two dependency
treebanks. The Prague Dependency Treebank (PDT)
consists of more than 1M words of newspaper text,
annotated on three levels, the morphological, ana-
lytical and tectogrammatical levels (Hajic?, 1998).
Our experiments all concern the analytical annota-
tion, and the first experiment is based only on the
training part. The Danish Dependency Treebank
(DDT) comprises about 100K words of text selected
from the Danish PAROLE corpus, with annotation
of primary and secondary dependencies (Kromann,
2003). The entire treebank is used in the experiment,
but only primary dependencies are considered.4 In
all experiments, punctuation tokens are included in
the data but omitted in evaluation scores.
In the first part of the experiment, dependency
graphs from the treebanks were projectivized using
the algorithm described in section 2. As shown in
Table 3, the proportion of sentences containing some
non-projective dependency ranges from about 15%
in DDT to almost 25% in PDT. However, the over-
all percentage of non-projective arcs is less than 2%
in PDT and less than 1% in DDT. The last four
4If secondary dependencies had been included, the depen-
dency graphs would not have satisfied the well-formedness con-
ditions formulated in section 2.
103
# Lifts in projectivization
Data set # Sentences % NonP # Tokens % NonP 1 2 3 >3
PDT training 73,088 23.15 1,255,333 1.81 93.79 5.60 0.51 0.11
DDT total 5,512 15.48 100,238 0.94 79.49 13.28 4.36 2.87
Table 3: Non-projective sentences and arcs in PDT and DDT (NonP = non-projective)
Data set Head H+P Path
PDT training (28 labels) 92.3 (230) 99.3 (314) 97.3 (84)
DDT total (54 labels) 92.3 (123) 99.8 (147) 98.3 (99)
Table 4: Percentage of non-projective arcs recovered correctly (number of labels in parentheses)
columns in Table 3 show the distribution of non-
projective arcs with respect to the number of lifts
required. It is worth noting that, although non-
projective constructions are less frequent in DDT
than in PDT, they seem to be more deeply nested,
since only about 80% can be projectivized with a
single lift, while almost 95% of the non-projective
arcs in PDT only require a single lift.
In the second part of the experiment, we applied
the inverse transformation based on breadth-first
search under the three different encoding schemes.
The results are given in Table 4. As expected, the
most informative encoding, Head+Path, gives the
highest accuracy with over 99% of all non-projective
arcs being recovered correctly in both data sets.
However, it can be noted that the results for the least
informative encoding, Path, are almost comparable,
while the third encoding, Head, gives substantially
worse results for both data sets. We also see that
the increase in the size of the label sets for Head
and Head+Path is far below the theoretical upper
bounds given in Table 1. The increase is gener-
ally higher for PDT than for DDT, which indicates a
greater diversity in non-projective constructions.
5 Experiment 2: Memory-Based Parsing
The second experiment is limited to data from PDT.5
The training part of the treebank was projectivized
under different encoding schemes and used to train
memory-based dependency parsers, which were run
on the test part of the treebank, consisting of 7,507
5Preliminary experiments using data from DDT indicated
that the limited size of the treebank creates a severe sparse data
problem with respect to non-projective constructions.
sentences and 125,713 tokens.6 The inverse trans-
formation was applied to the output of the parsers
and the result compared to the gold standard test set.
Table 5 shows the overall parsing accuracy at-
tained with the three different encoding schemes,
compared to the baseline (no special arc labels) and
to training directly on non-projective dependency
graphs. Evaluation metrics used are Attachment
Score (AS), i.e. the proportion of tokens that are at-
tached to the correct head, and Exact Match (EM),
i.e. the proportion of sentences for which the depen-
dency graph exactly matches the gold standard. In
the labeled version of these metrics (L) both heads
and arc labels must be correct, while the unlabeled
version (U) only considers heads.
The first thing to note is that projectivizing helps
in itself, even if no encoding is used, as seen from
the fact that the projective baseline outperforms the
non-projective training condition by more than half
a percentage point on attachment score, although the
gain is much smaller with respect to exact match.
The second main result is that the pseudo-projective
approach to parsing (using special arc labels to guide
an inverse transformation) gives a further improve-
ment of about one percentage point on attachment
score. With respect to exact match, the improvement
is even more noticeable, which shows quite clearly
that even if non-projective dependencies are rare on
the token level, they are nevertheless important for
getting the global syntactic structure correct.
All improvements over the baseline are statisti-
cally significant beyond the 0.01 level (McNemar?s
6The part-of-speech tagging used in both training and testing
was the uncorrected output of an HMM tagger distributed with
the treebank; cf. Hajic? et al (2001a).
104
Encoding UAS LAS UEM LEM
Non-projective 78.5 71.3 28.9 20.6
Baseline 79.1 72.0 29.2 20.7
Head 80.1 72.8 31.6 22.2
Head+Path 80.0 72.8 31.8 22.4
Path 80.0 72.7 31.6 22.0
Table 5: Parsing accuracy (AS = attachment score, EM = exact match; U = unlabeled, L = labeled)
Unlabeled Labeled
Encoding P R F P R F
Head 61.3 54.1 57.5 55.2 49.8 52.4
Head+Path 63.9 54.9 59.0 57.9 50.6 54.0
Path 58.2 49.5 53.4 51.0 45.7 48.2
Table 6: Precision, recall and F-measure for non-projective arcs
test). By contrast, when we turn to a comparison
of the three encoding schemes it is hard to find any
significant differences, and the overall impression is
that it makes little or no difference which encoding
scheme is used, as long as there is some indication
of which words are assigned their linear head instead
of their syntactic head by the projective parser. This
may seem surprising, given the experiments reported
in section 4, but the explanation is probably that the
non-projective dependencies that can be recovered at
all are of the simple kind that only requires a single
lift, where the encoding of path information is often
redundant. It is likely that the more complex cases,
where path information could make a difference, are
beyond the reach of the parser in most cases.
However, if we consider precision, recall and F-
measure on non-projective dependencies only, as
shown in Table 6, some differences begin to emerge.
The most informative scheme, Head+Path, gives
the highest scores, although with respect to Head
the difference is not statistically significant, while
the least informative scheme, Path ? with almost the
same performance on treebank transformation ? is
significantly lower (p < 0.01). On the other hand,
given that all schemes have similar parsing accuracy
overall, this means that the Path scheme is the least
likely to introduce errors on projective arcs.
The overall parsing accuracy obtained with the
pseudo-projective approach is still lower than for the
best projective parsers. Although the best published
results for the Collins parser is 80% UAS (Collins,
1999), this parser reaches 82% when trained on the
entire training data set, and an adapted version of
Charniak?s parser (Charniak, 2000) performs at 84%
(Jan Hajic?, pers. comm.). However, the accuracy is
considerably higher than previously reported results
for robust non-projective parsing of Czech, with a
best performance of 73% UAS (Holan, 2004).
Compared to related work on the recovery of
long-distance dependencies in constituency-based
parsing, our approach is similar to that of Dienes
and Dubey (2003) in that the processing of non-local
dependencies is partly integrated in the parsing pro-
cess, via an extension of the set of syntactic cate-
gories, whereas most other approaches rely on post-
processing only. However, while Dienes and Dubey
recognize empty categories in a pre-processing step
and only let the parser find their antecedents, we use
the parser both to detect dislocated dependents and
to predict either the type or the location of their syn-
tactic head (or both) and use post-processing only to
transform the graph in accordance with the parser?s
analysis.
6 Conclusion
We have presented a new method for non-projective
dependency parsing, based on a combination of
data-driven projective dependency parsing and
graph transformation techniques. The main result is
that the combined system can recover non-projective
dependencies with a precision sufficient to give a
significant improvement in overall parsing accuracy,
105
especially with respect to the exact match criterion,
leading to the best reported performance for robust
non-projective parsing of Czech.
Acknowledgements
This work was supported in part by the Swedish
Research Council (621-2002-4207). Memory-based
classifiers for the experiments were created using
TiMBL (Daelemans et al, 2003). Special thanks to
Jan Hajic? and Matthias Trautner Kromann for assis-
tance with the Czech and Danish data, respectively,
and to Jan Hajic?, Toma?s? Holan, Dan Zeman and
three anonymous reviewers for valuable comments
on a preliminary version of the paper.
References
Cahill, A., Burke, M., O?Donovan, R., Van Genabith, J. and
Way, A. 2004. Long-distance dependency resolution in
automatically acquired wide-coverage PCFG-based LFG ap-
proximations. In Proceedings of ACL.
Campbell, R. 2004. Using linguistic principles to recover
empty categories. In Proceedings of ACL.
Charniak, E. 2000. A maximum-entropy-inspired parser. In
Proceedings of NAACL.
Collins, M., Hajic?, J., Brill, E., Ramshaw, L. and Tillmann, C.
1999. A statistical parser for Czech. In Proceedings of ACL.
Collins, M. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
Covington, M. A. 1990. Parsing discontinuous constituents in
dependency grammar. Computational Linguistics, 16:234?
236.
Daelemans, W., Zavrel, J., van der Sloot, K. and van den Bosch,
A. 2003. TiMBL: Tilburg Memory Based Learner, version
5.0, Reference Guide. Technical Report ILK 03-10, Tilburg
University, ILK.
Dienes, P. and Dubey, A. 2003. Deep syntactic processing by
combining shallow methods. In Proceedings of ACL.
Duchier, D. and Debusmann, R. 2001. Topological dependency
trees: A constraint-based account of linear precedence. In
Proceedings of ACL.
Eisner, J. M. 1996. Three new probabilistic models for depen-
dency parsing: An exploration. In Proceedings of COLING.
Foth, K., Daum, M. and Menzel, W. 2004. A broad-coverage
parser for German based on defeasible constraints. In Pro-
ceedings of KONVENS.
Hajic?, J., Krbec, P., Oliva, K., Kveton, P. and Petkevic, V. 2001.
Serial combination of rules and statistics: A case study in
Czech tagging. In Proceedings of ACL.
Hajic?, J., Vidova Hladka, B., Panevova?, J., Hajic?ova?, E., Sgall,
P. and Pajas, P. 2001. Prague Dependency Treebank 1.0.
LDC, 2001T10.
Hajic?, J. 1998. Building a syntactically annotated corpus:
The Prague Dependency Treebank. In Issues of Valency and
Meaning, pages 106?132. Karolinum.
Hellwig, P. 2003. Dependency unification grammar. In Depen-
dency and Valency, pages 593?635. Walter de Gruyter.
Holan, T., Kubon?, V. and Pla?tek, M. 2001. Word-order re-
laxations and restrictions within a dependency grammar. In
Proceedings of IWPT.
Holan, T. 2004. Tvorba zavislostniho syntaktickeho analyza-
toru. In Proceedings of MIS?2004.
Jijkoun, V. and de Rijke, M. 2004. Enriching the output of
a parser using memory-based learning. In Proceedings of
ACL.
Johnson, M. 2002. A simple pattern-matching algorithm for re-
covering empty nodes and their antecedents. In Proceedings
of ACL.
Kahane, S., Nasr, A. and Rambow, O. 1998. Pseudo-
projectivity: A polynomially parsable non-projective depen-
dency grammar. In Proceedings of ACL-COLING.
Kromann, M. T. 2003. The Danish Dependency Treebank and
the DTAG treebank tool. In Proceedings of TLT 2003.
Levy, R. and Manning, C. 2004. Deep dependencies from
context-free statistical parsers: Correcting the surface depen-
dency approximation. In Proceedings of ACL.
Mel?c?uk, I. 1988. Dependency Syntax: Theory and Practice.
State University of New York Press.
Nivre, J. and Scholz, M. 2004. Deterministic dependency pars-
ing of English text. In Proceedings of COLING.
Nivre, J., Hall, J. and Nilsson, J. 2004. Memory-based depen-
dency parsing. In Proceedings of CoNLL.
Nivre, J. 2003. An efficient algorithm for projective depen-
dency parsing. In Proceedings of IWPT.
Oflazer, K., Say, B., Hakkani-Tu?r, D. Z. and Tu?r, G. 2003.
Building a Turkish treebank. In Treebanks: Building and
Using Parsed Corpora, pages 261?277. Kluwer Academic
Publishers.
Oflazer, K. 2003. Dependency parsing with an extended finite-
state approach. Computational Linguistics, 29:515?544.
Sleator, D. and Temperley, D. 1993. Parsing English with a
link grammar. In Proceedings of IWPT.
Tapanainen, P. and Ja?rvinen, T. 1997. A non-projective depen-
dency parser. In Proceedings of ANLP.
Wang, W. and Harper, M. P. 2004. A statistical constraint
dependency grammar (CDG) parser. In Proceedings of the
Workshop in Incremental Parsing (ACL).
Yamada, H. and Matsumoto, Y. 2003. Statistical dependency
analysis with support vector machines. In Proceedings of
IWPT.
106
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 257?264,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Graph Transformations in Data-Driven Dependency Parsing
Jens Nilsson
Va?xjo? University
jni@msi.vxu.se
Joakim Nivre
Va?xjo? University and
Uppsala University
nivre@msi.vxu.se
Johan Hall
Va?xjo? University
jha@msi.vxu.se
Abstract
Transforming syntactic representations in
order to improve parsing accuracy has
been exploited successfully in statistical
parsing systems using constituency-based
representations. In this paper, we show
that similar transformations can give sub-
stantial improvements also in data-driven
dependency parsing. Experiments on the
Prague Dependency Treebank show that
systematic transformations of coordinate
structures and verb groups result in a
10% error reduction for a deterministic
data-driven dependency parser. Combin-
ing these transformations with previously
proposed techniques for recovering non-
projective dependencies leads to state-of-
the-art accuracy for the given data set.
1 Introduction
It has become increasingly clear that the choice
of suitable internal representations can be a very
important factor in data-driven approaches to syn-
tactic parsing, and that accuracy can often be im-
proved by internal transformations of a given kind
of representation. This is well illustrated by the
Collins parser (Collins, 1997; Collins, 1999), scru-
tinized by Bikel (2004), where several transforma-
tions are applied in order to improve the analy-
sis of noun phrases, coordination and punctuation.
Other examples can be found in the work of John-
son (1998) and Klein and Manning (2003), which
show that well-chosen transformations of syntac-
tic representations can greatly improve the parsing
accuracy obtained with probabilistic context-free
grammars.
In this paper, we apply essentially the same
techniques to data-driven dependency parsing,
specifically targeting the analysis of coordination
and verb groups, two very common constructions
that pose special problems for dependency-based
approaches. The basic idea is that we can facili-
tate learning by transforming the training data for
the parser and that we can subsequently recover
the original representations by applying an inverse
transformation to the parser?s output.
The data used in the experiments come from
the Prague Dependency Treebank (PDT) (Hajic?,
1998; Hajic? et al, 2001), the largest avail-
able dependency treebank, annotated according to
the theory of Functional Generative Description
(FGD) (Sgall et al, 1986). The parser used is
MaltParser (Nivre and Hall, 2005; Nivre et al,
2006), a freely available system that combines a
deterministic parsing strategy with discriminative
classifiers for predicting the next parser action.
The paper is structured as follows. Section 2
provides the necessary background, including a
definition of dependency graphs, a discussion of
different approaches to the analysis of coordina-
tion and verb groups in dependency grammar, as
well as brief descriptions of PDT, MaltParser and
some related work. Section 3 introduces a set
of dependency graph transformations, specifically
defined to deal with the dependency annotation
found in PDT, which are experimentally evaluated
in section 4. While the experiments reported in
section 4.1 deal with pure treebank transforma-
tions, in order to establish an upper bound on what
can be achieved in parsing, the experiments pre-
sented in section 4.2 examine the effects of differ-
ent transformations on parsing accuracy. Finally,
in section 4.3, we combine these transformations
with previously proposed techniques in order to
optimize overall parsing accuracy. We conclude
in section 5.
257
2 Background
2.1 Dependency Graphs
The basic idea in dependency parsing is that the
syntactic analysis consists in establishing typed,
binary relations, called dependencies, between the
words of a sentence. This kind of analysis can be
represented by a labeled directed graph, defined as
follows:
? Let R = {r1, . . . , rm} be a set of dependency
types (arc labels).
? A dependency graph for a string of words
W = w1 . . . wn is a labeled directed graph
G = (W,A), where:
? W is the set of nodes, i.e. word tokens
in the input string, ordered by a linear
precedence relation <.
? A is a set of labeled arcs (wi, r, wj), wi,
wj ? W , r ? R.
? A dependency graph G = (W,A) is well-
formed iff it is acyclic and no node has an
in-degree greater than 1.
We will use the notation wi r? wj to symbolize
that (wi, r, wj) ? A, where wi is referred to as
the head and wj as the dependent. We say that
an arc is projective iff, for every word wj occur-
ring between wi and wk (i.e., wi < wj < wk
or wi > wj > wk), there is a path from wi to
wj . A graph is projective iff all its arcs are pro-
jective. Figure 1 shows a well-formed (projective)
dependency graph for a sentence from the Prague
Dependency Treebank.
2.2 Coordination and Verb Groups
Dependency grammar assumes that syntactic
structure consists of lexical nodes linked by binary
dependencies. Dependency theories are thus best
suited for binary syntactic constructions, where
one element can clearly be distinguished as the
syntactic head. The analysis of coordination is
problematic in this respect, since it normally in-
volves at least one conjunction and two conjuncts.
The verb group, potentially consisting of a whole
chain of verb forms, is another type of construc-
tion where the syntactic relation between elements
is not clear-cut in dependency terms.
Several solutions have been proposed to the
problem of coordination. One alternative is
to avoid creating dependency relations between
the conjuncts, and instead let the conjuncts
have a direct dependency relation to the same
head (Tesnie`re, 1959; Hudson, 1990). Another
approach is to make the conjunction the head and
let the conjuncts depend on the conjunction. This
analysis, which appears well motivated on seman-
tic grounds, is adopted in the FGD framework and
will therefore be called Prague style (PS). It is
exemplified in figure 1, where the conjunction a
(and) is the head of the conjuncts bojovnost?? and
tvrdost??. A different solution is to adopt a more
hierarchical analysis, where the conjunction de-
pends on the first conjunct, while the second con-
junct depends on the conjunction. In cases of
multiple coordination, this can be generalized to a
chain, where each element except the first depends
on the preceding one. This more syntactically
oriented approach has been advocated notably by
Mel?c?uk (1988) and will be called Mel?c?uk style
(MS). It is illustrated in figure 2, which shows a
transformed version of the dependency graph in
figure 1, where the elements of the coordination
form a chain with the first conjunct (bojovnost??) as
the topmost head. Lombardo and Lesmo (1998)
conjecture that MS is more suitable than PS for
incremental dependency parsing.
The difference between the more semantically
oriented PS and the more syntactically oriented
MS is seen also in the analysis of verb groups,
where the former treats the main verb as the head,
since it is the bearer of valency, while the latter
treats the auxiliary verb as the head, since it is the
finite element of the clause. Without questioning
the theoretical validity of either approach, we can
again ask which analysis is best suited to achieve
high accuracy in parsing.
2.3 PDT
PDT (Hajic?, 1998; Hajic? et al, 2001) consists of
1.5M words of newspaper text, annotated in three
layers: morphological, analytical and tectogram-
matical. In this paper, we are only concerned
with the analytical layer, which contains a surface-
syntactic dependency analysis, involving a set of
28 dependency types, and not restricted to projec-
tive dependency graphs.1 The annotation follows
FGD, which means that it involves a PS analysis of
both coordination and verb groups. Whether better
parsing accuracy can be obtained by transforming
1About 2% of all dependencies are non-projective and
about 25% of all sentences have a non-projective dependency
graph (Nivre and Nilsson, 2005).
258
(?The final of the tournament was distinguished by great fighting spirit and unexpected hardness?)
A7
Velkou
great
?
Atr
N7
bojovnost??
fighting-spirit
?
Obj Co
J?
a
and
?
Coord
A7
nec?ekanou
unexpected
?
Atr
N7
tvrdost??
hardness
?
Obj Co
P4
se
itself
?
AuxT
Vp
vyznac?ovalo
distinguished
N2
fina?le
final
?
Sb
N2
turnaje
of-the-tournament
?
Atr
Figure 1: Dependency graph for a Czech sentence from the Prague Dependency Treebank
(?The final of the tournament was distinguished by great fighting spirit and unexpected hardness?)
A7
Velkou
great
?
Atr
N7
bojovnost??
fighting-spirit
?
Obj
J?
a
and
?
Coord
A7
nec?ekanou
unexpected
?
Atr
N7
tvrdost??
hardness
?
Obj
P4
se
itself
?
AuxT
Vp
vyznac?ovalo
distinguished
N2
fina?le
final
?
Sb
N2
turnaje
of-the-tournament
?
Atr
Figure 2: Transformed dependency graph for a Czech sentence from the Prague Dependency Treebank
this to MS is one of the hypotheses explored in the
experimental study below.
2.4 MaltParser
MaltParser (Nivre and Hall, 2005; Nivre et al,
2006) is a data-driven parser-generator, which can
induce a dependency parser from a treebank, and
which supports several parsing algorithms and
learning algorithms. In the experiments below we
use the algorithm of Nivre (2003), which con-
structs a labeled dependency graph in one left-
to-right pass over the input. Classifiers that pre-
dict the next parser action are constructed through
memory-based learning (MBL), using the TIMBL
software package (Daelemans and Van den Bosch,
2005), and support vector machines (SVM), using
LIBSVM (Chang and Lin, 2005).
2.5 Related Work
Other ways of improving parsing accuracy with
respect to coordination include learning patterns
of morphological and semantical information for
the conjuncts (Park and Cho, 2000). More specifi-
cally for PDT, Collins et al (1999) relabel coordi-
nated phrases after converting dependency struc-
tures to phrase structures, and Zeman (2004) uses
a kind of pattern matching, based on frequencies
of the parts-of-speech of conjuncts and conjunc-
tions. Zeman also mentions experiments to trans-
form the dependency structure for coordination
but does not present any results.
Graph transformations in dependency parsing
have also been used in order to recover non-
projective dependencies together with parsers that
are restricted to projective dependency graphs.
Thus, Nivre and Nilsson (2005) improve parsing
accuracy for MaltParser by projectivizing training
data and applying an inverse transformation to the
output of the parser, while Hall and Nova?k (2005)
apply post-processing to the output of Charniak?s
parser (Charniak, 2000). In the final experi-
ments below, we combine these techniques with
the transformations investigated in this paper.
3 Dependency Graph Transformations
In this section, we describe algorithms for trans-
forming dependency graphs in PDT from PS to
MS and back, starting with coordination and con-
tinuing with verb groups.
3.1 Coordination
The PS-to-MS transformation for coordination
will be designated ?c(?), where ? is a data set.
The transformation begins with the identification
of a base conjunction, based on its dependency
type (Coord) and/or its part-of-speech (J?). For
example, the word a (and) in figure 1 is identified
as a base conjunction.
259
Before the actual transformation, the base con-
junction and all its dependents need to be classi-
fied into three different categories. First, the base
conjunction is categorized as a separator (S). If
the coordination consists of more than two con-
juncts, it normally has one or more commas sep-
arating conjuncts, in addition to the base conjunc-
tion. These are identified by looking at their de-
pendency type (mostly AuxX) and are also catego-
rized as S. The coordination in figure 1 contains
no commas, so only the word a will belong to S.
The remaining dependents of the base conjunc-
tion need to be divided into conjuncts (C) and
other dependents (D). To make this distinction,
the algorithm again looks at the dependency type.
In principle, the dependency type of a conjunct
has the suffix Co, although special care has to be
taken for coordinated prepositional cases and em-
bedded clauses (Bo?hmova? et al, 2003). The words
bojovnost?? and tvrdost?? in figure 1, both having the
dependency type Obj Co, belong to the category
C. Since there are no other dependents of a, the
coordination contains no instances of the category
D.
Given this classification of the words involved
in a coordination, the transformation ?c(?) is
straightforward and basically connects all the arcs
in a chain. Let C1, . . . , Cn be the elements of C,
ordered by linear precedence, and let S1i , . . . , Smi
be the separators occurring between Ci and Ci+1.
Then every Ci becomes the head of S1i , . . . , Smi ,
Smi becomes the head of Ci+1, and C1 becomes
the only dependent of the original head of the base
conjunction. The dependency types of the con-
juncts are truncated by removing the suffix Co.2
Also, each word in wd ? D becomes a dependent
of the conjunct closest to its left, and if such a word
does not exist, wd will depend on the leftmost con-
junct. After the transformation ?c(?), every coor-
dination forms a left-headed chain, as illustrated
in figure 2.
This new representation creates a problem,
however. It is no longer possible to distinguish the
dependents in D from other dependents of the con-
juncts. For example, the word Velkou in figure 2
is not distinguishable from a possible dependent
in D, which is an obvious drawback when trans-
forming back to PS. One way of distinguishing D
elements is to extend the set of dependency types.
2Preliminary results indicated that this increases parsing
accuracy.
The dependency type r of each wd ? D can be re-
placed by a completely new dependency type r+
(e.g., Atr+), theoretically increasing the number
of dependency types to 2 ? |R|.
The inverse transformation, ??1c (?), again
starts by identifying base conjunctions, using the
same conditions as before. For each identified
base conjunction, it calls a procedure that per-
forms the inverse transformation by traversing
the chain of conjuncts and separators ?upwards?
(right-to-left), collecting conjuncts (C), separators
(S) and potential conjunction dependents (Dpot).
When this is done, the former head of the left-
most conjunct (C1) becomes the head of the right-
most (base) conjunction (Smn?1). In figure 2,
the leftmost conjunct is bojovnost??, with the head
vyznac?ovalo, and the rightmost (and only) con-
junction is a, which will then have vyznac?ovalo as
its new head. All conjuncts in the chain become
dependents of the rightmost conjunction, which
means that the structure is converted back to the
one depicted in figure 1.
As mentioned above, the original structure in
figure 1 did not have any coordination dependents,
but Velkou ? Dpot. The last step of the inverse
transformation is therefore to sort out conjunction
dependents from conjunct dependents, where the
former will attach to the base conjunction. Four
versions have been implemented, two of which
take into account the fact that the dependency
types AuxG, AuxX, AuxY, and Pred are the only
dependency types that are more frequent as con-
junction dependents (D) than as conjunct depen-
dents in the training data set:
? ?c: Do not extend arc labels in ?c. Leave all
words in Dpot in place in ??1c .
? ?c? : Do not extend arc labels in ?c. Attach all
words with label AuxG, AuxX, AuxY or Pred
to the base conjunction in ??1c .
? ?c+: Extend arc labels from r to r+ for D
elements in ?c. Attach all words with label
r+ to the base conjunction (and change the
label to r) in ??1c .
? ?c+? : Extend arc labels from r to r+ for D
elements in ?c, except for the labels AuxG,
AuxX, AuxY and Pred. Attach all words with
label r+, AuxG, AuxX, AuxY, or Pred to the
base conjunction (and change the label to r if
necessary) in ??1c .
260
3.2 Verb Groups
To transform verb groups from PS to MS, the
transformation algorithm, ?v(?), starts by identi-
fying all auxiliary verbs in a sentence. These will
belong to the set A and are processed from left to
right. A word waux ? A iff wmain AuxV?? waux,
where wmain is the main verb. The transformation
into MS reverses the relation between the verbs,
i.e., waux AuxV?? wmain, and the former head of
wmain becomes the new head of waux. The main
verb can be located on either side of the auxiliary
verb and can have other dependents (whereas aux-
iliary verbs never have dependents), which means
that dependency relations to other dependents of
wmain may become non-projective through the
transformation. To avoid this, all dependents to
the left of the rightmost verb will depend on the
leftmost verb, whereas the others will depend on
the rightmost verb.
Performing the inverse transformation for verb
groups, ??1v (?), is quite simple and essentially
the same procedure inverted. Each sentence is tra-
versed from right to left looking for arcs of the
type waux AuxV?? wmain. For every such arc, the
head of waux will be the new head of wmain, and
wmain the new head of waux. Furthermore, since
waux does not have dependents in PS, all depen-
dents of waux in MS will become dependents of
wmain in PS.
4 Experiments
All experiments are based on PDT 1.0, which is
divided into three data sets, a training set (?t), a
development test set (?d), and an evaluation test
set (?e). Table 1 shows the size of each data set, as
well as the relative frequency of the specific con-
structions that are in focus here. Only 1.3% of all
words in the training data are identified as auxil-
iary verbs (A), whereas coordination (S and C)
is more common in PDT. This implies that coor-
dination transformations are more likely to have
a greater impact on overall accuracy compared to
the verb group transformations.
In the parsing experiments reported in sections
4.1?4.2, we use ?t for training, ?d for tuning, and
?e for the final evaluation. The part-of-speech
tagging used (both in training and testing) is the
HMM tagging distributed with the treebank, with
a tagging accuracy of 94.1%, and with the tagset
compressed to 61 tags as in Collins et al (1999).
Data #S #W %S %C %A
?t 73088 1256k 3.9 7.7 1.3
?d 7319 126k 4.0 7.8 1.4
?e 7507 126k 3.8 7.3 1.4
Table 1: PDT data sets; S = sentence, W = word;
S = separator, C = conjunct, A = auxiliary verb
T AS
?c 97.8
?c? 98.6
?c+ 99.6
?c+? 99.4
?v 100.0
Table 2: Transformations; T = transformation;
AS = attachment score (unlabeled) of ??1(?(?t))
compared to ?t
MaltParser is used with the parsing algorithm of
Nivre (2003) together with the feature model used
for parsing Czech by Nivre and Nilsson (2005).
In section 4.2 we use MBL, again with the same
settings as Nivre and Nilsson (2005),3 and in sec-
tion 4.2 we use SVM with a polynomial kernel of
degree 2.4 The metrics for evaluation are the at-
tachment score (AS) (labeled and unlabeled), i.e.,
the proportion of words that are assigned the cor-
rect head, and the exact match (EM) score (labeled
and unlabeled), i.e., the proportion of sentences
that are assigned a completely correct analysis.
All tokens, including punctuation, are included in
the evaluation scores. Statistical significance is as-
sessed using McNemar?s test.
4.1 Experiment 1: Transformations
The algorithms are fairly simple. In addition, there
will always be a small proportion of syntactic con-
structions that do not follow the expected pattern.
Hence, the transformation and inverse transforma-
tion will inevitably result in some distortion. In
order to estimate the expected reduction in pars-
ing accuracy due to this distortion, we first con-
sider a pure treebank transformation experiment,
where we compare ??1(?(?t)) to ?t, for all the
different transformations ? defined in the previous
section. The results are shown in table 2.
We see that, even though coordination is more
frequent, verb groups are easier to handle.5 The
3TIMBL parameters: -k5 -mM -L3 -w0 -dID.
4LIBSVM parameters: -s0 -t1 -d2 -g0.12 -r0 -c1 -e0.1.
5The result is rounded to 100.0% but the transformed tree-
261
coordination version with the least loss of infor-
mation (?c+) fails to recover the correct head for
0.4% of all words in ?t.
The difference between ?c+ and ?c is expected.
However, in the next section this will be contrasted
with the increased burden on the parser for ?c+,
since it is also responsible for selecting the correct
dependency type for each arc among as many as
2 ? |R| types instead of |R|.
4.2 Experiment 2: Parsing
Parsing experiments are carried out in four steps
(for a given transformation ? ):
1. Transform the training data set into ?(?t).
2. Train a parser p on ?(?t).
3. Parse a test set ? using p with output p(?).
4. Transform the parser output into ??1(p(?)).
Table 3 presents the results for a selection of trans-
formations using MaltParser with MBL, tested on
the evaluation test set ?e with the untransformed
data as baseline. Rows 2?5 show that transform-
ing coordinate structures to MS improves parsing
accuracy compared to the baseline, regardless of
which transformation and inverse transformation
are used. Moreover, the parser benefits from the
verb group transformation, as seen in row 6.
The final row shows the best combination of a
coordination transformation with the verb group
transformation, which amounts to an improvement
of roughly two percentage points, or a ten percent
overall error reduction, for unlabeled accuracy.
All improvements over the baseline are statis-
tically significant (McNemar?s test) with respect
to attachment score (labeled and unlabeled) and
unlabeled exact match, with p < 0.01 except for
the unlabeled exact match score of the verb group
transformation, where 0.01 < p < 0.05. For the
labeled exact match, no differences are significant.
The experimental results indicate that MS is
more suitable than PS as the target representation
for deterministic data-driven dependency parsing.
A relevant question is of course why this is the
case. A partial explanation may be found in the
?short-dependency preference? exhibited by most
parsers (Eisner and Smith, 2005), with MaltParser
being no exception. The first row of table 4 shows
the accuracy of the parser for different arc lengths
under the baseline condition (i.e., with no trans-
formations). We see that it performs very well on
bank contains 19 erroneous heads.
AS EM
T U L U L
None 79.08 72.83 28.99 21.15
?c 80.55 74.06 30.08 21.27
?c? 80.90 74.41 30.56 21.42
?c+ 80.58 74.07 30.42 21.17
?c+? 80.87 74.36 30.89 21.38
?v 79.28 72.97 29.53 21.38
?v??c+? 81.01 74.51 31.02 21.57
Table 3: Parsing accuracy (MBL, ?e); T = trans-
formation; AS = attachment score, EM = exact
match; U = unlabeled, L = labeled
AS ?e 90.1 83.6 70.5 59.5 45.9
Length: 1 2-3 4-6 7-10 11-
?t 51.9 29.4 11.2 4.4 3.0
?c(?t) 54.1 29.1 10.7 3.8 2.4
?v(?t) 52.9 29.2 10.7 4.2 2.9
Table 4: Baseline labeled AS per arc length on ?e
(row 1); proportion of arcs per arc length in ?t
(rows 3?5)
short arcs, but that accuracy drops quite rapidly
as the arcs get longer. This can be related to the
mean arc length in ?t, which is 2.59 in the un-
transformed version, 2.40 in ?c(?t) and 2.54 in
?v(?t). Rows 3-5 in table 4 show the distribution
of arcs for different arc lengths in different ver-
sions of the data set. Both ?c and ?v make arcs
shorter on average, which may facilitate the task
for the parser.
Another possible explanation is that learning is
facilitated if similar constructions are represented
similarly. For instance, it is probable that learning
is made more difficult when a unit has different
heads depending on whether it is part of a coordi-
nation or not.
4.3 Experiment 3: Optimization
In this section we combine the best results from
the previous section with the graph transforma-
tions proposed by Nivre and Nilsson (2005) to re-
cover non-projective dependencies. We write ?p
for the projectivization of training data and ??1p for
the inverse transformation applied to the parser?s
output.6 In addition, we replace MBL with SVM,
a learning algorithm that tends to give higher accu-
racy in classifier-based parsing although it is more
6More precisely, we use the variant called PATH in Nivre
and Nilsson (2005).
262
AS EM
T LA U L U L
None MBL 79.08 72.83 28.99 21.15
?p MBL 80.79 74.39 31.54 22.53
?p??v??c+? MBL 82.93 76.31 34.17 23.01
None SVM 81.09 75.68 32.24 25.02
?p SVM 82.93 77.28 35.99 27.05
?p??v??c+? SVM 84.55 78.82 37.63 27.69
Table 5: Optimized parsing results (SVM, ?e); T = transformation; LA = learning algorithm; AS =
attachment score, EM = exact match; U = unlabeled, L = labeled
T P:S R:S P:C R:C P:A R:A P:M R:M
None 52.63 72.35 55.15 67.03 82.17 82.21 69.95 69.07
?p??v??c+? 63.73 82.10 63.20 75.14 90.89 92.79 80.02 81.40
Table 6: Detailed results for SVM; T = transformation; P = unlabeled precision, R = unlabeled recall
costly to train (Sagae and Lavie, 2005).
Table 5 shows the results, for both MBL and
SVM, of the baseline, the pure pseudo-projective
parsing, and the combination of pseudo-projective
parsing with PS-to-MS transformations. We see
that pseudo-projective parsing brings a very con-
sistent increase in accuracy of at least 1.5 percent-
age points, which is more than that reported by
Nivre and Nilsson (2005), and that the addition
of the PS-to-MS transformations increases accu-
racy with about the same margin. We also see that
SVM outperforms MBL by about two percentage
points across the board, and that the positive effect
of the graph transformations is most pronounced
for the unlabeled exact match score, where the
improvement is more than five percentage points
overall for both MBL and SVM.
Table 6 gives a more detailed analysis of the
parsing results for SVM, comparing the optimal
parser to the baseline, and considering specifically
the (unlabeled) precision and recall of the cate-
gories involved in coordination (separators S and
conjuncts C) and verb groups (auxiliary verbs A
and main verbs M ). All figures indicate, with-
out exception, that the transformations result in
higher precision and recall for all directly involved
words. (All differences are significant beyond the
0.01 level.) It is worth noting that the error reduc-
tion is actually higher for A and M than for S and
C, although the former are less frequent.
With respect to unlabeled attachment score, the
results of the optimized parser are slightly below
the best published results for a single parser. Hall
and Nova?k (2005) report a score of 85.1%, apply-
ing a corrective model to the output of Charniak?s
parser; McDonald and Pereira (2006) achieve a
score of 85.2% using a second-order spanning tree
algorithm. Using ensemble methods and a pool of
different parsers, Zeman and ?Zabokrtsky? (2005)
attain a top score of 87.0%. For unlabeled exact
match, our results are better than any previously
reported results, including those of McDonald and
Pereira (2006). (For the labeled scores, we are not
aware of any comparable results in the literature.)
5 Conclusion
The results presented in this paper confirm that
choosing the right representation is important
in parsing. By systematically transforming the
representation of coordinate structures and verb
groups in PDT, we achieve a 10% error reduc-
tion for a data-driven dependency parser. Adding
graph transformations for non-projective depen-
dency parsing gives a total error reduction of
about 20% (even more for unlabeled exact match).
In this way, we achieve state-of-the-art accuracy
with a deterministic, classifier-based dependency
parser.
Acknowledgements
The research presented in this paper was partially
supported by the Swedish Research Council. We
are grateful to Jan Hajic? and Daniel Zeman for
help with the Czech data and to three anonymous
reviewers for helpful comments and suggestions.
263
References
Daniel M. Bikel. 2004. Intricacies of Collins? parsing
model. Computational Linguistics, 30:479?511.
Alena Bo?hmova?, Jan Hajic?, Eva Hajic?ova?, and Barbora
Hladka?. 2003. The Prague Dependency Treebank:
A three-level annotation scenario. In Anne Abeille?,
editor, Treebanks: Building and Using Syntactically
Annotated Corpora. Kluwer Academic Publishers.
Chih-Chung Chang and Chih-Jen Lin. 2005. LIB-
SVM: A library for support vector machines.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the First Meet-
ing of the North American Chapter of the Associa-
tion for Computational Linguistics (NAACL), pages
132?139.
Michael Collins, Jan Hajic?, Eric Brill, Lance Ramshaw,
and Christoph Tillmann. 1999. A statistical parser
for Czech. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 505?512.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annatual Meeting of the Association for Com-
putational Linguistics (ACL), pages 16?23.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Walter Daelemans and Antal Van den Bosch. 2005.
Memory-Based Language Processing. Cambridge
University Press.
Jason Eisner and Noah A. Smith. 2005. Parsing with
soft and hard constraints on dependency length. In
Proceedings of the 9th International Workshop on
Parsing Technologies (IWPT).
Jan Hajic?, Barbora Vidova Hladka, Jarmila Panevova?,
Eva Hajic?ova?, Petr Sgall, and Petr Pajas. 2001.
Prague Dependency Treebank 1.0. LDC, 2001T10.
Jan Hajic?. 1998. Building a Syntactically Annotated
Corpus: The Prague Dependency Treebank. In Is-
sues of Valency and Meaning, pages 12?19. Prague
Karolinum, Charles University Press.
Keith Hall and Vaclav Nova?k. 2005. Corrective mod-
eling for non-projective dependency parsing. In
Proceedings of the 9th International Workshop on
Parsing Technologies (IWPT).
Richard Hudson. 1990. English Word Grammar. Basil
Blackwell.
Mark Johnson. 1998. Pcfg models of linguistic
tree representations. Computational Linguistics,
24:613?632.
Dan Klein and Christopher Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 423?430.
Vincenzo Lombardo and Leonardo Lesmo. 1998.
Unit coordination and gapping in dependency the-
ory. In Proceedings of the Workshop on Processing
of Dependency-Based Grammars, pages 11?20.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of the 11th Conference of
the European Chapter of the Association for Com-
putational Linguistics (EACL).
Igor Mel?cuk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
Joakim Nivre and Johan Hall. 2005. MaltParser: A
language-independent system for data-driven depen-
dency parsing. In Proceedings of the Fourth Work-
shop on Treebanks and Linguistic Theories (TLT),
pages 137?148.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In Proceedings of
the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL), pages 99?106.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
MaltParser: A data-driven parser-generator for de-
pendency parsing. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT), pages 149?160.
Jong C. Park and Hyung Joon Cho. 2000. Informed
parsing for coordination with combinatory catego-
rial grammar. In Proceedings of the 18th Inter-
national Conference on Computational Linguistics
(COLING), pages 593?599.
Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of the 9th International Workshop on Parsing
Technologies (IWPT), pages 125?132.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The Meaning of the Sentence in Its Pragmatic As-
pects. Reidel.
Lucien Tesnie`re. 1959. ?Ele?ments de syntaxe struc-
turale. Editions Klincksieck.
Daniel Zeman and Zdene?k ?Zabokrtsky?. 2005. Improv-
ing parsing accuracy by combining diverse depen-
dency parsers. In Proceedings of the 9th Interna-
tional Workshop on Parsing Technologies (IWPT).
Daniel Zeman. 2004. Parsing with a Statistical De-
pendency Model. Ph.D. thesis, Charles University.
264
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 316?323,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Discriminative Classifiers for Deterministic Dependency Parsing
Johan Hall
Va?xjo? University
jni@msi.vxu.se
Joakim Nivre
Va?xjo? University and
Uppsala University
nivre@msi.vxu.se
Jens Nilsson
Va?xjo? University
jha@msi.vxu.se
Abstract
Deterministic parsing guided by treebank-
induced classifiers has emerged as a
simple and efficient alternative to more
complex models for data-driven parsing.
We present a systematic comparison of
memory-based learning (MBL) and sup-
port vector machines (SVM) for inducing
classifiers for deterministic dependency
parsing, using data from Chinese, English
and Swedish, together with a variety of
different feature models. The comparison
shows that SVM gives higher accuracy for
richly articulated feature models across all
languages, albeit with considerably longer
training times. The results also confirm
that classifier-based deterministic parsing
can achieve parsing accuracy very close to
the best results reported for more complex
parsing models.
1 Introduction
Mainstream approaches in statistical parsing are
based on nondeterministic parsing techniques,
usually employing some kind of dynamic pro-
gramming, in combination with generative prob-
abilistic models that provide an n-best ranking of
the set of candidate analyses derived by the parser
(Collins, 1997; Collins, 1999; Charniak, 2000).
These parsers can be enhanced by using a discrim-
inative model, which reranks the analyses out-
put by the parser (Johnson et al, 1999; Collins
and Duffy, 2005; Charniak and Johnson, 2005).
Alternatively, discriminative models can be used
to search the complete space of possible parses
(Taskar et al, 2004; McDonald et al, 2005).
A radically different approach is to perform
disambiguation deterministically, using a greedy
parsing algorithm that approximates a globally op-
timal solution by making a sequence of locally
optimal choices, guided by a classifier trained on
gold standard derivations from a treebank. This
methodology has emerged as an alternative to
more complex models, especially in dependency-
based parsing. It was first used for unlabeled de-
pendency parsing by Kudo and Matsumoto (2002)
(for Japanese) and Yamada and Matsumoto (2003)
(for English). It was extended to labeled depen-
dency parsing by Nivre et al (2004) (for Swedish)
and Nivre and Scholz (2004) (for English). More
recently, it has been applied with good results to
lexicalized phrase structure parsing by Sagae and
Lavie (2005).
The machine learning methods used to induce
classifiers for deterministic parsing are dominated
by two approaches. Support vector machines
(SVM), which combine the maximum margin
strategy introduced by Vapnik (1995) with the use
of kernel functions to map the original feature
space to a higher-dimensional space, have been
used by Kudo and Matsumoto (2002), Yamada and
Matsumoto (2003), and Sagae and Lavie (2005),
among others. Memory-based learning (MBL),
which is based on the idea that learning is the
simple storage of experiences in memory and that
solving a new problem is achieved by reusing so-
lutions from similar previously solved problems
(Daelemans and Van den Bosch, 2005), has been
used primarily by Nivre et al (2004), Nivre and
Scholz (2004), and Sagae and Lavie (2005).
Comparative studies of learning algorithms are
relatively rare. Cheng et al (2005b) report that
SVM outperforms MaxEnt models in Chinese de-
pendency parsing, using the algorithms of Yamada
and Matsumoto (2003) and Nivre (2003), while
Sagae and Lavie (2005) find that SVM gives better
316
performance than MBL in a constituency-based
shift-reduce parser for English.
In this paper, we present a detailed comparison
of SVM and MBL for dependency parsing using
the deterministic algorithm of Nivre (2003). The
comparison is based on data from three different
languages ? Chinese, English, and Swedish ? and
on five different feature models of varying com-
plexity, with a separate optimization of learning
algorithm parameters for each combination of lan-
guage and feature model. The central importance
of feature selection and parameter optimization in
machine learning research has been shown very
clearly in recent research (Daelemans and Hoste,
2002; Daelemans et al, 2003).
The rest of the paper is structured as follows.
Section 2 presents the parsing framework, includ-
ing the deterministic parsing algorithm and the
history-based feature models. Section 3 discusses
the two learning algorithms used in the experi-
ments, and section 4 describes the experimental
setup, including data sets, feature models, learn-
ing algorithm parameters, and evaluation metrics.
Experimental results are presented and discussed
in section 5, and conclusions in section 6.
2 Inductive Dependency Parsing
The system we use for the experiments uses no
grammar but relies completely on inductive learn-
ing from treebank data. The methodology is based
on three essential components:
1. Deterministic parsing algorithms for building
dependency graphs (Kudo and Matsumoto,
2002; Yamada and Matsumoto, 2003; Nivre,
2003)
2. History-based models for predicting the next
parser action (Black et al, 1992; Magerman,
1995; Ratnaparkhi, 1997; Collins, 1999)
3. Discriminative learning to map histories to
parser actions (Kudo and Matsumoto, 2002;
Yamada and Matsumoto, 2003; Nivre et al,
2004)
In this section we will define dependency graphs,
describe the parsing algorithm used in the experi-
ments and finally explain the extraction of features
for the history-based models.
2.1 Dependency Graphs
A dependency graph is a labeled directed graph,
the nodes of which are indices corresponding to
the tokens of a sentence. Formally:
Definition 1 Given a set R of dependency types
(arc labels), a dependency graph for a sentence
x = (w1, . . . , wn) is a labeled directed graph
G = (V,E, L), where:
1. V = Zn+1
2. E ? V ? V
3. L : E ? R
The set V of nodes (or vertices) is the set Zn+1 =
{0, 1, 2, . . . , n} (n ? Z+), i.e., the set of non-
negative integers up to and including n. This
means that every token index i of the sentence is a
node (1 ? i ? n) and that there is a special node
0, which does not correspond to any token of the
sentence and which will always be a root of the
dependency graph (normally the only root). We
use V + to denote the set of nodes corresponding
to tokens (i.e., V + = V ? {0}), and we use the
term token node for members of V +.
The set E of arcs (or edges) is a set of ordered
pairs (i, j), where i and j are nodes. Since arcs are
used to represent dependency relations, we will
say that i is the head and j is the dependent of
the arc (i, j). As usual, we will use the notation
i ? j to mean that there is an arc connecting i
and j (i.e., (i, j) ? E) and we will use the nota-
tion i ?? j for the reflexive and transitive closure
of the arc relation E (i.e., i ?? j if and only if
i = j or there is a path of arcs connecting i to j).
The function L assigns a dependency type (arc
label) r ? R to every arc e ? E.
Definition 2 A dependency graph G is well-
formed if and only if:
1. The node 0 is a root.
2. Every node has in-degree at most 1.
3. G is connected.1
4. G is acyclic.
5. G is projective.2
Conditions 1?4, which are more or less standard in
dependency parsing, together entail that the graph
is a rooted tree. The condition of projectivity, by
contrast, is somewhat controversial, since the anal-
ysis of certain linguistic constructions appears to
1To be more exact, we require G to be weakly connected,
which entails that the corresponding undirected graph is con-
nected, whereas a strongly connected graph has a directed
path between any pair of nodes.
2An arc (i, j) is projective iff there is a path from i to
every node k such that i < j < k or i > j > k. A graph G
is projective if all its arcs are projective.
317
JJ
Economic
 
?
NMOD
NN
news
 
?
SBJ
VB
had
JJ
little
 
?
NMOD
NN
effect
 
?
OBJ
IN
on
 
?
NMOD
JJ
financial
 
?
NMOD
NN
markets
 
?
PMOD
.
.
?
 
P
Figure 1: Dependency graph for an English sentence from the WSJ section of the Penn Treebank
require non-projective dependency arcs. For the
purpose of this paper, however, this assumption is
unproblematic, given that all the treebanks used in
the experiments are restricted to projective depen-
dency graphs.
Figure 1 shows a well-formed dependency
graph for an English sentence, where each word
of the sentence is tagged with its part-of-speech
and each arc labeled with a dependency type.
2.2 Parsing Algorithm
We begin by defining parser configurations and the
abstract data structures needed for the definition of
history-based feature models.
Definition 3 Given a set R = {r0, r1, . . . rm}
of dependency types and a sentence x =
(w1, . . . , wn), a parser configuration for x is a
quadruple c = (?, ?, h, d), where:
1. ? is a stack of tokens nodes.
2. ? is a sequence of token nodes.
3. h : V +x ? V is a function from token nodes
to nodes.
4. d : V +x ? R is a function from token nodes
to dependency types.
5. For every token node i ? V +x , h(i) = 0 if
and only if d(i) = r0.
The idea is that the sequence ? represents the re-
maining input tokens in a left-to-right pass over
the input sentence x; the stack ? contains partially
processed nodes that are still candidates for de-
pendency arcs, either as heads or dependents; and
the functions h and d represent a (dynamically de-
fined) dependency graph for the input sentence x.
We refer to the token node on top of the stack as
the top token and the first token node of the input
sequence as the next token.
When parsing a sentence x = (w1, . . . , wn),
the parser is initialized to a configuration c0 =
(?, (1, . . . , n), h0, d0) with an empty stack, with
all the token nodes in the input sequence, and with
all token nodes attached to the special root node
0 with a special dependency type r0. The parser
terminates in any configuration cm = (?, ?, h, d)
where the input sequence is empty, which happens
after one left-to-right pass over the input.
There are four possible parser transitions, two
of which are parameterized for a dependency type
r ? R.
1. LEFT-ARC(r) makes the top token i a (left)
dependent of the next token j with depen-
dency type r, i.e., j r? i, and immediately
pops the stack.
2. RIGHT-ARC(r) makes the next token j a
(right) dependent of the top token i with de-
pendency type r, i.e., i r? j, and immediately
pushes j onto the stack.
3. REDUCE pops the stack.
4. SHIFT pushes the next token i onto the stack.
The choice between different transitions is nonde-
terministic in the general case and is resolved by a
classifier induced from a treebank, using features
extracted from the parser configuration.
2.3 Feature Models
The task of the classifier is to predict the next
transition given the current parser configuration,
where the configuration is represented by a fea-
ture vector ?(1,p) = (?1, . . . , ?p). Each feature ?i
is a function of the current configuration, defined
in terms of an address function a?i , which identi-
fies a specific token in the current parser configu-
ration, and an attribute function f?i , which picks
out a specific attribute of the token.
Definition 4 Let c = (?, ?, h, d) be the current
parser configuration.
1. For every i (i ? 0), ?i and ?i are address
functions identifying the ith token of ? and
? , respectively (with indexing starting at 0).
318
2. If ? is an address function, then h(?), l(?),
and r(?) are address functions, identifying
the head (h), the leftmost child (l), and the
rightmost child (r), of the token identified by
? (according to the function h).
3. If ? is an address function, then p(?), w(?)
and d(?) are feature functions, identifying
the part-of-speech (p), word form (w) and de-
pendency type (d) of the token identified by
?. We call p, w and d attribute functions.
A feature model is defined by specifying a vector
of feature functions. In section 4.2 we will define
the feature models used in the experiments.
3 Learning Algorithms
The learning problem for inductive dependency
parsing, defined in the preceding section, is a pure
classification problem, where the input instances
are parser configurations, represented by feature
vectors, and the output classes are parser transi-
tions. In this section, we introduce the two ma-
chine learning methods used to solve this problem
in the experiments.
3.1 MBL
MBL is a lazy learning method, based on the idea
that learning is the simple storage of experiences
in memory and that solving a new problem is
achieved by reusing solutions from similar previ-
ously solved problems (Daelemans and Van den
Bosch, 2005). In essence, this is a k nearest neigh-
bor approach to classification, although a vari-
ety of sophisticated techniques, including different
distance metrics and feature weighting schemes
can be used to improve classification accuracy.
For the experiments reported in this paper we
use the TIMBL software package for memory-
based learning and classification (Daelemans and
Van den Bosch, 2005), which directly handles
multi-valued symbolic features. Based on results
from previous optimization experiments (Nivre et
al., 2004), we use the modified value difference
metric (MVDM) to determine distances between
instances, and distance-weighted class voting for
determining the class of a new instance. The para-
meters varied during experiments are the number
k of nearest neighbors and the frequency threshold
l below which MVDM is replaced by the simple
Overlap metric.
3.2 SVM
SVM in its simplest form is a binary classifier
that tries to separate positive and negative cases in
training data by a hyperplane using a linear kernel
function. The goal is to find the hyperplane that
separates the training data into two classes with
the largest margin. By using other kernel func-
tions, such as polynomial or radial basis function
(RBF), feature vectors are mapped into a higher
dimensional space (Vapnik, 1998; Kudo and Mat-
sumoto, 2001). Multi-class classification with
n classes can be handled by the one-versus-all
method, with n classifiers that each separate one
class from the rest, or the one-versus-one method,
with n(n ? 1)/2 classifiers, one for each pair of
classes (Vural and Dy, 2004). SVM requires all
features to be numerical, which means that sym-
bolic features have to be converted, normally by
introducing one binary feature for each value of
the symbolic feature.
For the experiments reported in this paper
we use the LIBSVM library (Wu et al, 2004;
Chang and Lin, 2005) with the polynomial kernel
K(xi, xj) = (?xTi xj +r)d, ? > 0, where d, ? and
r are kernel parameters. Other parameters that are
varied in experiments are the penalty parameter C,
which defines the tradeoff between training error
and the magnitude of the margin, and the termina-
tion criterion ?, which determines the tolerance of
training errors.
We adopt the standard method for converting
symbolic features to numerical features by bina-
rization, and we use the one-versus-one strategy
for multi-class classification. However, to reduce
training times, we divide the training data into
smaller sets, according to the part-of-speech of
the next token in the current parser configuration,
and train one set of classifiers for each smaller
set. Similar techniques have previously been used
by Yamada and Matsumoto (2003), among others,
without significant loss of accuracy. In order to
avoid too small training sets, we pool together all
parts-of-speech that have a frequency below a cer-
tain threshold t (set to 1000 in all the experiments).
4 Experimental Setup
In this section, we describe the experimental setup,
including data sets, feature models, parameter op-
timization, and evaluation metrics. Experimental
results are presented in section 5.
319
4.1 Data Sets
The data set used for Swedish comes from Tal-
banken (Einarsson, 1976), which contains both
written and spoken Swedish. In the experiments,
the professional prose section is used, consisting
of about 100k words taken from newspapers, text-
books and information brochures. The data has
been manually annotated with a combination of
constituent structure, dependency structure, and
topological fields (Teleman, 1974). This annota-
tion has been converted to dependency graphs and
the original fine-grained classification of gram-
matical functions has been reduced to 17 depen-
dency types. We use a pseudo-randomized data
split, dividing the data into 10 sections by allocat-
ing sentence i to section i mod 10. Sections 1?9
are used for 9-fold cross-validation during devel-
opment and section 0 for final evaluation.
The English data are from the Wall Street Jour-
nal section of the Penn Treebank II (Marcus et al,
1994). We use sections 2?21 for training, sec-
tion 0 for development, and section 23 for the
final evaluation. The head percolation table of
Yamada and Matsumoto (2003) has been used
to convert constituent structures to dependency
graphs, and a variation of the scheme employed
by Collins (1999) has been used to construct arc
labels that can be mapped to a set of 12 depen-
dency types.
The Chinese data are taken from the Penn Chi-
nese Treebank (CTB) version 5.1 (Xue et al,
2005), consisting of about 500k words mostly
from Xinhua newswire, Sinorama news magazine
and Hong Kong News. CTB is annotated with
a combination of constituent structure and gram-
matical functions in the Penn Treebank style, and
has been converted to dependency graphs using es-
sentially the same method as for the English data,
although with a different head percolation table
and mapping scheme. We use the same kind of
pseudo-randomized data split as for Swedish, but
we use section 9 as the development test set (train-
ing on section 1?8) and section 0 as the final test
set (training on section 1?9).
A standard HMM part-of-speech tagger with
suffix smoothing has been used to tag the test data
with an accuracy of 96.5% for English and 95.1%
for Swedish. For the Chinese experiments we have
used the original (gold standard) tags from the
treebank, to facilitate comparison with results pre-
viously reported in the literature.
Feature ?1 ?2 ?3 ?4 ?5
p(?0) + + + + +
p(?0) + + + + +
p(?1) + + + + +
p(?2) + +
p(?3) + +
p(?1) +
d(?0) + + + +
d(l(?0)) + + + +
d(r(?0)) + + + +
d(l(?0)) + + + +
w(?0) + + +
w(?0) + + +
w(?1) +
w(h(?0)) +
Table 1: Feature models
4.2 Feature Models
Table 1 describes the five feature models ?1??5
used in the experiments, with features specified
in column 1 using the functional notation defined
in section 2.3. Thus, p(?0) refers to the part-of-
speech of the top token, while d(l(?0)) picks out
the dependency type of the leftmost child of the
next token. It is worth noting that models ?1??2
are unlexicalized, since they do not contain any
features of the form w(?), while models ?3??5
are all lexicalized to different degrees.
4.3 Optimization
As already noted, optimization of learning algo-
rithm parameters is a prerequisite for meaningful
comparison of different algorithms, although an
exhaustive search of the parameter space is usu-
ally impossible in practice.
For MBL we have used the modified value
difference metric (MVDM) and class voting
weighted by inverse distance (ID) in all experi-
ments, and performed a grid search for the op-
timal values of the number k of nearest neigh-
bors and the frequency threshold l for switching
from MVDM to the simple Overlap metric (cf.
section 3.1). The best values are different for dif-
ferent combinations of data sets and models but
are generally found in the range 3?10 for k and in
the range 1?8 for l.
The polynomial kernel of degree 2 has been
used for all the SVM experiments, but the kernel
parameters ? and r have been optimized together
with the penalty parameter C and the termination
320
Swedish English Chinese
FM LM AS EM AS EM AS EM
U L U L U L U L U L U L
?1 MBL 75.3 68.7 16.0 11.4 *76.5 73.7 9.8 7.7 66.4 63.6 14.3 12.1
SVM 75.4 68.9 16.3 12.1 76.4 73.6 9.8 7.7 66.4 63.6 14.2 12.1
?2 MBL 81.9 74.4 31.4 19.8 81.2 78.2 19.8 14.9 73.0 70.7 22.6 18.8
SVM *83.1 *76.3 *34.3 *24.0 81.3 78.3 19.4 14.9 *73.2 *71.0 22.1 18.6
?3 MBL 85.9 81.4 37.9 28.9 85.5 83.7 26.5 23.7 77.9 76.3 26.3 23.4
SVM 86.2 *82.6 38.7 *32.5 *86.4 *84.8 *28.5 *25.9 *79.7 *78.3 *30.1 *25.9
?4 MBL 86.1 82.1 37.6 30.1 87.0 85.2 29.8 26.0 79.4 77.7 28.0 24.7
SVM 86.0 82.2 37.9 31.2 *88.4 *86.8 *33.2 *30.3 *81.7 *80.1 *31.0 *27.0
?5 MBL 86.6 82.3 39.9 29.9 88.0 86.2 32.8 28.4 81.1 79.2 30.2 25.9
SVM 86.9 *83.2 40.7 *33.7 *89.4 *87.9 *36.4 *33.1 *84.3 *82.7 *34.5 *30.5
Table 2: Parsing accuracy; FM: feature model; LM: learning method; AS: attachment score, EM: exact
match; U: unlabeled, L: labeled
criterion e. The intervals for the parameters are:
?: 0.16?0.40; r: 0?0.6; C: 0.5?1.0; e: 0.1?1.0.
4.4 Evaluation Metrics
The evaluation metrics used for parsing accuracy
are the unlabeled attachment score ASU , which is
the proportion of tokens that are assigned the cor-
rect head (regardless of dependency type), and the
labeled attachment score ASL, which is the pro-
portion of tokens that are assigned the correct head
and the correct dependency type. We also consider
the unlabeled exact match EMU , which is the pro-
portion of sentences that are assigned a completely
correct dependency graph without considering de-
pendency type labels, and the labeled exact match
EML, which also takes dependency type labels
into account. Attachment scores are presented as
mean scores per token, and punctuation tokens are
excluded from all counts. For all experiments we
have performed a McNemar test of significance at
? = 0.01 for differences between the two learning
methods. We also compare learning and parsing
times, as measured on an AMD 64-bit processor
running Linux.
5 Results and Discussion
Table 2 shows the parsing accuracy for the com-
bination of three languages (Swedish, English and
Chinese), two learning methods (MBL and SVM)
and five feature models (?1??5), with algorithm
parameters optimized as described in section 4.3.
For each combination, we measure the attachment
score (AS) and the exact match (EM). A signif-
icant improvement for one learning method over
the other is marked by an asterisk (*).
Independently of language and learning
method, the most complex feature model ?5
gives the highest accuracy across all metrics. Not
surprisingly, the lowest accuracy is obtained with
the simplest feature model ?1. By and large, more
complex feature models give higher accuracy,
with one exception for Swedish and the feature
models ?3 and ?4. It is significant in this context
that the Swedish data set is the smallest of the
three (about 20% of the Chinese data set and
about 10% of the English one).
If we compare MBL and SVM, we see that
SVM outperforms MBL for the three most com-
plex models ?3, ?4 and ?5, both for English and
Chinese. The results for Swedish are less clear,
although the labeled accuracy for ?3 and ?5 are
significantly better. For the ?1 model there is no
significant improvement using SVM. In fact, the
small differences found in the ASU scores are to
the advantage of MBL. By contrast, there is a large
gap between MBL and SVM for the model ?5 and
the languages Chinese and English. For Swedish,
the differences are much smaller (except for the
EML score), which may be due to the smaller size
of the Swedish data set in combination with the
technique of dividing the training data for SVM
(cf. section 3.2).
Another important factor when comparing two
learning methods is the efficiency in terms of time.
Table 3 reports learning and parsing time for the
three languages and the five feature models. The
learning time correlates very well with the com-
plexity of the feature model and MBL, being a lazy
learning method, is much faster than SVM. For the
unlexicalized feature models ?1 and ?2, the pars-
ing time is also considerably lower for MBL, espe-
cially for the large data sets (English and Chinese).
But as model complexity grows, especially with
the addition of lexical features, SVM gradually
gains an advantage over MBL with respect to pars-
ing time. This is especially striking for Swedish,
321
Method Model Swedish English Chinese
LT PT LT PT LT PT
?1 MBL 1 s 2 s 16 s 26 s 7 s 8 s
SVM 40 s 14 s 1.5 h 14 min 1.5 h 17 min
?2 MBL 3 s 5 s 35 s 32 s 13 s 14 s
SVM 40 s 13 s 1 h 11 min 1.5 h 15 min
?3 MBL 6 s 1 min 1.5 min 9.5 min 46 s 10 min
SVM 1 min 15 s 1 h 9 min 2 h 16 min
?4 MBL 8 s 2 min 1.5 min 9 min 45 s 12 min
SVM 2 min 18 s 2 h 12 min 2.5 h 14 min
?5 MBL 10 s 7 min 3 min 41 min 1.5 min 46 min
SVM 2 min 25 s 1.5 h 10 min 6 h 24 min
Table 3: Time efficiency; LT: learning time, PT: parsing time
where the training data set is considerably smaller
than for the other languages.
Compared to the state of the art in dependency
parsing, the unlabeled attachment scores obtained
for Swedish with model ?5, for both MBL and
SVM, are about 1 percentage point higher than the
results reported for MBL by Nivre et al (2004).
For the English data, the result for SVM with
model ?5 is about 3 percentage points below the
results obtained with the parser of Charniak (2000)
and reported by Yamada and Matsumoto (2003).
For Chinese, finally, the accuracy for SVM with
model ?5 is about one percentage point lower than
the best reported results, achieved with a deter-
ministic classifier-based approach using SVM and
preprocessing to detect root nodes (Cheng et al,
2005a), although these results are not based on
exactly the same dependency conversion and data
split as ours.
6 Conclusion
We have performed an empirical comparison of
MBL (TIMBL) and SVM (LIBSVM) as learning
methods for classifier-based deterministic depen-
dency parsing, using data from three languages
and feature models of varying complexity. The
evaluation shows that SVM gives higher parsing
accuracy and comparable or better parsing effi-
ciency for complex, lexicalized feature models
across all languages, whereas MBL is superior
with respect to training efficiency, even if training
data is divided into smaller sets for SVM. The best
accuracy obtained for SVM is close to the state of
the art for all languages involved.
Acknowledgements
The work presented in this paper was partially sup-
ported by the Swedish Research Council. We are
grateful to Hiroyasu Yamada and Yuan Ding for
sharing their head percolation tables for English
and Chinese, respectively, and to three anonymous
reviewers for helpful comments and suggestions.
References
Ezra Black, Frederick Jelinek, John D. Lafferty,
David M. Magerman, Robert L. Mercer, and Salim
Roukos. 1992. Towards history-based grammars:
Using richer models for probabilistic parsing. In
Proceedings of the 5th DARPA Speech and Natural
Language Workshop, pages 31?37.
Chih-Chung Chang and Chih-Jen Lin. 2005. LIB-
SVM: A library for support vector machines.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 173?180.
Eugene Charniak. 2000. A Maximum-Entropy-
Inspired Parser. In Proceedings of the First Annual
Meeting of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL),
pages 132?139.
Yuchang Cheng, Masayuki Asahara, and Yuji Mat-
sumoto. 2005a. Chinese deterministic dependency
analyzer: Examining effects of global features and
root node finder. In Proceedings of the Fourth
SIGHAN Workshop on Chinese Language Process-
ing, pages 17?24.
Yuchang Cheng, Masayuki Asahara, and Yuji Mat-
sumoto. 2005b. Machine learning-based depen-
dency analyzer for Chinese. In Proceedings of
the International Conference on Chinese Computing
(ICCC).
Michael Collins and Nigel Duffy. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31:25?70.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 16?23.
322
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Walter Daelemans and Veronique Hoste. 2002. Eval-
uation of machine learning methods for natural lan-
guage processing tasks. In Proceedings of the Third
International Conference on Language Resources
and Evaluation (LREC), pages 755?760.
Walter Daelemans and Antal Van den Bosch. 2005.
Memory-Based Language Processing. Cambridge
University Press.
Walter Daelemans, Veronique Hoste, Fien De Meulder,
and Bart Naudts. 2003. Combined optimization of
feature selection and algorithm parameter interac-
tion in machine learning of language. In Proceed-
ings of the 14th European Conference on Machine
Learning (ECML), pages 84?95.
Jan Einarsson. 1976. Talbankens skrift-
spra?kskonkordans. Lund University, Department of
Scandinavian Languages.
Mark Johnson, Stuart Geman, Steven Canon, Zhiyi
Chi, and Stefan Riezler. 1999. Estimators for
stochastic ?unification-based? grammars. In Pro-
ceedings of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
535?541.
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In Proceedings of
the Second Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL).
Taku Kudo and Yuji Matsumoto. 2002. Japanese de-
pendency analysis using cascaded chunking. In Pro-
ceedings of the Sixth Workshop on Computational
Language Learning (CoNLL), pages 63?69.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of the 33rd An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 276?283.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark
Ferguson, Karen Katz, and Britta Schasberger.
1994. The Penn Treebank: Annotating predicate-
argument structure. In Proceedings of the ARPA Hu-
man Language Technology Workshop, pages 114?
119.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 91?98.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In Proceedings
of the 20th International Conference on Computa-
tional Linguistics (COLING), pages 64?70.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Proceed-
ings of the 8th Conference on Computational Nat-
ural Language Learning (CoNLL), pages 49?56.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT), pages 149?160.
Adwait Ratnaparkhi. 1997. A linear observed time
statistical parser based on maximum entropy mod-
els. In Proceedings of the Second Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1?10.
Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of the 9th International Workshop on Parsing
Technologies (IWPT), pages 125?132.
Ben Taskar, Dan Klein, Michael Collins, Daphne
Koller, and Christopher Manning. 2004. Max-
margin parsing. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP), pages 1?8.
Ulf Teleman. 1974. Manual fo?r grammatisk beskriv-
ning av talad och skriven svenska. Studentlitteratur.
Vladimir Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer.
Vladimir Vapnik. 1998. Statistical Learning Theory.
John Wiley and Sons, New York.
Volkan Vural and Jennifer G. Dy. 2004. A hierarchi-
cal method for multi-class support vector machines.
ACM International Conference Proceeding Series,
69:105?113.
Ting-Fan Wu, Chih-Jen Lin, and Ruby C. Weng. 2004.
Probability estimates for multi-class classification
by pairwise coupling. Journal of Machine Learning
Research, 5:975?1005.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese Treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of the 8th International
Workshop on Parsing Technologies (IWPT), pages
195?206.
323
Incrementality in Deterministic Dependency Parsing
Joakim Nivre
School of Mathematics and Systems Engineering
Va?xjo? University
SE-35195 Va?xjo?
Sweden
joakim.nivre@msi.vxu.se
Abstract
Deterministic dependency parsing is a robust
and efficient approach to syntactic parsing of
unrestricted natural language text. In this pa-
per, we analyze its potential for incremental
processing and conclude that strict incremen-
tality is not achievable within this framework.
However, we also show that it is possible to min-
imize the number of structures that require non-
incremental processing by choosing an optimal
parsing algorithm. This claim is substantiated
with experimental evidence showing that the al-
gorithm achieves incremental parsing for 68.9%
of the input when tested on a random sample
of Swedish text. When restricted to sentences
that are accepted by the parser, the degree of
incrementality increases to 87.9%.
1 Introduction
Incrementality in parsing has been advocated
for at least two different reasons. The first is
mainly practical and has to do with real-time
applications such as speech recognition, which
require a continually updated analysis of the in-
put received so far. The second reason is more
theoretical in that it connects parsing to cog-
nitive modeling, where there is psycholinguis-
tic evidence suggesting that human parsing is
largely incremental (Marslen-Wilson, 1973; Fra-
zier, 1987).
However, most state-of-the-art parsing meth-
ods today do not adhere to the principle of in-
crementality, for different reasons. Parsers that
attempt to disambiguate the input completely
? full parsing ? typically first employ some
kind of dynamic programming algorithm to de-
rive a packed parse forest and then applies a
probabilistic top-down model in order to select
the most probable analysis (Collins, 1997; Char-
niak, 2000). Since the first step is essentially
nondeterministic, this seems to rule out incre-
mentality at least in a strict sense. By contrast,
parsers that only partially disambiguate the in-
put ? partial parsing ? are usually determin-
istic and construct the final analysis in one pass
over the input (Abney, 1991; Daelemans et al,
1999). But since they normally output a se-
quence of unconnected phrases or chunks, they
fail to satisfy the constraint of incrementality
for a different reason.
Deterministic dependency parsing has re-
cently been proposed as a robust and effi-
cient method for syntactic parsing of unre-
stricted natural language text (Yamada and
Matsumoto, 2003; Nivre, 2003). In some ways,
this approach can be seen as a compromise be-
tween traditional full and partial parsing. Es-
sentially, it is a kind of full parsing in that the
goal is to build a complete syntactic analysis for
the input string, not just identify major con-
stituents. But it resembles partial parsing in
being robust, efficient and deterministic. Taken
together, these properties seem to make de-
pendency parsing suitable for incremental pro-
cessing, although existing implementations nor-
mally do not satisfy this constraint. For exam-
ple, Yamada and Matsumoto (2003) use a multi-
pass bottom-up algorithm, combined with sup-
port vector machines, in a way that does not
result in incremental processing.
In this paper, we analyze the constraints
on incrementality in deterministic dependency
parsing and argue that strict incrementality is
not achievable. We then analyze the algorithm
proposed in Nivre (2003) and show that, given
the previous result, this algorithm is optimal
from the point of view of incrementality. Fi-
nally, we evaluate experimentally the degree of
incrementality achieved with the algorithm in
practical parsing.
2 Dependency Parsing
In a dependency structure, every word token
is dependent on at most one other word to-
ken, usually called its head or regent, which
PP
P?a
(In
 
?
adv
NN
60-talet
the-60?s
 
?
pr
VB
m?alade
painted
PN
han
he
 
?
sub
JJ
dja?rva
bold
 
?
att
NN
tavlor
pictures
 
?
obj
HP
som
which
 
?
att
VB
retade
annoyed
?
 
sub
PM
Nikita
Nikita
 
?
obj
PM
Chrusjtjov.
Chrustjev.)
 
?
id
Figure 1: Dependency graph for Swedish sentence
means that the structure can be represented as
a directed graph, with nodes representing word
tokens and arcs representing dependency rela-
tions. In addition, arcs may be labeled with
specific dependency types. Figure 1 shows a
labeled dependency graph for a simple Swedish
sentence, where each word of the sentence is la-
beled with its part of speech and each arc la-
beled with a grammatical function.
In the following, we will restrict our atten-
tion to unlabeled dependency graphs, i.e. graphs
without labeled arcs, but the results will ap-
ply to labeled dependency graphs as well. We
will also restrict ourselves to projective depen-
dency graphs (Mel?cuk, 1988). Formally, we de-
fine these structures in the following way:
1. A dependency graph for a string of words
W = w1? ? ?wn is a labeled directed graph
D = (W,A), where
(a) W is the set of nodes, i.e. word tokens
in the input string,
(b) A is a set of arcs (wi, wj) (wi, wj ? W ).
We write wi < wj to express that wi pre-
cedes wj in the string W (i.e., i < j); we
write wi ? wj to say that there is an arc
from wi to wj ; we use ?? to denote the re-
flexive and transitive closure of the arc re-
lation; and we use ? and ?? for the corre-
sponding undirected relations, i.e. wi ? wj
iff wi ? wj or wj ? wi.
2. A dependency graph D = (W,A) is well-
formed iff the five conditions given in Fig-
ure 2 are satisfied.
The task of mapping a string W = w1? ? ?wn
to a dependency graph satisfying these condi-
tions is what we call dependency parsing. For a
more detailed discussion of dependency graphs
and well-formedness conditions, the reader is re-
ferred to Nivre (2003).
3 Incrementality in Dependency
Parsing
Having defined dependency graphs, we may
now consider to what extent it is possible to
construct these graphs incrementally. In the
strictest sense, we take incrementality to mean
that, at any point during the parsing process,
there is a single connected structure represent-
ing the analysis of the input consumed so far.
In terms of our dependency graphs, this would
mean that the graph being built during parsing
is connected at all times. We will try to make
this more precise in a minute, but first we want
to discuss the relation between incrementality
and determinism.
It seems that incrementality does not by itself
imply determinism, at least not in the sense of
never undoing previously made decisions. Thus,
a parsing method that involves backtracking can
be incremental, provided that the backtracking
is implemented in such a way that we can always
maintain a single structure representing the in-
put processed up to the point of backtracking.
In the context of dependency parsing, a case in
point is the parsing method proposed by Kro-
mann (Kromann, 2002), which combines heuris-
tic search with different repair mechanisms.
In this paper, we will nevertheless restrict our
attention to deterministic methods for depen-
dency parsing, because we think it is easier to
pinpoint the essential constraints within a more
restrictive framework. We will formalize deter-
ministic dependency parsing in a way which is
inspired by traditional shift-reduce parsing for
context-free grammars, using a buffer of input
tokens and a stack for storing previously pro-
cessed input. However, since there are no non-
terminal symbols involved in dependency pars-
ing, we also need to maintain a representation of
the dependency graph being constructed during
processing.
We will represent parser configurations by
Unique label (wi r?wj ? wi r
?
?wj) ? r = r?
Single head (wi?wj ? wk?wj) ? wi = wk
Acyclic ?(wi?wj ? wj??wi)
Connected wi??wj
Projective (wi?wk ? wi<wj<wk) ? (wi??wj ? wk??wj)
Figure 2: Well-formedness conditions on dependency graphs
triples ?S, I, A?, where S is the stack (repre-
sented as a list), I is the list of (remaining) input
tokens, and A is the (current) arc relation for
the dependency graph. (Since the nodes of the
dependency graph are given by the input string,
only the arc relation needs to be represented ex-
plicitly.) Given an input string W , the parser is
initialized to ?nil,W, ?? and terminates when it
reaches a configuration ?S,nil, A? (for any list
S and set of arcs A). The input string W is
accepted if the dependency graph D = (W,A)
given at termination is well-formed; otherwise
W is rejected.
In order to understand the constraints on
incrementality in dependency parsing, we will
begin by considering the most straightforward
parsing strategy, i.e. left-to-right bottom-up
parsing, which in this case is essentially equiva-
lent to shift-reduce parsing with a context-free
grammar in Chomsky normal form. The parser
is defined in the form of a transition system,
represented in Figure 3 (where wi and wj are
arbitrary word tokens):
1. The transition Left-Reduce combines the
two topmost tokens on the stack, wi and
wj , by a left-directed arc wj ? wi and re-
duces them to the head wj .
2. The transition Right-Reduce combines
the two topmost tokens on the stack, wi
and wj , by a right-directed arc wi ? wj
and reduces them to the head wi.
3. The transition Shift pushes the next input
token wi onto the stack.
The transitions Left-Reduce and Right-
Reduce are subject to conditions that ensure
that the Single head condition is satisfied. For
Shift, the only condition is that the input list
is non-empty.
As it stands, this transition system is non-
deterministic, since several transitions can of-
ten be applied to the same configuration. Thus,
in order to get a deterministic parser, we need
to introduce a mechanism for resolving transi-
tion conflicts. Regardless of which mechanism
is used, the parser is guaranteed to terminate
after at most 2n transitions, given an input
string of length n. Moreover, the parser is guar-
anteed to produce a dependency graph that is
acyclic and projective (and satisfies the single-
head constraint). This means that the depen-
dency graph given at termination is well-formed
if and only if it is connected.
We can now define what it means for the pars-
ing to be incremental in this framework. Ide-
ally, we would like to require that the graph
(W ? I, A) is connected at all times. How-
ever, given the definition of Left-Reduce and
Right-Reduce, it is impossible to connect a
new word without shifting it to the stack first,
so it seems that a more reasonable condition is
that the size of the stack should never exceed
2. In this way, we require every word to be at-
tached somewhere in the dependency graph as
soon as it has been shifted onto the stack.
We may now ask whether it is possible
to achieve incrementality with a left-to-right
bottom-up dependency parser, and the answer
turns out to be no in the general case. This can
be demonstrated by considering all the possible
projective dependency graphs containing only
three nodes and checking which of these can be
parsed incrementally. Figure 4 shows the rele-
vant structures, of which there are seven alto-
gether.
We begin by noting that trees (2?5) can all be
constructed incrementally by shifting the first
two tokens onto the stack, then reducing ? with
Right-Reduce in (2?3) and Left-Reduce in
(4?5) ? and then shifting and reducing again ?
with Right-Reduce in (2) and (4) and Left-
Reduce in (3) and (5). By contrast, the three
remaining trees all require that three tokens are
Initialization ?nil,W, ??
Termination ?S,nil, A?
Left-Reduce ?wjwi|S, I, A? ? ?wj |S, I, A ? {(wj , wi)}? ??wk(wk, wi) ? A
Right-Reduce ?wjwi|S, I, A? ? ?wi|S, I, A ? {(wi, wj)}? ??wk(wk, wj) ? A
Shift ?S,wi|I, A? ? ?wi|S, I, A?
Figure 3: Left-to-right bottom-up dependency parsing
(1) a b c
 
?
 
? (2) a b c
 
?
 
? (3) a b c
 
?
 
? (4) a b c
 
?
 
?
(5) a b c
 
?
 
? (6) a b c
 
?
 
? (7) a b c
 
?
 
?
Figure 4: Projective three-node dependency structures
shifted onto the stack before the first reduction.
However, the reason why we cannot parse the
structure incrementally is different in (1) com-
pared to (6?7).
In (6?7) the problem is that the first two to-
kens are not connected by a single arc in the
final dependency graph. In (6) they are sisters,
both being dependents on the third token; in
(7) the first is the grandparent of the second.
And in pure dependency parsing without non-
terminal symbols, every reduction requires that
one of the tokens reduced is the head of the
other(s). This holds necessarily, regardless of
the algorithm used, and is the reason why it
is impossible to achieve strict incrementality in
dependency parsing as defined here. However,
it is worth noting that (2?3), which are the mir-
ror images of (6?7) can be parsed incrementally,
even though they contain adjacent tokens that
are not linked by a single arc. The reason is
that in (2?3) the reduction of the first two to-
kens makes the third token adjacent to the first.
Thus, the defining characteristic of the prob-
lematic structures is that precisely the leftmost
tokens are not linked directly.
The case of (1) is different in that here the
problem is caused by the strict bottom-up strat-
egy, which requires each token to have found
all its dependents before it is combined with its
head. For left-dependents this is not a problem,
as can be seen in (5), which can be processed
by alternating Shift and Left-Reduce. But in
(1) the sequence of reductions has to be per-
formed from right to left as it were, which rules
out strict incrementality. However, whereas the
structures exemplified in (6?7) can never be pro-
cessed incrementally within the present frame-
work, the structure in (1) can be handled by
modifying the parsing strategy, as we shall see
in the next section.
It is instructive at this point to make a com-
parison with incremental parsing based on ex-
tended categorial grammar, where the struc-
tures in (6?7) would normally be handled by
some kind of concatenation (or product), which
does not correspond to any real semantic com-
bination of the constituents (Steedman, 2000;
Morrill, 2000). By contrast, the structure in (1)
would typically be handled by function compo-
sition, which corresponds to a well-defined com-
positional semantic operation. Hence, it might
be argued that the treatment of (6?7) is only
pseudo-incremental even in other frameworks.
Before we leave the strict bottom-up ap-
proach, it can be noted that the algorithm de-
scribed in this section is essentially the algo-
rithm used by Yamada and Matsumoto (2003)
in combination with support vector machines,
except that they allow parsing to be performed
in multiple passes, where the graph produced in
one pass is given as input to the next pass.1 The
main motivation they give for parsing in multi-
ple passes is precisely the fact that the bottom-
up strategy requires each token to have found
all its dependents before it is combined with its
head, which is also what prevents the incremen-
tal parsing of structures like (1).
4 Arc-Eager Dependency Parsing
In order to increase the incrementality of deter-
ministic dependency parsing, we need to com-
bine bottom-up and top-down processing. More
precisely, we need to process left-dependents
bottom-up and right-dependents top-down. In
this way, arcs will be added to the dependency
graph as soon as the respective head and depen-
dent are available, even if the dependent is not
complete with respect to its own dependents.
Following Abney and Johnson (1991), we will
call this arc-eager parsing, to distinguish it from
the standard bottom-up strategy discussed in
the previous section.
Using the same representation of parser con-
figurations as before, the arc-eager algorithm
can be defined by the transitions given in Fig-
ure 5, where wi and wj are arbitrary word to-
kens (Nivre, 2003):
1. The transition Left-Arc adds an arc
wj r? wi from the next input token wj
to the token wi on top of the stack and
pops the stack.
2. The transition Right-Arc adds an arc
wi r? wj from the token wi on top of
the stack to the next input token wj , and
pushes wj onto the stack.
3. The transition Reduce pops the stack.
4. The transition Shift (SH) pushes the next
input token wi onto the stack.
The transitions Left-Arc and Right-Arc, like
their counterparts Left-Reduce and Right-
Reduce, are subject to conditions that ensure
1A purely terminological, but potentially confusing,
difference is that Yamada and Matsumoto (2003) use the
term Right for what we call Left-Reduce and the term
Left for Right-Reduce (thus focusing on the position
of the head instead of the position of the dependent).
that the Single head constraint is satisfied,
while the Reduce transition can only be ap-
plied if the token on top of the stack already
has a head. The Shift transition is the same as
before and can be applied as long as the input
list is non-empty.
Comparing the two algorithms, we see that
the Left-Arc transition of the arc-eager algo-
rithm corresponds directly to the Left-Reduce
transition of the standard bottom-up algorithm.
The only difference is that, for reasons of sym-
metry, the former applies to the token on top
of the stack and the next input token instead
of the two topmost tokens on the stack. If we
compare Right-Arc to Right-Reduce, how-
ever, we see that the former performs no re-
duction but simply shifts the newly attached
right-dependent onto the stack, thus making
it possible for this dependent to have right-
dependents of its own. But in order to allow
multiple right-dependents, we must also have
a mechanism for popping right-dependents off
the stack, and this is the function of the Re-
duce transition. Thus, we can say that the
action performed by the Right-Reduce tran-
sition in the standard bottom-up algorithm is
performed by a Right-Arc transition in combi-
nation with a subsequent Reduce transition in
the arc-eager algorithm. And since the Right-
Arc and the Reduce can be separated by an
arbitrary number of transitions, this permits
the incremental parsing of arbitrary long right-
dependent chains.
Defining incrementality is less straightfor-
ward for the arc-eager algorithm than for the
standard bottom-up algorithm. Simply consid-
ering the size of the stack will not do anymore,
since the stack may now contain sequences of
tokens that form connected components of the
dependency graph. On the other hand, since it
is no longer necessary to shift both tokens to be
combined onto the stack, and since any tokens
that are popped off the stack are connected to
some token on the stack, we can require that
the graph (S,AS) should be connected at all
times, where AS is the restriction of A to S, i.e.
AS = {(wi, wj) ? A|wi, wj ? S}.
Given this definition of incrementality, it is
easy to show that structures (2?5) in Figure 4
can be parsed incrementally with the arc-eager
algorithm as well as with the standard bottom-
up algorithm. However, with the new algorithm
we can also parse structure (1) incrementally, as
Initialization ?nil,W, ??
Termination ?S,nil, A?
Left-Arc ?wi|S,wj |I, A? ? ?S,wj |I, A ? {(wj , wi)}? ??wk(wk, wi) ? A
Right-Arc ?wi|S,wj |I, A? ? ?wj |wi|S, I, A ? {(wi, wj)}? ??wk(wk, wj) ? A
Reduce ?wi|S, I, A? ? ?S, I, A? ?wj(wj , wi) ? A
Shift ?S,wi|I, A? ? ?wi|S, I, A?
Figure 5: Left-to-right arc-eager dependency parsing
is shown by the following transition sequence:
?nil, abc, ??
? (Shift)
?a, bc, ??
? (Right-Arc)
?ba, c, {(a, b)}?
? (Right-Arc)
?cba,nil, {(a, b), (b, c)}?
We conclude that the arc-eager algorithm is op-
timal with respect to incrementality in depen-
dency parsing, even though it still holds true
that the structures (6?7) in Figure 4 cannot be
parsed incrementally. This raises the question
how frequently these structures are found in
practical parsing, which is equivalent to asking
how often the arc-eager algorithm deviates from
strictly incremental processing. Although the
answer obviously depends on which language
and which theoretical framework we consider,
we will attempt to give at least a partial answer
to this question in the next section. Before that,
however, we want to relate our results to some
previous work on context-free parsing.
First of all, it should be observed that the
terms top-down and bottom-up take on a slightly
different meaning in the context of dependency
parsing, as compared to their standard use in
context-free parsing. Since there are no nonter-
minal nodes in a dependency graph, top-down
construction means that a head is attached to
a dependent before the dependent is attached
to (some of) its dependents, whereas bottom-
up construction means that a dependent is at-
tached to its head before the head is attached to
its head. However, top-down construction of de-
pendency graphs does not involve the prediction
of lower nodes from higher nodes, since all nodes
are given by the input string. Hence, in terms of
what drives the parsing process, all algorithms
discussed here correspond to bottom-up algo-
rithms in context-free parsing. It is interest-
ing to note that if we recast the problem of de-
pendency parsing as context-free parsing with a
CNF grammar, then the problematic structures
(1), (6?7) in Figure 4 all correspond to right-
branching structures, and it is well-known that
bottom-up parsers may require an unbounded
amount of memory in order to process right-
branching structure (Miller and Chomsky, 1963;
Abney and Johnson, 1991).
Moreover, if we analyze the two algorithms
discussed here in the framework of Abney and
Johnson (1991), they do not differ at all as to
the order in which nodes are enumerated, but
only with respect to the order in which arcs are
enumerated; the first algorithm is arc-standard
while the second is arc-eager. One of the obser-
vations made by Abney and Johnson (1991), is
that arc-eager strategies for context-free pars-
ing may sometimes require less space than arc-
standard strategies, although they may lead
to an increase in local ambiguities. It seems
that the advantage of the arc-eager strategy
for dependency parsing with respect to struc-
ture (1) in Figure 4 can be explained along the
same lines, although the lack of nonterminal
nodes in dependency graphs means that there
is no corresponding increase in local ambigui-
ties. Although a detailed discussion of the re-
lation between context-free parsing and depen-
dency parsing is beyond the scope of this paper,
we conjecture that this may be a genuine advan-
tage of dependency representations in parsing.
Connected Parser configurations
components Number Percent
0 1251 7.6
1 10148 61.3
2 2739 16.6
3 1471 8.9
4 587 3.5
5 222 1.3
6 98 0.6
7 26 0.2
8 3 0.0
? 1 11399 68.9
? 3 15609 94.3
? 8 16545 100.0
Table 1: Number of connected components in (S,AS) during parsing
5 Experimental Evaluation
In order to measure the degree of incremental-
ity achieved in practical parsing, we have eval-
uated a parser that uses the arc-eager parsing
algorithm in combination with a memory-based
classifier for predicting the next transition. In
experiments reported in Nivre et al (2004), a
parsing accuracy of 85.7% (unlabeled attach-
ment score) was achieved, using data from a
small treebank of Swedish (Einarsson, 1976), di-
vided into a training set of 5054 sentences and
a test set of 631 sentences. However, in the
present context, we are primarily interested in
the incrementality of the parser, which we mea-
sure by considering the number of connected
components in (S,AS) at different stages dur-
ing the parsing of the test data.
The results can be found in Table 1, where
we see that out of 16545 configurations used in
parsing 613 sentences (with a mean length of
14.0 words), 68.9% have zero or one connected
component on the stack, which is what we re-
quire of a strictly incremental parser. We also
see that most violations of incrementality are
fairly mild, since more than 90% of all configu-
rations have no more than three connected com-
ponents on the stack.
Many violations of incrementality are caused
by sentences that cannot be parsed into a well-
formed dependency graph, i.e. a single projec-
tive dependency tree, but where the output of
the parser is a set of internally connected com-
ponents. In order to test the influence of incom-
plete parses on the statistics of incrementality,
we have performed a second experiment, where
we restrict the test data to those 444 sentences
(out of 613), for which the parser produces a
well-formed dependency graph. The results can
be seen in Table 2. In this case, 87.1% of all
configurations in fact satisfy the constraints of
incrementality, and the proportion of configu-
rations that have no more than three connected
components on the stack is as high as 99.5%.
It seems fair to conclude that, although strict
word-by-word incrementality is not possible in
deterministic dependency parsing, the arc-eager
algorithm can in practice be seen as a close ap-
proximation of incremental parsing.
6 Conclusion
In this paper, we have analyzed the potential
for incremental processing in deterministic de-
pendency parsing. Our first result is negative,
since we have shown that strict incrementality
is not achievable within the restrictive parsing
framework considered here. However, we have
also shown that the arc-eager parsing algorithm
is optimal for incremental dependency parsing,
given the constraints imposed by the overall
framework. Moreover, we have shown that in
practical parsing, the algorithm performs in-
cremental processing for the majority of input
structures. If we consider all sentences in the
test data, the share is roughly two thirds, but if
we limit our attention to well-formed output, it
is almost 90%. Since deterministic dependency
parsing has previously been shown to be com-
petitive in terms of parsing accuracy (Yamada
and Matsumoto, 2003; Nivre et al, 2004), we
believe that this is a promising approach for sit-
uations that require parsing to be robust, effi-
cient and (almost) incremental.
Connected Parser configurations
components Number Percent
0 928 9.2
1 7823 77.8
2 1000 10.0
3 248 2.5
4 41 0.4
5 8 0.1
6 1 0.0
? 1 8751 87.1
? 3 9999 99.5
? 6 10049 100.0
Table 2: Number of connected components in (S,AS) for well-formed trees
Acknowledgements
The work presented in this paper was sup-
ported by a grant from the Swedish Re-
search Council (621-2002-4207). The memory-
based classifiers used in the experiments were
constructed using the Tilburg Memory-Based
Learner (TiMBL) (Daelemans et al, 2003).
Thanks to three anonymous reviewers for con-
structive comments on the submitted paper.
References
Steven Abney and Mark Johnson. 1991. Mem-
ory requirements and local ambiguities of
parsing strategies. Journal of Psycholinguis-
tic Research, 20:233?250.
Steven Abney. 1991. Parsing by chunks.
In Principle-Based Parsing, pages 257?278.
Kluwer.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings NAACL-
2000.
Michael Collins. 1997. Three generative, lexi-
calised models for statistical parsing. In Pro-
ceedings of the 35th Annatual Meeting of the
Association for Computational Linguistics,
pages 16?23, Madrid, Spain.
Walter Daelemans, Sabine Buchholz, and Jorn
Veenstra. 1999. Memory-based shallow pars-
ing. In Proceedings of the 3rd Conference
on Computational Natural Language Learn-
ing (CoNLL), pages 77?89.
Walter Daelemans, Jakub Zavrel, Ko van der
Sloot, and Antal van den Bosch. 2003.
Timbl: Tilburg memory based learner, ver-
sion 5.0, reference guide. Technical Report
ILK 03-10, Tilburg University, ILK.
Jan Einarsson. 1976. Talbankens skriftspr?aks-
konkordans. Lund University.
Lyn Frazier. 1987. Syntactic processing: Ev-
idence from Dutch. Natural Language and
Linguistic Theory, 5:519?559.
Matthias Trautner Kromann. 2002. Optimality
parsing and local cost functions in Discontin-
uous Grammar. Electronic Notes of Theoret-
ical Computer Science, 52.
William Marslen-Wilson. 1973. Linguistic
structure and speech shadowing at very short
latencies. Nature, 244:522?533.
Igor Mel?cuk. 1988. Dependency Syntax: The-
ory and Practice. State University of New
York Press.
George A. Miller and Noam Chomsky. 1963.
Finitary models of language users. In R. D.
Luce, R. R. Bush, and E. Galanter, editors,
Handbook of Mathematical Psychology. Vol-
ume 2. Wiley.
Glyn Morrill. 2000. Inremental processing
and acceptability. Computational Linguis-
tics, 26:319?338.
Joakim Nivre, Johan Hall, and Jens Nils-
son. 2004. Memory-based dependency pars-
ing. In Proceedings of the 8th Conference
on Computational Natural Language Learn-
ing (CoNLL), pages 49?56.
Joakim Nivre. 2003. An efficient algorithm
for projective dependency parsing. In Pro-
ceedings of the 8th International Workshop
on Parsing Technologies (IWPT), pages 149?
160.
Mark Steedman. 2000. The Syntactic Process.
MIT Press.
Hiroyasu Yamada and Yuji Matsumoto. 2003.
Statistical dependency analysis with support
vector machines. In Proceedings of the 8th In-
ternational Workshop on Parsing Technolo-
gies (IWPT), pages 195?206.
Memory-Based Dependency Parsing
Joakim Nivre, Johan Hall and Jens Nilsson
School of Mathematics and Systems Engineering
Va?xjo? University
SE-35195 Va?xjo?
Sweden
firstname.lastname@msi.vxu.se
Abstract
This paper reports the results of experiments
using memory-based learning to guide a de-
terministic dependency parser for unrestricted
natural language text. Using data from a small
treebank of Swedish, memory-based classifiers
for predicting the next action of the parser are
constructed. The accuracy of a classifier as
such is evaluated on held-out data derived from
the treebank, and its performance as a parser
guide is evaluated by parsing the held-out por-
tion of the treebank. The evaluation shows that
memory-based learning gives a signficant im-
provement over a previous probabilistic model
based on maximum conditional likelihood esti-
mation and that the inclusion of lexical features
improves the accuracy even further.
1 Introduction
Deterministic dependency parsing has recently been pro-
posed as a robust and efficient method for syntactic pars-
ing of unrestricted natural language text (Yamada and
Matsumoto, 2003; Nivre, 2003). Dependency parsing
means that the goal of the parsing process is to construct
a dependency graph, of the kind depicted in Figure 1. De-
terministic parsing means that we always derive a single
analysis for each input string. Moreover, this single anal-
ysis is derived in a monotonic fashion with no redundancy
or backtracking, which makes it possible to parse natural
language sentences in linear time (Nivre, 2003).
In this paper, we report experiments using memory-
based learning (Daelemans, 1999) to guide the parser de-
scribed in Nivre (2003), using data from a small tree-
bank of Swedish (Einarsson, 1976). Unlike most pre-
vious work on data-driven dependency parsing (Eisner,
1996; Collins et al, 1999; Yamada and Matsumoto, 2003;
Nivre, 2003), we assume that dependency graphs are la-
beled with dependency types, although the evaluation
will give results for both labeled and unlabeled represen-
tations.
The paper is structured as follows. Section 2 gives
the necessary background definitions and introduces the
idea of guided parsing as well as memory-based learning.
Section 3 describes the data used in the experiments, the
evaluation metrics, and the models and algorithms used
in the learning process. Results from the experiments are
given in section 4, while conclusions and suggestions for
further research are presented in section 5.
2 Background
2.1 Dependency Graphs
The linguistic tradition of dependency grammar com-
prises a large and fairly diverse family of theories and for-
malisms that share certain basic assumptions about syn-
tactic structure, in particular the assumption that syntactic
structure consists of lexical nodes linked by binary re-
lations called dependencies (see, e.g., Tesnie`re (1959),
Sgall (1986), Mel?c?uk (1988), Hudson (1990)). Thus,
the common formal property of dependency structures,
as compared to the representations based on constituency
(or phrase structure), is the lack of nonterminal nodes.
In a dependency structure, every word token is depen-
dent on at most one other word token, usually called its
head or regent, which means that the structure can be
represented as a directed graph, with nodes representing
word tokens and arcs representing dependency relations.
In addition, arcs may be labeled with specific dependency
types. Figure 1 shows a labeled dependency graph for a
simple Swedish sentence, where each word of the sen-
tence is labeled with its part of speech and each arc la-
beled with a grammatical function.
Formally, we define dependency graphs in the follow-
ing way:
PP
Pa?
(In
 
?
ADV
NN
60-talet
the-60?s
 
?
PR
VB
ma?lade
painted
PN
han
he
 
?
SUB
JJ
dja?rva
bold
 
?
ATT
NN
tavlor
pictures
 
?
OBJ
HP
som
which
 
?
ATT
VB
retade
annoyed
?
 
SUB
PM
Nikita
Nikita
 
?
OBJ
PM
Chrusjtjov.
Chrustjev.)
 
?
ID
Figure 1: Dependency graph for Swedish sentence
1. Let R = {r1, . . . , rm} be the set of permissible de-
pendency types (arc labels).
2. A dependency graph for a string of words W =
w1? ? ?wn is a labeled directed graph D = (W,A),
where
(a) W is the set of nodes, i.e. word tokens in the
input string,
(b) A is a set of labeled arcs (wi, r, wj) (where
wi, wj ? W and r ? R).
We write wi < wj to express that wi precedes wj
in the string W (i.e., i < j); we write wi r? wj to
say that there is an arc from wi to wj labeled r, and
wi ? wj to say that there is an arc from wi to wj
(regardless of the label); we use ?? to denote the
reflexive and transitive closure of the unlabeled arc
relation; and we use ? and ?? for the correspond-
ing undirected relations, i.e. wi ? wj iff wi ? wj
or wj ? wi.
3. A dependency graph D = (W,A) is well-formed iff
the five conditions given in Figure 2 are satisfied.
For a more detailed discussion of dependency graphs
and well-formedness conditions, the reader is referred to
Nivre (2003).
2.2 Parsing Algorithm
The parsing algorithm presented in Nivre (2003) is in
many ways similar to the basic shift-reduce algorithm for
context-free grammars (Aho et al, 1986), although the
parse actions are different given that no nonterminal sym-
bols are used. Moreover, unlike the algorithm of Yamada
and Matsumoto (2003), the algorithm considered here ac-
tually uses a blend of bottom-up and top-down process-
ing, constructing left-dependencies bottom-up and right-
dependencies top-down, in order to achieve incremental-
ity. For a similar but nondeterministic approach to depen-
dency parsing, see Obrebski (2003).
Parser configurations are represented by triples
?S, I, A?, where S is the stack (represented as a list), I is
the list of (remaining) input tokens, and A is the (current)
arc relation for the dependency graph. Given an input
string W , the parser is initialized to ?nil,W, ?? and termi-
nates when it reaches a configuration ?S,nil, A? (for any
list S and set of arcs A). The input string W is accepted if
the dependency graph D = (W,A) given at termination
is well-formed; otherwise W is rejected. The behavior of
the parser is defined by the transitions defined in Figure
3 (where wi, wj and wk are arbitrary word tokens, and r
and r? are arbitrary dependency relations):
1. The transition Left-Arc (LA) adds an arc wj r?wi
from the next input token wj to the token wi on top
of the stack and reduces (pops) wi from the stack.
2. The transition Right-Arc (RA) adds an arc wi r?wj
from the token wi on top of the stack to the next in-
put token wj , and shifts (pushes) wj onto the stack.
3. The transition Reduce (RE) reduces (pops) the to-
ken wi on top of the stack.
4. The transition Shift (SH) shifts (pushes) the next in-
put token wi onto the stack.
The transitions Left-Arc and Right-Arc are subject to
conditions that ensure that the graph conditions Unique
label and Single head are satisfied. By contrast, the Re-
duce transition can only be applied if the token on top of
the stack already has a head. For Shift, the only condition
is that the input list is non-empty.
As it stands, this transition system is nondeterminis-
tic, since several transitions can often be applied to the
same configuration. Thus, in order to get a deterministic
parser, we need to introduce a mechanism for resolving
transition conflicts. Regardless of which mechanism is
used, the parser is guaranteed to terminate after at most
2n transitions, given an input string of length n (Nivre,
2003). This means that as long as transitions can be per-
formed in constant time, the running time of the parser
will be linear in the length of the input. Moreover, the
parser is guaranteed to produce a dependency graph that
is acyclic and projective (and satisfies the unique-label
and single-head constraints). This means that the depen-
dency graph given at termination is well-formed if and
only if it is connected (Nivre, 2003).
Unique label (wi r?wj ? wi r
?
?wj) ? r = r?
Single head (wi?wj ? wk?wj) ? wi = wk
Acyclic ?(wi?wj ? wj??wi)
Connected wi??wj
Projective (wi?wk ? wi<wj<wk) ? (wi??wj ? wk??wj)
Figure 2: Well-formedness conditions on dependency graphs
Initialization ?nil,W, ??
Termination ?S,nil, A?
Left-Arc ?wi|S,wj |I, A? ? ?S,wj |I, A ? {(wj , r, wi)}? ??wk?r?(wk, r?, wi) ? A
Right-Arc ?wi|S,wj |I, A? ? ?wj |wi|S, I, A ? {(wi, r, wj)}? ??wk?r?(wk, r?, wj) ? A
Reduce ?wi|S, I, A? ? ?S, I, A? ?wj?r(wj , r, wi) ? A
Shift ?S,wi|I, A? ? ?wi|S, I, A?
Figure 3: Parser transitions
2.3 Guided Parsing
One way of turning a nondeterministic parser into a deter-
ministic one is to use a guide (or oracle) that can inform
the parser at each nondeterministic choice point; cf. Kay
(2000), Boullier (2003). Guided parsing is normally used
to improve the efficiency of a nondeterministic parser,
e.g. by letting a simpler (but more efficient) parser con-
struct a first analysis that can be used to guide the choice
of the more complex (but less efficient) parser. This is the
approach taken, for example, in Boullier (2003).
In our case, we rather want to use the guide to im-
prove the accuracy of a deterministic parser, starting from
a baseline of randomized choice. One way of doing this
is to use a treebank, i.e. a corpus of analyzed sentences, to
train a classifier that can predict the next transition (and
dependency type) given the current configuration of the
parser. However, in order to maintain the efficiency of the
parser, the classifier must also be implemented in such a
way that each transition can still be performed in constant
time.
Previous work in this area includes the use of memory-
based learning to guide a standard shift-reduce parser
(Veenstra and Daelemans, 2000) and the use of sup-
port vector machines to guide a deterministic depen-
dency parser (Yamada and Matsumoto, 2003). In the
experiments reported in this paper, we apply memory-
based learning within a deterministic dependency parsing
framework.
2.4 Memory-Based Learning
Memory-based learning and problem solving is based on
two fundamental principles: learning is the simple stor-
age of experiences in memory, and solving a new problem
is achieved by reusing solutions from similar previously
solved problems (Daelemans, 1999). It is inspired by the
nearest neighbor approach in statistical pattern recogni-
tion and artificial intelligence (Fix and Hodges, 1952), as
well as the analogical modeling approach in linguistics
(Skousen, 1989; Skousen, 1992). In machine learning
terms, it can be characterized as a lazy learning method,
since it defers processing of input until needed and pro-
cesses input by combining stored data (Aha, 1997).
Memory-based learning has been successfully applied
to a number of problems in natural language process-
ing, such as grapheme-to-phoneme conversion, part-
of-speech tagging, prepositional-phrase attachment, and
base noun phrase chunking (Daelemans et al, 2002).
Most relevant in the present context is the use of memory-
based learning to predict the actions of a shift-reduce
parser, with promising results reported in Veenstra and
Daelemans (2000).
The main reason for using memory-based learning in
the present context is the flexibility offered by similarity-
based extrapolation when classifying previously unseen
configurations, since previous experiments with a proba-
bilistic model has shown that a fixed back-off sequence
does not work well in this case (Nivre, 2004). Moreover,
the memory-based approach can easily handle multi-class
classification, unlike the support vector machines used by
Yamada and Matsumoto (2003).
For the experiments reported in this paper, we have
used the software package TiMBL (Tilburg Memory
Based Learner), which provides a variety of metrics, al-
gorithms, and extra functions on top of the classical k
nearest neighbor classification kernel, such as value dis-
tance metrics and distance weighted class voting (Daele-
mans et al, 2003).
3 Method
3.1 Target Function and Approximation
The function we want to approximate is a mapping f
from parser configurations to parser actions, where each
action consists of a transition and (unless the transition is
Shift or Reduce) a dependency type:
f : Config ? {LA,RA,RE, SH} ? (R ? {nil})
Here Config is the set of all possible parser configura-
tions and R is the set of dependency types as before.
However, in order to make the problem tractable, we try
to learn a function f? whose domain is a finite space of
parser states, which are abstractions over configurations.
For this purpose we define a number of features that can
be used to define different models of parser state. The
features used in this study are listed in Table 1.
The first five features (TOP?TOP.RIGHT) deal with
properties of the token on top of the stack. In addition to
the word form itself (TOP), we consider its part-of-speech
(as assigned by an automatic part-of-speech tagger in a
preprocessing phase), the dependency type by which it is
related to its head (which may or may not be available in
a given configuration depending on whether the head is
to the left or to the right of the token in question), and
the dependency types by which it is related to its leftmost
and rightmost dependent, respectively (where the current
rightmost dependent may or may not be the rightmost de-
pendent in the complete dependency tree).
The following three features (NEXT?NEXT.LEFT) refer
to properties of the next input token. In this case, there are
no features corresponding to TOP.DEP and TOP.RIGHT,
since the relevant dependencies can never be present at
decision time. The final feature (LOOK) is a simple looka-
head, using the part-of-speech of the next plus one input
token.
In the experiments reported below, we have used
two different parser state models, one called the lexical
model, which includes all nine features, and one called
the non-lexical model, where the two lexical features
TOP and NEXT are omitted. For both these models, we
have used memory-based learning with different parame-
ter settings, as implemented TiMBL.
For comparison, we have included an earlier classifier
that uses the same features as the non-lexical model, but
where prediction is based on maximum conditional likeli-
hood estimation. This classifier always predicts the most
probable transition given the state and the most probable
dependency type given the transition and the state, with
conditional probabilities being estimated by the empiri-
cal distribution in the training data. Smoothing is per-
formed only for zero frequency events, in which case the
classifier backs off to more general models by omitting
first the features TOP.LEFT and LOOK and then the fea-
tures TOP.RIGHT and NEXT.LEFT; if even this does not
help, the classifier predicts Reduce if permissible and
Shift otherwise. This model, which we will refer to as the
MCLE model, is described in more detail in Nivre (2004).
3.2 Data
It is standard practice in data-driven approaches to nat-
ural language parsing to use treebanks both for training
and evaluation. Thus, the Penn Treebank of American
English (Marcus et al, 1993) has been used to train and
evaluate the best available parsers of unrestricted English
text (Collins, 1999; Charniak, 2000). One problem when
developing a parser for Swedish is that there is no com-
parable large-scale treebank available for Swedish.
For the experiments reported in this paper we have
used a manually annotated corpus of written Swedish,
created at Lund University in the 1970?s and consisting
mainly of informative texts from official sources (Einars-
son, 1976). Although the original annotation scheme is
an eclectic combination of constituent structure, depen-
dency structure, and topological fields (Teleman, 1974),
it has proven possible to convert the annotated sentences
to dependency graphs with fairly high accuracy.
In the conversion process, we have reduced the orig-
inal fine-grained classification of grammatical functions
to a more restricted set of 16 dependency types, which
are listed in Table 2. We have also replaced the origi-
nal (manual) part-of-speech annotation by using the same
automatic tagger that is used for preprocessing in the
parser. This is a standard probabilistic tagger trained on
the Stockholm-Umea? Corpus of written Swedish (SUC,
1997) and found to have an accuracy of 95?96% when
tested on held-out data.
Since the function we want to learn is a mapping from
parser states to transitions (and dependency types), the
treebank data cannot be used directly as training and test
Feature Description
TOP The token on top of the stack
TOP.POS The part-of-speech of TOP
TOP.DEP The dependency type of TOP (if any)
TOP.LEFT The dependency type of TOP?s leftmost dependent (if any)
TOP.RIGHT The dependency type of TOP?s rightmost dependent (if any)
NEXT The next input token
NEXT.POS The part-of-speech of NEXT
NEXT.LEFT The dependency type of NEXT?s leftmost dependent (if any)
LOOK.POS The part-of-speech of the next plus one input token
Table 1: Parser state features
data. Instead, we have to simulate the parser on the tree-
bank in order to derive, for each sentence, the transition
sequence corresponding to the correct dependency tree.
Given the result of this simulation, we can construct a
data set consisting of pairs ?s, t?, where s is a parser state
and t is the correct transition from that state (including
a dependency type if applicable). Unlike standard shift-
reduce parsing, the simulation of the current algorithm is
almost deterministic and is guaranteed to be correct if the
input dependency tree is well-formed.
The complete converted treebank contains 6316 sen-
tences and 97623 word tokens, which gives a mean sen-
tence length of 15.5 words. The treebank has been di-
vided into three non-overlapping data sets: 80% for train-
ing 10% for development/validation, and 10% for final
testing (random samples). The results presented below
are all from the validation set. (The final test set has not
been used at all in the experiments reported in this paper.)
When talking about test and validation data, we make
a distinction between the sentence data, which refers to
the original annotated sentences in the treebank, and the
transition data, which refers to the transitions derived by
simulating the parser on these sentences. While the sen-
tence data for validation consists of 631 sentences, the
corresponding transition data contains 15913 instances.
For training, only transition data is relevant and the train-
ing data set contains 371977 instances.
3.3 Evaluation
The output of the memory-based learner is a classifier that
predicts the next transition (including dependency type),
given the current state of the parser. The quality of this
classifier has been evaluated with respect to both predic-
tion accuracy and parsing accuracy.
Prediction accuracy refers to the quality of the clas-
sifier as such, i.e. how well it predicts the next transition
given the correct parser state, and is measured by the clas-
sification accuracy on unseen transition data (using a 0-1
loss function). We use McNemar?s test for statistical sig-
nificance.
Parsing accuracy refers to the quality of the classifier
as a guide for the deterministic parser and is measured
by the accuracy obtained when parsing unseen sentence
data. More precisely, parsing accuracy is measured by
the attachment score, which is a standard measure used
in studies of dependency parsing (Eisner, 1996; Collins
et al, 1999). The attachment score is computed as the
proportion of tokens (excluding punctuation) that are as-
signed the correct head (or no head if the token is a root).
Since parsing is a sentence-level task, we believe that
the overall attachment score should be computed as the
mean attachment score per sentence, which gives an es-
timate of the expected attachment score for an arbitrary
sentence. However, since most previous studies instead
use the mean attachment score per word (Eisner, 1996;
Collins et al, 1999), we will give this measure as well.
In order to measure label accuracy, we also define a la-
beled attachment score, where both the head and the label
must be correct, but which is otherwise computed in the
same way as the ordinary (unlabeled) attachment score.
For parsing accuracy, we use a paired t-test for statistical
significance.
4 Results
Table 3 shows the prediction accuracy achieved with
memory-based learning for the lexical and non-lexical
model, with two different parameter settings for the
learner. The results in the first column were obtained with
the default settings of the TiMBL package, in particular:
? The IB1 classification algorithm (Aha et al, 1991).
? The overlap distance metric.
? Features weighted by Gain Ratio (Quinlan, 1993).
? k = 1, i.e. classification based on a single nearest
neighbor.1
1In TiMBL, the value of k in fact refers to k nearest dis-
tances rather than k nearest neighbors, which means that, even
with k = 1, the nearest neighbor set can contain several in-
Label Dependency Type
ADV Adverbial modifier
APP Apposition
ATT Attribute
CC Coordination (conjunction or second conjunct)
DET Determiner
ID Non-first element of multi-word expression
IM Infinitive dependent on infinitive marker
IP Punctuation mark dependent on lexical head
INF Infinitival complement
OBJ Object
PR Complement of preposition
PRD Predicative complement
SUB Subject
UK Main verb of subordinate clause dependent on complementizer
VC Verb chain (nonfinite verb dependent on other verb)
XX Unclassifiable dependent
Table 2: Dependency types in Swedish treebank
Model Default Maximum
Non-lexical 86.8 87.4
Lexical 88.4 89.7
Table 3: Prediction accuracy for MBL models
The second column shows the accuracy for the best pa-
rameter settings found in the experiments (averaged over
both models), which differ from the default in the follow-
ing respects:
? Overlap metric replaced by the modified value dis-
tance metric (MVDM) (Stanfill and Waltz, 1986;
Cost and Salzberg, 1993).
? No weighting of features.
? k = 5, i.e. classification based on 5 nearest neigh-
bors.
? Distance weighted class voting with inverse distance
weighting (Dudani, 1976).
For more information about the different parameters and
settings, the reader is referred to Daelemans et al (2003).
The results show that the lexical model performs con-
sistently better than the non-lexical model, and that the
difference increases with the optimization of the learning
algorithm (all differences being significant at the .0001
level according to McNemar?s test). This confirms pre-
vious results from statistical parsing indicating that lex-
ical information is crucial for disambiguation (Collins,
stances that are equally distant to the test instance. This is dif-
ferent from the original IB1 algorithm, as described in Aha et
al. (1991).
1999; Charniak, 2000). As regards optimization, we may
note that although there is a significant improvement for
both models, the magnitude of the difference is relatively
small.
Table 4 shows the parsing accuracy obtained with the
optimized versions of the MBL models (lexical and non-
lexical), compared to the MCLE model described in sec-
tion 3. We see that MBL outperforms the MCLE model
even when limited to the same features (all differences
again being significant at the .0001 level according to
a paired t-test). This can probably be explained by the
fact that the similarity-based smoothing built into the
memory-based approach gives a better extrapolation than
the fixed back-off sequence in the MCLE model. We
also see that the lexical MBL model outperforms both
the other models. If we compare the labeled attachment
score to the prediction accuracy (which also takes depen-
dency types into account), we observe a substantial drop
(from 89.7 to 81.7 for the lexical model, from 87.4 to
76.5 for the non-lexical model), which is of course only
to be expected. The unlabeled attachment score is natu-
rally higher, and it is worth noting that the relative differ-
ence between the MBL lexical model and the other two
models is much smaller. This indicates that the advan-
tage of the lexical model mainly concerns the accuracy in
predicting dependency type in addition to transition.
Model Labeled Unlabeled
MCLE 74.7 (72.3) 81.5 (79.7)
MBL non-lexical 76.5 (74.7) 82.9 (81.7)
MBL lexical 81.7 (80.6) 85.7 (84.7)
Table 4: Parsing accuracy for MCLE and MBL models, attachment score per sentence (per word in parentheses)
If we compare the results concerning parsing accuracy
to those obtained for other languages (given that there
are no comparable results available for Swedish), we note
that the best unlabeled attachment score is lower than for
English, where the best results are above 90% (attach-
ment score per word) (Collins et al, 1999; Yamada and
Matsumoto, 2003), but higher than for Czech (Collins et
al., 1999). This is encouraging, given that the size of
the training set in our experiments is fairly small, only
about 10% of the standard training set for the Penn Tree-
bank. One reason why our results nevertheless compare
reasonably well with those obtained with the much larger
training set is probably that the conversion to dependency
trees is more accurate for the Swedish treebank, given the
explicit annotatation of grammatical functions. More-
over, the fact that our parser uses labeled dependencies
is probably also significant, since the possibility of us-
ing information from previously assigned (labeled) de-
pendencies during parsing seems to have a positive effect
on accuracy (Nivre, 2004).
Finally, it may be interesting to consider the accuracy
for individual dependency types. Table 5 gives labeled
precision, labeled recall and unlabeled attachment score
for four of the most important types with the MBL lex-
ical model. The results indicate that subjects have the
highest accuracy, especially when labels are taken into
account. Objects and predicative complements have com-
parable attachment accuracy, but are more often misclas-
sified with respect to dependency type. For adverbial
modifiers, finally, attachment accuracy is lower than for
the other dependency types, which is largely due to the
notorious PP-attachment problem.
5 Conclusion
In this paper we have shown that a combination of
memory-based learning and deterministic dependency
parsing can be used to construct a robust and efficient
parser for unrestricted natural language text, achieving a
parsing accuracy which is close to the state of the art even
with relatively limited amounts of training data. Clas-
sifiers based on memory-based learning achieve higher
parsing accuracy than previous probabilistic models, and
the improvement increases if lexical information is added
to the model.
Suggestions for further research includes the further
exploration of alternative models and parameter settings,
but also the combination of inductive and analytical
learning to impose high-level linguistic constraints, and
the development of new parsing methods (e.g. involving
multiple passes over the data). In addition, it is important
to evaluate the approach with respect to other languages
and corpora in order to increase the comparability with
other approaches.
Acknowledgements
The work presented in this paper was supported by a
grant from the Swedish Research Council (621-2002-
4207). The memory-based classifiers used in the experi-
ments were constructed using the Tilburg Memory-Based
Learner (TiMBL) (Daelemans et al, 2003). We are grate-
ful to three anonymous reviewers for constructive com-
ments on the preliminary version of the paper.
References
D. W. Aha, D. Kibler and M. Albert. 1991. Instance-
based Learning Algorithms. Machine Learning 6, 37?
66.
D. Aha. 1997. Lazy Learning. Dordrecht: Kluwer.
A. V. Aho, R. Sethi and J. D. Ullman. 1986. Compilers:
Principles Techniques, and Tools. Addison Wesley.
P. Boullier. 2003. Guided Earley Parsing. In G. van No-
ord (ed.) Proceedings of the 8th International Work-
shop on Parsing Technologies (IWPT 03), Nancy,
France, pp. 43?54.
E. Charniak. 2000. A Maximum-Entropy-Inspired
Parser. In Proceedings NAACL-2000.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. PhD Thesis, University of
Pennsylvania.
M. Collins, J. Hajic, E. Brill, L. Ramshaw and C. Till-
mann. 1999. A Statistical Parser of Czech. In Pro-
ceedings of 37th ACL Conference, University of Mary-
land, College Park, USA, pp. 505?512.
S. Cost and S. Salzberg. 1993. A Weighted Nearest
Neighbor Algorithm for Learning with Symbolic Fea-
tures. Machine Learning 10, 57?78.
Dependency type Precision Recall Attachment
SUB 84.3 82.7 89.2
OBJ 74.7 78.8 87.0
PRD 75.1 71.4 84.2
ADV 76.2 74.6 78.3
Table 5: Dependency type accuracy, MBL lexical model; labeled precision, labeled recall, unlabeled attachment score
W. Daelemans. 1999. Memory-Based Language Pro-
cessing. Introduction to the Special Issue. Journal
of Experimental and Theoretical Artificial Intelligence
11(3), 287?292.
W. Daelemans, A. van den Bosch, J. Zavrel. 2002. For-
getting Exceptions is Harmful in Language Learning.
Machine Learning 34, 11?43.
W. Daelemans, J. Zavrel, K. van der Sloot and
A. van den Bosch, . 2003. TiMBL: Tilburg Memory
Based Learner, version 5.0, Reference Guide. Techni-
cal Report ILK 03-10, Tilburg University.
S. A. Dudani. 1976. The Distance-Weighted K-nearest
Neighbor Rule. IEEE Transactions on Systems, Man,
and Cybernetics SMC-6, 325?327.
J. Einarsson. 1976. Talbankens skriftsprkskonkordans.
Lund University.
J. M. Eisner. 1996. Three New Probabilistic Models for
Dependency Parsing: An Exploration. In Proceedings
of COLING-96, Copenhagen.
E. Fix and J. Hodges. 1952. Discriminatory Analy-
sis: Nonparametric Discrimination: Consistency Prop-
erties. Technical Report 21-49-004-11, USAF School
of Aviation Medicine, Randolph Field, Texas.
R. A. Hudson. 1990. English Word Grammar. Black-
well.
M. Kay. 2000. Guides and Oracles for Linear-Time Pars-
ing. In Proceedings of the 6th International Workshop
on Parsing Technologies (IWPT 00), Trento, Italy, pp.
6?9.
M. P. Marcus, B. Santorini and M. A. Marcinkiewics.
1993. Building a Large Annotated Corpus of English:
The Penn Treebank. Computational Linguistics 19,
313?330.
I. Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
J. Nivre. 2003. An Efficient Algorithm for Projective De-
pendency Parsing. In G. van Noord (ed.) Proceedings
of the 8th International Workshop on Parsing Tech-
nologies (IWPT 03), Nancy, France, pp. 149?160.
J. Nivre. 2004. Inductive Dependency Parsing. Techni-
cal Report, Va?xjo? University.
T. Obrebski. 2003. Dependency Parsing Using Depen-
dency Graph. In G. van Noord (ed.) Proceedings of
the 8th International Workshop on Parsing Technolo-
gies (IWPT 03), Nancy, France, pp. 217?218.
J. R. Quinlan. 1993. C4.5: Programs for Machine
Learning. San Mateo, CA: Morgan Kaufmann.
P. Sgall, E. Hajicova? and J. Panevova?. 1986. The Mean-
ing of the Sentence in Its Pragmatic Aspects. Reidel.
R. Skousen. 1989. Analogical Modeling of Language.
Dordrecht: Kluwer.
R. Skousen. 1992. Analogy and Structure. Dordrecht:
Kluwer.
C. Stanfill and D. Waltz. 1986. Toward Memory-Based
Reasoning. Communications of the ACM 29(12),
1213?1228.
SUC 1997. Stockholm Umea? Corpus. Version 1.0. Pro-
duced by Department of Linguistics, Umea? University
and Department of Linguistics, Stockholm University.
U. Teleman. 1974. Manual fo?r grammatisk beskrivning
av talad och skriven svenska. Studentlitteratur.
L. Tesnie`re. 1959. Ele?ments de syntaxe structurale. Edi-
tions Klincksieck
J. Veenstra and W. Daelemans. 2000. A Memory-Based
Alternative for Connectionist Shift-Reduce Parsing.
Technical Report ILK-0012, University of Tilburg.
H. Yamada and Y. Matsumoto. 2003. Statistical De-
pendency Analysis with Support Vector Machines. In
G. van Noord (ed.) Proceedings of the 8th Interna-
tional Workshop on Parsing Technologies (IWPT 03),
Nancy, France, pp. 195?206.
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 221?225, New York City, June 2006. c?2006 Association for Computational Linguistics
Labeled Pseudo-Projective Dependency Parsing
with Support Vector Machines
Joakim Nivre
Johan Hall
Jens Nilsson
School of Mathematics
and Systems Engineering
Va?xjo? University
35195 Va?xjo?, Sweden
{nivre,jha,jni}@msi.vxu.se
Gu?ls?en Eryig?it
Department of
Computer Engineering
Istanbul Technical University
34469 Istanbul, Turkey
gulsen@cs.itu.edu.tr
Svetoslav Marinov
School of Humanities
and Informatics
University of Sko?vde
Box 408
54128 Sko?vde, Sweden
svetoslav.marinov@his.se
Abstract
We use SVM classifiers to predict the next
action of a deterministic parser that builds
labeled projective dependency graphs in
an incremental fashion. Non-projective
dependencies are captured indirectly by
projectivizing the training data for the
classifiers and applying an inverse trans-
formation to the output of the parser. We
present evaluation results and an error
analysis focusing on Swedish and Turkish.
1 Introduction
The CoNLL-X shared task consists in parsing texts
in multiple languages using a single dependency
parser that has the capacity to learn from treebank
data. Our methodology for performing this task is
based on four essential components:
? A deterministic algorithm for building labeled
projective dependency graphs (Nivre, 2006).
? History-based feature models for predicting the
next parser action (Black et al, 1992).
? Support vector machines for mapping histories
to parser actions (Kudo and Matsumoto, 2002).
? Graph transformations for recovering non-
projective structures (Nivre and Nilsson, 2005).
All experiments have been performed using Malt-
Parser (Nivre et al, 2006), version 0.4, which is
made available together with the suite of programs
used for pre- and post-processing.1
1www.msi.vxu.se/users/nivre/research/MaltParser.html
2 Parsing Methodology
2.1 Parsing Algorithm
The parsing algorithm used for all languages is the
deterministic algorithm first proposed for unlabeled
dependency parsing by Nivre (2003) and extended
to labeled dependency parsing by Nivre et al (2004).
The algorithm builds a labeled dependency graph in
one left-to-right pass over the input, using a stack
to store partially processed tokens and adding arcs
using four elementary actions (where top is the token
on top of the stack and next is the next token):
? SHIFT: Push next onto the stack.
? REDUCE: Pop the stack.
? RIGHT-ARC(r): Add an arc labeled r from top
to next; push next onto the stack.
? LEFT-ARC(r): Add an arc labeled r from next
to top; pop the stack.
Although the parser only derives projective graphs,
the fact that graphs are labeled allows non-projective
dependencies to be captured using the pseudo-
projective approach of Nivre and Nilsson (2005) .
Another limitation of the parsing algorithm is that
it does not assign dependency labels to roots, i.e., to
tokens having HEAD=0. To overcome this problem,
we have implemented a variant of the algorithm that
starts by pushing an artificial root token with ID=0
onto the stack. Tokens having HEAD=0 can now
be attached to the artificial root in a RIGHT-ARC(r)
action, which means that they can be assigned any
label. Since this variant of the algorithm increases
the overall nondeterminism, it has only been used
for the data sets that include informative root labels
(Arabic, Czech, Portuguese, Slovene).
221
FO L C P FE D
S: top + + + + + +
S: top?1 +
I: next + + + + +
I: next+1 + +
I: next+2 +
I: next+3 +
G: head of top +
G: leftmost dep of top +
G: rightmost dep of top +
G: leftmost dep of next +
Table 1: Base model; S: stack, I: input, G: graph;
FO: FORM, L: LEMMA , C: CPOS, P: POS,
FE: FEATS, D: DEPREL
2.2 History-Based Feature Models
History-based parsing models rely on features of the
derivation history to predict the next parser action.
The features used in our system are all symbolic
and extracted from the following fields of the data
representation: FORM, LEMMA, CPOSTAG, POSTAG,
FEATS, and DEPREL. Features of the type DEPREL
have a special status in that they are extracted during
parsing from the partially built dependency graph
and may therefore contain errors, whereas all the
other features have gold standard values during both
training and parsing.2
Based on previous research, we defined a base
model to be used as a starting point for language-
specific feature selection. The features of this model
are shown in Table 1, where rows denote tokens in
a parser configuration (defined relative to the stack,
the remaining input, and the partially built depen-
dency graph), and where columns correspond to data
fields. The base model contains twenty features, but
note that the fields LEMMA, CPOS and FEATS are not
available for all languages.
2.3 Support Vector Machines
We use support vector machines3 to predict the next
parser action from a feature vector representing the
history. More specifically, we use LIBSVM (Chang
and Lin, 2001) with a quadratic kernel K(xi, xj) =
(?xTi xj +r)2 and the built-in one-versus-all strategy
for multi-class classification. Symbolic features are
2The fields PHEAD and PDEPREL have not been used at all,
since we rely on pseudo-projective parsing for the treatment of
non-projective structures.
3We also ran preliminary experiments with memory-based
learning but found that this gave consistently lower accuracy.
converted to numerical features using the standard
technique of binarization, and we split values of the
FEATS field into its atomic components.4
For some languages, we divide the training data
into smaller sets, based on some feature s (normally
the CPOS or POS of the next input token), which may
reduce training times without a significant loss in
accuracy (Yamada and Matsumoto, 2003). To avoid
too small training sets, we pool together categories
that have a frequency below a certain threshold t.
2.4 Pseudo-Projective Parsing
Pseudo-projective parsing was proposed by Nivre
and Nilsson (2005) as a way of dealing with
non-projective structures in a projective data-driven
parser. We projectivize training data by a minimal
transformation, lifting non-projective arcs one step
at a time, and extending the arc label of lifted arcs
using the encoding scheme called HEAD by Nivre
and Nilsson (2005), which means that a lifted arc is
assigned the label r?h, where r is the original label
and h is the label of the original head in the non-
projective dependency graph.
Non-projective dependencies can be recovered by
applying an inverse transformation to the output of
the parser, using a left-to-right, top-down, breadth-
first search, guided by the extended arc labels r?h
assigned by the parser. This technique has been used
without exception for all languages.
3 Experiments
Since the projective parsing algorithm and graph
transformation techniques are the same for all data
sets, our optimization efforts have been focused on
feature selection, using a combination of backward
and forward selection starting from the base model
described in section 2.2, and parameter optimization
for the SVM learner, using grid search for an optimal
combination of the kernel parameters ? and r, the
penalty parameter C and the termination criterion ?,
as well as the splitting feature s and the frequency
threshold t. Feature selection and parameter opti-
mization have to some extent been interleaved, but
the amount of work done varies between languages.
4Preliminary experiments showed a slight improvement for
most languages when splitting the FEATS values, as opposed to
taking every combination of atomic values as a distinct value.
222
Ara Bul Chi Cze Dan Dut Ger Jap Por Slo Spa Swe Tur Total
LAS 66.71 87.41 86.92 78.42 84.77 78.59 85.82 91.65 87.60 70.30 81.29 84.58 65.68 80.19
UAS 77.52 91.72 90.54 84.80 89.80 81.35 88.76 93.10 91.22 78.72 84.67 89.50 75.82 85.48
LAcc 80.34 90.44 89.01 85.40 89.16 83.69 91.03 94.34 91.54 80.54 90.06 87.39 78.49 86.75
Table 2: Evaluation on final test set; LAS = labeled attachment score, UAS = unlabeled attachment score,
LAcc = label accuracy score; total score excluding Bulgarian
The main optimization criterion has been labeled
attachment score on held-out data, using ten-fold
cross-validation for all data sets with 100k tokens
or less, and an 80-20 split into training and devtest
sets for larger datasets. The number of features in
the optimized models varies from 16 (Turkish) to 30
(Spanish), but the models use all fields available for
a given language, except that FORM is not used for
Turkish (only LEMMA). The SVM parameters fall
into the following ranges: ?: 0.12?0.20; r: 0.0?0.6;
C: 0.1?0.7; ?: 0.01?1.0. Data has been split on the
POS of the next input token for Czech (t = 200),
German (t = 1000), and Spanish (t = 1000), and
on the CPOS of the next input token for Bulgarian
(t = 1000), Slovene (t = 600), and Turkish (t = 100).
(For the remaining languages, the training data has
not been split at all.)5 A dry run at the end of the
development phase gave a labeled attachment score
of 80.46 over the twelve required languages.
Table 2 shows final test results for each language
and for the twelve required languages together. The
total score is only 0.27 percentage points below the
score from the dry run, which seems to indicate that
models have not been overfitted to the training data.
The labeled attachment score varies from 91.65 to
65.68 but is above average for all languages. We
have the best reported score for Japanese, Swedish
and Turkish, and the score for Arabic, Danish,
Dutch, Portuguese, Spanish, and overall does not
differ significantly from the best one. The unlabeled
score is less competitive, with only Turkish having
the highest reported score, which indirectly indicates
that the integration of labels into the parsing process
primarily benefits labeled accuracy.
4 Error Analysis
An overall error analysis is beyond the scope of this
paper, but we will offer a few general observations
5Detailed specifications of the feature models and learning
algorithm parameters can be found on the MaltParser web page.
before we turn to Swedish and Turkish, focusing on
recall and precision of root nodes, as a reflection of
global syntactic structure, and on attachment score
as a function of arc length. If we start by considering
languages with a labeled attachment score of 85% or
higher, they are characterized by high precision and
recall for root nodes, typically 95/90, and by a grace-
ful degradation of attachment score as arcs grow
longer, typically 95?90?85, for arcs of length 1, 2
and 3?6. Typical examples are Bulgarian (Simov
et al, 2005; Simov and Osenova, 2003), Chinese
(Chen et al, 2003), Danish (Kromann, 2003), and
Swedish (Nilsson et al, 2005). Japanese (Kawata
and Bartels, 2000), despite a very high accuracy, is
different in that attachment score drops from 98%
to 85%, as we go from length 1 to 2, which may
have something to do with the data consisting of
transcribed speech with very short utterances.
A second observation is that a high proportion of
non-projective structures leads to fragmentation in
the parser output, reflected in lower precision for
roots. This is noticeable for German (Brants et al,
2002) and Portuguese (Afonso et al, 2002), which
still have high overall accuracy thanks to very high
attachment scores, but much more conspicuous for
Czech (Bo?hmova? et al, 2003), Dutch (van der Beek
et al, 2002) and Slovene (Dz?eroski et al, 2006),
where root precision drops more drastically to about
69%, 71% and 41%, respectively, and root recall is
also affected negatively. On the other hand, all three
languages behave like high-accuracy languages with
respect to attachment score. A very similar pattern
is found for Spanish (Civit Torruella and Mart?? An-
ton??n, 2002), although this cannot be explained by
a high proportion of non-projective structures. One
possible explanation in this case may be the fact that
dependency graphs in the Spanish data are sparsely
labeled, which may cause problem for a parser that
relies on dependency labels as features.
The results for Arabic (Hajic? et al, 2004; Smrz?
et al, 2002) are characterized by low root accuracy
223
as well as a rapid degradation of attachment score
with arc length (from about 93% for length 1 to 67%
for length 2). By contrast, Turkish (Oflazer et al,
2003; Atalay et al, 2003) exhibits high root accu-
racy but consistently low attachment scores (about
88% for length 1 and 68% for length 2). It is note-
worthy that Arabic and Turkish, being ?typological
outliers?, show patterns that are different both from
each other and from most of the other languages.
4.1 Swedish
A more fine-grained analysis of the Swedish results
reveals a high accuracy for function words, which
is compatible with previous studies (Nivre, 2006).
Thus, the labeled F-score is 100% for infinitive
markers (IM) and subordinating conjunctions (UK),
and above 95% for determiners (DT). In addition,
subjects (SS) have a score above 90%. In all these
cases, the dependent has a configurationally defined
(but not fixed) position with respect to its head.
Arguments of the verb, such as objects (DO, IO)
and predicative complements (SP), have a slightly
lower accuracy (about 85% labeled F-score), which
is due to the fact that they ?compete? in the same
structural positions, whereas adverbials (labels that
end in A) have even lower scores (often below 70%).
The latter result must be related both to the relatively
fine-grained inventory of dependency labels for ad-
verbials and to attachment ambiguities that involve
prepositional phrases. The importance of this kind
of ambiguity is reflected also in the drastic differ-
ence in accuracy between noun pre-modifiers (AT)
(F > 97%) and noun post-modifiers (ET) (F ? 75%).
Finally, it is worth noting that coordination, which
is often problematic in parsing, has high accuracy.
The Swedish treebank annotation treats the second
conjunct as a dependent of the first conjunct and as
the head of the coordinator, which seems to facil-
itate parsing.6 The attachment of the second con-
junct to the first (CC) has a labeled F-score above
80%, while the attachment of the coordinator to the
second conjunct (++) has a score well above 90%.
4.2 Turkish
In Turkish, very essential syntactic information is
contained in the rich morphological structure, where
6The analysis is reminiscent of the treatment of coordination
in the Collins parser (Collins, 1999).
concatenated suffixes carry information that in other
languages may be expressed by separate words. The
Turkish treebank therefore divides word forms into
smaller units, called inflectional groups (IGs), and
the task of the parser is to construct dependencies
between IGs, not (primarily) between word forms
(Eryig?it and Oflazer, 2006). It is then important
to remember that an unlabeled attachment score
of 75.8% corresponds to a word-to-word score of
82.7%, which puts Turkish on a par with languages
like Czech, Dutch and Spanish. Moreover, when
we break down the results according to whether the
head of a dependency is part of a multiple-IG word
or a complete (single-IG) word, we observe a highly
significant difference in accuracy, with only 53.2%
unlabeled attachment score for multiple-IG heads
versus 83.7% for single-IG heads. It is hard to say
at this stage whether this means that our methods
are ill-suited for IG-based parsing, or whether it is
mainly a case of sparse data for multiple-IG words.
When we break down the results by dependency
type, we can distinguish three main groups. The first
consists of determiners and particles, which have
an unlabeled attachment score over 80% and which
are found within a distance of 1?1.4 IGs from their
head.7 The second group mainly contains subjects,
objects and different kinds of adjuncts, with a score
in the range 60?80% and a distance of 1.8?5.2 IGs to
their head. In this group, information about case and
possessive features of nominals is important, which
is found in the FEATS field in the data representation.
We believe that one important explanation for our
relatively good results for Turkish is that we break
down the FEATS information into its atomic com-
ponents, independently of POS and CPOS tags, and
let the classifier decide which one to use in a given
situation. The third group contains distant depen-
dencies, such as sentence modifiers, vocatives and
appositions, which have a much lower accuracy.
5 Conclusion
The evaluation shows that labeled pseudo-projective
dependency parsing, using a deterministic parsing
algorithm and SVM classifiers, gives competitive
parsing accuracy for all languages involved in the
7Given that the average IG count of a word is 1.26 in the
treebank, this means that they are normally adjacent to the head
word.
224
shared task, although the level of accuracy varies
considerably between languages. To analyze in
depth the factors determining this variation, and to
improve our parsing methods accordingly to meet
the challenges posed by the linguistic diversity, will
be an important research goal for years to come.
Acknowledgments
We are grateful for the support from T ?UB?ITAK
(The Scientific and Technical Research Council of
Turkey) and the Swedish Research Council. We also
want to thank Atanas Chanev for assistance with
Slovene, the organizers of the shared task for all
their hard work, and the creators of the treebanks
for making the data available.
References
A. Abeille?, editor. 2003. Treebanks: Building and Using
Parsed Corpora, volume 20 of Text, Speech and Language
Technology. Kluwer Academic Publishers, Dordrecht.
S. Afonso, E. Bick, R. Haber, and D. Santos. 2002. ?Floresta
sinta?(c)tica?: a treebank for Portuguese. In Proc. of LREC-
2002, pages 1698?1703.
N. B. Atalay, K. Oflazer, and B. Say. 2003. The annotation
process in the Turkish treebank. In Proc. of LINC-2003.
E. Black, F. Jelinek, J. D. Lafferty, D. M. Magerman, R. L. Mer-
cer, and S. Roukos. 1992. Towards history-based grammars:
Using richer models for probabilistic parsing. In Proc. of the
5th DARPA Speech and Natural Language Workshop, pages
31?37.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003. The
PDT: a 3-level annotation scenario. In Abeille? (Abeille?,
2003), chapter 7.
S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith. 2002.
The TIGER treebank. In Proc. of TLT-2002.
C.-C. Chang and C.-J. Lin, 2001. LIBSVM: A Library
for Support Vector Machines. Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang, and
Z. Gao. 2003. Sinica treebank: Design criteria, representa-
tional issues and implementation. In Abeille? (Abeille?, 2003),
chapter 13, pages 231?248.
M. Civit Torruella and Ma A. Mart?? Anton??n. 2002. Design
principles for a Spanish treebank. In Proc. of TLT-2002.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
S. Dz?eroski, T. Erjavec, N. Ledinek, P. Pajas, Z. ?Zabokrtsky, and
A. ?Zele. 2006. Towards a Slovene dependency treebank. In
Proc. of LREC-2006.
G. Eryig?it and K. Oflazer. 2006. Statistical dependency parsing
of Turkish. In Proc. of EACL-2006.
J. Hajic?, O. Smrz?, P. Zema?nek, J. ?Snaidauf, and E. Bes?ka. 2004.
Prague Arabic dependency treebank: Development in data
and tools. In Proc. of NEMLAR-2004, pages 110?117.
Y. Kawata and J. Bartels. 2000. Stylebook for the Japanese
treebank in VERBMOBIL. Verbmobil-Report 240, Seminar
fu?r Sprachwissenschaft, Universita?t Tu?bingen.
M. T. Kromann. 2003. The Danish dependency treebank and
the underlying linguistic theory. In Proc. of TLT-2003.
T. Kudo and Y. Matsumoto. 2002. Japanese dependency anal-
ysis using cascaded chunking. In Proc. of CoNLL-2002,
pages 63?69.
J. Nilsson, J. Hall, and J. Nivre. 2005. MAMBA meets TIGER:
Reconstructing a Swedish treebank from antiquity. In Proc.
of the NODALIDA Special Session on Treebanks.
J. Nivre and J. Nilsson. 2005. Pseudo-projective dependency
parsing. In Proc. of ACL-2005, pages 99?106.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based depen-
dency parsing. In Proc. CoNLL-2004, pages 49?56.
J. Nivre, J. Hall, and J. Nilsson. 2006. MaltParser: A data-
driven parser-generator for dependency parsing. In Proc. of
LREC-2006.
J. Nivre. 2003. An efficient algorithm for projective depen-
dency parsing. In Proc. of IWPT-2003, pages 149?160.
J. Nivre. 2006. Inductive Dependency Parsing. Springer.
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r. 2003.
Building a Turkish treebank. In Abeille? (Abeille?, 2003),
chapter 15.
K. Simov and P. Osenova. 2003. Practical annotation scheme
for an HPSG treebank of Bulgarian. In Proc. of LINC-2003,
pages 17?24.
K. Simov, P. Osenova, A. Simov, and M. Kouylekov. 2005.
Design and implementation of the Bulgarian HPSG-based
treebank. In Journal of Research on Language and Com-
putation ? Special Issue, pages 495?522. Kluwer Academic
Publishers.
O. Smrz?, J. ?Snaidauf, and P. Zema?nek. 2002. Prague depen-
dency treebank for Arabic: Multi-level annotation of Arabic
corpus. In Proc. of the Intern. Symposium on Processing of
Arabic, pages 147?155.
L. van der Beek, G. Bouma, R. Malouf, and G. van Noord.
2002. The Alpino dependency treebank. In Computational
Linguistics in the Netherlands (CLIN).
H. Yamada and Y. Matsumoto. 2003. Statistical dependency
analysis with support vector machines. In Proc. of IWPT-
2003, pages 195?206.
225
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 641?648
Manchester, August 2008
Parsing the SYNTAGRUS Treebank of Russian
Joakim Nivre
V?axj?o University and
Uppsala University
joakim.nivre@vxu.se
Igor M. Boguslavsky
Universidad Polit?ecnica
de Madrid
Departamento de
Inteligencia Artificial
igor@opera.dia.fi.upm.es
Leonid L. Iomdin
Russian Academy
of Sciences
Institute for Information
Transmission Problems
iomdin@iitp.ru
Abstract
We present the first results on parsing the
SYNTAGRUS treebank of Russian with a
data-driven dependency parser, achieving
a labeled attachment score of over 82%
and an unlabeled attachment score of 89%.
A feature analysis shows that high parsing
accuracy is crucially dependent on the use
of both lexical and morphological features.
We conjecture that the latter result can be
generalized to richly inflected languages in
general, provided that sufficient amounts
of training data are available.
1 Introduction
Dependency-based syntactic parsing has become
increasingly popular in computational linguistics
in recent years. One of the reasons for the growing
interest is apparently the belief that dependency-
based representations should be more suitable for
languages that exhibit free or flexible word order
and where most of the clues to syntactic structure
are found in lexical and morphological features,
rather than in syntactic categories and word order
configurations. Some support for this view can be
found in the results from the CoNLL shared tasks
on dependency parsing in 2006 and 2007, where
a variety of data-driven methods for dependency
parsing have been applied with encouraging results
to languages of great typological diversity (Buch-
holz and Marsi, 2006; Nivre et al, 2007a).
However, there are still important differences in
parsing accuracy for different language types. For
? Joakim Nivre, Igor M. Boguslavsky, and Leonid
L. Iomdin, 2008. Licensed under the Creative Com-
mons Attribution-Noncommercial-Share Alike 3.0 Unported
license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
example, Nivre et al (2007a) observe that the lan-
guages included in the 2007 CoNLL shared task
can be divided into three distinct groups with re-
spect to top accuracy scores, with relatively low
accuracy for richly inflected languages like Arabic
and Basque, medium accuracy for agglutinating
languages like Hungarian and Turkish, and high
accuracy for more configurational languages like
English and Chinese. A complicating factor in this
kind of comparison is the fact that the syntactic an-
notation in treebanks varies across languages, in
such a way that it is very difficult to tease apart the
impact on parsing accuracy of linguistic structure,
on the one hand, and linguistic annotation, on the
other. It is also worth noting that the majority of
the data sets used in the CoNLL shared tasks are
not derived from treebanks with genuine depen-
dency annotation, but have been obtained through
conversion from other kinds of annotation. And
the data sets that do come with original depen-
dency annotation are generally fairly small, with
less than 100,000 words available for training, the
notable exception of course being the Prague De-
pendency Treebank of Czech (Haji?c et al, 2001),
which is one of the largest and most widely used
treebanks in the field.
This paper contributes to the growing litera-
ture on dependency parsing for typologically di-
verse languages by presenting the first results on
parsing the Russian treebank SYNTAGRUS (Bo-
guslavsky et al, 2000; Boguslavsky et al, 2002).
There are several factors that make this treebank
an interesting resource in this context. First of
all, it contains a genuine dependency annotation,
theoretically grounded in the long tradition of de-
pendency grammar for Slavic languages, repre-
sented by the work of Tesni`ere (1959) andMel??cuk
(1988), among others. Secondly, with close to
641
500,000 tokens, the treebank is larger than most
other available dependency treebanks and provides
a good basis for experimental investigations us-
ing data-driven methods. Thirdly, the Russian lan-
guage, which has not been included in previous ex-
perimental evaluations such as the CoNLL shared
tasks, is a richly inflected language with free word
order and thus representative of the class of lan-
guages that tend to pose problems for the currently
available parsing models. Taken together, these
factors imply that experiments using the SYNTA-
GRUS treebank may be able to shed further light
on the complex interplay between language type,
annotation scheme, and training set size, as deter-
minants of parsing accuracy for data-driven depen-
dency parsers.
The experimental parsing results presented in
this paper have been obtained using MaltParser,
a freely available system for data-driven depen-
dency parsing with state-of-the-art accuracy for
most languages in previous evaluations (Buchholz
and Marsi, 2006; Nivre et al, 2007a; Nivre et al,
2007b). Besides establishing a first benchmark for
the SYNTAGRUS treebank, we analyze the influ-
ence of different kinds of features on parsing ac-
curacy, showing conclusively that both lexical and
morphological features are crucial for obtaining
good parsing accuracy. All results are based on in-
put with gold standard annotations, which means
that the results can be seen to establish an upper
bound on what can be achieved when parsing raw
text. However, this also means that results are
comparable to those from the CoNLL shared tasks,
which have been obtained under the same condi-
tions.
The rest of the paper is structured as follows.
Section 2 introduces the SYNTAGRUS treebank,
section 3 describes the MaltParser system used in
the experiments, and section 4 presents experimen-
tal results and analysis. Section 5 contains conclu-
sions and future work.
2 The SYNTAGRUS Treebank
The Russian dependency treebank, SYNTAGRUS,
is being developed by the Computational Linguis-
tics Laboratory, Institute of Information Trans-
mission Problems, Russian Academy of Sciences.
Currently the treebank contains over 32,000 sen-
tences (roughly 460,000 words) belonging to texts
from a variety of genres (contemporary fiction,
popular science, newspaper and journal articles
dated between 1960 and 2008, texts of online
news, etc.) and it is growing steadily. It is an inte-
gral but fully autonomous part of the Russian Na-
tional Corpus developed in a nationwide research
project and can be freely consulted on the Web
(http://www.ruscorpora.ru/).
Since Russian is a language with relatively free
word order, SYNTAGRUS adopted a dependency-
based annotation scheme, in a way parallel to the
Prague Dependency Treebank (Haji?c et al, 2001).
The treebank is so far the only corpus of Russian
supplied with comprehensive morphological anno-
tation and syntactic annotation in the form of a
complete dependency tree provided for every sen-
tence.
Figure 1 shows the dependency tree for the
sentence Naibol~xee vozmuwenie uqastnikov
mitinga vyzval prodolawi$is rost cen
na benzin, ustanavlivaemyh neftnymi kom-
panimi (It was the continuing growth of petrol
prices set by oil companies that caused the greatest
indignation of the participants of the meeting). In
the dependency tree, nodes represent words (lem-
mas), annotated with parts of speech and morpho-
logical features, while arcs are labeled with syntac-
tic dependency types. There are over 65 distinct
dependency labels in the treebank, half of which
are taken from Mel??cuk?s Meaning?Text Theory
(Mel??cuk, 1988). Dependency types that are used
in figure 1 include:
1. predik (predicative), which, prototypically,
represents the relation between the verbal
predicate as head and its subject as depen-
dent;
2. 1-kompl (first complement), which denotes
the relation between a predicate word as head
and its direct complement as dependent;
3. agent (agentive), which introduces the rela-
tion between a predicate word (verbal noun
or verb in the passive voice) as head and its
agent in the instrumental case as dependent;
4. kvaziagent (quasi-agentive), which relates
any predicate noun as head with its first syn-
tactic actant as dependent, if the latter is
not eligible for being qualified as the noun?s
agent;
5. opred (modifier), which connects a noun
head with an adjective/participle dependent if
the latter serves as an adjectival modifier to
the noun;
642
Figure 1: A syntactically annotated sentence from the SYNTAGRUS treebank.
6. predl (prepositional), which accounts for the
relation between a preposition as head and a
noun as dependent.
Dependency trees in SYNTAGRUS may contain
non-projective dependencies. Normally, one token
corresponds to one node in the dependency tree.
There are however a noticeable number of excep-
tions, the most important of which are the follow-
ing:
1. compound words like ptidestitany$i
(fifty-storied), where one token corresponds
to two or more nodes;
2. so-called phantom nodes for the representa-
tion of hard cases of ellipsis, which do not
correspond to any particular token in the sen-
tence; for example,  kupil rubaxku, a on
galstuk (I bought a shirt and he a tie), which
is expanded into  kupil rubaxku, a on
kupil
PHANTOM
galstuk (I bought a shirt and
he bought
PHANTOM
a tie);
3. multiword expressions like po kra$ine$i mere
(at least), where several tokens correspond to
one node.
Syntactic annotation is performed semi-
automatically: sentences are first processed
by the rule-based Russian parser of an advanced
NLP system, ETAP-3 (Apresian et al, 2003) and
then edited manually by linguists who handle
errors of the parser as well as cases of ambiguity
that cannot be reliably resolved without extra-
linguistic knowledge. The parser processes raw
sentences without prior part-of-speech tagging.
Morphological annotation in SYNTAGRUS is
based on a comprehensive morphological dictio-
nary of Russian that counts about 130,000 entries
(over 4 million word forms). The ETAP-3 mor-
phological analyzer uses the dictionary to produce
morphological annotation of words belonging to
the corpus, including lemma, part-of-speech tag
and additional morphological features dependent
on the part of speech: animacy, gender, number,
case, degree of comparison, short form (of adjec-
tives and participles), representation (of verbs), as-
pect, tense, mood, person, voice, composite form,
and attenuation.
Statistics for the version of SYNTAGRUS used
for the experiments described in this paper are as
follows:
? 32,242 sentences, belonging to the fiction
genre (9.8%), texts of online news (12.4%),
newspaper and journal articles (77.8%);
? 461,297 tokens, including expressions with
non-alphabetical symbols (e.g., 10, 1.200,
$333, +70C, #) but excluding punctuation;
? 31,683 distinct word types, of which 635 with
a frequency greater than 100, 5041 greater
than 10, and 18231 greater than 1;
? 3,414 sentences (10.3%) with non-projective
643
POS DEP MOR LEM LEX
TOP + + + + +
TOP?1 +
HEAD(TOP) + +
LDEP(TOP) +
RDEP(TOP) +
NEXT + + + +
NEXT+1 + + + +
NEXT+2 +
NEXT+3 +
LDEP(NEXT) +
Table 1: History-based features (TOP = token on
top of stack; NEXT = next token in input buffer;
HEAD(w) = head of w; LDEP(w) = leftmost depen-
dent of w; RDEP(w) = leftmost dependent of w).
dependencies and 3,934 non-projective de-
pendency arcs in total;
? 478 sentences (1.5%) containing phantom
nodes and 631 phantom nodes in total.
3 MaltParser
MaltParser (Nivre et al, 2007b) is a language-
independent system for data-driven dependency
parsing, based on a transition-based parsing model
(McDonald and Nivre, 2007). More precisely, the
approach is based on four essential components:
? A transition-based deterministic algorithm
for building labeled projective dependency
graphs in linear time (Nivre, 2003).
? History-based feature models for predicting
the next parser action (Black et al, 1992;
Magerman, 1995; Ratnaparkhi, 1997).
? Discriminative classifiers for mapping histo-
ries to parser actions (Kudo and Matsumoto,
2002; Yamada and Matsumoto, 2003).
? Pseudo-projective parsing for recovering non-
projective structures (Nivre and Nilsson,
2005).
In the following subsections, we briefly describe
each of these four components in turn.
3.1 Parsing Algorithm
The parser uses the deterministic algorithm for la-
beled dependency parsing first proposed by Nivre
(2003). The algorithm builds a labeled dependency
graph in one left-to-right pass over the input, us-
ing a stack to store partially processed tokens and
adding arcs using four elementary actions (where
TOP is the token on top of the stack and NEXT is
the next token):
? Shift: Push NEXT onto the stack.
? Reduce: Pop the stack.
? Right-Arc(r): Add an arc labeled r from TOP
to NEXT; push NEXT onto the stack.
? Left-Arc(r): Add an arc labeled r from NEXT
to TOP; pop the stack.
Parser actions are predicted using a history-based
feature model (section 3.2) and SVM classifiers
(section 3.3). Although the parser only derives
projective graphs, the fact that these graphs are
labeled allows non-projective dependencies to be
captured using the pseudo-projective approach of
Nivre and Nilsson (2005) (section 3.4).
3.2 History-Based Feature Models
History-based parsing models rely on features of
the derivation history to predict the next parser ac-
tion (Black et al, 1992). The features used are
all symbolic and defined in terms of five different
node attributes:
? POS = part of speech (atomic)
? DEP = dependency type
? MOR = morphological features (set)
? LEM = lemma
? LEX = word form
Features of the type DEP have a special status in
that they are extracted during parsing from the par-
tially built dependency graph and are updated dy-
namically during parsing. The other four feature
types (LEX, LEM, POS, and MOR) are given as part
of the input to the parser and remain static during
the processing of a sentence. Of these four fea-
ture types, all except LEX presupposes that the in-
put has been preprocessed by a lemmatizer, tagger
and morphological analyzer, respectively, but for
the experiments reported below we use gold stan-
dard annotation from the treebank.
In order to study the influence of different fea-
tures, we have experimented with different combi-
nations of the five feature types, where the base-
line model contains only POS and DEP features,
while more complex models add MOR, LEM, and
LEX features in different combinations. The exact
644
features included for each feature type are shown
in table 1, where rows denote tokens in a parser
configuration (defined relative to the stack, the re-
maining input, and the partially built dependency
graph), and where columns correspond to feature
types. The selection of features in each group was
tuned on a development set as described in sec-
tion 4.
3.3 Discriminative Classifiers
We use support vector machines (Vapnik, 1995) to
predict the next parser action from a feature vector
representing the history. More specifically, we use
LIBSVM (Chang and Lin, 2001) with a quadratic
kernel K(x
i
, x
j
) = (?x
T
i
x
j
+ r)
2
and the built-
in one-versus-all strategy for multi-class classifica-
tion. Symbolic features are converted to numerical
features using the standard technique of binariza-
tion, and we split the set values of MOR features
into their atomic components. In order to speed
up training, we also divide the training data into
smaller bins according to the feature POS of NEXT,
and train separate classifiers on each bin.
3.4 Pseudo-Projective Parsing
Pseudo-projective parsing was proposed by Nivre
and Nilsson (2005) as a way of dealing with non-
projective structures in a projective data-driven
parser. We projectivize training data by a minimal
transformation, lifting non-projective arcs one step
at a time, and extending the arc label of lifted arcs
using the encoding scheme called HEAD by Nivre
and Nilsson (2005), which means that a lifted arc
is assigned the label r?h, where r is the original
label and h is the label of the original head in the
non-projective dependency graph.
Non-projective dependencies can be recovered
by an inverse transformation applied to the depen-
dency graph output by the parser, using a left-to-
right, top-down, breadth-first search, guided by the
extended arc labels r?h assigned by the parser.
4 Experiments
In this section we describe the first experiments on
parsing the SYNTAGRUS treebank using a data-
driven parser. The experimental setup is described
in section 4.1, while the experimental results are
presented and discussed in section 4.2.
4.1 Experimental Setup
All experiments have been performed on the ver-
sion of SYNTAGRUS described in section 2, con-
Model Count LAS UAS
Base = POS + DEP 46506 60.2 76.0
B1 = Base + MOR 46506 73.0 84.5
B2 = Base + LEM 46506 75.5 84.6
B3 = Base + LEX 46506 74.5 84.6
BM1 = B1 + LEM 46506 82.3 89.0
BM2 = B1 + LEX 46506 81.0 88.8
All = B1 + LEM + LEX 46506 82.3 89.1
Table 2: Parsing accuracy for different feature
models on the final test set (Count = Number of
tokens in the test set, LAS = Labeled attachment
score, UAS = Unlabeled attachment score).
verted to the CoNLL data format (Buchholz and
Marsi, 2006).
1
The available data were divided
into 80% for training, 10% for development, and
10% for final testing, using a pseudo-randomized
split. The development set was used for tuning
parameters of the parsing algorithm and pseudo-
projective parsing technique, and for feature selec-
tion within the feature groups not included in the
baseline model (i.e., MOR, LEM, and LEX). The
test set was used for evaluating the finally selected
models once.
The evaluation metrics used are labeled attach-
ment score (LAS) ? the percentage of tokens that
are assigned the correct head and dependency type
? and unlabeled attachment score (UAS) ? the per-
centage of tokens that are assigned the correct head
(regardless of dependency type). In addition, we
present precision and recall for non-projective de-
pendencies. Punctuation tokens are excluded in all
scores, but phantom tokens are included. We use
McNemar?s test for statistical significance.
4.2 Results and Discussion
Table 2 gives the parsing accuracy for different fea-
ture models on the held-out test set, measured as
labeled attachment score (LAS) and unlabeled at-
tachment score (UAS). With respect to LAS, there
are statistically significant differences between all
models except BM1 and All (p < 0.01). With re-
spect to UAS, there are statistically significant dif-
ferences between four groups, such that {Base} <
{B1, B2, B3} < {BM2} < {BM1, All}, but there
1
Since SYNTAGRUS only distinguishes ten different parts
of speech (not counting morphological features), the fields
CPOSTAG and POSTAG in the CoNLL format ? for coarse-
grained and fine-grained parts of speech ? were given the
same content.
645
are no differences within these groups.
2
Looking at the results for different models, we
see that while the baseline model (Base) achieves
a modest 60.2% LAS and 76.0% UAS, the addi-
tion of only one additional feature group (B1?B3)
boosts unlabeled accuracy by close to ten percent-
age points and labeled accuracy by up to fifteen
percentage points. Somewhat surprisingly, the dif-
ferences between models B1?B3 are very small,
and only differences with respect to LAS are statis-
tically significant, which may be taken to suggest
that morphological and lexical features capture the
same type of information. However, this hypothe-
sis is clearly refuted by the results for models BM1
and BM2, where the addition of lexical features on
top of morphological features gives a further gain
in LAS of eight to ten percentage points (and over
four percentage points for UAS).
Comparing the use of raw word forms (LEX) and
lemmas (LEM) as lexical features, we see a slight
advantage for the latter, at least for labeled accu-
racy. However, it must be remembered that the ex-
periments are based on gold standard input anno-
tation, which probably leads to an overestimation
of the value of LEM features. Finally, it is worth
noting that including both LEX and LEM features
does not result in a significant improvement over
the model with only LEM features, which may be a
sign of saturation, although this may again change
in the presence of noisy LEM features.
The experimental results show conclusively that
both morphological and lexical features are crucial
for achieving high parsing accuracy. It may seem
that they are most important for labeled accuracy,
where the gain in absolute percentage points is the
greatest with respect to the baseline, but it must
be remembered that the unlabeled scores start at a
higher level, thus leaving less room for improve-
ment. In fact, the total error reduction from Base
to All is over 50% for both LAS and UAS.
Table 3 gives a more detailed picture of parsing
performance for the best model (All), by breaking
down both LAS and UAS by the part-of-speech tag
of the dependent. We note that accuracy is higher
than average for nouns (S), adjectives (A), parti-
cles (PART), and reasonably good for verbs (V).
For prepositions (PR), conjunctions (CONJ), and
adverbs (ADV), accuracy is considerably lower,
which may be attributed to attachment ambigui-
2
For the difference BM2 < BM1, 0.01 < p < 0.05; for
all other differences, p < 0.01.
Part of Speech Count LAS UAS
S (noun) 7303 86.7 93.3
A (adjective) 7024 92.8 94.2
V (verb) 6946 81.9 85.8
PR (preposition) 5302 60.0 79.0
CONJ (conjunction) 2998 76.1 80.7
ADV (adverb) 2855 72.3 83.3
PART (particle) 1833 88.1 89.6
NUM (numeral) 807 88.7 93.6
NID (foreign word) 142 76.5 91.5
COM (compound) 32 93.8 96.9
P (proposition word) 7 57.1 85.7
INTJ (interjection) 5 0.0 20.0
Table 3: Accuracy by part of speech on the final
test set for All features (Count = Number of tokens
in the test set, LAS = Labeled attachment score,
UAS = Unlabeled attachment score).
ties. It is also worth noting that both prepositions
and adverbs have considerably higher UAS than
LAS (almost twenty percentage points for prepo-
sitions), which shows that even when they are at-
tached correctly they are are often mislabeled. The
remaining parts of speech are too infrequent to
warrant any conclusions.
Looking specifically at non-projective depen-
dencies, we find that the best model has a la-
beled precision of 68.8 and a labeled recall of 31.4.
The corresponding unlabeled figures are 73.3 and
33.4.
3
This confirms the results of previous studies
showing that the pseudo-projective parsing tech-
nique used by MaltParser tends to give high pre-
cision ? given that non-projective dependencies
are among the most difficult to parse correctly ?
but rather low recall (McDonald and Nivre, 2007).
It is also worth mentioning that phantom tokens,
i.e., empty tokens inserted for the analysis of cer-
tain elliptical constructions (see section 2), have
a labeled precision of 82.4 and a labeled recall
of 82.8 (89.2 and 89.6 unlabeled), which is very
close to the average accuracy, despite being very
infrequent. However, it must be remembered that
these tokens were given as part of the input in
these experiments. In order to correctly analyse
these tokens and their dependencies when pars-
ing raw text, they would have to be recovered in
a pre-processing phase along the lines of Dienes
3
The precision is the percentage of non-projective depen-
dencies predicted by the parser that were correct, while the
recall is the percentage of true non-projective dependencies
that were correctly predicted by the parser.
646
and Dubey (2003).
Summing up, the main result of the experimen-
tal evaluation is that both morphological and lex-
ical features are crucial for attaining high accu-
racy when training and evaluating on the repre-
sentations found in the SYNTAGRUS treebank of
Russian. With regard to morphological features
this is in line with a number of recent studies
showing the importance of morphology for pars-
ing languages with less rigid word order, includ-
ing work on Spanish (Cowan and Collins, 2005),
Hebrew (Tsarfaty, 2006; Tsarfaty and Sima?an,
2007), Turkish (Eryigit et al, 2006), and Swedish
(?vrelid and Nivre, 2007).
With regard to lexical features, the situation is
more complex in that there are a number of stud-
ies questioning the usefulness of lexical features
in statistical parsing and arguing that equivalent
or better results can be achieved with unlexical-
ized models provided that linguistic categories can
be split flexibly into more fine-grained categories,
either using hand-crafted splits, as in the seminal
work of Klein and Manning (2003), or using hid-
den variables and unsupervised learning, as in the
more recent work by Petrov et al (2006), among
others. There are even studies showing that lexi-
calization can be harmful when parsing richly in-
flected languages like German (Dubey and Keller,
2003) and Turkish (Eryi?git and Oflazer, 2006).
However, it is worth noting that most of these
results have been obtained either for models of
constituency-based parsing or for models of de-
pendency parsing suffering from sparse data.
4
In
the experiments presented here, we have used
a transition-based model for dependency parsing
that has much fewer parameters than state-of-the-
art probabilistic models for constituency parsing.
Moreover, we have been able to use a relatively
large training set, thereby minimizing the effect of
sparseness for lexical features. We therefore con-
jecture that the beneficial effect of lexical features
on parsing accuracy will generalize to other richly
inflected languages when similar conditions hold.
As far as we know, these are the first results for
a large-scale data-driven parser for Russian. There
do exist several rule-based parsers for Russian,
such as the ETAP-3 parser (Apresian et al, 2003)
and a Link Grammar parser,
5
as well as a prototype
of a hybrid system based on the ETAP-3 parser en-
4
The latter case applies to the probabilistic model of de-
pendency parsing explored by Eryi?git and Oflazer (2006).
5
http://sz.ru/parser/
riched with statistics extracted from SYNTAGRUS
(Boguslavsky et al, 2003; Chardin, 2004), but dif-
ferences in both input format and output repre-
sentations make it difficult to compare the perfor-
mance directly.
5 Conclusion
We have presented the first results on parsing the
SYNTAGRUS treebank of Russian using a data-
driven dependency parser. Besides establishing
a first benchmark for the SYNTAGRUS treebank,
we have analyzed the influence of different kinds
of features on parsing accuracy, showing conclu-
sively that both lexical and morphological features
are crucial for obtaining good parsing accuracy.
We hypothesize that this result can be generalized
to other richly inflected languages, provided that
sufficient amounts of data are available.
Future work includes a deeper analysis of the in-
fluence of individual features, both morphological
and lexical, as well as an evaluation of the parser
under more realistic conditions without gold stan-
dard annotation in the input. This will require not
only automatic morphological analysis and disam-
biguation but also a mechanism for inserting so-
called phantom tokens in elliptical constructions.
Acknowledgments
We want to thank Ivan Chardin for initiating this
collaboration and Jens Nilsson for converting the
SYNTAGRUS data to the CoNLL format. We are
grateful to the Russian Foundation of Basic Re-
search for partial support of this research (grant no.
07-06-00339).
References
Apresian, Ju., I. Boguslavsky, L. Iomdin, A. Lazursky,
V. Sannikov, V. Sizov, and L. Tsinman. 2003.
ETAP-3 linguistic processor: A full-fledged NLP
implementation of the MTT. In Proceedings of
the First International Conference on Meaning-Text
Theory, 279?288.
Black, E., F. Jelinek, J. D. Lafferty, D. M. Mager-
man, R. L. Mercer, and S. Roukos. 1992. To-
wards history-based grammars: Using richer models
for probabilistic parsing. In Proceedings of the 5th
DARPA Speech and Natural Language Workshop,
31?37.
Boguslavsky, I., S. Grigorieva, N. Grigoriev, L. Krei-
dlin, and N. Frid. 2000. Dependency treebank for
Russian: Concept, tools, types of information. In
Proceedings of COLING, 987?991.
647
Boguslavsky, I., I. Chardin, S. Grigorieva, N. Grigoriev,
L. Iomdin, L. Kreidlin, and N. Frid. 2002. Devel-
opment of a dependency treebank for Russian and
its possible applications in NLP. In Proceedings of
LREC, page 852856.
Boguslavsky, I. M., L. L. Iomdin, V. S. Sizov, and
I. Chardin. 2003. Parsing with a treebank. In Pro-
ceedings of the Conference on Cognitive Modeling
in Linguistics [In Russian].
Buchholz, S. and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Pro-
ceedings of CoNLL, 149?164.
Chang, C.-C. and C.-J. Lin, 2001. LIBSVM: A Library
for Support Vector Machines. Software available at
http://www.csie.ntu.edu.tw/?cjlin/libsvm.
Chardin, Ivan. 2004. Dependency Treebanks and Their
Use in Parsing. Ph.D. thesis, Russian Academy of
Science [In Russian].
Cowan, B. and M. Collins. 2005. Morphology and
reranking for the statistical parsing of spanish. In
Proceedings of HLT/EMNLP, 795?802.
Dienes, P. and A. Dubey. 2003. Deep syntactic pro-
cessing by combining shallow methods. In Proceed-
ings of ACL, 431?438.
Dubey, A. and F. Keller. 2003. Probabilistic parsing
for German using sister-head dependencies. In Pro-
ceedings of ACL, 96?103.
Eryi?git, G. and K. Oflazer. 2006. Statistical depen-
dency parsing of Turkish. In Proceedings of EACL,
89?96.
Eryigit, G., J. Nivre, and K. Oflazer. 2006. The in-
cremental use of morphological information and lex-
icalization in data-driven dependency parsing. In
Proceedings of the 21st International Conference
on the Computer Processing of Oriental Languages,
498?507.
Haji?c, J., B. Vidova Hladka, J. Panevov?a, E. Haji?cov?a,
P. Sgall, and P. Pajas. 2001. Prague Dependency
Treebank 1.0. LDC, 2001T10.
Klein, D. and C. D. Manning. 2003. Accurate unlexi-
calized parsing. In Proceedings of ACL, 423?430.
Kudo, T. and Y. Matsumoto. 2002. Japanese depen-
dency analysis using cascaded chunking. In Pro-
ceedings of CoNLL, 63?69.
Magerman, D. M. 1995. Statistical decision-tree mod-
els for parsing. In Proceedings of ACL, 276?283.
McDonald, R. and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proceedings of EMNLP-CoNLL, 122?131.
Mel??cuk, I. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
Nivre, J. and J. Nilsson. 2005. Pseudo-projective de-
pendency parsing. In Proceedings of ACL, 99?106.
Nivre, J., J. Hall, S. K?ubler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007a. The CoNLL 2007
shared task on dependency parsing. In Proceedings
of the CoNLL Shared Task of EMNLP-CoNLL 2007,
915?932.
Nivre, J., J. Hall, J. Nilsson, A. Chanev, G. Eryi?git, S.
K?ubler, S. Marinov, and E. Marsi. 2007b. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language En-
gineering, 13:95?135.
Nivre, J. 2003. An efficient algorithm for projective
dependency parsing. In Proceedings of IWPT, 149?
160.
?vrelid, L. and J. Nivre. 2007. When word order and
part-of-speech tags are not enough ? swedish depen-
dency parsing with rich linguistic features. In Pro-
ceedings of RANLP, 447?451.
Petrov, S., L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree
annotation. In Proceedings of COLING/ACL, 433?
440.
Ratnaparkhi, A. 1997. A linear observed time statis-
tical parser based on maximum entropy models. In
Proceedings of EMNLP, 1?10.
Tesni`ere, L. 1959.
?
El?ements de syntaxe structurale.
Editions Klincksieck.
Tsarfaty, R. and K. Sima?an. 2007. Three-dimensional
parametrization for parsing morphologically rich
languages. In Proceedings of IWPT, 156?167.
Tsarfaty, R. 2006. Integrated morphological and
syntactic disambiguation for modern hebrew. In
Proceedings of the COLING/ACL 2006 Student Re-
search Workshop, 49?54.
Vapnik, V. N. 1995. The Nature of Statistical Learning
Theory. Springer.
Yamada, H. and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
Proceedings of IWPT, 195?206.
648
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 122?131, Prague, June 2007. c?2007 Association for Computational Linguistics
Characterizing the Errors of Data-Driven Dependency Parsing Models
Ryan McDonald
Google Inc.
76 Ninth Avenue
New York, NY 10011
ryanmcd@google.com
Joakim Nivre
Va?xjo? University Uppsala University
35195 Va?xjo? 75126 Uppsala
Sweden Sweden
nivre@msi.vxu.se
Abstract
We present a comparative error analysis
of the two dominant approaches in data-
driven dependency parsing: global, exhaus-
tive, graph-based models, and local, greedy,
transition-based models. We show that, in
spite of similar performance overall, the two
models produce different types of errors, in
a way that can be explained by theoretical
properties of the two models. This analysis
leads to new directions for parser develop-
ment.
1 Introduction
Syntactic dependency representations have a long
history in descriptive and theoretical linguistics and
many formal models have been advanced (Hudson,
1984; Mel?c?uk, 1988; Sgall et al, 1986; Maruyama,
1990). A dependency graph of a sentence repre-
sents each word and its syntactic modifiers through
labeled directed arcs, as shown in Figure 1, taken
from the Prague Dependency Treebank (Bo?hmova? et
al., 2003). A primary advantage of dependency rep-
resentations is that they have a natural mechanism
for representing discontinuous constructions, aris-
ing from long distance dependencies or free word
order, through non-projective dependency arcs, ex-
emplified by the arc from jedna to Z in Figure 1.
Syntactic dependency graphs have recently
gained a wide interest in the computational lin-
guistics community and have been successfully em-
ployed for many problems ranging from machine
translation (Ding and Palmer, 2004) to ontology
Figure 1: Example dependency graph.
construction (Snow et al, 2004). In this work we
focus on a common parsing paradigm called data-
driven dependency parsing. Unlike grammar-based
parsing, data-driven approaches learn to produce de-
pendency graphs for sentences solely from an anno-
tated corpus. The advantage of such models is that
they are easily ported to any domain or language in
which annotated resources exist.
As evident from the CoNLL-X shared task on de-
pendency parsing (Buchholz and Marsi, 2006), there
are currently two dominant models for data-driven
dependency parsing. The first is what Buchholz and
Marsi (2006) call the ?all-pairs? approach, where ev-
ery possible arc is considered in the construction of
the optimal parse. The second is the ?stepwise? ap-
proach, where the optimal parse is built stepwise and
where the subset of possible arcs considered depend
on previous decisions. Theoretically, these models
are extremely different. The all-pairs models are
globally trained, use exact (or near exact) inference
algorithms, and define features over a limited history
of parsing decisions. The stepwise models use local
training and greedy inference algorithms, but define
features over a rich history of parse decisions. How-
ever, both models obtain similar parsing accuracies
122
McDonald Nivre
Arabic 66.91 66.71
Bulgarian 87.57 87.41
Chinese 85.90 86.92
Czech 80.18 78.42
Danish 84.79 84.77
Dutch 79.19 78.59
German 87.34 85.82
Japanese 90.71 91.65
Portuguese 86.82 87.60
Slovene 73.44 70.30
Spanish 82.25 81.29
Swedish 82.55 84.58
Turkish 63.19 65.68
Overall 80.83 80.75
Table 1: Labeled parsing accuracy for top scoring
systems at CoNLL-X (Buchholz and Marsi, 2006).
on a variety of languages, as seen in Table 1, which
shows results for the two top performing systems in
the CoNLL-X shared task, McDonald et al (2006)
(?all-pairs?) and Nivre et al (2006) (?stepwise?).
Despite the similar performance in terms of over-
all accuracy, there are indications that the two types
of models exhibit different behaviour. For example,
Sagae and Lavie (2006) displayed that combining
the predictions of both parsing models can lead to
significantly improved accuracies. In order to pave
the way for new and better methods, a much more
detailed error analysis is needed to understand the
strengths and weaknesses of different approaches.
In this work we set out to do just that, focusing on
the two top performing systems from the CoNLL-X
shared task as representatives of the two dominant
models in data-driven dependency parsing.
2 Two Models for Dependency Parsing
2.1 Preliminaries
Let L = {l1, . . . , l|L|} be a set of permissible arc
labels. Let x = w0, w1, . . . , wn be an input sen-
tence wherew0=root. Formally, a dependency graph
for an input sentence x is a labeled directed graph
G = (V,A) consisting of a set of nodes V and a
set of labeled directed arcs A ? V ? V ? L, i.e., if
(i, j, l) ? A for i, j ? V and l ? L, then there is an
arc from node i to node j with label l in the graph.
A dependency graph G for sentence x must satisfy
the following properties:
1. V = {0, 1, . . . , n}
2. If (i, j, l) ? A, then j 6= 0.
3. If (i, j, l) ? A, then for all i? ? V ? {i} and
l? ? L, (i?, j, l?) /? A.
4. For all j ? V ?{0}, there is a (possibly empty)
sequence of nodes i1, . . . , im?V and labels
l1, . . . , lm, l?L such that (0, i1, l1),(i1, i2, l2),
. . . , (im, j, l)?A.
The constraints state that the dependency graph
spans the entire input (1); that the node 0 is a root
(2); that each node has at most one incoming arc
in the graph (3); and that the graph is connected
through directed paths from the node 0 to every other
node in the graph (4). A dependency graph satisfy-
ing these constraints is a directed tree originating out
of the root node 0. We say that an arc (i, j, l) is non-
projective if not all words k occurring between i and
j in the linear order are dominated by i (where dom-
inance is the transitive closure of the arc relation).
2.2 Global, Exhaustive, Graph-Based Parsing
For an input sentence, x = w0, w1, . . . , wn consider
the dense graph Gx = (Vx, Ax) where:
1. Vx = {0, 1, . . . , n}
2. Ax = {(i, j, l) | ? i, j ? Vx and l ? L}
Let D(Gx) represent the subgraphs of graph Gx
that are valid dependency graphs for the sentence
x. Since Gx contains all possible labeled arcs, the
set D(Gx) must necessarily contain all valid depen-
dency graphs for x.
Assume that there exists a dependency arc scoring
function, s : V ? V ? L ? R. Furthermore, define
the score of a graph as the sum of its arc scores,
s(G = (V,A)) =
?
(i,j,l)?A
s(i, j, l)
The score of a dependency arc, s(i, j, l) represents
the likelihood of creating a dependency from word
wi to word wj with the label l. If the arc score func-
tion is known a priori, then the parsing problem can
be stated as,
123
G = argmax
G?D(Gx)
s(G) = argmax
G?D(Gx)
?
(i,j,l)?A
s(i, j, l)
This problem is equivalent to finding the highest
scoring directed spanning tree in the graph Gx origi-
nating out of the root node 0, which can be solved for
both the labeled and unlabeled case in O(n2) time
(McDonald et al, 2005b). In this approach, non-
projective arcs are produced naturally through the
inference algorithm that searches over all possible
directed trees, whether projective or not.
The parsing models of McDonald work primarily
in this framework. To learn arc scores, these mod-
els use large-margin structured learning algorithms
(McDonald et al, 2005a), which optimize the pa-
rameters of the model to maximize the score mar-
gin between the correct dependency graph and all
incorrect dependency graphs for every sentence in a
training set. The learning procedure is global since
model parameters are set relative to the classification
of the entire dependency graph, and not just over sin-
gle arc attachment decisions. The primary disadvan-
tage of these models is that the feature representa-
tion is restricted to a limited number of graph arcs.
This restriction is required so that both inference and
learning are tractable.
The specific model studied in this work is that
presented by McDonald et al (2006), which factors
scores over pairs of arcs (instead of just single arcs)
and uses near exhaustive search for unlabeled pars-
ing coupled with a separate classifier to label each
arc. We call this system MSTParser, which is also
the name of the freely available implementation.1
2.3 Local, Greedy, Transition-Based Parsing
A transition system for dependency parsing defines
1. a set C of parser configurations, each of which
defines a (partially built) dependency graph G
2. a set T of transitions, each a function t :C?C
3. for every sentence x = w0, w1, . . . , wn,
(a) a unique initial configuration cx
(b) a set Cx of terminal configurations
1http://mstparser.sourceforge.net
A transition sequence Cx,m = (cx, c1, . . . , cm) for a
sentence x is a sequence of configurations such that
cm ? Cx and, for every ci (ci 6= cx), there is a tran-
sition t ? T such that ci = t(ci?1). The dependency
graph assigned to x byCx,m is the graphGm defined
by the terminal configuration cm.
Assume that there exists a transition scoring func-
tion, s : C ? T ? R. The score of a transition
t in a configuration c, s(c, t), represents the likeli-
hood of taking transition t out of configuration c.
The parsing problem consists in finding a terminal
configuration cm ? Cx, starting from the initial
configuration cx and taking the optimal transition
t? = argmaxt?T s(c, t) out of every configuration
c. This can be seen as a greedy search for the optimal
dependency graph, based on a sequence of locally
optimal decisions in terms of the transition system.
Many transition systems for data-driven depen-
dency parsing are inspired by shift-reduce parsing,
where configurations contain a stack for storing par-
tially processed nodes. Transitions in such systems
add arcs to the dependency graph and/or manipu-
late the stack. One example is the transition system
defined by Nivre (2003), which parses a sentence
x = w0, w1, . . . , wn in O(n) time, producing a pro-
jective dependency graph satisfying conditions 1?4
in section 2.1, possibly after adding arcs (0, i, lr)
for every node i 6= 0 that is a root in the output
graph (where lr is a special label for root modifiers).
Nivre and Nilsson (2005) showed how the restric-
tion to projective dependency graphs could be lifted
by using graph transformation techniques to pre-
process training data and post-process parser output,
so-called pseudo-projective parsing.
To learn transition scores, these systems use dis-
criminative learning methods, e.g., memory-based
learning or support vector machines. The learning
procedure is local since only single transitions are
scored, not entire transition sequences. The primary
advantage of these models is that features are not re-
stricted to a limited number of graph arcs but can
take into account the entire dependency graph built
so far. The main disadvantage is that the greedy
parsing strategy may lead to error propagation.
The specific model studied in this work is that pre-
sented by Nivre et al (2006), which uses labeled
pseudo-projective parsing with support vector ma-
chines. We call this systemMaltParser, which is also
124
the name of the freely available implementation.2
2.4 Comparison
These models differ primarily with respect to three
important properties.
1. Inference: MaltParser uses a transition-based
inference algorithm that greedily chooses the
best parsing decision based on a trained clas-
sifier and current parser history. MSTParser
instead uses near exhaustive search over a
dense graphical representation of the sentence
to find the dependency graph that maximizes
the score.
2. Training: MaltParser trains a model to make
a single classification decision (choose the next
transition). MSTParser trains a model to maxi-
mize the global score of correct graphs.
3. Feature Representation: MaltParser can in-
troduce a rich feature history based on previ-
ous parser decisions. MSTParser is forced to
restrict the score of features to a single or pair
of nearby parsing decisions in order to make
exhaustive inference tractable.
These differences highlight an inherent trade-off be-
tween exhaustive inference algorithms plus global
learning and expressiveness of feature representa-
tions. MSTParser favors the former at the expense
of the latter and MaltParser the opposite.
3 The CoNLL-X Shared Task
The CoNLL-X shared task (Buchholz and Marsi,
2006) was a large-scale evaluation of data-driven de-
pendency parsers, with data from 13 different lan-
guages and 19 participating systems. The official
evaluation metric was the labeled attachment score
(LAS), defined as the percentage of tokens, exclud-
ing punctuation, that are assigned both the correct
head and the correct dependency label.3
The output of all systems that participated in the
shared task are available for download and consti-
tute a rich resource for comparative error analysis.
2http://w3.msi.vxu.se/users/nivre/research/MaltParser.html
3In addition, results were reported for unlabeled attachment
score (UAS) (tokens with the correct head) and label accuracy
(LA) (tokens with the correct label).
The data used in the experiments below are the out-
puts of MSTParser and MaltParser for all 13 lan-
guages, together with the corresponding gold stan-
dard graphs used in the evaluation. We constructed
the data by simply concatenating a system?s output
for every language. This resulted in a single out-
put file for each system and a corresponding single
gold standard file. This method is sound because the
data sets for each language contain approximately
the same number of tokens ? 5,000. Thus, evalu-
ating system performance over the aggregated files
can be roughly viewed as measuring system perfor-
mance through an equally weighted arithmetic mean
over the languages.
It could be argued that a language by language
comparison would be more appropriate than com-
paring system performance across all languages.
However, as table Table 1 shows, the difference in
accuracy between the two systems is typically small
for all languages, and only in a few cases is this
difference significant. Furthermore, by aggregating
over all languages we gain better statistical estimates
of parser errors, since the data set for each individual
language is very small.
4 Error Analysis
The primary purpose of this study is to characterize
the errors made by standard data-driven dependency
parsing models. To that end, we present a large set of
experiments that relate parsing errors to a set of lin-
guistic and structural properties of the input and pre-
dicted/gold standard dependency graphs. We argue
that the results can be correlated to specific theoreti-
cal aspects of each model ? in particular the trade-off
highlighted in Section 2.4.
For simplicity, all experiments report labeled
parsing accuracies. Identical experiments using un-
labeled parsing accuracies did not reveal any addi-
tional information. Furthermore, all experiments are
based on the data from all 13 languages together, as
explained in section 3.
4.1 Length Factors
It is well known that parsing systems tend to have
lower accuracies for longer sentences. Figure 2
shows the accuracy of both parsing models relative
to sentence length (in bins of size 10: 1?10, 11?20,
125
10 20 30 40 50 50+Sentence Length (bins of size 10)
0.7
0.72
0.74
0.76
0.78
0.8
0.82
0.84
Depe
nden
cy A
ccur
acy MSTParserMaltParser
Figure 2: Accuracy relative to sentence length.
etc.). System performance is almost indistinguish-
able. However, MaltParser tends to perform better
on shorter sentences, which require the greedy in-
ference algorithm to make less parsing decisions. As
a result, the chance of error propagation is reduced
significantly when parsing these sentences. The fact
that MaltParser has a higher accuracy (rather than
the same accuracy) when the likelihood of error
propagation is reduced comes from its richer feature
representation.
Another interesting property is accuracy relative
to dependency length. The length of a dependency
from word wi to word wj is simply equal to |i? j|.
Longer dependencies typically represent modifiers
of the root or the main verb in a sentence. Shorter
dependencies are often modifiers of nouns such as
determiners or adjectives or pronouns modifying
their direct neighbours. Figure 3 measures the pre-
cision and recall for each system relative to depen-
dency lengths in the predicted and gold standard de-
pendency graphs. Precision represents the percent-
age of predicted arcs of length d that were correct.
Recall measures the percentage of gold standard arcs
of length d that were correctly predicted.
Here we begin to see separation between the two
systems. MSTParser is far more precise for longer
dependency arcs, whereas MaltParser does better
for shorter dependency arcs. This behaviour can
be explained using the same reasoning as above:
shorter arcs are created before longer arcs in the
greedy parsing procedure of MaltParser and are less
prone to error propagation. Theoretically, MST-
Parser should not perform better or worse for edges
of any length, which appears to be the case. There
is still a slight degradation, but this can be attributed
to long dependencies occurring more frequently in
constructions with possible ambiguity. Note that
even though the area under the curve is much larger
for MSTParser, the number of dependency arcs with
a length greater than ten is much smaller than the
number with length less than ten, which is why the
overall accuracy of each system is nearly identical.
For all properties considered here, bin size generally
shrinks in size as the value on the x-axis increases.
4.2 Graph Factors
The structure of the predicted and gold standard de-
pendency graphs can also provide insight into the
differences between each model. For example, mea-
suring accuracy for arcs relative to their distance to
the artificial root node will detail errors at different
levels of the dependency graph. For a given arc, we
define this distance as the number of arcs in the re-
verse path from the modifier of the arc to the root.
Figure 4 plots the precision and recall of each sys-
tem for arcs of varying distance to the root. Preci-
sion is equal to the percentage of dependency arcs in
the predicted graph that are at a distance of d and are
correct. Recall is the percentage of dependency arcs
in the gold standard graph that are at a distance of d
and were predicted.
Figure 4 clearly shows that for arcs close to the
root, MSTParser is much more precise than Malt-
Parser, and vice-versa for arcs further away from the
root. This is probably the most compelling graph
given in this study since it reveals a clear distinction:
MSTParser?s precision degrades as the distance to
the root increases whereas MaltParser?s precision in-
creases. The plots essentially run in opposite direc-
tions crossing near the middle. Dependency arcs fur-
ther away from the root are usually constructed early
in the parsing algorithm of MaltParser. Again a re-
duced likelihood of error propagation coupled with
a rich feature representation benefits that parser sub-
stantially. Furthermore, MaltParser tends to over-
predict root modifiers, because all words that the
parser fails to attach as modifiers are automatically
connected to the root, as explained in section 2.3.
Hence, low precision for root modifiers (without a
corresponding drop in recall) is an indication that the
transition-based parser produces fragmented parses.
The behaviour of MSTParser is a little trickier to
explain. One would expect that its errors should be
distributed evenly over the graph. For the most part
this is true, with the exception of spikes at the ends
126
0 5 10 15 20 25 30Dependency Length
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Depe
nden
cy P
recis
ion MSTParserMaltParser
0 5 10 15 20 25 30Dependency Length
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Depe
nden
cy R
ecall
MSTParserMaltParser
Figure 3: Dependency arc precision/recall relative to predicted/gold dependency length.
of the plot. The high performance for root modifica-
tion (distance of 1) can be explained through the fact
that this is typically a low entropy decision ? usu-
ally the parsing algorithm has to determine the main
verb from a small set of possibilities. On the other
end of the plot there is a sharp downwards spike for
arcs of distance greater than 10. It turns out that
MSTParser over-predicts arcs near the bottom of the
graph. Whereas MaltParser pushes difficult parsing
decisions higher in the graph, MSTParser appears to
push these decisions lower.
The next graph property we will examine aims to
quantify the local neighbourhood of an arc within
a dependency graph. Two dependency arcs, (i, j, l)
and (i?, j?, l?) are classified as siblings if they repre-
sent syntactic modifications of the same word, i.e.,
i = i?. Figure 5 measures the precision and recall
of each system relative to the number of predicted
and gold standard siblings of each arc. There is
not much to distinguish between the parsers on this
metric. MSTParser is slightly more precise for arcs
that are predicted with more siblings, whereas Malt-
Parser has slightly higher recall on arcs that have
more siblings in the gold standard tree. Arcs closer
to the root tend to have more siblings, which ties this
result to the previous ones.
The final graph property we wish to look at is the
degree of non-projectivity. The degree of a depen-
dency arc from word w to word u is defined here
as the number of words occurring between w and u
that are not descendants ofw and modify a word that
does not occur between w and u (Nivre, 2006). In
the example from Figure 1, the arc from jedna to Z
has a degree of one, and all other arcs have a degree
of zero. Figure 6 plots dependency arc precision and
recall relative to arc degree in predicted and gold
standard dependency graphs. MSTParser is more
precise when predicting arcs with high degree and
MaltParser vice-versa. Again, this can be explained
by the fact that there is a tight correlation between a
high degree of non-projectivity, dependency length,
distance to root and number of siblings.
4.3 Linguistic Factors
It is important to relate each system?s accuracy to a
set of linguistic categories, such as parts of speech
and dependency types. Therefore, we have made
an attempt to distinguish a few broad categories
that are cross-linguistically identifiable, based on the
available documentation of the treebanks used in the
shared task.
For parts of speech, we distinguish verbs (includ-
ing both main verbs and auxiliaries), nouns (includ-
ing proper names), pronouns (sometimes also in-
cluding determiners), adjectives, adverbs, adposi-
tions (prepositions, postpositions), and conjunctions
(both coordinating and subordinating). For depen-
dency types, we distinguish a general root category
(for labels used on arcs from the artificial root, in-
cluding either a generic label or the label assigned
to predicates of main clauses, which are normally
verbs), a subject category, an object category (in-
cluding both direct and indirect objects), and various
categories related to coordination.
Figure 7 shows the accuracy of the two parsers
for different parts of speech. This figure measures
labeled dependency accuracy relative to the part of
speech of the modifier word in a dependency rela-
tion. We see that MaltParser has slightly better ac-
curacy for nouns and pronouns, while MSTParser
does better on all other categories, in particular con-
junctions. This pattern is consistent with previous
results insofar as verbs and conjunctions are often
involved in dependencies closer to the root that span
127
2 4 6 8 10Distance to Root
0.74
0.76
0.78
0.8
0.82
0.84
0.86
0.88
0.9
Depe
nden
cy P
recis
ion MSTParserMaltParser
2 4 6 8 10Distance to Root
0.76
0.78
0.8
0.82
0.84
0.86
0.88
Depe
nden
cy R
ecall
MSTParserMaltParser
Figure 4: Dependency arc precision/recall relative to predicted/gold distance to root.
0 2 4 6 8 10+Number of Modifier Siblings
0.5
0.6
0.7
0.8
0.9
Depe
nden
cy P
recis
ion MSTParserMaltParser
0 2 4 6 8 10+Number of Modifier Siblings
0.5
0.6
0.7
0.8
0.9
Depe
nden
cy R
ecall
MSTParserMaltParser
Figure 5: Dependency arc precision/recall relative to number of predicted/gold siblings.
longer distances, while nouns and pronouns are typ-
ically attached to verbs and therefore occur lower in
the graph, with shorter distances. Empirically, ad-
verbs resemble verbs and conjunctions with respect
to root distance but group with nouns and pronouns
for dependency length, so the former appears to be
more important. In addition, both conjunctions and
adverbs tend to have a high number of siblings, mak-
ing the results consistent with the graph in Figure 5.
Adpositions and especially adjectives constitute
a puzzle, having both high average root distance
and low average dependency length. Adpositions do
tend to have a high number of siblings on average,
which could explain MSTParser?s performance on
that category. However, adjectives on average occur
the furthest away from the root, have the shortest
dependency length and the fewest siblings. As such,
we do not have an explanation for this behaviour.
In the top half of Figure 8, we consider precision
and recall for dependents of the root node (mostly
verbal predicates), and for subjects and objects. As
already noted, MSTParser has considerably better
precision (and slightly better recall) for the root cat-
egory, but MaltParser has an advantage for the nomi-
nal categories, especially subjects. A possible expla-
nation for the latter result, in addition to the length-
based and graph-based factors invoked before, is that
60.0%65.0%70.0%
75.0%80.0%85.0%
90.0%95.0%
Verb Noun Pron Adj Adv Adpos ConjPart of Speech (POS)Labele
d Attachment Score (
LAS) MSTParserMaltParser
Figure 7: Accuracy for different parts of speech.
MaltParser integrates labeling into the parsing pro-
cess, so that previously assigned dependency labels
can be used as features, which may be important to
disambiguate subjects and objects.
Finally, in the bottom half of Figure 8, we dis-
play precision and recall for coordinate structures,
divided into different groups depending on the type
of analysis adopted in a particular treebank. The cat-
egory CCH (coordinating conjunction as head) con-
tains conjunctions analyzed as heads of coordinate
structures, with a special dependency label that does
not describe the function of the coordinate structure
in the larger syntactic structure, a type of category
found in the so-called Prague style analysis of coor-
dination and used in the data sets for Arabic, Czech,
128
0 1 2 3 4 5 6 7+Non-Projective Arc Degree
0.55
0.6
0.65
0.7
0.75
0.8
0.85
Depe
nden
cy P
recis
ion MSTParserMaltParser
0 1 2 3 4 5 6 7+Non-Projective Arc Degree
0.6
0.65
0.7
0.75
0.8
0.85
Depe
nden
cy R
ecall
MSTParserMaltParser
Figure 6: Dependency arc precision/recall relative to predicted/gold degree of non-projectivity.
65.0%70.0%
75.0%80.0%
85.0%90.0%
95.0%
Root Subj ObjDependency Type (DEP)De
pendency Precision MSTParserMaltParser
72.0%74.0%76.0%
78.0%80.0%82.0%
84.0%86.0%88.0%
90.0%
Root Subj ObjDependency Type (DEP)D
ependency Recall MSTParserMaltParser
0.0%10.0%20.0%
30.0%40.0%50.0%
60.0%70.0%80.0%
90.0%
CCH CCD CJCC CJCJDependency Type (DEP)De
pendency Precision MSTParserMaltParser
0.0%10.0%20.0%
30.0%40.0%50.0%
60.0%70.0%80.0%
90.0%
CCH CCD CJCC CJCJDependency Tyle (DEP)D
ependency Recall MSTParserMaltParser
Figure 8: Precision/recall for different dependency types.
and Slovene. The category CCD (coordinating con-
junction as dependent) instead denotes conjunctions
that are attached as dependents of one of the con-
juncts with a label that only marks them as conjunc-
tions, a type of category found in the data sets for
Bulgarian, Danish, German, Portuguese, Swedish
and Turkish. The two remaining categories con-
tain conjuncts that are assigned a dependency label
that only marks them as conjuncts and that are at-
tached either to the conjunction (CJCC) or to an-
other conjunct (CJCJ). The former is found in Bul-
garian, Danish, and German; the latter only in Por-
tuguese and Swedish. For most of the coordination
categories there is little or no difference between the
two parsers, but for CCH there is a difference in both
precision and recall of almost 20 percentage points
to MSTParser?s advantage. This can be explained by
noting that, while the categories CCD, CJCC, and
CJCJ denote relations that are internal to the coor-
dinate structure and therefore tend to be local, the
CCH relations hold between the coordinate struc-
ture and its head, which is often a relation that spans
over a greater distance and is nearer the root of the
dependency graph. It is likely that the difference in
accuracy for this type of dependency accounts for a
large part of the difference in accuracy noted earlier
for conjunctions as a part of speech.
4.4 Discussion
The experiments from the previous section highlight
the fundamental trade-off between global training
and exhaustive inference on the one hand and ex-
pressive feature representations on the other. Error
propagation is an issue for MaltParser, which typi-
129
cally performs worse on long sentences, long depen-
dency arcs and arcs higher in the graphs. But this is
offset by the rich feature representation available to
these models that result in better decisions for fre-
quently occurring arc types like short dependencies
or subjects and objects. The errors for MSTParser
are spread a little more evenly. This is expected,
as the inference algorithm and feature representation
should not prefer one type of arc over another.
What has been learned? It was already known that
the two systems make different errors through the
work of Sagae and Lavie (2006). However, in that
work an arc-based voting scheme was used that took
only limited account of the properties of the words
connected by a dependency arc (more precisely, the
overall accuracy of each parser for the part of speech
of the dependent). The analysis in this work not only
shows that the errors made by each system are dif-
ferent, but that they are different in a way that can be
predicted and quantified. This is an important step
in parser development.
To get some upper bounds of the improvement
that can be obtained by combining the strengths of
each models, we have performed two oracle experi-
ments. Given the output of the two systems, we can
envision an oracle that can optimally choose which
single parse or combination of sub-parses to predict
as a final parse. For the first experiment the oracle
is provided with the single best parse from each sys-
tem, say G = (V,A) and G? = (V ?, A?). The oracle
chooses a parse that has the highest number of cor-
rectly predicted labeled dependency attachments. In
this situation, the oracle accuracy is 84.5%. In the
second experiment the oracle chooses the tree that
maximizes the number of correctly predicted depen-
dency attachments, subject to the restriction that the
tree must only contain arcs from A ? A?. This can
be computed by setting the weight of an arc to 1 if
it is in the correct parse and in the set A ? A?. All
other arc weights are set to negative infinity. One can
then simply find the tree that has maximal sum of
arc weights using directed spanning tree algorithms.
This technique is similar to the parser voting meth-
ods used by Sagae and Lavie (2006). In this situa-
tion, the oracle accuracy is 86.9%.
In both cases we see a clear increase in accuracy:
86.9% and 84.5% relative to 81% for the individual
systems. This indicates that there is still potential
for improvement, just by combining the two existing
models. More interestingly, however, we can use
the analysis to get ideas for new models. Below we
sketch some possible new directions:
1. Ensemble systems: The error analysis pre-
sented in this paper could be used as inspiration
for more refined weighting schemes for ensem-
ble systems of the kind proposed by Sagae and
Lavie (2006), making the weights depend on a
range of linguistic and graph-based factors.
2. Hybrid systems: Rather than using an ensem-
ble of several parsers, we may construct a sin-
gle system integrating the strengths of each
parser described here. This could defer to
a greedy inference strategy during the early
stages of the parse in order to benefit from a
rich feature representation, but then default to
a global exhaustive model as the likelihood for
error propagation increases.
3. Novel approaches: The two approaches inves-
tigated are each based on a particular combina-
tion of training and inference methods. Wemay
naturally ask what other combinations may
prove fruitful. For example, what about glob-
ally trained, greedy, transition-based models?
This is essentially what Daume? III et al (2006)
provide, in the form of a general search-based
structured learning framework that can be di-
rectly applied to dependency parsing. The ad-
vantage of this method is that the learning can
set model parameters relative to errors resulting
directly from the search strategy ? such as error
propagation due to greedy search. When com-
bined with MaltParser?s rich feature represen-
tation, this could lead to significant improve-
ments in performance.
5 Conclusion
We have presented a thorough study of the dif-
ference in errors made between global exhaustive
graph-based parsing systems (MSTParser) and lo-
cal greedy transition-based parsing systems (Malt-
Parser). We have shown that these differences can
be quantified and tied to theoretical expectations of
each model, which may provide insights leading to
better models in the future.
130
References
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?.
2003. The PDT: A 3-level annotation scenario. In
A. Abeille?, editor, Treebanks: Building and Using
Parsed Corpora, chapter 7. Kluwer Academic Publish-
ers.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In Proc. CoNLL.
Hal Daume? III, John Langford, and Daniel Marcu. 2006.
Search-based structured prediction. In Submission.
Y. Ding and M. Palmer. 2004. Synchronous dependency
insertion grammars: A grammar formalism for syntax
based statistical MT. InWorkshop on Recent Advances
in Dependency Grammars (COLING).
R. Hudson. 1984. Word Grammar. Blackwell.
H. Maruyama. 1990. Structural disambiguation with
constraint propagation. In Proc. ACL.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proc. ACL.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005b.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. HLT/EMNLP.
R. McDonald, K. Lerman, and F. Pereira. 2006. Multi-
lingual dependency analysis with a two-stage discrim-
inative parser. In Proc. CoNLL.
I.A. Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
J. Nivre and J. Nilsson. 2005. Pseudo-projective depen-
dency parsing. In Proc. ACL.
J. Nivre, J. Hall, J. Nilsson, G. Eryigit, and S. Marinov.
2006. Labeled pseudo-projective dependency parsing
with support vector machines. In Proc. CoNLL.
J. Nivre. 2003. An efficient algorithm for projective de-
pendency parsing. In Proc. IWPT.
J. Nivre. 2006. Constraints on non-projective depen-
dency parsing. In Proc. EACL.
K. Sagae and A. Lavie. 2006. Parser combination by
reparsing. In Proc. HLT/NAACL.
P. Sgall, E. Hajic?ova?, and J. Panevova?. 1986. The Mean-
ing of the Sentence in Its Pragmatic Aspects. Reidel.
R. Snow, D. Jurafsky, and A. Y. Ng. 2004. Learning
syntactic patterns for automatic hypernym discovery.
In Proc. NIPS.
131
Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 915?932,
Prague, June 2007. c?2007 Association for Computational Linguistics
The CoNLL 2007 Shared Task on Dependency Parsing
Joakim Nivre?? Johan Hall? Sandra Ku?bler? Ryan McDonald??
Jens Nilsson? Sebastian Riedel?? Deniz Yuret??
?Va?xjo? University, School of Mathematics and Systems Engineering, first.last@vxu.se
?Uppsala University, Dept. of Linguistics and Philology, joakim.nivre@lingfil.uu.se
?Indiana University, Department of Linguistics, skuebler@indiana.edu
??Google Inc., ryanmcd@google.com
??University of Edinburgh, School of Informatics, S.R.Riedel@sms.ed.ac.uk
??Koc? University, Dept. of Computer Engineering, dyuret@ku.edu.tr
Abstract
The Conference on Computational Natural
Language Learning features a shared task, in
which participants train and test their learn-
ing systems on the same data sets. In 2007,
as in 2006, the shared task has been devoted
to dependency parsing, this year with both a
multilingual track and a domain adaptation
track. In this paper, we define the tasks of the
different tracks and describe how the data
sets were created from existing treebanks for
ten languages. In addition, we characterize
the different approaches of the participating
systems, report the test results, and provide
a first analysis of these results.
1 Introduction
Previous shared tasks of the Conference on Compu-
tational Natural Language Learning (CoNLL) have
been devoted to chunking (1999, 2000), clause iden-
tification (2001), named entity recognition (2002,
2003), and semantic role labeling (2004, 2005). In
2006 the shared task was multilingual dependency
parsing, where participants had to train a single
parser on data from thirteen different languages,
which enabled a comparison not only of parsing and
learning methods, but also of the performance that
can be achieved for different languages (Buchholz
and Marsi, 2006).
In dependency-based syntactic parsing, the task is
to derive a syntactic structure for an input sentence
by identifying the syntactic head of each word in the
sentence. This defines a dependency graph, where
the nodes are the words of the input sentence and the
arcs are the binary relations from head to dependent.
Often, but not always, it is assumed that all words
except one have a syntactic head, which means that
the graph will be a tree with the single independent
word as the root. In labeled dependency parsing, we
additionally require the parser to assign a specific
type (or label) to each dependency relation holding
between a head word and a dependent word.
In this year?s shared task, we continue to explore
data-driven methods for multilingual dependency
parsing, but we add a new dimension by also intro-
ducing the problem of domain adaptation. The way
this was done was by having two separate tracks: a
multilingual track using essentially the same setup
as last year, but with partly different languages, and
a domain adaptation track, where the task was to use
machine learning to adapt a parser for a single lan-
guage to a new domain. In total, test results were
submitted for twenty-three systems in the multilin-
gual track, and ten systems in the domain adaptation
track (six of which also participated in the multilin-
gual track). Not everyone submitted papers describ-
ing their system, and some papers describe more
than one system (or the same system in both tracks),
which explains why there are only (!) twenty-one
papers in the proceedings.
In this paper, we provide task definitions for the
two tracks (section 2), describe data sets extracted
from available treebanks (section 3), report results
for all systems in both tracks (section 4), give an
overview of approaches used (section 5), provide a
first analysis of the results (section 6), and conclude
with some future directions (section 7).
915
2 Task Definition
In this section, we provide the task definitions that
were used in the two tracks of the CoNLL 2007
Shard Task, the multilingual track and the domain
adaptation track, together with some background
and motivation for the design choices made. First
of all, we give a brief description of the data format
and evaluation metrics, which were common to the
two tracks.
2.1 Data Format and Evaluation Metrics
The data sets derived from the original treebanks
(section 3) were in the same column-based format
as for the 2006 shared task (Buchholz and Marsi,
2006). In this format, sentences are separated by a
blank line; a sentence consists of one or more to-
kens, each one starting on a new line; and a token
consists of the following ten fields, separated by a
single tab character:
1. ID: Token counter, starting at 1 for each new
sentence.
2. FORM: Word form or punctuation symbol.
3. LEMMA: Lemma or stem of word form, or an
underscore if not available.
4. CPOSTAG: Coarse-grained part-of-speech tag,
where the tagset depends on the language.
5. POSTAG: Fine-grained part-of-speech tag,
where the tagset depends on the language, or
identical to the coarse-grained part-of-speech
tag if not available.
6. FEATS: Unordered set of syntactic and/or mor-
phological features (depending on the particu-
lar language), separated by a vertical bar (|), or
an underscore if not available.
7. HEAD: Head of the current token, which is
either a value of ID or zero (0). Note that,
depending on the original treebank annotation,
there may be multiple tokens with HEAD=0.
8. DEPREL: Dependency relation to the HEAD.
The set of dependency relations depends on
the particular language. Note that, depending
on the original treebank annotation, the depen-
dency relation when HEAD=0 may be mean-
ingful or simply ROOT.
9. PHEAD: Projective head of current token,
which is either a value of ID or zero (0), or an
underscore if not available.
10. PDEPREL: Dependency relation to the
PHEAD, or an underscore if not available.
The PHEAD and PDEPREL were not used at all
in this year?s data sets (i.e., they always contained
underscores) but were maintained for compatibility
with last year?s data sets. This means that, in prac-
tice, the first six columns can be considered as input
to the parser, while the HEAD and DEPREL fields
are the output to be produced by the parser. Labeled
training sets contained all ten columns; blind test
sets only contained the first six columns; and gold
standard test sets (released only after the end of the
test period) again contained all ten columns. All data
files were encoded in UTF-8.
The official evaluation metric in both tracks was
the labeled attachment score (LAS), i.e., the per-
centage of tokens for which a system has predicted
the correct HEAD and DEPREL, but results were
also reported for unlabeled attachment score (UAS),
i.e., the percentage of tokens with correct HEAD,
and the label accuracy (LA), i.e., the percentage of
tokens with correct DEPREL. One important differ-
ence compared to the 2006 shared task is that all to-
kens were counted as ?scoring tokens?, including in
particular all punctuation tokens. The official eval-
uation script, eval07.pl, is available from the shared
task website.1
2.2 Multilingual Track
The multilingual track of the shared task was orga-
nized in the same way as the 2006 task, with an-
notated training and test data from a wide range of
languages to be processed with one and the same
parsing system. This system must therefore be able
to learn from training data, to generalize to unseen
test data, and to handle multiple languages, possi-
bly by adjusting a number of hyper-parameters. Par-
ticipants in the multilingual track were expected to
submit parsing results for all languages involved.
1http://depparse.uvt.nl/depparse-wiki/SoftwarePage
916
One of the claimed advantages of dependency
parsing, as opposed to parsing based on constituent
analysis, is that it extends naturally to languages
with free or flexible word order. This explains the
interest in recent years for multilingual evaluation
of dependency parsers. Even before the 2006 shared
task, the parsers of Collins (1997) and Charniak
(2000), originally developed for English, had been
adapted for dependency parsing of Czech, and the
parsing methodology proposed by Kudo and Mat-
sumoto (2002) and Yamada and Matsumoto (2003)
had been evaluated on both Japanese and English.
The parser of McDonald and Pereira (2006) had
been applied to English, Czech and Danish, and the
parser of Nivre et al (2007) to ten different lan-
guages. But by far the largest evaluation of mul-
tilingual dependency parsing systems so far was the
2006 shared task, where nineteen systems were eval-
uated on data from thirteen languages (Buchholz and
Marsi, 2006).
One of the conclusions from the 2006 shared task
was that parsing accuracy differed greatly between
languages and that a deeper analysis of the factors
involved in this variation was an important problem
for future research. In order to provide an extended
empirical foundation for such research, we tried to
select the languages and data sets for this year?s task
based on the following desiderata:
? The selection of languages should be typolog-
ically varied and include both new languages
and old languages (compared to 2006).
? The creation of the data sets should involve as
little conversion as possible from the original
treebank annotation, meaning that preference
should be given to treebanks with dependency
annotation.
? The training data sets should include at least
50,000 tokens and at most 500,000 tokens.2
The final selection included data from Arabic,
Basque, Catalan, Chinese, Czech, English, Greek,
Hungarian, Italian, and Turkish. The treebanks from
2The reason for having an upper bound on the training set
size was the fact that, in 2006, some participants could not train
on all the data for some languages because of time limitations.
Similar considerations also led to the decision to have a smaller
number of languages this year (ten, as opposed to thirteen).
which the data sets were extracted are described in
section 3.
2.3 Domain Adaptation Track
One well known characteristic of data-driven pars-
ing systems is that they typically perform much
worse on data that does not come from the train-
ing domain (Gildea, 2001). Due to the large over-
head in annotating text with deep syntactic parse
trees, the need to adapt parsers from domains with
plentiful resources (e.g., news) to domains with lit-
tle resources is an important problem. This prob-
lem is commonly referred to as domain adaptation,
where the goal is to adapt annotated resources from
a source domain to a target domain of interest.
Almost all prior work on domain adaptation as-
sumes one of two scenarios. In the first scenario,
there are limited annotated resources available in the
target domain, and many studies have shown that
this may lead to substantial improvements. This in-
cludes the work of Roark and Bacchiani (2003), Flo-
rian et al (2004), Chelba and Acero (2004), Daume?
and Marcu (2006), and Titov and Henderson (2006).
Of these, Roark and Bacchiani (2003) and Titov and
Henderson (2006) deal specifically with syntactic
parsing. The second scenario assumes that there are
no annotated resources in the target domain. This is
a more realistic situation and is considerably more
difficult. Recent work by McClosky et al (2006)
and Blitzer et al (2006) have shown that the exis-
tence of a large unlabeled corpus in the new domain
can be leveraged in adaptation. For this shared-task,
we are assuming the latter setting ? no annotated re-
sources in the target domain.
Obtaining adequate annotated syntactic resources
for multiple languages is already a challenging prob-
lem, which is only exacerbated when these resources
must be drawn from multiple and diverse domains.
As a result, the only language that could be feasibly
tested in the domain adaptation track was English.
The setup for the domain adaptation track was as
follows. Participants were provided with a large an-
notated corpus from the source domain, in this case
sentences from the Wall Street Journal. Participants
were also provided with data from three different
target domains: biomedical abstracts (development
data), chemical abstracts (test data 1), and parent-
child dialogues (test data 2). Additionally, a large
917
unlabeled corpus for each data set (training, devel-
opment, test) was provided. The goal of the task was
to use the annotated source data, plus any unlabeled
data, to produce a parser that is accurate for each of
the test sets from the target domains.3
Participants could submit systems in either the
?open? or ?closed? class (or both). The closed class
requires a system to use only those resources pro-
vided as part of the shared task. The open class al-
lows a system to use additional resources provided
those resources are not drawn from the same domain
as the development or test sets. An example might
be a part-of-speech tagger trained on the entire Penn
Treebank and not just the subset provided as train-
ing data, or a parser that has been hand-crafted or
trained on a different training set.
3 Treebanks
In this section, we describe the treebanks used in the
shared task and give relevant information about the
data sets created from them.
3.1 Multilingual Track
Arabic The analytical syntactic annotation
of the Prague Arabic Dependency Treebank
(PADT) (Hajic? et al, 2004) can be considered a
pure dependency annotation. The conversion, done
by Otakar Smrz, from the original format to the
column-based format described in section 2.1 was
therefore relatively straightforward, although not all
the information in the original annotation could be
transfered to the new format. PADT was one of the
treebanks used in the 2006 shared task but then only
contained about 54,000 tokens. Since then, the size
of the treebank has more than doubled, with around
112,000 tokens. In addition, the morphological
annotation has been made more informative. It
is also worth noting that the parsing units in this
treebank are in many cases larger than conventional
sentences, which partly explains the high average
number of tokens per ?sentence? (Buchholz and
Marsi, 2006).
3Note that annotated development data for the target domain
was only provided for the development domain, biomedical ab-
stracts. For the two test domains, chemical abstracts and parent-
child dialogues, the only annotated data sets were the gold stan-
dard test sets, released only after test runs had been submitted.
Basque For Basque, we used the 3LB Basque
treebank (Aduriz et al, 2003). At present, the tree-
bank consists of approximately 3,700 sentences, 334
of which were used as test data. The treebank com-
prises literary and newspaper texts. It is annotated
in a dependency format and was converted to the
CoNLL format by a team led by Koldo Gojenola.
Catalan The Catalan section of the CESS-ECE
Syntactically and Semantically Annotated Cor-
pora (Mart?? et al, 2007) is annotated with, among
other things, constituent structure and grammatical
functions. A head percolation table was used for
automatically converting the constituent trees into
dependency trees. The original data only contains
functions related to the verb, and a function table
was used for deriving the remaining syntactic func-
tions. The conversion was performed by a team led
by Llu??s Ma`rquez and Anto`nia Mart??.
Chinese The Chinese data are taken from the
Sinica treebank (Chen et al, 2003), which con-
tains both syntactic functions and semantic func-
tions. The syntactic head was used in the conversion
to the CoNLL format, carried out by Yu-Ming Hsieh
and the organizers of the 2006 shared task, and the
syntactic functions were used wherever it was pos-
sible. The training data used is basically the same
as for the 2006 shared task, except for a few correc-
tions, but the test data is new for this year?s shared
task. It is worth noting that the parsing units in this
treebank are sometimes smaller than conventional
sentence units, which partly explains the low aver-
age number of tokens per ?sentence? (Buchholz and
Marsi, 2006).
Czech The analytical syntactic annotation of the
Prague Dependency Treebank (PDT) (Bo?hmova? et
al., 2003) is a pure dependency annotation, just as
for PADT. It was also used in the shared task 2006,
but there are two important changes compared to
last year. First, version 2.0 of PDT was used in-
stead of version 1.0, and a conversion script was
created by Zdenek Zabokrtsky, using the new XML-
based format of PDT 2.0. Secondly, due to the upper
bound on training set size, only sections 1?3 of PDT
constitute the training data, which amounts to some
450,000 tokens. The test data is a small subset of the
development test set of PDT.
918
English For English we used the Wall Street Jour-
nal section of the Penn Treebank (Marcus et al,
1993). In particular, we used sections 2-11 for train-
ing and a subset of section 23 for testing. As a pre-
processing stage we removed many functions tags
from the non-terminals in the phrase structure repre-
sentation to make the representations more uniform
with out-of-domain test sets for the domain adapta-
tion track (see section 3.2). The resulting data set
was then converted to dependency structures using
the procedure described in Johansson and Nugues
(2007a). This work was done by Ryan McDonald.
Greek The Greek Dependency Treebank
(GDT) (Prokopidis et al, 2005) adopts a de-
pendency structure annotation very similar to those
of PDT and PADT, which means that the conversion
by Prokopis Prokopidis was relatively straightfor-
ward. GDT is one of the smallest treebanks in
this year?s shared task (about 65,000 tokens) and
contains sentences of Modern Greek. Just like PDT
and PADT, the treebank contains more than one
level of annotation, but we only used the analytical
level of GDT.
Hungarian For the Hungarian data, the Szeged
treebank (Csendes et al, 2005) was used. The tree-
bank is based on texts from six different genres,
ranging from legal newspaper texts to fiction. The
original annotation scheme is constituent-based, fol-
lowing generative principles. It was converted into
dependencies by Zo?ltan Alexin based on heuristics.
Italian The data set used for Italian is a subset
of the balanced section of the Italian Syntactic-
Semantic Treebank (ISST) (Montemagni et al,
2003) and consists of texts from the newspaper Cor-
riere della Sera and from periodicals. A team led
by Giuseppe Attardi, Simonetta Montemagni, and
Maria Simi converted the annotation to the CoNLL
format, using information from two different anno-
tation levels, the constituent structure level and the
dependency structure level.
Turkish For Turkish we used the METU-Sabanc?
Turkish Treebank (Oflazer et al, 2003), which was
also used in the 2006 shared task. A new test set of
about 9,000 tokens was provided by Gu?ls?en Eryig?it
(Eryig?it, 2007), who also handled the conversion to
the CoNLL format, which means that we could use
all the approximately 65,000 tokens of the original
treebank for training. The rich morphology of Turk-
ish requires the basic tokens in parsing to be inflec-
tional groups (IGs) rather than words. IGs of a single
word are connected to each other deterministically
using dependency links labeled DERIV, referred to
as word-internal dependencies in the following, and
the FORM and the LEMMA fields may be empty
(they contain underscore characters in the data files).
Sentences do not necessarily have a unique root;
most internal punctuation and a few foreign words
also have HEAD=0.
3.2 Domain Adaptation Track
As mentioned previously, the source data is drawn
from a corpus of news, specifically the Wall Street
Journal section of the Penn Treebank (Marcus et al,
1993). This data set is identical to the English train-
ing set from the multilingual track (see section 3.1).
For the target domains we used three different
labeled data sets. The first two were annotated
as part of the PennBioIE project (Kulick et al,
2004) and consist of sentences drawn from either
biomedical or chemical research abstracts. Like the
source WSJ corpus, this data is annotated using the
Penn Treebank phrase structure scheme. To con-
vert these sets to dependency structures we used the
same procedure as before (Johansson and Nugues,
2007a). Additional care was taken to remove sen-
tences that contained non-WSJ part-of-speech tags
or non-terminals (e.g., HYPH part-of-speech tag in-
dicating a hyphen). Furthermore, the annotation
scheme for gaps and traces was made consistent with
the Penn Treebank wherever possible. As already
mentioned, the biomedical data set was distributed
as a development set for the training phase, while
the chemical data set was only used for final testing.
The third target data set was taken from the
CHILDES database (MacWhinney, 2000), in partic-
ular the EVE corpus (Brown, 1973), which has been
annotated with dependency structures. Unfortu-
nately the dependency labels of the CHILDES data
were inconsistent with those of the WSJ, biomedi-
cal and chemical data sets, and we therefore opted
to only evaluate unlabeled accuracy for this data
set. Furthermore, there was an inconsistency in how
main and auxiliary verbs were annotated for this data
set relative to others. As a result of this, submitting
919
Multilingual Domain adaptation
Ar Ba Ca Ch Cz En Gr Hu It Tu PCHEM CHILDES
Language family Sem. Isol. Rom. Sin. Sla. Ger. Hel. F.-U. Rom. Tur. Ger.
Annotation d d c+f c+f d c+f d c+f c+f d c+f d
Training data Development data
Tokens (k) 112 51 431 337 432 447 65 132 71 65 5
Sentences (k) 2.9 3.2 15.0 57.0 25.4 18.6 2.7 6.0 3.1 5.6 0.2
Tokens/sentence 38.3 15.8 28.8 5.9 17.0 24.0 24.2 21.8 22.9 11.6 25.1
LEMMA Yes Yes Yes No Yes No Yes Yes Yes Yes No
No. CPOSTAG 15 25 17 13 12 31 18 16 14 14 25
No. POSTAG 21 64 54 294 59 45 38 43 28 31 37
No. FEATS 21 359 33 0 71 0 31 50 21 78 0
No. DEPREL 29 35 42 69 46 20 46 49 22 25 18
No. DEPREL H=0 18 17 1 1 8 1 22 1 1 1 1
% HEAD=0 8.7 9.7 3.5 16.9 11.6 4.2 8.3 4.6 5.4 12.8 4.0
% HEAD left 79.2 44.5 60.0 24.7 46.9 49.0 44.8 27.4 65.0 3.8 50.0
% HEAD right 12.1 45.8 36.5 58.4 41.5 46.9 46.9 68.0 29.6 83.4 46.0
HEAD=0/sentence 3.3 1.5 1.0 1.0 2.0 1.0 2.0 1.0 1.2 1.5 1.0
% Non-proj. arcs 0.4 2.9 0.1 0.0 1.9 0.3 1.1 2.9 0.5 5.5 0.4
% Non-proj. sent. 10.1 26.2 2.9 0.0 23.2 6.7 20.3 26.4 7.4 33.3 8.0
Punc. attached S S A S S A S A A S A
DEPRELS for punc. 10 13 6 29 16 13 15 1 10 12 8
Test data PCHEM CHILDES
Tokens 5124 5390 5016 5161 4724 5003 4804 7344 5096 4513 5001 4999
Sentences 131 334 167 690 286 214 197 390 249 300 195 666
Tokens/sentence 39.1 16.1 30.0 7.5 16.5 23.4 24.4 18.8 20.5 15.0 25.6 12.9
% New words 12.44 24.98 4.35 9.70 12.58 3.13 12.43 26.10 15.07 36.29 31.33 6.10
% New lemmas 2.82 11.13 3.36 n/a 5.28 n/a 5.82 14.80 8.24 9.95 n/a n/a
Table 1: Characteristics of the data sets for the 10 languages of the multilingual track and the development
set and the two test sets of the domain adaptation track.
920
results for the CHILDES data was considered op-
tional. Like the chemical data set, this data set was
only used for final testing.
Finally, a large corpus of unlabeled in-domain
data was provided for each data set and made avail-
able for training. This data was drawn from theWSJ,
PubMed.com (specific to biomedical and chemical
research literature), and the CHILDES data base.
The data was tokenized to be as consistent as pos-
sible with the WSJ training set.
3.3 Overview
Table 1 describes the characteristics of the data sets.
For the multilingual track, we provide statistics over
the training and test sets; for the domain adaptation
track, the statistics were extracted from the develop-
ment set. Following last year?s shared task practice
(Buchholz and Marsi, 2006), we use the following
definition of projectivity: An arc (i, j) is projective
iff all nodes occurring between i and j are dominated
by i (where dominates is the transitive closure of the
arc relation).
In the table, the languages are abbreviated to their
first two letters. Language families are: Semitic,
Isolate, Romance, Sino-Tibetan, Slavic, Germanic,
Hellenic, Finno-Ugric, and Turkic. The type of the
original annotation is either constituents plus (some)
functions (c+f) or dependencies (d). For the train-
ing data, the number of words and sentences are
given in multiples of thousands, and the average
length of a sentence in words (including punctua-
tion tokens). The following rows contain informa-
tion about whether lemmas are available, the num-
ber of coarse- and fine-grained part-of-speech tags,
the number of feature components, and the number
of dependency labels. Then information is given on
how many different dependency labels can co-occur
with HEAD=0, the percentage of HEAD=0 depen-
dencies, and the percentage of heads preceding (left)
or succeeding (right) a token (giving an indication of
whether a language is predominantly head-initial or
head-final). This is followed by the average number
of HEAD=0 dependencies per sentence and the per-
centage of non-projective arcs and sentences. The
last two rows show whether punctuation tokens are
attached as dependents of other tokens (A=Always,
S=Sometimes) and specify the number of depen-
dency labels that exist for punctuation tokens. Note
that punctuation is defined as any token belonging to
the UTF-8 category of punctuation. This means, for
example, that any token having an underscore in the
FORM field (which happens for word-internal IGs
in Turkish) is also counted as punctuation here.
For the test sets, the number of words and sen-
tences as well as the ratio of words per sentence are
listed, followed by the percentage of new words and
lemmas (if applicable). For the domain adaptation
sets, the percentage of new words is computed with
regard to the training set (Penn Treebank).
4 Submissions and Results
As already stated in the introduction, test runs were
submitted for twenty-three systems in the multilin-
gual track, and ten systems in the domain adaptation
track (six of which also participated in the multilin-
gual track). In the result tables below, systems are
identified by the last name of the teammember listed
first when test runs were uploaded for evaluation. In
general, this name is also the first author of a paper
describing the system in the proceedings, but there
are a few exceptions and complications. First of all,
for four out of twenty-seven systems, no paper was
submitted to the proceedings. This is the case for the
systems of Jia, Maes et al, Nash, and Zeman, which
is indicated by the fact that these names appear in
italics in all result tables. Secondly, two teams sub-
mitted two systems each, which are described in a
single paper by each team. Thus, the systems called
?Nilsson? and ?Hall, J.? are both described in Hall et
al. (2007a), while the systems called ?Duan (1)? and
?Duan (2)? are both described in Duan et al (2007).
Finally, please pay attention to the fact that there
are two teams, where the first author?s last name is
Hall. Therefore, we use ?Hall, J.? and ?Hall, K.?,
to disambiguate between the teams involving Johan
Hall (Hall et al, 2007a) and Keith Hall (Hall et al,
2007b), respectively.
Tables 2 and 3 give the scores for the multilingual
track in the CoNLL 2007 shared task. The Average
column contains the average score for all ten lan-
guages, which determines the ranking in this track.
Table 4 presents the results for the domain adapta-
tion track, where the ranking is determined based on
the PCHEM results only, since the CHILDES data
set was optional. Note also that there are no labeled
921
Team Average Arabic Basque Catalan Chinese Czech English Greek Hungarian Italian Turkish
Nilsson 80.32(1) 76.52(1) 76.94(1) 88.70(1) 75.82(15) 77.98(3) 88.11(5) 74.65(2) 80.27(1) 84.40(1) 79.79(2)
Nakagawa 80.29(2) 75.08(2) 72.56(7) 87.90(3) 83.84(2) 80.19(1) 88.41(3) 76.31(1) 76.74(8) 83.61(3) 78.22(5)
Titov 79.90(3) 74.12(6) 75.49(3) 87.40(6) 82.14(7) 77.94(4) 88.39(4) 73.52(10) 77.94(4) 82.26(6) 79.81(1)
Sagae 79.90(4) 74.71(4) 74.64(6) 88.16(2) 84.69(1) 74.83(8) 89.01(2) 73.58(8) 79.53(2) 83.91(2) 75.91(10)
Hall, J. 79.80(5)* 74.75(3) 74.99(5) 87.74(4) 83.51(3) 77.22(6) 85.81(12) 74.21(6) 78.09(3) 82.48(5) 79.24(3)
Carreras 79.09(6)* 70.20(11) 75.75(2) 87.60(5) 80.86(10) 78.60(2) 89.61(1) 73.56(9) 75.42(9) 83.46(4) 75.85(11)
Attardi 78.27(7) 72.66(8) 69.48(12) 86.86(7) 81.50(8) 77.37(5) 85.85(10) 73.92(7) 76.81(7) 81.34(8) 76.87(7)
Chen 78.06(8) 74.65(5) 72.39(8) 86.66(8) 81.24(9) 73.69(10) 83.81(13) 74.42(3) 75.34(10) 82.04(7) 76.31(9)
Duan (1) 77.70(9)* 69.91(13) 71.26(9) 84.95(10) 82.58(6) 75.34(7) 85.83(11) 74.29(4) 77.06(5) 80.75(9) 75.03(12)
Hall, K. 76.91(10)* 73.40(7) 69.81(11) 82.38(14) 82.77(4) 72.27(12) 81.93(15) 74.21(5) 74.20(11) 80.69(10) 77.42(6)
Schiehlen 76.18(11) 70.08(12) 66.77(14) 85.75(9) 80.04(11) 73.86(9) 86.21(9) 72.29(12) 73.90(12) 80.46(11) 72.48(15)
Johansson 75.78(12)* 71.76(9) 75.08(4) 83.33(12) 76.30(14) 70.98(13) 80.29(17) 72.77(11) 71.31(13) 77.55(14) 78.46(4)
Mannem 74.54(13)* 71.55(10) 65.64(15) 84.47(11) 73.76(17) 70.68(14) 81.55(16) 71.69(13) 70.94(14) 78.67(13) 76.42(8)
Wu 73.02(14)* 66.16(14) 70.71(10) 81.44(15) 74.69(16) 66.72(16) 79.49(18) 70.63(14) 69.08(15) 78.79(12) 72.52(14)
Nguyen 72.53(15)* 63.58(16) 58.18(17) 83.23(13) 79.77(12) 72.54(11) 86.73(6) 70.42(15) 68.12(17) 75.06(16) 67.63(17)
Maes 70.66(16)* 65.12(15) 69.05(13) 79.21(16) 70.97(18) 67.38(15) 69.68(21) 68.59(16) 68.93(16) 73.63(18) 74.03(13)
Canisius 66.99(17)* 59.13(18) 63.17(16) 75.44(17) 70.45(19) 56.14(17) 77.27(19) 60.35(18) 64.31(19) 75.57(15) 68.09(16)
Jia 63.00(18)* 63.37(17) 57.61(18) 23.35(20) 76.36(13) 54.95(18) 82.93(14) 65.45(17) 66.61(18) 74.65(17) 64.68(18)
Zeman 54.87(19) 46.06(20) 50.61(20) 62.94(19) 54.49(20) 50.21(20) 53.59(22) 55.29(19) 55.24(20) 62.13(19) 58.10(19)
Marinov 54.55(20)* 54.00(19) 51.24(19) 69.42(18) 49.87(21) 53.47(19) 52.11(23) 54.33(20) 44.47(21) 59.75(20) 56.88(20)
Duan (2) 24.62(21)* 82.64(5) 86.69(7) 76.89(6)
Nash 8.65(22)* 86.49(8)
Shimizu 7.20(23) 72.02(20)
Table 2: Labeled attachment score (LAS) for the multilingual track in the CoNLL 2007 shared task. Teams
are denoted by the last name of their first member, with italics indicating that there is no corresponding
paper in the proceedings. The number in parentheses next to each score gives the rank. A star next to a score
in the Average column indicates a statistically significant difference with the next lower rank.
Team Average Arabic Basque Catalan Chinese Czech English Greek Hungarian Italian Turkish
Nakagawa 86.55(1)* 86.09(1) 81.04(5) 92.86(4) 88.88(2) 86.28(1) 90.13(2) 84.08(1) 82.49(3) 87.91(1) 85.77(3)
Nilsson 85.71(2) 85.81(2) 82.84(1) 93.12(3) 84.52(12) 83.59(4) 88.93(5) 81.22(4) 83.55(1) 87.77(2) 85.77(2)
Titov 85.62(3) 83.18(7) 81.93(2) 93.40(1) 87.91(4) 84.19(3) 89.73(4) 81.20(5) 82.18(4) 86.26(6) 86.22(1)
Sagae 85.29(4)* 84.04(4) 81.19(3) 93.34(2) 88.94(1) 81.27(8) 89.87(3) 80.37(11) 83.51(2) 87.68(3) 82.72(9)
Carreras 84.79(5) 81.48(10) 81.11(4) 92.46(5) 86.20(9) 85.16(2) 90.63(1) 81.37(3) 79.92(9) 87.19(4) 82.41(10)
Hall, J. 84.74(6)* 84.21(3) 80.61(6) 92.20(6) 87.60(5) 82.35(6) 86.77(12) 80.66(9) 81.71(6) 86.26(5) 85.04(5)
Attardi 83.96(7)* 82.53(8) 76.88(11) 91.41(7) 86.73(8) 83.40(5) 86.99(10) 80.75(8) 81.81(5) 85.54(8) 83.56(7)
Chen 83.22(8) 83.49(5) 78.65(8) 90.87(8) 85.91(10) 80.14(11) 84.91(13) 81.16(6) 79.25(11) 85.91(7) 81.92(12)
Hall, K. 83.08(9) 83.45(6) 78.55(9) 87.80(15) 87.91(3) 78.47(12) 83.21(15) 82.04(2) 79.34(10) 84.81(9) 85.18(4)
Duan (1) 82.77(10) 79.04(13) 77.59(10) 89.71(12) 86.88(7) 80.82(10) 86.97(11) 80.77(7) 80.66(7) 84.20(11) 81.03(13)
Schiehlen 82.42(11)* 81.07(11) 73.30(14) 90.79(10) 85.45(11) 81.73(7) 88.91(6) 80.47(10) 78.61(12) 84.54(10) 79.33(15)
Johansson 81.13(12)* 80.91(12) 80.43(7) 88.34(13) 81.30(15) 77.39(13) 81.43(18) 79.58(12) 75.53(15) 81.55(15) 84.80(6)
Mannem 80.30(13) 81.56(9) 72.88(15) 89.81(11) 78.84(17) 77.20(14) 82.81(16) 78.89(13) 75.39(16) 82.91(12) 82.74(8)
Nguyen 80.00(14)* 73.46(18) 69.15(18) 88.12(14) 84.05(13) 80.91(9) 88.01(7) 77.56(15) 78.13(13) 80.40(16) 80.19(14)
Jia 78.46(15) 74.20(17) 70.24(16) 90.83(9) 83.39(14) 70.41(18) 84.37(14) 75.65(16) 77.19(14) 82.36(14) 75.96(17)
Wu 78.44(16)* 77.05(14) 75.77(12) 85.85(16) 79.71(16) 73.07(16) 81.69(17) 78.12(14) 72.39(18) 82.57(13) 78.15(16)
Maes 76.60(17)* 75.47(16) 75.27(13) 84.35(17) 76.57(18) 74.03(15) 71.62(21) 75.19(17) 72.93(17) 78.32(18) 82.21(11)
Canisius 74.83(18)* 76.89(15) 70.17(17) 81.64(18) 74.81(19) 72.12(17) 78.23(19) 72.46(18) 67.80(19) 79.08(17) 75.14(18)
Zeman 62.02(19)* 58.55(20) 57.42(20) 68.50(20) 62.93(20) 59.19(20) 58.33(22) 62.89(19) 59.78(20) 68.27(19) 64.30(19)
Marinov 60.83(20)* 64.27(19) 58.55(19) 74.22(19) 56.09(21) 59.57(19) 54.33(23) 61.18(20) 50.39(21) 65.52(20) 64.13(20)
Duan (2) 25.53(21)* 86.94(6) 87.87(8) 80.53(8)
Nash 8.77(22)* 87.71(9)
Shimizu 7.79(23) 77.91(20)
Table 3: Unlabeled attachment scores (UAS) for the multilingual track in the CoNLL 2007 shared task.
Teams are denoted by the last name of their first member, with italics indicating that there is no correspond-
ing paper in the proceedings. The number in parentheses next to each score gives the rank. A star next to a
score in the Average column indicates a statistically significant difference with the next lower rank.
922
LAS UAS
Team PCHEM-c PCHEM-o PCHEM-c PCHEM-o CHILDES-c CHILDES-o
Sagae 81.06(1) 83.42(1)
Attardi 80.40(2) 83.08(3) 58.67(3)
Dredze 80.22(3) 83.38(2) 61.37(1)
Nguyen 79.50(4)* 82.04(4)*
Jia 76.48(5)* 78.92(5)* 57.43(5)
Bick 71.81(6)* 78.48(1)* 74.71(6)* 81.62(1)* 58.07(4) 62.49(1)
Shimizu 64.15(7)* 63.49(2) 71.25(7)* 70.01(2)*
Zeman 50.61(8) 54.57(8) 58.89(2)
Schneider 63.01(3)* 66.53(3)* 60.27(2)
Watson 55.47(4) 62.79(4) 45.61(3)
Wu 52.89(6)
Table 4: Labeled (LAS) and unlabeled (UAS) attachment scores for the closed (-c) and open (-o) classes of
the domain adaptation track in the CoNLL 2007 shared task. Teams are denoted by the last name of their
first member, with italics indicating that there is no corresponding paper in the proceedings. The number
in parentheses next to each score gives the rank. A star next to a score in the PCHEM columns indicates a
statistically significant difference with the next lower rank.
attachment scores for the CHILDES data set, for rea-
sons explained in section 3.2. The number in paren-
theses next to each score gives the rank. A star next
to a score indicates that the difference with the next
lower rank is significant at the 5% level using a z-
test for proportions. A more complete presentation
of the results, including the significance results for
all the tasks and their p-values, can be found on the
shared task website.4
Looking first at the results in the multilingual
track, we note that there are a number of systems
performing at almost the same level at the top of the
ranking. For the average labeled attachment score,
the difference between the top score (Nilsson) and
the fifth score (Hall, J.) is no more than half a per-
centage point, and there are generally very few sig-
nificant differences among the five or six best sys-
tems, regardless of whether we consider labeled or
unlabeled attachment score. For the closed class of
the domain adaptation track, we see a very similar
pattern, with the top system (Sagae) being followed
very closely by two other systems. For the open
class, the results are more spread out, but then there
are very few results in this class. It is also worth not-
ing that the top scores in the closed class, somewhat
unexpectedly, are higher than the top scores in the
4http://nextens.uvt.nl/depparse-wiki/AllScores
open class. But before we proceed to a more detailed
analysis of the results (section 6), we will make an
attempt to characterize the approaches represented
by the different systems.
5 Approaches
In this section we give an overview of the models,
inference methods, and learning methods used in the
participating systems. For obvious reasons the dis-
cussion is limited to systems that are described by
a paper in the proceedings. But instead of describ-
ing the systems one by one, we focus on the basic
methodological building blocks that are often found
in several systems although in different combina-
tions. For descriptions of the individual systems, we
refer to the respective papers in the proceedings.
Section 5.1 is devoted to system architectures. We
then describe the two main paradigms for learning
and inference, in this year?s shared task as well as in
last year?s, which we call transition-based parsers
(section 5.2) and graph-based parsers (section 5.3),
adopting the terminology of McDonald and Nivre
(2007).5 Finally, we give an overview of the domain
adaptation methods that were used (section 5.4).
5This distinction roughly corresponds to the distinction
made by Buchholz and Marsi (2006) between ?stepwise? and
?all-pairs? approaches.
923
5.1 Architectures
Most systems perform some amount of pre- and
post-processing, making the actual parsing compo-
nent part of a sequential workflow of varying length
and complexity. For example, most transition-
based parsers can only build projective dependency
graphs. For languages with non-projective depen-
dencies, graphs therefore need to be projectivized
for training and deprojectivized for testing (Hall et
al., 2007a; Johansson and Nugues, 2007b; Titov and
Henderson, 2007).
Instead of assigning HEAD and DEPREL in a
single step, some systems use a two-stage approach
for attaching and labeling dependencies (Chen et al,
2007; Dredze et al, 2007). In the first step unlabeled
dependencies are generated, in the second step these
are labeled. This is particularly helpful for factored
parsing models, in which label decisions cannot be
easily conditioned on larger parts of the structure
due to the increased complexity of inference. One
system (Hall et al, 2007b) extends this two-stage ap-
proach to a three-stage architecture where the parser
and labeler generate an n-best list of parses which in
turn is reranked.6
In ensemble-based systems several base parsers
provide parsing decisions, which are added together
for a combined score for each potential dependency
arc. The tree that maximizes the sum of these com-
bined scores is taken as the final output parse. This
technique is used by Sagae and Tsujii (2007) and in
the Nilsson system (Hall et al, 2007a). It is worth
noting that both these systems combine transition-
based base parsers with a graph-based method for
parser combination, as first described by Sagae and
Lavie (2006).
Data-driven grammar-based parsers, such as Bick
(2007), Schneider et al (2007), and Watson and
Briscoe (2007), need pre- and post-processing in or-
der to map the dependency graphs provided as train-
ing data to a format compatible with the grammar
used, and vice versa.
5.2 Transition-Based Parsers
Transition-based parsers build dependency graphs
by performing sequences of actions, or transitions.
Both learning and inference is conceptualized in
6They also flip the order of the labeler and the reranker.
terms of predicting the correct transition based on
the current parser state and/or history. We can fur-
ther subclassify parsers with respect to the model (or
transition system) they adopt, the inference method
they use, and the learning method they employ.
5.2.1 Models
The most common model for transition-based
parsers is one inspired by shift-reduce parsing,
where a parser state contains a stack of partially
processed tokens and a queue of remaining input
tokens, and where transitions add dependency arcs
and perform stack and queue operations. This type
of model is used by the majority of transition-based
parsers (Attardi et al, 2007; Duan et al, 2007; Hall
et al, 2007a; Johansson and Nugues, 2007b; Man-
nem, 2007; Titov and Henderson, 2007; Wu et al,
2007). Sometimes it is combined with an explicit
probability model for transition sequences, which
may be conditional (Duan et al, 2007) or generative
(Titov and Henderson, 2007).
An alternative model is based on the list-based
parsing algorithm described by Covington (2001),
which iterates over the input tokens in a sequen-
tial manner and evaluates for each preceding token
whether it can be linked to the current token or not.
This model is used by Marinov (2007) and in com-
ponent parsers of the Nilsson ensemble system (Hall
et al, 2007a). Finally, two systems use models based
on LR parsing (Sagae and Tsujii, 2007; Watson and
Briscoe, 2007).
5.2.2 Inference
The most common inference technique in transition-
based dependency parsing is greedy deterministic
search, guided by a classifier for predicting the next
transition given the current parser state and history,
processing the tokens of the sentence in sequen-
tial left-to-right order7 (Hall et al, 2007a; Mannem,
2007; Marinov, 2007; Wu et al, 2007). Optionally
multiple passes over the input are conducted until no
tokens are left unattached (Attardi et al, 2007).
As an alternative to deterministic parsing, several
parsers use probabilistic models and maintain a heap
or beam of partial transition sequences in order to
pick the most probable one at the end of the sentence
7For diversity in parser ensembles, right-to-left parsers are
also used.
924
(Duan et al, 2007; Johansson and Nugues, 2007b;
Sagae and Tsujii, 2007; Titov and Henderson, 2007).
One system uses as part of their parsing pipeline a
?neighbor-parser? that attaches adjacent words and
a ?root-parser? that identifies the root word(s) of a
sentence (Wu et al, 2007). In the case of grammar-
based parsers, a classifier is used to disambiguate
in cases where the grammar leaves some ambiguity
(Schneider et al, 2007; Watson and Briscoe, 2007)
5.2.3 Learning
Transition-based parsers either maintain a classifier
that predicts the next transition or a global proba-
bilistic model that scores a complete parse. To train
these classifiers and probabilitistic models several
approaches were used: SVMs (Duan et al, 2007;
Hall et al, 2007a; Sagae and Tsujii, 2007), modified
finite Newton SVMs (Wu et al, 2007), maximum
entropy models (Sagae and Tsujii, 2007), multiclass
averaged perceptron (Attardi et al, 2007) and max-
imum likelihood estimation (Watson and Briscoe,
2007).
In order to calculate a global score or probabil-
ity for a transition sequence, two systems used a
Markov chain approach (Duan et al, 2007; Sagae
and Tsujii, 2007). Here probabilities from the output
of a classifier are multiplied over the whole sequence
of actions. This results in a locally normalized
model. Two other entries used MIRA (Mannem,
2007) or online passive-aggressive learning (Johans-
son and Nugues, 2007b) to train a globally normal-
ized model. Titov and Henderson (2007) used an in-
cremental sigmoid Bayesian network to model the
probability of a transition sequence and estimated
model parameters using neural network learning.
5.3 Graph-Based Parsers
While transition-based parsers use training data to
learn a process for deriving dependency graphs,
graph-based parsers learn a model of what it means
to be a good dependency graph given an input sen-
tence. They define a scoring or probability function
over the set of possible parses. At learning time
they estimate parameters of this function; at pars-
ing time they search for the graph that maximizes
this function. These parsers mainly differ in the
type and structure of the scoring function (model),
the search algorithm that finds the best parse (infer-
ence), and the method to estimate the function?s pa-
rameters (learning).
5.3.1 Models
The simplest type of model is based on a sum of
local attachment scores, which themselves are cal-
culated based on the dot product of a weight vector
and a feature representation of the attachment. This
type of scoring function is often referred to as a first-
order model.8 Several systems participating in this
year?s shared task used first-order models (Schiehlen
and Spranger, 2007; Nguyen et al, 2007; Shimizu
and Nakagawa, 2007; Hall et al, 2007b). Canisius
and Tjong Kim Sang (2007) cast the same type of
arc-based factorization as a weighted constraint sat-
isfaction problem.
Carreras (2007) extends the first-order model to
incorporate a sum over scores for pairs of adjacent
arcs in the tree, yielding a second-order model. In
contrast to previous work where this was constrained
to sibling relations of the dependent (McDonald and
Pereira, 2006), here head-grandchild relations can
be taken into account.
In all of the above cases the scoring function is
decomposed into functions that score local proper-
ties (arcs, pairs of adjacent arcs) of the graph. By
contrast, the model of Nakagawa (2007) considers
global properties of the graph that can take multi-
ple arcs into account, such as multiple siblings and
children of a node.
5.3.2 Inference
Searching for the highest scoring graph (usually a
tree) in a model depends on the factorization cho-
sen and whether we are looking for projective or
non-projective trees. Maximum spanning tree al-
gorithms can be used for finding the highest scor-
ing non-projective tree in a first-order model (Hall
et al, 2007b; Nguyen et al, 2007; Canisius and
Tjong Kim Sang, 2007; Shimizu and Nakagawa,
2007), while Eisner?s dynamic programming algo-
rithm solves the problem for a first-order factoriza-
tion in the projective case (Schiehlen and Spranger,
2007). Carreras (2007) employs his own exten-
sion of Eisner?s algorithm for the case of projective
trees and second-order models that include head-
grandparent relations.
8It is also known as an edge-factored model.
925
The methods presented above are mostly efficient
and always exact. However, for models that take
global properties of the tree into account, they can-
not be applied. Instead Nakagawa (2007) uses Gibbs
sampling to obtain marginal probabilities of arcs be-
ing included in the tree using his global model and
then applies a maximum spanning tree algorithm to
maximize the sum of the logs of these marginals and
return a valid cycle-free parse.
5.3.3 Learning
Most of the graph-based parsers were trained using
an online inference-based method such as passive-
aggressive learning (Nguyen et al, 2007; Schiehlen
and Spranger, 2007), averaged perceptron (Carreras,
2007), or MIRA (Shimizu and Nakagawa, 2007),
while some systems instead used methods based on
maximum conditional likelihood (Nakagawa, 2007;
Hall et al, 2007b).
5.4 Domain Adaptation
5.4.1 Feature-Based Approaches
One way of adapting a learner to a new domain with-
out using any unlabeled data is to only include fea-
tures that are expected to transfer well (Dredze et
al., 2007). In structural correspondence learning a
transformation from features in the source domain
to features of the target domain is learnt (Shimizu
and Nakagawa, 2007). The original source features
along with their transformed versions are then used
to train a discriminative parser.
5.4.2 Ensemble-Based Approaches
Dredze et al (2007) trained a diverse set of parsers
in order to improve cross-domain performance by
incorporating their predictions as features for an-
other classifier. Similarly, two parsers trained with
different learners and search directions were used
in the co-learning approach of Sagae and Tsujii
(2007). Unlabeled target data was processed with
both parsers. Sentences that both parsers agreed on
were then added to the original training data. This
combined data set served as training data for one of
the original parsers to produce the final system. In
a similar fashion, Watson and Briscoe (2007) used a
variant of self-training to make use of the unlabeled
target data.
5.4.3 Other Approaches
Attardi et al (2007) learnt tree revision rules for the
target domain by first parsing unlabeled target data
using a strong parser; this data was then combined
with labeled source data; a weak parser was applied
to this new dataset; finally tree correction rules are
collected based on the mistakes of the weak parser
with respect to the gold data and the output of the
strong parser.
Another technique used was to filter sentences of
the out-of-domain corpus based on their similarity
to the target domain, as predicted by a classifier
(Dredze et al, 2007). Only if a sentence was judged
similar to target domain sentences was it included in
the training set.
Bick (2007) used a hybrid approach, where a data-
driven parser trained on the labeled training data was
given access to the output of a Constraint Grammar
parser for English run on the same data. Finally,
Schneider et al (2007) learnt collocations and rela-
tional nouns from the unlabeled target data and used
these in their parsing algorithm.
6 Analysis
Having discussed the major approaches taken in the
two tracks of the shared task, we will now return to
the test results. For the multilingual track, we com-
pare results across data sets and across systems, and
report results from a parser combination experiment
involving all the participating systems (section 6.1).
For the domain adaptation track, we sum up the most
important findings from the test results (section 6.2).
6.1 Multilingual Track
6.1.1 Across Data Sets
The average LAS over all systems varies from 68.07
for Basque to 80.95 for English. Top scores vary
from 76.31 for Greek to 89.61 for English. In gen-
eral, there is a good correlation between the top
scores and the average scores. For Greek, Italian,
and Turkish, the top score is closer to the average
score than the average distance, while for Czech, the
distance is higher. The languages that produced the
most stable results in terms of system ranks with re-
spect to LAS are Hungarian and Italian. For UAS,
Catalan also falls into this group. The language that
926
Setup Arabic Chinese Czech Turkish
2006 without punctuation 66.9 90.0 80.2 65.7
2007 without punctuation 75.5 84.9 80.0 71.6
2006 with punctuation 67.0 90.0 80.2 73.8
2007 with punctuation 76.5 84.7 80.2 79.8
Table 5: A comparison of the LAS top scores from 2006 and 2007. Official scoring conditions in boldface.
For Turkish, scores with punctuation also include word-internal dependencies.
produced the most unstable results with respect to
LAS is Turkish.
In comparison to last year?s languages, the lan-
guages involved in the multilingual track this year
can be more easily separated into three classes with
respect to top scores:
? Low (76.31?76.94):
Arabic, Basque, Greek
? Medium (79.19?80.21):
Czech, Hungarian, Turkish
? High (84.40?89.61):
Catalan, Chinese, English, Italian
It is interesting to see that the classes are more easily
definable via language characteristics than via char-
acteristics of the data sets. The split goes across
training set size, original data format (constituent
vs. dependency), sentence length, percentage of un-
known words, number of dependency labels, and ra-
tio of (C)POSTAGS and dependency labels. The
class with the highest top scores contains languages
with a rather impoverished morphology. Medium
scores are reached by the two agglutinative lan-
guages, Hungarian and Turkish, as well as by Czech.
The most difficult languages are those that combine
a relatively free word order with a high degree of in-
flection. Based on these characteristics, one would
expect to find Czech in the last class. However, the
Czech training set is four times the size of the train-
ing set for Arabic, which is the language with the
largest training set of the difficult languages.
However, it would be wrong to assume that train-
ing set size alone is the deciding factor. A closer
look at table 1 shows that while Basque and Greek
in fact have small training data sets, so do Turk-
ish and Italian. Another factor that may be asso-
ciated with the above classification is the percent-
age of new words (PNW) in the test set. Thus, the
expectation would be that the highly inflecting lan-
guages have a high PNW while the languages with
little morphology have a low PNW. But again, there
is no direct correspondence. Arabic, Basque, Cata-
lan, English, and Greek agree with this assumption:
Catalan and English have the smallest PNW, and
Arabic, Basque, and Greek have a high PNW. But
the PNW for Italian is higher than for Arabic and
Greek, and this is also true for the percentage of
new lemmas. Additionally, the highest PNW can be
found in Hungarian and Turkish, which reach higher
scores than Arabic, Basque, and Greek. These con-
siderations suggest that highly inflected languages
with (relatively) free word order need more training
data, a hypothesis that will have to be investigated
further.
There are four languages which were included in
the shared tasks on multilingual dependency pars-
ing both at CoNLL 2006 and at CoNLL 2007: Ara-
bic, Chinese, Czech, and Turkish. For all four lan-
guages, the same treebanks were used, which allows
a comparison of the results. However, in some cases
the size of the training set changed, and at least one
treebank, Turkish, underwent a thorough correction
phase. Table 5 shows the top scores for LAS. Since
the official scores excluded punctuation in 2006 but
includes it in 2007, we give results both with and
without punctuation for both years.
For Arabic and Turkish, we see a great improve-
ment of approximately 9 and 6 percentage points.
For Arabic, the number of tokens in the training
set doubled, and the morphological annotation was
made more informative. The combined effect of
these changes can probably account for the substan-
tial improvement in parsing accuracy. For Turkish,
the training set grew in size as well, although only by
600 sentences, but part of the improvement for Turk-
ish may also be due to continuing efforts in error cor-
927
rection and consistency checking. We see that the
choice to include punctuation or not makes a large
difference for the Turkish scores, since non-final IGs
of a word are counted as punctuation (because they
have the underscore character as their FORM value),
which means that word-internal dependency links
are included if punctuation is included.9 However,
regardless of whether we compare scores with or
without punctuation, we see a genuine improvement
of approximately 6 percentage points.
For Chinese, the same training set was used.
Therefore, the drop from last year?s top score to this
year?s is surprising. However, last year?s top scor-
ing system for Chinese (Riedel et al, 2006), which
did not participate this year, had a score that was
more than 3 percentage points higher than the sec-
ond best system for Chinese. Thus, if we compare
this year?s results to the second best system, the dif-
ference is approximately 2 percentage points. This
final difference may be attributed to the properties of
the test sets. While last year?s test set was taken from
the treebank, this year?s test set contains texts from
other sources. The selection of the textual basis also
significantly changed average sentence length: The
Chinese training set has an average sentence length
of 5.9. Last year?s test set alo had an average sen-
tence length of 5.9. However, this year, the average
sentence length is 7.5 tokens, which is a significant
increase. Longer sentences are typically harder to
parse due to the increased likelihood of ambiguous
constructions.
Finally, we note that the performance for Czech
is almost exactly the same as last year, despite the
fact that the size of the training set has been reduced
to approximately one third of last year?s training set.
It is likely that this in fact represents a relative im-
provement compared to last year?s results.
6.1.2 Across Systems
The LAS over all languages ranges from 80.32 to
54.55. The comparison of the system ranks aver-
aged over all languages with the ranks for single lan-
9The decision to include word-internal dependencies in this
way can be debated on the grounds that they can be parsed de-
terministically. On the other hand, they typically correspond to
regular dependencies captured by function words in other lan-
guages, which are often easy to parse as well. It is therefore
unclear whether scores are more inflated by including word-
internal dependencies or deflated by excluding them.
guages show considerably more variation than last
year?s systems. Buchholz and Marsi (2006) report
that ?[f]or most parsers, their ranking differs at most
a few places from their overall ranking?. This year,
for all of the ten best performing systems with re-
spect to LAS, there is at least one language for which
their rank is at least 5 places different from their
overall rank. The most extreme case is the top per-
forming Nilsson system (Hall et al, 2007a), which
reached rank 1 for five languages and rank 2 for
two more languages. Their only outlier is for Chi-
nese, where the system occupies rank 14, with a
LAS approximately 9 percentage points below the
top scoring system for Chinese (Sagae and Tsujii,
2007). However, Hall et al (2007a) point out that
the official results for Chinese contained a bug, and
the true performance of their system was actually
much higher. The greatest improvement of a sys-
tem with respect to its average rank occurs for En-
glish, for which the system by Nguyen et al (2007)
improved from the average rank 15 to rank 6. Two
more outliers can be observed in the system of Jo-
hansson and Nugues (2007b), which improves from
its average rank 12 to rank 4 for Basque and Turkish.
The authors attribute this high performance to their
parser?s good performance on small training sets.
However, this hypothesis is contradicted by their re-
sults for Greek and Italian, the other two languages
with small training sets. For these two languages,
the system?s rank is very close to its average rank.
6.1.3 An Experiment in System Combination
Having the outputs of many diverse dependency
parsers for standard data sets opens up the interest-
ing possibility of parser combination. To combine
the outputs of each parser we used the method of
Sagae and Lavie (2006). This technique assigns to
each possible labeled dependency a weight that is
equal to the number of systems that included the de-
pendency in their output. This can be viewed as
an arc-based voting scheme. Using these weights
it is possible to search the space of possible depen-
dency trees using directed maximum spanning tree
algorithms (McDonald et al, 2005). The maximum
spanning tree in this case is equal to the tree that on
average contains the labeled dependencies that most
systems voted for. It is worth noting that variants
of this scheme were used in two of the participating
928
5 10 15 20Number of Systems
80
82
84
86
88
Accu
racy
Unlabeled AccuracyLabeled Accuracy
Figure 1: System Combination
systems, the Nilsson system (Hall et al, 2007a) and
the system of Sagae and Tsujii (2007).
Figure 1 plots the labeled and unlabeled accura-
cies when combining an increasing number of sys-
tems. The data used in the plot was the output of all
competing systems for every language in the mul-
tilingual track. The plot was constructed by sort-
ing the systems based on their average labeled accu-
racy scores over all languages, and then incremen-
tally adding each system in descending order.10 We
can see that both labeled and unlabeled accuracy are
significantly increased, even when just the top three
systems are included. Accuracy begins to degrade
gracefully after about ten different parsers have been
added. Furthermore, the accuracy never falls below
the performance of the top three systems.
6.2 Domain Adaptation Track
For this task, the results are rather surprising. A look
at the LAS and UAS for the chemical research ab-
stracts shows that there are four closed systems that
outperform the best scoring open system. The best
system (Sagae and Tsujii, 2007) reaches an LAS of
81.06 (in comparison to their LAS of 89.01 for the
English data set in the multilingual track). Consider-
ing that approximately one third of the words of the
chemical test set are new, the results are noteworthy.
The next surprise is to be found in the relatively
low UAS for the CHILDES data. At a first glance,
this data set has all the characteristics of an easy
10The reason that there is no data point for two parsers is
that the simple voting scheme adopted only makes sense with at
least three parsers voting.
set; the average sentence is short (12.9 words), and
the percentage of new words is also small (6.10%).
Despite these characteristics, the top UAS reaches
62.49 and is thus more than 10 percentage points
below the top UAS for the chemical data set. One
major reason for this is that auxiliary and main
verb dependencies are annotated differently in the
CHILDES data than in the WSJ training set. As a
result of this discrepancy, participants were not re-
quired to submit results for the CHILDES data. The
best performing system on the CHILDES corpus is
an open system (Bick, 2007), but the distance to
the top closed system is approximately 1 percent-
age point. In this domain, it seems more feasible to
use general language resources than for the chemi-
cal domain. However, the results prove that the extra
effort may be unnecessary.
7 Conclusion
Two years of dependency parsing in the CoNLL
shared task has brought an enormous boost to the
development of dependency parsers for multiple lan-
guages (and to some extent for multiple domains).
But even though nineteen languages have been cov-
ered by almost as many different parsing and learn-
ing approaches, we still have only vague ideas about
the strengths and weaknesses of different methods
for languages with different typological characteris-
tics. Increasing our knowledge of the multi-causal
relationship between language structure, annotation
scheme, and parsing and learning methods probably
remains the most important direction for future re-
search in this area. The outputs of all systems for all
data sets from the two shared tasks are freely avail-
able for research and constitute a potential gold mine
for comparative error analysis across languages and
systems.
For domain adaptation we have barely scratched
the surface so far. But overcoming the bottleneck
of limited annotated resources for specialized do-
mains will be as important for the deployment of
human language technology as being able to handle
multiple languages in the future. One result from
the domain adaptation track that may seem surpris-
ing at first is the fact that closed class systems out-
performed open class systems on the chemical ab-
stracts. However, it seems that the major problem in
929
adapting pre-existing parsers to the new domain was
not the domain as such but the mapping from the
native output of the parser to the kind of annotation
provided in the shared task data sets. Thus, find-
ing ways of reusing already invested development
efforts by adapting the outputs of existing systems
to new requirements, without substantial loss in ac-
curacy, seems to be another line of research that may
be worth pursuing.
Acknowledgments
First and foremost, we want to thank all the peo-
ple and organizations that generously provided us
with treebank data and helped us prepare the data
sets and without whom the shared task would have
been literally impossible: Otakar Smrz, Charles
University, and the LDC (Arabic); Maxux Aranz-
abe, Kepa Bengoetxea, Larraitz Uria, Koldo Go-
jenola, and the University of the Basque Coun-
try (Basque); Ma. Anto`nia Mart?? Anton??n, Llu??s
Ma`rquez, Manuel Bertran, Mariona Taule?, Difda
Monterde, Eli Comelles, and CLiC-UB (Cata-
lan); Shih-Min Li, Keh-Jiann Chen, Yu-Ming
Hsieh, and Academia Sinica (Chinese); Jan Hajic?,
Zdenek Zabokrtsky, Charles University, and the
LDC (Czech); Brian MacWhinney, Eric Davis, the
CHILDES project, the Penn BioIE project, and
the LDC (English); Prokopis Prokopidis and ILSP
(Greek); Csirik Ja?nos and Zolta?n Alexin (Hun-
garian); Giuseppe Attardi, Simonetta Montemagni,
Maria Simi, Isidoro Barraco, Patrizia Topi, Kiril
Ribarov, Alessandro Lenci, Nicoletta Calzolari,
ILC, and ELRA (Italian); Gu?ls?en Eryig?it, Kemal
Oflazer, and Ruket C?ak?c? (Turkish).
Secondly, we want to thank the organizers of last
year?s shared task, Sabine Buchholz, Amit Dubey,
Erwin Marsi, and Yuval Krymolowski, who solved
all the really hard problems for us and answered all
our questions, as well as our colleagues who helped
review papers: Jason Baldridge, Sabine Buchholz,
James Clarke, Gu?ls?en Eryig?it, Kilian Evang, Ju-
lia Hockenmaier, Yuval Krymolowski, Erwin Marsi,
Bea?ta Megyesi, Yannick Versley, and Alexander
Yeh. Special thanks to Bertjan Busser and Erwin
Marsi for help with the CoNLL shared task website
and many other things, and to Richard Johansson for
letting us use his conversion tool for English.
Thirdly, we want to thank the program chairs
for EMNLP-CoNLL 2007, Jason Eisner and Taku
Kudo, the publications chair, Eric Ringger, the
SIGNLL officers, Antal van den Bosch, Hwee Tou
Ng, and Erik Tjong Kim Sang, and members of the
LDC staff, Tony Castelletto and Ilya Ahtaridis, for
great cooperation and support.
Finally, we want to thank the following people,
who in different ways assisted us in the organi-
zation of the CoNLL 2007 shared task: Giuseppe
Attardi, Eckhard Bick, Matthias Buch-Kromann,
Xavier Carreras, Tomaz Erjavec, Svetoslav Mari-
nov, Wolfgang Menzel, Xue Nianwen, Gertjan van
Noord, Petya Osenova, Florian Schiel, Kiril Simov,
Zdenka Uresova, and Heike Zinsmeister.
References
A. Abeille?, editor. 2003. Treebanks: Building and Using
Parsed Corpora. Kluwer.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. Diaz de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency treebank.
In Proc. of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT).
G. Attardi, F. Dell?Orletta, M. Simi, A. Chanev, and
M. Ciaramita. 2007. Multilingual dependency pars-
ing and domain adaptation using desr. In Proc. of the
CoNLL 2007 Shared Task. EMNLP-CoNLL.
E. Bick. 2007. Hybrid ways to improve domain inde-
pendence in an ML dependency parser. In Proc. of the
CoNLL 2007 Shared Task. EMNLP-CoNLL.
J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain
adaptation with structural correspondence learning. In
Proc. of the Conf. on Empirical Methods in Natural
Language Processing (EMNLP).
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003.
The PDT: a 3-level annotation scenario. In Abeille?
(2003), chapter 7, pages 103?127.
R. Brown. 1973. A First Language: The Early Stages.
Harvard University Press.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
the Tenth Conf. on Computational Natural Language
Learning (CoNLL).
S. Canisius and E. Tjong Kim Sang. 2007. A constraint
satisfaction approach to dependency parsing. In Proc.
of the CoNLL 2007 Shared Task. EMNLP-CoNLL.
930
X. Carreras. 2007. Experiments with a high-order pro-
jective dependency parser. In Proc. of the CoNLL 2007
Shared Task. EMNLP-CoNLL.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proc. of the First Meeting of the North American
Chapter of the Association for Computational Linguis-
tics (NAACL).
C. Chelba and A. Acero. 2004. Adaptation of maxi-
mum entropy capitalizer: Little data can help a lot. In
Proc. of the Conf. on Empirical Methods in Natural
Language Processing (EMNLP).
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica treebank: Design criteria,
representational issues and implementation. In Abeille?
(2003), chapter 13, pages 231?248.
W. Chen, Y. Zhang, and H. Isahara. 2007. A two-stage
parser for multilingual dependency parsing. In Proc.
of the CoNLL 2007 Shared Task. EMNLP-CoNLL.
M. Collins. 1997. Three generative, lexicalised mod-
els for statistical parsing. In Proc. of the 35th Annual
Meeting of the Association for Computational Linguis-
tics (ACL).
M. A. Covington. 2001. A fundamental algorithm for
dependency parsing. In Proc. of the 39th Annual ACM
Southeast Conf.
D. Csendes, J. Csirik, T. Gyimo?thy, and A. Kocsor. 2005.
The Szeged Treebank. Springer.
H. Daume? and D. Marcu. 2006. Domain adaptation for
statistical classifiers. Journal of Artificial Intelligence
Research, 26:101?126.
M. Dredze, J. Blitzer, P. P. Talukdar, K. Ganchev,
J. Graca, and F. Pereira. 2007. Frustratingly hard do-
main adaptation for dependency parsing. In Proc. of
the CoNLL 2007 Shared Task. EMNLP-CoNLL.
X. Duan, J. Zhao, and B. Xu. 2007. Probabilistic parsing
action models for multi-lingual dependency parsing.
In Proc. of the CoNLL 2007 Shared Task. EMNLP-
CoNLL.
G. Eryig?it. 2007. ITU validation set for Metu-Sabanc?
Turkish Treebank. URL: http://www3.itu.edu.tr/
?gulsenc/papers/validationset.pdf.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-
hatla, A. Luo, N. Nicolov, and S. Roukos. 2004. A
statisical model for multilingual entity detection and
tracking. In Proc. of the Human Language Technology
Conf. and the Annual Meeting of the North American
Chapter of the Association for Computational Linguis-
tics (HLT/NAACL).
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In Proc. of the Conf. on Empirical Methods in
Natural Language Processing (EMNLP).
J. Hajic?, O. Smrz?, P. Zema?nek, J. S?naidauf, and E. Bes?ka.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools.
J. Hall, J. Nilsson, J. Nivre, G. Eryig?it, B. Megyesi,
M. Nilsson, and M. Saers. 2007a. Single malt or
blended? A study in multilingual parser optimization.
In Proc. of the CoNLL 2007 Shared Task. EMNLP-
CoNLL.
K. Hall, J. Havelka, and D. Smith. 2007b. Log-linear
models of non-projective trees, k-best MST parsing
and tree-ranking. In Proc. of the CoNLL 2007 Shared
Task. EMNLP-CoNLL.
R. Johansson and P. Nugues. 2007a. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conf. on Computational Lin-
guistics (NODALIDA).
R. Johansson and P. Nugues. 2007b. Incremental depen-
dency parsing using online learning. In Proc. of the
CoNLL 2007 Shared Task. EMNLP-CoNLL.
T. Kudo and Y. Matsumoto. 2002. Japanese dependency
analysis using cascaded chunking. In Proc. of the Sixth
Conf. on Computational Language Learning (CoNLL).
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc-
Donald, M. Palmer, A. Schein, and L. Ungar. 2004.
Integrated annotation for biomedical information ex-
traction. In Proc. of the Human Language Technology
Conf. and the Annual Meeting of the North American
Chapter of the Association for Computational Linguis-
tics (HLT/NAACL).
B. MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Erlbaum.
P. R. Mannem. 2007. Online learning for determinis-
tic dependency parsing. In Proc. of the CoNLL 2007
Shared Task. EMNLP-CoNLL.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
S. Marinov. 2007. Covington variations. In Proc. of the
CoNLL 2007 Shared Task. EMNLP-CoNLL.
M. A. Mart??, M. Taule?, L. Ma`rquez, and M. Bertran.
2007. CESS-ECE: A multilingual and multilevel
annotated corpus. Available for download from:
http://www.lsi.upc.edu/?mbertran/cess-ece/.
931
D. McClosky, E. Charniak, and M. Johnson. 2006.
Reranking and self-training for parser adaptation. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meeting
of the Association for Computational Linguistics.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proc. of the Joint Conf. on Empirical Methods in Nat-
ural Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL).
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Proc.
of the 11th Conf. of the European Chapter of the Asso-
ciation for Computational Linguistics (EACL).
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. of the Human Language
Technology Conf. and the Conf. on Empirical Methods
in Natural Language Processing (HLT/EMNLP).
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari,
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli,
M. Massetani, R. Raffaelli, R. Basili, M. T. Pazienza,
D. Saracino, F. Zanzotto, N. Nana, F. Pianesi, and
R. Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeille? (2003), chapter 11,
pages 189?210.
T. Nakagawa. 2007. Multilingual dependency parsing
using Gibbs sampling. In Proc. of the CoNLL 2007
Shared Task. EMNLP-CoNLL.
L.-M. Nguyen, T.-P. Nguyen, and A. Shimazu. 2007. A
multilingual dependency analysis system using online
passive-aggressive learning. In Proc. of the CoNLL
2007 Shared Task. EMNLP-CoNLL.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryig?it,
S. Ku?bler, S. Marinov, and E. Marsi. 2007. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language Engi-
neering, 13:95?135.
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r.
2003. Building a Turkish treebank. In Abeille? (2003),
chapter 15, pages 261?277.
P. Prokopidis, E. Desypri, M. Koutsombogera, H. Papa-
georgiou, and S. Piperidis. 2005. Theoretical and
practical issues in the construction of a Greek depen-
dency treebank. In Proc. of the 4th Workshop on Tree-
banks and Linguistic Theories (TLT).
S. Riedel, R. C?ak?c?, and I. Meza-Ruiz. 2006. Multi-
lingual dependency parsing with incremental integer
linear programming. In Proc. of the Tenth Conf. on
Computational Natural Language Learning (CoNLL).
B. Roark and M. Bacchiani. 2003. Supervised and un-
supervised PCFG adaptation to novel domains. In
Proc. of the Human Language Technology Conf. and
the Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(HLT/NAACL).
K. Sagae and A. Lavie. 2006. Parser combination by
reparsing. In Proc. of the Human Language Technol-
ogy Conf. of the North American Chapter of the Asso-
ciation of Computational Linguistics (HLT/NAACL).
K. Sagae and J. Tsujii. 2007. Dependency parsing
and domain adaptation with LR models and parser en-
sembles. In Proc. of the CoNLL 2007 Shared Task.
EMNLP-CoNLL.
M. Schiehlen and Kristina Spranger. 2007. Global learn-
ing of labelled dependency trees. In Proc. of the
CoNLL 2007 Shared Task. EMNLP-CoNLL.
G. Schneider, K. Kaljurand, F. Rinaldi, and T. Kuhn.
2007. Pro3Gres parser in the CoNLL domain adap-
tation shared task. In Proc. of the CoNLL 2007 Shared
Task. EMNLP-CoNLL.
N. Shimizu and H. Nakagawa. 2007. Structural corre-
spondence learning for dependency parsing. In Proc.
of the CoNLL 2007 Shared Task. EMNLP-CoNLL.
I. Titov and J. Henderson. 2006. Porting statistical
parsers with data-defined kernels. In Proc. of the Tenth
Conf. on Computational Natural Language Learning
(CoNLL).
I. Titov and J. Henderson. 2007. Fast and robust mul-
tilingual dependency parsing with a generative latent
variable model. In Proc. of the CoNLL 2007 Shared
Task. EMNLP-CoNLL.
R. Watson and T. Briscoe. 2007. Adapting the RASP
system for the CoNLL07 domain-adaptation task. In
Proc. of the CoNLL 2007 Shared Task. EMNLP-
CoNLL.
Y.-C. Wu, J.-C. Yang, and Y.-S. Lee. 2007. Multi-
lingual deterministic dependency parsing framework
using modified finite Newton method support vector
machines. In Proc. of the CoNLL 2007 Shared Task.
EMNLP-CoNLL.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
8th International Workshop on Parsing Technologies
(IWPT).
932
Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 933?939,
Prague, June 2007. c?2007 Association for Computational Linguistics
Single Malt or Blended? A Study in Multilingual Parser Optimization
Johan Hall? Jens Nilsson? Joakim Nivre??
Gu?ls?en Eryig?it? Bea?ta Megyesi? Mattias Nilsson? Markus Saers?
?Va?xjo? University, School of Mathematics and Systems Engineering
E-mail: firstname.lastname@vxu.se
?Uppsala University, Dept. of Linguistics and Philology
E-mail: firstname.lastname@lingfil.uu.se
?Istanbul Technical University, Computer Engineering Dept.
E-mail: gulsen.cebiroglu@itu.edu.tr
Abstract
We describe a two-stage optimization of the
MaltParser system for the ten languages in
the multilingual track of the CoNLL 2007
shared task on dependency parsing. The
first stage consists in tuning a single-parser
system for each language by optimizing pa-
rameters of the parsing algorithm, the fea-
ture model, and the learning algorithm. The
second stage consists in building an ensem-
ble system that combines six different pars-
ing strategies, extrapolating from the opti-
mal parameters settings for each language.
When evaluated on the official test sets, the
ensemble system significantly outperforms
the single-parser system and achieves the
highest average labeled attachment score.
1 Introduction
In the multilingual track of the CoNLL 2007 shared
task on dependency parsing, a single parser must be
trained to handle data from ten different languages:
Arabic (Hajic? et al, 2004), Basque (Aduriz et al,
2003), Catalan, (Mart?? et al, 2007), Chinese (Chen
et al, 2003), Czech (Bo?hmova? et al, 2003), English
(Marcus et al, 1993; Johansson and Nugues, 2007),
Greek (Prokopidis et al, 2005), Hungarian (Csendes
et al, 2005), Italian (Montemagni et al, 2003), and
Turkish (Oflazer et al, 2003).1 Our contribution is
a study in multilingual parser optimization using the
freely available MaltParser system, which performs
1For more information about the task and the data sets, see
Nivre et al (2007).
deterministic, classifier-based parsing with history-
based feature models and discriminative learning,
and which was one of the top performing systems
in the CoNLL 2006 shared task (Nivre et al, 2006).
In order to maximize parsing accuracy, optimiza-
tion has been carried out in two stages, leading to
two different, but related parsers. The first of these is
a single-parser system, similar to the one described
in Nivre et al (2006), which parses a sentence deter-
ministically in a single left-to-right pass, with post-
processing to recover non-projective dependencies,
and where the parameters of the MaltParser system
have been tuned for each language separately. We
call this system Single Malt, to emphasize the fact
that it consists of a single instance of MaltParser.
The second parser is an ensemble system, which
combines the output of six deterministic parsers,
each of which is a variation of the Single Malt parser
with parameter settings extrapolated from the first
stage of optimization. It seems very natural to call
this system Blended.
Section 2 summarizes the work done to optimize
the Single Malt parser, while section 3 explains how
the Blended parser was constructed from the Single
Malt parser. Section 4 gives a brief analysis of the
experimental results, and section 5 concludes.
2 The Single Malt Parser
The parameters available in the MaltParser system
can be divided into three groups: parsing algorithm
parameters, feature model parameters, and learn-
ing algorithm parameters.2 Our overall optimization
2For a complete documentation of these parameters, see
http://w3.msi.vxu.se/users/nivre/research/MaltParser.html.
933
strategy for the Single Malt parser was as follows:
1. Define a good baseline system with the same
parameter settings for all languages.
2. Tune parsing algorithm parameters once and
for all for each language (with baseline settings
for feature model and learning algorithm pa-
rameters).
3. Optimize feature model and learning algorithm
parameters in an interleaved fashion for each
language.
We used nine-fold cross-validation on 90% of the
training data for all languages with a training set size
smaller than 300,000 tokens and an 80%?10% train-
devtest split for the remaining languages (Catalan,
Chinese, Czech, English). The remaining 10% of
the data was in both cases saved for a final dry run,
where the parser was trained on 90% of the data for
each language and tested on the remaining (fresh)
10%. We consistently used the labeled attachment
score (LAS) as the single optimization criterion.
Below we describe the most important parameters
in each group, define baseline settings, and report
notable improvements for different languages during
development. The improvements for each language
from step 1 (baseline) to step 2 (parsing algorithm)
and step 3 (feature model and learning algorithm)
can be tracked in table 1.3
2.1 Parsing Algorithm
MaltParser implements several parsing algorithms,
but for the Single Malt system we stick to the one
used by Nivre et al (2006), which performs labeled
projective dependency parsing in linear time, using a
stack to store partially processed tokens and an input
queue of remaining tokens. There are three basic
parameters that can be varied for this algorithm:
1. Arc order: The baseline algorithm is arc-
eager, in the sense that right dependents are
attached to their head as soon as possible, but
there is also an arc-standard version, where the
attachment of right dependents has to be post-
poned until they have found all their own de-
pendents. The arc-standard order was found
3Complete specifications of all parameter settings for all
languages, for both Single Malt and Blended, are available at
http://w3.msi.vxu.se/users/jha/conll07/.
to improve parsing accuracy for Chinese, while
the arc-eager order was maintained for all other
languages.
2. Stack initialization: In the baseline version
the parser is initialized with an artificial root
node (with token id 0) on the stack, so that arcs
originating from the root can be added explic-
itly during parsing. But it is also possible to ini-
tialize the parser with an empty stack, in which
case arcs from the root are only added implic-
itly (to any token that remains a root after pars-
ing is completed). Empty stack initialization
(which reduces the amount of nondeterminism
in parsing) led to improved accuracy for Cata-
lan, Chinese, Hungarian, Italian and Turkish.4
3. Post-processing: The baseline parser performs
a single left-to-right pass over the input, but it
is possible to allow a second pass where only
unattached tokens are processed.5 Such post-
processing was found to improve results for
Basque, Catalan, Czech, Greek and Hungarian.
Since the parsing algorithm only produces projective
dependency graphs, we may use pseudo-projective
parsing to recover non-projective dependencies, i.e.,
projectivize training data and encode information
about these transformations in extended arc labels
to support deprojectivization of the parser output
(Nivre and Nilsson, 2005). Pseudo-projective pars-
ing was found to have a positive effect on over-
all parsing accuracy only for Basque, Czech, Greek
and Turkish. This result can probably be explained
in terms of the frequency of non-projective depen-
dencies in the different languages. For Basque,
Czech, Greek and Turkish, more than 20% of the
sentences have non-projective dependency graphs;
for all the remaining languages the corresponding
4For Arabic, Basque, Czech, and Greek, the lack of im-
provement can be explained by the fact that these data sets allow
more than one label for dependencies from the artificial root.
With empty stack initialization all such dependencies are as-
signed a default label, which leads to a drop in labeled attach-
ment score. For English, however, empty stack initialization did
not improve accuracy despite the fact that dependencies from
the artificial root have a unique label.
5This technique is similar to the one used by Yamada and
Matsumoto (2003), but with only a single post-processing pass
parsing complexity remains linear in string length.
934
Attributes
Tokens FORM LEMMA CPOSTAG POSTAG FEATS DEPREL
S: Top + + + + + +
S: Top?1 +
I: Next + + + + +
I: Next+1 + +
I: Next+2 +
I: Next+3 +
G: Head of Top +
G: Leftmost dependent of Top +
G: Rightmost dependent of Top +
G: Leftmost dependent of Next +
Figure 1: Baseline feature model (S = Stack, I = Input, G = Graph).
figure is 10% or less.6
The cumulative improvement after optimization
of parsing algorithm parameters was a modest 0.32
percentage points on average over all ten languages,
with a minimum of 0.00 (Arabic, English) and a
maximum of 0.83 (Czech) (cf. table 1).
2.2 Feature Model
MaltParser uses a history-based feature model for
predicting the next parsing action. Each feature of
this model is an attribute of a token defined relative
to the current stack S, input queue I, or partially built
dependency graph G, where the attribute can be any
of the symbolic input attributes in the CoNLL for-
mat: FORM, LEMMA, CPOSTAG, POSTAG and
FEATS (split into atomic attributes), as well as the
DEPREL attribute of tokens in the graph G. The
baseline feature model is depicted in figure 1, where
rows denote tokens, columns denote attributes, and
each cell containing a plus sign represents a model
feature.7 This model is an extrapolation from many
previous experiments on different languages and
usually represents a good starting point for further
optimization.
The baseline model was tuned for each of the ten
languages using both forward and backward feature
6In fact, for Arabic, which has about 10% sentences with
non-projective dependencies, it was later found that, with an
optimized feature model, it is beneficial to projectivize the train-
ing data without trying to recover non-projective dependencies
in the parser output. This was also the setting that was used for
Arabic in the dry run and final test.
7The names Top and Next refer to the token on top of the
stack S and the first token in the remaining input I, respectively.
selection. The total number of features in the tuned
models varies from 18 (Turkish) to 56 (Hungarian)
but is typically between 20 and 30. This feature se-
lection process constituted the major development
effort for the Single Malt parser and also gave the
greatest improvements in parsing accuracy, but since
feature selection was to some extent interleaved with
learning algorithm optimization, we only report the
cumulative effect of both together in table 1.
2.3 Learning Algorithm
MaltParser supports several learning algorithms but
the best results have so far been obtained with sup-
port vector machines, using the LIBSVM package
(Chang and Lin, 2001). We use a quadratic kernel
K(xi, xj) = (?xTi xj + r)
2 and LIBSVM?s built-
in one-versus-one strategy for multi-class classifica-
tion, converting symbolic features to numerical ones
using the standard technique of binarization. As our
baseline settings, we used ? = 0.2 and r = 0 for
the kernel parameters, C = 0.5 for the penalty para-
meter, and ? = 1.0 for the termination criterion. In
order to reduce training times during development,
we also split the training data for each language into
smaller sets and trained separate multi-class classi-
fiers for each set, using the POSTAG of Next as the
defining feature for the split.
The time spent on optimizing learning algorithm
parameters varies between languages, mainly due
to lack of time. For Arabic, Basque, and Catalan,
the baseline settings were used also in the dry run
and final test. For Chinese, Greek and Hungarian,
935
Development Dry Run Test Test: UAS
Language Base PA F+L SM B SM B SM B
Arabic 70.31 70.31 71.67 70.93 73.09 74.75 76.52 84.21 85.81
Basque 73.86 74.44 76.99 77.18 80.12 74.97 76.92 80.61 82.84
Catalan 85.43 85.51 86.88 86.65 88.00 87.74 88.70 92.20 93.12
Chinese 83.85 84.39 87.64 87.61 88.61 83.51 84.67 87.60 88.70
Czech 75.00 75.83 77.74 77.91 82.17 77.22 77.98 82.35 83.59
English 85.44 85.44 86.35 86.35 88.74 85.81 88.11 86.77 88.93
Greek 72.67 73.04 74.42 74.89 78.17 74.21 74.65 80.66 81.22
Hungarian 74.62 74.64 77.40 77.81 80.04 78.09 80.27 81.71 83.55
Italian 81.42 81.64 82.50 83.37 85.16 82.48 84.40 86.26 87.77
Turkish 75.12 75.80 76.49 75.87 77.09 79.24 79.79 85.04 85.77
Average 77.78 78.10 79.81 79.86 82.12 79.80 81.20 84.74 86.13
Table 1: Development results for Single Malt (Base = baseline, PA = parsing algorithm, F+L = feature model
and learning algorithm); dry run and test results for Single Malt (SM) and Blended (B) (with corrected test
scores for Blended on Chinese). All scores are labeled attachment scores (LAS) except the last two columns,
which report unlabeled attachment scores (UAS) on the test sets.
slightly better results were obtained by not splitting
the training data into smaller sets; for the remain-
ing languages, accuracy was improved by using the
CPOSTAG of Next as the defining feature for the
split (instead of POSTAG). With respect to the SVM
parameters (?, r, C, and ?), Arabic, Basque, Cata-
lan, Greek and Hungarian retain the baseline set-
tings, while the other languages have slightly dif-
ferent values for some parameters.
The cumulative improvement after optimization
of feature model and learning algorithm parameters
was 1.71 percentage points on average over all ten
languages, with a minimum of 0.69 (Turkish) and a
maximum of 3.25 (Chinese) (cf. table 1).
3 The Blended Parser
The Blended parser is an ensemble system based
on the methodology proposed by Sagae and Lavie
(2006). Given the output dependency graphs Gi
(1 ? i ? m) of m different parsers for an input sen-
tence x, we construct a new graph containing all the
labeled dependency arcs proposed by some parser
and weight each arc a by a score s(a) reflecting its
popularity among the m parsers. The output of the
ensemble system for x is the maximum spanning
tree of this graph (rooted at the node 0), which can
be extracted using the Chu-Liu-Edmonds algorithm,
as shown by McDonald et al (2005). Following
Sagae and Lavie (2006), we let s(a) =
?m
i=1 w
c
iai,
where wci is the average labeled attachment score of
parser i for the word class c8 of the dependent of a,
and ai is 1 if a ? Gi and 0 otherwise.
The Blended parser uses six component parsers,
with three different parsing algorithms, each of
which is used to construct one left-to-right parser
and one right-to-left parser. The parsing algorithms
used are the arc-eager baseline algorithm, the arc-
standard variant of the baseline algorithm, and the
incremental, non-projective parsing algorithm first
described by Covington (2001) and recently used
for deterministic classifier-based parsing by Nivre
(2007), all of which are available in MaltParser.
Thus, the six component parsers for each language
were instances of the following:
1. Arc-eager projective left-to-right
2. Arc-eager projective right-to-left
3. Arc-standard projective left-to-right
4. Arc-standard projective right-to-left
5. Covington non-projective left-to-right
6. Covington non-projective right-to-left
8We use CPOSTAG to determine the part of speech.
936
root 1 2 3?6 7+
Parser R P R P R P R P R P
Single Malt 87.01 80.36 95.08 94.87 86.28 86.67 77.97 80.23 68.98 71.06
Blended 92.09 74.20 95.71 94.92 87.55 88.12 78.66 83.02 65.29 78.14
Table 2: Recall (R) and precision (P) of Single Malt and Blended for dependencies of different length,
averaged over all languages (root = dependents of root node, regardless of length).
The final Blended parser was constructed by reusing
the tuned Single Malt parser for each language (arc-
standard left-to-right for Chinese, arc-eager left-to-
right for the remaining languages) and training five
additional parsers with the same parameter settings
except for the following mechanical adjustments:
1. Pseudo-projective parsing was not used for the
two non-projective parsers.
2. Feature models were adjusted with respect to
the most obvious differences in parsing strategy
(e.g., by deleting features that could never be
informative for a given parser).
3. Learning algorithm parameters were adjusted
to speed up training (e.g., by always splitting
the training data into smaller sets).
Having trained all parsers on 90% of the training
data for each language, the weights wci for each
parser i and coarse part of speech c was determined
by the labeled attachment score on the remaining
10% of the data. This means that the results obtained
in the dry run were bound to be overly optimistic for
the Blended parser, since it was then evaluated on
the same data set that was used to tune the weights.
Finally, we want to emphasize that the time for
developing the Blended parser was severely limited,
which means that several shortcuts had to be taken,
such as optimizing learning algorithm parameters
for speed rather than accuracy and using extrapo-
lation, rather than proper tuning, for other impor-
tant parameters. This probably means that the per-
formance of the Blended system can be improved
considerably by optimizing parameters for all six
parsers separately.
4 Results and Discussion
Table 1 shows the labeled attachment score results
from our internal dry run (training on 90% of the
training data, testing on the remaining 10%) and the
official test runs for both of our systems. It should
be pointed out that the test score for the Blended
parser on Chinese is different from the official one
(75.82), which was much lower than expected due
to a corrupted specification file required by Malt-
Parser. Restoring this file and rerunning the parser
on the Chinese test set, without retraining the parser
or changing any parameter settings, resulted in the
score reported here. This also improved the aver-
age score from 80.32 to 81.20, the former being the
highest reported official score.
For the Single Malt parser, the test results are on
average very close to the dry run results, indicating
that models have not been overfitted (although there
is considerably variation between languages). For
the Blended parser, there is a drop of almost one
percentage point, which can be explained by the fact
that weights could not be tuned on held-out data for
the dry run (as explained in section 3).
Comparing the results for different languages, we
see a tendency that languages with rich morphology,
usually accompanied by flexible word order, get
lower scores. Thus, the labeled attachment score is
below 80% for Arabic, Basque, Czech, Greek, Hun-
garian, and Turkish. By comparison, the more con-
figurational languages (Catalan, Chinese, English,
and Italian) all have scores above 80%. Linguis-
tic properties thus seem to be more important than,
for example, training set size, which can be seen by
comparing the results for Italian, with one of the
smallest training sets, and Czech, with one of the
largest. The development of parsing methods that
are better suited for morphologically rich languages
with flexible word order appears as one of the most
important goals for future research in this area.
Comparing the results of our two systems, we
see that the Blended parser outperforms the Single
Malt parser for all languages, with an average im-
937
provement of 1.40 percentage points, a minimum of
0.44 (Greek) and a maximum of 2.40 (English). As
shown by McDonald and Nivre (2007), the Single
Malt parser tends to suffer from two problems: error
propagation due to the deterministic parsing strat-
egy, typically affecting long dependencies more than
short ones, and low precision on dependencies orig-
inating in the artificial root node due to fragmented
parses.9 The question is which of these problems is
alleviated by the multiple views given by the compo-
nent parsers in the Blended system. Table 2 throws
some light on this by giving the precision and re-
call for dependencies of different length, treating de-
pendents of the artificial root node as a special case.
As expected, the Single Malt parser has lower preci-
sion than recall for root dependents, but the Blended
parser has even lower precision (and somewhat bet-
ter recall), indicating that the fragmentation is even
more severe in this case.10 By contrast, we see that
precision and recall for other dependencies improve
across the board, especially for longer dependencies,
which probably means that the effect of error propa-
gation is mitigated by the use of an ensemble system,
even if each of the component parsers is determinis-
tic in itself.
5 Conclusion
We have shown that deterministic, classifier-based
dependency parsing, with careful optimization, can
give highly accurate dependency parsing for a wide
range of languages, as illustrated by the performance
of the Single Malt parser. We have also demon-
strated that an ensemble of deterministic, classifier-
based dependency parsers, built on top of a tuned
single-parser system, can give even higher accuracy,
as shown by the results of the Blended parser, which
has the highest labeled attachment score for five lan-
guages (Arabic, Basque, Catalan, Hungarian, and
9A fragmented parse is a dependency forest, rather than a
tree, and is automatically converted to a tree by attaching all
(other) roots to the artificial root node. Hence, children of the
root node in the final output may not have been predicted as
such by the treebank-induced classifier.
10This conclusion is further supported by the observation
that the single most frequent ?frame confusion? of the Blended
parser, over all languages, is to attach two dependents with the
label ROOT to the root node, instead of only one. The frequency
of this error is more than twice as high for the Blended parser
(180) as for the Single Malt parser (83).
Italian), as well as the highest multilingual average
score.
Acknowledgements
We want to thank all treebank providers for making
the data available for the shared task and the (other)
organizers for their efforts in organizing it. Special
thanks to Ryan McDonald, for fruitful discussions
and assistance with the error analysis, and to Kenji
Sagae, for showing us how to produce a good blend.
Thanks also to two reviewers for useful comments.
References
A. Abeille?, editor. 2003. Treebanks: Building and Using
Parsed Corpora. Kluwer.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. Diaz de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency treebank.
In Proc. of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT), pages 201?204.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003.
The PDT: a 3-level annotation scenario. In Abeille?
(2003), chapter 7, pages 103?127.
C.-C. Chang and C.-J. Lin, 2001. LIBSVM: A Library
for Support Vector Machines. Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica treebank: Design criteria,
representational issues and implementation. In Abeille?
(2003), chapter 13, pages 231?248.
M. A. Covington. 2001. A fundamental algorithm for
dependency parsing. In Proc. of the 39th Annual ACM
Southeast Conf., pages 95?102.
D. Csendes, J. Csirik, T. Gyimo?thy, and A. Kocsor. 2005.
The Szeged Treebank. Springer.
J. Hajic?, O. Smrz?, P. Zema?nek, J. S?naidauf, and E. Bes?ka.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools,
pages 110?117.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
938
M. A. Mart??, M. Taule?, L. Ma`rquez, and M. Bertran.
2007. CESS-ECE: A multilingual and multilevel
annotated corpus. Available for download from:
http://www.lsi.upc.edu/?mbertran/cess-ece/.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proc. of the Joint Conf. on Empirical Methods in Nat-
ural Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL).
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. of the Human Language
Technology Conf. and the Conf. on Empirical Meth-
ods in Natural Language Processing (HLT/EMNLP),
pages 523?530.
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari,
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli,
M. Massetani, R. Raffaelli, R. Basili, M. T. Pazienza,
D. Saracino, F. Zanzotto, N. Nana, F. Pianesi, and
R. Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeille? (2003), chapter 11,
pages 189?210.
J. Nivre and J. Nilsson. 2005. Pseudo-projective depen-
dency. In Proc. of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
99?106.
J. Nivre, J. Hall, J. Nilsson, G. Eryig?it, and S. Marinov.
2006. Labeled pseudo-projective dependency parsing
with support vector machines. In Proc. of the Tenth
Conf. on Computational Natural Language Learning
(CoNLL), pages 221?225.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Proc. of the
Joint Conf. on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL).
J. Nivre. 2007. Incremental non-projective dependency
parsing. In Human Language Technologies: The An-
nual Conf. of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL-HLT),
pages 396?403.
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r.
2003. Building a Turkish treebank. In Abeille? (2003),
chapter 15, pages 261?277.
P. Prokopidis, E. Desypri, M. Koutsombogera, H. Papa-
georgiou, and S. Piperidis. 2005. Theoretical and
practical issues in the construction of a Greek depen-
dency treebank. In Proc. of the 4th Workshop on Tree-
banks and Linguistic Theories (TLT), pages 149?160.
K. Sagae and A. Lavie. 2006. Parser combination by
reparsing. In Proc. of the Human Language Technol-
ogy Conference of the NAACL, Companion Volume:
Short Papers, pages 129?132.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
8th International Workshop on Parsing Technologies
(IWPT), pages 195?206.
939
Constraints on Non-Projective Dependency Parsing
Joakim Nivre
Va?xjo? University, School of Mathematics and Systems Engineering
Uppsala University, Department of Linguistics and Philology
joakim.nivre@msi.vxu.se
Abstract
We investigate a series of graph-theoretic
constraints on non-projective dependency
parsing and their effect on expressivity,
i.e. whether they allow naturally occurring
syntactic constructions to be adequately
represented, and efficiency, i.e. whether
they reduce the search space for the parser.
In particular, we define a new measure
for the degree of non-projectivity in an
acyclic dependency graph obeying the
single-head constraint. The constraints are
evaluated experimentally using data from
the Prague Dependency Treebank and the
Danish Dependency Treebank. The results
indicate that, whereas complete linguistic
coverage in principle requires unrestricted
non-projective dependency graphs, limit-
ing the degree of non-projectivity to at
most 2 can reduce average running time
from quadratic to linear, while excluding
less than 0.5% of the dependency graphs
found in the two treebanks. This is a sub-
stantial improvement over the commonly
used projective approximation (degree 0),
which excludes 15?25% of the graphs.
1 Introduction
Data-driven approaches to syntactic parsing has
until quite recently been limited to representations
that do not capture non-local dependencies. This
is true regardless of whether representations are
based on constituency, where such dependencies
are traditionally represented by empty categories
and coindexation to avoid explicitly discontinuous
constituents, or on dependency, where it is more
common to use a direct encoding of so-called non-
projective dependencies.
While this ?surface dependency approximation?
(Levy and Manning, 2004) may be acceptable
for certain applications of syntactic parsing, it is
clearly not adequate as a basis for deep semantic
interpretation, which explains the growing body of
research devoted to different methods for correct-
ing this approximation. Most of this work has so
far focused either on post-processing to recover
non-local dependencies from context-free parse
trees (Johnson, 2002; Jijkoun and De Rijke, 2004;
Levy and Manning, 2004; Campbell, 2004), or on
incorporating nonlocal dependency information in
nonterminal categories in constituency represen-
tations (Dienes and Dubey, 2003; Hockenmaier,
2003; Cahill et al, 2004) or in the categories used
to label arcs in dependency representations (Nivre
and Nilsson, 2005).
By contrast, there is very little work on parsing
methods that allow discontinuous constructions to
be represented directly in the syntactic structure,
whether by discontinuous constituent structures
or by non-projective dependency structures. No-
table exceptions are Plaehn (2000), where discon-
tinuous phrase structure grammar parsing is ex-
plored, and McDonald et al (2005b), where non-
projective dependency structures are derived using
spanning tree algorithms from graph theory.
One question that arises if we want to pursue the
structure-based approach is how to constrain the
class of permissible structures. On the one hand,
we want to capture all the constructions that are
found in natural languages, or at least to provide
a much better approximation than before. On the
other hand, it must still be possible for the parser
not only to search the space of permissible struc-
tures in an efficient way but also to learn to select
the most appropriate structure for a given sentence
with sufficient accuracy. This is the usual tradeoff
73
between expressivity and complexity, where a less
restricted class of permissible structures can cap-
ture more complex constructions, but where the
enlarged search space makes parsing harder with
respect to both accuracy and efficiency.
Whereas extensions to context-free grammar
have been studied quite extensively, there are very
few corresponding results for dependency-based
systems. Since Gaifman (1965) proved that his
projective dependency grammar is weakly equiva-
lent to context-free grammar, Neuhaus and Bro?ker
(1997) have shown that the recognition problem
for a dependency grammar that can define arbi-
trary non-projective structures is NP complete,
but there are no results for systems of intermedi-
ate complexity. The pseudo-projective grammar
proposed by Kahane et al (1998) can be parsed
in polynomial time and captures non-local depen-
dencies through a form of gap-threading, but the
structures generated by the grammar are strictly
projective. Moreover, the study of formal gram-
mars is only partially relevant for research on data-
driven dependency parsing, where most systems
are not grammar-based but rely on inductive infer-
ence from treebank data (Yamada and Matsumoto,
2003; Nivre et al, 2004; McDonald et al, 2005a).
For example, despite the results of Neuhaus and
Bro?ker (1997), McDonald et al (2005b) perform
parsing with arbitrary non-projective dependency
structures in O(n2) time.
In this paper, we will therefore approach the
problem from a slightly different angle. Instead
of investigating formal dependency grammars and
their complexity, we will impose a series of graph-
theoretic constraints on dependency structures and
see how these constraints affect expressivity and
parsing efficiency. The approach is mainly ex-
perimental and we evaluate constraints using data
from two dependency-based treebanks, the Prague
Dependency Treebank (Hajic? et al, 2001) and the
Danish Dependency Treebank (Kromann, 2003).
Expressivity is investigated by examining how
large a proportion of the structures found in the
treebanks are parsable under different constraints,
and efficiency is addressed by considering the
number of potential dependency arcs that need to
be processed when parsing these structures. This
is a relevant metric for data-driven approaches,
where parsing time is often dominated by the com-
putation of model predictions or scores for such
arcs. The parsing experiments are performed with
a variant of Covington?s algorithm for dependency
parsing (Covington, 2001), using the treebank as
an oracle in order to establish an upper bound
on accuracy. However, the results are relevant
for a larger class of algorithms that derive non-
projective dependency graphs by treating every
possible word pair as a potential dependency arc.
The paper is structured as follows. In section 2
we define dependency graphs, and in section 3
we formulate a number of constraints that can
be used to define different classes of dependency
graphs, ranging from unrestricted non-projective
to strictly projective. In section 4 we introduce the
parsing algorithm used in the experiments, and in
section 5 we describe the experimental setup. In
section 6 we present the results of the experiments
and discuss their implications for non-projective
dependency parsing. We conclude in section 7.
2 Dependency Graphs
A dependency graph is a labeled directed graph,
the nodes of which are indices corresponding to
the tokens of a sentence. Formally:
Definition 1 Given a set R of dependency types
(arc labels), a dependency graph for a sentence
x = (w1, . . . , wn) is a labeled directed graph
G = (V,E,L), where:
1. V = Zn+1
2. E ? V ? V
3. L : E ? R
Definition 2 A dependency graph G is well-
formed if and only if:
1. The node 0 is a root (ROOT).
2. G is connected (CONNECTEDNESS).1
The set of V of nodes (or vertices) is the set
Zn+1 = {0, 1, 2, . . . , n} (n ? Z+), i.e., the set of
non-negative integers up to and including n. This
means that every token index i of the sentence is a
node (1 ? i ? n) and that there is a special node
0, which does not correspond to any token of the
sentence and which will always be a root of the
dependency graph (normally the only root).
The set E of arcs (or edges) is a set of ordered
pairs (i, j), where i and j are nodes. Since arcs are
used to represent dependency relations, we will
1To be more exact, we require G to be weakly connected,
which entails that the corresponding undirected graph is con-
nected, whereas a strongly connected graph has a directed
path between any pair of nodes.
74
(?Only one of them concerns quality.?)
0 1
R
Z
(Out-of
 
?
AuxP
2
P
nich
them
 
?
Atr
3
VB
je
is
 
?
Pred
4
T
jen
only
 
?
AuxZ
5
C
jedna
one-FEM-SG
 
?
Sb
6
R
na
to
 
?
AuxP
7
N4
kvalitu
quality
?
 
Adv
8
Z:
.
.)
 
?
AuxK
Figure 1: Dependency graph for Czech sentence from the Prague Dependency Treebank
say that i is the head and j is the dependent of
the arc (i, j). As usual, we will use the notation
i ? j to mean that there is an arc connecting i
and j (i.e., (i, j) ? E) and we will use the nota-
tion i?? j for the reflexive and transitive closure
of the arc relation E (i.e., i ?? j if and only if
i = j or there is a path of arcs connecting i to j).
The function L assigns a dependency type (arc
label) r ? R to every arc e ? E. Figure 1 shows
a Czech sentence from the Prague Dependency
Treebank with a well-formed dependency graph
according to Definition 1?2.
3 Constraints
The only conditions so far imposed on dependency
graphs is that the special node 0 be a root and that
the graph be connected. Here are three further
constraints that are common in the literature:
3. Every node has at most one head, i.e., if i?j
then there is no node k such that k 6= i and
k ? j (SINGLE-HEAD).
4. The graph G is acyclic, i.e., if i? j then not
j ?? i (ACYCLICITY).
5. The graph G is projective, i.e., if i ? j then
i?? k, for every node k such that i < k < j
or j < k < i (PROJECTIVITY).
Note that these conditions are independent in that
none of them is entailed by any (combination)
of the others. However, the conditions SINGLE-
HEAD and ACYCLICITY together with the basic
well-formedness conditions entail that the graph
is a tree rooted at the node 0. These constraints
are assumed in almost all versions of dependency
grammar, especially in computational systems.
By contrast, the PROJECTIVITY constraint is
much more controversial. Broadly speaking, we
can say that whereas most practical systems for
dependency parsing do assume projectivity, most
dependency-based linguistic theories do not. More
precisely, most theoretical formulations of depen-
dency grammar regard projectivity as the norm
but also recognize the need for non-projective
representations to capture non-local dependencies
(Mel?c?uk, 1988; Hudson, 1990).
In order to distinguish classes of dependency
graphs that fall in between arbitrary non-projective
and projective, we define a notion of degree of
non-projectivity, such that projective graphs have
degree 0 while arbitrary non-projective graphs
have unbounded degree.
Definition 3 Let G = (V,E,L) be a well-formed
dependency graph, satisfying SINGLE-HEAD and
ACYCLICITY, and let Ge be the subgraph of G
that only contains nodes between i and j for the
arc e = (i, j) (i.e., Ve = {i+1, . . . , j?1} if i < j
and Ve = {j+1, . . . , i?1} if i > j).
1. The degree of an arc e ? E is the number of
connected components c in Ge such that the
root of c is not dominated by the head of e.
2. The degree of G is the maximum degree of
any arc e ? E.
To exemplify the notion of degree, we note that
the dependency graph in Figure 1 (which satisfies
SINGLE-HEAD and ACYCLICITY) has degree 1.
The only non-projective arc in the graph is (5, 1)
and G(5,1) contains three connected components,
each of which consists of a single root node (2, 3
and 4). Since only one of these, 3, is not domi-
nated by 5, the arc (5, 1) has degree 1.
4 Parsing Algorithm
Covington (2001) describes a parsing strategy for
dependency representations that has been known
75
since the 1960s but not presented in the literature.
The left-to-right (or incremental) version of this
strategy can be formulated in the following way:
PARSE(x = (w1, . . . , wn))
1 for i = 1 up to n
2 for j = i? 1 down to 1
3 LINK(i, j)
The operation LINK(i, j) nondeterministically
chooses between (i) adding the arc i ? j (with
some label), (ii) adding the arc j ? i (with some
label), and (iii) adding no arc at all. In this way, the
algorithm builds a graph by systematically trying
to link every pair of nodes (i, j) (i > j). This
graph will be a well-formed dependency graph,
provided that we also add arcs from the root node
0 to every root node in {1, . . . , n}. Assuming that
the LINK(i, j) operation can be performed in some
constant time c, the running time of the algorithm
is
?n
i=1 c(n ? 1) = c(n
2
2 ? n2 ), which in terms of
asymptotic complexity is O(n2).
In the experiments reported in the following
sections, we modify this algorithm by making the
performance of LINK(i, j) conditional on the arcs
(i, j) and (j, i) being permissible under the given
graph constraints:
PARSE(x = (w1, . . . , wn))
1 for i = 1 up to n
2 for j = i? 1 down to 1
3 if PERMISSIBLE(i, j, C)
4 LINK(i, j)
The function PERMISSIBLE(i, j, C) returns true
iff i ? j and j ? i are permissible arcs relative
to the constraint C and the partially built graph
G. For example, with the constraint SINGLE-
HEAD, LINK(i, j) will not be performed if both
i and j already have a head in the dependency
graph. We call the pairs (i, j) (i > j) for which
LINK(i, j) is performed (for a given sentence and
set of constraints) the active pairs, and we use
the number of active pairs, as a function of sen-
tence length, as an abstract measure of running
time. This is well motivated if the time required
to compute PERMISSIBLE(i, j, C) is insignificant
compared to the time needed for LINK(i, j), as is
typically the case in data-driven systems, where
LINK(i, j) requires a call to a trained classifier,
while PERMISSIBLE(i, j, C) only needs access to
the partially built graph G.
The results obtained in this way will be partially
dependent on the particular algorithm used, but
they can in principle be generalized to any algo-
rithm that tries to link all possible word pairs and
that satisfies the following condition:
For any graph G = (V,E,L) derived by
the algorithm, if e, e? ? E and e covers
e?, then the algorithm adds e? before e.
This condition is satisfied not only by Covington?s
incremental algorithm but also by algorithms that
add arcs strictly in order of increasing length, such
as the algorithm of Eisner (2000) and other algo-
rithms based on dynamic programming.
5 Experimental Setup
The experiments are based on data from two tree-
banks. The Prague Dependency Treebank (PDT)
contains 1.5M words of newspaper text, annotated
in three layers (Hajic?, 1998; Hajic? et al, 2001)
according to the theoretical framework of Func-
tional Generative Description (Sgall et al, 1986).
Our experiments concern only the analytical layer
and are based on the dedicated training section of
the treebank. The Danish Dependency Treebank
(DDT) comprises 100K words of text selected
from the Danish PAROLE corpus, with annotation
of primary and secondary dependencies based on
Discontinuous Grammar (Kromann, 2003). Only
primary dependencies are considered in the exper-
iments, which are based on 80% of the data (again
the standard training section).
The experiments are performed by parsing each
sentence of the treebanks while using the gold
standard dependency graph for that sentence as an
oracle to resolve the nondeterministic choice in the
LINK(i, j) operation as follows:
LINK(i, j)
1 if (i, j) ? Eg
2 E ? E ? {(i, j)}
3 if (j, i) ? Eg
4 E ? E ? {(j, i)}
where Eg is the arc relation of the gold standard
dependency graph Gg and E is the arc relation of
the graph G built by the parsing algorithm.
Conditions are varied by cumulatively adding
constraints in the following order:
1. SINGLE-HEAD
2. ACYCLICITY
3. Degree d ? k (k ? 1)
4. PROJECTIVITY
76
Table 1: Proportion of dependency arcs and complete graphs correctly parsed under different constraints
in the Prague Dependency Treebank (PDT) and the Danish Dependency Treebank (DDT)
PDT DDT
Constraint Arcs Graphs Arcs Graphs
n = 1255590 n = 73088 n = 80193 n = 4410
PROJECTIVITY 96.1569 76.8498 97.7754 84.6259
d ? 1 99.7854 97.7507 99.8940 98.0272
d ? 2 99.9773 99.5731 99.9751 99.5238
d ? 3 99.9956 99.9179 99.9975 99.9546
d ? 4 99.9983 99.9863 100.0000 100.0000
d ? 5 99.9987 99.9945 100.0000 100.0000
d ? 10 99.9998 99.9986 100.0000 100.0000
ACYCLICITY 100.0000 100.0000 100.0000 100.0000
SINGLE-HEAD 100.0000 100.0000 100.0000 100.0000
None 100.0000 100.0000 100.0000 100.0000
The purpose of the experiments is to study how
different constraints influence expressivity and
running time. The first dimension is investigated
by comparing the dependency graphs produced
by the parser with the gold standard dependency
graphs in the treebank. This gives an indication of
the extent to which naturally occurring structures
can be parsed correctly under different constraints.
The results are reported both as the proportion of
individual dependency arcs (per token) and as the
proportion of complete dependency graphs (per
sentence) recovered correctly by the parser.
In order to study the effects on running time,
we examine how the number of active pairs varies
as a function of sentence length. Whereas the
asymptotic worst-case complexity remains O(n2)
under all conditions, the average running time will
decrease with the number of active pairs if the
LINK(i, j) operation is more expensive than the
call to PERMISSIBLE(i, j, C). For data-driven
dependency parsing, this is relevant not only for
parsing efficiency, but also because it may improve
training efficiency by reducing the number of pairs
that need to be included in the training data.
6 Results and Discussion
Table 1 displays the proportion of dependencies
(single arcs) and sentences (complete graphs) in
the two treebanks that can be parsed exactly with
Covington?s algorithm under different constraints.
Starting at the bottom of the table, we see that
the unrestricted algorithm (None) of course repro-
duces all the graphs exactly, but we also see that
the constraints SINGLE-HEAD and ACYCLICITY
do not put any real restrictions on expressivity
with regard to the data at hand. However, this is
primarily a reflection of the design of the treebank
annotation schemes, which in themselves require
dependency graphs to obey these constraints.2
If we go to the other end of the table, we see
that PROJECTIVITY, on the other hand, has a very
noticeable effect on the parser?s ability to capture
the structures found in the treebanks. Almost 25%
of the sentences in PDT, and more than 15% in
DDT, are beyond its reach. At the level of indi-
vidual dependencies, the effect is less conspicu-
ous, but it is still the case in PDT that one depen-
dency in twenty-five cannot be found by the parser
even with a perfect oracle (one in fifty in DDT). It
should be noted that the proportion of lost depen-
dencies is about twice as high as the proportion
of dependencies that are non-projective in them-
selves (Nivre and Nilsson, 2005). This is due to
error propagation, since some projective arcs are
blocked from the parser?s view because of missing
non-projective arcs.
Considering different bounds on the degree of
non-projectivity, finally, we see that even the tight-
est possible bound (d ? 1) gives a much better
approximation than PROJECTIVITY, reducing the
2It should be remembered that we are only concerned with
one layer of each annotation scheme, the analytical layer in
PDT and the primary dependencies in DDT. Taking several
layers into account simultaneously would have resulted in
more complex structures.
77
Table 2: Quadratic curve estimation for y = ax+ bx2 (y = number of active pairs, x = number of words)
PDT DDT
Constraint a b r2 a b r2
PROJECTIVITY 1.9181 0.0093 0.979 1.7591 0.0108 0.985
d ? 1 3.2381 0.0534 0.967 2.2049 0.0391 0.969
d ? 2 3.1467 0.1192 0.967 2.0273 0.0680 0.964
ACYCLICITY 0.3845 0.2587 0.971 1.4285 0.1106 0.967
SINGLE-HEAD 0.7187 0.2628 0.976 1.9003 0.1149 0.967
None ?0.5000 0.5000 1.000 ?0.5000 0.5000 1.000
proportion of non-parsable sentences with about
90% in both treebanks. At the level of individual
arcs, the reduction is even greater, about 95% for
both data sets. And if we allow a maximum degree
of 2, we can capture more than 99.9% of all depen-
dencies, and more than 99.5% of all sentences, in
both PDT and DDT. At the same time, there seems
to be no principled upper bound on the degree of
non-projectivity, since in PDT not even an upper
bound of 10 is sufficient to correctly capture all
dependency graphs in the treebank.3
Let us now see how different constraints affect
running time, as measured by the number of ac-
tive pairs in relation to sentence length. A plot of
this relationship for a subset of the conditions can
be found in Figure 2. For reasons of space, we
only display the data from DDT, but the PDT data
exhibit very similar patterns. Both treebanks are
represented in Table 2, where we show the result
of fitting the quadratic equation y = ax + bx2 to
the data from each condition (where y is the num-
ber of active words and x is the number of words in
the sentence). The amount of variance explained is
given by the r2 value, which shows a very good fit
under all conditions, with statistical significance
beyond the 0.001 level.4
Both Figure 2 and Table 2 show very clearly
that, with no constraints, the relationship between
words and active pairs is exactly the one predicted
by the worst case complexity (cf. section 4) and
that, with each added constraint, this relationship
becomes more and more linear in shape. When we
get to PROJECTIVITY, the quadratic coefficient b
is so small that the average running time is prac-
tically linear for the great majority of sentences.
3The single sentence that is not parsed correctly at d ? 10
has a dependency arc of degree 12.
4The curve estimation has been performed using SPSS.
However, the complexity is not much worse for
the bounded degrees of non-projectivity (d ? 1,
d ? 2). More precisely, for both data sets, the
linear term ax dominates the quadratic term bx2
for sentences up to 50 words at d ? 1 and up to
30 words at d ? 2. Given that sentences of 50
words or less represent 98.9% of all sentences in
PDT and 98.3% in DDT (the corresponding per-
centages for 30 words being 88.9% and 86.0%), it
seems that the average case running time can be
regarded as linear also for these models.
7 Conclusion
We have investigated a series of graph-theoretic
constraints on dependency structures, aiming to
find a better approximation than PROJECTIVITY
for the structures found in naturally occurring
data, while maintaining good parsing efficiency.
In particular, we have defined the degree of non-
projectivity in terms of the maximum number of
connected components that occur under a depen-
dency arc without being dominated by the head
of that arc. Empirical experiments based on data
from two treebanks, from different languages and
with different annotation schemes, have shown
that limiting the degree d of non-projectivity to
1 or 2 gives an average case running time that is
linear in practice and allows us to capture about
98% of the dependency graphs actually found in
the treebanks with d ? 1, and about 99.5% with
d ? 2. This is a substantial improvement over
the projective approximation, which only allows
75?85% of the dependency graphs to be captured
exactly. This suggests that the integration of such
constraints into non-projective parsing algorithms
will improve both accuracy and efficiency, but we
have to leave the corroboration of this hypothesis
as a topic for future research.
78
0.0 20.0 40.0 60.0 80.0 100.0
Words
0.00
1000.00
2000.00
3000.00
4000.00
Pa
irs
None
0.0 20.0 40.0 60.0 80.0 100.0
Words
0.00
200.00
400.00
600.00
800.00
1000.00
1200.00
Pa
irs
Single-Head
0.0 20.0 40.0 60.0 80.0 100.0
Words
0.00
200.00
400.00
600.00
800.00
1000.00
1200.00
Pa
irs
Acyclic
0.0 20.0 40.0 60.0 80.0 100.0
Words
0.00
200.00
400.00
600.00
800.00
Pa
irs
d <= 2
0.0 20.0 40.0 60.0 80.0 100.0
Words
0.00
100.00
200.00
300.00
400.00
500.00
600.00
Pa
irs
d <= 1
0.0 20.0 40.0 60.0 80.0 100.0
Words
0.00
50.00
100.00
150.00
200.00
250.00
Pa
irs
Projectivity
Figure 2: Number of active pairs as a function of sentence length under different constraints (DDT)
79
Acknowledgments
The research reported in this paper was partially
funded by the Swedish Research Council (621-
2002-4207). The insightful comments of three
anonymous reviewers helped improve the final
version of the paper.
References
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef
Van Genabith, and Andy Way. 2004. Long-
distance dependency resolution in automatically ac-
quiredwide-coverage PCFG-based LFG approxima-
tions. Proceedings of ACL, pp. 320?327.
Richard Campbell. 2004. Using linguistic principles
to recover empty categories. Proceedings of ACL,
pp. 646?653.
Michael Collins, Jan Hajic?, Eric Brill, Lance Ramshaw,
and Christoph Tillmann. 1999. A statistical parser
for Czech. Proceedings of ACL, pp. 505?512.
Michael A. Covington. 2001. A fundamental algo-
rithm for dependency parsing. Proceedings of the
39th Annual ACM Southeast Conference, pp. 95?
102.
Pe?ter Dienes and Amit Dubey. 2003. Deep syntac-
tic processing by combining shallow methods. Pro-
ceedings of ACL, pp. 431?438.
Jason M. Eisner. 2000. Bilexical grammars and their
cubic-time parsing algorithms. In Harry Bunt and
Anton Nijholt, editors, Advances in Probabilistic
and Other Parsing Technologies, pp. 29?62. Kluwer.
Haim Gaifman. 1965. Dependency systems and
phrase-structure systems. Information and Control,
8:304?337.
Jan Hajic?, Barbora Vidova Hladka, Jarmila Panevova?,
Eva Hajic?ova?, Petr Sgall, and Petr Pajas. 2001.
Prague Dependency Treebank 1.0. LDC, 2001T10.
Jan Hajic?. 1998. Building a syntactically annotated
corpus: The Prague Dependency Treebank. Issues
of Valency and Meaning, pp. 106?132. Karolinum.
Julia Hockenmaier. 2003. Data and Models for Sta-
tistical Parsing with Combinatory Categorial Gram-
mar. Ph.D. thesis, University of Edinburgh.
Richard A. Hudson. 1990. English Word Grammar.
Blackwell.
Valentin Jijkoun and Maarten De Rijke. 2004. En-
riching the output of a parser using memory-based
learning. Proceedings of ACL, pp. 312?319.
Mark Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. Proceedings of ACL, pp. 136?143.
Sylvain Kahane, Alexis Nasr and Owen Rambow.
Pseudo-Projectivity: A Polynomially Parsable Non-
Projective Dependency Grammar. Proceedings of
ACL-COLING, pp. 646?652.
Matthias Trautner Kromann. 2003. The Danish De-
pendency Treebank and the DTAG treebank tool.
Proceedings of TLT, pp. 217?220.
Roger Levy and Christopher Manning. 2004. Deep
dependencies from context-free statistical parsers:
Correcting the surface dependency approximation.
Proceedings of ACL, pp. 328?335.
Hiroshi Maruyama. 1990. Structural disambiguation
with constraint propagation. Proceedings of ACL,
pp. 31?38.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online large-margin training of de-
pendency parsers. Proceedings of ACL, pp. 91?98.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. Proceedings of
HLT/EMNLP, pp. 523?530.
Igor Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
Peter Neuhaus and Norbert Bro?ker. 1997. The com-
plexity of recognition of linguistically adequate de-
pendency grammars. Proceedings of ACL-EACL,
pages 337?343.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. Proceedings ACL,
pp. 99?106.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. Proceedings of
CoNLL, pp. 49?56.
Oliver Plaehn. 2000. Computing the most probably
parse for a discontinuous phrase structure grammar.
Proceedings of IWPT.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The Meaning of the Sentence in Its Pragmatic As-
pects. Reidel.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. Proceedings of IWPT, pp. 195?206.
80
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 968?975,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Generalizing Tree Transformations for Inductive Dependency Parsing
Jens Nilsson? Joakim Nivre??
?Va?xjo? University, School of Mathematics and Systems Engineering, Sweden
?Uppsala University, Dept. of Linguistics and Philology, Sweden
{jni,nivre,jha}@msi.vxu.se
Johan Hall?
Abstract
Previous studies in data-driven dependency
parsing have shown that tree transformations
can improve parsing accuracy for specific
parsers and data sets. We investigate to
what extent this can be generalized across
languages/treebanks and parsers, focusing
on pseudo-projective parsing, as a way of
capturing non-projective dependencies, and
transformations used to facilitate parsing of
coordinate structures and verb groups. The
results indicate that the beneficial effect of
pseudo-projective parsing is independent of
parsing strategy but sensitive to language or
treebank specific properties. By contrast, the
construction specific transformations appear
to be more sensitive to parsing strategy but
have a constant positive effect over several
languages.
1 Introduction
Treebank parsers are trained on syntactically anno-
tated sentences and a major part of their success can
be attributed to extensive manipulations of the train-
ing data as well as the output of the parser, usually
in the form of various tree transformations. This
can be seen in state-of-the-art constituency-based
parsers such as Collins (1999), Charniak (2000), and
Petrov et al (2006), and the effects of different trans-
formations have been studied by Johnson (1998),
Klein andManning (2003), and Bikel (2004). Corre-
sponding manipulations in the form of tree transfor-
mations for dependency-based parsers have recently
gained more interest (Nivre and Nilsson, 2005; Hall
and Nova?k, 2005; McDonald and Pereira, 2006;
Nilsson et al, 2006) but are still less studied, partly
because constituency-based parsing has dominated
the field for a long time, and partly because depen-
dency structures have less structure to manipulate
than constituent structures.
Most of the studies in this tradition focus on a par-
ticular parsing model and a particular data set, which
means that it is difficult to say whether the effect
of a given transformation is dependent on a partic-
ular parsing strategy or on properties of a particu-
lar language or treebank, or both. The aim of this
study is to further investigate some tree transforma-
tion techniques previously proposed for data-driven
dependency parsing, with the specific aim of trying
to generalize results across languages/treebanks and
parsers. More precisely, we want to establish, first
of all, whether the transformation as such makes
specific assumptions about the language, treebank
or parser and, secondly, whether the improved pars-
ing accuracy that is due to a given transformation is
constant across different languages, treebanks, and
parsers.
The three types of syntactic phenomena that will
be studied here are non-projectivity, coordination
and verb groups, which in different ways pose prob-
lems for dependency parsers. We will focus on tree
transformations that combine preprocessing with
post-processing, and where the parser is treated as
a black box, such as the pseudo-projective parsing
technique proposed by Nivre and Nilsson (2005)
and the tree transformations investigated in Nils-
son et al (2006). To study the influence of lan-
968
guage and treebank specific properties we will use
data from Arabic, Czech, Dutch, and Slovene, taken
from the CoNLL-X shared task on multilingual de-
pendency parsing (Buchholz and Marsi, 2006). To
study the influence of parsing methodology, we will
compare two different parsers: MaltParser (Nivre et
al., 2004) and MSTParser (McDonald et al, 2005).
Note that, while it is possible in principle to distin-
guish between syntactic properties of a language as
such and properties of a particular syntactic annota-
tion of the language in question, it will be impossi-
ble to tease these apart in the experiments reported
here, since this would require having not only mul-
tiple languages but also multiple treebanks for each
language. In the following, we will therefore speak
about the properties of treebanks (rather than lan-
guages), but it should be understood that these prop-
erties in general depend both on properties of the
language and of the particular syntactic annotation
adopted in the treebank.
The rest of the paper is structured as follows. Sec-
tion 2 surveys tree transformations used in depen-
dency parsing and discusses dependencies between
transformations, on the one hand, and treebanks and
parsers, on the other. Section 3 introduces the four
treebanks used in this study, and section 4 briefly
describes the two parsers. Experimental results are
presented in section 5 and conclusions in section 6.
2 Background
2.1 Non-projectivity
The tree transformations that have attracted most in-
terest in the literature on dependency parsing are
those concerned with recovering non-projectivity.
The definition of non-projectivity can be found in
Kahane et al (1998). Informally, an arc is projec-
tive if all tokens it covers are descendants of the arc?s
head token, and a dependency tree is projective if all
its arcs are projective.1
The full potential of dependency parsing can only
be realized if non-projectivity is allowed, which
pose a problem for projective dependency parsers.
Direct non-projective parsing can be performed with
good accuracy, e.g., using the Chu-Liu-Edmonds al-
1If dependency arcs are drawn above the linearly ordered
sequence of tokens, preceded by a special root node, then a non-
projective dependency tree always has crossing arcs.
gorithm, as proposed byMcDonald et al (2005). On
the other hand, non-projective parsers tend, among
other things, to be slower. In order to maintain the
benefits of projective parsing, tree transformations
techniques to recover non-projectivity while using a
projective parser have been proposed in several stud-
ies, some described below.
In discussing the recovery of empty categories in
data-driven constituency parsing, Campbell (2004)
distinguishes between approaches based on pure
post-processing and approaches based on a combi-
nation of preprocessing and post-processing. The
same division can be made for the recovery of non-
projective dependencies in data-driven dependency
parsing.
Pure Post-processing
Hall and Nova?k (2005) propose a corrective model-
ing approach. The motivation is that the parsers of
Collins et al (1999) and Charniak (2000) adapted
to Czech are not able to create the non-projective
arcs present in the treebank, which is unsatisfac-
tory. They therefore aim to correct erroneous arcs in
the parser?s output (specifically all those arcs which
should be non-projective) by training a classifier that
predicts the most probable head of a token in the
neighborhood of the head assigned by the parser.
Another example is the second-order approximate
spanning tree parser developed by McDonald and
Pereira (2006). It starts by producing the highest
scoring projective dependency tree using Eisner?s al-
gorithm. In the second phase, tree transformations
are performed, replacing lower scoring projective
arcs with higher scoring non-projective ones.
Preprocessing with Post-processing
The training data can also be preprocessed to facili-
tate the recovery of non-projective arcs in the output
of a projective parser. The pseudo-projective trans-
formation proposed by Nivre and Nilsson (2005) is
such an approach, which is compatible with differ-
ent parser engines.
First, the training data is projectivized by making
non-projective arcs projective using a lifting oper-
ation. This is combined with an augmentation of
the dependency labels of projectivized arcs (and/or
surrounding arcs) with information that probably re-
veals their correct non-projective positions. The out-
969
(PS)
C1
 
?
S1
?
C2
 
?
(MS)
C1
?
S1
 
?
C2
 
?
(CS)
C1
?
S1
 
?
C2
 
?
Figure 1: Dependency structure for coordination
put of the parser, trained on the projectivized data,
is then deprojectivized by a heuristic search using
the added information in the dependency labels. The
only assumption made about the parser is therefore
that it can learn to derive labeled dependency struc-
tures with augmented dependency labels.
2.2 Coordination and Verb Groups
The second type of transformation concerns linguis-
tic phenomena that are not impossible for a projec-
tive parser to process but which may be difficult to
learn, given a certain choice of dependency analy-
sis. This study is concerned with two such phe-
nomena, coordination and verb groups, for which
tree transformations have been shown to improve
parsing accuracy for MaltParser on Czech (Nils-
son et al, 2006). The general conclusion of this
study is that coordination and verb groups in the
Prague Dependency Treebank (PDT), based on the-
ories of the Prague school (PS), are annotated in a
way that is difficult for the parser to learn. By trans-
forming coordination and verb groups in the train-
ing data to an annotation similar to that advocated
by Mel?c?uk (1988) and then performing an inverse
transformation on the parser output, parsing accu-
racy can therefore be improved. This is again an
instance of the black-box idea.
Schematically, coordination is annotated in the
Prague school as depicted in PS in figure 1, where
the conjuncts are dependents of the conjunction. In
Mel?c?uk style (MS), on the other hand, conjuncts
and conjunction(s) form a chain going from left to
right. A third way of treating coordination, not dis-
cussed by Nilsson et al (2006), is used by the parser
of Collins (1999), which internally represents coor-
dination as a direct relation between the conjuncts.
This is illustrated in CS in figure 1, where the con-
junction depends on one of the conjuncts, in this
case on the rightmost one.
Nilsson et al (2006) also show that the annotation
of verb groups is not well-suited for parsing PDT
using MaltParser, and that transforming the depen-
dency structure for verb groups has a positive impact
on parsing accuracy. In PDT, auxiliary verbs are de-
pendents of the main verb, whereas it according to
Mel?c?uk is the (finite) auxiliary verb that is the head
of the main verb. Again, the parsing experiments in
this study show that verb groups are more difficult
to parse in PS than in MS.
2.3 Transformations, Parsers, and Treebanks
Pseudo-projective parsing and transformations for
coordination and verb groups are instances of the
same general methodology:
1. Apply a tree transformation to the training data.
2. Train a parser on the transformed data.
3. Parse new sentences.
4. Apply an inverse transformation to the output
of the parser.
In this scheme, the parser is treated as a black
box. All that is assumed is that it is a data-driven
parser designed for (projective) labeled dependency
structures. In this sense, the tree transformations
are independent of parsing methodology. Whether
the beneficial effect of a transformation, if any, is
also independent of parsing methodology is another
question, which will be addressed in the experimen-
tal part of this paper.
The pseudo-projective transformation is indepen-
dent not only of parsing methodology but also of
treebank (and language) specific properties, as long
as the target representation is a (potentially non-
projective) labeled dependency structure. By con-
trast, the coordination and verb group transforma-
tions presuppose not only that the language in ques-
tion contains these constructions but also that the
treebank adopts a PS annotation. In this sense, they
are more limited in their applicability than pseudo-
projective parsing. Again, it is a different question
whether the transformations have a positive effect
for all treebanks (languages) to which they can be
applied.
3 Treebanks
The experiments are mostly conducted using tree-
bank data from the CoNLL shared task 2006. This
970
Slovene Arabic Dutch Czech
SDT PADT Alpino PDT
# T 29 54 195 1249
# S 1.5 1.5 13.3 72.7
%-NPS 22.2 11.2 36.4 23.2
%-NPA 1.8 0.4 5.4 1.9
%-C 9.3 8.5 4.0 8.5
%-A 8.8 - - 1.3
Table 1: Overview of the data sets (ordered by size),
where # S * 1000 = number of sentences, # T * 1000
= number of tokens, %-NPS = percentage of non-
projective sentences, %-NPA = percentage of non-
projective arcs, %-C = percentage of conjuncts, %-A
= percentage of auxiliary verbs.
subsection summarizes some of the important char-
acteristics of these data sets, with an overview in ta-
ble 1. Any details concerning the conversion from
the original formats of the various treebanks to the
CoNLL format, a pure dependency based format, are
found in documentation referred to in Buchholz and
Marsi (2006).
PDT (Hajic? et al, 2001) is the largest manually
annotated treebank, and as already mentioned, it
adopts PS for coordination and verb groups. As
the last four rows reveal, PDT contains a quite high
proportion of non-projectivity, since almost every
fourth dependency graph contains at least one non-
projective arc. The table also shows that coordina-
tion is more common than verb groups in PDT. Only
1.3% of the tokens in the training data are identified
as auxiliary verbs, whereas 8.5% of the tokens are
identified as conjuncts.
Both Slovene Dependency Treebank (Dz?eroski et
al., 2006) (SDT) and Prague Arabic Dependency
Treebank (Hajic? et al, 2004) (PADT) annotate co-
ordination and verb groups as in PDT, since they too
are influenced by the theories of the Prague school.
The proportions of non-projectivity and conjuncts in
SDT are in fact quite similar to the proportions in
PDT. The big difference is the proportion of auxil-
iary verbs, with many more auxiliary verbs in SDT
than in PDT. It is therefore plausible that the trans-
formations for verb groups will have a larger impact
on parser accuracy in SDT.
Arabic is not a Slavic languages such as Czech
and Slovene, and the annotation in PADT is there-
fore more dissimilar to PDT than SDT is. One such
example is that Arabic does not have auxiliary verbs.
Table 1 thus does not give figures verb groups. The
amount of coordination is on the other hand compa-
rable to both PDT and SDT. The table also reveals
that the amount of non-projective arcs is about 25%
of that in PDT and SDT, although the amount of
non-projective sentences is still as large as 50% of
that in PDT and SDT.
Alpino (van der Beek et al, 2002) in the CoNLL
format, the second largest treebank in this study,
is not as closely tied to the theories of the Prague
school as the others, but still treats coordination in
a way similar to PS. The table shows that coor-
dination is less frequent in the CoNLL version of
Alpino than in the three other treebanks. The other
characteristic of Alpino is the high share of non-
projectivity, where more than every third sentence
is non-projective. Finally, the lack of information
about the share of auxiliary verbs is not due to the
non-existence of such verbs in Dutch but to the fact
that Alpino adopts an MS annotation of verb groups
(i.e., treating main verbs as dependents of auxiliary
verbs), which means that the verb group transforma-
tion of Nilsson et al (2006) is not applicable.
4 Parsers
The parsers used in the experiments are Malt-
Parser (Nivre et al, 2004) and MSTParser (Mc-
Donald et al, 2005). These parsers are based on
very different parsing strategies, which makes them
suitable in order to test the parser independence
of different transformations. MaltParser adopts a
greedy, deterministic parsing strategy, deriving a la-
beled dependency structure in a single left-to-right
pass over the input and uses support vector ma-
chines to predict the next parsing action. MST-
Parser instead extracts a maximum spanning tree
from a dense weighted graph containing all possi-
ble dependency arcs between tokens (with Eisner?s
algorithm for projective dependency structures or
the Chu-Liu-Edmonds algorithm for non-projective
structures), using a global discriminative model and
online learning to assign weights to individual arcs.2
2The experiments in this paper are based on the first-order
factorization described in McDonald et al (2005)
971
5 Experiments
The experiments reported in section 5.1?5.2 below
are based on the training sets from the CoNLL-X
shared task, except where noted. The results re-
ported are obtained by a ten-fold cross-validation
(with a pseudo-randomized split) for all treebanks
except PDT, where 80% of the data was used for
training and 20% for development testing (again
with a pseudo-randomized split). In section 5.3, we
give results for the final evaluation on the CoNLL-
X test sets using all three transformations together
with MaltParser.
Parsing accuracy is primarily measured by the un-
labeled attachment score (ASU ), i.e., the propor-
tion of tokens that are assigned the correct head, as
computed by the official CoNLL-X evaluation script
with default settings (thus excluding all punctuation
tokens). In section 5.3 we also include the labeled
attachment score (ASL) (where a token must have
both the correct head and the correct dependency la-
bel to be counted as correct), which was the official
evaluation metric in the CoNLL-X shared task.
5.1 Comparing Treebanks
We start by examining the effect of transformations
on data from different treebanks (languages), using
a single parser: MaltParser.
Non-projectivity
The question in focus here is whether the effect of
the pseudo-projective transformation for MaltParser
varies with the treebank. Table 2 presents the un-
labeled attachment score results (ASU ), compar-
ing the pseudo-projective parsing technique (P-Proj)
with two baselines, obtained by training the strictly
projective parser on the original (non-projective)
training data (N-Proj) and on projectivized train-
ing data with no augmentation of dependency labels
(Proj).
The first thing to note is that pseudo-projective
parsing gives a significant improvement for PDT,
as previously reported by Nivre and Nilsson (2005),
but also for Alpino, where the improvement is even
larger, presumably because of the higher proportion
of non-projective dependencies in the Dutch tree-
bank. By contrast, there is no significant improve-
ment for either SDT or PADT, and even a small drop
N-Proj Proj P-Proj
SDT 77.27 76.63?? 77.11
PADT 76.96 77.07? 77.07?
Alpino 82.75 83.28?? 87.08??
PDT 83.41 83.32?? 84.42??
Table 2: ASU for pseudo-projective parsing with
MaltParser. McNemar?s test: ? = p < .05 and
?? = p < 0.01 compared to N-Proj.
1 2 3 >3
SDT 88.4 9.1 1.7 0.84
PADT 66.5 14.4 5.2 13.9
Alpino 84.6 13.8 1.5 0.07
PDT 93.8 5.6 0.5 0.1
Table 3: The number of lifts for non-projective arcs.
in the accuracy figures for SDT. Finally, in contrast
to the results reported by Nivre and Nilsson (2005),
simply projectivizing the training data (without us-
ing an inverse transformation) is not beneficial at all,
except possibly for Alpino.
But why does not pseudo-projective parsing im-
prove accuracy for SDT and PADT? One possi-
ble factor is the complexity of the non-projective
constructions, which can be measured by counting
the number of lifts that are required to make non-
projective arcs projective. The more deeply nested
a non-projective arc is, the more difficult it is to re-
cover because of parsing errors as well as search er-
rors in the inverse transformation. The figures in ta-
ble 3 shed some interesting light on this factor.
For example, whereas 93.8% of all arcs in PDT
require only one lift before they become projec-
tive (88.4% and 84.6% for SDT and Alpino, respec-
tively), the corresponding figure for PADT is as low
as 66.5%. PADT also has a high proportion of very
deeply nested non-projective arcs (>3) in compari-
son to the other treebanks, making the inverse trans-
formation for PADT more problematic than for the
other treebanks. The absence of a positive effect for
PADT is therefore understandable given the deeply
nested non-projective constructions in PADT.
However, one question that still remains is why
SDT and PDT, which are so similar in terms of both
nesting depth and amount of non-projectivity, be-
972
Figure 2: Learning curves for Alpino measured as
error reduction for ASU .
have differently with respect to pseudo-projective
parsing. Another factor that may be important here
is the amount of training data available. As shown
in table 1, PDT is more than 40 times larger than
SDT. To investigate the influence of training set
size, a learning curve experiment has been per-
formed. Alpino is a suitable data set for this due
to its relatively large amount of both data and non-
projectivity.
Figure 2 shows the learning curve for pseudo-
projective parsing (P-Proj), compared to using only
projectivized training data (Proj), measured as error
reduction in relation to the original non-projective
training data (N-Proj). The experiment was per-
formed by incrementally adding cross-validation
folds 1?8 to the training set, using folds 9?0 as static
test data.
One can note that the error reduction for Proj is
unaffected by the amount of data. While the error
reduction varies slightly, it turns out that the error
reduction is virtually the same for 10% of the train-
ing data as for 80%. That is, there is no correla-
tion if information concerning the lifts are not added
to the labels. However, with a pseudo-projective
transformation, which actively tries to recover non-
projectivity, the learning curve clearly indicates that
the amount of data matters. Alpino, with 36% non-
projective sentences, starts at about 17% and has a
climbing curve up to almost 25%.
Although this experiment shows that there is a
correlation between the amount of data and the accu-
racy for pseudo-projective parsing, it does probably
not tell the whole story. If it did, one would expect
that the error reduction for the pseudo-projective
transformation would be much closer to Proj when
None Coord VG
SDT 77.27 79.33?? 77.92??
PADT 76.96 79.05?? -
Alpino 82.75 83.38?? -
PDT 83.41 85.51?? 83.58??
Table 4: ASU for coordination and verb group trans-
formations with MaltParser (None = N-Proj). Mc-
Nemar?s test: ?? = p < .01 compared to None.
the amount of data is low (to the left in the fig-
ure) than they apparently are. Of course, the dif-
ference is likely to diminish with even less data, but
it should be noted that 10% of Alpino has about half
the size of PADT, for which the positive impact of
pseudo-projective parsing is absent. The absence
of increased accuracy for SDT can partially be ex-
plained by the higher share of non-projective arcs in
Alpino (3 times more).
Coordination and Verb Groups
The corresponding parsing results using MaltParser
with transformations for coordination and verb
groups are shown in table 4. For SDT, PADT and
PDT, the annotation of coordination has been trans-
formed from PS to MS, as described in Nilsson et
al. (2006). For Alpino, the transformation is from
PS to CS (cf. section 2.2), which was found to give
slightly better performance in preliminary experi-
ments. The baseline with no transformation (None)
is the same as N-Proj in table 2.
As the figures indicate, transforming coordination
is beneficial not only for PDT, as reported by Nilsson
et al (2006), but also for SDT, PADT, and Alpino. It
is interesting to note that SDT, PADT and PDT, with
comparable amounts of conjuncts, have compara-
ble increases in accuracy (about 2 percentage points
each), despite the large differences in training set
size. It is therefore not surprising that Alpino, with
a much smaller amount of conjuncts, has a lower in-
crease in accuracy. Taken together, these results in-
dicate that the frequency of the construction is more
important than the size of the training set for this
type of transformation.
The same generalization over treebanks holds for
verb groups too. The last column in table 4 shows
that the expected increase in accuracy for PDT is ac-
973
Algorithm N-Proj Proj P-Proj
Eisner 81.79 83.23 86.45
CLE 86.39
Table 5: Pseudo-projective parsing results (ASU ) for
Alpino with MSTParser.
companied by a even higher increase for SDT. This
can probably be attributed to the higher frequency of
auxiliary verbs in SDT.
5.2 Comparing Parsers
The main question in this section is to what extent
the positive effect of different tree transformations
is dependent on parsing strategy, since all previ-
ous experiments have been performed with a single
parser (MaltParser). For comparison we have per-
formed two experiments with MSTParser, version
0.1, which is based on a very different parsing meth-
dology (cf. section 4). Due to some technical dif-
ficulties (notably the very high memory consump-
tion when using MSTParser for labeled dependency
parsing), we have not been able to replicate the ex-
periments from the preceding section exactly. The
results presented below must therefore be regarded
as a preliminary exploration of the dependencies be-
tween tree transformations and parsing strategy.
Table 5 presents ASU results for MSTParser in
combination with pseudo-projective parsing applied
to the Alpino treebank of Dutch.3 The first row
contains the result for Eisner?s algorithm using no
transformation (N-Proj), projectivized training data
(Proj), and pseudo-projective parsing (P-Proj). The
figures show a pattern very similar to that for Malt-
Parser, with a boost in accuracy for Proj compared
to N-Proj, and with a significantly higher accuracy
for P-Proj over Proj. It is also worth noting that the
error reduction between N-Proj and P-Proj is actu-
ally higher for MSTParser here than for MaltParser
in table 2.
The second row contains the result for the Chu-
Liu-Edmonds algorithm (CLE), which constructs
non-projective structures directly and therefore does
3The figures are not completely comparable to the previ-
ously presented Dutch results for MaltParser, sinceMaltParser?s
feature model has access to all the information in the CoNLL
data format, whereas MSTParser in this experiment only could
handle word forms and part-of-speech tags.
Trans. None Coord VG
ASU 84.5 83.5 84.5
Table 6: Coordination and verb group transforma-
tions for PDT with the CLE algorithm.
Dev Eval Niv McD
SDT ASU 80.40 82.01 78.72 83.17
ASL 71.06 72.44 70.30 73.44
PADT ASU 78.97 78.56 77.52 79.34
ASL 67.63 67.58 66.71 66.91
Alpino ASU 87.63 82.85 81.35 83.57
ASL 84.02 79.73 78.59 79.19
PDT ASU 85.72 85.98 84.80 87.30
ASL 78.56 78.80 78.42 80.18
Table 7: Evaluation on CoNLL-X test data; Malt-
Parser with all transformations (Dev = development,
Eval = CoNLL test set, Niv = Nivre et al (2006),
McD = McDonald et al (2006))
not require the pseudo-projective transformation.
A comparison between Eisner?s algorithm with
pseudo-projective transformation and CLE reveals
that pseudo-projective parsing is at least as accurate
as non-projective parsing for ASU . (The small dif-
ference is not statistically significant.)
By contrast, no positive effect could be detected
for the coordination and verb group transformations
togther with MSTParser. The figures in table 6 are
not based on CoNLL data, but instead on the evalu-
ation test set of the original PDT 1.0, which enables
a direct comparison to McDonald et. al. (2005) (the
None column). We see that there is even a negative
effect for the coordination transformation. These re-
sults clearly indicate that the effect of these transfor-
mations is at least partly dependent on parsing strat-
egy, in contrast to what was found for the pseudo-
projective parsing technique.
5.3 Combining Transformations
In order to assess the combined effect of all three
transformations in relation to the state of the art,
we performed a final evaluation using MaltParser on
the dedicated test sets from the CoNLL-X shared
task. Table 7 gives the results for both develop-
ment (cross-validation for SDT, PADT, and Alpino;
974
development set for PDT) and final test, compared
to the two top performing systems in the shared
task, MSTParser with approximate second-order
non-projective parsing (McDonald et al, 2006) and
MaltParser with pseudo-projective parsing (but no
coordination or verb group transformations) (Nivre
et al, 2006). Looking at the labeled attachment
score (ASL), the official scoring metric of the
CoNLL-X shared task, we see that the combined ef-
fect of the three transformations boosts the perfor-
mance of MaltParser for all treebanks and in two
cases out of four outperforms MSTParser (which
was the top scoring system for all four treebanks).
6 Conclusion
In this paper, we have examined the generality
of tree transformations for data-driven dependency
parsing. The results indicate that the pseudo-
projective parsing technique has a positive effect
on parsing accuracy that is independent of parsing
methodology but sensitive to the amount of training
data as well as to the complexity of non-projective
constructions. By contrast, the construction-specific
transformations targeting coordination and verb
groups appear to have a more language-independent
effect (for languages to which they are applicable)
but do not help for all parsers. More research is
needed in order to know exactly what the dependen-
cies are between parsing strategy and tree transfor-
mations. Regardless of this, however, it is safe to
conclude that pre-processing and post-processing is
important not only in constituency-based parsing, as
previously shown in a number of studies, but also for
inductive dependency parsing.
References
D. Bikel. 2004. Intricacies of Collins? parsing model. Compu-
tational Linguistics, 30:479?511.
S. Buchholz and E. Marsi. 2006. CoNLL-X Shared Task
on Multilingual Dependency Parsing. In Proceedings of
CoNLL, pages 1?17.
R. Campbell. 2004. Using Linguistic Principles to Recover
Empty Categories. In Proceedings of ACL, pages 645?652.
E. Charniak. 2000. A Maximum-Entropy-Inspired Parser. In
Proceedings of NAACL, pages 132?139.
M. Collins, J. Hajic?, L. Ramshaw, and C. Tillmann. 1999. A
statistical parser for Czech. In Proceedings of ACL, pages
100?110.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
S. Dz?eroski, T. Erjavec, N. Ledinek, P. Pajas, Z. Z?abokrtsky, and
A. Z?ele. 2006. Towards a Slovene Dependency Treebank.
In LREC.
J. Hajic?, B. V. Hladka, J. Panevova?, Eva Hajic?ova?, Petr Sgall,
and Petr Pajas. 2001. Prague Dependency Treebank 1.0.
LDC, 2001T10.
J. Hajic?, O. Smrz?, P. Zema?nek, J. S?naidauf, and E. Bes?ka. 2004.
Prague Arabic Dependency Treebank: Development in Data
and Tools. In NEMLAR, pages 110?117.
K. Hall and V. Nova?k. 2005. Corrective modeling for non-
projective dependency parsing. In Proceedings of IWPT,
pages 42?52.
M. Johnson. 1998. PCFG Models of Linguistic Tree Represen-
tations. Computational Linguistics, 24:613?632.
S. Kahane, A. Nasr, and O. Rambow. 1998. Pseudo-
Projectivity: A Polynomially Parsable Non-Projective De-
pendency Grammar. In Proceedings of COLING/ACL, pages
646?652.
D. Klein and C. Manning. 2003. Accurate unlexicalized pars-
ing. In Proceedings of ACL, pages 423?430.
R. McDonald and F. Pereira. 2006. Online Learning of Ap-
proximate Dependency Parsing Algorithms. In Proceedings
of EACL, pages 81?88.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005.
Non-projective dependency parsing using spanning tree al-
gorithms. In Proceedings of HLT/EMNLP, pages 523?530.
R. McDonald, K. Lerman, and F. Pereira. 2006. Multilingual
dependency analysis with a two-stage discriminative parser.
In Proceedings of CoNLL, pages 216?220.
I. Mel?c?uk. 1988. Dependency Syntax: Theory and Practice.
State University of New York Press.
J. Nilsson, J. Nivre, and J. Hall. 2006. Graph Transforma-
tions in Data-Driven Dependency Parsing. In Proceedings
of COLING/ACL, pages 257?264.
J. Nivre and J. Nilsson. 2005. Pseudo-Projective Dependency
Parsing. In Proceedings of ACL, pages 99?106.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based Depen-
dency Parsing. In H. T. Ng and E. Riloff, editors, Proceed-
ings of CoNLL, pages 49?56.
J. Nivre, J. Hall, J. Nilsson, G. Eryig?it, and S. Marinov. 2006.
Labeled Pseudo-Projective Dependency Parsing with Sup-
port Vector Machines. In Proceedings of CoNLL, pages
221?225.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learning
Accurate, Compact, and Interpretable Tree Annotation. In
Proceedings of COLING/ACL, pages 433?440.
L. van der Beek, G. Bouma, R. Malouf, and G. van Noord.
2002. The Alpino dependency treebank. In Computational
Linguistics in the Netherlands (CLIN).
975
Proceedings of ACL-08: HLT, pages 950?958,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Integrating Graph-Based and Transition-Based Dependency Parsers
Joakim Nivre
Va?xjo? University Uppsala University
Computer Science Linguistics and Philology
SE-35195 Va?xjo? SE-75126 Uppsala
nivre@msi.vxu.se
Ryan McDonald
Google Inc.
76 Ninth Avenue
New York, NY 10011
ryanmcd@google.com
Abstract
Previous studies of data-driven dependency
parsing have shown that the distribution of
parsing errors are correlated with theoretical
properties of the models used for learning and
inference. In this paper, we show how these
results can be exploited to improve parsing
accuracy by integrating a graph-based and a
transition-based model. By letting one model
generate features for the other, we consistently
improve accuracy for both models, resulting
in a significant improvement of the state of
the art when evaluated on data sets from the
CoNLL-X shared task.
1 Introduction
Syntactic dependency graphs have recently gained
a wide interest in the natural language processing
community and have been used for many problems
ranging from machine translation (Ding and Palmer,
2004) to ontology construction (Snow et al, 2005).
A dependency graph for a sentence represents each
word and its syntactic dependents through labeled
directed arcs, as shown in figure 1. One advantage
of this representation is that it extends naturally to
discontinuous constructions, which arise due to long
distance dependencies or in languages where syntac-
tic structure is encoded in morphology rather than in
word order. This is undoubtedly one of the reasons
for the emergence of dependency parsers for a wide
range of languages. Many of these parsers are based
on data-driven parsing models, which learn to pro-
duce dependency graphs for sentences solely from
an annotated corpus and can be easily ported to any
Figure 1: Dependency graph for an English sentence.
language or domain in which annotated resources
exist.
Practically all data-driven models that have been
proposed for dependency parsing in recent years can
be described as either graph-based or transition-
based (McDonald and Nivre, 2007). In graph-based
parsing, we learn a model for scoring possible de-
pendency graphs for a given sentence, typically by
factoring the graphs into their component arcs, and
perform parsing by searching for the highest-scoring
graph. This type of model has been used by, among
others, Eisner (1996), McDonald et al (2005a), and
Nakagawa (2007). In transition-based parsing, we
instead learn a model for scoring transitions from
one parser state to the next, conditioned on the parse
history, and perform parsing by greedily taking the
highest-scoring transition out of every parser state
until we have derived a complete dependency graph.
This approach is represented, for example, by the
models of Yamada and Matsumoto (2003), Nivre et
al. (2004), and Attardi (2006).
Theoretically, these approaches are very different.
The graph-based models are globally trained and use
exact inference algorithms, but define features over a
limited history of parsing decisions. The transition-
based models are essentially the opposite. They use
local training and greedy inference algorithms, but
950
define features over a rich history of parsing deci-
sions. This is a fundamental trade-off that is hard
to overcome by tractable means. Both models have
been used to achieve state-of-the-art accuracy for a
wide range of languages, as shown in the CoNLL
shared tasks on dependency parsing (Buchholz and
Marsi, 2006; Nivre et al, 2007), but McDonald and
Nivre (2007) showed that a detailed error analysis
reveals important differences in the distribution of
errors associated with the two models.
In this paper, we consider a simple way of inte-
grating graph-based and transition-based models in
order to exploit their complementary strengths and
thereby improve parsing accuracy beyond what is
possible by either model in isolation. The method
integrates the two models by allowing the output
of one model to define features for the other. This
method is simple ? requiring only the definition of
new features ? and robust by allowing a model to
learn relative to the predictions of the other.
2 Two Models for Dependency Parsing
2.1 Preliminaries
Given a set L = {l1, . . . , l|L|} of arc labels (depen-
dency relations), a dependency graph for an input
sentence x = w0, w1, . . . , wn (where w0 = ROOT) is
a labeled directed graph G = (V,A) consisting of a
set of nodes V = {0, 1, . . . , n}1 and a set of labeled
directed arcs A ? V ?V ?L, i.e., if (i, j, l) ? A for
i, j ? V and l ? L, then there is an arc from node
i to node j with label l in the graph. A dependency
graphG for a sentence xmust be a directed tree orig-
inating out of the root node 0 and spanning all nodes
in V , as exemplified by the graph in figure 1. This
is a common constraint in many dependency parsing
theories and their implementations.
2.2 Graph-Based Models
Graph-based dependency parsers parameterize a
model over smaller substructures in order to search
the space of valid dependency graphs and produce
the most likely one. The simplest parameterization
is the arc-factored model that defines a real-valued
score function for arcs s(i, j, l) and further defines
the score of a dependency graph as the sum of the
1We use the common convention of representing words by
their index in the sentence.
score of all the arcs it contains. As a result, the de-
pendency parsing problem is written:
G = argmax
G=(V,A)
?
(i,j,l)?A
s(i, j, l)
This problem is equivalent to finding the highest
scoring directed spanning tree in the complete graph
over the input sentence, which can be solved in
O(n2) time (McDonald et al, 2005b). Additional
parameterizations are possible that take more than
one arc into account, but have varying effects on
complexity (McDonald and Satta, 2007). An advan-
tage of graph-based methods is that tractable infer-
ence enables the use of standard structured learning
techniques that globally set parameters to maximize
parsing performance on the training set (McDonald
et al, 2005a). The primary disadvantage of these
models is that scores ? and as a result any feature
representations ? are restricted to a single arc or a
small number of arcs in the graph.
The specific graph-based model studied in this
work is that presented by McDonald et al (2006),
which factors scores over pairs of arcs (instead of
just single arcs) and uses near exhaustive search for
unlabeled parsing coupled with a separate classifier
to label each arc. We call this system MSTParser, or
simply MST for short, which is also the name of the
freely available implementation.2
2.3 Transition-Based Models
Transition-based dependency parsing systems use a
model parameterized over transitions of an abstract
machine for deriving dependency graphs, such that
every transition sequence from the designated initial
configuration to some terminal configuration derives
a valid dependency graph. Given a real-valued score
function s(c, t) (for transition t out of configuration
c), parsing can be performed by starting from the ini-
tial configuration and taking the optimal transition
t? = argmaxt?T s(c, t) out of every configuration
c until a terminal configuration is reached. This can
be seen as a greedy search for the optimal depen-
dency graph, based on a sequence of locally optimal
decisions in terms of the transition system.
Many transition systems for data-driven depen-
dency parsing are inspired by shift-reduce parsing,
2http://mstparser.sourceforge.net
951
where each configuration c contains a stack ?c for
storing partially processed nodes and a buffer ?c
containing the remaining input. Transitions in such a
system add arcs to the dependency graph and mani-
pulate the stack and buffer. One example is the tran-
sition system defined by Nivre (2003), which parses
a sentence x = w0, w1, . . . , wn in O(n) time.
To learn a scoring function on transitions, these
systems rely on discriminative learning methods,
such as memory-based learning or support vector
machines, using a strictly local learning procedure
where only single transitions are scored (not com-
plete transition sequences). The main advantage of
these models is that features are not restricted to a
limited number of graph arcs but can take into ac-
count the entire dependency graph built so far. The
major disadvantage is that the greedy parsing strat-
egy may lead to error propagation.
The specific transition-based model studied in
this work is that presented by Nivre et al (2006),
which uses support vector machines to learn transi-
tion scores. We call this system MaltParser, or Malt
for short, which is also the name of the freely avail-
able implementation.3
2.4 Comparison and Analysis
These models differ primarily with respect to three
properties: inference, learning, and feature repre-
sentation. MaltParser uses an inference algorithm
that greedily chooses the best parsing decision based
on the current parser history whereas MSTParser
uses exhaustive search algorithms over the space of
all valid dependency graphs to find the graph that
maximizes the score. MaltParser trains a model
to make a single classification decision (choose the
next transition) whereas MSTParser trains a model
to maximize the global score of correct graphs.
MaltParser can introduce a rich feature history based
on previous parser decisions, whereas MSTParser is
forced to restrict features to a single decision or a
pair of nearby decisions in order to retain efficiency.
These differences highlight an inherent trade-off
between global inference/learning and expressive-
ness of feature representations. MSTParser favors
the former at the expense of the latter andMaltParser
the opposite. This difference was highlighted in the
3http://w3.msi.vxu.se/?jha/maltparser/
study of McDonald and Nivre (2007), which showed
that the difference is reflected directly in the error
distributions of the parsers. Thus, MaltParser is less
accurate than MSTParser for long dependencies and
those closer to the root of the graph, but more accu-
rate for short dependencies and those farthest away
from the root. Furthermore, MaltParser is more ac-
curate for dependents that are nouns and pronouns,
whereas MSTParser is more accurate for verbs, ad-
jectives, adverbs, adpositions, and conjunctions.
Given that there is a strong negative correlation
between dependency length and tree depth, and
given that nouns and pronouns tend to be more
deeply embedded than (at least) verbs and conjunc-
tions, these patterns can all be explained by the same
underlying factors. Simply put, MaltParser has an
advantage in its richer feature representations, but
this advantage is gradually diminished by the nega-
tive effect of error propagation due to the greedy in-
ference strategy as sentences and dependencies get
longer. MSTParser has a more even distribution of
errors, which is expected given that the inference al-
gorithm and feature representation should not prefer
one type of arc over another. This naturally leads
one to ask: Is it possible to integrate the two models
in order to exploit their complementary strengths?
This is the topic of the remainder of this paper.
3 Integrated Models
There are many conceivable ways of combining the
two parsers, including more or less complex en-
semble systems and voting schemes, which only
perform the integration at parsing time. However,
given that we are dealing with data-driven models,
it should be possible to integrate at learning time, so
that the two complementary models can learn from
one another. In this paper, we propose to do this by
letting one model generate features for the other.
3.1 Feature-Based Integration
As explained in section 2, both models essentially
learn a scoring function s : X ? R, where the
domain X is different for the two models. For the
graph-based model, X is the set of possible depen-
dency arcs (i, j, l); for the transition-based model,
X is the set of possible configuration-transition pairs
(c, t). But in both cases, the input is represented
952
MSTMalt ? defined over (i, j, l) (? = any label/node)
Is (i, j, ?) in GMaltx ?
Is (i, j, l) in GMaltx ?
Is (i, j, ?) not in GMaltx ?
Is (i, j, l) not in GMaltx ?
Identity of l? such that (?, j, l?) is in GMaltx ?
Identity of l? such that (i, j, l?) is in GMaltx ?
MaltMST ? defined over (c, t) (? = any label/node)
Is (?0c , ?
0
c , ?) in G
MST
x ?
Is (?0c , ?
0
c , ?) in G
MST
x ?
Head direction for ?0c in G
MST
x (left/right/ROOT)
Head direction for ?0c in G
MST
x (left/right/ROOT)
Identity of l such that (?, ?0c , l) is in G
MST
x ?
Identity of l such that (?, ?0c , l) is in G
MST
x ?
Table 1: Guide features for MSTMalt and MaltMST.
by a k-dimensional feature vector f : X ? Rk.
In the feature-based integration we simply extend
the feature vector for one model, called the base
model, with a certain number of features generated
by the other model, which we call the guide model
in this context. The additional features will be re-
ferred to as guide features, and the version of the
base model trained with the extended feature vector
will be called the guided model. The idea is that the
guided model should be able to learn in which situ-
ations to trust the guide features, in order to exploit
the complementary strength of the guide model, so
that performance can be improved with respect to
the base parser. This method of combining classi-
fiers is sometimes referred to as classifier stacking.
The exact form of the guide features depend on
properties of the base model and will be discussed
in sections 3.2?3.3 below, but the overall scheme for
the feature-based integration can be described as fol-
lows. To train a guided version BC of base model B
with guide model C and training set T , the guided
model is trained, not on the original training set T ,
but on a version of T that has been parsed with the
guide model C under a cross-validation scheme (to
avoid overlap with training data for C). This means
that, for every sentence x ? T , BC has access at
training time to both the gold standard dependency
graph Gx and the graph GCx predicted by C, and it is
the latter that forms the basis for the additional guide
features. When parsing a new sentence x? with BC ,
x? is first parsed with model C (this time trained on
the entire training set T ) to derive GCx? , so that the
guide features can be extracted also at parsing time.
3.2 The Guided Graph-Based Model
The graph-based model, MSTParser, learns a scor-
ing function s(i, j, l) ? R over labeled dependen-
cies. More precisely, dependency arcs (or pairs of
arcs) are first represented by a high dimensional fea-
ture vector f(i, j, l) ? Rk, where f is typically a bi-
nary feature vector over properties of the arc as well
as the surrounding input (McDonald et al, 2005a;
McDonald et al, 2006). The score of an arc is de-
fined as a linear classifier s(i, j, l) = w ? f(i, j, l),
where w is a vector of feature weights to be learned
by the model.
For the guided graph-based model, which we call
MSTMalt, this feature representation is modified to
include an additional argument GMaltx , which is the
dependency graph predicted by MaltParser on the
input sentence x. Thus, the new feature represen-
tation will map an arc and the entire predicted Malt-
Parser graph to a high dimensional feature repre-
sentation, f(i, j, l, GMaltx ) ? R
k+m. These m ad-
ditional features account for the guide features over
the MaltParser output. The specific features used by
MSTMalt are given in table 1. All features are con-
joined with the part-of-speech tags of the words in-
volved in the dependency to allow the guided parser
to learn weights relative to different surface syntac-
tic environments. Though MSTParser is capable of
defining features over pairs of arcs, we restrict the
guide features over single arcs as this resulted in
higher accuracies during preliminary experiments.
3.3 The Guided Transition-Based Model
The transition-based model, MaltParser, learns a
scoring function s(c, t) ? R over configurations and
transitions. The set of training instances for this
learning problem is the set of pairs (c, t) such that
t is the correct transition out of c in the transition
sequence that derives the correct dependency graph
Gx for some sentence x in the training set T . Each
training instance (c, t) is represented by a feature
vector f(c, t) ? Rk, where features are defined in
terms of arbitrary properties of the configuration c,
including the state of the stack ?c, the input buffer
?c, and the partially built dependency graph Gc. In
particular, many features involve properties of the
two target tokens, the token on top of the stack ?c
(?0c ) and the first token in the input buffer ?c (?
0
c ),
953
which are the two tokens that may become con-
nected by a dependency arc through the transition
out of c. The full set of features used by the base
model MaltParser is described in Nivre et al (2006).
For the guided transition-based model, which we
call MaltMST, training instances are extended to
triples (c, t, GMSTx ), where G
MST
x is the dependency
graph predicted by the graph-based MSTParser for
the sentence x to which the configuration c belongs.
We define m additional guide features, based on
properties of GMSTx , and extend the feature vector
accordingly to f(c, t, GMSTx ) ? R
k+m. The specific
features used by MaltMST are given in table 1. Un-
like MSTParser, features are not explicitly defined
to conjoin guide features with part-of-speech fea-
tures. These features are implicitly added through
the polynomial kernel used to train the SVM.
4 Experiments
In this section, we present an experimental evalua-
tion of the two guided models based on data from
the CoNLL-X shared task, followed by a compar-
ative error analysis including both the base models
and the guided models. The data for the experiments
are training and test sets for all thirteen languages
from the CoNLL-X shared task on multilingual de-
pendency parsing with training sets ranging in size
from from 29,000 tokens (Slovene) to 1,249,000 to-
kens (Czech). The test sets are all standardized to
about 5,000 tokens each. For more information on
the data sets, see Buchholz and Marsi (2006).
The guided models were trained according to the
scheme explained in section 3, with two-fold cross-
validation when parsing the training data with the
guide parsers. Preliminary experiments suggested
that cross-validation with more folds had a negli-
gible impact on the results. Models are evaluated
by their labeled attachment score (LAS) on the test
set, i.e., the percentage of tokens that are assigned
both the correct head and the correct label, using
the evaluation software from the CoNLL-X shared
task with default settings.4 Statistical significance
was assessed using Dan Bikel?s randomized pars-
ing evaluation comparator with the default setting of
10,000 iterations.5
4http://nextens.uvt.nl/?conll/software.html
5http://www.cis.upenn.edu/?dbikel/software.html
Language MST MSTMalt Malt MaltMST
Arabic 66.91 68.64 (+1.73) 66.71 67.80 (+1.09)
Bulgarian 87.57 89.05 (+1.48) 87.41 88.59 (+1.18)
Chinese 85.90 88.43 (+2.53) 86.92 87.44 (+0.52)
Czech 80.18 82.26 (+2.08) 78.42 81.18 (+2.76)
Danish 84.79 86.67 (+1.88) 84.77 85.43 (+0.66)
Dutch 79.19 81.63 (+2.44) 78.59 79.91 (+1.32)
German 87.34 88.46 (+1.12) 85.82 87.66 (+1.84)
Japanese 90.71 91.43 (+0.72) 91.65 92.20 (+0.55)
Portuguese 86.82 87.50 (+0.68) 87.60 88.64 (+1.04)
Slovene 73.44 75.94 (+2.50) 70.30 74.24 (+3.94)
Spanish 82.25 83.99 (+1.74) 81.29 82.41 (+1.12)
Swedish 82.55 84.66 (+2.11) 84.58 84.31 (?0.27)
Turkish 63.19 64.29 (+1.10) 65.58 66.28 (+0.70)
Average 80.83 82.53 (+1.70) 80.74 82.01 (+1.27)
Table 2: Labeled attachment scores for base parsers and
guided parsers (improvement in percentage points).
10 20 30 40 50 60Sentence Length
0.7
0.75
0.8
0.85
0.9
Accu
racy
MaltMSTMalt+MSTMST+Malt
Figure 2: Accuracy relative to sentence length.
4.1 Results
Table 2 shows the results, for each language and on
average, for the two base models (MST, Malt) and
for the two guided models (MSTMalt, MaltMST).
First of all, we see that both guided models show
a very consistent increase in accuracy compared to
their base model, even though the extent of the im-
provement varies across languages from about half
a percentage point (MaltMST on Chinese) up to al-
most four percentage points (MaltMST on Slovene).6
It is thus quite clear that both models have the capa-
city to learn from features generated by the other
model. However, it is also clear that the graph-based
MST model shows a somewhat larger improvement,
both on average and for all languages except Czech,
6The only exception to this pattern is the result for MaltMST
on Swedish, where we see an unexpected drop in accuracy com-
pared to the base model.
954
2 4 6 8 10 12             14      15+Dependency Length
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
Recal
l
MaltMSTMalt+MSTMST+Malt
2 4 6 8 10 12             14      15+Dependency Length0.55
0.6
0.65
0.7
0.75
0.8
0.85
Precis
ion
MaltMSTMalt+MSTMST+Malt
1 2 3 4 5 6 7+Distance to Root
0.8
0.82
0.84
0.86
0.88
0.9
Recall
MaltMSTMalt+MSTMST+Malt
1 2 3 4 5 6 7+Distance to Root
0.78
0.8
0.82
0.84
0.86
0.88
0.9
0.92
Precis
ion
MaltMSTMalt+MSTMST+Malt
(a) (b)
Figure 3: Dependency arc precision/recall relative to predicted/gold for (a) dependency length and (b) distance to root.
German, Portuguese and Slovene. Finally, given
that the two base models had the previously best
performance for these data sets, the guided models
achieve a substantial improvement of the state of the
art. While there is no statistically significant differ-
ence between the two base models, they are both
outperformed by MaltMST (p < 0.0001), which in
turn has significantly lower accuracy than MSTMalt
(p < 0.0005).
An extension to the models described so far would
be to iteratively integrate the two parsers in the
spirit of pipeline iteration (Hollingshead and Roark,
2007). For example, one could start with a Malt
model, use it to train a guided MSTMalt model, then
use that as the guide to train a MaltMSTMalt model,
etc. We ran such experiments, but found that accu-
racy did not increase significantly and in some cases
decreased slightly. This was true regardless of which
parser began the iterative process. In retrospect, this
result is not surprising. Since the initial integration
effectively incorporates knowledge from both pars-
ing systems, there is little to be gained by adding
additional parsers in the chain.
4.2 Analysis
The experimental results presented so far show that
feature-based integration is a viable approach for
improving the accuracy of both graph-based and
transition-based models for dependency parsing, but
they say very little about how the integration benefits
the two models and what aspects of the parsing pro-
cess are improved as a result. In order to get a better
understanding of these matters, we replicate parts of
the error analysis presented by McDonald and Nivre
(2007), where parsing errors are related to different
structural properties of sentences and their depen-
dency graphs. For each of the four models evalu-
ated, we compute error statistics for labeled attach-
ment over all twelve languages together.
Figure 2 shows accuracy in relation to sentence
length, binned into ten-word intervals (1?10, 11-20,
etc.). As expected, Malt and MST have very simi-
lar accuracy for short sentences but Malt degrades
more rapidly with increasing sentence length be-
cause of error propagation (McDonald and Nivre,
2007). The guided models, MaltMST and MSTMalt,
behave in a very similar fashion with respect to each
other but both outperform their base parser over the
entire range of sentence lengths. However, except
for the two extreme data points (0?10 and 51?60)
there is also a slight tendency for MaltMST to im-
prove more for longer sentences and for MSTMalt to
improve more for short sentences, which indicates
that the feature-based integration allows one parser
to exploit the strength of the other.
Figure 3(a) plots precision (top) and recall (bot-
tom) for dependency arcs of different lengths (pre-
dicted arcs for precision, gold standard arcs for re-
call). With respect to recall, the guided models ap-
pear to have a slight advantage over the base mod-
955
Part of Speech MST MSTMalt Malt MaltMST
Verb 82.6 85.1 (2.5) 81.9 84.3 (2.4)
Noun 80.0 81.7 (1.7) 80.7 81.9 (1.2)
Pronoun 88.4 89.4 (1.0) 89.2 89.3 (0.1)
Adjective 89.1 89.6 (0.5) 87.9 89.0 (1.1)
Adverb 78.3 79.6 (1.3) 77.4 78.1 (0.7)
Adposition 69.9 71.5 (1.6) 68.8 70.7 (1.9)
Conjunction 73.1 74.9 (1.8) 69.8 72.5 (2.7)
Table 3: Accuracy relative to dependent part of speech
(improvement in percentage points).
els for short and medium distance arcs. With re-
spect to precision, however, there are two clear pat-
terns. First, the graph-based models have better pre-
cision than the transition-based models when pre-
dicting long arcs, which is compatible with the re-
sults of McDonald and Nivre (2007). Secondly, both
the guided models have better precision than their
base model and, for the most part, also their guide
model. In particular MSTMalt outperformsMST and
is comparable to Malt for short arcs. More inter-
estingly, MaltMST outperforms both Malt and MST
for arcs up to length 9, which provides evidence that
MaltMST has learned specifically to trust the guide
features from MST for longer dependencies. The
reason that accuracy does not improve for dependen-
cies of length greater than 9 is probably that these
dependencies are too rare for MaltMST to learn from
the guide parser in these situations.
Figure 3(b) shows precision (top) and recall (bot-
tom) for dependency arcs at different distances from
the root (predicted arcs for precision, gold standard
arcs for recall). Again, we find the clearest pat-
terns in the graphs for precision, where Malt has
very low precision near the root but improves with
increasing depth, while MST shows the opposite
trend (McDonald and Nivre, 2007). Considering
the guided models, it is clear that MaltMST im-
proves in the direction of its guide model, with a
5-point increase in precision for dependents of the
root and smaller improvements for longer distances.
Similarly, MSTMalt improves precision in the range
where its base parser is inferior to Malt and for dis-
tances up to 4 has an accuracy comparable to or
higher than its guide parser Malt. This again pro-
vides evidence that the guided parsers are learning
from their guide models.
Table 3 gives the accuracy for arcs relative to de-
pendent part-of-speech. As expected, we see that
MST does better than Malt for all categories except
nouns and pronouns (McDonald and Nivre, 2007).
But we also see that the guided models in all cases
improve over their base parser and, in most cases,
also over their guide parser. The general trend is that
MST improves more thanMalt, except for adjectives
and conjunctions, where Malt has a greater disad-
vantage from the start and therefore benefits more
from the guide features.
Considering the results for parts of speech, as well
as those for dependency length and root distance, it
is interesting to note that the guided models often
improve even in situations where their base parsers
are more accurate than their guide models. This sug-
gests that the improvement is not a simple function
of the raw accuracy of the guide model but depends
on the fact that labeled dependency decisions inter-
act in inference algorithms for both graph-based and
transition-based parsing systems. Thus, if a parser
can improve its accuracy on one class of dependen-
cies, e.g., longer ones, then we can expect to see im-
provements on all types of dependencies ? as we do.
The interaction between different decisions may
also be part of the explanation why MST benefits
more from the feature-based integration than Malt,
with significantly higher accuracy for MSTMalt than
for MaltMST as a result. Since inference is global
(or practically global) in the graph-based model,
an improvement in one type of dependency has a
good chance of influencing the accuracy of other de-
pendencies, whereas in the transition-based model,
where inference is greedy, some of these additional
benefits will be lost because of error propagation.
This is reflected in the error analysis in the following
recurrent pattern: Where Malt does well, MaltMST
does only slightly better. But where MST is good,
MSTMalt is often significantly better.
Another part of the explanation may have to do
with the learning algorithms used by the systems.
Although both Malt and MST use discriminative
algorithms, Malt uses a batch learning algorithm
(SVM) and MST uses an online learning algorithm
(MIRA). If the original rich feature representation
of Malt is sufficient to separate the training data,
regularization may force the weights of the guided
features to be small (since they are not needed at
training time). On the other hand, an online learn-
956
ing algorithm will recognize the guided features as
strong indicators early in training and give them a
high weight as a result. Features with high weight
early in training tend to have the most impact on the
final classifier due to both weight regularization and
averaging. This is in fact observed when inspecting
the weights of MSTMalt.
5 Related Work
Combinations of graph-based and transition-based
models for data-driven dependency parsing have
previously been explored by Sagae and Lavie
(2006), who report improvements of up to 1.7 per-
centage points over the best single parser when
combining three transition-based models and one
graph-based model for unlabeled dependency pars-
ing, evaluated on data from the Penn Treebank. The
combined parsing model is essentially an instance of
the graph-based model, where arc scores are derived
from the output of the different component parsers.
Unlike the models presented here, integration takes
place only at parsing time, not at learning time, and
requires at least three different base parsers. The
same technique was used by Hall et al (2007) to
combine six transition-based parsers in the best per-
forming system in the CoNLL 2007 shared task.
Feature-based integration in the sense of letting a
subset of the features for one model be derived from
the output of a different model has been exploited
for dependency parsing by McDonald (2006), who
trained an instance of MSTParser using features
generated by the parsers of Collins (1999) and Char-
niak (2000), which improved unlabeled accuracy by
1.7 percentage points, again on data from the Penn
Treebank. In addition, feature-based integration has
been used by Taskar et al (2005), who trained a
discriminative word alignment model using features
derived from the IBM models, and by Florian et al
(2004), who trained classifiers on auxiliary data to
guide named entity classifiers.
Feature-based integration also has points in com-
mon with co-training, which have been applied to
syntactic parsing by Sarkar (2001) and Steedman et
al. (2003), among others. The difference, of course,
is that standard co-training is a weakly supervised
method, where guide features replace, rather than
complement, the gold standard annotation during
training. Feature-based integration is also similar to
parse re-ranking (Collins, 2000), where one parser
produces a set of candidate parses and a second-
stage classifier chooses the most likely one. How-
ever, feature-based integration is not explicitly con-
strained to any parse decisions that the guide model
might make and only the single most likely parse is
used from the guide model, making it significantly
more efficient than re-ranking.
Finally, there are several recent developments in
data-driven dependency parsing, which can be seen
as targeting the specific weaknesses of graph-based
and transition-based models, respectively, though
without integrating the two models. Thus, Naka-
gawa (2007) and Hall (2007) both try to overcome
the limited feature scope of graph-based models by
adding global features, in the former case using
Gibbs sampling to deal with the intractable infer-
ence problem, in the latter case using a re-ranking
scheme. For transition-based models, the trend is
to alleviate error propagation by abandoning greedy,
deterministic inference in favor of beam search with
globally normalized models for scoring transition
sequences, either generative (Titov and Henderson,
2007a; Titov and Henderson, 2007b) or conditional
(Duan et al, 2007; Johansson and Nugues, 2007).
6 Conclusion
In this paper, we have demonstrated how the two
dominant approaches to data-driven dependency
parsing, graph-based models and transition-based
models, can be integrated by letting one model learn
from features generated by the other. Our experi-
mental results show that both models consistently
improve their accuracy when given access to fea-
tures generated by the other model, which leads to
a significant advancement of the state of the art in
data-driven dependency parsing. Moreover, a com-
parative error analysis reveals that the improvements
are largely predictable from theoretical properties of
the two models, in particular the tradeoff between
global learning and inference, on the one hand, and
rich feature representations, on the other. Directions
for future research include a more detailed analysis
of the effect of feature-based integration, as well as
the exploration of other strategies for integrating dif-
ferent parsing models.
957
References
Giuseppe Attardi. 2006. Experiments with a multilan-
guage non-projective dependency parser. In Proceed-
ings of CoNLL, pages 166?170.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL, pages 132?139.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proceedings of ICML, pages
175?182.
Yuan Ding and Martha Palmer. 2004. Synchronous de-
pendency insertion grammars: A grammar formalism
for syntax based statistical MT. In Proceedings of the
Workshop on Recent Advances in Dependency Gram-
mar, pages 90?97.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Probabilis-
tic parsing action models for multi-lingual dependency
parsing. In Proceedings of EMNLP-CoNLL, pages
940?946.
Jason M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proceed-
ings of COLING, pages 340?345.
Radu Florian, Hany Hassan, Abraham Ittycheriah,
Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,
Nicolas Nicolov, and Salim Roukos. 2004. A statisti-
cal model for multilingual entity detection and track-
ing. In Proceedings of NAACL/HLT.
Johan Hall, Jens Nilsson, Joakim Nivre, Gu?lsen Eryig?it,
Bea?ta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? A study in multilin-
gual parser optimization. In Proceedings of EMNLP-
CoNLL.
Keith Hall. 2007. K-best spanning tree parsing. In Pro-
ceedings of ACL, pages 392?399.
Kristy Hollingshead and Brian Roark. 2007. Pipeline
iteration. In Proceedings of ACL, pages 952?959.
Richard Johansson and Pierre Nugues. 2007. Incremen-
tal dependency parsing using online learning. In Pro-
ceedings of EMNLP-CoNLL, pages 1134?1138.
Ryan McDonald and Joakim Nivre. 2007. Characteriz-
ing the errors of data-driven dependency parsing mod-
els. In Proceedings of EMNLP-CoNLL, pages 122?
131.
Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency pars-
ing. In Proceedings of IWPT, pages 122?131.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005a. Online large-margin training of dependency
parsers. In Proceedings of ACL, pages 91?98.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
HLT/EMNLP, pages 523?530.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of CoNLL,
pages 216?220.
Ryan McDonald. 2006. Discriminative Learning and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Tetsuji Nakagawa. 2007. Multilingual dependency pars-
ing using global features. In Proceedings of EMNLP-
CoNLL, pages 952?956.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Proceedings
of CoNLL, pages 49?56.
Joakim Nivre, Johan Hall, Jens Nilsson, Gu?lsen Eryig?it,
and Svetoslav Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vector ma-
chines. In Proceedings of CoNLL, pages 221?225.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of EMNLP-CoNLL, pages
915?932.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of IWPT,
pages 149?160.
Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In Proceedings of NAACL: Short Papers,
pages 129?132.
Anoop Sarkar. 2001. Applying co-training methods to
statistical parsing. In Proceedings of NAACL, pages
175?182.
Rion Snow, Dan Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Proceedings of NIPS.
Mark Steedman, Rebecca Hwa, Miles Osborne, and
Anoop Sarkar. 2003. Corrected co-training for statis-
tical parsers. In Proceedings of ICML, pages 95?102.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein. 2005.
A discriminative matching approach to word align-
ment. In Proceedings of HLT/EMNLP, pages 73?80.
Ivan Titov and James Henderson. 2007a. Fast and ro-
bust multilingual dependency parsing with a genera-
tive latent variable model. In Proceedings of EMNLP-
CoNLL, pages 947?951.
Ivan Titov and James Henderson. 2007b. A latent vari-
able model for generative dependency parsing. In Pro-
ceedings of IWPT, pages 144?155.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of IWPT, pages 195?206.
958
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 351?359,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Non-Projective Dependency Parsing in Expected Linear Time
Joakim Nivre
Uppsala University, Department of Linguistics and Philology, SE-75126 Uppsala
Va?xjo? University, School of Mathematics and Systems Engineering, SE-35195 Va?xjo?
E-mail: joakim.nivre@lingfil.uu.se
Abstract
We present a novel transition system for
dependency parsing, which constructs arcs
only between adjacent words but can parse
arbitrary non-projective trees by swapping
the order of words in the input. Adding
the swapping operation changes the time
complexity for deterministic parsing from
linear to quadratic in the worst case, but
empirical estimates based on treebank data
show that the expected running time is in
fact linear for the range of data attested in
the corpora. Evaluation on data from five
languages shows state-of-the-art accuracy,
with especially good results for the labeled
exact match score.
1 Introduction
Syntactic parsing using dependency structures has
become a standard technique in natural language
processing with many different parsing models, in
particular data-driven models that can be trained
on syntactically annotated corpora (Yamada and
Matsumoto, 2003; Nivre et al, 2004; McDonald
et al, 2005a; Attardi, 2006; Titov and Henderson,
2007). A hallmark of many of these models is that
they can be implemented very efficiently. Thus,
transition-based parsers normally run in linear or
quadratic time, using greedy deterministic search
or fixed-width beam search (Nivre et al, 2004; At-
tardi, 2006; Johansson and Nugues, 2007; Titov
and Henderson, 2007), and graph-based models
support exact inference in at most cubic time,
which is efficient enough to make global discrim-
inative training practically feasible (McDonald et
al., 2005a; McDonald et al, 2005b).
However, one problem that still has not found
a satisfactory solution in data-driven dependency
parsing is the treatment of discontinuous syntactic
constructions, usually modeled by non-projective
dependency trees, as illustrated in Figure 1. In a
projective dependency tree, the yield of every sub-
tree is a contiguous substring of the sentence. This
is not the case for the tree in Figure 1, where the
subtrees rooted at node 2 (hearing) and node 4
(scheduled) both have discontinuous yields.
Allowing non-projective trees generally makes
parsing computationally harder. Exact inference
for parsing models that allow non-projective trees
is NP hard, except under very restricted indepen-
dence assumptions (Neuhaus and Bro?ker, 1997;
McDonald and Pereira, 2006; McDonald and
Satta, 2007). There is recent work on algorithms
that can cope with important subsets of all non-
projective trees in polynomial time (Kuhlmann
and Satta, 2009; Go?mez-Rodr??guez et al, 2009),
but the time complexity is at best O(n6), which
can be problematic in practical applications. Even
the best algorithms for deterministic parsing run in
quadratic time, rather than linear (Nivre, 2008a),
unless restricted to a subset of non-projective
structures as in Attardi (2006) and Nivre (2007).
But allowing non-projective dependency trees
also makes parsing empirically harder, because
it requires that we model relations between non-
adjacent structures over potentially unbounded
distances, which often has a negative impact on
parsing accuracy. On the other hand, it is hardly
possible to ignore non-projective structures com-
pletely, given that 25% or more of the sentences
in some languages cannot be given a linguistically
adequate analysis without invoking non-projective
structures (Nivre, 2006; Kuhlmann and Nivre,
2006; Havelka, 2007).
Current approaches to data-driven dependency
parsing typically use one of two strategies to deal
with non-projective trees (unless they ignore them
completely). Either they employ a non-standard
parsing algorithm that can combine non-adjacent
substructures (McDonald et al, 2005b; Attardi,
2006; Nivre, 2007), or they try to recover non-
351
ROOT0 A1
 
?
DET
hearing2
 
?
SBJ
is3
 
?
ROOT
scheduled4
 
?
VG
on5
 
?
NMOD
the6
 
?
DET
issue7
 
?
PC
today8
 
?
ADV
.9
?
 
P
Figure 1: Dependency tree for an English sentence (non-projective).
projective dependencies by post-processing the
output of a strictly projective parser (Nivre and
Nilsson, 2005; Hall and Nova?k, 2005; McDonald
and Pereira, 2006). In this paper, we will adopt
a different strategy, suggested in recent work by
Nivre (2008b) and Titov et al (2009), and pro-
pose an algorithm that only combines adjacent
substructures but derives non-projective trees by
reordering the input words.
The rest of the paper is structured as follows.
In Section 2, we define the formal representations
needed and introduce the framework of transition-
based dependency parsing. In Section 3, we first
define a minimal transition system and explain
how it can be used to perform projective depen-
dency parsing in linear time; we then extend the
system with a single transition for swapping the
order of words in the input and demonstrate that
the extended system can be used to parse unre-
stricted dependency trees with a time complexity
that is quadratic in the worst case but still linear
in the best case. In Section 4, we present experi-
ments indicating that the expected running time of
the new system on naturally occurring data is in
fact linear and that the system achieves state-of-
the-art parsing accuracy. We discuss related work
in Section 5 and conclude in Section 6.
2 Background Notions
2.1 Dependency Graphs and Trees
Given a set L of dependency labels, a dependency
graph for a sentence x = w1, . . . , wn is a directed
graph G = (Vx, A), where
1. Vx = {0, 1, . . . , n} is a set of nodes,
2. A ? Vx ? L? Vx is a set of labeled arcs.
The set Vx of nodes is the set of positive integers
up to and including n, each corresponding to the
linear position of a word in the sentence, plus an
extra artificial root node 0. The set A of arcs is a
set of triples (i, l, j), where i and j are nodes and l
is a label. For a dependency graph G = (Vx, A) to
be well-formed, we in addition require that it is a
tree rooted at the node 0, as illustrated in Figure 1.
2.2 Transition Systems
Following Nivre (2008a), we define a transition
system for dependency parsing as a quadruple S =
(C, T, cs, Ct), where
1. C is a set of configurations,
2. T is a set of transitions, each of which is a
(partial) function t : C ? C,
3. cs is an initialization function, mapping a
sentence x = w1, . . . , wn to a configuration
c ? C,
4. Ct ? C is a set of terminal configurations.
In this paper, we take the set C of configurations
to be the set of all triples c = (?, B,A) such that
? and B are disjoint sublists of the nodes Vx of
some sentence x, andA is a set of dependency arcs
over Vx (and some label set L); we take the initial
configuration for a sentence x = w1, . . . , wn to
be cs(x) = ([0], [1, . . . , n], { }); and we take the
set Ct of terminal configurations to be the set of
all configurations of the form c = ([0], [ ], A) (for
any arc set A). The set T of transitions will be
discussed in detail in Sections 3.1?3.2.
We will refer to the list? as the stack and the list
B as the buffer, and we will use the variables ? and
? for arbitrary sublists of ? and B, respectively.
For reasons of perspicuity, we will write ? with its
head (top) to the right and B with its head to the
left. Thus, c = ([?|i], [j|?], A) is a configuration
with the node i on top of the stack ? and the node
j as the first node in the buffer B.
Given a transition system S = (C, T, cs, Ct), a
transition sequence for a sentence x is a sequence
C0,m = (c0, c1, . . . , cm) of configurations, such
that
1. c0 = cs(x),
2. cm ? Ct,
3. for every i (1 ? i ? m), ci = t(ci?1) for
some t ? T .
352
Transition Condition
LEFT-ARCl ([?|i, j], B,A)? ([?|j], B,A?{(j, l, i)}) i 6= 0
RIGHT-ARCl ([?|i, j], B,A)? ([?|i], B,A?{(i, l, j)})
SHIFT (?, [i|?], A)? ([?|i], ?, A)
SWAP ([?|i, j], ?, A)? ([?|j], [i|?], A) 0 < i < j
Figure 2: Transitions for dependency parsing; Tp = {LEFT-ARCl, RIGHT-ARCl, SHIFT}; Tu = Tp ? {SWAP}.
The parse assigned to S by C0,m is the depen-
dency graph Gcm = (Vx, Acm), where Acm is the
set of arcs in cm.
A transition system S is sound for a class G of
dependency graphs iff, for every sentence x and
transition sequence C0,m for x in S, Gcm ? G. S
is complete for G iff, for every sentence x and de-
pendency graph G for x in G, there is a transition
sequence C0,m for x in S such that Gcm = G.
2.3 Deterministic Transition-Based Parsing
An oracle for a transition system S is a function
o : C ? T . Ideally, o should always return the
optimal transition t for a given configuration c, but
all we require formally is that it respects the pre-
conditions of transitions in T . That is, if o(c) = t
then t is permissible in c. Given an oracle o, deter-
ministic transition-based parsing can be achieved
by the following simple algorithm:
PARSE(o, x)
1 c? cs(x)
2 while c 6? Ct
3 do t? o(c); c? t(c)
4 return Gc
Starting in the initial configuration cs(x), the
parser repeatedly calls the oracle function o for the
current configuration c and updates c according to
the oracle transition t. The iteration stops when a
terminal configuration is reached. It is easy to see
that, provided that there is at least one transition
sequence in S for every sentence, the parser con-
structs exactly one transition sequence C0,m for a
sentence x and returns the parse defined by the ter-
minal configuration cm, i.e., Gcm = (Vx, Acm).
Assuming that the calls o(c) and t(c) can both be
performed in constant time, the worst-case time
complexity of a deterministic parser based on a
transition system S is given by an upper bound on
the length of transition sequences in S.
When building practical parsing systems, the
oracle can be approximated by a classifier trained
on treebank data, a technique that has been used
successfully in a number of systems (Yamada and
Matsumoto, 2003; Nivre et al, 2004; Attardi,
2006). This is also the approach we will take in
the experimental evaluation in Section 4.
3 Transitions for Dependency Parsing
Having defined the set of configurations, including
initial and terminal configurations, we will now
focus on the transition set T required for depen-
dency parsing. The total set of transitions that will
be considered is given in Figure 2, but we will start
in Section 3.1 with the subset Tp (p for projective)
consisting of the first three. In Section 3.2, we
will add the fourth transition (SWAP) to get the full
transition set Tu (u for unrestricted).
3.1 Projective Dependency Parsing
The minimal transition set Tp for projective depen-
dency parsing contains three transitions:
1. LEFT-ARCl updates a configuration with i, j
on top of the stack by adding (j, l, i) to A and
replacing i, j on the stack by j alone. It is
permissible as long as i is distinct from 0.
2. RIGHT-ARCl updates a configuration with
i, j on top of the stack by adding (i, l, j) to
A and replacing i, j on the stack by i alone.
3. SHIFT updates a configuration with i as the
first node of the buffer by removing i from
the buffer and pushing it onto the stack.
The system Sp = (C, Tp, cs, Ct) is sound and
complete for the set of projective dependency
trees (over some label set L) and has been used,
in slightly different variants, by a number of
transition-based dependency parsers (Yamada and
Matsumoto, 2003; Nivre, 2004; Attardi, 2006;
353
Transition Stack (?) Buffer (B) Added Arc
[ROOT0] [A1, . . . , .9]
SHIFT [ROOT0,A1] [hearing2, . . . , .9]
SHIFT [ROOT0,A1, hearing2] [is3, . . . , .9]
LADET [ROOT0, hearing2] [is3, . . . , .9] (2, DET, 1)
SHIFT [ROOT0, hearing2, is3] [scheduled4, . . . , .9]
SHIFT [ROOT0, . . . , is3, scheduled4] [on5, . . . , .9]
SHIFT [ROOT0, . . . , scheduled4, on5] [the6, . . . , .9]
SWAP [ROOT0, . . . , is3, on5] [scheduled4, . . . , .9]
SWAP [ROOT0, hearing2, on5] [is3, . . . , .9]
SHIFT [ROOT0, . . . , on5, is3] [scheduled4, . . . , .9]
SHIFT [ROOT0, . . . , is3, scheduled4] [the6, . . . , .9]
SHIFT [ROOT0, . . . , scheduled4, the6] [issue7, . . . , .9]
SWAP [ROOT0, . . . , is3, the6] [scheduled4, . . . , .9]
SWAP [ROOT0, . . . , on5, the6] [is3, . . . , .9]
SHIFT [ROOT0, . . . , the6, is3] [scheduled4, . . . , .9]
SHIFT [ROOT0, . . . , is3, scheduled4] [issue7, . . . , .9]
SHIFT [ROOT0, . . . , scheduled4, issue7] [today8, .9]
SWAP [ROOT0, . . . , is3, issue7] [scheduled4, . . . , .9]
SWAP [ROOT0, . . . , the6, issue7] [is3, . . . , .9]
LADET [ROOT0, . . . , on5, issue7] [is3, . . . , .9] (7, DET, 6)
RAPC [ROOT0, hearing2, on5] [is3, . . . , .9] (5, PC, 7)
RANMOD [ROOT0, hearing2] [is3, . . . , .9] (2, NMOD, 5)
SHIFT [ROOT0, . . . , hearing2, is3] [scheduled4, . . . , .9]
LASBJ [ROOT0, is3] [scheduled4, . . . , .9] (3, SBJ, 2)
SHIFT [ROOT0, is3, scheduled4] [today8, .9]
SHIFT [ROOT0, . . . , scheduled4, today8] [.9]
RAADV [ROOT0, is3, scheduled4] [.9] (4, ADV, 8)
RAVG [ROOT0, is3] [.9] (3, VG, 4)
SHIFT [ROOT0, is3, .9] [ ]
RAP [ROOT0, is3] [ ] (3, P, 9)
RAROOT [ROOT0] [ ] (0, ROOT, 3)
Figure 3: Transition sequence for parsing the sentence in Figure 1 (LA = LEFT-ARC, RA = REFT-ARC).
Nivre, 2008a). For proofs of soundness and com-
pleteness, see Nivre (2008a).
As noted in section 2, the worst-case time com-
plexity of a deterministic transition-based parser is
given by an upper bound on the length of transition
sequences. In Sp, the number of transitions for a
sentence x = w1, . . . , wn is always exactly 2n,
since a terminal configuration can only be reached
after n SHIFT transitions (moving nodes 1, . . . , n
from B to ?) and n applications of LEFT-ARCl or
RIGHT-ARCl (removing the same nodes from ?).
Hence, the complexity of deterministic parsing is
O(n) in the worst case (as well as in the best case).
3.2 Unrestricted Dependency Parsing
We now consider what happens when we add the
fourth transition from Figure 2 to get the extended
transition set Tu. The SWAP transition updates
a configuration with stack [?|i, j] by moving the
node i back to the buffer. This has the effect that
the order of the nodes i and j in the appended list
?+B is reversed compared to the original word
order in the sentence. It is important to note that
SWAP is only permissible when the two nodes on
top of the stack are in the original word order,
which prevents the same two nodes from being
swapped more than once, and when the leftmost
node i is distinct from the root node 0. Note also
that SWAP moves the node i back to the buffer, so
that LEFT-ARCl, RIGHT-ARCl or SWAP can sub-
sequently apply with the node j on top of the stack.
The fact that we can swap the order of nodes,
implicitly representing subtrees, means that we
can construct non-projective trees by applying
354
o(c) =
?
????
????
LEFT-ARCl if c = ([?|i, j], B,Ac), (j, l, i)?A and Ai ? Ac
RIGHT-ARCl if c = ([?|i, j], B,Ac), (i, l, j)?A and Aj ? Ac
SWAP if c = ([?|i, j], B,Ac) and j <G i
SHIFT otherwise
Figure 4: Oracle function for Su = (C, Tu, cs, Ct) with target tree G = (Vx, A). We use the notation Ai
to denote the subset of A that only contains the outgoing arcs of the node i.
LEFT-ARCl or RIGHT-ARCl to subtrees whose
yields are not adjacent according to the original
word order. This is illustrated in Figure 3, which
shows the transition sequence needed to parse the
example in Figure 1. For readability, we represent
both the stack ? and the bufferB as lists of tokens,
indexed by position, rather than abstract nodes.
The last column records the arc that is added to
the arc set A in a given transition (if any).
Given the simplicity of the extension, it is rather
remarkable that the system Su = (C, Tu, cs, Ct)
is sound and complete for the set of all depen-
dency trees (over some label set L), including all
non-projective trees. The soundness part is triv-
ial, since any terminating transition sequence will
have to move all the nodes 1, . . . , n from B to ?
(using SHIFT) and then remove them from ? (us-
ing LEFT-ARCl or RIGHT-ARCl), which will pro-
duce a tree with root 0.
For completeness, we note first that projectiv-
ity is not a property of a dependency tree in itself,
but of the tree in combination with a word order,
and that a tree can always be made projective by
reordering the nodes. For instance, let x be a sen-
tence with dependency tree G = (Vx, A), and let
<G be the total order on Vx defined by an inorder
traversal of G that respects the local ordering of a
node and its children given by the original word
order. Regardless of whether G is projective with
respect to x, it must by necessity be projective with
respect to <G. We call <G the projective order
corresponding to x and G and use it as our canoni-
cal way of finding a node order that makes the tree
projective. By way of illustration, the projective
order for the sentence and tree in Figure 1 is: A1
<G hearing2 <G on5 <G the6 <G issue7 <G is3
<G scheduled4 <G today8 <G .9.
If the words of a sentence x with dependency
tree G are already in projective order, this means
that G is projective with respect to x and that we
can parse the sentence using only transitions in Tp,
because nodes can be pushed onto the stack in pro-
jective order using only the SHIFT transition. If
the words are not in projective order, we can use
a combination of SHIFT and SWAP transitions to
ensure that nodes are still pushed onto the stack in
projective order. More precisely, if the next node
in the projective order is the kth node in the buffer,
we perform k SHIFT transitions, to get this node
onto the stack, followed by k?1 SWAP transitions,
to move the preceding k ? 1 nodes back to the
buffer.1 In this way, the parser can effectively sort
the input nodes into projective order on the stack,
repeatedly extracting the minimal element of <G
from the buffer, and build a tree that is projective
with respect to the sorted order. Since any input
can be sorted using SHIFT and SWAP, and any pro-
jective tree can be built using SHIFT, LEFT-ARCl
and RIGHT-ARCl, the system Su is complete for
the set of all dependency trees.
In Figure 4, we define an oracle function o for
the system Su, which implements this ?sort and
parse? strategy and predicts the optimal transition
t out of the current configuration c, given the tar-
get dependency tree G = (Vx, A) and the pro-
jective order <G. The oracle predicts LEFT-ARCl
or RIGHT-ARCl if the two top nodes on the stack
should be connected by an arc and if the depen-
dent node of this arc is already connected to all its
dependents; it predicts SWAP if the two top nodes
are not in projective order; and it predicts SHIFT
otherwise. This is the oracle that has been used to
generate training data for classifiers in the experi-
mental evaluation in Section 4.
Let us now consider the time complexity of the
extended system Su = (C, Tu, cs, Ct) and let us
begin by observing that 2n is still a lower bound
on the number of transitions required to reach a
terminal configuration. A sequence of 2n transi-
1This can be seen in Figure 3, where transitions 4?8, 9?
13, and 14?18 are the transitions needed to make sure that
on5, the6 and issue7 are processed on the stack before is3 and
scheduled4.
355
Figure 5: Abstract running time during training (black) and parsing (white) for Arabic (1460/146 sen-
tences) and Danish (5190/322 sentences).
tions occurs when no SWAP transitions are per-
formed, in which case the behavior of the system
is identical to the simpler system Sp. This is im-
portant, because it means that the best-case com-
plexity of the deterministic parser is still O(n) and
that the we can expect to observe the best case for
all sentences with projective dependency trees.
The exact number of additional transitions
needed to reach a terminal configuration is deter-
mined by the number of SWAP transitions. Since
SWAP moves one node from ? to B, there will
be one additional SHIFT for every SWAP, which
means that the total number of transitions is 2n +
2k, where k is the number of SWAP transitions.
Given the condition that SWAP can only apply in a
configuration c = ([?|i, j], B,A) if 0 < i < j, the
number of SWAP transitions is bounded by n(n?1)2 ,
which means that 2n + n(n ? 1) = n + n2 is an
upper bound on the number of transitions in a ter-
minating sequence. Hence, the worst-case com-
plexity of the deterministic parser is O(n2).
The running time of a deterministic transition-
based parser using the system Su is O(n) in the
best case and O(n2) in the worst case. But what
about the average case? Empirical studies, based
on data from a wide range of languages, have
shown that dependency trees tend to be projective
and that most non-projective trees only contain
a small number of discontinuities (Nivre, 2006;
Kuhlmann and Nivre, 2006; Havelka, 2007). This
should mean that the expected number of swaps
per sentence is small, and that the running time is
linear on average for the range of inputs that occur
in natural languages. This is a hypothesis that will
be tested experimentally in the next section.
4 Experiments
Our experiments are based on five data sets from
the CoNLL-X shared task: Arabic, Czech, Danish,
Slovene, and Turkish (Buchholz and Marsi, 2006).
These languages have been selected because the
data come from genuine dependency treebanks,
whereas all the other data sets are based on some
kind of conversion from another type of represen-
tation, which could potentially distort the distribu-
tion of different types of structures in the data.
4.1 Running Time
In section 3.2, we hypothesized that the expected
running time of a deterministic parser using the
transition system Su would be linear, rather than
quadratic. To test this hypothesis, we examine
how the number of transitions varies as a func-
tion of sentence length. We call this the abstract
running time, since it abstracts over the actual
time needed to compute each oracle prediction and
transition, which is normally constant but depen-
dent on the type of classifier used.
We first measured the abstract running time on
the training sets, using the oracle to derive the
transition sequence for every sentence, to see how
many transitions are required in the ideal case. We
then performed the same measurement on the test
sets, using classifiers trained on the oracle transi-
tion sequences from the training sets (as described
below in Section 4.2), to see whether the trained
parsers deviate from the ideal case.
The result for Arabic and Danish can be seen
356
Arabic Czech Danish Slovene Turkish
System AS EM AS EM AS EM AS EM AS EM
Su 67.1 (9.1) 11.6 82.4 (73.8) 35.3 84.2 (22.5) 26.7 75.2 (23.0) 29.9 64.9 (11.8) 21.5
Sp 67.3 (18.2) 11.6 80.9 (3.7) 31.2 84.6 (0.0) 27.0 74.2 (3.4) 29.9 65.3 (6.6) 21.0
Spp 67.2 (18.2) 11.6 82.1 (60.7) 34.0 84.7 (22.5) 28.9 74.8 (20.7) 26.9 65.5 (11.8) 20.7
Malt-06 66.7 (18.2) 11.0 78.4 (57.9) 27.4 84.8 (27.5) 26.7 70.3 (20.7) 19.7 65.7 (9.2) 19.3
MST-06 66.9 (0.0) 10.3 80.2 (61.7) 29.9 84.8 (62.5) 25.5 73.4 (26.4) 20.9 63.2 (11.8) 20.2
MSTMalt 68.6 (9.4) 11.0 82.3 (69.2) 31.2 86.7 (60.0) 29.8 75.9 (27.6) 26.6 66.3 (9.2) 18.6
Table 1: Labeled accuracy; AS = attachment score (non-projective arcs in brackets); EM = exact match.
in Figure 5, where black dots represent training
sentences (parsed with the oracle) and white dots
represent test sentences (parsed with a classifier).
For Arabic there is a very clear linear relationship
in both cases with very few outliers. Fitting the
data with a linear function using the least squares
method gives us m = 2.06n (R2 = 0.97) for the
training data and m = 2.02n (R2 = 0.98) for the
test data, where m is the number of transitions in
parsing a sentence of length n. For Danish, there
is clearly more variation, especially for the train-
ing data, but the least-squares approximation still
explains most of the variance, with m = 2.22n
(R2 = 0.85) for the training data and m = 2.07n
(R2 = 0.96) for the test data. For both languages,
we thus see that the classifier-based parsers have
a lower mean number of transitions and less vari-
ance than the oracle parsers. And in both cases, the
expected number of transitions is only marginally
greater than the 2n of the strictly projective transi-
tion system Sp.
We have chosen to display results for Arabic
and Danish because they are the two extremes in
our sample. Arabic has the smallest variance and
the smallest linear coefficients, and Danish has the
largest variance and the largest coefficients. The
remaining three languages all lie somewhere in
the middle, with Czech being closer to Arabic and
Slovene closer to Danish. Together, the evidence
from all five languages strongly corroborates the
hypothesis that the expected running time for the
system Su is linear in sentence length for naturally
occurring data.
4.2 Parsing Accuracy
In order to assess the parsing accuracy that can
be achieved with the new transition system, we
trained a deterministic parser using the new tran-
sition system Su for each of the five languages.
For comparison, we also trained two parsers using
Sp, one that is strictly projective and one that uses
the pseudo-projective parsing technique to recover
non-projective dependencies in a post-processing
step (Nivre and Nilsson, 2005). We will refer to
the latter system as Spp. All systems use SVM
classifiers with a polynomial kernel to approxi-
mate the oracle function, with features and para-
meters taken from Nivre et al (2006), which was
the best performing transition-based system in the
CoNLL-X shared task.2
Table 1 shows the labeled parsing accuracy of
the parsers measured in two ways: attachment
score (AS) is the percentage of tokens with the
correct head and dependency label; exact match
(EM) is the percentage of sentences with a com-
pletely correct labeled dependency tree. The score
in brackets is the attachment score for the (small)
subset of tokens that are connected to their head
by a non-projective arc in the gold standard parse.
For comparison, the table also includes results
for the two best performing systems in the origi-
nal CoNLL-X shared task, Malt-06 (Nivre et al,
2006) and MST-06 (McDonald et al, 2006), as
well as the integrated system MSTMalt, which is
a graph-based parser guided by the predictions of
a transition-based parser and currently has the best
reported results on the CoNLL-X data sets (Nivre
and McDonald, 2008).
Looking first at the overall attachment score, we
see that Su gives a substantial improvement over
Sp (and outperforms Spp) for Czech and Slovene,
where the scores achieved are rivaled only by the
combo system MSTMalt. For these languages,
there is no statistical difference between Su and
MSTMalt, which are both significantly better than
all the other parsers, except Spp for Czech (Mc-
Nemar?s test, ? = .05). This is accompanied
by an improvement on non-projective arcs, where
2Complete information about experimental settings can
be found at http://stp.lingfil.uu.se/?nivre/exp/.
357
Su outperforms all other systems for Czech and
is second only to the two MST parsers (MST-06
and MSTMalt) for Slovene. It is worth noting that
the percentage of non-projective arcs is higher for
Czech (1.9%) and Slovene (1.9%) than for any of
the other languages.
For the other three languages, Su has a drop
in overall attachment score compared to Sp, but
none of these differences is statistically signifi-
cant. In fact, the only significant differences in
attachment score here are the positive differences
betweenMSTMalt and all other systems for Arabic
and Danish, and the negative difference between
MST-06 and all other systems for Turkish. The
attachment scores for non-projective arcs are gen-
erally very low for these languages, except for the
two MST parsers on Danish, but Su performs at
least as well as Spp on Danish and Turkish. (The
results for Arabic are not very meaningful, given
that there are only eleven non-projective arcs in
the entire test set, of which the (pseudo-)projective
parsers found two and Su one, while MSTMalt and
MST-06 found none at all.)
Considering the exact match scores, finally, it is
very interesting to see that Su almost consistently
outperforms all other parsers, including the combo
system MSTMalt, and sometimes by a fairly wide
margin (Czech, Slovene). The difference is statis-
tically significant with respect to all other systems
except MSTMalt for Slovene, all except MSTMalt
and Spp for Czech, and with respect to MSTMalt
for Turkish. For Arabic and Danish, there are no
significant differences in the exact match scores.
We conclude that Su may increase the probabil-
ity of finding a completely correct analysis, which
is sometimes reflected also in the overall attach-
ment score, and we conjecture that the strength of
the positive effect is dependent on the frequency
of non-projective arcs in the language.
5 Related Work
Processing non-projective trees by swapping the
order of words has recently been proposed by both
Nivre (2008b) and Titov et al (2009), but these
systems cannot handle unrestricted non-projective
trees. It is worth pointing out that, although the
system described in Nivre (2008b) uses four tran-
sitions bearing the same names as the transitions
of Su, the two systems are not equivalent. In par-
ticular, the system of Nivre (2008b) is sound but
not complete for the class of all dependency trees.
There are also affinities to the system of Attardi
(2006), which combines non-adjacent nodes on
the stack instead of swapping nodes and is equiva-
lent to a restricted version of our system, where no
more than two consecutive SWAP transitions are
permitted. This restriction preserves linear worst-
case complexity at the expense of completeness.
Finally, the algorithm first described by Covington
(2001) and used for data-driven parsing by Nivre
(2007), is complete but has quadratic complexity
even in the best case.
6 Conclusion
We have presented a novel transition system for
dependency parsing that can handle unrestricted
non-projective trees. The system reuses standard
techniques for building projective trees by com-
bining adjacent nodes (representing subtrees with
adjacent yields), but adds a simple mechanism for
swapping the order of nodes on the stack, which
gives a system that is sound and complete for the
set of all dependency trees over a given label set
but behaves exactly like the standard system for
the subset of projective trees. As a result, the time
complexity of deterministic parsing is O(n2) in
the worst case, which is rare, but O(n) in the best
case, which is common, and experimental results
on data from five languages support the conclusion
that expected running time is linear in the length
of the sentence. Experimental results also show
that parsing accuracy is competitive, especially
for languages like Czech and Slovene where non-
projective dependency structures are common, and
especially with respect to the exact match score,
where it has the best reported results for four out
of five languages. Finally, the simplicity of the
system makes it very easy to implement.
Future research will include an in-depth error
analysis to find out why the system works better
for some languages than others and why the exact
match score improves even when the attachment
score goes down. In addition, we want to explore
alternative oracle functions, which try to minimize
the number of swaps by allowing the stack to be
temporarily ?unsorted?.
Acknowledgments
Thanks to Johan Hall and Jens Nilsson for help
with implementation and evaluation, and to Marco
Kuhlmann and three anonymous reviewers for
useful comments.
358
References
Giuseppe Attardi. 2006. Experiments with a multi-
language non-projective dependency parser. In Pro-
ceedings of CoNLL, pages 166?170.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164.
Michael A. Covington. 2001. A fundamental algo-
rithm for dependency parsing. In Proceedings of the
39th Annual ACM Southeast Conference, pages 95?
102.
Carlos Go?mez-Rodr??guez, David Weir, and John Car-
roll. 2009. Parsing mildly non-projective depen-
dency structures. In Proceedings of EACL, pages
291?299.
Keith Hall and Vaclav Nova?k. 2005. Corrective mod-
eling for non-projective dependency parsing. In
Proceedings of IWPT, pages 42?52.
Jiri Havelka. 2007. Beyond projectivity: Multilin-
gual evaluation of constraints and measures on non-
projective structures. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 608?615.
Richard Johansson and Pierre Nugues. 2007. Incre-
mental dependency parsing using online learning. In
Proceedings of the Shared Task of EMNLP-CoNLL,
pages 1134?1138.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly
non-projective dependency structures. In Proceed-
ings of the COLING/ACL Main Conference Poster
Sessions, pages 507?514.
Marco Kuhlmann and Giorgio Satta. 2009. Treebank
grammar techniques for non-projective dependency
parsing. In Proceedings of EACL, pages 478?486.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL, pages 81?88.
Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency
parsing. In Proceedings of IWPT, pages 122?131.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online large-margin training of de-
pendency parsers. In Proceedings of ACL, pages 91?
98.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT/EMNLP, pages 523?530.
RyanMcDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a
two-stage discriminative parser. In Proceedings of
CoNLL, pages 216?220.
Peter Neuhaus and Norbert Bro?ker. 1997. The com-
plexity of recognition of linguistically adequate de-
pendency grammars. In Proceedings of ACL/EACL,
pages 337?343.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proceedings of ACL, pages 950?958.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In Proceedings of
ACL, pages 99?106.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Proceedings
of CoNLL, pages 49?56.
Joakim Nivre, Johan Hall, Jens Nilsson, Gu?lsen
Eryig?it, and Svetoslav Marinov. 2006. Labeled
pseudo-projective dependency parsing with support
vector machines. In Proceedings of CoNLL, pages
221?225.
Joakim Nivre. 2004. Incrementality in deterministic
dependency parsing. In Proceedings of the Work-
shop on Incremental Parsing: Bringing Engineering
and Cognition Together (ACL), pages 50?57.
Joakim Nivre. 2006. Constraints on non-projective de-
pendency graphs. In Proceedings of EACL, pages
73?80.
Joakim Nivre. 2007. Incremental non-projective de-
pendency parsing. In Proceedings of NAACL HLT,
pages 396?403.
Joakim Nivre. 2008a. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34:513?553.
Joakim Nivre. 2008b. Sorting out dependency pars-
ing. In Proceedings of the 6th International Con-
ference on Natural Language Processing (GoTAL),
pages 16?27.
Ivan Titov and James Henderson. 2007. A latent vari-
able model for generative dependency parsing. In
Proceedings of IWPT, pages 144?155.
Ivan Titov, James Henderson, Paola Merlo, and
Gabriele Musillo. 2009. Online graph planarization
for synchronous parsing of semantic and syntactic
dependencies. In Proceedings of IJCAI.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of IWPT, pages 195?206.
359
Proceedings of the ACL-08: HLT Workshop on Parsing German (PaGe-08), pages 47?54,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Dependency-Driven Parser for German
Dependency and Constituency Representations
Johan Hall
Va?xjo? University
Sweden
johan.hall@vxu.se
Joakim Nivre
Va?xjo? University and
Uppsala University
Sweden
joakim.nivre@vxu.se
Abstract
We present a dependency-driven parser that
parses both dependency structures and con-
stituent structures. Constituency representa-
tions are automatically transformed into de-
pendency representations with complex arc la-
bels, which makes it possible to recover the
constituent structure with both constituent la-
bels and grammatical functions. We report a
labeled attachment score close to 90% for de-
pendency versions of the TIGER and Tu?Ba-
D/Z treebanks. Moreover, the parser is able to
recover both constituent labels and grammat-
ical functions with an F-Score over 75% for
Tu?Ba-D/Z and over 65% for TIGER.
1 Introduction
Is it really that difficult to parse German? Ku?bler et
al. (2006) point out three grammatical features that
could make parsing of German more difficult: finite
verb placement, flexible phrase ordering and discon-
tinuous constituents. Earlier studies by Dubey and
Keller (2003) and Dubey (2005) using the Negra
treebank (Skut et al, 1997) reports that lexicaliza-
tion of PCFGs decrease the parsing accuracy when
parsing Negra?s flat constituent structures. However,
Ku?bler et al (2006) present a comparative study
that suggests that it is not harder to parse German
than for example English. By contrast, Rehbein and
van Genabith (2007) study different parser evalua-
tion metrics by simulating parser errors on two Ger-
man treebanks (with different treebank annotation
schemes) and they claim that the question whether
German is harder to parse than English is still unde-
cided.
This paper does not try to answer the question
above, but presents a new way of parsing constituent
structures that can output the whole structure with
all grammatical functions. The shared task on pars-
ing German was to parse both the constituency ver-
sion and the dependency version of the two Ger-
man treebanks: TIGER (Brants et al, 2002) and
Tu?Ba-D/Z (Telljohann et al, 2005). We present a
dependency-driven parser that parses both depen-
dency structures and constituent structures using an
extended version of MaltParser 1.0.1 The focus of
this paper is how MaltParser parses the constituent
structures with a dependency-based algorithm.
This paper is structured as follows. Section 2
briefly describes the MaltParser system, while sec-
tion 3 continues with presenting the dependency
parsing. Section 4 explains how a transition-based
dependency-driven parser can be turned into a con-
stituency parser. Section 5 presents the experimen-
tal evaluation and discusses the results. Finally sec-
tion 6 concludes.
2 MaltParser
MaltParser is a transition-based parsing system
which was one of the top performing systems on
multilingual dependency parsing in the CoNLL
2006 shared task (Buchholz and Marsi, 2006; Nivre
et al, 2006) and the CoNLL shared task 2007 (Nivre
et al, 2007; Hall et al, 2007). The basic idea of
MaltParser is to derive dependency graphs using a
greedy parsing algorithm that approximates a glob-
1MaltParser is distributed with an open-source license
and can be downloaded free of charge from following page:
http://www.vxu.se/msi/users/jha/maltparser/
47
ally optimal solution by making a sequence of lo-
cally optimal choices. The system is equipped with
several parsing algorithms, but we have chosen to
only optimize Nivre?s parsing algorithm for both
the dependency track and the constituency track.
Nivre?s algorithm is a deterministic algorithm for
building labeled projective dependency structures in
linear time (Nivre, 2006). There are two essential
parameters that can be varied for this algorithm. The
first is the arc order and we selected the arc-eager or-
der that attaches the right dependents to their head as
soon as possible. The second is the stack initializa-
tion and we chose to use an empty stack initializa-
tion that attaches root dependents with a default root
label after completing the left-to-right pass over the
input.
The algorithm uses two data structures: a stack
to store partially processed tokens and a queue of
remaining input tokens. The arc-eager transition-
system has four parser actions:
1. LEFT-ARC(r): Adds an arc labeled r from the
next input token to the top token of the stack,
the top token is popped from the stack because
it must be complete with respect to left and
right dependents at this point.
2. RIGHT-ARC(r): Adds an arc labeled r from
the top token of the stack to the next input token
and pushes the next input token onto the stack
(because it may have dependents further to the
right).
3. REDUCE: Pops the top token of the stack. This
transition can be performed only if the top to-
ken has been assigned a head and is needed for
popping a node that was pushed in a RIGHT-
ARC(r) transition and which has since found
all its right dependents.
4. SHIFT: Pushes the next input token onto the
stack. This is correct when the next input token
has its head to the right or should be attached
to the root.
MaltParser uses history-based feature models for
predicting the next parser action at nondeterminis-
tic choice points. Previously, MaltParser combined
the prediction of the transition with the prediction of
the arc label r into one complex prediction with one
feature model. The experiments presented in this pa-
per use another prediction strategy, which divide the
prediction of the parser action into several predic-
tions. First the transition is predicted; if the transi-
tion is SHIFT or REDUCE the nondeterminism is re-
solved, but if the predicted transition is RIGHT-ARC
or LEFT-ARC the parser continues to predict the arc
label r. This prediction strategy enables the system
to have three different feature models: one for pre-
dicting the transition and two for predicting the arc
label r (RIGHT-ARC and LEFT-ARC). We will see
in section 4 that this change makes it more feasi-
ble to encode the inverse mapping into complex arc
labels for an arbitrary constituent structure without
losing any information.
All symbolic features were converted to nu-
merical features and we use the quadratic kernel
K(xi, xj) = (?xTi xj + r)
2 of the LIBSVM pack-
age (Chang and Lin, 2001) for mapping histories to
parser actions and arc labels. All results are based
on the following settings of LIBSVM: ? = 0.2 and
r = 0 for the kernel parameters, C = 0.5 for the
penalty parameter, and  = 1.0 for the termination
criterion. We also split the training instances into
smaller sets according to the fine-grained part-of-
speech of the next input token to train separate one-
versus-one multi-class LIBSVM-classifiers.
3 Dependency Parsing
Parsing sentences with dependency structures like
the one in Figure 1 is straightforward using Malt-
Parser. During training, the parser reconstructs the
correct transition sequence needed to derive the gold
standard dependency graph of a sentence. This in-
volves choosing a label r for each arc, which in
a pure dependency structure is an atomic symbol.
For example, in Figure 1, the arc from hat to Beck-
meyer is labeled SUBJ. This is handled by train-
ing a separate labeling model for RIGHT-ARC and
LEFT-ARC. During parsing, the sentence is pro-
cessed in the same way as during training except that
the parser requests the next transition from the tran-
sition classifier. If the predicted transition is an arc
transition (RIGHT-ARC or LEFT-ARC), it then asks
the corresponding classifier for the arc label r.
One complication when parsing the dependency
version of the two German treebanks is that they
48
Figure 1: The sentence ?For this statement has Beckmeyer until now not presented any evidence.? is taken from
dependency version of Tu?Ba-D/Z treebank.
contain non-projective structures, such as the depen-
dency graph illustrated in Figure 1. Nivre?s pars-
ing algorithm only produces projective dependency
structures, and therefore we used pseudo-projective
parsing for recovering non-projective structures.
The training data are projectivized and information
about these transformations is encoded into the arc
labels to enable deprojectivizition of the parser out-
put (Nivre and Nilsson, 2005).
4 Constituency Parsing
This section explains how a transition-based depen-
dency parser can be used for parsing constituent
structures. The basic idea is to use the common
practice of transforming a constituent structure into
a dependency graph and encode the inverse mapping
with complex arc labels. Note that the goal is not to
create the best dependency representation of a con-
stituent structure. Instead the main objective is to
find a general method to transform constituency to
dependency so that is easy to do the inverse trans-
formation without losing any information. More-
over, another goal is to transform the constituent
structures so that it is feasible for a transition-based
dependency parser to induce a parser model based
on the resulting dependency graphs and during pars-
ing use this parser model to derive constituent struc-
tures with the highest accuracy possible. Hence, the
transformation described below is not designed with
the purpose of deriving a linguistically sound depen-
dency graph from a constituent structure.
Our strategy for turning a dependency parser into
a constituency parser can be summarized with the
following steps:
1. Identify the lexical head of every constituent in
the constituent structure.
2. Identify the head of every token in the depen-
dency structure.
3. Build a labeled dependency graph that encodes
the inverse mapping in the arc labels.
4. Induce a parser model based on the labeled de-
pendency graphs.
5. Use the induced parser model to parse new sen-
tences into dependency graphs.
6. Derive the constituent structure by performing
the inverse mapping encoded in the dependency
graph produced in step 5.
4.1 Identify the Heads
The first steps are basically the steps that are used
to convert a constituent structure to a dependency
structure. One way of doing this is to traverse the
constituent structure from the root node and iden-
tify the head-child and the lexical head of all con-
stituent nodes in a recursive depth-first search. Usu-
ally this process is governed by pre-defined head-
finding rules that define the direction of the search
for each distinct constituent label. Moreover, it
is quite common that the head-finding rules define
some kind of priority lists over which part of speech
or grammatical function is the more preferable head-
child.
For our experiment on German we have kept this
search of the head-child and lexical head very sim-
ple. For the TIGER treebank we perform a left-
to-right search to find the leftmost lexical child. If
no lexical child can be found, the head-child of the
49
constituent will be the leftmost constituent child and
the lexical head will be the lexical child of the head
child recursively. For the Tu?Ba-D/Z treebank we got
higher accuracy if we varied the direction of search
according to the label of the target constituent.2 We
also tried more complex and linguistically motivated
head rules, but unfortunately no improvement in ac-
curacy could be found. We want to stress that the
use of more complex head rules was done late in the
parser optimization process and it would not be a
surprise if more careful experiments resulted in the
opposite conclusion.
Given that all constituents have been assigned a
lexical head it is a straightforward process to iden-
tify the head and the dependents of all input tokens.
The algorithm investigates, for each input token, the
containing constituent?s lexical head, and if the to-
ken is not the lexical head of the constituent it takes
the lexical head as its head in the dependency graph;
otherwise the head will be assigned the lexical head
of a higher constituent in the structure. The root of
the dependency graph will be the lexical head of the
root of the constituent structure.
4.2 Build a Labeled Dependency Graph
The next step builds a labeled dependency represen-
tation that encodes the inverse mapping in the arc
labels of the dependency graph. Each arc label is a
quadruple consisting of four sublabels (dependency
relation, head relations, constituent labels, attach-
ment). The meaning of each sublabel is following:
? The dependency relation is the grammatical
function of the highest constituent of which the
dependent is the lexical head.
? The head relations encode the path of function
labels from the dependent to the highest con-
stituent of which is the lexical head (with path
elements separated by |).
? The constituent labels encode the path of con-
stituent labels from the dependent to the highest
constituent of which is the lexical head (with
path elements separated by |).
2It was beneficial to make a right-to-left search for the fol-
lowing labels: ADJX, ADVX, DM, DP, NX, PX
? The attachment is a non-negative integer i that
encodes the attachment level of the highest con-
stituent of which it is the lexical head.
4.3 Encoding Example
Figure 2 illustrates the procedure of encoding the
constituency representation as a dependency graph
with complex arc labels for a German sentence.
The constituent structure is shown above the sen-
tence and below we can see the resulting depen-
dency graph after the transformation. We want to
stress that the resulting dependency graph is not lin-
guistically sound, and the main purpose is to demon-
strate how a constituent structure can be encoded in
a dependency graph that have all information need
for the inverse transformation.
For example, the constituent MF has no lexical
child and therefore the head-child is the leftmost
constituent NX. The lexical head of MF is the token
Beckmeyer because it is the lexical head of NX. For
the same reason the lexical head of the constituent
SIMPX is the token Fu?r and this token will be the
head of the token Beckmeyer, because SIMPX dom-
inates MF. In the dependency graph this is illustrated
with an arc from the head Fu?r to its dependent Beck-
meyer.
The arc Fu?r to Beckmeyer is labeled with a com-
plex label (??, HD|ON, NX|MF, 2), which consists
of four sublabels. The first sublabel is the grammat-
ical function above MF and because this is missing
a dummy label ?? is used instead. The sublabel
HD|ON encodes a sequence of head relations from
the lexical head Beckmeyer to MF. The constituent
labels are encoded in the same way in the third sub-
label NX|MF. Finally, the fourth sublabel indicates
the attachment level of the constituent MF. In this
case, MF should be attached to the constituent two
levels up in the structure with respect to the head
Fu?r.3
The two arcs diese to Behauptung and keinen to
Nachweis both have the complex arc label (HD, *, *,
0), because the tokens Behauptung and Nachweis are
attached to a constituent without being a lexical head
of any dominating constituent. Consequently, there
are no sequences of head relations and constituent
3If the fourth sublabel had an attachment level of 1, then the
constituent MF would be attached to the constituent VF instead
of the constituent SIMPX.
50
Figure 2: The sentence ?For this statement has Beckmeyer until now not presented any evidence.? is taken from
Tu?Ba-D/Z treebank and show the encoding of a constituent structure as a dependency graph.
labels to encode, and these are therefore marked *.
The encoding of the virtual root VROOT is treated
in a special way and the label VROOT is regarded as
a dependency relation instead of a constituent label.
If we compare the dependency graphs in Figure 1
and Figure 2, we can see large differences. The more
linguistically motivated dependency graph (LDG) in
Figure 1 has a completely difference structure and
different arc labels compared to the automatically
generated dependency graph (ADG) in Figure 2.
There are several reasons, some of which are listed
here:
? Different conversions strategies: LDG is based
on a conversion that sometimes leads to non-
projective structures for non-local dependen-
cies. For example, in Figure 2, the extracted
PP Fu?r diese Behauptung has the grammati-
cal function OAMOD, which indicates that it
is a modifier (MOD) of a direct object (OA)
elsewhere in the structure (in this case keinen
Nachweis). In LDG, this is converted to a non-
projective dependency from Nachweis to Fu?r
(with the label PP). No such transformtion is
attempted in ADC, which simply attaches Fu?r
to the lexical head of the containing constituent.
? Different head-finding rules: ADG are derived
without almost no rules at all. Most likely, the
conversion of LDG makes use of several lin-
guistically sound head-finding rules. A striking
difference is the root of the dependency graph,
where LDG has its root at the linguistically mo-
tivated token hat. Whereas ADG has its root at
the end of the sentence, because the leftmost
lexical child of the virtual root VROOT is the
punctuation.
? Different arc labels: ADG encodes the con-
stituent structure in the complex arc labels to
be able to recover the constituent structure,
whereas LDG have linguistically motivated de-
pendency relations that are not present in the
constituent structure.
We believe that our simplistic approach can be fur-
ther improved by using ideas from the conversion
process of LDG.
51
4.4 Inverse Mapping
The last step of our presented strategy is to make the
inverse transformation from a dependency graph to
a constituent structure. This is done by a bottom-
up and top-down process of the dependency graph.
First we iterate over all tokens in the dependency
graph and restore the sequence of constituent nodes
with constituent labels and grammatical functions
for each individual token using the information of
the sublabels head relations and constituent labels.
After this bottom-up process we have the lineage of
constituents for each token where the token is the
lexical head. The top-down process then traverse
the dependency graph recursively from the root with
pre-order depth-first search. For each token, the
highest constituent of the lineage of the token is at-
tached to its head lineage at an attachment level ac-
cording to the sublabel attachment. Finally, the edge
between the dominating constituent and the highest
constituent of the lineage is labeled with a grammat-
ical function according to the sublabel dependency
relation.
4.5 Parsing
For the constituency versions of both TIGER and
Tu?Ba-D/Z we can recover the constituent structure
without any loss of information, if we transform
from constituency to dependency and back again to
constituency. During parsing we predict the sub-
labels separately with separate feature models for
RIGHT-ARC and LEFT-ARC. Moreover, the parsed
constituent structure can contain discontinuous con-
stituency because of wrong attachment levels of con-
stituents. To overcome this problem, the structure
is post-processed and the discontinuous constituents
are forced down in the structure so that the parser
output can be represented in a nested bracketing for-
mat.
5 Experiments
The shared task on parsing German consisted of
parsing either the dependency version or the con-
stituency version of two German treebanks, al-
though we chose to parse both versions. This section
first presents the data sets used. We continue with a
brief overview of how we optimized the four differ-
ent parser models. Finally, the results are discussed.
5.1 Data Sets
The prepared training and development data dis-
tributed by the organizers were based on the German
TIGER (Brants et al, 2002) and Tu?Ba-D/Z (Telljo-
hann et al, 2005) treebanks, one dependency and
one constituency version for each treebank. Both
treebanks contain German newspaper text and the
prepared data sets were of the same size. The devel-
opment set contained 2611 sentences and the train-
ing set contained 20894 sentences. The dependency
and constituency versions contained the same set of
sentences.
The dependency data were formated according
to the CoNLL dependency data format.4 The
LEMMA, FEATS, PHEAD and PDEPREL columns
of the CoNLL format were not used at all.
The constituency data have been converted into a
bracketing format similar to the Penn Treebank for-
mat. All trees are dominated by a VROOT node
and all constituents are continuous. The test data
consisted of sentences with gold-standard part-of-
speech tags and also the gold-standard grammatical
functions attached to the part-of-speech tags. Unfor-
tunately, we were not aware of that the grammatical
functions attached to the part-of-speech tags should
be regarded as input to the parser and therefore our
presented results are based on not using the gram-
matical functions attached to the part-of-speech tags
as input to the parser.
We divided the development data into two sets,
one set used for parser optimization (80%) and the
other 20% we saved for final preparation before the
release of the test data. For the final test run we
trained parser models on all the data, both the train-
ing data and the development data.
5.2 Parser optimization
We ran several experiments to optimize the four dif-
ferent parser models. The optimization of the de-
pendency versions was conducted in a way simi-
lar to the parser optimization of MaltParser in the
CoNLL shared tasks (Nivre et al, 2006; Hall et al,
2007). A new parameter for the extended version
4More information about the CoNLL dependency data for-
mat can be found at: http://nextens.uvt.nl/ conll/#dataformat.
Yannick Versley has done work of converting both treebanks to
a dependency annotation that is similar to the Hamburg depen-
dency format.
52
of MaltParser 1.0 is the prediction strategy, where
we could choose between combining the prediction
of the transition with the prediction of the arc label
into one complex prediction or dividing the predic-
tion of the parser action into two predictions (one
model for predicting the transition and two models
for predicting the arc label depending on the out-
come of the transition-model). It was beneficial to
use the divided predication strategy for all four data
sets. In the next step we performed a feature opti-
mization with both forward and backward selection,
starting from a model extrapolated from many pre-
vious experiments on different languages. Because
we chose to use the divided predication strategy this
step was more complicated compared to using the
combined strategy, because we needed to optimize
three feature models (one transition-model and two
arc-label models, one for RIGHT-ARC and one for
LEFT-ARC).
The optimization of the constituency versions was
even more complex because each parser model con-
tained nine feature models (one transition-model,
two models for each sublabel). Another problem
for the parser optimization was the fact that we tried
out new ideas and for example changed the encod-
ing a couple of times. Due to the time constraints
of the shared task it was not possible to start parser
optimization all over again for every change. We
also performed some late experiments with different
head-finding rules to make the intermediate depen-
dency graphs more linguistically sound, but unfor-
tunately these experiments did not improve the pars-
ing accuracy. We want to emphasize that the time
for developing the extended version of MaltParser
to handle constituency was severely limited, espe-
cially the implementation of head-finding rules, so
it is very likely that head-finding rules can improve
parsing accuracy after more careful testing and ex-
periments.
5.3 Results and Discussion
The results based on the prepared test data for the de-
pendency and constituency tracks are shown in table
1. The label attachment score (LAS) was used by the
organizer for evaluating the dependency versions,
that is, the proportion of tokens that are assigned the
correct head and the correct arc label (punctuation
included). We can see that the dependency results
Dependency Constituency
Treebank LAS LP LR LF
TIGER 90.80 67.06 63.40 65.18
Tu?Ba-D/Z 88.64 76.44 74.79 75.60
Table 1: The results for the extended version of Malt-
Parser 1.0 in the shared task on parsing German depen-
dency and constituency representations.
are close to 90% for both the treebanks, 90.80 for
TIGER and 88.64 for Tu?ba-D/Z, which were the un-
challenged best scores in the shared task. The high-
est score on parsing German in the CoNLL-X shared
task was obtained by the system of McDonald et al
(2006) with a LAS of 87.34 based on the TIGER
treebank, but we want to stress that these results
are not comparable due to different data sets (and
a different policy regarding the inclusion of punctu-
ation).
The constituency versions were evaluated accord-
ing to the labeled recall (LR), labeled precision
(LP) and labeled F-score (LF). Labeled in this con-
text means that both the constituent label and the
grammatical function should agree with the gold-
standard, but grammatical functions labeling the
edge between a constituent and a token were not in-
cluded in the evaluation. The labeled F-scores are
75.60 for Tu?ba-D/Z and 65.18 for TIGER and these
results are the second best results in the shared task
out of three systems. We want to emphasize that the
results may not be strictly comparable because of
different use of the grammatical functions attached
to the parts of speech in the bracketing format. We
did not use these grammatical functions as input,
instead these were assigned by the parser. Our re-
sults are competitive if we compare with Ku?bler et
al. (2006), who report 51.41 labeled F-score on the
Negra treebank and 75.33 on the Tu?Ba-D/Z treebank
using the unlexicalized, markovized PCFG version
of the Stanford parser.
We believe that our results for the constituency
representations can be improved upon by investi-
gating different methods for encoding the inverse
mapping in the complex arc labels and performing
a more careful evaluation of head-finding rules to
derive a more linguistically sound dependency rep-
resentation. Another interesting line of future work
is to try to parse discontinuous constituents by using
53
a non-projective parsing algorithm like the Coving-
ton algorithm (Covington, 2001) or using pseudo-
projective parsing for discontinuous constituency
parsing (Nivre and Nilsson, 2005).
6 Conclusion
We have shown that a transition-based dependency-
driven parser can be used for parsing German with
both dependency and constituent representations.
We can report state-of-the-art results for parsing the
dependency versions of two German treebanks, and
we have demonstrated, with promising results, how
a dependency parser can parse full constituent struc-
tures by encoding the inverse mapping in complex
arc labels of the dependency graph. We believe that
this method can be improved by using, for example,
head-finding rules.
Acknowledgments
We want to thank the treebank providers for making
the data available for the shared task and the orga-
nizers for their efforts in organizing it. Thanks also
to two reviewers for useful comments.
References
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER Tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories Sozopol, pages 1?18.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X Shared Task on Multilingual Dependency Parsing.
In Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning (CoNLL-X), pages
149?164.
Chih-Chung Chang and Chih-Jen Lin. 2001. LIBSVM:
A Library for Support Vector Machines.
Michael A. Covington. 2001. A Fundamental Algorithm
for Dependency Parsing. In Proceedings of the 39th
Annual ACM Southeast Conference, pages 95?102.
Amit Dubey and Frank Keller. 2003. Probabilistic Pars-
ing for German using Sister-Head Dependencies. In
Proceedings of the 41st Annual Meeting of the Associ-
ation for Computational Linguistics (ACL), pages 96?
103.
Amit Dubey. 2005. What to do when Lexicaliza-
tion fails: Parsing German with Suffix Analysis and
Smoothing. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 314?321.
Johan Hall, Jens Nilsson, Joakim Nivre, Gu?ls?en Eryig?it,
Bea?ta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single Malt or Blended? A Study in Mul-
tilingual Parser Optimization. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007,
pages 933?939.
Sandra Ku?bler, Erhard W. Hinrichs, and Wolfgang Maier.
2006. Is it Really that Difficult to Parse German.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2006), pages 111?119.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual Dependency Analysis with a
Two-Stage Discriminative Parser. In Proceedings of
the Tenth Conference on Computational Natural Lan-
guage Learning (CoNLL-X), pages 216?220.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-Projective
Dependency Parsing. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 99?106.
Joakim Nivre, Johan Hall, Jens Nilsson, Gu?ls?en Eryig?it,
and Svetoslav Marinov. 2006. Labeled Pseudo-
Projective Dependency Parsing with Support Vector
Machines. In Proceedings of the Tenth Conference on
Computational Natural Language Learning (CoNLL-
X), pages 221?225.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 Shared Task on Dependency
Parsing. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 915?932.
Joakim Nivre. 2006. Inductive Dependency Parsing.
Springer.
Ines Rehbein and Josef van Genabith. 2007. Treebank
Annotation Schemes and Parser Evaluation for Ger-
man. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning (
EMNLP-CoNLL 2007), pages 630?639.
Wojciech Skut, Brigitte Krenn, Thorsten Brants, and
Hans Uszkoreit. 1997. An Annotation Scheme for
Free Word Order Languages. In Proceedings of the
Fifth Conference on Applied Natural Language Pro-
cessing (ANLP), pages 314?321.
Heike Telljohann, Erhard W. Hinrichs, Sandra Ku?bler,
and Heike Zinsmeister. 2005. Stylebook for
the Tu?bingen Treebank of Written German (Tu?Ba-
D/Z). Seminar fu?r Sprachwissenschaft, Universita?t
Tu?bingen, Germany.
54
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 49?60,
Paris, October 2009. c?2009 Association for Computational Linguistics
Parsing Formal Languages using Natural Language Parsing Techniques
Jens Nilsson? Welf Lo?we? Johan Hall?? Joakim Nivre??
?Va?xjo? University, School of Mathematics and Systems Engineering, Sweden
?Uppsala University, Department of Linguistics and Philology, Sweden
{jens.nilsson|welf.lowe|johan.hall|joakim.nivre}@vxu.se
Abstract
Program analysis tools used in software
maintenance must be robust and ought to
be accurate. Many data-driven parsing ap-
proaches developed for natural languages
are robust and have quite high accuracy
when applied to parsing of software. We
show this for the programming languages
Java, C/C++, and Python. Further studies
indicate that post-processing can almost
completely remove the remaining errors.
Finally, the training data for instantiating
the generic data-driven parser can be gen-
erated automatically for formal languages,
as opposed to the manually development
of treebanks for natural languages. Hence,
our approach could improve the robust-
ness of software maintenance tools, proba-
bly without showing a significant negative
effect on their accuracy.
1 Introduction
Software engineering, especially software mainte-
nance, is supported by numerous program anal-
ysis tools. Maintenance tasks include program
comprehension (understanding unknown code for
fixing bugs or further development), quality as-
sessment (judging code, e.g., in code reviews),
and reverse-engineering (reifying the design doc-
uments for given source code). To extract infor-
mation from the programs, the tools first parse the
program code and produce an abstract syntax tree
(AST) for further analysis and abstraction (Strein
et al, 2007). As long as the program conforms
to the syntax of a programming language, clas-
sical parsing techniques known from the field of
compiler construction may be applied. This, how-
ever, cannot be assumed in general, as the pro-
grams to analyze can be incomplete, erroneous, or
conform to a (yet unknown) dialect or version of
the language. Despite error stabilization, classi-
cal parsers then lose a lot of information or simply
break down. This is unsatisfactory for tools sup-
porting maintenance. Therefore, quite some effort
has gone into the development of robust parsers of
programs for these tools (cf. our related work sec-
tion 5). This effort, however, has to be repeated
for every programming language.
The development of robust parsers is of special
interest for languages like C/C++ due to their nu-
merous dialects in use (Anderson, 2008). Also,
tools for languages frequently coming in new ver-
sions, like Java, benefit from robust parsing. Fi-
nally, there are languages like HTML where exist-
ing browsers are forgiving if documents do not ad-
here to the formal standard with the consequence
that there exist many formally erroneous docu-
ments. In such cases, robust parsing is even a pre-
requisite for tool-supported maintenance.
The accuracy of parsing is a secondary goal
in the context of software maintenance. Tasks
like program comprehension, quality assessment,
and reverse-engineering are fuzzy by their nature.
There is no well-defined notion of correctness?
rather an empirical answer to the question: Did
it help the software engineers in fulfilling their
tasks? Moreover, the information provided to the
engineers abstracts anyway from the concrete pro-
gram syntax and semantics, i.e., inaccuracies in
the input may disappear in the output. Finally, pro-
gram analyses are often heuristics themselves, ap-
proximating computationally hard problems like
pattern matching and optimal clustering.
The natural language processing (NLP) com-
munity has for many years developed parsing tech-
nology that is both completely robust and highly
accurate. The present approach applies this tech-
nology to programming languages. It is robust in
the sense that, for each program, the parser always
gives a meaningful model even for slightly incor-
rect and incomplete programs. The approach is,
49
however, not accurate to 100%, i.e., even correct
programs may lead to slightly incorrect models.
As we will show, it is quite accurate when applied
to programming languages.
The data-driven dependency parsing approach
applied here only needs correct examples of the
source and the expected analysis model. Then it
automatically trains and adapts a generic parser.
As we will show, training data for adapting to a
new programming language can even be gener-
ated automatically. Hence, the effort for creating
a parser for a new programming language is quite
small.
The basic idea ? applying natural language pars-
ing to programming languages ? has been pre-
sented to the program maintenance community be-
fore (Nilsson et al, 2009). This paper contributes
with experimental results on
1. data-driven dependency parsing of the pro-
gramming languages C/C++, Java, and
Python,
2. transformations between dependency struc-
ture and phrase structure adapted to program-
ming languages,
3. generic parser model selection and its effect
on parsing accuracy.
Section 2 gives an introduction to the parsing tech-
nology applied here. In section 3, the preparation
of the training examples necessary is described,
while section 4 presents the experimental results.
Section 5 discusses related work in information
extraction for software maintenance. We end with
conclusions and future work in section 6.
2 NLP Background
Dependency structure is one way of representing
the syntax of natural languages. Dependency trees
form labeled, directed and rooted trees, as shown
in figure 1. One essential difference compared to
context-free grammar is the absence of nontermi-
nals. Another difference is that the syntactic struc-
ture is composed of lexical tokens (also called ter-
minals or words) linked by binary and directed re-
lations called dependencies. Each token in the fig-
ure is labeled with a part-of-speech, shown at the
bottom of the figure. Each dependency relation is
also labeled.
The parsing algorithm used in the experiments
of section 4, known as the Nivre?s arc-eager al-
Figure 1: Sentence with a dependency tree.
gorithm (Nivre, 2003), can produce such depen-
dency trees. It bears a resemblance to the shift-
reduce parser for context-free grammars, with the
most apparent difference being that terminals (not
nonterminals) are pushed onto the stack. Parser
configurations are represented by a stack, a list
of (remaining) input tokens, and the (current) set
of arcs for the dependency tree. Similar to the
shift-reduce parser, the construction of syntactic
structure is created by a sequence of transitions.
The parser starts with an empty stack and termi-
nates when the input queue is empty, parsing in-
put from left to right. It has four transitions (Left-
Arc, Right-Arc, Reduce and Shift), manipulating
these data structures. The algorithm has a linear
time complexity as it is guaranteed to terminate
after at most 2n transitions, given that the length
of the input sentence is n.
In contrast to a parser guided by a grammar
(e.g., ordinary shift-reduce parsing for context-
free grammars), this parser is guided by a clas-
sifier induced from empirical data using machine
learning (Nivre et al, 2004). Hence, the parser re-
quires training data containing dependency trees.
In other words, the parser has a training phase
where the training data is used by the training
module in order to learn the correct sequence of
transitions. The training data can contain depen-
dency trees for sentences of any language irrespec-
tively of whether the language is a natural or for-
mal one.
The training module produces the correct tran-
sition sequences using the dependency trees of
the training data. These correct parser configura-
tions and transition sequences are then provided as
training data to a classifier, which predicts the cor-
rect transitions (including a dependency label for
Left-Arc, Right-Arc) given parser configurations.
A parser configuration contains a vast amount of
information located in the data-structures. It is
therefore necessary to abstract it into a set of fea-
tures. Possible features are word forms and parts-
50
of-speech of tokens on the stack and in the list
of input tokens, and dependency labels of depen-
dency arcs created so far.
The parser produces exactly one syntactic anal-
ysis for every input, even if the input does not con-
form to a grammar. The price we have to pay for
this robustness is that any classifier is bound to
commit errors even if the input is acceptable ac-
cording to a grammar.
3 General Approach
In section 2, we presented a parsing algorithm for
producing dependency trees for natural languages.
Here we will show how it can be used to produce
syntactic structures for programming languages.
Since the framework requires training data form-
ing correct dependency trees, we need an approach
for converting source code to dependency trees.
The general approach can be divided into two
phases, training and production. In order to be
able to perform both these phases in this study, we
need to adapt natural language parsing to the needs
of information extraction from programming lan-
guage code, i.e., we need to automatically produce
training data. Therefore, we apply:
(a) Source Code ? Syntax Tree: the classical
approach for generating syntax trees for cor-
rect and complete source code of a program-
ming language.
(b) Syntax Tree ? Dependency Tree: an ap-
proach for encoding the syntax trees as de-
pendency trees adapted to programming lan-
guages.
(c) Dependency Tree ? Syntax Tree: an ap-
proach to convert the dependency trees back
to syntax trees.
These approaches have been accomplished as pre-
sented below. In the training phase, we need to
train and adapt the generic parsing approach to a
specific programming language. Therefore:
(1) Generate training data automatically by
producing syntax trees and then dependency
trees for correct programs using approaches
(a) and (b).
(2) Train the generic parser with the training
data.
This automated training phase needs to be done
for every new programming language we adapt to.
Finally, in the production phase, we extract the in-
formation from (not necessarily correct and com-
plete) programs:
(3) Parse the new source code into dependency
trees.
(4) Convert the dependency trees into syntax
trees using approach (c).
This automated production phase needs to be exe-
cuted for every project we analyze.
Steps (2) and (3) have already been discussed in
section 2 for parsing natural languages. They can
be generalized to parsing programming languages
as described in section 3.1. Both the training phase
and the production phase are complete, once the
steps (a)?(c) have been accomplished. We present
them in sections 3.2, 3.3, and 3.4, respectively.
3.1 Adapting the Input
As mentioned, the parsing algorithm described
in section 2 has been developed for natural lan-
guages, which makes it necessary to resolve a
number of issues that arise when the parser is
adapted for source code as input. First, the parsing
algorithm takes a sequence of words as input, and
for simplicity, we map the tokens in a program-
ming language to words.
One slightly more problematic issue is how to
define a ?sentence? in source code. A natural
language text syntactically decomposes into a se-
quence of sentences in a relatively natural way.
But is there also a natural way of splitting source
code into sentences? The most apparent approach
may be to define a sentence as a compilation unit,
that is, a file of source code. This can however re-
sult in practical problems since a sentence in a nat-
ural language text is usually on average between
15?25 words long, partially depending on the au-
thor and the type of text. The sequence of tokens
in a source file may on the other hand be much
longer. Time complexity is usually in practice of
less importance when the average sentence length
is as low as in natural languages, but that is hardly
the case when there can be several thousands to-
kens in a sentence to parse.
Other approaches could for instance be to let
one method be a sentence. However, then we need
to deal with other types of source code construc-
tions explicitly. We have in this study for sim-
plicity let one compilation unit be one sentence.
This is possible in practice due to the linear time
51
complexity of the parsing algorithm of section 2,
a quite unusual property compared to other NLP
parsers guided by machine learning with state-of-
the-art accuracy.
3.2 Source Code? Syntax Tree
In order to produce training data for the parser
for a programming language, an analyzer that
constructs syntax trees for correct and complete
source code of the programming language is
needed. We are in this study focusing on Java,
Python and C/C++, and consequently need one
such analyzer for each language. For example, fig-
ure 2 shows the concrete syntax tree of the follow-
ing fragments of Java:
Example (1):
public String getName() {
return name;
}
Example (2):
while (count > 0) {
stack[--count]=null;
}
We also map the output of the lexical ana-
lyzer to the parts-of-speech for the words (e.g.,
Identifier for String and getName). All
source code comments and indentation informa-
tion (except for Python where the indentation con-
veys hierarchical information) have been excluded
from the syntax trees. All string and character
literals have also been mapped to ?string? and
?char?, respectively. This does not entail that the
approach is lossy, since all this information can
be retained in a post-processing step, if neces-
sary. As pointed out by, for instance, Collard et
al. (2003), comments and indentation may among
other things be of interest when trying to under-
stand source code.
3.3 Syntax Tree? Dependency Tree
Here we will discuss the conversion of syntax trees
into dependency trees. We use a method that has
been successfully applied for natural languages
for converting syntax trees into a convertible de-
pendency tree that makes it possible to perform
the inverse conversion, meaning that information
about the syntax tree is saved in complex arc la-
bels (Hall and Nivre, 2008). We also present re-
sults in section 4 using the dependency trees that
cannot be used for the inverse conversion, which
we call non-convertible dependency trees.
The conversion is performed in a two-step ap-
proach. First, the algorithm traverses the syntax
tree from the root and identifies the head-child and
the terminal head for all nonterminals in a recur-
sive depth-first search. To identify the head-child
for each nonterminal, the algorithm uses heuristics
called head-finding rules, inspired by, for instance,
Magerman (1995). Three head-finding strategies
have been investigated. For each nonterminal:
1. FREQ: Let the element with the most fre-
quently occurring name be the head, but ex-
clude the token ?;? as a potential head. If two
tokens have the same frequency, let the left-
most occurring element be the head.
2. LEFT: let the leftmost terminal in the entire
subtree of the nonterminal be the head of all
other elements.
3. RIGHT: let the rightmost terminal in the en-
tire subtree of the nonterminal be the head of
all other elements.
The dependency trees in figures 3 and 4 use LEFT
and FREQ. LEFT and RIGHT induce that all arcs
are pointing to the right and left, respectively. The
head-finding rules for FREQ are automatically cre-
ated by counting the children?s names for each
distinct non-terminal name in the syntax trees of
the training data. The priority list is then com-
piled by ordering the elements by descending fre-
quency for each distinct non-terminal name. For
instance, given that the syntax trees are grammati-
cally correct, every non-terminal While will con-
tain the tokens (, ) and while. These tokens
have thus the highest priority, and while there-
fore becomes the head in the lower dependency
tree of figure 4. This is the same as choosing the
left-most mandatory element for each left-hand
side in the grammar. An interesting observation
is that binary operators and the copy assignment
operator become the heads of their operands for
FREQ, which is the case for < and = in figure 4.
Note also that the element names of terminals act
as part-of-speech tags, e.g., the part-of-speech for
String is Identifier.
In the second step, a dependency tree is created
according to the identified terminal heads. The
arcs in the convertible dependency tree are labeled
with complex arc labels, where each complex arc
label consists of two sublabels:
52
Figure 2: Syntax trees for examples (1) and (2).
Figure 3: Non-convertible dependency trees for example (1) using LEFT (upper) and FREQ (lower).
1. Encode the dependent spine, i.e., the se-
quence of nonterminal labels from the de-
pendent terminal to the highest nonterminal
where the dependent terminal is the terminal
head; ?|? separates the nonterminal labels,
2. Encode the attachment point in the head
spine, a non-negative integer value a, which
means that the dependent spine is attached a
steps up in the head spine.
By encoding the arc labels with these two subla-
bels, it is possible to perform the inverse conver-
sion, (see subsection 3.4).
The non-convertible dependency labels allow us
to reduce the complexity of the arc labels, making
the learning problem simpler due to fewer distinct
arc labels. This may result in a higher accuracy
during parsing and can be used as input for fur-
ther processing directly without taking the detour
back to syntax trees. This can be motivated by
the fact that all information in the syntax trees is
usually not needed anyway in many reverse engi-
neering tasks, but labels indicating method calls
and declarations ? the most important information
for most program comprehension tasks ? are pre-
served. This is exemplified by the fact that both
dependency structures in figure 3 contain the la-
bel MethodsDecl.. We thus believe that all the
necessary information is also captured in this less
informative dependency tree. Each dependency la-
bel is the highest nonterminal name of the spine,
that is, the single nonterminal name that is closest
to its head. The non-convertible dependency label
also excludes the attachment point value, making
the learning problem even simpler. Figures 3 and
4 show the non-convertible dependency labels of
the syntax trees (or phrase structure trees) in the
same figures, where each label contains just a sin-
gle nonterminal name of the original syntax trees.
3.4 Dependency Tree? Syntax Tree
The inverse conversion is a bottom-up and top-
down process on the convertible dependency tree
53
Figure 4: Non-convertible dependency trees for example (2) using LEFT (upper) and FREQ (lower).
(must contain complex arc labels). First, the algo-
rithm visits every terminal in the convertible de-
pendency tree and restores the spines of nontermi-
nals with labels for each terminal using the infor-
mation in the first sublabel of the incoming arc.
Thus, the bottom-up process results in a spine of
zero or more arcs from each terminal to the highest
nonterminal of which the terminal is the terminal
head. Secondly, the spines are weaved together ac-
cording to the arcs of the dependency tree. This is
achieved by traversing the dependency tree recur-
sively from the root using a pre-order depth-first
search, where the dependent spine is attached to
its head spine or to the root of the syntax tree. The
attachment point a, given by the second sublabel,
specifies the number of nonterminals between the
terminal head and the attachment nonterminal.
4 Experiments
We will in this section present parsing experiments
and evaluate the accuracy of the syntax trees pro-
duced by the parser. As mentioned in section 2,
the parsing algorithm is robust in the sense that it
always produces a syntactic analysis no matter the
input, but it can commit errors even for correct in-
put. This section investigates the accuracy for cor-
rect input, when varying feature set, head-finding
rules and language. We begin with the experimen-
tal setup.
4.1 Experimental Setup
The open-source software MaltParser (malt-
parser.org) (Nivre et al, 2006) is used in the ex-
periments. It contains an implementation of the
parsing algorithm, as well as an implementation
of the conversion strategy from syntax trees to
dependency trees and back, presented in subsec-
tions 3.3 and 3.4. It comes with the machine
learner LIBSVM (Chang and Lin, 2001), pro-
ducing the most accurate results for parsing nat-
ural languages compared to other evaluated ma-
chine learners (Hall et al, 2006). LIBSVM re-
quires training data. The source files of the follow-
ing projects have been converted into dependency
trees:
? For Java: Recoder 0.83 (Gutzmann et al,
2007), using all source files in the directory
?src? (having 400 source files with 92k LOC
and 335k tokens).
? For C/C++: Elsa 2005.08.22b (McPeak,
2005), where 1389 source files were used,
including the 978 C/C++ benchmark files in
the distribution (thus comprising 1389 source
files with 265k LOC and 691k tokens).
? For Python: Natural Language Toolkit
0.9.5 (Bird et al, 2008), where all source files
in the directory ?nltk? were used (having 160
source files with 65k LOC and 280k tokens).
To construct the syntax tree for the source code
file of Recoder, we have used Recoder. It cre-
ates an abstract syntax tree for a source file, but
we are currently interested in the concrete syntax
tree with all the original tokens. In this first con-
version step, the tokens of the syntax trees are thus
retained. For example, the syntax trees in figure 2
are generated by Recoder.
54
The same strategy was adopted for Elsa with the
difference that CDT 4.0.3, a plug-in to the Eclipse
IDE to produce syntax trees for source code of
C/C++, was used for producing the abstract syntax
trees.1 It produces abstract syntax trees just like
Recoder, so the concrete syntax trees have also
been created by retaining the tokens.
The Python 2.5 interpreter is actually shipped
with an analyzer that produces concrete syn-
tax trees (using the Python imports from
ast import PyCF ONLY AST and import
parser), which we have utilized for the Python
project above. Hence, no additional processing is
needed in order prepare the concrete syntax trees
as training data.
For the experiments, the source files have been
divided into a training set T and a development
test set D, where the former comprises 80% of the
dependency trees and the latter 10%. The remain-
ing 10% (E) has been left untouched for later use.
The source files have been ordered alphabetically
by the file names including the path. The depen-
dency trees have then been distributed into the data
sets in a pseudo-randomized way. Every tenth de-
pendency tree starting at index 9 (i.e. dependency
trees 9, 19, 29, . . . ) will belong to D, and every
tenth dependency trees starting at index 0 to E.
The remaining trees constitute the training set T .
4.2 Metrics
The standard evaluation metric for parse trees for
natural languages based on context-free grammar
is F-score, the harmonic mean of precision and
recall. F-score compares constituents ? defined
by triples ?i, j,XP ? spanning between terminals
i and j ? derived from the test data with those
derived from the parser. A constituent in the
parser output matches a constituent in the test data
when they span over the same terminals in the
input string. Recall is the ratio of matched con-
stituents over all constituents in the test data. Pre-
cision is the ratio of matched constituents over
all constituents found by the parser. F-score
comes in two versions, one unlabeled (FU ) and
one labeled (FL), where each correct constituent
in the latter also must have the correct nontermi-
nal name (i.e., XP ). The metric is implemented
in Evalb (Collins and Sekine, 2008).
1It is worth noting that CDT failed to produce syntax trees
for 2.2% of these source files, which were consequently ex-
cluded from the experiments. This again indicates the diffi-
cult of parsing C/C++ due to its different dialects.
FL FU
FR LE RI FR LE RI
UL 82.1 93.5 74.6 92.3 97.9 90.6
L 89.7 97.7 80.8 95.8 99.3 92.1
Table 1: F-score for various parser models and
head-finding rules for Java, where FR = FREQ, LE
= LEFT and RI = RIGHT.
The standard evaluation metric measuring accu-
racy for dependency parsing for natural language
is, on the other hand, labeled (ASL) and unlabeled
(ASU ) attachment score. ASU is the ratio of to-
kens attached to its correct head. ASL is the same
as ASU with the additional requirement that the
dependency label should be correct as well.
4.3 Results
This section presents the parsing results. The first
experiment was conducted for Java, using the in-
verse transformation back to syntax trees. Two
feature models are evaluated, one unlexicalized
feature sets (UL) containing 13 parts-of-speech
and 4 dependency label features, and one lexical-
ized feature sets (L) containing all these 17 fea-
tures and 13 additional word form features, de-
veloped by manual feature optimization. Table 1
compares these two feature sets, as well as the dif-
ferent head-finding rules discussed previously.
The figures give a clear answer to the question
whether lexical information is beneficial or not.
Every figure in the row L is higher than its cor-
responding figure in the row UL. This means that
names of variables, methods, classes, etc., actu-
ally contain valuable information for the classifier.
This is in contrast to ordinary syntactic parsing us-
ing a grammar of programming languages where
all names are mapped to the same value (e.g. Iden-
tifier), and, e.g., integer constants to IntLiteral, be-
fore the parse. One potential contributing factor
of the difference is the naming conventions that
programmers normally follow. For example, nam-
ing classes, class attributes and local variables, etc.
using typical methods names, such as equals in
Java, is usually avoided by programmers.
It is just as clear that the choice of head-finding
strategy is very important. For both FL and FU ,
the best choice is with a wide margin LEFT, fol-
lowed by FREQ. RIGHT is consequently the least
accurate one. A higher amount of arcs pointing to
the right seems to be beneficial for the strategy of
55
ASL ASU
FR LE RI FR LE RI
CO 87.6 96.6 86.6 90.9 98.2 90.7
NC 91.0 99.1 89.5 92.1 99.7 90.7
Table 2: Attachment score for Java and the lexical
feature set, where CO = convertible and NC = non-
convertible dependency trees.
Python C/C++
FL FU FL FU
UL 91.5 92.1 95.6 96.4
L 99.1 99.2 96.5 96.9
Table 3: F-score for various parser models and
head-finding rules LEFT for Python and C/C++.
parsing from left to right.
Table 1 can be compared to the accuracy on
the parser output before conversion from depen-
dency trees to syntax trees. This is shown in the
first row (CO) of table 2, where all information
in the complex dependency label is concatenated
and placed in the dependency label. The relation-
ships between the head-finding strategies remain
the same, but it is worth noting that the accuracies
for FREQ and RIGHT are closer to each other, en-
tailing a more difficult conversion to syntax trees
for the latter. The first row can also be compared
to the second row (NC) in the same table, show-
ing the accuracies when training and parsing with
non-convertible dependency trees. One observa-
tion is that each figure in NC is higher than its
corresponding figure in CO (even ASU for RIGHT
with more decimals), probably attributed to the
lower burden on the parser. Both ASU and ASL
are above 99% for the non-convertible dependency
trees using LEFT.
We can see that choosing an appropriate repre-
sentation of syntactic structure to be used during
parsing is just as important for programming lan-
guages as for natural languages, when using data-
driven natural language parsers (Bikel, 2004).
The parser output in table 1 can more eas-
ily be used as input to existing program com-
prehension tools, normally requiring abstract syn-
tax trees. However, the highly accurate output
for LEFT using non-convertible dependency trees
could be worth using instead, but it requires some
additional processing.
In order to investigate the language indepen-
dence of our approach, table 3 contains the cor-
responding figures as in table 1 for Python and
C/C++, restricted to LEFT, which is the best
head-finding strategy for these languages as well.
Again, each lexicalized feature set (L) outper-
forms its corresponding unlexicalized feature set
(UL). Python has higher FL and virtually the same
FU as Java, whereas C/C++ has the lowest accu-
racies for L. However, the UL figures are not far
behind the L figures for C/C++, and C/C++ has
in fact higher FL for UL compared to Java and
Python. These results can maybe be explained by
the fact that C/C++ has less verbose syntax than
both Java and Python, making the lexical features
less informative.
The FL figures for Java, Python and C/C++ us-
ing LEFT can also be compared to the correspond-
ing figures in Nilsson et al (2009). They use the
same data sets but a slightly different head-finding
strategy. Instead of selecting the leftmost element
(terminal or non-terminal) as in LEFT, they always
select the leftmost terminal, resulting in FL=99.5
for Java, FL=98.3 for Python and FL=96.5 for
C/C++. That is, our results are slightly lower for
Java, higher for Python, and slightly higher for
C/C++. The same holds for FU as well. That
is, having only arcs pointing to the right results in
high accuracy for all languages (which is the case
for Left described in section 3), but small devia-
tions from this head-finding strategy can in fact be
beneficial for some languages.
We are not aware of any similar studies for
programming languages2 so we compare the re-
sults to natural language parsing. First, the fig-
ures in table 2 for dependency structure are better
than figures reported for natural languages. Some
natural languages are easier to parse than others,
and the parsing results of the CoNLL shared task
2007 (Nivre et al, 2007) for dependency structure
indicate that English and Catalan are relatively
easy, with ASL around 88-89% and ASU around
90-94% for the best dependency parsers.
Secondly, compared to parsing German with
phrase structure with the same approach as here,
with FU = 81.4 and FL = 78.7%, and Swedish,
with FU = 76.8 and FL = 74.0 (Hall and Nivre,
2A comparative experiment using another data-driven
NLP parser for context-free grammar could be of theoreti-
cal interest. However, fast parsing time is important in pro-
gram comprehension tasks, and data-driven NLP parsers for
context-free grammar have worse than a linear time complex-
ity. As, e.g., the Java project has 838 tokens per source file,
linear time complexity is a prerequisite in practice.
56
Correct Label Parsing Label
66 FieldReference VariableReference
25 VariableReference FieldReference
12 MethodDeclaration LocalVariableDeclaration
9 Conditional FieldReference
5 NotEquals MethodReference
4 Plus MethodReference
4 Positive *
4 LessThan FieldReference
4 GreaterOrEquals FieldReference
4 Divide FieldReference
4 Modulo FieldReference
4 LessOrEquals FieldReference
3 Equals NotEquals
3 LessOrEquals Equals
3 NotEquals Equals
Table 4: Confusion matrix for Java using non-
convertible dependency trees with LEFT, ordered
by descending frequency.
2008), the figures reported in tables 1 and 3 are
also much better. It is however worth noting that
natural languages are more complex and less reg-
ular compared to programming languages. Al-
though it remains to be shown, we conjecture that
these figures are sufficiently high for a large num-
ber of program comprehension tasks.
4.4 Error Analysis
This subsection will study the result for Java with
non-convertible dependency trees (NC) and LEFT,
in order to get a deeper insight into the types of
errors that the parser commits. Specifically, the
labeling mistakes caused by the parser are investi-
gated here. This is done by producing a confusion
matrix based on the dependency labels. That is,
how often does a parser confuse label X with la-
bel Y . This is shown in table 4 for the 15 most
common errors.
The two most frequent errors show that the
parser confuses FieldReference and VariableRef-
erence. A FieldReference refers to a class attribute
whereas a VariableReference could refer to either
an attribute or a local variable. The parser mixes a
reference to a class attribute with a reference that
could also be a local variable or vice versa. The
error is understandable, since the parser obviously
has no knowledge about where the variables are
declared. This is an error that type and name anal-
ysis can easily resolve. On the use-occurrence of a
name (reference), analysis looks up for both pos-
sible define-occurrences of the name (declaration),
first a LocalVariableDeclaration and then a Field-
Declaration. It uses the one that is found first.
Another type of confusion involves declara-
tions, where a MethodDeclaration is misinter-
preted as a LocalVariableDeclaration. This type
of error can be resolved by a simple post-
processing step: a LocalVariableDeclaration fol-
lowed by opening parenthesis (always recognized
correctly) is a MethodDeclaration.
Errors that involve binary operators, e.g., Con-
ditional, NotEqual, Plus, are at rank 4 and below
in the list of the most frequent errors. They are
most likely a result of the incremental left-to-right
parsing strategy. The whole expression should be
labeled in accordance with its binary operator (see
count > 0 in figure 4 for LEFT), but is incor-
rectly labeled as either MethodReference, Field-
Reference or some other operator instead. The ref-
erences actually occur in the left-hand side sub-
expression of the binary operators. This means
that subexpressions and bracketing were recog-
nized correctly, but the type of the top expression
node was mixed up. Extending the lookahead in
the list of remaining input tokens, making it pos-
sible for the classifier in the parser to look at even
more yet unparsed tokens, might be one possible
solution. However, these errors are by and large
relatively harmless anyway. Hence, no correction
is in practice needed.
Figure 5 displays some typical mistakes for the
example program fragment
return (fw.unitIndex == unitIndex &&
fw.unitIndex.equals(unitList));
The parser mixes up a ParenthesizedExpression
with a Conditional, a boolean ParenthesizedEx-
pression only occurring in conditional statements
and expressions. Then it incorrectly assigns the
label Equals to the arc between the first left paren-
thesis and the first fw instead of the correct la-
bel LogicalAnd. It mixes up the type of the whole
expression, an Equals- (i.e., ==) is taken for an
LogicalAnd-expression (i.e., &&). Finally, the two
FieldReferences are taken as more general Vari-
ableReferences, which is corrigible as discussed.
In addition to a possible error correction in a
post-processing step, the parsing errors could dis-
appear due to the abstraction of subsequent anal-
yses as commonly used in software maintenance
tools. For instance, without any error correction,
the type reference graphs of our test program, the
correct one and the one constructed using the not
quite correct parsing results, are identical.
57
Correct:
Parsed:
Figure 5: Typical errors for LEFT using by non-convertible dependency trees.
5 Related Work
Classical parsers for formal languages have been
known for many years. They (conventionally) ac-
cept a context-free language defined by a context-
free grammar. For each program, the parsers
produce a phrase structure referred to as an ab-
stract syntax tree (AST) which is also defined by a
context-free language. Parsers including error sta-
bilization and AST-constructors can be generated
from context-free grammars for parsers (Kastens
et al, 2007). A parser for a new language still
requires the development of a complex specifica-
tion. Moreover, error stabilization often throws
away large parts of the source ? it is robust but
does not care about maximizing accuracy.
Breadth-First Parsing (Ophel, 1997) was de-
signed to provide better error stabilization than tra-
ditional parsers and parser generators. It uses a
two phase approach: the first phase identifies high-
level entities ? the second phase parses the struc-
ture with these entities as root nonterminals (ax-
ioms).
Fuzzy Parsing (Koppler, 1997) was designed
to efficiently develop parsers by performing the
analysis on selected parts of the source instead
of the whole input. It is specified by a set of
(sub)grammars each with their own axioms. The
actual approach is then similar to Breadth-First
Parsing: it scans for instances of the axioms and
then parses according to the grammar. It makes
parsing more robust in the sense that it ignores
source fragments ? including missing parts, errors
and deviations therein ? that subsequent analyses
abstract from anyway. A prominent tool using
the fuzzy parsing approach for information extrac-
tion in reverse-engineering tools is Sniff (Bischof-
berger, 1992) for analyzing C++ code.
Island grammars (Moonen, 2001) generalize on
Fuzzy Parsing. Parsing is controlled by two gram-
mar levels (island and sea) where the sea-level is
used when no island-level production applies. The
island-level corresponds to the sub-grammars of
fuzzy parsing. Island grammars have been applied
in reverse-engineering, specifically, to bank soft-
ware (Moonen, 2002).
Syntactic approximation based on lexical anal-
ysis was developed with the same motivation as
our work: when maintenance tools need syntac-
tic information but the documents could not be
parsed for some reason, hierarchies of regular ex-
pression analyses could be used to approximate
the information with high accuracy (Murphy and
Notkin, 1995; Cox and Clarke, 2003). Their in-
formation extraction approach is characterized as
?lightweight? in the sense that it requires little
specification effort.
A similar robust and light-weight approach for
information extraction constructs XML formats
(JavaML and srcML) from C/C++/Java programs
first, before further processing with XML tools
like Xpath (Badros, 2000; Collard et al, 2003). It
combines lexical and context free analyses. Lex-
ical pattern matching is also used in combination
with context free parsing in order to extract facts
from semi-structured specific comments and con-
58
figuration specifications in frameworks (Knodel
and Pinzger, 2003).
TXL is a rule-based language defining informa-
tion extraction and transformation rules for formal
languages (Cordy et al, 1991). It makes it possible
to incrementally extend the rule base and to adapt
to language dialects and extensions. As the rules
are context-sensitive, TXL goes beyond the lexical
and context-free approaches discussed before.
The fundamental difference of our approach
compared to lexical, context-free, and context-
sensitive approaches (and combinations thereof) is
that we use automated machine learning instead of
manual specification for defining and adapting the
information extraction.
General NLP techniques have been applied for
extracting facts from general source code com-
ments to support software maintenance (Etzkorn
et al, 1999). Comments are extracted from source
code using classical lexical analysis; additional in-
formation is extracted (and then added) with clas-
sical compiler front-end technology.
NLP has also been applied to other informa-
tion extraction tasks in software maintenance to
analyze unstructured or very large information
sources, e.g., for analyzing requirement speci-
fications (Sawyer et al, 2002), in clone detec-
tion (Marcus and Maletic, 2001; Grant and Cordy,
2009), and to connect program documentation to
source code (Marcus and Maletic, 2003).
6 Conclusions and Future Work
In this paper, we applied natural language parsing
techniques to programming languages. One ad-
vantage is that it offers robustness, since it always
produces some output even if the input is incorrect
or incomplete. Completely correct analysis can,
however, not be guaranteed even for correct input.
However, the experiments showed that accuracy is
in fact close to 100%.
In contrast to robust information extractors used
so far for formal languages, the approach pre-
sented here is rapidly adaptable to new languages.
We automatically generate the language specific
information extractor using machine learning and
training of a generic parsing, instead of explicitly
specifying the information extractor using gram-
mar and transformation rules. Also the training
data can be generated automatically. This could
increase the development efficiency of parsers,
since no language specification has to be provided,
only examples.
Regarding accuracy, the experiments showed
that selecting the syntactic base representation
used by the parser internally has a major impact.
Incorporating, for instance, class, method and
variable names in the set of features of the parser
improves the accuracy more than expected. The
detailed error analysis showed that many errors
committed by the parser are forgivable, as they
are anyway abstracted in later processing phases.
Other errors are easily corrigible. We can also
see that the best results presented here are much
higher than the best parsing results for natural lan-
guages.
Besides efficient information extractor develop-
ment, efficient parsing itself is important. Applied
to programs which can easily contain several mil-
lion lines of code, a parser with more than linear
time complexity is not acceptable. The data-driven
parser utilized here has linear parsing time.
These results are only the first (promising) step
towards natural language parsing leveraging infor-
mation extraction for software maintenance. How-
ever, the only way to really evaluate the usefulness
of the approach is to use its output as input to client
analyses, e.g., software measurement and archi-
tecture recovery, which we plan to do in the fu-
ture. Another direction for future work is to apply
the approach to more dialects of C/C++, such as
analyzing correct, incomplete, and erroneous pro-
grams for both standard C and its dialects.
References
Paul Anderson. 2008. 90 % Perspiration: Engineering
Static Analysis Techniques for Industrial Applica-
tions. In Proceedings of the 8th IEEE International
Working Conference on Source Code Analysis and
Manipulation, pages 3?12.
Greg J. Badros. 2000. JavaML: a Markup Language
for Java Source Code. In Proceedings of the 9th
International World Wide Web conference on Com-
puter networks : the international journal of com-
puter and telecommunications networking, pages
159?177.
Daniel M. Bikel. 2004. Intricacies of Collins? Parsing
Model. Computational Linguistics, 30(4):479?511.
Steven Bird, Edward Loper, and Ewan Klein.
2008. Natural Language Toolkit (NLTK) 0.9.5.
http://nltk.org/.
Walter R. Bischofberger. 1992. Sniff: A Pragmatic
Approach to a C++ Programming Environment. In
USENIX C++ Conference, pages 67?82.
59
Chih-Chung Chang and Chih-Jen Lin. 2001. LIB-
SVM: A Library for Support Vector Machines.
Michael L. Collard, Huzefa H. Kagdi, and Jonathan I.
Maletic. 2003. An XML-Based Lightweight C++
Fact Extractor. In 11th IEEE International Work-
shop on Program Comprehension, pages 134?143.
Michael Collins and Satoshi Sekine. 2008. Evalb.
http://nlp.cs.nyu.edu/evalb/.
James R. Cordy, Charles D. Halpern-Hamu, and Eric
Promislow. 1991. TXL: a Rapid Prototyping Sys-
tem for Programming Language Dialects. Computer
Languages, 16(1):97?107.
Anthony Cox and Charles L. A. Clarke. 2003. Syntac-
tic Approximation Using Iterative Lexical Analysis.
In Proceedings of the 11th IEEE International Work-
shop on Program Comprehension, pages 154?163.
Letha H. Etzkorn, Lisa L. Bowen, and Carl G. Davis.
1999. An Approach to Program Understanding by
Natural Language Understanding. Natural Lan-
guage Engineering, 5(3):219?236.
Scott Grant and James R. Cordy. 2009. Vector Space
Analysis of Software Clones. In Proceedings of
the IEEE 17th International Conference on Program
Comprehension, pages 233?237.
Tobias Gutzmann, Dirk Heuzeroth, and Mircea Trifu.
2007. Recoder 0.83. http://recoder.sourceforge.net/.
Johan Hall and Joakim Nivre. 2008. Parsing Discon-
tinuous Phrase Structure with Grammatical Func-
tions. In Proceedings of GoTAL, pages 169?180.
Johan Hall, Joakim Nivre, and Jens Nilsson. 2006.
Discriminative Classifiers for Deterministic Depen-
dency Parsing. In Proceedings of COLING-ACL,
pages 316?323.
Uwe Kastens, Anthony M. Sloane, and William M.
Waite. 2007. Generating Software from Specifica-
tions. Jones and Bartlett Publishers.
Jens Knodel and Martin Pinzger. 2003. Improving
Fact Extraction of Framework-Based Software Sys-
tems. In Proceedings of 10th Working Conference
on Reverse Engineering, pages 186?195.
Rainer Koppler. 1997. A Systematic Approach to
Fuzzy Parsing. Software - Practice and Experience,
27(6):637?649.
David M. Magerman. 1995. Statistical Decision-tree
Models for Parsing. In Proceedings of ACL, pages
276?283.
Andrian Marcus and Jonathan I. Maletic. 2001. Iden-
tification of High-Level Concept Clones in Source
Code. In Proceedings of the 16th IEEE interna-
tional conference on Automated software engineer-
ing, page 107.
Andrian Marcus and Jonathan I. Maletic. 2003. Re-
covering Documentation-to-Source-Code Traceabil-
ity Links using Latent Semantic Indexing. In Pro-
ceedings of the 25th International Conference on
Software Engineering, pages 125?135.
Scott McPeak. 2005. Elsa: The
Elkhound-based C/C++ Parser.
http://www.cs.berkeley.edu/?smcpeak.
Leon Moonen. 2001. Generating Robust Parsers using
Island Grammars. In Proceedings of the 8th Work-
ing Conference on Reverse Engineering, pages 13?
22.
Leon Moonen. 2002. Lightweight Impact Analysis us-
ing Island Grammars. In Proceedings of the 10th In-
ternational Workshop on Program Comprehension,
pages 219?228.
Gail C. Murphy and David Notkin. 1995. Lightweight
Source Model Extraction. SIGSOFT Software Engi-
neering Notes, 20(4):116?127.
Jens Nilsson, Welf Lo?we, Johan Hall, and Joakim
Nivre. 2009. Natural Language Parsing for Fact Ex-
traction from Source Code. In Proceedings of 17th
IEEE International Conference on Program Com-
prehension, pages 223?227.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based Dependency Parsing. In Proceed-
ings of CoNLL, pages 49?56.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
MaltParser: A Data-Driven Parser-Generator for
Dependency Parsing. In Proceedings of LREC,
pages 2216?2219.
Joakim Nivre, Johan Hall, Sanda Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 Shared Task on De-
pendency Parsing. In Proceedings of CoNLL/ACL,
pages 915?932.
Joakim Nivre. 2003. An Efficient Algorithm for
Projective Dependency Parsing. In Proceedings of
IWPT, pages 149?160.
John Ophel. 1997. Breadth-First
Parsing. citeseerx.ist.psu.edu/view-
doc/summary?doi=10.1.1.50.3035.
Pete Sawyer, Paul Rayson, and Roger Garside. 2002.
REVERE: Support for Requirements Synthesis
from Documents. Information Systems Frontiers,
4(11):343?353.
Dennis Strein, Ru?diger Lincke, Jonas Lundberg, and
Welf Lo?we. 2007. An Extensible Meta-Model for
Program Analysis. IEEE Transactions on Software
Engineering, 33(9):592?607.
60
Dependency Parsing of Turkish
Gu?ls?en Eryig?it?
Istanbul Technical University
Joakim Nivre?? ?
Va?xjo? University, Uppsala University
Kemal Oflazer?
Sabanc? University
The suitability of different parsing methods for different languages is an important topic in
syntactic parsing. Especially lesser-studied languages, typologically different from the languages
for which methods have originally been developed, pose interesting challenges in this respect. This
article presents an investigation of data-driven dependency parsing of Turkish, an agglutinative,
free constituent order language that can be seen as the representative of a wider class of languages
of similar type. Our investigations show that morphological structure plays an essential role in
finding syntactic relations in such a language. In particular, we show that employing sublexical
units called inflectional groups, rather than word forms, as the basic parsing units improves
parsing accuracy. We test our claim on two different parsing methods, one based on a probabilis-
tic model with beam search and the other based on discriminative classifiers and a deterministic
parsing strategy, and show that the usefulness of sublexical units holds regardless of the parsing
method. We examine the impact of morphological and lexical information in detail and show that,
properly used, this kind of information can improve parsing accuracy substantially. Applying
the techniques presented in this article, we achieve the highest reported accuracy for parsing the
Turkish Treebank.
1. Introduction
Robust syntactic parsing of natural language is an area in which we have seen tremen-
dous development during the last 10 to 15 years, mainly on the basis of data-driven
methods but sometimes in combination with grammar-based approaches. Despite this,
most of the approaches in this field have only been tested on a relatively small set
of languages, mostly English but to some extent also languages like Chinese, Czech,
Japanese, and German.
? Department of Computer Engineering, Istanbul Technical University, 34469 Istanbul, Turkey.
E-mail: gulsen.cebiroglu@itu.edu.tr.
?? School of Mathematics and Systems Engineering, Va?xjo? University, 35260 Va?xjo?, Sweden.
E-mail: joakim.nivre@msi.vxu.se.
? Department of Linguistics and Philology, Uppsala University, Box 635, 75126 Uppsala, Sweden.
? Faculty of Engineering and Natural Sciences, Sabanc? University, 34956 Istanbul, Turkey.
E-mail: oflazer@sabanciuniv.edu.
Submission received: 5 October 2006; revised submission received: 3 April 2007; accepted for publication:
16 May 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 3
An important issue in this context is to what extent our models and algorithms
are tailored to properties of specific languages or language groups. This issue is es-
pecially pertinent for data-driven approaches, where one of the claimed advantages
is portability to new languages. The results so far mainly come from studies where a
parser originally developed for English, such as the Collins parser (Collins 1997, 1999),
is applied to a new language, which often leads to a significant decrease in the measured
accuracy (Collins et al 1999; Bikel and Chiang 2000; Dubey and Keller 2003; Levy and
Manning 2003; Corazza et al 2004). However, it is often quite difficult to tease apart the
influence of different features of the parsing methodology in the observed degradation
of performance.
A related issue concerns the suitability of different kinds of syntactic representation
for different types of languages. Whereas most of the work on English has been based
on constituency-based representations, partly influenced by the availability of data
resources such as the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), it
has been argued that free constituent order languages can be analyzed more adequately
using dependency-based representations, which is also the kind of annotation found,
for example, in the Prague Dependency Treebank of Czech (Hajic? et al 2001). Recently,
dependency-based parsing has been applied to 13 different languages in the shared
task of the 2006 Conference on Computational Natural Language Learning (CoNLL)
(Buchholz and Marsi 2006).
In this article, we focus on dependency-based parsing of Turkish, a language that
is characterized by rich agglutinative morphology, free constituent order, and predom-
inantly head-final syntactic constructions. Thus, Turkish can be viewed as the repre-
sentative of a class of languages that are very different from English and most other
languages that have been studied in the parsing literature. Using data from the recently
released Turkish Treebank (Oflazer et al 2003), we investigate the impact of different
design choices in developing data-driven parsers. There are essentially three sets of
issues that are addressed in these experiments.
 The first set includes issues relating to the treatment of morphology in
syntactic parsing, which becomes crucial when dealing with languages
where the most important clues to syntactic functions are often found in
the morphology rather than in word order patterns. Thus, for Turkish, it
has previously been shown that parsing accuracy can be improved by
taking morphologically defined units rather than word forms as the basic
units of syntactic structure (Eryig?it and Oflazer 2006). In this article, we
corroborate this claim showing that it holds in both approaches we
explore. We also study the impact of different morphological feature
representations on parsing accuracy.
 The second set of issues concerns lexicalization, a topic that has been very
prominent in the parsing literature lately. Whereas the best performing
parsers for English all make use of lexical information, the real benefits of
lexicalization for English as well as other languages remains controversial
(Dubey and Keller, 2003; Klein and Manning 2003; Arun and Keller 2005).
 The third set concerns the basic parsing methodology, including both
parsing algorithms and learning algorithms. We first introduce a statistical
parser using a conditional probabilistic model which is very sensitive to
the selected representational features and thus clearly exposes the ones
358
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
with crucial importance for parsing Turkish. We then implement our
models on a deterministic classifier-based parser using discriminative
learning, which is one of the best performing dependency parsers
evaluated on a wide range of different languages.
Additionally we address the following issues:
 We investigate learning curves and provide an error analysis for the best
performing parser.
 For most of our experiments we use as input the gold-standard tags from
the treebank. However, in our last experiments we evaluate the impact of
automatic statistical morphological disambiguation on the performance of
our best performing parser.
The rest of the article is structured as follows. Section 2 gives a very brief introduc-
tion to Turkish morphology and syntax and discusses the representation of morpholog-
ical information and syntactic dependency relations in the Turkish Treebank. Section 3
is devoted to methodological issues, in particular the data sets and evaluation metrics
used in experiments. The following two sections present two different dependency
parsers trained and evaluated on the Turkish Treebank: a probabilistic parser (Section 4)
and a classifier-based parser (Section 5). Section 6 investigates the impact of lexicaliza-
tion and morphological information on the two parsers, and Section 7 examines their
learning curves. Section 8 presents an error analysis for the best performing parser,
and Section 9 analyzes the degradation in parsing performance when using automatic
morphological disambiguation. Section 10 discusses related work, and Section 11 sum-
marizes the main conclusions from our study.
2. Turkish: Morphology and Dependency Relations
Turkish displays rather different characteristics compared to the more well-studied
languages in the parsing literature. Most of these characteristics are also found in
many agglutinative languages such as Basque, Estonian, Finnish, Hungarian, Japanese,
and Korean.1 Turkish is a flexible constituent order language. Even though in written
texts the constituent order predominantly conforms to the SOV order, constituents may
freely change their position depending on the requirements of the discourse context
(Erguvanl? 1979; Hoffman 1994). However, from a dependency structure point of view,
Turkish is predominantly (but not exclusively) head final.
Turkish has a very rich agglutinative morphological structure. Nouns can give rise
to about 100 inflected forms and verbs to many more. Furthermore, Turkish words may
be formed through very productive derivations, increasing substantially the number of
possible word forms that can be generated from a root word. It is not uncommon to find
up to four or five derivations in a single word. Previous work on Turkish (Hakkani-
Tu?r, Oflazer, and Tu?r 2002; Oflazer 2003; Oflazer et al 2003; Eryig?it and Oflazer 2006)
has represented the morphological structure of Turkish words by splitting them into
inflectional groups (IGs). The root and derivational elements of a word are represented
1 We, however, do not necessarily suggest that the morphological sublexical representation that we use for
Turkish later in this article is applicable to these languages.
359
Computational Linguistics Volume 34, Number 3
by different IGs, separated from each other by derivational boundaries (DB). Each IG is
then annotated with its own part of speech and any inflectional features as illustrated
in the following example:2
araban?zdayd?
(?it was in your car?)
araban?zda DB yd?
araba+Noun+A3sg+P2pl+Loc
? ?? ?
IG1
DB +Verb+Zero+Past+A3sg
? ?? ?
IG2
?in your car? ?it was?
In this example, the root of the word araban?zdayd? is araba (?car?) and its part of speech is
noun. From this, a verb is derived in a separate IG. So, the word is composed of two IGs
where the first one, araban?zda (?in your car?), is a noun in locative case and in second
plural possessive form, and the second one is a verbal derivation from this noun in past
tense and third person singular form.
2.1 Dependency Relations in Turkish
Because most syntactic information is mediated by morphology, it is not sufficient
for the parser to only find dependency relations between orthographic words;3 the
correct IGs involved in the relations should also be identified. We can motivate this
with the following very simple example: In the phrase spor araban?zdayd? (?it was in
your sports car?), the adjective spor (?sports?) should be connected to the first IG of
the second word. It is the word araba (?car?) which is modified by the adjective, not
the derived verb form araban?zdayd? (?it was in your car?). So a parser should not just
say that the first word is a dependent of the second but also state that the syntactic
relation is between the last IG of the first word and the first IG of the second word, as
shown:
spor
Mod

araban?zda DB yd?
In Figure 1 we see a complete dependency tree for a Turkish sentence laid on top of the
words segmented along IG boundaries. The rounded rectangles show the words and
IGs within words are marked with dashed rounded rectangles. The first thing to note
in this figure is that the dependency links always emanate from the last IG of a word,
because that IG determines the role of that word as a dependent. The dependency links
land on one of the IGs of a (head) word (almost always to the right). The non-final IGs
(e.g., the first IG of the word okuldaki in Figure 1) may only have incoming dependency
2 +A3sg = 3sg number agreement, +P2pl = 2pl possessive agreement, +Loc = Locative Case.
3 For the same reason, Bozsahin (2002) uses morphemes as sublexical constituents in a CCG framework.
Because the lexicon was organized in terms of morphemes each with its own CCG functor, the grammar
had to account for both the morphotactics and the syntax at the same time.
360
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
Figure 1
Dependency links in an example Turkish sentence.
?+?s indicate morpheme boundaries. The rounded rectangles show words, and IGs within words
that have more than one IG are indicated by the dashed rounded rectangles. The inflectional
features of each IG as produced by the morphological analyzer are listed below the IG.
links and are assumed to be morphologically linked to the next IG to the right (but we
do not explicitly show these links).4
The noun phrase formed by the three words o?g?rencilerin en ak?ll?s? in this example
highlights the importance of the IG-based representation of syntactic relations. Here
in the word ak?ll?s?, we have three IGs: The first contains the singular noun ak?l (?intelli-
gence?), the second IG indicates the derivation into an adjective ak?ll? (?intelligence-with?
? ?intelligent?). The preceding word en (?most?), an intensifier adverb, is linked to this IG
as a modifier (thus forming ?most intelligent?). The third IG indicates another derivation
into a noun (?a singular entity that is most intelligent?). This last IG is the head of a
dependency link emanating from the word o?g?rencilerin with genitive case-marking (?of
the students? or ?students? ?) which acts as the possessor of the last noun IG of the third
word ak?ll?s?. Finally, this word is the subject of the verb IG of the last word, through its
last IG.
2.2 The Turkish Treebank
We have used the Turkish Treebank (Oflazer et al 2003), created by the Middle East
Technical University and Sabanc? University, in the experiments we report in this ar-
ticle. The Turkish Treebank is based on a small subset of the Metu Turkish Corpus
(www.ii.metu.edu.tr/?corpus/corpus.html), a balanced collection of post-1990 Turk-
ish text from 10 genres. The version that has been used in this article is the version used
in the CoNLL-X shared task publicly available from www.ii.metu.edu.tr/?corpus/
treebank.html.
This treebank comprises 5,635 sentences in which words are represented with IG-
based gold-standard morphological representation and dependency links between IGs.
4 It is worth pointing out that arrows in this representation point from dependents to heads, because
representations with arrows in the opposite direction also exist in the literature.
361
Computational Linguistics Volume 34, Number 3
The average number of IGs per word is 1.26 in running text, but the figure is higher for
open class words and 1 for high frequency function words which do not inflect. Of all
the dependencies in the treebank, 95% are head-final5 and 97.5% are projective.6
Even though the number of sentences in the Turkish Treebank is in the same range
as for many other available treebanks for languages such as Danish (Kromann 2003),
Swedish (Nivre, Nilsson, and Hall 2006), and Bulgarian (Simov, Popova, and Osenova
2002), the number of words is considerably smaller (54K as opposed to 70?100K for the
other treebanks). This corresponds to a relatively short average sentence length in the
treebank of about 8.6 words, which is mainly due to the richness of the morphological
structure, because often one word in Turkish may correspond to a whole sentence in
another language.
3. Dependency Parsing of Turkish
In the following sections, we investigate different approaches to dependency parsing
of Turkish and show that using parsing units smaller than words improves the parsing
accuracy. We start by describing our evaluation metrics and the data sets used, and
continue by presenting our baseline parsers: two naive parsers, which link a dependent
to an IG in the next word, and one rule-based parser. We then present our data-driven
parsers in the subsequent sections: a statistical parser using a conditional probabilistic
model (from now on referred to as the probabilistic parser) in Section 4 and a deter-
ministic classifier-based parser using discriminative learning (from now on referred to
as the classifier-based parser) in Section 5.
3.1 Data Sets and Evaluation Metrics
Our experiments are carried out on the entire treebank and all our results are reported
for this data set. We use ten-fold cross-validation for the evaluation of the parsers, except
for the baseline parsers which do not need to be trained. We divide the treebank data
into ten equal parts and in each iteration use nine parts as training data and test the
parser on the remaining part.
We report the results as mean scores of the ten-fold cross-validation, with standard
error. The main evaluation metrics that we use are the unlabeled attachment score
(ASU) and labeled attachment score (ASL), namely, the proportion of IGs that are
attached to the correct head (with the correct label for ASL). A correct attachment is
one in which the dependent IG (the last IG in the dependent word) is not only attached
to the correct head word but also to the correct IG within the head word. Where relevant, we
also report the (unlabeled) word-to-word score (WWU), which only measures whether
a dependent word is connected to (some IG in) the correct head word. It should be
clear from the discussion in Section 2.1 and from Figure 1 that the IG-to-IG evaluation
is the right one to use for Turkish even though it is more stringent than word-to-
word evaluation. Dependency links emanating from punctuation are excluded in all
5 Half of the head-initial dependencies are actually not real head-initial structures; these are caused by
some enclitics (addressed in detail in the following sections) which can easily be recovered with some
predefined rules.
6 A dependency between a dependent i and a head j is projective if and only if all the words or IGs that
occur between i and j in the linear order of the sentence are dominated by j. A dependency analysis with
only projective dependencies corresponds to a constituent analysis with only continuous constituents.
362
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
evaluation scores. Non-final IGs of a word are assumed to link to the next IG within the
word, but these links, referred to as InnerWord links, are not considered as dependency
relations and are excluded in evaluation scoring.
3.2 Baseline Parsers
We implemented three baseline parsers to assess the performance of our probabilistic
and classifier-based parsers. The first baseline parser attaches each word (from the last
IG) to the first IG of the next word while the second parser attaches each word to the
final IG of the next word. Obviously these two baseline parsers behave the same when
the head word has only one IG. The final punctuation of each sentence is assumed to
be the root of the sentence and it is not connected to any head. The first two lines of Ta-
ble 1 give the unlabeled attachment scores of these parsers. We observe that attaching
the link to the first IG instead of the last one gives better results.
The third baseline parser is a rule-based parser that uses a modified version of the
deterministic parsing algorithm by Nivre (2006). This parsing algorithm, which will be
explained in detail in Section 5, is a linear-time algorithm that derives a dependency
graph in one left-to-right pass over the input, using a stack to store partially processed
tokens and a list to store remaining input tokens in a way similar to a shift-reduce
parser. In the rule-based baseline parser, the next parsing action is determined according
to 31 predefined hand-written rules (Eryig?it 2006; Eryig?it, Adal?, and Oflazer 2006).
The rules determine whether or not to connect the units (words or IGs) on top of the
stack and at the head of the input list (regardless of dependency labels). It can be
seen that the rule-based parser provides an improvement of about 15 percentage points
compared to the relatively naive simpler baseline parsers which cannot recover head-
initial dependencies.
4. Probabilistic Dependency Parser
A well-studied approach to dependency parsing is a statistical approach where the
parser takes a morphologically tagged and disambiguated sentence as input, and
outputs the most probable dependency tree by using probabilities induced from the
training data. Such an approach comprises three components:
1. A parsing algorithm for building the dependency analyses (Eisner 1996;
Sekine, Uchimoto, and Isahara 2000)
2. A conditional probability model to score the analyses (Collins 1996)
Table 1
Unlabeled attachment scores and unlabeled word-to-word scores for the baseline parsers.
Parsing Model ASU WWU
Attach-to-next (first IG) 56.0 63.3
Attach-to-next (last IG) 54.1 63.3
Rule-based 70.5 79.3
363
Computational Linguistics Volume 34, Number 3
3. Maximum likelihood estimation to make inferences about the underlying
probability models (Collins 1996; Chung and Rim 2004)
4.1 Methodology
The aim of our probabilistic model is to assign a probability to each candidate depen-
dency link by using the frequencies of similar dependencies computed from a training
set. The aim of the parsing algorithm is then to explore the search space in order to find
the most probable dependency tree. This can be formulated with Equation (1) where S
is a sequence of n units (words or IGs) and T ranges over possible dependency trees
consisting of dependency links dep(ui,uH(i) ), with uH(i) denoting the head unit to which
the dependent unit ui is linked and the probability of a given tree is the product of the
dependency links that it comprises.
T? = argmax
T
P(T|S) = argmax
T
n?1
?
i=1
P(dep (ui,uH(i) ) |S) (1)
The observation that 95% of the dependencies in the Turkish treebank are head-
final dependencies motivated us to employ the backward beam search dependency
parsing algorithm by Sekine, Uchimoto, and Isahara (2000) (originally designed for
Japanese, another head-final language), adapted to our morphological representation
with IGs. This algorithm parses a sentence starting from the end moving towards the
beginning, trying at each step to link the dependents to a unit to the right. It uses a
beam which keeps track of the most probable dependency structures for the partially
processed sentence. However, in order to handle head-initial dependencies, it employs
three predefined lexicalized rules7 (also used in our rule-based baseline parser). For
every new word, the parser starts by checking if any of the rules apply. If so, it constructs
a right-to-left link whereH(i) < i and directly assigns 1.0 as the dependency probability
(P(dep (ui,uH(i) ) |S) = 1.0). If none of the rules apply, it instead uses probabilities for
head-final dependencies.
For the probability model, we adopt the approach by Chung and Rim (2004), which
itself is a modified version of the statistical model used in Collins (1996).8 In this model
in Equation (2), the probability of a dependency link P(dep (ui,uH(i) ) |S) linking ui to a
head uH(i) is approximated with the product of two probabilities:
P(dep (ui,uH(i) ) |S) ? P(link(ui,uH(i) ) |?i ?H(i) ) (2)
P(ui links to some head dist(i,H(i)) away |?i)
7 The rules check for enclitics such as de, ki, mi, written on the right side of and separately from the word
they attach to, for the verb deg?il, which gives a negative meaning to the word coming before it and for
nominals which do not have any verbs on their right side.
8 The statistical model in Collins (1996) is actually used in a phrase-structure-based parsing approach, but
it uses the same idea of computing probabilities between dependents and head units. We also tried to
employ the statistical model of Collins, where the distance measure ?i,H(i) is included in the link
probability formula (P(dep (ui,uH(i) ) |S) ? P(link(ui,uH(i) ) |?i,H(i) )) , but we obtained worse results
with this.
364
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
In this equation,
 P(link(ui,uH(i) ) |?i ?H(i) ) is the probability of seeing the same dependency
within a similar context where ?i represents the context around the
dependent ui and ?H(i) represents the context around the head uH(i), and
 P(ui links to some head dist(i,H(i)) away |?i) is the probability of seeing
the dependent linking to some head a distance dist(i,H(i)) away, in the
context ?i.
In all of the following models, dist(i,H(i)) is taken as the number of actual word
boundaries between the dependent and the head unit regardless of whether full words
or IGs were used as units of parsing.9
To alleviate the data sparseness, we use the interpolation of other estimates while
calculating the probabilities in Equation (2).10 We use a strategy similar to Collins (1996)
and we interpolate with estimates based on less context:
P(x|y) ? ? ? P1(x|y) + (1 ? ?) ? P2(x) (3)
where ? = ?/(?+ 1) and ? is the count of the x occurrences
During the actual runs, the smoothed probability P(link(ui,uH(i) ) |?i ?H(i) ) is estimated
by interpolating two unsmoothed empirical estimates extracted from the treebank:
P1(link(ui,uH(i) ) |?i ?H(i) ) and P2(link(ui,uH(i) )). A similar approach was employed for
P(ui links to some head dist(i,H(i)) away |?i) and it is estimated by interpolating
P1(ui links to some head dist(i,H(i)) away |?i) and P2(ui links to some head dist(i,H(i))
away). If even after interpolation, the probability is 0, then a very small value is
used. Further, distances larger than a certain threshold value were assigned the same
probability, as explained later.
4.2 The Choice of Parsing Units
In the probabilistic dependency parsing experiments, we experimented with three dif-
ferent ways of choosing and representing the units for parsing:11
1. Word-based model #1: In this model, the units of parsing are the actual words
and each word is represented by a combination of the representations of all the IGs
that make it up. Note that although all IGs are used in representing a word, not all the
information provided by an IG has to be used, as we will see shortly. This representation,
however, raises the following question: If we use the words as the parsing units and
9 We also tried other distance functions, for example, the number of IGs between dependent and head
units, but this choice fared better than the alternatives.
10 We tried many other different interpolation and backoff models where we tried to remove the neighbors
one by one or the inflectional features. But we obtained the best results with a two-level interpolation by
removing the contextual information all at once.
11 Clarifying examples of these representations will be provided in the immediately following section.
365
Computational Linguistics Volume 34, Number 3
find the dependencies between these, how can we translate these to the dependencies
between the IGs, since our goal is to find dependencies between IGs? The selection
of the IG of the dependent word is an easy decision, as it is the last IG in the word.
The selection of the head IG is obviously more difficult. Because such a word-based
model will not provide much information about the underlying IGs structure, we will
have to make some assumptions about the head IG. The observation that 85.6% of the
dependency links in the treebank land on the first (and possibly the only) IG of the
head word and the fact that our first baseline model (attaching to the first IG) gives
better performance than our second baseline model (attaching to the last IG), suggest
that after identifying the correct word, choosing the first IG as the head IG may be a
reasonable heuristic. Another approach to determining the correct IG in the head word
could be to develop a post-processor which selects this IG using additional rules. Such
a post-processor could be worth developing if the WWU accuracy obtained with this
model proves to be higher than all of the other models, that is, if this is the best way
of finding the correct dependencies between words without considering which IGs are
connected. However, as we will see in Section 4.4, this model does not give the best
WWU.
2. Word-based model #2: This model is just like the previous model but we rep-
resent a word using its final IGs rather than the concatenation of all their IGs when it
is used as a dependent. The representation is the same as in Word-based model #1 when
the word is a head. This results in a dynamic selection of the representation during
parsing as the representation of a word will be determined according to its role at that
moment. The representation of the neighboring units in context will again be selected
with respect to the word in question: any context unit on the left will be represented
with its dependent representation (just the last IG) and any neighbor on the right will
be represented with its representation as a head. The selection of the IG in the head
word is the same as in the first model.
3. IG-based model: In this model, we use IGs as units in parsing. We split the IG-
based representation of each word and reindex these IGs in order to use them as single
units in parsing. Figure 2 shows this transfer to the IG-based model. We still, however,
need to know which IGs are word-final as they will be the dependent IGs (shown in
the figure by asterisks). The contextual elements that are used in this model are the
IGs to the left (starting with the last IG of the preceding word) and to the right of the
dependent and the head IG.
Figure 2
Mapping from word-based to IG-based representation of a sentence.
366
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
4.3 Reduced Dynamic Representations for IGs
In all three models, it is certainly possible to use all the information supplied by the
full morphological analysis in representing the IGs.12 This includes the root words
themselves, major and minor parts of speech,13 number and person agreement markers,
possessive agreement markers, case markers, tense, aspect, mood markers, and other
miscellaneous inflectional and semantic markers especially for derivations. Not all of
these features may be relevant to the parsing task, and further, different features may
be relevant depending on whether the IG is being used as a dependent or a head. Also,
in order to alleviate the data sparseness problem that may result from the relatively
modest size of the treebank, an ?unlexicalized? representation that does not contain the
root word needs to be considered so that statistics from IGs that are otherwise the same
except for the root word (if any) can be conflated.14 After some preliminary experimen-
tation, we decided that a reduced representation for IGs that is dynamically selected
depending on head or dependent status would give us the best performance. We explain
the representation of the IGs and the parameters that we used in the three models.
 When used as a dependent (or part of a dependent word in models 1
and 2) during parsing;
? Nominal IGs (nouns, pronouns, and other derived forms that
inflect with the same paradigm as nouns, including infinitives, past
and future participles) are represented only with the case marker,
because that essentially determines the syntactic function of that IG
as a dependent, and only nominals have cases.
? Any other IG is just represented with its minor part of speech.
 When used as a head (or part of a head word in models 1 and 2);
? Nominal IGs and adjective IGs with participle minor part of
speech15 are represented with the minor part of speech and the
possessive agreement marker.
? Any other IG is just represented with its minor part of speech.
Figures 3?5 show, for the first three words in Figure 1, the unlexicalized reduced
representations that are used in the three models when units are used as dependents
and heads during parsing.
12 See Figure 1 for a sample of such information.
13 A minor part-of-speech category is available for some major part-of-speech categories: pronouns are
further divided into personal pronouns, demonstrative pronouns, interrogative pronouns, and so on. The
minor part-of-speech category always implies the major part of speech. For derived IGs, the minor part of
speech mostly indicates a finer syntactic or semantic characterization of the derived word. When no
minor part of speech is available the major part of speech is used.
14 Remember that only the first IG in a word has the root word.
15 These are modifiers derived from verbs. They have adjective as their major part of speech and
past/future participle as their minor part of speech. They are the only types of IGs that have possessive
agreement markers other than nominals.
367
Computational Linguistics Volume 34, Number 3
Figure 3
Reduced IG representation for Word-based model #1.
Figure 4
Reduced IG representation for Word-based model #2.
4.4 Experimental Results
In this section, we first evaluate the performance of the models described in Section 4.2.
We then investigate the impact of different choices of morphological features on the best
performing IG-based model. In addition to the parsing model, the parser is given the
following parameters:
 the number of left and right neighbors of the dependent (Dl, Dr) to define
the dependent context ?i,
16
 the number of left and right neighbors of the head (Hl, Hr) to define the
head context ?H(i),
 the size of the beam (beamsize), and
16 In terms of parsing units, the number of words for word-based models and the number of IGs for
IG-based models.
368
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
Figure 5
Reduced IG representation for IG-based model.
 the distance threshold value beyond which P(ui links to some head
dist(i,H(i)) away |?i) is assigned the same probability.
Table 2 gives the ASU scores for the word-based and IG-based models for the
best combinations of contexts used for each case. We also provide WWU scores for
comparison, but again stress that the main evaluation criterion is the ASU score. For
all three models, the beamsize value is selected as 3 and distance threshold is selected
as 6.17 It can be seen that the performance of the word-based models is lower than
our rule-based baseline parser (Table 1) with ASU = 70.5, even though it is better than
the first two rather naive baselines. On the other hand, the IG-based model outper-
forms all of the baseline parsers and word-based models. It should also be noted that
the IG-based model improves not only the ASU accuracy but also the word-to-word
accuracy compared, to the word-based models. Thus, the IG-based model not only
helps to recover the relations between correct IGs but also to find the correct head
word.
In Table 3, we also present results from experiments employing different represen-
tations for the IGs. A more detailed investigation about the use of limited lexicalization
and inflectional features will be presented later in Section 6. Here, we will see what
would have happened if we had used alternative reduced IG representations compared
to the representation described earlier, which is used in the best performing IG-based
model.
Table 3 gives the results for each change to the representational model. One can
see that none of these representational changes improves the performance of the best
performing model. Only employing major part-of-speech tags (#1) actually comes close,
and the difference is not statistically significant. Lexicalization of the model results in
a drastic decrease in performance: Using the surface form (#6) gives somewhat better
17 As stated earlier in Section 4.1, our distance function is calculated according to the word boundaries
between the dependent and the head units. In the treebank, 95% of the dependency links link to a word
that is less than six words away. Thus all the distances larger than or equal to six are conflated into the
same small probability.
369
Computational Linguistics Volume 34, Number 3
Table 2
Unlabeled attachment scores and unlabeled word-to-word scores for the probabilistic parser.
Parsing Model (parameters) ASU WWU
Word-based model #1 (Dl=1, Dr=1, Hl=1, Hr=1) 68.1?0.4 77.1?0.7
Word-based model #2 (Dl=1, Dr=1, Hl=1, Hr=1) 68.3?0.3 77.6?0.5
IG-based model (Dl=1, Dr=1, Hl=0, Hr=1) 72.1?0.3 79.0?0.7
results than using root information (#5). Also, dynamic selection of tags seems to help
performance (#3) but using all available inflectional information performs significantly
worse possibly due to data sparseness.
5. Classifier-Based Dependency Parser
Our second data-driven parser is based on a parsing strategy that has achieved a high
parsing accuracy across a variety of different languages (Nivre et al 2006, 2007). This
strategy consists of the combination of the following three techniques:
1. Deterministic parsing algorithms for building dependency graphs (Kudo
and Matsumoto 2002; Nivre 2003; Yamada and Matsumoto 2003)
Table 3
Unlabeled attachment scores for different choices for morphological features.
Model ASU
IG-based model
# (Dl=1, Dr=1, Hl=0, Hr=1) 72.1?0.3
1 Using major part of speech 71.2?0.2
instead of minor part of speech
2 Using only minor part of speech and 68.3?0.2
no other inflectional features
3 Using minor part of speech for all 71.0?0.3
types of IGs together with case and
possessive markers for nominals
and possessive marker for adjectives
(but no dynamic selection)
4 Using all inflectional features in 46.5?0.4
addition to minor part of speech
5 Adding root information to the best 53.7?0.2
performing IG-based model
6 Adding surface form information to the best 54.4?0.2
performing IG-based model
370
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
2. History-based models for predicting the next parser action (Black et al
1992; Magerman 1995; Ratnaparkhi 1997; Collins 1999)
3. Discriminative classifiers to map histories to parser actions (Kudo and
Matsumoto 2002; Yamada and Matsumoto 2003; Nivre, Hall, and Nilsson
2004)
A system of this kind employs no grammar but relies completely on inductive learning
from treebank data for the analysis of new sentences, and on deterministic parsing
for disambiguation. This combination of methods guarantees that the parser is robust,
never failing to produce an analysis for an input sentence, and efficient, typically
deriving this analysis in time that is linear in the length of the sentence.
In the following sections, we will first present the parsing methodology and then
results that show that the IG-based model again outperforms the word-based model. We
will then explore how we can further improve the accuracy by exploiting the advantages
of this parser. All experiments are performed using the freely available implementation
MaltParser.18
5.1 Methodology
For the experiments in this article, we use a variant of the parsing algorithm proposed
by Nivre (2003, 2006), a linear-time algorithm that derives a labeled dependency graph
in one left-to-right pass over the input, using a stack to store partially processed tokens
and a list to store remaining input tokens. However, in contrast to the original arc-eager
parsing strategy, we use an arc-standard bottom-up algorithm, as described in Nivre
(2004). Like many algorithms used for dependency parsing, this algorithm is restricted
to projective dependency graphs.
The parser uses two elementary data structures, a stack ? of partially analyzed
tokens and an input list ? of remaining input tokens. The parser is initialized with an
empty stack and with all the tokens of a sentence in the input list; it terminates as soon
as the input list is empty. In the following, we use subscripted indices, starting from 0,
to refer to particular tokens in ? and ?. Thus, ?0 is the token on top of the stack ? (the
top token) and ?0 is the first token in the input list ? (the next token); ?0 and ?0 are
collectively referred to as the target tokens, because they are the tokens considered as
candidates for a dependency relation by the parsing algorithm.
There are three different parsing actions, or transitions, that can be performed in
any non-terminal configuration of the parser:
 Shift: Push the next token onto the stack.
 Left-Arcr: Add a dependency arc from the next token to the top token,
labeled r, then pop the stack.
 Right-Arcr: Add a dependency arc from the top token to the next token,
labeled r, then replace the next token by the top token at the head of the
input list.
18 http://w3.msi.vxu.se/users/nivre/research/MaltParser.html.
371
Computational Linguistics Volume 34, Number 3
In order to perform deterministic parsing in linear time, we need to be able to predict
the correct parsing action (including the choice of a dependency type r for Left-Arcr
and Right-Arcr) at any point during the parsing of a sentence. This is what we use a
history-based classifier for.
The features of the history-based model can be defined in terms of different linguis-
tic features of tokens, in particular the target tokens. In addition to the target tokens,
features can be based on neighboring tokens, both on the stack and in the remaining
input, as well as dependents or heads of these tokens in the partially built dependency
graph. The linguistic attributes available for a given token are the following:
 Lexical form (root) (LEX)
 Part-of-speech category (POS)
 Inflectional features (INF)
 Dependency type to the head if available (DEP)
To predict parser actions from histories, represented as feature vectors, we use sup-
port vector machines (SVMs), which combine the maximum margin strategy introduced
by Vapnik (1995) with the use of kernel functions to map the original feature space
to a higher-dimensional space. This type of classifier has been used successfully in
deterministic parsing by Kudo and Matsumoto (2002), Yamada and Matsumoto (2003),
and Sagae and Lavie (2005), among others. To be more specific, we use the LIBSVM
library for SVM learning (Chang and Lin 2001), with a polynomial kernel of degree 2,
with binarization of symbolic features, and with the one-versus-one strategy for multi-
class classification.19
This approach has some advantages over the probabilistic parser, in that
 it can process both left-to-right and right-to-left dependencies due to its
parsing algorithm,
 it assigns dependency labels simultaneously with dependencies and can
use these as features in the history-based model, and
 it does not necessarily require expert knowledge about the choice of
linguistically relevant features to use in the representations because SVM
training involves implicit feature selection.
However, we still exclude sentences with non-projective dependencies during train-
ing.20 Because the classifier-based parser not only builds dependency structures but also
assigns dependency labels, we give ASL scores as well as ASU scores.
19 Experiments have also been performed using memory-based learning (Daelemans and Bosch 2005). They
were found to give lower parsing accuracy.
20 Because the frequency of non-projective dependencies in the Turkish Treebank is not high enough to
learn such dependencies and mostly due to the unconnected punctuations with which we are dealing by
adding an extra dependency label, we did not observe any improvement when applying the
pseudo-projective processing of Nivre and Nilsson (2005), which is reported to improve accuracy for
other languages.
372
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
5.2 Experimental Results
In this section, our first aim is to confirm the claim that using IGs as the units in parsing
improves performance. For this purpose, we start by using models similar to those
described in the previous section. We use an unlexicalized feature model where the
parser uses only the minor POS and the DEP of tokens and compare the results with the
probabilistic parser. We then show in the second part how we can improve accuracy by
exploiting the morphological structure of Turkish and taking advantage of the special
features of this parser.
5.2.1 Comparison with the Probabilistic Parser. In order to compare with the results of the
previous section, we adopt the same strategy that we used earlier in order to present
inflectional groups. We employ two representation models:
 Word-based model, where each word is represented by the concatenation
of its IGs,
 IG-based model, where the units are inflectional groups.
We take the minor POS category plus the case and possessive agreement markers for
nominals and participle adjectives to make up the POS feature of each IG.21 However,
we do not employ dynamic selection of these features and just use the same strategy
for both dependents and the heads. The reason is that, in this parser, we do not make
the assumption that the head is always on the right side of the dependent, but also
try to find head-initial dependencies, and the parser does not know at a given stage
if a unit is a candidate head or dependent. In the IG-based model, InnerWord relations
(Figure 5), which are actually determined by the morphological analyzer, are processed
deterministically without consulting the SVM classifiers.22
The feature model (Feature Model #1) to be used in these experiments is shown
in Figure 6. This feature model uses five POS features, defined by the POS of the two
topmost stack tokens (?0, ?1), the first two tokens of the remaining input (?0, ?1), and
the token which comes just after the topmost stack token in the actual sentence (?0 + 1).
The dependency type features involve the top token on the stack (?0), its leftmost and
rightmost dependent (l(?0), r(?0)), and the leftmost dependent of the next input token
(l(?0)).
The results for this feature model and the two representation models can be seen
in Table 4. We again see that the IG-based model outperforms the word-based model.
When we compare the unlabeled (ASU) scores with the results of the probabilistic parser
(from Table 2), we see that we do not obtain any improvements neither for the IG-based
model nor for the word-based model. This is probably the combined effect of not using
21 Thus, we are actually combining some inflectional features with the part-of-speech category and use
them together in the POS feature.
22 Because only the first IG of a word carries the stem information (and the remaining IGs has null ? ?
values for this field), a lexicalized model can easily determine the InnerWord links without need for a
deterministic model. For the unlexicalized models, it is necessary to process InnerWord relations
deterministically in order to get the full benefit of IG-based parsing, because the classifiers cannot
correctly predict these relations without lexical information (Eryig?it, Nivre, and Oflazer 2006). However,
for the lexicalized models, adding deterministic InnerWord processing has no impact at all on parsing
accuracy, but it reduces training and parsing time by reducing the number of training instances for the
SVM classifiers.
373
Computational Linguistics Volume 34, Number 3
Figure 6
Feature models for the classifier-based parser.
Table 4
Unlabeled and labeled attachment scores for the unlexicalized classifier-based parser.
Parsing Model ASU ASL
Word-based model 67.1?0.3 57.8?0.3
IG-based model 70.6?0.2 60.9?0.3
the lexical information for head-initial dependencies that we use in our rules in the
probabilistic parser, and of not using dynamic selection.23
5.2.2 Exploiting the Advantages of the Classifier-Based Parser. To exploit the advantages
of the classifier-based parser, we now describe a setting which does not rely on any
linguistic knowledge on the selection of inflectional features and lets the classifier of the
parser select the useful combinations of the features. As SVMs can perform such tasks
successfully, we now explore different representations of the morphological data in the
IG-based model to see if the performance can be improved.
As shown in earlier examples, the inflectional information available for a given
token normally consists of a complex combination of atomic features such as +A3sg,
+Pnon, and +Loc. Eryig?it, Nivre, and Oflazer (2006) showed that adding inflectional
features as atomic values to the feature models was better than taking certain subsets
with linguistic intuition and trying to improve on them. Thus we now present results
with the feature model where the POS component only comprises the minor part of
speech and the INF comprises all the other inflectional features provided by the tree-
bank without any reduction. We investigate the impact of this approach first with an
unlexicalized model (Feature Model #2 in Figure 6) and then with a lexicalized model
(Feature Model #3 in Figure 6) where we investigate two different kinds of lexicalization:
one using just the root information and one using the complete surface form as lexical
features.
Table 5 gives the results for both unlexicalized and lexicalized models with INF
features included in the feature model. We can see the benefit of using inflectional
features separately and split into atomic components, by comparing the first line of
the table with the best results for the IG-based model in Table 4. We can also note
23 Actually, the equivalent of this IG-based model is the probabilistic model #3 in Table 3 (with no dynamic
selection), which does not do significantly better than this classifier-based model.
374
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
Table 5
Unlabeled and labeled attachment scores for enhancements of the IG-based model.
Feature Model ASU ASL
Feature Model #2 (no lexicalization) 72.4?0.2 63.1?0.3
Feature Model #3 (lex. with surface forms) 75.7?0.2 66.6?0.3
Feature Model #3 (lex. with roots) 76.0?0.2 67.0?0.3
the improvement that lexicalized models bring:24 In contrast to the probabilistic parser,
lexicalization using root information rather than surface form gives better performance,
even though the difference is not statistically significant. The improvement in ASU
score is 3.9 percentage points for the lexicalized model (with root) over the IG-based
model of the probabilistic parser with ASU=72.1?0.3. A similar case can be observed for
WWU accuracies: Including INF and lexicalization with roots gives WWU=82.7?0.5 on
the entire treebank, which provides an improvement of 3.3 percentage points over the
IG-based model of the probabilistic parser (withWWU=79.0?0.7).
6. The Impact of Inflectional Features and Lexicalization
In the previous sections, we presented our parsers using optimized parameters and
feature representations. We have observed that using complete inflectional features and
lexicalized models improves the accuracy of the classifier-based parser significantly,
whereas for the probabilistic parser adding these features has a negative impact on
accuracy. In this section, we investigate the influence of different inflectional features
and lexical information on both parsers using the best performing IG-based models,
in order to get a more fine-grained picture. The results of the experiments with the
classifier-based parser are not strictly comparable to those of other experiments, because
the training data have here been divided into smaller sets (based on the major part of
speech category of the next token) as a way of reducing SVM training times without a
significant decrease in accuracy. For the probabilistic parser, we have not used dynamic
selection while investigating the impact of inflectional features.
6.1 Inflectional Features
In order to see the influence of inflectional features, we tested six different sets, where
each set includes the previous one and adds some more inflectional features. The
following list describes each set in relation to the previous one:
Set 1 No inflectional features except for minor part of speech
Set 2 Set 1 + case and possessive markers for nominals, possessive markers for partici-
ple adjectives
Set 3 Set 2 + person/number agreement features for nominals and verbs
Set 4 Set 3 + all inflectional features for nominals
24 The unlabeled exact match score (that is, the percentage of sentences for which all dependencies are
correctly determined) for this best performing model is 37.5% upon IG-based evaluation and 46.5% upon
word-based evaluation.
375
Computational Linguistics Volume 34, Number 3
Set 5 Set 4 + all inflectional features for verbs
Set 6 Set 5 + all inflectional features
Figure 7 shows the results for both the probabilistic and the classifier-based parser.
The results shown in Figures 7b confirm the importance of case and possessive features,
which was presupposed in the manual selection of features in Section 4. Besides these,
the number/person agreement features available for nominals and verbs are also impor-
tant inflectional features even though they do not provide any statistically significant
increase in accuracy (except for ASU in Figure 7b [Set 3]). Another point that merits
attention is the fact that the labeled accuracy is affected more by the usage of inflectional
features compared to unlabeled accuracy. The difference between Set 1 and Set 2 (in
Figure 7b) is nearly 4 percentage points for ASU and 10 percentage points for ASL. It
thus appears that inflectional features are especially important in order to determine the
type of the relationship between the dependent and head units. This is logical because
in Turkish it is usually not the word order that determines the roles of the constituents
in a sentence, but the inflectional features (especially the case markers). We again see
from these figures that the classifier-based parser does not suffer from sparse data even
if we use the full set of inflectional features (Set 6) provided by the treebank, whereas the
probabilistic parser starts having this problem even with Set 3 (Figure 7a). The problem
gets worse when we add the complete set of inflectional features.
6.2 Lexicalization
In order to get a more fine-grained view of the role of lexicalization, we have investi-
gated the effect of lexicalizing IGs from different major part-of-speech categories. We
expand this analysis into POS categories where relevant. The results are shown in Ta-
ble 6, where the first column gives the part-of-speech tag of the lexicalized units, and
the second and third columns give the total frequency and the frequency of distinct roots
for that part-of-speech tag. We again see that the probabilistic parser suffers from sparse
data especially for part-of-speech tags that appear with a high number of distinct roots.
We cannot observe any increase with the lexicalization of any category. The situation is
different for the classifier-based parser. None of the individual lexicalizations causes a
decrease. We see that the lexicalization of nouns causes a significant increase in accuracy.
Figure 7
Accuracy for feature sets 1?6:
a) Unlabeled accuracy for probabilistic parser
b) Unlabeled and labeled accuracy for classifier-based parser
376
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
Table 6
Unlabeled and labeled attachment scores for limited lexicalization (n = count, d = number of
distinct roots).
Probabilistic Classifier-based
n d ASU ASU ASL
None - - 72.1?0.3 72.8?0.2 63.2?0.3
Adjectives 6446 735 68.7?0.2 72.9?0.2 63.2?0.3
Adverbs 3033 221 69.8?0.3 73.1?0.2 63.4?0.3
Conjunctions 2200 44 67.8?0.4 74.1?0.2 64.2?0.3
Determiners 1998 13 71.8?0.3 72.8?0.2 63.3?0.3
Duplications 11 9 72.0?0.3 72.8?0.2 63.2?0.3
Interjections 100 34 72.0?0.3 72.8?0.2 63.2?0.3
Nouns 21860 3935 53.7?0.3 73.9?0.2 64.6?0.3
Numbers 850 226 71.4?0.3 72.9?0.2 63.3?0.3
Post-positions 1250 46 70.9?0.3 72.9?0.2 63.2?0.3
Pronouns 2145 28 72.0?0.2 72.8?0.2 63.2?0.3
Punctuations 10420 16 72.1?0.3 73.4?0.2 63.7?0.3
Questions 228 6 71.9?0.2 72.8?0.2 63.2?0.3
Verbs 14641 1256 59.9?0.4 72.9?0.2 63.8?0.3
Lexicalization of verbs also gives a noticeable increase in the labeled accuracy even
though this is not statistically significant. A further investigation on the minor parts of
speech of nouns25 shows that only common nouns have this positive effect, whereas the
lexicalization of proper nouns does not improve accuracy. We see that the lexicalization
of conjunctions also improves the accuracy significantly. This improvement can be at-
tributed to the enclitics (such as de, ki,mi, written on the right side of and separately from
the word they attach to), which give rise to head-initial dependencies. These enclitics,
which are annotated as conjunctions in the treebank, can be differentiated from other
conjunctions by lexicalization which makes it very easy to connect them to their head
on the left.
Because we did not observe any improvement in the probabilistic parser, we con-
tinued further experimentation only with the classifier-based parser. We tried partially
lexicalized models by lexicalizing various combinations of certain POS categories (see
Figure 8). The results show that, whereas lexicalization certainly improves parsing
accuracy for Turkish, only the lexicalization of conjunctions and nouns together has
an impact on accuracy. Similarly to the experiments on inflectional features, we again
see that the classifier-based parser has no sparse data problem even if we use a totally
lexicalized model.
Although the effect of lexicalization has been discussed in several studies recently
(Dubey and Keller 2003; Klein and Manning 2003; Arun and Keller 2005), it is often
investigated as an all-or-nothing affair, except for a few studies that analyze the distri-
butions of lexical items, for example, Bikel (2004) and Gildea (2001). The results for
25 IGs with a noun part-of-speech tag other than common nouns are marked with an additional minor part
of speech that indicates whether the nominal is a proper noun or a derived form?one of future
participle, past participle, infinitive, or a form involving a zero-morpheme derivation. These latter four
do not contain any root information.
377
Computational Linguistics Volume 34, Number 3
Figure 8
Unlabeled and labeled attachment scores for incrementally extended lexicalization for the
classifier-based parser.
Turkish clearly show that the effect of lexicalization is not uniform across syntactic
categories, and that a more fine-grained analysis is necessary to determine in what
respects lexicalization may have a positive or negative influence. For some models
(especially those suffering from sparse data), it may even be a better choice to use some
kind of limited lexicalization instead of full lexicalization, although the experiments
in this article do not show any example of that. The results from the previous section
suggests that the same is true for morphological information, but this time showing that
limited addition of inflectional features (instead of using them fully) helps to improve
the accuracy of the probabilistic parser.
7. The Impact of Training Set Size
In order to see the influence of the training set size on the performance of our parsers,
we designed the experiments shown in Figure 9, where the x-axis shows the number
of cross validation subsets that we used for training in each step. Figure 9 gives the
ASU scores for the probabilistic parser (unlexicalized except for head-initial rules) and
the classifier-based parser (unlexicalized and lexicalized). We observe that the relative
improvement with growing training set size is largest for the classifier-based lexicalized
model with a relative difference of 5.2?0.2 between using nine training subsets and one
training subset, whereas this number is 4.6?0.3 for the unlexicalized classifier-based
model and 2.5?0.2 for the unlexicalized probabilistic model. We can state that despite its
lower accuracy, the probabilistic model is less affected by the size of the training data.
We can see from this chart that the relative ranking of the models remain the same,
except for sizes 1?3, where the probabilistic parser does better (or no worse than) the
unlexicalized classifier-based models. Another conclusion may be that classifier-based
models are better at extracting information with the increasing size of the data in hand,
whereas the probabilistic model cannot be improved very much with the increasing size
of the data. We can observe this situation especially in the lexicalized model which is
improved significantly between size = 6 subsets and size = 9 subsets, whereas there is
no significant improvement on the unlexicalized models within this interval.
378
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
68
69
70
71
72
73
74
75
76
77
1 2 3 4 5 6 7 8 9
# cross validation sets used in training
probabilistic unlex.
classifier-based unlex.
classifier-based lex.
Figure 9
Unlabeled attachment score for different training set sizes.
8. Error Analysis
In this section, we present a detailed error analysis on the results of our best per-
forming parser. We first evaluate our results on different dependency types. We then
investigate the error distribution in terms of distance between the head assigned by
the parser and the actual head. Finally, we look at the error distribution in relation
to sentence length. In the analysis, the results are aggregated over all ten folds of the
cross-validation.
8.1 Accuracy per Dependency Type
Table 7 gives theASU, labeled precision, labeled recall and labeled F-score for individual
dependency types. The table is sorted according to the ASU results, and the average
distance between head and dependent is given for each type.
We see that the parser cannot find labeled dependencies for the types that have
fewer than 100 occurrences in the treebank, with the single exception of RELATIVIZER,
the enclitic ki (conjunction), written separately from the word it attaches to. Because this
dependency type always occurs with the same particle, there is no sparse data problem.
If we exclude the low-frequency types, we can divide the results into three main
groups. The first group consists of determiners, particles, and nominals that have an
ASU score over 79% and link to nearby heads. The second group mainly contains
subjects, objects, and different kinds of adjuncts, with a score in the range 55?79% and
a distance of 1.8?4.6 IGs to their head. This is the group where inflectional features are
most important for finding the correct dependency. The third group contains distant
dependencies with a much lower accuracy. These are generally relations like sentence
modifier, vocative, and apposition, which are hard to find for the parser because they
cannot be differentiated from other nominals used as subjects, objects, or normal mod-
ifiers. Another construction that is hard to parse correctly is coordination, which may
require a special treatment.
379
Computational Linguistics Volume 34, Number 3
Table 7
Attachment score (ASU), labeled precision (P), labeled recall (R) and labeled F-score for each
dependency type in the treebank (n = count, dist = dependency length).
Label n dist ASU P R F
SENTENCE 7,252 1.5 90.5 87.4 89.2 88.3
DETERMINER 1,952 1.3 90.0 84.6 85.3 85.0
QUESTION.PARTICLE 288 1.3 86.1 80.0 76.4 78.2
INTENSIFIER 903 1.2 85.9 80.7 80.3 80.5
RELATIVIZER 85 1.2 84.7 56.6 50.6 53.4
CLASSIFIER 2,048 1.2 83.7 74.6 71.7 73.1
POSSESSOR 1,516 1.9 79.4 81.6 73.6 77.4
NEGATIVE.PARTICLE 160 1.4 79.4 76.4 68.8 72.4
OBJECT 7,956 1.8 75.9 63.3 62.5 62.9
MODIFIER 11,685 2.6 71.9 66.5 64.8 65.7
DATIVE.ADJUNCT 1,360 2.4 70.8 46.4 50.2 48.2
FOCUS.PARTICLE 23 1.1 69.6 0.0 0.0 0.0
SUBJECT 4,479 4.6 68.6 50.9 56.2 53.4
ABLATIVE.ADJUNCT 523 2.5 68.1 44.0 54.5 48.7
INSTRUMENTAL.ADJUNCT 271 3.0 62.7 29.8 21.8 25.2
ETOL 10 4.2 60.0 0.0 0.0 0.0
LOCATIVE.ADJUNCT 1,142 4.2 56.9 43.3 48.4 45.7
COORDINATION 814 3.4 54.1 53.1 49.8 51.4
S.MODIFIER 594 9.6 50.8 42.2 45.8 43.9
EQU.ADJUNCT 16 3.7 50.0 0.0 0.0 0.0
APPOSITION 187 6.4 49.2 49.2 16.6 24.8
VOCATIVE 241 3.4 42.3 27.2 18.3 21.8
COLLOCATION 51 3.3 41.2 0.0 0.0 0.0
ROOT 16 - 0.0 0.0 0.0 0.0
Total 43,572 2.5 76.0 67.0 67.0 67.0
8.2 Error Distance
When we evaluate our parser based on the dependency direction, we obtain an ASU
of 72.2 for head-initial dependencies and 76.2 for head-final ones. Figure 10a and
Figure 10b give the error distance distributions for head-initial and head-final depen-
dencies based on the unlabeled performance of the parser. The x-axis in the figures gives
the difference between indexes of the assigned head IG and the real head IG.
As stated previously, the head-initial dependencies constitute 5% of the entire de-
pendencies in the treebank. Figure 10a shows that for head-initial dependencies the
parser has a tendency to connect the dependents to a head closer than the real head
or in the wrong direction. When we investigate these dependencies, we see that 70%
of them are connected to a head adjacent to the dependent and the parser finds 90% of
these dependencies correctly. Thus, we can say that the parser has no problem in finding
adjacent head-initial dependencies. Moreover, 87% of the errors where the error distance
is equal to 1 (Figure 10a)26 are due to the dependents being connected to the wrong IG
26 Meaning that the actual head and assigned head are adjacent.
380
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
Figure 10
Error distance distributions a) for head-initial dependencies b) for head-final dependencies.
of the correct head word. When we investigate the ability of the parser in finding the
dependency direction, we see that our parser has a high precision value (91%) and a
relatively lower recall value (80%).
The parser is 100% successful in finding the direction of head-final dependencies.
Furthermore, the errors that it makes while determining the correct head have a roughly
normal distance distribution, as can be seen from Figure 10b.27 We can see from the same
figure that 57% of the errors fall within the interval of ?2 IGs away from the actual
head.
8.3 Sentence Length
Figure 11 shows the distribution of errors over sentences of different lengths. The
x-axis shows the sentence length (measured in number of dependencies), the y-axis
shows the error count, and the z-axis shows the number of sentences. As expected,
the distribution is dominated by short sentences with few errors (especially sentences
of up to seven dependencies with one error). The mean number of errors appears to
be a linear function of sentence length, which would imply that the error probability
27 Error distances with less than 40 occurrences are excluded from the figure.
381
Computational Linguistics Volume 34, Number 3
Figure 11
Error distribution versus sentence length.
per word does not increase with sentence length. This is interesting in that it seems to
indicate that the classifier-based parser does not suffer from error propagation despite
its greedy, deterministic parsing strategy.
9. The Impact of Morphological Disambiguation
In all of the experiments reported herein, we have used the gold-standard tags provided
by the treebank. Another point that deserves investigation is therefore the impact of
using tags automatically assigned by a morphological disambiguator, in other words
the accuracy of the parser on raw text. The role of morphological disambiguators for
highly inflectional languages is far more complex than assigning a single main POS
category (e.g., Noun, Verb, Adj) to a word, and also involves assigning the correct mor-
phological information which is crucial for higher level applications. The complexity of
morphological disambiguation in an agglutinative language like Turkish is due to the
large number of morphological feature tag combinations that can be assigned to words.
The number of potential morphological tag combinations in Turkish for all practical
purposes is very large due to productively derived forms.28
The two subsequent examples, for the words kalemi and asmadan, expose the two
phenomena that a Turkish morphological disambiguator should deal with. The outputs
of the morphological analyzer are listed below the words. The first example shows that
all three possible analyses of the word kalemi have ?Noun? as the POS category but they
differ in that they have different stems and inflectional features. In the second example
28 For the treebank data, the number of distinct combinations of morphological features is 718 for the
word-based model of the classifier-based parser and 108 for the IG-based model.
382
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
we see that the possible analyses also have different IG segmentations; the first two
analyses of the word asmadan consists of two IGs whereas the last one has one IG.
kalemi
kale +Noun+A3sg+P1sg+Acc (?my castle? in accusative form)
kalem +Noun+A3sg+P3sg+Nom (?his pencil?)
kalem +Noun+A3sg+Pnon+Acc (?the pencil? in accusative form)
asmadan
as +Verb+Pos DB +Adverb+WithoutHavingDoneSo (?without having hanged (it)?)
as +Verb+Pos DB +Noun+Inf2+A3sg+Pnon+Abl (?from hanging (it)?)
asma +Noun+A3sg+Pnon+Abl (?from the vine?)
The task of the morphological disambiguator is to choose one of the possible mor-
phological analyses and thus to find the correct inflectional features including parts
of speech, and the IG structure. We first used the two-level morphological analyzer of
Oflazer (1994) to analyze all the words in the treebank.29 This morphological analyzer
simultaneously produces the IG segmentation and the relevant features encoded in all
analyses of a word form. We then used the morphological disambiguator of Yu?ret and
Tu?re (2006), which has a reported accuracy of 96% for Turkish.
When tested on our treebank data, the accuracy of the morphological disambiguator
is 88.4%, including punctuation (which is unambiguous) and using a lookup table for
the words that are not recognized by the morphological analyzer.30 The lower accuracy
of the morphological disambiguator on the treebank can be due to different selections
in the annotation process of the morphological disambiguator training data (Yu?ret and
Tu?re 2006), which is totally different from the treebank data.
In order to investigate the impact of morphological disambiguation errors, we used
our best IG-based model and a lexicalized word-based model with our classifier-based
parser.31 We again evaluated our parsing models with ASU, ASL, and WWU scores.
There is no problem when evaluating withWWU scores because this metric only takes
into account whether the head word assigned to a dependent is correct or not, which
means that any errors of the morphological disambiguator can be ignored. Similarly, in
calculating ASU and ASL scores for the word-based model, dependencies are assumed
to be connected to the first IG of the head word without taking into consideration any
errors in tags caused by the morphological disambiguator. But when evaluating with
the ASU and ASL scores for the IG-based model, one problem that may appear is that
the disambiguator may have assigned a totally different IG structure to the head word,
compared to the gold standard (cf. the three analyses of the word asmadan). In this case,
we accept a dependency link to be correct if the dependent is connected to the correct
head word and the head IG has the same POS category as the gold-standard. This is
reasonable because we know that some of the errors in inflectional features do not affect
the type of dependency very much. For example, if we put the adjective ku?c?u?k (?small?)
29 We noted that 39% of the words were ambiguous and 17% had more than two distinct morphological
analyses.
30 The words not recognized by the morphological analyzer are generally proper nouns, numbers, and
some combined words that are created in the development stage of the treebank and constitute 6.2% of
the whole treebank. If these words are excluded, the accuracy of the tagger is 84.6%.
31 For this model, we added LEX features for ?0, ?0, ?1 to the feature model of our word-based model in
Table 4.
383
Computational Linguistics Volume 34, Number 3
Table 8
Impact of morphological disambiguation on unlabeled and labeled attachment scores and
word-to-word scores.
ASU ASL WWU
Word-based Gold standard 71.2?0.3 62.3?0.3 82.1?0.9
Tagged 69.5?0.3 59.3?0.3 80.2?0.9
IG-based Gold standard 76.0?0.2 67.0?0.3 82.7?0.5
Tagged 73.3?0.3 63.2?0.3 80.6?0.7
in front of the example given previously (ku?c?u?k kalemi), then the choice of morphological
analysis of the noun has no impact on the fact that the adjective should be connected
to the noun with dependency type ?MODIFIER?. Moreover, most of the errors in POS
categories will actually prevent the parser from finding the correct head word, which
can be observed from the drop inWWU accuracy.
Table 8 shows that the IG-based model and the word-based model are equally
affected by the tagging errors and have a drop in accuracy within similar ranges. (It
can also be seen that, even with automatically tagged data, the IG-based model gives
better accuracy than the word-based model.) We can say that the use of an automatic
morphological analyzer and disambiguator causes a drop in the range of 3 percentage
points for unlabeled accuracy and 4 percentage points for labeled accuracy (for both
word-based and IG-based models).
10. Related Work
The first results on the Turkish Treebank come from Eryig?it and Oflazer (2006) where the
authors used only a subset of the treebank sentences containing exclusively head-final
and projective dependencies. The parser used in that paper is a preliminary version of
the probabilistic parser used in this article. The first results on the entire treebank appear
in Nivre et al (2007), where the authors use memory-based learning to predict parser
actions, and in Eryig?it, Adal?, and Oflazer (2006), which introduces the rule-based parser
used in this article.
The Turkish Treebank has recently been parsed by 17 research groups in the CoNLL-
X shared task on multilingual dependency parsing (Buchholz and Marsi 2006), where it
was seen as the most difficult language by the organizers and most of the groups.32 The
following quote is taken from Buchholz and Marsi (page 161): ?The most difficult data
set is clearly the Turkish one. It is rather small, and in contrast to Arabic and Slovene,
which are equally small or smaller, it covers 8 genres, which results in a high percentage
of new FORM and LEMMA values in the test set.?
The results for Turkish are given in Table 9. Our classifier-based parser obtained
the best results for Turkish (with ASU=75.8 and ASL=65.7) and also for Japanese, which
is the only agglutinative and head-final language in the shared task other than Turkish
(Nivre et al 2006). The groups were asked to find the correct IG-to-IG dependency links.
When we look at the results, we observe that most of the best performing parsers use
32 The Turkish data used in the shared task is actually a modified version of the treebank used in this article;
some conversions are made on punctuation structures in order to keep consistency between all languages.
384
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
Table 9
CoNLL-X shared task results on Turkish (taken from Table 5 in Buchholz and Marsi [2006]).
Teams ASU ASL
Nivre et al (2006) 75.8 65.7
Johansson and Nugues (2006) 73.6 63.4
McDonald, Lerman, and Pereira (2006) 74.7 63.2
Corston-Oliver and Aue (2006) 73.1 61.7
Cheng, Asahara, and Matsumoto (2006) 74.5 61.2
Chang, Do, and Roth (2006) 73.2 60.5
Yu?ret (2006) 71.5 60.3
Riedel, C?ak?c?, and Meza-Ruiz (2006) 74.1 58.6
Carreras, Surdeanu, and Marquez (2006) 70.1 58.1
Wu, Lee, and Yang (2006) 69.3 55.1
Shimizu (2006) 68.8 54.2
Bick (2006) 65.5 53.9
Canisius et al (2006) 64.2 51.1
Schiehlen and Spranger (2006) 61.6 49.8
Dreyer, Smith, and Smith (2006) 60.5 46.1
Liu et al (2006) 56.9 41.7
Attardi (2006) 65.3 37.8
one of the parsing algorithms of Eisner (1996), Nivre (2003), or Yamada and Matsumoto
(2003) together with a learning method based on the maximum margin strategy. We
can also see that a common property of the parsers which fall below the average
(ASL=55.4) is that they do not make use of inflectional features, which is crucial for
Turkish.33
Another recent study that has promising results is C?ak?c? and Baldridge (2006),
where the authors use the MSTParser (McDonald, Lerman, and Pereira 2006), also used
in the CoNLL-X shared task (line 3 in Table 9). Following the work of Eryig?it and Oflazer
(2006) and Nivre et al (2006), they use the stem information and the case information
for nominals and they also report an increase in performance by using these features.
Similar to one of the models (?INF as a single feature?) in Eryig?it, Nivre, and Oflazer
(2006), where the feature names of the suffixes provided by the morphological analyzer
are concatenated and used as a feature to the classifier, they use the surface forms of
the suffixes as a whole. We can say that the models in this article cover this approach in
that each suffix is used as a single feature name (which is shown to perform better than
using them concatenated to each other in Eryig?it, Nivre, and Oflazer [2006]). Because in
Turkish, the same suffixes take different forms under vowel harmony34 and the surface
forms of some different suffixes are structurally ambiguous,35 using them with their
feature names is actually more meaningful. C?ak?c? and Baldridge (2006) report a word-
to-word accuracy of 84.9%, which seems competitive, but unfortunately from this we
33 Actually, there are two parsers (Bick 2006 and Attardi 2006 in Table 9) in this group which try to use parts
of the inflectional features under special circumstances.
34 For example, in the words ev+de (?at home?) and okul+da (?at school?), the suffixes -de and -da are the same
locative case suffixes (+Loc) but they take different forms due to vowel harmony.
35 For example, in the word ev+in, the surface morpheme -inmay indicate both a second singular possessive
suffix (+P2sg) which will give the word the meaning of ?your house? and a genitive case (+Gen) which
will give the word the meaning of ?of the house?, as the underlying lexicalmorphemes are different.
385
Computational Linguistics Volume 34, Number 3
are not able to gauge the IG-to-IG accuracy which we have argued is the right metric
to use for Turkish, and their results are not comparable to any of the results in the
literature, because they have not based their experiments on any of the official releases
of the treebank. In addition, they use an evaluation metric different from the ones in
the literature in that they only excluded some of the punctuations from the evaluation
score.
11. Conclusions
In this article, we have investigated a number of issues in data-driven dependency pars-
ing of Turkish. One of the main results is that IG-based models consistently outperform
word-based models. This result holds regardless of whether we evaluate accuracy on
the word level or on the IG level; it holds regardless of whether we use the probabilistic
parser or the classifier-based parser; and it holds even if we take into account the
problem caused by errors in automatic morphological analysis and disambiguation.
Another important conclusion is that the use of morphological information can
increase parsing accuracy substantially. Again, this result has been obtained both for the
probabilistic and the classifier-based parser, although the probabilistic parser requires
careful manual selection of relevant features to counter the effect of data sparseness.
A similar result has been obtained with respect to lexicalization, although in this case
an improvement has only been demonstrated for the classifier-based parser, which is
probably due to its greater resilience to data sparseness.
By combining the deterministic classifier-based parsing approach with an adequate
use of IG-based representations, morphological information, and lexicalization, we have
been able to achieve the highest reported accuracy for parsing the Turkish Treebank.
Acknowledgments
We are grateful for the financial support
from TUBITAK (The Scientific and Technical
Research Council of Turkey) and Istanbul
Technical University. We want to thank Johan
Hall and Jens Nilsson in the language
technology group at Va?xjo? University for
their contributions to the classifier-based
parser framework (MaltParser) within which
we developed the classifier-based parser for
Turkish. We also want to thank Deniz Yu?ret
for providing us with his morphological
disambiguator, and Es?ref Adal? for his
valuable comments. Finally, we want to
thank our three anonymous reviewers for
insightful comments and suggestions
that helped us improve the final version of
the article.
References
Arun, Abhishek and Frank Keller. 2005.
Lexicalization in crosslinguistic
probabilistic parsing: The case of French.
In Proceedings of ACL?05, pages 302?313,
Ann Arbor, MI.
Attardi, Giuseppe. 2006. Experiments with a
multilanguage non-projective dependency
parser. In Proceedings of CONLL-X,
pages 166?170, New York, NY.
Bick, Eckhard. 2006. Lingpars, a linguistically
inspired, language-independent machine
learner for dependency treebanks. In
Proceedings of CONLL-X, pages 171?175,
New York, NY.
Bikel, Daniel M. 2004. A distributional
analysis of a lexicalized statistical parsing
model. In Proceedings of the Conference on
Empirical Methods in Natural Language
Processing, pages 182?189, Barcelona.
Bikel, Daniel M. and David Chiang. 2000.
Two statistical parsing models applied to
the Chinese treebank. In Proceedings of the
2nd Chinese Language Processing Workshop,
pages 1?6, Hong Kong.
Black, Ezra, Frederick Jelinek, John D.
Lafferty, David M. Magerman, Robert L.
Mercer, and Salim Roukos. 1992. Towards
history-based grammars: Using richer
models for probabilistic parsing. In
Proceedings of the 5th DARPA Speech and
Natural Language Workshop, pages 31?37,
New York, NY.
386
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
Bozs?ahin, Cem. 2002. The combinatory
morphemic lexicon. Computational
Linguistics, 28(2):145?186.
Buchholz, Sabine and Erwin Marsi.
2006. CONLL-X shared task on
multilingual dependency parsing. In
Proceedings of CONLL-X, pages 149?164,
New York, NY.
C?ak?c?, Ruket and Jason Baldridge. 2006.
Projective and non-projective Turkish
parsing. In Proceedings of the 5th
International Treebanks and Linguistic
Theories Conference, pages 43?54, Prague.
Canisius, Sander, Toine Bogers, Antal
van den Bosch, Jeroen Geertzen, and Erik
Tjong Kim Sang. 2006. Dependency
parsing by inference over high-recall
dependency predictions. In Proceedings of
CONLL-X, pages 176?180, New York, NY.
Carreras, Xavier, Mihai Surdeanu, and Lluis
Marquez. 2006. Projective dependency
parsing with perceptron. In Proceedings of
CONLL-X, pages 181?185, New York, NY.
Chang, Chih-Chung and Chih-Jen Lin, 2001.
LIBSVM: A Library for Support Vector
Machines. Software available at
www.csie.ntu.edu.tw/?cjlin/libsvm.
Chang, Ming-Wei, Quang Do, and Dan Roth.
2006. A pipeline model for bottom-up
dependency parsing. In Proceedings of
CONLL-X, pages 186?190, New York, NY.
Cheng, Yuchang, Masayuki Asahara, and
Yuji Matsumoto. 2006. Multi-lingual
dependency parsing at NAIST. In
Proceedings of CONLL-X, pages 191?195,
New York, NY.
Chung, Hoojung and Hae-Chang Rim. 2004.
Unlexicalized dependency parser for
variable word order languages based on
local contextual pattern. In Proceedings of
the 5th International Conference on Intelligent
Text Processing and Computational
Linguistics, pages 109?120, Seoul.
Collins, Michael. 1996. A new statistical
parser based on bigram lexical
dependencies. In Proceedings of ACL?96,
pages 184?191, Santa Cruz, CA.
Collins, Michael. 1997. Three generative,
lexicalised models for statistical parsing.
In Proceedings of ACL?97, pages 16?23,
Madrid.
Collins, Michael. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania,
Philadelphia.
Collins, Michael, Jan Hajic, Lance Ramshaw,
and Christoph Tillmann. 1999. A statistical
parser for Czech. In Proceedings of ACL?99,
pages 505?518, College Park, MD.
Corazza, Anna, Alberto Lavelli, Giorgio
Satta, and Roberto Zanoli. 2004. Analyzing
an Italian treebank with state-of-the-art
statistical parsers. In Proceedings of the 3rd
Workshop on Treebanks and Linguistic
Theories, pages 39?50, Tu?bingen.
Corston-Oliver, Simon and Anthony Aue.
2006. Dependency parsing with reference
to Slovene, Spanish and Swedish. In
Proceedings of CONLL-X, pages 196?200,
New York, NY.
Daelemans, Walter and Antal Vanden
Bosch. 2005.Memory-Based Language
Processing. Cambridge University Press,
Cambridge.
Dreyer, Markus, David A. Smith, and
Noah A. Smith. 2006. Vine parsing and
minimum risk reranking for speed and
precision. In Proceedings of CONLL-X,
pages 201?205, New York, NY.
Dubey, Amit and Frank Keller. 2003.
Probabilistic parsing for German using
sister-head dependencies. In Proceedings
of ACL?03, pages 96?103, Sapporo.
Eisner, Jason. 1996. Three new probabilistic
models for dependency parsing: An
exploration. In Proceedings of the 16th
International Conference on Computational
Linguistics, pages 340?345, Copenhagen.
Erguvanl?, Eser Emine. 1979. The Function
of Word Order in Turkish Grammar.
Ph.D. thesis, UCLA.
Eryig?it, Gu?ls?en. 2006. Tu?rkc?enin Bag?l?l?k
Ayr?s?t?rmas? (Dependency Parsing of Turkish).
Ph.D. thesis, Istanbul Technical University.
Eryig?it, Gu?ls?en, Es?ref Adal?, and Kemal
Oflazer. 2006. Tu?rkc?e cu?mlelerin kural
tabanl? bag?l?l?k analizi [Rule-based
dependency parsing of Turkish sentences].
In Proceedings of the 15th Turkish Symposium
on Artificial Intelligence and Neural
Networks, pages 17?24, Mug?la.
Eryig?it, Gu?ls?en, Joakim Nivre, and Kemal
Oflazer. 2006. The incremental use of
morphological information and
lexicalization in data-driven dependency
parsing. In Computer Processing of Oriental
Languages, Beyond the Orient: The Research
Challenges Ahead, pages 498?507,
Singapore.
Eryig?it, Gu?ls?en and Kemal Oflazer. 2006.
Statistical dependency parsing of Turkish.
In Proceedings of EACL?06, pages 89?96,
Trento.
Gildea, Daniel. 2001. Corpus variation and
parser performance. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing, pages 167?202,
Pittsburgh, PA.
387
Computational Linguistics Volume 34, Number 3
Hajic?, Jan, Eva Hajic?ova?, Petr Pajas, Jarmila
Panevova?, Petr Sgall, and Barbora Hladka?.
2001. Prague dependency treebank 1.0
(final production label). CDROM CAT:
LDC2001T10., ISBN 1-58563-212-0.
Hakkani-Tu?r, Dilek, Kemal Oflazer, and
Go?khan Tu?r. 2002. Statistical
morphological disambiguation for
agglutinative languages. Journal of
Computers and Humanities, 36(4):381?410.
Hoffman, Beryl. 1994. Generating context
appropriate word orders in Turkish. In
Proceedings of the Seventh International
Workshop on Natural Language Generation,
pages 117?126, Kennebunkport, ME.
Johansson, Richard and Pierre Nugues. 2006.
Investigating multilingual dependency
parsing. In Proceedings of CONLL-X,
pages 206?210, New York, NY.
Klein, Dan and Christopher D. Manning.
2003. Accurate unlexicalized parsing. In
Proceedings of ACL?03, pages 423?430,
Sapporo.
Kromann, Matthias T. 2003. The Danish
dependency treebank and the underlying
linguistic theory. In Proceedings of the 2nd
Workshop on Treebanks and Linguistic
Theories, pages 217?220, Va?xjo?.
Kudo, Taku and Yuji Matsumoto. 2002.
Japanese dependency analysis using
cascaded chunking. In Proceedings of the
Conference on Computational Natural
Language Learning, pages 63?69, Taipei.
Levy, Roger and Christopher Manning. 2003.
Is it harder to parse Chinese, or the
Chinese treebank? In Proceedings of ACL?03,
pages 439?446, Sapporo.
Liu, Ting, Jinshan Ma, Huijia Zhu, and
Sheng Li. 2006. Dependency parsing based
on dynamic local optimization. In
Proceedings of CONLL-X, pages 211?215,
New York, NY.
Magerman, David M. 1995. Statistical
decision-tree models for parsing. In
Proceedings of ACL?95, pages 276?283,
Cambridge, MA.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: The
Penn treebank. Computational Linguistics,
19(2):313?330.
McDonald, Ryan, Kevin Lerman, and
Fernando Pereira. 2006. Multilingual
dependency analysis with a two-stage
discriminative parser. In Proceedings of
CONLL-X, pages 216?220, New York, NY.
Nivre, Joakim. 2003. An efficient algorithm
for projective dependency parsing. In
Proceedings of the 8th International Workshop
on Parsing Technologies, pages 149?160,
Nancy.
Nivre, Joakim. 2004. Incrementality in
deterministic dependency parsing. In
Proceedings of the Workshop on Incremental
Parsing: Bringing Engineering and Cognition
Together, pages 50?57, Barcelona.
Nivre, Joakim. 2006. Inductive Dependency
Parsing. Springer, Dordrecht.
Nivre, Joakim, Johan Hall, and Jens Nilsson.
2004. Memory-based dependency parsing.
In Proceedings of the Conference on
Computational Natural Language Learning,
pages 49?56, Boston, MA.
Nivre, Joakim, Johan Hall, Jens Nilsson,
Atanas Chanev, Gu?ls?en Eryig?it, Sandra
Ku?bler, Stetoslav Marinov, and Erwin
Marsi. 2007. Maltparser: A
language-independent system for
data-driven dependency parsing. Natural
Language Engineering Journal, 13(2):95?135.
Nivre, Joakim, Johan Hall, Jens Nilsson,
Gu?ls?en Eryig?it, and Stetoslav Marinov.
2006. Labeled pseudo-projective
dependency parsing with support vector
machines. In Proceedings of CONLL-X,
pages 221?225, New York, NY.
Nivre, Joakim and Jens Nilsson. 2005.
Pseudo-projective dependency parsing. In
Proceedings of ACL?05, pages 99?106, Ann
Arbor, MI.
Nivre, Joakim, Jens Nilsson, and Johan Hall.
2006. Talbanken05: A Swedish treebank
with phrase structure and dependency
annotation. In Proceedings of LREC,
pages 1392?1395, Genoa.
Oflazer, Kemal. 1994. Two-level description
of Turkish morphology. Literary and
Linguistic Computing, 9(2):137?148.
Oflazer, Kemal. 2003. Dependency parsing
with an extended finite-state approach.
Computational Linguistics, 29(4):515?544.
Oflazer, Kemal, Bilge Say, Dilek Z.
Hakkani-Tu?r, and Go?khan Tu?r. 2003.
Building a Turkish treebank. In A. Abeille?,
editor, Treebanks: Building and Using Parsed
Corpora. Kluwer, London, pages 261?277.
Ratnaparkhi, Adwait. 1997. A linear
observed time statistical parser based on
maximum entropy models. In Proceedings
of the Conference on Empirical Methods in
Natural Language Processing, pages 1?10,
Providence, RI.
Riedel, Sebastian, Ruket C?ak?c?, and
Ivan Meza-Ruiz. 2006. Multi-lingual
dependency parsing with incremental
integer linear programming. In
Proceedings of CONLL-X, pages 226?230,
New York, NY.
388
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
Sagae, Kenji and Alon Lavie. 2005. A
classifier-based parser with linear run-time
complexity. In Proceedings of the 9th
International Workshop on Parsing
Technologies, pages 125?132, Vancouver.
Schiehlen, Michael and Kristina Spranger.
2006. Language independent probabilistic
context-free parsing bolstered by machine
learning. In Proceedings of CONLL-X,
pages 231?235, New York, NY.
Sekine, Satoshi, Kiyotaka Uchimoto, and
Hitoshi Isahara. 2000. Backward beam
search algorithm for dependency analysis
of Japanese. In Proceedings of the 17th
International Conference on Computational
Linguistics, pages 754?760, Saarbru?cken.
Shimizu, Nobuyuki. 2006. Maximum
spanning tree algorithm for non-projective
labeled dependency parsing. In
Proceedings of CONLL-X, pages 236?240,
New York, NY.
Simov, Kiril, Gergana Popova, and Petya
Osenova. 2002. HPSG-based syntactic
treebank of Bulgarian (BulTreeBank). In
Andrew Wilson, Paul Rayson, and Tony
McEnery, editors, A Rainbow of Corpora:
Corpus Linguistics and the Languages of
the World. Lincom-Europa, Munich,
pages 135?142.
Vapnik, Vladimir N. 1995. The Nature of
Statistical Learning Theory. Springer, New
York, NY.
Wu, Yu-Chieh, Yue-Shi Lee, and Jie-Chi
Yang. 2006. The exploration of
deterministic and efficient dependency
parsing. In Proceedings of CONLL-X,
pages 241?245, New York, NY.
Yamada, Hiroyasu and Yuji Matsumoto.
2003. Statistical dependency analysis with
support vector machines. In Proceedings of
the 8th International Workshop on Parsing
Technologies, pages 195?206, Nancy.
Yu?ret, Deniz. 2006. Dependency parsing as a
classification problem. In Proceedings of
CONLL-X, pages 246?250, New York, NY.
Yu?ret, Deniz and Ferhan Tu?re. 2006.
Learning morphological disambiguation
rules for Turkish. In Proceedings of
HLT/NAACL?06, pages 328?334,
New York, NY.
389

This article has been cited by:
Algorithms for Deterministic Incremental
Dependency Parsing
Joakim Nivre?,??
Va?xjo? University, Uppsala University
Parsing algorithms that process the input from left to right and construct a single derivation
have often been considered inadequate for natural language parsing because of the massive
ambiguity typically found in natural language grammars. Nevertheless, it has been shown
that such algorithms, combined with treebank-induced classifiers, can be used to build highly
accurate disambiguating parsers, in particular for dependency-based syntactic representations.
In this article, we first present a general framework for describing and analyzing algorithms
for deterministic incremental dependency parsing, formalized as transition systems. We then
describe and analyze two families of such algorithms: stack-based and list-based algorithms.
In the former family, which is restricted to projective dependency structures, we describe an
arc-eager and an arc-standard variant; in the latter family, we present a projective and a non-
projective variant. For each of the four algorithms, we give proofs of correctness and complexity.
In addition, we perform an experimental evaluation of all algorithms in combination with
SVM classifiers for predicting the next parsing action, using data from thirteen languages. We
show that all four algorithms give competitive accuracy, although the non-projective list-based
algorithm generally outperforms the projective algorithms for languages with a non-negligible
proportion of non-projective constructions. However, the projective algorithms often produce
comparable results when combined with the technique known as pseudo-projective parsing. The
linear time complexity of the stack-based algorithms gives them an advantage with respect to
efficiency both in learning and in parsing, but the projective list-based algorithm turns out to
be equally efficient in practice. Moreover, when the projective algorithms are used to implement
pseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning)
than the non-projective list-based algorithm. Although most of the algorithms have been partially
described in the literature before, this is the first comprehensive analysis and evaluation of the
algorithms within a unified framework.
1. Introduction
Because parsers for natural language have to cope with a high degree of ambigu-
ity and nondeterminism, they are typically based on different techniques than the
ones used for parsing well-defined formal languages?for example, in compilers for
? School of Mathematics and Systems Engineering, Va?xjo? University, 35195 Va?xjo?, Sweden.
E-mail: joakim.nivre@vxu.se.
?? Department of Linguistics and Philology, Uppsala University, Box 635, 75126 Uppsala, Sweden.
E-mail: joakim.nivre@lingfil.uu.se.
Submission received: 29 May 2007; revised submission received 22 September 2007; accepted for publication:
3 November 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 4
programming languages. Thus, the mainstream approach to natural language parsing
uses algorithms that efficiently derive a potentially very large set of analyses in parallel,
typically making use of dynamic programming and well-formed substring tables or
charts. When disambiguation is required, this approach can be coupled with a statistical
model for parse selection that ranks competing analyses with respect to plausibility.
Although it is often necessary, for efficiency reasons, to prune the search space prior
to the ranking of complete analyses, this type of parser always has to handle multiple
analyses.
By contrast, parsers for formal languages are usually based on deterministic parsing
techniques, which are maximally efficient in that they only derive one analysis. This
is possible because the formal language can be defined by a non-ambiguous formal
grammar that assigns a single canonical derivation to each string in the language, a
property that cannot be maintained for any realistically sized natural language gram-
mar. Consequently, these deterministic parsing techniques have been much less popular
for natural language parsing, except as a way of modeling human sentence process-
ing, which appears to be at least partly deterministic in nature (Marcus 1980; Shieber
1983).
More recently, however, it has been shown that accurate syntactic disambiguation
for natural language can be achieved using a pseudo-deterministic approach, where
treebank-induced classifiers are used to predict the optimal next derivation step when
faced with a nondeterministic choice between several possible actions. Compared to
the more traditional methods for natural language parsing, this can be seen as a severe
form of pruning, where parse selection is performed incrementally so that only a single
analysis is derived by the parser. This has the advantage of making the parsing process
very simple and efficient but the potential disadvantage that overall accuracy suffers
because of the early commitment enforced by the greedy search strategy. Somewhat
surprisingly, though, research has shown that, with the right choice of parsing algorithm
and classifier, this type of parser can achieve state-of-the-art accuracy, especially when
used with dependency-based syntactic representations.
Classifier-based dependency parsing was pioneered by Kudo and Matsumoto
(2002) for unlabeled dependency parsing of Japanese with head-final dependencies
only. The algorithm was generalized to allow both head-final and head-initial depen-
dencies by Yamada and Matsumoto (2003), who reported very good parsing accuracy
for English, using dependency structures extracted from the Penn Treebank for training
and testing. The approach was extended to labeled dependency parsing by Nivre, Hall,
and Nilsson (2004) (for Swedish) and Nivre and Scholz (2004) (for English), using a
different parsing algorithm first presented in Nivre (2003). At a recent evaluation of
data-driven systems for dependency parsing with data from 13 different languages
(Buchholz and Marsi 2006), the deterministic classifier-based parser of Nivre et al (2006)
reached top performance together with the system of McDonald, Lerman, and Pereira
(2006), which is based on a global discriminative model with online learning. These
results indicate that, at least for dependency parsing, deterministic parsing is possible
without a drastic loss in accuracy. The deterministic classifier-based approach has also
been applied to phrase structure parsing (Kalt 2004; Sagae and Lavie 2005), although the
accuracy for this type of representation remains a bit below the state of the art. In this
setting, more competitive results have been achieved using probabilistic classifiers and
beam search, rather than strictly deterministic search, as in the work by Ratnaparkhi
(1997, 1999) and Sagae and Lavie (2006).
A deterministic classifier-based parser consists of three essential components: a
parsing algorithm, which defines the derivation of a syntactic analysis as a sequence
514
Nivre Deterministic Incremental Dependency Parsing
of elementary parsing actions; a feature model, which defines a feature vector represen-
tation of the parser state at any given time; and a classifier, which maps parser states,
as represented by the feature model, to parsing actions. Although different types of
parsing algorithms, feature models, and classifiers have been used for deterministic
dependency parsing, there are very few studies that compare the impact of different
components. The notable exceptions are Cheng, Asahara, and Matsumoto (2005), who
compare two different algorithms and two types of classifier for parsing Chinese, and
Hall, Nivre, and Nilsson (2006), who compare two types of classifiers and several types
of feature models for parsing Chinese, English, and Swedish.
In this article, we focus on parsing algorithms. More precisely, we describe two
families of algorithms that can be used for deterministic dependency parsing, supported
by classifiers for predicting the next parsing action. The first family uses a stack to store
partially processed tokens and is restricted to the derivation of projective dependency
structures. The algorithms of Kudo and Matsumoto (2002), Yamada and Matsumoto
(2003), and Nivre (2003, 2006b) all belong to this family. The second family, represented
by the algorithms described by Covington (2001) and recently explored for classifier-
based parsing in Nivre (2007), instead uses open lists for partially processed tokens,
which allows arbitrary dependency structures to be processed (in particular, structures
with non-projective dependencies). We provide a detailed analysis of four different
algorithms, two from each family, and give proofs of correctness and complexity for
each algorithm. In addition, we perform an experimental evaluation of accuracy and
efficiency for the four algorithms, combined with state-of-the-art classifiers, using data
from 13 different languages. Although variants of these algorithms have been partially
described in the literature before, this is the first comprehensive analysis and evaluation
of the algorithms within a unified framework.
The remainder of the article is structured as follows. Section 2 defines the task of
dependency parsing and Section 3 presents a formal framework for the characterization
of deterministic incremental parsing algorithms. Sections 4 and 5 contain the formal
analysis of four different algorithms, defined within the formal framework, with proofs
of correctness and complexity. Section 6 presents the experimental evaluation; Section 7
reports on related work; and Section 8 contains our main conclusions.
2. Dependency Parsing
Dependency-based syntactic theories are based on the idea that syntactic structure can
be analyzed in terms of binary, asymmetric dependency relations holding between the
words of a sentence. This basic conception of syntactic structure underlies a variety of
different linguistic theories, such as Structural Syntax (Tesnie`re 1959), Functional Gener-
ative Description (Sgall, Hajic?ova?, and Panevova? 1986), Meaning-Text Theory (Mel?c?uk
1988), and Word Grammar (Hudson 1990). In computational linguistics, dependency-
based syntactic representations have in recent years been used primarily in data-driven
models, which learn to produce dependency structures for sentences solely from an
annotated corpus, as in the work of Eisner (1996), Yamada and Matsumoto (2003), Nivre,
Hall, and Nilsson (2004), and McDonald, Crammer, and Pereira (2005), among others.
One potential advantage of such models is that they are easily ported to any domain or
language in which annotated resources exist.
In this kind of framework the syntactic structure of a sentence is modeled by a depen-
dency graph, which represents each word and its syntactic dependents through labeled
directed arcs. This is exemplified in Figure 1, for a Czech sentence taken from the Prague
515
Computational Linguistics Volume 34, Number 4
(?Only one of them concerns quality.?)
ROOT0 Z1
(Out-of
? 
?
AuxP
nich2
them
? 
?
Atr
je3
is
? 
?
Pred
jen4
only
? 
?
AuxZ
jedna5
one-FEM-SG
? 
?
Sb
na6
to
? 
?
AuxP
kvalitu7
quality
?
? 
Adv
.8
.)
? 
?
AuxK
Figure 1
Dependency graph for a Czech sentence from the Prague Dependency Treebank.
ROOT0 Economic1
? 
?
NMOD
news2
? 
?
SBJ
had3
? 
?
ROOT
little4
? 
?
NMOD
effect5
? 
?
OBJ
on6
? 
?
NMOD
financial7
? 
?
NMOD
markets8
? 
?
PMOD
.9
?
? 
P
Figure 2
Dependency graph for an English sentence from the Penn Treebank.
Dependency Treebank (Hajic? et al 2001; Bo?hmova? et al 2003), and in Figure 2, for an
English sentence taken from the Penn Treebank (Marcus, Santorini, and Marcinkiewicz
1993; Marcus et al 1994).1 An artificial word ROOT has been inserted at the beginning
of each sentence, serving as the unique root of the graph. This is a standard device that
simplifies both theoretical definitions and computational implementations.
Definition 1
Given a set L = {l1, . . . , l|L|} of dependency labels, a dependency graph for a sentence
x = (w0,w1, . . . ,wn) is a labeled directed graph G = (V,A), where
1. V = {0, 1, . . . ,n} is a set of nodes,
2. A ? V ? L? V is a set of labeled directed arcs.
The set V of nodes (or vertices) is the set of non-negative integers up to and including
n, each corresponding to the linear position of a word in the sentence (including ROOT).
The set A of arcs (or directed edges) is a set of ordered triples (i, l, j), where i and j
are nodes and l is a dependency label. Because arcs are used to represent dependency
relations, we will say that i is the head and l is the dependency type of j. Conversely,
we say that j is a dependent of i.
1 In the latter case, the dependency graph has been derived automatically from the constituency-based
annotation in the treebank, using the Penn2Malt program, available at http://w3.msi.vxu.se/users/
nivre/research/Penn2Malt.html.
516
Nivre Deterministic Incremental Dependency Parsing
Definition 2
A dependency graph G = (V,A) is well-formed if and only if:
1. The node 0 is a root, that is, there is no node i and label l such that
(i, l, 0) ? A.
2. Every node has at most one head and one label, that is, if (i, l, j) ? A then
there is no node i? and label l? such that (i?, l?, j) ? A and i = i? or l = l?.
3. The graph G is acyclic, that is, there is no (non-empty) subset of arcs
{(i0, l1, i1), (i1, l2, i2), . . . , (ik?1, lk, ik)} ? A such that i0 = ik.
We will refer to conditions 1?3 as ROOT, SINGLE-HEAD, and ACYCLICITY, respectively.
Any dependency graph satisfying these conditions is a dependency forest; if it is also
connected, it is a dependency tree, that is, a directed tree rooted at the node 0. It is worth
noting that any dependency forest can be turned into a dependency tree by adding arcs
from the node 0 to all other roots.
Definition 3
A dependency graph G = (V,A) is projective if and only if, for every arc (i, l, j) ?
A and node k ? V, if i < k < j or j < k < i then there is a subset of arcs {(i, l1, i1),
(i1, l2, i2), . . . (ik?1, lk, ik)} ? A such that ik = k.
In a projective dependency graph, every node has a continuous projection, where the
projection of a node i is the set of nodes reachable from i in the reflexive and transitive
closure of the arc relation. This corresponds to the ban on discontinuous constituents
in orthodox phrase structure representations. We call this condition PROJECTIVITY.
When discussing PROJECTIVITY, we will often use the notation i?? j to mean that j
is reachable from i in the reflexive and transitive closure of the arc relation.
Example 1
For the graphs depicted in Figures 1 and 2, we have:
Figure 1: G1 = (V1,A1)
V1 = {0, 1, 2, 3, 4, 5, 6, 7, 8}
A1 = {(0, Pred, 3), (0,AuxK, 8), (1,Atr, 2), (3, Sb, 5), (3, AuxP, 6),
(5,AuxP, 1), (5,AuxZ, 4), (6,Adv, 7)}
Figure 2: G2 = (V2,A2)
V2 = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}
A2 = {(0, ROOT, 3), (2, NMOD, 1), (3, SBJ, 2), (3, OBJ, 5), (3, P, 9),
(5, NMOD, 4), (5, NMOD, 6), (6, PMOD, 8), (8, NMOD, 7)}
Both G1 and G2 are well-formed dependency forests (dependency trees, to be specific),
but only G2 is projective. In G1, the arc (5,AuxP, 1) spans node 3, which is not reachable
from node 5 by following dependency arcs.
3. Deterministic Incremental Dependency Parsing
In this section, we introduce a formal framework for the specification of deterministic
dependency parsing algorithms in terms of two components: a transition system, which
517
Computational Linguistics Volume 34, Number 4
is nondeterministic in the general case, and an oracle, which always picks a single
transition out of every parser configuration. The use of transition systems to study
computation is a standard technique in theoretical computer science, which is here
combined with the notion of oracles in order to characterize parsing algorithms with
deterministic search. In data-driven dependency parsing, oracles normally take the
form of classifiers, trained on treebank data, but they can also be defined in terms of
grammars and heuristic disambiguation rules (Nivre 2003).
The main reason for introducing this framework is to allow us to characterize
algorithms that have previously been described in different traditions and to compare
their formal properties within a single unified framework. In particular, whereas this
type of framework has previously been used to characterize algorithms in the stack-
based family (Nivre 2003, 2006b; Attardi 2006), it is here being used also for the list-
based algorithms first discussed by Covington (2001).
Definition 4
A transition system for dependency parsing is a quadruple S = (C,T, cs,Ct), where
1. C is a set of configurations, each of which contains a buffer ? of
(remaining) nodes and a set A of dependency arcs,
2. T is a set of transitions, each of which is a (partial) function t : C? C,
3. cs is an initialization function, mapping a sentence x = (w0,w1, . . . ,wn) to a
configuration with ? = [1, . . . ,n],
4. Ct ? C is a set of terminal configurations.
A configuration is required to contain at least a buffer ?, initially containing the nodes
[1, . . . ,n] corresponding to the real words of a sentence x = (w0,w1, . . . ,wn), and a set
A of dependency arcs, defined on the nodes in V = {0, 1, . . . ,n}, given some set of
dependency labels L. The specific transition systems defined in Sections 4 and 5 will
extend this basic notion of configuration with different data structures, such as stacks
and lists. We use the notation ?c and Ac to refer to the value of ? and A, respectively, in
a configuration c; we also use |?| to refer to the length of ? (i.e., the number of nodes in
the buffer) and we use [ ] to denote an empty buffer.
Definition 5
Let S = (C,T, cs,Ct) be a transition system. A transition sequence for a sentence x =
(w0,w1, . . . ,wn) in S is a sequence C0,m = (c0, c1, . . . , cm) of configurations, such that
1. c0 = cs(x),
2. cm ? Ct,
3. for every i (1 ? i ? m), ci = t(ci?1) for some t ? T.
The parse assigned to x by C0,m is the dependency graph Gcm = ({0, 1, . . . ,n},Acm ),
where Acm is the set of dependency arcs in cm.
Starting from the initial configuration for the sentence to be parsed, transitions will
manipulate ? and A (and other available data structures) until a terminal configuration
is reached. Because the node set V is given by the input sentence itself, the set Acm of
dependency arcs in the terminal configuration will determine the output dependency
graph Gcm = (V,Acm ).
518
Nivre Deterministic Incremental Dependency Parsing
Definition 6
A transition system S = (C,T, cs,Ct) is incremental if and only if, for every configuration
c ? C and transition t ? T, it holds that:
1. if ?c = [ ] then c ? Ct,
2. |?c| ? |?t(c)|,
3. if a ? Ac then a ? At(c).
The first two conditions state that the buffer ? never grows in size and that parsing
terminates as soon as it becomes empty; the third condition states that arcs added
to A can never be removed. Note that this is only one of several possible notions of
incrementality in parsing. A weaker notion would be to only require that the set of arcs
is built monotonically (the third condition); a stronger notion would be to require also
that nodes in ? are processed strictly left to right.
Definition 7
Let S = (C,T, cs,Ct) be a transition system for dependency parsing.
1. S is sound for a class G of dependency graphs if and only if, for every
sentence x and every transition sequence C0,m for x in S, the parse Gcm ? G.
2. S is complete for a class G of dependency graphs if and only if, for every
sentence x and every dependency graph Gx for x in G, there is a transition
sequence C0,m for x in S such that Gcm = Gx.
3. S is correct for a class G of dependency graphs if and only if it is sound
and complete for G.
The notions of soundness and completeness, as defined here, can be seen as correspond-
ing to the notions of soundness and completeness for grammar parsing algorithms,
according to which an algorithm is sound if it only derives parses licensed by the
grammar and complete if it derives all such parses (Shieber, Schabes, and Pereira 1995).
Depending on the nature of a transition system S, there may not be a transition
sequence for every sentence, or there may be more than one such sequence. The
systems defined in Sections 4 and 5 will all be such that, for any input sentence
x = (w0,w1, . . . ,wn), there is always at least one transition sequence for x (and usually
more than one).
Definition 8
An oracle for a transition system S = (C,T, cs,Ct) is a function o : C? T.
Given a transition system S = (C,T, cs,Ct) and an oracle o, deterministic parsing can be
achieved by the following simple algorithm:
PARSE(x = (w0,w1, . . . ,wn))
1 c? cs(x)
2 while c ? Ct
3 c? [o(c)](c)
4 return Gc
519
Computational Linguistics Volume 34, Number 4
It is easy to see that, provided that there is at least one transition sequence in S for
every sentence, such a parser constructs exactly one transition sequence C0,m for a
sentence x and returns the parse defined by the terminal configuration cm, that is,
Gcm = ({0, 1, . . . ,n},Acm ). The reason for separating the oracle o, which maps a configu-
ration c to a transition t, from the transition t itself, which maps a configuration c to a
new configuration c?, is to have a clear separation between the abstract machine defined
by the transition system, which determines formal properties such as correctness and
complexity, and the search mechanism used when executing the machine.
In the experimental evaluation in Section 6, we will use the standard technique
of approximating oracles with classifiers trained on treebank data. However, in the
formal characterization of different parsing algorithms in Sections 4 and 5, we will
concentrate on properties of the underlying transition systems. In particular, assuming
that both o(c) and t(c) can be performed in constant time (for every o, t and c), which
is reasonable in most cases, the worst-case time complexity of a deterministic parser
based on a transition system S is given by an upper bound on the length of transition
sequences in S. And the space complexity is given by an upper bound on the size of
a configuration c ? C, because only one configuration needs to be stored at any given
time in a deterministic parser.
4. Stack-Based Algorithms
The stack-based algorithms make use of a stack to store partially processed tokens, that
is, tokens that have been removed from the input buffer but which are still considered
as potential candidates for dependency links, either as heads or as dependents. A parser
configuration is therefore defined as a triple, consisting of a stack, an input buffer, and
a set of dependency arcs.
Definition 9
A stack-based configuration for a sentence x = (w0,w1, . . . ,wn) is a triple c = (?,?,A),
where
1. ? is a stack of tokens i ? k (for some k ? n),
2. ? is a buffer of tokens j > k,
3. A is a set of dependency arcs such that G = ({0, 1, . . . ,n},A) is a
dependency graph for x.
Both the stack and the buffer will be represented as lists, although the stack will have its
head (or top) to the right for reasons of perspicuity. Thus, ?|i represents a stack with top
i and tail ?, and j|? represents a buffer with head j and tail ?.2 We use square brackets
for enumerated lists, for example, [1, 2, . . . ,n], with [ ] for the empty list as a special case.
Definition 10
A stack-based transition system is a quadruple S = (C,T, cs,Ct), where
1. C is the set of all stack-based configurations,
2. cs(x = (w0,w1, . . .wn)) = ([0], [1, . . . ,n], ?),
2 The operator | is taken to be left-associative for the stack and right-associative for the buffer.
520
Nivre Deterministic Incremental Dependency Parsing
Transitions
LEFT-ARCl (?|i, j|?,A) ? (?, j|?,A?{(j, l, i)})
RIGHT-ARCsl (?|i, j|?,A) ? (?, i|?,A?{(i, l, j)})
SHIFT (?, i|?,A) ? (?|i,?,A)
Preconditions
LEFT-ARCl ?[i = 0]
??k?l?[(k, l?, i) ? A]
RIGHT-ARCsl ??k?l
?[(k, l?, j) ? A]
Figure 3
Transitions for the arc-standard, stack-based parsing algorithm.
3. T is a set of transitions, each of which is a function t : C? C,
4. Ct = {c ? C|c = (?, [ ],A)}.
A stack-based parse of a sentence x = (w0,w1, . . . ,wn) starts with the artificial root node
0 on the stack ?, all the nodes corresponding to real words in the buffer ?, and an
empty set A of dependency arcs; it ends as soon as the buffer ? is empty. The transitions
used by stack-based parsers are essentially composed of two types of actions: adding
(labeled) arcs to A and manipulating the stack ? and input buffer ?. By combining such
actions in different ways, we can construct transition systems that implement different
parsing strategies. We will now define two such systems, which we call arc-standard
and arc-eager, respectively, adopting the terminology of Abney and Johnson (1991).
4.1 Arc-Standard Parsing
The transition set T for the arc-standard, stack-based parser is defined in Figure 3 and
contains three types of transitions:
1. Transitions LEFT-ARCl (for any dependency label l) add a dependency arc
(j, l, i) to A, where i is the node on top of the stack ? and j is the first node in
the buffer ?. In addition, they pop the stack ?. They have as a precondition
that the token i is not the artificial root node 0 and does not already have
a head.
2. Transitions RIGHT-ARCsl (for any dependency label l) add a dependency
arc (i, l, j) to A, where i is the node on top of the stack ? and j is the first
node in the buffer ?. In addition, they pop the stack ? and replace j by i
at the head of ?. They have as a precondition that the token j does not
already have a head.3
3. The transition SHIFT removes the first node i in the buffer ? and pushes
it on top of the stack ?.
3 The superscript s is used to distinguish these transitions from the non-equivalent RIGHT-ARCel transitions
in the arc-eager system.
521
Computational Linguistics Volume 34, Number 4
Transition Configuration
( [0], [1, . . . , 9], ? )
SHIFT =? ( [0, 1], [2, . . . , 9], ? )
LEFT-ARCNMOD =? ( [0], [2, . . . , 9], A1 = {(2, NMOD, 1)} )
SHIFT =? ( [0, 2], [3, . . . , 9], A1 )
LEFT-ARCSBJ =? ( [0], [3, . . . , 9], A2 = A1?{(3, SBJ, 2)} )
SHIFT =? ( [0, 3], [4, . . . , 9], A2 )
SHIFT =? ( [0, 3, 4], [5, . . . , 9], A2 )
LEFT-ARCNMOD =? ( [0, 3], [5, . . . , 9], A3 = A2?{(5, NMOD, 4)} )
SHIFT =? ( [0, 3, 5], [6, . . . , 9], A3 )
SHIFT =? ( [0, . . . 6], [7, 8, 9], A3 )
SHIFT =? ( [0, . . . , 7], [8, 9], A3 )
LEFT-ARCNMOD =? ( [0, . . . 6], [8, 9], A4 = A3?{(8, NMOD, 7)} )
RIGHT-ARCsPMOD =? ( [0, 3, 5], [6, 9], A5 = A4?{(6, PMOD, 8)} )
RIGHT-ARCsNMOD =? ( [0, 3], [5, 9], A6 = A5?{(5, NMOD, 6)} )
RIGHT-ARCsOBJ =? ( [0], [3, 9], A7 = A6?{(3, OBJ, 5)} )
SHIFT =? ( [0, 3], [9], A7 )
RIGHT-ARCsP =? ( [0], [3], A8 = A7?{(3, P, 9)} )
RIGHT-ARCsROOT =? ( [ ], [0], A9 = A8?{(0, ROOT, 3)} )
SHIFT =? ( [0], [ ], A9 )
Figure 4
Arc-standard transition sequence for the English sentence in Figure 2.
The arc-standard parser is the closest correspondent to the familiar shift-reduce parser
for context-free grammars (Aho, Sethi, and Ullman 1986). The LEFT-ARCl and RIGHT-
ARCsl transitions correspond to reduce actions, replacing a head-dependent structure
with its head, whereas the SHIFT transition is exactly the same as the shift action. One
peculiarity of the transitions, as defined here, is that the ?reduce? transitions apply to
one node on the stack and one node in the buffer, rather than two nodes on the stack.
The reason for this formulation is to facilitate comparison with the arc-eager parser
described in the next section and to simplify the definition of terminal configurations.
By way of example, Figure 4 shows the transition sequence needed to parse the English
sentence in Figure 2.
Theorem 1
The arc-standard, stack-based algorithm is correct for the class of projective dependency
forests.
Proof 1
To show the soundness of the algorithm, we show that the dependency graph defined
by the initial configuration, Gcs(x) = (Vx, ?), is a projective dependency forest, and that
every transition preserves this property. We consider each of the relevant conditions in
turn, keeping in mind that the only transitions that modify the graph are LEFT-ARCl
and RIGHT-ARCsl .
1. ROOT: The node 0 is a root in Gcs(x), and adding an arc of the form (i, l, 0) is
prevented by an explicit precondition of LEFT-ARCl.
522
Nivre Deterministic Incremental Dependency Parsing
2. SINGLE-HEAD: Every node i ? Vx has in-degree 0 in Gcs(x), and both
LEFT-ARCl and RIGHT-ARC
s
l have as a precondition that the dependent
of the new arc has in-degree 0.
3. ACYCLICITY: Gcs(x) is acyclic, and adding an arc (i, l, j) will create a cycle
only if there is a directed path from j to i. But this would require that a
previous transition had added an arc of the form (k, l?, i) (for some k, l?),
in which case i would no longer be in ? or ?.
4. PROJECTIVITY: Gcs(x) is projective, and adding an arc (i, l, j) will make the
graph non-projective only if there is a node k such that i < k < j or j < k < i
and neither i?? k nor j?? k. Let C0,m be a configuration sequence for
x = (w0,w1, . . . ,wn) and let ?(p, i, j) (for 0 < p < m, 0 ? i < j ? n) be the
claim that, for every k such that i < k < j, i?? k or j?? k in Gcp . To prove
that no arc can be non-projective, we need to prove that, if cp ? C0,m and
cp = (?|i, j|?,Acp ), then ?(p, i, j). (If cp = (?|i, j|?,Acp ) and ?(p, i, j),
then ?(p?, i, j) for all p? such that p < p?, since in cp every node k such that
i < k < jmust already have a head.) We prove this by induction over the
number ?(p) of transitions leading to cp from the first configuration
cp??(p) ? C0,m such that cp??(p) = (?|i,?,Acp??(p) ) (i.e., the first
configuration where i is on the top of the stack).
Basis: If ?(p) = 0, then i and j are adjacent and ?(p, i, j) holds
vacuously.
Inductive step: Assume that ?(p, i, j) holds if ?(p) ? q (for some
q > 0) and that ?(p) = q+ 1. Now consider the transition tp that
results in configuration cp. There are three cases:
Case 1: If tp = RIGHT-ARC
s
l (for some l), then there is a node k
such that j < k, ( j, l, k) ? Acp , and cp?1 = (?|i|j, k|?,Acp?
{( j, l, k)}). This entails that there is an earlier configuration
cp?r (2 ? r ? ?(p)) such that cp?r = (?|i, j|?,Acp?r ). Because
?(p? r) = ?(p)? r ? q, we can use the inductive hypothesis
to infer ?(p? r, i, j) and hence ?(p, i, j).
Case 2: If tp = LEFT-ARCl (for some l), then there is a node k
such that i < k < j, ( j, l, k) ? Acp , and cp?1 = (?|i|k, j|?,Acp?
{( j, l, k)}). Because ?(p? 1) ? q, we can use the inductive
hypothesis to infer ?(p? 1, k, j) and, from this, ?(p, k, j).
Moreover, because there has to be an earlier configuration
cp?r (r < ?(p)) such that cp?r = (?|i, k|?,Acp?r ) and
?(p? r) ? q, we can use the inductive hypothesis again
to infer ?(p? r, i, k) and ?(p, i, k). ?(p, i, k), ?(p, k, j) and
( j, l, k) ? Acp together entail ?(p, i, j).
Case 3: If the transition tp = SHIFT, then it must have been
preceded by a RIGHT-ARCsl transition (for some l), because
otherwise i and jwould be adjacent. This means that there
is a node k such that i < k < j, (i, l, k) ? Acp , and cp?2 =
(?|i, k|j|?,Acp ? {(i, l, k)}). Because ?(p? 2) ? q, we can
again use the inductive hypothesis to infer ?(p? 2, i, k)
and ?(p, i, k). Furthermore, it must be the case that either k
and j are adjacent or there is an earlier configuration cp?r
523
Computational Linguistics Volume 34, Number 4
(r < ?(p)) such that cp?r = (?|k, j|?,Acp?r ); in both cases it
follows that ?(p, k, j) (in the latter through the inductive
hypothesis via ?(p? r, k, j)). As before, ?(p, i, k), ?(p, k, j)
and (i, l, k) ? Acp together entail ?(p, i, j).
For completeness, we need to show that for any sentence x and projective dependency
forest Gx = (Vx,Ax) for x, there is a transition sequence C0,m such that Gcm = Gx. We
prove this by induction on the length |x| of x = (w0,w1, . . . ,wn).
Basis: If |x| = 1, then the only projective dependency forest for x is
G = ({0}, ?) and Gcm = Gx for C0,m = (cs(x)).
Inductive step: Assume that the claim holds if |x| ? p (for some p > 1) and
assume that |x| = p+ 1 and Gx = (Vx,Ax) (Vx = {0, 1, . . . , p}). Consider
the subgraph Gx? = (Vx ? {p},A?p), where A?p = Ax ? {(i, l, j)|i = p ? j =
p}, that is, the graph Gx? is exactly like Gx except that the node p and all
the arcs going into or out of this node are missing. It is obvious that,
if Gx is a projective dependency forest for the sentence x = (w0,w1, . . . ,wp),
then Gx? is a projective dependency forest for the sentence x
? = (w0,w1, . . . ,
wp?1), and that, because |x?| = p, there is a transition sequence C0,q such
that Gcq = Gx? (in virtue of the inductive hypothesis). The terminal
configuration of G0,q must have the form cq = (?cq , [ ],A
?p), where i ? ?cq
if and only if i is a root in Gx? (else iwould have been removed in a
LEFT-ARCl or RIGHT-ARC
s
l transition). It follows that, in Gx, i is either a
root or a dependent of p. In the latter case, any j such that j ? ?cq and i < j
must also be a dependent of p (else Gx would not be projective, given that
i and j are both roots in Gx? ). Moreover, if p has a head k in Gx, then k
must be the topmost node in ?cq that is not a dependent of p (anything else
would again be inconsistent with the assumption that Gx is projective).
Therefore, we can construct a transition sequence C0,m such that Gcm =
Gx, by starting in c0 = cs(x) and applying exactly the same q transitions
as in C0,q, followed by as many LEFT-ARCl transitions as there are left
dependents of p in Gx, followed by a RIGHT-ARC
s
l transition if and only
if p has a head in Gx, followed by a SHIFT transition (moving the head of
p back to the stack and emptying the buffer). 
Theorem 2
The worst-case time complexity of the arc-standard, stack-based algorithm is O(n),
where n is the length of the input sentence.
Proof 2
Assuming that the oracle and transition functions can be computed in some constant
time, the worst-case running time is bounded by the maximum number of transitions
in a transition sequence C0,m for a sentence x = (w0,w1, . . . ,wn). Since a SHIFT transition
decreases the length of the buffer ? by 1, no other transition increases the length of ?,
and any configuration where ? = [ ] is terminal, the number of SHIFT transitions in C0,m
is bounded by n. Moreover, since both LEFT-ARCl and RIGHT-ARC
s
l decrease the height
of the stack by 1, only SHIFT increases the height of the stack by 1, and the initial height
of the stack is 1, the combined number of instances of LEFT-ARCl and RIGHT-ARC
s
l in
C0,m is also bounded by n. Hence, the worst case time complexity is O(n). 
524
Nivre Deterministic Incremental Dependency Parsing
Remark 1
The assumption that the oracle function can be computed in constant time will be dis-
cussed at the end of Section 6.1, where we approximate oracles with treebank-induced
classifiers in order to experimentally evaluate the different algorithms. The assumption
that every transition can be performed in constant time can be justified by noting that
the only operations involved are those of adding an arc to the graph, removing the first
element from the buffer, and pushing or popping the stack.
Theorem 3
The worst-case space complexity of the arc-standard, stack-based algorithm is O(n),
where n is the length of the input sentence.
Proof 3
Given the deterministic parsing algorithm, only one configuration c = (?,?,A) needs to
be stored at any given time. Assuming that a single node can be stored in some constant
space, the space needed to store ? and ?, respectively, is bounded by the number of
nodes. The same holds for A, given that a single arc can be stored in constant space,
because the number of arcs in a dependency forest is bounded by the number of nodes.
Hence, the worst-case space complexity is O(n). 
4.2 Arc-Eager Parsing
The transition set T for the arc-eager, stack-based parser is defined in Figure 5 and
contains four types of transitions:
1. Transitions LEFT-ARCl (for any dependency label l) add a dependency arc
( j, l, i) to A, where i is the node on top of the stack ? and j is the first node in
the buffer ?. In addition, they pop the stack ?. They have as a precondition
that the token i is not the artificial root node 0 and does not already have
a head.
Transitions
LEFT-ARCl (?|i, j|?,A) ? (?, j|?,A?{( j, l, i)})
RIGHT-ARCel (?|i, j|?,A) ? (?|i|j,?,A?{(i, l, j)})
REDUCE (?|i,?,A) ? (?,?,A)
SHIFT (?, i|?,A) ? (?|i,?,A)
Preconditions
LEFT-ARCl ?[i = 0]
??k?l?[(k, l?, i) ? A]
RIGHT-ARCel ??k?l
?[(k, l?, j) ? A]
REDUCE ?k?l[(k, l, i) ? A]
Figure 5
Transitions for the arc-eager, stack-based parsing algorithm.
525
Computational Linguistics Volume 34, Number 4
2. Transitions RIGHT-ARCel (for any dependency label l) add a dependency
arc (i, l, j) to A, where i is the node on top of the stack ? and j is the first
node in the buffer ?. In addition, they remove the first node j in the buffer
? and push it on top of the stack ?. They have as a precondition that the
token j does not already have a head.
3. The transition REDUCE pops the stack ? and is subject to the precondition
that the top token has a head.
4. The transition SHIFT removes the first node i in the buffer ? and pushes it
on top of the stack ?.
The arc-eager parser differs from the arc-standard one by attaching right dependents
(using RIGHT-ARCel transitions) as soon as possible, that is, before the right dependent
has found all its right dependents. As a consequence, the RIGHT-ARCel transitions
cannot replace the head-dependent structure with the head, as in the arc-standard
system, but must store both the head and the dependent on the stack for further
processing. The dependent can be popped from the stack at a later time through the
REDUCE transition, which completes the reduction of this structure. The arc-eager
system is illustrated in Figure 6, which shows the transition sequence needed to parse
the English sentence in Figure 2 with the same output as the arc-standard sequence in
Figure 4.
Theorem 4
The arc-eager, stack-based algorithm is correct for the class of projective dependency
forests.
Transition Configuration
( [0], [1, . . . , 9], ? )
SHIFT =? ( [0, 1], [2, . . . , 9], ? )
LEFT-ARCNMOD =? ( [0], [2, . . . , 9], A1 = {(2, NMOD, 1)} )
SHIFT =? ( [0, 2], [3, . . . , 9], A1 )
LEFT-ARCSBJ =? ( [0], [3, . . . , 9], A2 = A1?{(3, SBJ, 2)} )
RIGHT-ARCeROOT =? ( [0, 3], [4, . . . , 9], A3 = A2?{(0, ROOT, 3)} )
SHIFT =? ( [0, 3, 4], [5, . . . , 9], A3 )
LEFT-ARCNMOD =? ( [0, 3], [5, . . . , 9], A4 = A3?{(5, NMOD, 4)} )
RIGHT-ARCeOBJ =? ( [0, 3, 5], [6, . . . , 9], A5 = A4?{(3, OBJ, 5)} )
RIGHT-ARCeNMOD =? ( [0, . . . , 6], [7, 8, 9], A6 = A5?{(5, NMOD, 6)} )
SHIFT =? ( [0, . . . , 7], [8, 9], A6 )
LEFT-ARCNMOD =? ( [0, . . . 6], [8, 9], A7 = A6?{(8, NMOD, 7)} )
RIGHT-ARCePMOD =? ( [0, . . . , 8], [9], A8 = A7?{(6, PMOD, 8)} )
REDUCE =? ( [0, . . . , 6], [9], A8 )
REDUCE =? ( [0, 3, 5], [9], A8 )
REDUCE =? ( [0, 3], [9], A8 )
RIGHT-ARCeP =? ( [0, 3, 9], [ ], A9 = A8?{(3, P, 9)} )
Figure 6
Arc-eager transition sequence for the English sentence in Figure 2.
526
Nivre Deterministic Incremental Dependency Parsing
Proof 4
To show the soundness of the algorithm, we show that the dependency graph defined
by the initial configuration, Gc0(x) = (Vx, ?), is a projective dependency forest, and that
every transition preserves this property. We consider each of the relevant conditions in
turn, keeping in mind that the only transitions that modify the graph are LEFT-ARCl
and RIGHT-ARCel .
1. ROOT: Same as Proof 1.
2. SINGLE-HEAD: Same as Proof 1 (substitute RIGHT-ARCel for RIGHT-ARC
s
l ).
3. ACYCLICITY: Gcs(x) is acyclic, and adding an arc (i, l, j) will create a cycle
only if there is a directed path from j to i. In the case of LEFT-ARCl, this
would require the existence of a configuration cp = (?|j, i|?,Acp ) such
that (k, l?, i) ? Acp (for some k and l
?), which is impossible because any
transition adding an arc (k, l?, i) has as a consequence that i is no longer in
the buffer. In the case of RIGHT-ARCel , this would require a configuration
cp = (?|i, j|?,Acp ) such that, given the arcs in Acp , there is a directed path
from j to i. Such a path would have to involve at least one arc (k, l?, k?) such
that k? ? i < k, which would entail that i is no longer in ?. (If k? = i, then i
would be popped in the LEFT-ARCl? transition adding the arc; if k
? < i,
then iwould have to be popped before the arc could be added.)
4. PROJECTIVITY: To prove that, if cp ? C0,m and cp = (?|i, j|?,Acp ), then
?(p, i, j), we use essentially the same technique as in Proof 1, only with
different cases in the inductive step because of the different transitions.
As before, we let ?(p) be the number of transitions that it takes to reach
cp from the first configuration that has i on top of the stack.
Basis: If ?(p) = 0, then i and j are adjacent, which entails ?(p, i, j).
Inductive step: We assume that ?(p, i, j) holds if ?(p) ? q (for some
q > 0) and that ?(p) = q+ 1, and we concentrate on the transition
tp that results in configuration cp. For the arc-eager algorithm, there
are only two cases to consider, because if tp = RIGHT-ARC
e
l (for
some l) or tp = SHIFT then ?(p) = 0, which contradicts our
assumption that ?(p) > q > 0. (This follows because the arc-eager
algorithm, unlike its arc-standard counterpart, does not allow
nodes to be moved back from the stack to the buffer.)
Case 1: If tp = LEFT-ARCl (for some l), then there is a node k
such that i < k < j, ( j, l, k) ? Acp , and cp?1 = (?|i|k, j|?,Acp?
{( j, l, k)}). Because ?(p? 1) ? q, we can use the inductive
hypothesis to infer ?(p? 1, k, j) and, from this, ?(p, k, j).
Moreover, because there has to be an earlier configuration
cp?r (r < ?(p)) such that cp?r = (?|i, k|?,Acp?r ) and
?(p? r) ? q, we can use the inductive hypothesis again
to infer ?(p? r, i, k) and ?(p, i, k). ?(p, i, k), ?(p, k, j) and
( j, l, k) ? Acp together entail ?(p, i, j).
Case 2: If the transition tp = REDUCE, then there is a node k
such that i < k < j, (i, l, k) ? Acp , and cp?1 = (?|i|k, j|?,Acp ).
Because ?(p? 1) ? q, we can again use the inductive
hypothesis to infer ?(p? 1, k, j) and ?(p, k, j). Moreover,
527
Computational Linguistics Volume 34, Number 4
there must be an earlier configuration cp?r (r < ?(p)) such
that cp?r = (?|i, k|?,Acp?r ) and ?(p? r) ? q, which entails
?(p? r, i, k) and ?(p, i, k). As before, ?(p, i, k), ?(p, k, j) and
(i, l, k) ? Acp together entail ?(p, i, j).
For completeness, we need to show that for any sentence x and projective depen-
dency forest Gx = (Vx,Ax) for x, there is a transition sequence C0,m such that Gcm = Gx.
Using the same idea as in Proof 1, we prove this by induction on the length |x| of
x = (w0,w1, . . . ,wn).
Basis: If |x| = 1, then the only projective dependency forest for x is
G = ({0}, ?) and Gcm = Gx for C0,m = (cs(x)).
Inductive step: Assume that the claim holds if |x| ? p (for some p > 1)
and assume that |x| = p+ 1 and Gx = (Vx,Ax) (Vx = {0, 1, . . . , p}). As in
Proof 1, we may now assume that there exists a transition sequence C0,q
for the sentence x? = (w0,w1,wp?1) and subgraph Gx? = (Vx ? {p},A?p),
where the terminal configuration has the form cq = (?cq , [ ],A
?p). For the
arc-eager algorithm, if i is a root in Gx? then i ? ?cq ; but if i ? ?cq then i is
either a root or has a head j such that j < i in Gx? . (This is because imay
have been pushed onto the stack in a RIGHT-ARCel transition and may
or may not have been popped in a later REDUCE transition.) Apart from the
possibility of unreduced right dependents, we can use the same reasoning
as in Proof 1 to show that, for any i ? ?cq that is a root in Gx? , if i is a
dependent of p in Gx then any j such that j ? ?cq , i < j and j is a root in
Gx? must also be a dependent of p in Gx (or else Gx would fail to be
projective). Moreover, if p has a head k in Gx, then kmust be in ?cq and
any j such that j ? ?cq and k < jmust either be a dependent of p in Gx or
must have a head to the left in both Gx? and Gx (anything else would again
be inconsistent with the assumption that Gx is projective). Therefore, we
can construct a transition sequence C0,m such that Gcm = Gx, by starting
in c0 = cs(x) and applying exactly the same q transitions as in C0,q,
followed by as many LEFT-ARCl transitions as there are left dependents of
p in Gx, interleaving REDUCE transitions whenever the node on top of the
stack already has a head, followed by a RIGHT-ARCel transition if p has a
head in Gx and a SHIFT transition otherwise (in both cases moving p to the
stack and emptying the buffer). 
Theorem 5
The worst-case time complexity of the arc-eager, stack-based algorithm is O(n), where
n is the length of the input sentence.
Proof 5
The proof is essentially the same as Proof 2, except that both SHIFT and RIGHT-ARCel
decrease the length of ? and increase the height of ?, while both REDUCE and LEFT-
ARCl decrease the height of ?. Hence, the combined number of SHIFT and RIGHT-ARC
e
l
transitions, as well as the combined number of REDUCE and LEFT-ARCl transitions, are
bounded by n. 
528
Nivre Deterministic Incremental Dependency Parsing
Theorem 6
The worst-case space complexity of the arc-eager, stack-based algorithm is O(n), where
n is the length of the input sentence.
Proof 6
Same as Proof 3. 
5. List-Based Algorithms
The list-based algorithms make use of two lists to store partially processed tokens, that
is, tokens that have been removed from the input buffer but which are still considered
as potential candidates for dependency links, either as heads or as dependents. A parser
configuration is therefore defined as a quadruple, consisting of two lists, an input buffer,
and a set of dependency arcs.
Definition 11
A list-based configuration for a sentence x = (w0,w1, . . . ,wn) is a quadruple c =
(?1, ?2,?,A), where
1. ?1 is a list of tokens i1 ? k1 (for some k1 ? n),
2. ?2 is a list of tokens i2 ? k2 (for some k2, k1 < k2 ? n),
3. ? is a buffer of tokens j > k,
4. A is a set of dependency arcs such that G = ({0, 1, . . . ,n},A) is a
dependency graph for x.
The list ?1 has its head to the right and stores nodes in descending order, and the list ?2
has its head to the left and stores nodes in ascending order. Thus, ?1|i represents a list
with head i and tail ?1, whereas j|?2 represents a list with head j and tail ?2.4 We use
square brackets for enumerated lists as before, and we write ?1.?2 for the concatenation
of ?1 and ?2, so that, for example, [0, 1].[2, 3, 4] = [0, 1, 2, 3, 4]. The notational conven-
tions for the buffer ? and the set A of dependency arcs are the same as before.
Definition 12
A list-based transition system is a quadruple S = (C,T, cs,Ct), where
1. C is the set of all list-based configurations,
2. cs(x = (w0,w1, . . .wn)) = ([0], [ ], [1, . . . ,n], ?),
3. T is a set of transitions, each of which is a function t : C? C,
4. Ct = {c ? C|c = (?1, ?2, [ ],A)}.
A list-based parse of a sentence x = (w0,w1, . . . ,wn) starts with the artificial root node 0
as the sole element of ?1, an empty list ?2, all the nodes corresponding to real words in
the buffer ?, and an empty set A of dependency arcs; it ends as soon as the buffer ? is
empty. Thus, the only difference compared to the stack-based systems is that we have
two lists instead of a single stack. Otherwise, both initialization and termination are
4 The operator | is taken to be left-associative for ?1 and right-associative for ?2.
529
Computational Linguistics Volume 34, Number 4
Transitions
LEFT-ARCnl (?1|i, ?2, j|?,A) ? (?1, i|?2, j|?,A?{( j, l, i)})
RIGHT-ARCnl (?1|i, ?2, j|?,A) ? (?1, i|?2, j|?,A?{(i, l, j)})
NO-ARCn (?1|i, ?2,?,A) ? (?1, i|?2,?,A)
SHIFT? (?1, ?2, i|?,A) ? (?1.?2|i, [ ],?,A)
Preconditions
LEFT-ARCnl ?[i = 0]
??k?l?[(k, l?, i) ? A]
?[i?? j]A
RIGHT-ARCnl ??k?l
?[(k, l?, j) ? A]
?[j?? i]A
Figure 7
Transitions for the non-projective, list-based parsing algorithm.
essentially the same. The transitions used by list-based parsers are again composed of
two types of actions: adding (labeled) arcs toA and manipulating the lists ?1 and ?2, and
the input buffer ?. By combining such actions in different ways, we can construct transi-
tion systems with different properties. We will now define two such systems, which we
call non-projective and projective, respectively, after the classes of dependency graphs
that they can handle.
A clarification may be in order concerning the use of lists instead of stacks for this
family of algorithms. In fact, most of the transitions to be defined subsequently make
no essential use of this added flexibility and could equally well have been formalized
using two stacks instead. However, we will sometimes need to append two lists into
one, and this would not be a constant-time operation using standard stack operations.
We therefore prefer to define these structures as lists, even though they will mostly be
used as stacks.
5.1 Non-Projective Parsing
The transition set T for the non-projective, list-based parser is defined in Figure 7 and
contains four types of transitions:
1. Transitions LEFT-ARCnl (for any dependency label l) add a dependency arc
( j, l, i) to A, where i is the head of the list ?1 and j is the first node in the
buffer ?. In addition, they move i from the list ?1 to the list ?2. They have
as a precondition that the token i is not the artificial root node and does
not already have a head. In addition, there must not be a path from i to j in
the graph G = ({0, 1, . . . ,n},A).5
5 We use the notation [i ??? j]A to signify that there is a path connecting i and j in G = ({0, 1, . . . ,n},A).
530
Nivre Deterministic Incremental Dependency Parsing
2. Transitions RIGHT-ARCnl (for any dependency label l) add a dependency
arc (i, l, j) to A, where i is the head of the list ?1 and j is the first node in the
buffer ?. In addition, they move i from the list ?1 to the list ?2. They have
as a precondition that the token j does not already have a head and that
there is no path from j to i in G = ({0, 1, . . . ,n},A).
3. The transition NO-ARC removes the head i of the list ?1 and inserts it at
the head of the list ?2.
4. The transition SHIFT removes the first node i in the buffer ? and inserts it
at the head of a list obtained by concatenating ?1 and ?2. This list becomes
the new ?1, whereas ?2 is empty in the resulting configuration.
The non-projective, list-based parser essentially builds a dependency graph by consid-
ering every pair of nodes (i, j) (i < j) and deciding whether to add a dependency arc
between them (in either direction), although the SHIFT transition allows it to skip certain
pairs. More precisely, if i is the head of ?1 and j is the first node in the buffer ? when
a SHIFT transition is performed, then all pairs (k, j) such that k < i are ignored. The fact
that both the head and the dependent are kept in either ?2 or ? makes it possible to
construct non-projective dependency graphs, because the NO-ARCn transition allows
a node to be passed from ?1 to ?2 even if it does not (yet) have a head. However, an
arc can only be added between two nodes i and j if the dependent end of the arc is
not the artificial root 0 and does not already have a head, which would violate ROOT
and SINGLE-HEAD, respectively, and if there is no path connecting the dependent to the
head, which would cause a violation of ACYCLICITY. As an illustration, Figure 8 shows
the transition sequence needed to parse the Czech sentence in Figure 1, which has a
non-projective dependency graph.
Theorem 7
The non-projective, list-based algorithm is correct for the class of dependency forests.
Proof 7
To show the soundness of the algorithm, we simply observe that the dependency graph
defined by the initial configuration, Gc0(x) = ({0, 1, . . . ,n}, ?), satisfies ROOT, SINGLE-
HEAD, and ACYCLICITY, and that none of the four transitions may lead to a violation
of these constraints. (The transitions SHIFT? and NO-ARCn do not modify the graph at
all, and LEFT-ARCnl and RIGHT-ARC
n
l have explicit preconditions to prevent this.)
For completeness, we need to show that for any sentence x and dependency for-
est Gx = (Vx,Ax) for x, there is a transition sequence C0,m such that Gcm = Gx. Using
the same idea as in Proof 1, we prove this by induction on the length |x| of x =
(w0,w1, . . . ,wn).
Basis: If |x| = 1, then the only dependency forest for x is G = ({0}, ?) and
Gcm = Gx for C0,m = (cs(x)).
Inductive step: Assume that the claim holds if |x| ? p (for some p > 1)
and assume that |x| = p+ 1 and Gx = (Vx,Ax) (Vx = {0, 1, . . . , p}). As in
Proof 1, we may now assume that there exists a transition sequence C0,q
for the sentence x? = (w0,w1,wp?1) and subgraph Gx? = (Vx ? {p},A?p),
but the terminal configuration now has the form cq = (?cq , [ ], [ ],A
?p),
where ?cq = [0, 1, . . . , p? 1]. In order to construct a transition sequence
C0,m such that Gcm = Gx we instead start from the configuration
531
Computational Linguistics Volume 34, Number 4
Transition Configuration
( [0], [ ], [1, . . . , 8], ? )
SHIFT? =? ( [0, 1], [ ], [2, . . . , 8], ? )
RIGHT-ARCnAtr =? ( [0], [1], [2, . . . , 8], A1 = {(1,Atr, 2)} )
SHIFT? =? ( [0, 1, 2], [ ], [3, . . . , 8], A1 )
NO-ARCn =? ( [0, 1], [2], [3, . . . , 8], A1 )
NO-ARCn =? ( [0], [1, 2], [3, . . . , 8], A1 )
RIGHT-ARCnPred =? ( [ ], [0, 1, 2], [3, . . . , 8], A2 = A1?{(0, Pred, 3)} )
SHIFT? =? ( [0, . . . , 3], [ ], [4, . . . , 8], A2 )
SHIFT? =? ( [0, . . . , 4], [ ], [5, . . . , 8], A2 )
LEFT-ARCnAuxZ =? ( [0, . . . , 3], [4], [5, . . . , 8], A3 = A2?{(5,AuxZ, 4)} )
RIGHT-ARCnSb =? ( [0, 1, 2], [3, 4], [5, . . . , 8], A4 = A3?{(3, Sb, 5)} )
NO-ARCn =? ( [0, 1], [2, 3, 4], [5, . . . , 8], A4 )
LEFT-ARCnAuxP =? ( [0], [1, . . . , 4], [5, . . . , 8], A5 = A4?{(5,AuxP, 1)} )
SHIFT? =? ( [0, . . . , 5], [ ], [6, 7, 8], A5 )
NO-ARCn =? ( [0, . . . , 4], [5], [6, 7, 8], A5 )
NO-ARCn =? ( [0, . . . , 3], [4, 5], [6, 7, 8], A5 )
RIGHT-ARCnAuxP =? ( [0, 1, 2], [3, 4, 5], [6, 7, 8], A6 = A5?{(3,AuxP, 6)} )
SHIFT? =? ( [0, . . . , 6], [ ], [7, 8], A6 )
RIGHT-ARCnAdv =? ( [0, . . . , 5], [6], [7, 8], A7 = A6?{(6,Adv, 7)} )
SHIFT? =? ( [0, . . . , 7], [ ], [8], A7 )
NO-ARCn =? ( [0, . . . , 6], [7], [8], A7 )
NO-ARCn =? ( [0, . . . , 5], [6, 7], [8], A7 )
NO-ARCn =? ( [0, . . . , 4], [5, 6, 7], [8], A7 )
NO-ARCn =? ( [0, . . . , 3], [4, . . . , 7], [8], A7 )
NO-ARCn =? ( [0, 1, 2], [3, . . . , 7], [8], A7 )
NO-ARCn =? ( [0, 1], [2, . . . , 7], [8], A7 )
NO-ARCn =? ( [0], [1, . . . , 7], [8], A7 )
RIGHT-ARCnAuxK =? ( [ ], [0, . . . , 7], [8], A8 = A7?{(0,AuxK, 8)} )
SHIFT? =? ( [0, . . . , 8], [ ], [ ], A8 )
Figure 8
Non-projective transition sequence for the Czech sentence in Figure 1.
c0 = cs(x) and apply exactly the same q transitions, reaching the
configuration cq = (?cq , [ ], [p],A
?p). We then perform exactly p transitions,
in each case choosing LEFT-ARCnl if the token i at the head of ?1 is a
dependent of p in Gx (with label l), RIGHT-ARC
n
l? if i is the head of p (with
label l?) and NO-ARCn otherwise. One final SHIFT? transition takes us to
the terminal configuration cm = (?cq |p, [ ], [ ],Ax). 
Theorem 8
The worst-case time complexity of the non-projective, list-based algorithm is O(n2),
where n is the length of the input sentence.
Proof 8
Assuming that the oracle and transition functions can be performed in some constant
time, the worst-case running time is bounded by the maximum number of transitions
532
Nivre Deterministic Incremental Dependency Parsing
in a transition sequence C0,m for a sentence x = (w0,w1, . . . ,wn). As for the stack-based
algorithms, there can be at most n SHIFT? transitions in C0,m. Moreover, because each of
the three other transitions presupposes that ?1 is non-empty and decreases its length by
1, there can be at most i such transitions between the i?1th and the ith SHIFT transition.
It follows that the total number of transitions in C0,m is bounded by
?n
i=1 i+1, which is
O(n2). 
Remark 2
The assumption that transitions can be performed in constant time can be justified by
the same kind of considerations as for the stack-based algorithms (cf. Remark 1). The
only complication is the SHIFT? transition, which involves appending the two lists ?1
and ?2, but this can be handled with an appropriate choice of data structures. A more
serious complication is the need to check the preconditions of LEFT-ARCnl and RIGHT-
ARCnl , but if we assume that it is the responsibility of the oracle to ensure that the
preconditions of any predicted transition are satisfied, we can postpone the discussion
of this problem until the end of Section 6.1.
Theorem 9
The worst-case space complexity of the non-projective, list-based algorithm is O(n),
where n is the length of the input sentence.
Proof 9
Given the deterministic parsing algorithm, only one configuration c = (?1, ?2,?,A)
needs to be stored at any given time. Assuming that a single node can be stored in
some constant space, the space needed to store ?1, ?2, and ?, respectively, is bounded
by the number of nodes. The same holds for A, given that a single arc can be stored in
constant space, because the number of arcs in a dependency forest is bounded by the
number of nodes. Hence, the worst-case space complexity is O(n). 
5.2 Projective Parsing
The transition set T for the projective, list-based parser is defined in Figure 9 and
contains four types of transitions:
1. Transitions LEFT-ARC
p
l
(for any dependency label l) add a dependency arc
( j, l, i) to A, where i is the head of the list ?1 and j is the first node in the
buffer ?. In addition, they remove i from the list ?1 and empty ?2. They
have as a precondition that the token i is not the artificial root node and
does not already have a head.
2. Transitions RIGHT-ARC
p
l
(for any dependency label l) add a dependency
arc (i, l, j) to A, where i is the head of the list ?1 and j is the first node in the
buffer ?. In addition, they move j from the buffer ? and empty the list ?2.
They have as a precondition that the token j does not already have a head.
3. The transition NO-ARCp removes the head i of the list ?1 and inserts it at
the head of the list ?2. It has as a precondition that the node i already has
a head.
4. The transition SHIFT? removes the first node i in the buffer ? and inserts it
at the head of a list obtained by concatenating ?1 and ?2. This list becomes
the new ?1, while ?2 is empty in the resulting configuration.
533
Computational Linguistics Volume 34, Number 4
Transitions
LEFT-ARC
p
l
(?1|i, ?2, j|?,A) ? (?1, [ ], j|?,A?{( j, l, i)})
RIGHT-ARC
p
l
(?1|i, ?2, j|?,A) ? (?1|i|j, [ ],?,A?{(i, l, j)})
NO-ARCp (?1|i, ?2,?,A) ? (?1, i|?2,?,A)
SHIFT? (?1, ?2, i|?,A) ? (?1.?2|i, [ ],?,A)
Preconditions
LEFT-ARC
p
l
?[i = 0]
??k?l?[(k, l?, i) ? A]
RIGHT-ARC
p
l
??k?l?[(k, l?, j) ? A]
NO-ARCp ?k?l[(k, l, i) ? A]
Figure 9
Transitions for the projective, list-based parsing algorithm.
The projective, list-based parser uses the same basic strategy as its non-projective coun-
terpart, but skips any pair (i, j) that could give rise to a non-projective dependency arc.
The essential differences are the following:
1. While LEFT-ARCnl stores the dependent i in the list ?2, allowing it to have
dependents to the right of j, LEFT-ARC
p
l
deletes it and in addition empties
?2 because any dependency arc linking i, or any node between i and j, to a
node succeeding j would violate PROJECTIVITY.
2. While RIGHT-ARCnl allows the dependent j to seek dependents to the
left of i, by simply moving i from ?1 to ?2, RIGHT-ARC
p
l
essentially
incorporates a SHIFT? transition by moving j to ?1|i, because any
dependency arc linking j to a node preceding iwould violate
PROJECTIVITY. In addition, it does not move any nodes from ?2 to ?1,
since these nodes can no longer be linked to any node succeeding j
without violating PROJECTIVITY.
3. While NO-ARCn is permissible as long as ?1 is not empty, NO-ARC
p
requires that the node i already has a head because any dependency arc
spanning a root node would violate PROJECTIVITY (regardless of which
arcs are added later).
The fact that the projective algorithm skips many node pairs that are considered by the
non-projective algorithm makes it more efficient in practice, although the worst-case
time complexity remains the same. Figure 10 shows the transition sequence needed
to parse the English sentence in Figure 2 with the same output as the stack-based
sequences in Figures 4 and 6.
Theorem 10
The projective, list-based algorithm is correct for the class of projective dependency
forests.
534
Nivre Deterministic Incremental Dependency Parsing
Transition Configuration
( [0], [ ], [1, . . . , 9], ? )
SHIFT? =? ( [0, 1], [ ], [2, . . . , 9], ? )
LEFT-ARC
p
NMOD =? ( [0], [ ], [2, . . . , 9], A1 = {(2, NMOD, 1)} )
SHIFT? =? ( [0, 2], [ ], [3, . . . , 9], A1 )
LEFT-ARC
p
SBJ =? ( [0], [ ], [3, . . . , 9], A2 = A1?{(3, SBJ, 2)} )
RIGHT-ARC
p
ROOT =? ( [0, 3], [ ], [4, . . . , 9], A3 = A2?{(0, ROOT, 3)} )
SHIFT? =? ( [0, 3, 4], [ ], [5, . . . , 9], A3 )
LEFT-ARC
p
NMOD =? ( [0, 3], [ ], [5, . . . , 9], A4 = A3?{(5, NMOD, 4)} )
RIGHT-ARC
p
OBJ =? ( [0, 3, 5], [ ], [6, . . . , 9], A5 = A4?{(3, OBJ, 5)} )
RIGHT-ARC
p
NMOD =? ( [0, . . . , 6], [ ], [7, 8, 9], A6 = A5?{(5, NMOD, 6)} )
SHIFT? =? ( [0, . . . , 7], [ ], [8, 9], A6 )
LEFT-ARCNMOD =? ( [0, . . . 6], [ ], [8, 9], A7 = A6?{(8, NMOD, 7)} )
RIGHT-ARC
p
PMOD =? ( [0, . . . , 8], [ ], [9], A8 = A7?{(6, PMOD, 8)} )
NO-ARCp =? ( [0, . . . , 6], [8], [9], A8 )
NO-ARCp =? ( [0, 3, 5], [6, 8], [9], A8 )
NO-ARCp =? ( [0, 3], [5, 6, 8], [9], A8 )
RIGHT-ARC
p
P =? ( [0, 3, 9], [ ], [ ], A9 = A8?{(3, P, 9)} )
Figure 10
Projective transition sequence for the English sentence in Figure 2.
Proof 10
To show the soundness of the algorithm, we show that the dependency graph defined
by the initial configuration, Gc0(x) = (V, ?), is a projective dependency forest, and that
every transition preserves this property. We consider each of the relevant conditions in
turn, keeping in mind that the only transitions that modify the graph are LEFT-ARC
p
l
and RIGHT-ARC
p
l
.
1. ROOT: Same as Proof 1 (substitute LEFT-ARC
p
l
for LEFT-ARCl).
2. SINGLE-HEAD: Same as Proof 1 (substitute LEFT-ARC
p
l
and RIGHT-ARC
p
l
for LEFT-ARCl and RIGHT-ARC
s
l , respectively).
3. ACYCLICITY: Gcs(x) is acyclic, and adding an arc (i, l, j) will create a cycle
only if there is a directed path from j to i. In the case of LEFT-ARC
p
l
, this
would require the existence of a configuration cp = (?1|j, ?2, i|?,Acp ) such
that (k, l?, i) ? Acp (for some k < i and l
?), which is impossible because any
transition adding an arc (k, l?, i) has as a consequence that i is no longer in
the buffer. In the case of RIGHT-ARC
p
l
, this would require a configuration
cp = (?1|i, ?2, j|?,Acp ) such that, given the arcs in Acp , there is a directed
path from j to i. Such a path would have to involve at least one arc (k, l?, k?)
such that k? ? i < k, which would entail that i is no longer in ?1 or ?2.
(If k? = i, then iwould be removed from ?1?and not added to ?2?in the
LEFT-ARC
p
l?
transition adding the arc; if k? < i, then i would have to be
moved to ?2 before the arc can be added and removed as this list is
emptied in the LEFT-ARC
p
l?
transition.)
535
Computational Linguistics Volume 34, Number 4
4. PROJECTIVITY: Gcs(x) is projective, and adding an arc (i, l, j) will make the
graph non-projective only if there is a node k such that i < k < j or j < k < i
and neither i?? k nor j?? k. Let C0,m be a configuration sequence for
x = (w0,w1, . . . ,wn) and let ?(p, i, j) (for 0 < p < m, 0 ? i < j ? n) be the
claim that, for every k such that i < k < j, i?? k or j?? k in Gcp . To prove
that no arc can be non-projective, we need to prove that, if cp ? C0,m and
cp = (?1|i, ?2, j|?,Acp ), then ?(p, i, j). (If cp = (?1|i, ?2, j|?,Acp ) and ?(p, i, j),
then ?(p?, i, j) for all p? such that p < p?, because in cp every node k such
that i < k < jmust already have a head.) We prove this by induction over
the number ?(p) of transitions leading to cp from the first configuration
cp??(p) ? C0,m such that cp??(p) = (?1, ?2, j|?,Acp??(p) ) (i.e., the first
configuration where j is the first node in the buffer).
Basis: If ?(p) = 0, then i and j are adjacent and ?(p, i, j) holds
vacuously.
Inductive step: Assume that ?(p, i, j) holds if ?(p) ? q (for some
q > 0) and that ?(p) = q+ 1. Now consider the transition tp that
results in configuration cp. For the projective, list-based algorithm,
there are only two cases to consider, because if tp = RIGHT-ARC
p
l
(for some l) or tp = SHIFT then ?(p) = 0, which contradicts our
assumption that ?(p) > q > 0. (This follows because there is no
transition that moves a node back to the buffer.)
Case 1: If tp = LEFT-ARC
p
l
(for some l), then there is a node k
such that i < k < j, ( j, l, k) ? Acp , cp?1 = (?1|i|k, ?2, j|?,Acp?
{( j, l, k)}), and cp = (?1|i, [ ], j|?,Acp ). Because ?(p? 1) ? q,
we can use the inductive hypothesis to infer ?(p? 1, k, j)
and, from this, ?(p, k, j). Moreover, because there has to
be an earlier configuration cp?r (r < ?(p)) such that cp?r =
(?1|i, ?2? , k|?,Acp?r ) and ?(p? r) ? q, we can use the
inductive hypothesis again to infer ?(p? r, i, k) and ?(p, i, k).
?(p, i, k), ?(p, k, j), and ( j, l, k) ? Acp together entail ?(p, i, j).
Case 2: If the transition tp = NO-ARC
p, then there is a node k
such that i < k < j, (i, l, k) ? Acp , cp?1 = (?1|i|k, ?2, j|?,Acp ),
and cp = (?1|i, k|?2, j|?,Acp ). Because ?(p? 1) ? q, we can
again use the inductive hypothesis to infer ?(p? 1, k, j) and
?(p, k, j). Moreover, there must be an earlier configuration
cp?r (r < ?(p)) such that cp?r = (?1|i, ?2? , k|?,Acp?r ) and
?(p? r) ? q, which entails ?(p? r, i, k) and ?(p, i, k). As
before, ?(p, i, k), ?(p, k, j), and (i, l, k) ? Acp together entail
?(p, i, j).
For completeness, we need to show that for any sentence x and dependency forest Gx =
(Vx,Ax) for x, there is a transition sequence C0,m such that Gcm = Gx. The proof is by
induction on the length |x| and is essentially the same as Proof 7 up to the point where
we assume the existence of a transition sequence C0,q for the sentence x
? = (w0,w1,wp?1)
and subgraphGx? = (Vx ? {p},A?p), where the terminal configuration still has the form
cq = (?cq , [ ], [ ],A
?p), but where it can no longer be assumed that ?cq = [0, 1, . . . , p? 1].
If i is a root in Gx? then i ? ?cq ; but if i ? ?cq then i is either a root or has a head j such
536
Nivre Deterministic Incremental Dependency Parsing
that j < i in Gx? . (This is because a RIGHT-ARC
p
l
transition leaves the dependent in ?1
while a LEFT-ARC
p
l
removes it.) Moreover, for any i ? ?cq that is a root in Gx? , if i is a
dependent of p in Gx then any j such that j ? ?cq , i < j and j is a root in Gx? must also be
a dependent of p in Gx (else Gx would fail to be projective). Finally, if p has a head k in
Gx, then kmust be in ?cq and any j such that j ? ?cq and k < jmust either be a dependent
of p in Gx or must have a head to the left in both Gx? and Gx (anything else would again
be inconsistent with the assumption that Gx is projective). Therefore, we can construct
a transition sequence C0,m such that Gcm = Gx, by starting in c0 = cs(x) and applying
exactly the same q transitions as in C0,q, followed by as many LEFT-ARC
p
l
transitions
as there are left dependents of p in Gx, interleaving NO-ARC
p transitions whenever the
node at the head of ?1 already has a head, followed by a RIGHT-ARC
p
l
transition if p
has a head in Gx. One final SHIFT
n transition takes us to the terminal configuration
cm = (?cm , [ ], [ ],Ax). 
Theorem 11
The worst-case time complexity of the projective, list-based algorithm is O(n2), where n
is the length of the input sentence.
Proof 11
Same as Proof 8. 
Theorem 12
The worst-case space complexity of the projective, list-based algorithm is O(n), where n
is the length of the input sentence.
Proof 12
Same as Proof 9. 
6. Experimental Evaluation
We have defined four different transition systems for incremental dependency parsing,
proven their correctness for different classes of dependency graphs, and analyzed their
time and space complexity under the assumption that there exists a constant-time oracle
for predicting the next transition. In this section, we present an experimental evaluation
of the accuracy and efficiency that can be achieved with these systems in deterministic
data-driven parsing, that is, when the oracle is approximated by a classifier trained
on treebank data. The purpose of the evaluation is to compare the performance of the
four algorithms under realistic conditions, thereby complementing the purely formal
analysis presented so far. The purpose is not to produce state-of-the-art results for all
algorithms on the data sets used, which would require extensive experimentation and
optimization going well beyond the limits of this study.
6.1 Experimental Setup
The data sets used are taken from the CoNLL-X shared task on multilingual dependency
parsing (Buchholz and Marsi 2006). We have used all the available data sets, taken
537
Computational Linguistics Volume 34, Number 4
Table 1
Data sets. Tok = number of tokens (?1000); Sen = number of sentences (?1000); T/S = tokens
per sentence (mean); Lem = lemmatization present; CPoS = number of coarse-grained
part-of-speech tags; PoS = number of (fine-grained) part-of-speech tags; MSF = number of
morphosyntactic features (split into atoms); Dep = number of dependency types; NPT =
proportion of non-projective dependencies/tokens (%); NPS = proportion of non-projective
dependency graphs/sentences (%).
Language Tok Sen T/S Lem CPoS PoS MSF Dep NPT NPS
Arabic 54 1.5 37.2 yes 14 19 19 27 0.4 11.2
Bulgarian 190 14.4 14.8 no 11 53 50 18 0.4 5.4
Chinese 337 57.0 5.9 no 22 303 0 82 0.0 0.0
Czech 1,249 72.7 17.2 yes 12 63 61 78 1.9 23.2
Danish 94 5.2 18.2 no 10 24 47 52 1.0 15.6
Dutch 195 13.3 14.6 yes 13 302 81 26 5.4 36.4
German 700 39.2 17.8 no 52 52 0 46 2.3 27.8
Japanese 151 17.0 8.9 no 20 77 0 7 1.1 5.3
Portuguese 207 9.1 22.8 yes 15 21 146 55 1.3 18.9
Slovene 29 1.5 18.7 yes 11 28 51 25 1.9 22.2
Spanish 89 3.3 27.0 yes 15 38 33 21 0.1 1.7
Swedish 191 11.0 17.3 no 37 37 0 56 1.0 9.8
Turkish 58 5.0 11.5 yes 14 30 82 25 1.5 11.6
from treebanks of thirteen different languages with considerable typological variation.
Table 1 gives an overview of the training data available for each language.
For data sets that include a non-negligible proportion of non-projective dependency
graphs, it can be expected that the non-projective list-based algorithm will achieve
higher accuracy than the strictly projective algorithms. In order to make the comparison
more fair, we therefore also evaluate pseudo-projective versions of the latter algorithms,
making use of graph transformations in pre- and post-processing to recover non-
projective dependency arcs, following Nivre and Nilsson (2005). For each language,
seven different parsers were therefore trained as follows:
1. For the non-projective list-based algorithm, one parser was trained
without preprocessing the training data.
2. For the three projective algorithms, two parsers were trained after
preprocessing the training data as follows:
(a) For the strictly projective parser, non-projective dependency graphs
in the training data were transformed by lifting non-projective
arcs to the nearest permissible ancestor of the real head. This
corresponds to the Baseline condition in Nivre and Nilsson (2005).
(b) For the pseudo-projective parser, non-projective dependency
graphs in the training data were transformed by lifting
non-projective arcs to the nearest permissible ancestor of the
real head, and augmenting the arc label with the label of the real
head. The output of this parser was post-processed by lowering
dependency arcs with augmented labels using a top-down,
left-to-right, breadth-first search for the first descendant of the
head that matches the augmented arc label. This corresponds to
the Head condition in Nivre and Nilsson (2005).
538
Nivre Deterministic Incremental Dependency Parsing
Table 2
Feature models. Rows represent tokens defined relative to the current configuration (L[i] = ith
element of list/stack L of length n; hd(x) = head of x; ld(x) = leftmost dependent of x; rd(x) =
rightmost dependent of x). Columns represent attributes of tokens (Form = word form; Lem =
lemma; CPoS = coarse part-of-speech; FPoS = fine part-of-speech; Feats = morphosyntactic
features; Dep = dependency label). Filled cells represent features used by one or more
algorithms (All = all algorithms; S = arc-standard, stack-based; E = arc-eager, stack-based;
N = non-projective, list-based; P = projective, list-based).
Attributes
Tokens Form Lem CPoS FPoS Feats Dep
?[0] All All All All All N
?[1] All All
?[2] All
?[3] All
ld(?[0]) All
rd(?[0]) S
?[0] SE SE SE SE SE E
?[1] SE
hd(?[0]) E
ld(?[0]) SE
rd(?[0]) SE
?1[0] NP NP NP NP NP NP
?1[1] NP
hd(?1[0]) NP
ld(?1[0]) NP
rd(?1[0]) NP
?2[0] N
?2[n] N
All parsers were trained using the freely available MaltParser system,6 which provides
implementations of all the algorithms described in Sections 4 and 5. MaltParser also
incorporates the LIBSVM library for support vector machines (Chang and Lin 2001),
which was used to train classifiers for predicting the next transition. Training data for
the classifiers were generated by parsing each sentence in the training set using the gold-
standard dependency graph as an oracle. For each transition t(c) in the oracle parse, a
training instance (?(c), t) was created, where ?(c) is a feature vector representation of
the parser configuration c. Because the purpose of the experiments was not to optimize
parsing accuracy as such, no work was done on feature selection for the different
algorithms and languages. Instead, all parsers use a variant of the simple feature model
used for parsing English and Swedish in Nivre (2006b), with minor modifications to suit
the different algorithms.
Table 2 shows the feature sets used for different parsing algorithms.7 Each row
represents a node defined relative to the current parser configuration, where nodes
6 Available at http://w3.msi.vxu.se/users/nivre/research/MaltParser.html.
7 For each of the three projective algorithms, the strictly projective and the pseudo-projective variants
used exactly the same set of features, although the set of values for the dependency label features
were different because of the augmented label set introduced by the pseudo-projective technique.
539
Computational Linguistics Volume 34, Number 4
defined relative to the stack ? are only relevant for stack-based algorithms, whereas
nodes defined relative to the lists ?1 and ?2 are only relevant for list-based algorithms.
We use the notation L[i], for arbitrary lists or stacks, to denote the ith element of L, with
L[0] for the first element (top element of a stack) and L[n] for the last element. Nodes
defined relative to the partially-built dependency graph make use of the operators hd, ld,
and rd, which return, respectively, the head, the leftmost dependent, and the rightmost
dependent of a node in the dependency graph Gc defined by the current configuration
c, if such a node exists, and a null value otherwise. The columns in Table 2 represent
attributes of nodes (tokens) in the input (word form, lemma, coarse part-of-speech, fine
part-of-speech, morphosyntactic features) or in the partially-built dependency graph
(dependency label), which can be used to define features. Each cell in the table thus
represents a feature fij = aj(ni), defined by selecting the attribute aj in the jth column
from the node ni characterized in the ith row. For example, the feature f11 is the word
form of the first input node (token) in the buffer ?. The symbols occurring in filled
cells indicate for which parsing algorithms the feature is active, where S stands for arc-
standard stack-based, E for arc-eager stack-based, N for non-projective list-based, and
P for projective list-based. Features that are used for some but not all algorithms are
typically not meaningful for all algorithms. For example, a right dependent of the first
node in the buffer ? can only exist (at decision time) when using the arc-standard stack-
based algorithm. Hence, this feature is inactive for all other algorithms.
The SVM classifiers were trained with a quadratic kernel K(xi, xj) = (?x
T
i xj + r)
2
and LIBSVM?s built-in one-versus-one strategy for multi-class classification, convert-
ing symbolic features to numerical ones using the standard technique of binarization.
The parameter settings were ? = 0.2 and r = 0 for the kernel parameters, C = 0.5 for
the penalty parameter, and  = 1.0 for the termination criterion. These settings were
extrapolated from many previous experiments under similar conditions, using cross-
validation or held-out subsets of the training data for tuning, but in these experiments
they were kept fixed for all parsers and languages. In order to reduce training times, the
set of training instances derived from a given training set was split into smaller sets, for
which separate multi-class classifiers were trained, using FPoS(?[0]), that is, the (fine-
grained) part of speech of the first node in the buffer, as the defining feature for the
split.
The seven different parsers for each language were evaluated by running them on
the dedicated test set from the CoNLL-X shared task, which consists of approximately
5,000 tokens for all languages. Because the dependency graphs in the gold standard
are always trees, each output graph was converted, if necessary, from a forest to a tree
by attaching every root node i (i > 0) to the special root node 0 with a default label
ROOT. Parsing accuracy was measured by the labeled attachment score (LAS), that is,
the percentage of tokens that are assigned the correct head and dependency label, as
well as the unlabeled attachment score (UAS), that is, the percentage of tokens with
the correct head, and the label accuracy (LA), that is, the percentage of tokens with the
correct dependency label. All scores were computed with the scoring software from
the CoNLL-X shared task, eval.pl, with default settings. This means that punctuation
tokens are excluded in all scores. In addition to parsing accuracy, we evaluated
efficiency by measuring the learning time and parsing time in seconds for each data set.
Before turning to the results of the evaluation, we need to fulfill the promise from
Remarks 1 and 2 to discuss the way in which treebank-induced classifiers approximate
oracles and to what extent they satisfy the condition of constant-time operation that
was assumed in all the results on time complexity in Sections 4 and 5. When pre-
dicting the next transition at run-time, there are two different computations that take
540
Nivre Deterministic Incremental Dependency Parsing
place: the first is the classifier returning a transition t as the output class for an input
feature vector?(c), and the second is a check whether the preconditions of t are satisfied
in c. If the preconditions are satisfied, the transition t is performed; otherwise a default
transition (with no preconditions) is performed instead.8 (The default transition is SHIFT
for the stack-based algorithms and NO-ARC for the list-based algorithms.) The time
required to compute the classification t of ?(c) depends on properties of the classifier,
such as the number of support vectors and the number of classes for a multi-class SVM
classifier, but is independent of the length of the input and can therefore be regarded
as a constant as far as the time complexity of the parsing algorithm is concerned.9
The check of preconditions is a trivial constant-time operation in all cases except one,
namely the need to check whether there is a path between two nodes for the LEFT-ARCnl
and RIGHT-ARCnl transitions of the non-projective list-based algorithm. Maintaining the
information needed for this check and updating it with each addition of a new arc
to the graph is equivalent to the union-find operations for disjoint set data structures.
Using the techniques of path compression and union by rank, the amortized time per
operation is O(?(n)) per operation, where n is the number of elements (nodes in this
case) and ?(n) is the inverse of the Ackermann function, which means that ?(n) is
less than 5 for all remotely practical values of n and is effectively a small constant
(Cormen, Leiserson, and Rivest 1990). With this proviso, all the complexity results from
Sections 4 and 5 can be regarded as valid also for the classifier-based implementation of
deterministic, incremental dependency parsing.
6.2 Parsing Accuracy
Table 3 shows the parsing accuracy obtained for each of the 7 parsers on each of the
13 languages, as well as the average over all languages, with the top score in each
row set in boldface. For comparison, we also include the results of the two top scoring
systems in the CoNLL-X shared task, those of McDonald, Lerman, and Pereira (2006)
and Nivre et al (2006). Starting with the LAS, we see that the multilingual average
is very similar across the seven parsers, with a difference of only 0.58 percentage
points between the best and the worst result, obtained with the non-projective and
the strictly projective version of the list-based parser, respectively. However, given the
large amount of data, some of the differences are nevertheless statistically significant
(according to McNemar?s test, ? = .05). Broadly speaking, the group consisting of the
non-projective, list-based parser and the three pseudo-projective parsers significantly
outperforms the group consisting of the three projective parsers, whereas there are
no significant differences within the two groups.10 This shows that the capacity to
capture non-projective dependencies does make a significant difference, even though
such dependencies are infrequent in most languages.
The best result is about one percentage point below the top scores from the original
CoNLL-X shared task, but it must be remembered that the results in this article have
8 A more sophisticated strategy would be to back off to the second best choice of the oracle, assuming that
the oracle provides a ranking of all the possible transitions. On the whole, however, classifiers very rarely
predict transitions that are not legal in the current configuration.
9 The role of this constant in determining the overall running time is similar to that of a grammar constant
in grammar-based parsing.
10 The only exception to this generalization is the pseudo-projective, list-based parser, which is significantly
worse than the non-projective, list-based parser, but not significantly better than the projective,
arc-standard, stack-based parser.
541
Computational Linguistics Volume 34, Number 4
Table 3
Parsing accuracy for 7 parsers on 13 languages, measured by labeled attachment score (LAS),
unlabeled attachment score (UAS) and label accuracy (LA). NP-L = non-projective list-based;
P-L = projective list-based; PP-L = pseudo-projective list-based; P-E = projective arc-eager
stack-based; PP-E = pseudo-projective arc-eager stack-based; P-S = projective arc-standard
stack-based; PP-S = pseudo-projective arc-standard stack-based; McD = McDonald, Lerman
and Pereira (2006); Niv = Nivre et al (2006).
Labeled Attachment Score (LAS)
Language NP-L P-L PP-L P-E PP-E P-S PP-S McD Niv
Arabic 63.25 63.19 63.13 64.93 64.95 65.79 66.05 66.91 66.71
Bulgarian 87.79 87.75 87.39 87.75 87.41 86.42 86.71 87.57 87.41
Chinese 85.77 85.96 85.96 85.96 85.96 86.00 86.00 85.90 86.92
Czech 78.12 76.24 78.04 76.34 77.46 78.18 80.12 80.18 78.42
Danish 84.59 84.15 84.35 84.25 84.45 84.17 84.15 84.79 84.77
Dutch 77.41 74.71 76.95 74.79 76.89 73.27 74.79 79.19 78.59
German 84.42 84.21 84.38 84.23 84.46 84.58 84.58 87.34 85.82
Japanese 90.97 90.57 90.53 90.83 90.89 90.59 90.63 90.71 91.65
Portuguese 86.70 85.91 86.20 85.83 86.12 85.39 86.09 86.82 87.60
Slovene 70.06 69.88 70.12 69.50 70.22 72.00 71.88 73.44 70.30
Spanish 80.18 79.80 79.60 79.84 79.60 78.70 78.42 82.25 81.29
Swedish 83.03 82.63 82.41 82.63 82.41 82.12 81.54 82.55 84.58
Turkish 64.69 64.49 64.37 64.37 64.43 64.67 64.73 63.19 65.68
Average 79.77 79.19 79.49 79.33 79.63 79.38 79.67 80.83 80.75
Unlabeled Attachment Score (UAS)
Language NP-L P-L PP-L P-E PP-E P-S PP-S McD Niv
Arabic 76.43 76.19 76.49 76.31 76.25 77.76 77.98 79.34 77.52
Bulgarian 91.68 91.92 91.62 91.92 91.64 90.80 91.10 92.04 91.72
Chinese 89.42 90.24 90.24 90.24 90.24 90.42 90.42 91.07 90.54
Czech 84.88 82.82 84.58 82.58 83.66 84.50 86.44 87.30 84.80
Danish 89.76 89.40 89.42 89.52 89.52 89.26 89.38 90.58 89.80
Dutch 80.25 77.29 79.59 77.25 79.47 75.95 78.03 83.57 81.35
German 87.80 87.10 87.38 87.14 87.42 87.46 87.44 90.38 88.76
Japanese 92.64 92.60 92.56 92.80 92.72 92.50 92.54 92.84 93.10
Portuguese 90.56 89.82 90.14 89.74 90.08 89.00 89.64 91.36 91.22
Slovene 79.06 78.78 79.06 78.42 78.98 80.72 80.76 83.17 78.72
Spanish 83.39 83.75 83.65 83.79 83.65 82.35 82.13 86.05 84.67
Swedish 89.54 89.30 89.03 89.30 89.03 88.81 88.39 88.93 89.50
Turkish 75.12 75.24 75.02 75.32 74.81 75.94 75.76 74.67 75.82
Average 85.43 84.96 85.29 84.95 85.19 85.04 85.39 87.02 85.96
Label Accuracy (LA)
Language NP-L P-L PP-L P-E PP-E P-S PP-S McD Niv
Arabic 75.93 76.15 76.09 78.46 78.04 79.30 79.10 79.50 80.34
Bulgarian 90.68 90.68 90.40 90.68 90.42 89.53 89.81 90.70 90.44
Chinese 88.01 87.95 87.95 87.95 87.95 88.33 88.33 88.23 89.01
Czech 84.54 84.54 84.42 84.80 84.66 86.00 86.58 86.72 85.40
Danish 88.90 88.60 88.76 88.62 88.82 89.00 88.98 89.22 89.16
Dutch 82.43 82.59 82.13 82.57 82.15 80.71 80.13 83.89 83.69
German 89.74 90.08 89.92 90.06 89.94 90.79 90.36 92.11 91.03
Japanese 93.54 93.14 93.14 93.54 93.56 93.12 93.18 93.74 93.34
Portuguese 90.56 90.54 90.34 90.46 90.26 90.42 90.26 90.46 91.54
Slovene 78.88 79.70 79.40 79.32 79.48 81.61 81.37 82.51 80.54
Spanish 89.46 89.04 88.66 89.06 88.66 88.30 88.12 90.40 90.06
Swedish 85.50 85.40 84.92 85.40 84.92 84.66 84.01 85.58 87.39
Turkish 77.79 77.32 77.02 77.00 77.39 77.02 77.20 77.45 78.49
Average 85.84 85.83 85.63 85.99 85.87 86.06 85.96 86.96 87.03
542
Nivre Deterministic Incremental Dependency Parsing
been obtained without optimization of feature representations or learning algorithm
parameters. The net effect of this can be seen in the result for the pseudo-projective
version of the arc-eager, stack-based parser, which is identical to the system used by
Nivre et al (2006), except for the lack of optimization, and which suffers a loss of 1.12
percentage points overall.
The results for UAS show basically the same pattern as the LAS results, but with
even less variation between the parsers. Nevertheless, there is still a statistically sig-
nificant margin between the non-projective, list-based parser and the three pseudo-
projective parsers, on the one hand, and the strictly projective parsers, on the other.11
For label accuracy (LA), finally, the most noteworthy result is that the strictly projec-
tive parsers consistently outperform their pseudo-projective counterparts, although the
difference is statistically significant only for the projective, list-based parser. This can
be explained by the fact that the pseudo-projective parsing technique increases the
number of distinct dependency labels, using labels to distinguish not only between
different syntactic functions but also between ?lifted? and ?unlifted? arcs. It is there-
fore understandable that the pseudo-projective parsers suffer a drop in pure labeling
accuracy.
Despite the very similar performance of all parsers on average over all languages,
there are interesting differences for individual languages and groups of languages.
These differences concern the impact of non-projective, pseudo-projective, and strictly
projective parsing, on the one hand, and the effect of adopting an arc-eager or an arc-
standard parsing strategy for the stack-based parsers, on the other. Before we turn
to the evaluation of efficiency, we will try to analyze some of these differences in a
little more detail, starting with the different techniques for capturing non-projective
dependencies.
First of all, we may observe that the non-projective, list-based parser outperforms its
strictly projective counterpart for all languages except Chinese. The result for Chinese
is expected, given that it is the only data set that does not contain any non-projective
dependencies, but the difference in accuracy is very slight (0.19 percentage points).
Thus, it seems that the non-projective parser can also be used without loss in accuracy
for languages with very few non-projective structures. The relative improvement in
accuracy for the non-projective parser appears to be roughly linear in the percent-
age of non-projective dependencies found in the data set, with a highly significant
correlation (Pearson?s r = 0.815, p = 0.0007). The only language that clearly diverges
from this trend is German, where the relative improvement is much smaller than
expected.
If we compare the non-projective, list-based parser to the strictly projective stack-
based parsers, we see essentially the same pattern but with a little more variation. For
the arc-eager, stack-based parser, the only anomaly is the result for Arabic, which is
significantly higher than the result for the non-projective parser, but this seems to be
due to a particularly bad performance of the list-based parsers as a group for this
language.12 For the arc-standard, stack-based parser, the data is considerably more
noisy, which is related to the fact that the arc-standard parser in itself has a higher
11 The exception this time is the pseudo-projective, arc-eager parser, which has a statistically significant
difference up to the non-projective parser but a non-significant difference down to the projective,
arc-standard parser.
12 A possible explanation for this result is the extremely high average sentence length for Arabic, which
leads to a greater increase in the number of potential arcs considered for the list-based parsers than for
the stack-based parsers.
543
Computational Linguistics Volume 34, Number 4
variance than the other parsers, an observation that we will return to later on. Still, the
correlation between relative improvement in accuracy and percentage of non-projective
dependencies is significant for both the arc-eager parser (r = 0.766, p = 0.001) and the
arc-standard parser (r = 0.571, p = 0.02), although clearly not as strong as for the list-
based parser. It therefore seems reasonable to conclude that the non-projective parser in
general can be expected to outperform a strictly projective parser with a margin that is
directly related to the proportion of non-projective dependencies in the data.
Having compared the non-projective, list-based parser to the strictly projective
parsers, we will now scrutinize the results obtained when coupling the projective
parsers with the pseudo-projective parsing technique, as an alternative method for
capturing non-projective dependencies. The overall pattern is that pseudo-projective
parsing improves the accuracy of a projective parser for languages with more than 1%
of non-projective dependencies, as seen from the results for Czech, Dutch, German, and
Portuguese. For these languages, the pseudo-projective parser is never outperformed
by its strictly projective counterpart, and usually does considerably better, although the
improvements for German are again smaller than expected. For Slovene and Turkish,
we find improvement only for two out of three parsers, despite a relatively high share
of non-projective dependencies (1.9% for Slovene, 1.5% for Turkish). Given that Slovene
and Turkish have the smallest training data sets of all languages, this is consistent with
previous studies showing that pseudo-projective parsing is sensitive to data sparseness
(Nilsson, Nivre, and Hall 2007). For languages with a lower percentage of non-projective
dependencies, the pseudo-projective technique seems to hurt performance more often
than not, possibly as a result of decreasing the labeling accuracy, as noted previously.
It is worth noting that Chinese is a special case in this respect. Because there are no
non-projective dependencies in this data set, the projectivized training data set will be
identical to the original one, which means that the pseudo-projective parser will behave
exactly as the projective one.
Comparing non-projective parsing to pseudo-projective parsing, it seems clear that
both can improve parsing accuracy in the presence of significant amounts of non-
projective dependencies, but the former appears to be more stable in that it seldom
or never hurts performance, whereas the latter can be expected to have a negative effect
on accuracy when the amount of training data or non-projective dependencies (or both)
is not high enough. Moreover, the non-projective parser tends to outperform the best
pseudo-projective parsers, both on average and for individual languages. In fact, the
pseudo-projective technique outperforms the non-projective parser only in combination
with the arc-standard, stack-based parsing algorithm, and this seems to be due more to
the arc-standard parsing strategy than to the pseudo-projective technique as such. The
relevant question here is therefore why arc-standard parsing seems to work particularly
well for some languages, with or without pseudo-projective parsing.
Going through the results for individual languages, it is clear that the arc-standard
algorithm has a higher variance than the other algorithms. For Bulgarian, Dutch, and
Spanish, the accuracy is considerably lower than for the other algorithms, in most
cases by more than one percentage point. But for Arabic, Czech, and Slovene, we find
exactly the opposite pattern, with the arc-standard parsers sometimes outperforming
the other parsers by more than two percentage points. For the remaining languages,
the arc-standard algorithm performs on a par with the other algorithms.13 In order to
13 The arc-standard algorithm achieves the highest score also for Chinese, German, and Turkish, but in
these cases only by a small margin.
544
Nivre Deterministic Incremental Dependency Parsing
explain this pattern we need to consider the way in which properties of the algorithms
interact with properties of different languages and the way they have been annotated
syntactically.
First of all, it is important to note that the two list-based algorithms and the arc-
eager variant of the stack-based algorithm are all arc-eager in the sense that an arc (i, l, j)
is always added at the earliest possible moment, that is, in the first configuration where
i and j are the target tokens. For the arc-standard stack-based parser, this is still true for
left dependents (i.e., arcs (i, l, j) such that j < i) but not for right dependents, where an
arc (i, l, j) (i < j) should be added only at the point where all arcs of the form ( j, l?, k) have
already been added (i.e., when the dependent j has already found all its dependents).
This explains why the results for the two list-based parsers and the arc-eager stack-
based parser are so well correlated, but it does not explain why the arc-standard strategy
works better for some languages but not for others.
The arc-eager strategy has an advantage in that a right dependent j can be attached
to its head i at any time without having to decide whether j itself should have a right
dependent. By contrast, with the arc-standard strategy it is necessary to decide not only
whether j is a right dependent of i but also whether it should be added now or later,
which means that two types of errors are possible even when the decision to attach j
to i is correct. Attaching too early means that right dependents can never be attached
to j; postponing the attachment too long means that j will never be added to i. None of
these errors can occur with the arc-eager strategy, which therefore can be expected to
work better for data sets where this kind of ?ambiguity? is commonly found. In order
for this to be the case, there must first of all be a significant proportion of left-headed
structures in the data. Thus, we find that in all the data sets for which the arc-standard
parsers do badly, the percentage of left-headed dependencies is in the 50?75% range.
However, it must also be pointed out that the highest percentage of all is found in
Arabic (82.9%), which means that a high proportion of left-headed structures may be
a necessary but not sufficient condition for the arc-eager strategy to work better than
the arc-standard strategy. We conjecture that an additional necessary condition is an
annotation style that favors more deeply embedded structures, giving rise to chains
of left-headed structures where each node is dependent on the preceding one, which
increases the number of points at which an incorrect decision can be made by an arc-
standard parser. However, we have not yet fully verified the extent to which this condi-
tion holds for all the data sets where the arc-eager parsers outperform their arc-standard
counterparts.
Although the arc-eager strategy has an advantage in that the decisions involved in
attaching a right dependent are simpler, it has the disadvantage that it has to commit
early. This may either lead the parser to add an arc (i, l, j) (i < j) when it is not correct
to do so, or fail to add the same arc in a situation where it should have been added, in
both cases because the information available at an early point makes the wrong decision
look probable. In the first case, the arc-standard parser may still get the analysis right,
if it also seems probable that j should have a right dependent (in which case it will
postpone the attachment); in the second case, it may get a second chance to add the arc
if it in fact adds a right dependent to j at a later point. It is not so easy to predict what
type of structures and annotation will favor the arc-standard parser in this way, but it
is likely that having many right dependents attached to (or near) the root could cause
problems for the arc-eager algorithms, since these dependencies determine the global
structure and often span long distances, which makes it harder to make correct decisions
early in the parsing process. This is consistent with earlier studies showing that parsers
using the arc-eager, stack-based algorithm tend to predict dependents of the root with
545
Computational Linguistics Volume 34, Number 4
lower precision than other algorithms.14 Interestingly, the three languages for which
the arc-standard parser has the highest improvement (Arabic, Czech, Slovene) have a
very similar annotation, based on the Prague school tradition of dependency grammar,
which not only allows multiple dependents of the root but also uses several different
labels for these dependents, which means that they will be analyzed correctly only if a
RIGHT-ARC transition is performed with the right label at exactly the right point in time.
This is in contrast to annotation schemes that use a default label ROOT, for dependents
of the root, where such dependents can often be correctly recovered in post-processing
by attaching all remaining roots to the special root node with the default label. We
can see the effect of this by comparing the two stack-based parsers (in their pseudo-
projective versions) with respect to precision and recall for the dependency type PRED
(predicate), which is the most important label for dependents of the root in the data sets
for Arabic, Czech, and Slovene. While the arc-standard parser has 78.02% precision and
70.22% recall, averaged over the three languages, the corresponding figures for the arc-
eager parser are as low as 68.93% and 65.93%, respectively, which represents a drop of
almost ten percentage points in precision and almost five percentage points in recall.
Summarizing the results of the accuracy evaluation, we have seen that all four algo-
rithms can be used for deterministic, classifier-based parsing with competitive accuracy.
The results presented are close to the state of the art without any optimization of feature
representations and learning algorithm parameters. Comparing different algorithms,
we have seen that the capacity to capture non-projective dependencies makes a signif-
icant difference in general, but with language-specific effects that depend primarily on
the frequency of non-projective constructions. We have also seen that the non-projective
list-based algorithm is more stable and predictable in this respect, compared to the use
of pseudo-projective parsing in combination with an essentially projective parsing algo-
rithm. Finally, we have observed quite strong language-specific effects for the difference
between arc-standard and arc-eager parsing for the stack-based algorithms, effects that
can be tied to differences in linguistic structure and annotation style between different
data sets, although a much more detailed error analysis is needed before we can draw
precise conclusions about the relative merits of different parsing algorithms for different
languages and syntactic representations.
6.3 Efficiency
Before we consider the evaluation of efficiency in both learning and parsing, it is
worth pointing out that the results will be heavily dependent on the choice of support
vector machines for classification, and cannot be directly generalized to the use of
deterministic incremental parsing algorithms together with other kinds of classifiers.
However, because support vector machines constitute the state of the art in classifier-
based parsing, it is still worth examining how learning and parsing times vary with the
parsing algorithm while parameters of learning and classification are kept constant.
Table 4 gives the results of the efficiency evaluation. Looking first at learning times,
it is obvious that learning time depends primarily on the number of training instances,
which is why we can observe a difference of several orders of magnitude in learning
time between the biggest training set (Czech) and the smallest training set (Slovene)
14 This is shown by Nivre and Scholz (2004) in comparison to the iterative, arc-standard algorithm of
Yamada and Matsumoto (2003) and by McDonald and Nivre (2007) in comparison to the spanning
tree algorithm of McDonald, Lerman, and Pereira (2006).
546
Nivre Deterministic Incremental Dependency Parsing
Table 4
Learning and parsing time for seven parsers on six languages, measured in seconds.
NP-L = non-projective list-based; P-L = projective list-based; PP-L = pseudo-projective list-based;
P-E = projective arc-eager stack-based; PP-E = pseudo-projective arc-eager stack-based; P-S =
projective arc-standard stack-based; PP-S = pseudo-projective arc-standard stack-based.
Learning Time
Language NP-L P-L PP-L P-E PP-E P-S PP-S
Arabic 1,814 614 603 650 647 1,639 1,636
Bulgarian 6,796 2,918 2,926 2,919 2,939 3,321 3,391
Chinese 17,034 13,019 13,019 13,029 13,029 13,705 13,705
Czech 546,880 250,560 248,511 279,586 280,069 407,673 406,857
Danish 2,964 1,248 1,260 1,246 1,262 643 647
Dutch 7,701 3,039 2,966 3,055 2,965 7,000 6,812
German 48,699 16,874 17,600 16,899 17,601 24,402 24,705
Japanese 211 191 188 203 208 199 199
Portuguese 25,621 8,433 8,336 8,436 8,335 7,724 7,731
Slovene 167 78 90 93 99 86 90
Spanish 1,999 562 566 565 565 960 959
Swedish 2,410 942 1,020 945 1,022 1,350 1,402
Turkish 720 498 519 504 516 515 527
Average 105,713 46,849 46,616 51,695 51,876 74,798 74,691
Parsing Time
Language NP-L P-L PP-L P-E PP-E P-S PP-S
Arabic 213 103 131 108 135 196 243
Bulgarian 139 93 102 93 103 135 147
Chinese 1,008 855 855 855 855 803 803
Czech 5,244 3,043 5,889 3,460 6,701 3,874 7,437
Danish 109 66 83 66 83 82 106
Dutch 349 209 362 211 363 253 405
German 781 456 947 455 945 494 1,004
Japanese 10 8 8 9 10 7 7
Portuguese 670 298 494 298 493 437 717
Slovene 69 44 62 47 65 43 64
Spanish 133 67 75 67 75 80 91
Swedish 286 202 391 201 391 242 456
Turkish 218 162 398 162 403 153 380
Average 1,240 712 1,361 782 1,496 897 1,688
for a given parsing algorithm. Broadly speaking, for any given parsing algorithm, the
ranking of languages with respect to learning time follows the ranking with respect
to training set size, with a few noticeable exceptions. Thus, learning times are shorter
than expected, relative to other languages, for Swedish and Japanese, but longer than
expected for Arabic and (except in the case of the arc-standard parsers) for Danish.
However, the number of training instances for the SVM learner depends not only
on the number of tokens in the training set, but also on the number of transitions
required to parse a sentence of length n. This explains why the non-projective list-based
algorithm, with its quadratic complexity, consistently has longer learning times than
the linear stack-based algorithms. However, it can also be noted that the projective, list-
based algorithm, despite having the same worst-case complexity as the non-projective
547
Computational Linguistics Volume 34, Number 4
algorithm, in practice behaves much more like the arc-eager stack-based algorithm and
in fact has a slightly lower learning time than the latter on average. The arc-standard
stack-based algorithm, finally, again shows much more variation than the other algo-
rithms. On average, it is slower to train than the arc-eager algorithm, and sometimes
very substantially so, but for a few languages (Danish, Japanese, Portuguese, Slovene)
it is actually faster (and considerably so for Danish). This again shows that learning time
depends on other properties of the training sets than sheer size, and that some data sets
may be more easily separable for the SVM learner with one parsing algorithm than with
another.
It is noteworthy that there are no consistent differences in learning time between
the strictly projective parsers and their pseudo-projective counterparts, despite the fact
that the pseudo-projective technique increases the number of distinct classes (because of
its augmented arc labels), which in turn increases the number of binary classifiers that
need to be trained in order to perform multi-class classification with the one-versus-one
method. The number of classifiers is
m(m?1)
2 , where m is the number of classes, and the
pseudo-projective technique with the encoding scheme used here can theoretically lead
to a quadratic increase in the number of classes. The fact that this has no noticeable effect
on efficiency indicates that learning time is dominated by other factors, in particular the
number of training instances.
Turning to parsing efficiency, we may first note that parsing time is also dependent
on the size of the training set, through a dependence on the number of support vectors,
which tend to grow with the size of the training set. Thus, for any given algorithm, there
is a strong tendency that parsing times for different languages follow the same order as
training set sizes. The notable exceptions are Arabic, Turkish, and Chinese, which have
higher parsing times than expected (relative to other languages), and Japanese, where
parsing is surprisingly fast. Because these deviations are the same for all algorithms, it
seems likely that they are related to specific properties of these data sets. It is also worth
noting that for Arabic and Japanese the deviations are consistent across learning and
parsing (slower than expected for Arabic, faster than expected for Japanese), whereas
for Chinese there is no consistent trend (faster than expected in learning, slower than
expected in parsing).
Comparing algorithms, we see that the non-projective list-based algorithm is slower
than the strictly projective stack-based algorithms, which can be expected from the
difference in time complexity. But we also see that the projective list-based algorithm,
despite having the same worst-case complexity as the non-projective algorithm, in
practice behaves like the linear-time algorithms and is in fact slightly faster on average
than the arc-eager stack-based algorithm, which in turn outperforms the arc-standard
stack-based algorithm. This is consistent with the results from oracle parsing reported in
Nivre (2006a), which show that, with the constraint of projectivity, the relation between
sentence length and number of transitions for the list-based parser can be regarded
as linear in practice. Comparing the arc-eager and the arc-standard variants of the
stack-based algorithm, we find the same kind of pattern as for learning time in that
the arc-eager parser is faster for all except a small set of languages: Chinese, Japanese,
Slovene, and Turkish. Only two of these, Japanese and Slovene, are languages for which
learning is also faster with the stack-based algorithm, which again shows that there is
no straightforward correspondence between learning time and parsing time.
Perhaps the most interesting result of all, as far as efficiency is concerned, is to be
found in the often dramatic differences in parsing time between the strictly projective
parsers and their pseudo-projective counterparts. Although we did not see any clear
effect of the increased number of classes, hence classifiers, on learning time earlier, it is
548
Nivre Deterministic Incremental Dependency Parsing
quite clear that there is a noticeable effect on parsing time, with the pseudo-projective
parsers always being substantially slower. In fact, in some cases the pseudo-projective
parsers are also slower than the non-projective list-based parser, despite the difference
in time complexity that exists at least for the stack-based parsers. This result holds on
average over all languages and for five out of thirteen of the individual languages and
shows that the advantage of linear-time parsing complexity (for the stack-based parsers)
can be outweighed by the disadvantage of a more complex classification problem in
pseudo-projective parsing. In other words, the larger constant associated with a larger
cohort of SVM classifiers for the pseudo-projective parser can be more important than
the better asymptotic complexity of the linear-time algorithm in the range of sentence
lengths typically found in natural language. Looking more closely at the variation in
sentence length across languages, we find that the pseudo-projective parsers are faster
than the non-projective parser for all data sets with an average sentence length above
18. For data sets with shorter sentences, the non-projective parser is more efficient in all
except three cases: Bulgarian, Chinese, and Japanese. For Chinese this is easily explained
by the absence of non-projective dependencies, making the performance of the pseudo-
projective parsers identical to their strictly projective counterparts. For the other two
languages, the low number of distinct dependency labels for Japanese and the low per-
centage of non-projective dependencies for Bulgarian are factors that mitigate the effect
of enlarging the set of dependency labels in pseudo-projective parsing. We conclude
that the relative efficiency of non-projective and pseudo-projective parsing depends
on several factors, of which sentence length appears to be the most important, but
where the number of distinct dependency labels and the percentage of non-projective
dependencies also play a role.
7. Related Work
Data-driven dependency parsing using supervised machine learning was pioneered by
Eisner (1996), who showed how traditional chart parsing techniques could be adapted
for dependency parsing to give efficient parsing with exact inference over a probabilistic
model where the score of a dependency tree is the sum of the scores of individual arcs.
This approach has been further developed in particular by Ryan McDonald and his
colleagues (McDonald, Crammer, and Pereira 2005; McDonald et al 2005; McDonald
and Pereira 2006) and is now known as spanning tree parsing, because the problem
of finding the most probable tree under this type of model is equivalent to finding
an optimum spanning tree in a dense graph containing all possible dependency arcs.
If we assume that the score of an individual arc is independent of all other arcs, this
problem can be solved efficiently for arbitrary non-projective dependency trees using
the Chu-Liu-Edmonds algorithm, as shown by McDonald et al (2005). Spanning tree
algorithms have so far primarily been combined with online learning methods such as
MIRA (McDonald, Crammer, and Pereira 2005).
The approach of deterministic classifier-based parsing was first proposed for
Japanese by Kudo and Matsumoto (2002) and for English by Yamada and Matsumoto
(2003). In contrast to spanning tree parsing, this can be characterized as a greedy
inference strategy, trying to construct a globally optimal dependency graph by making
a sequence of locally optimal decisions. The first strictly incremental parser of this kind
was described in Nivre (2003) and used for classifier-based parsing of Swedish by Nivre,
Hall, and Nilsson (2004) and English by Nivre and Scholz (2004). Altogether it has now
been applied to 19 different languages (Nivre et al 2006, 2007; Hall et al 2007). Most
algorithms in this tradition are restricted to projective dependency graphs, but it is
549
Computational Linguistics Volume 34, Number 4
possible to recover non-projective dependencies using pseudo-projective parsing
(Nivre and Nilsson 2005). More recently, algorithms for non-projective classifier-based
parsing have been proposed by Attardi (2006) and Nivre (2006a). The strictly deter-
ministic parsing strategy has been relaxed in favor of n-best parsing by Johansson
and Nugues (2006), among others. The dominant learning method in this tradition is
support vector machines (Kudo and Matsumoto 2002; Yamada and Matsumoto 2003;
Nivre et al 2006) but memory-based learning has also been used (Nivre, Hall, and
Nilsson 2004; Nivre and Scholz 2004; Attardi 2006).
Of the algorithms described in this article, the arc-eager stack-based algorithm is
essentially the algorithm proposed for unlabeled dependency parsing in Nivre (2003),
extended to labeled dependency parsing in Nivre, Hall, and Nilsson (2004), and most
fully described in Nivre (2006b). The major difference is that the parser is now initialized
with the special root node on the stack, whereas earlier formulations had an empty
stack at initialization.15 The arc-standard stack-based algorithm is briefly described in
Nivre (2004) but can also be seen as an incremental version of the algorithm of Yamada
and Matsumoto (2003), where incrementality is achieved by only allowing one left-to-
right pass over the input, whereas Yamada and Matsumoto perform several iterations
in order to construct the dependency graph bottom-up, breadth-first as it were. The
list-based algorithms are both inspired by the work of Covington (2001), although the
formulations are not equivalent. They have previously been explored for deterministic
classifier-based parsing in Nivre (2006a, 2007). A more orthodox implementation of
Covington?s algorithms for data-driven dependency parsing is found in Marinov (2007).
8. Conclusion
In this article, we have introduced a formal framework for deterministic incremental
dependency parsing, where parsing algorithms can be defined in terms of transition
systems that are deterministic only together with an oracle for predicting the next
transition. We have used this framework to analyze four different algorithms, proving
the correctness of each algorithm relative to a relevant class of dependency graphs, and
giving complexity results for each algorithm.
To complement the formal analysis, we have performed an experimental evaluation
of accuracy and efficiency, using SVM classifiers to approximate oracles, and using data
from 13 languages. The comparison shows that although strictly projective dependency
parsing is most efficient both in learning and in parsing, the capacity to produce non-
projective dependency graphs leads to better accuracy unless it can be assumed that
all structures are strictly projective. The evaluation also shows that using the non-
projective, list-based parsing algorithm gives a more stable improvement in this respect
than applying the pseudo-projective parsing technique to a strictly projective parsing
algorithm. Moreover, despite its quadratic time complexity, the non-projective parser is
often as efficient as the pseudo-projective parsers in practice, because the extended set
of dependency labels used in pseudo-projective parsing slows down classification. This
demonstrates the importance of complementing the theoretical analysis of complexity
with practical running time experiments.
Although the non-projective, list-based algorithm can be said to give the best trade-
off between accuracy and efficiency when results are averaged over all languages in the
sample, we have also observed important language-specific effects. In particular, the
15 The current version was first used in the CoNLL-X shared task (Nivre et al 2006).
550
Nivre Deterministic Incremental Dependency Parsing
arc-eager strategy inherent not only in the arc-eager, stack-based algorithm but also in
both versions of the list-based algorithm appears to be suboptimal for some languages
and syntactic representations. In such cases, using the arc-standard parsing strategy,
with or without pseudo-projective parsing, may lead to significantly higher accuracy.
More research is needed to determine exactly which properties of linguistic structures
and their syntactic analysis give rise to these effects.
On the whole, however, the four algorithms investigated in this article give very
similar performance both in terms of accuracy and efficiency, and several previous
studies have shown that both the stack-based and the list-based algorithms can achieve
state-of-the-art accuracy together with properly trained classifiers (Nivre et al 2006;
Nivre 2007; Hall et al 2007).
Acknowledgments
I want to thank my students Johan Hall and
Jens Nilsson for fruitful collaboration and for
their contributions to the MaltParser system,
which was used for all experiments. I also
want to thank Sabine Buchholz, Matthias
Buch-Kromann, Walter Daelemans, Gu?ls?en
Eryig?it, Jason Eisner, Jan Hajic?, Sandra
Ku?bler, Marco Kuhlmann, Yuji Matsumoto,
Ryan McDonald, Kemal Oflazer, Kenji Sagae,
Noah A. Smith, and Deniz Yuret for useful
discussions on topics relevant to this article.
I am grateful to three anonymous reviewers
for many helpful suggestions that helped
improve the final version of the article. The
work has been partially supported by the
Swedish Research Council.
References
Abney, Steven and Mark Johnson. 1991.
Memory requirements and local
ambiguities of parsing strategies. Journal
of Psycholinguistic Research, 20:233?250.
Aho, Alfred V., Ravi Sethi, and Jeffrey D.
Ullman. 1986. Compilers: Principles,
Techniques, and Tools. Addison-Wesley,
Reading, MA.
Attardi, Giuseppe. 2006. Experiments
with a multilanguage non-projective
dependency parser. In Proceedings of
the Tenth Conference on Computational
Natural Language Learning (CoNLL-X),
pages 166?170, New York.
Bo?hmova?, Alena, Jan Hajic?, Eva Hajic?ova?,
and Barbora Hladka?. 2003. The Prague
Dependency Treebank: A three-level
annotation scenario. In Anne Abeille?,
editor, Treebanks: Building and Using
Parsed Corpora. Kluwer Academic
Publishers, Dordrecht, pages 103?127.
Buchholz, Sabine and Erwin Marsi. 2006.
CoNLL-X shared task on multilingual
dependency parsing. In Proceedings of
the Tenth Conference on Computational
Natural Language Learning, pages 149?164,
New York.
Chang, Chih-Chung and Chih-Jen Lin,
2001. LIBSVM: A Library for Support
Vector Machines. Software available
at http://www.csie.ntu.edu.tw/
?cjlin/libsvm.
Cheng, Yuchang, Masayuki Asahara,
and Yuji Matsumoto. 2005. Machine
learning-based dependency analyzer for
Chinese. In Proceedings of International
Conference on Chinese Computing (ICCC),
pages 66?73, Singapore.
Cormen, Thomas H., Charles E. Leiserson,
and Ronald L. Rivest. 1990. Introduction to
Algorithms. MIT Press, Cambridge, MA.
Covington, Michael A. 2001. A fundamental
algorithm for dependency parsing. In
Proceedings of the 39th Annual ACM
Southeast Conference, pages 95?102,
Athens, GA.
Eisner, Jason M. 1996. Three new
probabilistic models for dependency
parsing: An exploration. In Proceedings
of the 16th International Conference on
Computational Linguistics (COLING),
pages 340?345, Copenhagen.
Hajic?, Jan, Barbora Vidova Hladka, Jarmila
Panevova?, Eva Hajic?ova?, Petr Sgall, and
Petr Pajas. 2001. Prague Dependency
Treebank 1.0. LDC, 2001T10.
Hall, J., J. Nilsson, J. Nivre, G. Eryig?it,
B. Megyesi, M. Nilsson, and M. Saers.
2007. Single malt or blended? A study
in multilingual parser optimization. In
Proceedings of the CoNLL shared task of
EMNLP-CoNLL 2007, pages 933?939,
Prague.
Hall, Johan, Joakim Nivre, and Jens Nilsson.
2006. Discriminative classifiers for
deterministic dependency parsing. In
Proceedings of the COLING/ACL 2006 Main
Conference Poster Sessions, pages 316?323,
Sydney.
551
Computational Linguistics Volume 34, Number 4
Hudson, Richard A. 1990. English Word
Grammar. Blackwell, Oxford.
Johansson, Richard and Pierre Nugues.
2006. Investigating multilingual
dependency parsing. In Proceedings
of the Tenth Conference on Computational
Natural Language Learning (CoNLL-X),
pages 206?210, New York.
Kalt, Tom. 2004. Induction of greedy
controllers for deterministic treebank
parsers. In Proceedings of the Conference
on Empirical Methods in Natural Language
Processing (EMNLP), pages 17?24,
Barcelona.
Kudo, Taku and Yuji Matsumoto. 2002.
Japanese dependency analysis using
cascaded chunking. In Proceedings of the
Sixth Workshop on Computational Language
Learning (CoNLL), pages 63?69, Taipei.
Marcus, Mitchell P. 1980. A Theory of Syntactic
Recognition for Natural Language. MIT
Press, Cambridge, MA.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19:313?330.
Marcus, Mitchell P., Beatrice Santorini,
Mary Ann Marcinkiewicz, Robert
MacIntyre, Ann Bies, Mark Ferguson,
Karen Katz, and Britta Schasberger.
1994. The Penn Treebank: Annotating
predicate-argument structure. In
Proceedings of the ARPA Human Language
Technology Workshop, pages 114?119,
Plainsboro, NJ.
Marinov, S. 2007. Covington variations. In
Proceedings of the CoNLL Shared Task of
EMNLP-CoNLL 2007, pages 1144?1148,
Prague.
McDonald, Ryan, Koby Crammer, and
Fernando Pereira. 2005. Online
large-margin training of dependency
parsers. In Proceedings of the 43rd
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 91?98, Ann Arbor, MI.
McDonald, Ryan, Kevin Lerman, and
Fernando Pereira. 2006. Multilingual
dependency analysis with a two-stage
discriminative parser. In Proceedings
of the Tenth Conference on Computational
Natural Language Learning (CoNLL),
pages 216?220.
McDonald, Ryan and Joakim Nivre. 2007.
Characterizing the errors of data-driven
dependency parsing models. In Proceedings
of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and
Computational Natural Language Learning
(EMNLP-CoNLL), pages 122?131, Prague.
McDonald, Ryan and Fernando Pereira.
2006. Online learning of approximate
dependency parsing algorithms. In
Proceedings of the 11th Conference of the
European Chapter of the Association for
Computational Linguistics (EACL),
pages 81?88, Trento.
McDonald, Ryan, Fernando Pereira,
Kiril Ribarov, and Jan Hajic?. 2005.
Non-projective dependency parsing using
spanning tree algorithms. In Proceedings of
the Human Language Technology Conference
and the Conference on Empirical Methods in
Natural Language Processing (HLT/EMNLP),
pages 523?530, Vancouver.
Mel?c?uk, Igor. 1988. Dependency Syntax:
Theory and Practice. State University of
New York Press, New York.
Nilsson, Jens, Joakim Nivre, and Johan Hall.
2007. Generalizing tree transformations
for inductive dependency parsing. In
Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics
(ACL), pages 968?975, Prague.
Nivre, Joakim. 2003. An efficient algorithm
for projective dependency parsing.
In Proceedings of the 8th International
Workshop on Parsing Technologies (IWPT),
pages 149?160, Nancy.
Nivre, Joakim. 2004. Incrementality in
deterministic dependency parsing. In
Proceedings of the Workshop on Incremental
Parsing: Bringing Engineering and Cognition
Together (ACL), pages 50?57, Barcelona.
Nivre, Joakim. 2006a. Constraints on
non-projective dependency graphs.
In Proceedings of the 11th Conference
of the European Chapter of the Association
for Computational Linguistics (EACL),
pages 73?80, Trento.
Nivre, Joakim. 2006b. Inductive Dependency
Parsing. Springer, Dordrecht.
Nivre, Joakim. 2007. Incremental
non-projective dependency parsing. In
Proceedings of Human Language Technologies:
The Annual Conference of the North American
Chapter of the Association for Computational
Linguistics (NAACL HLT), pages 396?403,
Rochester, NY.
Nivre, Joakim, Johan Hall, and Jens Nilsson.
2004. Memory-based dependency parsing.
In Proceedings of the 8th Conference on
Computational Natural Language Learning
(CoNLL), pages 49?56, Boston, MA.
Nivre, Joakim, Johan Hall, Jens Nilsson,
Atanas Chanev, Gu?ls?en Eryig?it, Sandra
Ku?bler, Svetoslav Marinov, and
552
Nivre Deterministic Incremental Dependency Parsing
Erwin Marsi. 2007. MaltParser: A
language-independent system for
data-driven dependency parsing.
Natural Language Engineering, 13:95?135.
Nivre, Joakim, Johan Hall, Jens Nilsson,
Gu?lsen Eryig?it, and Svetoslav Marinov.
2006. Labeled pseudo-projective
dependency parsing with support
vector machines. In Proceedings of
the Tenth Conference on Computational
Natural Language Learning (CoNLL),
pages 221?225, New York, NY.
Nivre, Joakim and Jens Nilsson. 2005.
Pseudo-projective dependency parsing.
In Proceedings of the 43rd Annual Meeting
of the Association for Computational
Linguistics (ACL), pages 99?106,
Ann Arbor, MI.
Nivre, Joakim and Mario Scholz. 2004.
Deterministic dependency parsing
of English text. In Proceedings of the
20th International Conference on
Computational Linguistics (COLING),
pages 64?70, Geneva.
Ratnaparkhi, Adwait. 1997. A linear
observed time statistical parser based
on maximum entropy models. In
Proceedings of the Second Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 1?10,
Providence, RI.
Ratnaparkhi, Adwait. 1999. Learning to
parse natural language with maximum
entropy models.Machine Learning,
34:151?175.
Sagae, Kenji and Alon Lavie. 2005. A
classifier-based parser with linear
run-time complexity. In Proceedings of the
9th International Workshop on Parsing
Technologies (IWPT), pages 125?132,
Vancouver.
Sagae, Kenji and Alon Lavie. 2006. A
best-first probabilistic shift-reduce
parser. In Proceedings of the COLING/ACL
2006 Main Conference Poster Sessions,
pages 691?698, Sydney.
Sgall, Petr, Eva Hajic?ova?, and Jarmila
Panevova?. 1986. The Meaning of the Sentence
in Its Pragmatic Aspects. Reidel, Dordrecht.
Shieber, Stuart M. 1983. Sentence
disambiguation by a shift-reduce parsing
technique. In Proceedings of the 21st
Conference on Association for Computational
Linguistics (ACL), pages 113?118,
Cambridge, MA.
Shieber, Stuart M., Yves Schabes, and
Fernando C. N. Pereira. 1995. Principles
and implementation of deductive parsing.
Journal of Logic Programming, 24:3?36.
Tesnie`re, Lucien. 1959. E?le?ments de syntaxe
structurale. Editions Klincksieck, Paris.
Yamada, Hiroyasu and Yuji Matsumoto.
2003. Statistical dependency analysis with
support vector machines. In Proceedings of
the 8th International Workshop on Parsing
Technologies (IWPT), pages 195?206, Nancy.
553

Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 1?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
The CoNLL-2009 Shared Task:
Syntactic and Semantic Dependencies in Multiple Languages
Jan Hajic?? Massimiliano Ciaramita? Richard Johansson? Daisuke Kawahara?
Maria Anto`nia Mart???? Llu??s Ma`rquez?? Adam Meyers?? Joakim Nivre?? Sebastian Pado???
Jan ?Ste?pa?nek? Pavel Stran?a?k? Mihai Surdeanu?? Nianwen Xue?? Yi Zhang??
?: Charles University in Prague, {hajic,stepanek,stranak}@ufal.mff.cuni.cz
?: Google Inc., massi@google.com
?: University of Trento, johansson@disi.unitn.it
?: National Institute of Information and Communications Technology, dk@nict.go.jp
??: University of Barcelona, amarti@ub.edu
??: Technical University of Catalonia, Barcelona, lluism@lsi.upc.edu
??: New York University, meyers@cs.nyu.edu
??: Uppsala University and Va?xjo? University, joakim.nivre@lingfil.uu.se
??: Stuttgart University, pado@ims.uni-stuttgart.de
??: Stanford University, mihais@stanford.edu
??: Brandeis University, xuen@brandeis.edu
??: Saarland University, yzhang@coli.uni-sb.de
Abstract
For the 11th straight year, the Conference
on Computational Natural Language Learn-
ing has been accompanied by a shared task
whose purpose is to promote natural language
processing applications and evaluate them in
a standard setting. In 2009, the shared task
was dedicated to the joint parsing of syntac-
tic and semantic dependencies in multiple lan-
guages. This shared task combines the shared
tasks of the previous five years under a unique
dependency-based formalism similar to the
2008 task. In this paper, we define the shared
task, describe how the data sets were created
and show their quantitative properties, report
the results and summarize the approaches of
the participating systems.
1 Introduction
Every year since 1999, the Conference on Com-
putational Natural Language Learning (CoNLL)
launches a competitive, open ?Shared Task?. A
common (?shared?) task is defined and datasets are
provided for its participants. In 2004 and 2005, the
shared tasks were dedicated to semantic role label-
ing (SRL) in a monolingual setting (English). In
2006 and 2007 the shared tasks were devoted to
the parsing of syntactic dependencies, using corpora
from up to 13 languages. In 2008, the shared task
(Surdeanu et al, 2008) used a unified dependency-
based formalism, which modeled both syntactic de-
pendencies and semantic roles for English. The
CoNLL-2009 Shared Task has built on the 2008 re-
sults by providing data for six more languages (Cata-
lan, Chinese, Czech, German, Japanese and Span-
ish) in addition to the original English1. It has thus
naturally extended the path taken by the five most
recent CoNLL shared tasks.
As in 2008, the CoNLL-2009 shared task com-
bined dependency parsing and the task of identify-
ing and labeling semantic arguments of verbs (and
other parts of speech whenever available). Partici-
pants had to choose from two tasks:
? Joint task (syntactic dependency parsing and
semantic role labeling), or
? SRL-only task (syntactic dependency parses
have been provided by the organizers, using
state-of-the art parsers for the individual lan-
guages).
1There are some format changes and deviations from the
2008 task data specification; see Sect. 2.3
1
In contrast to the previous year, the evaluation data
indicated which words were to be dealt with (for the
SRL task). In other words, (predicate) disambigua-
tion was still part of the task, whereas the identi-
fication of argument-bearing words was not. This
decision was made to compensate for the significant
differences between languages and between the an-
notation schemes used.
The ?closed? and ?open? challenges have been
kept from last year as well; participants could have
chosen one or both. In the closed challenge, systems
had to be trained strictly with information contained
in the given training corpus; in the open challenge,
systems could have been developed making use of
any kind of external tools and resources.
This paper is organized as follows. Section 2 de-
fines the task, including the format of the data, the
evaluation metrics, and the two challenges. A sub-
stantial portion of the paper (Section 3) is devoted
to the description of the conversion and develop-
ment of the data sets in the additional languages.
Section 4 shows the main results of the submitted
systems in the Joint and SRL-only tasks. Section 5
summarizes the approaches implemented by partic-
ipants. Section 6 concludes the paper. In all sec-
tions, we will mention some of the differences be-
tween last year?s and this year?s tasks while keeping
the text self-contained whenever possible; for details
and observations on the English data, please refer to
the overview paper of the CoNLL-2008 Shared Task
(Surdeanu et al, 2008) and to the references men-
tioned in the sections describing the other languages.
2 Task Definition
In this section we provide the definition of the shared
task; after introducing the two challenges and the
two tasks the participants were to choose, we con-
tinue with the format of the shared task data, fol-
lowed by a description of the evaluation metrics
used.
For three of the languages (Czech, English and
German), out-of-domain data (OOD) have also been
prepared for the final evaluation, following the same
guidelines and formats.
2.1 Closed and Open Challenges
Similarly to the CoNLL-2005 and CoNLL-2008
shared tasks, this shared task evaluation is separated
into two challenges:
Closed Challenge The aim of this challenge was to
compare performance of the participating systems in
a fair environment. Systems had to be built strictly
with information contained in the given training cor-
pus, and tuned with the development section. In
addition, the lexical frame files (such as the Prop-
Bank and NomBank for English, the valency dictio-
nary PDT-Vallex for Czech etc.) were provided and
may have been used. These restrictions mean that
outside parsers (not trained by the participants? sys-
tems) could not be used. However, we did provide
the output of a single, state-of-the-art dependency
parser for each language so that participants could
build a SRL-only system (using the provided parses
as inputs) within the closed challenge (as opposed to
the 2008 shared task).
Open Challenge Systems could have been devel-
oped making use of any kind of external tools and
resources. The only condition was that such tools or
resources must not have been developed with the an-
notations of the test set, both for the input and output
annotations of the data. In this challenge, we were
interested in learning methods which make use of
any tools or resources that might improve the per-
formance. The comparison of different systems in
this setting may not be fair, and thus ranking of sys-
tems is not necessarily important.
2.2 Joint and SRL-only tasks
In 2008, systems participating in the open challenge
could have used state-of-the-art parsers for the syn-
tactic dependency part of the task. This year, we
have provided the output of these parsers for all the
languages in an uniform way, thus allowing an or-
thogonal combination of the two tasks and the two
challenges. For the SRL-only task, participants in
the closed challenge simply had to use the provided
parses only.
Despite the provisions for the SRL-only task, we
are more interested in the approaches and results of
the Joint task. Therefore, primary system ranking is
provided for the Joint task while additional measures
2
are computed for various combinations of parsers
and SRL methods across the tasks and challenges.
2.3 Data Format
The data format used in this shared task has been
based on the CoNLL-2008 shared task, with some
differences. The data follows these general rules:
? The files contain sentences separated by a blank
line.
? A sentence consists of one or more tokens and
the information for each token is represented on
a separate line.
? A token consists of at least 14 fields. The fields
are separated by one or more whitespace char-
acters (spaces or tabs). Whitespace characters
are not allowed within fields.
The data is thus a large table with whitespace-
separated fields (columns). The fields provided in
the data are described in Table 1. They are identical
for all languages, but they may differ in contents;
for example, some fields might not be filled for all
the languages provided (such as the FEAT or PFEAT
fields).
For the SRL-only task, participants have been
provided will all the data but the PRED and
APREDs, which they were supposed to fill in with
their correct values. However, they did not have
to determine which tokens are predicates (or more
precisely, which are the argument-bearing tokens),
since they were marked by ?Y? in the FILLPRED
field.
For the Joint task, participants could not (in ad-
dition to the PRED and APREDs) see the gold-
standard nor the predicted syntactic dependencies
(HEAD, PHEAD) and their labels (DEPREL, PDE-
PREL). These syntactic dependencies were also to
be filled by participants? systems.
In both tasks, participants have been free to
use any other data (columns) provided, except the
LEMMA, POS and FEAT columns (to get more ?re-
alistic? results using only their automatically pre-
dicted variants PLEMMA, PPOS and PFEAT).
Besides the corpus proper, predicate dictionaries
have been provided to participants in order to be able
to properly match the predicates to the tokens in the
corpus; their contents could have been used e.g. as
features for the PRED/APREDs predictions (or even
for the syntactic dependencies, i.e., for filling in the
PHEAD and PDEPREL fields).
The system of filling-in the APREDs follows
the 2008 pattern; for each argument-bearing token
(predicate), a new APREDn column is created in the
order in which the predicate token is encountered
within the sentence (i.e., based on its ID seen as a
numerical value). Then, for each token in the sen-
tence, the value in the intersection of the APREDn
column and the token row is either left unfilled
(if the token is not an argument), or a predicate-
argument label(s) is(are) filled in.
The differences between the English-only 2008
task and this year?s multilingual task can be briefly
summarized as follows:
? only ?split?2 lemmas and forms have been pro-
vided in the English datasets (for the other lan-
guages, original tokenization from the respec-
tive treebanks has been used);
? rich morphological features have been added
wherever available;
? syntactic dependencies by state-of-the-art
parsers have been provided (for the SRL-only
task);
? multiple semantic labels for a single token have
been allowed (and properly evaluated) in the
APREDs columns;
? predicates have been pre-identified and marked
in both the training and test data;
? some of the fields (e.g. the APREDx) and val-
ues (ARG0? A0 etc.) have been renamed.
2.4 Evaluation Measures
It was required that participants submit results in all
seven languages in the chosen task and in any of (or
both) the challenges. Submission of out-of-domain
data files has been optional.
The main evaluation measure, according to which
systems are primarily compared, is the Joint task,
2Splitting of forms and lemmas in English has been intro-
duced in the 2008 shared task to match the tokenization con-
vention for the arguments in NomBank.
3
Field # Name Description
1 ID Token counter, starting at 1 for each new sentence
2 FORM Form or punctuation symbol (the token; ?split? for English)
3 LEMMA Gold-standard lemma of FORM
4 PLEMMA Automatically predicted lemma of FORM
5 POS Gold-standard POS (major POS only)
6 PPOS Automatically predicted major POS by a language-specific tagger
7 FEAT Gold-standard morphological features (if applicable)
8 PFEAT Automatically predicted morphological features (if applicable)
9 HEAD Gold-standard syntactic head of the current token (ID or 0 if root)
10 PHEAD Automatically predicted syntactic head
11 DEPREL Gold-standard syntactic dependency relation (to HEAD)
12 PDEPREL Automatically predicted dependency relation to PHEAD
13 FILLPRED Contains ?Y? for argument-bearing tokens
14 PRED (sense) identifier of a semantic ?predicate? coming from a current token
15... APREDn Columns with argument labels for each semantic predicate (in the ID order)
Table 1: Description of the fields (columns) in the data provided. The values of columns 9, 11 and 14 and above are
not provided in the evaluation data; for the Joint task, columns 9?12 are also empty in the evaluation data.
closed challenge, Macro F1 score. However, scores
can also be computed for a number of other condi-
tions:
? Task: Joint or SRL-only
? Challenge: open or closed
? Domain: in-domain data (IDD, separated from
training corpus) or out-of-domain data (OOD)
Joint task participants are also evaluated separately
on the syntactic dependency task (labeled attach-
ment score, LAS). Finally, systems competing in
both tasks are compared on semantic role labeling
alone, to assess the impact of the the joint pars-
ing/SRL task compared to an SRL-only task on pre-
parsed data.
Finally, as an explanatory measure, precision and
recall of the semantic labeling task have been com-
puted and tabulated.
We have decided to omit several evaluation fig-
ures that were reported in previous years, such as the
percentage of completely correct sentences (?Exact
Match?), unlabeled scores, etc. With seven lan-
guages, two tasks (plus two challenges, and the
IDD/OOD distinction), there are enough results to
get lost even as it is.
2.4.1 Syntactic Dependency Measures
The LAS score is defined similarly as in the pre-
vious shared tasks, as the percentage of tokens for
which a system has predicted the correct HEAD and
DEPREL columns. The unlabeled attachment score
(UAS), i.e., the percentage of tokens with correct
HEAD regardless if the DEPREL is correct, has not
been officially computed this year. No precision and
recall measures are applicable, since all systems are
supposed to output a single dependency with a single
label (see also below the footnote to the description
of the combined score).
2.4.2 Semantic Labeling Measures
The semantic propositions are evaluated by con-
verting them to semantic dependencies, i.e., we cre-
ate n semantic dependencies from every predicate
to its n arguments. These dependencies are labeled
with the labels of the corresponding arguments. Ad-
ditionally, we create a semantic dependency from
each predicate to a virtual ROOT node. The latter
dependencies are labeled with the predicate senses.
This approach guarantees that the semantic depen-
dency structure conceptually forms a single-rooted,
connected (but not necessarily acyclic) graph. More
importantly, this scoring strategy implies that if a
system assigns the incorrect predicate sense, it still
receives some points for the arguments correctly as-
signed. For example, for the correct proposition:
verb.01: A0, A1, AM-TMP
the system that generates the following output for
the same argument tokens:
4
verb.02: A0, A1, AM-LOC
receives a labeled precision score of 2/4 because two
out of four semantic dependencies are incorrect: the
dependency to ROOT is labeled 02 instead of 01
and the dependency to the AM-TMP is incorrectly la-
beled AM-LOC. Using this strategy we compute pre-
cision, recall, and F1 scores for semantic dependen-
cies (labeled only).
For some languages (Czech, Japanese) there may
be more than one label in a given argument position;
for example, this happens in Czech in special cases
of reciprocity when the same token serves as two or
more arguments to the same predicate. The scorer
takes this into account and considers such cases to
be (as if) multiple predicate-argument relations for
the computation of the evaluation measures.
For example, for the correct proposition:
v1f1: ACT|EFF, ADDR
the system that generates the following output for
the same argument tokens:
v1f1: ACT, ADDR|PAT
receives a labeled precision score of 3/4 because
the PAT is incorrect and labeled recall 3/4 be-
cause the EFF is missing (should the ACT|EFF and
ADDR|PAT be taken as atomic values, the scores
would then be zero).
2.4.3 Combined Syntactic and Semantic Score
We combine the syntactic and semantic measures
into one global measure using macro averaging. We
compute macro precision and recall scores by aver-
aging the labeled precision and recall for semantic
dependencies with the LAS for syntactic dependen-
cies:3
LMP = Wsem ? LPsem + (1?Wsem) ? LAS (1)
LMR = Wsem ? LRsem + (1 ?Wsem) ? LAS (2)
where LMP is the labeled macro precision and
LPsem is the labeled precision for semantic depen-
dencies. Similarly, LMR is the labeled macro re-
call and LRsem is the labeled recall for semantic
dependencies. Wsem is the weight assigned to the
3We can do this because the LAS for syntactic dependen-
cies is a special case of precision and recall, where the predicted
number of dependencies is equal to the number of gold depen-
dencies.
semantic task.4 The macro labeled F1 score, which
was used for the ranking of the participating sys-
tems, is computed as the harmonic mean of LMP
and LMR.
3 Data
The unification of the data formats for the various
languages appeared to be a challenge in itself. We
will briefly describe the processes of the conversion
of the existing treebanks in the seven languages of
the CoNLL-2009 shared task. In many instances,
the original treebanks had to be not only converted
format-wise, but also merged with other resources in
order to generate useful training and testing data that
fit the task description.
3.1 The Input Corpora
The data used as the input for the transformations
aimed at arriving at the data contents and format de-
scribed in Sect. 2.3 are described in (Taule? et al,
2008), (Xue and Palmer, 2009), (Hajic? et al, 2006),
(Surdeanu et al, 2008), (Burchardt et al, 2006) and
(Kawahara et al, 2002).
In the subsequent sections, the procedures for the
data conversion for the individual languages are de-
scribed. The data has been collected by the main
organization site and checked for format errors, and
repackaged for distribution.
There were three packages of the data distributed
to the participants: Trial, Training plus Develop-
ment, and Evaluation. The Trial data were rather
small, just to give the feeling of the format and
languages involved. A visual representation of the
Trial data was also created to make understanding
of the data easier. Any data in the same format
can be transformed and displayed in the Tree Editor
TrEd5 (Pajas and ?Ste?pa?nek, 2008) with the CoNLL
2009 Shared Task extension that can be installed
from within the editor. A sample visualization of an
English sentence after its conversion to the shared
task format (Sect. 2.3) is in Fig. 1.
Due to licensing requirements, every package of
the data had to be split into two portions. One
portion (Catalan, German, Japanese, and Spanish
data) was published on the task?s webpage for down-
4We assign equal weight to the two tasks, i.e., Wsem = 0.5.
5http://ufal.mff.cuni.cz/?pajas/tred
5
$QG
'(3 &&
VRPHWLPHV
703 5%
D
102' '7
UHSXWDEOH
102' --
FKDULW\
6%- 11
ZLWK
102' ,1
D
102' '7
KRXVHKROG
102' 11
QDPH QDPH
302' 11
JHWV JHW
5227 9%=
XVHG XVH
9& 9%1
DQG
&225' &&
GRHV
&21- 9%=
Q
W
$'9 5%
HYHQ
$'9 5%
NQRZ NQRZ
9& 9%
LW
2%- 353

3  
$0703$0703$0703
 

$$$$

 
 

$
 

 

$



$01(*

$0$'9
 


$

Figure 1: Visualisation of the English sentence ?And sometimes a reputable charity with a houshold name gets used
and doesn?t even know it.? (Penn Treebank, wsj 0559) showing jointly the labeled syntactic and semantic depen-
dencies. The basic tree shape comes from the syntactic dependencies; syntactic labels and POS tags are on the 2nd
line at each node. Semantic dependencies which do not follow the syntactic ones use dotted lines. Predicate senses
in parentheses (use:01, ...) follow the word label. SRLs (A0, AM-TMP, ...) are on the last line. Please note that
multiple semantic dependencies (e.g., there are four for charity: A0? know, A1? gets, A1? used, A1? name)
and self-dependencies (name) appear in this sentence.
load, the other portion (Czech, English, and Chinese
data) was invoiced and distributed by the Linguistic
Data Consortium under a special agreement free of
charge.
Distribution of the Evaluation package was a bit
more complicated, because there were two types of
the packages - one for the Joint task and one for the
SRL-only task. Every participant had to subscribe
to one of the two tasks; subsequently, they obtained
the appropriate data (again, from the webpage and
LDC).
Prior to release, each data file was checked to
eliminate errors. The following test were carried
out:
? For every sentence, number of PREDs rows
matches the number of APREDs columns.
? The first line of each file is never empty, while
the last line always is.
? The first character on a non-empty line is al-
ways a digit, the last one is never a whitespace.
? The number of empty lines (i.e. the number
of sentences) equals the number of lines begin-
ning with ?1?.
? The data contain no spaces nor double tabs.
Some statistics on the data can be seen in Ta-
bles 2, 3 and 4. Whereas the training sizes of the
data have not been that different as they were e.g.
for the 2007 shared task on multilingual dependency
parsing (Nivre et al, 2007)6, substantial differences
existed in the distribution of the predicates and ar-
guments, the input features, the out-of-vocabulary
rates, and other statistical characteristics of the data.
Data sizes have been relatively uniform in all the
datasets, with Japanese having the smallest dataset
6http://nextens.uvt.nl/depparse-wiki/
DataOverview
6
containing data for SRL annotation training. To
compensate at least for the dependency parsing part,
an additional, large Japanese corpus with syntactic
dependency annotation has been provided.
The average sentence length, the vocabulary sizes
for FORM and LEMMA fields and the OOV rates
characterize quite naturally the properties of the re-
spective languages (in the domain of the training and
evaluation data). It is no surprise that the FORM
OOV rate is the highest for Czech, a highly inflec-
tional language, and that the LEMMA OOV rate is
the highest for German (as a consequence of keeping
compounds as a single lemma). The other statistics
also reflect (to a large extent) the annotation speci-
fication and conventions used for the original tree-
banks and/or the result of the conversion process to
the unified CoNLL-2009 Shared Task format.
Starting with the POS and FEAT fields, it can be
seen that Catalan, Czech and Spanish use only the
12 major part-of-speech categories as values of the
POS field (with richly populated FEAT field); En-
glish and Chinese are the opposite extreme, disre-
garding the use of the FEAT field completely and
coding everything as a POS value. While for Chi-
nese this is quite understandable, English follows the
PTB tradition in this respect. German and Japanese
use relatively rich set of values in both the POS and
FEAT fields.
For the dependency relations (DEPREL), all
the languages use a similarly-sized set except for
Japanese, which only encodes the distinction be-
tween a root and a dependent node (and some in-
frequent special ones).
Evaluation data are over 10% of the size of the
training data for Catalan, Chinese, Czech, Japanese
and Spanish and roughly 5% for English and Ger-
man.
Table 3 shows the distribution of the five most fre-
quent dependency relations (determined as part of
the subtask of syntactic parsing). With the exception
of Japanese, which essentially does not label depen-
dency relations at this level, all the other languages
show little difference in this distribution. For exam-
ple, the unconditioned probability of ?subjects? is
almost the same for all the six other languages (be-
tween 6 and 8 percent). The probability mass cov-
ered by the first five most frequent DEPRELs is also
almost the same (again, except for Japanese), sug-
gesting that the labeling task might have similar dif-
ficulty7. The most skewed one is for Czech (after
Japanese).
Table 4 shows similar statistics for the argument
labels (PRED/APREDs); it also adds the average
number of arguments per ?predicate? token, since
this is part of the SRL task8. It is apparent from the
comparison of the ?Total? rows in this table and Ta-
ble 3 that the first five argument labels cover more
that their syntactic counterparts. For example, the
arguments A0-A4 account for all but 3% of all ar-
guments labels, whereas Spanish and Catalan have
much more rich set of argument labels, with a high
entropy of the most-frequent-label distribution.
3.2 Catalan and Spanish
The Catalan and Spanish datasets (Taule? et al, 2008)
were generated from the AnCora corpora9 through
an automatic conversion process from a constituent-
based formalism to dependencies (Civit et al, 2006).
AnCora corpora contain about half million words
for Catalan and Spanish annotated with syntactic
and semantic information. Text sources for the Cata-
lan corpus are EFE news agency (?75Kw), ACN
Catalan news agency (?225Kw), and ?El Perio?dico?
newspaper (?200Kw). The Spanish corpus comes
from the Lexesp Spanish balanced corpus (?75Kw),
the EFE Spanish news agency (?225Kw), and the
Spanish version of ?El Perio?dico? (?200Kw). The
subset from ?El Perio?dico? corresponds to the same
news in Catalan and Spanish, spanning from January
to December 2000.
Linguistic annotation is the same in both lan-
guages and includes: PoS tags with morphologi-
cal features (gender, number, person, etc.), lemma-
tization, syntactic dependencies (syntactic func-
tions), semantic dependencies (arguments and the-
matic roles), named entities and predicate semantic
classes (Lexical Semantic Structure, LSS). Tag sets
are shared by the two languages.
If we take into account the complete PoS tags,
7Yes, this is overgeneralization since this distribution does
not condition on the features, dependencies etc. But as a rough
measure, it often correlates well with the results.
8A number below 1 means there are some argument-bearing
words (often nouns) which have no arguments in the particular
sentence in which they appear.
9http://clic.ub.edu/ancora
7
Characteristic Catalan Chinese Czech English German Japanese Spanish
Training data size (sentences) 13200 22277 38727 39279 36020 4393a 14329
Training data size (tokens) 390302 609060 652544 958167 648677 112555a 427442
Avg. sentence length (tokens) 29.6 27.3 16.8 24.4 18.0 25.6 29.8
Tokens with argumentsb (%) 9.6 16.9 63.5 18.7 2.7 22.8 10.3
DEPREL types 50 41 49 69 46 5 49
POS types 12 41 12 48 56 40 12
FEAT types 237 1 1811 1 267 302 264
FORM vocabulary size 33890 40878 86332 39782 72084 36043 40964
LEMMA vocabulary size 24143 40878 37580 28376 51993 30402 26926
Evaluation data size (sent.) 1862 2556 4213 2399 2000 500 1725
Evaluation data size (tokens) 53355 73153 70348 57676 31622 13615 50630
Evaluation FORM OOVc 5.40 3.92 7.98/8.62d 1.58/3.76d 7.93/7.57d 6.07 5.63
Evaluation LEMMA OOVc 4.14 3.92 3.03/4.29d 1.08/2.30d 5.83/7.36d 5.21 3.69
Table 2: Elementary data statistics for the CoNLL-2009 Shared Task languages. The data themselves, the original
treebanks they were derived from and the conversion process are described in more detail in sections 3.2-3.7. All
evaluation data statistics are derived from the in-domain evaluation data.
aThere were additional 33257 sentences (839947 tokens) available for syntactic dependency parsing of Japanese; the type and
vocabulary statistics are computed using this larger dataset.
bPercentage of tokens with FILLPRED=?Y?.
cPercentage of FORM/LEMMA tokens not found in the respective vocabularies derived solely from the training data.
dOOV percentage for in-domain/out-of-domain data.
DEPREL Catalan Chinese Czech English German Japanese Spanish
sn 0.16 COMP 0.21 Atr 0.26 NMOD 0.27 NK 0.31 D 0.93 sn 0.16
spec 0.15 NMOD 0.14 AuxP 0.10 P 0.11 PUNC 0.14 ROOT 0.04 spec 0.15
Labels f 0.11 ADV 0.10 Adv 0.10 PMOD 0.10 MO 0.12 P 0.03 f 0.12
sp 0.09 UNK 0.09 Obj 0.07 SBJ 0.07 SB 0.07 A 0.00 sp 0.08
suj 0.07 SBJ 0.08 Sb 0.06 OBJ 0.06 ROOT 0.06 I 0.00 suj 0.08
Total 0.58 0.62 0.59 0.61 0.70 1.00 0.59
Table 3: Unigram probability for the five most frequent DEPREL labels in the training data of the CoNLL-2009
Shared Task is shown. Total is the probability mass covered by the five dependency labels shown.
APRED Catalan Chinese Czech English German Japanese Spanish
arg1-pat 0.22 A1 0.30 RSTR 0.30 A1 0.37 A0 0.40 GA 0.33 arg1-pat 0.20
arg0-agt 0.18 A0 0.27 PAT 0.18 A0 0.25 A1 0.39 WO 0.15 arg0-agt 0.19
Labels arg1-tem 0.15 ADV 0.20 ACT 0.17 A2 0.12 A2 0.12 NO 0.15 arg1-tem 0.15
argM-tmp 0.08 TMP 0.07 APP 0.06 AM-TMP 0.06 A3 0.06 NI 0.09 arg2-atr 0.08
arg2-atr 0.08 DIS 0.04 LOC 0.04 AM-MNR 0.03 A4 0.01 DE 0.06 argM-tmp 0.08
Total 0.71 0.91 0.75 0.83 0.97 0.78 0.70
Avg. 2.25 2.26 0.88 2.20 1.97 1.71 2.26
Table 4: Unigram probability for the five most frequent APRED labels in the training data of the CoNLL-2009
Shared Task is shown. Total is the probability mass covered by the five argument labels shown. The ?Avg.? line
shows the average number of arguments per predicate or other argument-bearing token (i.e. for those marked by
FILLPRED=?Y?).
8
AnCora has 280 different labels. Considering only
the main syntactic categories, the tag set is reduced
to 47 tags. The syntactic tag set consists of 50 dif-
ferent syntactic functions. Regarding semantic ar-
guments, we distinguish Arg0, Arg1, Arg2, Arg3,
Arg4, ArgM, and ArgL. The first five tags are num-
bered from less to more obliqueness with respect
to the verb, ArgM corresponds to adjuncts. The
list of thematic roles consists of 20 different labels:
AGT (Agent), AGI (Induced Agent), CAU (Cause),
EXP (Experiencer), SCR (Source), PAT (Patient),
TEM (Theme), ATR (Attribute), BEN (Beneficiary),
EXT (Extension), INS (Instrument), LOC (Loca-
tive), TMP (Time), MNR (Manner), ORI (Origin),
DES (Goal), FIN (Purpose), EIN (Initial State), EFI
(Final State), and ADV (Adverbial). Each argument
position can map onto specific thematic roles. By
way of example, Arg1 can be PAT, TEM or EXT. For
Named Entities, we distinguish six types: Organiza-
tion, Person, Location, Date, Number, and Others.
An incremental process guided the annotation of
AnCora, since semantics depends on morphosyntax,
and syntax relies on morphology. This procedure
made it possible to check, correct, and complete
the previous annotations, thus guaranteeing the final
quality of the corpora and minimizing the error rate.
The annotation process was carried out sequentially
from lower to upper layers of linguistic description.
All resulting layers are independent of each other,
thus making easier the data management. The ini-
tial annotation was performed manually for syntax,
semiautomatically in the case of arguments and the-
matic roles, and fully automatically for PoS (Mart??
et al, 2007; Ma`rquez et al, 2007).
The Catalan and Spanish AnCora corpora were
straightforwardly translated into the CoNLL-2009
shared task formatting (information about named
entities was skipped in this process). The resulting
Catalan corpus (including training, development and
test partitions) contains 16,786 sentences with an av-
erage length of 29.59 lexical tokens per sentence.
Long sentences abound in this corpus. For instance,
10.73% of the sentences are longer than 50 tokens,
and 4.42% are longer than 60. The corpus con-
tains 47,537 annotated predicates (2.83 predicates
per sentence, on average) with 107,171 arguments
(2.25 arguments per predicate, on average). From
the latter, 73.89% correspond to core arguments and
26.11% to adjuncts. Numbers for the Spanish cor-
pus are comparable in all aspects: 17,709 sentences
with 29.84 lexical tokens on average (11.58% of the
sentences longer than 50 tokens, 4.07% longer than
60); 54,075 predicates (3.05 per sentence, on aver-
age) and 122,478 arguments (2.26 per predicate, on
average); 73.34% core arguments and 26.66% ad-
juncts.
The following are important features of the Cata-
lan and Spanish corpora in the CoNLL-2009 shared
task setting: (1) all dependency trees are projective;
(2) no word can be the argument of more than one
predicate in a sentence; (3) semantic dependencies
completely match syntactic dependency structures
(i.e., no new edges are introduced by the semantic
structure); (4) only verbal predicates are annotated
(with exceptional cases referring to words that can
be adjectives and past participles); (5) the corpus is
segmented so multi-words, named entities, temporal
expressions, compounds, etc. are grouped together;
and (6) segmentation also accounts for elliptical pro-
nouns (there are marked as empty lexical tokens ?_?
with a pronoun POS tag).
Finally, the predicted columns (PLEMMA,
PPOS, and PFEAT) have been generated with the
FreeLing Open source suite of Language Analyz-
ers10. Accuracy in PLEMMA and PPOS columns
is above 95% for the two languages. PHEAD
and PDEPREL columns have been generated using
MaltParser11. Parsing accuracy (LAS) is above 86%
for the the two languages.
3.3 Chinese
The Chinese Corpus for the 2009 CoNLL Shared
Task was generated by merging the Chinese Tree-
bank (Xue et al, 2005) and the Chinese Proposition
Bank (Xue and Palmer, 2009) and then converting
the constituent structure to a dependency formalism
as specified in the CoNLL Shared Task. The Chi-
nese data used in the shared task is based on Chinese
Treebank 6.0 and the Chinese Proposition Bank 2.0,
both of which are publicly available via the Linguis-
tic Data Consortium.
The Chinese Treebank Project originated at Penn
and was later moved to University of Colorado at
10http://www.lsi.upc.es/?nlp/freeling
11http://w3.msi.vxu.se/?jha/maltparser
9
Boulder. Now it is the process of being to moved
to Brandeis University. The data sources of the Chi-
nese Treebank range from Xinhua newswire (main-
land China), Hong Kong news, and Sinorama Maga-
zine (Taiwan). More recently under DARPA GALE
funding it has been expanded to include broadcast
news, broadcast conversation, news groups and web
log data. It currently has over one million words
and is fully segmented, POS-tagged and annotated
with phrase structure. The version of the Chinese
Treebank used in this shared task, CTB 6.0, includes
newswire, magazine articles, and transcribed broad-
cast news 12. The training set has 609,060 tokens,
the development set has 49,620 tokens, and the test
set has 73,153 tokens.
The Chinese Proposition Bank adds a layer of se-
mantic annotation to the syntactic parses in the Chi-
nese Treebank. This layer of semantic annotation
mainly deals with the predicate-argument structure
of Chinese verbs and their nominalizations. Each
major sense (called frameset) of a predicate takes a
number of core arguments annotated with numeri-
cal labels Arg0 through Arg5 which are defined in
a predicate-specific manner. The Chinese Proposi-
tion Bank also annotates adjunctive arguments such
as locative, temporal and manner modifiers of the
predicate. The version of the Chinese Propbank used
in this CoNLL Shared Task is CPB 2.0, but nominal
predicates are excluded because the annotation is in-
complete.
Since the Chinese Treebank is annotated with
constituent structures, the conversion and merging
procedure converts the constituent structures to de-
pendencies by identifying the head for each con-
stituent in a parse tree and making its sisters its de-
pendents. The Chinese Propbank pointers are then
shifted from the entire constituent to the head of that
constituent. The conversion procedure identifies the
head by first exploiting the structural information
in the syntactic parse and detecting six broad cate-
gories of syntactic relations that hold between the
head and its dependents (predication, modification,
complementation, coordination, auxiliary, and flat)
and then designating the head based on these rela-
tions. In particular, the first conjunct of a coordina-
12A small number of files were taken out of the CoNLL
shared task data due to conversion problems and time con-
straints to fix them.
tion structure is designated as the head and the heads
of the other conjuncts are the conjunctions preced-
ing them. The conjunctions all ?modify? the first
conjunct.
3.4 Czech
For the training, development and evaluation data,
Prague Dependency Treebank 2.0 was used (Hajic?
et al, 2006). For the out-of-domain evaluation data,
part of the Czech side of the Prague Czech-English
Dependency Treebank (version 2, under construc-
tion) was used13, see also ( ?Cmejrek et al, 2004). For
the OOD data, no manual annotation of LEMMA,
POS, and FEAT existed, so the predicted values
were used. The same conversion procedure has been
applied to both sources.
The FORM column was created from the form
element of the morphological layer, not from the
?token? from the word-form layer. Therefore, most
typos, errors in word segmentation and tokenization
are corrected and numerals are normalized.
The LEMMA column was created from the
lemma element of the morphological layer. Only
the initial string of the element was used, so there is
no distinction between homonyms. However, some
components of the detailed lemma explanation were
incorporated into the FEAT column (see below).
The POS column was created form the morpho-
logical tag element, its first character more pre-
cisely.
The FEAT column was created from the remain-
ing characters of the tag element. In addition, the
special feature ?Sem? corresponds to a semantic fea-
ture of the lemma.
For the HEAD and DEPREL columns, the PDT
analytical layer was used. The DEPREL was taken
from the analytic function (the afun node at-
tribtue). There are 27 possible values for afun el-
ement: Pred, Pnom, AuxV, Sb, Obj, Atr, Adv,
Atv, AtvV, Coord, Apos, ExD, and a number
of auxiliary and ?double-function? labels. The first
nine of these are the ?most interesting? from the
point of view of the shared task, since they relate to
semantics more closely than the rest (at least from
the linguistic point of view). The HEAD is a pointer
to its parent, which means the PDT?s ord attribute
13http://ufal.mff.cuni.cz/pedt
10
(within-sentence ID / word position number) of the
parent. If a node is a member of a coordination
or apposition (is_member element), its DEPREL
obtains the _M suffix. The parenthesis annotation
(is_parenthesis_root element) was ignored.
The PRED and APREDs columns were created
from the tectogrammatical layer of PDT 2.0 and the
valency lexicon PDT-Vallex according to the follow-
ing rules:
? Every line corresponding to an analytical node
referenced by a lexical reference (a/lex.rf)
from the tectogrammatical layer has a PRED
value filled. If the referring non-generated
tectogrammatical node (is_generated not
equal to 1) has a valency frame assigned
(val_frame.rf), the value of PRED is the
identifier of the frame. Otherwise, it is set to
the same value as the LEMMA column.
? For every tectogrammatical node, a corre-
sponding analytical node is searched for:
1. If the tectogrammatical node is not
generated and has a lexical reference
(a/lex.rf), the referenced node is
taken.
2. Otherwise, if the tectogrammatical node
has a coreference (coref_text.rf or
coref_gram.rf) or complement refer-
ence (compl.rf) to a node that has an
analytical node assigned (by 1. or 2.), the
assigned node is taken.
APRED columns are filled with respect to the
following correspondence: for a tectogrammatical
node P and its effective child C with functor F, the
column for P?s corresponding analytical node at the
row for C?s corresponding analytical node is filled
with F. Some nodes can thus have several functors
in one APRED column, separated by a vertical bar
(see Sect. 2.4.2).
PLEMMA, PPOS and PFEAT were gener-
ated by the (cross-trained) morphological tagger
MORCE (Spoustova? et al, 2009), which gives full
combined accuracy (PLEMMA+PPOS+PFEAT)
slightly under 96%.
PHEAD and PDEPREL were generated by
the (cross-trained) MST parser for Czech (Chu?
Liu/Edmonds algorithm, (McDonald et al, 2005)),
which has typical dependency accuracy around
85%.
The valency lexicon, converted from (Hajic? et al,
2003), has four columns:
1. lemma (can occur several times in the lexicon,
with different frames)
2. frame identifier (as found in the PRED column)
3. list of space-separated actants and obligatory
members of the frame
4. example(s)
The source of the out-of-domain data uses an
extended valency lexicon (because of out-of-
vocabulary entries). For simplicity, the extended
lexicon was not provided; instead, such words were
not marked as predicates in the OOD data (their
FILLPRED was set to ?_?) and thus not evaluated.
3.5 English
The English corpus is almost identical to the cor-
pus used in the closed challenge in the CoNLL-2008
shared task evaluation (Surdeanu et al, 2008). This
corpus was generated through a process that merges
several input corpora and converts them from the
constituent-based formalism to dependencies. The
following corpora were used as input to the merging
procedure:
? Penn Treebank 3 ? The Penn Treebank 3 cor-
pus (Marcus et al, 1994) consists of hand-
coded parses of the Wall Street Journal (test,
development and training) and a small subset
of the Brown corpus (W. N. Francis and H.
Kucera, 1964) (test only).
? BBN Pronoun Coreference and Entity Type
Corpus ? BBN?s NE annotation of the Wall
Street Journal corpus (Weischedel and Brun-
stein, 2005) takes the form of SGML inline
markup of text, tokenized to be completely
compatible with the Penn Treebank annotation.
For the CoNLL-2008 shared task evaluation,
this corpus was extended by the task organizers
to cover the subset of the Brown corpus used as
a secondary testing dataset. From this corpus
we only used NE boundaries to derive NAME
11
dependencies between NE tokens, e.g., we cre-
ate a NAME dependency from Mary to Smith
given the NE mention Mary Smith.
? Proposition Bank I (PropBank) ? The Prop-
Bank annotation (Palmer et al, 2005) classifies
the arguments of all the main verbs in the Penn
Treebank corpus, other than be. Arguments are
numbered (Arg0, Arg1, . . .) based on lexical
entries or frame files. Different sets of argu-
ments are assumed for different rolesets. De-
pendent constituents that fall into categories in-
dependent of the lexical entries are classified as
various types of adjuncts (ArgM-TMP, -ADV,
etc.).
? NomBank ? NomBank annotation (Meyers et
al., 2004) uses essentially the same framework
as PropBank to annotate arguments of nouns.
Differences between PropBank and NomBank
stem from differences between noun and verb
argument structure; differences in treatment of
nouns and verbs in the Penn Treebank; and dif-
ferences in the sophistication of previous re-
search about noun and verb argument structure.
Only the subset of nouns that take arguments
are annotated in NomBank and only a subset of
the non-argument siblings of nouns are marked
as ArgM.
The complete merging process and the conversion
from the constituent representation to dependencies
is detailed in (Surdeanu et al, 2008).
The main difference between the 2008 and 2009
version of the corpora is the generation of word lem-
mas. In the 2008 version the only lemmas pro-
vided were predicted using the built-in lemmatizer
in WordNet (Fellbaum, 1998) based on the most fre-
quent sense for the form and the predicted part-of-
speech tag. These lemmas are listed in the 2009
corpus under the PLEMMA column. The LEMMA
column in the 2009 version of the corpus contains
lemmas generated using the same algorithm but us-
ing the correct Treebank part-of-speech tags. Addi-
tionally, the PHEAD and PDEPREL columns were
generated using MaltParser14, similarly to the open
challenge corpus in the CoNLL 2008 shared task.
14http://w3.msi.vxu.se/?nivre/research/
MaltParser.html
3.6 German
The German in-domain dataset is based on the an-
notated verb instances of the SALSA corpus (Bur-
chardt et al, 2006), a total of around 40k sen-
tences15. SALSA provides manual semantic role
annotation on top of the syntactically annotated
TIGER newspaper corpus, one of the standard Ger-
man treebanks. The original SALSA corpus uses se-
mantic roles in the FrameNet paradigm. We con-
structed mappings between FrameNet frame ele-
ments and PropBank argument positions at the level
of frame-predicate pairs semi-automatically. For the
frame elements of each frame-predicate pair, we first
identified the semantically defined PropBank Arg-
0 and Arg-1 positions. To do so, we annotated a
small number of very abstract frame elements with
these labels (Agent, Actor, Communicator as Arg-
0, and Theme, Effect, Message as Arg-1) and per-
colated these labels through the FrameNet hierar-
chy, adding further manual labels where necessary.
Then, we used frequency and grammatical realiza-
tion information to map the remaining roles onto
higher-numbered Arg roles. We considerably sim-
plified the annotations provided by SALSA, which
use a rather complex annotation scheme. In partic-
ular, we removed annotation for multi-word expres-
sions (which may be non-contiguous), annotations
involving multiple frames for the same predicate
(metaphors, underspecification), and inter-sentence
roles.
The out-of-domain dataset was taken from a study
on the multi-lingual projection of FrameNet annota-
tion (Pado and Lapata, 2005). It is sampled from
the EUROPARL corpus and was chosen to maxi-
mize the lexical coverage, i.e., it contains of a large
number of infrequent predicates. Both syntactic and
semantic structure were annotated manually, in the
TIGER and SALSA format, respectively. Since it
uses a simplified annotation schemes, we did not
have to discard any annotation.
For both datasets, we converted the syntactic
TIGER (Brants et al, 2002) representations into de-
pendencies with a similar set of head-finding rules
used for the preparation of the CoNLL-X shared task
German dataset. Minor modifications (for the con-
15Note, however, that typically not all predicates in each sen-
tence are annotated (cf. Table 2).
12
version of person names and coordinations) were
made to achieve better consistency with datasets
of other languages. Since the TIGER annotation
allows non-contiguous constituents, the resulting
dependencies can be non-projective. Secondary
edges were discarded in the conversion. As for the
automatically constructed features, we used Tree-
Tagger (Schmid, 1994) to produce the PLEMMA
and PPOS columns, and the Morphisto morphol-
ogy (Zielinski and Simon, 2008) for PFEAT.
3.7 Japanese
For Japanese, we used the Kyoto University Text
Corpus (Kawahara et al, 2002), which consists of
approximately 40k sentences taken from Mainichi
Newspapers. Out of them, approximately 5k sen-
tences are annotated with syntactic and semantic de-
pendencies, and are used the training, development
and test data of this year?s shared task. The remain-
ing sentences, which are annotated with only syntac-
tic dependencies, are provided for the training cor-
pus of syntactic dependency parsers.
This corpus adopts a dependency structure repre-
sentation, and thus the conversion to the CoNLL-
2009 format was relatively straightforward. How-
ever, since the original dependencies are annotated
on the basis of phrases (Japanese bunsetsu), we
needed to automatically convert the original annota-
tions to word-based ones using several criteria. We
used the following basic criteria: the words except
the last word in a phrase depend on the next (right)
word, and the last word in a phrase basically depends
on the head word of the governing phrase.
Semantic dependencies are annotated for both
verbal predicates and nominal predicates. The se-
mantic roles (APRED columns) consist of 41 sur-
face cases, many of which are case-marking post-
positions such as ga (nominative), wo (accusative)
and ni (dative). Semantic frame discrimination is not
annotated, and so the PRED column is the same as
the LEMMA column. The original corpus contains
coreference annotations and inter-sentential seman-
tic dependencies, such as inter-sentential zero pro-
nouns and bridging references, but we did not use
these annotations, which are not the target of this
year?s shared task.
To produce the PLEMMA, PPOS and PFEAT
columns, we used the morphological analyzer JU-
MAN 16 and the dependency and case structure an-
alyzer KNP 17. To produce the PHEAD and PDE-
PREL columns, we used the MSTParser 18.
4 Submissions and Results
Participants uploaded the results through the shared
task website, and the official evaluation was per-
formed centrally. Feedback was provided if any for-
mal problems were encountered (for a list of checks,
see the previous section). One submission had to
be rejected because only English results were pro-
vided. After the evaluation period had passed, the
results were anonymized and published on the web.
A total of 20 systems participated in the closed
challenge; 13 of them in the Joint task and seven in
the SRL-only task. Two systems participated in the
open challenge (Joint task). Moreover, 17 systems
provided output in the out-of-domain part of the task
(11 in the OOD Joint task and six in the OOD SRL-
only task).
The main results for the core task - the Joint task
(dependency syntax and semantic relations) in the
context of the closed challenge - are summarized and
ranked in Table 5.
The largest number of systems can be compared
in the SRL results table (Table 6), where all the sys-
tems have been evaluated solely on the SRL perfor-
mance regardless whether they participated in the
Joint or SRL-only task. However, since the results
might have been influenced by the supplied parser,
separate ranking is provided for both types of the
systems.
Additional breakdown of the results (open chal-
lenge, precision and recall tables for the semantic
labeling task, etc.) are available from the CoNLL-
2009 Shared Task website19.
5 Approaches
Table 7 summarizes the properties of the systems
that participated in the closed the open challenges.
16http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/juman-e.html
17http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp-e.html
18http://sourceforge.net/projects/
mstparser
19http://ufal.mff.cuni.cz/conll2009-st
13
Rank System Average Catalan Chinese Czech English German Japanese Spanish
1 Che 82.64 81.84 76.38 83.27 87.00 82.44 85.65 81.90
2 Chen 82.52 83.01 76.23 80.87 87.69 81.22 85.28 83.31
3 Merlo 82.14 82.66 76.15 83.21 86.03 79.59 84.91 82.43
4 Bohnet 80.85 80.44 75.91 79.57 85.14 81.60 82.51 80.75
5 Asahara 78.43 75.91 73.43 81.43 86.40 69.84 84.86 77.12
6 Brown 77.27 77.40 72.12 75.66 83.98 77.86 76.65 77.21
7 Zhang 76.49 75.00 73.42 76.93 82.88 73.76 78.17 75.25
8 Dai 73.98 72.09 72.72 67.14 81.89 75.00 80.89 68.14
9 Lu Li 73.97 71.32 65.53 75.85 81.92 70.93 80.49 71.72
10 Llu??s 71.49 56.64 66.18 75.95 81.69 72.31 81.76 65.91
11 Vallejo 70.81 73.75 67.16 60.50 78.19 67.51 77.75 70.78
12 Ren 67.81 59.42 75.90 60.18 77.83 65.77 77.63 57.96
13 Zeman 51.07 49.61 43.50 57.95 50.27 49.57 57.69 48.90
Table 5: Official results of the Joint task, closed challenge. Teams are denoted by the last name (first name added
only where needed) of the author who registered for the evaluation data. Results are sorted in descending order of the
language-averaged macro F1 score on the closed challenge Joint task. Bold numbers denote the best result for a given
language.
Rank Rank in task System Average Catalan Chinese Czech English German Japanese Spanish
1 1 (SRLonly) Zhao 80.47 80.32 77.72 85.19 85.44 75.99 78.15 80.46
2 2 (SRLonly) Nugues 80.31 80.01 78.60 85.41 85.63 79.71 76.30 76.52
3 1 (Joint) Chen 79.96 80.10 76.77 82.04 86.15 76.19 78.17 80.29
4 2 (Joint) Che 79.94 77.10 77.15 86.51 85.51 78.61 78.26 76.47
5 3 (Joint) Merlo 78.42 77.44 76.05 86.02 83.24 71.78 77.23 77.19
6 3 (SRLonly) Meza-Ruiz 77.46 78.00 77.73 75.75 83.34 73.52 76.00 77.91
7 4 (Joint) Bohnet 76.00 74.53 75.29 79.02 80.39 75.72 72.76 74.31
8 5 (Joint) Asahara 75.65 72.35 74.17 84.69 84.26 63.66 77.93 72.50
9 6 (Joint) Brown 72.85 72.18 72.43 78.02 80.43 73.40 61.57 71.95
10 7 (Joint) Dai 70.78 66.34 71.57 75.50 78.93 67.43 71.02 64.64
11 8 (Joint) Zhang 70.31 67.34 73.20 78.28 77.85 62.95 64.71 67.81
12 9 (Joint) Lu Li 69.72 66.95 67.06 79.08 77.17 61.98 69.58 66.23
13 4 (SRLonly) Baoli Li 69.26 74.06 70.37 57.46 69.63 67.76 72.03 73.54
14 10 (Joint) Vallejo 68.95 70.14 66.71 71.49 75.97 61.01 68.82 68.48
15 5 (SRLonly) Moreau 66.49 65.60 67.37 71.74 72.14 66.50 57.75 64.33
16 11 (Joint) Llu??s 63.06 46.79 59.72 76.90 75.86 62.66 71.60 47.88
17 6 (SRLonly) Ta?ckstro?m 61.27 57.11 63.41 71.05 67.64 53.42 54.74 61.51
18 7 (SRLonly) Lin 57.18 61.70 70.33 60.43 65.66 59.51 23.78 58.87
19 12 (Joint) Ren 56.69 41.00 72.58 62.82 67.56 54.31 58.73 39.80
20 13 (Joint) Zeman 32.14 24.19 34.71 58.13 36.05 16.44 30.13 25.36
Table 6: Official results of the semantic labeling, closed challenge, all systems. Teams are denoted by the last name
(first name added only where needed) of the author who registered for the evaluation data. Results are sorted in
descending order of the semantic labeled F1 score (closed challenge). Bold numbers denote the best result for a given
language. Separate ranking is provided for SRL-only systems.
The second column of the table highlights the over-
all architectures. We used + to indicate that the
components are sequentially connected. The lack of
a + sign indicates that the corresponding tasks are
performed jointly.
It is perhaps not surprising that most of the obser-
vations from the 2008 shared task still hold; namely,
the best systems overall do not use joint learning or
optimization (the best such system was placed third
in the Joint task, and there were only four systems
where the learning methodology can be considered
?joint?).
Therefore, most of the observations and conclu-
sions from 2008 shared task hold as well for the
current results. For details, we will leave it to the
reader to interpret the architectures and methods
14
O
v
er
a
ll
D
D
D
PA
PA
PA
Jo
in
t
M
L
Sy
st
em
a
A
rc
h.
b
A
rc
h.
C
o
m
b.
In
fe
re
n
ce
c
A
rc
h.
C
o
m
b.
In
fe
re
n
ce
Le
a
rn
in
g/
O
pt
.
M
et
ho
ds
Zh
ao
PA
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
/g
lo
ba
l
se
ar
ch
(S
R
L-
o
n
ly
)
M
E
N
u
gu
es
(P
C+
A
I+
A
C)
+
A
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
be
am
se
ar
ch
+
re
ra
n
ki
n
g
(S
R
L-
o
n
ly
)
L2
-
re
gu
la
riz
ed
lin
.
re
gr
es
sio
n
Ch
en
P
+
PC
+
A
I+
A
C
gr
ap
h
pa
rt
ia
lly
M
ST
C
L
/E
cl
as
s
n
o
gr
ee
dy
(?)
n
o
M
E
Ch
e
D
+
PC
+
A
IC
gr
ap
h
n
o
M
ST
H
O
E
cl
as
s
n
o
IL
P
n
o
SV
M
,
M
E
M
er
lo
D
PA
IC
+
D
ge
n
er
at
iv
e,
tr
an
s
n
o
be
am
se
ar
ch
tr
an
s
n
o
be
am
se
ar
ch
sy
n
ch
ro
n
iz
ed
de
riv
at
io
n
IS
B
N
M
ez
a-
R
u
iz
PA
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
M
ar
ko
v
LN
n
o
Cu
tti
n
g
Pl
an
e
(S
R
L-
o
n
ly
)
M
IR
A
B
o
hn
et
D
+
A
I+
A
C
+
PC
gr
ap
h
n
o
M
ST
C
+
re
ar
ra
n
ge
cl
as
s
n
o
gr
ee
dy
n
o
SV
M
(M
IR
A
)
A
sa
ha
ra
D
+
PI
C
+
A
IC
gr
ap
h
n
o
M
ST
C
cl
as
s
n
o
n
-
be
st
re
la
x
.
n
o
pe
rc
ep
tr
o
n
D
ai
D
+
PC
+
A
C
gr
ap
h
n
o
M
ST
C
cl
as
s
n
o
pr
o
b
ite
ra
tiv
e
M
E
Zh
an
g
D
+
A
I+
A
C
+
PC
gr
ap
h
n
o
M
ST
E
cl
as
s
n
o
cl
as
sifi
ca
tio
n
n
o
M
IR
A
,
M
E
Lu
Li
D
+
(P
C
||
A
IC
)
gr
ap
h
fo
r
ea
ch
la
n
g.
M
ST
C
L
/E
,
M
ST
E
cl
as
s
n
o
gr
ee
dy
n
o
M
E
B
ao
li
Li
PC
+
A
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
(S
R
L-
o
n
ly
)
SV
M
,
kN
N
,
M
E
Va
lle
jod
[D
+
P+
A
]C
+
D
I
cl
as
s
n
o
re
ra
n
ki
n
g
cl
as
s
n
o
re
ra
n
ki
n
g
u
n
ifi
ed
la
be
ls
M
B
L
M
o
re
au
D
+
PI
+
Cl
u
st
er
in
g
+
A
I+
A
C
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
CR
F
(S
R
L-
o
n
ly
)
CR
F
Ll
u
??s
D
+
D
A
IC
+
PC
gr
ap
h
n
o
M
ST
E
gr
ap
h
n
o
M
ST
E
ye
s,
M
ST
E
Av
g.
Pe
rc
ep
tr
o
n
Ta?
ck
st
ro?
m
D
+
PI
+
A
I
+
A
C
+
Co
n
st
ra
in
tS
at
isf
ac
tio
n
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
(S
R
L-
o
n
ly
)
SV
M
R
en
D
+
PC
+
A
IC
tr
an
s
n
o
gr
ee
dy
cl
as
s
n
o
gr
ee
dy
n
o
SV
M
(M
al
t),
M
E
Ze
m
an
D
I+
D
C+
PC
+
A
I+
A
C
tr
an
s
n
o
gr
ee
dy
w
ith
he
u
ris
tic
s
cl
as
s
n
o
gr
ee
dy
n
o
co
o
cc
u
rr
en
ce
Ta
bl
e
7:
Su
m
m
ar
y
o
fs
ys
te
m
ar
ch
ite
ct
u
re
s
fo
r
th
e
Co
N
LL
-
20
09
sh
ar
ed
ta
sk
;
al
ls
ys
te
m
s
ar
e
in
cl
u
de
d.
SR
L-
o
n
ly
sy
st
em
s
do
n
o
t
ha
v
e
th
e
D
co
lu
m
n
s
an
d
th
e
Jo
in
t
Le
ar
in
g/
O
pt
.
co
lu
m
n
s
fil
le
d
in
.
Th
e
sy
st
em
s
ar
e
so
rt
ed
by
th
e
se
m
an
tic
la
be
le
d
F 1
sc
o
re
av
er
ag
ed
o
v
er
al
lt
he
la
n
gu
ag
es
(sa
m
e
as
in
Ta
bl
e
6).
O
n
ly
th
e
sy
st
em
s
th
at
ha
v
e
a
co
rr
es
po
n
di
n
g
pa
pe
r
in
th
e
pr
o
ce
ed
in
gs
ar
e
in
cl
u
de
d.
A
cr
o
n
ym
s
u
se
d:
D
-
sy
n
ta
ct
ic
de
pe
n
de
n
ci
es
,
P
-
pr
ed
ic
at
e,
A
-
ar
gu
m
en
t,
I-
id
en
tifi
ca
tio
n
,
C
-
cl
as
sifi
ca
tio
n
.
O
v
er
a
ll
a
rc
h.
st
an
ds
fo
r
th
e
co
m
pl
et
e
sy
st
em
ar
ch
ite
ct
u
re
;D
A
rc
h.
st
an
ds
fo
r
th
e
ar
ch
ite
ct
u
re
o
ft
he
sy
n
ta
ct
ic
pa
rs
er
;D
C
o
m
b.
in
di
ca
te
s
if
th
e
fin
al
pa
rs
er
o
u
tp
u
tw
as
ge
n
er
at
ed
u
sin
g
pa
rs
er
co
m
bi
n
at
io
n
;D
In
fe
re
n
ce
st
an
ds
fo
r
th
e
ty
pe
o
fi
n
fe
re
n
ce
u
se
d
fo
r
sy
n
ta
ct
ic
pa
rs
in
g;
PA
A
rc
h.
st
an
ds
th
e
ty
pe
o
fa
rc
hi
te
ct
u
re
u
se
d
fo
r
PA
IC
;P
A
C
o
m
b.
in
di
ca
te
s
if
th
e
PA
o
u
tp
u
t
w
as
ge
n
er
at
ed
th
ro
u
gh
sy
st
em
co
m
bi
n
at
io
n
;P
A
In
fe
re
n
ce
st
an
ds
fo
r
th
e
th
e
ty
pe
o
fi
n
fe
re
n
ce
u
se
d
fo
r
PA
IC
;J
o
in
tL
ea
rn
in
g/
O
pt
.
in
di
ca
te
s
if
so
m
e
fo
rm
o
fjo
in
tl
ea
rn
in
g
o
r
o
pt
im
iz
at
io
n
w
as
im
pl
em
en
te
d
fo
r
th
e
sy
n
ta
ct
ic
+
se
m
an
tic
gl
o
ba
lt
as
k;
M
L
M
et
ho
ds
lis
ts
th
e
M
L
m
et
ho
ds
u
se
d
th
ro
u
gh
o
u
tt
he
co
m
pl
et
e
sy
st
em
.
a
A
u
th
o
rs
o
ft
w
o
sy
st
em
s:
?
B
ro
w
n
?
an
d
?
Li
n
?
di
dn
?
ts
u
bm
it
a
pa
pe
r,
so
th
ei
r
sy
st
em
s?
ar
ch
ite
ct
u
re
s
ar
e
u
n
kn
ow
n
.
b T
he
sy
m
bo
l+
in
di
ca
te
s
se
qu
en
tia
lp
ro
ce
ss
in
g
(ot
he
rw
ise
,
pa
ra
lle
l/jo
in
t).
Th
e
||
m
ea
n
s
th
at
se
v
er
al
di
ffe
re
n
ta
rc
hi
te
ct
u
re
s
sp
an
n
in
g
m
u
lti
pl
e
su
bt
as
ks
ra
n
in
pa
ra
lle
l.
c
M
ST
C
L
/E
as
u
se
d
by
M
cD
o
n
al
d
(20
05
),
M
ST
C
by
Ca
rr
er
as
(20
07
),M
ST
E
by
Ei
sn
er
(20
00
),
M
ST
H
O
E
=
M
ST
E
w
ith
hi
gh
er
-
o
rd
er
fe
at
u
re
s
(si
bl
in
gs
+
al
lg
ra
n
dc
hi
ld
re
n
).
d T
he
sy
st
em
u
n
ifi
es
th
e
sy
n
ta
ct
ic
an
d
se
m
an
tic
la
be
ls
in
to
o
n
e
la
be
l,
an
d
tr
ai
n
s
cl
as
sifi
er
s
o
v
er
th
em
.
It
is
th
u
s
di
ffi
cu
lt
to
sp
lit
th
e
sy
st
em
ch
ar
ac
te
ris
tic
in
to
a
?
D
?
/?
PA
?
pa
rt
.
15
when comparing Table 7 with the Tables 5 and 6).
6 Conclusion
This year?s task has been demanding in several re-
spects, but certainly the most difficulty came from
the fact that participants had to tackle all seven lan-
guages. It is encouraging that despite this added af-
fort the number of participating systems has been
almost the same as last year (20 vs. 22 in 2008).
There are several positive outcomes from this
year?s enterprise:
? we have prepared a unified format and data for
several very different lanaguages, as a basis
for possible extensions towards other languages
and unified treatment of syntactic depenndecies
and semantic role labeling across natural lan-
guages;
? 20 participants have produced SRL results for
all seven languages, using several different
methods, giving hope for a combined system
with even substantially better performance;
? initial results have been provided for three lan-
guages on out-of-domain data (being in fact
quite close to the in-domain results).
Only four systems tried to apply what can be de-
scribed as joint learning for the syntactic and seman-
tic parts of the task. (Morante et al, 2009) use a true
joint learning formulation that phrases syntactico-
semantic parsing as a series of classification where
the class labels are concatenations of syntactic and
semantic edge labels. They predict (a), the set of
syntactico-semantic edge labels for each pair of to-
kens; (b), the set of incoming syntactico-semantic
edge labels for each individual token; and (c), the
existence of an edge between each pair of tokens.
Subsequently, they combine the (possibly conflict-
ing) output of the three classifiers by a ranking ap-
proach to determine the most likely structure that
meets all well-formedness constraints. (Llu??s et al,
2009) present a joint approach based on an exten-
sion of Eisner?s parser to accommodate also seman-
tic dependency labels. This architecture is similar
to the one presented by the same authors in the past
edition, with the extension to a second-order syn-
tactic parsing and a particular setting for Catalan
and Spanish. (Gesmundo et al, 2009) use an in-
cremental parsing model with synchronous syntac-
tic and semantic derivations and a joint probability
model for syntactic and semantic dependency struc-
tures. The system uses a single input queue but two
separate stacks and synchronizes syntactic and se-
mantic derivations at every word. The synchronous
derivations are modeled with an Incremental Sig-
moid Belief Network that has latent variables for
both syntactic and semantic states and connections
from syntax to semantics and vice versa. (Dai et
al., 2009) designed an iterative system to exploit
the inter-connections between the different subtasks
of the CoNLL shared task. The idea is to decom-
pose the joint learning problem into four subtasks
? syntactic dependency identification, syntactic de-
pendency labeling, semantic dependency identifica-
tion and semantic dependency labeling. The initial
step is to use a pipeline approach to use the input of
one subtask as input to the next, in the order speci-
fied. The iterative steps then use additional features
that are not available in the initial step to improve the
accuracy of the overall system. For example, in the
iterative steps, semantic information becomes avail-
able as features to syntactic parsing, so on and so
forth.
Despite these results, it is still not clear whether
joint learning has a significant advantage over other
approaches (and if yes, then for what languages). It
is thus necessary to carefully plan the next shared
tasks; it might be advantageous to bring up a sim-
ilar task in the future once again, and/or couple it
with selected application(s). There, (we hope) the
benefits of the dependency representation combined
with semantic roles the way we have formulated it
in 2008 and 2009 will really show up.
Acknowledgments
We would like to thank the Linguistic Data Consor-
tium, mainly to Denise DiPersio, Tony Casteletto
and Christopher Cieri for their help and handling
of invoicing and distribution of the data for which
LDC has a license. For all of the trial, training and
evaluation data they had to act a very short notice.
All the data has been at the participants? disposal
(again) free of charge. We are grateful to all of them
for LDC?s continuing support of the CoNLL Shared
16
Tasks.
We would also like to thank organizers of the pre-
vious four shared tasks: Sabine Buchholz, Xavier
Carreras, Ryan McDonald, Amit Dubey, Johan Hall,
Yuval Krymolowski, Sandra Ku?bler, Erwin Marsi,
Jens Nilsson, Sebastian Riedel and Deniz Yuret.
This shared task would not have been possible with-
out their previous effort.
We also acknowledge the support of the M?SMT
of the Czech Republic, projects MSM0021620838
and LC536; the Grant Agency of the Academy of
sciences of the Czech Republic 1ET201120505 (for
Jan Hajic?, Jan ?Ste?pa?nek and Pavel Stran?a?k).
Llu??s Ma`rquez and M. Anto`nia Mart?? partici-
pation was supported by the Spanish Ministry of
Education and Science, through the OpenMT and
TextMess research projects (TIN2006-15307-C03-
02, TIN2006-15265-C06-06).
The following individuals directly contributed to
the Chinese Treebank (in alphabetic order): Meiyu
Chang, Fu-Dong Chiou, Shizhe Huang, Zixin Jiang,
Tony Kroch, Martha Palmer, Mitch Marcus, Fei
Xia, Nianwen Xue. The contributors to the Chi-
nese Proposition Bank include (in alphabetic order):
Meiyu Chang, Gang Chen, Helen Chen, Zixin Jiang,
Martha Palmer, Zhiyi Song, Nianwen Xue, Ping Yu,
Hua Zhong. The Chinese Treebank and the Chinese
Proposition Bank were funded by DOD, NSF and
DARPA.
Adam Meyers? work on the shared task has been
supported by the NSF Grant IIS-0534700 ?Structure
Alignment-based MT.?
We thank the Mainichi Newspapers for the per-
mission of distributing the sentences of the Kyoto
University Text Corpus for this shared task.
References
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories, Sozopol.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceedings of
EMNLP-CoNLL 2007, pages 957?961, June. Prague,
Czech Republic.
Montserrat Civit, M. Anto`nia Mart??, and Nu?ria Buf??.
2006. Cat3LB and Cast3LB: from constituents to
dependencies. In Proceedings of the 5th Interna-
tional Conference on Natural Language Processing,
FinTAL, pages 141?153, Turku, Finland. Springer Ver-
lag, LNAI 4139.
Qifeng Dai, Enhong Chen, and Liu Shi. 2009. An it-
erative approach for joint dependency parsing and se-
mantic role labeling. In Proceedings of the 13th Con-
ference on Computational Natural Language Learning
(CoNLL-2009), June 4-5, Boulder, Colorado, USA.
June 4-5.
Jason Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Harry Bunt and Anton
Nijholt, editors, Advances in Probabilistic and Other
Parsing Tehcnologies, pages 29?62. Kluwer Academic
Publishers.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press, Cambridge.
Andrea Gesmundo, James Henderson, Paola Merlo, and
Ivan Titov. 2009. A latent variable model of syn-
chronous syntactic-semantic parsing for multiple lan-
guages. In Proceedings of the 13th Conference on
Computational Natural Language Learning (CoNLL-
2009), June 4-5, Boulder, Colorado, USA. June 4-5.
Jan Hajic?, Jarmila Panevova?, Zden?ka Ures?ova?, Alevtina
Be?mova?, Veronika Kola?r?ova?- ?Rezn??c?kova??, and Petr
Pajas. 2003. PDT-VALLEX: Creating a Large-
coverage Valency Lexicon for Treebank Annotation.
In J. Nivre and E. Hinrichs, editors, Proceedings of The
Second Workshop on Treebanks and Linguistic Theo-
ries, pages 57?68, Vaxjo, Sweden. Vaxjo University
Press.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan ?Ste?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k ?Zabokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013, Las Palmas, Canary
Islands.
Xavier Llu??s, Stefan Bott, and Llu??s Ma`rquez. 2009.
A second-order joint eisner model for syntactic and
semantic dependency parsing. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA. June 4-5.
17
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguistics,
19(2):313?330.
Llu??s Ma`rquez, Luis Villarejo, M. Anto`nia Mart??, and
Mariona Taule?. 2007. SemEval-2007 Task 09: Mul-
tilevel semantic annotation of catalan and spanish.
In Proceedings of the 4th International Workshop on
Semantic Evaluations (SemEval-2007), pages 42?47,
Prague, Czech Republic.
M. Anto`nia Mart??, Mariona Taule?, Llu??s Ma`rquez, and
Manu Bertran. 2007. Anotacio?n semiautoma?tica
con papeles tema?ticos de los corpus CESS-ECE.
Procesamiento del Lenguaje Natural, SEPLN Journal,
38:67?76.
Ryan McDonald, Fernando Pereira, Jan Hajic?, and Kiril
Ribarov. 2005. Non-projective dependency parsing
using spanning tree algortihms. In Proceedings of
NAACL-HLT?05, Vancouver, Canada, pages 523?530.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The Nom-
Bank Project: An Interim Report. In NAACL/HLT
2004 Workshop Frontiers in Corpus Annotation,
Boston.
Roser Morante, Vincent Van Asch, and Antal van den
Bosch. 2009. A simple generative pipeline approach
to dependency parsing and semantic role labeling. In
Proceedings of the 13th Conference on Computational
Natural Language Learning (CoNLL-2009), Boulder,
Colorado, USA. June 4-5.
Joakim Nivre, Johann Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The conll 2007 shared task on depen-
dency parsing. In Proceedings of the EMNLP-CoNLL
2007 Conference, pages 915?932, Prague, Czech Re-
public.
Sebastian Pado and Mirella Lapata. 2005. Cross-lingual
projection of role-semantic information. In Proceed-
ings of the Human Language Technology Conference
and Conference on Empirical Methods in Natural Lan-
guage Processing (HLT/EMNLP-2005), pages 859?
866, Vancouver, BC.
Petr Pajas and Jan ?Ste?pa?nek. 2008. Recent advances in
a feature-rich framework for treebank annotation. In
The 22nd International Conference on Computational
Linguistics - Proceedings of the Conference (COL-
ING?08), pages 673?680, Manchester.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing.
Drahom??ra ?Johanka? Spoustova?, Jan Hajic?, Jan Raab,
and Miroslav Spousta. 2009. Semi-supervised train-
ing for the averaged perceptron POS tagger. In Pro-
ceedings of the European ACL Cenference EACL?09,
Athens, Greece.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008), pages 159?177.
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
Martin ?Cmejrek, Jan Cur???n, Jan Hajic?, Jir??? Havelka,
and Vladislav Kubon?. 2004. Prague Czech-English
Dependency Treebank: Syntactically Anntoated Re-
sources for Machine Translation. In Proceedings of
the 4th International Conference on Language Re-
sources and Evaluation (LREC-2004), pages 1597?
1600, Lisbon, Portugal.
W. N. Francis and H. Kucera. 1964. Brown Corpus Man-
ual of Information to accompany A Standard Corpus
of Present-Day Edited American English, for use with
Digital Computers. Revised 1971, Revised and Am-
plified 1979, available at www.clarinet/brown.
R. Weischedel and A. Brunstein. 2005. BBN pronoun
coreference and entity type corpus. Technical report,
Lin- guistic Data Consortium.
Nianwen Xue and Martha Palmer. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Nianwen Xue, Fei Xia, Fu Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
Structure Annotation of a Large Corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Andrea Zielinski and Christian Simon. 2008. Morphisto:
An open-source morphological analyzer for german.
In Proceedings of the Conference on Finite State Meth-
ods in Natural Language Processing.
18
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 73?76,
Paris, October 2009. c?2009 Association for Computational Linguistics
An Improved Oracle for Dependency Parsing with Online Reordering
Joakim Nivre?? Marco Kuhlmann? Johan Hall?
?Uppsala University, Department of Linguistics and Philology, SE-75126 Uppsala
?V?xj? University, School of Mathematics and Systems Engineering, SE-35195 V?xj?
E-mail: FIRSTNAME.LASTNAME@lingfil.uu.se
Abstract
We present an improved training strategy
for dependency parsers that use online re-
ordering to handle non-projective trees.
The new strategy improves both efficiency
and accuracy by reducing the number of
swap operations performed on non-project-
ive trees by up to 80%. We present state-of-
the-art results for five languages with the
best ever reported results for Czech.
1 Introduction
Recent work on dependency parsing has resulted in
considerable progress with respect to both accuracy
and efficiency, not least in the treatment of discon-
tinuous syntactic constructions, usually modeled
by non-projective dependency trees. While non-
projective dependency relations tend to be rare in
most languages (Kuhlmann and Nivre, 2006), it
is not uncommon that up to 25% of the sentences
have a structure that involves at least one non-pro-
jective relation, a relation that may be crucial for
an adequate analysis of predicate-argument struc-
ture. This makes the treatment of non-projectivity
central for accurate dependency parsing.
Unfortunately, parsing with unrestricted non-pro-
jective structures is a hard problem, for which exact
inference is not possible in polynomial time except
under drastic independence assumptions (McDon-
ald and Satta, 2007), and most data-driven parsers
therefore use approximate methods (Nivre et al,
2006; McDonald et al, 2006). One recently ex-
plored approach is to perform online reordering
by swapping adjacent words of the input sentence
while building the dependency structure. Using this
technique, the system of Nivre (2009) processes
unrestricted non-projective structures with state-of-
the-art accuracy in observed linear time.
The normal procedure for training a transition-
based parser is to use an oracle that predicts an
optimal transition sequence for every dependency
tree in the training set, and then approximate this
oracle by a classifier. In this paper, we show that
the oracle used for training by Nivre (2009) is sub-
optimal because it eagerly swaps words as early
as possible and therefore makes a large number of
unnecessary transitions, which potentially affects
both efficiency and accuracy. We propose an altern-
ative oracle that reduces the number of transitions
by building larger structures before swapping, but
still handles arbitrary non-projective structures.
2 Background
The fundamental reason why sentences with non-
projective dependency trees are hard to parse is that
they contain dependencies between non-adjacent
substructures. The basic idea in online reordering
is to allow the parser to swap input words so that
all dependency arcs can be constructed between
adjacent subtrees. This idea is implemented in the
transition system proposed by Nivre (2009). The
first three transitions of this system (LEFT-ARC,
RIGHT-ARC, and SHIFT) are familiar from many
systems for transition-based dependency parsing
(Nivre, 2008). The only novelty is the SWAP trans-
ition, which permutes two nodes by moving the
second-topmost node from the stack back to the
input buffer while leaving the top node on the stack.
To understand how we can parse sentences with
non-projective dependency trees, in spite of the
fact that dependencies can only be added between
nodes that are adjacent on the stack, note that, for
any sentence x with dependency tree G, there is
always some permutation x? of x such thatG is pro-
jective with respect to x?. There may be more than
one such permutation, but Nivre (2009) defines the
canonical projective order <G for x given G as
the order given by an inorder traversal of G that
respects the order < between a node and its direct
dependents. This is illustrated in Figure 1, where
the words of a sentence with a non-projective tree
73
ROOT Who did you send the letter to ?
0 6 1 2 3 4 5 7 8
ROOT
NMOD
P
VG
SUBJ
OBJ2
OBJ1
DET
Figure 1: Dependency tree for an English sentence with projective order annotation.
have been annotated with their positions in the pro-
jective order; reading the words in this order gives
the permuted string Did you send the letter who to?
3 Training Oracles
In order to train classifiers for transition-based pars-
ing, we need a training oracle, that is, a function
that maps every dependency tree T in the training
set to a transition sequence that derives T . While
every complete transition sequence determines a
unique dependency tree, the inverse does not neces-
sarily hold. This also means that it may be possible
to construct different training oracles. For simple
systems that are restricted to projective dependency
trees, such differences are usually trivial, but for
a system that allows online reordering there may
be genuine differences that can affect both the effi-
ciency and accuracy of the resulting parsers.
3.1 The Old Oracle
Figure 2 defines the original training oracle ?1 pro-
posed by Nivre (2009). This oracle follows an
eager reordering strategy; it predicts SWAP in every
configuration where this is possible. The basic in-
sight in this paper is that, by postponing swaps and
building as much of the tree structure as possible
before swapping, we can significantly decrease the
length of the transition sequence for a given sen-
tence and tree. This may benefit the efficiency of
the parser trained using the oracle, as each trans-
ition takes a certain time to predict and to execute.
Longer transition sequences may also be harder to
learn than shorter ones, which potentially affects
the accuracy of the parser.
3.2 A New Oracle
While it is desirable to delay a SWAP transition
for as long as possible, it is not trivial to find the
right time point to actually do the swap. To see
this, consider the dependency tree in Figure 1. In a
parse of this tree, the first configuration in which
swapping is possible is when who6 and did1 are the
two top nodes on the stack. In this configuration we
can delay the swap until did has combined with its
subject you by means of a RIGHT-ARC transition,
but if we do not swap in the second configuration
where this is possible, we eventually end up with
the stack [ROOT0,who6, did1, send3, to7]. Here we
cannot attach who to to by means of a LEFT-ARC
transition and get stuck.
In order to define the new oracle, we introduce
an auxiliary concept. Consider a modification of
the oracle ?1 from Figure 2 that cannot predict
SWAP transitions. This oracle will be able to pro-
duce valid transition sequences only for projective
target trees; for non-projective trees, it will fail to
reconstruct all dependency arcs. More specifically,
a parse with this oracle will end up in a configur-
ation in which the set of constructed dependency
arcs forms a set of projective dependency trees, not
necessarily a single such tree. We call the elements
of this set the maximal projective components of
the target tree. To illustrate the notion, we have
drawn boxes around nodes in the same component
in Figures 1.
Based on the concept of maximal projective com-
ponents, we define a new training oracle ?2, which
delays swapping as long as the next node in the
input is in the same maximal projective compon-
ent as the top node on the stack. The definition
of the new oracle ?2 is identical to that of ?1 ex-
cept that the third line is replaced by ?SWAP if
c = ([?|i, j], [k|?], Ac), j <G i, and MPC(j) 6=
MPC(k)?, where MPC(i) is the maximal project-
ive component of node i. As a special case, ?2
predicts SWAP if j <G i and the buffer B is empty.
74
?1(c) =
?
????
????
LEFT-ARCl if c = ([?|i, j], B,Ac), (j, l, i)?A and Ai ? Ac
RIGHT-ARCl if c = ([?|i, j], B,Ac), (i, l, j)?A and Aj ? Ac
SWAP if c = ([?|i, j], B,Ac) and j <G i
SHIFT otherwise
Figure 2: Training oracle ?1 for an arbitrary target tree G = (Vx, A), following the notation of Nivre
(2009), where c = (?,B,Ac) denotes a configuration c with stack ?, input buffer B and arc set Ac. We
write Ai to denote the subset of A that only contains the outgoing arcs of the node i. (Note that Ac is the
arc set in configuration c, while A is the arc set in the target tree G.)
For example, in extracting the transition se-
quence for the target tree in Figure 1, the new oracle
will postpone swapping of did when you is the next
node in the input, but not postpone when the next
node is send. We can show that a parser informed
by the new training oracle can always proceed to
a terminal configuration, and still derive all (even
non-projective) dependency trees.
4 Experiments
We now test the hypothesis that the new training
oracle can improve both the accuracy and the ef-
ficiency of a transition-based dependency parser.
Our experiments are based on the same five data
sets as Nivre (2009). The training sets vary in size
from 28,750 tokens (1,534 sentences) for Slovene
to 1,249,408 tokens (72,703 sentences) for Czech,
while the test sets all consist of about 5,000 tokens.
4.1 Number of Transitions
For each language, we first parsed the training set
with both the old and the new training oracle. This
allowed us to compare the number of SWAP trans-
itions needed to parse all sentences with the two
oracles, shown in Table 1. We see that the reduction
is very substantial, ranging from 55% (for Czech)
to almost 84% (for Arabic). While this difference
does not affect the asymptotic complexity of pars-
ing, it may reduce the number of calls to the classi-
fier, which is where transition-based parsers spend
most of their time.
4.2 Parsing Accuracy
In order to assess whether the reduced number of
SWAP transitions also has a positive effect on pars-
ing accuracy, we trained two parsers for each of
the five languages, one for each oracle. All sys-
tems use SVM classifiers with a polynomial kernel
with features and parameters optimized separately
for each language and training oracle. The train-
ing data for these classifiers consist only of the
sequences derived by the oracles, which means that
the parser has no explicit notion of projective order
or maximal projective components at parsing time.
Table 2 shows the labeled parsing accuracy of the
parsers measured by the overall attachment score
(AS), as well as labeled precision, recall and (bal-
anced) F-score for non-projective dependencies.1
For comparison, we also give results for the two
best performing systems in the original CoNLL-X
shared task, Malt (Nivre et al, 2006) and MST (Mc-
Donald et al, 2006), as well as the combo system
MSTMalt, (Nivre and McDonald, 2008).
Looking first at the overall labeled attachment
score, we see that the new training oracle consist-
ently gives higher accuracy than the old one, with
differences of up to 0.5 percentage points (for Ar-
abic and Slovene), which is substantial given that
the frequency of non-projective dependencies is
only 0.4?1.9%. Because the test sets are so small,
none of the differences is statistically significant
(McNemar?s test, ? = .05), but the consistent im-
provement over all languages nevertheless strongly
suggests that this is a genuine difference.
In relation to the state of the art, we note that
the parsers with online reordering significantly out-
perform Malt and MST on Czech and Slovene,
and MST on Turkish, and have significantly lower
scores than the combo system MSTMalt only for
Arabic and Danish. For Czech, the parser with
the new oracle actually has the highest attachment
score ever reported, although the difference with
respect to MSTMalt is not statistically significant.
Turning to the scores for non-projective depend-
encies, we again see that the new oracle consist-
ently gives higher scores than the old oracle, with
1These metrics are not meaningful for Arabic as the test
set only contains 11 non-projective dependencies.
75
Arabic Czech Danish Slovene Turkish
Old (?1) 1416 57011 8296 2191 2828
New (?2) 229 26208 1497 690 1253
Reduction (%) 83.8 55.0 82.0 68.5 55.7
Table 1: Number of SWAP transitions for the old (?1) and new (?2) training oracle.
Arabic Czech Danish Slovene Turkish
System AS AS P R F AS P R F AS P R F AS P R F
Old (?1) 67.2 82.5 74.7 72.9 73.8 84.2 30.0 30.0 30.0 75.2 33.3 26.4 29.5 64.7 12.5 11.4 11.9
New (?2) 67.5 82.7 79.3 71.0 79.3 84.3 38.2 32.5 35.1 75.7 60.6 27.6 37.9 65.0 14.3 13.2 13.7
Malt 66.7 78.4 76.3 57.9 65.8 84.8 45.8 27.5 34.4 70.3 45.9 20.7 25.1 65.7 16.7 9.2 11.9
MST 66.9 80.2 60.5 61.7 61.1 84.8 54.0 62.5 57.9 73.4 33.7 26.4 29.6 63.2 ? 11.8 ?
MSTMalt 68.6 82.3 63.9 69.2 66.1 86.7 63.0 60.0 61.5 75.9 31.6 27.6 29.5 66.3 11.1 9.2 10.1
Table 2: Labeled attachment score (AS) overall; precision (P), recall (R) and balanced F-score (F) for
non-projective dependencies. Old = ?1; New = ?2; Malt = Nivre et al (2006), MST = McDonald et al
(2006), MSTMalt = Nivre and McDonald (2008).
the single exception that the old one has marginally
higher recall for Czech. Moreover, the reordering
parser with the new oracle has higher F-score than
any other system for all languages except Danish.
Especially the result for Czech, with 79.3% preci-
sion and 71.0% recall, is remarkably good, making
the parser almost as accurate for non-projective de-
pendencies as it is for projective dependencies. It
seems likely that the good results for Czech are due
to the fact that Czech has the highest percentage of
non-projective structures in combination with the
(by far) largest training set.
5 Conclusion
We have presented a new training oracle for the
transition system originally presented in Nivre
(2009). This oracle postpones swapping as long as
possible but still fulfills the correctness criterion.
Our experimental results show that the new training
oracle can reduce the necessary number of swaps
by more than 80%, and that parsers trained in this
way achieve higher precision and recall on non-
projective dependency arcs as well as higher at-
tachment score overall. The results are particularly
good for languages with a high percentage of non-
projective dependencies, with an all-time best over
all metrics for Czech.
An interesting theoretical question is whether
the new oracle defined in this paper is optimal with
respect to minimizing the number of swaps. The an-
swer turns out to be negative, and it is possible to re-
duce the number of swaps even further by general-
izing the notion of maximal projective components
to maximal components that may be non-projective.
However, the characterization of these generalized
maximal components is non-trivial, and is therefore
an important problem for future research.
References
Marco Kuhlmann and Joakim Nivre. 2006. Mildly
non-projective dependency structures. In Proceed-
ings of the COLING/ACL 2006 Main Conference
Poster Sessions, pages 507?514.
Ryan McDonald and Giorgio Satta. 2007. On the
complexity of non-projective data-driven depend-
ency parsing. In Proceedings of IWPT, pages 122?
131.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a
two-stage discriminative parser. In Proceedings of
CoNLL, pages 216?220.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency pars-
ers. In Proceedings of ACL, pages 950?958.
Joakim Nivre, Johan Hall, Jens Nilsson, G?lsen Ery-
ig?it, and Svetoslav Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vector
machines. In Proceedings of CoNLL, pages 221?
225.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34:513?553.
Joakim Nivre. 2009. Non-projective dependency pars-
ing in expected linear time. In Proceedings of ACL-
IJCNLP.
76
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 833?841,
Beijing, August 2010
Evaluation of Dependency Parsers on Unbounded Dependencies
Joakim Nivre Laura Rimell Ryan McDonald Carlos Go?mez-Rodr??guez
Uppsala University Univ. of Cambridge Google Inc. Universidade da Corun?a
joakim.nivre@lingfil.uu.se laura.rimell@cl.cam.ac.uk ryanmcd@google.com cgomezr@udc.es
Abstract
We evaluate two dependency parsers,
MSTParser and MaltParser, with respect
to their capacity to recover unbounded de-
pendencies in English, a type of evalu-
ation that has been applied to grammar-
based parsers and statistical phrase struc-
ture parsers but not to dependency parsers.
The evaluation shows that when combined
with simple post-processing heuristics,
the parsers correctly recall unbounded
dependencies roughly 50% of the time,
which is only slightly worse than two
grammar-based parsers specifically de-
signed to cope with such dependencies.
1 Introduction
Though syntactic parsers for English are re-
ported to have accuracies over 90% on the Wall
Street Journal (WSJ) section of the Penn Tree-
bank (PTB) (McDonald et al, 2005; Sagae and
Lavie, 2006; Huang, 2008; Carreras et al, 2008),
broad-coverage parsing is still far from being a
solved problem. In particular, metrics like attach-
ment score for dependency parsers (Buchholz and
Marsi, 2006) and Parseval for constituency parsers
(Black et al, 1991) suffer from being an aver-
age over a highly skewed distribution of differ-
ent grammatical constructions. As a result, in-
frequent yet semantically important construction
types could be parsed with accuracies far below
what one might expect.
This shortcoming of aggregate parsing met-
rics was highlighted in a recent study by Rimell
et al (2009), introducing a new parser evalua-
tion corpus containing around 700 sentences an-
notated with unbounded dependencies in seven
different grammatical constructions. This corpus
was used to evaluate five state-of-the-art parsers
for English, focusing on grammar-based and sta-
tistical phrase structure parsers. For example, in
the sentence By Monday, they hope to have a
sheaf of documents both sides can trust., parsers
should recognize that there is a dependency be-
tween trust and documents, an instance of object
extraction out of a (reduced) relative clause. In the
evaluation, the recall of state-of-the-art parsers on
this kind of dependency varies from a high of 65%
to a low of 1%. When averaging over the seven
constructions in the corpus, none of the parsers
had an accuracy higher than 61%.
In this paper, we extend the evaluation of
Rimell et al (2009) to two dependency parsers,
MSTParser (McDonald, 2006) and MaltParser
(Nivre et al, 2006a), trained on data from the
PTB, converted to Stanford typed dependencies
(de Marneffe et al, 2006), and combined with a
simple post-processor to extract unbounded de-
pendencies from the basic dependency tree. Ex-
tending the evaluation to dependency parsers is of
interest because it sheds light on whether highly
tuned grammars or computationally expensive
parsing formalisms are necessary for extracting
complex linguistic phenomena in practice. Unlike
the best performing grammar-based parsers stud-
ied in Rimell et al (2009), neither MSTParser nor
MaltParser was developed specifically as a parser
for English, and neither has any special mecha-
nism for dealing with unbounded dependencies.
Dependency parsers are also often asymptotically
faster than grammar-based or constituent parsers,
e.g., MaltParser parses sentences in linear time.
Our evaluation ultimately shows that the re-
call of MSTParser and MaltParser on unbounded
dependencies is much lower than the average
(un)labeled attachment score for each system.
Nevertheless, the two dependency parsers are
found to perform only slightly worse than the best
grammar-based parsers evaluated in Rimell et al
833
Each must match Wisman 's "pie" with the fragment that he carries with him
nsubj dobj
prepaux pobjposs've
prep rcmoddobj
nsubjdet
pobj
poss
dobj
a: ObRC
Five things you can do for $ 15,000  or less
pobjnsubjaux
rcmod
num num
prep cc conj
dobj
b: ObRed
They will remain on a lower-priority list that includes 17 other countries
pobjnsubj
aux
rcmod
nsubj
num
prep amod
det
nsubj
c: SbRC
amod
dobj
What you see are self-help projects
nsubj
csubj cop
amod
dobj
dobj d: Free
What effect does a prism have on light
pobjnsubj
aux
det
det prep
dobj
dobj
e: ObQ
Now he felt ready for the many actions he saw spreading out before him
pobj rcmod
prtmod
detprepacompnsubj
nsubj
amod xcompnsubj
prep
pobj
g: SbEmadv
The men were at first puzz led, then angered by the aimless tacking
pobj
cop conj
advmod
detadvmod
prep
det
prep
amod
pobjnsubjpass
f : RNR
Figure 1: Examples of seven unbounded dependency constructions (a?g). Arcs drawn below each sentence represent the
dependencies scored in the evaluation, while the tree above each sentence is the Stanford basic dependency representation,
with solid arcs indicating crucial dependencies (cf. Section 4). All examples are from the development sets.
(2009) and considerably better than the other sta-
tistical parsers in that evaluation. Interestingly,
though the two systems have similar accuracies
overall, there is a clear distinction between the
kinds of errors each system makes, which we ar-
gue is consistent with observations by McDonald
and Nivre (2007).
2 Unbounded Dependency Evaluation
An unbounded dependency involves a word or
phrase interpreted at a distance from its surface
position, where an unlimited number of clause
boundaries may in principle intervene. The
unbounded dependency corpus of Rimell et al
(2009) includes seven grammatical constructions:
object extraction from a relative clause (ObRC),
object extraction from a reduced relative clause
(ObRed), subject extraction from a relative clause
(SbRC), free relatives (Free), object questions
(ObQ), right node raising (RNR), and subject ex-
traction from an embedded clause (SbEm), all
chosen for being relatively frequent and easy to
identify in PTB trees. Examples of the con-
structions can be seen in Figure 1. The evalu-
ation set contains 80 sentences per construction
(which may translate into more than 80 depen-
dencies, since sentences containing coordinations
may have more than one gold-standard depen-
dency), while the development set contains be-
tween 13 and 37 sentences per construction. The
data for ObQ sentences was obtained from various
years of TREC, and for the rest of the construc-
tions from the WSJ (0-1 and 22-24) and Brown
sections of the PTB.
Each sentence is annotated with one or more
gold-standard dependency relations representing
the relevant unbounded dependency. The gold-
standard dependencies are shown as arcs below
the sentences in Figure 1. The format of the de-
pendencies in the corpus is loosely based on the
Stanford typed dependency scheme, although the
evaluation procedure permits alternative represen-
tations and does not require that the parser out-
put match the gold-standard exactly, as long as the
?spirit? of the construction is correct.
The ability to recover unbounded dependencies
is important because they frequently form part of
the basic predicate-argument structure of a sen-
tence. Subject and object dependencies in par-
ticular are crucial for a number of tasks, includ-
ing information extraction and question answer-
ing. Moreover, Rimell et al (2009) show that,
although individual types of unbounded depen-
dencies may be rare, the unbounded dependency
types in the corpus, considered as a class, occur in
as many as 10% of sentences in the PTB.
In Rimell et al (2009), five state-of-the-art
parsers were evaluated for their recall on the gold-
standard dependencies. Three of the parsers were
based on grammars automatically extracted from
the PTB: the C&C CCG parser (Clark and Curran,
2007), the Enju HPSG parser (Miyao and Tsujii,
2005), and the Stanford parser (Klein and Man-
ning, 2003). The two remaining systems were the
834
RASP parser (Briscoe et al, 2006), using a man-
ually constructed grammar and a statistical parse
selection component, and the DCU post-processor
of PTB parsers (Cahill et al, 2004) using the out-
put of the Charniak and Johnson reranking parser
(Charniak and Johnson, 2005). Because of the
wide variation in parser output representations, a
mostly manual evaluation was performed to en-
sure that each parser got credit for the construc-
tions it recovered correctly. The parsers were run
essentially ?out of the box?, meaning that the de-
velopment set was used to confirm input and out-
put formats, but no real tuning was performed. In
addition, since a separate question model is avail-
able for C&C, this was also evaluated on ObQ
sentences. The best overall performers were C&C
and Enju, which is unsurprising since they are
deep parsers based on grammar formalisms de-
signed to recover just such dependencies. The
DCU post-processor performed somewhat worse
than expected, often identifying the existence of
an unbounded dependency but failing to iden-
tify the grammatical class (subject, object, etc.).
RASP and Stanford, although not designed to re-
cover such dependencies, nevertheless recovered
a subset of them. Performance of the parsers also
varied widely across the different constructions.
3 Dependency Parsers
In this paper we repeat the study of Rimell et al
(2009) for two dependency parsers, with the goal
of evaluating how parsers based on dependency
grammars perform on unbounded dependencies.
MSTParser1 is a freely available implementa-
tion of the parsing models described in McDon-
ald (2006). According to the categorization of
parsers in Ku?bler et al (2008) it is a graph-based
parsing system in that core parsing algorithms can
be equated to finding directed maximum span-
ning trees (either projective or non-projective)
from a dense graph representation of the sentence.
Graph-based parsers typically rely on global train-
ing and inference algorithms, where the goal is to
learn models in which the weight/probability of
correct trees is higher than that of incorrect trees.
At inference time a global search is run to find the
1http://mstparser.sourceforge.net
highest weighted dependency tree. Unfortunately,
global inference and learning for graph-based de-
pendency parsing is typically NP-hard (McDonald
and Satta, 2007). As a result, graph-based parsers
(including MSTParser) often limit the scope of
their features to a small number of adjacent arcs
(usually two) and/or resort to approximate infer-
ence (McDonald and Pereira, 2006).
MaltParser2 is a freely available implementa-
tion of the parsing models described in Nivre et
al. (2006a) and Nivre et al (2006b). MaltParser is
categorized as a transition-based parsing system,
characterized by parsing algorithms that produce
dependency trees by transitioning through abstract
state machines (Ku?bler et al, 2008). Transition-
based parsers learn models that predict the next
state given the current state of the system as well
as features over the history of parsing decisions
and the input sentence. At inference time, the
parser starts in an initial state, then greedily moves
to subsequent states ? based on the predictions of
the model ? until a termination state is reached.
Transition-based parsing is highly efficient, with
run-times often linear in sentence length. Further-
more, transition-based parsers can easily incorpo-
rate arbitrary non-local features, since the current
parse structure is fixed by the state. However, the
greedy nature of these systems can lead to error
propagation if early predictions place the parser
in incorrect states.
McDonald and Nivre (2007) compared the ac-
curacy of MSTParser and MaltParser along a
number of structural and linguistic dimensions.
They observed that, though the two parsers ex-
hibit indistinguishable accuracies overall, MST-
Parser tends to outperform MaltParser on longer
dependencies as well as those dependencies closer
to the root of the tree (e.g., verb, conjunction and
preposition dependencies), whereas MaltParser
performs better on short dependencies and those
further from the root (e.g., pronouns and noun de-
pendencies). Since long dependencies and those
near to the root are typically the last constructed
in transition-based parsing systems, it was con-
cluded that MaltParser does suffer from some
form of error propagation. On the other hand, the
2http://www.maltparser.org
835
richer feature representations of MaltParser led to
improved performance in cases where error prop-
agation has not occurred. However, that study did
not investigate unbounded dependencies.
4 Methodology
In this section, we describe the methodological
setup for the evaluation, including parser training,
post-processing, and evaluation.3
4.1 Parser Training
One important difference between MSTParser and
MaltParser, on the one hand, and the best perform-
ing parsers evaluated in Rimell et al (2009), on
the other, is that the former were never developed
specifically as parsers for English. Instead, they
are best understood as data-driven parser gener-
ators, that is, tools for generating a parser given
a training set of sentences annotated with de-
pendency structures. Over the years, both sys-
tems have been applied to a wide range of lan-
guages (see, e.g., McDonald et al (2006), Mc-
Donald (2006), Nivre et al (2006b), Hall et al
(2007), Nivre et al (2007)), but they come with
no language-specific enhancements and are not
equipped specifically to deal with unbounded de-
pendencies.
Since the dependency representation used in
the evaluation corpus is based on the Stanford
typed dependency scheme (de Marneffe et al,
2006), we opted for using the WSJ section of
the PTB, converted to Stanford dependencies, as
our primary source of training data. Thus, both
parsers were trained on section 2?21 of the WSJ
data, which we converted to Stanford dependen-
cies using the Stanford parser (Klein and Man-
ning, 2003). The Stanford scheme comes in sev-
eral varieties, but because both parsers require the
dependency structure for each sentence to be a
tree, we had to use the so-called basic variety (de
Marneffe et al, 2006).
It is well known that questions are very rare
in the WSJ data, and Rimell et al (2009) found
that parsers trained only on WSJ data generally
performed badly on the questions included in the
3To ensure replicability, we provide all experimental
settings, post-processing scripts and additional information
about the evaluation at http://stp.ling.uu.se/?nivre/exp/.
evaluation corpus, while the C&C parser equipped
with a model trained on a combination of WSJ
and question data had much better performance.
To investigate whether the performance of MST-
Parser and MaltParser on questions could also be
improved by adding more questions to the train-
ing data, we trained one variant of each parser
using data that was extended with 3924 ques-
tions taken from QuestionBank (QB) (Judge et al,
2006).4 Since the QB sentences are annotated in
PTB style, it was possible to use the same conver-
sion procedure as for the WSJ data. However, it is
clear that the conversion did not always produce
adequate dependency structures for the questions,
an observation that we will return to in the error
analysis below.
In comparison to the five parsers evaluated in
Rimell et al (2009), it is worth noting that MST-
Parser and MaltParser were trained on the same
basic data as four of the five, but with a differ-
ent kind of syntactic representation ? dependency
trees instead of phrase structure trees or theory-
specific representations from CCG and HPSG. It
is especially interesting to compare MSTParser
and MaltParser to the Stanford parser, which es-
sentially produces the same kind of dependency
structures as output but uses the original phrase
structure trees from the PTB as input to training.
For our experiments we used MSTParser with
the same parsing algorithms and features as re-
ported in McDonald et al (2006). However, un-
like that work we used an atomic maximum en-
tropy model as the second stage arc predictor as
opposed to the more time consuming sequence la-
beler. McDonald et al (2006) showed that there is
negligible accuracy loss when using atomic rather
than structured labeling. For MaltParser we used
the projective Stack algorithm (Nivre, 2009) with
default settings and a slightly enriched feature
model. All parsing was projective because the
Stanford dependency trees are strictly projective.
4QB contains 4000 questions, but we removed all ques-
tions that also occurred in the test or development set of
Rimell et al (2009), who sampled their questions from the
same TREC QA test sets.
836
4.2 Post-Processing
All the development and test sets in the corpus
of Rimell et al (2009) were parsed using MST-
Parser and MaltParser after part-of-speech tagging
the input using SVMTool (Gime?nez and Ma`rquez,
2004) trained on section 2?21 of the WSJ data in
Stanford basic dependency format. The Stanford
parser has an internal module that converts the
basic dependency representation to the collapsed
representation, which explicitly represents addi-
tional dependencies, including unbounded depen-
dencies, that can be inferred from the basic rep-
resentation (de Marneffe et al, 2006). We per-
formed a similar conversion using our own tool.
Broadly speaking, there are three ways in which
unbounded dependencies can be inferred from the
Stanford basic dependency trees, which we will
refer to as simple, complex, and indirect. In the
simple case, the dependency coincides with a sin-
gle, direct dependency relation in the tree. This
is the case, for example, in Figure 1d?e, where
all that is required is that the parser identifies
the dependency relation from a governor to an
argument (dobj(see, What), dobj(have,
effect)), which we call the Arg relation; no
post-processing is needed.
In the complex case, the dependency is repre-
sented by a path of direct dependencies in the tree,
as exemplified in Figure 1a. In this case, it is
not enough that the parser correctly identifies the
Arg relation dobj(carries, that); it must
also find the dependency rcmod(fragment,
carries). We call this the Link relation, be-
cause it links the argument role inside the relative
clause to an element outside the clause. Other ex-
amples of the complex case are found in Figure 1c
and in Figure 1f.
In the indirect case, finally, the dependency
cannot be defined by a path of labeled depen-
dencies, whether simple or complex, but must
be inferred from a larger context of the tree us-
ing heuristics. Consider Figure 1b, where there
is a Link relation (rcmod(things, do)), but
no corresponding Arg relation inside the relative
clause (because there is no overt relative pro-
noun). However, given the other dependencies,
we can infer with high probability that the im-
plicit relation is dobj. Another example of the
indirect case is in Figure 1g. Our post-processing
tool performs more heuristic inference for the in-
direct case than the Stanford parser does (cf. Sec-
tion 4.3).
In order to handle the complex and indirect
cases, our post-processor is triggered by the oc-
currence of a Link relation (rcmod or conj) and
first tries to add dependencies that are directly im-
plied by a single Arg relation (relations involving
relative pronouns for rcmod, shared heads and
dependents for conj). If there is no overt rela-
tive pronoun, or the function of the relative pro-
noun is underspecified, the post-processor relies
on the obliqueness hierarchy subj < dobj <
pobj and simply picks the first ?missing func-
tion?, unless it finds a clausal complement (indi-
cated by the labels ccomp and xcomp), in which
case it descends to the lower clause and restarts
the search there.
4.3 Parser Evaluation
The evaluation was performed using the same cri-
teria as in Rimell et al (2009). A dependency
was considered correctly recovered if the gold-
standard head and dependent were correct and
the label was an ?acceptable match? to the gold-
standard label, indicating the grammatical func-
tion of the extracted element at least to the level
of subject, passive subject, object, or adjunct.
The evaluation in Rimell et al (2009) took
into account a wide variety of parser output for-
mats, some of which differed significantly from
the gold-standard. Since MSTParser and Malt-
Parser produced Stanford dependencies for this
experiment, evaluation required less manual ex-
amination than for some of the other parsers, as
was also the case for the output of the Stanford
parser in the original evaluation. However, a man-
ual evaluation was still performed in order to re-
solve questionable cases.
5 Results
The results are shown in Table 1, where the ac-
curacy for each construction is the percentage of
gold-standard dependencies recovered correctly.
The Avg column represents a macroaverage, i.e.
the average of the individual scores on the seven
constructions, while the WAvg column represents
837
Parser ObRC ObRed SbRC Free ObQ RNR SbEm Avg WAvg
MST 34.1 47.3 78.9 65.5 13.8 45.4 37.6 46.1 63.4
Malt 40.7 50.5 84.2 70.2 16.2 39.7 23.5 46.4 66.9
MST-Q 41.2 50.0
Malt-Q 31.2 48.5
Table 1: Parser accuracy on the unbounded dependency corpus.
Parser ObRC ObRed SbRC Free ObQ RNR SbEm Avg WAvg
C&C 59.3 62.6 80.0 72.6 81.2 49.4 22.4 61.1 69.9
Enju 47.3 65.9 82.1 76.2 32.5 47.1 32.9 54.9 70.9
MST 34.1 47.3 78.9 65.5 41.2 45.4 37.6 50.0 63.4
Malt 40.7 50.5 84.2 70.2 31.2 39.7 23.5 48.5 66.9
DCU 23.1 41.8 56.8 46.4 27.5 40.8 5.9 34.6 47.0
RASP 16.5 1.1 53.7 17.9 27.5 34.5 15.3 23.8 34.1
Stanford 22.0 1.1 74.7 64.3 41.2 45.4 10.6 37.0 50.3
Table 2: Parser accuracy on the unbounded dependency corpus. The ObQ score for C&C, MSTParser, and MaltParser is for
a model trained with additional questions (without this C&C scored 27.5; MSTParser and MaltParser as in Table 1).
a weighted macroaverage, where the construc-
tions are weighted proportionally to their relative
frequency in the PTB. WAvg excludes ObQ sen-
tences, since frequency statistics were not avail-
able for this construction in Rimell et al (2009).
Our first observation is that the accuracies for
both systems are considerably below the ?90%
unlabeled and ?88% labeled attachment scores
for English that have been reported previously
(McDonald and Pereira, 2006; Hall et al, 2006).
Comparing the two parsers, we see that Malt-
Parser is more accurate on dependencies in rela-
tive clause constructions (ObRC, ObRed, SbRC,
and Free), where argument relations tend to be
relatively local, while MSTParser is more accu-
rate on dependencies in RNR and SbEm, which
involve more distant relations. Without the ad-
ditional QB training data, the average scores for
the two parsers are indistinguishable, but MST-
Parser appears to have been better able to take
advantage of the question training, since MST-Q
performs better than Malt-Q on ObQ sentences.
On the weighted average MaltParser scores 3.5
points higher, because the constructions on which
it outperforms MSTParser are more frequent in
the PTB, and because WAvg excludes ObQ, where
MSTParser is more accurate.
Table 2 shows the results for MSTParser and
MaltParser in the context of the other parsers eval-
uated in Rimell et al (2009).5 For the parsers
5The average scores reported differ slightly from those in
which have a model trained on questions, namely
C&C, MSTParser, and MaltParser, the figure
shown for ObQ sentences is that of the question
model. It can be seen that MSTParser and Malt-
Parser perform below C&C and Enju, but above
the other parsers, and that MSTParser achieves the
highest score on SbEm sentences and MaltParser
on SbRC sentences. It should be noted, however,
that Table 2 does not represent a direct compar-
ison across all parsers, since most of the other
parsers would have benefited from heuristic post-
processing of the kind implemented here for MST-
Parser and MaltParser. This is especially true for
RASP, where the grammar explicitly leaves some
types of attachment decisions for post-processing.
For DCU, improved labeling heuristics would sig-
nificantly improve performance. It is instructive to
compare the dependency parsers to the Stanford
parser, which uses the same output representation
and has been used to prepare the training data for
our experiments. Stanford has very low recall on
ObRed and SbEm, the categories where heuristic
inference plays the largest role, but mirrors MST-
Parser for most other categories.
6 Error Analysis
We now proceed to a more detailed error analy-
sis, based on the development sets, and classify
Rimell et al (2009), where a microaverage (i.e., average over
all dependencies in the corpus, regardless of construction)
was reported.
838
the errors made by the parsers into three cate-
gories: A global error is one where the parser
completely fails to build the relevant clausal struc-
ture ? the relative clause in ObRC, ObRed, SbRC,
Free, SbEmb; the interrogative clause in ObQ; and
the clause headed by the higher conjunct in RNR
? often as a result of surrounding parsing errors.
When a global error occurs, it is usually mean-
ingless to further classify the error, which means
that this category excludes the other two. An Arg
error is one where the parser has constructed the
relevant clausal structure but fails to find the Arg
relation ? in the simple and complex cases ? or the
set of surrounding Arg relations needed to infer
an implicit Arg relation ? in the indirect case (cf.
Section 4.2). A Link error is one where the parser
fails to find the crucial Link relation ? rcmod
in ObRC, ObRed, SbRC, SbEmb; conj in RNR
(cf. Section 4.2). Link errors are not relevant for
Free and ObQ, where all the crucial relations are
clause-internal.
Table 3 shows the frequency of different error
types for MSTParser (first) and MaltParser (sec-
ond) in the seven development sets. First of all,
we can see that the overall error distribution is
very similar for the two parsers, which is proba-
bly due to the fact that they have been trained on
exactly the same data with exactly the same an-
notation (unlike the five parsers previously eval-
uated). However, there is a tendency for MST-
Parser to make fewer Link errors, especially in
the relative clause categories ObRC, ObRed and
SbRC, which is compatible with the observation
from the test results that MSTParser does better
on more global dependencies, while MaltParser
has an advantage on more local dependencies, al-
though this is not evident from the statistics from
the relatively small development set.
Comparing the different grammatical construc-
tions, we see that Link errors dominate for the rel-
ative clause categories ObRC, ObRed and SbRC,
where the parsers make very few errors with
respect to the internal structure of the relative
clauses (in fact, no errors at all for MaltParser
on SbRC). This is different for SbEm, where the
analysis of the argument structure is more com-
plex, both because there are (at least) two clauses
involved and because the unbounded dependency
Type Glo
bal
Arg Lin
k
A+
L
Err
ors
# D
eps
ObRC 0/1 1/1 7/11 5/3 13/16 20
ObRed 0/1 0/1 6/7 3/4 9/13 23
SbRC 2/1 1/0 7/13 0/0 10/14 43
Free 2/1 3/5 ? ? 5/6 22
ObQ 4/7 13/13 ? ? 17/20 25
RNR 6/4 4/6 0/0 4/5 14/15 28
SbEm 3/4 3/2 0/0 3/3 9/9 13
Table 3: Distribution of error types in the development
sets; frequencies for MSTParser listed first and MaltParser
second. The columns Arg and Link give frequencies for
Arg/Link errors occurring without the other error type, while
A+L give frequencies for joint Arg and Link errors.
can only be inferred indirectly from the basic de-
pendency representation (cf. Section 4.2). An-
other category where Arg errors are frequent is
RNR, where all such errors consist in attaching
the relevant dependent to the second conjunct in-
stead of to the first.6 Thus, in the example in Fig-
ure 1f, both parsers found the conj relation be-
tween puzzled and angered but attached by to the
second verb.
Global errors are most frequent for RNR, prob-
ably indicating that coordinate structures are diffi-
cult to parse in general, and for ObQ (especially
for MaltParser), probably indicating that ques-
tions are not well represented in the training set
even after the addition of QB data.7 As noted
in Section 4.1, this may be partly due to the fact
that conversion to Stanford dependencies did not
seem to work as well for QB as for the WSJ data.
Another problem is that the part-of-speech tagger
used was trained on WSJ data only and did not
perform as well on the ObQ data. Uses of What as
a determiner were consistently mistagged as pro-
nouns, which led to errors in parsing. Thus, for
the example in Figure 1e, both parsers produced
the correct analysis except that, because of the tag-
ging error, they treated What rather than effect as
the head of the wh-phrase, which counts as an er-
ror in the evaluation.
In order to get a closer look specifically at the
Arg errors, Table 4 gives the confusion matrix
6In the Stanford scheme, an argument or adjunct must be
attached to the first conjunct in a coordination to indicate that
it belongs to both conjuncts.
7Parsers trained without QB had twice as many global
errors.
839
Sb Ob POb EmSb EmOb Other Total
Sb ? 0/0 0/0 0/0 0/0 2/1 2/1
Ob 2/3 ? 0/0 0/1 0/0 4/2 6/6
POb 2/0 7/5 ? 0/0 0/0 5/8 14/13
EmSb 1/1 4/2 0/0 ? 0/0 1/2 6/5
EmOb 0/0 3/1 0/0 0/0 ? 1/6 4/7
Total 5/4 14/8 0/0 0/1 0/0 13/19 32/32
Table 4: Confusion matrix for Arg errors (excluding RNR
and using parsers trained on QB for ObQ); frequencies for
MSTParser listed first and MaltParser second. The column
Other covers errors where the function is left unspecified or
the argument is attached to the wrong head.
for such errors, showing which grammatical func-
tions are mistaken for each other, with an extra
category Other for cases where the function is left
unspecified by the parser or the error is an attach-
ment error rather than a labeling error (and ex-
cluding the RNR category because of the special
nature of the Arg errors in this category). The
results again confirm that the two parsers make
very few errors on subjects and objects clause-
internally. The few cases where an object is
mistaken as a subject occur in ObQ, where both
parsers perform rather poorly in general. By con-
trast, there are many more errors on prepositional
objects and on embedded subjects and objects. We
believe an important part of the explanation for
this pattern is to be found in the Stanford depen-
dency representation, where subjects and objects
are marked as such but all other functions real-
ized by wh elements are left unspecified (using the
generic rel dependency), which means that the re-
covery of these functions currently has to rely on
heuristic rules as described in Section 4.2. Finally,
we think it is possible to observe the tendency for
MaltParser to be more accurate at local labeling
decisions ? reflected in fewer cross-label confu-
sions ? and for MSTParser to perform better on
more distant attachment decisions ? reflected in
fewer errors in the Other category (and in fewer
Link errors).
7 Conclusion
In conclusion, the capacity of MSTParser and
MaltParser to recover unbounded dependencies is
very similar on the macro and weighted macro
level, but there is a clear distinction in their
strengths ? constructions involving more distant
dependencies such as ObQ, RNR and SbEm for
MSTParser and constructions with more locally
defined configurations such as ObRC, ObRed,
SbRC and Free for MaltParser. This is a pattern
that has been observed in previous evaluations of
the parsers and can be explained by the global
learning and inference strategy of MSTParser and
the richer feature space of MaltParser (McDonald
and Nivre, 2007).
Perhaps more interestingly, the accuracies of
MSTParser and MaltParser are only slightly be-
low the best performing systems in Rimell et al
(2009) ? C&C and Enju. This is true even though
MSTParser and MaltParser have not been engi-
neered specifically for English and lack special
mechanisms for handling unbounded dependen-
cies, beyond the simple post-processing heuristic
used to extract them from the output trees. Thus,
it is reasonable to speculate that the addition of
such mechanisms could lead to computationally
lightweight parsers with the ability to extract un-
bounded dependencies with high accuracy.
Acknowledgments
We thank Marie-Catherine de Marneffe for great
help with the Stanford parser and dependency
scheme, Llu??s Ma`rquez and Jesu?s Gime?nez for
great support with SVMTool, Josef van Gen-
abith for sharing the QuestionBank data, and
Stephen Clark and Mark Steedman for helpful
comments on the evaluation process and the pa-
per. Laura Rimell was supported by EPSRC grant
EP/E035698/1 and Carlos Go?mez-Rodr??guez
by MEC/FEDER (HUM2007-66607-C04) and
Xunta de Galicia (PGIDIT07SIN005206PR, Re-
des Galegas de PL e RI e de Ling. de Corpus,
Bolsas Estadas INCITE/FSE cofinanced).
References
Black, E., S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, S. Roukos, B. Santorini,
and T. Strzalkowski. 1991. A procedure for quanti-
tatively comparing the syntactic coverage of English
grammars. In Proceedings of 4th DARPAWorkshop,
306?311.
Briscoe, T., J. Carroll, and R. Watson. 2006. The sec-
ond release of the RASP system. In Proceedings
840
of the COLING/ACL 2006 Interactive Presentation
Sessions, 77?80.
Buchholz, S. and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Pro-
ceedings of CoNLL, 149?164.
Cahill, A., M. Burke, R. O?Donovan, J. Van Genabith,
and A. Way. 2004. Long-distance dependency
resolution in automatically acquired wide-coverage
PCFG-based LFG approximations. In Proceedings
of ACL, 320?327.
Carreras, X., M. Collins, and T. Koo. 2008. TAG,
dynamic programming, and the perceptron for ef-
ficient, feature-rich parsing. In Proceedings of
CoNLL, 9?16.
Charniak, E. and M. Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking.
In Proceedings of ACL, 173?180.
Clark, S. and J. R. Curran. 2007. Wide-coverage ef-
ficient statistical parsing with CCG and log-linear
models. Computational Linguistics, 33:493?552.
de Marneffe, M.-C., B. MacCartney, and C. D. Man-
ning. 2006. Generating typed dependency parses
from phrase structure parses. In Proceedings of
LREC.
Gime?nez, J. and L. Ma`rquez. 2004. SVMTool: A gen-
eral POS tagger generator based on support vector
machines. In Proceedings of LREC.
Hall, J., J. Nivre, and J. Nilsson. 2006. Discriminative
classifiers for deterministic dependency parsing. In
Proceedings of the COLING/ACL 2006 Main Con-
ference Poster Sessions, 316?323.
Hall, J., J. Nilsson, J. Nivre, G. Eryig?it, B. Megyesi,
M. Nilsson, and M. Saers. 2007. Single malt or
blended? A study in multilingual parser optimiza-
tion. In Proceedings of the CoNLL Shared Task.
Huang, L. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL, 586?594.
Judge, J., A. Cahill, and J. van Genabith. 2006. Ques-
tionBank: Creating a corpus of parse-annotated
questions. In Proceedings of COLING-ACL, 497?
504.
Klein, D. and C. D. Manning. 2003. Accurate unlexi-
calized parsing. In Proceedings of ACL, 423?430.
Ku?bler, S., R. McDonald, and J. Nivre. 2008. Depen-
dency Parsing. Morgan and Claypool.
McDonald, R. and J. Nivre. 2007. Characterizing
the errors of data-driven dependency parsing mod-
els. In Proceedings of EMNLP-CoNLL, 122?131.
McDonald, R. and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proceedings of EACL, 81?88.
McDonald, R. and G. Satta. 2007. On the complexity
of non-projective data-driven dependency parsing.
In Proceedings of IWPT, 122?131.
McDonald, R., K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of ACL, 91?98.
McDonald, R., K. Lerman, and F. Pereira. 2006. Mul-
tilingual dependency analysis with a two-stage dis-
criminative parser. In Proceedings of CoNLL, 216?
220.
McDonald, R.. 2006. Discriminative Learning and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Miyao, Y. and J. Tsujii. 2005. Probabilistic disam-
biguation models for wide-coverage HPSG parsing.
In Proceedings of ACL, 83?90.
Nivre, J., J. Hall, and J. Nilsson. 2006a. MaltParser:
A data-driven parser-generator for dependency pars-
ing. In Proceedings of LREC, 2216?2219.
Nivre, J., J. Hall, J. Nilsson, G. Eryig?it, and S. Mari-
nov. 2006b. Labeled pseudo-projective dependency
parsing with support vector machines. In Proceed-
ings of CoNLL, 221?225.
Nivre, J., J. Hall, J. Nilsson, A. Chanev, G. Eryig?it,
S. Ku?bler, S. Marinov, and E. Marsi. 2007. Malt-
parser: A language-independent system for data-
driven dependency parsing. Natural Language En-
gineering, 13:95?135.
Nivre, J. 2009. Non-projective dependency parsing
in expected linear time. In Proceedings of ACL-
IJCNLP, 351?359.
Rimell, L., S. Clark, and M. Steedman. 2009. Un-
bounded dependency recovery for parser evaluation.
In Proceedings EMNLP, 813?821.
Sagae, K. and A. Lavie. 2006. Parser combination
by reparsing. In Proceedings of NAACL HLT: Short
Papers, 129?132.
841
Coling 2010: Poster Volume, pages 108?116,
Beijing, August 2010
Benchmarking of Statistical Dependency Parsers for French
Marie Candito!, Joakim Nivre!, Pascal Denis! and Enrique Henestroza Anguiano!
! Alpage (Universit? Paris 7/INRIA)
! Uppsala University, Department of Linguistics and Philology
marie.candito@linguist.jussieu.fr {pascal.denis, henestro}@inria.fr joakim.nivre@ling?l.uu.se
Abstract
We compare the performance of three
statistical parsing architectures on the
problem of deriving typed dependency
structures for French. The architectures
are based on PCFGs with latent vari-
ables, graph-based dependency parsing
and transition-based dependency parsing,
respectively. We also study the in?u-
ence of three types of lexical informa-
tion: lemmas, morphological features,
and word clusters. The results show that
all three systems achieve competitive per-
formance, with a best labeled attachment
score over 88%. All three parsers bene?t
from the use of automatically derived lem-
mas, while morphological features seem
to be less important. Word clusters have a
positive effect primarily on the latent vari-
able parser.
1 Introduction
In this paper, we compare three statistical parsers
that produce typed dependencies for French. A
syntactic analysis in terms of typed grammatical
relations, whether encoded as functional annota-
tions in syntagmatic trees or in labeled depen-
dency trees, appears to be useful for many NLP
tasks including question answering, information
extraction, and lexical acquisition tasks like collo-
cation extraction.
This usefulness holds particularly for French,
a language for which bare syntagmatic trees
are often syntactically underspeci?ed because
of a rather free order of post-verbal comple-
ments/adjuncts and the possibility of subject in-
version. Thus, the annotation scheme of the
French Treebank (Abeill? and Barrier, 2004)
makes use of ?at syntagmatic trees without VP
nodes, with no structural distinction between
complements, adjuncts or post-verbal subjects,
but with additional functional annotations on de-
pendents of verbs.
Parsing is commonly enhanced by using more
abstract lexical information, in the form of mor-
phological features (Tsarfaty, 2006), lemmas
(Seddah et al, 2010), or various forms of clusters
(see (Candito and Seddah, 2010) for references).
In this paper, we explore the integration of mor-
phological features, lemmas, and linear context
clusters.
Typed dependencies can be derived using many
different parsing architectures. As far as statistical
approaches are concerned, the dominant paradigm
for English has been to use constituency-based
parsers, the output of which can be converted
to typed dependencies using well-proven conver-
sion procedures, as in the Stanford parser (Klein
and Manning, 2003). In recent years, it has
also become popular to use statistical dependency
parsers, which are trained directly on labeled de-
pendency trees and output such trees directly, such
as MSTParser (McDonald, 2006) and MaltParser
(Nivre et al, 2006). Dependency parsing has been
applied to a fairly broad range of languages, espe-
cially in the CoNLL shared tasks in 2006 and 2007
(Buchholz and Marsi, 2006; Nivre et al, 2007).
We present a comparison of three statistical
parsing architectures that output typed dependen-
cies for French: one constituency-based architec-
ture featuring the Berkeley parser (Petrov et al,
2006), and two dependency-based systems using
radically different parsing methods, MSTParser
(McDonald et al, 2006) and MaltParser (Nivre et
al., 2006). These three systems are compared both
in terms of parsing accuracy and parsing times, in
realistic settings that only use predicted informa-
tion. By using freely available software packages
that implement language-independent approaches
108
and applying them to a language different from
English, we also hope to shed some light on the
capacity of different methods to cope with the
challenges posed by different languages.
Comparative evaluation of constituency-based
and dependency-based parsers with respect to la-
beled accuracy is rare, despite the fact that parser
evaluation on typed dependencies has been ad-
vocated for a long time (Lin, 1995; Carroll et
al., 1998). Early work on statistical dependency
parsing often compared constituency-based and
dependency-based methods with respect to their
unlabeled accuracy (Yamada and Matsumoto,
2003), but comparison of different approaches
with respect to labeled accuracy is more recent.
Cer et al (2010) present a thorough analysis of
the best trade-off between speed and accuracy in
deriving Stanford typed dependencies for English
(de Marneffe et al, 2006), comparing a number of
constituency-based and dependency-based parsers
on data from the Wall Street Journal. They con-
clude that the highest accuracy is obtained using
constituency-based parsers, although some of the
dependency-based parsers are more ef?cient.
For German, the 2008 ACL workshop on pars-
ing German (K?bler, 2008) featured a shared task
with two different tracks, one for constituency-
based parsing and one for dependency-based pars-
ing. Both tracks had their own evaluation metrics,
but the accuracy with which parsers identi?ed
subjects, direct objects and indirect objects was
compared across the two tracks, and the results
in this case showed an advantage for dependency-
based parsing.
In this paper, we contribute results for a
third language, French, by benchmarking both
constituency-based and dependency-based meth-
ods for deriving typed dependencies. In addi-
tion, we investigate the usefulness of morphologi-
cal features, lemmas and word clusters for each of
the different parsing architectures. The rest of the
paper is structured as follows. Section 2 describes
the French Treebank, and Section 3 describes the
three parsing systems. Section 4 presents the ex-
perimental evaluation, and Section 5 contains a
comparative error analysis of the three systems.
Section 6 concludes with suggestions for future
research.
2 Treebanks
For training and testing the statistical parsers, we
use treebanks that are automatically converted
from the French Treebank (Abeill? and Barrier,
2004) (hereafter FTB), a constituency-based tree-
bank made up of 12, 531 sentences from the Le
Monde newspaper. Each sentence is annotated
with a constituent structure and words bear the
following features: gender, number, mood, tense,
person, de?niteness, wh-feature, and clitic case.
Nodes representing dependents of a verb are la-
beled with one of 8 grammatical functions.1
We use two treebanks automatically obtained
from FTB, both described in Candito et al
(2010). FTB-UC is a modi?ed version of the
original constituency-based treebank, where the
rich morphological annotation has been mapped
to a simple tagset of 28 part-of-speech tags, and
where compounds with regular syntax are bro-
ken down into phrases containing several simple
words while remaining sequences annotated as
compounds in FTB are merged into a single token.
Function labels are appended to syntactic category
symbols and are either used or ignored, depending
on the task.
FTB-UC-DEP is a dependency treebank de-
rived from FTB-UC using the classic technique of
head propagation rules, ?rst proposed for English
by Magerman (1995). Function labels that are
present in the original treebank serve to label the
corresponding dependencies. The remaining un-
labeled dependencies are labeled using heuristics
(for dependents of non-verbal heads). With this
conversion technique, output dependency trees are
necessarily projective, and extracted dependen-
cies are necessarily local to a phrase, which means
that the automatically converted trees can be re-
garded as pseudo-projective approximations to the
correct dependency trees (Kahane et al, 1998).
Candito et al (2010) evaluated the converted trees
for 120 sentences, and report a 98% labeled at-
tachment score when comparing the automatically
converted dependency trees to the manually cor-
rected ones.
1These are SUJ (subject), OBJ (object), A-OBJ/DE-OBJ
(indirect object with preposition ? / de), P-OBJ (indirect
object with another preposition / locatives), MOD (modi?er),
ATS/ATO (subject/object predicative complement).
109
SNP-SUJ
DET
une
NC
lettre
VN
V
avait
VPP
?t?
VPP
envoy?e
NP-MOD
DET
la
NC
semaine
ADJ
derni?re
PP-A_OBJ
P+D
aux
NP
NC
salari?s
une lettre avait ?t? envoy?e la semaine derni?re aux salari?s
de
t
suj
au
x-t
ps
au
x-
pa
ss mod
de
t mod
a_obj
obj
Figure 1: An example of constituency tree of the FTB-UC (left), and the corresponding dependency tree
(right) for A letter had been sent the week before to the employees.
Figure 1 shows two parallel trees from FTB-UC
and FTB-UC-DEP. In all reported experiments in
this paper, we use the usual split of FTB-UC: ?rst
10% as test set, next 10% as dev set, and the re-
maining sentences as training set.
3 Parsers
Although all three parsers compared are statis-
tical, they are based on fairly different parsing
methodologies. The Berkeley parser (Petrov et
al., 2006) is a latent-variable PCFG parser, MST-
Parser (McDonald et al, 2006) is a graph-based
dependency parser, and MaltParser (Nivre et al,
2006) is a transition-based dependency parser.
The choice to include two different dependency
parsers but only one constituency-based parser is
motivated by the study of Seddah et al (2009),
where a number of constituency-based statisti-
cal parsers were evaluated on French, including
Dan Bikel?s implementation of the Collins parser
(Bikel, 2002) and the Charniak parser (Charniak,
2000). The evaluation showed that the Berke-
ley parser had signi?cantly better performance for
French than the other parsers, whether measured
using a parseval-style labeled bracketing F-score
or a CoNLL-style unlabeled attachment score.
Contrary to most of the other parsers in that study,
the Berkeley parser has the advantage of a strict
separation of parsing model and linguistic con-
straints: linguistic information is encoded in the
treebank only, except for a language-dependent
suf?x list used for handling unknown words.
In this study, we compare the Berkeley parser
to MSTParser and MaltParser, which have the
same separation of parsing model and linguistic
representation, but which are trained directly on
labeled dependency trees. The two dependency
parsers use radically different parsing approaches
but have achieved very similar performance for a
wide range of languages (McDonald and Nivre,
2007). We describe below the three architectures
in more detail.2
3.1 The Berkeley Parser
The Berkeley parser is a freely available imple-
mentation of the statistical training and parsing
algorithms described in (Petrov et al, 2006) and
(Petrov and Klein, 2007). It exploits the fact that
PCFG learning can be improved by splitting sym-
bols according to structural and/or lexical proper-
ties (Klein and Manning, 2003). Following Mat-
suzaki et al (2005), the Berkeley learning algo-
rithm uses EM to estimate probabilities on sym-
bols that are automatically augmented with la-
tent annotations, a process that can be viewed
as symbol splitting. Petrov et al (2006) pro-
posed to score the splits in order to retain only the
most bene?cial ones, and keep the grammar size
manageable: the splits that induce the smallest
losses in the likelihood of the treebank are merged
back. The algorithm starts with a very general
treebank-induced binarized PCFG, with order h
horizontal markovisation. created, where at each
level a symbol appears without track of its orig-
inal siblings. Then the Berkeley algorithm per-
forms split/merge/smooth cycles that iteratively
re?ne the binarized grammar: it adds two latent
annotations on each symbol, learns probabilities
for the re?ned grammar, merges back 50% of the
splits, and smoothes the ?nal probabilities to pre-
vent over?tting. All our experiments are run us-
ing BerkeleyParser 1.0,3 modi?ed for handling
2For replicability, models, preprocessing tools and ex-
perimental settings are available at http://alpage.
inria.fr/statgram/frdep.html.
3http://www.eecs.berkeley.edu/
\~petrov/berkeleyParser
110
French unknown words by Crabb? and Candito
(2008), with otherwise default settings (order 0
horizontal markovisation, order 1 vertical marko-
visation, 5 split/merge cycles).
The Berkeley parser could in principle be
trained on functionally annotated phrase-structure
trees (as shown in the left half of ?gure 1), but
Crabb? and Candito (2008) have shown that this
leads to very low performance, because the split-
ting of symbols according to grammatical func-
tions renders the data too sparse. Therefore, the
Berkeley parser was trained on FTB-UC without
functional annotation. Labeled dependency trees
were then derived from the phrase-structure trees
output by the parser in two steps: (1) function la-
bels are assigned to phrase structure nodes that
have functional annotation in the FTB scheme;
and (2) dependency trees are produced using the
same procedure used to produce the pseudo-gold
dependency treebank from the FTB (cf. Section 2).
The functional labeling relies on the Maximum
Entropy labeler described in Candito et al (2010),
which encodes the problem of functional label-
ing as a multiclass classi?cation problem. Specif-
ically, each class is of the eight grammatical func-
tions used in FTB, and each head-dependent pair
is treated as an independent event. The feature
set used in the labeler attempt to capture bilexi-
cal dependencies between the head and the depen-
dent (using stemmed word forms, parts of speech,
etc.) as well as more global sentence properties
like mood, voice and inversion.
3.2 MSTParser
MSTParser is a freely available implementation
of the parsing models described in McDonald
(2006). These models are often described as
graph-based because they reduce the problem
of parsing a sentence to the problem of ?nding
a directed maximum spanning tree in a dense
graph representation of the sentence. Graph-based
parsers typically use global training algorithms,
where the goal is to learn to score correct trees
higher than incorrect trees. At parsing time a
global search is run to ?nd the highest scoring
dependency tree. However, unrestricted global
inference for graph-based dependency parsing
is NP-hard, and graph-based parsers like MST-
Parser therefore limit the scope of their features
to a small number of adjacent arcs (usually two)
and/or resort to approximate inference (McDon-
ald and Pereira, 2006). For our experiments, we
use MSTParser 0.4.3b4 with 1-best projective de-
coding, using the algorithm of Eisner (1996), and
second order features. The labeling of dependen-
cies is performed as a separate sequence classi?-
cation step, following McDonald et al (2006).
To provide part-of-speech tags to MSTParser,
we use the MElt tagger (Denis and Sagot, 2009),
a Maximum Entropy Markov Model tagger en-
riched with information from a large-scale dictio-
nary.5 The tagger was trained on the training set
to provide POS tags for the dev and test sets, and
we used 10-way jackkni?ng to generate tags for
the training set.
3.3 MaltParser
MaltParser6 is a freely available implementation
of the parsing models described in (Nivre, 2006)
and (Nivre, 2008). These models are often char-
acterized as transition-based, because they reduce
the problem of parsing a sentence to the prob-
lem of ?nding an optimal path through an abstract
transition system, or state machine. This is some-
times equated with shift-reduce parsing, but in
fact includes a much broader range of transition
systems (Nivre, 2008). Transition-based parsers
learn models that predict the next state given the
current state of the system, including features over
the history of parsing decisions and the input sen-
tence. At parsing time, the parser starts in an ini-
tial state and greedily moves to subsequent states
? based on the predictions of the model ? until a
terminal state is reached. The greedy, determinis-
tic parsing strategy results in highly ef?cient pars-
ing, with run-times often linear in sentence length,
and also facilitates the use of arbitrary non-local
features, since the partially built dependency tree
is ?xed in any given state. However, greedy in-
ference can also lead to error propagation if early
predictions place the parser in incorrect states. For
the experiments in this paper, we use MaltParser
4http://mstparser.sourceforge.net
5Denis and Sagot (2009) report a tagging accuracy of
97.7% (90.1% on unknown words) on the FTB-UC test set.
6http://www.maltparser.org
111
1.3.1 with the arc-eager algorithm (Nivre, 2008)
and use linear classi?ers from the LIBLINEAR
package (Fan et al, 2008) to predict the next state
transitions. As for MST, we used the MElt tagger
to provide input part-of-speech tags to the parser.
4 Experiments
This section presents the parsing experiments that
were carried out in order to assess the state of the
art in labeled dependency parsing for French and
at the same time investigate the impact of different
types of lexical information on parsing accuracy.
We present the features given to the parsers, dis-
cuss how they were extracted/computed and inte-
grated within each parsing architecture, and then
summarize the performance scores for the differ-
ent parsers and feature con?gurations.
4.1 Experimental Space
Our experiments focus on three types of lexical
features that are used either in addition to or as
substitutes for word forms: morphological fea-
tures, lemmas, and word clusters. In the case
of MaltParser and MSTParser, these features are
used in conjunction with POS tags. Motivations
for these features are rooted in the fact that French
has a rather rich in?ectional morphology.
The intuition behind using morphological fea-
tures like tense, mood, gender, number, and per-
son is that some of these are likely to provide ad-
ditional cues for syntactic attachment or function
type. This is especially true given that the 29 tags
used by the MElt tagger are rather coarse-grained.
The use of lemmas and word clusters, on the
other hand, is motivated by data sparseness con-
siderations: these provide various degrees of gen-
eralization over word forms. As suggested by Koo
et al (2008), the use of word clusters may also re-
duce the need for annotated data.
All our features are automatically produced:
no features except word forms originate from the
treebank. Our aim was to assess the performance
currently available for French in a realistic setting.
Lemmas Lemmatized forms are extracted us-
ing Lefff (Sagot, 2010), a large-coverage morpho-
syntactic lexicon for French, and a set of heuristics
for unknown words. More speci?cally, Lefff is
queried for each (word, pos), where pos is the
tag predicted by the MElt tagger. If the pair is
found, we use the longest lemma associated with
it in Lefff. Otherwise, we rely on a set of simple
stemming heuristics using the form and the pre-
dicted tag to produce the lemma. We use the form
itself for all other remaining cases.7
Morphological Features Morphological fea-
tures were extracted in a way similar to lemmas,
again by querying Lefff and relying on heuristics
for out-of-dictionary words. Here are the main
morphological attributes that were extracted from
the lexicon: mood and tense for verbs; person
for verbs and pronouns; number and gender for
nouns, past participles, adjectives and pronouns;
whether an adverb is negative; whether an adjec-
tive, pronoun or determiner is cardinal, ordinal,
de?nite, possessive or relative. Our goal was to
predict all attributes found in FTB that are recov-
erable from the word form alone.
Word Form Clusters Koo et al (2008) have
proposed to use unsupervised word clusters as
features in MSTParser, for parsing English and
Czech. Candito and Crabb? (2009) showed that,
for parsing French with the Berkeley parser, us-
ing the same kind of clusters as substitutes for
word forms improves performance. We now ex-
tend their work by comparing the impact of such
clusters on two additional parsers.
We use the word clusters computed by Can-
dito and Crabb? (2009) using Percy Liang?s im-
plementation8 of the Brown unsupervised cluster-
ing algorithm (Brown et al, 1992). It is a bottom-
up hierarchical clustering algorithm that uses a bi-
gram language model over clusters. The result-
ing cluster ids are bit-strings, and various lev-
els of granularity can be obtained by retaining
only the ?rst x bits. Candito and Crabb? (2009)
used the L?Est R?publicain corpus, a 125 mil-
lion word journalistic corpus.9 To reduce lexi-
7Candito and Seddah (2010) report the following cover-
age for the Lefff : around 96% of the tokens, and 80.1% of
the token types are present in the Lefff (leaving out punctua-
tion and numeric tokens, and ignoring case differences).
8http://www.eecs.berkeley.edu/~pliang/
software
9http://www.cnrtl.fr/corpus/
estrepublicain
112
cal data sparseness caused by in?ection, they ran
a lexicon-based stemming process on the corpus
that removes in?ection marks without adding or
removing lexical ambiguity. The Brown algo-
rithm was then used to compute 1000 clusters of
stemmed forms, limited to forms that appeared at
least 20 times.
We tested the use of clusters with different val-
ues for two parameters: nbbits = the cluster pre-
?x length in bits, to test varying granularities, and
minocc = the minimum number of occurrences in
the L?Est R?publicain corpus for a form to be re-
placed by a cluster or for a cluster feature to be
used for that form.
4.2 Parser-Specific Configurations
Since the three parsers are based on different ma-
chine learning algorithms and parsing algorithms
(with different memory requirements and parsing
times), we cannot integrate the different features
described above in exactly the same way. For the
Berkeley parser we use the setup of Candito and
Seddah (2010), where additional information is
encoded within symbols that are used as substi-
tutes for word forms. For MaltParser and MST-
Parser, which are based on discriminative models
that permit the inclusion of interdependent fea-
tures, additional information may be used either
in addition to or as substitutes for word forms.
Below we summarize the con?gurations that have
been explored for each parser:
? Berkeley:
1. Morphological features: N/A.
2. Lemmas: Concatenated with POS tags
and substituted for word forms.
3. Clusters: Concatenated with morpho-
logical suf?xes and substituted for word
forms; grid search for optimal values of
nbbits and minocc.
? MaltParser and MSTParser:
1. Morphological features: Added as
features.
2. Lemmas: Substituted for word forms
or added as features.
3. Clusters: Substituted for word forms or
added as features; grid search for opti-
mal values of nbbits and minocc.
4.3 Results
Table 1 summarizes the experimental results. For
each parser we give results on the development
set for the baseline (no additional features), the
best con?guration for each individual feature type,
and the best con?guration for any allowed combi-
nation of the three features types. For the ?nal
test set, we only evaluate the baseline and the best
combination of features. Scores on the test set
were compared using a ?2-test to assess statisti-
cal signi?cance: unless speci?ed, all differences
therein were signi?cant at p ? 0.01.
The MSTParser system achieves the best la-
beled accuracy on both the development set and
the test set. When adding lemmas, the best con-
?guration is to use them as substitutes for word
forms, which slightly improves the UAS results.
For the clusters, their use as substitutes for word
forms tends to degrade results, whereas using
them as features alone has almost no impact. This
means that we could not replicate the positive ef-
fect10 reported by Koo et al (2008) for English
and Czech. However, the best combined con-
?guration is obtained using lemmas instead of
words, a reduced set of morphological features,11
and clusters as features, with minocc=50, 000 and
nbbits=10.
MaltParser has the second best labeled accu-
racy on both the development set and the test set,
although the difference with Berkeley is not sig-
ni?cant on the latter. MaltParser has the lowest
unlabeled accuracy of all three parsers on both
datasets. As opposed to MSTParser, all three fea-
ture types work best for MaltParser when used in
addition to word forms, although the improvement
is statistically signi?cant only for lemmas and
clusters. Again, the best model uses all three types
of features, with cluster features minocc=600 and
nbbits=7. MaltParser shows the smallest discrep-
ancy from unlabeled to labeled scores. This might
be because it is the only architecture where label-
ing is directly done as part of parsing.
10Note that the two experiments cannot be directly com-
pared. Koo et al (2008) use their own implementation of an
MST parser, which includes extra second-order features (e.g.
grand-parent features on top of sibling features).
11As MSTParser training is memory-intensive, we re-
moved the features containing information already encoded
part-of-speech tags.
113
Development Set Test Set
Baseline Morpho Lemma Cluster Best Baseline Best
Parser LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS
Berkeley 85.1 89.3 ? ? 85.9 90.0 86.5 90.8 86.5 90.8 85.6 89.6 86.8 91.0
MSTParser 87.2 90.0 87.2 90.2 87.2 90.1 87.2 90.1 87.5 90.3 87.6 90.3 88.2 90.9
MaltParser 86.2 89.0 86.3 89.0 86.6 89.2 86.5 89.2 86.9 89.4 86.7 89.3 87.3 89.7
Table 1: Experimental results for the three parsing systems. LAS=labeled accuracy, UAS=unlabeled accuracy, for sentences
of any length, ignoring punctuation tokens. Morpho/Lemma/Cluster=best con?guration when using morphological features
only (resp. lemmas only, clusters only), Best=best con?guration using any combination of these.
For Berkeley, the lemmas improve the results
over the baseline, and its performance reaches that
of MSTParser for unlabeled accuracy (although
the difference between the two parsers is not sig-
ni?cant on the test set). The best setting is ob-
tained with clusters instead of word forms, using
the full bit strings. It also gives the best unlabeled
accuracy of all three systems on both the devel-
opment set and the test set. For the more impor-
tant labeled accuracy, the point-wise labeler used
is not effective enough.
Overall, MSTParser has the highest labeled ac-
curacy and Berkeley the highest unlabeled ac-
curacy. However, results for all three systems
on the test set are roughly within one percent-
age point for both labeled and unlabeled ac-
curacy, which means that we do not ?nd the
same discrepancy between constituency-based
and dependency-based parser that was reported
for English by Cer et al (2010).
Table 2 gives parsing times for the best con?g-
uration of each parsing architecture. MaltParser
runs approximately 9 times faster than the Berke-
ley system, and 10 times faster than MSTParser.
The difference in ef?ciency is mainly due to the
fact that MaltParser uses a linear-time parsing al-
gorithm, while the other two parsers have cubic
time complexity. Given the rather small differ-
ence in labeled accuracy, MaltParser seems to be
a good choice for processing very large corpora.
5 Error Analysis
We provide a brief analysis of the errors made by
the best performing models for Berkeley, MST-
Parser and MaltParser on the development set, fo-
cusing on labeled and unlabeled attachment for
nouns, prepositions and verbs. For nouns, Berke-
Bky Malt MST
Tagging _ 0:27 0:27
Parsing 12:19 0:58 (0:18) 14:12 (12:44)
Func. Lab. 0:23 _ _
Dep. Conv. 0:4 _ _
Total 12:46 1:25 14:39
Table 2: Parsing times (min:sec) for the dev set, for the
three architectures, on an imac 2.66GHz. The ?gures within
brackets show the pure parsing time without the model load-
ing time, when available.
ley has the best unlabeled attachment, followed by
MSTParser and then MaltParser, while for labeled
attachment Berkeley and MSTParser are on a par
with MaltParser a bit behind. For prepositions,
MSTParser is by far the best for both labeled and
unlabeled attachment, with Berkeley and Malt-
Parser performing equally well on unlabeled at-
tachment and MaltParser performing better than
Berkeley on labeled attachment.12 For verbs,
Berkeley has the best performance on both labeled
and unlabeled attachment, with MSTParser and
MaltParser performing about equally well. Al-
though Berkeley has the best unlabeled attach-
ment overall, it also has the worst labeled attach-
ment, and we found that this is largely due to the
functional role labeler having trouble assigning
the correct label when the dependent is a prepo-
sition or a clitic.
For errors in attachment as a function of word
distance, we ?nd that precision and recall on de-
pendencies of length > 2 tend to degrade faster
for MaltParser than for MSTParser and Berkeley,
12In the dev set, for MSTParser, 29% of the tokens that
do not receive the correct governor are prepositions (883 out
of 3051 errors), while these represent 34% for Berkeley (992
out of 2914), and 30% for MaltParser (1016 out of 3340).
114
with Berkeley being the most robust for depen-
dencies of length > 6. In addition, Berkeley is
best at ?nding the correct root of sentences, while
MaltParser often predicts more than one root for a
given sentence. The behavior of MSTParser and
MaltParser in this respect is consistent with the re-
sults of McDonald and Nivre (2007).
6 Conclusion
We have evaluated three statistical parsing ar-
chitectures for deriving typed dependencies for
French. The best result obtained is a labeled at-
tachment score of 88.2%, which is roughly on a
par with the best performance reported by Cer et
al. (2010) for parsing English to Stanford depen-
dencies. Note two important differences between
their results and ours: First, the Stanford depen-
dencies are in a way deeper than the surface de-
pendencies tested in our work. Secondly, we ?nd
that for French there is no consistent trend fa-
voring either constituency-based or dependency-
based methods, since they achieve comparable re-
sults both for labeled and unlabeled dependencies.
Indeed, the differences between parsing archi-
tectures are generally small. The best perfor-
mance is achieved using MSTParser, enhanced
with predicted part-of-speech tags, lemmas, mor-
phological features, and unsupervised clusters of
word forms. MaltParser achieves slightly lower
labeled accuracy, but is probably the best option
if speed is crucial. The Berkeley parser has high
accuracy for unlabeled dependencies, but the cur-
rent labeling method does not achieve a compara-
bly high labeled accuracy.
Examining the use of lexical features, we ?nd
that predicted lemmas are useful in all three ar-
chitectures, while morphological features have a
marginal effect on the two dependency parsers
(they are not used by the Berkeley parser). Unsu-
pervised word clusters, ?nally, give a signi?cant
improvement for the Berkeley parser, but have a
rather small effect for the dependency parsers.
Other results for statistical dependency pars-
ing of French include the pilot study of Candito
et al (2010), and the work ofSchluter and van
Genabith (2009), which resulted in an LFG sta-
tistical French parser. However, the latter?s re-
sults are obtained on a modi?ed subset of the FTB,
and are expressed in terms of F-score on LFG f-
structure features, which are not comparable to
our attachment scores. There also exist a num-
ber of grammar-based parsers, evaluated on gold
test sets annotated with chunks and dependen-
cies (Paroubek et al, 2005; de la Clergerie et al,
2008). Their annotation scheme is different from
that of the FTB, but we plan to evaluate the statis-
tical parsers on the same data in order to compare
the performance of grammar-based and statistical
approaches.
Acknowledgments
The ?rst, third and fourth authors? work was sup-
ported by ANR Sequoia (ANR-08-EMER-013).
We are grateful to our anonymous reviewers for
their comments.
References
Abeill?, A. and N. Barrier. 2004. Enriching a french
treebank. In LREC?04.
Bikel, D. M. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In HLT-02.
Brown, P., V. Della Pietra, P. Desouza, J. Lai, and
R. Mercer. 1992. Class-based n-gram models of
natural language. Computational linguistics, 18(4).
Buchholz, S. and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In CoNLL
2006.
Candito, M. and B. Crabb?. 2009. Improving gener-
ative statistical parsing with semi-supervised word
clustering. In IWPT?09.
Candito, M. and D. Seddah. 2010. Parsing word clus-
ters. In NAACL/HLT Workshop SPMRL 2010.
Candito, M., B. Crabb?, and P. Denis. 2010. Statis-
tical french dependency parsing : treebank conver-
sion and ?rst results. In LREC 2010.
Carroll, J., E. Briscoe, and A. San?lippo. 1998. Parser
evaluation: A survey and a new proposal. In LREC
1998.
Cer, D., M.-C. de Marneffe, D. Jurafsky, and C. Man-
ning. 2010. Parsing to stanford dependencies:
Trade-offs between speed and accuracy. In LREC
2010.
Charniak, E. 2000. A maximum entropy inspired
parser. In NAACL 2000.
115
Crabb?, B. and M. Candito. 2008. Exp?riences
d?analyse syntaxique statistique du fran?ais. In
TALN 2008.
de la Clergerie, E. V., C. Ayache, G. de Chalendar,
G. Francopoulo, C. Gardent, and P. Paroubek. 2008.
Large scale production of syntactic annotations for
french. In First International Workshop on Auto-
mated Syntactic Annotations for Interoperable Lan-
guage Resources.
de Marneffe, M.-C., B. MacCartney, and C. D. Man-
ning. 2006. Generating typed dependency parses
from phrase structure parses. In LREC 2006.
Denis, P. and B. Sagot. 2009. Coupling an an-
notated corpus and a morphosyntactic lexicon for
state-of-the-art pos tagging with less human effort.
In PACLIC 2009.
Eisner, J. 1996. Three new probabilistic models for
dependency parsing: An exploration. In COLING
1996.
Fan, R.-E., K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
C.-J. Lin. 2008. LIBLINEAR: A library for large
linear classi?cation. Journal of Machine Learning
Research, 9.
Kahane, S., A. Nasr, and O. Rambow. 1998.
Pseudo-projectivity: A polynomially parsable non-
projective dependency grammar. In ACL/COLING
1998.
Klein, D. and C. D. Manning. 2003. Accurate unlexi-
calized parsing. In ACL 2003.
Koo, T., X. Carreras, and M. Collins. 2008. Sim-
ple semi-supervised dependency parsing. In ACL-
08:HLT.
K?bler, S. 2008. The PaGe 2008 shared task on pars-
ing german. In ACL-08 Workshop on Parsing Ger-
man.
Lin, D. 1995. A dependency-based method for evalu-
ating broad-coverage parsers. In IJCAI-95.
Magerman, D. M. 1995. Statistical decision-tree mod-
els for parsing. In ACL 1995.
Matsuzaki, T., Y. Miyao, and J. Tsujii. 2005. Proba-
bilistic cfg with latent annotations. In ACL 2005.
McDonald, R. and J. Nivre. 2007. Characterizing
the errors of data-driven dependency parsing mod-
els. In EMNLP-CoNLL 2007.
McDonald, R. and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
EACL 2006.
McDonald, R., K. Lerman, and F. Pereira. 2006. Mul-
tilingual dependency analysis with a two-stage dis-
criminative parser. In CoNLL 2006.
McDonald, R. 2006. Discriminative Learning and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Nivre, J., Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for depen-
dency parsing. In LREC 2006.
Nivre, J., J. Hall, S. K?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In CoNLL
Shared Task of EMNLP-CoNLL 2007.
Nivre, J. 2006. Inductive Dependency Parsing.
Springer.
Nivre, J. 2008. Algorithms for deterministic incre-
mental dependency parsing. Computational Lin-
guistics, 34.
Paroubek, P., L.-G. Pouillot, I. Robba, and A. Vilnat.
2005. Easy : Campagne d??valuation des analy-
seurs syntaxiques. In TALN 2005, EASy workshop :
campagne d??valuation des analyseurs syntaxiques.
Petrov, S. and D. Klein. 2007. Improved inference for
unlexicalized parsing. In NAACL-07: HLT.
Petrov, S., L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree
annotation. In ACL 2006.
Sagot, B. 2010. The Lefff, a freely available and large-
coverage morphological and syntactic lexicon for
french. In LREC 2010.
Schluter, N. and J. van Genabith. 2009. Dependency
parsing resources for french: Converting acquired
lfg f-structure. In NODALIDA 2009.
Seddah, D., M. Candito, and B. Crabb?. 2009. Cross
parser evaluation and tagset variation: a french tree-
bank study. In IWPT 2009.
Seddah, D., G. Chrupa?a, O. Cetinoglu, J. van Gen-
abith, and M. Candito. 2010. Lemmatization and
statistical lexicalized parsing of morphologically-
rich languages. In NAACL/HLT Workshop SPMRL
2010.
Tsarfaty, R. 2006. Integrated morphological and syn-
tactic disambiguation for modern hebrew. In COL-
ING/ACL 2006 Student Research Workshop.
Yamada, H. and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
IWPT 2003.
116
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 13?25,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Predicting Thread Discourse Structure over Technical Web Forums
Li Wang,?? Marco Lui,?? Su Nam Kim,?? Joakim Nivre? and Timothy Baldwin??
? Dept. of Computer Science and Software Engineering, University of Melbourne
? NICTA Victoria Research Laboratory
? Dept. of Linguistics and Philology, Uppsala University
li.wang.d@gmail.com, saffsd@gmail.com,
sunamkim@gmail.com, joakim.nivre@lingfil.uu.se, tb@ldwin.net
Abstract
Online discussion forums are a valuable
means for users to resolve specific information
needs, both interactively for the participants
and statically for users who search/browse
over historical thread data. However, the com-
plex structure of forum threads can make it
difficult for users to extract relevant informa-
tion. The discourse structure of web forum
threads, in the form of labelled dependency re-
lationships between posts, has the potential to
greatly improve information access over web
forum archives. In this paper, we present the
task of parsing user forum threads to deter-
mine the labelled dependencies between posts.
Three methods, including a dependency pars-
ing approach, are proposed to jointly clas-
sify the links (relationships) between posts
and the dialogue act (type) of each link. The
proposed methods significantly surpass an in-
formed baseline. We also experiment with ?in
situ? classification of evolving threads, and es-
tablish that our best methods are able to per-
form equivalently well over partial threads as
complete threads.
1 Introduction
Web user forums (or simply ?forums?) are online
platforms for people to discuss information and ob-
tain information via a text-based threaded discourse,
generally in a pre-determined domain (e.g. IT sup-
port or DSLR cameras). With the advent of Web
2.0, there has been an explosion of web authorship in
this area, and forums are now widely used in various
areas such as customer support, community devel-
opment, interactive reporting and online eduction.
In addition to providing the means to interactively
participate in discussions or obtain/provide answers
to questions, the vast volumes of data contained in
forums make them a valuable resource for ?support
sharing?, i.e. looking over records of past user inter-
actions to potentially find an immediately applica-
ble solution to a current problem. On the one hand,
more and more answers to questions over a wide
range of domains are becoming available on forums;
on the other hand, it is becoming harder and harder
to extract and access relevant information due to the
sheer scale and diversity of the data.
This research aims at enhancing information ac-
cess and support sharing, by mining the discourse
structure of troubleshooting-oriented web user fo-
rum threads. Previous research has shown that sim-
ple thread structure information (e.g. reply-to struc-
ture) can enhance tasks such as forum information
retrieval (Seo et al, 2009) and post quality assess-
ment (Lui and Baldwin, 2009). We aim to move be-
yond simple threading, to predict not only the links
between posts, but also show the manner of each
link, in the form of the discourse structure of the
thread. In doing so, we hope to be able to perform
richer visualisation of thread structure (e.g. high-
lighting the key posts which appear to have led to
a successful resolution to a problem), and more fine-
grained weighting of posts in threads for search pur-
poses.
To illustrate the task, we use an example thread,
made up of 5 posts from 4 distinct participants, from
the CNET forum dataset of Kim et al (2010b), as
shown in Figure 1. The discourse structure of the
thread is modelled as a rooted directed acyclic graph
13
HTML Input Code...Please can someone tell me how to create an input box that asks the user to enter their ID, and then allows them to press go. It will then redirect to the page ...
User APost 1
User BPost 2
User CPost 3
Re: html input codePart 1: create a form with a text field. See ... Part 2: give it a Javascript action
asp.net c\# videoI?ve prepared for you video.link click ...
Thank You!Thanks a lot for that ... I have Microsoft Visual Studio 6, what program should I do this in? Lastly, how do I actually include this in my site? ...
A little more help... You would simply do it this way: ... You could also just ... An example of this is ...
User APost 4
User DPost 5
0+Question-Question
2+Answer-Answer
4+Answer-Answer
1+Answer-Answer
1+Answer-Confirmation
3+Question-Add
?
Figure 1: A snippeted and annotated CNET thread
(DAG) with a dialogue act label associated with each
edge of the graph. In this example, UserA initiates
the thread with a question (dialogue act = Question-
Question) in the first post, by asking how to create
an interactive input box on a webpage. In response,
UserB and UserC provide independent answers (di-
alogue act = Answer-Answer). UserA responds to
UserC to confirm the details of the solution (dia-
logue act = Answer-Confirmation), and at the same
time, adds extra information to his/her original ques-
tion (dialogue act = Question-Add); i.e., this one
post has two distinct dependency links associated
with it. Finally, UserD proposes a different solution
again to the original question.
To predict thread discourse structure of this type,
we jointly classify the links and dialogue acts be-
tween posts, experimenting with a variety of su-
pervised classification methods, namely dependency
parsing and linear-chain conditional random fields.
In this, we build on the earlier work of Kim et al
(2010b) who first proposed the task of thread dis-
course analysis, but only carried out experiments on
post linking and post dialogue act classification as
separate tasks. In addition to achieving state-of-the-
art accuracy over the task, we carry out in-depth
analysis of classification effectiveness at different
thread depths, and establish that the accuracy of our
method over partial threads is equivalent to that over
full threads, indicating that the method is applica-
ble to in-situ thread classification. Finally, we in-
vestigate the role of user-level features in discourse
structure analysis.
2 Related Work
This work builds directly on earlier work of a subset
of the authors (Kim et al, 2010b), whereby a novel
post-level dialogue act set was proposed, and used
as the basis for annotation of a set of threads taken
from CNET. In the original work, we proposed a set
of novel features, which we applied to the separate
tasks of post link classification and dialogue act clas-
sification. We later applied the same basic method-
ology to dialogue act classification over one-on-one
live chat data with provided message dependencies
(Kim et al, 2010a), demonstrating the generalisabil-
ity of the original method. In both cases, however,
we tackled only a single task, either link classifica-
tion (optionally given dialogue act tags) or dialogue
act classification, but never the two together. In this
paper, we take the obvious step of exploring joint
classification of post link and dialogue act tags, to
generate full thread discourse structures.
Discourse disentanglement (i.e. link classifica-
tion) and dialogue act tagging have been studied
largely as independent tasks. Discourse disentangle-
ment is the task of dividing a conversation thread
(Elsner and Charniak, 2008; Lemon et al, 2002)
or document thread (Wolf and Gibson, 2005) into
a set of distinct sub-discourses. The disentangled
discourse is sometimes assumed to take the form of
a tree structure (Grosz and Sidner, 1986; Lemon et
al., 2002; Seo et al, 2009), an acyclic graph struc-
ture (Rose? et al, 1995; Schuth et al, 2007; Elsner
and Charniak, 2008; Wang et al, 2008; Lin et al,
2009), or a more general cyclic chain graph struc-
ture (Wolf and Gibson, 2005). Dialogue acts are
used to describe the function or role of an utterance
in a discourse, and have been applied to the anal-
ysis of mediums of communication including con-
versational speech (Stolcke et al, 2000; Shriberg et
al., 2004; Murray et al, 2006), email (Cohen et al,
2004; Carvalho and Cohen, 2005; Lampert et al,
2008), instant messaging (Ivanovic, 2008; Kim et
al., 2010a), edited documents (Soricut and Marcu,
2003; Sagae, 2009) and online forums (Xi et al,
14
2004; Weinberger and Fischer, 2006; Wang et al,
2007; Fortuna et al, 2007; Kim et al, 2010b). For a
more complete review of models for discourse dis-
entanglement and dialogue act tagging, see Kim et
al. (2010b).
Joint classification has been applied in a number
of different contexts, based on the intuition that it
should be possible to harness interactions between
different sub-tasks to the mutual benefit of both.
Warnke et al (1997) jointly performed segmenta-
tion and dialogue act classification over a German
spontaneous speech corpus. In their approach, the
predictions of a multi-layer perceptron classifier on
dialogue act boundaries were fed into an n-gram
language model, which was used for the joint seg-
mentation and classification of dialogue acts. Sut-
ton and McCallum (2005) performed joint parsing
and semantic role labelling (SRL), using the results
of a probabilistic SRL system to improve the accu-
racy of a probabilistic parser. Finkel and Manning
(2009) built a joint, discriminative model for pars-
ing and named entity recognition (NER), address-
ing the problem of inconsistent annotations across
the two tasks, and demonstrating that NER bene-
fited considerably from the interaction with parsing.
Dahlmeier et al (2009) proposed a joint probabilis-
tic model for word sense disambiguation (WSD) of
prepositions and SRL of prepositional phrases (PPs),
and achieved state-of-the-art results over both tasks.
There has been a recent growth in user-level
research over forums. Lui and Baldwin (2009)
explored a range of user-level features, including
replies-to and co-participation graph analysis, for
post quality classification. Lui and Baldwin (2010)
introduced a novel user classification task where
each user is classified against four attributes: clar-
ity, proficiency, positivity and effort. User commu-
nication roles in web forums have also been studied
(Chan and Hayes, 2010; Chan et al, 2010).
Threading information has been shown to en-
hance retrieval effectiveness for post-level retrieval
(Xi et al, 2004; Seo et al, 2009), thread-level
retrieval (Seo et al, 2009; Elsas and Carbonell,
2009), sentence-level shallow information extrac-
tion (Sondhi et al, 2010), and near-duplicate thread
detection (Muthmann et al, 2009). These results
suggest that the thread structural representation used
in this research, which includes both linking struc-
ture and the dialogue act associated with each link,
could potentially provide even greater leverage in
these retrieval tasks.
Another related research area is post-level classi-
fication, such as general post quality classification
(Weimer et al, 2007; Weimer and Gurevych, 2007;
Wanas et al, 2008; Lui and Baldwin, 2009), and
post descriptiveness in particular domains (e.g. med-
ical forums: Leaman et al (2010)). It has been
demonstrated (Wanas et al, 2008; Lui and Bald-
win, 2009) that thread discourse structure can signif-
icantly improve the classification accuracy for post-
level tasks.
Initiation?response pairs (e.g. question?answer,
assessment?agreement, and blame?denial) from on-
line forums have the potential to enhance thread
summarisation or automatically generate knowledge
bases for Community Question Answering (cQA)
services such as Yahoo! Answers. While initiation?
response pair identification has been explored as a
pairwise ranking problem (Wang and Rose?, 2010),
question?answer pair identification has been ap-
proached via the two separate sub-tasks of ques-
tion classification and answer detection (Cong et al,
2008; Ding et al, 2008; Cao et al, 2009). Our
thread discourse structure prediction task includes
joint classification of post roles (i.e. dialogue acts)
and links, and could potentially be performed at the
sub-post sentence level to extract initiation?response
pairs.
3 Task Description and Data Set
The main task performed in this research is joint
classification of inter-post links (Link) and dialogue
acts (DA) within forum threads. In this, we assume
that a post can only link to an earlier post (or a vir-
tual root node), and that dialogue acts are labels on
edges. It is possible for there to be multiple edges
from a given post, e.g. if a post both confirms the va-
lidity of an answer and adds extra information to the
original question (as happens in Post4 in Figure 1).
We experiment with two different approaches to
joint classification: (1) a linear-chain CRF over
combined Link/DA post labels; and (2) a depen-
dency parser. The joint classification task is a nat-
ural fit for dependency parsing, in that the task is
intrinsically one of inferring labelled dependencies
15
between posts, but it has a number of special prop-
erties that distinguish it from standard dependency
parsing:
strict reverse-chronological directionality: the
head always precedes the dependent, in terms
of the chronological sequencing of posts.
non-projective dependencies: threads can contain
non-projective dependencies, e.g. in a 4-post
thread, posts 2 and 3 may be dependent on
post 1, and post 4 dependent on post 2; around
2% of the threads in our dataset contain non-
projective dependencies.
multi-headedness: it is possible for a given post to
have multiple heads, including the possibility
of multiple dependency links to the same post
(e.g. adding extra information to a question
[Question-Add] as well as retracting infor-
mation from the original question [Question-
Correction]); around 6% of the threads in our
dataset contain multi-headed dependencies.
disconnected sub-graphs: it is possible for there to
be disconnected sub-graphs, e.g. in instances
where a user hijacks a thread to ask their
own unrelated question, or submit an unrelated
spam post; around 2% of the threads in our
dataset contain disconnected sub-graphs.
The first constraint potentially simplifies depen-
dency parsing, and non-projective dependencies are
relatively well understood in the dependency parsing
community (Tapanainen and Jarvinen, 1997; Mc-
Donald et al, 2005). Multi-headedness and dis-
connected sub-graphs pose greater challenges to de-
pendency parsing, although there has been research
done on both (McDonald and Pereira, 2006; Sagae
and Tsujii, 2008; Eisner and Smith, 2005). The
combination of non-projectivity, multi-headedness
and disconnected sub-graphs in a single dataset,
however, poses a challenge for dependency parsing.
In addition to performing evaluation in batch
mode over complete threads, we consider the task of
?in situ thread classification?, whereby we predict
the discourse structure of a thread after each post.
This is intended to simulate the more realistic set-
ting of incrementally crawling/updating thread data,
but needing to predict discourse structure for partial
threads. We are interested in determining the rela-
tive degradation in accuracy for in situ classification
vs. batch classification.
As our dataset, we use the CNET forum dataset
of Kim et al (2010b),1 which contains 1332 an-
notated posts spanning 315 threads, collected from
the Operating System, Software, Hardware and Web
Development sub-forums of cnet.2 Each post is la-
belled with one or more links (including the possi-
bility of null-links, where the post doesn?t link to
any other post), and each link is labelled with a di-
alogue act. The dialogue act set is made up of 5
super-categories: Question, Answer, Resolution
(confirmation of the question being resolved), Re-
production (external confirmation of a proposed so-
lution working) and Other. The Question category
contains 4 sub-classes: Question, Add, Confirma-
tion and Correction. Similarly, the Answer cate-
gory contains 5 sub-classes: Answer, Add, Confir-
mation, Correction and Objection. For example,
the label Question-Add signifies the Question su-
perclass and Add subclass, i.e. addition of extra in-
formation to a question. For full details of the dia-
logue act tagset, see Kim et al (2010b).
Dependency links are represented by their relative
position in the chronologically-sorted list of posts,
e.g. 1 indicates a link back to the preceding post,
and 2 indicates a link back two posts.
Unless otherwise noted, evaluation is over the
combined link and dialogue act tag, including the
combination of superclass and subclass for the
Question and Answer dialogue acts. For ex-
ample, 1+Answer-Answer indicates a dependency
link back one post, which is an answer to a question.
The most common label in the dataset is 1+Answer-
answer (28.4%).
4 Learners and Features
4.1 Learners
To predict thread discourse structure, we use a struc-
tured classification approach ? based on the find-
ings of Kim et al (2010b) and Kim et al (2010a)
? and a dependency parser. The structured clas-
sification approach we experiment with is a linear-
1Available from http://www.csse.unimelb.edu.
au/research/lt/resources/conll2010-thread/
2http://forums.cnet.com/
16
chain conditional random field learner (CRF: Laf-
ferty et al (2001)), within which we explore two
simple approaches to joint classification, as is ex-
plained in Section 5.1. Dependency parsing (Ku?bler
et al, 2009) is the task of automatically predicting
the dependency structure of a token sequence, in
the form of binary asymmetric dependency relations
with dependency types.
Standardly, CRFs have been applied to tasks such
as part-of-speech tagging, named entity recognition,
semantic role labelling and supertagging, where the
individual tokens are single words. Similarly, de-
pendency parsing is conventionally applied to sen-
tences, with single-word tokens. In our case, our
tokens are thread posts, with much greater scope for
feature engineering than single words, and techni-
cal challenges in scaling the underlying implemen-
tations to handle potentially much larger feature sets.
As our learners, we deployed CRFSGD (Bot-
tou, 2011) to learn the CRF, and MaltParser (Nivre
et al, 2007) as our dependency parser. CRFSGD
uses stochastic gradient descent to efficiently solve
the convex optimisation problem, and scales well to
large feature sets. We used the default parameter set-
tings for CRFSGD, with feature templates includ-
ing all unigram features of the current token as well
as bigram features combining the previous output to-
ken with the current token.
MaltParser implements transition-based parsing,
where no formal grammar is considered, and a tran-
sition system, or state machine, is learned to map a
sentence onto its dependency graph. One feature of
MaltParser that makes it well suited to our task is
that it is possible to define feature models of arbi-
trary complexity for each token. In presenting the
thread data to MaltParser, we represent the null-
link from the initial post of each thread, as well as
any disconnected posts, as the root.
To the best of our knowledge, there is no past
work on using dependency parsing to learn thread
discourse structure. Based on extensive experimen-
tation, we determined that the MaltParser configu-
ration that obtains the best results for our task is the
Nivre algorithm in arc-standard mode (Nivre, 2003;
Nivre, 2004), using LIBSVM (Chang and Lin, 2011)
with a linear kernel as the learner, and a feature
model with exhaustive combinations of features re-
lating to the features and predictions of the first/top
three tokens from both ?Input? and ?Stack?.3 As
such, MaltParser is actually unable to predict any
non-projective structures, as experiments with algo-
rithms supporting non-projective structures invari-
ably led to lower results. In our choice of parsing al-
gorithm, we are also unable to detect posts with mul-
tiple heads, but can potentially detect disconnected
sub-graphs.
4.2 Features
The features used in our classifiers are as follows:
Structural Features:
Initiator a binary feature indicating whether the
current post?s author is the thread initiator.
Position the relative position of the current post,
as a ratio over the total number of posts in the
thread.
Semantic Features:
TitSim the relative location of the post which has
the most similar title (based on unweighted co-
sine similarity) to the current post.
PostSim the relative location of the post which
has the most similar content (based on un-
weighted cosine similarity) to the current post.
Punct the number of question marks (QuCount),
exclamation marks (ExCount) and URLs
(UrlCount) in the current post.
UserProf the class distribution (in the training
thread) of the author of the current post.
These features are drawn largely from the work
of Kim et al (2010b), with two major differences:
(1) we do not use post context features because our
learners (i.e. CRFSGD and MaltParser) inherently
capture Markov chains; and (2) our UserProf fea-
tures are customised to the class set associated with
the task at hand, e.g. the UserProf features for the
standalone linking task take the form of the link la-
bels (and not dialogue act labels) of the posts by the
relevant author in the training data. Table 1 shows
the feature representation of the third post in a thread
17
Feature Value Explanation
Initiator 1.0 post from the initiator
ExCount 4.0 4 exclamation marks
QuCount 0.0 0 question marks
UrlCount 0.0 0 URLs
Position 0.25 i?1n = 3?18PostSim 2.0 most similar to post 1
TitSim 2.0 most similar to post 1
UserProf ~x counts for posts of each
class from the same author
in the training data
Table 1: The feature presentation of the third post in a
thread of length 8
of length 8. The values of each feature are scaled to
the range [0, 1] before being fed into the learners.
We also experimented with other features,
including raw bag-of-words lexical features,
dimensionality-reduced lexical features (using
principal components analysis), and different post
similarity measures such as longest common subse-
quence (LCS) match. While we were able to obtain
gains in isolation, when combined with the other
features, these features had no impact, and are thus
not included in the results presented in this paper.
5 Classification Methodology
All our experiments were carried out based on strati-
fied 10-fold cross-validation, stratifying at the thread
level to ensure that all posts from a given thread
occur in a single fold. The results are primarily
evaluated using post-level micro-averaged F-score
(F?: ? = 1), and additionally with thread-level F-
score/classification accuracy (i.e. the proportion of
threads where all posts have been correctly classi-
fied4), where space allows. Statistical significance
is tested using randomised estimation (Yeh, 2000)
with p < 0.05. Initial experiments showed it is
hard for learners to discover which posts have multi-
ple links, largely due to the sparsity of multi-headed
posts (which account for less than 5% of the total
posts). Therefore, only the the most recent link for
3http://maltparser.org/userguide.html#
parsingalg
4Classification accuracy = F-score at the thread-level, as
each thread is assigned a single label of correct or incorrect.
each multi-headed post was included in training, but
evaluation still considers all links.
5.1 Joint classification
In our experiments, we test two basic approaches to
joint classification for the CRF: (1) classifying the
Link and DA separately, and composing the predic-
tions to form the joint classification (Composition);
and (2) combining the Link and DA labels into a sin-
gle class, and applying the learner over the posts
with the combined class (Combine). Note that
Composition has the potential for mismatches in
the number of Link and DA predictions it gener-
ates, causing complications in the class composition.
Even if the same number of labels is predicted for
both Link and DA, if multiple tags are predicted in
both cases, we are left with the problem of determin-
ing which link label to combine with which dialogue
act label. As such, we have our reservations about
Composition, but as the CRF performs strict 1-of-
n labelling, these are not issues in the experiments
reported herein.
MaltParser natively handles the combination of
Link and DA in its dependency parsing formulation.
5.2 In Situ Thread Classification
One of the biggest challenges in classifying the dis-
course structure of a forum thread is that threads
evolve over time, as new posts are posted. In or-
der to capture this phenomenon, and compare the
accuracy of different models when applied to partial
thread data (artificially cutting off a thread at post
N ) vs. complete threads.5 This is done in the fol-
lowing way: classification over the first two posts
only ([1, 2]), the first four posts ([1, 4]), the first six
posts ([1, 6]), the first eight posts ([1, 8]), and all
posts ([all]). In each case, we limit the test data
only, meaning that the only variable in play is the
extent of thread context used to learn the thread dis-
course structure for the given set of posts. We break
down the results in each case into the indicated sub-
threads, e.g. we take the predictions for [all], and
break them down into the results for [1, 2], [1, 4],
[1, 6], [1, 8] and [all], for direct comparison with the
predictions over the respective sub-thread data.
5In practice, completeness is defined at a given point in time,
when the crawl was done, and it is highly likely that some of the
?complete? threads had extra posts after the crawl.
18
Method Link DA
Kim et al (2010b) .863 / .676 .751 / .543
CRFSGD .891 / .727 .795 / .609
Table 2: Post/thread-level component-wise classification
F-scores for Link and DA classes
6 Experiments and Analysis
6.1 Joint classification
As our baseline for the task, we first use a sim-
ple majority class classifier in the form of the sin-
gle joint class of 1+Answer-Answer for all posts,
which has a post-level F-score of 0.284. A stronger
baseline is to classify all first posts as 0+Question-
Question and all subsequent posts as 1+Answer-
answer, which achieves a post-level F-score of
0.515 (labelled as Heuristic).
As described in Section 5.1, one approach to joint
classification with CRFSGD is to firstly conduct
component-wise classification over Link and DA
separately, and compose the predictions. The results
for the separate Link and DA classification tasks are
presented in Table 2, along with the best results for
Link and DA classification from Kim et al (2010b).
At the component-wise tasks, our method is superior
to Kim et al (2010b), based on a different learner
and slightly different feature set.
Next, we compose the component-wise clas-
sifications for the CRF into joint classifications
(Composition). We contrast this with the com-
bined class approach for CRFSGD and MaltParser
(jointly presented as Joint in Table 3). With the
combined class results, we additionally ablate each
of the feature types from Section 4.2, and also
present results for a dummy model, where no fea-
tures are provided and the prediction is based simply
on sequential priors (Dummy). The results are pre-
sented in Table 3, along with the Heuristic baseline
result.
Several interesting things can be observed from
the post-level F-score results in Table 3. First, with
no features (Dummy), while CRFSGD performs
slightly worse than the Heuristic baseline, Malt-
Parser significantly surpasses the baseline. This is
due to the richer sequential context model of Malt-
Parser. Second, the single feature with the greatest
impact on results is UserProf, i.e. user profile fea-
Method CRFSGD MaltParser
Heuristic .515?/ .311?
Dummy .508?/ .394? .533?/ .356?
Composition .728?/ .553? ?
Joint +ALL .756 / .578 .738 / .578
?Initiator .745 / .569 .708?/ .534?
?Position .750 / .565 .736 / .568
?PostSim .753 / .578 .737 / .568
?TitSim .760 / .587 .734 / .571
?Punct .745 / .571 .735 / .578
?UserProf .672?/ .527? .701?/ .536?
Table 3: Post/thread-level Link-DA joint classification F-
scores (??? signifies a significantly worse result than that
for the same learner with ALL features)
tures extracted from the training data; CRFSGD in
particular benefits from this feature. We return to ex-
plore this effect in Section 6.4. Third, although the
Initiator feature does not have much effect on CRF-
SGD, it affects the performance of MaltParser sig-
nificantly. Further experiments shown that the com-
bination of Initiator and UserProf is sufficient to
achieve a competitive result (i.e. 0.731). It therefore
seems that MaltParser is more robust than CRF-
SGD, whose performance relies crucially on user-
level features which must be learned from the train-
ing data (i.e. UserProf).
Looking to the thread-level F-scores, we observe
some interesting divergences from the post-level F-
score results. First, with no features (Dummy),
CRFSGD significantly outperforms both the base-
line and MaltParser. This appears to be because
CRFSGD performs particularly well over short
threads (e.g. of length 3 and 4), but worse over
longer threads. Second, the best thread-level F-
scores from CRFSGD (i.e. 0.587) and MaltParser
(i.e. 0.578) are not significantly different, despite the
discrepancy in post-level F-score (where CRFSGD
is markedly superior in this case). With the extra
features, the performance of MaltParser on short
threads appears to pick up noticeably, and the differ-
ence in post-level predictions is over longer threads.
If we evaluate the two models over DA super-
classes only (ignoring mismatches at the subclass
level for Question and Answer), the post-level F-
scores for joint classification with ALL features for
CRFSGD and MaltParser are 0.803 and 0.787, re-
spectively.
19
Approaches Link DA
Component-wise .891 / .727? .795 / .609
CRFSGD decomp .893 / .749 .785 / .603
MaltParser decomp .870?/ .730? .766?/ .571?
Table 4: Post/thread-level Link and DA F-scores from
component-wise classification, and from Link-DA clas-
sification decomposition (??? signifies a significantly
worse result than the best result in that column)
Looking at the performance of CRFSGD (in
Combine mode) and MaltParser on disconnected
sub-graphs, while both models did predict a small
number of non-initial posts with null-links (includ-
ing MaltParser predicting 5 out of 6 posts in a sin-
gle thread as having null-links), none were correct,
and neither model was able to correctly predict any
of the 6 actual non-initial instances of null-links in
the dataset.
Finally, we took the joint classification results
from CRFSGD and MaltParser using ALL fea-
tures, and decomposed the predictions into Link and
DA. The results are presented in Table 4, along with
the results for component-wise classification from
Table 2. Somewhat surprisingly, the decomposed
predictions are mostly slightly worse than the re-
sults for the component-wise classification, despite
achieving higher F-score for the joint classification
task. This is simply due to the combined method
tending to get both labels correct or both labels
wrong, for a given post.
6.2 Post Position-based Result Breakdown
One question in thread discourse structure classifica-
tion is how accurate the predictions are at different
depths in a thread (e.g. the first two posts vs. the sec-
ond two posts). A breakdown of results across posts
at different positions is presented in Figure 2.
The overall trend for both CRFSGD and Malt-
Parser is that it becomes increasingly hard to clas-
sify posts as we continue through a thread, due to
greater variability in discourse structure and greater
sparsity in the data. However, it is interesting to note
that the results for CRFSGD actually improve from
posts 7 and 8 ([7, 8]) to posts 9 and onwards ([9, ]).
To further investigate this effect, we performed class
decomposition over the joint classification predic-
tions, and performed a similar breakdown of posts
[1,2] [3,4] [5,6] [7,8] [9,] All0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Posts
F ?
 
 CRFSGDMaltParser
Figure 2: Breakdown of post-level Link-DA results for
CRFSGD and MaltParser based on post position
[1,2] [3,4] [5,6] [7,8] [9,] All0
0.5
1
Posts
F ?
Decomposed Link
 
 CRFSGDMaltParser
[1,2] [3,4] [5,6] [7,8] [9,] All0
0.5
1
Posts
F ?
Decomposed DA
 
 CRFSGDMaltParser
Figure 3: Breakdown of post-level Link and DA F-score
based on the decomposition of CRFSGD and Malt-
Parser classifications
for Link and DA; the results are presented in Fig-
ure 3. It is clear that the anomaly for CRFSGD
comes from the DA component, due to there being
greater predictability in the dialogue for final posts
in a thread (users tend to confirm a successful reso-
lution of the problem, or report on successful exter-
nal reproduction of the solution). MaltParser seems
less adept at identifying that a post is at the end
of a thread, and predicting the dialogue act accord-
ingly. This observation is congruous with the find-
ings of McDonald and Nivre (2007) that errors prop-
agate, due to MaltParser?s greedy inference strat-
egy. The higher results for Link are to be expected,
as throughout the thread, most posts tend to link lo-
cally.
20
XXXXXXXXXTest
B/down [1, 2] [1, 4] [1, 6] [1, 8] [All]
[1, 2] .947/.947 ? ? ? ?
[1, 4] .946/.947 .836/.841 ? ?
[1, 6] .946/.947 .840/.841 .800/.794 ? ?
[1, 8] .946/.947 .840/.841 .800/.794 .780/.769 ?
[All] .946/.946 .840/.838 .800/.791 .776/.767 .756/.738
Table 5: Post-level Link-DA F-score for CRFSGD/MaltParser, based on in situ classification over sub-threads of
different lengths (indicated in the rows), broken down over different post extents (indicated in the columns)
6.3 In Situ Structure Prediction
As described in Section 5.2, we simulate in situ
thread discourse structure prediction by removing
differing numbers of posts from the tail of the thread,
and applying the trained model over the resultant
sub-threads. The results for in situ classification are
presented in Table 5, with the rows indicating the
size of the test sub-thread, and the columns being a
breakdown of results over different portions of the
classified thread. The reason that we do not pro-
vide numbers for all cells in the table is that the size
of the test sub-thread determines the post extents we
can breakdown the results into, e.g. we cannot return
results for posts 1?4 ([1, 4]) when the size of the test
thread was only two posts ([1, 2]).
From the results, we can see that both CRFSGD
and MaltParser are very robust when applied to par-
tial threads, to the extent that we actually achieve
higher results over shortened versions of the thread
than over the complete thread in some instances, al-
though the only difference that is statistically signif-
icant is over [1, 8] for CRFSGD, where the predic-
tion over the partial thread is actually superior to that
over the complete thread. From this, we can con-
clude that it is possible to apply our method to partial
threads without any reduction in effectiveness rela-
tive to classification over complete threads. As such,
our method is shown to be robust when applied to
real-time analysis of dynamically evolving threads.
6.4 User profile feature analysis
In our experiments, we noticed that the user profile
feature (UserProf) is the most effective feature for
both CRFSGD and MaltParser. To gain a deeper
insight into the behaviour of the feature, we binned
the posts according to the number of times the author
had posted in the training data, evaluated based on a
Bin uscore Posts Total Totalper user users posts
High 224.6 251 1 251
Medium 1?41.7 4?48 45 395
Low 0 2?4 157 377
Very Low 0 1 309 309
Table 6: Statistics for the 4 groups of users
user score (uscore) for each user:
uscorei =
?ni
j=1 spi,j
ni
where ni is the number of posts by user i, and spi,j is
the number of posts by user i that occur as training
instances for other posts by the same author. uscore
reflects the average training?test post ratio per user
in cross-validation. Note that as we include all posts
from a given thread in a single partition during cross-
validation, it is possible for an author to have posted
4 times, but have a uscore of 0 due to those posts all
occurring in the same thread.
We ranked the users in the dataset in descending
order of uscore, sub-ranking on ni in cases of a tie
in uscore. The users were binned into 4 groups
of roughly equal post size. The detailed statistics
are shown in Table 6, noting that the high-frequency
bin (?High?) contains posts from a single user. We
present the post-level micro-averaged F-score for
posts in each bin based on CRFSGD, with and with-
out user profile features, in Figure 4.
Contrary to expectation, the UserProf features
have the greatest impact for users with fewer posts.
In fact, a statistically significant difference was ob-
served only for users with no posts in the training
data (uscore = 0), where the F-score jumped over
10% in absolute terms for both the Low and Very
Low bins. Our explanation for this effect is that the
21
High Median Low Very Low0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
User Group
F ?
 
 With UserProfWithout UserProf
Figure 4: Post-level joint classification results for users
binned by uscore, based on CRFSGD with and without
UserProf features)
lack of user profile information is predictive of the
sort of posts we can expect from a user (i.e. they
tend to be newbie users, asking questions).
7 Conclusions and Future Work
In this research, we explored the joint classification
of web user forum thread discourse structure, in the
form of a rooted directed acyclic graph over posts,
with edges labelled with dialogue acts. Three classi-
fication approaches were proposed: separately pre-
dicting Link and DA labels, and composing them
into a joint class; predicting a combined Link-DA
class using a structured classifier; and applying de-
pendency parsing to the problem. We found the
combined approach based on CRFSGD to perform
best over the task, closely followed by dependency
parsing with MaltParser.
We also examined the task of in situ classification
of dialogue structure, in the form of predicting the
discourse structure of partial threads, as contrasted
with classifying only complete threads. We found
that there was no drop in F-score over different sub-
extents of the thread in classifying partial threads,
despite the relative lack of thread context.
In future work, we plan to delve further into de-
pendency parsing, looking specifically at the impli-
cations of multi-headedness and disconnected sub-
graphs on dependency parsing. We also intend to
carry out meta-classification, combining the predic-
tions of CRFSGD and MaltParser.
Our user profile features were found to be the
pick of our features, but counter-intuitively, to bene-
fit users with no posts in the training data, rather than
prolific users. We wish to explore this effect further,
including incorporating unsupervised user-level fea-
tures into our classifiers.
Acknowledgements
The authors wish to acknowledge the development
efforts of Johan Hall in configuring MaltParser to
handle numeric features, and be able to parse thread
structures. NICTA is funded by the Australian gov-
ernment as represented by Department of Broad-
band, Communication and Digital Economy, and the
Australian Research Council through the ICT Centre
of Excellence programme.
References
Le?on Bottou. 2011. CRFSGD software. http://
leon.bottou.org/projects/sgd.
Xin Cao, Gao Cong, Bin Cui, Christian S. Jensen, and
Ce Zhang. 2009. The use of categorization infor-
mation in language models for question retrieval. In
Proceedings of the 18th ACM Conference on Informa-
tion and Knowledge Management (CIKM 2009), pages
265?274, Hong Kong, China.
Vitor R. Carvalho and William W. Cohen. 2005. On
the collective classification of email ?speech acts?. In
Proceedings of 28th International ACM-SIGIR Con-
ference on Research and Development in Information
Retrieval (SIGIR 2005), pages 345?352.
Jeffrey Chan and Conor Hayes. 2010. Decomposing dis-
cussion forums using user roles. In Proceedings of the
WebSci10: Extending the Frontiers of Society On-Line
(WebSci10), pages 1?8, Raleigh, USA.
Jeffrey Chan, Conor Hayes, and Elizabeth M. Daly.
2010. Decomposing discussion forums using user
roles. In Proceedings of the Fourth International AAAI
Conference on Weblogs and Social Media (ICWSM
2010), pages 215?8, Washington, USA.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2(3):27:1?27:27. Software available at http://
www.csie.ntu.edu.tw/?cjlin/libsvm.
William W. Cohen, Vitor R. Carvalho, and Tom M.
Mitchell. 2004. Learning to classify email into
?speech acts?. In Proceedings of the 2004 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP 2004), pages 309?316, Barcelona, Spain.
Gao Cong, Long Wang, Chin-Yew Lin, Young-In Song,
and Yueheng Sun. 2008. Finding question-answer
22
pairs from online forums. In Proceedings of 31st Inter-
national ACM-SIGIR Conference on Research and De-
velopment in Information Retrieval (SIGIR?08), pages
467?474, Singapore.
Daniel Dahlmeier, Hwee Tou Ng, and Tanja Schultz.
2009. Joint learning of preposition senses and seman-
tic roles of prepositional phrases. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2009), pages 450?458,
Singapore. Association for Computational Linguistics.
Shilin Ding, Gao Cong, Chin-Yew Lin, and Xiaoyan Zhu.
2008. Using conditional random fields to extract con-
text and answers of questions from online forums. In
Proceedings of the 46th Annual Meeting of the ACL:
HLT (ACL 2008), pages 710?718, Columbus, USA.
Jason Eisner and Noah A. Smith. 2005. Parsing with soft
and hard constraints on dependency length. In Pro-
ceedings of the Ninth International Workshop on Pars-
ing Technology, pages 30?41, Vancouver, Canada.
Jonathan L. Elsas and Jaime G. Carbonell. 2009. It
pays to be picky: An evaluation of thread retrieval
in online forums. In Proceedings of 32nd Interna-
tional ACM-SIGIR Conference on Research and De-
velopment in Information Retrieval (SIGIR?09), pages
714?715, Boston, USA.
Micha Elsner and Eugene Charniak. 2008. You talk-
ing to me? a corpus and algorithm for conversation
disentanglement. In Proceedings of the 46th Annual
Meeting of the ACL: HLT (ACL 2008), pages 834?842,
Columbus, USA.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL HLT 2009), pages 326?334, Boulder, Col-
orado. Association for Computational Linguistics.
Blaz Fortuna, Eduarda Mendes Rodrigues, and Natasa
Milic-Frayling. 2007. Improving the classification of
newsgroup messages through social network analysis.
In Proceedings of the 16th ACM Conference on In-
formation and Knowledge Management (CIKM 2007),
pages 877?880, Lisbon, Portugal.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intention and the structure of discourse. Compu-
tational Linguistics, 12(3):175?204.
Edward Ivanovic. 2008. Automatic instant messaging
dialogue using statistical models and dialogue acts.
Master?s thesis, University of Melbourne.
Su Nam Kim, Lawrence Cavedon, and Timothy Bald-
win. 2010a. Classifying dialogue acts in one-on-one
live chats. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2010), pages 862?871, Boston, USA.
Su Nam Kim, Li Wang, and Timothy Baldwin. 2010b.
Tagging and linking web forum posts. In Proceedings
of the 14th Conference on Computational Natural Lan-
guage Learning (CoNLL-2010), pages 192?202, Upp-
sala, Sweden.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency parsing. Synthesis Lectures on Hu-
man Language Technologies, 2(1):1?127.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the 18th International Conference on Ma-
chine Learning, pages 282?289, Williamstown, USA.
Andrew Lampert, Robert Dale, and Ce?cile Paris. 2008.
The nature of requests and commitments in email mes-
sages. In Proceedings of the AAAI 2008 Workshop on
Enhanced Messaging, pages 42?47, Chicago, USA.
Robert Leaman, Laura Wojtulewicz, Ryan Sullivan, An-
nie Skariah, Jian Yang, and Graciela Gonzalez. 2010.
Towards internet-age pharmacovigilance: Extracting
adverse drug reactions from user posts in health-
related social networks. In Proceedings of the 2010
Workshop on Biomedical Natural Language Process-
ing (ACL 2010), pages 117?125, Uppsala, Sweden.
Oliver Lemon, Alex Gruenstein, and Stanley Peters.
2002. Collaborative activities and multi-tasking in di-
alogue systems. Traitement Automatique des Langues
(TAL), Special Issue on Dialogue, 43(2):131?154.
Chen Lin, Jiang-Ming Yang, Rui Cai, Xin-Jing Wang,
Wei Wang, and Lei Zhang. 2009. Modeling semantics
and structure of discussion threads. In Proceedings of
the 18th International Conference on the World Wide
Web (WWW 2009), pages 1103?1104, Madrid, Spain.
Marco Lui and Timothy Baldwin. 2009. You are what
you post: User-level features in threaded discourse. In
Proceedings of the 14th Australasian Document Com-
puting Symposium (ADCS 2009), Sydney, Australia.
Marco Lui and Timothy Baldwin. 2010. Classifying
user forum participants: Separating the gurus from the
hacks, and other tales of the internet. In Proceedings
of the 2010 Australasian Language Technology Work-
shop (ALTW 2010), pages 49?57, Melbourne, Aus-
tralia.
Ryan McDonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency parsing
models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL 2007), pages 122?131, Prague,
Czech Republic.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of the 11th Conference of
23
the European Chapter of the Association for Computa-
tional Linguistics (EACL 2006), pages 81?88, Trento,
Italy.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 523?530, Vancouver, Canada.
Gabriel Murray, Steve Renals, Jean Carletta, and Johanna
Moore. 2006. Incorporating speaker and discourse
features into speech summarization. In Proceedings
of the Main Conference on Human Language Technol-
ogy Conference of the North American Chapter of the
Association of Computational Linguistics, pages 367?
374.
Klemens Muthmann, Wojciech M. Barczyn?ski, Falk
Brauer, and Alexander Lo?ser. 2009. Near-duplicate
detection for web-forums. In Proceedings of the 2009
International Database Engineering & Applications
Symposium (IDEAS 2009), pages 142?151, Cetraro,
Italy.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(02):95?135.
Joakim Nivre. 2003. An efficient algorithm for projec-
tive dependency parsing. In Proceedings of the 8th In-
ternational Workshop on Parsing Technologies (IWPT
03), pages 149?160, Nancy, France.
Joakim Nivre. 2004. Incrementality in determinis-
tic dependency parsing. In Proceedings of the ACL
Workshop Incremental Parsing: Bringing Engineer-
ing and Cognition Together (ACL-2004), pages 50?57,
Barcelona, Spain.
Carolyn Penstein Rose?, Barbara Di Eugenio, Lori S.
Levin, and Carol Van Ess-Dykema. 1995. Discourse
processing of dialogues with multiple threads. In Pro-
ceedings of the 33rd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 31?38,
Cambridge, USA.
Kenji Sagae and Jun?ichi Tsujii. 2008. Shift-reduce
dependency DAG parsing. In Proceedings of the
22nd International Conference on Computational Lin-
guistics (COLING 2008), pages 753?760, Manchester,
UK.
Kenji Sagae. 2009. Analysis of discourse structure with
syntactic dependencies and data-driven shift-reduce
parsing. In Proceedings of the 11th International Con-
ference on Parsing Technologies (IWPT-09), pages 81?
84, Paris, France.
Anne Schuth, Maarten Marx, and Maarten de Rijke.
2007. Extracting the discussion structure in comments
on news-articles. In Proceedings of the 9th Annual
ACM International Workshop on Web Information and
Data Management, pages 97?104, Lisboa, Portugal.
Jangwon Seo, W. Bruce Croft, and David A. Smith.
2009. Online community search using thread struc-
ture. In Proceedings of the 18th ACM Conference
on Information and Knowledge Management (CIKM
2009), pages 1907?1910, Hong Kong, China.
Elinzabeth Shriberg, Raj Dhillon, Sonali Bhagat, Jeremy
Ang, and Hannah Carvey. 2004. The ICSI meeting
recorder dialog act (MRDA) corpus. In Proceedings of
the 5th SIGdial Workshop on Discourse and Dialogue,
pages 97?100, Cambridge, USA.
Parikshit Sondhi, Manish Gupta, ChengXiang Zhai, and
Julia Hockenmaier. 2010. Shallow information ex-
traction from medical forum data. In Proceedings of
the 23rd International Conference on Computational
Linguistics (COLING 2010), Posters Volume, pages
1158?1166, Beijing, China.
Radu Soricut and Daniel Marcu. 2003. Sentence level
discourse parsing using syntactic and lexical infor-
mation. In Proceedings of the 2003 Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics (HLT-NAACL 2003), pages 149?156, Edmonton,
Canada.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Pail
Taylor, Rachel Martin, Carol Van Ess-Dykema, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339?373.
Charles Sutton and Andrew McCallum. 2005. Joint
parsing and semantic role labeling. In Proceedings of
the Ninth Conference on Computational Natural Lan-
guage Learning (CoNLL-2005), pages 225?228, Ann
Arbor, Michigan. Association for Computational Lin-
guistics.
Pasi Tapanainen and Timo Jarvinen. 1997. A non-
projective dependency parser. In Proceedings of the
Fifth Conference on Applied Natural Language Pro-
cessing, pages 64?71, Washington, USA.
Nayer Wanas, Motaz El-Saban, Heba Ashour, and
Waleed Ammar. 2008. Automatic scoring of online
discussion posts. In Proceeding of the 2nd ACM work-
shop on Information credibility on the web (WICOW
?08), pages 19?26, Napa Valley, USA.
Yi-Chia Wang and Carolyn P. Rose?. 2010. Mak-
ing conversational structure explicit: identification of
initiation-response pairs within online discussions. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL HLT
2010), pages 673?676.
24
Yi-Chia Wang, Mahesh Joshi, and Carolyn Rose?. 2007.
A feature based approach to leveraging context for
classifying newsgroup style discussion segments. In
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Sessions
(ACL 2007), pages 73?76, Prague, Czech Republic.
Yi-Chia Wang, Mahesh Joshi, William W. Cohen, and
Carolyn Rose?. 2008. Recovering implicit thread
structure in newsgroup style conversations. In Pro-
ceedings of the Second International Conference on
Weblogs and Social Media (ICWSM 2008), pages 152?
160, Seattle, USA.
V. Warnke, R. Kompe, H. Niemann, and E. No?th. 1997.
Integrated dialog act segmentation and classification
using prosodic features and language models. In Proc.
Eurospeech, volume 1, pages 207?210.
Markus Weimer and Iryna Gurevych. 2007. Predicting
the perceived quality of web forum posts. In Proceed-
ings of the 2007 International Conference on Recent
Advances in Natural Language Processing (RANLP
2007), pages 643?648, Borovets, Bulgaria.
Markus Weimer, Iryna Gurevych, and Max Mu?hlha?user.
2007. Automatically assessing the post quality in on-
line discussions on software. In Proceedings of the
45th Annual Meeting of the ACL: Interactive Poster
and Demonstration Sessions, pages 125?128, Prague,
Czech Republic.
Armin Weinberger and Frank Fischer. 2006. A
framework to analyze argumentative knowledge con-
struction in computer-supported collaborative learn-
ing. Computers & Education, 46:71?95, January.
Florian Wolf and Edward Gibson. 2005. Representing
discourse coherence: A corpus-based study. Compu-
tational Linguistics, 31(2):249?287.
Wensi Xi, Jesper Lind, and Eric Brill. 2004. Learning
effective ranking functions for newsgroup search. In
Proceedings of 27th International ACM-SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval (SIGIR 2004), pages 394?401. Sheffield,
UK.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proceed-
ings of the 18th International Conference on Compu-
tational Linguistics (COLING 2000), pages 947?953,
Saarbru?cken, Germany.
25
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 385?396,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Evaluating Dependency Parsing:
Robust and Heuristics-Free Cross-Annotation Evaluation
Reut Tsarfaty
Uppsala University
Sweden
Joakim Nivre
Uppsala University
Sweden
Evelina Andersson
Uppsala University
Sweden
Abstract
Methods for evaluating dependency parsing
using attachment scores are highly sensitive
to representational variation between depen-
dency treebanks, making cross-experimental
evaluation opaque. This paper develops a ro-
bust procedure for cross-experimental eval-
uation, based on deterministic unification-
based operations for harmonizing different
representations and a refined notion of tree
edit distance for evaluating parse hypothe-
ses relative to multiple gold standards. We
demonstrate that, for different conversions of
the Penn Treebank into dependencies, perfor-
mance trends that are observed for parsing
results in isolation change or dissolve com-
pletely when parse hypotheses are normalized
and brought into the same common ground.
1 Introduction
Data-driven dependency parsing has seen a consid-
erable surge of interest in recent years. Dependency
parsers have been tested on parsing sentences in En-
glish (Yamada and Matsumoto, 2003; Nivre and
Scholz, 2004; McDonald et al, 2005) as well as
many other languages (Nivre et al, 2007a). The
evaluation metric traditionally associated with de-
pendency parsing is based on scoring labeled or
unlabeled attachment decisions, whereby each cor-
rectly identified pair of head-dependent words is
counted towards the success of the parser (Buchholz
and Marsi, 2006). As it turns out, however, such
evaluation procedures are sensitive to the annotation
choices in the data on which the parser was trained.
Different annotation schemes often make differ-
ent assumptions with respect to how linguistic con-
tent is represented in a treebank (Rambow, 2010).
The consequence of such annotation discrepancies is
that when we compare parsing results across differ-
ent experiments, even ones that use the same parser
and the same set of sentences, the gap between re-
sults in different experiments may not reflect a true
gap in performance, but rather a difference in the an-
notation decisions made in the respective treebanks.
Different methods have been proposed for making
dependency parsing results comparable across ex-
periments. These methods include picking a single
gold standard for all experiments to which the parser
output should be converted (Carroll et al, 1998; Cer
et al, 2010), evaluating parsers by comparing their
performance in an embedding task (Miyao et al,
2008; Buyko and Hahn, 2010), or neutralizing the
arc direction in the native representation of depen-
dency trees (Schwartz et al, 2011).
Each of these methods has its own drawbacks.
Picking a single gold standard skews the results in
favor of parsers which were trained on it. Trans-
forming dependency trees to a set of pre-defined la-
beled dependencies, or into task-based features, re-
quires the use of heuristic rules that run the risk of
distorting correct information and introducing noise
of their own. Neutralizing the direction of arcs is
limited to unlabeled evaluation and local context,
and thus may not cover all possible discrepancies.
This paper proposes a new three-step protocol for
cross-experiment parser evaluation, and in particu-
lar for comparing parsing results across data sets
that adhere to different annotation schemes. In the
385
first step all structures are brought into a single for-
mal space of events that neutralizes representation
peculiarities (for instance, arc directionality). The
second step formally computes, for each sentence
in the data, the common denominator of the differ-
ent gold standards, containing all and only linguistic
content that is shared between the different schemes.
The last step computes the normalized distance from
this common denominator to parse hypotheses, mi-
nus the cost of distances that reflect mere annotation
idiosyncrasies. The procedure that implements this
protocol is fully deterministic and heuristics-free.
We use the proposed procedure to compare de-
pendency parsing results trained on Penn Treebank
trees converted into dependency trees according to
five different sets of linguistic assumptions. We
show that when starting off with the same set of
sentences and the same parser, training on differ-
ent conversion schemes yields apparently significant
performance gaps. When results across schemes are
normalized and compared against the shared linguis-
tic content, these performance gaps decrease or dis-
solve completely. This effect is robust across parsing
algorithms. We conclude that it is imperative that
cross-experiment parse evaluation be a well thought-
through endeavor, and suggest ways to extend the
protocol to additional evaluation scenarios.
2 The Challenge: Treebank Theories
Dependency treebanks contain information about
the grammatically meaningful elements in the utter-
ance and the grammatical relations between them.
Even if the formal representation in a dependency
treebank is well-defined according to current stan-
dards (Ku?bler et al, 2009), there are different ways
in which the trees can be used to express syntactic
content (Rambow, 2010). Consider, for instance, al-
gorithms for converting the phrase-structure trees in
the Penn Treebank (Marcus et al, 1993) into depen-
dency structures. Different conversion algorithms
implicitly make different assumptions about how to
represent linguistic content in the data. When mul-
tiple conversion algorithms are applied to the same
data, we end up with different dependency trees for
the same sentences (Johansson and Nugues, 2007;
Choi and Palmer, 2010; de Marneffe et al, 2006).
Some common cases of discrepancies are as follows.
Lexical vs. Functional Head Choice. In linguis-
tics, there is a distinction between lexical heads and
functional heads. A lexical head carries the seman-
tic gist of a phrase while a functional one marks its
relation to other parts of the sentence. The two kinds
of heads may or may not coincide in a single word
form (Zwicky, 1993). Common examples refer to
prepositional phrases, such as the phrase ?on Sun-
day?. This phrase has two possible analyses, one se-
lects a lexical head (1a) and the other selects a func-
tional one (1b), as depicted below.
(1a) Sunday
on
(1b) on
Sunday
Similar choices are found in phrases which contain
functional elements such as determiners, coordina-
tion markers, subordinating elements, and so on.
Multi-Headed Constructions. Some phrases are
considered to have multiple lexical heads, for in-
stance, coordinated structures. Since dependency-
based formalisms require us to represent all con-
tent as binary relations, there are different ways we
could represent such constructions. Let us consider
the coordination of nominals below. We can choose
between a functional head (1a) and a lexical head
(2b, 2c). We can further choose between a flat rep-
resentation in which the first conjunct is a single
head (2b), or a nested structure where each con-
junct/marker is the head of the following element
(2c). All three alternatives empirically exist. Exam-
ple (2a) reflects the structures in the CoNLL 2007
shared task data (Nivre et al, 2007a). Johansson
and Nugues (2007) use structures like (2b). Exam-
ple (2c) reflects the analysis of Mel?c?uk (1988).
(2a) and
earth wind fire
(2b) earth
wind and fire
(2c) earth
wind
and
fire
Periphrastic Marking. When a phrase includes
periphrastic marking ? such as the tense and modal
marking in the phrase ?would have worked? below
? there are different ways to consider its division
into phrases. One way to analyze this phrase would
be to choose auxiliaries as heads, as in (3a). Another
alternative would be to choose the final verb as the
prep pobj
con
j con
j
conj cc
coord
conj
coordconj
conj
386
Experiment Gold Parse
#1 arrive
on
Sunday
arrive
on
Sunday
#2 arrive
Sunday
on
arrive
Sunday
on
Gold: #1 # 2
Parse
#1 1.0 0.0
#2 0.0 1.0
Figure 1: Calculating cross-experiment LAS results
main head, and let the auxiliaries create a verb chain
with different levels of projection. Each annotation
decision dictates a different direction of the arcs and
imposes its own internal division into phrases.
(3a) would
have
worked
(3b) worked
have
would
In standard settings, an experiment that uses
a data set which adheres to a certain annotation
scheme reports results that are compared against the
annotation standard that the parser was trained on.
But if parsers were trained on different annotation
standards, the empirical results are not comparable
across experiments. Consider, for instance, the ex-
ample in Figure 1. If parse1 and parse2 are com-
pared against gold2 using labeled attachment scores
(LAS), then parse1 results are lower than the results
of parse2, even though both parsers produced lin-
guistically correct and perfectly useful output.
Existing methods for making parsing results com-
parable across experiments include heuristics for
converting outputs into dependency trees of a prede-
fined standard (Briscoe et al, 2002; Cer et al, 2010)
or evaluating the performance of a parser within an
embedding task (Miyao et al, 2008; Buyko and
Hahn, 2010). However, heuristic rules for cross-
annotation conversion are typically hand written and
error prone, and may not cover all possible discrep-
ancies. Task-based evaluation may be sensitive to
the particular implementation of the embedding task
and the procedures that extract specific task-related
features from the different parses. Beyond that,
conversion heuristics and task-based procedures are
currently developed almost exclusively for English.
Other languages typically lack such resources.
A recent study by Schwartz et al (2011) takes
a different approach towards cross-annotation eval-
uation. They consider different directions of
head-dependent relations (such as on?Sunday
and Sunday?on) and different parent-child and
grandparent-child relations in a chain (such as
arrive?on and arrive?sunday in ?arrive on sun-
day?) as equivalent. They then score arcs that fall
within corresponding equivalence sets. Using these
new scores Schwartz et al (2011) neutralize certain
annotation discrepancies that distort parse compar-
ison. However, their treatment is limited to local
context and does not treat structures larger than two
sequential arcs. Additionally, since arcs in differ-
ent directions are typically labeled differently, this
method only applies for unlabeled dependencies.
What we need is a fully deterministic and for-
mally precise procedure for comparing any set of la-
beled or unlabeled dependency trees, by consolidat-
ing the shared linguistic content of the complete de-
pendency trees in different annotation schemes, and
comparing parse hypotheses through sound metrics
that can take into account multiple gold standards.
3 The Proposal: Cross-Annotation
Evaluation in Three Simple Steps
We propose a new protocol for cross-experiment
parse evaluation, consisting of three fundamental
components: (i) abstracting away from annotation
peculiarities, (ii) generalizing theory-specific struc-
tures into a single linguistically coherent gold stan-
dard that contains all and only consistent informa-
tion from all sources, and (iii) defining a sound met-
ric that takes into account the different gold stan-
dards that are being considered in the experiments.
In this section we first define functional trees as
the common space of formal objects and define a de-
terministic conversion procedure from dependency
trees to functional trees. Next we define a set of for-
mal operations on functional trees that compute, for
every pair of corresponding trees of the same yield, a
single gold tree that resolves inconsistencies among
gold standard alternatives and combines the infor-
mation that they share. Finally, we define scores
based on tree edit distance, refined to consider the
distance from parses to the overall gold tree as well
as the different annotation alternatives.
vg vg
vgvg
tmod
pobj pobj
prepprep
tmod
tmod
tmod
387
Preliminaries. Let T be a finite set of terminal
symbols and let L be a set of grammatical relation
labels. A dependency graph d is a directed graph
which consists of nodes Vd and arcs Ad ? Vd ? Vd.
We assume that all nodes in Vd are labeled by ter-
minal symbols via a function labelV : Vd ? T . A
well-formed dependency graph d = (Vd, Ad) for a
sentence S = t1, t2, ..., tn is any dependency graph
that is a directed tree originating out of a node v0
labeled t0 = ROOT , and spans all terminals in
the sentence, that is, for every ti ? S there exists
vj ? Vd labeled labelV (vj) = ti. For simplicity we
assume that every node vj is indexed according to
the position of the terminal label, i.e., that for each
ti labeling vj , i always equals j. In a labeled de-
pendency tree, arcs in Ad are labeled by elements
of L via a function labelA : Ad ? L that encodes
the grammatical relation between the terminals la-
beling the connected nodes. We define two auxiliary
functions on nodes in dependency trees. The func-
tion subtree : Vd ? P(Vd) assigns to every node
v ? Vd the set of nodes accessible by it through
the reflexive transitive closure of the arc relation Ad.
The function span : Vd ? P(T ) assigns to every
node v ? Vd a set of terminals such that span(v) =
{t ? T |t = labelV (u) and u ? subtree(v)}.1
Step 1: Functional Representation Our first goal
is to define a representation format that keeps all
functional relationships that are represented in the
dependency trees intact, but remains neutral with
respect to the directionality of the head-dependent
relations. To do so we define functional trees
? linearly-ordered labeled trees which, instead of
head-to-head binary relations, represent the com-
plete functional structure of a sentence. Assuming
the same sets of terminal symbols T and grammat-
ical relation labels L, and assuming extended sets
of nodes V and arcs A ? V ? V , a functional tree
pi = (V,A) is a directed tree originating from a sin-
gle root v0 ? V where all non-terminal nodes in
pi are labeled with grammatical relation labels that
signify the grammatical function of the chunk they
dominate inside the tree via labelNT : V ? L. All
1If a dependency tree d is projective, than for all v ? Vd the
terminals in span(v) form a contiguous segment of S. The cur-
rent discussion assumes that all trees are projective. We com-
ment on non-projective dependencies in Section 4.
terminal nodes in pi are labeled with terminal sym-
bols via a labelT : V ? T function. The function
span : V ? P(V ) now picks out the set of ter-
minal labels of the terminal nodes accessible by a
node v ? V via A. We obtain functional trees from
dependency trees using the following procedure:
? Initialize the set of nodes and arcs in the tree.
V := Vd
A := Ad
? Label each node v ? V with the label of its
incoming arc.
labelNT (v) = labelA(u, v)
? In case |span(v)| > 1 add a new node u as a
daughter designating the lexical head, labeled
with the wildcard symbol *:
V := V ? {u}
A := A ? {(v, u)}
labelNT (u) = ?
? For each node v such that |span(v)| = 1, add a
new node u as a daughter, labeled with its own
terminal:
V := V ? {u}
A := A ? {(v, u)}
if (labelNT (v) = ?)
labelT (u) := labelV (v)
else
labelT (u) := labelV (parent(v))
That is to say, we label all nodes with spans
greater than 1 with the grammatical function of their
head, and for each node we add a new daughter u
designating the head word, labeled with its gram-
matical function. Wildcard labels are compatible
with any, more specific, grammatical function of the
word inside the phrase. This gives us a constituency-
like representation of dependency trees labeled with
functional information, which retains the linguis-
tic assumptions reflected in the dependency trees.
When applying this procedure, examples (1)?(3) get
transformed into (4)?(6) respectively.
(4a) ...
prep
on
*
Sunday
(4b) ...
*
on
pobj
Sunday
388
(5a) ...
conj
earth
conj
wind
*
and
conj
fire
(5b) ...
*
earth
conj
wind
cc
and
conj
fire
(5c) ...
*
earth
coord
*
wind
coord
*
and
conj
fire
(6a) ...
*
would
vg
*
have
vg
worked
(6b) ...
vg
vg
would
*
have
*
worked
Considering the functional trees resulting from
our procedure, it is easy to see that for tree pairs
(4a)?(4b) and (5a)?(5b) the respective functional
trees are identical modulo wildcards, while tree pairs
(5b)?(5c) and (6a)?(6b) end up with different tree
structures that realize different assumptions con-
cerning the internal structure of the tree. In order
to compare, combine or detect inconsistencies in the
information inherent in different functional trees, we
define a set of formal operations that are inspired by
familiar notions from unification-based formalisms
(Shieber (1986) and references therein).
Step 2: Formal Operations on Trees The intu-
ition behind the formal operations we define is sim-
ple. A completely flat tree over a span is the most
general structural description that can be given to it.
The more nodes dominate a span, the more linguis-
tic assumptions are made with respect to its struc-
ture. If an arc structure in one tree merely elaborates
an existing flat span in another tree, the theories un-
derlying the schemes are compatible, and their in-
formation can be combined. Otherwise, there exists
a conflict in the linguistic assumptions, and we need
to relax some of the assumptions, i.e., remove func-
tional nodes, in order to obtain a coherent structure
that contains the information on which they agree.
Let pi1, pi2 be functional trees over the same yield
t1, .., tn. Let the function span(v) pick out the ter-
minals labeling terminal nodes that are accessible
via a node v ? V in the functional tree through the
relation A. We define first the tree subsumption re-
lation for comparing the amount of information in-
herent in the arc-structure of two trees.2
T-Subsumption, denoted t, is a relation be-
tween trees which indicates that a tree pi1 is
consistent with and more general than tree
pi2. Formally: pi1 t pi2 iff for every node
n ? pi1 there exists a node m ? pi2 such
that span(n) = span(m) and label(n) =
label(m).
Looking at the functional trees of (4a)?(4b) we
see that their unlabeled skeletons mutually subsume
each other. In their labeled versions, however, each
tree contains labeling information that is lacking in
the other. In the functional trees (5b)?(5c) a flat
structure over a span in (5b) is more elaborated in
(5c). In order to combine information in trees with
compatible arc structures, we define tree unification.
T-Unification, denoted unionsqt, is the operation that
returns the most general tree structure pi3 that
is subsumed by both pi1, pi2 if such exists, and
fails otherwise. Formally: pi1 unionsqt pi2 = pi3 iff
pi1 t pi3 and pi2 t pi3, and for all pi4 such that
pi1 t pi4 and pi2 t pi4 it holds that pi3 t pi4.
Tree unification collects the information from two
trees into a single result if they are consistent, and
detects an inconsistency otherwise. In case of an
inconsistency, as is the case in the functional trees
(6a) and (6b), we cannot unify the structures due
to a conflict concerning the internal division of an
expression into phrases. However, we still want to
generalize these two trees into one tree that contains
all and only the information that they share. For that
we define the tree generalization operation.
T-Generalization, denoted t, is the operation
that returns the most specific tree that is more
general than both trees. Formally, pi1 t pi2 =
pi3 iff pi3 t pi1 and pi3 t pi2, and for every pi4
such that pi4 t pi1 and pi4 t pi2 it holds that
pi4 t pi3.
2Note that the wildcard symbol * is equal to any other sym-
bol. In case the node labels consist of complex feature structures
made of attribute-value lists, we replace label(n) = label(m)
in the subsumption definition with label(n)  label(m) in the
sense of (Shieber, 1986).
389
Unlike unification, generalization can never fail.
For every pair of trees there exists a tree that is more
general than both: in the extreme case, pick the com-
pletely flat structure over the yield, which is more
general than any other structure. For (6a)?(6b), for
instance, we get that (6a)t(6b) is a flat tree over
pre-terminals where ?would? and ?have? are labeled
with ?vg? and ?worked? is the head, labeled with ?*?.
The generalization of two functional trees pro-
vides us with one structure that reflects the common
and consistent content of the two trees. These struc-
tures thus provide us with a formally well-defined
gold standard for cross-treebank evaluation.
Step 3: Measuring Distances. Our functional
trees superficially look like constituency-based
trees, so a simple proposal would be to use Parse-
val measures (Black et al, 1991) for comparing the
parsed trees against the new generalized gold trees.
Parseval scores, however, have two significant draw-
backs. First, they are known to be too restrictive
with respect to some errors and too permissive with
respect to others (Carroll et al, 1998; Ku?bler and
Telljohann, 2002; Roark, 2002; Rehbein and van
Genabith, 2007). Secondly, F1 scores would still
penalize structures that are correct with respect to
the original gold, but are not there in the generalized
structure. Here we propose to adopt measures that
are based on tree edit distance (TED) instead. TED-
based measures are, in fact, an extension of attach-
ment scores for dependency trees. Consider, for in-
stance, the following operations on dependency arcs.
reattach-arc remove arc (u, v) ? Ad and add
an arc Ad ? {(w, v)}.
relabel-arc relabel arc l1(u, v) as l2(u, v)
Assuming that each operation is assigned a cost,
the attachment score of comparing two dependency
trees is simply the cost of all edit operations that are
required to turn a parse tree into its gold standard,
normalized with respect to the overall size of the de-
pendency tree and subtracted from a unity.3 Here
we apply the idea of defining scores by TED costs
normalized relative to the size of the tree and sub-
stracted from a unity, and extend it from fixed-size
dependency trees to ordered trees of arbitrary size.
3The size of a dependency tree, either parse or gold, is al-
ways fixed by the number of terminals.
Our formalization follows closely the formulation
of the T-Dice measure of Emms (2008), building on
his thorough investigation of the formal and empir-
ical differences between TED-based measures and
Parseval. We first define for any ordered and labeled
tree pi the following operations.
relabel-node change the label of node v in pi
delete-node delete a non-root node v in pi with
parent u, making the children of v the children
of u, inserted in the place of v as a subsequence
in the left-to-right order of the children of u.
insert-node insert a node v as a child of u in
pi making it the parent of a consecutive subse-
quence of the children of u.
An edit script ES(pi1, pi2) = {e0, e1....ek} between
pi1 and pi2 is a set of edit operations required for turn-
ing pi1 into pi2. Now, assume that we are given a cost
function defined for each edit operation. The cost of
ES(pi1, pi2) is the sum of the costs of the operations
in the script. An optimal edit script is an edit script
between pi1 and pi2 of minimum cost.
ES?(pi1, pi2) = argminES(pi1,pi2)
?
e?ES(pi1,pi2)
cost(e)
The tree edit distance problem is defined to be the
problem of finding the optimal edit script and com-
puting the corresponding distance (Bille, 2005).
A simple way to calculate the error ? of a parse
would be to define it as the edit distance between
the parse hypothesis pi1 and the gold standard pi2.
?(pi1, pi2) = cost(ES?(pi1, pi2))
However, in such cases the parser may still get pe-
nalized for recovering nodes that are lacking in the
generalization. To solve this, we refine the distance
between a parse tree and the generalized gold tree
to discard edit operations on nodes that are there in
the native gold tree but are eliminated through gen-
eralization. We compute the intersection of the edit
script turning the parse tree into the generalize gold
with the edit script turning the native gold tree into
the generalized gold, and discard its cost. That is, if
parse1 and parse2 are compared against gold1 and
gold2 respectively, and if we set gold3 to be the re-
sult of gold1tgold2, then ?new is defined as:
390
Figure 2: The evaluation pipeline. Different versions of the treebank go into different experiments, resulting in
different parse and gold files. All trees are transformed into functional trees. All gold files enter generalization to
yield a new gold. The different ? arcs represent the different tree distances used for calculating the TED-based scores.
?new(parse1, gold1,gold3) =
?(parse1,gold3)
?cost(ES?(parse1,gold3)?ES?(gold1,gold3))
Now, if gold1 and gold3 are identi-
cal, then ES?(gold1,gold3)=? and we fall
back on the simple tree edit distance score
?new(parse1,gold1,gold3)=?(parse1, gold3).
When parse1 and gold1 are identical,
i.e., the parser produced perfect out-
put with respect to its own scheme, then
?new(parse1,gold1,gold3)=?new(gold1,gold1,gold3)
=?(gold1,gold3)? cost(ES?(gold1,gold3))=0, and
the parser does not get penalized for recovering a
correct structure in gold1 that is lacking in gold3.
In order to turn distances into accuracy measures
we have to normalize distances relative to the maxi-
mal number of operations that is conceivable. In the
worst case, we would have to remove all the internal
nodes in the parse tree and add all the internal nodes
of the generalized gold, so our normalization factor
? is defined as follows, where |pi| is the size4 of pi.
?(parse1,gold3) = |parse1| + |gold3|
We now define the score of parse1 as follows:5
1? ?new(parse1,gold1,gold3)?(parse1,gold3)
Figure 2 summarizes the steps in the evalu-
ation procedure we defined so far. We start
off with two versions of the treebank, TB1 and
TB2, which are parsed separately and provide their
own gold standards and parse hypotheses in a la-
beled dependencies format. All dependency trees
4Following common practice, we equate size |pi| with the
number of nodes in pi, discarding the terminals and root node.
5If the trees have only root and leaves, ? = 0, score := 1.
are then converted into functional trees, and we
compute the generalization of each pair of gold
trees for each sentence in the data. This pro-
vides the generalized gold standard for all exper-
iments, here marked as gold3.6 We finally com-
pute the distances ?new(parse1,gold1,gold3) and
?new(parse2,gold2,gold3) using the different tree
edit distances that are now available, and we repeat
the procedure for each sentence in the test set.
To normalize the scores for an entire test set of
size n we can take the arithmetic mean of the scores.
?|test-set|
i=1 score(parse1i,gold1i,gold3i)
|test-set|
Alternatively we can globally average of all edit dis-
tance costs, normalized by the maximally possible
edits on parse trees turned into generalized trees.
1?
?|test-set|
i=1 ?new(parse1i,gold1i,gold3i)?|test-set|
i=1 ?(parse1i,gold3i)
The latter score, global averaging over the entire test
set, is the metric we use in our evaluation procedure.
4 Experiments
We demonstrate the application of our procedure to
comparing dependency parsing results on different
versions of the Penn Treebank (Marcus et al, 1993).
The Data We use data from the PTB, converted
into dependency structures using the LTH soft-
ware, a general purpose tool for constituency-to-
dependency conversion (Johansson and Nugues,
2007). We use LTH to implement the five different
annotation standards detailed in Table 3.
6Generalization is an associative and commutative opera-
tion, so it can be extended for n experiments in any order.
TB1 parse1.dep
gold1.dep
parse2.dep
gold2.dep
parse1
gold3
gold1
parse2
gold2TB2
parse
parse
? (parse1,gold3)
? (gold1,gold3)
? (pars
e2,gold
3)
? (gold2,gold
3)
parse transform generalizeparse
391
Train Default Old LTH CoNLL07
Gold
Default UAS 0.9142 0.6077 0.7772
LAS 0.8820 0.4801 0.6454
U-TED 0.9488 0.8926 0.9237
L-TED 0.9241 0.7811 0.8441
Old LTH UAS 0.6053 0.8955 0.6508
LAS 0.4816 0.8644 0.5771
U-TED 0.8931 0.9564 0.9092
L-TED 0.7811 0.9317 0.8197
CoNLL07 UAS 0.7734 0.6474 0.8917
LAS 0.6479 0.5722 0.8736
U-TED 0.9260 0.9097 0.9474
L-TED 0.8480 0.8204 0.9233
Default-OldLTH U-TED 0.9500 0.9543
L-TED 0.9278 0.9324
Default-CoNLL07 U-TED 0.9444? 0.9453?
L-TED 0.9266? 0.9260?
oldLTH-CoNLL07 U-TED 0.9519 0.9490
L-TED 0.9323 0.9283
default-oldLTH-CoNLL U-TED 0.9464? 0.9515 0.9471?
L-TED 0.9281? 0.9336 0.9280?
Train CoNLL07 Functional Lexical
Gold
CoNLL07 UAS 0.8917 0.8054 0.6986
LAS 0.8736 0.7895 0.6831
U-TED 0.9474 0.9357 0.9237
L-TED 0.9233 0.8960 0.8606
Functional UAS 0.8040 0.8970 0.6110
LAS 0.7873 0.8793 0.5977
U-TED 0.9347 0.9466 0.9107
L-TED 0.8948 0.9239 0.8316
Lexical UAS 0.7013 0.6138 0.8823
LAS 0.6875 0.6022 0.8635
U-TED 0.9252 0.9132 0.9500
L-TED 0.8623 0.8345 0.9266
CoNLL07-Functional U-TED 0.9473? 0.9473?
L-TED 0.9233 0.9247
CoNLL07-Lexical U-TED 0.9490? 0.9500?
L-TED 0.9253? 0.9266?
Functional-Lexical U-TED 0.9489? 0.9501?
L-TED 0.9266? 0.9267?
CoNLL07-Functional-Lexical U-TED 0.9489? 0.9489? 0.9501?
L-TED 0.9254? 0.9266? 0.9267?
Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan-
dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported
in the same row. The ? sign marks pairwise results where the difference is not statistically significant.
Train Default Old LTH CoNLL07
Gold
Default UAS 0.9173 0.6085 0.7709
LAS 0.8833 0.4780 0.6414
U-TED 0.9513 0.8903 0.9236
L-TED 0.9249 0.7727 0.8424
Old LTH UAS 0.6078 0.8952 0.6415
LAS 0.4809 0.8471 0.5669
U-TED 0.8960 0.9550 0.9096
L-TED 0.7823 0.9224 0.8170
CoNLL07 UAS 0.7767 0.6517 0.8991
LAS 0.6504 0.5725 0.8709
U-TED 0.9289 0.9087 0.9479
L-TED 0.8502 0.8159 0.9208
Default-oldLTH U-TED 0.9533 0.9515
L-TED 0.9289 0.9224
Default-CoNLL U-TED 0.9474? 0.9460?
L-TED 0.9281 0.9238
OldLTH-CoNLL U-TED 0.9479 0.9493
L-TED 0.9234 0.9258
Default-OldLTH-CoNLL U-TED 0.9492? 0.9461 0.9480?
L-TED 0.9298 0.9241? 0.9258?
Train CoNLL07 Functional Lexical
Gold
CoNLL07 UAS 0.8991 0.8077 0.7018
LAS 0.8709 0.7902 0.6804
U-TED 0.9479 0.9373 0.9221
L-TED 0.9208 0.8955 0.8505
Functional UAS 0.8083 0.8978 0.6150
LAS 0.7895 0.8782 0.5975
U-TED 0.9356 0.9476 0.9092
L-TED 0.8929 0.9226 0.8218
Lexical UAS 0.6997 0.6161 0.8826
LAS 0.6835 0.6034 0.8491
U-TED 0.9259 0.9152 0.9483
L-TED 0.8593 0.8340 0.9160
CoNLL-Functional U-TED 0.9479? 0.9487?
L-TED 0.9209 0.9237
CoNLL-Lexical U-TED 0.9497 0.9483
L-TED 0.9228 0.9161
Functional-Lexical U-TED 0.9504 0.9483
L-TED 0.9258 0.9161
CoNLL-Functional-Lexical U-TED 0.9498 0.9504? 0.9483?
L-TED 0.9229 0.9258 0.9161
Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We
report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results
reported in the same row. The ? sign marks pairwise results where the difference is not statistically significant.
ID Description
Default The LTH conversion default settings
OldLTH The conversion used in Johansson and Nugues (2007)
CoNLL07 The conversion used in the CoNLL shared task (Nivre et al, 2007a)
Lexical Same as CoNLL, but selecting only lexical heads when a choice exists
Functional Same as CoNLL, but selecting only functional heads when a choice exists
Table 3: LTH conversion schemes used in the experiments. The LTH conversion settings in terms of the complete
feature-value pairs associated with the LTH parameters in different schemes are detailed in the supplementary material.
392
The Default, OldLTH and CoNLL schemes
mainly differ in their coordination structure, and the
Functional and Lexical schemes differ in their selec-
tion of a functional and a lexical head, respectively.
All schemes use the same inventory of labels.7 The
LTH parameter settings for the different schemes are
elaborated in the supplementary material.
The Setup We use two different parsers: (i) Malt-
Parser (Nivre et al, 2007b) with the arc eager algo-
rithm as optimized for English in (Nivre et al, 2010)
and (ii) MSTParser with the second-order projec-
tive model of McDonald and Pereira (2006). Both
parsers were trained on the different instances of
sections 2-21 of the PTB obeying the different an-
notation schemes in Table 3. Each trained model
was used to parse section 23. All non-projective de-
pendencies in the training and gold sets were projec-
tivized prior to training and parsing using the algo-
rithm of Nivre and Nilsson (2005). A more princi-
pled treatment of non-projective dependency trees is
an important topic for future research. We evaluated
the parses using labeled and unlabeled attachment
scores, and using our TEDEVAL software package.
Evaluation Our TEDEVAL software package im-
plements the pipeline described in Section 3. We
convert all parse and gold trees into functional
trees using the algorithm defined in Section 3, and
for each pair of parsing experiments we calculate
a shared gold standard using generalization deter-
mined through a chart-based greedy algorithm.8 Our
scoring procedure uses the TED algorithm defined
by Zhang and Shasha (1989).9 The unlabeled score
is obtained by assigning cost(e) = 0 for every e re-
labeling operation. To calculate pairwise statistical
significance we use a shuffling test with 10,000 it-
erations (Cohen, 1995). A sample of all files in the
evaluation pipeline for a subset of 10 PTB sentences
is available in the supplementary materials.10
7In case the labels are not taken from the same inventory,
e.g., subjects in one scheme are marked as SUB and in the other
marked as SBJ, it is possible define a a set of zero-cost operation
types ? in such case, to the operation relabel(SUB,SBJ) ? in
order not to penalize string label discrepancies.
8Our algorithm has space and runtime complexity ofO(n2).
9Available via http://web.science.mq.edu.au/
?swan/howtos/treedistance/
10The TEDEVAL software package is available via http:
//stp.lingfil.uu.se/?tsarfaty/unipar
Results Table 1 reports the results for the inter-
and cross-experiment evaluation of parses produced
by MaltParser. The left hand side of the table
presents the parsing results for a set of experiments
in which we compare parsing results trained on the
Default, OldLTH and CoNLL07 schemes. In a sec-
ond set of experiments we compare the CoNLL07,
Lexical and Functional schemes. Table 2 reports the
evaluation of the parses produced by MSTParser for
the same experimental setup. Our goal here is not to
compare the parsers, but to verify that the effects of
switching from LAS to TEDEVAL are robust across
parsing algorithms.
In each of the tables, the top three groups of four
rows compare results of parsed dependency trees
trained on a particular scheme against gold trees of
the same and the other schemes. The next three
groups of two rows report the results for compar-
ing pairwise sets of experiments against a general-
ized gold using our proposed procedure. In the last
group of two rows we compare all parsing results
against a single gold obtained through a three-way
generalization.
As expected, every parser appears to perform at
its best when evaluated against the scheme it was
trained on. This is the case for both LAS and TEDE-
VAL measures and the performance gaps are statis-
tically significant. When moving to pairwise evalu-
ation against a single generalized gold, for instance,
when comparing CoNLL07 to the Default settings,
there is still a gap in performance, e.g., between
OldLTH and CoNLL07, and between OldLTH and
Default. This gap is however a lot smaller and is not
always statistically significant. In fact, when evalu-
ating the effect of linguistically disparate annotation
variations such as Lexical and Functional on the per-
formance of MaltParser, Table 1 shows that when
using TEDEVAL and a generalized gold the perfor-
mance gaps are small and statistically insignificant.
Moreover, observed performance trends when
evaluating individual experiments on their original
training scheme may change when compared against
a generalized gold. The Default scheme, for Malt-
Parser, appears better than OldLTH when both are
evaluated against their training schemes. But look-
ing at the pairwise-evaluated experiments, it is the
other way round (the difference is smaller, but statis-
tically significant). In evaluating against a three-way
393
generalization, all the results obtained for different
training schemes are on a par with one another, with
minor gaps in performance, rarely statistically sig-
nificant. This suggests that apparent performance
trends between experiments when evaluating with
respect to the training schemes may be misleading.
These observations are robust across parsing algo-
rithms. In each of the tables, results obtained against
the training schemes show significant differences
whereas applying our cross-experimental procedure
shows small to no gaps in performance across dif-
ferent schemes. Annotation variants which seem to
have crucial effects have a relatively small influence
when parsed structures are brought into the same
formal and theoretical common ground for compar-
ison. Of course, it may be the case that one parser is
better trained on one scheme while the other utilizes
better another scheme, but objective performance
gaps can only be observed when they are compared
against shared linguistic content.
5 Discussion and Extensions
This paper addresses the problem of cross-
experiment evaluation. As it turns out, this prob-
lem arises in NLP in different shapes and forms;
when evaluating a parser against different annota-
tion schemes, when evaluating parsing performance
across parsers and different formalisms, and when
comparing parser performance across languages.
We consider our contribution successful if after
reading it the reader develops a healthy suspicion to
blunt comparison of numbers across experiments, or
better yet, across different papers. Cross-experiment
comparison should be a careful and well thought-
through endeavor, in which we retain as much infor-
mation as we can from the parsed structures, avoid
lossy conversions, and focus on an object of evalua-
tion which is agreed upon by all variants.
Our proposal introduces one way of doing so in
a streamlined, efficient and formally worked out
way. While individual components may be further
refined or improved, the proposed setup and imple-
mentation can be straightforwardly applied to cross-
parser and cross-framework evaluation. In the fu-
ture we plan to use this procedure for comparing
constituency and dependency parsers. A conversion
from constituency-based trees into functional trees
is straightforward to define: simply replace the node
labels with the grammatical function of their domi-
nating arc ? and the rest of the pipeline follows.
A pre-condition for cross-framework evaluation
is that all representations encode the same set of
grammatical relations by, e.g., annotating arcs in de-
pendency trees or decorating nodes in constituency
trees. For some treebanks this is already the case
(Nivre and Megyesi, 2007; Skut et al, 1997; Hin-
richs et al, 2004) while for others this is still lack-
ing. Recent studies (Briscoe et al, 2002; de Marn-
effe et al, 2006) suggest that evaluation through a
single set of grammatical relations as the common
denominator is a linguistically sound and practically
useful way to go. To guarantee extensions for cross-
framework evaluation it would be fruitful to make
sure that resources use the same set of grammatical
relation labels across different formal representation
types. Moreover, we further aim to inquire whether
we can find a single set of grammatical relation la-
bels that can be used across treebanks for multiple
languages. This would then pave the way for the de-
velopment of cross-language evaluation procedures.
6 Conclusion
We propose an end-to-end procedure for compar-
ing dependency parsing results across experiments
based on three steps: (i) converting dependency trees
to functional trees, (ii) generalizing functional trees
to harmonize information from different sources,
and (iii) using distance-based metrics that take the
different sources into account. When applied to
parsing results of different dependency schemes,
dramatic gaps observed when comparing parsing re-
sults obtained in isolation decrease or dissolve com-
pletely when using our proposed pipeline.
Acknowledgments We thank the developers of
the LTH and TED software who made their code
available for our use. We thank Richard Johansson
for providing us with the LTH parameter settings of
existing dependency schemes. We thank Ari Rap-
poport, Omri Abend, Roy Schwartz and members of
the NLP lab at the Hebrew University of Jerusalem
for stimulating discussion. We finally thank three
anonymous reviewers for useful comments on an
earlier draft. The research reported in the paper was
partially funded by the Swedish Research Council.
394
References
Philip Bille. 2005. A survey on tree edit distance
and related. problems. Theoretical Computer Science,
337:217?239.
Ezra Black, Steven P. Abney, D. Flickenger, Claudia
Gdaniec, Ralph Grishman, P. Harrison, Donald Hin-
dle, Robert Ingria, Frederick Jelinek, Judith L. Kla-
vans, Mark Liberman, Mitchell P. Marcus, Salim
Roukos, Beatrice Santorini, and Tomek Strzalkowski.
1991. Procedure for quantitatively comparing the syn-
tactic coverage of English grammars. In E. Black, ed-
itor, Proceedings of the workshop on Speech and Nat-
ural Language, HLT, pages 306?311. Association for
Computational Linguistics.
Ted Briscoe, John Carroll, Jonathan Graham, and Ann
Copestake. 2002. Relational evaluation schemes.
In Proceedings of LREC Workshop?Beyond Parseval
? Towards improved evaluation measures for parsing
systems?.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL-X, pages 149?164.
Ekaterina Buyko and Udo Hahn. 2010. Evaluating
the impact of alternative dependency graph encodings
on solving event extraction tasks. In Proceedings of
EMNLP, pages 982?992.
John Carroll, Ted Briscoe, and Antonio Sanfilippo. 1998.
Parser evaluation: a survey and a new proposal. In
Proceedings of LREC, pages 447?454.
Daniel Cer, Marie-Catherine de Marneffe, Daniel Juraf-
sky, and Christopher D. Manning. 2010. Parsing to
stanford dependencies: Trade-offs between speed and
accuracy. In Proceedings of LREC.
Jinho D. Choi and Martha Palmer. 2010. Robust
constituent-to-dependency conversion for English. In
Proceedings of TLT.
Paul Cohen. 1995. Empirical Methods for Artificial In-
telligence. The MIT Press.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC, pages 449?454.
Martin Emms. 2008. Tree-distance and some other vari-
ants of evalb. In Proceedings of LREC.
Erhard Hinrichs, Sandra Ku?bler, Karin Naumann, Heike
Telljohan, and Julia Trushkina. 2004. Recent develop-
ment in linguistic annotations of the Tu?Ba-D/Z Tree-
bank. In Proceedings of TLT.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proceedings of NODALIDA.
Sandra Ku?bler and Heike Telljohann. 2002. Towards
a dependency-oriented evaluation for partial parsing.
In Proceedings of LREC Workshop?Beyond Parseval
? Towards improved evaluation measures for parsing
systems?.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Number 2 in Synthesis
Lectures on Human Language Technologies. Morgan
& Claypool Publishers.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19:313?330.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of EACL, pages 81?88.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL, pages 91?98.
Igor Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
Yusuke Miyao, Rune S?tre, Kenji Sagae, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2008. Task-oriented eval-
uation of syntactic parsers and their representations. In
Proceedings of ACL, pages 46?54.
Joakim Nivre and Beata Megyesi. 2007. Bootstrapping
a Swedish Treebank using cross-corpus harmonization
and annotation projection. In Proceedings of TLT.
Joakim Nivre and Jens Nilsson. 2005. Pseudo projective
dependency parsing. In Proceeding of ACL, pages 99?
106.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In Proceedings of
COLING, pages 64?70.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007a. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 915?932.
Joakim Nivre, Jens Nilsson, Johan Hall, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007b. Maltparser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(1):1?41.
JoakimNivre, Laura Rimell, RyanMcDonald, and Carlos
Go?mez-Rodr??guez. 2010. Evaluation of dependency
parsers on unbounded dependencies. pages 813?821.
Owen Rambow. 2010. The Simple Truth about Depen-
dency and Phrase Structure Representations: An Opin-
ion Piece. In Proceedings of HLT-ACL, pages 337?
340.
Ines Rehbein and Josef van Genabith. 2007. Why is it so
difficult to compare treebanks? Tiger and Tu?Ba-D/Z
revisited. In Proceedings of TLT, pages 115?126.
395
Brian Roark. 2002. Evaluating parser accuracy us-
ing edit distance. In Proceedings of LREC Work-
shop?Beyond Parseval ? Towards improved evaluation
measures for parsing systems?.
Roy Schwartz, Omri Abend, Roi Reichart, and Ari Rap-
poport. 2011. Neutralizing linguistically problematic
annotations in unsupervised dependency parsing eval-
uation. In Proceedings of ACL, pages 663?672.
Stuart M. Shieber. 1986. An Introduction to Unification-
Based Grammars. Center for the Study of Language
and Information.
Wojciech Skut, Brigitte Krenn, Thorsten Brants, and
Hans Uszkoreit. 1997. An annotation scheme for free
word-order languages. In Proceedings of the fifth con-
ference on Applied natural language processing, pages
88?95.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceeding of IWPT, pages 195?206.
Kaizhong Zhang and Dennis Shasha. 1989. Simple fast
algorithms for the editing distance between trees and
related problems. In SIAM Journal of Computing, vol-
ume 18, pages 1245?1262.
Arnold M. Zwicky. 1993. Heads, bases, and functors.
In G.G. Corbett, N. Fraser, and S. McGlashan, editors,
Heads in Grammatical Theory, pages 292?315. Cam-
bridge University Press.
396
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1179?1190, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Document-Wide Decoding for
Phrase-Based Statistical Machine Translation
Christian Hardmeier Joakim Nivre Jo?rg Tiedemann
Uppsala University
Department of Linguistics and Philology
Box 635, 751 26 Uppsala, Sweden
firstname.lastname@lingfil.uu.se
Abstract
Independence between sentences is an as-
sumption deeply entrenched in the models and
algorithms used for statistical machine trans-
lation (SMT), particularly in the popular dy-
namic programming beam search decoding al-
gorithm. This restriction is an obstacle to re-
search on more sophisticated discourse-level
models for SMT. We propose a stochastic lo-
cal search decoding method for phrase-based
SMT, which permits free document-wide de-
pendencies in the models. We explore the sta-
bility and the search parameters of this method
and demonstrate that it can be successfully
used to optimise a document-level semantic
language model.
1 Motivation
In the field of translation studies, it is undisputed that
discourse-wide context must be considered care-
fully for good translation results (Hatim and Mason,
1990). By contrast, the state of the art in statistical
machine translation (SMT), despite significant ad-
vances in the last twenty years, still assumes that
texts can be translated sentence by sentence under
strict independence assumptions, even though it is
well known that certain linguistic phenomena such
as pronominal anaphora cannot be translated cor-
rectly without referring to extra-sentential context.
This is true both for the phrase-based and the syntax-
based approach to SMT. In the rest of this paper, we
shall concentrate on phrase-based SMT.
One reason why it is difficult to experiment
with document-wide models for phrase-based SMT
is that the dynamic programming (DP) algorithm
which has been used almost exclusively for decod-
ing SMT models in the recent literature has very
strong assumptions of locality built into it. DP
beam search for phrase-based SMT was described
by Koehn et al(2003), extending earlier work on
word-based SMT (Tillmann et al 1997; Och et al
2001; Tillmann and Ney, 2003). This algorithm con-
structs output sentences by starting with an empty
hypothesis and adding output words at the end until
translations for all source words have been gener-
ated. The core models of phrase-based SMT, in par-
ticular the n-gram language model (LM), only de-
pend on a constant number of output words to the
left of the word being generated. This fact is ex-
ploited by the search algorithm with a DP technique
called hypothesis recombination (Och et al 2001),
which permits the elimination of hypotheses from
the search space if they coincide in a certain number
of final words with a better hypothesis and no future
expansion can possibly invert the relative ranking of
the two hypotheses under the given models. Hypoth-
esis recombination achieves a substantial reduction
of the search space without affecting search optimal-
ity and makes it possible to use aggressive pruning
techniques for fast search while still obtaining good
results.
The downside of this otherwise excellent ap-
proach is that it only works well with models that
have a local dependency structure similar to that
of an n-gram language model, so they only de-
pend on a small context window for each target
word. Sentence-local models with longer dependen-
cies can be added, but doing so greatly increases
the risk for search errors by inhibiting hypothesis
recombination. Cross-sentence dependencies can-
not be directly integrated into DP SMT decoding in
1179
any obvious way, especially if joint optimisation of
a number of interdependent decisions over an entire
document is required. Research into models with
a more varied, non-local dependency structure is to
some extent stifled by the difficulty of decoding such
models effectively, as can be seen by the problems
some researchers encountered when they attempted
to solve discourse-level problems. Consider, for in-
stance, the work on cache-based language models
by Tiedemann (2010) and Gong et al(2011), where
error propagation was a serious issue, or the works
on pronominal anaphora by Le Nagard and Koehn
(2010), who implemented cross-sentence dependen-
cies with an ad-hoc two-pass decoding strategy, and
Hardmeier and Federico (2010) with the use of an
external decoder driver to manage backward-only
dependencies between sentences.
In this paper, we present a method for decoding
complete documents in phrase-based SMT. Our de-
coder uses a local search approach whose state con-
sists of a complete translation of an entire document
at any time. The initial state is improved by the ap-
plication of a series of operations using a hill climb-
ing strategy to find a (local) maximum of the score
function. This setup gives us complete freedom to
define scoring functions over the entire document.
Moreover, by optionally initialising the state with
the output of a traditional DP decoder, we can en-
sure that the final hypothesis is no worse than what
would have been found by DP search alone. We start
by describing the decoding algorithm and the state
operations used by our decoder, then we present em-
pirical results demonstrating the effectiveness of our
approach and its usability with a document-level se-
mantic language model, and finally we discuss some
related work.
2 SMT Decoding by Hill Climbing
In this section, we formally describe the phrase-
based SMT model implemented by our decoder as
well as the decoding algorithm we use.
2.1 SMT Model
Our decoder is based on local search, so its state at
any time is a representation of a complete translation
of the entire document. Even though the decoder op-
erates at the document level, it is important to keep
track of sentence boundaries, and the individual op-
erations that are applied to the state are still confined
to sentence scope, so it is useful to decompose the
state of a document into the state of its sentences,
and we define the overall state S as a sequence of
sentence states:
S = S1S2 . . .SN , (1)
where N is the number of sentences. This implies
that we constrain the decoder to emit exactly one
output sentence per input sentence.
Let i be the number of a sentence and mi the num-
ber of input tokens of this sentence, p and q (with
1 ? p ? q ? mi) be positions in the input sentence
and [p;q] denote the set of positions from p up to and
including q. We say that [p;q] precedes [p?;q?], or
[p;q]? [p?;q?], if q < p?. Let ?i([p;q]) be the set of
translations for the source phrase covering positions
[p;q] in the input sentence i as given by the phrase
table. We call A = ?[p;q],?? an anchored phrase
pair with coverage C(A) = [p;q] if ? ? ?i([p;q]) is
a target phrase translating the source words at posi-
tions [p;q]. Then a sequence of ni anchored phrase
pairs
Si = A1A2 . . .Ani (2)
is a valid sentence state for sentence i if the follow-
ing two conditions hold:
1. The coverage sets C(A j) for j in 1, . . . ,ni are
mutually disjoint, and
2. the anchored phrase pairs jointly cover the
complete input sentence, or
ni?
j=1
C(A j) = [1;mi]. (3)
Let f (S) be a scoring function mapping a state S
to a real number. As usual in SMT, it is assumed that
the scoring function can be decomposed into a linear
combination of K feature functions hk(S), each with
a constant weight ?k, so
f (S) =
K
?
k=1
?khk(S). (4)
The problem addressed by the decoder is the search
for the state S? with maximal score, such that
S? = argmax
S
f (S). (5)
1180
The feature functions implemented in our baseline
system are identical to the ones found in the popular
Moses SMT system (Koehn et al 2007). In particu-
lar, our decoder has the following feature functions:
1. phrase translation scores provided by the
phrase table including forward and backward
conditional probabilities, lexical weights and a
phrase penalty (Koehn et al 2003),
2. n-gram language model scores implemented
with the KenLM toolkit (Heafield, 2011),
3. a word penalty score,
4. a distortion model with geometric decay
(Koehn et al 2003), and
5. a feature indicating the number of times a given
distortion limit is exceeded in the current state.
In our experiments, the last feature is used with a
fixed weight of negative infinity in order to limit the
gaps between the coverage sets of adjacent anchored
phrase pairs to a maximum value. In DP search, the
distortion limit is usually enforced directly by the
search algorithm and is not added as a feature. In
our decoder, however, this restriction is not required
to limit complexity, so we decided to add it among
the scoring models.
2.2 Decoding Algorithm
The decoding algorithm we use (algorithm 1) is
very simple. It starts with a given initial document
state. In the main loop, which extends from line 3
to line 12, it generates a successor state S? for the
current state S by calling the function Neighbour,
which non-deterministically applies one of the oper-
ations described in section 3 of this paper to S. The
score of the new state is compared to that of the pre-
vious one. If it meets a given acceptance criterion,
S? becomes the current state, else search continues
from the previous state S. For the experiments in
this paper, we use the hill climbing acceptance cri-
terion, which simply accepts a new state if its score
is higher than that of the current state. Other accep-
tance criteria are possible and could be used to en-
dow the search algorithm with stochastic behaviour.
The main loop is repeated until a maximum num-
ber of steps (step limit) is reached or until a maxi-
mum number of moves are rejected in a row (rejec-
tion limit).
Algorithm 1 Decoding algorithm
Input: an initial document state S;
search parameters maxsteps and maxrejected
Output: a modified document state
1: nsteps? 0
2: nrejected? 0
3: while nsteps < maxsteps and
nrejected < maxrejected do
4: S?? Neighbour(S)
5: if Accept( f (S?), f (S)) then
6: S? S?
7: nrejected? 0
8: else
9: nrejected? nrejected+1
10: end if
11: nsteps? nsteps+1
12: end while
13: return S
A notable difference between this algorithm and
other hill climbing algorithms that have been used
for SMT decoding (Germann et al 2004; Langlais
et al 2007) is its non-determinism. Previous work
for sentence-level decoding employed a steepest as-
cent strategy which amounts to enumerating the
complete neighbourhood of the current state as de-
fined by the state operations and selecting the next
state to be the best state found in the neighbourhood
of the current one. Enumerating all neighbours of
a given state, costly as it is, has the advantage that
it makes it easy to prove local optimality of a state
by recognising that all possible successor states have
lower scores. It can be rather inefficient, since at
every step only one modification will be adopted;
many of the modifications that are discarded will
very likely be generated anew in the next iteration.
As we extend the decoder to the document level,
the size of the neighbourhood that would have to be
explored in this way increases considerably. More-
over, the inefficiency of the steepest ascent approach
potentially increases as well. Very likely, a promis-
ing move in one sentence will remain promising af-
ter a modification has been applied to another sen-
1181
tence, even though this is not guaranteed to be true
in the presence of cross-sentence models. We there-
fore adopt a first-choice hill climbing strategy that
non-deterministically generates successor states and
accepts the first one that meets the acceptance cri-
terion. This frees us from the necessity of gener-
ating the full set of successors for each state. On
the downside, if the full successor set is not known,
it is no longer possible to prove local optimality of a
state, so we are forced to use a different condition for
halting the search. We use a combination of two lim-
its: The step limit is a hard limit on the resources the
user is willing to expend on the search problem. The
value of the rejection limit determines how much of
the neighbourhood is searched for better successors
before a state is accepted as a solution; it is related
to the probability that a state returned as a solution
is in fact locally optimal.
To simplify notations in the description of the in-
dividual state operations, we write
Si ?? S
?
i (6)
to signify that a state operation, when presented with
a document state as in equation 1 and acting on sen-
tence i, returns a new document state of
S? = S1 . . .Si?1 S
?
i Si+1 . . .SN . (7)
Similarly,
Si : A j . . .A j+h?1 ?? A
?
1 . . .A
?
h? (8)
is equivalent to
Si ?? A1 . . .A j?1 A
?
1 . . .A
?
h? A j+h . . .Ani (9)
and indicates that the operation returns a state in
which a sequence of h consecutive anchored phrase
pairs has been replaced by another sequence of h?
anchored phrase pairs.
2.3 Efficiency Considerations
When implementing the feature functions for the de-
coder, we have to exercise some care to avoid re-
computing scores for the whole document at every
iteration. To achieve this, the scores are computed
completely only once, at the beginning of the de-
coding run. In subsequent iterations, scoring func-
tions are presented with the scores of the previous
iteration and a list of modifications produced by the
state operation, a set of tuples ?i,r,s,A?1 . . .A
?
h??, each
indicating that the document should be modified as
described by
Si : Ar . . .As ?? A
?
1 . . .A
?
h? . (10)
If a feature function is decomposable in some way,
as all the standard features developed under the con-
straints of DP search are, it can then update the state
simply by subtracting and adding score components
pertaining to the modified parts of the document.
Feature functions have the possibility to store their
own state information along with the document state
to make sure the required information is available.
Thus, the framework makes it possible to exploit de-
composability for efficient scoring without impos-
ing any particular decomposition on the features as
beam search does.
To make scoring even more efficient, scores are
computed in two passes: First, every feature func-
tion is asked to provide an upper bound on the score
that will be obtained for the new state. In some
cases, it is possible to calculate reasonable upper
bounds much more efficiently than computing the
exact feature value. If the upper bound fails to meet
the acceptance criterion, the new state is discarded
right away; if not, the full score is computed and the
acceptance criterion is tested again.
Among the basic SMT models, this two-pass
strategy is only used for the n-gram LM, which re-
quires fairly expensive parameter lookups for scor-
ing. The scores of all the other baseline models are
fully computed during the first scoring pass. The
n-gram model is more complex. In its state informa-
tion, it keeps track of the LM score and LM library
state for each word. The first scoring pass then iden-
tifies the words whose LM scores are affected by the
current search step. This includes the words changed
by the search operation as well as the words whose
LM history is modified. The range of the history de-
pendencies can be determined precisely by consider-
ing the ?valid state length? information provided by
the KenLM library. In the first pass, the LM scores
of the affected words are subtracted from the total
score. The model only looks up the new LM scores
for the affected words and updates the total score
if the new search state passes the first acceptance
check. This two-pass scoring approach allows us
1182
to avoid LM lookups altogether for states that will
be rejected anyhow because of low scores from the
other models, e. g. because the distortion limit is vi-
olated.
Model score updates become more complex and
slower as the number of dependencies of a model in-
creases. While our decoding algorithm does not im-
pose any formal restrictions on the number or type
of dependencies that can be handled, there will be
practical limits beyond which decoding becomes un-
acceptably slow or the scoring code becomes very
difficult to maintain. These limits are however fairly
independent of the types of dependencies handled
by a model, which permits the exploration of more
varied model types than those handled by DP search.
2.4 State Initialisation
Before the hill climbing decoding algorithm can be
run, an initial state must be generated. The closer the
initial state is to an optimum, the less work remains
to be done for the algorithm. If the algorithm is to be
self-contained, initialisation must be relatively unin-
formed and can only rely on some general prior as-
sumptions about what might be a good initial guess.
On the other hand, if optimal results are sought after,
it pays off to invest some effort into a good starting
point. One way to do this is to run DP search first.
For uninformed initialisation, we chose to imple-
ment a very simple procedure based only on the ob-
servation that, at least for language pairs involving
the major European languages, it is usually a good
guess to keep the word order of the output very sim-
ilar to that of the input. We therefore create the ini-
tial state by selecting, for each sentence in the docu-
ment, a sequence of anchored phrase pairs covering
the input sentence in monotonic order, that is, such
that for all pairs of adjacent anchored phrase pairs
A j and A j+1, we have that C(A j)?C(A j+1).
For initialisation with DP search, we first run the
Moses decoder (Koehn et al 2007) with default
search parameters and the same models as those
used by our decoder. Then we extract the best output
hypothesis from the search graph of the decoder and
map it into a sequence of anchored phrase pairs in
the obvious way. When the document-level decoder
is used with models that are incompatible with beam
search, Moses can be run with a subset of the mod-
els in order to find an approximation of the solution
which is then refined with the complete feature set.
3 State Operations
Given a document state S, the decoder uses a neigh-
bourhood function Neighbour to simulate a move
in the state space. The neighbourhood function non-
deterministically selects a type of state operation and
a location in the document to apply it to and returns
the resulting new state. We use a set of three opera-
tions that has the property that every possible docu-
ment state can be reached from every other state in
a sequence of moves.
Designing operations for state transitions in lo-
cal search for phrase-based SMT is a problem that
has been addressed in the literature (Langlais et
al., 2007; Arun et al 2010). Our decoder?s first-
choice hill climbing strategy never enumerates the
full neighbourhood of a state. We therefore place
less emphasis than previous work on defining a com-
pact neighbourhood, but allow the decoder to make
quite extensive changes to a state in a single step
with a certain probability. Otherwise our operations
are similar to those used by Arun et al(2010).
All of the operations described in this paper make
changes to a single sentence only. Each time it is
called, the Neighbour function selects a sentence
in the document with a probability proportional to
the number of input tokens in each sentence to en-
sure a fair distribution of the decoder?s attention over
the words in the document regardless of varying sen-
tence lengths.
3.1 Changing Phrase Translations
The change-phrase-translation operation re-
places the translation of a single phrase with a ran-
dom translation with the same coverage taken from
the phrase table. Formally, the operation selects an
anchored phrase pair A j by drawing uniformly from
the elements of Si and then draws a new translation
? ? uniformly from the set ?i(C(A j)). The new state
is given by
Si : A j ?? ?C(A j),? ??. (11)
3.2 Changing Word Order
The swap-phrases operation affects the output
word order without changing the phrase translations.
1183
It exchanges two anchored phrase pairs A j and A j+h,
resulting in an output state of
Si : A j . . .A j+h ?? A j+h A j+1 . . .A j+h?1 A j. (12)
The start location j is drawn uniformly from the el-
igible sentence positions; the swap range h comes
from a geometric distribution with configurable de-
cay. Other word-order changes such as a one-way
move operation that does not require another move-
ment in exchange or more advanced permutations
can easily be defined.
3.3 Resegmentation
The most complex operation is resegment, which
allows the decoder to modify the segmentation of the
source phrase. It takes a number of anchored phrase
pairs that form a contiguous block both in the input
and in the output and replaces them with a new set
of phrase pairs covering the same span of the input
sentence. Formally,
Si : A j . . .A j+h?1 ?? A
?
1 . . .A
?
h? (13)
such that
j+h?1?
j?= j
C(A j?) =
h??
j?=1
C(A?j?) = [p;q] (14)
for some p and q, where, for j? = 1, . . . ,h?, we
have that A?j? = ?[p j? ;q j? ],? j??, all [p j? ;q j? ] are mu-
tually disjoint and each ? j? is randomly drawn from
?i([p j? ;q j? ]).
Regardless of the ordering of A j . . .A j+h?1, the
resegment operation always generates a sequence
of anchored phrase pairs in linear order, such that
C(A?j?)?C(A
?
j?+1) for j
? = 1, . . . ,h??1.
As for the other operations, j is generated uni-
formly and h is drawn from a geometric distribution
with a decay parameter. The new segmentation is
generated by extending the sequence of anchored
phrase pairs with random elements starting at the
next free position, proceeding from left to right until
the whole range [p;q] is covered.
4 Experimental Results
In this section, we present the results of a series
of experiments with our document decoder. The
goal of our experiments is to demonstrate the be-
haviour of the decoder and characterise its response
to changes in the fundamental search parameters.
The SMT models for our experiments were cre-
ated with a subset of the training data for the
English-French shared task at the WMT 2011 work-
shop (Callison-Burch et al 2011). The phrase ta-
ble was trained on Europarl, news-commentary and
UN data. To reduce the training data to a manage-
able size, singleton phrase pairs were removed be-
fore the phrase scoring step. Significance-based fil-
tering (Johnson et al 2007) was applied to the re-
sulting phrase table. The language model was a 5-
gram model with Kneser-Ney smoothing trained on
the monolingual News corpus with IRSTLM (Fed-
erico et al 2008). Feature weights were trained with
Minimum Error-Rate Training (MERT) (Och, 2003)
on the news-test2008 development set using the DP
beam search decoder and the MERT implementation
of the Moses toolkit (Koehn et al 2007). Experi-
mental results are reported for the newstest2009 test
set, a corpus of 111 newswire documents totalling
2,525 sentences or 65,595 English input tokens.
4.1 Stability
An important difference between our decoder and
the classical DP decoder as well as previous work in
SMT decoding with local search is that our decoder
is inherently non-deterministic. This implies that re-
peated runs of the decoder with the same search pa-
rameters, input and models will not, in general, find
the same local maximum of the score space. The
first empirical question we ask is therefore how dif-
ferent the results are under repeated runs. The re-
sults in this and the next section were obtained with
random state initialisation, i. e. without running the
DP beam search decoder.
Figure 1 shows the results of 7 decoder runs with
the models described above, translating the news-
test2009 test set, with a step limit of 227 and a rejec-
tion limit of 100,000. The x-axis of both plots shows
the number of decoding steps on a logarithmic scale,
so the number of steps is doubled between two adja-
cent points on the same curve. In the left plot, the
y-axis indicates the model score optimised by the
decoder summed over all 2525 sentences of the doc-
ument. In the right plot, the case-sensitive BLEU
score (Papineni et al 2002) of the current decoder
1184
Figure 1: Score stability in repeated decoder runs
state against a reference translation is displayed.
We note, as expected, that the decoder achieves
a considerable improvement of the initial state with
diminishing returns as decoding continues. Be-
tween 28 and 214 steps, the score increases at a
roughly logarithmic pace, then the curve flattens out,
which is partly due to the fact that decoding for
some documents effectively stopped when the max-
imum number of rejections was reached. The BLEU
score curve shows a similar increase, from an initial
score below 5 % to a maximum of around 21.5 %.
This is below the score of 22.45 % achieved by the
beam search decoder with the same models, which
is not surprising considering that our decoder ap-
proximates a more difficult search problem, from
which a number of strong independence assump-
tions have been lifted, without, at the moment, hav-
ing any stronger models at its disposal to exploit this
additional freedom for better translation.
In terms of stability, there are no dramatic differ-
ences between the decoder runs. Indeed, the small
differences that exist are hardly discernible in the
plots. The model scores at the end of the decod-
ing run range between ?158767.9 and ?158716.9,
a relative difference of only about 0.03 %. Final
BLEU scores range from 21.41 % to 21.63 %, an in-
terval that is not negligible, but comparable to the
variance observed when, e. g., feature weights from
repeated MERT runs are used with one and the same
SMT system. Note that these results were obtained
with random state initialisation. With DP initialisa-
tion, score differences between repeated runs rarely
exceed 0.02 absolute BLEU percentage points.
Overall, we conclude that the decoding results of
our algorithm are reasonably stable despite the non-
determinism inherent in the procedure. In our sub-
sequent experiments, the evaluation scores reported
are calculated as the mean of three runs for each ex-
periment.
4.2 Search Algorithm Parameters
The hill climbing algorithm we use has two param-
eters which govern the trade-off between decoding
time and the accuracy with which a local maximum
is identified: The step limit stops the search pro-
cess after a certain number of steps regardless of the
search progress made or lack thereof. The rejection
limit stops the search after a certain number of un-
successful attempts to make a step, when continued
search does not seem to be promising. In most of our
experiments, we used a step limit of 227 ? 1.3 ? 108
and a rejection limit of 105. In practice, decoding
terminates by reaching the rejection limit for the vast
majority of documents. We therefore examined the
effect of different rejection limits on the learning
curves. The results are shown in figure 2.
The results show that continued search does pay
off to a certain extent. Indeed, the curve for re-
jection limit 107 seems to indicate that the model
score increases roughly logarithmically, albeit to a
higher base, even after the curve has started to flat-
ten out at 214 steps. At a certain point, however, the
probability of finding a good successor state drops
rather sharply by about two orders of magnitude, as
1185
Figure 2: Search performance at different rejection limits
evidenced by the fact that a rejection limit of 106
does not give a large improvement over one of 105,
while one of 107 does. The continued model score
improvement also results in an increase in BLEU
scores, and with a BLEU score of 22.1 % the system
with rejection limit 107 is fairly close to the score of
22.45 % obtained by DP beam search.
Obviously, more exact search comes at a cost, and
in this case, it comes at a considerable cost, which is
an explosion of the time required to decode the test
set from 4 minutes at rejection limit 103 to 224 min-
utes at rejection limit 105 and 38 hours 45 minutes
at limit 107. The DP decoder takes 31 minutes for
the same task. We conclude that the rejection limit
of 105 selected for our experiments, while techni-
cally suboptimal, realises a good trade-off between
decoding time and accuracy.
4.3 A Semantic Document Language Model
In this section, we present the results of the applica-
tion of our decoder to an actual SMT model with
cross-sentence features. Our model addresses the
problem of lexical cohesion. In particular, it rewards
the use of semantically related words in the trans-
lation output by the decoder, where semantic dis-
tance is measured with a word space model based
on Latent Semantic Analysis (LSA). LSA has been
applied to semantic language modelling in previous
research with some success (Coccaro and Jurafsky,
1998; Bellegarda, 2000; Wandmacher and Antoine,
2007). In SMT, it has mostly been used for domain
adaptation (Kim and Khudanpur, 2004; Tam et al
2007), or to measure sentence similarities (Banchs
and Costa-jussa`, 2011).
The model we use is inspired by Bellegarda
(2000). It is a Markov model, similar to a stan-
dard n-gram model, and assigns to each content
word a score given a history of n preceding content
words, where n = 30 below. Scoring relies on a 30-
dimensional LSA word vector space trained with the
S-Space software (Jurgens and Stevens, 2010). The
score is defined based on the cosine similarity be-
tween the word vector of the predicted word and the
mean word vector of the words in the history, which
is converted to a probability by histogram lookup
as suggested by Bellegarda (2000). The model is
structurally different from a regular n-gram model
in that word vector n-grams are defined over content
words occurring in the word vector model only and
can cross sentence boundaries. Stop words, identi-
fied by an extensive stop word list and amounting to
around 60 % of the tokens, are scored by a different
mechanism based on their relative frequency (undis-
counted unigram probability) in the training corpus.
In sum, the score produced by the semantic docu-
ment LM has the following form:
h(w|h)=
?
??
??
punigr(w) if w is a stop word, else
? pcos(w|h) if w is known, else
? if w is unknown,
(15)
where ? is the proportion of content words in the
training corpus and ? is a small fixed probability.
It is integrated into the decoder as an extra feature
function. Since we lack an automatic method for
1186
training the feature weights of document-wide fea-
tures, its weight was selected by grid search over
a number of values, comparing translation perfor-
mance for the newstest2009 test set.
In these experiments, we used DP beam search
to initialise the state of our local search decoder.
Three results are presented (table 1): The first table
row shows the baseline performance using DP beam
search with standard sentence-local features only.
The scores in the second row were obtained by run-
ning the hill climbing decoder with DP initialisation,
but without adding any models. A marginal increase
in scores for all three test sets demonstrates that the
hill climbing decoder manages to fix some of the
search errors made by the DP search. The last row
contains the scores obtained by adding in the seman-
tic language model. Scores are presented for three
publicly available test sets from recent WMT Ma-
chine Translation shared tasks, of which one (news-
test2009) was used to monitor progress during de-
velopment and select the final model.
Adding the semantic language model results in a
small increase in NIST scores (Doddington, 2002)
for all three test sets as well as a small BLEU score
gain (Papineni et al 2002) for two out of three cor-
pora. We note that the NIST score turned out to re-
act more sensitively to improvements due to the se-
mantic LM in all our experiments, which is reason-
able because the model specifically targets content
words, which benefit from the information weight-
ing done by the NIST score. While the results
we present do not constitute compelling evidence
in favour of our semantic LM in its current form,
they do suggest that this model could be improved
to realise higher gains from cross-sentence seman-
tic information. They support our claim that cross-
sentence models should be examined more closely
and that existing methods should be adapted to deal
with them, a problem addressed by our main contri-
bution, the local search document decoder.
5 Related Work
Even though DP beam search (Koehn et al 2003)
has been the dominant approach to SMT decoding
in recent years, methods based on local search have
been explored at various times. For word-based
SMT, greedy hill-climbing techniques were advo-
cated as a faster replacement for beam search (Ger-
mann et al 2001; Germann, 2003; Germann et al
2004), and a problem formulation specifically tar-
geting word reordering with an efficient word re-
ordering algorithm has been proposed (Eisner and
Tromble, 2006).
A local search decoder has been advanced as a
faster alternative to beam search also for phrase-
based SMT (Langlais et al 2007; Langlais et al
2008). That work anticipates many of the features
found in our decoder, including the use of local
search to refine an initial hypothesis produced by
DP beam search. The possibility of using models
that do not fit well into the beam search paradigm is
mentioned and illustrated with the example of a re-
versed n-gram language model, which the authors
claim would be difficult to implement in a beam
search decoder. Similarly to the work by Germann
et al(2001), their decoder is deterministic and ex-
plores the entire neighbourhood of a state in order
to identify the most promising step. Our main con-
tribution with respect to the work by Langlais et al
(2007) is the introduction of the possibility of han-
dling document-level models by lifting the assump-
tion of sentence independence. As a consequence,
enumerating the entire neighbourhood becomes too
expensive, which is why we resort to a ?first-choice?
strategy that non-deterministically generates states
and accepts the first one encountered that meets the
acceptance criterion.
More recently, Gibbs sampling was proposed as
a way to generate samples from the posterior distri-
bution of a phrase-based SMT decoder (Arun et al
2009; Arun et al 2010), a process that resembles
local search in its use of a set of state-modifying
operators to generate a sequence of decoder states.
Where local search seeks for the best state attainable
from a given initial state, Gibbs sampling produces
a representative sample from the posterior. Like all
work on SMT decoding that we know of, the Gibbs
sampler presented by Arun et al(2010) assumes in-
dependence of sentences and considers the complete
neighbourhood of each state before taking a sample.
6 Conclusion
In the last twenty years of SMT research, there has
been a strong assumption that sentences in a text
1187
newstest2009 newstest2010 newstest2011
BLEU NIST BLEU NIST BLEU NIST
DP search only 22.56 6.513 27.27 7.034 24.94 7.170
DP + hill climbing 22.60 6.518 27.33 7.046 24.97 7.169
with semantic LM 22.71 6.549 27.53 7.087 24.90 7.199
Table 1: Experimental results with a cross-sentence semantic language model
are independent of one another, and discourse con-
text has been largely neglected. Several factors have
contributed to this. Developing good discourse-level
models is difficult, and considering the modest trans-
lation quality that has long been achieved by SMT,
there have been more pressing problems to solve and
lower hanging fruit to pick. However, we argue that
the popular DP beam search algorithm, which deliv-
ers excellent decoding performance, but imposes a
particular kind of local dependency structure on the
feature models, has also had its share in driving re-
searchers away from discourse-level problems.
In this paper, we have presented a decoding pro-
cedure for phrase-based SMT that makes it possi-
ble to define feature models with cross-sentence de-
pendencies. Our algorithm can be combined with
DP beam search to leverage the quality of the tradi-
tional approach with increased flexibility for models
at the discourse level. We have presented prelimi-
nary results on a cross-sentence semantic language
model addressing the problem of lexical cohesion to
demonstrate that this kind of models is worth explor-
ing further. Besides lexical cohesion, cross-sentence
models are relevant for other linguistic phenomena
such as pronominal anaphora or verb tense selection.
We believe that SMT research has reached a point of
maturity where discourse phenomena should not be
ignored any longer, and we consider our decoder to
be a step towards this goal.
References
Abhishek Arun, Chris Dyer, Barry Haddow, Phil Blun-
som, Adam Lopez, and Philipp Koehn. 2009. Monte
carlo inference and maximization for phrase-based
translation. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), pages 102?110, Boulder, Colorado,
June. Association for Computational Linguistics.
Abhishek Arun, Barry Haddow, Philipp Koehn, Adam
Lopez, Chris Dyer, and Phil Blunsom. 2010. Monte
Carlo techniques for phrase-based translation. Ma-
chine translation, 24(2):103?121.
Rafael E. Banchs and Marta R. Costa-jussa`. 2011. A se-
mantic feature for Statistical Machine Translation. In
Proceedings of Fifth Workshop on Syntax, Semantics
and Structure in Statistical Translation, pages 126?
134, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Jerome R. Bellegarda. 2000. Exploiting latent semantic
information in statistical language modeling. Proceed-
ings of the IEEE, 88(8):1279?1296.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 Work-
shop on Statistical Machine Translation. In Proceed-
ings of the Sixth Workshop on Statistical Machine
Translation, pages 22?64, Edinburgh, Scotland, July.
Association for Computational Linguistics.
Noah Coccaro and Daniel Jurafsky. 1998. Towards bet-
ter integration of semantic predictors in statistical lan-
guage modeling. In Proceedings of the 5th Interna-
tional Conference on Spoken Language Processing,
Sydney.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of the second Interna-
tional conference on Human Language Technology
Research, pages 138?145, San Diego.
Jason Eisner and Roy W. Tromble. 2006. Local search
with very large-scale neighborhoods for optimal per-
mutations in machine translation. In Proceedings of
the HLT-NAACL Workshop on Computationally Hard
Problems and Joint Inference in Speech and Language
Processing, pages 57?75.
Marcello Federico, Nicola Bertoldi, and Mauro Cettolo.
2008. IRSTLM: an open source toolkit for handling
large scale language models. In Interspeech 2008,
pages 1618?1621. ISCA.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast decoding and
optimal decoding for machine translation. In Proceed-
ings of 39th Annual Meeting of the Association for
Computational Linguistics, pages 228?235, Toulouse,
France, July. Association for Computational Linguis-
tics.
1188
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2004. Fast and optimal de-
coding for machine translation. Artificial Intelligence,
154(1?2):127?143.
Ulrich Germann. 2003. Greedy decoding for Statis-
tical Machine Translation in almost linear time. In
Proceedings of the 2003 Human Language Technol-
ogy Conference of the North American Chapter of the
Association for Computational Linguistics.
Zhengxian Gong, Min Zhang, and Guodong Zhou.
2011. Cache-based document-level Statistical Ma-
chine Translation. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 909?919, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Christian Hardmeier and Marcello Federico. 2010. Mod-
elling Pronominal Anaphora in Statistical Machine
Translation. In Proceedings of the seventh Inter-
national Workshop on Spoken Language Translation
(IWSLT), pages 283?289.
Basil Hatim and Ian Mason. 1990. Discourse and the
Translator. Language in Social Life Series. Longman,
London.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving translation quality by dis-
carding most of the phrasetable. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 967?
975, Prague, Czech Republic, June. Association for
Computational Linguistics.
David Jurgens and Keith Stevens. 2010. The S-Space
package: An open source package for word space
models. In Proceedings of the ACL 2010 System
Demonstrations, pages 30?35, Uppsala, Sweden, July.
Association for Computational Linguistics.
Woosung Kim and Sanjeev Khudanpur. 2004. Cross-
lingual latent semantic analysis for language model-
ing. In IEEE international conference on acoustics,
speech, and signal processing (ICASSP), volume 1,
pages 257?260, Montre?al.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 conference of the North Ameri-
can chapter of the Association for Computational Lin-
guistics on Human Language Technology, pages 48?
54, Edmonton.
Philipp Koehn, Hieu Hoang, Alexandra Birch, et al
2007. Moses: open source toolkit for Statistical Ma-
chine Translation. In Annual meeting of the Associ-
ation for Computational Linguistics: Demonstration
session, pages 177?180, Prague.
Philippe Langlais, Alexandre Patry, and Fabrizio Gotti.
2007. A greedy decoder for phrase-based statistical
machine translation. In TMI-2007: Proceedings of
the 11th International Conference on Theoretical and
Methodological Issues in Machine Translation, pages
104?113, Sko?vde.
Philippe Langlais, Alexandre Patry, and Fabrizio Gotti.
2008. Recherche locale pour la traduction statistique
par segments. In TALN 2008, pages 119?128, Avi-
gnon, France, June. ATALA.
Ronan Le Nagard and Philipp Koehn. 2010. Aiding pro-
noun translation with co-reference resolution. In Pro-
ceedings of the Joint Fifth Workshop on Statistical Ma-
chine Translation and MetricsMATR, pages 252?261,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Franz Josef Och, Nicola Ueffing, and Hermann Ney.
2001. An efficient A* search algorithm for Statisti-
cal Machine Translation. In Proceedings of the Data-
Driven Machine Translation Workshop, 39th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 55?62, Toulouse.
Franz Josef Och. 2003. Minimum error rate training in
Statistical Machine Translation. In Proceedings of the
41st annual meeting of the Association for Computa-
tional Linguistics, pages 160?167, Sapporo (Japan).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of Machine Translation. In Proceedings of the
40th annual meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia. ACL.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007.
Bilingual LSA-based adaptation for Statistical Ma-
chine Translation. Machine Translation, 21(4):187?
207.
Jo?rg Tiedemann. 2010. To cache or not to cache? Ex-
periments with adaptive models in Statistical Machine
Translation. In Proceedings of the ACL 2010 Joint
Fifth Workshop on Statistical Machine Translation and
Metrics MATR, pages 189?194, Uppsala, Sweden. As-
sociation for Computational Linguistics.
Christoph Tillmann and Hermann Ney. 2003. Word re-
ordering and a Dynamic Programming beam search al-
gorithm for Statistical Machine Translation. Compu-
tational linguistics, 29(1):97?133.
Christoph Tillmann, Stephan Vogel, Hermann Ney, and
Alex Zubiaga. 1997. A DP-based search using mono-
tone alignments in Statistical Translation. In Proceed-
ings of the 35th Annual Meeting of the Association for
1189
Computational Linguistics, pages 289?296, Madrid,
Spain, July. Association for Computational Linguis-
tics.
Tonio Wandmacher and Jean-Yves Antoine. 2007.
Methods to integrate a language model with seman-
tic information for a word prediction component. In
Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 506?513, Prague, Czech Republic,
June. Association for Computational Linguistics.
1190
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1455?1465, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
A Transition-Based System for Joint Part-of-Speech Tagging
and Labeled Non-Projective Dependency Parsing
Bernd Bohnet
Institute for Natural Language Processing
University Stuttgart
bohnet@ims.uni-stuttgart.de
Joakim Nivre
Department of Linguistics and Philology
Uppsala University
joakim.nivre@lingfil.uu.se
Abstract
Most current dependency parsers presuppose
that input words have been morphologically
disambiguated using a part-of-speech tagger
before parsing begins. We present a transition-
based system for joint part-of-speech tagging
and labeled dependency parsing with non-
projective trees. Experimental evaluation on
Chinese, Czech, English and German shows
consistent improvements in both tagging and
parsing accuracy when compared to a pipeline
system, which lead to improved state-of-the-
art results for all languages.
1 Introduction
Dependency-based syntactic parsing has been the
focus of intense research efforts during the last
decade, and the state of the art today is represent-
ed by globally normalized discriminative models
that are induced using structured learning. Graph-
based models parameterize the parsing problem by
the structure of the dependency graph and normally
use dynamic programming for inference (McDonald
et al 2005; McDonald and Pereira, 2006; Carreras,
2007; Koo and Collins, 2010; Bohnet, 2010), but
other inference methods have been explored espe-
cially for non-projective parsing (Riedel and Clarke,
2006; Smith and Eisner, 2008; Martins et al 2009;
Martins et al 2010; Koo et al 2010). Transition-
based models parameterize the problem by elemen-
tary parsing actions and typically use incremental
beam search (Titov and Henderson, 2007; Zhang
and Clark, 2008; Zhang and Clark, 2011). Despite
notable differences in model structure, graph-based
and transition-based parsers both give state-of-the-
art accuracy with proper feature selection and opti-
mization (Koo and Collins, 2010; Zhang and Nivre,
2011; Bohnet, 2011).
It is noteworthy, however, that almost all depen-
dency parsers presuppose that the words of an input
sentence have been morphologically disambiguated
using (at least) a part-of-speech tagger. This is in s-
tark contrast to the best parsers based on PCFG mod-
els, such as the Brown parser (Charniak and John-
son, 2005) and the Berkeley parser (Petrov et al
2006; Petrov and Klein, 2007), which not only can
perform their own part-of-speech tagging but nor-
mally give better parsing accuracy when they are al-
lowed to do so. This suggests that joint models for
tagging and parsing might improve accuracy also in
the case of dependency parsing.
It has been argued that joint morphological and
syntactic disambiguation is especially important for
richly inflected languages, where there is consid-
erable interaction between morphology and syntax
such that neither can be fully disambiguated with-
out considering the other. Thus, Lee et al(2011)
show that a discriminative model for joint morpho-
logical disambiguation and dependency parsing out-
performs a pipeline model in experiments on Latin,
Ancient Greek, Czech and Hungarian. However, Li
et al(2011) and Hatori et al(2011) report improve-
ments with a joint model also for Chinese, which
is not a richly inflected language but is nevertheless
rich in part-of-speech ambiguities.
In this paper, we present a transition-based mod-
el for joint part-of-speech tagging and labeled de-
pendency parsing with non-projective trees. Exper-
1455
iments show that joint modeling improves both tag-
ging and parsing accuracy, leading to state-of-the-art
accuracy for richly inflected languages like Czech
and German as well as more configurational lan-
guages like Chinese and English. To our knowledge,
this is the first joint system that performs labeled de-
pendency parsing. It is also the first joint system that
achieves state-of-the-art accuracy for non-projective
dependency parsing.
2 Transition-Based Tagging and Parsing
Transition-based dependency parsing was pioneered
by Yamada and Matsumoto (2003) and Nivre et al
(2004), who used classifiers trained to predict indi-
vidual actions of a deterministic shift-reduce parser.
Recent research has shown that better accuracy can
be achieved by using beam search and optimizing
models on the entire sequence of decisions needed
to parse a sentence instead of single actions (Zhang
and Clark, 2008; Huang and Sagae, 2010; Zhang
and Clark, 2011; Zhang and Nivre, 2011; Bohnet,
2011). In addition, a number of different transition
systems have been proposed, in particular for deal-
ing with non-projective dependencies, which were
beyond the scope of early systems (Attardi, 2006;
Nivre, 2007; Nivre, 2009; Titov et al 2009).
In this section, we start by defining a transition
system for joint tagging and parsing based on the
non-projective transition system proposed in Nivre
(2009). We then show how to perform beam search
and structured online learning with this model, and
conclude by discussing feature representations.
2.1 Transition System
Given a set P of part-of-speech tags and a set D
of dependency labels, a tagged dependency tree for
a sentence x = w1, . . . , wn is a directed tree T =
(Vx, A) with labeling functions pi and ? such that:
1. Vx = {0, 1, . . . , n} is a set of nodes,
2. A ? Vx ? Vx is a set of arcs,
3. pi : Vx ? P is a labeling function for nodes,
4. ? : A? D is a labeling function for arcs,
5. 0 is the root of the tree.
The set Vx of nodes is the set of positive integers up
to and including n, each corresponding to the lin-
ear position of a word in the sentence, plus an extra
artificial root node 0. The set A of arcs is a set of
pairs (i, j), where i is the head node and j is the
dependent node. The functions pi and ? assign a u-
nique part-of-speech label to each node/word and a
unique dependency label to each arc, respectively.
This notion of dependency tree differs from the s-
tandard definition only by including part-of-speech
labels as well as dependency labels (Ku?bler et al
2009).
Following Nivre (2008), we define a transition
system for dependency parsing as a quadruple S =
(C, T, cs, Ct), where
1. C is a set of configurations,
2. T is a set of transitions, each of which is a (par-
tial) function t : C ? C,
3. cs is an initialization function, mapping a sen-
tence x to a configuration c ? C,
4. Ct ? C is a set of terminal configurations.
A transition sequence for a sentence x in S is a
sequence of configuration-transition pairs C0,m =
[(c0, t0), (c1, t1), . . . , (cm, tm)] where c0 = cs(x),
tm(cm) ? Ct and ti(ci) = ci+1 (0 ? i < m).1
In this paper, we take the set C of configurations
to be the set of all 5-tuples c = (?, B,A, pi, ?) such
that ? (the stack) and B (the buffer) are disjoin-
t sublists of the nodes Vx of some sentence x, A
is a set of dependency arcs over Vx, and pi and ?
are labeling functions as defined above. We take the
initial configuration for a sentence x = w1, . . . , wn
to be cs(x) = ([0], [1, . . . , n], { },?,?), where ?
is the function that is undefined for all arguments,
and we take the set Ct of terminal configurations
to be the set of all configurations of the form c =
([0], [ ], A, pi, ?) (for anyA, pi and ?). The tagged de-
pendency tree defined for x by c = (?, B,A, pi, ?)
is the tree (Vx, A) with labeling functions pi and ?,
which we write TREE(x, c).
The set T of transitions is shown in Figure 1. The
LEFT-ARCd and RIGHT-ARCd transitions both add
an arc (with dependency label d) between the two
nodes on top of the stack and replaces these nodes
by the head node of the new arc (which is the right-
most node for LEFT-ARCd and the leftmost node for
RIGHT-ARCd). The SHIFTp transition extracts the
1This definition of transition sequence differs from that of
Nivre (2008) but is equivalent and suits our presentation better.
1456
Transition Condition
LEFT-ARCd ([?|i, j], B,A, pi, ?)? ([?|j], B,A?{(j, i)}, pi, ?[(j, i)? d]) i 6= 0
RIGHT-ARCd ([?|i, j], B,A, pi, ?)? ([?|i], B,A?{(i, j)}, pi, ?[(i, j)? d])
SHIFTp (?, [i|?], A, pi, ?)? ([?|i], ?, A, pi[i? p], ?)
SWAP ([?|i, j], ?, A, pi, ?)? ([?|j], [i|?], A, pi, ?) 0 < i < j
Figure 1: Transitions for joint tagging and dependency parsing extending the system of Nivre (2009). The stack ? is
represented as a list with its head to the right (and tail ?) and the buffer B as a list with its head to the left (and tail ?).
The notation f [a? b] is used to denote the function that is exactly like f except that it maps a to b.
first node in the buffer, pushes it onto the stack and
labels it with the part-of-speech tag p. The SWAP
transition extracts the second topmost node from the
stack and moves it back to the buffer, subject to the
condition that the two top nodes on the stack are still
in the order given by the sentence.
Except for the addition of a tag parameter p to
the SHIFT transition, this is equivalent to the sys-
tem described in Nivre (2009), which thanks to the
SWAP transition can handle arbitrary non-projective
trees. The soundness and completeness results giv-
en in that paper trivially carry over to the new sys-
tem. The only thing to note is that, before a terminal
configuration can be reached, every word has to be
pushed onto the stack in a SHIFTp transition, which
ensures that every node/word in the output tree will
be tagged.
2.2 Inference and Learning
While early transition-based parsers generally used
greedy best-first inference and locally trained clas-
sifiers, recent work has shown that higher accura-
cy can be obtained using beam search and global
structure learning to mitigate error propagation. In
particular, it seems that the globally learned models
can exploit a much richer feature space than local-
ly trained classifiers, as shown by Zhang and Nivre
(2011). Since joint tagging and parsing increases the
size of the search space and is likely to require nov-
el features, we use beam search in combination with
structured perceptron learning.
The beam search algorithm used to derive the best
parse y for a sentence x is outlined in Figure 2. In
addition to the sentence x, it takes as input a weight
vector w corresponding to a linear model for scor-
ing transitions out of configurations and two prun-
PARSE(x,w, b1, b2)
1 h0.c? cs(x)
2 h0.s? 0.0
3 h0.f? {0.0}dim(w)
4 BEAM ? [h0]
5 while ?h ? BEAM : h.c 6? Ct
6 TMP ? [ ]
7 foreach h ? BEAM
8 foreach t ? T : PERMISSIBLE(h.c, t)
9 h.f? h.f + f(h.c, t)
10 h.s? h.s+ f(h.c, t) ? w
11 h.c? t(h.c)
12 TMP ? INSERT(h, TMP)
13 BEAM? PRUNE(TMP, b1, b2)
14 h? TOP(BEAM)
15 y ? TREE(x, h.c)
16 return y
Figure 2: Beam search algorithm for joint tagging and de-
pendency parsing of input sentence x with weight vector
w and beam parameters b1 and b2. The symbols h.c, h.s
and h.f denote, respectively, the configuration, score and
feature representation of a hypothesis h; h.c.A denotes
the arc set of h.c.
ing parameters b1 and b2. A parse hypothesis h is
represented by a configuration h.c, a score h.s and
a feature vector h.f for the transition sequence up to
h.c. Hypotheses are stored in the list BEAM, which
is sorted by descending scores and initialized to hold
the hypothesis h0 corresponding to the initial con-
figuration cs(x) with score 0.0 and all features set
to 0.0 (lines 1?4). In the main loop (lines 5?13), a
set of new hypotheses is derived and stored in the
list TMP, which is finally pruned and assigned as
the new value of BEAM. The main loop terminates
1457
when all hypotheses in BEAM contain terminal con-
figurations, and the dependency tree extracted from
the top scoring hypothesis is returned (lines 14?16).
The set of new hypotheses is created in two nest-
ed loops (lines 7?12), where every hypothesis h in
BEAM is updated using every permissible transition
t for the configuration h.c. The feature representa-
tion of the new hypothesis is obtained by adding the
feature vector f(t, h.c) for the current configuration-
transition pair to the feature vector of the old hy-
pothesis (line 9). Similarly, the score of the new
hypothesis is the sum of the score f(t, h.c) ? w of
the current configuration-transition pair and the s-
core of the old hypothesis (line 10). The feature
representation/score of a complete parse y for x
with transition sequence C0,m is thus the sum of the
feature representations/scores of the configuration-
transition pairs in C0,m:
f(x, y) =
?
(c,t)?C0,m
f(c, t)
s(x, y) =
?
(c,t)?C0,m
f(c, t) ? w
Finally, the configuration of the new hypothesis is
obtained by evaluating t(h.c) (line 11). The new hy-
pothesis is then inserted into TMP in score-sorted or-
der (line 12).
The pruning parameters b1 and b2 determine the
number of hypotheses allowed in the beam and at
the same time control the tradeoff between syntactic
and morphological ambiguity. First, we extract the
b1 highest scoring hypotheses with distinct depen-
dency trees. Then we extract the b2 highest scoring
remaining hypotheses, which will typically be tag-
ging variants of dependency trees that are already in
the beam. In this way, we prevent the beam from
getting filled up with too many tagging variants of
the same dependency tree, which was found to be
harmful in preliminary experiments.
One final thing to note about the inference algo-
rithm is that the notion of permissibility for a transi-
tion t out of a configuration c can be used to capture
not only formal constraints on transitions ? such as
the fact that it is impossible to perform a SHIFTp
transition with an empty buffer or illegal to perform
a LEFT-ARCd transition with the special root node
on top of the stack ? but also to filter out unlike-
ly dependency labels or tags. Thus, in the experi-
ments later on, we will typically constrain the parser
so that SHIFTp is permissible only if p is one of the
k best part-of-speech tags with a score no more than
? below the score of the 1-best tag, as determined by
a preprocessing tagger. We also filter out instances
of LEFT-ARCd and RIGHT-ARCd, where d does not
occur in the training data for the predicted part-of-
speech tag combination of the head and dependent.
This procedure leads to a significant speed up.
In order to learn a weight vector w from a training
set {(xj , yj)}Tj=1 of sentences with their tagged de-
pendency trees, we use a variant of the structured
perceptron, introduced by Collins (2002), which
makes N iterations over the training data and up-
dates the weight vector for every sentence xj where
the highest scoring parse y? is different from yj .
More precisely, we use the passive-aggressive up-
date of Crammer et al(2006):
wi+1 = wi + ?(f(xj , yj)? f(xj , y?))
where
? =
f(xj , yj)? f(xj , y?)
||f(xj , yj)? f(xj , y?)||2
We also use the early update strategy found benefi-
cial for parsing in several previous studies (Collins
and Roark, 2004; Zhang and Clark, 2008; Huang
and Sagae, 2010), which means that, during learn-
ing, we terminate the beam search as soon as the
hypothesis corresponding to the gold parse yj falls
out of the beam and update with respect to the par-
tial transition sequence constructed up to that point.
Finally, we use the standard technique of averaging
over all weight vectors, as originally proposed by
Collins (2002).
2.3 Feature Representations
As already noted, the feature representation f(x, y)
of an input sentence x with parse y decomposes into
feature representations f(c, t) for the transitions t(c)
needed to derive y from cs(x). Features may refer to
any aspect of a configuration, as encoded in the stack
?, the bufferB, the arc setA and the labelings pi and
?. In addition, we assume that each word w in the
input is assigned up to k candidate part-of-speech
tags pii(w) with corresponding scores s(pii(w)).
1458
Features involving word prefixes and suffixes
pii(B0)p2(B0), pii(B0)s2(B0), pii(B0)p1(B0)p1(?0)
pii(?0)p1(?0)p1(?1), pii(?0)s1(?0)s1(?0)
pii(?0)p2(?0)s3(?1),pii(?0)s3(?0)p2(?1)
pii(?0)w(B0)s1(?0), pii(?0)w(B0)s2(?0)
Features involving tag score differences and ranks
pii(B0)[s(pi1(B0))? s(pii(B0))]
pii(B0)pii(?0)[s(pi1(B0))? s(pii(B0))] i
pii(B0)[s(pi1(B0))? s(pii(B0))]pi(?0)
w(B0)[s(pi1(B0))? s(pii(B0))]pi(?0)
Figure 3: Specialized feature templates for tagging. We
use ?i and Bi to denote the ith token in the stack ? and
bufferB, respectively, with indexing starting at 0, and we
use the following functors to extract properties of a token:
pii() = ith best tag; s(pii()) = score of ith best tag; pi() =
finally predicted tag; w() = word form; pi() = word prefix
of i characters; si() = word suffix of i characters. Score
differences are binned in discrete steps of 0.05.
The bulk of features used in our system are tak-
en from Zhang and Nivre (2011), although with t-
wo important differences. First of all, like Hatori et
al. (2011), we have omitted all features that presup-
pose an arc-eager parsing order, since our transition
system defines an arc-standard order. Secondly, any
feature that refers to the part-of-speech tag of a word
w in the buffer B will in our system refer to the top-
scoring tag pi1(w), rather than the finally predicted
tag. By contrast, for a word in the stack ?, part-of-
speech features refer to the tag pi(w) chosen when
shifting w onto the stack (which may or may not be
the same as pi1(w)).
In addition to the standard features for transition-
based dependency parsing, we have added features
specifically to improve the tagging step in the joint
model. The templates for these features, which are
specified in Figure 3, all involve the ith best tag as-
signed to the first word of the buffer B (the next
word to be shifted in a SHIFTp transition) in combi-
nation with neighboring words, word prefixes, word
suffixes, score differences and tag rank.
Finally, in some experiments, we make use of two
additional feature sets, which we call graph features
(G) and cluster features (C), respectively. Graph fea-
tures are defined over the factors of a graph-based
dependency parser, which was shown to improve the
accuracy of a transition-based parser by Zhang and
Clark (2008). However, while their features were
limited to certain first- and second-order factors, we
use features over second- and third-order factors as
found in the parsers of Bohnet and Kuhn (2012).
These features are scored as soon as the factors are
completed, using a technique that is similar to what
Hatori et al(2011) call delayed features, although
they use it for part-of-speech tags in the lookahead
while we use it for subgraphs of the dependency tree.
Cluster features, finally, are features over word clus-
ters, as first used by Koo et al(2008), which replace
part-of-speech tag features.2
We use a hash kernel to map features to weights.
It has been observed that most of the computing time
in feature-rich parsers is spent retrieving the index
of each feature in the weight vector (Bohnet, 2010).
This is usually done via a hash table, but significan-
t speedups can be achieved by using a hash kernel,
which simply replaces table lookup by a hash func-
tion (Bloom, 1970; Shi et al 2009; Bohnet, 2010).
The price to pay for these speedups is that there may
be collisions, so that different features are mapped to
the same index, but this is often compensated by the
fact that the lower time and memory requirements of
the hash kernel enables the use of negative features,
that is, features that are never seen in the training set
but occur in erroneous hypotheses at training time
and can therefore be helpful also at inference time.
As a result, the hash kernel often improves accuracy
as well as efficiency compared to traditional tech-
niques that only make use of features that occur in
gold standard parses (Bohnet, 2010).
3 Experiments
We have evaluated the model for joint tagging and
dependency parsing on four typologically diverse
languages: Chinese, Czech, English, and German.
3.1 Setup
Most of the experiments use the CoNLL 2009 da-
ta sets with the training, development and test s-
plit used in the Shared Task (Hajic? et al 2009),
but for better comparison with previous work we
also report results for the standard benchmark data
sets for Chinese and English. For Chinese, this is
the Penn Chinese Treebank 5.1 (CTB5), converted
2For replicability, a complete description of all features can
be found at http://stp.lingfil.uu.se/?nivre/exp/emnlp12.html.
1459
Parser Chinese Czech English German
k ? TLAS LAS UAS POS TLAS LAS UAS POS TLAS LAS UAS POS TLAS LAS UAS POS
1 0.0 73.85 76.12 80.01 92.78 82.36 82.65 88.03 93.26 85.82 87.17 90.41 97.32 85.08 86.60 89.17 97.24
2 0.1 74.39 76.52 80.41 93.37 82.74 83.01 88.34 99.39 86.43 87.79 91.02 97.49 86.12 87.22 89.69 97.85
3 0.1 74.47 76.63 80.50 93.38 82.76 82.97 88.33 99.40 86.40 87.78 90.99 97.43 86.03 87.27 89.60 97.74
3 0.2 74.35 76.48 80.38 93.43 82.85 83.11 88.44 99.32 86.35 87.79 91.01 97.52 86.24 87.37 89.72 97.90
3 0.3 74.18 76.33 80.28 93.48 82.78 83.05 88.38 99.33 85.94 87.57 90.87 96.97 86.35 87.46 89.86 97.90
3 0.4 86.14 87.23 89.66 97.79
Table 1: Accuracy scores for the CoNLL 2009 shared task development sets as a function of the number of tags k and
the score threshold ?. Beam parameters fixed at b1 = 40, b2 = 4.
with the head-finding rules and conversion tools of
Zhang and Clark (2008), and with the same split as
in Zhang and Clark (2008) and Li et al(2011).3 For
English, this is the WSJ section of the Penn Tree-
bank, converted with the head-finding rules of Ya-
mada and Matsumoto (2003) and the labeling rules
of Nivre (2006).4
In order to assign k-best part-of-speech tags and
scores to words in the training set, we used a per-
ceptron tagger with 10-fold jack-knifing. The same
type of tagger was trained on the entire training set
in order to supply tags for the development and test
sets. The feature set of the tagger was optimized
for English and German and provides state-of-the-
art accuracy for these two languages. The 1-best
tagging accuracy for section 23 of the Penn Tree-
bank is 97.28, which is on a par with Toutanova et
al. (2003). For German, we obtain a tagging accura-
cy of 97.24, which is close to the 97.39 achieved by
the RF-Tagger (Schmid and Laws, 2008), which to
our knowledge is the best tagger for German.5 The
results are not directly comparable to the RF-Tagger
as it was evaluated on a different part of the Tiger
Treebank and trained on a larger part of the Tree-
bank. We could not use the larger training set as
it contains the test set of the CoNLL 2009 data that
we use to evaluate the joint model. For Czech, the 1-
best tagging accuracy is 99.11 and for Chinese 92.65
on the CoNLL 2009 test set.
We trained parsers with 25 iterations and report
3Training: 001?815, 1001?1136. Development: 886?931,
1148?1151. Test: 816?885, 1137?1147.
4Training: 02-21. Development: 24. Test: 23.
5The RF-Tagger can take advantage of an additional lexicon
and then reaches 97.97. The lexicon supplies entries for addi-
tional words that are not found in the training corpus and addi-
tional tags for words that do occur in the training data (Schmid
and Laws, 2008).
results for the model obtained after the last iteration.
For cluster features, available only for English and
German, we used standard Brown clusters based on
the English and German Gigaword Corpus. We re-
stricted the vocabulary to words that occur at least
10 times, used 800 clusters, and took cluster prefix-
es of length 6 to define features.
We report the following evaluation metrics: part-
of-speech accuracy (POS), unlabeled attachment s-
core (UAS), labeled attachment score (LAS), and
tagged labeled attachment score (TLAS). TLAS is
a new metric defined as the percentage of words that
are assigned the correct part-of-speech tag, the cor-
rect head and the correct dependency label. In line
with previous work, punctuation is included in the
evaluation for the CoNLL data sets but excluded for
the two benchmark data sets.
3.2 Results
Table 1 presents results on the development sets of
the CoNLL 2009 shared task with varying values
of the two tag parameters k (number of candidates)
and ? (maximum score difference to 1-best tag) and
beam parameters fixed at b1 = 40 and b2 = 4. We
use the combined TLAS score on the development
set to select the optimal settings for each language.
For Chinese, we obtain the best result with 3 tags
and a threshold of 0.1.6 Compared to the baseline,
we observe a POS improvement of 0.60 and a LAS
improvement of 0.51. For Czech, we get the best T-
LAS with k = 3 and ? = 0.2, where POS improves
by 0.06 and LAS by 0.46. For English, the best set-
ting is k = 2 and ? = 0.1 with a POS improvement of
0.17 and a LAS improvement of 0.62. For German,
finally, we see the greatest improvement with k = 3
6While tagging accuracy (POS) increases with larger values
of ?, TLAS decreases because of a drop in LAS.
1460
Parser Chinese Czech English German
TLAS LAS UAS POS TLAS LAS UAS POS TLAS LAS UAS POS TLAS LAS UAS POS
Gesmundo et al(2009) 76.11 92.37 80.38 99.33 88.79 97.48 87.28 95.46
Bohnet (2010) 76.99 92.37 80.96 99.33 90.33 97.48 88.06 95.46
Baseline (k = 1), b1 = 40 73.66 76.55 80.77 92.65 82.07 82.44 87.83 99.11 87.89 89.19 91.74 97.57 86.11 87.78 90.13 97.24
Best dev setting, b1 = 40 74.72 77.00 81.18 93.06 82.56 82.70 88.07 99.32 88.26 89.54 92.06 97.77 86.91 88.23 90.43 97.63
Adding G, b1 = 80 75.84 78.51 82.52 93.19 83.38 83.73 88.82 99.33 88.92 90.20 92.60 97.77 87.86 89.05 91.16 97.78
Adding G+C, b1 = 80 89.22 90.60 92.87 97.84 88.31 89.38 91.37 98.05
Table 2: Accuracy scores for the CoNLL 2009 shared task test sets. Rows 1?2: Top performing systems in the shared
CoNLL Shared Task 2009; Gesmundo et al(2009) was placed first in the shared task; for Bohnet (2010), we include
the updated scores later reported due to some improvements of the parser. Rows 3?4: Baseline (k = 1) and best settings
for k and ? on development set. Rows 5?6: Wider beam (b1 = 80) and added graph features (G) and cluster features
(C). Second beam parameter b2 fixed at 4 in all cases.
and ? = 0.3, where POS improves by 0.66 and LAS
by 0.86.
Table 2 shows the results on the CoNLL 2009 test
sets. For all languages except English, we obtain
state-of-the-art results already with b1 = 40 (row 4),
and for all languages both tagging and parsing ac-
curacy improve compared to the baseline (row 3).
The improvement in TLAS is statistically significant
with p < 0.01 for all languages (paired t-test). Row
5 shows the scores with a beam of 80 and the addi-
tional graph features. Here the LAS scores for Chi-
nese, Czech and German are higher than the best re-
sults on the CoNLL 2009 data sets, and the score
for English is highly competitive. For Chinese, we
achieve 78.51 LAS, which is 1.5 percentage points
higher than the reference score, while the POS s-
core is 0.54 higher than our baseline. For Czech, we
get 83.73 LAS, which is by far the highest score re-
ported for this data set, together with state-of-the-art
POS accuracy. For German, we obtain 89.05 LAS
and 97.78 POS, which in both cases is substantially
better than in the CoNLL shared task. We believe
it is also the highest POS accuracy ever reported for
a tagger/parser trained only on the Tiger Treebank.
Row 6, finally, presents results with added cluster
features for English and German, which results in
additional improvements in all metrics.
Table 3 gives the results for the Penn Treebank
converted with the head-finding rules of Yamada and
Matsumoto (2003) and the labeling rules of Nivre
(2006). We use k = 3 and ? = 0.4, which gave the
best results on the development set. The UAS im-
proves by 0.24 when we do joint tagging and pars-
ing. The POS accuracy improves slightly by 0.12
Parser TLAS UAS LAS POS
McDonald et al(2005) 90.9
McDonald and Pereira (2006) 91.5
Zhang and Clark (2008) 92.1
Huang and Sagae (2010) 92.1
Koo and Collins (2010) 93.04
Zhang and Nivre (2011) 92.9
Martins et al(2010) 93.26
Koo et al(2008) ? 93.16
Carreras et al(2008) ? 93.5
Suzuki et al(2009) ? 93.79
Baseline (k = 1), b1 = 40 89.42 92.79 91.71 97.28
Best dev setting, b1 = 40 89.75 93.03 91.92 97.40
Adding G, b1 = 40 90.12 93.38 92.44 97.33
Adding G+C, b1 = 80 ? 90.41 93.67 92.68 97.42
Table 3: Accuracy scores for WSJ-PTB converted with
head rules of Yamada and Matsumoto (2003) and labeling
rules of Nivre (2006). Best dev setting: k = 3, ? = 0.4.
Results marked with ? use additional information sources
and are not directly comparable to the others.
but to a lower degree than for the English CoNL-
L data where we observed an improvement of 0.20.
Nonetheless, the improvement in the joint TLAS s-
core is statistically significant at p < 0.01 (paired
t-test). Our joint tagger and dependency parser with
graph features gives very competitive unlabeled de-
pendency scores for English with 93.38 UAS. To
the best of our knowledge, this is the highest score
reported for a (transition-based) dependency parser
that does not use additional information sources. By
adding cluster features and widening the beam to
b1 = 80, we achieve 93.67 UAS. We also obtain
a POS accuracy of 97.42, which is on a par with the
best results obtained using semi-supervised taggers
1461
Parser TLAS UAS LAS POS
MSTParser1 75.56 93.51
MSTParser2 77.73 93.51
Li et al(2011) 3rd-order 80.60 92.80
Li et al(2011) 2nd-order 80.55 93.08
Hatori et al(2011) HS 79.60 94.01
Hatori et al(2011) ZN 81.20 93.94
Baseline (k = 1), b1 = 40 61.95 80.33 76.79 92.81
Best dev setting, b1 = 40 62.54 80.59 77.06 93.11
Adding G, b1 = 80 63.20 81.42 77.91 93.24
Table 4: Accuracy scores for Penn Chinese Treebank
converted with the head rules of Zhang and Clark (2008).
Best dev setting: k = 3, ? = 0.1. MSTParser results from
Li et al(2011). UAS scores from Li et al(2011) and Ha-
tori et al(2011) recalculated from the separate accuracy
scores for root words and non-root words reported in the
original papers.
(S?gaard, 2011).
Table 4 shows the results for the Chinese Penn
Treebank CTB 5.1 together with related work. In ex-
periments with the development set, we could con-
firm the results from the Chinese CoNLL data set
and obtained the best results with the same settings
(k = 3, ? = 0.1). With b1 = 40, UAS improves by
0.25 and POS by 0.30, and the TLAS improvement
is again highly significant (p < 0.01, paired t-test).
We get the highest UAS, 81.42, with a beam of 80
and added graph features, in which case POS accu-
racy increases from 92.81 to 93.24. Since our tagger
was not optimized for Chinese, we have lower base-
line results for the tagger than both Li et al(2011)
and Hatori et al(2011) but still manage to achieve
the highest reported UAS.
The speed of the joint tagger and dependency
parser is quite reasonable with about 0.4 seconds
per sentence on the WSJ-PTB test set, given that we
perform tagging and labeled parsing with a beam of
80 while incorporating the features of a third-order
graph-based model. Experiments were performed
on a computer with an Intel i7-3960X CPU (3.3 GHz
and 6 cores). These performance values are prelim-
inary since we are still working on the speed-up of
the parser.
3.3 Analysis
In order to better understand the benefits of the joint
model, we performed an error analysis for German
Confusion Baseline Joint
Freq F-score Freq F-score
VVINF? VVFIN 28
91.1
2
97.7
VVINF? VVPP|ADJ*|NN 5 9
VVFIN? VVINF 43
94.2
5
98.5
VVFIN? VVPP 20 2
VAINF? VAFIN 10 99.1 1 99.9
NE? NN 184
90.7
128
92.4NE? ADJ*|ADV|FM 24 18
NE? XY 12 21
NN? NE 85
97.5
67
98.1
NN? ADJ*|XY|ADV|VV* 39 29
PRELS? ART 13
92.9
5
95.4
PRELS? PWS 0 2
Table 5: Selected entries from the confusion matrix for
parts of speech in German with F-scores for the left-hand-
side category. ADJ* (ADJD or ADJA) = adjective; ADV
= adverb; ART = determiner; APPR = preposition; NE
= proper noun; NN = common noun; PRELS = relative
pronoun; VVFIN = finite verb; VVINF = non-finite verb;
VAFIN = finite auxiliary verb; VAINF = non-finite auxil-
iary verb; VVPP = participle; XY = not a word. We use
?* to denote the set of categories with ? as a prefix.
and English, where we compared the baseline and
the joint model with respect to F-scores for individu-
al part-of-speech categories and dependency labels.
For the part-of-speech categories, we found an im-
provement across the board for both languages, with
no category having a significant decrease in F-score,
but we also found some interesting patterns for cat-
egories that improved more than the average.
Table 5 shows selected entries from the confu-
sion matrix for German, where we see substantial
improvements for finite and non-finite verbs, which
are often morphologically ambiguous but which can
be disambiguated using syntactic context. We al-
so see improved accuracies for common and proper
nouns, which are both capitalized in standard Ger-
man orthography and therefore often mistagged, and
for relative pronouns, which are less often confused
for determiners in the joint model.
Table 6 gives a similar snapshot for English, and
we again see improvements for verb categories that
are often morphologically ambiguous, such as past
participles, which can be confused for past tense
verbs, and present tense verbs in third person sin-
gular, which can be confused for nouns. We also
see some improvement for the singular noun catego-
1462
Confusion Baseline Joint
Freq F-score Freq F-score
VBN? VBD 40
90.5
19
91.5
VBN? JJ|VB|VBP|NN 13 18
VBZ? NN|NNS 19
97.8
13
98.3
VBZ? POS|JJ|RB 6 6
NN? VBG|VB|VBN|VBD 72
96.8
58
97.2NN? JJ|JJR 79 69
NN? NN*|RB|IN|DT 58 57
RB? IN 126
92.4
93
92.9
RB? JJ*|RP|NN*|RBR|UH 86 89
Table 6: Selected entries from the confusion matrix for
parts of speech in English with F-scores for the left-hand-
side category. DT = determiner; IN = preposition or sub-
ordinating conjunction; JJ = adjective; JJR = compara-
tive adjective; NN = singular or mass noun; NNS = plural
noun; POS = possessive clitic; RB = adverb; RBR = com-
parative adverb; RP = particle; UH = interjection; VB =
base form verb; VBD = past tense verb; VBG = gerund or
present participle; VBN = past participle; VBP = present
tense verb, not 3rd person singular; VBZ = present tense
verb, 3rd person singular. We use ?* to denote the set of
categories with ? as a prefix.
ry and for adverbs, which are less often confused for
prepositions or subordinating conjunctions thanks to
the syntactic information in the joint model.
For dependency labels, it is hard to extract any
striking patterns and it seems that we mainly see an
improvement in overall parsing accuracy thanks to
less severe tagging errors. However, it is worth ob-
serving that, for both English and German, we see
significant F-score improvements for the core gram-
matical functions subject (91.3? 92.1 for German,
95.6 ? 96.1 for English) and object (86.9 ? 87.9
for German, 90.2? 91.9 for English).
4 Related Work
Our work is most closely related to Lee et al(2011),
Li et al(2011) and Hatori et al(2011), who al-
l present discriminative models for joint tagging and
dependency parsing. However, all three models only
perform unlabeled parsing, while our model incor-
porates dependency labels into the parsing process.
Whereas Lee et al(2011) and Li et al(2011) take
a graph-based approach to dependency parsing, Ha-
tori et al(2011) use a transition-based model similar
to ours but limited to projective dependency trees.
Both Li et al(2011) and Hatori et al(2011) only
evaluate their model on Chinese, and of these only
Hatori et al(2011) report consistent improvements
in both tagging and parsing accuracy. Like our sys-
tem, the parser of Lee et al(2011) can handle non-
projective trees and experimental results are present-
ed for four languages, but their graph-based model
is relatively simple and the baselines therefore well
below the state of the art. We are thus the first to
show consistent improvements in both tagging and
(labeled) parsing accuracy across typologically di-
verse languages at the state-of-the-art level. More-
over, the capacity to handle non-projective depen-
dencies, which is crucial to attain good performance
on Czech and German, does not seem to hurt per-
formance on English and Chinese, where the bench-
mark sets contain only projective trees.
The use of beam search in transition-based depen-
dency parsing in order to mitigate the problem of
error propagation was first proposed by Johansson
and Nugues (2006), although they still used a local-
ly trained model. Globally normalized models were
first explored by Titov and Henderson (2007), who
were also the first to use a parameterized SHIFT tran-
sition like the one found in both Hatori et al(2011)
and our own work, although Titov and Henderson
(2007) used it to define a generative model by pa-
rameterizing the SHIFT transition by an input word.
Zhang and Clark (2008) was the first to combine
beam search with a globally normalized discrimi-
native model, using structured perceptron learning
and the early update strategy of Collins and Roark
(2004), and also explored the addition of graph-
based features to a transition-based parser. This
approach was further pursued in Zhang and Clark
(2011) and was used by Zhang and Nivre (2011) to
achieve state-of-the-art results in dependency pars-
ing for both Chinese and English through the ad-
dition of rich non-local features. Huang and Sagae
(2010) combined structured perceptron learning and
beam search with the use of a graph-structured stack
to allow ambiguity packing in the beam, a technique
that was reused by Hatori et al(2011).
Finally, as noted in the introduction, although
joint tagging and parsing is rare in dependency pars-
ing, most state-of-the-art parsers based on PCFG
models naturally incorporate part-of-speech tagging
and usually achieve better parsing accuracy (albeit
not always tagging accuracy) with a joint model than
1463
with a pipeline approach (Collins, 1997; Charniak,
2000; Charniak and Johnson, 2005; Petrov et al
2006). Models that in addition incorporate mor-
phological analysis and segmentation have been ex-
plored by Tsarfaty (2006), Cohen and Smith (2007),
and Goldberg and Tsarfaty (2008) with special ref-
erence to Hebrew parsing.
5 Conclusion
We have presented the first system for joint part-
of-speech tagging and labeled dependency parsing
with non-projective dependency trees. Evaluation
on four languages shows consistent improvements
in both tagging and parsing accuracy over a pipeline
system with state-of-the-art results across the board.
The error analysis reveals improvements in tagging
accuracy for syntactically central categories, mainly
verbs, with improvement in syntactic accuracy for
core grammatical functions as a result. In future
work we intend to explore joint models that incorpo-
rate not only basic part-of-speech tags but also more
fine-grained morphological features.
References
Giuseppe Attardi. 2006. Experiments with a multilan-
guage non-projective dependency parser. In Proceed-
ings of CoNLL, pages 166?170.
Burton H. Bloom. 1970. Space/time trade-offs in hash
coding with allowable errors. Communications of the
ACM, 13:422?426.
Bernd Bohnet and Jonas Kuhn. 2012. The best of
both worlds ? a graph-based completion model for
transition-based parsers. In Proceedings of EACL,
pages 77?87.
Bernd Bohnet. 2010. Top accuracy and fast dependen-
cy parsing is not a contradiction. In Proceedings of
COLING, pages 89?97.
Bernd Bohnet. 2011. Comparing advanced graph-based
and transition-based dependency parsers. In Proceed-
ings of the International Conference on Dependency
Linguistics (Depling), pages 282?289.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
Tag, dynamic programming, and the perceptron for ef-
ficient, feature-rich parsing. In Proceedings of CoNL-
L, pages 9?16.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task of EMNLP-CoNLL 2007, pages
957?961.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of ACL, pages 173?180.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL, pages 132?139.
Shay B. Cohen and Noah A. Smith. 2007. Joint morpho-
logical and syntactic disambiguation. In Proceedings
of EMNLP-CoNLL, pages 208?217.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of ACL, pages 112?119.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of ACL-
EACL, pages 16?23.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP, pages 1?8.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
Andrea Gesmundo, James Henderson, Paola Merlo, and
Ivan Titov. 2009. A latent variable model of syn-
chronous syntactic-semantic parsing for multiple lan-
guages. In Proceedings of the 2009 CoNLL Shared
Task, pages 37?42.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gener-
ative model for joint morphological segmentation and
syntactic parsing. In Proceedings of ACL, pages 371?
379.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, L-
lu??s Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In Proceedings of the 2009 CoN-
LL Shared Task, pages 1?18.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2011. Incremental joint pos tagging
and dependency parsing in chinese. In Proceedings of
IJCNLP, pages 1216?1224.
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-
ings of ACL, pages 1077?1086.
Richard Johansson and Pierre Nugues. 2006. Investigat-
ing multilingual dependency parsing. In Proceedings
of CoNLL, pages 206?210.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of ACL,
pages 1?11.
1464
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL, pages 595?603.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decomposi-
tion for parsing with non-projective head automata. In
Proceedings of EMNLP, pages 1288?1298.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Morgan and Claypool.
John Lee, Jason Naradowsky, and David A. Smith. 2011.
A discriminative model for joint morphological disam-
biguation and dependency parsing. In Proceedings of
ACL, pages 885?894.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wen-
liang Chen, and Haizhou Li. 2011. Joint models for
chinese POS tagging and dependency parsing. In Pro-
ceedings of EMNLP, pages 1180?1191.
Andre Martins, Noah Smith, and Eric Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proceedings of ACL-IJCNLP,
pages 342?350.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference.
In Proceedings of EMNLP, pages 34?44.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of EACL, pages 81?88.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL, pages 91?98.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Proceedings
of CoNLL, pages 49?56.
Joakim Nivre. 2006. Inductive Dependency Parsing.
Springer.
Joakim Nivre. 2007. Incremental non-projective depen-
dency parsing. In Proceedings of NAACL HLT, pages
396?403.
Joakim Nivre. 2008. Algorithms for deterministic incre-
mental dependency parsing. Computational Linguis-
tics, 34:513?553.
Joakim Nivre. 2009. Non-projective dependency pars-
ing in expected linear time. In Proceedings of ACL-
IJCNLP, pages 351?359.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
HLT, pages 404?411.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of COL-
ING/ACL, pages 433?440.
Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective depen-
dency parsing. In Proceedings of EMNLP, pages 129?
137.
Helmut Schmid and Florian Laws. 2008. Estimation of
conditional probabilities with decision trees and an ap-
plication to fine-grained POS tagging. In Proceedings
of COLING, pages 777?784.
Qinfeng Shi, James Petterson, Gideon Dror, John Lang-
ford, Alex Smola, Alex Strehl, and S V N Vish-
wanathan. 2009. Hash Kernels for Structured Data.
In Journal of Machine Learning.
David Smith and Jason Eisner. 2008. Dependency pars-
ing by belief propagation. In Proceedings of EMNLP,
pages 145?156.
Anders S?gaard. 2011. Semi-supervised condensed n-
earest neighbor for part-of-speech tagging. In Pro-
ceedings of ACL, pages 48?52.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael
Collins. 2009. An empirical study of semi-supervised
structured conditional models for dependency parsing.
In Proceedings of EMNLP, pages 551?560.
Ivan Titov and James Henderson. 2007. A latent variable
model for generative dependency parsing. In Proceed-
ings of IWPT, pages 144?155.
Ivan Titov, James Henderson, Paola Merlo, and Gabriele
Musillo. 2009. Online graph planarization for syn-
chronous parsing of semantic and syntactic dependen-
cies. In Proceedings of IJCAI.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of NAACL, pages 252?259.
Reut Tsarfaty. 2006. Integrated morphological and syn-
tactic disambiguation for modern hebrew. In Pro-
ceedings of the COLING/ACL 2006 Student Research
Workshop, pages 49?54.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of IWPT, pages 195?206.
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based and
transition-based dependency parsing. In Proceedings
of EMNLP, pages 562?571.
Yue Zhang and Stephen Clark. 2011. Syntactic process-
ing using the generalized perceptron and beam search.
Computational Linguistics, 37:105?151.
Yue Zhang and Joakim Nivre. 2011. Transition-based
parsing with rich non-local features. In Proceedings
of ACL.
1465
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 380?391,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Latent Anaphora Resolution for Cross-Lingual Pronoun Prediction
Christian Hardmeier J?rg Tiedemann Joakim Nivre
Uppsala University
Department of Linguistics and Philology
Box 635, 751 26 Uppsala, Sweden
firstname.lastname@lingfil.uu.se
Abstract
This paper addresses the task of predicting the
correct French translations of third-person sub-
ject pronouns in English discourse, a problem
that is relevant as a prerequisite for machine
translation and that requires anaphora resolu-
tion. We present an approach based on neu-
ral networks that models anaphoric links as
latent variables and show that its performance
is competitive with that of a system with sep-
arate anaphora resolution while not requiring
any coreference-annotated training data. This
demonstrates that the information contained in
parallel bitexts can successfully be used to ac-
quire knowledge about pronominal anaphora
in an unsupervised way.
1 Motivation
When texts are translated from one language into
another, the translation reconstructs the meaning or
function of the source text with the means of the
target language. Generally, this has the effect that
the entities occurring in the translation and their mu-
tual relations will display similar patterns as the enti-
ties in the source text. In particular, coreference pat-
terns tend to be very similar in translations of a text,
and this fact has been exploited with good results to
project coreference annotations from one language
into another by using word alignments (Postolache
et al, 2006; Rahman and Ng, 2012).
On the other hand, what is true in general need
not be true for all types of linguistic elements. For
instance, a substantial percentage of the English third-
person subject pronouns he, she, it and they does
not get realised as pronouns in French translations
(Hardmeier, 2012). Moreover, it has been recognised
by various authors in the statistical machine transla-
tion (SMT) community (Le Nagard and Koehn, 2010;
Hardmeier and Federico, 2010; Guillou, 2012) that
pronoun translation is a difficult problem because,
even when a pronoun does get translated as a pro-
noun, it may require choosing the correct word form
based on agreement features that are not easily pre-
dictable from the source text.
The work presented in this paper investigates
the problem of cross-lingual pronoun prediction for
English-French. Given an English pronoun and its
discourse context as well as a French translation of
the same discourse and word alignments between
the two languages, we attempt to predict the French
word aligned to the English pronoun. As far as we
know, this task has not been addressed in the litera-
ture before. In our opinion, it is interesting for several
reasons. By studying pronoun prediction as a task in
its own right, we hope to contribute towards a better
understanding of pronoun translation with a long-
term view to improving the performance of SMT
systems. Moreover, we believe that this task can lead
to interesting insights about anaphora resolution in a
multi-lingual context. In particular, we show in this
paper that the pronoun prediction task makes it possi-
ble to model the resolution of pronominal anaphora
as a latent variable and opens up a way to solve a
task relying on anaphora resolution without using
any data annotated for anaphora. This is what we
consider the main contribution of our present work.
We start by modelling cross-lingual pronoun pre-
diction as an independent machine learning task after
doing anaphora resolution in the source language
(English) using the BART software (Broscheit et
al., 2010). We show that it is difficult to achieve
satisfactory performance with standard maximum-
380
The latest version released in March is equipped with ... It is sold at ...
La derni?re version lanc?e en mars est dot?e de ... ? est vendue ...
Figure 1: Task setup
entropy classifiers especially for low-frequency pro-
nouns such as the French feminine plural pronoun
elles. We propose a neural network classifier that
achieves better precision and recall and manages to
make reasonable predictions for all pronoun cate-
gories in many cases.
We then go on to extend our neural network archi-
tecture to include anaphoric links as latent variables.
We demonstrate that our classifier, now with its own
source language anaphora resolver, can be trained
successfully with backpropagation. In this setup, we
no longer use the machine learning component in-
cluded in the external coreference resolution system
(BART) to predict anaphoric links. Anaphora reso-
lution is done by our neural network classifier and
requires only some quantity of word-aligned parallel
data for training, completely obviating the need for a
coreference-annotated training set.
2 Task Setup
The overall setup of the classification task we address
in this paper is shown in Figure 1. We are given an
English discourse containing a pronoun along with
its French translation and word alignments between
the two languages, which in our case were computed
automatically using a standard SMT pipeline with
GIZA++ (Och and Ney, 2003). We focus on the four
English third-person subject pronouns he, she, it and
they. The output of the classifier is a multinomial
distribution over six classes: the four French subject
pronouns il, elle, ils and elles, corresponding to mas-
culine and feminine singular and plural, respectively;
the impersonal pronoun ce/c?, which occurs in some
very frequent constructions such as c?est (it is); and
a sixth class OTHER, which indicates that none of
these pronouns was used. In general, a pronoun may
be aligned to multiple words; in this case, a training
example is counted as a positive example for a class
if the target word occurs among the words aligned
to the pronoun, irrespective of the presence of other
0 0 0 1 0version
0 1 0 0 0la
0 0 1 0 0elle
0 .5 0 .5 0
0 0 1 0 0
0 .05 .9 .05 0
p1 = .9
p2 =
.1
word candidate training ex.
Figure 2: Antecedent feature aggregation
aligned tokens.
This task setup resembles the problem that an
SMT system would have to solve to make informed
choices when translating pronouns, an aspect of trans-
lation neglected by most existing SMT systems. An
important difference between the SMT setup and our
own classifiers is that we use context from human-
made translations for prediction. This potentially
makes the task both easier and more difficult; easier,
because the context can be relied on to be correctly
translated, and more difficult, because human transla-
tors frequently create less literal translations than an
SMT system would. Integrating pronoun prediction
into the translation process would require significant
changes to the standard SMT decoding setup in order
to take long-range dependencies in the target lan-
guage into account, which is why we do not address
this issue in our current work.
In all the experiments presented in this paper, we
used features from two different sources:
? Anaphora context features describe the source
language pronoun and its immediate context
consisting of three words to its left and three
words to its right. They are encoded as vec-
tors whose dimensionality is equal to the source
vocabulary size with a single non-zero compo-
nent indicating the word referred to (one-hot
vectors).
? Antecedent features describe an antecedent can-
didate. Antecedent candidates are represented
by the target language words aligned to the syn-
tactic head of the source language markable
381
TED News
ce 16.3 % 6.4 %
elle 7.1 % 10.1 %
elles 3.0 % 3.9 %
il 17.1 % 26.5 %
ils 15.6 % 15.1 %
OTHER 40.9 % 38.0 %
Table 1: Distribution of classes in the training data
noun phrase as identified by the Collins head
finder (Collins, 1999).
The different handling of anaphora context features
and antecedent features is due to the fact that we al-
ways consider a constant number of context words
on the source side, whereas the number of word
vectors to be considered depends on the number of
antecedent candidates and on the number of target
words aligned to each antecedent.
The encoding of the antecedent features is illus-
trated in Figure 2 for a training example with two
antecedent candidates translated to elle and la ver-
sion, respectively. The target words are represented
as one-hot vectors with the dimensionality of the tar-
get language vocabulary. These vectors are then av-
eraged to yield a single vector per antecedent candi-
date. Finally, the vectors of all candidates for a given
training example are weighted by the probabilities
assigned to them by the anaphora resolver (p1 and
p2) and summed to yield a single vector per training
example.
3 Data Sets and External Tools
We run experiments with two different test sets. The
TED data set consists of around 2.6 million tokens of
lecture subtitles released in the WIT3 corpus (Cet-
tolo et al, 2012). The WIT3 training data yields
71,052 examples, which were randomly partitioned
into a training set of 63,228 examples and a test set
of 7,824 examples. The official WIT3 development
and test sets were not used in our experiments. The
news-commentary data set is version 6 of the parallel
news-commentary corpus released as a part of the
WMT 2011 training data1. It contains around 2.8 mil-
lion tokens of news text and yields 31,017 data points,
1http://www.statmt.org/wmt11/translation-task.
html (3 July 2013).
which were randomly split into 27,900 training exam-
ples and 3,117 test instances. The distribution of the
classes in the two training sets is shown in Table 1.
One thing to note is the dominance of the OTHER
class, which pools together such different phenom-
ena as translations with other pronouns not in our list
(e. g., celui-ci) and translations with full noun phrases
instead of pronouns. Splitting this group into more
meaningful subcategories is not straightforward and
must be left to future work.
The feature setup of all our classifiers requires
the detection of potential antecedents and the extrac-
tion of features pairing anaphoric pronouns with an-
tecedent candidates. Some of our experiments also
rely on an external anaphora resolution component.
We use the open-source anaphora resolver BART to
generate this information. BART (Broscheit et al,
2010) is an anaphora resolution toolkit consisting of
a markable detection and feature extraction pipeline
based on a variety of standard natural language pro-
cessing (NLP) tools and a machine learning com-
ponent to predict coreference links including both
pronominal anaphora and noun-noun coreference. In
our experiments, we always use BART?s markable
detection and feature extraction machinery. Mark-
able detection is based on the identification of noun
phrases in constituency parses generated with the
Stanford parser (Klein and Manning, 2003). The set
of features extracted by BART is an extension of the
widely used mention-pair anaphora resolution feature
set by Soon et al (2001) (see below, Section 6).
In the experiments of the next two sections, we
also use BART to predict anaphoric links for pro-
nouns. The model used with BART is a maximum
entropy ranker trained on the ACE02-npaper corpus
(LDC2003T11). In order to obtain a probability dis-
tribution over antecedent candidates rather than one-
best predictions or coreference sets, we modified the
ranking component with which BART resolves pro-
nouns to normalise and output the scores assigned
by the ranker to all candidates instead of picking the
highest-scoring candidate.
4 Baseline Classifiers
In order to create a simple, but reasonable baseline
for our task, we trained a maximum entropy (ME)
382
TED
(Accuracy: 0.685)
P R F
ce 0.593 0.728 0.654
elle 0.798 0.523 0.632
elles 0.812 0.164 0.273
il 0.764 0.550 0.639
ils 0.632 0.949 0.759
OTHER 0.724 0.692 0.708
News commentary
(Accuracy: 0.576)
P R F
ce 0.508 0.294 0.373
elle 0.530 0.312 0.393
elles 0.538 0.062 0.111
il 0.600 0.666 0.631
ils 0.593 0.769 0.670
OTHER 0.564 0.609 0.586
Table 2: Maximum entropy classifier results
TED
(Accuracy: 0.700)
P R F
ce 0.634 0.747 0.686
elle 0.756 0.617 0.679
elles 0.679 0.319 0.434
il 0.719 0.591 0.649
ils 0.663 0.940 0.778
OTHER 0.743 0.678 0.709
News commentary
(Accuracy: 0.576)
P R F
ce 0.477 0.344 0.400
elle 0.498 0.401 0.444
elles 0.565 0.116 0.193
il 0.655 0.626 0.640
ils 0.570 0.834 0.677
OTHER 0.567 0.573 0.570
Table 3: Neural network classifier with anaphoras resolved by BART
classifier with the MegaM software package2 using
the features described in the previous section and the
anaphora links found by BART. Results are shown
in Table 2. The baseline results show an overall
higher accuracy for the TED data than for the news-
commentary data. While the precision is above 50 %
in all categories and considerably higher in some,
recall varies widely.
The pronoun elles is particularly interesting. This
is the feminine plural of the personal pronoun, and
it usually corresponds to the English pronoun they,
which is not marked for gender. In French, elles is a
marked choice which is only used if the antecedent
exclusively refers to females or feminine-gendered
objects. The presence of a single item with mascu-
line grammatical gender in the antecedent will trigger
the use of the masculine plural pronoun ils instead.
This distinction cannot be predicted from the English
source pronoun or its context; making correct pre-
dictions requires knowledge about the antecedent of
the pronoun. Moreover, elles is a low-frequency pro-
noun. There are only 1,909 occurrences of this pro-
2http://www.umiacs.umd.edu/~hal/megam/ (20 June
2013).
noun in the TED training data, and 1,077 in the news-
commentary training set. Because of these special
properties of the feminine plural class, we argue that
the performance of a classifier on elles is a good indi-
cator of how well it can represent relevant knowledge
about pronominal anaphora as opposed to overfitting
to source contexts or acting on prior assumptions
about class frequencies.
In accordance with the general linguistic prefer-
ence for ils, the classifier tends to predict ils much
more often than elles when encountering an English
plural pronoun. This is reflected in the fact that elles
has much lower recall than ils. Clearly, the classifier
achieves a good part of its accuracy by making ma-
jority choices without exploiting deeper knowledge
about the antecedents of pronouns.
An additional experiment with a subset of 27,900
training examples from the TED data confirms that
the difference between TED and news commentaries
is not just an effect of training data size, but that TED
data is genuinely easier to predict than news com-
mentaries. In the reduced data TED condition, the
classifier achieves an accuracy of 0.673. Precision
and recall of all classifiers are much closer to the
383
E
P
R1
L1
R2
L2
R3
L3
p3p2p1
321
H
S
A
Figure 3: Neural network for pronoun prediction
large-data TED condition than to the news commen-
tary experiments, except for elles, where we obtain
an F-score of 0.072 (P 0.818, R 0.038), indicating
that small training data size is a serious problem for
this low-frequency class.
5 Neural Network Classifier
In the previous section, we saw that a simple multi-
class maximum entropy classifier, while making cor-
rect predictions for much of the data set, has a signifi-
cant bias towards making majority class decisions, re-
lying more on prior assumptions about the frequency
distribution of the classes than on antecedent features
when handling examples of less frequent classes. In
order to create a system that can be trained to rely
more explicitly on antecedent information, we cre-
ated a neural network classifier for our task. The intro-
duction of a hidden layer should enable the classifier
to learn abstract concepts such as gender and number
that are useful across multiple output categories, so
that the performance of sparsely represented classes
can benefit from the training examples of the more
frequent classes.
The overall structure of the network is shown in
Figure 3. As inputs, the network takes the same fea-
tures that were available to the baseline ME classifier,
based on the source pronoun (P) with three words
of context to its left (L1 to L3) and three words to
its right (R1 to R3) as well as the words aligned to
the syntactic head words of all possible antecedent
candidates as found by BART (A). All words are
encoded as one-hot vectors whose dimensionality is
equal to the vocabulary size. If multiple words are
aligned to the syntactic head of an antecedent candi-
date, their word vectors are averaged with uniform
weights. The resulting vectors for each antecedent
are then averaged with weights defined by the pos-
terior distribution of the anaphora resolver in BART
(p1 to p3).
The network has two hidden layers. The first layer
(E) maps the input word vectors to a low-dimensional
representation. In this layer, the embedding weights
for all the source language vectors (the pronoun
and its 6 context words) are tied, so if two words
are the same, they are mapped to the same lower-
dimensional embedding irrespective of their position
relative to the pronoun. The embedding of the an-
tecedent word vectors is independent, as these word
vectors represent target language words. The entire
embedding layer is then mapped to another hidden
layer (H), which is in turn connected to a softmax out-
put layer (S) with 6 outputs representing the classes
ce, elle, elles, il, ils and OTHER. The non-linearity of
both hidden layers is the logistic sigmoid function,
f (x) = 1/(1+ e?x).
In all experiments reported in this paper, the dimen-
sionality of the source and target language word em-
beddings is 20, resulting in a total embedding layer
size of 160, and the size of the last hidden layer is
equal to 50. These sizes are fairly small. In experi-
ments with larger layer sizes, we were able to obtain
similar, but no better results.
384
The neural network is trained with mini-batch
stochastic gradient descent with backpropagated gra-
dients using the RMSPROP algorithm with cross-
entropy as the objective function.3 In contrast to
standard gradient descent, RMSPROP normalises the
magnitude of the gradient components by dividing
them by a root-mean-square moving average. We
found this led to faster convergence. Other features
of our training algorithm include the use of momen-
tum to even out gradient oscillations, adaptive learn-
ing rates for each weight as well as adaptation of
the global learning rate as a function of current train-
ing progress. The network is regularised with an `2
weight penalty. Good settings of the initial learning
rate and the weight cost parameter (both around 0.001
in most experiments) were found by manual experi-
mentation. Generally, we train our networks for 300
epochs, compute the validation error on a held-out
set of some 10 % of the training data after each epoch
and use the model that achieved the lowest validation
error for testing.
Since the source context features are very infor-
mative and it is comparatively more difficult to learn
from the antecedents, the network sometimes had a
tendency to overfit to the source features and disre-
gard antecedent information. We found that this prob-
lem can be solved effectively by presenting a part of
the training without any source features, forcing the
network to learn from the information contained in
the antecedents. In all experiments in this paper, we
zero out all source features (input layers P, L1 to L3
and R1 to R3) with a probability of 50 % in each
training example. At test time, no information is ze-
roed out.
Classification results with this network are shown
in Table 3. We note that the accuracy has increased
slightly for the TED test set and remains exactly the
same for the news commentary corpus. However, a
closer look on the results for individual classes re-
veals that the neural network makes better predictions
for almost all classes. In terms of F-score, the only
class that becomes slightly worse is the OTHER class
for the news commentary corpus because of lower
recall, indicating that the neural network classifier is
less biased towards using the uninformative OTHER
3Our training procedure is greatly inspired by a series of on-
line lectures held by Geoffrey Hinton in 2012 (https://www.
coursera.org/course/neuralnets, 10 September 2013).
category. Recall for elle and elles increases consider-
ably, but especially for elles it is still quite low. The
increase in recall comes with some loss in precision,
but the net effect on F-score is clearly positive.
6 Latent Anaphora Resolution
Considering Figure 1 again, we note that the bilin-
gual setting of our classification task adds some in-
formation not available to the monolingual anaphora
resolver that can be helpful when determining the
correct antecedent for a given pronoun. Knowing the
gender of the translation of a pronoun limits the set
of possible antecedents to those whose translation is
morphologically compatible with the target language
pronoun. We can exploit this fact to learn how to
resolve anaphoric pronouns without requiring data
with manually annotated anaphoric links.
To achieve this, we extend our neural network with
a component to predict the probability of each an-
tecedent candidate to be the correct antecedent (Fig-
ure 4). The extended network is identical to the previ-
ous version except for the upper left part dealing with
anaphoric link features. The only difference between
the two networks is the fact that anaphora resolution
is now performed by a part of our neural network
itself instead of being done by an external module
and provided to the classifier as an input.
In this setup, we still use some parts of the BART
toolkit to extract markables and compute features.
However, we do not make use of the machine learn-
ing component in BART that makes the actual pre-
dictions. Since this is the only component trained on
coreference-annotated data in a typical BART con-
figuration, no coreference annotations are used any-
where in our system even though we continue to rely
on the external anaphora resolver for preprocessing
to avoid implementing our own markable and feature
extractors and to make comparison easier.
For each candidate markable identified by BART?s
preprocessing pipeline, the anaphora resolution
model receives as input a link feature vector (T) de-
scribing relevant aspects of the antecedent candidate-
anaphora pair. This feature vector is generated by the
feature extraction machinery in BART and includes
a standard feature set for coreference resolution par-
tially based on work by Soon et al (2001). We use
the following feature extractors in BART, each of
385
E
H
S
1
2
3
L3
R3
L2
R2
L1
R1
P
T
U
V
1 2 3
A
Figure 4: Neural network with latent anaphora resolution
which can generate multiple features:
? Anaphora mention type
? Gender match
? Number match
? String match
? Alias feature (Soon et al, 2001)
? Appositive position feature (Soon et al, 2001)
? Semantic class (Soon et al, 2001)
? Semantic class match
? Binary distance feature
? Antecedent is first mention in sentence
Our baseline set of features was borrowed whole-
sale from a working coreference system and includes
some features that are not relevant to the task at hand,
e. g., features indicating that the anaphora is a pro-
noun, is not a named entity, etc. After removing all
features that assume constant values in the training
set when resolving antecedents for the set of pro-
nouns we consider, we are left with a basic set of 37
anaphoric link features that are fed as inputs to our
network. These features are exactly the same as those
available to the anaphora resolution classifier in the
BART system used in the previous section.
Each training example for our network can have
an arbitrary number of antecedent candidates, each of
which is described by an antecedent word vector (A)
and by an anaphoric link vector (T). The anaphoric
link features are first mapped to a regular hidden layer
with logistic sigmoid units (U). The activations of the
hidden units are then mapped to a single value, which
functions as an element in a softmax layer over all an-
tecedent candidates (V). This softmax layer assigns
a probability to each antecedent candidate, which we
then use to compute a weighted average over the an-
tecedent word vector, replacing the probabilities pi
in Figures 2 and 3.
At training time, the network?s anaphora resolu-
tion component is trained in exactly the same way as
the rest of the network. The error signal from the em-
bedding layer is backpropagated both to the weight
matrix defining the antecedent word embedding and
to the anaphora resolution subnetwork. Note that the
number of weights in the network is the same for
all training examples even though the number of an-
tecedent candidates varies because all weights related
to antecedent word features and anaphoric link fea-
tures are shared between all antecedent candidates.
One slightly uncommon feature of our neural net-
work is that it contains an internal softmax layer to
generate normalised probabilities over all possible
antecedent candidates. Moreover, weights are shared
between all antecedent candidates, so the inputs of
our internal softmax layer share dependencies on
the same weight variables. When computing deriva-
tives with backpropagation, these shared dependen-
cies must be taken into account. In particular, the
outputs yi of the antecedent resolution layer are the re-
sult of a softmax applied to functions of some shared
variables q:
yi =
exp fi(q)
?k exp fk(q)
(1)
386
The derivatives of any yi with respect to q, which
can be any of the weights in the anaphora resolution
subnetwork, have dependencies on the derivatives of
the other softmax inputs with respect to q:
?yi
?q = yi
(
? fi(q)
?q ??k
yk
? fk(q)
?q
)
(2)
This makes the implementation of backpropagation
for this part of the network somewhat more compli-
cated, but in the case of our networks, it has no major
impact on training time.
Experimental results for this network are shown
in Table 4. Compared with Table 3, we note that the
overall accuracy is only very slightly lower for TED,
and for the news commentaries it is actually better.
When it comes to F-scores, the performance for elles
improves by a small amount, while the effect on the
other classes is a bit more mixed. Even where it gets
worse, the differences are not dramatic considering
that we eliminated a very knowledge-rich resource
from the training process. This demonstrates that it
is possible, in our classification task, to obtain good
results without using any data manually annotated for
anaphora and to rely entirely on unsupervised latent
anaphora resolution.
7 Further Improvements
The results presented in the preceding section repre-
sent a clear improvement over the ME classifiers in
Table 2, even though the overall accuracy increased
only slightly. Not only does our neural network clas-
sifier achieve better results on the classification task
at hand without requiring an anaphora resolution clas-
sifier trained on manually annotated data, but it per-
forms clearly better for the feminine categories that
reflect minority choices requiring knowledge about
the antecedents. Nevertheless, the performance is still
not entirely satisfactory.
By subjecting the output of our classifier on a de-
velopment set to a manual error analysis, we found
that a fairly large number of errors belong to two error
types: On the one hand, the preprocessing pipeline
used to identify antecedent candidates does not al-
ways include the correct antecedent in the set pre-
sented to the neural network. Whenever this occurs,
it is obvious that the classifier cannot possibly find
the correct antecedent. Out of 76 examples of the cat-
egory elles that had been mistakenly predicted as ils,
we found that 43 suffered from this problem. In other
classes, the problem seems to be somewhat less com-
mon, but it still exists. On the other hand, in many
cases (23 out of 76 for the category mentioned be-
fore) the anaphora resolution subnetwork does iden-
tify an antecedent manually recognised to belong to
the right gender/number group, but still predicts an in-
correct pronoun. This may indicate that the network
has difficulties learning a correct gender/number rep-
resentation for all words in the vocabulary.
7.1 Relaxing Markable Extraction
The pipeline we use to extract potential antecedent
candidates is borrowed from the BART anaphora
resolution toolkit. BART uses a syntactic parser to
identify noun phrases as markables. When extract-
ing antecedent candidates for coreference prediction,
it starts by considering a window consisting of the
sentence in which the anaphoric pronoun is located
and the two immediately preceding sentences. Mark-
ables in this window are checked for morphological
compatibility in terms of gender and number with the
anaphoric pronoun, and only compatible markables
are extracted as antecedent candidates. If no compat-
ible markables are found in the initial window, the
window is successively enlarged one sentence at a
time until at least one suitable markable is found.
Our error analysis shows that this procedure
misses some relevant markables both because the ini-
tial two-sentence extraction window is too small and
because the morphological compatibility check incor-
rectly filters away some markables that should have
been considered as candidates. By contrast, the ex-
traction procedure does extract quite a number of first
and second person noun phrases (I, we, you and their
oblique forms) in the TED talks which are extremely
unlikely to be the antecedent of a later occurrence of
he, she, it or they. As a first step, we therefore adjust
the extraction criteria to our task by increasing the
initial extraction window to five sentences, exclud-
ing first and second person markables and removing
the morphological compatibility requirement. The
compatibility check is still used to control expansion
of the extraction window, but it is no longer applied
to filter the extracted markables. This increases the
accuracy to 0.701 for TED and 0.602 for the news
387
TED
(Accuracy: 0.696)
P R F
ce 0.618 0.722 0.666
elle 0.754 0.548 0.635
elles 0.737 0.340 0.465
il 0.718 0.629 0.670
ils 0.652 0.916 0.761
OTHER 0.741 0.682 0.711
News commentary
(Accuracy: 0.597)
P R F
ce 0.419 0.368 0.392
elle 0.547 0.460 0.500
elles 0.539 0.135 0.215
il 0.623 0.719 0.667
ils 0.596 0.783 0.677
OTHER 0.614 0.544 0.577
Table 4: Neural network classifier with latent anaphora resolution
TED
(Accuracy: 0.713)
P R F
ce 0.611 0.723 0.662
elle 0.749 0.596 0.664
elles 0.602 0.616 0.609
il 0.733 0.638 0.682
ils 0.710 0.884 0.788
OTHER 0.760 0.704 0.731
News commentary
(Accuracy: 0.626)
P R F
ce 0.492 0.324 0.391
elle 0.526 0.439 0.478
elles 0.547 0.558 0.552
il 0.599 0.757 0.669
ils 0.671 0.878 0.761
OTHER 0.681 0.526 0.594
Table 5: Final classifier results
commentaries, while the performance for elles im-
proves to F-scores of 0.531 (TED; P 0.690, R 0.432)
and 0.304 (News commentaries; P 0.444, R 0.231),
respectively. Note that these and all the following re-
sults are not directly comparable to the ME baseline
results in Table 2, since they include modifications
and improvements to the training data extraction pro-
cedure that might possibly lead to benefits in the ME
setting as well.
7.2 Adding Lexicon Knowledge
In order to make it easier for the classifier to iden-
tify the gender and number properties of infrequent
words, we extend the word vectors with features indi-
cating possible morphological features for each word.
In early experiments with ME classifiers, we found
that our attempts to do proper gender and number
tagging in French text did not improve classification
performance noticeably, presumably because the an-
notation was too noisy. In more recent experiments,
we just add features indicating all possible morpho-
logical interpretations of each word, rather than try-
ing to disambiguate them. To do this, we look up
the morphological annotations of the French words
in the Lefff dictionary (Sagot et al, 2006) and intro-
duce a set of new binary features to indicate whether
a particular reading of a word occurs in that dictio-
nary. These features are then added to the one-hot
representation of the antecedent words. Doing so im-
proves the classifier accuracy to 0.711 (TED) and
0.604 (News commentaries), while the F-scores for
elles reach 0.589 (TED; P 0.649, R 0.539) and 0.500
(News commentaries; P 0.545, R 0.462), respectively.
7.3 More Anaphoric Link Features
Even though the modified antecedent candidate ex-
traction with its larger context window and without
the morphological filter results in better performance
on both test sets, additional error analysis reveals
that the classifiers has greater problems identifying
the correct markable in this setting. One reason for
this may be that the baseline anaphoric link feature
set described above (Section 6) only includes two
very rough binary distance features which indicate
whether or not the anaphora and the antecedent can-
didate occur in the same or in immediately adjacent
sentences. With the larger context window, this may
be too unspecific. In our final experiment, we there-
fore enable some additional features which are avail-
able in BART, but disabled in the baseline system:
388
? Distance in number of markables
? Distance in number of sentences
? Sentence distance, log-transformed
? Distance in number of words
? Part of speech of head word
Most of these encode the distance between the
anaphora and the antecedent candidate in more pre-
cise ways. Complete results for this final system are
presented in Table 5.
Including these additional features leads to another
slight increase in accuracy for both corpora, with sim-
ilar or increased classifier F-scores for most classes
except elle in the news commentary condition. In par-
ticular, we should like to point out the performance
of our benchmark classifier for elles, which suffered
from extremely low recall in the first classifiers and
approaches the performance of the other classes, with
nearly balanced precision and recall, in this final sys-
tem. Since elles is a low-frequency class and cannot
be reliably predicted using source context alone, we
interpret this as evidence that our final neural network
classifier has incorporated some relevant knowledge
about pronominal anaphora that the baseline ME clas-
sifier and earlier versions of our network have no ac-
cess to. This is particularly remarkable because no
data manually annotated for coreference was used for
training.
8 Related work
Even though it was recognised years ago that the
information contained in parallel corpora may pro-
vide valuable information for the improvement of
anaphora resolution systems, there have not been
many attempts to cash in on this insight. Mitkov and
Barbu (2003) exploit parallel data in English and
French to improve pronominal anaphora resolution
by combining anaphora resolvers for the individual
languages with handwritten rules to resolve conflicts
between the output of the language-specific resolvers.
Veselovsk? et al (2012) apply a similar strategy to
English-Czech data to resolve different uses of the
pronoun it. Other work has used word alignments to
project coreference annotations from one language
to another with a view to training anaphora resolvers
in the target language (Postolache et al, 2006; de
Souza and Ora?san, 2011). Rahman and Ng (2012)
instead use machine translation to translate their test
data into a language for which they have an anaphora
resolver and then project the annotations back to the
original language. Completely unsupervised mono-
lingual anaphora resolution has been approached us-
ing, e. g., Markov logic (Poon and Domingos, 2008)
and the Expectation-Maximisation algorithm (Cherry
and Bergsma, 2005; Charniak and Elsner, 2009). To
the best of our knowledge, the direct application of
machine learning techniques to parallel data in a task
related to anaphora resolution is novel in our work.
Neural networks and deep learning techniques
have recently gained some popularity in natural lan-
guage processing. They have been applied to tasks
such as language modelling (Bengio et al, 2003;
Schwenk, 2007), translation modelling in statistical
machine translation (Le et al, 2012), but also part-of-
speech tagging, chunking, named entity recognition
and semantic role labelling (Collobert et al, 2011).
In tasks related to anaphora resolution, standard feed-
forward neural networks have been tested as a clas-
sifier in an anaphora resolution system (Stuckardt,
2007), but the network design presented in our work
is novel.
9 Conclusion
In this paper, we have introduced cross-lingual pro-
noun prediction as an independent natural language
processing task. Even though it is not an end-to-end
task, pronoun prediction is interesting for several rea-
sons. It is related to the problem of pronoun transla-
tion in SMT, a currently unsolved problem that has
been addressed in a number of recent research publi-
cations (Le Nagard and Koehn, 2010; Hardmeier and
Federico, 2010; Guillou, 2012) without reaching a
major breakthrough. In this work, we have shown that
pronoun prediction can be effectively modelled in a
neural network architecture with relatively simple
features. More importantly, we have demonstrated
that the task can be exploited to train a classifier with
a latent representation of anaphoric links. With paral-
lel text as its only supervision this classifier achieves
a level of performance that is similar to, if not bet-
ter than, that of a classifier using a regular anaphora
resolution system trained with manually annotated
data.
389
References
Yoshua Bengio, R?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Research,
3:1137?1155.
Samuel Broscheit, Massimo Poesio, Simone Paolo
Ponzetto, Kepa Joseba Rodriguez, Lorenza Romano,
Olga Uryupina, Yannick Versley, and Roberto Zanoli.
2010. BART: A multilingual anaphora resolution sys-
tem. In Proceedings of the 5th International Work-
shop on Semantic Evaluations (SemEval-2010), Upp-
sala, Sweden, 15?16 July 2010.
Mauro Cettolo, Christian Girardi, and Marcello Federico.
2012. WIT3: Web inventory of transcribed and trans-
lated talks. In Proceedings of the 16th Conference
of the European Association for Machine Translation
(EAMT), pages 261?268, Trento, Italy.
Eugene Charniak and Micha Elsner. 2009. EM works
for pronoun anaphora resolution. In Proceedings of the
12th Conference of the European Chapter of the ACL
(EACL 2009), pages 148?156, Athens, Greece.
Colin Cherry and Shane Bergsma. 2005. An Expecta-
tion Maximization approach to pronoun resolution. In
Proceedings of the Ninth Conference on Computational
Natural Language Learning (CoNLL-2005), pages 88?
95, Ann Arbor, Michigan.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
Ronan Collobert, Jason Weston, L?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
Journal of Machine Learning Research, 12:2461?2505.
Jos? de Souza and Constantin Ora?san. 2011. Can pro-
jected chains in parallel corpora help coreference reso-
lution? In Iris Hendrickx, Sobha Lalitha Devi, Ant?nio
Branco, and Ruslan Mitkov, editors, Anaphora Process-
ing and Applications, volume 7099 of Lecture Notes in
Computer Science, pages 59?69. Springer, Berlin.
Liane Guillou. 2012. Improving pronoun translation for
statistical machine translation. In Proceedings of the
Student Research Workshop at the 13th Conference of
the European Chapter of the Association for Computa-
tional Linguistics, pages 1?10, Avignon, France.
Christian Hardmeier and Marcello Federico. 2010. Mod-
elling pronominal anaphora in statistical machine trans-
lation. In Proceedings of the seventh International
Workshop on Spoken Language Translation (IWSLT),
pages 283?289, Paris, France.
Christian Hardmeier. 2012. Discourse in statistical ma-
chine translation: A survey and a case study. Discours,
11.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423?430, Sapporo, Japan.
Hai-Son Le, Alexandre Allauzen, and Fran?ois Yvon.
2012. Continuous space translation models with neural
networks. In Proceedings of the 2012 Conference of the
North American Chapter of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 39?48, Montr?al, Canada.
Ronan Le Nagard and Philipp Koehn. 2010. Aiding pro-
noun translation with co-reference resolution. In Pro-
ceedings of the Joint Fifth Workshop on Statistical Ma-
chine Translation and MetricsMATR, pages 252?261,
Uppsala, Sweden.
Ruslan Mitkov and Catalina Barbu. 2003. Using bilingual
corpora to improve pronoun resolution. Languages in
Contrast, 4(2):201?211.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment models.
Computational linguistics, 29:19?51.
Hoifung Poon and Pedro Domingos. 2008. Joint un-
supervised coreference resolution with Markov Logic.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 650?
659, Honolulu, Hawaii.
Oana Postolache, Dan Cristea, and Constantin Ora?san.
2006. Transferring coreference chains through word
alignment. In Proceedings of the 5th Conference
on International Language Resources and Evaluation
(LREC-2006), pages 889?892, Genoa.
Altaf Rahman and Vincent Ng. 2012. Translation-based
projection for multilingual coreference resolution. In
Proceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 720?
730, Montr?al, Canada.
Beno?t Sagot, Lionel Cl?ment, ?ric Villemonte de La
Clergerie, and Pierre Boullier. 2006. The Lefff 2
syntactic lexicon for French: architecture, acquisition,
use. In Proceedings of the 5th Conference on Inter-
national Language Resources and Evaluation (LREC-
2006), pages 1348?1351, Genoa.
Holger Schwenk. 2007. Continuous space language mod-
els. Computer Speech and Language, 21(3):492?518.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational lin-
guistics, 27(4):521?544.
Roland Stuckardt. 2007. Applying backpropagation net-
works to anaphor resolution. In Ant?nio Branco, editor,
Anaphora: Analysis, Algorithms and Applications. 6th
390
Discourse Anaphora and Anaphor Resolution Collo-
quium, DAARC 2007, number 4410 in Lecture Notes
in Artificial Intelligence, pages 107?124, Berlin.
Kater?ina Veselovsk?, Ngu.y Giang Linh, and Michal
Nov?k. 2012. Using Czech-English parallel corpora in
automatic identification of it. In Proceedings of the 5th
Workshop on Building and Using Comparable Corpora,
pages 112?120, Istanbul, Turkey.
391
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 44?54,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Cross-Framework Evaluation for Statistical Parsing
Reut Tsarfaty Joakim Nivre Evelina Andersson
Uppsala University, Box 635, 75126 Uppsala, Sweden
tsarfaty@stp.lingfil.uu.se,{joakim.nivre,evelina.andersson}@lingfil.uu.se
Abstract
A serious bottleneck of comparative parser
evaluation is the fact that different parsers
subscribe to different formal frameworks
and theoretical assumptions. Converting
outputs from one framework to another is
less than optimal as it easily introduces
noise into the process. Here we present a
principled protocol for evaluating parsing
results across frameworks based on func-
tion trees, tree generalization and edit dis-
tance metrics. This extends a previously
proposed framework for cross-theory eval-
uation and allows us to compare a wider
class of parsers. We demonstrate the useful-
ness and language independence of our pro-
cedure by evaluating constituency and de-
pendency parsers on English and Swedish.
1 Introduction
The goal of statistical parsers is to recover a for-
mal representation of the grammatical relations
that constitute the argument structure of natural
language sentences. The argument structure en-
compasses grammatical relationships between el-
ements such as subject, predicate, object, etc.,
which are useful for further (e.g., semantic) pro-
cessing. The parses yielded by different parsing
frameworks typically obey different formal and
theoretical assumptions concerning how to rep-
resent the grammatical relationships in the data
(Rambow, 2010). For example, grammatical rela-
tions may be encoded on top of dependency arcs
in a dependency tree (Mel?c?uk, 1988), they may
decorate nodes in a phrase-structure tree (Marcus
et al 1993; Maamouri et al 2004; Sima?an et
al., 2001), or they may be read off of positions in
a phrase-structure tree using hard-coded conver-
sion procedures (de Marneffe et al 2006). This
diversity poses a challenge to cross-experimental
parser evaluation, namely: How can we evaluate
the performance of these different parsers relative
to one another?
Current evaluation practices assume a set of
correctly annotated test data (or gold standard)
for evaluation. Typically, every parser is eval-
uated with respect to its own formal representa-
tion type and the underlying theory which it was
trained to recover. Therefore, numerical scores
of parses across experiments are incomparable.
When comparing parses that belong to different
formal frameworks, the notion of a single gold
standard becomes problematic, and there are two
different questions we have to answer. First, what
is an appropriate gold standard for cross-parser
evaluation? And secondly, how can we alle-
viate the differences between formal representa-
tion types and theoretical assumptions in order to
make our comparison sound ? that is, to make sure
that we are not comparing apples and oranges?
A popular way to address this has been to
pick one of the frameworks and convert all
parser outputs to its formal type. When com-
paring constituency-based and dependency-based
parsers, for instance, the output of constituency
parsers has often been converted to dependency
structures prior to evaluation (Cer et al 2010;
Nivre et al 2010). This solution has vari-
ous drawbacks. First, it demands a conversion
script that maps one representation type to another
when some theoretical assumptions in one frame-
work may be incompatible with the other one.
In the constituency-to-dependency case, some
constituency-based structures (e.g., coordination
44
and ellipsis) do not comply with the single head
assumption of dependency treebanks. Secondly,
these scripts may be labor intensive to create, and
are available mostly for English. So the evalua-
tion protocol becomes language-dependent.
In Tsarfaty et al(2011) we proposed a gen-
eral protocol for handling annotation discrepan-
cies when comparing parses across different de-
pendency theories. The protocol consists of three
phases: converting all structures into function
trees, for each sentence, generalizing the different
gold standard function trees to get their common
denominator, and employing an evaluation mea-
sure based on tree edit distance (TED) which dis-
cards edit operations that recover theory-specific
structures. Although the protocol is potentially
applicable to a wide class of syntactic represen-
tation types, formal restrictions in the procedures
effectively limit its applicability only to represen-
tations that are isomorphic to dependency trees.
The present paper breaks new ground in the
ability to soundly compare the accuracy of differ-
ent parsers relative to one another given that they
employ different formal representation types and
obey different theoretical assumptions. Our solu-
tion generally confines with the protocol proposed
in Tsarfaty et al(2011) but is re-formalized to
allow for arbitrary linearly ordered labeled trees,
thus encompassing constituency-based as well as
dependency-based representations. The frame-
work in Tsarfaty et al(2011) assumes structures
that are isomorphic to dependency trees, bypass-
ing the problem of arbitrary branching. Here we
lift this restriction, and define a protocol which
is based on generalization and TED measures to
soundly compare the output of different parsers.
We demonstrate the utility of this protocol by
comparing the performance of different parsers
for English and Swedish. For English, our
parser evaluation across representation types al-
lows us to analyze and precisely quantify previ-
ously encountered performance tendencies. For
Swedish we show the first ever evaluation be-
tween dependency-based and constituency-based
parsing models, all trained on the Swedish tree-
bank data. All in all we show that our ex-
tended protocol, which can handle linearly-
ordered labeled trees with arbitrary branch-
ing, can soundly compare parsing results across
frameworks in a representation-independent and
language-independent fashion.
2 Preliminaries: Relational Schemes for
Cross-Framework Parse Evaluation
Traditionally, different statistical parsers have
been evaluated using specially designated evalu-
ation measures that are designed to fit their repre-
sentation types. Dependency trees are evaluated
using attachment scores (Buchholz and Marsi,
2006), phrase-structure trees are evaluated using
ParsEval (Black et al 1991), LFG-based parsers
postulate an evaluation procedure based on f-
structures (Cahill et al 2008), and so on. From a
downstream application point of view, there is no
significance as to which formalism was used for
generating the representation and which learning
methods have been utilized. The bottom line is
simply which parsing framework most accurately
recovers a useful representation that helps to un-
ravel the human-perceived interpretation.
Relational schemes, that is, schemes that en-
code the set of grammatical relations that con-
stitute the predicate-argument structures of sen-
tences, provide an interface to semantic interpre-
tation. They are more intuitively understood than,
say, phrase-structure trees, and thus they are also
more useful for practical applications. For these
reasons, relational schemes have been repeatedly
singled out as an appropriate level of representa-
tion for the evaluation of statistical parsers (Lin,
1995; Carroll et al 1998; Cer et al 2010).
The annotated data which statistical parsers are
trained on encode these grammatical relationships
in different ways. Dependency treebanks provide
a ready-made representation of grammatical rela-
tions on top of arcs connecting the words in the
sentence (Ku?bler et al 2009). The Penn Tree-
bank and phrase-structure annotated resources en-
code partial information about grammatical rela-
tions as dash-features decorating phrase structure
nodes (Marcus et al 1993). Treebanks like Tiger
for German (Brants et al 2002) and Talbanken
for Swedish (Nivre and Megyesi, 2007) explic-
itly map phrase structures onto grammatical rela-
tions using dedicated edge labels. The Relational-
Realizational structures of Tsarfaty and Sima?an
(2008) encode relational networks (sets of rela-
tions) projected and realized by syntactic cate-
gories on top of ordinary phrase-structure nodes.
Function trees, as defined in Tsarfaty et al
(2011), are linearly ordered labeled trees in which
every node is labeled with the grammatical func-
45
(a) -ROOT- John loves Marysbj objroot ? root
sbj
John
hd
loves
obj
Mary
(b) S-root
NP-sbj
NN-hd
John
VP-prd
V-hd
loves
NP-obj
NN-hd
Mary
? root
sbj
hd
John
prd
hd
loves
obj
hd
Mary
(c) S
{sbj,prd,obj}
sbj
NP
{hd}
hd
NN
John
prd
V
loves
obj
NP
{hd}
hd
NN
Mary
? root
sbj
hd
John
prd
loves
obj
hd
Mary
Figure 1: Deterministic conversion into function trees.
The algorithm for extracting a function tree from a de-
pendency tree as in (a) is provided in Tsarfaty et al
(2011). For a phrase-structure tree as in (b) we can re-
place each node label with its function (dash-feature).
In a relational-realizational structure like (c) we can re-
move the projection nodes (sets) and realization nodes
(phrase labels), which leaves the function nodes intact.
tion of the dominated span. Function trees ben-
efit from the same advantages as other relational
schemes, namely that they are intuitive to under-
stand, they provide the interface for semantic in-
terpretation, and thus may be useful for down-
stream applications. Yet they do not suffer from
formal restrictions inherent in dependency struc-
tures, for instance, the single head assumption.
For many formal representation types there ex-
ists a fully deterministic, heuristics-free, proce-
dure mapping them to function trees. In Figure 1
we illustrate some such procedures for a simple
transitive sentence. Now, while all the structures
at the right hand side of Figure 1 are of the same
formal type (function trees), they have different
tree structures due to different theoretical assump-
tions underlying the original formal frameworks.
(t1) root
f1
f2
w
(t2) root
f2
f1
w
(t3) root
{f1,f2}
w
Figure 2: Unary chains in function trees
Once we have converted framework-specific
representations into function trees, the problem of
cross-framework evaluation can potentially be re-
duced to a cross-theory evaluation following Tsar-
faty et al(2011). The main idea is that once
all structures have been converted into function
trees, one can perform a formal operation called
generalization in order to harmonize the differ-
ences between theories, and measure accurately
the distance of parse hypotheses from the gener-
alized gold. The generalization operation defined
in Tsarfaty et al(2011), however, cannot handle
trees that may contain unary chains, and therefore
cannot be used for arbitrary function trees.
Consider for instance (t1) and (t2) in Figure 2.
According to the definition of subsumption in
Tsarfaty et al(2011), (t1) is subsumed by (t2)
and vice versa, so the two trees should be identi-
cal ? but they are not. The interpretation we wish
to give to a function tree such as (t1) is that the
word w has both the grammatical function f1 and
the grammatical function f2. This can be graphi-
cally represented as a set of labels dominating w,
as in (t3). We call structures such as (t3) multi-
function trees. In the next section we formally de-
fine multi-function trees, and then use them to de-
velop our protocol for cross-framework and cross-
theory evaluation.
3 The Proposal: Cross-Framework
Evaluation with Multi-Function Trees
Our proposal is a three-phase evaluation proto-
col in the spirit of Tsarfaty et al(2011). First,
we obtain a formal common ground for all frame-
works in terms of multi-function trees. Then we
obtain a theoretical common ground by means
of tree-generalization on gold trees. Finally, we
calculate TED-based scores that discard the cost
of annotation-specific edits. In this section, we
define multi-function trees and update the tree-
generalization and TED-based metrics to handle
multi-function trees that reflect different theories.
46
Figure 3: The Evaluation Protocol. Different formal frameworks yield different parse and gold formal types.
All types are transformed into multi-function trees. All gold trees enter generalization to yield a new gold for
each sentence. The different ? arcs represent the different edit scripts used for calculating the TED-based scores.
3.1 Defining Multi-Function Trees
An ordinary function tree is a linearly ordered tree
T = (V,A) with yield w1, ..., wn, where internal
nodes are labeled with grammatical function la-
bels drawn from some set L. We use span(v)
and label(v) to denote the yield and label, respec-
tively, of an internal node v. A multi-function tree
is a linearly ordered tree T = (V,A) with yield
w1, ..., wn, where internal nodes are labeled with
sets of grammatical function labels drawn from L
and where v 6= v? implies span(v) 6= span(v?)
for all internal nodes v, v?. We use labels(v) to
denote the label set of an internal node v.
We interpret multi-function trees as encoding
sets of functional constraints over spans in func-
tion trees. Each node v in a multi-function tree
represents a constraint of the form: for each
l ? labels(v), there should be a node v? in the
function tree such that span(v) = span(v?) and
label(v?) = l. Whenever we have a conversion for
function trees, we can efficiently collapse them
into multi-function trees with no unary produc-
tions, and with label sets labeling their nodes.
Thus, trees (t1) and (t2) in Figure 2 would both
be mapped to tree (t3), which encodes the func-
tional constraints encoded in either of them.
For dependency trees, we assume the conver-
sion to function trees defined in Tsarfaty et al
(2011), where head daughters always get the la-
bel ?hd?. For PTB style phrase-structure trees, we
replace the phrase-structure labels with functional
dash-features. In relational-realization structures
we remove projection and realization nodes. De-
terministic conversions exist also for Tiger style
treebanks and frameworks such as LFG, but we
do not discuss them here.1
1All the conversions we use are deterministic and are
defined in graph-theoretic and language-independent terms.
We make them available at http://stp.lingfil.uu.
se/?tsarfaty/unipar/index.html.
3.2 Generalizing Multi-Function Trees
Once we obtain multi-function trees for all the
different gold standard representations in the sys-
tem, we feed them to a generalization operation
as shown in Figure 3. The goal of this opera-
tion is to provide a consensus gold standard that
captures the linguistic structure that the different
gold theories agree on. The generalization struc-
tures are later used as the basis for the TED-based
evaluation. Generalization is defined by means of
subsumption. A multi-function tree subsumes an-
other one if and only if all the constraints defined
by the first tree are also defined by the second tree.
So, instead of demanding equality of labels as in
Tsarfaty et al(2011), we demand set inclusion:
T-Subsumption, denoted vt, is a relation
between multi-function trees that indicates
that a tree pi1 is consistent with and more
general than tree pi2. Formally: pi1 vt pi2
iff for every node n ? pi1 there exists a node
m ? pi2 such that span(n) = span(m) and
labels(n) ? labels(m).
T-Unification, denoted unionsqt, is an operation
that returns the most general tree structure
that contains the information from both input
trees, and fails if such a tree does not exist.
Formally: pi1 unionsqt pi2 = pi3 iff pi1 vt pi3 and
pi2 vt pi3, and for all pi4 such that pi1 vt pi4
and pi2 vt pi4 it holds that pi3 vt pi4.
T-Generalization, denoted ut, is an opera-
tion that returns the most specific tree that
is more general than both trees. Formally,
pi1utpi2 = pi3 iff pi3 vt pi1 and pi3 vt pi2, and
for every pi4 such that pi4 vt pi1 and pi4 vt pi2
it holds that pi4 vt pi3.
The generalization tree contains all nodes that ex-
ist in both trees, and for each node it is labeled by
47
the intersection of the label sets dominating the
same span in both trees. The unification tree con-
tains nodes that exist in one tree or another, and
for each span it is labeled by the union of all label
sets for this span in either tree. If we generalize
two trees and one tree has no specification for la-
bels over a span, it does not share anything with
the label set dominating the same span in the other
tree, and the label set dominating this span in the
generalized tree is empty. If the trees do not agree
on any label for a particular span, the respective
node is similarly labeled with an empty set. When
we wish to unify theories, then an empty set over
a span is unified with any other set dominating the
same span in the other tree, without altering it.
Digression: Using Unification to Merge Infor-
mation From Different Treebanks In Tsarfaty
et al(2011), only the generalization operation
was used, providing the common denominator of
all the gold structures and serving as a common
ground for evaluation. The unification operation
is useful for other NLP tasks, for instance, com-
bining information from two different annotation
schemes or enriching one annotation scheme with
information from a different one. In particular,
we can take advantage of the new framework to
enrich the node structure reflected in one theory
with grammatical functions reflected in an anno-
tation scheme that follows a different theory. To
do so, we define the Tree-Labeling-Unification
operation on multi-function trees.
TL-Unification, denoted unionsqtl, is an opera-
tion that returns a tree that retains the struc-
ture of the first tree and adds labels that ex-
ist over its spans in the second tree. For-
mally: pi1 unionsqtl pi2 = pi3 iff for every node
n ? pi1 there exists a node m ? pi3 such
that span(m) = span(n) and labels(m) =
labels(n) ? labels(pi2, span(n)).
Where labels(pi2, span(n)) is the set of labels of
the node with yield span(n) in pi2 if such a node
exists and ? otherwise. We further discuss the TL-
Unification and its use for data preparation in ?4.
3.3 TED Measures for Multi-Function Trees
The result of the generalization operation pro-
vides us with multi-function trees for each of the
sentences in the test set representing sets of con-
straints on which the different gold theories agree.
We would now like to use distance-based met-
rics in order to measure the gap between the gold
and predicted theories. The idea behind distance-
based evaluation in Tsarfaty et al(2011) is that
recording the edit operations between the native
gold and the generalized gold allows one to dis-
card their cost when computing the cost of a parse
hypothesis turned into the generalized gold. This
makes sure that different parsers do not get penal-
ized, or favored, due to annotation specific deci-
sions that are not shared by other frameworks.
The problem is now that TED is undefined with
respect to multi-function trees because it cannot
handle complex labels. To overcome this, we
convert multi-function trees into sorted function
trees, which are simply function trees in which
any label set is represented as a unary chain of
single-labeled nodes, and the nodes are sorted ac-
cording to the canonical order of their labels.2 In
case of an empty set, a 0-length chain is created,
that is, no node is created over this span. Sorted
function trees prevent reordering nodes in a chain
in one tree to fit the order in another tree, since it
would violate the idea that the set of constraints
over a span in a multi-function tree is unordered.
The edit operations we assume are add-
node(l, i, j) and delete-node(l, i, j) where l ? L
is a grammatical function label and i < j define
the span of a node in the tree. Insertion into a
unary chain must confine with the canonical order
of the labels. Every operation is assigned a cost.
An edit script is a sequence of edit operations that
turns a function tree pi1 into pi2, that is:
ES(pi1, pi2) = ?e1, . . . , ek?
Since all operations are anchored in spans, the se-
quence can be determined to have a unique order
of traversing the tree (say, DFS). Different edit
scripts then only differ in their set of operations
on spans. The edit distance problem is finding the
minimal cost script, that is, one needs to solve:
ES?(pi1, pi2) = min
ES(pi1,pi2)
?
e?ES(pi1,pi2)
cost(e)
In the current setting, when using only add and
delete operations on spans, there is only one edit
script that corresponds to the minimal edit cost.
So, finding the minimal edit script entails finding
a single set of operations turning pi1 into pi2.
2The ordering can be alphabetic, thematic, etc.
48
We can now define ? for the ith framework, as
the error of parsei relative to its native gold stan-
dard goldi and to the generalized gold gen. This
is the edit cost minus the cost of the script turning
parsei into gen intersected with the script turning
goldi into gen. The underlying intuition is that
if an operation that was used to turn parsei into
gen is used to discard theory-specific information
from goldi, its cost should not be counted as error.
?(parsei, goldi, gen) = cost(ES
?(parsei, gen))
?cost(ES?(parsei, gen) ? ES
?(goldi, gen))
In order to turn distance measures into parse-
scores we now normalize the error relative to the
size of the trees and subtract it from a unity. So
the Sentence Score for parsing with framework i
is:
score(parsei, goldi, gen) =
1?
?(parsei, goldi,gen)
|parsei|+ |gen|
Finally, Test-Set Average is defined by macro-
avaraging over all sentences in the test-set:
1?
?|testset|
j=1 ?(parseij , goldij , genj)
?|testset|
j=1 |parseij |+ |genj |
This last formula represents the TEDEVAL metric
that we use in our experiments.
A Note on System Complexity Conversion of
a dependency or a constituency tree into a func-
tion tree is linear in the size of the tree. Our
implementation of the generalization and unifica-
tion operation is an exact, greedy, chart-based al-
gorithm that runs in polynomial time (O(n2) in
n the number of terminals). The TED software
that we utilize builds on the TED efficient algo-
rithm of Zhang and Shasha (1989) which runs in
O(|T1||T2|min(d1, n1)min(d2, n2)) time where
di is the tree degree (depth) and ni is the number
of terminals in the respective tree (Bille, 2005).
4 Experiments
We validate our cross-framework evaluation pro-
cedure on two languages, English and Swedish.
For English, we compare the performance of
two dependency parsers, MaltParser (Nivre et al
2006) and MSTParser (McDonald et al 2005),
and two constituency-based parsers, the Berkeley
parser (Petrov et al 2006) and the Brown parser
(Charniak and Johnson, 2005). All experiments
use Penn Treebank (PTB) data. For Swedish,
we compare MaltParser and MSTParser with two
variants of the Berkeley parser, one trained on
phrase structure trees, and one trained on a vari-
ant of the Relational-Realizational representation
of Tsarfaty and Sima?an (2008). All experiments
use the Talbanken Swedish Treebank (STB) data.
4.1 English Cross-Framework Evaluation
We use sections 02?21 of the WSJ Penn Tree-
bank for training and section 00 for evaluation and
analysis. We use two different native gold stan-
dards subscribing to different theories of encoding
grammatical relations in tree structures:
? THE DEPENDENCY-BASED THEORY is the
theory encoded in the basic Stanford Depen-
dencies (SD) scheme. We obtain the set of
basic stanford dependency trees using the
software of de Marneffe et al(2006) and
train the dependency parsers directly on it.
? THE CONSTITUENCY-BASED THEORY is
the theory reflected in the phrase-structure
representation of the PTB (Marcus et al
1993) enriched with function labels compat-
ible with the Stanford Dependencies (SD)
scheme. We obtain trees that reflect this
theory by TL-Unification of the PTB multi-
function trees with the SD multi-function
trees (PTBunionsqtlSD) as illustrated in Figure 4.
The theory encoded in the multi-function trees
corresponding to SD is different from the one
obtained by our TL-Unification, as may be seen
from the difference between the flat SD multi-
function tree and the result of the PTBunionsqtlSD in
Figure 4. Another difference concerns coordina-
tion structures, encoded as binary branching trees
in SD and as flat productions in the PTBunionsqtlSD.
Such differences are not only observable but also
quantifiable, and using our redefined TED metric
the cross-theory overlap is 0.8571.
The two dependency parsers were trained using
the same settings as in Tsarfaty et al(2011), using
SVMTool (Gime?nez and Ma`rquez, 2004) to pre-
dict part-of-speech tags at parsing time. The two
constituency parsers were used with default set-
tings and were allowed to predict their own part-
of-speech tags. We report three different evalua-
tion metrics for the different experiments:
49
(PTB) S
NP
NN
John
VP
V
loves
NP
NN
Mary
?
John loves
Mary
? ?
?
John
?
?
loves
?
Mary
(SD) -ROOT- John loves Marysbj objroot ? root
sbj
John
hd
loves
obj
Mary
? {root}
{sbj}
John
{hd}
loves
{obj}
Mary
(PTB) unionsqtl (SD) = {root}
{sbj}
John
?
{hd}
loves
{obj}
Mary
Figure 4: Conversion of PTB and SD tree to multi-
function trees, followed by TL-Unification of the trees.
Note that some PTB nodes remain without an SD label.
? LAS/UAS (Buchholz and Marsi, 2006)
? PARSEVAL (Black et al 1991)
? TEDEVAL as defined in Section 3
We use LAS/UAS for dependency parsers that
were trained on the same dependency theory. We
use ParseEval to evaluate phrase-structure parsers
that were trained on PTB trees in which dash-
features and empty traces are removed. We
use our implementation of TEDEVAL to evaluate
parsing results across all frameworks under two
different scenarios:3 TEDEVAL SINGLE evalu-
ates against the native gold multi-function trees.
TEDEVAL MULTIPLE evaluates against the gen-
eralized (cross-theory) multi-function trees. Un-
labeled TEDEVAL scores are obtained by sim-
ply removing all labels from the multi-function
nodes, and using unlabeled edit operations. We
calculate pairwise statistical significance using a
shuffling test with 10K iterations (Cohen, 1995).
Tables 1 and 2 present the results of our cross-
framework evaluation for English Parsing. In the
left column of Table 1 we report ParsEval scores
for constituency-based parsers. As expected, F-
Scores for the Brown parser are higher than the
F-Scores of the Berkeley parser. F-Scores are
however not applicable across frameworks. In
the rightmost column of Table 1 we report the
LAS/UAS results for all parsers. If a parser yields
3Our TedEval software can be downloaded at
http://stp.lingfil.uu.se/?tsarfaty/
unipar/download.html.
a constituency tree, it is converted to and evalu-
ated on SD. Here we see that MST outperforms
Malt, though the differences for labeled depen-
dencies are insignificant. We also observe here a
familiar pattern from Cer et al(2010) and others,
where the constituency parsers significantly out-
perform the dependency parsers after conversion
of their output into dependencies.
The conversion to SD allows one to compare
results across formal frameworks, but not with-
out a cost. The conversion introduces a set of an-
notation specific decisions which may introduce
a bias into the evaluation. In the middle column
of Table 1 we report the TEDEVAL metrics mea-
sured against the generalized gold standard for all
parsing frameworks. We can now confirm that
the constituency-based parsers significantly out-
perform the dependency parsers, and that this is
not due to specific theoretical decisions which are
seen to affect LAS/UAS metrics (Schwartz et al
2011). For the dependency parsers we now see
that Malt outperforms MST on labeled dependen-
cies slightly, but the difference is insignificant.
The fact that the discrepancy in theoretical as-
sumptions between different frameworks indeed
affects the conversion-based evaluation procedure
is reflected in the results we report in Table 2.
Here the leftmost and rightmost columns report
TEDEVAL scores against the own native gold
(SINGLE) and the middle column against the gen-
eralized gold (MULTIPLE). Had the theories
for SD and PTBunionsqtlSD been identical, TEDEVAL
SINGLE and TEDEVAL MULTIPLE would have
been equal in each line. Because of theoretical
discrepancies, we see small gaps in parser perfor-
mance between these cases. Our protocol ensures
that such discrepancies do not bias the results.
4.2 Cross-Framework Swedish Parsing
We use the standard training and test sets of the
Swedish Treebank (Nivre and Megyesi, 2007)
with two gold standards presupposing different
theories:
? THE DEPENDENCY-BASED THEORY is the
dependency version of the Swedish Tree-
bank. All trees are projectivized (STB-Dep).
? THE CONSTITUENCY-BASED THEORY is
the standard Swedish Treebank with gram-
matical function labels on the edges of con-
stituency structures (STB).
50
Formalism PS Trees MF Trees Dep Trees
Theory PTB unionsqlt SD (PTB unionsqlt SD) SD
ut SD
Metrics PARSEVAL TEDEVAL ATTSCORES
MALT N/A U: 0.9525L: 0.9088
U: 0.8962
L: 0.8772
MST N/A U: 0.9549L: 0.9049
U: 0.9059
L: 0.8795
BERKELEY F-Scores0.9096
U: 0.9677
L: 0.9227
U: 0.9254
L: 0.9031
BROWN F-Scores0.9129
U: 0.9702
L: 0.9264
U: 0.9289
L: 0.9057
Table 1: English cross-framework evaluation: Three
measures as applicable to the different schemes. Bold-
face scores are highest in their column. Italic scores
are the highest for dependency parsers in their column.
Formalism PS Trees MF Trees Dep Trees
Theory PTB unionsqlt SD (PTB unionsqlt SD) SD
ut SD
Metrics TEDEVAL TEDEVAL TEDEVAL
SINGLE MULTIPLE SINGLE
MALT N/A U: 0.9525L: 0.9088
U: 0.9524
L: 0.9186
MST N/A U: 0.9549L: 0.9049
U: 0.9548
L: 0.9149
BERKELEY U: 0.9645L: 0.9271
U: 0.9677
L: 0.9227
U: 0.9649
L: 0.9324
BROWN U: 0.9667L: 0.9301
U: 9702
L: 9264
U: 0.9679
L: 0.9362
Table 2: English cross-framework evaluation: TEDE-
VAL scores against gold and generalized gold. Bold-
face scores are highest in their column. Italic scores
are highest for dependency parsers in their column.
Because there are no parsers that can out-
put the complete STB representation including
edge labels, we experiment with two variants of
this theory, one which is obtained by simply re-
moving the edge labels and keeping only the
phrase-structure labels (STB-PS) and one which
is loosely based on the Relational-Realizational
scheme of Tsarfaty and Sima?an (2008) but ex-
cludes the projection set nodes (STB-RR). RR
trees only add function nodes to PS trees, and
it holds that STB-PSutSTB-RR=STB-PS. The
overlap between the theories expressed in multi-
function trees originating from STB-Dep and
STB-RR is 0.7559. Our evaluation protocol takes
into account such discrepancies while avoiding
biases that may be caused due to these differences.
We evaluate MaltParser, MSTParser and two
versions of the Berkeley parser, one trained on
STB-PS and one trained on STB-RR. We use
predicted part-of-speech tags for the dependency
Formalism PS Trees MF Trees Dep Trees
Theory STB STB ut Dep Dep
Metrics PARSEVAL TEDEVAL ATTSCORE
MALT N/A U: 0.9266L: 0.8225
U: 0.8298
L: 0.7782
MST N/A U: 0.9275L: 0.8121
U: 0.8438
L: 0.7824
BKLY/STB-RR F-Score0.7914
U: 0.9281
L: 0.7861 N/A
BKLY/STB-PS F-Score0.7855 N/A N/A
Table 3: Swedish cross-framework evaluation: Three
measures as applicable to the different schemes. Bold-
face scores are the highest in their column.
Formalism PS Trees MF Trees Dep Trees
Theory STB STB ut Dep Dep
Metrics TEDEVAL TEDEVAL TEDEVAL
SINGLE MULTIPLE SINGLE
MALT N/A U: 0.9266L: 0.8225
U: 0.9264
L: 0.8372
MST N/A U: 0.9275L: 0.8121
U: 0.9272
L: 0.8275
BKLY-STB-RR U: 0.9239L: 0.7946
U: 0.9281
L: 0.7861 N/A
Table 4: Swedish cross-framework evaluation: TEDE-
VAL scores against the native gold and the generalized
gold. Boldface scores are the highest in their column.
parsers, using the HunPoS tagger (Megyesi,
2009), but let the Berkeley parser predict its own
tags. We use the same evaluation metrics and pro-
cedures as before. Prior to evaluating RR trees
using ParsEval we strip off the added function
nodes. Prior to evaluating them using TedEval we
strip off the phrase-structure nodes.
Tables 3 and 4 summarize the parsing results
for the different Swedish parsers. In the leftmost
column of table 3 we present the constituency-
based evaluation measures. Interestingly, the
Berkeley RR instantiation performs better than
when training the Berkeley parser on PS trees.
These constituency-based scores however have a
limited applicability, and we cannot use them to
compare constituency and dependency parsers. In
the rightmost column of Table 3 we report the
LAS/UAS results for the two dependency parsers.
Here we see higher performance demonstrated by
MST on both labeled and unlabeled dependen-
cies, but the differences on labeled dependencies
are insignificant. Since there is no automatic pro-
cedure for converting bare-bone phrase-structure
Swedish trees to dependency trees, we cannot use
51
LAS/UAS to compare across frameworks, and we
use TEDEVAL for cross-framework evaluation.
Training the Berkeley parser on RR trees which
encode a mapping of PS nodes to grammatical
functions allows us to compare parse results for
trees belonging to the STB theory with trees obey-
ing the STB-Dep theory. For unlabeled TEDE-
VAL scores, the dependency parsers perform at the
same level as the constituency parser, though the
difference is insignificant. For labeled TEDEVAL
the dependency parsers significantly outperform
the constituency parser. When considering only
the dependency parsers, there is a small advantage
for Malt on labeled dependencies, and an advan-
tage for MST on unlabeled dependencies, but the
latter is insignificant. This effect is replicated in
Table 4 where we evaluate dependency parsers us-
ing TEDEVAL against their own gold theories. Ta-
ble 4 further confirms that there is a gap between
the STB and the STB-Dep theories, reflected in
the scores against the native and generalized gold.
5 Discussion
We presented a formal protocol for evaluating
parsers across frameworks and used it to soundly
compare parsing results for English and Swedish.
Our approach follows the three-phase protocol of
Tsarfaty et al(2011), namely: (i) obtaining a for-
mal common ground for the different representa-
tion types, (ii) computing the theoretical common
ground for each test sentence, and (iii) counting
only what counts, that is, measuring the distance
between the common ground and the parse tree
while discarding annotation-specific edits.
A pre-condition for applying our protocol is the
availability of a relational interpretation of trees in
the different frameworks. For dependency frame-
works this is straightforward, as these relations
are encoded on top of dependency arcs. For con-
stituency trees with an inherent mapping of nodes
onto grammatical relations (Merlo and Musillo,
2005; Gabbard et al 2006; Tsarfaty and Sima?an,
2008), a procedure for reading relational schemes
off of the trees is trivial to implement.
For parsers that are trained on and parse into
bare-bones phrase-structure trees this is not so.
Reading off the relational structure may be more
costly and require interjection of additional theo-
retical assumptions via manually written scripts.
Scripts that read off grammatical relations based
on tree positions work well for configurational
languages such as English (de Marneffe et al
2006) but since grammatical relations are re-
flected differently in different languages (Postal
and Perlmutter, 1977; Bresnan, 2000), a proce-
dure to read off these relations in a language-
independent fashion from phrase-structure trees
does not, and should not, exist (Rambow, 2010).
The crucial point is that even when using ex-
ternal scripts for recovering a relational scheme
for phrase-structure trees, our protocol has a clear
advantage over simply scoring converted trees.
Manually created conversion scripts alter the the-
oretical assumptions inherent in the trees and thus
may bias the results. Our generalization operation
and three-way TED make sure that theory-specific
idiosyncrasies injected through such scripts do
not lead to over-penalizing or over-crediting
theory-specific structural variations.
Certain linguistic structures cannot yet be eval-
uated with our protocol because of the strict as-
sumption that the labeled spans in a parse form a
tree. In the future we plan to extend the protocol
for evaluating structures that go beyond linearly-
ordered trees in order to allow for non-projective
trees and directed acyclic graphs. In addition, we
plan to lift the restriction that the parse yield is
known in advance, in order to allow for evalua-
tion of joint parse-segmentation hypotheses.
6 Conclusion
We developed a protocol for comparing parsing
results across different theories and representa-
tion types which is framework-independent in the
sense that it can accommodate any formal syntac-
tic framework that encodes grammatical relations,
and it is language-independent in the sense that
there is no language specific knowledge encoded
in the procedure. As such, this protocol is ad-
equate for parser evaluation in cross-framework
and cross-language tasks and parsing competi-
tions, and using it across the board is expected
to open new horizons in our understanding of the
strengths and weaknesses of different parsers in
the face of different theories and different data.
Acknowledgments We thank David McClosky,
Marco Khulmann, Yoav Goldberg and three
anonymous reviewers for useful comments. We
further thank Jennifer Foster for the Brown parses
and parameter files. This research is partly funded
by the Swedish National Science Foundation.
52
References
Philip Bille. 2005. A survey on tree edit distance and
related. problems. Theoretical Computer Science,
337:217?239.
Ezra Black, Steven P. Abney, D. Flickenger, Clau-
dia Gdaniec, Ralph Grishman, P. Harrison, Don-
ald Hindle, Robert Ingria, Frederick Jelinek, Ju-
dith L. Klavans, Mark Liberman, Mitchell P. Mar-
cus, Salim Roukos, Beatrice Santorini, and Tomek
Strzalkowski. 1991. A procedure for quantitatively
comparing the syntactic coverage of English gram-
mars. In Proceedings of the DARPA Workshop on
Speech and Natural Language, pages 306?311.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The Tiger
treebank. In Proceedings of TLT.
Joan Bresnan. 2000. Lexical-Functional Syntax.
Blackwell.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL-X, pages 149?164.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Stefan
Riezler, Josef van Genabith, and Andy Way. 2008.
Wide-coverage deep statistical parsing using auto-
matic dependency structure annotation. Computa-
tional Linguistics, 34(1):81?124.
John Carroll, Edward Briscoe, and Antonio Sanfilippo.
1998. Parser evaluation: A survey and a new pro-
posal. In Proceedings of LREC, pages 447?454.
Daniel Cer, Marie-Catherine de Marneffe, Daniel Ju-
rafsky, and Christopher D. Manning. 2010. Pars-
ing to Stanford Dependencies: Trade-offs between
speed and accuracy. In Proceedings of LREC.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of ACL.
Paul Cohen. 1995. Empirical Methods for Artificial
Intelligence. The MIT Press.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, pages 449?454.
Ryan Gabbard, Mitchell Marcus, and Seth Kulick.
2006. Fully parsing the Penn treebank. In Proceed-
ing of HLT-NAACL, pages 184?191.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. SVMTool:
A general POS tagger generator based on support
vector machines. In Proceedings of LREC.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Number 2 in Synthesis
Lectures on Human Language Technologies. Mor-
gan & Claypool Publishers.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In Proceedings
of IJCAI-95, pages 1420?1425.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic treebank:
Building a large-scale annotated Arabic corpus. In
Proceedings of NEMLAR International Conference
on Arabic Language Resources and Tools.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19:313?330.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In HLT ?05:
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 523?530, Morristown, NJ,
USA. Association for Computational Linguistics.
Beata Megyesi. 2009. The open source tagger Hun-
PoS for Swedish. In Proceedings of the 17th Nordic
Conference of Computational Linguistics (NODAL-
IDA), pages 239?241.
Igor Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
Paola Merlo and Gabriele Musillo. 2005. Accurate
function parsing. In Proceedings of EMNLP, pages
620?627.
Joakim Nivre and Beata Megyesi. 2007. Bootstrap-
ping a Swedish Treebank using cross-corpus har-
monization and annotation projection. In Proceed-
ings of TLT.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data-driven parser-generator for de-
pendency parsing. In Proceedings of LREC, pages
2216?2219.
Joakim Nivre, Laura Rimell, Ryan McDonald, and
Carlos Go?mez-Rodr??guez. 2010. Evaluation of de-
pendency parsers on unbounded dependencies. In
Proceedings of COLING, pages 813?821.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of ACL.
Paul M. Postal and David M. Perlmutter. 1977. To-
ward a universal characterization of passivization.
In Proceedings of the 3rd Annual Meeting of the
Berkeley Linguistics Society, pages 394?417.
Owen Rambow. 2010. The simple truth about de-
pendency and phrase structure representations: An
opinion piece. In Proceedings of HLT-ACL, pages
337?340.
Roy Schwartz, Omri Abend, Roi Reichart, and Ari
Rappoport. 2011. Neutralizing linguistically prob-
lematic annotations in unsupervised dependency
parsing evaluation. In Proceedings of ACL, pages
663?672.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altman,
and Noa Nativ. 2001. Building a Tree-Bank for
Modern Hebrew Text. In Traitement Automatique
des Langues.
53
Reut Tsarfaty and Khalil Sima?an. 2008. Relational-
Realizational parsing. In Proceedings of CoLing.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2011. Evaluating dependency parsing: Robust and
heuristics-free cross-framework evaluation. In Pro-
ceedings of EMNLP.
Kaizhong Zhang and Dennis Shasha. 1989. Sim-
ple fast algorithms for the editing distance between
trees and related problems. In SIAM Journal of
Computing, volume 18, pages 1245?1262.
54
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 58?62,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
MaltOptimizer: An Optimization Tool for MaltParser
Miguel Ballesteros
Complutense University of Madrid
Spain
miballes@fdi.ucm.es
Joakim Nivre
Uppsala University
Sweden
joakim.nivre@lingfil.uu.se
Abstract
Data-driven systems for natural language
processing have the advantage that they can
easily be ported to any language or domain
for which appropriate training data can be
found. However, many data-driven systems
require careful tuning in order to achieve
optimal performance, which may require
specialized knowledge of the system. We
present MaltOptimizer, a tool developed to
facilitate optimization of parsers developed
using MaltParser, a data-driven dependency
parser generator. MaltOptimizer performs
an analysis of the training data and guides
the user through a three-phase optimization
process, but it can also be used to perform
completely automatic optimization. Exper-
iments show that MaltOptimizer can im-
prove parsing accuracy by up to 9 percent
absolute (labeled attachment score) com-
pared to default settings. During the demo
session, we will run MaltOptimizer on dif-
ferent data sets (user-supplied if possible)
and show how the user can interact with the
system and track the improvement in pars-
ing accuracy.
1 Introduction
In building NLP applications for new languages
and domains, we often want to reuse components
for tasks like part-of-speech tagging, syntactic
parsing, word sense disambiguation and semantic
role labeling. From this perspective, components
that rely on machine learning have an advantage,
since they can be quickly adapted to new settings
provided that we can find suitable training data.
However, such components may require careful
feature selection and parameter tuning in order to
give optimal performance, a task that can be dif-
ficult for application developers without special-
ized knowledge of each component.
A typical example is MaltParser (Nivre et al
2006), a widely used transition-based dependency
parser with state-of-the-art performance for many
languages, as demonstrated in the CoNLL shared
tasks on multilingual dependency parsing (Buch-
holz and Marsi, 2006; Nivre et al 2007). Malt-
Parser is an open-source system that offers a wide
range of parameters for optimization. It imple-
ments nine different transition-based parsing al-
gorithms, each with its own specific parameters,
and it has an expressive specification language
that allows the user to define arbitrarily complex
feature models. Finally, any combination of pars-
ing algorithm and feature model can be combined
with a number of different machine learning al-
gorithms available in LIBSVM (Chang and Lin,
2001) and LIBLINEAR (Fan et al 2008). Just
running the system with default settings when
training a new parser is therefore very likely to
result in suboptimal performance. However, se-
lecting the best combination of parameters is a
complicated task that requires knowledge of the
system as well as knowledge of the characteris-
tics of the training data.
This is why we present MaltOptimizer, a tool
for optimizing MaltParser for a new language
or domain, based on an analysis of the train-
ing data. The optimization is performed in three
phases: data analysis, parsing algorithm selec-
tion, and feature selection. The tool can be run
in ?batch mode? to perform completely automatic
optimization, but it is also possible for the user to
manually tune parameters after each of the three
phases. In this way, we hope to cater for users
58
without specific knowledge of MaltParser, who
can use the tool for black box optimization, as
well as expert users, who can use it interactively
to speed up optimization. Experiments on a num-
ber of data sets show that using MaltOptimizer for
completely automatic optimization gives consis-
tent and often substantial improvements over the
default settings for MaltParser.
The importance of feature selection and param-
eter optimization has been demonstrated for many
NLP tasks (Kool et al 2000; Daelemans et al
2003), and there are general optimization tools for
machine learning, such as Paramsearch (Van den
Bosch, 2004). In addition, Nilsson and Nugues
(2010) has explored automatic feature selection
specifically for MaltParser, but MaltOptimizer is
the first system that implements a complete cus-
tomized optimization process for this system.
In the rest of the paper, we describe the opti-
mization process implemented in MaltOptimizer
(Section 2), report experiments (Section 3), out-
line the demonstration (Section 4), and conclude
(Section 5). A more detailed description of Malt-
Optimizer with additional experimental results
can be found in Ballesteros and Nivre (2012).
2 The MaltOptimizer System
MaltOptimizer is written in Java and implements
an optimization procedure for MaltParser based
on the heuristics described in Nivre and Hall
(2010). The system takes as input a training
set, consisting of sentences annotated with depen-
dency trees in CoNLL data format,1 and outputs
an optimized MaltParser configuration together
with an estimate of the final parsing accuracy.
The evaluation metric that is used for optimiza-
tion by default is the labeled attachment score
(LAS) excluding punctuation, that is, the percent-
age of non-punctuation tokens that are assigned
the correct head and the correct label (Buchholz
and Marsi, 2006), but other options are available.
For efficiency reasons, MaltOptimizer only ex-
plores linear multiclass SVMs in LIBLINEAR.
2.1 Phase 1: Data Analysis
After validating that the data is in valid CoNLL
format, using the official validation script from
the CoNLL-X shared task,2 the system checks the
1http://ilk.uvt.nl/conll/#dataformat
2http://ilk.uvt.nl/conll/software.html#validate
minimum Java heap space needed given the size
of the data set. If there is not enough memory
available on the current machine, the system in-
forms the user and automatically reduces the size
of the data set to a feasible subset. After these ini-
tial checks, MaltOptimizer checks the following
characteristics of the data set:
1. Number of words/sentences.
2. Existence of ?covered roots? (arcs spanning
tokens with HEAD = 0).
3. Frequency of labels used for tokens with
HEAD = 0.
4. Percentage of non-projective arcs/trees.
5. Existence of non-empty feature values in the
LEMMA and FEATS columns.
6. Identity (or not) of feature values in the
CPOSTAG and POSTAG columns.
Items 1?3 are used to set basic parameters in the
rest of phase 1 (see below); 4 is used in the choice
of parsing algorithm (phase 2); 5 and 6 are rele-
vant for feature selection experiments (phase 3).
If there are covered roots, the system checks
whether accuracy is improved by reattaching
such roots in order to eliminate spurious non-
projectivity. If there are multiple labels for to-
kens with HEAD=0, the system tests which label
is best to use as default for fragmented parses.
Given the size of the data set, the system sug-
gests different validation strategies during phase
1. If the data set is small, it recommends us-
ing 5-fold cross-validation during subsequent op-
timization phases. If the data set is larger, it rec-
ommends using a single development set instead.
But the user can override either recommendation
and select either validation method manually.
When these checks are completed, MaltOpti-
mizer creates a baseline option file to be used as
the starting point for further optimization. The
user is given the opportunity to edit this option
file and may also choose to stop the process and
continue with manual optimization.
2.2 Phase 2: Parsing Algorithm Selection
MaltParser implements three groups of transition-
based parsing algorithms:3 (i) Nivre?s algorithms
(Nivre, 2003; Nivre, 2008), (ii) Covington?s algo-
rithms (Covington, 2001; Nivre, 2008), and (iii)
3Recent versions of MaltParser contains additional algo-
rithms that are currently not handled by MaltOptimizer.
59
Figure 1: Decision tree for best projective algorithm.
Figure 2: Decision tree for best non-projective algo-
rithm (+PP for pseudo-projective parsing).
Stack algorithms (Nivre, 2009; Nivre et al 2009)
Both the Covington group and the Stack group
contain algorithms that can handle non-projective
dependency trees, and any projective algorithm
can be combined with pseudo-projective parsing
to recover non-projective dependencies in post-
processing (Nivre and Nilsson, 2005).
In phase 2, MaltOptimizer explores the parsing
algorithms implemented in MaltParser, based on
the data characteristics inferred in the first phase.
In particular, if there are no non-projective depen-
dencies in the training set, then only projective
algorithms are explored, including the arc-eager
and arc-standard versions of Nivre?s algorithm,
the projective version of Covington?s projective
parsing algorithm and the projective Stack algo-
rithm. The system follows a decision tree consid-
ering the characteristics of each algorithm, which
is shown in Figure 1.
On the other hand, if the training set con-
tains a substantial amount of non-projective de-
pendencies, MaltOptimizer instead tests the non-
projective versions of Covington?s algorithm and
the Stack algorithm (including a lazy and an eager
variant), and projective algorithms in combination
with pseudo-projective parsing. The system then
follows the decision tree shown in Figure 2.
If the number of trees containing non-
projective arcs is small but not zero, the sys-
tem tests both projective algorithms and non-
projective algorithms, following the decision trees
in Figure 1 and Figure 2 and picking the algorithm
that gives the best results after traversing both.
Once the system has finished testing each of the
algorithms with default settings, MaltOptimizer
tunes some specific parameters of the best per-
forming algorithm and creates a new option file
for the best configuration so far. The user is again
given the opportunity to edit the option file (or
stop the process) before optimization continues.
2.3 Phase 3: Feature Selection
In the third phase, MaltOptimizer tunes the fea-
ture model given all the parameters chosen so far
(especially the parsing algorithm). It starts with
backward selection experiments to ensure that all
features in the default model for the given pars-
ing algorithm are actually useful. In this phase,
features are omitted as long as their removal does
not decrease parsing accuracy. The system then
proceeds with forward selection experiments, try-
ing potentially useful features one by one. In this
phase, a threshold of 0.05% is used to determine
whether an improvement in parsing accuracy is
sufficient for the feature to be added to the model.
Since an exhaustive search for the best possible
feature model is impossible, the system relies on
a greedy optimization strategy using heuristics de-
rived from proven experience (Nivre and Hall,
2010). The major steps of the forward selection
experiments are the following:4
1. Tune the window of POSTAG n-grams over
the parser state.
2. Tune the window of FORM features over the
parser state.
3. Tune DEPREL and POSTAG features over
the partially built dependency tree.
4. Add POSTAG and FORM features over the
input string.
5. Add CPOSTAG, FEATS, and LEMMA fea-
tures if available.
6. Add conjunctions of POSTAG and FORM
features.
These six steps are slightly different depending
on which algorithm has been selected as the best
in phase 2, because the algorithms have different
parsing orders and use different data structures,
4For an explanation of the different feature columns such
as POSTAG, FORM, etc., see Buchholz and Marsi (2006) or
see http://ilk.uvt.nl/conll/#dataformat
60
Language Default Phase 1 Phase 2 Phase 3 Diff
Arabic 63.02 63.03 63.84 65.56 2.54
Bulgarian 83.19 83.19 84.00 86.03 2.84
Chinese 84.14 84.14 84.95 84.95 0.81
Czech 69.94 70.14 72.44 78.04 8.10
Danish 81.01 81.01 81.34 83.86 2.85
Dutch 74.77 74.77 78.02 82.63 7.86
German 82.36 82.36 83.56 85.91 3.55
Japanese 89.70 89.70 90.92 90.92 1.22
Portuguese 84.11 84.31 84.75 86.52 2.41
Slovene 66.08 66.52 68.40 71.71 5.63
Spanish 76.45 76.45 76.64 79.38 2.93
Swedish 83.34 83.34 83.50 84.09 0.75
Turkish 57.79 57.79 58.29 66.92 9.13
Table 1: Labeled attachment score per phase and with
comparison to default settings for the 13 training sets
from the CoNLL-X shared task (Buchholz and Marsi,
2006).
but the steps are roughly equivalent at a certain
level of abstraction. After the feature selection
experiments are completed, MaltOptimizer tunes
the cost parameter of the linear SVM using a sim-
ple stepwise search. Finally, it creates a complete
configuration file that can be used to train Malt-
Parser on the entire data set. The user may now
continue to do further optimization manually.
3 Experiments
In order to assess the usefulness and validity of
the optimization procedure, we have run all three
phases of the optimization on all the 13 data sets
from the CoNLL-X shared task on multilingual
dependency parsing (Buchholz and Marsi, 2006).
Table 1 shows the labeled attachment scores with
default settings and after each of the three opti-
mization phases, as well as the difference between
the final configuration and the default.5
The first thing to note is that the optimization
improves parsing accuracy for all languages with-
out exception, although the amount of improve-
ment varies considerably from about 1 percentage
point for Chinese, Japanese and Swedish to 8?9
points for Dutch, Czech and Turkish. For most
languages, the greatest improvement comes from
feature selection in phase 3, but we also see sig-
5Note that these results are obtained using 80% of the
training set for training and 20% as a development test set,
which means that they are not comparable to the test results
from the original shared task, which were obtained using the
entire training set for training and a separate held-out test set
for evaluation.
nificant improvement from phase 2 for languages
with a substantial amount of non-projective de-
pendencies, such as Czech, Dutch and Slovene,
where the selection of parsing algorithm can be
very important. The time needed to run the op-
timization varies from about half an hour for the
smaller data sets to about one day for very large
data sets like the one for Czech.
4 System Demonstration
In the demonstration, we will run MaltOptimizer
on different data sets and show how the user can
interact with the system while keeping track of
improvements in parsing accuracy. We will also
explain how to interpret the output of the system,
including the final feature specification model, for
users that are not familiar with MaltParser. By re-
stricting the size of the input data set, we can com-
plete the whole optimization procedure in 10?15
minutes, so we expect to be able to complete a
number of cycles with different members of the
audience. We will also let the audience contribute
their own data sets for optimization, provided that
they are in CoNLL format.6
5 Conclusion
MaltOptimizer is an optimization tool for Malt-
Parser, which is primarily aimed at application
developers who wish to adapt the system to a
new language or domain and who do not have
expert knowledge about transition-based depen-
dency parsing. Another potential user group con-
sists of researchers who want to perform compar-
ative parser evaluation, where MaltParser is often
used as a baseline system and where the use of
suboptimal parameter settings may undermine the
validity of the evaluation. Finally, we believe the
system can be useful also for expert users of Malt-
Parser as a way of speeding up optimization.
Acknowledgments
The first author is funded by the Spanish Ministry
of Education and Science (TIN2009-14659-C03-
01 Project), Universidad Complutense de Madrid
and Banco Santander Central Hispano (GR58/08
Research Group Grant). He is under the support
of the NIL Research Group (http://nil.fdi.ucm.es)
from the same university.
6The system is available for download under an open-
source license at http://nil.fdi.ucm.es/maltoptimizer
61
References
Miguel Ballesteros and Joakim Nivre. 2012. MaltOp-
timizer: A System for MaltParser Optimization. In
Proceedings of the Eighth International Conference
on Language Resources and Evaluation (LREC).
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the 10th Conference on Computa-
tional Natural Language Learning (CoNLL), pages
149?164.
Chih-Chung Chang and Chih-Jen Lin, 2001.
LIBSVM: A Library for Support Vec-
tor Machines. Software available at
http://www.csie.ntu.edu.tw/?cjlin/libsvm.
Michael A. Covington. 2001. A fundamental algo-
rithm for dependency parsing. In Proceedings of
the 39th Annual ACM Southeast Conference, pages
95?102.
Walter Daelemans, Ve?ronique Hoste, Fien De Meul-
der, and Bart Naudts. 2003. Combined optimiza-
tion of feature selection and algorithm parameters
in machine learning of language. In Nada Lavrac,
Dragan Gamberger, Hendrik Blockeel, and Ljupco
Todorovski, editors, Machine Learning: ECML
2003, volume 2837 of Lecture Notes in Computer
Science. Springer.
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
C.-J. Lin. 2008. LIBLINEAR: A library for large
linear classification. Journal of Machine Learning
Research, 9:1871?1874.
Anne Kool, Jakub Zavrel, and Walter Daelemans.
2000. Simultaneous feature selection and param-
eter optimization for memory-based natural lan-
guage processing. In A. Feelders, editor, BENE-
LEARN 2000. Proceedings of the Tenth Belgian-
Dutch Conference on Machine Learning, pages 93?
100. Tilburg University, Tilburg.
Peter Nilsson and Pierre Nugues. 2010. Automatic
discovery of feature sets for dependency parsing. In
COLING, pages 824?832.
Joakim Nivre and Johan Hall. 2010. A quick guide
to MaltParser optimization. Technical report, malt-
parser.org.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In Proceedings of
the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL), pages 99?106.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data-driven parser-generator for de-
pendency parsing. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (LREC), pages 2216?2219.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proceedings of the CoNLL
Shared Task of EMNLP-CoNLL 2007, pages 915?
932.
Joakim Nivre, Marco Kuhlmann, and Johan Hall.
2009. An improved oracle for dependency parsing
with online reordering. In Proceedings of the 11th
International Conference on Parsing Technologies
(IWPT?09), pages 73?76.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technolo-
gies (IWPT), pages 149?160.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34:513?553.
Joakim Nivre. 2009. Non-projective dependency
parsing in expected linear time. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP
(ACL-IJCNLP), pages 351?359.
Antal Van den Bosch. 2004. Wrapped progressive
sampling search for optimizing learning algorithm
parameters. In Proceedings of the 16th Belgian-
Dutch Conference on Artificial Intelligence.
62
Erratum
In Section 5 of the article ?Dependency Parsing of Turkish? by Gu?ls?en Eryig?it, Joakim
Nivre, and Kemal Oflazer (September 2008, Vol. 34, No. 3: 357?389), some abbreviations
were misinterpreted during the copyediting process.
The third sentence of Section 5.2 should be as follows: ?We use an unlexicalized
feature model where the parser uses only the minor part-of-speech category (as POS)
and dependency type of tokens (as DEP) and compare the results with the probabilistic
parser.?
The first sentence of the second paragraph of Section 5.2.1 should start as follows:
?We take the minor part-of-speech category. . . .?
The ?POS? abbreviation used on page 20 should be read as ?minor part-of-
speech,? and the ?POS? abbreviations on pages 21, 26, 27, and 28 should be read as
?part-of-speech.?
? 2008 Association for Computational Linguistics
This article has been cited by:
Analyzing and Integrating
Dependency Parsers
Ryan McDonald?
Google Inc.
Joakim Nivre??
Uppsala University
There has been a rapid increase in the volume of research on data-driven dependency parsers in
the past five years. This increase has been driven by the availability of treebanks in a wide variety
of languages?due in large part to the CoNLL shared tasks?as well as the straightforward
mechanisms by which dependency theories of syntax can encode complex phenomena in free word
order languages. In this article, our aim is to take a step back and analyze the progress that has
been made through an analysis of the two predominant paradigms for data-driven dependency
parsing, which are often called graph-based and transition-based dependency parsing. Our
analysis covers both theoretical and empirical aspects and sheds light on the kinds of errors each
type of parser makes and how they relate to theoretical expectations. Using these observations,
we present an integrated system based on a stacking learning framework and show that such a
system can learn to overcome the shortcomings of each non-integrated system.
1. Introduction
Syntactic dependency representations have a long history in descriptive and theoretical
linguistics and many formal models have been advanced, most notably Word Gram-
mar (Hudson 1984), Meaning-Text Theory (Mel?c?uk 1988), Functional Generative De-
scription (Sgall, Hajic?ova?, and Panevova? 1986), and Constraint Dependency Grammar
(Maruyama 1990). Common to all theories is the notion of directed syntactic depen-
dencies between the words of a sentence, an example of which is given in Figure 1 for
the sentence A hearing is scheduled on the issue today, which has been extracted from the
Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993). A dependency graph of
a sentence represents each word and its syntactic modifiers through labeled directed
arcs, where each arc label comes from some finite set representing possible syntactic
roles. Returning to our example in Figure 1, we can see multiple instances of labeled
dependency relations such as the one from the finite verb is to hearing labeled SBJ
indicating that hearing is the head of the syntactic subject of the finite verb. An artificial
word has been inserted at the beginning of the sentence that will always serve as the
single root of the graph and is primarily a means to simplify computation.
? 76 Ninth Ave., New York, NY 10011. E-mail: ryanmcd@google.com.
?? Department of Linguistics and Philology, Box 635, SE-75126 Uppsala, Sweden.
E-mail: joakim.nivre@lingfil.uu.se.
Submission received: 25 August 2009; revised submission received: 20 August 2010; accepted for publication:
7 October 2010.
? 2011 Association for Computational Linguistics
Computational Linguistics Volume 37, Number 1
Figure 1
Dependency graph for an English sentence.
Syntactic dependency graphs have recently gained a wide interest in the computa-
tional linguistics community and have been successfully employed for many problems
ranging from machine translation (Ding and Palmer 2004) to ontology construction
(Snow, Jurafsky, and Ng 2005). A primary advantage of dependency representations
is that they have a natural mechanism for representing discontinuous constructions,
which arise due to long-distance dependencies or in languages where grammatical
relations are often signaled by morphology instead of word order. This is undoubt-
edly one of the reasons for the emergence of dependency parsers for a wide range of
languages (Buchholz and Marsi 2006; Nivre et al 2007). Thus, the example in Figure 1
contains an instance of a discontinuous construction through the subgraph rooted at
the word hearing. Specifically, the dependency arc from hearing to on spans the words
is and scheduled, which are not nodes in this subgraph. An arc of this kind is said to be
non-projective.
In this article we focus on a common paradigm called data-driven dependency
parsing, which encompasses parsing systems that learn to produce dependency graphs
for sentences from a corpus of sentences annotated with dependency graphs. The
advantage of such models is that they are easily ported to any domain or language
in which annotated resources exist. Many data-driven parsing systems are grammar-
less, in that they do not assume the existence of a grammar that defines permissible
sentences of the language. Instead, the goal of most data-driven parsing systems is to
discriminate good parses from bad for a given sentence, regardless of its grammaticality.
Alternatively, one can view such systems as parsers for a grammar that induces the
language of all strings.
The rise of statistical methods in natural language processing coupled with the
availability of dependency annotated corpora for multiple languages?most notably
from the 2006 and 2007 CoNLL shared tasks (Buchholz and Marsi 2006; Nivre et al
2007)?has led to a boom in research on data-driven dependency parsing. Making sense
of this work is a challenging problem, but an important one if the field is to continue to
make advances. Of the many important questions to be asked, three are perhaps most
crucial at this stage in the development of parsers:
1. How can we formally categorize the different approaches to data-driven
dependency parsing?
2. Can we characterize the kinds of errors each category of parser makes
through an empirical analysis?
3. Can we benefit from such an error analysis and build improved parsers?
The organizers of the CoNLL-X shared task on dependency parsing (Buchholz and
Marsi 2006) point out that there are currently two dominant approaches for data-driven
198
McDonald and Nivre Analyzing and Integrating Dependency Parsers
dependency parsing. The first category parameterizes models over dependency sub-
graphs and learns these parameters to globally score correct graphs above incorrect
ones. Inference is also global, in that systems attempt to find the highest scoring graph
among the set of all graphs. We call such systems graph-based parsing models to
reflect the fact that parameterization is over the graph. Graph-based models are mainly
associated with the pioneering work of Eisner (Eisner 1996), as well as McDonald and
colleagues (McDonald, Crammer, and Pereira 2005; McDonald et al 2005; McDonald
and Pereira 2006; McDonald, Lerman, and Pereira 2006) and others (Riedel, C?ak?c?, and
Meza-Ruiz 2006; Carreras 2007; Koo et al 2007; Nakagawa 2007; Smith and Smith 2007).
The second category of parsing systems parameterizes models over transitions from one
state to another in an abstract state-machine. Parameters in these models are typically
learned using standard classification techniques that learn to predict one transition from
a set of permissible transitions given a state history. Inference is local, in that systems
start in a fixed initial state and greedily construct the graph by taking the highest
scoring transitions at each state entered until a termination condition is met. We call
such systems transition-based parsing models to reflect the fact that parameterization
is over possible state transitions. Transition-based models have been promoted by
the groups of Matsumoto (Kudo and Matsumoto 2002; Yamada and Matsumoto 2003;
Cheng, Asahara, and Matsumoto 2006), Nivre (Nivre, Hall, and Nilsson 2004; Nivre and
Nilsson 2005; Nivre et al 2006), and others (Attardi 2006; Attardi and Ciaramita 2007;
Johansson and Nugues 2007; Duan, Zhao, and Xu 2007; Titov and Henderson 2007a,
2007b).
It is important to note that there is no a priori reason why a graph-based pa-
rameterization should require global learning and inference, and a transition-based
parameterization would necessitate local learning and greedy inference. Nevertheless,
as observed by Buchholz and Marsi (2006), it is striking that recent work on data-driven
dependency parsing has been dominated by global, exhaustive, graph-based models, on
the one hand, and local, greedy, transition-based models, on the other. Therefore, a careful
comparative analysis of these model types appears highly relevant, and this is what we
will try to provide in this article. For convenience, we will use the shorthand terms
?graph-based? and ?transition-based? for these models, although both graph-based
and transition-based parameterizations can be (and have been) combined with different
types of learning and inference. For example, the system described by Zhang and Clark
(2008) could be characterized as a transition-based model with global learning, and the
ensemble system of Zeman and Z?abokrtsky` (2005) as a graph-based model with greedy
inference.
Perhaps the most interesting reason to study the canonical graph-based and
transition-based models is that even though they appear to be quite different theoret-
ically (see Section 2), recent empirical studies show that both obtain similar parsing
accuracies on a variety of languages. For example, Table 1 shows the results of the two
top performing systems in the CoNLL-X shared task, those of McDonald, Lerman, and
Pereira (2006) (graph-based) and Nivre et al (2006) (transition-based), which exhibit
no statistically significant difference in accuracy when averaged across all languages.
This naturally leads us to our Question 2, that is, can we empirically characterize the
errors of these systems to understand whether, in practice, these errors are the same
or distinct? Towards this end, Section 2 describes in detail the theoretical properties
and expectations of these two parsing systems and Section 4 provides a fine-grained
error analysis of each system on the CoNLL-X shared task data sets (Buchholz and
Marsi 2006). The result of this analysis strongly suggests that (1) the two systems do
make different, yet complementary, errors, which lends support to the categorization of
199
Computational Linguistics Volume 37, Number 1
Table 1
Labeled parsing accuracy for top-scoring systems at CoNLL-X (Buchholz and Marsi 2006).
Language Graph-based Transition-based
(McDonald, Lerman, and Pereira 2006) (Nivre et al 2006)
Arabic 66.91 66.71
Bulgarian 87.57 87.41
Chinese 85.90 86.92
Czech 80.18 78.42
Danish 84.79 84.77
Dutch 79.19 78.59
German 87.34 85.82
Japanese 90.71 91.65
Portuguese 86.82 87.60
Slovene 73.44 70.30
Spanish 82.25 81.29
Swedish 82.55 84.58
Turkish 63.19 65.68
Average 80.83 80.75
parsers as graph-based and transition-based, and (2) the errors made by each system
are directly correlated with our expectations, based on their theoretical underpinnings.
This leads to our Question 3: Can we use these insights to integrate parsers and
achieve improved accuracies? In Section 5 we consider a simple way of integrating
graph-based and transition-based models in order to exploit their complementary
strengths and thereby improve parsing accuracy beyond what is possible by either
model in isolation. The method integrates the two models by allowing the output of one
model to define features for the other, which is commonly called ?classifier stacking.?
This method is simple?requiring only the definition of new features?and robust by
allowing a model to learn relative to the predictions of the other. More importantly, we
rerun the error analysis and show that the integrated models do indeed take advantage
of the complementary strengths of both the graph-based and transition-based parsing
systems.
Combining the strengths of different machine learning systems, and even parsing
systems, is by no means new as there are a number of previous studies that have looked
at combining phrase-structure parsers (Henderson and Brill 1999), dependency parsers
(Zeman and Z?abokrtsky` 2005), or both (McDonald 2006). Of particular note is past work
on combining graph-based and transition-based dependency parsers. Sagae and Lavie
(2006) present a system that combines multiple transition-based parsers with a single
graph-based parser by weighting each potential dependency relation by the number of
parsers that predicted it. A final dependency graph is predicted by using spanning tree
inference algorithms from the graph-based parsing literature (McDonald et al 2005).
Sagae and Lavie report improvements of up to 1.7 percentage points over the best single
parser when combining three transition-based models and one graph-based model for
unlabeled dependency parsing, evaluated on data from the Penn Treebank. The same
technique was used by Hall et al (2007) to combine six transition-based parsers in the
best performing system in the CoNLL 2007 shared task.
Zhang and Clark (2008) propose a parsing system that uses global learning coupled
with beam search over a transition-based backbone incorporating both graph-based
200
McDonald and Nivre Analyzing and Integrating Dependency Parsers
and transition-based features, that is, features over both sub-graphs and transitions.
Huang and Sagae (2010) go even further and show how transition-based parsing can be
tabularized to allow for dynamic programming, which in turn permits an exponentially
larger search space. Martins et al (2008) present a method for integrating graph-based
and transition-based parsers based on stacking, which is similar to the approach taken
in this work. Other studies have tried to overcome the weaknesses of parsing models
by changing the underlying model structure directly. For example, Hall (2007), Riedel,
C?ak?c?, and Meza-Ruiz (2006), Nakagawa (2007), Smith and Eisner (2008), and Martins,
Smith, and Xing (2009) attempt to overcome local restrictions in feature scope for graph-
based parsers through both approximations and exact solutions with integer linear
programming.
Our work differs from past studies in that we attempt to quantify exactly the
types of errors these parsers make, tie them to their theoretical expectations, and show
that integrating graph-based and transition-based parsers not only increases overall
accuracy, but does so directly exploiting the strengths of each system. Thus, this is the
first large-scale error analysis of modern data-driven dependency parsers.1 The rest
of the article is structured as follows: Section 2 describes canonical graph-based and
transition-based parsing systems and discusses their theoretical benefits and limitations
with respect to one another; Section 3 introduces the experimental setup based on the
CoNLL-X shared task data sets that incorporate dependency treebanks from 13 diverse
languages; Section 4 gives a fine-grained error analysis for the two parsers in this setup;
Section 5 describes a stacking-based dependency parser combination framework;
Section 6 evaluates the stacking-based parsers in comparison to the original systems
with a detailed error analysis; we conclude in Section 7.
2. Two Models for Dependency Parsing
In this section we introduce central notation and define canonical graph-based and
transition-based dependency parsing at an abstract level. We further compare and
contrast their theoretical underpinnings with an eye to understanding the kinds of
errors each system is likely to make in practice.
2.1 Preliminaries
Let L = {l1, . . . , l|L|} be a set of permissible arc labels. Let x = w0, w1, . . . , wn be an input
sentence where w0 = ROOT. Formally, a dependency graph for an input sentence x is a
labeled directed graph G = (V, A) consisting of a set of nodes V and a set of labeled
directed arcs A ? V ? V ? L; that is, if (i, j, l) ? A for i, j ? V and l ? L, then there is
an arc from node i to node j with label l in the graph. In terms of standard linguistic
dependency theory nomenclature, we say that (i, j, l) ? A if there is a dependency with
head wi, dependent wj, and syntactic role l.
A dependency graph G for sentence x must satisfy the following properties:
1. V = {0, 1, . . . , n}.
2. If (i, j, l) ? A, then j = 0.
1 This work has previously been published partially in McDonald and Nivre (2007) and Nivre and
McDonald (2008).
201
Computational Linguistics Volume 37, Number 1
3. If (i, j, l) ? A, then for all arcs (i?, j, l?) ? A, i = i? and l = l?.
4. For all j ? V ? {0}, either (0, j, l) for some l ? L or there is a non-empty
sequence of nodes i1, . . . , im ? V and labels l1, . . . , lm+1 ? L such that
(0, i1, l1),(i1, i2, l2), . . . , (im, j, lm+1)?A.
The first constraint states that the dependency graph spans the entire input. The sec-
ond constraint states that the node 0 is a root. The third constraint states that each
node has at most one incoming arc in the graph. The final constraint states that the
graph is connected through directed paths from the node 0 to every other node in
the graph. It is not difficult to show that a dependency graph satisfying these con-
straints is in fact a directed tree originating out of the root node 0 and we will use
the term dependency tree to refer to any valid dependency graph. The characterization
of syntactic dependency graphs as trees is consistent with most formal theories (e.g.,
Sgall, Hajic?ova?, and Panevova? 1986; Mel?c?uk 1988). Exceptions include Word Grammar
(Hudson 1984), which allows a word to modify multiple other words in the sentence,
which results in directed acyclic graphs with nodes possibly having multiple incoming
arcs.
We define an arc (i, j, l) connecting words wi and wj as non-projective if at least
one word occurring between wi and wj in the input sentence is not a descendant of
wi (where ?descendant? is the transitive closure of the arc relation). Alternatively, we
can view non-projectivity in trees as breaking the nested property, which can be seen
through the arcs that cross in the example in Figure 1. Non-projective dependencies
are typically difficult to represent or parse in phrase-based models of syntax. This can
either be due to nested restrictions arising in context-free formalisms or computationally
expensive operations in mildly context-sensitive formalisms (e.g., adjunction in TAG
frameworks).
2.2 Global, Exhaustive, Graph-Based Parsing
For an input sentence, x = w0, w1, . . . , wn consider the dense graph Gx = (Vx, Ax) de-
fined as:
1. Vx = {0, 1, . . . , n}.
2. Ax = {(i, j, l) | i, j ? Vx and l ? L}.
Let D(Gx) represent the subgraphs of graph Gx that are valid dependency graphs for the
sentence x, that is, dependency trees. Because Gx contains all possible labeled arcs, the
set D(Gx) must necessarily contain all dependency trees for x.
Assume that there exists a dependency arc scoring function, s : V ? V ? L ? R.
Furthermore, define the score of a graph as the sum of its arc scores,
s(G = (V, A)) =
?
(i,j,l)?A
s(i, j, l)
The score of an arc, s(i, j, l) represents the likelihood of creating a dependency from
head wi to modifier wj with the label l in a dependency tree. This score is commonly
defined to be the product of a high dimensional feature representation of the arc and a
202
McDonald and Nivre Analyzing and Integrating Dependency Parsers
learned parameter vector, s(i, j, l) = w ? f(i, j, l). If the arc score function is known, then
the parsing problem can be stated as
G? = arg max
G?D(Gx )
s(G) = arg max
G?D(Gx )
?
(i,j,l)?A
s(i, j, l) (1)
An example graph Gx and the dependency tree maximizing the scoring function are
given in Figure 2 for the sentence John saw Mary. We omit arcs into the root node for
simplicity.
McDonald et al (2005) showed that this problem is equivalent to finding the highest
scoring directed spanning tree for the graph Gx originating out of the root node 0. It is
not difficult to see this, because both dependency trees and spanning trees must contain
all nodes of the graph and must have a tree structure with root 0. The directed spanning
tree problem (also known as the r-arborescence problem) can be solved for both the
labeled and unlabeled case using the Chu-Liu-Edmonds algorithm (Chu and Liu 1965;
Edmonds 1967), a variant of which can be shown to have an O(n2) runtime (Tarjan
1977). Non-projective arcs are produced naturally through the inference algorithm that
searches over all possible directed trees, whether projective or not.
Graph-based parsers are typically trained using structured learning algorithms
(McDonald, Crammer, and Pereira 2005; Koo et al 2007; Smith and Smith 2007), which
optimize the parameters of the model to maximize the difference in score/probability
between the correct dependency graph and all incorrect dependency graphs for every
sentence in a training set. Such a learning procedure is global because model parameters
are set relative to the classification of the entire dependency graph, and not just over
single arc attachment decisions. Although a learning procedure that only optimizes the
score of individual arcs is conceivable, it would not be likely to produce competitive
results.
Going beyond arc-factored models, McDonald and Pereira (2006) presented a
system where scores are increased in scope to include pairs of adjacent arcs in the
dependency graph. In the case of projective dependency trees, polynomial time
parsing algorithms were shown to exist, but non-projective trees required approximate
inference that used an exhaustive projective algorithm followed by transformations
to the graph that incrementally introduce non-projectivity. In general, inference and
learning for graph-based dependency parsing is NP-hard when the score is factored
Figure 2
A graph-based parsing example. A dense graph Gx is shown on the left (arcs into the root are
omitted) with corresponding arc scores. On the right is the predicted dependency tree based on
Equation (1).
203
Computational Linguistics Volume 37, Number 1
over anything larger than arcs (McDonald and Satta 2007). Thus, graph-based parsing
systems cannot easily condition on any extended scope of the dependency graph
beyond a single arc, which is their primary shortcoming relative to transition-based
systems. McDonald, Crammer, and Pereira (2005) show that a rich feature set over
the input space, including lexical and surface syntactic features of neighboring words,
can partially alleviate this problem, and both Carreras (2007) and Koo et al (2010)
explore higher-order models for projective trees. Additionally, work has been done on
approximate non-factored parsing systems (McDonald and Pereira 2006; Hall 2007;
Nakagawa 2007; Smith and Eisner 2008) as well as exact solutions through integer linear
programming (Riedel, C?ak?c?, and Meza-Ruiz 2006; Martins, Smith, and Xing 2009).
The specific graph-based system studied in this work is that presented by
McDonald, Lerman, and Pereira (2006), which uses pairwise arc scoring and approx-
imate exhaustive search for unlabeled parsing. A separate arc label classifier is then
used to label each arc. This two-stage process was adopted primarily for computational
reasons and often does not affect performance significantly (see McDonald [2006] for
more). Throughout the rest of this study we will refer to this system as MSTParser (or
MST for short), which is also the name of the freely available implementation.2
2.3 Local, Greedy, Transition-Based Parsing
A transition system for dependency parsing defines
1. a set C of parser configurations, each of which defines a (partially built)
dependency graph G,
2. a set T of transitions, each of which is a partial function t : C ? C,
3. for every sentence x = w0, w1, . . . , wn,
(a) a unique initial configuration cx,
(b) a set Cx of terminal configurations.
A transition sequence C0,m = (c0, c1, . . . , cm) for a sentence x = w0, w1, . . . , wn is a se-
quence of configurations such that c0 = cx, cm ? Cx, and, for every ci (1 ? i ? m), there
is a transition t ? T such that ci = t(ci?1). The dependency graph assigned to a sentence
x = w0, w1, . . . , wn by a sequence C0,m = (c0, c1, . . . , cm) is the graph Gm defined by the
terminal configuration cm.
Assume that there exists a transition scoring function, s : C ? T ? R. The score of a
transition t in a configuration c, s(c, t), represents the likelihood of taking transition t out
of configuration c in a transition sequence leading to the optimal dependency graph for
the given sentence. This score is usually defined by a classifier g taking as input a high
dimensional feature representation of the configuration, s(c, t) = g(f(c), t).
Given a transition scoring function, the parsing problem consists in finding a ter-
minal configuration cm ? Cx, starting from the initial configuration cx and taking the
optimal transition t? out of every configuration c:
t? = arg max
t?T
s(c, t)
2 http://mstparser.sourceforge.net.
204
McDonald and Nivre Analyzing and Integrating Dependency Parsers
This can be seen as a greedy search for the optimal dependency graph, based on a
sequence of locally optimal decisions in terms of the transition system.
By way of example, we consider the transition system first presented in Nivre
(2003), where a parser configuration is a triple c = (?,?, A), consisting of a stack ?
of partially processed nodes, a buffer ? of remaining input nodes, and a set A of
labeled dependency arcs. The initial configuration for a sentence x = w0, w1, . . . , wn is
cx = ([0], [1, . . . , n], ?) and the set of terminal configurations Cx contains all configu-
rations of the form c = (?, [ ], A) (that is, all configurations with an empty buffer and
with arbitrary ? and A). The set T of transitions for this system is specified in Figure 3.
The transitions LEFT-ARCl and RIGHT-ARCl extend the arc set A with an arc (labeled l)
connecting the top node i on the stack and the first node j of the buffer. In the case of
LEFT-ARCl, the node i becomes the dependent and is also popped from the stack; in
the case of RIGHT-ARCl, the node j becomes the dependent and is also pushed onto the
stack. The REDUCE transition pops the stack (and presupposes that the top node has
already been attached to its head in a previous RIGHT-ARCl transition), and the SHIFT
transition extracts the first node of the buffer and pushes it onto the stack.
This system can derive any projective dependency tree G for an input sentence
x and in doing so always adds arcs as early as possible. For this reason, the system
is often referred to as arc-eager. When coupled with the greedy deterministic parsing
strategy, the system guarantees termination after at most 2n transitions (for a sentence
of length n), which means that the time complexity is O(n) given that transitions can be
performed in constant time. The dependency graph given at termination is guaranteed
to be acyclic and projective and to satisfy dependency graph conditions 1?3, which
means that it can always be turned into a well-formed dependency graph by adding
arcs (0, i, lr) for every node i = 0 that is a root in the output graph (where lr is a spe-
cial label for root modifiers). Whereas the initial formulation in Nivre (2003) was lim-
ited to unlabeled dependency graphs, the system was extended to labeled graphs in
Nivre, Hall, and Nilsson (2004), and Nivre and Nilsson (2005) showed how the restric-
tion to projective dependency graphs could be lifted by using graph transformation
Figure 3
Transitions for dependency parsing (with preconditions).
205
Computational Linguistics Volume 37, Number 1
techniques to pre-process training data and post-process parser output, a technique
called pseudo-projective parsing. Transition systems that derive non-projective trees
directly have been explored by Attardi (2006) and Nivre (2007, 2009), among others.
To learn a scoring function, transition-based parsers use discriminative learning
methods, such as memory-based learning or support vector machines. The training
data are obtained by constructing transition sequences corresponding to gold standard
parses from a treebank. The typical learning procedure is local because only single
transitions are scored?not entire transition sequences?but more global optimization
methods have also been proposed. The primary advantage of these models is that the
feature representation is not restricted to a limited number of graph arcs but can take
into account the entire dependency graph built so far, including previously assigned
labels, and still support efficient inference and learning. The main disadvantage is that
the greedy parsing strategy may lead to error propagation as false early predictions can
eliminate valid trees due to structural constraints and are also used to create features
when making future predictions. Using beam search instead of strictly deterministic
parsing can to some extent alleviate this problem but does not eliminate it.
The specific transition-based system studied in this work is that presented by Nivre
et al (2006), which uses the projective, arc-eager system described here in combina-
tion with pseudo-projective parsing, which uses support vector machines to learn the
scoring function for transitions and which uses greedy, deterministic one-best search at
parsing time. We will refer to this system as MaltParser (or Malt for short), which is also
the name of the freely available implementation.3
2.4 Comparison
In the previous two sections we have outlined the theoretical characteristics of canonical
graph-based and transition-based dependency parsing systems. From now on, our
experiments will rely on two standard implementations: MSTParser, a graph-based
system, and MaltParser, a transition-based system. Here we contrast the two parsing
systems with respect to how they are trained, how they produce dependency trees for
new sentences, and what kinds of features they use.
Training Algorithms. Both systems use large-margin learning for linear classifiers. MST-
Parser uses on-line algorithms (McDonald, Crammer, and Pereira 2005; Crammer et al
2006) and MaltParser uses support vector machines (Cortes and Vapnik 1995). The
primary difference is that MaltParser trains the model to make a single classification
decision (create arc, shift, reduce, etc.), whereas MSTParser trains the model to maxi-
mize the global score of correct graphs relative to incorrect graphs. It has been argued
that locally trained algorithms can suffer from label bias issues (Lafferty, McCallum,
and Pereira 2001). However, it is expensive to train global models since the complexity
of learning is typically proportional to inference. In addition, MaltParser makes use of
kernel functions, which eliminates the need for explicit conjunctions of features.
Inference. MaltParser uses a transition-based inference algorithm that greedily chooses
the best parsing decision based on a trained classifier and current parser history. MST-
Parser instead uses exhaustive search over a dense graphical representation of the
3 http://w3.msi.vxu.se/users/nivre/research/MaltParser.html.
206
McDonald and Nivre Analyzing and Integrating Dependency Parsers
sentence to find the dependency graph that maximizes the score. On the one hand,
the greedy algorithm is far quicker computationally (O(n) vs. O(n2) for the Chu-Liu-
Edmonds algorithm and O(n3) for Eisner?s algorithm). On the other hand, it may be
prone to error propagation when early incorrect decisions negatively influence the
parser at later stages. In particular, MaltParser uses the projective arc-eager transition
system first described in Nivre (2003), which has consequences for the form of error
propagation we may expect to see because the system determines the order in which
arcs must be added to the graph. On the one hand, if an arc (i, m, l) covers another
arc ( j, k, l?) (i.e., i ? j and k ? m), then the smaller arc ( j, k, l?) has to be added to the
graph first (because of projectivity). On the other hand, if two arcs (i, j, l) and (k, m, l?)
do not overlap, then the leftmost arc has to be added first (because of arc-eagerness).
Therefore, we can expect error propagation from shorter to longer overlapping arcs and
from preceding to succeeding arcs.
Feature Representation. Due to the nature of their inference and training algorithms, the
feature representations of the two systems differ substantially. MaltParser can introduce
a rich feature space based on the history of previous parser decisions. This is because
the greedy nature of the algorithm allows it to fix the structure of the graph and
use this structure to help improve future parsing decisions. By contrast, MSTParser is
forced to restrict the scope of features to a single or pair of nearby parsing decisions
in order to make exhaustive inference tractable. As a result, the feature representation
available to the locally trained greedy models is much richer than the globally trained
exhaustive models. Concisely, we can characterize MSTParser as using global training
and inference with local features and MaltParser as using local training and inference
with global features. (For more information about the features used in the two systems,
see Sections 3.2 and 3.3.)
These differences highlight an inherent trade-off between exhaustive inference algo-
rithms plus global learning and expressiveness of feature representations. MSTParser
favors the former at the expense of the latter and MaltParser the opposite. When
analyzing, and ultimately explaining, the empirical difference between the systems,
understanding this trade-off will be of central importance.
3. Experimental Setup
The experiments presented in this article are all based on data from the CoNLL-X
shared task (Buchholz and Marsi 2006). In this section we first describe the task and the
resources created there and then describe how MSTParser and MaltParser were trained
for the task, including feature representations and learning algorithms.
3.1 The CoNLL-X Shared Task
The CoNLL-X shared task (Buchholz and Marsi 2006) was a large-scale evaluation
of data-driven dependency parsers, with data from 13 different languages and 19 par-
ticipating systems. The data sets were quite heterogeneous, both with respect to size
and with respect to linguistic annotation principles, and the best reported parsing
accuracy varied from 65.7% for Turkish to 91.7% for Japanese. The official evaluation
metric was the labeled attachment score (LAS), defined as the percentage of tokens,
207
Computational Linguistics Volume 37, Number 1
Table 2
Data sets. Tok = number of tokens (?1000); Sen = number of sentences (?1000); T/S = tokens per
sentence (mean); Lem = lemmatization present; CPoS = number of coarse-grained part-of-speech
tags; PoS = number of (fine-grained) part-of-speech tags; MSF = number of morphosyntactic
features (split into atoms); Dep = number of dependency types; NPT = proportion of
non-projective dependencies/tokens (%); NPS = proportion of non-projective dependency
graphs/sentences (%).
Language Tok Sen T/S Lem CPoS PoS MSF Dep NPT NPS
Arabic 54 1.5 37.2 yes 14 19 19 27 0.4 11.2
Bulgarian 190 14.4 14.8 no 11 53 50 18 0.4 5.4
Chinese 337 57.0 5.9 no 22 303 0 82 0.0 0.0
Czech 1,249 72.7 17.2 yes 12 63 61 78 1.9 23.2
Danish 94 5.2 18.2 no 10 24 47 52 1.0 15.6
Dutch 195 13.3 14.6 yes 13 302 81 26 5.4 36.4
German 700 39.2 17.8 no 52 52 0 46 2.3 27.8
Japanese 151 17.0 8.9 no 20 77 0 7 1.1 5.3
Portuguese 207 9.1 22.8 yes 15 21 146 55 1.3 18.9
Slovene 29 1.5 18.7 yes 11 28 51 25 1.9 22.2
Spanish 89 3.3 27.0 yes 15 38 33 21 0.1 1.7
Swedish 191 11.0 17.3 no 37 37 0 56 1.0 9.8
Turkish 58 5.0 11.5 yes 14 30 82 25 1.5 11.6
excluding punctuation, that are assigned both the correct head and the correct depen-
dency label.4
The outputs of all systems that participated in the shared task are available for
download and constitute a rich resource for comparative error analysis. In Section 4,
we will use the outputs of MSTParser and MaltParser for all 13 languages, together
with the corresponding gold standard graphs used in the evaluation, as the basis for an
in-depth error analysis designed to answer Question 2 from Section 1. In Section 6, we
will then evaluate our stacking-based parsers on the same data sets and repeat the error
analysis. This will allow us to compare the error profiles of the new and old systems at
a much finer level of detail than in standard evaluation campaigns.
Table 2 gives an overview of the training sets available for the 13 languages. First
of all, we see that training set size varies from over 1.2 million words and close to
73,000 sentences for Czech to only 29,000 words and 1,500 sentences for Slovene. We
also see that the average sentence length varies from close to 40 words for Arabic, using
slightly different principles for sentence segmentation than the other languages, to less
than 10 words for Japanese, where the data consist of transcribed spoken dialogues.
Differences such as these can be expected to have a large impact on the parsing accuracy
obtained for different languages, and it is probably significant that Japanese has the
highest top score of all languages, whereas Arabic has the second lowest. We also see
that the amount of information available in the input, in the form of lemmas (Lem),
coarse and fine part-of-speech tags (CPoS, PoS), and morphosyntactic features (MSF)
varies considerably, as does the granularity of the dependency label sets (Dep). Fi-
nally, the proportion of non-projective structures, whether measured on the token level
(NPT) or on the sentence level (NPS), is another important source of variation.
4 In addition, results were reported for unlabeled attachment score (UAS) (tokens with the correct head)
and label accuracy (LA) (tokens with the correct label).
208
McDonald and Nivre Analyzing and Integrating Dependency Parsers
The final test set for each language has been standardized in size to about 5,000
words, which makes it possible to evaluate the performance of a system over all lan-
guages by simply concatenating the system?s output for all test sets and comparing this
to the concatenation of the gold standard test sets. Thus, most of the statistics used in the
subsequent error analysis are based on the concatenation of all test sets. Because some of
the phenomena under study are relatively rare, this allows us to get more reliable esti-
mates, even though these estimates inevitably hide important inter-language variation.
Analyzing individual languages in more detail would be an interesting complementary
study but is beyond the scope of this article.
3.2 Training MSTParser
MSTParser operates primarily over arc-scores, s(i, j, l), which are parameterized by a
linear combination of a parameter vector, w, and a corresponding feature vector for the
arc, f(i, j, l). We use a two-stage approach to training. The first-stage learns a model to
predict unlabeled dependency trees for a sentence. Thus, arc scores do not condition
on possible labels and are parameterized by features only over the head-modifier pair,
s(i, j) = w ? f(i, j). As a result, for the first stage of training the parser, we must define
the feature representation f(i, j), which is outlined in Table 3(a) and Table 3(b) for a
potential unlabeled arc (i, j). These features represent both information about the head
and modifier in the dependency relation as well as the context of the dependency via
local part-of-speech information. We include context part-of-speech features for both
the fine-grained and coarse-grained tags (when available).
As mentioned in Section 2.2, the implementation of MSTParser used in our exper-
iments also contains features over adjacent arcs, (i, j) and (i, k), which we will denote
compactly as (i, j  k). Scores for adjacent arcs are also defined as a linear combination
between weights and a feature vector s(i, j  k) = w ? f(i, j  k), thus requiring us to de-
fine the feature representation f(i, j  k), which is outlined in Table 3(c). These features
Table 3
Features for MSTParser. ? indicates a conjunction of features. ? indicates that all back-off
versions of a conjunction feature are included as well. A back-off version of a conjunction feature
is one where one or more base features are disregarded. ? indicates that all back-off versions are
included where a single base feature is disregarded.
Base features for sentence: x = w0, w1, . . . , wn
Lexical features: Identity of wi, wi ? x
Affix features: 3-gram lexical prefix/suffix identity of Pref(wi)/Suff(wi), wi ? x
Part-of-speech features: Identity of PoS(wi), wi ? x
Morphosyntactic features: For all morphosyntactic features MSFk for a word wi, identity of MSFk(wi), wi ? x
Label features: Identity of l in some labeled arc (i, j, l)
(a) Head-modifier features for unlabeled arc (i, j)
wi ? PoS(wi ) ? wj ? PoS(wj ) ?
Pref(wi ) ? PoS(wi ) ? Pref(wj ) ? PoS(wj ) ?
Suff(wi ) ? PoS(wi ) ? Suff(wj ) ? PoS(wj ) ?
?k, k? : MSFk(wi ) ? PoS(wi ) ? MSFk? (wj ) ? PoS(wj ) ?
(b) PoS-context features for unlabeled arc (i, j)
?k, i < k < j : PoS(wi ) ? PoS(wk ) ? PoS(wj )
PoS(wi?1) ? PoS(wi ) ? PoS(wj?1) ? PoS(wj ) ?
PoS(wi?1) ? PoS(wi ) ? PoS(wj ) ? PoS(wj+1 ) ?
PoS(wi ) ? PoS(wi+1 ) ? PoS(wj?1) ? PoS(wj ) ?
PoS(wi ) ? PoS(wi+1 ) ? PoS(wj ) ? PoS(wj+1) ?
(c) Head-modifier features for unlabeled arc pair (i, j 
 k)
wj ? wk
wj ? PoS(wk )
PoS(wj ) ? wk
PoS(wj ) ? PoS(wk )
PoS(wi ) ? PoS(wj ) ? PoS(wk )
(d) Arc-label features for labeled arc (i, j, l)
wi ? PoS(wi ) ? wj ? PoS(wj ) ? l ?
?k, i < k < j : PoS(wi ) ? PoS(wk ) ? PoS(wj ) ? l
PoS(wj?1) ? PoS(wj ) ? PoS(wj+1 ) ? l ?
PoS(wi?1) ? PoS(wi ) ? PoS(wi+1 ) ? l ?
209
Computational Linguistics Volume 37, Number 1
attempt to capture likely properties about adjacent arcs in the tree via their lexical and
part-of-speech information. Finally, all features in Table 3(a)?(c) contain two versions.
The first is the standard feature outlined in the table, and the second is the feature
conjoined with both the direction of dependency attachment (left or right) as well as the
bucketed distance between the head and modifier in buckets of 0 (adjacent), 1, 2, 3, 4,
5?9, and 10+.
For the second stage label classifier we use a log-linear classifier, which is again
parameterized by a vector of weights and a corresponding feature vector s(l|i, j) =
w ? f(i, j, l), where the score of a label l is now conditioned on a fixed dependency arc
(i, j) produced from the first-stage unlabeled parser. The feature representation f(i, j, l) is
defined in Table 3(d). These features provide the lexical and part-of-speech context for
determining whether a given arc label is suitable for a given head and modifier. Each
feature in Table 3(d) again has two versions, except this time the second version is only
conjoined with attachment direction.
These feature representations were used to train an on-line large-margin unlabeled
dependency parser (McDonald, Crammer, and Pereira 2005; McDonald and Pereira
2006) and a log-linear arc-labeler (Berger, Pietra, and Pietra 1996) regularized with a
zero mean Gaussian prior with the variance hyper-parameter set to 1.0. The unlabeled
dependency parser was trained for 10 iterations and the log-linear arc-labeler was
trained for 100 iterations. The feature sets and model hyper-parameters were fixed for
all languages. The only exception is that features containing coarse part-of-speech or
morphosyntactic information were ignored if this information was not available in a
corresponding treebank.
3.3 Training MaltParser
Training MaltParser amounts to estimating a function for scoring configuration-
transition pairs (c, t), represented by a feature vector f(c, t) ? Rk. Features are defined
in terms of arbitrary properties of the configuration c, including the state of the stack
?c, the input buffer ?c, and the partially built dependency graph Gc (represented in the
configuration by the arc set Ac). In particular, many features involve properties of the
two target tokens, the token on top of the stack ?c (denoted ?
0
c ) and the first token in
the input buffer ?c (denoted ?
0
c ), which are the two tokens that may become connected
by a dependency arc through the transition out of c. The basic feature representation
used for all languages in the CoNLL-X shared task included three groups of features:5
 Part-of-speech features: Identity of PoS(w), w ? {?0c ,?
1
c ,?
0
c ,?
1
c ,?
2
c ,?
3
c}.
 Lexical features: Identity of w, w ? {?0c ,?
0
c ,?
1
c} or (w,?
0
c , l) ? Gc.
 Arc features: Identity of l, (w, w?, l) ? Gc and w ? {?0c ,?
0
c} or w
? ? {?0c}.
Note in particular that features can be defined with respect to the partially built de-
pendency graph Gc. This is most obvious for the arc features, which extract the labels
of particular arcs in the graph, but it is also true of the last lexical feature, which picks
out the word form of the syntactic head of the word on top of the stack. This is pre-
cisely what gives transition-based parsers a richer feature space than their graph-based
5 We use the notation ?ic and ?
i
c to denote the ith element from the top/head of the stack/buffer (with
index 0 for the first element).
210
McDonald and Nivre Analyzing and Integrating Dependency Parsers
counterparts, even though graph-defined features are usually limited to a fairly small
region of the graph around ?0c and ?
0
c , such as their leftmost and rightmost dependents
and their syntactic head (if available).
Over and above the basic features described here, additional features were added
for some languages depending on availability in the training data. This included the
following:
 Coarse part-of-speech features: Identity of CPoS(w), w ? {?0c ,?
0
c ,?
1
c}.
 Lemma features: Identity of Lem(w), w ? {?0c ,?
0
c ,?
1
c}.
 Morphosyntactic features: Identity of MSF(w), w ? {?0c ,?
0
c ,?
1
c}.
Additional feature selection experiments were carried out for each language to the
extent that time permitted. Complete information about feature representations can be
found in Nivre et al (2006) and on the companion web site.6
The feature representations described here were used to train support vector ma-
chines as implemented in the LIBSVM library (Chang and Lin 2001), with a quadratic
kernel K(xi, xj) = (?x
T
i xj + r)
2 and LIBSVM?s built-in one-versus-one strategy for multi-
class classification, converting symbolic features to numerical ones using the standard
technique of binarization.7 One thing to note is that the quadratic kernel implicitly adds
features corresponding to pairs of explicit features, thus obviating the need for explicit
feature conjunctions as seen in the feature representations of MSTParser.
4. Error Analysis
A primary goal of this study is to characterize the errors made by standard data-driven
dependency parsing models. To that end, this section presents a number of experiments
that relate parsing errors to a set of linguistic and structural properties of the input and
predicted/gold standard dependency trees. We argue throughout that the results of this
analysis can be correlated to specific theoretical aspects of each model?in particular the
trade-off previously highlighted in Section 2.4.
For simplicity, all experiments report labeled parsing metrics (either accuracy, preci-
sion, or recall). Identical experiments using unlabeled parsing accuracies did not reveal
any additional information. Statistical significance was measured?for each metric at
each point along the operating curve?by employing randomized stratified shuffling at
the instance level using 10,000 iterations.8 Furthermore, all experiments report aggre-
gate statistics over the data from all 13 languages together, as explained in Section 3.
Finally, in all figures and tables, MSTParser and MaltParser are referred to as MST and
Malt, respectively, for short.
4.1 Length Factors
It is well known that parsing systems tend to have lower accuracies for longer sentences.
This is primarily due to the increased presence of complex syntactic constructions
6 http://maltparser.org/conll/conllx.
7 For details about parameter settings, we again refer to Nivre et al (2006) and the companion web site
http://maltparser.org/conll/conllx/.
8 This is the method used by the CoNLL-X shared task on dependency parsing.
211
Computational Linguistics Volume 37, Number 1
Figure 4
Accuracy relative to sentence length. Differences statistically significant (p < 0.05) at no
positions.
involving prepositions, conjunctions, and multi-clause sentences. Figure 4 shows the
accuracy of both parsing models relative to sentence length (in bins of size 10: 1?10,
11?20, etc.). System performance is almost indistinguishable, but MaltParser tends to
perform better on shorter sentences, which require the greedy inference algorithm to
make fewer parsing decisions. As a result, the chance of error propagation is reduced
significantly when parsing these sentences. However, if this was the only difference
between the two systems, we would expect them to have equal accuracy for shorter
sentences. The fact that MaltParser actually has higher accuracy when the likelihood
of error propagation is reduced is probably due to its richer feature space relative to
MSTParser.
Another interesting property is accuracy relative to dependency length as opposed
to sentence length. We define the length of a dependency from word wi to word wj
as equal to |i ? j|. Longer dependencies typically represent modifiers of the root or
the main verb in a sentence. Shorter dependencies are often modifiers of nouns such
as determiners or adjectives or pronouns modifying their direct neighbors. Figure 5
measures the precision and recall for each system relative to dependency lengths in the
predicted and gold standard dependency graphs. Precision represents the percentage
of predicted arcs of length d that were correct. Recall measures the percentage of gold
standard arcs of length d that were predicted.
Here we begin to see separation between the two systems. MSTParser is far more
precise for longer dependency arcs, whereas MaltParser does better for shorter depen-
dency arcs. This behavior can be explained using the same reasoning as above: Shorter
dependency arcs are usually created first in the greedy parsing procedure of MaltParser
and are less prone to error propagation. In contrast, longer dependencies are typically
constructed at the later stages of the parsing algorithm and are affected more by error
propagation. Theoretically, MSTParser should not perform better or worse for arcs of
any length. However, due to the fact that longer dependencies are typically harder
to parse, there is still a degradation in performance for MSTParser?up to 20% in the
212
McDonald and Nivre Analyzing and Integrating Dependency Parsers
Figure 5
Dependency arc precision/recall relative to predicted/gold dependency length. Precision
statistically significant (p < 0.05) at 1, 2, 4, 7, 8, 10 through >14. Recall statistically significant
(p < 0.05) at >14.
extreme. However, the precision curve for MSTParser is much flatter than MaltParser,
which sees a drop of up to 40% in the extreme. Note that even though the area under the
curve is much larger for MSTParser, the number of dependency arcs with a length >10
is much smaller than the number with length <10, which is why the overall accuracy of
the two systems is nearly identical.
4.2 Graph Factors
The structure of the predicted and gold standard dependency graphs can also provide
insight into the differences between each model. For example, measuring accuracy for
arcs relative to their distance to the artificial root node will detail errors at different
levels of the dependency graph. For a given arc, we define this distance as the number
of arcs in the reverse path from the modifier of the arc to the root. For example, the
dependency arc from ROOT to is in Figure 1 would have a distance of 1 and the arc from
hearing to A a distance of 3. Figure 6 plots the precision and recall of each system for arcs
of varying distance to the root. Precision is equal to the percentage of dependency arcs
Figure 6
Dependency arc precision/recall relative to the predicted/gold distance to root. Precision
statistically significant (p < 0.05) at 1, 2, 4 through >6. Recall statistically significant (p < 0.05) at
1, 2, 3.
213
Computational Linguistics Volume 37, Number 1
in the predicted graph that are at a distance of d and are correct. Recall is the percentage
of dependency arcs in the gold standard graph that are at a distance of d and were
predicted.
Figure 6 clearly shows that for arcs close to the root, MSTParser is much more pre-
cise than MaltParser, and vice versa for arcs further away from the root. This is probably
the most compelling graph given in this study because it reveals a clear distinction:
MSTParser?s precision degrades as the distance to the root increases whereas Malt-
Parser?s precision increases. The plots essentially run in opposite directions crossing
near the middle. Dependency arcs further away from the root are usually constructed
early in the parsing algorithm of MaltParser. Again a reduced likelihood of error propa-
gation coupled with a rich feature representation benefits that parser substantially. Fur-
thermore, MaltParser tends to over-predict root modifiers, which comes at the expense
of its precision. This is because all words that the parser fails to attach as modifiers are
automatically connected to the root, as explained in Section 2.3. Hence, low precision
for root modifiers (without a corresponding drop in recall) is an indication that the
transition-based parser produces fragmented parses.
The behavior of MSTParser is a little trickier to explain. One would expect that
its errors should be distributed evenly over the graph. For the most part this is true,
with the exception of spikes at the ends of the plot. The high performance for root
modification (distance of 1) can be explained through the fact that this is typically a
low-entropy decision?usually the parsing algorithm has to determine the main verb
from a small set of possibilities. On the other end of the plot there is a slight downward
trend for arcs of distance greater than 3 from the root. An examination of dependency
length for predicted arcs shows that MSTParser predicts many more arcs of length 1
than MaltParser, which naturally leads to over-predicting more arcs at larger distances
from the root due to the presence of chains, which in turn will lower precision for these
arcs. In ambiguous situations, it is not surprising that MSTParser predicts many length-
1 dependencies, as this is the most common dependency length across treebanks. Thus,
whereas MaltParser pushes difficult parsing decisions higher in the graph, MSTParser
appears to push these decisions lower.
The final graph property we will examine aims to quantify the local neighborhood
of an arc within a dependency graph. Two dependency arcs, (i, j, l) and (i?, j?, l?), are clas-
sified as siblings if they represent syntactic modifications of the same word (i.e., i = i?).
In Figure 1 the arcs from the word is to the words hearing, scheduled, and the period are
all considered siblings under this definition. Figure 7 measures the precision and recall
of each system relative to the number of predicted and gold standard siblings of each
arc. There is not much to distinguish between the parsers on this metric. MSTParser is
slightly more precise for arcs that are predicted with more siblings, whereas MaltParser
has slightly higher recall on arcs that have more siblings in the gold standard tree. Arcs
closer to the root tend to have more siblings, which ties this result to the previous ones.
4.3 Linguistic Factors
It is important to relate each system?s accuracy to a set of linguistic categories, such
as parts of speech and dependency types. However, given the important typological
differences that exist between languages, as well as the diversity of annotation schemes
used in different treebanks, it is far from straightforward to compare these categories
across languages. Nevertheless, we have made an attempt to distinguish a few broad
categories that are cross-linguistically identifiable, based on the available documenta-
tion of the treebanks used in the shared task.
214
McDonald and Nivre Analyzing and Integrating Dependency Parsers
Figure 7
Dependency arc precision/recall relative to the number of predicted/gold siblings. Precision
statistically significant (p < 0.05) at 0, 4, 6, 7, 9. Recall statistically significant (p < 0.05) at 5, >9.
For parts of speech, we distinguish verbs (including both main verbs and auxil-
iaries), nouns (including proper names), pronouns (sometimes also including deter-
miners), adjectives, adverbs, adpositions (prepositions, postpositions), and conjunctions
(both coordinating and subordinating). For dependency types, we have only managed
to distinguish a general root category (for labels used on arcs from the artificial root,
including either a generic label or the label assigned to predicates of main clauses, which
are normally verbs), a subject category, and an object category (including both direct
and indirect objects). Unfortunately, we had to exclude many interesting types that
could not be identified with high enough precision across languages, such as adverbials,
which cannot be clearly distinguished in annotation schemes that subsume them under
a general modifier category, and coordinate structures, which are sometimes annotated
with special dependency types, sometimes with ordinary dependency types found also
in non-coordinated structures.
Table 4(a) shows the accuracy of the two parsers for different parts of speech.
This figure measures labeled dependency accuracy relative to the part of speech of
the modifier word in a dependency relation. We see that MaltParser has slightly better
accuracy for nouns and pronouns, and MSTParser does better on all other categories, in
particular conjunctions. This pattern is consistent with previous results insofar as verbs
Table 4
(a) Accuracy relative to dependent part of speech. (b) Precision/recall for different dependency
types.
Part of Speech MST Malt
Verb 82.6 81.9
Noun 80.0 80.7
Pronoun 88.4 89.2
Adjective 89.1 87.9
Adverb 78.3 77.4
Adposition 69.9 68.8
Conjunction 73.1 69.8
(a)
Dependency MST Malt
Type Precision/Recall Precision/Recall
Root 89.9 / 88.7 84.7 / 87.5
Subject 79.9 / 78.9 80.3 / 80.7
Object 76.5 / 77.7 77.2 / 77.6
(b)
215
Computational Linguistics Volume 37, Number 1
and conjunctions are often involved in dependencies closer to the root that span longer
distances, whereas nouns and pronouns are typically attached to verbs and therefore
occur lower in the graph and with shorter distances. Thus, the average distance to the
root is 3.1 for verbs and 3.8 for conjunctions, but 4.7 for nouns and 4.9 for pronouns;
the average dependency length is 4.2 for verbs, 4.8 for conjunctions, 2.3 for nouns,
and 1.6 for pronouns. Adverbs resemble verbs and conjunctions with respect to root
distance (3.7) but group with nouns and pronouns for dependency length (2.3), so it
appears that the former is more important here. Furthermore, adverb modifiers have
2.4 siblings on average, which is greater than the sibling average for conjunctions (2.1),
adpositions (1.9), pronouns (1.7), verbs (1.3), nouns (1.3), and adjectives (1.2). This
would be consistent with the graph in Figure 7.
Adpositions and especially adjectives constitute a puzzle. With a root distance of 4.4
and 5.2, respectively, a dependency length of 2.5/1.5 and a sibling average of 1.9/1.2, we
would expect MaltParser to do better than MSTParser for these categories. Adpositions
do tend to have a high number of siblings on average, which could explain MSTParser?s
performance on that category. However, adjectives on average occur the furthest away
from the root, have the shortest dependency length, and the fewest siblings. At present,
we do not have an explanation for this behavior.
Finally, in Table 4(b), we consider precision and recall for dependents of the root
node (mostly verbal predicates), and for subjects and objects. As already noted, MST-
Parser has considerably better precision (and slightly better recall) for the root category,
but MaltParser has an advantage for the nominal categories, especially subjects. A pos-
sible explanation for the latter result, in addition to the length-based and graph-based
factors invoked before, is that MaltParser integrates labeling into the parsing process,
which means that previously assigned dependency labels can be used as features.
This may sometimes be important to disambiguate subjects and objects, especially in
free-word order languages where a dependent?s position relative to the verb does not
determine its syntactic role.
4.4 Discussion
The experiments in this section highlight the fundamental trade-off between global
training and exhaustive inference on the one hand and expressive feature representa-
tions on the other. Error propagation is an issue for MaltParser, which typically performs
worse on long sentences, long dependency arcs, and arcs higher in the graphs. But this
is offset by the rich feature representation available to these models that result in better
decisions for frequently occurring classes of arcs like short dependencies or subject
and object modifiers. The errors for MSTParser are spread a little more evenly. This
is expected, as the inference algorithm and feature representation should not prefer one
type of arc over another.
What has been learned? It was already known that the two systems make different
errors through the work of Sagae and Lavie (2006). However, in that work an arc-based
majority voting scheme was used that took only limited account of the properties of
the words connected by a dependency arc (more precisely, the overall accuracy of each
parser for the part of speech of the dependent). The analysis in this work not only shows
that the errors made by each system are different, but that they are different in a way
that can be predicted and quantified. This is an important step in parser development.
By understanding the strengths and weaknesses of each model we have gained insights
towards new and better models for dependency parsing.
216
McDonald and Nivre Analyzing and Integrating Dependency Parsers
To get some upper bounds on the improvement that can be obtained by combining
the strengths of each model, we can perform two oracle experiments. Given the output
of the two systems, we can envision an oracle that can optimally choose which single
parse or combination of sub-parses to predict as a final parse. For the first experiment
the oracle is provided with the single best parse from each system, say G = (V, A) and
G? = (V?, A?). The oracle chooses a parse that has the highest number of correctly pre-
dicted labeled dependency attachments. In this situation, the oracle labeled attachment
score is 84.5%. In the second experiment the oracle chooses the tree that maximizes the
number of correctly predicted dependency attachments, subject to the restriction that
the tree must only contain arcs from A ? A?. This can be computed by setting the weight
of an arc to 1 if it is in the correct parse and in the set A ? A?. All other arc weights are
set to negative infinity. One can then simply find the tree that has maximal sum of arc
weights using directed spanning tree algorithms. This technique is similar to the parser
voting methods used by Sagae and Lavie (2006). In this situation, the oracle accuracy
is 86.9%.
In both cases we see a clear increase in accuracy: 86.9% and 84.5% relative to 81%
for the individual systems. This indicates that there is still potential for improvement,
just by combining the two existing models. More interestingly, however, we can use
the analysis from this section to generate ideas for new models. Below we sketch some
possible new directions:
1. Ensemble systems: The error analysis presented in this article could be used
as inspiration for more refined weighting schemes for ensemble systems of
the kind proposed by Sagae and Lavie (2006), making the weights depend
on a range of linguistic and graph-based factors.
2. Integrated/Hybrid systems: Rather than using an ensemble of several
independent parsers, we may construct systems that trust different parsers
in different situations, possibly based on the characteristics of the input
and predicted dependency trees. The oracle results reported here show
that such an approach could potentially result in substantial
improvements.
3. Novel approaches: The theoretical analysis presented in this article reveals
that the two dominant approaches are each based on a particular
combination of training and inference methods, which raises the question
of which other combinations can fruitfully be explored. For example, can
we construct globally trained, greedy, transition-based parsers? Or
graph-based parsers with global features? To some extent the former
characterization fits the approach of Zhang and Clark (2008) and Huang
and Sagae (2010), and the latter that of Riedel and Clarke (2006),
Nakagawa (2007), and others. The analysis presented in this section
explains the relative success of such approaches.
In the next two sections we explore a model that falls into category 2. The system we
propose uses a two-stage stacking framework, where a second-stage parser conditions
on the predictions of a first-stage parser during inference. The second-stage parser is
also learned with access to the first-stage parser?s decisions and thus learns when to
trust the first-stage parser?s predictions and when to trust its own. The method is not a
traditional ensemble, because the parsers are not learned independently of one another.
217
Computational Linguistics Volume 37, Number 1
5. Integrated Models
As just discussed, there are many conceivable ways of combining the two parsers,
including more or less complex ensemble systems and voting schemes, which only
perform the integration at parsing time. However, given that we are dealing with data-
driven models, it should be possible to integrate at learning time, so that the two
complementary models can learn from one another. In this article, we propose to do this
by letting one model generate features for the other in a stacked learning framework.
Feature-based integration in this sense has previously been exploited for depen-
dency parsing by McDonald (2006), who trained an instance of MSTParser using fea-
tures generated by the parsers of Collins (1999) and Charniak (2000), which improved
unlabeled accuracy by 1.7 percentage points on data from the Penn Treebank. In other
NLP domains, feature-based integration has been used by Taskar, Lacoste-Julien, and
Klein (2005), who trained a discriminative word alignment model using features de-
rived from the IBM models, by Florian et al (2004), who trained classifiers on auxiliary
data to guide named entity classifiers, and by others.
Feature-based integration also has points in common with co-training, which has
been applied to syntactic parsing by Sarkar (2001) and Steedman et al (2003), among
others. The difference, of course, is that standard co-training is a weakly supervised
method, where the first-stage parser?s predictions replace, rather than complement, the
gold standard annotation during training. Feature-based integration is also similar to
parse reranking (Collins 2000), where one parser produces a set of candidate parses
and a second-stage classifier chooses the most likely one. However, feature-based in-
tegration is not explicitly constrained to any parse decisions that the first-stage parser
might make. Furthermore, as only the single most likely parse is used from the first-
stage model, it is significantly more efficient than reranking, which requires both com-
putationally and conceptually more complex parsing algorithms (Huang and Chiang
2005).
5.1 Parser Stacking with Rich Features
As explained in Section 2, both models essentially learn a scoring function s : X ? R,
where the domain X is different for the two models. For the graph-based model, X is
the set of possible dependency arcs (i, j, l); for the transition-based model, X is the set of
possible configuration-transition pairs (c, t). But in both cases, the input is represented
by a k-dimensional feature vector f : X ? Rk. In a stacked parsing system we simply
extend the feature vector for one model, called the base model, with a certain number
of features generated by the other model, which we call the guide model in this context.
The additional features will be referred to as guide features, and the version of the base
model trained with the extended feature vector will be called the guided model. The
idea is that the guided model should be able to learn in which situations to trust the
guide features, in order to exploit the complementary strength of the guide model, so
that performance can be improved with respect to the base model.
The exact form of the guide features depends on properties of the base model and
will be discussed in Sections 5.2?5.3, but the overall scheme for the stacked parsing
model can be described as follows. Assume as input a training set T = {(xt, Gxt )}
|T|
t=1 of
input sentences xt and corresponding gold standard dependency trees Gxt . In order to
train the guide model we use a cross-validation scheme and divide T into n different
disjoint subsets Ti (i.e., T =
?n
i=1 Ti). Let M[T] be the result of training the model M on T
218
McDonald and Nivre Analyzing and Integrating Dependency Parsers
and let M[T](x) be the result of parsing a new input sentence x with M[T]. Now, consider
a guide model C, base model B, and guided model BC. For each x in T, define
GCx = C[T ? Ti](x) if x ? Ti
GCx is the prediction of model C on training input x when C is trained on all the subsets
of T, except the one containing x. The reason for using this cross-validation scheme is
that if C had been trained on all of T, then GCx would not be representative of the types of
errors that C might make when parsing sentence x. Using cross-validation in this way is
similar to how it is used in parse reranking (Collins 2000). Now, define a new training set
of the form T? = {(?xt, GCxt?, Gxt )}
|T|
t=1. That is, T
? is identical to T, except that each training
input x is augmented with the cross-validation prediction of model C. Finally, let
BC = B[T
?]
This means that, for every sentence x ? T, BC has access at training time to both the
gold standard dependency graph Gx and the graph G
C
x predicted by C. Thus, BC is able
to define guide features over GCx , which can prove beneficial if features over G
C
x can be
used to discern when parsing model C outperforms or underperforms parsing model
B. When parsing a new sentence x with BC, x is first parsed with model C[T] (this time
trained on the entire training set) to derive an input ?x, GCx ?, so that the guide features
can be extracted also at parsing time. This input is then passed through model BC.
5.2 The Guided Graph-Based Model
The graph-based model, MSTParser, learns a scoring function s(i, j, l) ? R over labeled
dependencies. As described in Section 3.2, dependency arcs (or pairs of arcs) are repre-
sented by a high dimensional feature vector f(i, j, l) ? Rk, where f is typically a binary
feature vector over properties of the arc as well as the surrounding input (McDonald,
Crammer, and Pereira 2005; McDonald, Lerman, and Pereira 2006). For the guided
graph-based model, which we call MSTMalt, this feature representation is modified to
include an additional argument GMaltx , which is the dependency graph predicted by
MaltParser on the input sentence x. Thus, the new feature representation will map an arc
and the entire predicted MaltParser graph to a high dimensional feature representation,
f(i, j, l, GMaltx ) ? R
k+m. These m additional features account for the guide features over
the MaltParser output. The specific features used by MSTMalt are given in Table 5.
All features are conjoined with the part-of-speech tags of the words involved in the
Table 5
Guide features for MSTMalt and MaltMST.
MSTMalt ? defined over (i, j, l) MaltMST ? defined over (c, t)
(? = any label/node) (? = any label/node)
Is (i, j, ?) in GMaltx ? Is (?
0
c ,?
0
c , ?) in G
MST
x ?
Is (i, j, l) in GMaltx ? Is (?
0
c ,?
0
c , ?) in G
MST
x ?
Is (i, j, ?) not in GMaltx ? Head direction for ?
0
c in G
MST
x (left/right/ROOT)
Is (i, j, l) not in GMaltx ? Head direction for ?
0
c in G
MST
x (left/right/ROOT)
Identity of l? such that (?, j, l?) is in GMaltx ? Identity of l such that (?,?
0
c , l) is in G
MST
x ?
Identity of l? such that (i, j, l?) is in GMaltx ? Identity of l such that (?,?
0
c , l) is in G
MST
x ?
219
Computational Linguistics Volume 37, Number 1
dependency to allow the guided model to learn weights relative to different surface
syntactic environments. Features that include the arc label l are only included in the
second-stage arc-labeler. Though MSTParser is capable of defining features over pairs
of arcs, we restrict the guide features to single arcs as this resulted in higher accuracies
during preliminary experiments.
5.3 The Guided Transition-Based Model
The transition-based model, MaltParser, learns a scoring function s(c, t) ? R over con-
figurations and transitions. The set of training instances for this learning problem is the
set of pairs (c, t) such that t is the correct transition out of c in the transition sequence
that derives the correct dependency graph Gx for some sentence x in the training set
T. As described in Section 3.3, each training instance (c, t) is represented by a feature
vector f(c, t) ? Rk, where features are defined in terms of arbitrary properties of the
configuration c.
For the guided transition-based model, which we call MaltMST, training instances
are extended to triples (c, t, GMSTx ), where G
MST
x is the dependency graph predicted by
the graph-based MSTParser for the sentence x to which the configuration c belongs.
We define m additional guide features, based on properties of GMSTx , and extend the
feature vector accordingly to f(c, t, GMSTx ) ? R
k+m. The specific features used by MaltMST
are given in Table 5. Unlike MSTParser, features are not explicitly defined to conjoin
guide features with part-of-speech features. These features are implicitly added through
the polynomial kernel used to train the SVM.
6. Integrated Parsing Experiments
In this section, we present an experimental evaluation of the two guided models fol-
lowed by a comparative error analysis including both the base models and the guided
models. The data sets used in these experiments are identical to those used in Section 4.
The guided models were trained according to the scheme explained in Section 5, with
two-fold cross-validation when parsing the training data with the guide parsers. Pre-
liminary experiments suggested that cross-validation with more folds had a negligible
impact on the results. Models are evaluated by their labeled attachment score on the test
set using the evaluation software from the CoNLL-X shared task with default settings.9
Statistical significance was assessed using Dan Bikel?s randomized parsing evaluation
comparator with the default setting of 10,000 iterations.10
6.1 Results
Table 6 shows the results, for each language and on average, for the two base models
(MST, Malt) and for the two guided models (MSTMalt, MaltMST). We also give oracle
combination scores based on both by taking the best graph or the best set of arcs
relative to the gold standard, as discussed in Section 4.4. First of all, we see that both
guided models show a consistent increase in accuracy compared to their base model,
even though the extent of the improvement varies across languages from about half a
percentage point (MaltMST on Chinese) up to almost four percentage points (MaltMST on
9 http://nextens.uvt.nl/?conll/software.html.
10 http://www.cis.upenn.edu/?dbikel/software.html.
220
McDonald and Nivre Analyzing and Integrating Dependency Parsers
Table 6
Labeled attachment scores for base parsers and guided parsers (improvement in percentage
points).
Oracle
Language MST MSTMalt Malt MaltMST graph arc
Arabic 66.91 68.64 (+1.73) 66.71 67.80 (+1.09) 70.3 75.8
Bulgarian 87.57 89.05 (+1.48) 87.41 88.59 (+1.18) 90.7 92.4
Chinese 85.90 88.43 (+2.53) 86.92 87.44 (+0.52) 90.8 91.5
Czech 80.18 82.26 (+2.08) 78.42 81.18 (+2.76) 84.2 86.6
Danish 84.79 86.67 (+1.88) 84.77 85.43 (+0.66) 87.9 89.6
Dutch 79.19 81.63 (+2.44) 78.59 79.91 (+1.32) 83.5 86.4
German 87.34 88.46 (+1.12) 85.82 87.66 (+1.84) 89.9 92.0
Japanese 90.71 91.43 (+0.72) 91.65 92.20 (+0.55) 93.2 94.1
Portuguese 86.82 87.50 (+0.68) 87.60 88.64 (+1.04) 90.0 91.6
Slovene 73.44 75.94 (+2.50) 70.30 74.24 (+3.94) 77.2 80.7
Spanish 82.25 83.99 (+1.74) 81.29 82.41 (+1.12) 85.4 88.2
Swedish 82.55 84.66 (+2.11) 84.58 84.31 (?0.27) 86.8 88.8
Turkish 63.19 64.29 (+1.10) 65.58 66.28 (+0.70) 69.3 72.6
Average 80.83 82.53 (+1.70) 80.75 82.01 (+1.27) 84.5 86.9
Slovene).11 It is thus quite clear that both models have the capacity to learn from features
generated by the other model. However, it is also clear that the graph-based MST model
shows a somewhat larger improvement, both on average and for all languages except
Czech, German, Portuguese, and Slovene. Finally, given that the two base models had
the best performance for these data sets at the CoNLL-X shared task, the guided models
achieve a substantial improvement of the state of the art.12 Although there is no statis-
tically significant difference between the two base models, they are both outperformed
by MaltMST (p < 0.0001), which in turn has significantly lower accuracy than MSTMalt
(p < 0.0005).
An extension to the models described so far would be to iteratively integrate the two
parsers in the spirit of pipeline iteration (Hollingshead and Roark 2007). For example,
one could start with a Malt model, use it to train a guided MSTMalt model, then use that
as the guide to train a MaltMSTMalt model, and so forth. We ran such experiments, but
found that accuracy did not increase significantly and in some cases decreased slightly.
This was true regardless of which parser began the iterative process. In retrospect, this
result is not surprising. Because the initial integration effectively incorporates knowl-
edge from both parsing systems, there is little to be gained by adding additional parsers
in the chain.
6.2 Error Analysis
The experimental results presented so far show that feature-based integration (stacking)
is a viable approach for improving the accuracy of both graph-based and transition-
based models for dependency parsing, but they say very little about how the integration
11 The only exception to this pattern is the result for MaltMST on Swedish, where we see an unexpected drop
in accuracy compared to the base model.
12 Martins et al (2008) and Martins, Smith, and Xing (2009) report additional improvements.
221
Computational Linguistics Volume 37, Number 1
benefits the two models and what aspects of the parsing process are improved as a
result. In order to get a better understanding of these matters, we replicate parts of
the error analysis presented in Section 4, but include both integrated models into the
analysis. As in Section 4, for each of the four models evaluated, we aggregate error
statistics for labeled attachment over all 13 languages together.
Figure 8 shows accuracy in relation to sentence length, binned into 10-word
intervals (1?10, 11-20, etc.). As mentioned earlier, Malt and MST have very similar
accuracy for short sentences but Malt degrades more rapidly with increasing sentence
length because of error propagation. The guided models, MaltMST and MSTMalt, behave
in a very similar fashion with respect to each other but both outperform their base
parser over the entire range of sentence lengths. However, except for the two extreme
data points (0?10 and 51?60) there is also a slight tendency for MaltMST to improve more
for longer sentences (relative to its base model) and for MSTMalt to improve more for
short sentences (relative to its base model). Thus, whereas most of the improvement for
the guided parsers seems to come from a higher accuracy in predicting arcs in general,
there is also some evidence that the feature-based integration allows one parser to
exploit the strength of the other.
Figure 9 plots precision (left) and recall (right) for dependency arcs of different
lengths (predicted arcs for precision, gold standard arcs for recall). With respect to recall,
the guided models appear to have a slight advantage over the base models for short and
medium distance arcs. With respect to precision, however, there are two clear patterns.
First, the graph-based models have better precision than the transition-based models
when predicting long arcs, as discussed earlier. Secondly, both the guided models have
better precision than their base model and, for the most part, also their guide model.
In particular MSTMalt outperforms MST for all dependency lengths and is comparable
to Malt for short arcs. More interestingly, MaltMST outperforms both Malt and MST for
arcs up to length 9, which provides evidence that MaltMST has learned specifically to
Figure 8
Accuracy relative to sentence length. Differences between MST+Malt and MST statistically
significant (p < 0.05) at all positions. Differences between Malt+MST and Malt statistically
significant (p < 0.05) at all positions. Differences between MST+Malt and Malt+MST
statistically significant (p < 0.05) at 11?20, 21?30, and 31?40.
222
McDonald and Nivre Analyzing and Integrating Dependency Parsers
Figure 9
Dependency arc precision/recall relative to predicted/gold for dependency length. Precision
between MST+Malt and MST statistically significant (p < 0.05) at 1?7, 9?12, 14, and >14.
Recall between MST+Malt and MST statistically significant (p < 0.05) at 1?7, 9, 14, and >14.
Precision between Malt+MST and Malt statistically significant (p < 0.05) at 1?8, 10?13,
and > 14. Recall between Malt+MST and Malt statistically significant (p < 0.05) at 1?12, 14,
and >14. Precision between MST+Malt and Malt+MST statistically significant (p < 0.05) at 1 and
9?>14. Recall between MST+Malt and Malt+MST statistically significant (p < 0.05) at 1, 2, 3, 14,
and >14.
trust the guide features from MST for longer dependencies (those greater than length 4)
and its own base features for shorter dependencies (those less than or equal to length 4).
However, for dependencies of length greater than 9, the performance of MaltMST begins
to degrade. Because the absolute number of dependencies of length greater than 9 in
the training sets is relatively small, it might be difficult for MaltMST to learn from the
guide parser in these situations. Interestingly, both models seem to improve most in
the medium range (roughly 8?12 words), although this pattern is clearer for MSTParser
than for MaltParser.
Figure 10 shows precision (left) and recall (right) for dependency arcs at different
distances from the root (predicted arcs for precision, gold standard arcs for recall).
Figure 10
Dependency arc precision/recall relative to predicted/gold for distance to root. Precision
between MST+Malt and MST statistically significant (p < 0.05) at 1?6. Recall between MST+Malt
and MST statistically significant (p < 0.05) at all positions. Precision between Malt+MST and
Malt statistically significant (p < 0.05) at 1?4. Recall between Malt+MST and Malt statistically
significant (p < 0.05) at all positions. Precision between MST+Malt and Malt+MST statistically
significant (p < 0.05) at 1, 2, 3, 6, and >6. Recall between MST+Malt and Malt+MST
statistically significant (p < 0.05) at 4 and >6.
223
Computational Linguistics Volume 37, Number 1
Again, we find the clearest patterns in the graphs for precision, where Malt has very
low precision near the root but improves with increasing depth, whereas MST shows
the opposite trend, as observed earlier. Considering the guided models, it is clear that
MaltMST improves in the direction of its guide model, with a five-point increase in
precision for dependents of the root and smaller improvements for longer distances
(where its base model is most accurate). Similarly, MSTMalt improves precision the
largest in the range where its base model is inferior to Malt (roughly distances of 2?
6) and is always superior to its base model. This again indicates that the guided models
are learning from their guide models as they improve the most in situations where the
base model has inferior accuracy.
Table 7 gives the accuracy for arcs relative to dependent part of speech. As observed
earlier, we see that MST does better than Malt for all categories except nouns and
pronouns. But we also see that the guided models in all cases improve over their base
model and, in most cases, also over their guide model. The general trend is that MST
improves more than Malt, except for adjectives and conjunctions, where Malt has a
greater disadvantage from the start and therefore benefits more from the guide features.
The general trend is that the parser with worse performance for a particular part-of-
speech tag improves the most in terms of absolute accuracy (5 out of 7 cases), again
suggesting that the guided models are learning when to trust their guide features. The
exception here is verbs and adverbs, where MST has superior performance to Malt, but
MSTMalt has a larger increase in accuracy than MaltMST.
Considering the results for parts of speech, as well as those for dependency length
and root distance, it is interesting to note that the guided models often improve even
in situations where their base models are more accurate than their guide models. This
suggests that the improvement is not a simple function of the raw accuracy of the guide
model but depends on the fact that labeled dependency decisions interact in inference
algorithms for both graph-based and transition-based parsing systems. Thus, if a parser
can improve its accuracy on one class of dependencies (for example, longer ones), then
we can expect to see improvements on all types of dependencies?as we do.
6.3 Discussion
In summation, it is clear that both guided models benefit from a higher accuracy in
predicting arcs in general, which results in better performance regardless of sentence
length, dependency length, or dependency depth. However, there is strong evidence
that MSTMalt improves in the direction of Malt, with a slightly larger improvement
compared to its base model for short sentences and short dependencies (but not for deep
Table 7
Accuracy relative to dependent part of speech (improvement in percentage points).
Part of Speech MST MSTMalt Malt MaltMST
Verb 82.6 85.1 (2.5) 81.9 84.3 (2.4)
Noun 80.0 81.7 (1.7) 80.7 81.9 (1.2)
Pronoun 88.4 89.4 (1.0) 89.2 89.3 (0.1)
Adjective 89.1 89.6 (0.5) 87.9 89.0 (1.1)
Adverb 78.3 79.6 (1.3) 77.4 78.1 (0.7)
Adposition 69.9 71.5 (1.6) 68.8 70.7 (1.9)
Conjunction 73.1 74.9 (1.8) 69.8 72.5 (2.7)
224
McDonald and Nivre Analyzing and Integrating Dependency Parsers
dependencies). Conversely, MaltMST improves in the direction of MST, with a larger
improvement for long sentences and for dependents of the root.
The question remains why MST generally benefits more from the feature-based
integration. The likely explanation is the previously mentioned interaction between
different dependency decisions at inference time. Because inference in MST is exact
(or nearly exact), an improvement in one type of dependency has a good chance of
influencing the accuracy of other dependencies, whereas in the transition-based model,
where inference is greedy, some of these additional benefits will be lost because of error
propagation. This is reflected in the error analysis in the following recurrent pattern:
Where Malt does well, MaltMST does only slightly better. But where MST is good,
MSTMalt is often significantly better. Furthermore, this observation easily explains the
limited increases in accuracy of words with verb and adverb modifiers that is observed
in MaltMST relative to MSTMalt (Table 7) as these dependencies occur close to the root
and have increased likelihood of being affected by error propagation.
Another part of the explanation may have to do with the learning algorithms used
by the systems. Although both Malt and MST use discriminative algorithms, Malt uses
a batch learning algorithm (SVM) and MST uses an on-line learning algorithm (MIRA).
If the original rich feature representation of Malt is sufficient to separate the training
data, regularization may force the weights of the guided features to be small (as they
are not needed at training time). On the other hand, an on-line learning algorithm will
recognize the guided features as strong indicators early in training and give them a high
weight as a result. Frequent features with high weight early in training tend to have the
most impact on the final classifier due to both weight regularization and averaging. This
is in fact observed when inspecting the weights of MSTMalt.
Finally, comparing the results of the guided models to the oracle results discussed
in Section 4.4, we see that there should be room for further improvement, as the best
guided parser (MSTMalt) does not quite reach the level of the graph selection oracle,
let alne that of the arc selection oracle. Further exploration of the space of possible
systems, as outlined in Section 6.3, will undoubtedly be necessary to close this gap.
As already noted, there are several recent developments in data-driven dependency
parsing, which can be seen as targeting the specific weaknesses of traditional graph-
based and transition-based models, respectively. For graph-based parsers, McDonald
and Pereira (2006), Hall (2007), Nakagawa (2007), and Smith and Eisner (2008) attempt
to overcome the limited feature scope of graph-based models by adding global features
in conjunction with approximate inference. Additionally, Riedel and Clarke (2006) and
Martins, Smith, and Xing (2009) integrate global features and maintain exact inference
through integer linear programming solutions. For transition-based models, the trend
is to alleviate error propagation by abandoning greedy, deterministic inference in fa-
vor of beam search with globally normalized models for scoring transition sequences,
either generative (Titov and Henderson 2007a, 2007b) or conditional (Duan, Zhao,
and Xu 2007; Johansson and Nugues 2007). In addition, Zhang and Clark (2008) has
proposed a learning method for transition-based parsers based on global optimization
similar to that traditionally used for graph-based parsers, albeit only with approxi-
mate inference through beam search, and Huang and Sagae (2010) has shown how a
subclass of transition-based parsers can be tabularized to permit the use of dynamic
programming.
One question that can be asked, given the correlation provided here between ob-
served errors and algorithmic expectations, is whether it is possible to characterize the
errors of a new parsing system simply by analyzing its theoretical properties. This is a
difficult question to answer. Consider a parsing system that uses greedy inference. One
225
Computational Linguistics Volume 37, Number 1
can speculate that it will result in error propagation and, as a result, a large number
of parsing errors on long dependencies as well as those close to the root. However,
if the algorithm is run on data that contains only deterministic local decisions and
complex global decisions, such a system might not suffer from error propagation. This
is because the early local decisions are made correctly. Furthermore, saying something
about specific linguistic constructions is also difficult, due to the wide spectrum of
difficulty when parsing certain phenomena across languages. Ultimately, this is an
empirical question. What we have shown here is that, on a number of data sets, our
algorithmic expectations about two widely used dependency parsing paradigms are
confirmed.
7. Conclusion
In this article, we have shown that the two dominant approaches to data-driven depen-
dency parsing?global, exhaustive, graph-based models and local, greedy, transition-
based models?have distinctive error distributions despite often having very similar
parsing accuracy overall. We have demonstrated that these error distributions can be
explained by theoretical properties of the two models, in particular related to the funda-
mental tradeoff between global learning and inference, traditionally favored by graph-
based parsers, and a rich feature space, typically found in transition-based parsers.
Based on this analysis, we have proposed new directions of research on data-driven
dependency parsing, some of which are already beginning to be explored.
We have also demonstrated how graph-based and transition-based models can be
integrated by letting one model learn from features generated by the other, using the
technique known as stacking in the machine learning community. Our experimental
results show that both models consistently improve their accuracy when given access
to features generated by the other model, which leads to a significant advancement
of the state of the art in data-driven dependency parsing. Moreover, a comparative
error analysis reveals that the improvements are predictable from the same theoretical
properties identified in the initial error analysis, such as the tradeoff between global
learning and inference, on the one hand, and rich feature representations, on the other.
On a more general note, we believe that this shows the importance of careful error
analysis, informed by theoretical predictions, for the further advancement of data-
driven methods in natural language processing.
Acknowledgments
We want to thank our collaborators for great
support in developing the parsing
technology, the organizers of the CoNLL-X
shared task for creating the data, and three
anonymous reviewers for their feedback that
substantially improved the article.
References
Attardi, Giuseppe. 2006. Experiments with a
multilanguage non-projective dependency
parser. In Proceedings of the 10th Conference
on Computational Natural Language
Learning (CoNLL), pages 166?170,
New York, NY.
Attardi, Giuseppe and Massimiliano
Ciaramita. 2007. Tree revision learning for
dependency parsing. In Proceedings of
Human Language Technologies: The Annual
Conference of the North American Chapter of
the Association for Computational Linguistics
(NAACL HLT), pages 388?395,
Rochester, NY.
Berger, Adam L., Stephen A. Della Pietra,
and Vincent J. Della Pietra. 1996. A
maximum entropy approach to natural
language processing. Computational
Linguistics, 22:39?71.
Buchholz, Sabine and Erwin Marsi. 2006.
CoNLL-X shared task on multilingual
dependency parsing. In Proceedings of the
226
McDonald and Nivre Analyzing and Integrating Dependency Parsers
10th Conference on Computational Natural
Language Learning (CoNLL), pages 149?164,
New York, NY.
Carreras, Xavier. 2007. Experiments with a
higher-order projective dependency
parser. In Proceedings of the CoNLL Shared
Task of EMNLP-CoNLL 2007,
pages 957?961, Prague.
Chang, Chih-Chung and Chih-Jen Lin. 2001.
LIBSVM: A Library for Support Vector
Machines. Software available at www.csie.
ntu.edu.tw/?cjlin/libsvm.
Charniak, Eugene. 2000. A maximum-
entropy-inspired parser. In Proceedings of
the First Meeting of the North American
Chapter of the Association for Computational
Linguistics (NAACL), pages 132?139,
Seattle, WA.
Cheng, Yuchang, Masayuki Asahara, and
Yuji Matsumoto. 2006. Multi-lingual
dependency parsing at NAIST. In
Proceedings of the 10th Conference on
Computational Natural Language
Learning (CoNLL), pages 191?195,
New York, NY.
Chu, Yoeng-Jin and Tseng-Hong Liu. 1965.
On the shortest arborescence of a
directed graph. Scientia Sinica,
14:1396?1400.
Collins, Michael. 1999. Head-Driven
Statistical Models for Natural Language
Parsing. Ph.D. thesis, University of
Pennsylvania.
Collins, Michael. 2000. Discriminative
reranking for natural language parsing. In
Proceedings of the International Conference on
Machine Learning (ICML), pages 175?182,
Stanford, CA.
Cortes, Corinna and Vladimir Vapnik. 1995.
Support-vector networks. Machine
Learning, 20(3):273?297.
Crammer, Koby, Ofer Dekel, Joseph Keshet,
Shai Shalev-Shwartz, and Yoram Singer.
2006. Online passive-aggressive
algorithms. The Journal of Machine Learning
Research, 7:551?585.
Ding, Yuan and Martha Palmer. 2004.
Synchronous dependency insertion
grammars: A grammar formalism for
syntax based statistical MT. In Workshop
on Recent Advances in Dependency
Grammars (COLING), pages 90?97,
Geneva.
Duan, Xiangyu, Jun Zhao, and Bo Xu. 2007.
Probabilistic parsing action models for
multi-lingual dependency parsing. In
Proceedings of the CoNLL Shared Task
of EMNLP-CoNLL 2007, pages 940?946,
Prague.
Edmonds, Jack. 1967. Optimum branchings.
Journal of Research of the National Bureau of
Standards, 71B:233?240.
Eisner, Jason M. 1996. Three new
probabilistic models for dependency
parsing: An exploration. In Proceedings of
the 16th International Conference on
Computational Linguistics (COLING),
pages 340?345, Copenhagen.
Florian, Radu, Hany Hassan, Abraham
Ittycheriah, Hongyan Jing, Nanda
Kambhatla, Xiaoqiang Luo, Nicolas
Nicolov, and Salim Roukos. 2004. A
statistical model for multilingual entity
detection and tracking. In Proceedings of
Human Language Technology and the
Conference of the North American Chapter
of the Association for Computational
Linguistics (HLT-NAACL), pages 1?8,
Boston, MA.
Hall, Keith. 2007. K-best spanning tree
parsing. In Proceedings of the 45th
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 392?399, Prague.
Hall, Johan, Jens Nilsson, Joakim Nivre,
Gu?lsen Eryig?it, Bea?ta Megyesi, Mattias
Nilsson, and Markus Saers. 2007. Single
malt or blended? A study in multilingual
parser optimization. In Proceedings of the
CoNLL Shared Task of EMNLP-CoNLL 2007,
pages 933?939, Prague.
Henderson, John C. and Eric Brill. 1999.
Exploiting diversity in natural language
processing: Combining parsers. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing
(EMNLP), pages 188?194, College
Park, MD.
Hollingshead, Kristy and Brian Roark. 2007.
Pipeline iteration. In Proceedings of the
45th Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 952?959, Prague.
Huang, Liang and David Chiang. 2005.
Better k-best parsing. In Proceedings of the
9th International Workshop on Parsing
Technologies (IWPT), pages 53?64,
Vancouver.
Huang, Liang and Kenjie Sagae. 2010.
Dynamic programming for linear-time
incremental parsing. In Proceedings of the
48th Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 1077?1086, Uppsala.
Hudson, Richard A. 1984. Word Grammar.
Blackwell, Oxford.
Johansson, Richard and Pierre Nugues. 2007.
Incremental dependency parsing using
227
Computational Linguistics Volume 37, Number 1
online learning. In Proceedings of the
CoNLL Shared Task of EMNLP-CoNLL 2007,
pages 1134?1138, Prague.
Koo, Terry, Amir Globerson, Xavier
Carreras, and Michael Collins. 2010.
Efficient third-order dependency
parsers. In Proceedings of the 48th
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 1?11, Uppsala.
Koo, Terry, Amir Globerson, Xavier Carreras,
and Michael Collins. 2007. Structured
prediction models via the matrix-tree
theorem. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural
Language Processing and Computational
Natural Language Learning (EMNLP-
CoNLL), pages 141?150, Prague.
Kudo, Taku and Yuji Matsumoto. 2002.
Japanese dependency analysis using
cascaded chunking. In Proceedings of the
Sixth Workshop on Computational Language
Learning (CoNLL), pages 63?69, Edmonton.
Lafferty, John, Andrew McCallum, and
Fernando Pereira. 2001. Conditional
random fields: Probabilistic models for
segmenting and labeling sequence data. In
Proceedings of the International Conference on
Machine Learning (ICML), pages 282?289,
Williamstown, MA.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19:313?330.
Martins, Andre F. T., Dipanjan Das,
Noah A. Smith, and Eric P. Xing. 2008.
Stacking dependency parsers. In
Proceedings of the Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 157?166,
Honolulu, HI.
Martins, Andre F. T., Noah A. Smith, and
Eric P. Xing. 2009. Concise integer linear
programming formulations for
dependency parsing. In Proceedings of the
Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint
Conference on Natural Language Processing of
the AFNLP (ACL-IJCNLP), pages 342?350,
Singapore.
Maruyama, Hiroshi. 1990. Structural
disambiguation with constraint
propagation. In Proceedings of the 28th
Meeting of the Association for Computational
Linguistics (ACL), pages 31?38,
Pittsburgh, PA.
McDonald, Ryan. 2006. Discriminative
Learning and Spanning Tree Algorithms for
Dependency Parsing. Ph.D. thesis,
University of Pennsylvania.
McDonald, Ryan, Koby Crammer, and
Fernando Pereira. 2005. Online
large-margin training of dependency
parsers. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 91?98,
Ann Arbor, MI.
McDonald, Ryan, Kevin Lerman, and
Fernando Pereira. 2006. Multilingual
dependency analysis with a two-stage
discriminative parser. In Proceedings of the
10th Conference on Computational Natural
Language Learning (CoNLL), pages 216?220,
New York, NY.
McDonald, Ryan and Joakim Nivre. 2007.
Characterizing the errors of data-driven
dependency parsing models. In Proceedings
of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing
and Computational Natural Language
Learning (EMNLP-CoNLL), pages 122?131,
Prague.
McDonald, Ryan and Fernando Pereira. 2006.
Online learning of approximate
dependency parsing algorithms. In
Proceedings of the 11th Conference of the
European Chapter of the Association for
Computational Linguistics (EACL),
pages 81?88, Trento.
McDonald, Ryan, Fernando Pereira,
Kiril Ribarov, and Jan Hajic?. 2005.
Non-projective dependency parsing
using spanning tree algorithms. In
Proceedings of the Human Language
Technology Conference and the Conference on
Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 523?530,
Vancouver.
McDonald, Ryan and Giorgio Satta. 2007.
On the complexity of non-projective
data-driven dependency parsing. In
Proceedings of the 10th International
Conference on Parsing Technologies (IWPT),
pages 122?131, Prague.
Mel?c?uk, Igor. 1988. Dependency Syntax:
Theory and Practice. State University of
New York Press.
Nakagawa, Tetsuji. 2007. Multilingual
dependency parsing using global features.
In Proceedings of the CoNLL Shared Task of
EMNLP-CoNLL 2007, pages 952?956,
Prague.
Nivre, Joakim. 2003. An efficient algorithm
for projective dependency parsing. In
Proceedings of the 8th International
Workshop on Parsing Technologies (IWPT),
pages 149?160, Nancy.
228
McDonald and Nivre Analyzing and Integrating Dependency Parsers
Nivre, Joakim. 2007. Incremental
non-projective dependency parsing.
In Proceedings of Human Language
Technologies: The Annual Conference
of the North American Chapter of the
Association for Computational Linguistics
(NAACL HLT), pages 396?403,
Rochester, NY.
Nivre, Joakim. 2009. Non-projective
dependency parsing in expected linear
time. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on
Natural Language Processing of the
AFNLP (ACL-IJCNLP), pages 351?359,
Singapore.
Nivre, Joakim, Johan Hall, Sandra Ku?bler,
Ryan McDonald, Jens Nilsson, Sebastian
Riedel, and Deniz Yuret. 2007. The CoNLL
2007 shared task on dependency parsing.
In Proceedings of the CoNLL Shared Task of
EMNLP-CoNLL 2007, pages 915?932,
Prague.
Nivre, Joakim, Johan Hall, and Jens Nilsson.
2004. Memory-based dependency parsing.
In Proceedings of the 8th Conference on
Computational Natural Language Learning,
pages 49?56, Boston, MA.
Nivre, Joakim, Johan Hall, Jens Nilsson,
Gu?lsen Eryig?it, and Svetoslav Marinov.
2006. Labeled pseudo-projective
dependency parsing with support vector
machines. In Proceedings of the 10th
Conference on Computational Natural
Language Learning (CoNLL), pages 221?225,
New York, NY.
Nivre, Joakim and Ryan McDonald.
2008. Integrating graph-based and
transition-based dependency parsers.
In Proceedings of the 46th Annual Meeting
of the Association for Computational
Linguistics (ACL), pages 950?958,
Columbus, OH.
Nivre, Joakim and Jens Nilsson. 2005.
Pseudo-projective dependency parsing. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics
(ACL), pages 99?106, Ann Arbor, MI.
Riedel, Sebastian and James Clarke. 2006.
Incremental integer linear programming
for non-projective dependency parsing.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing
(EMNLP), pages 129?137, Sydney.
Riedel, Sebastian, Ruket C?ak?c?, and Ivan
Meza-Ruiz. 2006. Multi-lingual
dependency parsing with incremental
integer linear programming. In Proceedings
of the 10th Conference on Computational
Natural Language Learning (CoNLL),
pages 226?230, New York, NY.
Sagae, Kenji and Alon Lavie. 2006. Parser
combination by reparsing. In Proceedings
of NAACL: Short Papers, pages 129?132,
New York, NY.
Sarkar, Anoop. 2001. Applying co-training
methods to statistical parsing. In
Proceedings of the Second Meeting of the
North American Chapter of the Association for
Computational Linguistics (NAACL),
pages 175?182, Pittsburgh, PA.
Sgall, Petr, Eva Hajic?ova?, and Jarmila
Panevova?. 1986. The Meaning of the
Sentence in Its Pragmatic Aspects. Reidel,
Dordrecht.
Smith, David A. and Jason Eisner. 2008.
Dependency parsing by belief
propagation. Proceedings of the Conference
on Empirical Methods in Natural Language
Processing (EMNLP), pages 145?156,
Honolulu, HI.
Smith, David A. and Noah A. Smith. 2007.
Probabilistic models of nonprojective
dependency trees. In Proceedings of the 2007
Joint Conference on Empirical Methods in
Natural Language Processing and
Computational Natural Language Learning
(EMNLP-CoNLL), pages 132?140, Prague.
Snow, Rion, Dan Jurafsky, and Andrew Y.
Ng. 2005. Learning syntactic patterns for
automatic hypernym discovery. In
Advances in Neural Information Processing
Systems (NIPS), pages 1297?1304,
Vancouver.
Steedman, Mark, Rebecca Hwa, Miles
Osborne, and Anoop Sarkar. 2003.
Corrected co-training for statistical
parsers. In Proceedings of the International
Conference on Machine Learning (ICML),
pages 95?102, Washington, DC.
Tarjan, Robert E. 1977. Finding optimum
branchings. Networks, 7:25?35.
Taskar, Ben, Simon Lacoste-Julien, and Dan
Klein. 2005. A discriminative matching
approach to word alignment. In
Proceedings of the Human Language
Technology Conference and the Conference on
Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 73?80,
Vancouver.
Titov, Ivan and James Henderson. 2007a.
Fast and robust multilingual dependency
parsing with a generative latent variable
model. In Proceedings of the CoNLL
Shared Task of EMNLP-CoNLL 2007,
pages 947?951, Prague.
Titov, Ivan and James Henderson. 2007b. A
latent variable model for generative
229
Computational Linguistics Volume 37, Number 1
dependency parsing. In Proceedings of the
10th International Conference on Parsing
Technologies (IWPT), pages 144?155,
Prague.
Yamada, Hiroyasu and Yuji Matsumoto.
2003. Statistical dependency analysis with
support vector machines. In Proceedings of
the 8th International Workshop on Parsing
Technologies (IWPT), pages 195?206,
Nancy.
Zeman, Daniel and Zdene?k Z?abokrtsky`.
2005. Improving parsing accuracy by
combining diverse dependency parsers.
Proceedings of the International Workshop
on Parsing Technologies, pages 171?178,
Vancouver.
Zhang, Yue and Stephen Clark. 2008.
A tale of two parsers: Investigating
and combining graph-based and
transition-based dependency parsing.
Proceedings of the Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 562?571,
Honolulu, HI.
230
Going to the Roots of Dependency Parsing
Miguel Ballesteros?
Complutense University of Madrid
Joakim Nivre??
Uppsala University
Dependency trees used in syntactic parsing often include a root node representing a dummy
word prefixed or suffixed to the sentence, a device that is generally considered a mere technical
convenience and is tacitly assumed to have no impact on empirical results. We demonstrate that
this assumption is false and that the accuracy of data-driven dependency parsers can in fact be
sensitive to the existence and placement of the dummy root node. In particular, we show that
a greedy, left-to-right, arc-eager transition-based parser consistently performs worse when the
dummy root node is placed at the beginning of the sentence (following the current convention
in data-driven dependency parsing) than when it is placed at the end or omitted completely.
Control experiments with an arc-standard transition-based parser and an arc-factored graph-
based parser reveal no consistent preferences but nevertheless exhibit considerable variation in
results depending on root placement. We conclude that the treatment of dummy root nodes in
data-driven dependency parsing is an underestimated source of variation in experiments and
may also be a parameter worth tuning for some parsers.
1. Introduction
It is a lesson learned in many studies on natural language processing that choosing the
right linguistic representation can be crucial for obtaining high accuracy on a given task.
In constituency-based parsing, for example, adding or deleting nodes in syntactic trees
can have a substantial impact on the performance of a statistical parser. In dependency
parsing, the syntactic representations used offer less opportunity for transformation,
given that the nodes of a dependency tree are basically determined by the tokens of
the input sentence, except for the possible addition of a dummy word acting as the
root of the tree. In this article, we show that even this seemingly trivial modification
can make a difference, and that the exact placement of the dummy root node can
have a significant impact on the accuracy of a given parser. This suggests that the
placement of the dummy root is a parameter worth tuning for certain parsing systems
as well as a source of variation to be taken into account when interpreting experimental
results.
? Universidad Complutense de Madrid, Departamento de Ingenier??a del Software e Inteligencia Artificial,
C/ Prof. Jose? Garc??a Santesmases, s/n, 28040 Madrid, Spain. E-mail: miballes@fdi.ucm.es.
?? Uppsala University, Department of Linguistics and Philology, Box 635, SE-75126 Uppsala, Sweden.
E-mail: joakim.nivre@lingfil.uu.se.
Submission received: 25 July 2012; revised submission received: 13 October 2012; accepted for publication:
19 October 2012.
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 1
2. Dependency Graphs
Dependency-based approaches to syntactic parsing assume that the syntactic structure
of a sentence can be analyzed in terms of binary dependency relations between lexical
units, units that in the simplest case are taken to correspond directly to the tokens
of the sentence. It is very natural to represent this structure by a directed graph,
with nodes representing input tokens and arcs representing dependency relations.
In addition, we can add labels to arcs in order to distinguish different dependency
types or grammatical functions (e.g., subject, object, adverbial). We call such a graph a
dependency graph.
Dependency graphs are normally assumed to satisfy certain formal constraints,
such as the single-head constraint, which forbids more than one incoming arc to a node,
and the acyclicity constraint, ruling out cyclic graphs. Many dependency theories and
annotation schemes further require that the graph should be a tree, with a unique root
token on which all other tokens are transitively dependent, whereas other frameworks
allow more than one token to be a root in the sense of not having any incoming arc. A
simple and elegant way of reconciling such cross-framework differences and arriving
at a single formalization of dependency structures is to add a dummy root node, a special
node that does not correspond to any input token, and to require that the dependency
graph is a tree rooted at this node. The original tree constraint can then be enforced
by requiring that the special node has exactly one child, but not all frameworks need to
enforce this constraint. An additional advantage of adding a dummy root node is that its
outgoing arcs can be labeled to indicate the functional status of what would otherwise
simply be unlabeled root nodes. With a slight misuse of terminology, we call such labels
informative root labels.1
Because the dummy root node does not correspond to an input token, it has no well-
defined position in the node sequence defined by the word order of a sentence and could
in principle be inserted anywhere (or nowhere at all). One option that can be found in
the literature is to insert it at the end of this sequence, but the more common convention
in contemporary research on dependency parsing is to insert it at the beginning, hence
treating it as a dummy word prefixed to the sentence. This is also the choice implicitly
assumed in the CoNLL data format, used in the CoNLL shared tasks on dependency
parsing in 2006 and 2007 (Buchholz and Marsi 2006; Nivre et al 2007) and the current
de facto standard for exchange of dependency annotated data.
The question that concerns us here is whether the use of a dummy root node is just
a harmless technicality permitting us to treat different dependency theories uniformly,
and whether its placement in the input sequence is purely arbitrary, or whether both of
these choices may in fact have an impact on the parsing accuracy that can be achieved
with a given parsing model. In order to investigate this question empirically, we define
three different types of dependency graphs that differ only with respect to the existence
and placement of the dummy root node:
1. None: Only nodes corresponding to tokens are included in the graph.
2. First: A dummy root node is added as the first token in the sentence.
3. Last: A dummy root node is added as the last token in the sentence.
1 For example, in the Prague Dependency Treebank, which allows multiple children of the dummy root
node, the label may indicate whether the child functions as a main predicate, as the head of a coordinate
structure, or as final punctuation.
6
Ballesteros and Nivre Going to the Roots of Dependency Parsing
Figure 1 illustrates the three types of dependency graphs with examples taken from
the Penn Treebank of English (Marcus, Santorini, and Marcinkiewicz 1993) converted
to dependency structure using the procedure described in Nivre (2006), and the Prague
Dependency Treebank of Czech (Hajic? et al 2001; Bo?hmova? et al 2003). In the former
case, it is assumed that the dummy root node always has exactly one child, with a
dummy dependency label ROOT. In the latter case, the dummy root node may have
several children and these children have informative root labels indicating their func-
tion (Pred and AuxK in the example). Note also that the Czech dependency graph of
type None is not a tree, but a forest, because it consists of two disjoint trees.
3. Experiments
In order to test the hypothesis that the existence and placement of the dummy root node
can have an impact on parsing accuracy, we performed an experiment using two widely
used data-driven dependency parsers, MaltParser (Nivre, Hall, and Nilsson 2006) and
MSTParser (McDonald 2006), and all the 13 data sets from the CoNLL 2006 shared
task on multilingual dependency parsing (Buchholz and Marsi 2006) as well as the
English Penn Treebank converted to Stanford dependencies (de Marneffe, MacCartney,
and Manning 2006). We created three different versions of each data set, corresponding
to the representation types None, First, and Last, and used them to evaluate MaltParser
with two different transition systems?arc-eager (Nivre 2003) and arc-standard (Nivre
2004)?and MSTParser with the arc-factored non-projective algorithm (McDonald
et al 2005). The results are shown in Table 1.
When creating the data sets, we took the original version from the CoNLL-X shared
task as None, because it does not include the dummy root node as an explicit input
token. In this representation, the tokens of a sentence are indexed from 1 to n and the
dependency graph is specified by giving each word a head index ranging from 0 to n,
where 0 signifies that the token is not a dependent on any other token in the sentence.
The First version was created by adding an extra token at the beginning of the sentence
with index 1 and head index 0, increasing all other token and head indices by 1, meaning
that all tokens that previously had a head index of 0 would now be attached to the new
Economic1
 

NMOD
news2
 

SBJ
had3 little4
 

NMOD
effect5
 

OBJ
on6
 

NMOD
financial7
 

NMOD
markets8
 

PMOD
.9

 
P
Z1
 

AuxP
nich2
 

Atr
je3 jen4
 

AuxZ
jedna5
 

Sb
na6
 

AuxP
kvalitu7

 
Adv
.8
ROOT1 Economic2
 

NMOD
news3
 

SBJ
had4
 

ROOT
little5
 

NMOD
effect6
 

OBJ
on7
 

NMOD
financial8
 

NMOD
markets9
 

PMOD
.10

 
P
ROOT1 Z2
 

AuxP
nich3
 

Atr
je4
 

Pred
jen5
 

AuxZ
jedna6
 

Sb
na7
 

AuxP
kvalitu8

 
Adv
.9
 

AuxK
ROOT10Economic1
 

NMOD
news2
 

SBJ
had3
 

ROOT
little4
 

NMOD
effect5
 

OBJ
on6
 

NMOD
financial7
 

NMOD
markets8
 

PMOD
.9

 
P
ROOT9Z1
 

AuxP
nich2
 

Atr
je3
 

Pred
jen4
 

AuxZ
jedna5
 

Sb
na6
 

AuxP
kvalitu7

 
Adv
.8
 

AuxK
Figure 1
Dependency graph types None (top), First (middle), and Last (bottom) for an English
sentence from the Penn Treebank (left) and a Czech sentence taken from the Prague
Dependency Treebank (right). (Gloss of Czech sentence: Z/Out-of nich/them je/is jen/
only jedna/one-FEM-SG na/to kvalitu/quality ./. = ?Only one of them concerns quality.?)
7
Computational Linguistics Volume 39, Number 1
Table 1
Experimental results for arc-eager (AE), arc-standard (AS), and maximum spanning tree
parsing (MST) on all the CoNLL-X data sets plus the English Penn Treebank converted to
Stanford dependencies with three different dependency graph types (None, First, Last).
Evaluation metrics are labeled attachment score (LAS), unlabeled attachment score (UAS), root
attachment (or no attachment in the case of None) measured as recall (RR) and precision (RP).
Scores in bold are best in their column (per language); scores in italic are not comparable to the
rest because of informative arc labels that cannot be predicted with the None representation.
AE AS MST
Language Type LAS UAS RR RP LAS UAS RR RP LAS UAS RR RP
Arabic
None 60.00 75.17 74.24 69.52 60.48 77.29 81.69 80.60 66.73 78.96 84.07 83.78
First 63.63 74.57 84.75 73.75 64.93 77.09 83.73 81.79 66.41 78.32 83.39 90.44
Last 64.15 74.97 73.56 68.24 65.29 77.31 82.71 81.06 66.41 78.32 78.31 87.83
Bulgarian
None 85.76 90.80 94.22 90.80 85.06 90.33 91.96 91.96 86.30 91.64 98.24 98.24
First 84.64 89.83 90.20 87.56 85.12 90.33 91.71 91.71 86.32 91.28 97.49 97.49
Last 85.76 90.78 94.22 90.58 85.16 90.33 91.96 91.96 86.14 91.28 97.24 97.24
Chinese
None 85.13 89.68 93.63 88.42 85.25 90.08 93.06 93.06 86.88 90.82 94.33 94.33
First 84.59 89.09 92.25 89.55 85.23 90.10 92.82 92.82 86.54 90.68 94.33 94.33
Last 85.15 89.70 93.63 88.42 85.17 90.00 92.82 92.82 86.36 90.52 93.87 93.97
Czech
None 68.30 81.14 80.51 74.61 68.36 81.96 87.85 73.35 76.70 85.98 82.20 80.83
First 72.98 79.96 83.33 72.66 74.88 82.52 86.44 88.95 77.04 86.34 85.88 84.92
Last 73.96 81.16 81.07 75.53 74.28 81.78 87.01 73.16 77.68 86.70 89.55 89.55
Danish
None 82.36 87.88 91.33 88.06 81.64 87.86 92.88 93.17 83.39 89.46 92.57 91.44
First 80.60 86.59 86.69 82.11 81.66 87.86 92.88 93.17 83.97 89.84 94.74 94.94
Last 82.38 87.94 91.64 88.36 81.52 87.74 92.88 92.31 83.43 89.42 92.26 92.26
Dutch
None 71.09 74.51 65.56 66.08 70.67 74.43 69.84 72.53 79.05 83.49 79.77 79.92
First 70.81 74.41 72.18 57.88 71.07 75.23 64.20 82.09 78.91 83.43 74.32 83.59
Last 71.05 74.51 65.76 66.67 70.65 74.45 69.84 72.38 78.25 82.95 75.10 85.21
English
None 88.63 90.47 88.70 81.75 88.15 90.07 87.83 86.42 87.55 89.91 87.70 87.70
First 88.00 90.04 83.99 85.42 88.04 89.95 86.20 86.24 87.63 90.00 90.20 90.17
Last 88.57 90.46 88.58 81.73 88.16 90.07 87.91 86.61 87.69 90.06 88.45 88.38
German
None 83.85 86.64 94.68 85.14 84.31 87.22 93.84 93.84 85.64 89.54 97.76 97.76
First 83.29 86.08 89.64 90.40 84.37 87.24 93.84 93.84 85.74 89.66 97.48 97.48
Last 83.93 86.72 94.68 85.14 84.35 87.22 93.84 93.84 85.34 89.50 97.76 97.76
Japanese
None 89.85 92.10 92.74 85.20 90.15 92.30 92.53 85.42 90.45 93.02 93.38 88.47
First 88.79 91.27 88.15 89.20 89.13 91.57 87.83 90.94 90.83 93.36 92.85 91.58
Last 89.77 92.12 92.96 84.56 90.01 92.28 92.64 85.60 90.47 93.06 92.21 90.66
Portuguese
None 79.12 88.60 92.01 85.76 78.32 87.78 90.28 90.28 84.87 89.74 89.58 89.58
First 83.77 88.36 87.85 86.35 83.45 87.82 90.62 90.62 85.19 90.26 91.67 91.67
Last 84.17 88.62 92.01 85.76 83.47 87.80 90.62 90.62 84.89 89.26 90.62 90.31
Spanish
None 78.64 82.51 83.25 76.28 77.88 81.69 81.73 81.73 79.40 83.57 79.70 78.50
First 78.14 82.15 79.70 73.36 77.64 81.51 81.22 81.22 79.20 83.41 83.76 84.18
Last 78.64 82.49 83.76 76.39 77.72 81.55 80.71 81.12 79.48 83.53 84.77 83.50
Swedish
None 83.49 89.60 93.32 90.07 82.65 89.42 92.03 91.56 81.36 88.29 89.97 90.21
First 83.13 89.29 91.77 89.47 82.53 89.38 91.77 91.77 81.76 88.59 91.52 91.75
Last 83.59 89.70 93.32 90.07 82.65 89.36 91.77 91.30 81.66 88.35 92.03 92.03
Slovene
None 64.25 79.56 73.98 63.46 63.67 79.28 75.26 67.35 71.44 82.47 79.08 75.98
First 67.73 77.84 71.94 64.83 69.40 79.42 73.47 78.69 71.72 82.67 79.34 79.34
Last 69.98 79.62 75.00 64.05 69.42 79.28 75.26 67.66 71.64 82.33 76.79 79.21
Turkish
None 56.66 72.18 90.29 92.25 57.00 72.06 90.59 92.13 58.49 74.55 93.47 86.88
First 56.48 71.86 88.77 93.00 56.80 72.12 89.68 94.86 58.59 74.59 92.56 94.28
Last 56.64 72.16 90.14 91.95 56.88 72.10 90.59 92.13 58.89 74.83 93.02 94.45
Average
None 76.94 84.35 86.32 81.24 76.69 84.41 87.24 85.24 79.88 86.53 88.70 87.40
First 77.61 83.67 85.09 81.11 78.16 84.44 86.17 88.48 79.99 86.60 89.25 90.44
Last 78.41 84.35 86.45 81.25 78.20 84.38 87.18 85.18 79.88 86.44 88.71 90.17
8
Ballesteros and Nivre Going to the Roots of Dependency Parsing
dummy root token. The Last version was created by adding an extra token at the end
of the sentence with index n+1, and changing every head index that previously was 0
to n+1. In both First and Last, we made sure that the new dummy token had a unique
word form and unique values for all other features, so that it could not be mistaken for
any real word. For First and Last, we applied an inverse transformation to the parser
output before evaluation.
Both MaltParser and MSTParser by default add a dummy root node at the be-
ginning of the sentence internally before parsing, so we had to modify the parsers
so that they only considered arcs involving nodes corresponding to input tokens. For
MaltParser this only required setting a flag that makes the parser start with an empty
stack instead of a stack containing an extra dummy root node.2 For MSTParser, we
modified the parser implementation so that it extracts a maximum spanning tree that
is still rooted in an extra dummy root node but where the score of a tree is based only
on the scores of arcs connecting real token nodes. Finally, because MaltParser with the
arc-eager and arc-standard transition systems can only construct projective dependency
graphs, we projectivized all training sets before training the MaltParser models using
the baseline pseudo-projective transformation of Nivre and Nilsson (2005).3 Except for
these modifications, all parsers were run with out-of-the-box settings.
3.1 Deterministic Arc-Eager Parsing
The arc-eager transition-based parser first described in Nivre (2003) parses a sentence
in a single pass from left to right, using a stack to store partially processed tokens
and greedily choosing the highest-scoring parsing action at each point. The arc-eager
property entails that every arc in the output graph is added at the earliest possible
opportunity, which means that right-dependents are attached to their head before they
have found their own right-dependents. This can be an advantage because the early
attachment neither implies nor precludes the later addition of right-dependents, but
it can also be a drawback because it forces the parser to make an early commitment
about right-dependents. In this context, it is especially relevant that the addition of a
dummy root node at the beginning of the sentence (First) forces the parser to make
an early commitment regarding dependents of this root node. By contrast, if a dummy
root node is added at the end of a sentence (Last), decisions regarding root dependents
will be postponed until the end. Similarly, if no root node is added (None), then these
decisions will not be explicitly modeled at all, meaning that whatever nodes remain
on the stack after parsing will be treated as root dependents.
As can be seen from Table 1, the arc-eager parser performs consistently worse under
the First condition, with an average unlabeled attachment score (UAS) of 83.67 over the
14 languages, to be compared with 84.35 for None and Last. The difference in accuracy
between First and None/Last ranges from 0.10 for Dutch to 1.72/1.78 for Slovene, and
the difference in means is highly statistically significant (p < 0.001, Wilcoxon signed-
rank test). The difference between None and Last is never greater than 0.20 (and very
far from statistically significant), indicating that either postponing or excluding root
attachment decisions leads to very similar performance for the arc-eager parser. The
2 The exact commandline flag is -allow root false. A side effect of this flag is that all unattached tokens
get the dummy label ROOT, meaning that informative root labels cannot be predicted for representations
of type None. See http://maltparser.org for more information.
3 This is done with the MaltParser flag -pp baseline.
9
Computational Linguistics Volume 39, Number 1
same pattern is found for labeled attachment score (LAS), but here we can only directly
compare First and Last because the Arabic, Czech, Portuguese, and Slovene data sets
contain informative root labels that cannot be predicted under the None condition (cf.
footnote 2). The difference in means between First and Last is 0.80 and again highly
statistically significant (p < 0.001, Wilcoxon signed-rank test). A closer look at the root
accuracy suggests that most of the difference stems from a lower recall on root depen-
dents with the First representation, but this pattern is not completely consistent across
languages and Arabic, Czech, and Dutch actually have higher recall. For Czech and
Dutch this is accompanied by lower precision, but for Arabic the First representation
actually gives the best recall and precision of root dependents (but nevertheless the
worst overall attachment score). It is probably significant that the Arabic data set has
the longest sentences with the root word often appearing early in the sentence. Hence,
an early commitment to root attachment is more likely to be correct in this case, even if
it is more error prone in general.
3.2 Deterministic Arc-Standard Parsing
The arc-standard transition-based parser first described in Nivre (2004) is similar to the
arc-eager parser in that it parses a sentence in a single pass from left to right, using
a stack to store partially processed tokens and greedily choosing the highest-scoring
parsing action at each point. It differs by postponing the attachment of right-dependents
until the complete subtree under the dependent itself has been built. As a consequence,
the dependency tree is built strictly bottom?up, which means that attachment to a
dummy root node will always happen at the end, regardless of whether the dummy
root node is positioned at the beginning or at the end of the sentence. There is therefore
no reason to expect that the placement of the dummy root node should have the same
impact as for the arc-eager parser.
Looking at the results for the arc-standard parser in Table 1 confirms this expecta-
tion, with the three conditions giving very similar mean UAS (84.41 for None, 84.44 for
First, 84.38 for Last) and none of the differences being statistically significant. For LAS,
we can again only directly compare First and Last, but there is practically no difference
in the means here either (78.16 for First vs. 78.20 for Last). Nevertheless, it is worth
noting that, for individual languages, differences in scores can be quite substantial.
Thus, for Dutch, the First condition outperforms the None/Last condition by 0.80/0.78
in UAS and 0.40/0.42 in LAS. Conversely, for Japanese, the None/Last conditions are
better than First by 0.73/0.71 in UAS and 1.02/0.88 in LAS. Although the general trend
is that None and Last give the most similar results, just as for the arc-eager parser,
there are also cases like Chinese where None and First are both slightly better than
Last. Zooming in on root accuracy, we see a clear gain in precision (and marginal drop
in recall) for the First representation, which is probably an effect of the arc-standard
strategy where the attachment of right-dependents often have to be delayed whereas
left-dependents can be attached eagerly.
3.3 Maximum Spanning Tree Parsing
The maximum spanning tree parser described in McDonald et al (2005) uses a very
different parsing model compared with the two transition-based parsers. Instead of
scoring individual parsing actions, it scores all possible dependency arcs in the sentence
and then uses exact inference to extract the highest-scoring complete dependency tree
10
Ballesteros and Nivre Going to the Roots of Dependency Parsing
under an arc-factored model, where the score of each tree is the sum of the scores of
its component arcs. Because the parsing algorithm does not impose any ordering at all
on different attachments, we would expect even less impact from the placement of the
dummy root node than for the deterministic arc-standard parser.
The results in Table 1 do not quite confirm this expectation. The mean UAS varies
from 86.44 for the Last condition to 86.60 for the First condition, and the mean LAS
is 79.88 for None and Last but 79.99 for First. Although none of these differences is
statistically significant on the aggregate level, differences can be quite substantial for
individual languages, with First outperforming Last by a whole percentage point in
UAS for Portuguese (but only 0.30 in LAS) and None outperforming both First and Last
by 0.64 for Arabic (and 0.32 in LAS). The fact that LAS differences tend to be smaller
than UAS differences can probably be explained by the fact that MSTParser uses a two-
stage approach, where the second labeling stage is the same for all three conditions.
With respect to root accuracy, the most interesting observation is that both First and
Last seem to given higher precision than None, which suggests that it is an advantage
to represent root attachments explicitly so that features over these arcs can contribute to
the overall score of a parse tree. It is also worth noting that these features are different
for First and Last, despite the arc-factored model, because of the so-called ?in-between
features? that record the part-of-speech tags of words occurring between the head
and the dependent of an arc, which in turn explains why these two conditions do not
always give the same results.
4. Discussion
The main conclusion we draw from this experiment is that the addition of a dummy
word prefixed or suffixed to a sentence is not a mere technical convenience without
impact on empirical results. Whether we include a dummy word representing the root
of the dependency tree and, if so, where we place this word in the sequence of input
tokens, can have a non-negligible effect on parsing accuracy for different parsers?in
some cases resulting in statistically significant differences with a magnitude of several
percentage points according to standard evaluation metrics.
The nature and magnitude of the impact definitely depends on the parsing model
used. Whereas the deterministic arc-eager parser gives consistently worse results with a
dummy root node positioned at the beginning of the sentence, neither the deterministic
arc-standard parser nor the maximum spanning tree parser has any clear preference in
this respect. Although the overall patterns emerging when averaging over many data
sets can largely be explained in this way, there is also considerable variation across data
sets that we do not yet fully understand, however. Zooming in on root accuracy has
allowed us to start forming hypotheses, such as the impact of long sentences in com-
bination with predominantly head-initial structures for Arabic, but a full exploration
of the interaction of parsing models and language-specific properties is clearly outside
the scope of this article and has to be left for future research. Another limitation of
the current study is that it only examines three different parsers, and although this
is clearly sufficient to prove the existence of the phenomenon it will be interesting to
see whether the same patterns can be found if we examine more recent state-of-the-art
methods, going from deterministic parsing to beam search for transition-based parsing
and from arc-factored to higher-order models for graph-based parsing. In this context,
it is also relevant to mention previous work, such as Hall et al (2007) and Attardi
and Dell?Orletta (2009), which have tried to improve parsing accuracy by switching or
11
Computational Linguistics Volume 39, Number 1
combining parsing directions, which implicitly has the effect of changing the position
of the root node (if present).
In conclusion, we believe there may be two methodological lessons to learn from
our experiments. The first is that, for certain parsing models, the existence and place-
ment of the dummy root node is in fact a parameter worth tuning for best performance.
Thus, for the deterministic arc-eager parser, it seems that we can obtain higher parsing
accuracy by placing the dummy root node at the end of the sentence (or omitting
it completely) instead of placing it at the beginning in the sentence, as is currently
the norm in data-driven dependency parsing. The second lesson is that, because the
differences observed between different conditions are sometimes at least as large as
the differences considered significant when comparing different parsing models, the
status of the dummy root node may be an underestimated source of variation and a
variable that needs to be controlled for in experimental evaluations. The current practice
of consistently placing the root node at the beginning of the sentence is one way of
ensuring comparability of results, but given the arbitrariness of this decision together
with our experimental results, it may be worth exploring other representations as well.
Acknowledgments
Thanks to Ryan McDonald, Yoav Goldberg,
and three anonymous reviewers for useful
comments, and to Ryan also for help in
modifying MSTParser. Miguel Ballesteros is
funded by the Spanish Ministry of Education
and Science (TIN2009-14659-C03-01 Project).
References
Attardi, Giuseppe and Felice Dell?Orletta.
2009. Reverse revision and linear tree
combination for dependency parsing. In
Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North
American Chapter of the Association for
Computational Linguistics (NAACL HLT),
pages 261?264, Boulder, CO.
Bo?hmova?, Alena, Jan Hajic?, Eva Hajic?ova?,
and Barbora Hladka?. 2003. The Prague
Dependency Treebank: A three-level
annotation scenario. In Anne Abeille?,
editor, Treebanks: Building and Using
Parsed Corpora. Kluwer, pages 103?127.
Buchholz, Sabine and Erwin Marsi. 2006.
CoNLL-X shared task on multilingual
dependency parsing. In Proceedings of
the 10th Conference on Computational
Natural Language Learning (CoNLL),
pages 149?164, New York, NY.
de Marneffe, Marie-Catherine, Bill
MacCartney, and Christopher D.
Manning. 2006. Generating typed
dependency parses from phrase
structure parses. In Proceedings of
the 5th International Conference on
Language Resources and Evaluation
(LREC), pages 449?454, Genoa.
Hajic?, Jan, Barbora Vidova Hladka, Jarmila
Panevova?, Eva Hajic?ova?, Petr Sgall, and
Petr Pajas. 2001. Prague Dependency
Treebank 1.0. LDC, 2001T10.
Hall, Johan, Jens Nilsson, Joakim Nivre,
Gu?lsen Eryig?it, Bea?ta Megyesi, Mattias
Nilsson, and Markus Saers. 2007. Single
malt or blended? A study in multilingual
parser optimization. In Proceedings of the
CoNLL Shared Task of EMNLP-CoNLL 2007,
pages 933?939, Prague.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19:313?330.
McDonald, Ryan. 2006. Discriminative
Learning and Spanning Tree Algorithms
for Dependency Parsing. Ph.D. thesis,
University of Pennsylvania.
McDonald, Ryan, Fernando Pereira,
Kiril Ribarov, and Jan Hajic?. 2005.
Non-projective dependency parsing using
spanning tree algorithms. In Proceedings of
the Human Language Technology Conference
and the Conference on Empirical Methods in
Natural Language Processing (HLT/EMNLP),
pages 523?530, Vancouver.
Nivre, Joakim. 2003. An efficient algorithm
for projective dependency parsing.
In Proceedings of the 8th International
Workshop on Parsing Technologies (IWPT),
pages 149?160, Nancy.
Nivre, Joakim. 2004. Incrementality in
deterministic dependency parsing.
In Proceedings of the Workshop on
Incremental Parsing: Bringing Engineering
and Cognition Together (ACL), pages 50?57,
Barcelona.
12
Ballesteros and Nivre Going to the Roots of Dependency Parsing
Nivre, Joakim. 2006. Inductive Dependency
Parsing. Springer, Berlin.
Nivre, Joakim, Johan Hall, Sandra Ku?bler,
Ryan McDonald, Jens Nilsson, Sebastian
Riedel, and Deniz Yuret. 2007. The CoNLL
2007 shared task on dependency parsing.
In Proceedings of the CoNLL Shared Task of
EMNLP-CoNLL 2007, pages 915?932,
Prague.
Nivre, Joakim, Johan Hall, and Jens Nilsson.
2006. Maltparser: A data-driven
parser-generator for dependency parsing.
In Proceedings of the 5th International
Conference on Language Resources and
Evaluation (LREC), pages 2216?2219,
Genoa.
Nivre, Joakim and Jens Nilsson. 2005.
Pseudo-projective dependency parsing.
In Proceedings of the 43rd Annual Meeting
of the Association for Computational
Linguistics (ACL), pages 99?106,
Ann Arbor, MI.
13

Parsing Morphologically Rich Languages:
Introduction to the Special Issue
Reut Tsarfaty?
Uppsala University
Djame? Seddah??
Universite? Paris-Sorbonne/INRIA
Sandra Ku?bler?
Indiana University
Joakim Nivre?
Uppsala University
Parsing is a key task in natural language processing. It involves predicting, for each natural
language sentence, an abstract representation of the grammatical entities in the sentence and
the relations between these entities. This representation provides an interface to compositional
semantics and to the notions of ?who did what to whom.? The last two decades have seen great
advances in parsing English, leading to major leaps also in the performance of applications that
use parsers as part of their backbone, such as systems for information extraction, sentiment
analysis, text summarization, and machine translation. Attempts to replicate the success of
parsing English for other languages have often yielded unsatisfactory results. In particular,
parsing languages with complex word structure and flexible word order has been shown to
require non-trivial adaptation. This special issue reports on methods that successfully address
the challenges involved in parsing a range of morphologically rich languages (MRLs). This
introduction characterizes MRLs, describes the challenges in parsing MRLs, and outlines the
contributions of the articles in the special issue. These contributions present up-to-date research
efforts that address parsing in varied, cross-lingual settings. They show that parsing MRLs
addresses challenges that transcend particular representational and algorithmic choices.
1. Parsing MRLs
Parsing is a central task in natural language processing, where a system accepts a
sentence in a natural language as input and provides a syntactic representation of the
? Uppsala University, Department of Linguistics and Philology, Box 635, 75126 Uppsala, Sweden.
E-mail: tsarfaty@stp.lingfil.uu.se.
?? Inria?s Alpage project & Universite? Paris Sorbonne, Maison de la Recherche, 28 rue Serpentes, 75006
Paris, France. E-mail: djame.seddah@paris-sorbonne.fr.
? Indiana University, Department of Linguistics, Memorial Hall 322, Bloomington IN-47405, USA.
E-mail: skuebler@indiana.edu.
? Uppsala University, Department of Linguistics and Philology, Box 635, 75126 Uppsala, Sweden.
E-mail: joakim.nivre@lingfil.uu.se.
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 1
entities and grammatical relations in the sentence as output. The input sentences to a
parser reflect language-specific properties (in terms of the order of words, the word
forms, the lexical items, and so on), whereas the output abstracts away from these
properties in order to yield a structured, formal representation that reflects the functions
of the different elements in the sentence.
The best broad-coverage parsing systems to date use statistical models, possibly
in combination with hand-crafted grammars. They use machine learning techniques
that allow the system to generalize the syntactic patterns characterizing the data. These
machine learning methods are trained on a treebank, that is, a collection of natural
language sentences which are annotated with their correct syntactic analyses. Based on
the patterns and frequencies observed in the treebank, parsing algorithms are designed
to suggest and score novel analyses for unseen sentences, and search for the most likely
analysis.
The release of a large-scale annotated corpus for English, the Wall Street Journal
Penn Treebank (PTB) (Marcus, Santorini, and Marcinkiewicz 1993), led to a significant
leap in the performance of statistical parsing for English (Magerman 1995; Collins 1997;
Charniak 2000; Charniak and Johnson 2005; Petrov et al 2006; Huang 2008; Finkel,
Kleeman, and Manning 2008; Carreras, Collins, and Koo 2008). At the time of their
publication, each of these models improved the state-of-the-art of English parsing,
bringing constituency-based parsing performance on the standard test set of the PTB
to the level of 92% F1-score using the PARSEVAL evaluation metrics (Black et al 1991).
The last decade has seen the development of large-scale annotated treebanks
for languages such as Arabic (Maamouri et al 2004), French (Abeille?, Cle?ment, and
Toussenel 2003), German (Uszkoreit 1987; Skut et al 1997), Hebrew (Sima?an et al
2001), Swedish (Nivre and Megyesi 2007), and others. The availability of syntactically
annotated corpora for these languages had initially raised the hope of attaining the
same level of parsing performance on these languages, by simply porting the existing
models to the newly available corpora.
Early attempts to apply the aforementioned constituency-based parsing models to
other languages have demonstrated that the success of these approaches was rather lim-
ited. This observation was confirmed for individual languages such as Czech (Collins
et al 1999), German (Dubey and Keller 2003), Italian (Corazza et al 2004), French (Arun
and Keller 2005), Modern Standard Arabic (Kulick, Gabbard, andMarcus 2006), Modern
Hebrew (Tsarfaty and Sima?an 2007), and many more (Tsarfaty et al 2010).
The same observation was independently confirmed by parallel research efforts on
data-driven dependency-based parsing (Ku?bler, McDonald, and Nivre 2009). Results
coming from multilingual parsing evaluation campaigns, such as the CoNLL shared
tasks on multilingual dependency parsing, showed significant variation in the results
of the same models applied to a range of typologically different languages. In partic-
ular, these results demonstrated that the morphologically rich nature of some of those
languages makes them inherently harder to parse, regardless of the parsing technique
used (Buchholz and Marsi 2006; Nivre et al 2007a).
Morphologically rich languages (MRLs) express multiple levels of information al-
ready at the word level. The lexical information for each word form in an MRL may
be augmented with information concerning the grammatical function of the word in
the sentence, its grammatical relations to other words, pronominal clitics, inflectional
affixes, and so on. In English, many of these notions are expressed implicitly by word
order and adjacency: The direct object, for example, is generally the first NP after the
verb and thus does not necessarily need an explicit marking. Expressing such functional
information morphologically allows for a high degree of word-order variation, since
16
Tsarfaty et al Parsing Morphologically Rich Languages:
grammatical functions need no longer be strongly associated with syntactic positions.
Furthermore, lexical items appearing in different syntactic contexts may be realized in
different forms. This leads to a high level of word-form variation and complicates lexical
acquisition from small sized corpora.
2. The Overarching Challenges
The complexity of the linguistic patterns found in MRLs was shown to challenge
parsing in many ways. For instance, standard models assume that a word always
corresponds to a unique terminal in the parse tree. In Arabic, Hebrew, Turkish, and other
languages, an input word-token may correspond to multiple terminals. Furthermore,
models developed primarily to parse English draw substantial inference based onword-
order patterns. Parsing non-configurational languages such as Hungarian may require
relying on morphological information to infer equivalent functions. Parsing Czech or
German is further complicated by case syncretism, which precludes a deterministic
correlation between morphological case and grammatical functions. In languages such
as Hungarian or Finnish, the diversity of word forms leads to a high rate of out-of-
vocabulary words unseen in the annotated data. MRL parsing is thus often associated
with increased lexical data sparseness. An MRL parser requires robust statistical meth-
ods for analyzing such phenomena.
Following Tsarfaty et al (2010) we distinguish three overarching challenges that are
associated with parsing MRLs.
(i) The Architectural Challenge. Contrary to English, where the input signal uniquely
determines the sequence of tree terminals, word forms in an MRLmay contain multiple
units of information (morphemes). These morphemes have to be segmented in order to
reveal the basic units of analysis. Furthermore, morphological analysis of MRL words
may be highly ambiguous, and morphological segmentation may be a non-trivial task
for certain languages. Therefore, a parsing architecture for an MRL must contain, at the
very least, a morphological component for segmentation and a syntactic component for
parsing. The challenge is thus to determine how these two models should be combined
in the overall parsing architecture: Should we assume a pipeline architecture, where the
morphological segmentation is disambiguated prior to parsing? Or should we construct
a joint architecture where the model picks out a parse tree and a segmentation at once?
(ii) The Modeling Challenge. The design of a statistical parsing model requires specifying
three formal elements: the formal output representation, the events that can be observed
in the data, and the independence assumptions between these events. For anMRL, com-
plex morphosyntactic interactions may impose constraints on the form of events and on
their possible combination. In such cases, we may need to incorporate morphological
information in the syntactic model explicitly. How should morphological information
be treated in the syntactic model: as explicit tree decoration, as hidden variables, or as
complex objects in their own right? Which morphological features should be explicitly
encoded? Where should we mark morphological features: at the part-of-speech level, at
phrase level, on dependency arcs? How domorphological and syntactic events interact,
and how can we exploit these interactions for inferring correct overall structures?
(iii) The Lexical Challenge. A parsing model for MRLs requires recognizing the morpho-
logical information in each word form. Due to the high level of morphological variation,
however, data-driven systems are not guaranteed to observe all morphological variants
17
Computational Linguistics Volume 39, Number 1
of a word form in a given annotated corpus. How can we assign correct morphological
signatures to the lexical items in the face of such extreme data spareseness? When
devising a model for parsing MRLs, one may want to make use of whatever additional
resources one has access to?morphological analyzers, unlabeled data, and lexica?in
order to extend the coverage of the parser and obtain robust and accurate predictions.
3. Contributions of this Special Issue
This special issue draws attention to the different ways in which researchers work-
ing on parsing MRLs address the challenges described herein. It contains six stud-
ies discussing parsing results for six languages, using both constituency-based and
dependency-based frameworks (cf. Table 1). The first three studies (Seeker and Kuhn;
Fraser et al; Kallmeyer and Maier) focus on parsing European languages and deal
with phenomena that lie within their flexible phrase ordering and rich morphology,
including problems posed by case syncretism. The next two papers (Goldberg and
Elhadad; Marton et al) focus on Semitic languages and study the application of general-
purpose parsing algorithms (constituency-based and dependency-based, respectively)
to parsing such data. They empirically show gaps in performance between different
architectures (pipeline vs. joint , gold vs. machine-predicted input), feature choices, and
techniques for increasing lexical coverage of the parser. The last paper (Green et al) is
a comparative study on multi-word expression (MWE) recognition via two specialized
parsing models applied to both French and Modern Standard Arabic. Let us briefly
outline the individual contributions made by each of the articles in this special issue.
Seeker and Kuhn present a comparative study of dependency parsing for three
European MRLs from different typological language families: German (Germanic),
Czech (Slavonic), and Hungarian (Finno-Ugric). Although all these languages possess
richer morphological marking than English, there is variation among these languages
in terms of the richness of the morphological information encoded in the word forms,
and the ambiguity of these morphological markers. Hungarian is agglutinating, that
is, morphological markers in Hungarian are non-ambiguous and easy to recognize.
German and Czech are fusional languages with different types of case syncretism.
Seeker and Kuhn use the Bohnet Parser (Bohnet 2010) to parse all these languages, and
show that not using morphological information in the statistical feature model is detri-
mental. Using gold morphology significantly improves results for all these languages,
whereas automatically predicted morphology leads to smaller improvements for the
fusional languages, relative to the agglutinating one. To combat this loss in performance,
they add linguistic constraints to the decoder, restricting the possible structures. They
show that a decoding algorithm which filters out dependency parses that do not obey
Table 1
Contributions to the CL special issue on parsing morphologically rich languages (CL-PMRL).
Constituency-Based Dependency-Based
Arabic Green, de Marneffe, and Manning 2013 Marton, Habash, and Rambow 2013
Czech Seeker and Kuhn 2013
French Green, de Marneffe, and Manning 2013
German Kallmeyer and Maier 2013 Seeker and Kuhn 2013
Fraser et al 2013
Hebrew Goldberg and Elhadad 2013
Hungarian Seeker and Kuhn 2013
18
Tsarfaty et al Parsing Morphologically Rich Languages:
predicate-argument constraints allows the authors to obtain more substantial gains
from morphology.
Fraser et al also focus on parsing German, though in a constituency-based setting.
They use a PCFG-based unlexicalized chart parser (Schmid 2004) along with a set of
manual treebank annotations that bring the treebank grammar performance to the level
of automatically predicted states learned by Petrov et al (2006). As in the previous
study, syncretism is shown to cause ambiguity that hurts parsing performance. To
combat this added ambiguity, they use external information sources. In particular, they
show different ways of using information from monolingual and bilingual data sets
in a re-ranking framework for improving parsing accuracy. The bilingual approach
is inspired by machine translation studies and exploits the variation in marking the
same grammatical functions differently across languages for increasing the confidence
of a disambiguation decision in one language by observing a parallel non-ambiguous
structure in the other one.
These two studies use German corpora stripped of discontinuous constituents in
order to benchmark their parsers. In each of these cases, the discontinuities are con-
verted into pure tree structures, thus ignoring the implied long distance dependencies.
Kallmeyer and Maier propose an alternative approach for parsing such languages
by presenting an overall solution for parsing discontinuous structures directly. They
present a parsing model based on Probabilistic Linear Context-Free Rewriting Systems
(PLCFRS), which implements many of the technological advances that were developed
in the context of parsing with PCFGs. In particular, they present a decoding algorithm
based on weighted deductive CKY parsing, and use it in conjunction with PLCFRS
parameters directly estimated from treebank data. Because PLCFRS is a powerful for-
malism, the parser needs to be tuned for speed. The authors present several admissible
heuristics that facilitate faster A* parsing. The authors present parsing results that are
competitive with constituency-based parsing of German while providing invaluable
information concerning discontinuous constituents and long distance dependencies.
Goldberg and Elhadad investigate constituency parsing for Modern Hebrew
(Semitic), a language which is known to have a very rich and ambiguous morpho-
logical structure. They empirically show that an application of the split-and-merge
general-purpose model of Petrov et al (2006) for parsing Hebrew does not guarantee
accurate parsing in and of itself. In order to obtain competitive parsing performance,
they address all three challenges we have noted. In order to deal with the problem of
word segmentation (the architectural challenge), they extend the chart-based decoder
of Petrov et al with a lattice-based decoder. In order to handle morphological marking
patterns (the modeling challenge), they refine the initial treebank with particularly
targeted state-splits, and add a set of linguistic constraints that act as a filter ruling
out trees that violate agreement. Finally, they add information from an external wide-
coverage lexicon to combat lexical sparseness (the lexical challenge). They show that the
contribution of these different methods is cumulative, yielding state-of-the-art results
on constituency parsing of Hebrew.
Marton et al study dependency parsing of Modern Standard Arabic (Semitic)
and attend to the same challenges. They show that for two transition-based parsers,
MaltParser (Nivre et al 2007b) and EasyFirst (Goldberg and Elhadad 2010), controlling
the architectural and modeling choices leads to similar effects. For instance, when
comparing parsing performance on gold and machine-predicted input conditions, they
show that rich informative tag sets are preferred in gold conditions, but smaller tag
sets are preferred in machine-predicted conditions. They further isolate a set of mor-
phological features which leads to significant improvements in the machine-predicted
19
Computational Linguistics Volume 39, Number 1
condition, for both frameworks. They also show that function-based morphological
features are more informative than surface-based features, and that performance loss
that is due to errors in part-of-speech tagging may be restored by training the model
on a joint set of trees encoding gold tags and machine-predicted tags. At the same time,
undirected parsing of EasyFirst shows better accuracy, possibly due to the flexiblity in
phrase ordering. The emerging insight is that tuning morphological information inside
general-purpose parsing systems is of crucial importance for obtaining competitive
performance.
Focusing on Modern Standard Arabic (Semitic) and French (Romance), the last
article of this special issue, by Green et al, may be seen as an applications paper,
treating the task of MWE recognition as a side effect of a joint model for parsing and
MWE identification. The key problem here is knowing what to consider a minimal
unit for parsing, and how to handle parsing in realistic scenarios where MWEs have
not yet been identified. The authors present two parsing models for such a task: a
factored model including a factored lexicon that integrates morphological knowledge
into the Stanford Parser word model (Klein and Manning 2003), and a Dirichlet Process
Tree Substitution Grammar based model (Cohn, Blunsom, and Goldwater 2010). The
latter can be roughly described as Data Oriented Parsing (Bod 1992; Bod, Scha, and
Sima?an 2003) in a Bayesian framework, extended to include specific features that ease
the extraction of tree fragments matching MWEs. Interestingly, those very different
models do provide the same range of performance when confronted with predicted
morphology input. Additional important challenges that are exposed in the context of
this study concern the design of experiments for cross-linguistic comparison in the face
of delicate asymmetries between the French and Arabic data sets.
4. Conclusion
This special issue highlights actively studied areas of research that address parsing
MRLs. Most approaches described in this issue rely on extending existing parsing
models to address three overarching challenges. The joint parsing and segmentation
architecture scenario can be addressed by extending a general-purpose CKY decoder
into a lattice-based decoder. The modeling challenge may be addressed by explicitly
marking morphological features as syntactic state-splits, by modeling discontinuities
in the formal syntactic representation directly, by incorporating hard-coded linguistic
constraints as filters, and so on. The lexical challenge can be addressed by using external
resources such as a wide-coverage lexicon for analyzing unknown words, and the use
of additional monolingual and bilingual data in order to obtain robust statistics in the
face of extreme sparseness.
An empirical observation reflected in the results presented here is that languages
which we refer to as MRLs exhibit their own cross-lingual variation and thus should not
be treated as a single, homogeneous class of languages. Some languages show richer
morphology than others; some languages possess more flexible word ordering than
others; some fusional languages show syncretism (coarse-grained underspecifiedmark-
ers) whereas others use a large set of fine-grained and unambiguous morphological
markers. The next challenge would then be to embrace these variations, and investigate
whether the typological properties of languages can inform us more directly concerning
the adequate methods that can be used to effectively parse them.
As the next research goal, we then set out to obtain a deeper understanding of
how annotation choices paired up with modeling choices systematically correlate with
parsing performance for different languages. Further work in the line of the studies
20
Tsarfaty et al Parsing Morphologically Rich Languages:
presented here is required in order to draw relevant generalizations. Furthermore,
the time is ripe for another multilingual parser evaluation campaign, which would
encourage the community to develop parsing systems that can easily be transferred
from one language type to another. By compiling these recent contributions, we hope
to encourage not only the development of novel systems for parsing individual MRLs,
but also to facilitate the search for more robust, generic cross-linguistic solutions.
Acknowledgments
As guest editors of this special issue, we
wish to thank the regular members and
the guest members of the Computational
Linguistics editorial board for their thorough
work, which allowed us to assemble this
special issue of high quality contributions in
the emerging field of parsing MRLs. We also
want to thank Marie Candito, Jennifer Foster,
Yoav Goldberg, Ines Rehbein, Lamia Tounsi,
and Yannick Versley for their contribution
to the initial proposal for this special issue.
Finally, we want to express our gratitude
to Robert Dale and Suzy Howlett for their
invaluable support throughout the editorial
process.
References
Abeille?, Anne, Lionel Cle?ment, and Franc?ois
Toussenel. 2003. Building a treebank for
French. In Anne Abeille?, editor, Treebanks.
Kluwer, Dordrecht, pages 165?188.
Arun, Abhishek and Frank Keller. 2005.
Lexicalization in crosslinguistic
probabilistic parsing: The case of French.
In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics,
pages 306?313, Ann Arbor, MI.
Black, Ezra, Steven Abney, Dan Flickinger,
Claudia Gdaniec, Ralph Grishman, Philip
Harrison, Donald Hindle, Robert Ingria,
Frederick Jelinek, Judith Klavans, Mark
Liberman, Mitchell Marcus, Salim Roukos,
Beatrice Santorini, and Tomek
Strzalkowski. 1991. A procedure for
quantitatively comparing the syntactic
coverage of English grammars. Speech
Communication, 33(1,2):306?311.
Bod, Rens. 1992. A computational model
of language performance: Data oriented
parsing. In Proceedings of the 14th Conference
on Computational linguistics-Volume 3,
pages 855?859, Nantes.
Bod, Rens, Remko Scha, and Khalil Sima?an,
editors. 2003. Data-Oriented Parsing. CSLI,
Stanford, CA.
Bohnet, Bernd. 2010. Top accuracy and fast
dependency parsing is not a contradiction.
In Proceedings of CoLing, pages 89?97, Sydney.
Buchholz, Sabine and Erwin Marsi. 2006.
CoNLL-X shared task on multilingual
dependency parsing. In Proceedings of
the Tenth Conference on Computational
Language Learning (CoNLL),
pages 149?164, New York, NY.
Carreras, Xavier, Michael Collins, and
Terry Koo. 2008. TAG, dynamic
programming, and the perceptron
for efficient, feature-rich parsing.
In Proceedings of the Twelfth Conference on
Computational Natural Language Learning
(CoNLL), pages 9?16, Manchester.
Charniak, Eugene. 2000. A maximum-
entropy-inspired parser. In Proceedings
of the 1st Annual Meeting of the North
American Chapter of the ACL (NAACL),
pages 132?139, Seattle, WA.
Charniak, Eugene and Mark Johnson.
2005. Coarse-to-fine n-best parsing
and maxent discriminative reranking.
In Proceedings of the 43rd Annual Meeting
of the Association for Computational
Linguistics (ACL 2005), pages 173?180,
Ann Arbor, MI.
Cohn, Trevor, Phil Blunsom, and Sharon
Goldwater. 2010. Inducing tree-
substitution grammars. The Journal of
Machine Learning Research, 11:3053?3096.
Collins, Michael. 1997. Three generative,
lexicalized models for statistical parsing.
In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics,
pages 16?23, Madrid.
Collins, Michael, Jan Hajic?, Lance Ramshaw,
and Christoph Tillmann. 1999. A statistical
parser for Czech. In Proceedings of the 37th
Annual Meeting of the ACL, pages 505?512,
College Park, MD.
Corazza, Anna, Alberto Lavelli, Giogio Satta,
and Roberto Zanoli. 2004. Analyzing an
Italian treebank with state-of-the-art
statistical parsers. In Proceedings of the
Third Workshop on Treebanks and Linguistic
Theories (TLT 2004), pages 39?50, Tu?bingen.
Dubey, Amit and Frank Keller. 2003.
Probabilistic parsing for German using
sister-head dependencies. In Proceedings of
the 41st Annual Meeting of the Association for
Computational Linguistics, pages 96?103,
Ann Arbor, MI.
21
Computational Linguistics Volume 39, Number 1
Finkel, Jenny Rose, Alex Kleeman, and
Christopher D. Manning. 2008. Efficient,
feature-based, conditional random field
parsing. In Proceedings of ACL-08: HLT,
pages 959?967, Columbus, OH.
Goldberg, Yoav and Michael Elhadad. 2010.
An efficient algorithm for easy-first
non-directional dependency parsing.
In Human Language Technologies: The
2010 Annual Conference of the North
American Chapter of the Association
for Computational Linguistics,
pages 742?750, Los Angeles, CA.
Huang, Liang. 2008. Forest reranking:
Discriminative parsing with non-local
features. In Proceedings of ACL-08: HLT,
pages 586?594, Columbus, OH.
Klein, Dan and Christopher D. Manning.
2003. Accurate unlexicalized parsing.
In Proceedings of the 41st Annual Meeting on
Association for Computational Linguistics,
pages 423?430, Sapporo.
Ku?bler, Sandra, Ryan McDonald, and Joakim
Nivre. 2009. Dependency Parsing. Number 2
in Synthesis Lectures on Human Language
Technologies. Morgan & Claypool
Publishers.
Kulick, Seth, Ryan Gabbard, and Mitchell
Marcus. 2006. Parsing the Arabic treebank:
Analysis and improvements. In Proceedings
of the 5th International Workshop on
Treebanks and Linguistic Theories (TLT),
pages 31?42, Prague.
Maamouri, Mohamed, Anne Bies, Tim
Buckwalter, and Wigdan Mekki. 2004.
The Penn Arabic Treebank: Building a
large-scale annotated Arabic corpus. In
Proceedings of the NEMLAR Conference
on Arabic Language Resources and Tools,
pages 102?109, Cairo.
Magerman, David M. 1995. Statistical
decision-tree models for parsing.
In Proceedings of the 33rd Annual Meeting on
Association for Computational Linguistics,
pages 276?283, Cambridge, MA.
Marcus, Mitchell, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English:
The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Nivre, Joakim, Johan Hall, Sandra Ku?bler,
Ryan McDonald, Jens Nilsson, Sebastian
Riedel, and Deniz Yuret. 2007a. The
CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL 2007
Shared Task. Joint Conference on Empirical
Methods in Natural Language Processing and
Computational Natural Language Learning
(EMNLP-CoNLL), pages 915?932, Prague.
Nivre, Joakim, Johan Hall, Jens Nilsson,
Atanas Chanev, Gu?ls?en Eryig?it,
Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007b. MaltParser:
A language-independent system for
data-driven dependency parsing. Natural
Language Engineering, 13(2):95?135.
Nivre, Joakim and Beata Megyesi. 2007.
Bootstrapping a Swedish Treebank
using cross-corpus harmonization and
annotation projection. In Proceedings of the
Sixth International Workshop on Treebanks
and Linguistic Theories (TLT), pages 97?102,
Bergen.
Petrov, Slav, Leon Barrett, Romain Thibaux,
and Dan Klein. 2006. Learning accurate,
compact, and interpretable tree
annotation. In Proceedings of the 21st
International Conference on Computational
Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics,
pages 433?440, Sydney.
Schmid, Helmut. 2004. Efficient parsing of
highly ambiguous context-free grammars
with bit vectors. In Proceedings of the 20th
International Conference on Computational
Linguistics (COLING 2004), pages 162?168,
Geneva.
Sima?an, Khalil, Alon Itai, Yoad Winter,
Alon Altmann, and Noa Nativ. 2001.
Building a tree-bank of Modern Hebrew
text. Traitement Automatique des Langues,
42:347?380.
Skut, Wojciech, Brigitte Krenn, Thorsten
Brants, and Hans Uszkoreit. 1997.
An annotation scheme for free word
order languages. In Proceedings
of the Fifth Conference on Applied
Natural Language Processing (ANLP),
pages 88?95, Washington, D.C.
Tsarfaty, Reut, Djame? Seddah, Yoav
Goldberg, Sandra Ku?bler, Marie
Candito, Jennifer Foster, Yannick Versley,
Ines Rehbein, and Lamia Tounsi. 2010.
Statistical parsing of morphologically
rich languages (SPMRL): What, how and
whither. In Proceedings of the NAACL
Workshop on Statistical Parsing of
Morphologically Rich Languages, pages 1?12,
Los Angeles, CA.
Tsarfaty, Reut and Khalil Sima?an. 2007.
Three-dimensional parametrization for
parsing morphologically rich languages.
In Proceedings of the Tenth International
Conference on Parsing Technologies,
pages 156?167, Prague.
Uszkoreit, Hans. 1987.Word Order and
Constituent Structure in German. CSLI,
Stanford, CA.
22
Constrained Arc-Eager Dependency Parsing
Joakim Nivre?
Uppsala University
Yoav Goldberg??
Bar-Ilan University
Ryan McDonald?
Google
Arc-eager dependency parsers process sentences in a single left-to-right pass over the input
and have linear time complexity with greedy decoding or beam search. We show how such
parsers can be constrained to respect two different types of conditions on the output dependency
graph: span constraints, which require certain spans to correspond to subtrees of the graph,
and arc constraints, which require certain arcs to be present in the graph. The constraints are
incorporated into the arc-eager transition system as a set of preconditions for each transition and
preserve the linear time complexity of the parser.
1. Introduction
Data-driven dependency parsers in general achieve high parsing accuracy without re-
lying on hard constraints to rule out (or prescribe) certain syntactic structures (Yamada
and Matsumoto 2003; Nivre, Hall, and Nilsson 2004; McDonald, Crammer, and Pereira
2005; Zhang and Clark 2008; Koo and Collins 2010). Nevertheless, there are situations
where additional information sources, not available at the time of training the parser,
may be used to derive hard constraints at parsing time. For example, Figure 1 shows
the parse of a greedy arc-eager dependency parser trained on the Wall Street Journal
section of the Penn Treebank before (left) and after (right) being constrained to build a
single subtree over the span corresponding to the named entity ?Cat on a Hot Tin Roof,?
which does not occur in the training set but can easily be found in on-line databases. In
this case, adding the span constraint fixes both prepositional phrase attachment errors.
Similar constraints can also be derived from dates, times, or other measurements that
can often be identified with high precision using regular expressions (Karttunen et al.
1996), but are under-represented in treebanks.
? Uppsala University, Department of Linguistics and Philology, Box 635, SE-75126, Uppsala, Sweden.
E-mail: joakim.nivre@lingfil.uu.se.
?? Bar-Ilan University, Department of Computer Science, Ramat-Gan, 5290002, Israel.
E-mail: yoav.goldberg@gmail.com.
? Google, 76 Buckingham Palace Road, London SW1W9TQ, United Kingdom.
E-mail: ryanmcd@google.com.
Submission received: 26 June 2013; accepted for publication: 10 October 2013.
doi:10.1162/COLI a 00184
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 2
Figure 1
Span constraint derived from a title assisting parsing. Left: unconstrained. Right: constrained.
In this article, we examine the problem of constraining transition-based dependency
parsers based on the arc-eager transition system (Nivre 2003, 2008), which perform a
single left-to-right pass over the input, eagerly adding dependency arcs at the earliest
possible opportunity, resulting in linear time parsing. We consider two types of con-
straints: span constraints, exemplified earlier, require the output graph to have a single
subtree over one or more (non-overlapping) spans of the input; arc constraints instead
require specific arcs to be present in the output dependency graph. The main contri-
bution of the article is to show that both span and arc constraints can be implemented
as efficiently computed preconditions on parser transitions, thus maintaining the linear
runtime complexity of the parser.1
Demonstrating accuracy improvements due to hard constraints is challenging, be-
cause the phenomena we wish to integrate as hard constraints are by definition not
available in the parser?s training and test data. Moreover, adding hard constraints may
be desirable even if it does not improve parsing accuracy. For example, many organi-
zations have domain-specific gazetteers and want the parser output to be consistent
with these even if the output disagrees with gold treebank annotations, sometimes
because of expectations of downstream modules in a pipeline. In this article, we con-
centrate on the theoretical side of constrained parsing, but we nevertheless provide
some experimental evidence illustrating how hard constraints can improve parsing
accuracy.
2. Preliminaries and Notation
Dependency Graphs. Given a set L of dependency labels, we define a dependency graph
for a sentence x = w1, . . . , wn as a labeled directed graph G = (Vx, A), consisting of a
set of nodes Vx = {1, . . . , n}, where each node i corresponds to the linear position of a
word wi in the sentence, and a set of labeled arcs A ? Vx ? L ? Vx, where each arc (i, l, j)
represents a dependency with head wi, dependent wj, and label l. We assume that the
final word wn is always a dummy word ROOT and that the corresponding node n is a
designated root node.
Given a dependency graph G for sentence x, we say that a subgraph G[i,j] =
(V[i,j], A[i,j]) of G is a projective spanning tree over the interval [i, j] (1 ? i ? j ? n) iff
(i) G[i,j] contains all nodes corresponding to words between wi and wj inclusive, (ii) G[i,j]
is a directed tree, and (iii) it holds for every arc (i, l, j) ? G[i,j] that there is a directed path
1 Although span and arc constraints can easily be added to other dependency parsing frameworks, this
often affects parsing complexity. For example, in graph-based parsing (McDonald, Crammer, and Pereira
2005) arc constraints can be enforced within the O(n3) Eisner algorithm (Eisner 1996) by pruning out
inconsistent chart cells, but span constraints require the parser to keep track of full subtree end points,
which would necessitate the use of O(n4) algorithms (Eisner and Satta 1999).
250
Nivre, Goldberg, and McDonald Constrained Arc-Eager Dependency Parsing
from i to every node k such that min(i, j) < k < max(i, j) (projectivity). We now define
two constraints on a dependency graph G for a sentence x:
r G is a projective dependency tree (PDT) if and only if it is a projective
spanning tree over the interval [1, n] rooted at node n.
r G is a projective dependency graph (PDG) if and only if it can be extended
to a projective dependency tree simply by adding arcs.
It is clear from the definitions that every PDT is also a PDG, but not the other way around.
Every PDG can be created by starting with a PDT and removing some arcs.
Arc-Eager Transition-Based Parsing. In the arc-eager transition system of Nivre (2003), a
parser configuration is a triple c = (?|i, j|?, A) such that ? and B are disjoint sublists of
the nodes Vx of some sentence x, and A is a set of dependency arcs over Vx (and some
label set L). Following Ballesteros and Nivre (2013), we take the initial configuration
for a sentence x = w1, . . . , wn to be cs(x) = ([ ], [1, . . . , n], { }), where n is the designated
root node, and we take a terminal configuration to be any configuration of the form
c = ([ ], [n], A) (for any arc set A). We will refer to the list ? as the stack and the list B
as the buffer, and we will use the variables ? and ? for arbitrary sublists of ? and B,
respectively. For reasons of perspicuity, we will write ? with its head (top) to the right
and B with its head to the left. Thus, c = (?|i, j|?, A) is a configuration with the node i on
top of the stack ? and the node j as the first node in the buffer B.
There are four types of transitions for going from one configuration to the next,
defined formally in Figure 2 (disregarding for now the Added Preconditions column):
r LEFT-ARCl adds the arc (j, l, i) to A, where i is the node on top of the stack
and j is the first node in the buffer, and pops the stack. It has as a
precondition that the token i does not already have a head.
r RIGHT-ARCl adds the arc (i, l, j) to A, where i is the node on top of the stack
and j is the first node in the buffer, and pushes j onto the stack. It has as a
precondition that j 6= n.
r REDUCE pops the stack and requires that the top token has a head.
r SHIFT removes the first node in the buffer and pushes it onto the stack,
with the precondition that j 6= n.
A transition sequence for a sentence x is a sequence C0,m = (c0, c1, . . . , cm) of configu-
rations, such that c0 is the initial configuration cs(x), cm is a terminal configuration, and
there is a legal transition t such that ci = t(ci?1) for every i, 1 ? i ? m. The dependency
graph derived by C0,m is Gcm = (Vx, Acm ), where Acm is the set of arcs in cm.
Complexity and Correctness. For a sentence of length n, the number of transitions in
the arc-eager system is bounded by 2n (Nivre 2008). This means that a parser using
greedy inference (or constant width beam search) will run in O(n) time provided that
transitions plus required precondition checks can be performed in O(1) time. This holds
for the arc-eager system and, as we will demonstrate, its constrained variants as well.
The arc-eager transition system as presented here is sound and complete for the set
of PDTs (Nivre 2008). For a specific sentence x = w1, . . . , wn, this means that any transi-
tion sequence for x produces a PDT (soundness), and that any PDT for x is generated by
251
Computational Linguistics Volume 40, Number 2
Transition Added Preconditions
LEFT-ARCl (?|i, j|?, A) ? (?, j|?, A?{(j, l, i)}) ARC CONSTRAINTS
??a ? AC : da = i ? [ha ? ? ? la 6= l]
??a ? A : da = i ??a ? AC : ha = i ? da ? j|?
SPAN CONSTRAINTS
?[IN-SPAN(i) ? s(i) = s(j) ? i = r(s(i))]
?[IN-SPAN(i) ? s(i) 6= s(j) ? i 6= r(s(i))]
?[NONE? IN-SPAN(j) ? s(i) 6= s(j)]
?[ROOT ? IN-SPAN(j) ? s(i) 6= s(j) ? j 6= r(s(j))]
RIGHT-ARCl (?|i, j|?, A) ? (?|i|j,?, A?{(i, l, j)}) ARC CONSTRAINTS
??a ? AC : da = j ? [ha ? ? ? la 6= l]
j 6= n ??a ? AC : ha = j ? da ? i|?
SPAN CONSTRAINTS
?[ENDS-SPAN(j) ? #CC > 1]
?[IN-SPAN(j) ? s(i) = s(j) ? j = r(s(j))]
?[IN-SPAN(j) ? s(i) 6= s(j) ? j 6= r(s(j))]
?[NONE? IN-SPAN(i) ? s(i) 6= s(j)]
?[ROOT ? IN-SPAN(i) ? s(i) 6= s(j) ? i 6= r(s(i))]
REDUCE (?|i, j|?, A) ? (?, j|?, A) ARC CONSTRAINTS
??a ? AC : ha = i ? da ? j|?
?a ? A : da = i SPAN CONSTRAINTS
?[IN-SPAN(i) ? s(i) = s(j) ? i = r(s(i))]
SHIFT (?, i|?, A) ? (?|i,?, A) ARC CONSTRAINTS
??a ? AC : da = j ? ha ? i|?
i 6= n ??a ? AC : ha = j ? da ? i|?
SPAN CONSTRAINTS
?[ENDS-SPAN(j) ? #CC > 0]
Figure 2
Transitions for the arc-eager transition system with preconditions for different constraints. The
symbols ha, la, and da are used to denote the head node, label, and dependent node, respectively,
of an arc a (that is, a = (ha, la, da )); IN-SPAN(i) is true if i is contained in a span in SC; END-SPAN(i)
is true if i is the last word in a span in SC; s(i) denotes the span containing i (with a dummy span
for all words that are not contained in any span); r(s) denotes the designated root of span s
(if any); #CC records the number of connected components in the current span up to and
including the last word that was pushed onto the stack; NONE and ROOT are true if we allow no
outgoing arcs from spans and if we allow outgoing arcs only from the span root, respectively.
some transition sequence (completeness).2 In constrained parsing, we want to restrict
the system so that, when applied to a sentence x, it is sound and complete for the subset
of PDTs that satisfy all constraints.
3. Parsing with Arc Constraints
Arc Constraints. Given a sentence x = w1, . . . , wn and a label set L, an arc constraint
set is a set AC of dependency arcs (i, l, j) (1 ? i, j ? n, i 6= j 6= n, l ? L), where each arc
is required to be included in the parser output. Because the arc-eager system can only
derive PDTs, the arc constraint set has to be such that the constraint graph GC = (Vx, AC)
can be extended to a PDT, which is equivalent to requiring that GC is a PDG. Thus, the
task of arc-constrained parsing can be defined as the task of deriving a PDT G such
2 Although the transition system in Nivre (2008) is complete but not sound, it is trivial to show that the
system as presented here (with the root node at the end of the buffer) is both sound and complete.
252
Nivre, Goldberg, and McDonald Constrained Arc-Eager Dependency Parsing
that GC ? G. An arc-constrained transition system is sound if it only derives proper
extensions of the constraint graph and complete if it derives all such extensions.
Added Preconditions. We know that the unconstrained arc-eager system can derive any
PDT for the input sentence x, which means that any arc in Vx ? L ? Vx is reachable
from the initial configuration, including any arc in the arc constraint set AC. Hence, in
order to make the parser respect the arc constraints, we only need to add preconditions
that block transitions that would make an arc in AC unreachable.3 We achieve this
through the following preconditions, defined formally in Figure 2 under the heading
ARC CONSTRAINTS for each transition:
r LEFT-ARCl in a configuration (?|i, j|?, A) adds the arc (j, l, i) and makes
unreachable any arc that involves i and a node in the buffer (other than
(j, l, i)). Hence, we permit LEFT-ARCl only if no such arc is in AC.
r RIGHT-ARCl in a configuration (?|i, j|?, A) adds the arc (i, l, j) and makes
unreachable any arc that involves j and a node on the stack (other than
(i, l, j)). Hence, we permit RIGHT-ARCl only if no such arc is in AC.
r REDUCE in a configuration (?|i, j|?, A) pops i from the stack and makes
unreachable any arc that involves i and a node in the buffer. Hence, we
permit REDUCE only if no such arc is in AC.
r SHIFT in a configuration (?, i|?, A) moves i to the stack and makes
unreachable any arc that involves j and a node on the stack. Hence,
we permit SHIFTl only if no such arc is in AC.
Complexity and Correctness. Because the transitions remain the same, the arc-constrained
parser will terminate after at most 2n transitions, just like the unconstrained system.
However, in order to guarantee termination, we must also show that at least one
transition is applicable in every non-terminal configuration. This is trivial in the un-
constrained system, where the SHIFT transition can apply to any configuration that
has a non-empty buffer. In the arc-constrained system, SHIFT will be blocked if there
is an arc a ? AC involving the node i to be shifted and some node on the stack, and
we need to show that one of the three remaining transitions is then permissible. If a
involves i and the node on top of the stack, then either LEFT-ARCl and RIGHT-ARCl
is permissible (in fact, required). Otherwise, either LEFT-ARCl or REDUCE must be
permissible, because their preconditions are implied by the fact that AC is a PDG.
In order to obtain linear parsing complexity, we must also be able to check all pre-
conditions in constant time. This can be achieved by preprocessing the sentence x and
arc constraint set AC and recording for each node i ? Vx its constrained head (if any),
its leftmost constrained dependent (if any), and its rightmost constrained dependent (if
any), so that we can evaluate the preconditions in each configuration without having
to scan the stack and buffer linearly. Because there are at most O(n) arcs in the arc
constraint set, the preprocessing will not take more than O(n) time but guarantees that
all permissibility checks can be performed in O(1) time.
Finally, we note that the arc-constrained system is sound and complete in the sense
that it derives all and only PDTs compatible with a given arc constraint set AC for a sen-
tence x. Soundness follows from the fact that, for every arc (i, l, j) ? AC, the preconditions
3 For further discussion of reachability in the arc-eager system, see Goldberg and Nivre (2012, 2013).
253
Computational Linguistics Volume 40, Number 2
force the system to reach a configuration of the form (?|min(i, j),max(i, j)|?, A) in which
either LEFT-ARCl (i > j) or RIGHT-ARCl (i < j) will be the only permissible transition.
Completeness follows from the observation that every PDT G compatible with AC is also
a PDG and can therefore be viewed as a larger constraint set for which every transition
sequence (given soundness) derives G exactly.
Empirical Case Study: Imperatives. Consider the problem of parsing commands to
personal assistants such as Siri or Google Now. In this setting, the distribution of
utterances is highly skewed towards imperatives making them easy to identify.
Unfortunately, parsers trained on treebanks like the Penn Treebank (PTB) typically
do a poor job of parsing such utterances (Hara et al. 2011). However, we know that
if the first word of a command is a verb, it is likely the root of the sentence. If we
take an arc-eager beam search parser (Zhang and Nivre 2011) trained on the PTB, it
gets 82.14 labeled attachment score on a set of commands.4 However, if we constrain
the same parser so that the first word of the sentence must be the root, accuracy
jumps dramatically to 85.56. This is independent of simply knowing that the first
word of the sentence is a verb, as both parsers in this experiment had access to gold
part-of-speech tags.
4. Parsing with Span Constraints
Span Constraints. Given a sentence x = w1, . . . , wn, we take a span constraint set to be
a set SC of non-overlapping spans [i, j] (1 ? i < j ? n). The task of span-constrained
parsing can then be defined as the task of deriving a PDT G such that, for every span
[i, j] ? SC, G[i,j] is a (projective) spanning tree over the interval [i, j]. A span-constrained
transition system is sound if it only derives dependency graphs compatible with the
span constraint set and complete if it derives all such graphs. In addition, we may add
the requirement that no word inside a span may have dependents outside the span
(NONE), or that only the root of the span may have such dependents (ROOT).
Added Preconditions. Unlike the case of arc constraints, parsing with span constraints
cannot be reduced to simply enforcing (and blocking) specific dependency arcs. In
this sense, span constraints are more global than arc constraints as they require en-
tire subgraphs of the dependency graph to have a certain property. Nevertheless,
we can use the same basic technique as before and enforce span constraints by
adding new preconditions to transitions, but these preconditions need to refer to vari-
ables that are updated dynamically during parsing. We need to keep track of two
things:
r Which word is the designated root of a span? A word becomes the
designated root r(s) of its span s if it acquires a head outside the span or
if it acquires a dependent outside the span under the ROOT condition.
r How many connected components are in the subgraph over the current
span up to and including the last word pushed onto the stack? A variable
#CC is set to 1 when the first span word enters the stack, incremented by
1 for every SHIFT and decremented by 1 for every LEFT-ARCl.
4 Data and splits from the Web Treebank of Petrov and McDonald (2012). Commands used for evaluation
were sentences from the test set that had a sentence initial verb root.
254
Nivre, Goldberg, and McDonald Constrained Arc-Eager Dependency Parsing
Given this information, we need to add preconditions that guarantee the following:
r The designated root must not acquire a head inside the span.
r No word except the designated root may acquire a head outside the span.
r The designated root must not be popped from the stack before the last
word of the span has been pushed onto the stack.
r The last word of a span must not be pushed onto the stack in a
RIGHT-ARCl transition if #CC > 1.
r The last word of a span must not be pushed onto the stack in a SHIFT
transition if #CC > 0.
In addition, we must block outside dependents of all words in a span under the NONE
condition, and of all words in a span other than the designated root under the ROOT
condition. All the necessary preconditions are given in Figure 2 under the heading
SPAN CONSTRAINTS.
Complexity and Correctness. To show that the span-constrained parser always terminates
after at most 2n transitions, it is again sufficient to show that there is at least one
permissible transition for every non-terminal configuration. Here, SHIFT is blocked if
the word i to be shifted is the last word of a span and #CC > 0. But in this case, one of the
other three transitions must be permissible. If #CC = 1, then RIGHT-ARCl is permissible;
if #CC > 1 and the word on top of the stack does not have a head, then LEFT-ARCl is
permissible; and if #CC > 1 and the word on top of the stack already has a head, then
REDUCE is permissible (as #CC > 1 rules out the possibility that the word on top of the
stack has its head outside the span). In order to obtain linear parsing complexity, all
preconditions should be verifiable in constant time. This can be achieved during initial
sentence construction by recording the span s(i) for every word i (with a dummy span
for words that are not inside a span) and by updating r(s) (for every span s) and #CC as
described herein.
Finally, we note that the span-constrained system is sound and complete in the
sense that it derives all and only PDTs compatible with a given span constraint set SC for
a sentence x. Soundness follows from the observation that failure to have a connected
subgraph G[i,j] for some span [i, j] ? SC can only arise from pushing j onto the stack in
a SHIFT with #CC > 0 or a RIGHT-ARCl with #CC > 1, which is explicitly ruled out by
the added preconditions. Completeness can be established by showing that a transition
sequence that derives a PDT G compatible with SC in the unconstrained system cannot
violate any of the added preconditions, which is straightforward but tedious.
Empirical Case Study: Korean Parsing. In Korean, white-space-separated tokens corre-
spond to phrasal units (similar to Japanese bunsetsus) and not to basic syntactic cat-
egories like nouns, adjectives, or verbs. For this reason, a further segmentation step is
needed in order to transform the space-delimited tokens to units that are a suitable input
for a parser and that will appear as the leaves of a syntactic tree. Here, the white-space
boundaries are good candidates for posing hard constraints on the allowed sentence
structure, as only a single dependency link is allowed between different phrasal units,
and all the other links are phrase-internal. An illustration of the process is given in
Figure 3. Experiments on the Korean Treebank from McDonald et al. (2013) show that
adding span constraints based on white space indeed improves parsing accuracy for
an arc-eager beam search parser (Zhang and Nivre 2011). Unlabeled attachment score
255
Computational Linguistics Volume 40, Number 2
Figure 3
Parsing a Korean sentence (the man writes the policy decisions) using span constraints derived from
original white space cues indicating phrasal chunks.
increases from an already high 94.10 without constraints to 94.92, and labeled attach-
ment score increases from 89.91 to 90.75.
Combining Constraints. What happens if we want to add arc constraints on top of
the span constraints? In principle, we can simply take the conjunction of the added
preconditions from the arc constraint case and the span constraint case, but some
care is required to enforce correctness. First of all, we have to check that the arc
constraints are consistent with the span constraints and do not require, for example,
that there are two words with outside heads inside the the same span. In addition, we
need to update the variables r(s) already in the preprocessing phase in case the arc
constraints by themselves fix the designated root because they require a word inside
the span to have an outside head or (under the ROOT condition) to have an outside
dependent.
5. Conclusion
We have shown how the arc-eager transition system for dependency parsing can
be modified to take into account both arc constraints and span constraints, without
affecting the linear runtime and while preserving natural notions of soundness and
completeness. Besides the practical applications discussed in the introduction and case
studies, constraints can also be used as partial oracles for parser training.
References
Ballesteros, Miguel and Joakim Nivre.
2013. Getting to the roots of dependency
parsing. Computational Linguistics,
39:5?13.
Eisner, Jason and Giorgio Satta. 1999.
Efficient parsing for bilexical context-free
grammars and head automaton grammars.
In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics,
pages 457?464, Santa Cruz, CA.
Eisner, Jason M. 1996. Three new
probabilistic models for dependency
parsing: An exploration. In Proceedings
of the 16th International Conference on
Computational Linguistics (COLING),
pages 340?345, Copenhagen.
Goldberg, Yoav and Joakim Nivre.
2012. A dynamic oracle for arc-eager
dependency parsing. In Proceedings
of the 24th International Conference on
Computational Linguistics, pages 959?976,
Shanghai.
Goldberg, Yoav and Joakim Nivre. 2013.
Training deterministic parsers with
non-deterministic oracles. Transactions
of the Association for Computational
Linguistics, 1:403?414.
Hara, Tadayoshi, Takuya Matsuzaki, Yusuke
Miyao, and Jun?ichi Tsujii. 2011. Exploring
difficulties in parsing imperatives and
questions. In Proceedings of the 5th
International Joint Conference on Natural
Language Processing, pages 749?757,
Chiang Mai.
Karttunen, Lauri, Jean-Pierre Chanod,
Gregory Grefenstette, and Anne Schiller.
1996. Regular expressions for language
engineering. Natural Language Engineering,
2(4):305?328.
256
Nivre, Goldberg, and McDonald Constrained Arc-Eager Dependency Parsing
Koo, Terry and Michael Collins. 2010.
Efficient third-order dependency parsers.
In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics,
pages 1?11, Uppsala.
McDonald, Ryan, Koby Crammer, and
Fernando Pereira. 2005. Online
large-margin training of dependency
parsers. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics, pages 91?98, Ann Arbor, MI.
McDonald, Ryan, Joakim Nivre, Yvonne
Quirmbach-Brundage, Yoav Goldberg,
Dipanjan Das, Kuzman Ganchev, Keith
Hall, Slav Petrov, Hao Zhang, Oscar
Ta?ckstro?m, Claudia Bedini, Nu?ria
Bertomeu Castello?, and Jungmee Lee.
2013. Universal dependency annotation
for multilingual parsing. In Proceedings of
the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 2:
Short Papers), pages 92?97, Sofia.
Nivre, Joakim. 2003. An efficient algorithm
for projective dependency parsing. In
Proceedings of the 8th International Workshop
on Parsing Technologies, pages 149?160,
Nancy.
Nivre, Joakim. 2008. Algorithms for
deterministic incremental dependency
parsing. Computational Linguistics,
34:513?553.
Nivre, Joakim, Johan Hall, and Jens Nilsson.
2004. Memory-based dependency parsing.
In Proceedings of the 8th Conference on
Computational Natural Language Learning,
pages 49?56, Boston, MA.
Petrov, Slav and Ryan McDonald. 2012.
Overview of the 2012 shared task on
parsing the web. In Notes of the First
Workshop on Syntactic Analysis of
Non-Canonical Language (SANCL),
Montreal.
Yamada, Hiroyasu and Yuji Matsumoto.
2003. Statistical dependency analysis
with support vector machines.
In Proceedings of the 8th International
Workshop on Parsing Technologies,
pages 195?206, Nancy.
Zhang, Yue and Stephen Clark. 2008.
A tale of two parsers: Investigating
and combining graph-based and
transition-based dependency parsing.
In Proceedings of the Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 562?571,
Honolulu, HI.
Zhang, Yue and Joakim Nivre. 2011.
Transition-based parsing with rich
non-local features. In Proceedings of the
49th Annual Meeting of the Association for
Computational Linguistics, pages 188?193,
Portland, OR.
257

Arc-Eager Parsing with the Tree Constraint
Joakim Nivre?
Uppsala University
Daniel Ferna?ndez-Gonza?lez??
University of Vigo
The arc-eager system for transition-based dependency parsing is widely used in natural language
processing despite the fact that it does not guarantee that the output is a well-formed dependency
tree. We propose a simple modification to the original system that enforces the tree constraint
without requiring any modification to the parser training procedure. Experiments on multiple
languages show that the method on average achieves 72% of the error reduction possible and
consistently outperforms the standard heuristic in current use.
1. Introduction
One of the most widely used transition systems for dependency parsing is the arc-
eager system first described in Nivre (2003), which has been used as the backbone
for greedy deterministic dependency parsers (Nivre, Hall, and Nilsson 2004; Goldberg
and Nivre 2012), beam search parsers with structured prediction (Zhang and Clark
2008; Zhang and Nivre 2011), neural network parsers with latent variables (Titov and
Henderson 2007), and delexicalized transfer parsers (McDonald, Petrov, and Hall 2011).
However, in contrast to most similar transition systems, the arc-eager system does
not guarantee that the output is a well-formed dependency tree, which sometimes
leads to fragmented parses and lower parsing accuracy. Although various heuristics
have been proposed to deal with this problem, there has so far been no clean the-
oretical solution that also gives good parsing accuracy. In this article, we present a
modified version of the original arc-eager system, which is provably correct for the
class of projective dependency trees, which maintains the linear time complexity of
greedy (or beam search) parsers, and which does not require any modifications to the
parser training procedure. Experimental evaluation on the CoNLL-X data sets show
that the new system consistently outperforms the standard heuristic in current use,
on average achieving 72% of the error reduction possible (compared with 41% for the
old heuristic).
? Uppsala University, Department of Linguistics and Philology, Box 635, SE-75126, Uppsala, Sweden.
E-mail: joakim.nivre@lingfil.uu.se.
?? Universidad de Vigo, Departamento de Informa?tica, Campus As Lagoas, 32004, Ourense, Spain.
E-mail: danifg@uvigo.es.
Submission received: 25 June 2013; accepted for publication: 4 November 2013.
doi:10.1162/COLI a 00185
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 2
2. The Problem
The dependency parsing problem is usually defined as the task of mapping a sen-
tence x = w1, . . . ,wn to a dependency tree T, which is a directed tree with one node
for each input token wi, plus optionally an artificial root node corresponding to a
dummy word w0, and with arcs representing dependency relations, optionally labeled
with dependency types (Ku?bler, McDonald, and Nivre 2009). In this article, we will
furthermore restrict our attention to dependency trees that are projective, meaning that
every subtree has a contiguous yield. Figure 1 shows a labeled projective dependency
tree.
Transition-based dependency parsing views parsing as heuristic search through a
non-deterministic transition system for deriving dependency trees, guided by a statisti-
cal model for scoring transitions from one configuration to the next. Figure 2 shows the
arc-eager transition system for dependency parsing (Nivre 2003, 2008). A parser con-
figuration consists of a stack ?, a buffer ?, and a set of arcs A. The initial configuration
for parsing a sentence x = w1, . . . ,wn has an empty stack, a buffer containing the words
w1, . . . ,wn, and an empty arc set. A terminal configuration is any configuration with
an empty buffer. Whatever arcs have then been accumulated in the arc set A defines
the output dependency tree. There are four possible transitions from a configuration
He1
 
?
SBJ
wrote2 her3
 
?
IOBJ
a4
 
?
DET
letter5
 
?
DOBJ
.6
?
 
P
Figure 1
Projective labeled dependency tree for an English sentence.
Initial: ([ ], [w1, . . . ,wn], { })
Terminal: (?, [ ],A)
Shift: (?,wi|?,A) ? (?|wi,?,A)
Reduce: (?|wi,?,A) ? (?,?,A) HEAD(wi)
Right-Arc: (?|wi,wj|?,A) ? (?|wi|wj,?,A ? {wi ? wj})
Left-Arc: (?|wi,wj|?,A) ? (?,wj|?,A ? {wi ? wj}) ?HEAD(wi)
Figure 2
Arc-eager transition system for dependency parsing. We use | as list constructor, meaning
that ?|wi is a stack with top wi and remainder ? and wj|? is a buffer with head wj and tail ?.
The condition HEAD(wi) is true in a configuration (?,?,A) if A contains an arc wk ? wi
(for some k).
260
Nivre and Ferna?ndez-Gonza?lez Arc-Eager Parsing with the Tree Constraint
where top is the word on top of the stack (if any) and next is the first word of the
buffer:1
1. Shift moves next to the stack.
2. Reduce pops the stack; allowed only if top has a head.
3. Right-Arc adds a dependency arc from top to next and moves next to the
stack.
4. Left-Arc adds a dependency arc from next to top and pops the stack;
allowed only if top has no head.
The arc-eager system defines an incremental left-to-right parsing order, where left
dependents are added bottom?up and right dependents top?down, which is advanta-
geous for postponing certain attachment decisions. However, a fundamental problem
with this system is that it does not guarantee that the output parse is a projective
dependency tree, only a projective dependency forest, that is, a sequence of adjacent,
non-overlapping projective trees (Nivre 2008). This is different from the closely related
arc-standard system (Nivre 2004), which constructs all dependencies bottom?up and
can easily be constrained to only output trees. The failure to implement the tree con-
straint may lead to fragmented parses and lower parsing accuracy, especially with
respect to the global structure of the sentence. Moreover, even if the loss in accuracy
is not substantial, this may be problematic when using the parser in applications where
downstream components may not function correctly if the parser output is not a well-
formed tree.
The standard solution to this problem in practical implementations, such as Malt-
Parser (Nivre, Hall, and Nilsson 2006), is to use an artificial root node and to attach
all remaining words on the stack to the root node at the end of parsing. This fixes the
formal problem, but normally does not improve accuracy because it is usually unlikely
that more than one word should attach to the artificial root node. Thus, in the error
analysis presented by McDonald and Nivre (2007), MaltParser tends to have very low
precision on attachments to the root node. Other heuristic solutions have been tried,
usually by post-processing the nodes remaining on the stack in some way, but these
techniques often require modifications to the training procedure and/or undermine the
linear time complexity of the parsing system. In any case, a clean theoretical solution to
this problem has so far been lacking.
3. The Solution
We propose a modified version of the arc-eager system, which guarantees that the arc
set A in a terminal configuration forms a projective dependency tree. The new system,
shown in Figure 3, differs in four ways from the old system:
1. Configurations are extended with a boolean variable e, keeping track of
whether we have seen the end of the input, that is, whether we have
passed through a configuration with an empty buffer.
1 For simplicity, we only consider unlabeled parsing here. In labeled parsing, which is used in all
experiments, Right-Arc and Left-Arc also have to select a label for the new arc.
261
Computational Linguistics Volume 40, Number 2
Initial: ([ ], [w1, . . . ,wn], { }, false)
Terminal: ([wi], [ ],A, true)
Shift: (?,wi|?,A, false) ? (?|wi,?,A, [[? = [ ]]])
Unshift: (?|wi, [ ],A, true) ? (?, [wi],A, true) ?HEAD(wi)
Reduce: (?|wi,?,A, e) ? (?,?,A, e) HEAD(wi)
Right-Arc: (?|wi,wj|?,A, e) ?
(?|wi|wj,?,A ? {wi ? wj}, [[e ? ? = [ ]]])
Left-Arc: (?|wi,wj|?,A, e) ? (?,wj|?,A ? {wi ? wj}, e) ?HEAD(wi)
Figure 3
Arc-eager transition system enforcing the tree constraint. The expression [[?]] evaluates to true if
? is true and false otherwise.
2. Terminal configurations have the form ([wi], [ ],A, true), that is, they have
an empty buffer, exactly one word on the stack, and e = true.
3. The Shift transition is allowed only if e = false.
4. There is a new transition Unshift, which moves top back to the buffer and
which is allowed only if top has no head and the buffer is empty.
The new system behaves exactly like the old system until we reach a configuration with
an empty buffer, after which there are two alternatives. If the stack contains exactly one
word, we terminate and output a tree, which was true also in the old system. However,
if the stack contains more than one word, we now go on parsing but are forbidden to
make any Shift transitions. After this point, there are two cases. If the buffer is empty,
wemake a deterministic choice between Reduce and Unshift depending onwhether top
has a head or not. If the buffer is not empty, we non-deterministically choose between
Right-Arc and either Left-Arc or Reduce (the latter again depending on whether top
has a head). Because the new Unshift transition is only used in completely deterministic
cases, we can use the same statistical model to score transitions both before and after we
have reached the end of the input, as long as we make sure to block any Shift transition
favored by the model.
We first show that the new system is still guaranteed to terminate and that the
maximum number of transitions is O(n), where n is the length of the input sentence,
which guarantees linear parsing complexity for greedy (and beam search) parsers with
constant-time model predictions and transitions. From previous results, we know that
the system is guaranteed to reach a configuration of the form (?, [ ],A) in 2n? k transi-
tions, where k = |?| (Nivre 2008).2 In any non-terminal configuration arising from this
point on, we can always perform Reduce or Unshift (in case the buffer is empty) or
Right-Arc (otherwise), which means that termination is guaranteed if we can show that
the number of additional transitions is bounded.
2 This holds because we must move n words from the buffer to the stack (in either a Shift or a Right-Arc
transition) and pop n? k words from the stack (in either a Reduce or Left-Arc transition).
262
Nivre and Ferna?ndez-Gonza?lez Arc-Eager Parsing with the Tree Constraint
Note first that we can perform atmost k? 1 Unshift transitionsmoving aword back
to the buffer (because a word can only be moved back to the buffer if it has no head,
which can only happen once since Shift is now forbidden).3 Therefore, we can perform
at most k? 1 Right-Arc transitions, moving a word back to the stack and attaching
it to its head. Finally, we can perform at most k? 1 Reduce and Left-Arc transitions,
removing a word from the stack (regardless of whether it has first been moved back
to the buffer). In total, we can thus perform at most 2n? k+ 3(k? 1) < 4n transitions,
which means that the number of transitions is O(n).
Having shown that the new system terminates after a linear number of transitions,
we now show that it also guarantees that the output is a well-formed dependency tree.
In order to reach a terminal configuration, we must pop n? 1 words from the stack,
each of which has exactly one incoming arc and is therefore connected to at least one
other node in the graph. Because the word remaining in the stack has no incoming arc
but must be connected to (at least) the last word that was popped, it follows that the
resulting graph is connected with exactly n? 1 arcs, which entails that it is a tree.
It is worth noting that, although the new system can always construct a tree over
the unattached words left on the stack in the first configuration of the form (?, [ ],A),
it may not be able to construct every possible tree over these nodes. More precisely,
a sequence of words wj, . . . ,wk can only attach to a word on the left in the form of
a chain (not as siblings) and can only attach to a word on the right as siblings (not
as a chain). Nevertheless, the new system is both sound and complete for the class
of projective dependency trees, because every terminating transition sequence derives
a projective tree (soundness) and every projective tree is derived by some transition
sequence (completeness). By contrast, the original arc-eager system is complete but not
sound for the class of projective trees.
4. Experiments
In our empirical evaluation we make use of the open-source system MaltParser (Nivre,
Hall, and Nilsson 2006), which is a data-driven parser-generator for transition-based
dependency parsing supporting the use of different transition systems. Besides the
original arc-eager system, which is already implemented in MaltParser, we have added
an implementation of the new modified system. The training procedure used in Malt-
Parser derives an oracle transition sequence for each sentence and gold tree in the
training corpus and uses every configuration?transition pair in these sequences as a
training instance for a multi-class classifier. Because the oracle sequences in the arc-
eager system always produce a well-formed tree, there will be no training instances
corresponding to the extended transition sequences in the new system (i.e., sequences
containing one or more non-terminal configurations of the form (?, [ ],A)). However,
because the Unshift transition is only used in completely deterministic cases, where the
classifier is not called upon to rank alternative transitions, we can make use of exactly
the same classifier for both the old and the new system.4
We compare the original andmodified arc-eager systems on all 13 data sets from the
CoNLL-X shared task on multilingual dependency parsing (Buchholz and Marsi 2006),
3 The number is k? 1, rather than k, because Unshift requires an empty buffer, which together with only
one word on the stack would imply a terminal configuration.
4 Although this greatly simplifies the integration of the new system into existing parsing frameworks, it is
conceivable that accuracy could be improved further through specialized training methods, for example,
using a dynamic oracle along the lines of Goldberg and Nivre (2012). We leave this for future research.
263
Computational Linguistics Volume 40, Number 2
which all assume the existence of a dummy root word prefixed to the sentence. We tune
the feature representations separately for each language and projectivize the training
data for languages with non-projective dependencies but otherwise use default settings
in MaltParser (including the standard heuristic of attaching any unattached tokens to
the artificial root node at the end of parsing for the original system). Because we want
to perform a detailed error analysis for fragmented parses, we initially avoid using the
dedicated test set for each language and instead report results on a development set
created by splitting off 10% of the training data.
Table 1 (columns 2?3) shows the unlabeled attachment score (including punctua-
tion) achieved with the two systems. We see that the new system improves over the
old one by 0.19 percentage points on average, with individual improvements ranging
from 0.00 (Japanese) to 0.50 (Slovene). These differences may seem quantitatively small,
but it must be remembered that the unattached tokens left on the stack in fragmented
parses constitute a very small fraction of the total number of tokens on which these
scores are calculated. In order to get a more fine-grained picture of the behavior of the
two systems, we therefore zoom in specifically on these tokens in the rest of Table 1.
Column 4 shows the number of unattached tokens left on the stack when reaching
the end of the input (excluding the artificial root node). Column 5 shows for how many
of these tokens the correct head is also on the stack (including the artificial root node).
Both statistics are summed over all sentences in the development set. We see from these
figures that the amount of fragmentation varies greatly between languages, from only
four unattached tokens for Japanese to 230 tokens for Slovene. These tendencies seem
to reflect properties of the data sets, with Japanese having the lowest average sentence
length of all languages and Slovene having a high percentage of non-projective depen-
dencies and a very small training set. They also partly explain why these languages
show the smallest and largest improvement, respectively, in overall attachment score.
Table 1
Experimental results for the old and new arc-eager transition systems (development sets).
UAS = unlabeled attachment score; Stack-Token = number of unattached tokens left in the stack
when reaching the end of the input (excluding the artificial root node); Stack-Head = number of
unattached tokens for which the head is also left in the stack (including the artificial root node);
Correct = number of tokens left in the stack that are correctly attached in the final parser output;
Recall = Correct/Stack-Head (%).
UAS Stack Correct Recall
Language Old New Token Head Old New Old New
Arabic 77.38 77.74 38 24 3 22 12.50 91.67
Bulgarian 90.32 90.42 20 15 7 13 46.67 86.67
Chinese 89.28 89.48 33 31 8 18 25.81 58.06
Czech 83.26 83.46 41 36 8 21 22.22 58.33
Danish 88.10 88.17 29 23 18 22 78.26 95.65
Dutch 86.23 86.49 63 57 13 28 22.80 49.12
German 87.44 87.75 66 39 6 27 15.38 69.23
Japanese 93.53 93.53 4 4 4 4 100.00 100.00
Portuguese 87.68 87.84 44 36 15 26 41.67 72.22
Slovene 76.50 77.00 230 173 50 97 28.90 56.07
Spanish 81.43 81.59 57 42 13 22 30.95 52.38
Swedish 88.18 88.33 43 28 13 24 46.43 85.71
Turkish 81.44 81.45 24 16 9 10 56.25 62.50
Average 85.44 85.63 53.23 40.31 12.85 25.69 40.60 72.12
264
Nivre and Ferna?ndez-Gonza?lez Arc-Eager Parsing with the Tree Constraint
Table 2
Results on final test sets. LAS = labeled attachment score. UAS = unlabeled attachment score.
Ara Bul Cze Chi Dan Dut Ger Jap Por Slo Spa Swe Tur Ave
LAS Old 65.90 87.39 86.11 77.82 84.11 77.28 85.42 90.88 83.52 70.30 79.72 83.19 73.92 80.43
New 66.13 87.45 86.27 77.93 84.16 77.37 85.51 90.84 83.76 70.86 79.73 83.19 73.98 80.55
UAS Old 76.33 90.51 89.79 83.09 88.74 79.95 87.92 92.44 86.28 77.09 82.47 88.70 81.09 84.95
New 76.49 90.58 89.94 83.09 88.82 80.18 88.00 92.40 86.33 77.34 82.49 88.76 81.20 85.05
Columns 6 and 7 show, for the old and the new system, howmany of the unattached
tokens on the stack are attached to their correct head in the final parser output, as a
result of heuristic root attachment for the old system and extended transition sequences
for the new system. Columns 8 and 9 show the same results expressed in terms of recall
or error reduction (dividing column 6/7 by column 5). These results clearly demonstrate
the superiority of the new system over the old system with heuristic root attachment.
Whereas the old system correctly attaches 40.60% of the tokens for which a head can be
found on the stack, the new system finds correct attachments in 72.12% of the cases. For
some languages, the effect is dramatic, with Arabic improving from just above 10% to
over 90% and German from about 15% to almost 70%, but all languages clearly benefit
from the new technique for enforcing the tree constraint.
Variation across languages can to a large extent be explained by the proportion
of unattached tokens that should be attached to the artificial root node. Because the
old root attachment heuristic attaches all tokens to the root, it will have 100% recall
on tokens for which this is the correct attachment and 0% recall on all other tokens.
This explains why the old system gets 100% recall on Japanese, where all four tokens
left on the stack should indeed be attached to the root. It also means that, on average,
root attachment is only correct for about 40% of the cases (which is the overall recall
achieved by this method). By contrast, the new system only achieves a recall of 82.81%
on root attachments, but this is easily compensated by a recall of 63.50% on non-root
attachments.
For completeness, we report also the labeled and unlabeled attachment scores (in-
cluding punctuation) on the dedicated test sets from the CoNLL-X shared task, shown
in Table 2. The results are perfectly consistent with those analyzed in depth for the
development sets. The average improvement is 0.12 for LAS and 0.10 for UAS. The
largest improvement is again found for Slovene (0.58 LAS, 0.25 UAS) and the smallest
for Japanese, where there is in fact a marginal drop in accuracy (0.04 LAS/UAS).5 For
all other languages, however, the new system is at least as good as the old system
and in addition guarantees a well-formed output without heuristic post-processing.
Moreover, although the overall improvement is small, there is a statistically significant
improvement in either LAS or UAS for all languages except Bulgarian, Czech, Japanese,
Spanish, and Swedish, and in both LAS and UAS on average over all languages accord-
ing to a randomized permutation test (? = .05) (Yeh 2000). Finally, it is worth noting
that there is no significant difference in running time between the old and the new
system.
5 As we saw in the previous analysis, fragmentation happens very rarely for Japanese and all unattached
tokens should normally be attached to the root node, which gives 100% recall for the baseline parser.
265
Computational Linguistics Volume 40, Number 2
5. Conclusion
In conclusion, we have presented a modified version of the arc-eager transition system
for dependency parsing, which, unlike the old system, guarantees that the output is
a well-formed dependency tree. The system is provably sound and complete for the
class of projective dependency trees, and the number of transitions is still linear in
the length of the sentence, which is important for efficient parsing. The system can be
used without modifying the standard training procedure for greedy transition-based
parsers, because the statistical model used to score transitions is the same as for the
old system. An empirical evaluation on all 13 languages from the CoNLL-X shared task
shows that the new system consistently outperforms the old system with the standard
heuristic of attaching all unattached tokens to the artificial root node. Whereas the
old method only recovers about 41% of the attachments that are still feasible, the new
system achieves an average recall of 72%. Although this gives only a marginal effect on
overall attachment score (at most 0.5%), being able to guarantee that output parses are
always well formed may be critical for downstream modules that take these as input.
Moreover, the proposed method achieves this guarantee as a theoretical property of the
transition system without having to rely on ad hoc post-processing and works equally
well regardless of whether a dummy root word is used or not.
Acknowledgments
This research has been partially funded by
the Spanish Ministry of Economy and
Competitiveness and FEDER (project
TIN2010-18552-C03-01), Ministry of
Education (FPU Grant Program), and Xunta
de Galicia (projects CN 2012/319 and CN
2012/317).
References
Buchholz, Sabine and Erwin Marsi. 2006.
CoNLL-X shared task on multilingual
dependency parsing. In Proceedings of the
10th Conference on Computational Natural
Language Learning (CoNLL), pages 149?164,
New York, NY.
Goldberg, Yoav and Joakim Nivre. 2012.
A dynamic oracle for arc-eager
dependency parsing. In Proceedings
of the 24th International Conference on
Computational Linguistics (COLING),
pages 959?976, Jeju Island.
Ku?bler, Sandra, Ryan McDonald, and
Joakim Nivre. 2009. Dependency Parsing.
Morgan and Claypool.
McDonald, Ryan and Joakim Nivre. 2007.
Characterizing the errors of data-driven
dependency parsing models. In Proceedings
of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing
and Computational Natural Language
Learning (EMNLP-CoNLL), pages 122?131,
Prague.
McDonald, Ryan, Slav Petrov, and Keith
Hall. 2011. Multi-source transfer of
delexicalized dependency parsers. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing
(EMNLP), pages 62?72, Edinburgh.
Nivre, Joakim. 2003. An efficient algorithm
for projective dependency parsing.
In Proceedings of the 8th International
Workshop on Parsing Technologies (IWPT),
pages 149?160, Nancy.
Nivre, Joakim. 2004. Incrementality in
deterministic dependency parsing.
In Proceedings of the Workshop on
Incremental Parsing: Bringing Engineering
and Cognition Together (ACL), pages 50?57,
Stroudsburg, PA.
Nivre, Joakim. 2008. Algorithms for
deterministic incremental dependency
parsing. Computational Linguistics,
34:513?553.
Nivre, Joakim, Johan Hall, and Jens Nilsson.
2004. Memory-based dependency parsing.
In Proceedings of the 8th Conference on
Computational Natural Language Learning
(CoNLL), pages 49?56, Boston, MA.
Nivre, Joakim, Johan Hall, and Jens Nilsson.
2006. Maltparser: A data-driven
parser-generator for dependency parsing.
In Proceedings of the 5th International
Conference on Language Resources and
Evaluation (LREC), pages 2,216?2,219,
Genoa.
Titov, Ivan and James Henderson. 2007.
A latent variable model for generative
266
Nivre and Ferna?ndez-Gonza?lez Arc-Eager Parsing with the Tree Constraint
dependency parsing. In Proceedings of the
10th International Conference on Parsing
Technologies (IWPT), pages 144?155,
Prague.
Yeh, Alexander. 2000. More accurate tests
for the statistical significance of result
differences. In Proceedings of the 18th
International Conference on Computational
Linguistics (COLING), pages 947?953,
Saarbru?ken.
Zhang, Yue and Stephen Clark. 2008.
A tale of two parsers: Investigating
and combining graph-based and
transition-based dependency parsing.
In Proceedings of the Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 562?571,
Honolulu, HI.
Zhang, Yue and Joakim Nivre. 2011.
Transition-based parsing with rich
non-local features. In Proceedings of the
49th Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 188?193, Portland, OR.
267

Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 341?344,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Word Alignment with
Stochastic Bracketing Linear Inversion Transduction Grammar
Markus SAERS and Joakim NIVRE
Computational Linguistics Group
Dept. of Linguistics and Philology
Uppsala University
Sweden
first.last@lingfil.uu.se
Dekai WU
Human Language Technology Center
Dept. of Computer Science and Engineering
HKUST
Hong Kong
dekai@cs.ust.hk
Abstract
The class of Linear Inversion Transduction
Grammars (LITGs) is introduced, and used to
induce a word alignment over a parallel cor-
pus. We show that alignment via Stochas-
tic Bracketing LITGs is considerably faster
than Stochastic Bracketing ITGs, while still
yielding alignments superior to the widely-
used heuristic of intersecting bidirectional
IBM alignments. Performance is measured as
the translation quality of a phrase-based ma-
chine translation system built upon the word
alignments, and an improvement of 2.85 BLEU
points over baseline is noted for French?
English.
1 Introduction
Machine translation relies heavily on word align-
ments, which are usually produced by training IBM-
models (Brown et al, 1993) in both directions and
combining the resulting alignments via some heuris-
tic. Automatically training an Inversion Transduc-
tion Grammar (ITG) has been suggested as a viable
way of producing superior alignments (Saers and
Wu, 2009). The main problem of using Bracket-
ing ITGs for alignment is that exhaustive biparsing
runs in O(n6) time. Several ways to lower the com-
plexity of ITGs has been suggested, but in this paper,
a different approach is taken. Instead of using full
ITGs, we explore the possibility of subjecting the
grammar to a linear constraint, making exhaustive
biparsing of a sentence pair in O(n4) time possible.
This can be further improved by applying pruning.
2 Background
A transduction is the bilingual version of a language.
A language (Ll) can be formally viewed as a set of
sentences, sequences of tokens taken from a speci-
fied vocabulary (Vl). A transduction (Te,f ) between
two languages (Le and Lf ) is then a set of sentence
pairs, sequences of bitokens from the cross produc-
tion of the vocabularies of the two languages being
transduced (Ve,f = Ve ? Vf ). This adds an extra
layer of complexity to finding transductions from
raw bitexts, as an alignment has to be imposed.
Simple (STG) and Syntax Directed (SDTG) Trans-
duction Grammars (Aho and Ullman, 1972) can
be used to parse transductions between context-free
languages. Both work fine as long as a grammar is
given and parsing is done as transduction, that is: a
sentence in one language is rewritten into the other
language. In NLP, interest has shifted away from
hand-crafted grammars, towards stochastic gram-
mars induced from corpora. To induce a stochas-
tic grammar from a parallel corpus, expectations of
all possible parses over a sentence pair are typically
needed. STGs can biparse sentence pairs in polyno-
mial time, but are unable to account for the complex-
ities typically found in natural languages. SDTGs do
account for the complexities in natural languages,
but are intractable for biparsing.
Inversion transductions (Wu, 1995; Wu, 1997) are
a special case of transductions that are not mono-
tone, but where permutations are severely limited.
By limiting the possible permutations, biparsing be-
comes tractable. This in turn means that ITGs can be
induced from parallel corpora in polynomial time,
341
as well as account for most of the reorderings found
between natural languages.
An Inversion transduction is limited so that it
must be expressible as non-overlapping groups, in-
ternally permuted either by the identity permuta-
tion or the inversion permutation (hence the name).
This requirement also means that the grammar is bi-
narizable, yielding a two-normal form. A produc-
tion with the identity permutation is written inside
square brackets, while productions with the inver-
sion permutation is written inside angled brackets.
This gives us a two-normal form that looks like this
(where e/f is a biterminal):
A ? [B C]
A ? ?B C?
A ? e/f
The time complexity for exhaustive ITG biparsing is
O(Gn6), which is typically too large to be applica-
ble to large grammars and long sentence. The gram-
mar constant G can be eliminated by limiting the
grammar to a bracketing ITG (BITG), which only has
one nonterminal symbol. Saers & Wu (2009) show
that it is possible to apply exhaustive biparsing to a
large parallel corpus (? 100, 000 sentence pairs) of
short sentences (?10 tokens in both language). The
word alignments read off the Viterbi parse also in-
creased translation quality when used instead of the
alignments from bidirectional IBM alignments.
The O(n6) time complexity is somewhat pro-
hibitive for large corpora, so pruning in some form is
needed. Saers, Nivre & Wu (2009) introduce a beam
pruning scheme, which reduces time complexity to
O(bn3). They also show that severe pruning is pos-
sible without significant deterioration in alignment
quality. Haghighi et. al (2009) use a simpler aligner
as guidance for pruning, which reduce the time com-
plexity by two orders of magnitude, and also intro-
duce block ITG, which gives many-to-one instead of
one-to-one alignments. Zhang et. al (2008) present
a method for evaluating spans in the sentence pair to
determine whether they should be excluded or not.
The algorithm has a best case time complexity of
O(n3).
In this paper we introduce Linear ITG (LITG), and
apply it to a word-alignment task which is evaluated
by the phrase-based statistical machine translation
(PBSMT) system that can be built from that.
3 Stochastic Bracketing Linear Inversion
Transduction Grammar
A Bracketing Linear Inversion Transduction Gram-
mar (BLITG) is a BITG where rules may have at most
one nonterminal symbol in their production. This
gives us a normal form that is somewhat different
from the usual ITG:
X ? [Xe/f ]
X ? [e/fX]
X ? ?Xe/f?
X ? ?e/fX?
X ? ?/?
where one but not both of the tokens in the bitermi-
nal may be the empty string ?, if a nonterminal is
produced. By associating each rule with a probabil-
ity, we get a Stochastic BLITG (SBLITG).
3.1 Biparsing Algorithm
The sentence pair to be biparsed consists of two vec-
tors of tokens (e and f ). An item is represented
as a nonterminal (X), and one span in each of the
languages (es..t and fu..v). For notational conve-
nience, an item will be written as the nonterminal
with the spans as subscripts (Xs,t,u,v). The length of
an item is defined as the sum of the length of the two
spans: |Xs,t,u,v| = t ? s + v ? u. Items are gath-
ered in buckets, Bn, according to their length so that
Xs,t,u,v ? B|Xs,t,u,v|. The algorithm is initialized
with the item spanning the entire sentence pair:
X0,|e|,0,|f | ? B|X0,|e|,0,|f||
Starting from this top bucket, buckets are processed
in larger to smaller order: Bn, Bn?1, . . . , B1. While
processing a bucket, only smaller items are added,
meaning that B0 is fully constructed by the time B1
has been processed. Each item in B0 can have the
rule X ? ?/? applied to it, eliminating the nonter-
minal and halting processing. If there are no items
in B0, parsing has failed.
To process a bucket, each item is extended by all
applicable rules, and the nonterminals in the produc-
tions are added to their respective buckets.
342
System BLEU NIST Phrases
GIZA++ (intersect) 0.2629 6.7968 146,581,109
GIZA++ (grow-diag-final) 0.2632 6.7410 1,298,566
GIZA++ (grow-diag-final-and) 0.2742 6.9228 7,340,369
SBLITG (b = 25) 0.3027 7.3664 13,551,915
SBLITG (b = ?) 0.3008 7.3303 12,673,361
Table 1: Results for French?English.
Xs,t,u,v ?
[es,s+1/fu,u+1 Xs+1,t,u+1,v]
| [Xs,t?1,u,v?1 et?1,t/fv?1,v ]
| ?es,s+1/fv?1,v Xs+1,t,u,v?1?
| ?Xs,t?1,u+1,v et?1,t/fu,u+1?
| [es,s+1/? Xs+1,t,u,v] | ?es,s+1/? Xs+1,t,u,v?
| [?/fu,u+1 Xs,t,u+1,v] | ?Xs,t,u+1,v ?/fu,u+1?
| [Xs,t?1,u,v et?1,t/?] | ?Xs,t?1,u,v et?1,t/??
| [Xs,t,u,v?1 ?/fv?1,v] | ??/fv?1,v Xs,t,u,v?1?
Note that there are two productions on each of the
four last rows. These are distinct rules, but the
symbols in the productions are identical. This phe-
nomenon is due to the fact that the empty sym-
bols can be ?read? off either end of the span. In
our experiments, such rules were merged into their
non-inverting form, effectively eliminating the last
four inverted rules (productions enclosed in angled
brackets) above.
3.2 Analysis
Let n be the length of the longer sentence in the
pair. The number of buckets will be O(n), since
the longest item will be at most 2n long. Within a
bucket, there can be O(n2) starting points for items,
but once the length of one of the spans is fixed, the
length of the other follows, adding a factor O(n),
making the total number of items in a bucket O(n3).
Each item in a bucket can be analyzed in 8 possible
ways, requiring O(1) time. In summary, we have:
O(n)?O(n3)?O(1) = O(n4)
The pruning scheme works by limiting the num-
ber of items that are processed from each bucket, re-
ducing the cost of processing a bucket from O(n3)
to O(b), where b is the beam width. This gives time
complexity O(n) ?O(b) ?O(1) = O(bn).
4 Experiments
We used the guidelines of the shared task of
WMT?081 to train our baseline system as well as
our experimental system. This includes induction of
word alignments with GIZA++ (Och and Ney, 2003),
induction of a Phrase-based SMT system (Koehn et
al., 2007), and tuning with minimum error rate train-
ing (Och, 2003), as well as applying some utility
scripts provided for the workshop. The translation
model is combined with a 5-gram language model
(Stolcke, 2002).
Our experimental system uses alignments from
the Viterbi parses, extracted during EM training of an
SBLITG on the training corpus, instead of GIZA++.
Since EM will converge fairly slowly, it was limited
to 10 iterations, after which it was halted.
We used the French?English part of the WMT?08
shared task, but limited the training set to sentence
pairs where both sentences were of length 20 or less.
This was necessary in order to carry out exhaustive
search in the SBLITG algorithm. In total, we had
381,780 sentence pairs for training, and 2,000 sen-
tence pairs each for tuning and testing. The language
model was trained with the entire training set.
To evaluate the systems we used BLEU (Papineni
et al, 2002) and NIST (Doddington, 2002)
Results are presented in Table 1. It is interesting
to note that there is no correlation between the num-
ber of phrases extracted and translation quality. The
only explanation for the results we are seeing is that
the SBLITGs find better phrases. Since the only dif-
ference is the word alignment strategy, this suggests
that the word alignments from SBLITGs are better
suited for phrase extraction than those from bidirec-
tional IBM-models. The fact that SBLITGs extract
more phrases than bidirectional IBM-models under
1http://www.statmt.org/wmt08/
343
the grow-diag-x heuristics is significant, since
more phrases means that more translation possibil-
ities are extracted. The fact that SBLITGs extract
fewer phrases than bidirectional IBM-models under
the intersect heuristic is also significant, since
it implies that simply adding more phrases is a bad
strategy. Combined, the two observations leads us
to believe that there are some alignments missed by
the bidirectional IBM-models that are found by the
SBLITG-models. It is also interesting to see that the
pruned version outperforms the exhaustive version.
We believe this to be because the pruned version ap-
proaches the correct grammar faster than the exhaus-
tive. That would mean that the exhaustive SBLITG
would be better in the limit, but the experiment was
limited to 10 iterations.
5 Conclusion
In this paper we have focused on the benefits of ap-
plying SBLITGs to the task of inducing word align-
ments, which leads to a 2.85 BLEU points improve-
ment compared to the standard model (heuristically
combined bidirectional IBM-models). In the future,
we hope that LITGs will be a spring board towards
full ITGs, with more interesting nonterminals than
the BITGs seen in the literature so far. With the pos-
sibility of inducing full ITG from parallel corpora it
becomes viable to use ITG decoders directly as ma-
chine translation systems.
Acknowledgments
This work was funded by the Swedish National Gradu-
ate School of Language Technology (GSLT), the Defense
Advanced Research Projects Agency (DARPA) under
GALE Contract No. HR0011-06-C-0023, and the Hong
Kong Research Grants Council (RGC) under research
grants GRF621008, DAG03/04.EG09, RGC6256/00E,
and RGC6083/99E. Any opinions, findings and conclu-
sions or recommendations expressed in this material are
those of the authors and do not necessarily reflect the
views of DARPA.The computations were performed on
UPPMAX resources under project p2007020.
References
A. V. Aho and J. D. Ullman. 1972. The Theory of Pars-
ing, Translation, and Compiling. Prentice-Halll, En-
glewood Cliffs, New Jersey.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In Proceedings of Human Language Technology
conference (HLT-2002), San Diego, California.
A. Haghighi, J. Blitzer, J. DeNero, and D. Klein. 2009.
Better word alignments with supervised itg models.
In Proceedings of ACL/IJCNLP 2009, pages 923?931,
Singapore.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings of the
ACL 2007 Demo and Poster Session, pages 177?180,
Prague, Czech Republic.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
F. J. Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proceedings of ACL 2003,
pages 160?167, Sapporo, Japan.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of ACL 2002, pages 311?
318, Philadelphia, Pennsylvania.
M. Saers and D. Wu. 2009. Improving phrase-based
translation via word alignments from Stochastic In-
version Transduction Grammars. In Proceedings of
SSST-3 at NAACL HLT 2009, pages 28?36, Boulder,
Colorado.
M. Saers, J. Nivre, and D. Wu. 2009. Learning stochas-
tic bracketing inversion transduction grammars with
a cubic time biparsing algorithm. In Proceedings of
IWPT?09, pages 29?32, Paris, France.
A. Stolcke. 2002. SRILM ? an extensible language mod-
eling toolkit. In International Conference on Spoken
Language Processing, Denver, Colorado.
D. Wu. 1995. An algorithm for simultaneously bracket-
ing parallel texts by aligning words. In Proceedings of
WVLC-3, pages 69?82, Cambridge, Massachusetts.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?403.
H. Zhang, C. Quirk, R. C. Moore, and D. Gildea. 2008.
Bayesian learning of non-compositional phrases with
synchronous parsing. In Proceedings of ACL/HLT
2008, pages 97?105, Columbus, Ohio.
344
Proceedings of NAACL-HLT 2013, pages 1061?1071,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Target Language Adaptation of Discriminative Transfer Parsers
Oscar T?ckstr?m?
SICS | Uppsala University
Sweden
oscar@sics.se
Ryan McDonald
Google
New York
ryanmcd@google.com
Joakim Nivre?
Uppsala University
Sweden
joakim.nivre@lingfil.uu.se
Abstract
We study multi-source transfer parsing for
resource-poor target languages; specifically
methods for target language adaptation of
delexicalized discriminative graph-based de-
pendency parsers. We first show how recent
insights on selective parameter sharing, based
on typological and language-family features,
can be applied to a discriminative parser by
carefully decomposing its model features. We
then show how the parser can be relexicalized
and adapted using unlabeled target language
data and a learning method that can incorporate
diverse knowledge sources through ambiguous
labelings. In the latter scenario, we exploit
two sources of knowledge: arc marginals de-
rived from the base parser in a self-training
algorithm, and arc predictions from multiple
transfer parsers in an ensemble-training algo-
rithm. Our final model outperforms the state of
the art in multi-source transfer parsing on 15
out of 16 evaluated languages.
1 Introduction
Many languages still lack access to core NLP tools,
such as part-of-speech taggers and syntactic parsers.
This is largely due to the reliance on fully supervised
learning methods, which require large quantities of
manually annotated training data. Recently, meth-
ods for cross-lingual transfer have appeared as a
promising avenue for overcoming this hurdle for both
part-of-speech tagging (Yarowsky et al, 2001; Das
and Petrov, 2011) and syntactic dependency parsing
(Hwa et al, 2005; Zeman and Resnik, 2008; Ganchev
et al, 2009; McDonald et al, 2011; Naseem et al,
?Work primarily carried out while at Google, NY.
2012). While these methods do not yet compete with
fully supervised approaches, they can drastically out-
perform both unsupervised methods (Klein and Man-
ning, 2004) and weakly supervised methods (Naseem
et al, 2010; Berg-Kirkpatrick and Klein, 2010).
A promising approach to cross-lingual transfer
of syntactic dependency parsers is to use multiple
source languages and to tie model parameters across
related languages. This idea was first explored for
weakly supervised learning (Cohen and Smith, 2009;
Snyder et al, 2009; Berg-Kirkpatrick and Klein,
2010) and recently by Naseem et al (2012) for multi-
source cross-lingual transfer. In particular, Naseem
et al showed that by selectively sharing parameters
based on typological features of each language, sub-
stantial improvements can be achieved, compared
to using a single set of parameters for all languages.
However, these methods all employ generative mod-
els with strong independence assumptions and weak
feature representations, which upper bounds their ac-
curacy far below that of feature-rich discriminative
parsers (McDonald et al, 2005; Nivre, 2008).
In this paper, we improve upon the state of the art
in cross-lingual transfer of dependency parsers from
multiple source languages by adapting feature-rich
discriminatively trained parsers to a specific target
language. First, in ?4 we show how selective sharing
of model parameters based on typological traits can
be incorporated into a delexicalized discriminative
graph-based parsing model. This requires a careful
decomposition of features into language-generic and
language-specific sets in order to tie specific target
language parameters to their relevant source language
counterparts. The resulting parser outperforms the
method of Naseem et al (2012) on 12 out of 16 eval-
uated languages. Second, in ?5 we introduce a train-
1061
ing method that can incorporate diverse knowledge
sources through ambiguously predicted labelings of
unlabeled target language data. This permits effective
relexicalization and target language adaptation of the
transfer parser. Here, we experiment with two differ-
ent knowledge sources: arc sets, which are filtered by
marginal probabilities from the cross-lingual transfer
parser, are used in an ambiguity-aware self-training
algorithm (?5.2); these arc sets are then combined
with the predictions of a different transfer parser in an
ambiguity-aware ensemble-training algorithm (?5.3).
The resulting parser provides significant improve-
ments over a strong baseline parser and achieves a
13% relative error reduction on average with respect
to the best model of Naseem et al (2012), outper-
forming it on 15 out of the 16 evaluated languages.
2 Multi-Source Delexicalized Transfer
The methods proposed in this paper fall into the delex-
icalized transfer approach to multilingual syntactic
parsing (Zeman and Resnik, 2008; McDonald et al,
2011; Cohen et al, 2011; S?gaard, 2011). In contrast
to annotation projection approaches (Yarowsky et al,
2001; Hwa et al, 2005; Ganchev et al, 2009; Spreyer
and Kuhn, 2009), delexicalized transfer methods do
not rely on any bitext. Instead, a parser is trained
on annotations in a source language, relying solely
on features that are available in both the source
and the target language, such as ?universal? part-of-
speech tags (Zeman and Resnik, 2008; Naseem et al,
2010; Petrov et al, 2012), cross-lingual word clusters
(T?ckstr?m et al, 2012) or type-level features derived
from bilingual dictionaries (Durrett et al, 2012).1
This parser is then directly used to parse the target
language. For languages with similar typology, this
method can be quite accurate, especially when com-
pared to purely unsupervised methods. For instance,
a parser trained on English with only part-of-speech
features can correctly parse the Greek sentence in Fig-
ure 1, even without knowledge of the lexical items
since the sequence of part-of-speech tags determines
the syntactic structure almost unambiguously.
Learning with multiple languages has been shown
to benefit unsupervised learning (Cohen and Smith,
1Note that T?ckstr?m et al (2012) and Durrett et al (2012)
do require bitext or a bilingual dictionary. The same holds for
most cross-lingual representations, e.g., Klementiev et al (2012).
? ???? ????? ???? ????? ?? ?????? .
(The) (John) (gave) (to-the) (Maria) (the) (book) .
DET NOUN VERB ADP NOUN DET NOUN P
DET NSUBJ
ROOT
PREP POBJ
DOBJ
DET
PUNC
Figure 1: A Greek sentence which is correctly parsed by a
delexicalized English parser, provided that part-of-speech
tags are available in both the source and target language.
2009; Snyder et al, 2009; Berg-Kirkpatrick and
Klein, 2010). Annotations in multiple languages
can be combined in delexicalized transfer as well, as
long as the parser features are available across the in-
volved languages. This idea was explored by McDon-
ald et al (2011), who showed that target language
accuracy can be improved by simply concatenating
delexicalized treebanks in multiple languages. In
similar work, Cohen et al (2011) proposed a mixture
model in which the parameters of a generative target
language parser is expressed as a linear interpola-
tion of source language parameters, whereas S?gaard
(2011) showed that target side language models can
be used to selectively subsample training sentences
to improve accuracy. Recently, inspired by the phylo-
genetic prior of Berg-Kirkpatrick and Klein (2010),
S?gaard and Wulff (2012) proposed ? among other
ideas ? a typologically informed weighting heuristic
for linearly interpolating source language parameters.
However, this weighting did not provide significant
improvements over uniform weighting.
The aforementioned approaches work well for
transfer between similar languages. However, their
assumptions cease to hold for typologically divergent
languages; a target language can rarely be described
as a linear combination of data or model parameters
from a set of source languages, as languages tend
to share varied typological traits; this critical insight
is discussed further in ?4. To account for this issue,
Naseem et al (2012) recently introduced a novel gen-
erative model of dependency parsing, in which the
generative process is factored into separate steps for
the selection of dependents and their ordering. The
parameters used in the selection step are all language
independent, capturing only head-dependent attach-
ment preferences. In the ordering step, however, pa-
rameters are selectively shared between subsets of
1062
Feature Description
81A Order of Subject, Object and Verb
85A Order of Adposition and Noun
86A Order of Genitive and Noun
87A Order of Adjective and Noun
88A Order of Demonstrative and Noun
89A Order of Numeral and Noun
Table 1: Typological features from WALS (Dryer and
Haspelmath, 2011), proposed for selective sharing by
Naseem et al (2012). Feature 89A has the same value for
all studied languages, while 88A differs only for Basque.
These features are therefore subsequently excluded.
source languages based on typological features of
the languages extracted from WALS ? the World
Atlas of Language Structures (Dryer and Haspelmath,
2011) ? as shown in Table 1. In the transfer scenario,
where no supervision is available in the target lan-
guage, this parser achieves the hitherto best published
results across a number of languages; in particular
for target languages with a word order divergent from
the source languages.
However, the generative model of Naseem et al is
quite impoverished. In the fully supervised setting,
it obtains substantially lower accuracies compared
to a standard arc-factored graph-based parser (Mc-
Donald et al, 2005). Averaged across 16 languages,2
the generative model trained with full supervision on
the target language obtains an accuracy of 67.1%. A
comparable lexicalized discriminative arc-factored
model (McDonald et al, 2005) obtains 84.1%. Even
when delexicalized, this model reaches 78.9%. This
gap in supervised accuracy holds for all 16 languages.
Thus, while selective sharing is a powerful device for
transferring parsers across languages, the underly-
ing generative model used by Naseem et al (2012)
restricts its potential performance.
3 Basic Models and Experimental Setup
Inspired by the superiority of discriminative graph-
based parsing in the supervised scenario, we inves-
tigate whether the insights of Naseem et al (2012)
on selective parameter sharing can be incorporated
into such models in the transfer scenario. We first re-
view the basic graph-based parser framework and the
2Based on results in Naseem et al (2012), excluding English.
experimental setup that we will use throughout. We
then delve into details on how to incorporate selec-
tive sharing in this model in ?4. In ?5, we show how
learning with ambiguous labelings in this parser can
be used for further target language adaptation, both
through self-training and through ensemble-training.
3.1 Discriminative Graph-Based Parser
Let x denote an input sentence and let y ? Y(x)
denote a dependency tree, where Y(x) is the set of
well-formed dependency trees spanning x. Hence-
forth, we restrictY(x) to projective dependency trees,
but all our methods are equally applicable in the non-
projective case. Provided a vector of model parame-
ters ?, the probability of a dependency tree y ? Y(x),
conditioned on a sentence x, has the following form:
p?(y | x) =
exp
{
?>?(x, y)
}
?
y??Y(x) exp {?
>?(x, y?)}
.
Without loss of generality, we restrict ourselves to
first-order models, where the feature function ?(x, y)
factors over individual arcs (h,m) in y, such that
?(x, y) =
?
(h,m)?y
?(x, h,m) ,
where h ? [0, |x|] and m ? [1, |x|] are the indices
of the head word and the dependent word of the
arc; h = 0 represents a dummy ROOT token. The
model parameters are estimated by maximizing the
log-likelihood of the training dataD = {(xi, yi)}ni=1,
L(?;D) =
n?
i=1
log p?(yi | xi) .
We use the standard gradient-based L-BFGS algo-
rithm (Liu and Nocedal, 1989) to maximize the log-
likelihood. Eisner?s algorithm (Eisner, 1996) is used
for inference of the Viterbi parse and arc-marginals.
3.2 Data Sets and Experimental Setup
To facilitate comparison with the state of the art, we
use the same treebanks and experimental setup as
Naseem et al (2012). Notably, we use the map-
ping proposed by Naseem et al (2010) to map from
fine-grained treebank specific part-of-speech tags to
coarse-grained ?universal? tags, rather than the more
recent mapping proposed by Petrov et al (2012). For
1063
l[l]? h.p
[l]? m.p
[l]? h.p? m.p
d? w.81A? 1[h.p = VERB ? m.p = NOUN]
d? w.81A? 1[h.p = VERB ? m.p = PRON]
d? w.85A? 1[h.p = ADP ? m.p = NOUN]
d? w.85A? 1[h.p = ADP ? m.p = PRON]
d? w.86A? 1[h.p = NOUN ? m.p = NOUN]
d? w.87A? 1[h.p = NOUN ? m.p = ADJ]
d? l; [d? l]? h.p; [d? l]? m.p
[d? l]? h.p? m.p
[d? l]? h.p? h+1.p? m?1.p? m.p
[d? l]? h?1.p? h.p? m?1.p? m.p
[d? l]? h.p? h+1.p? m.p? m+1.p
[d? l]? h?1.p? h.p? m.p? m+1.p
h.p? between.p? m.p
Delexicalized MSTParser Selectively sharedBare
Figure 2: Arc-factored feature templates for graph-based parsing. Direction: d ? {LEFT, RIGHT}; dependency length:
l ? {1, 2, 3, 4, 5+}; part of speech of head / dependent / words between head and dependent: h.p / m.p / between.p ?
{NOUN, VERB, ADJ, ADV, PRON, DET, ADP, NUM, CONJ, PRT, PUNC, X}; token to the left / right of z: z?1 / z+1; WALS
features: w.X for X = 81A, 85A, 86A, 87A (see Table 1). [?] denotes an optional template, e.g., [d? l] ? h.p? m.p
expands to templates d? l? h.p? m.p and h.p? m.p, so that the template also falls back on its undirectional variant.
each target language evaluated, the treebanks of the
remaining languages are used as labeled training data,
while the target language treebank is used for testing
only (in ?5 a different portion of the target language
treebank is additionally used as unlabeled training
data). We refer the reader to Naseem et al (2012) for
detailed information on the different treebanks. Due
to divergent treebank annotation guidelines, which
makes fine-grained evaluation difficult, all results
are evaluated in terms of unlabeled attachment score
(UAS). In line with Naseem et al (2012), we use gold
part-of-speech tags and evaluate only on sentences
of length 50 or less excluding punctuation.
3.3 Baseline Models
We compare our models to two multi-source base-
line models. The first baseline, NBG, is the gener-
ative model with selective parameter sharing from
Naseem et al (2012).3 This model is trained without
target language data, but we investigate the use of
such data in ?5.4. The second baseline, Delex, is a
delexicalized projective version of the well-known
graph-based MSTParser (McDonald et al, 2005).
The feature templates used by this model are shown
to the left in Figure 2. Note that there is no selective
sharing in this model.
The second and third columns of Table 2 show the
unlabeled attachment scores of the baseline models
for each target language. We see that Delex performs
well on target languages that are related to the major-
ity of the source languages. However, for languages
3Model ?D-,To? in Table 2 from Naseem et al (2012).
that diverge from the Indo-European majority family,
the selective sharing model, NBG, achieves substan-
tially higher accuracies.
4 Feature-Based Selective Sharing
The results for the baseline models are not surpris-
ing considering the feature templates used by Delex.
There are two fundamental issues with these fea-
tures when used for direct transfer. First, all but
one template include the arc direction. Second,
some features are sensitive to local word order; e.g.,
[d? l]? h.p? h+1.p? m?1.p? m.p, which models
direction as well as word order in the local contexts
of the head and the dependent. Such features do not
transfer well across typologically different languages.
In order to verify that these issues are the cause of
the poor performance of the Delex model, we remove
all directional features and all features that model
local word order from Delex. The feature templates
of the resulting Bare model are shown in the center
of Figure 2. These features only model selectional
preferences and dependency length, analogously to
the selection component of NBG. The performance
of Bare is shown in the fourth column of Table 2.
The removal of most of the features results in a per-
formance drop on average. However, for languages
outside of the Indo-European family, Bare is often
more accurate, especially for Basque, Hungarian and
Japanese, which supports our hypothesis.
4.1 Sharing Based on Typological Features
After removing all directional features, we now care-
fully reintroduce them. Inspired by Naseem et al
1064
Graph-Based Models
Lang. NBG Delex Bare Share Similar Family
ar 57.2 43.3 43.1 52.7 52.7 52.7
bg 67.6 64.5 56.1 65.4 62.4 65.4
ca 71.9 72.0 58.1 66.1 80.2 77.6
cs 43.9 40.5 43.1 42.5 45.3 43.5
de 54.0 57.0 49.3 55.2 58.1 59.2
el 61.9 63.2 57.7 62.9 59.9 63.2
es 62.3 66.9 52.6 59.3 69.0 67.1
eu 39.7 29.5 43.3 46.8 46.8 46.8
hu 56.9 56.2 60.5 64.5 64.5 64.5
it 68.0 70.8 55.7 63.5 74.6 72.5
ja 62.3 38.9 50.6 57.1 64.6 65.9
nl 56.2 57.9 51.6 55.0 51.8 56.8
pt 76.2 77.5 63.0 72.7 78.4 78.4
sv 52.0 61.4 55.9 58.8 48.8 63.5
tr 59.1 37.4 36.0 41.7 59.5 59.4
zh 59.9 45.1 47.9 54.8 54.8 54.8
avg 59.3 55.1 51.5 57.4 60.7 62.0
Table 2: Unlabeled attachment scores of the multi-source
transfer models. Boldface numbers indicate the best result
per language. Underlined numbers indicate languages
whose group is not represented in the training data (these
default to Share under Similarity and Family). NBG is the
?D-,To? model in Table 2 from Naseem et al (2012).
(2012), we make use of the typological features from
WALS (Dryer and Haspelmath, 2011), listed in Ta-
ble 1, to selectively share directional parameters be-
tween languages. As a natural first attempt at sharing
parameters, one might consider forming the cross-
product of all features of Delex with all WALS prop-
erties, similarly to a common domain adaptation tech-
nique (Daum? III, 2007; Finkel and Manning, 2009).
However, this approach has two issues. First, it re-
sults in a huge number of features, making the model
prone to overfitting. Second, and more critically, it
ties together languages via features for which they
are not typologically similar. Consider English and
French, which are both prepositional and thus have
the same value for WALS property 85A. These lan-
guages will end up sharing a parameter for the feature
[d? l]?h.p = NOUN?m.p = ADJ?w.85A; yet they
have the exact opposite direction of attachment pref-
erence when it comes to nouns and adjectives. This
problem applies to any method for parameter mixing
that treats all the parameters as equal.
Like Naseem et al (2012), we instead share pa-
rameters more selectively. Our strategy is to use the
relevant part-of-speech tags of the head and depen-
dent to select which parameters to share, based on
very basic linguistic knowledge. The resulting fea-
tures are shown to the right in Figure 2. For example,
there is a shared directional feature that models the or-
der of Subject, Object and Verb by conjoining WALS
feature 81A with the arc direction and an indicator
feature that fires only if the head is a verb and the de-
pendent is a noun. These features would not be very
useful by themselves, so we combine them with the
Bare features. The accuracy of the resulting Share
model is shown in column five of Table 2. Although
this model still performs worse than NBG, it is an
improvement over the Delex baseline and actually
outperforms the former on 5 out of the 16 languages.
4.2 Sharing Based on Language Groups
While Share models selectional preferences and arc
directions for a subset of dependency relations, it
does not capture the rich local word order informa-
tion captured by Delex. We now consider two ways of
selectively including such information based on lan-
guage similarity. While more complex sharing could
be explored (Berg-Kirkpatrick and Klein, 2010), we
use a flat structure and consider two simple groupings
of the source and target languages.
First, the Similar model consists of the features
used by Share together with the features from Delex
in Figure 2. The latter are conjoined with an indicator
feature that fires only when the source and target
languages share values for all the WALS features in
Table 1. This is accomplished by adding the template
f? [w.81A? w.85A? w.86A? w.87A? w.88A]
for each template f in Delex. This groups: 1) Cata-
lan, Italian, Portuguese and Spanish; 2) Bulgarian,
Czech and English; 3) Dutch, German and Greek;
and 4) Japanese and Turkish. The remaining lan-
guages do not share all WALS properties with at
least one source language and thus revert to Share,
since they cannot exploit these grouped features.
Second, instead of grouping languages according
to WALS, the Family model is based on a simple
subdivision into Indo-European languages (Bulgar-
ian, Catalan, Czech, Greek, English, Spanish, Italian,
1065
Dutch, Portuguese, Swedish) and Altaic languages
(Japanese, Turkish). This is accomplished with in-
dicator features analogous to those used in Similar.
The remaining languages are again treated as isolates
and revert to Similar.
The results for these models are given in the last
two columns of Table 2. We see that by adding these
rich features back into the fold, but having them fire
only for languages in the same group, we can sig-
nificantly increase the performance ? from 57.4%
to 62.0% on average when considering Family. If
we consider our original Delex baseline, we see an
absolute improvement of 6.9% on average and a rela-
tive error reduction of 15%. Particular gains are seen
for non-Indo-European languages; e.g., Japanese in-
creases from 38.9% to 65.9%. Furthermore, Family
achieves a 7% relative error reduction over the NBG
baseline and outperforms it on 12 of the 16 languages.
This shows that a discriminative graph-based parser
can achieve higher accuracies compared to generative
models when the features are carefully constructed.
5 Target Language Adaptation
While some higher-level linguistic properties of the
target language have been incorporated through se-
lective sharing, so far no features specific to the target
language have been employed. Cohen et al (2011)
and Naseem et al (2012) have shown that using
expectation-maximization (EM) to this end can in
some cases bring substantial accuracy gains. For dis-
criminative models, self-training has been shown to
be quite effective for adapting monolingual parsers to
new domains (McClosky et al, 2006), as well as for
relexicalizing delexicalized parsers using unlabeled
target language data (Zeman and Resnik, 2008). Sim-
ilarly T?ckstr?m (2012) used self-training to adapt a
multi-source direct transfer named-entity recognizer
(T?ckstr?m et al, 2012) to different target languages,
?relexicalizing? the model with word cluster features.
However, as discussed in ?5.2, standard self-training
is not optimal for target language adaptation.
5.1 Ambiguity-Aware Training
In this section, we propose a related training method:
ambiguity-aware training. In this setting a discrimi-
native probabilistic model is induced from automat-
ically inferred ambiguous labelings over unlabeled
target language data, in place of gold-standard depen-
dency trees. The ambiguous labelings can combine
multiple sources of evidence to guide the estimation
or simply encode the underlying uncertainty from the
base parser. This uncertainty is marginalized out dur-
ing training. The structure of the output space, e.g.,
projectivity and single-headedness constraints, along
with regularities in the feature space, can together
guide the estimation, similar to what occurs with the
expectation-maximization algorithm.
Core to this method is the idea of an ambiguous
labeling y?(x) ? Y(x), which encodes a set of pos-
sible dependency trees for an input sentence x. In
subsequent sections we describe how to define such
labelings. Critically, y?(x) should be large enough to
capture the correct labeling, but on the other hand
small enough to provide concrete guidance for model
estimation. Ideally, y?(x) will capture heterogenous
knowledge that can aid the parser in target language
adaptation. In a first-order arc-factored model, we
define y?(x) in terms of a collection of ambiguous
arc setsA(x) = {A(x,m)}|x|m=1, whereA(x,m) de-
notes the set of ambiguously specified heads for the
mth token in x. Then, y?(x) is defined as the set of
all projective dependency trees spanning x that can
be assembled from the arcs in A(x).
Methods for learning with ambiguous labelings
have previously been proposed in the context of
multi-class classification (Jin and Ghahramani, 2002),
sequence-labeling (Dredze et al, 2009), log-linear
LFG parsing (Riezler et al, 2002), as well as for
discriminative reranking of generative constituency
parsers (Charniak and Johnson, 2005). In contrast to
Dredze et al, who allow for weights to be assigned
to partial labels, we assume that the ambiguous arcs
are weighted uniformly. For target language adapta-
tion, these weights would typically be derived from
unreliable sources and we do not want to train the
model to simply mimic their beliefs. Furthermore,
with this assumption, learning is simply achieved
by maximizing the marginal log-likelihood of the
ambiguous training set D? = {(xi, y?(xi)}ni=1,
L(?; D?) =
n?
i=1
log
?
?
?
?
y?y?(xi)
p?(y | xi)
?
?
?
? ? ???22 .
In maximizing the marginal log-likelihood, the model
is free to distribute probability mass among the trees
1066
in the ambiguous labeling to its liking, as long as the
marginal log-likelihood improves. The same objec-
tive function is used by Riezler et al (2002) and Char-
niak and Johnson (2005). A key difference is that in
these works, the ambiguity is constrained through a
supervised signal, while we use ambiguity as a way
to achieve self-training, using the base-parser itself,
or some other potentially noisy knowledge source as
the sole constraints. Note that we have introduced
an `2-regularizer, weighted by ?. This is important
as we are now training lexicalized target language
models which can easily overfit. In all experiments,
we optimize parameters with L-BFGS. Note also that
the marginal likelihood is non-concave, so that we
are only guaranteed to find a local maximum.
5.2 Ambiguity-Aware Self-Training
In standard self-training ? hereafter referred to as
Viterbi self-training ? a base parser is used to la-
bel each unlabeled sentence with its most probable
parse tree to create a self-labeled data set, which is
subsequently used to train a supervised parser. There
are two reasons why this simple approach may work.
First, if the base parser?s errors are not too systematic
and if the self-training model is not too expressive,
self-training can reduce the variance on the new do-
main. Second, self-training allows for features in the
new domain with low support ? or no support in the
case of lexicalized features ? in the base parser to
be ?filled in? by exploiting correlations in the feature
representation. However, a potential pitfall of this
approach is that the self-trained parser is encouraged
to blindly mimic the base parser, which leads to error
reinforcement. This may be particularly problematic
when relexicalizing a transfer parser, since the lexical
features provide the parser with increased power and
thereby an increased risk of overfitting to the noise.
To overcome this potential problem, we propose an
ambiguity-aware self-training (AAST) method that is
able to take the noise of the base parser into account.
We use the arc-marginals of the base parser to
construct the ambiguous labeling y?(x) for a sentence
x. For each token m ? [1, |x|], we first sort the set of
arcs in which m is the dependent, {(h,m)}|x|h=0, by
the marginal probabilities of the arcs:
p?(h,m | x) =
?
{y?Y(x) | (h,m)?y}
p?(y | x)
We next construct the ambiguous arc set A(x,m) by
adding arcs (h,m) in order of decreasing probability,
until their cumulative probability exceeds ?, i.e. until
?
(h,m)?A(x,m)
p?(h,m | x) ? ? .
Lower values of ? result in more aggressive pruning,
with ? = 0 corresponding to including no arc and
? = 1 corresponding to including all arcs. We always
add the highest scoring tree y? to y?(x) to ensure that
it contains at least one complete projective tree.
Figure 3 outlines an example of how (and why)
AAST works. In the Greek example, the genitive
phrase ? pi??????? ?????? (the stay of vessels) is
incorrectly analyzed as a flat noun phrase. This is not
surprising given that the base parser simply observes
this phrase as DET NOUN NOUN. However, looking
at the arc marginals we can see that the correct anal-
ysis is available during AAST, although the actual
marginal probabilities are quite misleading. Further-
more, the genitive noun ?????? also appears in other
less ambiguous contexts, where the base parser cor-
rectly predicts it to modify a noun and not a verb.
This allows the training process to add weight to the
corresponding lexical feature pairing ?????? with a
noun head and away from the feature pairing it with
a verb. The resulting parser correctly predicts the
genitive construction.
5.3 Ambiguity-Aware Ensemble-Training
While ambiguous labelings can be used as a means
to improve self-training, any information that can
be expressed as hard arc-factored constraints can be
incorporated, including linguistic expert knowledge
and annotation projected via bitext. Here we explore
another natural source of information: the predic-
tions of other transfer parsers. It is well known that
combining several diverse predictions in an ensem-
ble often leads to improved predictions. However, in
most ensemble methods there is typically no learning
involved once the base learners have been trained
(Sagae and Lavie, 2006). An exception is the method
of Sagae and Tsujii (2007), who combine the outputs
of many parsers on unlabeled data to train a parser
for a new domain. However, in that work the learner
is not exposed to the underlying ambiguity of the
base parsers; it is only given the Viterbi parse of the
combination system as the gold standard. In contrast,
1067
? pi??????? ?????? ?pi????pi???? ???? ?? ????
DET NOUN NOUN VERB ADV DET NOUN
0.55
0.44
0.62
0.36
0.10
0.87
? pi??????? ?????? ?pi????pi???? ???? ?? ????
DET NOUN NOUN VERB ADV DET NOUN
? pi??????? ?????? ?pi????pi???? ???? ?? ????
DET NOUN NOUN VERB ADV DET NOUN
Figure 3: An example of ambiguity-aware self-training
(AAST) on a sentence from the Greek self-training data.
The sentence roughly translates to The stay of vessels
is permitted only for the day. Top: Arcs from the base
model?s Viterbi parse are shown above the sentence. When
only the part-of-speech tags are observed, the parser tends
to treat everything to the left of the verb as a head-final
noun phrase. The dashed arcs below the sentence are
the arcs for the true genitive construction stay of vessels.
These arcs and the corresponding incorrect arcs in the
Viterbi parse are marked with their marginal probabilities.
Middle: The ambiguous labeling y?(x), which is used
as supervision in AAST. Additional non-Viterbi arcs are
present in y?(x); for clarity, these are not shown. When
learning with AAST, probability mass will be pushed to-
wards any tree consistent with y?(x). Marginal probabili-
ties are ignored at this stage, so that all arcs in y?(x) are
treated as equals. Bottom: The Viterbi parse of the AAST
model, which has selected the correct arcs from y?(x).
we propose an ambiguity-aware ensemble-training
(AAET) method that treats the union of the ensemble
predictions for a sentence x as an ambiguous labeling
y?(x). An additional advantage of this approach is
that the ensemble is compiled into a single model
and therefore does not require multiple models to be
stored and used at runtime.
It is straightforward to construct y?(x) from multi-
ple parsers. Let Ak(x,m) be the set of arcs for the
mth token in x according to the kth parser in the en-
semble. When arc-marginals are used to construct the
ambiguity set, |Ak(x,m)| ? 1, but when the Viterbi-
parse is used, Ak(x,m) is a singleton. We next form
A(x,m) =
?
kAk(x,m) as the ensemble arc ambi-
guity set from which y?(x) is assembled. In this study,
we combine the arc sets of two base parsers: first, the
arc-marginal ambiguity set of the base parser (?5.2);
and second, the Viterbi arc set from the NBG parser
of Naseem et al (2012) in Table 2.4 Thus, the lat-
ter will have singleton arc ambiguity sets, but when
combined with the arc-marginal ambiguity sets of
our base parser, the result will encode uncertainty
derived from both parsers.
5.4 Adaptation Experiments
We now study the different approaches to target lan-
guage adaptation empirically. As in Naseem et al
(2012), we use the CoNLL training sets, stripped of
all dependency information, as the unlabeled target
language data in our experiments. We use the Family
model as the base parser, which is used to label the
unlabeled target data with the Viterbi parses as well
as with the ambiguous labelings. The final model
is then trained on this data using standard lexical-
ized features (McDonald et al, 2005). Since labeled
training data is unavailable in the target language,
we cannot tune any hyper-parameters and simply set
?= 1 and ?= 0.95 throughout. Although the latter
may suggest that y?(x) contains a high degree of am-
biguity, in reality, the marginal distributions of the
base model have low entropy and after filtering with
? = 0.95, the average number of potential heads per
dependent ranges from 1.4 to 3.2, depending on the
target language.
The ambiguity-aware training methods, that is
ambiguity-aware self-training (AAST) and ambiguity-
aware ensemble-training (AAET), are compared to
three baseline systems. First, NBG+EM is the gen-
erative model of Naseem et al (2012) trained with
expectation-maximization on additional unlabeled
target language text. Second, Family is the best dis-
criminative model from the previous section. Third,
Viterbi is the basic Viterbi self-training model. The
results of each of these models are shown in Table 3.
There are a number of things that can be observed.
First, Viterbi self-training helps slightly on average,
but the gains are not consistent and there are even
drops in accuracy for some languages. Second, AAST
outperforms the Viterbi variant on all languages and
4We do not have access to the marginals of NBG.
1068
Target Adaptation
Lang. NBG+EM Family Viterbi AAST AAET
ar 59.3 52.7 52.6 53.5 58.7
bg 67.0 65.4 66.4 67.9 73.0
ca 71.7 77.6 78.0 79.9 76.1
cs 44.3 43.5 43.6 44.4 48.3
de 54.1 59.2 59.7 62.5 61.5
el 67.9 63.2 64.5 65.5 69.6
es 62.0 67.1 68.2 68.5 66.9
eu 47.8 46.8 47.5 48.6 49.4
hu 58.6 64.5 64.6 65.6 67.5
it 65.6 72.5 71.6 72.4 73.4
ja 64.1 65.9 65.7 68.8 72.0
nl 56.6 56.8 57.9 58.1 60.2
pt 75.8 78.4 79.9 80.7 79.9
sv 61.7 63.5 63.4 65.5 65.5
tr 59.4 59.4 59.5 64.1 64.2
zh 51.0 54.8 54.8 57.9 60.7
avg 60.4 62.0 62.4 64.0 65.4
Table 3: Target language adaptation using unlabeled tar-
get data. AAST: ambiguity-aware self-training. AAET:
ambiguity-aware ensemble-training. Boldface numbers
indicate the best result per language. Underlined numbers
indicate the best result, excluding AAET. NBG+EM is the
?D+,To? model from Naseem et al (2012).
nearly always improves on the base parser, although
it sees a slight drop for Italian. AAST improves the
accuracy over the base model by 2% absolute on av-
erage and by as much as 5% absolute for Turkish.
Comparing this model to the NBG+EM baseline, we
observe an improvement by 3.6% absolute, outper-
forming it on 14 of the 16 languages. Furthermore,
ambiguity-aware self-training appears to help more
than expectation-maximization for generative (unlex-
icalized) models. Naseem et al observed an increase
from 59.3% to 60.4% on average by adding unlabeled
target language data and the gains were not consistent
across languages. AAST, on the other hand, achieves
consistent gains, rising from 62.0% to 64.0% on av-
erage. Third, as shown in the rightmost column of
Table 3, ambiguity-aware ensemble-training is indeed
a successful strategy; AAET outperforms the previ-
ous best self-trained model on 13 and NB&G+EM
on 15 out of 16 languages. The relative error reduc-
tion with respect to the base Family model is 9% on
average, while the average reduction with respect to
NBG+EM is 13%.
Before concluding, two additional points are worth
making. First, further gains may potentially be
achievable with feature-rich discriminative models.
While the best generative transfer model of Naseem
et al (2012) approaches its upper-bounding super-
vised accuracy (60.4% vs. 67.1%), our relaxed self-
training model is still far below its supervised coun-
terpart (64.0% vs. 84.1%). One promising statistic
along these lines is that the oracle accuracy for the
ambiguous labelings of AAST is 75.7%, averaged
across languages, which suggests that other training
algorithms, priors or constraints could improve the
accuracy substantially. Second, relexicalization is a
key component of self-training. If we use delexical-
ized features during self-training, we only observe a
small average improvement from 62.0% to 62.1%.
6 Conclusions
We contributed to the understanding of multi-source
syntactic transfer in several complementary ways.
First, we showed how selective parameter sharing,
based on typological features and language family
membership, can be incorporated in a discriminative
graph-based model of dependency parsing. We then
showed how ambiguous labelings can be used to in-
tegrate heterogenous knowledge sources in parser
training. Two instantiations of this framework were
explored. First, an ambiguity-aware self-training
method that can be used to effectively relexicalize
and adapt a delexicalized transfer parser using unla-
beled target language data. Second, an ambiguity-
aware ensemble-training method, in which predic-
tions from different parsers can be incorporated and
further adapted. On average, our best model provides
a relative error reduction of 13% over the state-of-
the-art model of Naseem et al (2012), outperforming
it on 15 out of 16 evaluated languages.
Acknowledgments We thank Alexander Rush for
help with the hypergraph framework used for inference.
Tahira Naseem kindly provided us with her data sets and
the predictions of her systems. This work benefited from
many discussions with Yoav Goldberg and members of the
Google parsing team. We finally thank the three anony-
mous reviewers for their valuable feedback. The work of
the first author was partly funded by the Swedish National
Graduate School of Language Technology (GSLT).
1069
References
Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phyloge-
netic grammar induction. In Proceedings of ACL.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative reranking.
In Proceedings of ACL.
Shay B. Cohen and Noah A. Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying in
unsupervised grammar induction. In Proceedings of
NAACL.
Shay B. Cohen, Dipanjan Das, and Noah A. Smith. 2011.
Unsupervised structure prediction with non-parallel
multilingual guidance. In Proceedings of EMNLP.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of ACL-HLT.
Hal Daum? III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of ACL.
Mark Dredze, Partha Pratim Talukdar, and Koby Cram-
mer. 2009. Sequence learning from data with multiple
labels. In Proceedings of the ECML/PKDD Workshop
on Learning from Multi-Label Data.
Matthew S. Dryer and Martin Haspelmath, editors. 2011.
The World Atlas of Language Structures Online. Mu-
nich: Max Planck Digital Library. http://wals.info/.
Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syntac-
tic transfer using a bilingual lexicon. In Proceedings of
EMNLP-CoNLL.
Jason Eisner. 1996. Three new probabilistic models for
dependency parsing: an exploration. In Proceedings of
COLING.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Hierarchical Bayesian domain adaptation. In Proceed-
ings of HLT-NAACL.
Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.
2009. Dependency grammar induction via bitext pro-
jection constraints. In Proceedings of ACL-IJCNLP.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(03):311?325.
Rong Jin and Zoubin Ghahramani. 2002. Learning with
multiple labels. In Proceedings of NIPS.
Dan Klein and Chris D. Manning. 2004. Corpus-based
induction of syntactic structure: models of dependency
and constituency. In Proceedings of ACL.
Alexandre Klementiev, Ivan Titov, and Binod Bhattarai.
2012. Inducing crosslingual distributed representations
of words. In Proceedings of COLING.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adaptation.
In Proceedings of ACL.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of EMNLP.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proceedings of
EMNLP.
Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective sharing for multilingual dependency
parsing. In Proceedings of ACL.
Joakim Nivre. 2008. Algorithms for deterministic incre-
mental dependency parsing. Computational Linguis-
tics, 34(4):513?553.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard
Crouch, John T. Maxwell, III, and Mark Johnson.
2002. Parsing the Wall Street Journal using a lexical-
functional grammar and discriminative estimation tech-
niques. In Proceedings of ACL.
Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In Proceedings of NAACL.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with LR models and parser
ensembles. In Proceedings of EMNLP-CoNLL.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, and
Regina Barzilay. 2009. Adding more languages im-
proves unsupervised multilingual part-of-speech tag-
ging: A Bayesian non-parametric approach. In Pro-
ceedings of NAACL.
Anders S?gaard and Julie Wulff. 2012. An empirical
study of non-lexical extensions to delexicalized transfer.
In Proceedings of COLING.
Anders S?gaard. 2011. Data point selection for cross-
language adaptation of dependency parsers. In Pro-
ceedings of ACL.
Kathrin Spreyer and Jonas Kuhn. 2009. Data-driven
dependency parsing of new languages using incomplete
and noisy training data. In Proceedings of CONLL.
Oscar T?ckstr?m, Ryan McDonald, and Jakob Uszkoreit.
2012. Cross-lingual word clusters for direct transfer of
linguistic structure. In Proceedings of NAACL-HLT.
Oscar T?ckstr?m. 2012. Nudging the envelope of direct
transfer methods for multilingual named entity recog-
nition. In Proceedings of the NAACL-HLT 2012 Work-
shop on Inducing Linguistic Structure (WILS 2012).
1070
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001. Inducing multilingual text analysis tools via ro-
bust projection across aligned corpora. In Proceedings
of HLT.
Daniel Zeman and Philip Resnik. 2008. Cross-language
parser adaptation between related languages. In IJC-
NLP Workshop: NLP for Less Privileged Languages.
1071
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1492?1501,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A Transition-Based Parser for 2-Planar Dependency Structures
Carlos Go?mez-Rodr??guez
Departamento de Computacio?n
Universidade da Corun?a, Spain
carlos.gomez@udc.es
Joakim Nivre
Department of Linguistics and Philology
Uppsala University, Sweden
joakim.nivre@lingfil.uu.se
Abstract
Finding a class of structures that is rich
enough for adequate linguistic represen-
tation yet restricted enough for efficient
computational processing is an important
problem for dependency parsing. In this
paper, we present a transition system for
2-planar dependency trees ? trees that can
be decomposed into at most two planar
graphs ? and show that it can be used
to implement a classifier-based parser that
runs in linear time and outperforms a state-
of-the-art transition-based parser on four
data sets from the CoNLL-X shared task.
In addition, we present an efficient method
for determining whether an arbitrary tree
is 2-planar and show that 99% or more of
the trees in existing treebanks are 2-planar.
1 Introduction
Dependency-based syntactic parsing has become
a widely used technique in natural language pro-
cessing, and many different parsing models have
been proposed in recent years (Yamada and Mat-
sumoto, 2003; Nivre et al, 2004; McDonald et al,
2005a; Titov and Henderson, 2007; Martins et al,
2009). One of the unresolved issues in this area
is the proper treatment of non-projective depen-
dency trees, which seem to be required for an ad-
equate representation of predicate-argument struc-
ture, but which undermine the efficiency of depen-
dency parsing (Neuhaus and Bro?ker, 1997; Buch-
Kromann, 2006; McDonald and Satta, 2007).
Caught between the Scylla of linguistically in-
adequate projective trees and the Charybdis of
computationally intractable non-projective trees,
some researchers have sought a middle ground by
exploring classes of mildly non-projective depen-
dency structures that strike a better balance be-
tween expressivity and complexity (Nivre, 2006;
Kuhlmann and Nivre, 2006; Kuhlmann and Mo?hl,
2007; Havelka, 2007). Although these proposals
seem to have a very good fit with linguistic data,
in the sense that they often cover 99% or more of
the structures found in existing treebanks, the de-
velopment of efficient parsing algorithms for these
classes has met with more limited success. For
example, while both Kuhlmann and Satta (2009)
and Go?mez-Rodr??guez et al (2009) have shown
how well-nested dependency trees with bounded
gap degree can be parsed in polynomial time, the
best time complexity for lexicalized parsing of this
class remains a prohibitive O(n7), which makes
the practical usefulness questionable.
In this paper, we explore another characteri-
zation of mildly non-projective dependency trees
based on the notion of multiplanarity. This was
originally proposed by Yli-Jyra? (2003) but has so
far played a marginal role in the dependency pars-
ing literature, because no algorithm was known
for determining whether an arbitrary tree was m-
planar, and no parsing algorithm existed for any
constant value of m. The contribution of this pa-
per is twofold. First, we present a procedure for
determining the minimal number m such that a
dependency tree is m-planar and use it to show
that the overwhelming majority of sentences in de-
pendency treebanks have a tree that is at most 2-
planar. Secondly, we present a transition-based
parsing algorithm for 2-planar dependency trees,
developed in two steps. We begin by showing how
the stack-based algorithm of Nivre (2003) can be
generalized from projective to planar structures.
We then extend the system by adding a second
stack and show that the resulting system captures
exactly the set of 2-planar structures. Although the
contributions of this paper are mainly theoretical,
we also present an empirical evaluation of the 2-
planar parser, showing that it outperforms the pro-
jective parser on four data sets from the CoNLL-X
shared task (Buchholz and Marsi, 2006).
1492
2 Preliminaries
2.1 Dependency Graphs
Let w = w1 . . . wn be an input string.1 An inter-
val (with endpoints i and j) of the string w is a set
of the form [i, j] = {wk | i ? k ? j}.
Definition 1. A dependency graph for w is a di-
rected graph G = (Vw, E), where Vw = [1, n] and
E ? Vw ? Vw.
We call an edge (wi, wj) in a dependency graph G
a dependency link2 from wi to wj . We say that wi
is the parent (or head) of wj and, conversely, that
wj is a syntactic child (or dependent) of wi. For
convenience, we write wi ? wj ? E if the link
(wi, wj) exists; wi ? wj ? E if there is a link
from wi to wj or from wj to wi; wi ?? wj ? E if
there is a (possibly empty) directed path from wi
to wj ; and wi ?? wj ? E if there is a (possibly
empty) path between wi and wj in the undirected
graph underlying G (omitting reference to E when
clear from the context). The projection of a node
wi, denoted bwic, is the set of reflexive-transitive
dependents of wi: bwic = {wj ? V | wi ?? wj}.
Most dependency representations do not allow
arbitrary dependency graphs but typically require
graphs to be acyclic and have at most one head per
node. Such a graph is called a dependency forest.
Definition 2. A dependency graph G for a string
w1 . . . wn is said to be a forest iff it satisfies:
1. Acyclicity: If wi ?? wj , then not wj ? wi.
2. Single-head: If wj ? wi, then not wk ? wi
(for every k 6= j).
Nodes in a forest that do not have a head are called
roots. Some frameworks require that dependency
forests have a unique root (i.e., are connected).
Such a forest is called a dependency tree.
2.2 Projectivity
For reasons of computational efficiency, many de-
pendency parsers are restricted to work with pro-
jective dependency structures, that is, forests in
which the projection of each node corresponds to
a contiguous substring of the input:
1For notational convenience, we will assume throughout
the paper that all symbols in an input string are distinct, i.e.,
i 6= j ? wi 6= wj . This can be guaranteed in practice by
annotating each terminal symbol with its position in the input.
2In practice, dependency links are usually labeled, but to
simplify the presentation we will ignore labels throughout
most of the paper. However, all the results and algorithms
presented can be applied to labeled dependency graphs and
will be so applied in the experimental evaluation.
Definition 3. A dependency forest G for a string
w1 . . . wn is projective iff bwic is an interval for
every word wi ? [1, n].
Projective dependency trees correspond to the set
of structures that can be induced from lexicalised
context-free derivations (Kuhlmann, 2007; Gaif-
man, 1965). Like context-free grammars, projec-
tive dependency trees are not sufficient to repre-
sent all the linguistic phenomena observed in natu-
ral languages, but they have the advantage of being
efficiently parsable: their parsing problem can be
solved in cubic time with chart parsing techniques
(Eisner, 1996; Go?mez-Rodr??guez et al, 2008),
while in the case of general non-projective depen-
dency forests, it is only tractable under strong in-
dependence assumptions (McDonald et al, 2005b;
McDonald and Satta, 2007).
2.3 Planarity
The concept of planarity (Sleator and Temperley,
1993) is closely related to projectivity3 and can be
informally defined as the property of a dependency
forest whose links can be drawn above the words
without crossing.4 To define planarity more for-
mally, we first define crossing links as follows:
let (wi, wk) and (wj , wl) be dependency links in
a dependency graph G. Without loss of general-
ity, we assume that min(i, k) ? min(j, l). Then,
the links are said to be crossing if min(i, k) <
min(j, l) < max (i, k) < max (j, l).
Definition 4. A dependency graph is planar iff it
does not contain a pair of crossing links.
2.4 Multiplanarity
The concept of planarity on its own does not seem
to be very relevant as an extension of projectiv-
ity for practical dependency parsing. According
to the results by Kuhlmann and Nivre (2006), most
non-projective structures in dependency treebanks
are also non-planar, so being able to parse planar
structures will only give us a modest improvement
in coverage with respect to a projective parser.
However, our interest in planarity is motivated by
the fact that it can be generalised to multipla-
narity (Yli-Jyra?, 2003):
3For dependency forests that are extended with a unique
artificial root located at position 0, as is commonly done, the
two notions are equivalent.
4Planarity in the context of dependency structures is not to
be confused with the homonymous concept in graph theory,
which does not restrict links to be drawn above the nodes.
1493
Figure 1: A 2-planar dependency structure with
two different ways of distributing its links into two
planes (represented by solid and dotted lines).
Definition 5. A dependency graph G = (V,E)
is m-planar iff there exist planar dependency
graphs G1 = (V,E1), . . . , Gm = (V,Em) (called
planes) such that E = E1 ? ? ? ? ? Em.
Intuitively, we can associate planes with colours
and say that a dependency graph G is m-planar if it
is possible to assign one of m colours to each of its
links in such a way that links with the same colour
do not cross. Note that there may be multiple
ways of dividing an m-planar graph into planes,
as shown in the example of Figure 1.
3 Determining Multiplanarity
Several constraints on non-projective dependency
structures have been proposed recently that seek a
good balance between parsing efficiency and cov-
erage of non-projective phenomena present in nat-
ural language treebanks. For example, Kuhlmann
and Nivre (2006) and Havelka (2007) have shown
that the vast majority of structures present in exist-
ing treebanks are well-nested and have a small gap
degree (Bodirsky et al, 2005), leading to an inter-
est in parsers for these kinds of structures (Go?mez-
Rodr??guez et al, 2009). No similar analysis has
been performed for m-planar structures, although
Yli-Jyra? (2003) provides evidence that all except
two structures in the Danish dependency treebank
are at most 3-planar. However, his analysis is
based on constraints that restrict the possible ways
of assigning planes to dependency links, and he is
not guaranteed to find the minimal number m for
which a given structure is m-planar.
In this section, we provide a procedure for find-
ing the minimal number m such that a dependency
graph is m-planar and use it to show that the vast
majority of sentences in dependency treebanks are
Figure 2: The crossings graph corresponding to
the dependency structure of Figure 1.
at most 2-planar, with a coverage comparable to
that of well-nestedness. The idea is to reduce
the problem of determining whether a dependency
graph G = (V,E) is m-planar, for a given value
of m, to a standard graph colouring problem. Con-
sider first the following undirected graph:
U(G) = (E,C) where
C = {{ei, ej} | ei, ej are crossing links in G}
This graph, which we call the crossings graph of
G, has one node corresponding to each link in the
dependency graph G, with an undirected link be-
tween two nodes if they correspond to crossing
links in G. Figure 2 shows the crossings graph
of the 2-planar structure in Figure 1.
As noted in Section 2.4, a dependency graph G
is m-planar if each of its links can be assigned
one of m colours in such a way that links with the
same colours do not cross. In terms of the cross-
ings graph, this means that G is m-planar if each
of the nodes of U(G) can be assigned one of m
colours such that no two neighbours have the same
colour. This amounts to solving the well-known k-
colouring problem for U(G), where k = m.
For k = 1 the problem is trivial: a graph is 1-
colourable only if it has no edges. For k = 2, the
problem can be solved in time linear in the size of
the graph by simple breadth-first search. Given a
graph U = (V,E), we pick an arbitrary node v
and give it one of two colours. This forces us to
give the other colour to all its neighbours, the first
colour to the neighbours? neighbours, and so on.
This process continues until we have processed all
the nodes in the connected component of v. If this
has resulted in assigning two different colours to
the same node, the graph is not 2-colourable. Oth-
erwise, we have obtained a 2-colouring of the con-
nected component of U that contains v. If there
are still unprocessed nodes, we repeat the process
by arbitrarily selecting one of them, continue with
the rest of the connected components, and in this
way obtain a 2-colouring of the whole graph if it
1494
Language Structures Non-Projective Not Planar Not 2-Planar Not 3-Pl. Not 4-pl. Ill-nested
Arabic 2995 205 ( 6.84%) 158 ( 5.28%) 0 (0.00%) 0 (0.00%) 0 (0.00%) 1 (0.03%)
Czech 87889 20353 (23.16%) 16660 (18.96%) 82 (0.09%) 0 (0.00%) 0 (0.00%) 96 (0.11%)
Danish 5512 853 (15.48%) 827 (15.00%) 1 (0.02%) 1 (0.02%) 0 (0.00%) 6 (0.11%)
Dutch 13349 4865 (36.44%) 4115 (30.83%) 162 (1.21%) 1 (0.01%) 0 (0.00%) 15 (0.11%)
German 39573 10927 (27.61%) 10908 (27.56%) 671 (1.70%) 0 (0.00%) 0 (0.00%) 419 (1.06%)
Portuguese 9071 1718 (18.94%) 1713 (18.88%) 8 (0.09%) 0 (0.00%) 0 (0.00%) 7 (0.08%)
Swedish 6159 293 ( 4.76%) 280 ( 4.55%) 5 (0.08%) 0 (0.00%) 0 (0.00%) 14 (0.23%)
Turkish 5510 657 (11.92%) 657 (11.92%) 10 (0.18%) 0 (0.00%) 0 (0.00%) 20 (0.36%)
Table 1: Proportion of dependency trees classified by projectivity, planarity, m-planarity and ill-
nestedness in treebanks for Arabic (Hajic? et al, 2004), Czech (Hajic? et al, 2006), Danish (Kromann,
2003), Dutch (van der Beek et al, 2002), German (Brants et al, 2002), Portuguese (Afonso et al, 2002),
Swedish (Nilsson et al, 2005) and Turkish (Oflazer et al, 2003; Atalay et al, 2003).
exists. Since this process can be completed by vis-
iting each node and edge of the graph U once, its
complexity is O(V + E). The crossings graph of
a dependency graph with n nodes can trivially be
built in time O(n2) by checking each pair of de-
pendency links to determine if they cross, and can-
not contain more than n2 edges, which means that
we can check if the dependency graph for a sen-
tence of length n is 2-planar in O(n2) time.
For k > 2, the k-colouring problem is known
to be NP-complete (Karp, 1972). However, we
have found this not to be a problem when measur-
ing multiplanarity in natural language treebanks,
since the effective problem size can be reduced
by noting that each connected component of the
crossings graph can be treated separately, and that
nodes that are not part of a cycle need not be
considered.5 Given that non-projective sentences
in natural language tend to have a small propor-
tion of non-projective links (Nivre and Nilsson,
2005), the connected components of their cross-
ings graphs are very small, and k-colourings for
them can quickly be found by brute-force search.
By applying these techniques to dependency
treebanks of several languages, we obtain the data
shown in Table 1. As we can see, the coverage
provided by the 2-planarity constraint is compa-
rable to that of well-nestedness. In most of the
treebanks, well over 99% of the sentences are 2-
planar, and 3-planarity has almost total coverage.
As we will see below, the class of 2-planar depen-
dency structures not only has good coverage of lin-
guistic phenomena in existing treebanks but is also
efficiently parsable with transition-based parsing
methods, making it a practically interesting sub-
class of non-projective dependency structures.
5If we have a valid colouring for all the cycles in the
graph, the rest of the nodes can be safely coloured by breadth-
first search as in the k = 2 case.
4 Parsing 1-Planar Structures
In this section, we present a deterministic linear-
time parser for planar dependency structures. The
parser is a variant of Nivre?s arc-eager projec-
tive parser (Nivre, 2003), modified so that it can
also handle graphs that are planar but not projec-
tive. As seen in Table 1, this only gives a modest
improvement in coverage compared to projective
parsing, so the main interest of this algorithm lies
in the fact that it can be generalised to deal with
2-planar structures, as shown in the next section.
4.1 Transition Systems
In the transition-based framework of Nivre (2008),
a deterministic dependency parser is defined by a
non-deterministic transition system, specifying a
set of elementary operations that can be executed
during the parsing process, and an oracle that de-
terministically selects a single transition at each
choice point of the parsing process.
Definition 6. A transition system for dependency
parsing is a quadruple S = (C, T, cs, Ct) where
1. C is a set of possible parser configurations,
2. T is a set of transitions, each of which is a
partial function t : C ? C,
3. cs is a function that maps each input sentence
w to an initial configuration cs(w) ? C,
4. Ct ? C is a set of terminal configurations.
Definition 7. An oracle for a transition system
S = (C, T, cs, Ct) is a function o : C ? T .
An input sentence w can be parsed using a tran-
sition system S = (C, T, cs, Ct) and an oracle o
by starting in the initial configuration cs(w), call-
ing the oracle function on the current configuration
c, and updating the configuration by applying the
transition o(c) returned by the oracle. This pro-
cess is repeated until a terminal configuration is
1495
Initial configuration: cs(w1 . . . wn) = ?[], [w1 . . . wn], ??
Terminal configurations: Cf = {??, [], A? ? C}
Transitions: SHIFT ??, wi|B,A? ? ??|wi, B,A?
REDUCE ??|wi, B,A? ? ??, B,A?
LEFT-ARC ??|wi, wj |B,A? ? ??|wi, wj |B,A ? {(wj , wi)}?
only if 6 ?k|(wk, wi) ? A (single-head) and not wi ?? wj ? A (acyclicity).
RIGHT-ARC ??|wi, wj |B,A? ? ??|wi, wj |B,A ? {(wi, wj)}?
only if 6 ?k|(wk, wj) ? A (single-head) and not wi ?? wj ? A (acyclicity).
Figure 3: Transition system for planar dependency parsing.
reached, and the dependency analysis of the sen-
tence is defined by the terminal configuration.
Each sequence of configurations that the parser
can traverse from an initial configuration to a ter-
minal configuration for some input w is called a
transition sequence. If we associate each config-
uration c of a transition system S = (C, T, cs, Ct)
with a dependency graph g(c), we can say that
S is sound for a class of dependency graphs G
if, for every sentence w and transition sequence
(cs(w), c1, . . . , cf ) of S, g(cf ) is in G, and that S
is complete for G if, for every sentence w and de-
pendency graph G ? G for w, there is a transition
sequence (cs(w), c1, . . . , cf ) such that g(cf ) = G.
A transition system that is sound and complete for
G is said to be correct for G.
Note that, apart from a correct transition system,
a practical parser needs a good oracle to achieve
the desired results, since a transition system only
specifies how to reach all the possible dependency
graphs that could be associated to a sentence, but
not how to select the correct one. Oracles for prac-
tical parsers can be obtained by training classifiers
on treebank data (Nivre et al, 2004).
4.2 A Transition System for Planar
Structures
A correct transition system for the class of planar
dependency forests can be obtained as a variant of
the arc-eager projective system by Nivre (2003).
As in that system, the set of configurations of the
planar transition system is the set of all triples
c = ??, B,A? such that ? and B are disjoint lists
of words from Vw (for some input w), and A is a
set of dependency links over Vw. The list B, called
the buffer, is initialised to the input string and is
used to hold the words that are still to be read from
the input. The list ?, called the stack, is initially
empty and holds words that have dependency links
pending to be created. The system is shown in Fig-
ure 3, where we use the notation ?|wi for a stack
with top wi and tail ?, and we invert the notation
for the buffer for clarity (i.e., wi|B is a buffer with
top wi and tail B).
The system reads the input from left to right and
creates links in a left-to-right order by executing
its four transitions:
1. SHIFT: pops the first (leftmost) word in the
buffer, and pushes it to the stack.
2. LEFT-ARC: adds a link from the first word in
the buffer to the top of the stack.
3. RIGHT-ARC: adds a link from the top of the
stack to the first word in the buffer.
4. REDUCE: pops the top word from the stack,
implying that we have finished building links
to or from it.
Note that the planar parser?s transitions are more
fine-grained than those of the arc-eager projective
parser by Nivre (2003), which pops the stack as
part of its LEFT-ARC transition and shifts a word
as part of its RIGHT-ARC transition. Forcing these
actions after creating dependency links rules out
structures whose root is covered by a dependency
link, which are planar but not projective. In order
to support these structures, we therefore simplify
the ARC transitions (LEFT-ARC and RIGHT-ARC)
so that they only create an arc. For the same rea-
son, we remove the constraint in Nivre?s parser by
which words without a head cannot be reduced.
This has the side effect of making the parser able
to output cyclic graphs. Since we are interested
in planar dependency forests, which do not con-
tain cycles, we only apply ARC transitions after
checking that there is no undirected path between
the nodes to be linked. This check can be done
without affecting the linear-time complexity of the
1496
parser by storing the weakly connected component
of each node in g(c).
The fine-grained transitions used by this parser
have also been used by Sagae and Tsujii (2008)
to parse DAGs. However, the latter parser differs
from ours in the constraints, since it does not allow
the reduction of words without a head (disallowing
forests with covered roots) and does not enforce
the acyclicity constraint (which is guaranteed by
post-processing the graphs to break cycles).
4.3 Correctness and Complexity
For reasons of space, we can only give a sketch
of the correctness proof. We wish to prove that
the planar transition system is sound and com-
plete for the set Fp of all planar dependency
forests. To prove soundness, we have to show
that, for every sentence w and transition sequence
(cs(w), c1, . . . , cf ), the graph g(cf ) associated
with cf is in Fp. We take the graph associated
with a configuration c = (?, B,A) to be g(c) =
(Vw, A). With this, we prove the stronger claim
that g(c) ? Fp for every configuration c that be-
longs to some transition sequence starting with
cs(w). This amounts to showing that in every con-
figuration c reachable from cs(w), g(c) meets the
following three conditions that characterise a pla-
nar dependency forest: (1) g(c) does not contain
nodes with more than one head; (2) g(c) is acyclic;
and (3) g(c) contains no crossing links. (1) is triv-
ially guaranteed by the single-head constraint; (2)
follows from (1) and the acyclicity constraint; and
(3) can be established by proving that there is no
transition sequence that will invoke two ARC tran-
sitions on node pairs that would create crossing
links. At the point when a link from wi to wj is
created, we know that all the words strictly located
between wi and wj are not in the stack or in the
buffer, so no links can be created to or from them.
To prove completeness, we show that every
planar dependency forest G = (V,E) ? Fp
for a sentence w can be produced by apply-
ing the oracle function that maps a configuration
??|wi, wj |B,A? to:
1. LEFT-ARC if wj ? wi ? (E \A),
2. RIGHT-ARC if wi ? wj ? (E \A),
3. REDUCE if ?x[x<i][wx ? wj ? (E \A)],
4. SHIFT otherwise.
We show completeness by setting the following in-
variants on transitions traversed by the application
of the oracle:
1. ?a, b[a,b<j][wa?wb?E ? wa?wb?A]
2. [wi?wj?A?
?k[i<k<j][wk?wj?E ? wk?wj?A]]
3. ?k[k<j][wk 6???
?l[l>k][wk?wl?E ? wk?wl?A]]
We can show that each branch of the oracle func-
tion keeps these invariants true. When we reach a
terminal configuration (which always happens af-
ter a finite number of transitions, since every tran-
sition generating a configuration c = ??, B,A?
decreases the value of the variant function |E| +
|?| + 2|B| ? |A|), it can be deduced from the in-
variant that A = E, which proves completeness.
The worst-case complexity of a deterministic
transition-based parser is given by an upper bound
on transition sequence length (Nivre, 2008). For
the planar system, like its projective counterpart,
the length is clearly O(n) (where n is the number
of input words), since there can be no more than
n SHIFT transitions, n REDUCE transitions, and n
ARC transitions in a transition sequence.
5 Parsing 2-Planar Structures
The planar parser introduced in the previous sec-
tion can be extended to parse all 2-planar depen-
dency structures by adding a second stack to the
system and making REDUCE and ARC transitions
apply to only one of the stacks at a time. This
means that the set of links created in the context
of each individual stack will be planar, but pairs
of links created in different stacks are allowed to
cross. In this way, the parser will build a 2-planar
dependency forest by using each of the stacks to
construct one of its two planes.
The 2-planar transition system, shown in Figure
4, has configurations of the form ??0,?1, B,A?,
where we call ?0 the active stack and ?1 the in-
active stack, and the following transitions:
1. SHIFT: pops the first (leftmost) word in the
buffer, and pushes it to both stacks.
2. LEFT-ARC: adds a link from the first word in
the buffer to the top of the active stack.
3. RIGHT-ARC: adds a link from the top of the
active stack to the first word in the buffer.
4. REDUCE: pops the top word from the active
stack, implying that we have added all links
to or from it on the plane tied to that stack.
5. SWITCH: makes the active stack inactive and
vice versa, changing the plane the parser is
working with.
1497
Initial configuration: cs(w1 . . . wn) = ?[], [], [w1 . . . wn], ??
Terminal configurations: Cf = {??0,?1, [], A? ? C}
Transitions: SHIFT ??0,?1, wi|B,A? ? ??0|wi,?1|wi, B,A?
REDUCE ??0|wi,?1, B,A? ? ??0,?1, B,A?
LEFT-ARC ??0|wi,?1, wj |B,A? ? ??0|wi,?1, wj |B,A ? {(wj , wi)}?
only if 6 ?k | (wk, wi) ? A (single-head) and not wi ?? wj ? A (acyclicity).
RIGHT-ARC ??0|wi,?1, wj |B,A? ? ??0|wi,?1, wj |B,A ? {(wi, wj)}?
only if 6 ?k|(wk, wj) ? A (single-head) and not wi ?? wj ? A (acyclicity).
SWITCH ??0,?1, B,A? ? ??1,?0, B,A?
Figure 4: Transition system for 2-planar dependency parsing.
5.1 Correctness and Complexity
As in the planar case, we provide a brief sketch
of the proof that the transition system in Figure 4
is correct for the set F2p of 2-planar dependency
forests. Soundness follows from a reasoning anal-
ogous to the planar case, but applying the proof
of planarity separately to each stack. In this way,
we prove that the sets of dependency links cre-
ated by linking to or from the top of each of the
two stacks are always planar graphs, and thus their
union (which is the dependency graph stored in A)
is 2-planar. This, together with the single-head and
acyclicity constraints, guarantees that the depen-
dency graphs associated with reachable configura-
tions are always 2-planar dependency forests.
For completeness, we assume an extended form
of the transition system where transitions take the
form ??0,?1, B,A, p?, where p is a flag taking
values in {0, 1} which equals 0 for initial config-
urations and gets flipped by each application of a
SWITCH transition. Then we show that every 2-
planar dependency forest G ? F2p, with planes
G0 = (V,E0) and G1 = (V,E1), can be produced
by this system by applying the oracle function that
maps a configuration ??0|wi,?1, wj |B,A, p? to:
1. LEFT-ARC if wj?wi?(Ep \A),
2. RIGHT-ARC if wi?wj ?(Ep \A),
3. REDUCE if ?x[x<i][wx?wj ?(Ep \A)?
??y[x<y?i][wy?wj ?(Ep \A)]],
4. SWITCH if ?x<j : (wx, wj) or (wj , wx) ? (Ep\A),
5. SHIFT otherwise.
This can be shown by employing invariants analo-
gous to the planar case, with the difference that the
third invariant applies to each stack and its corre-
sponding plane: if ?y is associated with the plane
Ex,6 we have:
3. ?k[k<j][wk 6? ?y]?
?l[l>k][wk?wl?Ex]? [wk?wl?A]
Since the presence of the flag p in configurations
does not affect the set of dependency graphs gen-
erated by the system, the completeness of the sys-
tem extended with the flag p implies that of the
system in Figure 4.
We can show that the complexity of the 2-planar
system is O(n) by the same kind of reasoning as
for the 1-planar system, with the added complica-
tion that we must constrain the system to prevent
two adjacent SWITCH transitions. In fact, without
this restriction, the parser is not even guaranteed
to terminate.
5.2 Implementation
In practical settings, oracles for transition-based
parsers can be approximated by classifiers trained
on treebank data (Nivre, 2008). To do this, we
need an oracle that will generate transition se-
quences for gold-standard dependency graphs. In
the case of the planar parser of Section 4.2, the or-
acle of 4.3 is suitable for this purpose. However,
in the case of the 2-planar parser, the oracle used
for the completeness proof in Section 5.1 cannot
be used directly, since it requires the gold-standard
trees to be divided into two planes in order to gen-
erate a transition sequence.
Of course, it is possible to use the algorithm
presented in Section 3 to obtain a division of sen-
tences into planes. However, for training purposes
and to obtain a robust behaviour if non-2-planar
6The plane corresponding to each stack in a configuration
changes with each SWITCH transition: ?x is associated with
Ex in configurations where p = 0, and with Ex in those
where p = 1.
1498
Czech Danish German Portuguese
Parser LAS UAS NPP NPR LAS UAS NPP NPR LAS UAS NPP NPR LAS UAS NPP NPR
2-planar 79.24 85.30 68.9 60.7 83.81 88.50 66.7 20.0 86.50 88.84 57.1 45.8 87.04 90.82 82.8 33.8
Malt P 78.18 84.12 ? ? 83.31 88.30 ? ? 85.36 88.06 ? ? 86.60 90.20 ? ?
Malt PP 79.80 85.70 76.7 56.1 83.67 88.52 41.7 25.0 85.76 88.66 58.1 40.7 87.08 90.66 83.3 46.2
Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)
pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;
NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
sentences are found, it is more convenient that
the oracle can distribute dependency links into the
planes incrementally, and that it produces a dis-
tribution of links that only uses SWITCH transi-
tions when it is strictly needed to account for non-
planarity. Thus we use a more complex version of
the oracle which performs a search in the crossings
graph to check if a dependency link can be built on
the plane of the active stack, and only performs a
switch when this is not possible. This has proved
to work well in practice, as will be observed in the
results in the next section.
6 Empirical Evaluation
In order to get a first estimate of the empirical ac-
curacy that can be obtained with transition-based
2-planar parsing, we have evaluated the parser
on four data sets from the CoNLL-X shared task
(Buchholz and Marsi, 2006): Czech, Danish, Ger-
man and Portuguese. As our baseline, we take
the strictly projective arc-eager transition system
proposed by Nivre (2003), as implemented in the
freely available MaltParser system (Nivre et al,
2006a), with and without the pseudo-projective
parsing technique for recovering non-projective
dependencies (Nivre and Nilsson, 2005). For the
two baseline systems, we use the parameter set-
tings used by Nivre et al (2006b) in the original
shared task, where the pseudo-projective version
of MaltParser was one of the two top performing
systems (Buchholz and Marsi, 2006). For our 2-
planar parser, we use the same kernelized SVM
classifiers as MaltParser, using the LIBSVM pack-
age (Chang and Lin, 2001), with feature models
that are similar to MaltParser but extended with
features defined over the second stack.7
In Table 2, we report labeled (LAS) and un-
labeled (UAS) attachment score on the four lan-
guages for all three systems. For the two systems
that are capable of recovering non-projective de-
7Complete information about experimental settings can
be found at http://stp.lingfil.uu.se/ nivre/exp/.
pendencies, we also report precision (NPP) and
recall (NPR) specifically on non-projective depen-
dency arcs. The results show that the 2-planar
parser outperforms the strictly projective variant
of MaltParser on all metrics for all languages,
and that it performs on a par with the pseudo-
projective variant with respect to both overall at-
tachment score and precision and recall on non-
projective dependencies. These results look very
promising in view of the fact that very little effort
has been spent on optimizing the training oracle
and feature model for the 2-planar parser so far.
It is worth mentioning that the 2-planar parser
has two advantages over the pseudo-projective
parser. The first is simplicity, given that it is based
on a single transition system and makes a single
pass over the input, whereas the pseudo-projective
parsing technique involves preprocessing of train-
ing data and post-processing of parser output
(Nivre and Nilsson, 2005). The second is the fact
that it parses a well-defined class of dependency
structures, with known coverage8, whereas no for-
mal characterization exists of the class of struc-
tures parsable by the pseudo-projective parser.
7 Conclusion
In this paper, we have presented an efficient algo-
rithm for deciding whether a dependency graph is
2-planar and a transition-based parsing algorithm
that is provably correct for 2-planar dependency
forests, neither of which existed in the literature
before. In addition, we have presented empirical
results showing that the class of 2-planar depen-
dency forests includes the overwhelming majority
of structures found in existing treebanks and that
a deterministic classifier-based implementation of
the 2-planar parser gives state-of-the-art accuracy
on four different languages.
8If more coverage is desired, the 2-planar parser can be
generalised to m-planar structures for larger values of m by
adding additional stacks. However, this comes at the cost of
more complex training models, making the practical interest
of increasing m beyond 2 dubious.
1499
Acknowledgments
The first author has been partially supported by
Ministerio de Educacio?n y Ciencia and FEDER
(HUM2007-66607-C04) and Xunta de Galicia
(PGIDIT07SIN005206PR, Rede Galega de Proce-
samento da Linguaxe e Recuperacio?n de Infor-
macio?n, Rede Galega de Lingu???stica de Corpus,
Bolsas Estad??as INCITE/FSE cofinanced).
References
Susana Afonso, Eckhard Bick, Renato Haber, and Di-
ana Santos. 2002. ?Floresta sinta?(c)tica?: a tree-
bank for Portuguese. In Proceedings of the 3rd In-
ternational Conference on Language Resources and
Evaluation (LREC 2002), pages 1968?1703, Paris,
France. ELRA.
Nart B. Atalay, Kemal Oflazer, and Bilge Say. 2003.
The annotation process in the Turkish treebank.
In Proceedings of EACL Workshop on Linguisti-
cally Interpreted Corpora (LINC-03), pages 243?
246, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Leonoor van der Beek, Gosse Bouma, Robert Malouf,
and Gertjan van Noord. 2002. The Alpino depen-
dency treebank. In Language and Computers, Com-
putational Linguistics in the Netherlands 2001. Se-
lected Papers from the Twelfth CLIN Meeting, pages
8?22, Amsterdam, the Netherlands. Rodopi.
Manuel Bodirsky, Marco Kuhlmann, and Mathias
Mo?hl. 2005. Well-nested drawings as models of
syntactic structure. In 10th Conference on Formal
Grammar and 9th Meeting on Mathematics of Lan-
guage, Edinburgh, Scotland, UK.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The tiger
treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories, September 20-21,
Sozopol, Bulgaria.
Matthias Buch-Kromann. 2006. Discontinuous Gram-
mar: A Model of Human Parsing and Language
Acquisition. Ph.D. thesis, Copenhagen Business
School.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency parsing.
In Proceedings of the 10th Conference on Computa-
tional Natural Language Learning (CoNLL), pages
149?164.
Chih-Chung Chang and Chih-Jen Lin, 2001.
LIBSVM: A Library for Support Vec-
tor Machines. Software available at
http://www.csie.ntu.edu.tw/?cjlin/libsvm.
Jason Eisner. 1996. Three new probabilistic mod-
els for dependency parsing: An exploration. In
Proceedings of the 16th International Conference
on Computational Linguistics (COLING-96), pages
340?345, San Francisco, CA, USA, August. ACL /
Morgan Kaufmann.
Haim Gaifman. 1965. Dependency systems and
phrase-structure systems. Information and Control,
8:304?337.
Carlos Go?mez-Rodr??guez, John Carroll, and David
Weir. 2008. A deductive approach to depen-
dency parsing. In Proceedings of the 46th An-
nual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies
(ACL?08:HLT), pages 968?976, Morristown, NJ,
USA. Association for Computational Linguistics.
Carlos Go?mez-Rodr??guez, David Weir, and John Car-
roll. 2009. Parsing mildly non-projective depen-
dency structures. In Proceedings of the 12th Con-
ference of the European Chapter of the Association
for Computational Linguistics (EACL), pages 291?
299.
Jan Hajic?, Otakar Smrz?, Petr Zema?nek, Jan S?naidauf,
and Emanuel Bes?ka. 2004. Prague Arabic de-
pendency treebank: Development in data and tools.
In Proceedings of the NEMLAR International Con-
ference on Arabic Language Resources and Tools,
pages 110?117.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Jarmila
Panevova?, Petr Sgall, Petr Pajas, Jan S?te?pa?nek,
Jir??? Havelka, and Marie Mikulova?. 2006.
Prague Dependency Treebank 2.0. CDROM
CAT: LDC2006T01, ISBN 1-58563-370-4. Linguis-
tic Data Consortium.
Jiri Havelka. 2007. Beyond projectivity: Multilin-
gual evaluation of constraints and measures on non-
projective structures. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 608?615.
Richard M. Karp. 1972. Reducibility among combi-
natorial problems. In R. Miller and J. Thatcher, ed-
itors, Complexity of Computer Computations, pages
85?103. Plenum Press.
Matthias T. Kromann. 2003. The Danish dependency
treebank and the underlying linguistic theory. In
Proceedings of the 2nd Workshop on Treebanks and
Linguistic Theories (TLT), pages 217?220, Va?xjo?,
Sweden. Va?xjo? University Press.
Marco Kuhlmann and Mathias Mo?hl. 2007. Mildly
context-sensitive dependency languages. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, pages 160?167.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly
non-projective dependency structures. In Proceed-
ings of the COLING/ACL 2006 Main Conference
Poster Sessions, pages 507?514.
1500
Marco Kuhlmann and Giorgio Satta. 2009. Treebank
grammar techniques for non-projective dependency
parsing. In Proceedings of the 12th Conference of
the European Chapter of the Association for Com-
putational Linguistics (EACL), pages 478?486.
Marco Kuhlmann. 2007. Dependency Structures and
Lexicalized Grammars. Doctoral dissertation, Saar-
land University, Saarbru?cken, Germany.
Andre Martins, Noah Smith, and Eric Xing. 2009.
Concise integer linear programming formulations
for dependency parsing. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP (ACL-
IJCNLP), pages 342?350.
Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency
parsing. In Proceedings of the 10th International
Conference on Parsing Technologies (IWPT), pages
122?131.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 91?98.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In HLT/EMNLP
2005: Proceedings of the conference on Human
Language Technology and Empirical Methods in
Natural Language Processing, pages 523?530, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Peter Neuhaus and Norbert Bro?ker. 1997. The com-
plexity of recognition of linguistically adequate de-
pendency grammars. In Proceedings of the 35th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL) and the 8th Conference of
the European Chapter of the Association for Com-
putational Linguistics (EACL), pages 337?343.
Jens Nilsson, Johan Hall, and Joakim Nivre. 2005.
MAMBA meets TIGER: Reconstructing a Swedish
treebank from antiquity. In Proceedings of NODAL-
IDA 2005 Special Session on Treebanks, pages 119?
132. Samfundslitteratur, Frederiksberg, Denmark,
May.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In ACL ?05: Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 99?106,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Proceed-
ings of the 8th Conference on Computational Nat-
ural Language Learning (CoNLL-2004), pages 49?
56, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006a.
MaltParser: A data-driven parser-generator for de-
pendency parsing. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (LREC), pages 2216?2219.
Joakim Nivre, Johan Hall, Jens Nilsson, Gu?lsen
Eryig?it, and Svetoslav Marinov. 2006b. Labeled
pseudo-projective dependency parsing with support
vector machines. In Proceedings of the 10th Confer-
ence on Computational Natural Language Learning
(CoNLL), pages 221?225.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT), pages 149?160.
Joakim Nivre. 2006. Constraints on non-projective de-
pendency graphs. In Proceedings of the 11th Con-
ference of the European Chapter of the Association
for Computational Linguistics (EACL), pages 73?
80.
Joakim Nivre. 2008. Algorithms for Deterministic In-
cremental Dependency Parsing. Computational Lin-
guistics, 34(4):513?553.
Kemal Oflazer, Bilge Say, Dilek Zeynep Hakkani-Tu?r,
and Go?khan Tu?r. 2003. Building a Turkish tree-
bank. In A. Abeille (ed.), Building and Exploiting
Syntactically-annotated Corpora, pages 261?277,
Dordrecht, the Netherlands. Kluwer.
Kenji Sagae and Jun?ichi Tsujii. 2008. Shift-reduce
dependency DAG parsing. In COLING ?08: Pro-
ceedings of the 22nd International Conference on
Computational Linguistics, pages 753?760, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Daniel Sleator and Davy Temperley. 1993. Parsing
English with a Link Grammar. In Proceedings of the
Third International Workshop on Parsing Technolo-
gies (IWPT?93), pages 277?292. ACL/SIGPARSE.
Ivan Titov and James Henderson. 2007. A latent vari-
able model for generative dependency parsing. In
Proceedings of the 10th International Conference on
Parsing Technologies (IWPT), pages 144?155.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of the 8th International
Workshop on Parsing Technologies (IWPT), pages
195?206.
Anssi Mikael Yli-Jyra?. 2003. Multiplanarity ? a
model for dependency structures in treebanks. In
Joakim Nivre and Erhard Hinrichs, editors, TLT
2003. Proceedings of the Second Workshop on Tree-
banks and Linguistic Theories, volume 9 of Mathe-
matical Modelling in Physics, Engineering and Cog-
nitive Sciences, pages 189?200, Va?xjo?, Sweden, 14-
15 November. Va?xjo? University Press.
1501
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 188?193,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Transition-based Dependency Parsing with Rich Non-local Features
Yue Zhang
University of Cambridge
Computer Laboratory
yue.zhang@cl.cam.ac.uk
Joakim Nivre
Uppsala University
Department of Linguistics and Philology
joakim.nivre@lingfil.uu.se
Abstract
Transition-based dependency parsers gener-
ally use heuristic decoding algorithms but can
accommodate arbitrarily rich feature represen-
tations. In this paper, we show that we can im-
prove the accuracy of such parsers by consid-
ering even richer feature sets than those em-
ployed in previous systems. In the standard
Penn Treebank setup, our novel features im-
prove attachment score form 91.4% to 92.9%,
giving the best results so far for transition-
based parsing and rivaling the best results
overall. For the Chinese Treebank, they give a
signficant improvement of the state of the art.
An open source release of our parser is freely
available.
1 Introduction
Transition-based dependency parsing (Yamada and
Matsumoto, 2003; Nivre et al, 2006b; Zhang and
Clark, 2008; Huang and Sagae, 2010) utilize a deter-
ministic shift-reduce process for making structural
predictions. Compared to graph-based dependency
parsing, it typically offers linear time complexity
and the comparative freedom to define non-local fea-
tures, as exemplified by the comparison between
MaltParser and MSTParser (Nivre et al, 2006b; Mc-
Donald et al, 2005; McDonald and Nivre, 2007).
Recent research has addressed two potential dis-
advantages of systems like MaltParser. In the
aspect of decoding, beam-search (Johansson and
Nugues, 2007; Zhang and Clark, 2008; Huang et
al., 2009) and partial dynamic-programming (Huang
and Sagae, 2010) have been applied to improve upon
greedy one-best search, and positive results were re-
ported. In the aspect of training, global structural
learning has been used to replace local learning on
each decision (Zhang and Clark, 2008; Huang et al,
2009), although the effect of global learning has not
been separated out and studied alone.
In this short paper, we study a third aspect in a
statistical system: feature definition. Representing
the type of information a statistical system uses to
make predictions, feature templates can be one of
the most important factors determining parsing ac-
curacy. Various recent attempts have been made
to include non-local features into graph-based de-
pendency parsing (Smith and Eisner, 2008; Martins
et al, 2009; Koo and Collins, 2010). Transition-
based parsing, by contrast, can easily accommodate
arbitrarily complex representations involving non-
local features. Complex non-local features, such as
bracket matching and rhythmic patterns, are used
in transition-based constituency parsing (Zhang and
Clark, 2009; Wang et al, 2006), and most transition-
based dependency parsers incorporate some non-
local features, but current practice is nevertheless to
use a rather restricted set of features, as exemplified
by the default feature models in MaltParser (Nivre et
al., 2006a). We explore considerably richer feature
representations and show that they improve parsing
accuracy significantly.
In standard experiments using the Penn Treebank,
our parser gets an unlabeled attachment score of
92.9%, which is the best result achieved with a
transition-based parser and comparable to the state
of the art. For the Chinese Treebank, our parser gets
a score of 86.0%, the best reported result so far.
188
2 The Transition-based Parsing Algorithm
In a typical transition-based parsing process, the in-
put words are put into a queue and partially built
structures are organized by a stack. A set of shift-
reduce actions are defined, which consume words
from the queue and build the output parse. Recent
research have focused on action sets that build pro-
jective dependency trees in an arc-eager (Nivre et
al., 2006b; Zhang and Clark, 2008) or arc-standard
(Yamada and Matsumoto, 2003; Huang and Sagae,
2010) process. We adopt the arc-eager system1, for
which the actions are:
? Shift, which removes the front of the queue
and pushes it onto the top of the stack;
? Reduce, which pops the top item off the stack;
? LeftArc, which pops the top item off the
stack, and adds it as a modifier to the front of
the queue;
? RightArc, which removes the front of the
queue, pushes it onto the stack and adds it as
a modifier to the top of the stack.
Further, we follow Zhang and Clark (2008) and
Huang et al (2009) and use the generalized percep-
tron (Collins, 2002) for global learning and beam-
search for decoding. Unlike both earlier global-
learning parsers, which only perform unlabeled
parsing, we perform labeled parsing by augmenting
the LeftArc and RightArc actions with the set
of dependency labels. Hence our work is in line with
Titov and Henderson (2007) in using labeled transi-
tions with global learning. Moreover, we will see
that label information can actually improve link ac-
curacy.
3 Feature Templates
At each step during a parsing process, the
parser configuration can be represented by a tuple
?S,N,A?, where S is the stack, N is the queue of
incoming words, and A is the set of dependency
arcs that have been built. Denoting the top of stack
1It is very likely that the type of features explored in this
paper would be beneficial also for the arc-standard system, al-
though the exact same feature templates would not be applicable
because of differences in the parsing order.
from single words
S0wp; S0w; S0p; N0wp; N0w; N0p;
N1wp; N1w; N1p; N2wp; N2w; N2p;
from word pairs
S0wpN0wp; S0wpN0w; S0wN0wp; S0wpN0p;
S0pN0wp; S0wN0w; S0pN0p
N0pN1p
from three words
N0pN1pN2p; S0pN0pN1p; S0hpS0pN0p;
S0pS0lpN0p; S0pS0rpN0p; S0pN0pN0lp
Table 1: Baseline feature templates.
w ? word; p ? POS-tag.
distance
S0wd; S0pd; N0wd; N0pd;
S0wN0wd; S0pN0pd;
valency
S0wvr; S0pvr; S0wvl; S0pvl; N0wvl; N0pvl;
unigrams
S0hw; S0hp; S0l; S0lw; S0lp; S0ll;
S0rw; S0rp; S0rl;N0lw; N0lp; N0ll;
third-order
S0h2w; S0h2p; S0hl; S0l2w; S0l2p; S0l2l;
S0r2w; S0r2p; S0r2l; N0l2w; N0l2p; N0l2l;
S0pS0lpS0l2p; S0pS0rpS0r2p;
S0pS0hpS0h2p; N0pN0lpN0l2p;
label set
S0wsr; S0psr; S0wsl; S0psl; N0wsl; N0psl;
Table 2: New feature templates.
w ? word; p ? POS-tag; vl, vr ? valency; l ?
dependency label, sl, sr ? labelset.
with S0, the front items from the queue with N0,
N1, and N2, the head of S0 (if any) with S0h, the
leftmost and rightmost modifiers of S0 (if any) with
S0l and S0r, respectively, and the leftmost modifier
of N0 (if any) with N0l, the baseline features are
shown in Table 1. These features are mostly taken
from Zhang and Clark (2008) and Huang and Sagae
(2010), and our parser reproduces the same accura-
cies as reported by both papers. In this table, w and
p represents the word and POS-tag, respectively. For
example, S0pN0wp represents the feature template
that takes the word and POS-tag of N0, and com-
bines it with the word of S0.
189
In this short paper, we extend the baseline feature
templates with the following:
Distance between S0 and N0
Direction and distance between a pair of head and
modifier have been used in the standard feature
templates for maximum spanning tree parsing (Mc-
Donald et al, 2005). Distance information has
also been used in the easy-first parser of (Goldberg
and Elhadad, 2010). For a transition-based parser,
direction information is indirectly included in the
LeftArc and RightArc actions. We add the dis-
tance between S0 and N0 to the feature set by com-
bining it with the word and POS-tag of S0 and N0,
as shown in Table 2.
It is worth noticing that the use of distance in-
formation in our transition-based model is different
from that in a typical graph-based parser such as
MSTParser. The distance between S0 and N0 will
correspond to the distance between a pair of head
and modifier when an LeftArc action is taken, for
example, but not when a Shift action is taken.
Valency of S0 and N0
The number of modifiers to a given head is used
by the graph-based submodel of Zhang and Clark
(2008) and the models of Martins et al (2009) and
Sagae and Tsujii (2007). We include similar infor-
mation in our model. In particular, we calculate the
number of left and right modifiers separately, call-
ing them left valency and right valency, respectively.
Left and right valencies are represented by vl and vr
in Table 2, respectively. They are combined with the
word and POS-tag of S0 and N0 to form new feature
templates.
Again, the use of valency information in our
transition-based parser is different from the afore-
mentioned graph-based models. In our case,
valency information is put into the context of the
shift-reduce process, and used together with each
action to give a score to the local decision.
Unigram information for S0h, S0l, S0r and N0l
The head, left/rightmost modifiers of S0 and the
leftmost modifier of N0 have been used by most
arc-eager transition-based parsers we are aware of
through the combination of their POS-tag with infor-
mation from S0 and N0. Such use is exemplified by
the feature templates ?from three words? in Table 1.
We further use their word and POS-tag information
as ?unigram? features in Table 2. Moreover, we
include the dependency label information in the
unigram features, represented by l in the table. Uni-
gram label information has been used in MaltParser
(Nivre et al, 2006a; Nivre, 2006).
Third-order features of S0 and N0
Higher-order context features have been used by
graph-based dependency parsers to improve accura-
cies (Carreras, 2007; Koo and Collins, 2010). We
include information of third order dependency arcs
in our new feature templates, when available. In
Table 2, S0h2, S0l2, S0r2 and N0l2 refer to the head
of S0h, the second leftmost modifier and the second
rightmost modifier of S0, and the second leftmost
modifier of N0, respectively. The new templates
include unigram word, POS-tag and dependency
labels of S0h2, S0l2, S0r2 and N0l2, as well as
POS-tag combinations with S0 and N0.
Set of dependency labels with S0 and N0
As a more global feature, we include the set of
unique dependency labels from the modifiers of S0
and N0. This information is combined with the word
and POS-tag of S0 and N0 to make feature templates.
In Table 2, sl and sr stands for the set of labels on
the left and right of the head, respectively.
4 Experiments
Our experiments were performed using the Penn
Treebank (PTB) and Chinese Treebank (CTB) data.
We follow the standard approach to split PTB3, using
sections 2 ? 21 for training, section 22 for develop-
ment and 23 for final testing. Bracketed sentences
from PTB were transformed into dependency for-
mats using the Penn2Malt tool.2 Following Huang
and Sagae (2010), we assign POS-tags to the training
data using ten-way jackknifing. We used our imple-
mentation of the Collins (2002) tagger (with 97.3%
accuracy on a standard Penn Treebank test) to per-
form POS-tagging. For all experiments, we set the
beam size to 64 for the parser, and report unlabeled
and labeled attachment scores (UAS, LAS) and un-
labeled exact match (UEM) for evaluation.
2http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html
190
feature UAS UEM
baseline 92.18% 45.76%
+distance 92.25% 46.24%
+valency 92.49% 47.65%
+unigrams 92.89% 48.47%
+third-order 93.07% 49.59%
+label set 93.14% 50.12%
Table 3: The effect of new features on the development
set for English. UAS = unlabeled attachment score; UEM
= unlabeled exact match.
UAS UEM LAS
Z&C08 transition 91.4% 41.8% ?
H&S10 91.4% ? ?
this paper baseline 91.4% 42.5% 90.1%
this paper extended 92.9% 48.0% 91.8%
MSTParser 91.5% 42.5% ?
K08 standard 92.0% ? ?
K&C10 model 1 93.0% ? ?
K&C10 model 2 92.9% ? ?
Table 4: Final test accuracies for English. UAS = unla-
beled attachment score; UEM = unlabeled exact match;
LAS = labeled attachment score.
4.1 Development Experiments
Table 3 shows the effect of new features on the de-
velopment test data for English. We start with the
baseline features in Table 1, and incrementally add
the distance, valency, unigram, third-order and label
set feature templates in Table 2. Each group of new
feature templates improved the accuracies over the
previous system, and the final accuracy with all new
features was 93.14% in unlabeled attachment score.
4.2 Final Test Results
Table 4 shows the final test results of our
parser for English. We include in the table
results from the pure transition-based parser of
Zhang and Clark (2008) (row ?Z&C08 transition?),
the dynamic-programming arc-standard parser of
Huang and Sagae (2010) (row ?H&S10?), and graph-
based models including MSTParser (McDonald and
Pereira, 2006), the baseline feature parser of Koo et
al. (2008) (row ?K08 baeline?), and the two models
of Koo and Collins (2010). Our extended parser sig-
nificantly outperformed the baseline parser, achiev-
UAS UEM LAS
Z&C08 transition 84.3% 32.8% ?
H&S10 85.2% 33.7% ?
this paper extended 86.0% 36.9% 84.4%
Table 5: Final test accuracies for Chinese. UAS = unla-
beled attachment score; UEM = unlabeled exact match;
LAS = labeled attachment score.
ing the highest attachment score reported for a
transition-based parser, comparable to those of the
best graph-based parsers.
Our experiments were performed on a Linux plat-
form with a 2GHz CPU. The speed of our baseline
parser was 50 sentences per second. With all new
features added, the speed dropped to 29 sentences
per second.
As an alternative to Penn2Malt, bracketed sen-
tences can also be transformed into Stanford depen-
dencies (De Marneffe et al, 2006). Our parser gave
93.5% UAS, 91.9% LAS and 52.1% UEM when
trained and evaluated on Stanford basic dependen-
cies, which are projective dependency trees. Cer et
al. (2010) report results on Stanford collapsed de-
pendencies, which allow a word to have multiple
heads and therefore cannot be produced by a reg-
ular dependency parser. Their results are relevant
although not directly comparable with ours.
4.3 Chinese Test Results
Table 5 shows the results of our final parser, the pure
transition-based parser of Zhang and Clark (2008),
and the parser of Huang and Sagae (2010) on Chi-
nese. We take the standard split of CTB and use gold
segmentation and POS-tags for the input. Our scores
for this test set are the best reported so far and sig-
nificantly better than the previous systems.
5 Conclusion
We have shown that enriching the feature repre-
sentation significantly improves the accuracy of our
transition-based dependency parser. The effect of
the new features appears to outweigh the effect of
combining transition-based and graph-based mod-
els, reported by Zhang and Clark (2008), as well
as the effect of using dynamic programming, as in-
Huang and Sagae (2010). This shows that feature
definition is a crucial aspect of transition-based pars-
191
ing. In fact, some of the new feature templates in this
paper, such as distance and valency, are among those
which are in the graph-based submodel of Zhang
and Clark (2008), but not the transition-based sub-
model. Therefore our new features to some extent
achieved the same effect as their model combina-
tion. The new features are also hard to use in dy-
namic programming because they add considerable
complexity to the parse items.
Enriched feature representations have been stud-
ied as an important factor for improving the accu-
racies of graph-based dependency parsing also. Re-
cent research including the use of loopy belief net-
work (Smith and Eisner, 2008), integer linear pro-
gramming (Martins et al, 2009) and an improved
dynamic programming algorithm (Koo and Collins,
2010) can be seen as methods to incorporate non-
local features into a graph-based model.
An open source release of our parser, together
with trained models for English and Chinese, are
freely available.3
Acknowledgements
We thank the anonymous reviewers for their useful
comments. Yue Zhang is supported by the Euro-
pean Union Seventh Framework Programme (FP7-
ICT-2009-4) under grant agreement no. 247762.
References
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP/CoNLL, pages
957?961, Prague, Czech Republic.
Daniel Cer, Marie-Catherine de Marneffe, Dan Juraf-
sky, and Chris Manning. 2010. Parsing to stan-
ford dependencies: Trade-offs between speed and ac-
curacy. In Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC?10).
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP, pages 1?8, Philadelphia, USA.
Marie-catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC.
3http://www.sourceforge.net/projects/zpar. version 0.5.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Porceedings of HLT/NAACL, pages
742?750, Los Angeles, California, June.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In Pro-
ceedings of ACL, pages 1077?1086, Uppsala, Sweden,
July.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of EMNLP, pages 1222?1231,
Singapore.
Richard Johansson and Pierre Nugues. 2007. Incre-
mental dependency parsing using online learning. In
Proceedings of CoNLL/EMNLP, pages 1134?1138,
Prague, Czech Republic.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of ACL,
pages 1?11, Uppsala, Sweden, July.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL/HLT, pages 595?603, Columbus,
Ohio, June.
Andre Martins, Noah Smith, and Eric Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proceedings of ACL/IJCNLP,
pages 342?350, Suntec, Singapore, August.
Ryan McDonald and Joakim Nivre. 2007. Characteriz-
ing the errors of data-driven dependency parsing mod-
els. In Proceedings of EMNLP/CoNLL, pages 122?
131, Prague, Czech Republic.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL, pages 81?88, Trento,
Italy, April.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL, pages 91?98, Ann
Arbor, Michigan, June.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006a.
Maltparser: A data-driven parser-generator for depen-
dency parsing. pages 2216?2219.
Joakim Nivre, Johan Hall, Jens Nilsson, Gu?ls?en Eryig?it,
and Svetoslav Marinov. 2006b. Labeled pseudo-
projective dependency parsing with support vector ma-
chines. In Proceedings of CoNLL, pages 221?225,
New York, USA.
Joakim Nivre. 2006. Inductive Dependency Parsing.
Springer.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with LR models and parser
ensembles. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 1044?1050,
192
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
David Smith and Jason Eisner. 2008. Dependency pars-
ing by belief propagation. In Proceedings of EMNLP,
pages 145?156, Honolulu, Hawaii, October.
Ivan Titov and James Henderson. 2007. A latent variable
model for generative dependency parsing. In Proceed-
ings of IWPT, pages 144?155, Prague, Czech Repub-
lic, June.
Xinhao Wang, Xiaojun Lin, Dianhai Yu, Hao Tian, and
Xihong Wu. 2006. Chinese word segmentation with
maximum entropy and n-gram language model. In
Proceedings of SIGHAN Workshop, pages 138?141,
Sydney, Australia, July.
H Yamada and Y Matsumoto. 2003. Statistical depen-
dency analysis using support vector machines. In Pro-
ceedings of IWPT, Nancy, France.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-based
and transition-based dependency parsing using beam-
search. In Proceedings of EMNLP, Hawaii, USA.
Yue Zhang and Stephen Clark. 2009. Transition-based
parsing of the Chinese Treebank using a global dis-
criminative model. In Proceedings of IWPT, Paris,
France, October.
193
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 699?703,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Improving Dependency Parsing with Semantic Classes 
Eneko Agirre*, Kepa Bengoetxea*, Koldo Gojenola*, Joakim Nivre+ 
* Department of Computer Languages and Systems, University of the Basque Country 
UPV/EHU 
+ Department of Linguistics and Philosophy, Uppsala University 
{e.agirre, kepa.bengoetxea, koldo.gojenola}@ehu.es joakim.nivre@lingfil.uu.se 
 
 
 
 
Abstract  
This paper presents the introduction of 
WordNet semantic classes in a dependency 
parser, obtaining improvements on the full 
Penn Treebank for the first time. We tried 
different combinations of some basic se-
mantic classes and word sense disambigua-
tion algorithms. Our experiments show that 
selecting the adequate combination of se-
mantic features on development data is key 
for success. Given the basic nature of the 
semantic classes and word sense disam-
biguation algorithms used, we think there is 
ample room for future improvements. 
1 Introduction 
Using semantic information to improve parsing 
performance has been an interesting research ave-
nue since the early days of NLP, and several re-
search works have tried to test the intuition that 
semantics should help parsing, as can be exempli-
fied by the classical PP attachment experiments 
(Ratnaparkhi, 1994). Although there have been 
some significant results (see Section 2), this issue 
continues to be elusive. In principle, dependency 
parsing offers good prospects for experimenting 
with word-to-word-semantic relationships. 
We present a set of experiments using semantic 
classes in dependency parsing of the Penn Tree-
bank (PTB). We extend the tests made in Agirre et 
al. (2008), who used different types of semantic 
information, obtaining significant improvements in 
two constituency parsers, showing how semantic 
information helps in constituency parsing.  
As our baseline parser, we use MaltParser 
(Nivre, 2006). We will evaluate the parser on both 
the full PTB (Marcus et al 1993) and on a sense-
annotated subset of the Brown Corpus portion of 
PTB, in order to investigate the upper bound per-
formance of the models given gold-standard sense 
information, as in Agirre et al (2008). 
2 Related Work 
Agirre et al (2008) trained two state-of-the-art sta-
tistical parsers (Charniak, 2000; Bikel, 2004) on 
semantically-enriched input, where content words 
had been substituted with their semantic classes. 
This was done trying to overcome the limitations 
of lexicalized approaches to parsing (Magerman, 
1995; Collins, 1996; Charniak, 1997; Collins, 
2003), where related words, like scissors and knife 
cannot be generalized. This simple method allowed 
incorporating lexical semantic information into the 
parser. They tested the parsers in both a full pars-
ing and a PP attachment context. The experiments 
showed that semantic classes gave significant im-
provement relative to the baseline, demonstrating 
that a simplistic approach to incorporating lexical 
semantics into a parser significantly improves its 
performance. This work presented the first results 
over both WordNet and the Penn Treebank to show 
that semantic processing helps parsing.  
Collins (2000) tested a combined parsing/word 
sense disambiguation model based in WordNet 
which did not obtain improvements in parsing. 
Koo et al (2008) presented a semisupervised 
method for training dependency parsers, using 
word clusters derived from a large unannotated 
corpus as features. They demonstrate the effective-
ness of the approach in a series of dependency 
parsing experiments on PTB and the Prague De-
pendency Treebank, showing that the cluster-based 
features yield substantial gains in performance 
across a wide range of conditions. Suzuki et al 
(2009) also experiment with the same method 
combined with semi-supervised learning. 
699
Ciaramita and Attardi (2007) show that adding 
semantic features extracted by a named entity tag-
ger (such as PERSON or MONEY) improves the 
accuracy of a dependency parser, yielding a 5.8% 
relative error reduction on the full PTB. 
Candito and Seddah (2010) performed experi-
ments in statistical parsing of French, where termi-
nal forms were replaced by more general symbols, 
particularly clusters of words obtained through 
unsupervised clustering. The results showed that 
word clusters had a positive effect. 
Regarding dependency parsing of the English 
PTB, currently Koo and Collins (2010) and Zhang 
and Nivre (2011) hold the best results, with 93.0 
and 92.9 unlabeled attachment score, respectively. 
Both works used the Penn2Malt constituency-to-
dependency converter, while we will make use of 
PennConverter (Johansson and Nugues, 2007). 
Apart from these, there have been other attempts 
to make use of semantic information in different 
frameworks and languages, as in (Hektoen 1997; 
Xiong et al 2005; Fujita et al 2007). 
3 Experimental Framework 
In this section we will briefly describe the data-
driven parser used for the experiments (subsection 
3.1), followed by the PTB-based datasets (subsec-
tion 3.2). Finally, we will describe the types of se-
mantic representation used in the experiments. 
3.1 MaltParser 
MaltParser (Nivre et al 2006) is a trainable de-
pendency parser that has been successfully applied 
to typologically different languages and treebanks. 
We will use one of its standard versions (version 
1.4). The parser obtains deterministically a de-
pendency tree in linear-time in a single pass over 
the input using two main data structures: a stack of 
partially analyzed items and the remaining input 
sequence. To determine the best action at each 
step, the parser uses history-based feature models 
and SVM classifiers. One of the main reasons for 
using MaltParser for our experiments is that it eas-
ily allows the introduction of semantic informa-
tion, adding new features, and incorporating them 
in the training model. 
3.2 Dataset 
We used two different datasets: the full PTB and 
the Semcor/PTB intersection (Agirre et al 2008). 
The full PTB allows for comparison with the state-
of-the-art, and we followed the usual train-test 
split. The Semcor/PTB intersection contains both 
gold-standard sense and parse tree annotations, and 
allows to set an upper bound of the relative impact 
of a given semantic representation on parsing. We 
use the same train-test split of Agirre et al (2008), 
with a total of 8,669 sentences containing 151,928 
words partitioned into 3 sets: 80% training, 10% 
development and 10% test data. This dataset is 
available on request to the research community. 
We will evaluate the parser via Labeled Attach-
ment Score (LAS). We will use Bikel?s random-
ized parsing evaluation comparator to test the 
statistical significance of the results using word 
sense information, relative to the respective base-
line parser using only standard features.  
We used PennConverter (Johansson and 
Nugues, 2007) to convert constituent trees in the 
Penn Treebank annotation style into dependency 
trees. Although in general the results from parsing 
Pennconverter?s output are lower than with other 
conversions, Johansson and Nugues (2007) claim 
that this conversion is better suited for semantic 
processing, with a richer structure and a more fine-
grained set of dependency labels. For the experi-
ments, we used the best configuration for English 
at the CoNLL 2007 Shared Task on Dependency 
Parsing (Nivre et al, 2007) as our baseline.  
3.3 Semantic representation and disambigua-
tion methods 
We will experiment with the range of semantic 
representations used in Agirre et al (2008), all of 
which are based on WordNet 2.1. Words in Word-
Net (Fellbaum, 1998) are organized into sets of 
synonyms, called synsets (SS). Each synset in turn 
belongs to a unique semantic file (SF). There are a 
total of 45 SFs (1 for adverbs, 3 for adjectives, 15 
for verbs, and 26 for nouns), based on syntactic 
and semantic categories. For example, noun se-
mantic files (SF_N) differentiate nouns denoting 
acts or actions, and nouns denoting animals, 
among others. We experiment with both full syn-
sets and SFs as instances of fine-grained and 
coarse-grained semantic representation, respec-
tively. As an example of the difference in these 
two representations, knife in its tool sense is in the 
EDGE TOOL USED AS A CUTTING 
INSTRUMENT singleton synset, and also in the 
ARTIFACT SF along with thousands of other 
700
words including cutter. Note that these are the two 
extremes of semantic granularity in WordNet. 
As a hybrid representation, we also tested the ef-
fect of merging words with their corresponding SF 
(e.g. knife+ARTIFACT). This is a form of seman-
tic specialization rather than generalization, and 
allows the parser to discriminate between the dif-
ferent senses of each word, but not generalize 
across words. For each of these three semantic rep-
resentations, we experimented with using each of: 
(1) all open-class POSs (nouns, verbs, adjectives 
and adverbs), (2) nouns only, and (3) verbs only. 
There are thus a total of 9 combinations of repre-
sentation type and target POS: SS (synset), SS_N 
(noun synsets), SS_V (verb synsets), SF (semantic 
file), SF_N (noun semantic files), SF_V (verb se-
mantic files), WSF (wordform+SF), WSF_N 
(wordform+SF for nouns) and WSF_V (for verbs).  
For a given semantic representation, we need 
some form of WSD to determine the semantics of 
each token occurrence of a target word. We ex-
perimented with three options: a) gold-standard 
(GOLD) annotations from SemCor, which gives 
the upper bound performance of the semantic rep-
resentation, b) first Sense (1ST), where all token 
instances of a given word are tagged with their 
most frequent sense in WordNet, and c) automatic 
Sense Ranking (ASR) which uses the sense re-
turned by an unsupervised system based on an in-
dependent corpus (McCarthy et al 2004). For the 
full Penn Treebank experiments, we only had ac-
cess to the first sense, taken from Wordnet 1.7. 
4 Results 
In the following two subsections, we will first pre-
sent the results in the SemCor/PTB intersection, 
with the option of using gold, 1st sense and auto-
matic sense information (subsection 4.1) and the 
next subsection (4.2) will show the results on the 
full PTB, using 1st sense information. All results 
are shown as labelled attachment score (LAS). 
4.1 Semcor/PTB (GOLD/1ST/ASR) 
We conducted a series of experiments testing: 
? Each individual semantic feature, which 
gives 9 possibilities, also testing different 
learning configurations for each one. 
? Combinations of semantic features, for in-
stance, SF+SS_N+WSF would combine the 
semantic file with noun synsets and word-
form+semantic file. 
Although there were hundreds of combinations, 
we took the best combination of semantic features 
on the development set for the final test. For that 
reason, the table only presents 10 results for each 
disambiguation method, 9 for the individual fea-
tures and one for the best combination. 
Table 1 presents the results obtained for each of 
the disambiguation methods (gold standard sense 
information, 1st sense, and automatic sense rank-
ing) and individual semantic feature. In all cases 
except two, the use of semantic classes is benefi-
 System            LAS 
Baseline  81.10  
SS 81.18 +0.08 
SS_N 81.40 +0.30 
SS_V *81.58 +0.48 
SF **82.05 +0.95 
SF_N
 81.51 +0.41 
SF_V 81.51 +0.41 
WSF 81.51 +0.41 
WSF_N 81.43 +0.33 
WSF_V *81.51 +0.41 
 
 
Gold 
SF+SF_N+SF_V+SS+WSF_N *81.74 +0.64 
SS 81.30 +0.20 
SS_N *81.56 +0.46 
SS_V *81.49 +0.39 
SF 81.00 -0.10 
SF_N
 80.97 -0.13 
SF_V **81.66 +0.56 
WSF 81.32 +0.22 
WSF_N *81.62 +0.52 
WSF_V **81.72 +0.62 
 
 
ASR 
SF_V+SS_V 81.41 +0.31 
SS 81.40 +0.30 
SS_N 81.39 +0.29 
SS_V *81.48 +0.38 
SF *81.59 +0.49 
SF_N
 81.38 +0.28 
SF_V *81.52 +0.42 
WSF *81.57 +0.46 
WSF_N 81.40 +0.30 
WSF_V 81.42 +0.32 
 
 
1ST 
SF+SS_V+WSF_N **81.92 +0.81 
Table 1. Evaluation results on the test set for the 
Semcor-Penn intersection. Individual semantic 
features and best combination. 
(**: statistically significant, p < 0.005; *: p < 0.05) 
 
701
cial albeit small. Regarding individual features, the 
SF feature using GOLD senses gives the best im-
provement. However, GOLD does not seem to 
clearly improve over 1ST and ASR on the rest of 
the features. Comparing the automatically obtained 
classes, 1ST and ASR, there is no evident clue 
about one of them being superior to the other. 
Regarding the best combination as selected in 
the training data, each WSD method yields a dif-
ferent combination, with best results for 1ST. The 
improvement is statistically significant for both 
1ST and GOLD. In general, the results in Table 1 
do not show any winning feature across all WSD 
algorithms. The best results are obtained when us-
ing the first sense heuristic, but the difference is 
not statistically significant. This shows that perfect 
WSD is not needed to obtain improvements, but it 
also shows that we reached the upperbound of our 
generalization and learning method. 
4.2 Penn Treebank and 1st sense 
We only had 1st sense information available for 
the full PTB. We tested MaltParser on the best 
configuration obtained for the reduced Sem-
cor/PTB on the full treebank, taking sections 2-21 
for training and section 23 for the final test. Table 
2 presents the results, showing that several of the 
individual features and the best combination give 
significant improvements. To our knowledge, this 
is the first time that WordNet semantic classes help 
to obtain improvements on the full Penn Treebank. 
It is interesting to mention that, although not 
shown on the tables, using lemmatization to assign 
semantic classes to wordforms gave a slight in-
crease for all the tests (0.1 absolute point approxi-
mately), as it helped to avoid data sparseness. We 
applied Schmid?s (1994) TreeTagger. This can be 
seen as an argument in favour of performing mor-
phological analysis, an aspect that is many times 
neglected when processing morphologically poor 
languages as English. 
We also did some preliminary experiments us-
ing Koo et al?s (2008) word clusters, both inde-
pendently and also combined with the WordNet-
based features, without noticeable improvements. 
5 Conclusions 
We tested the inclusion of several types of seman-
tic information, in the form of WordNet semantic 
classes in a dependency parser, showing that: 
? Semantic information gives an improvement 
on a transition-based deterministic depend-
ency parsing. 
? Feature combinations give an improvement 
over using a single feature. Agirre et al 
(2008) used a simple method of substituting 
wordforms with semantic information, 
which only allowed using a single semantic 
feature. MaltParser allows the combination 
of several semantic features together with 
other features such as wordform, lemma or 
part of speech. Although tables 1 and 2 only 
show the best combination for each type of 
semantic information, this can be appreci-
ated on GOLD and 1ST in Table 1. Due to 
space reasons, we only have showed the best 
combination, but we can say that in general 
combining features gives significant in-
creases over using a single semantic feature. 
? The present work presents a statistically sig-
nificant improvement for the full treebank 
using WordNet-based semantic information 
for the first time. Our results extend those of 
Agirre et al (2008), which showed im-
provements on a subset of the PTB. 
Given the basic nature of the semantic classes 
and WSD algorithms, we think there is room for 
future improvements, incorporating new kinds of 
semantic information, such as WordNet base con-
cepts, Wikipedia concepts, or similarity measures. 
 
 System            LAS 
Baseline  86.27  
SS *86.53 +0.26 
SS_N 86.33 +0.06 
SS_V *86.48 +0.21 
SF **86.63 +0.36 
SF_N
 *86.56 +0.29 
SF_V 86.34 +0.07 
WSF *86.50 +0.23 
WSF_N 86.25 -0.02 
WSF_V *86.51 +0.24 
 
 
1ST 
SF+SS_V+WSF_N *86.60 +0.33 
 
Table 1. Evaluation results (LAS) on the test 
set for the full PTB. Individual features and 
best combination. 
(**: statistically, p < 0.005; *: p < 0.05) 
 
702
References  
Eneko Agirre, Timothy Baldwin, and David Martinez. 
2008. Improving parsing and PP attachment perform-
ance with sense information. In Proceedings of ACL-
08: HLT, pages 317?325, Columbus, Ohio. 
Daniel M. Bikel. 2004. Intricacies of Collins? parsing 
model. Computational Linguistics, 30(4):479?511. 
Candito, M. and D. Seddah. 2010. Parsing word clus-
ters. In Proceedings of the NAACL HLT 2010 First 
Workshop on Statistical Parsing of Morphologically-
Rich Language, Los Angeles, USA. 
M. Ciaramita and G. Attardi. 2007. Dependency Parsing 
with Second-Order Feature Maps and Annotated Se-
mantic Information, In Proceedings of the 10th In-
ternational Conference on Parsing Technology.  
Eugene Charniak. 1997. Statistical parsing with a con-
text-free grammar and word statistics. In Proc. of the 
15th Annual Conference on Artificial Intelligence 
(AAAI-97), pages 598?603, Stanford, USA. 
Eugene Charniak. 2000. A maximum entropy-based 
parser. In Proc. of the 1st Annual Meeting of the 
North American Chapter of Association for Compu-
tational Linguistics (NAACL2000), Seattle, USA. 
Michael J. Collins. 1996. A new statistical parser based 
on lexical dependencies. In Proc. of the 34th Annual 
Meeting of the ACL, pages 184?91, USA. 
Michael Collins. 2000. A Statistical Model for Parsing 
and Word-Sense Disambiguation. Proceedings of the 
Conference on Empirical Methods in Natural Lan-
guage Processing and Very Large Corpora. 
Michael Collins. 2003. Head-driven statistical models 
for natural language parsing. Computational Linguis-
tics, 29(4):589?637. 
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge. 
Sanae Fujita, Francis Bond, Stephan Oepen, and Taka-
aki Tanaka. 2007. Exploiting semantic information 
for HPSG parse selection. In Proc. of the ACL 2007 
Workshop on Deep Linguistic Processing. 
Richard Johansson and Pierre Nugues. 2007. Extended 
Constituent-to-dependency Conversion for English. 
In Proceedings of NODALIDA 2007, Tartu, Estonia. 
Erik Hektoen. 1997. Probabilistic parse selection based 
on semantic cooccurrences. In Proc. of the 5th Inter-
national Workshop on Parsing Technologies. 
Terry Koo, Xavier Carreras, and Michael Collins. 2008. 
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL-08, pages 595?603, USA. 
Terry Koo, and Michael Collins. 2008. Efficient Third-
order Dependency Parsers. In Proceedings of ACL-
2010, pages 1?11, Uppsala, Sweden. 
Shari Landes, Claudia Leacock, and Randee I. Tengi. 
1998. Building semantic concordances. In Christiane 
Fellbaum, editor, WordNet: An Electronic Lexical 
Database. MIT Press, Cambridge, USA. 
David M. Magerman. 1995. Statistical decision-tree 
models for parsing. In Proc. of the 33rd Annual 
Meeting of the ACL, pages 276?83, USA. 
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann 
Marcinkiewicz. 1993. Building a large annotated 
corpus of English: the Penn treebank. Computational 
Linguistics, 19(2):313?30. 
Diana McCarthy, Rob Koeling, Julie Weeds, and John 
Carroll. 2004. Finding predominant senses in 
untagged text. In Proc. of the 42nd Annual Meeting 
of the ACL, pages 280?7, Barcelona, Spain. 
Joakim Nivre. 2006. Inductive Dependency Parsing. 
Text, Speech and Language Technology series, 
Springer. 2006, XI, ISBN: 978-1-4020-4888-3. 
Joakim Nivre, Johan Hall, Sandra K?bler, Ryan 
McDonald, Jens Nilsson,  Sebastian Riedel and 
Deniz Yuret. 2007b. The CoNLL 2007 Shared Task 
on Dependency Parsing. Proceedings of EMNLP-
CoNLL. Prague, Czech Republic. 
Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos. 
1994. A maximum entropy model for prepositional 
phrase attachment. In HLT ?94: Proceedings of the 
Workshop on Human Language Technology, USA. 
Helmut Schmid. 1994. Probabilistic Part-of-Speech 
Tagging Using Decision Trees. In Proceedings of In-
ternational Conference on New Methods in Lan-
guage Processing. September 1994 
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Mi-
chael Collins. 2009. An Empirical Study of Semi-
supervised Structured Conditional Models for De-
pendency Parsing. In Proceedings of EMNLP, pages 
551?560. Association for Computational Linguistics. 
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, and 
Yueliang Qian. 2005. Parsing the Penn Chinese 
Treebank with semantic knowledge. In Proc. of the 
2nd International Joint Conference on Natural Lan-
guage Processing (IJCNLP-05), Korea. 
Yue Zhang, and Joakim Nivre. 2011. Transition-Based 
Parsing with Rich Non-Local Features. In Proceed-
ings of the 49th Annual Meeting of the Association 
for Computational Linguistics. 
703
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 6?10,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Joint Evaluation of Morphological Segmentation and Syntactic Parsing
Reut Tsarfaty Joakim Nivre Evelina Andersson
Box 635, 751 26, Uppsala University, Uppsala, Sweden
tsarfaty@stp.lingfil.uu.se, {joakim.nivre, evelina.andersson}@lingfil.uu.se
Abstract
We present novel metrics for parse evalua-
tion in joint segmentation and parsing sce-
narios where the gold sequence of terminals
is not known in advance. The protocol uses
distance-based metrics defined for the space
of trees over lattices. Our metrics allow us
to precisely quantify the performance gap be-
tween non-realistic parsing scenarios (assum-
ing gold segmented and tagged input) and re-
alistic ones (not assuming gold segmentation
and tags). Our evaluation of segmentation and
parsing for Modern Hebrew sheds new light
on the performance of the best parsing systems
to date in the different scenarios.
1 Introduction
A parser takes a sentence in natural language as in-
put and returns a syntactic parse tree representing
the sentence?s human-perceived interpretation. Cur-
rent state-of-the-art parsers assume that the space-
delimited words in the input are the basic units of
syntactic analysis. Standard evaluation procedures
and metrics (Black et al, 1991; Buchholz and Marsi,
2006) accordingly assume that the yield of the parse
tree is known in advance. This assumption breaks
down when parsing morphologically rich languages
(Tsarfaty et al, 2010), where every space-delimited
word may be effectively composed of multiple mor-
phemes, each of which having a distinct role in the
syntactic parse tree. In order to parse such input the
text needs to undergo morphological segmentation,
that is, identifying the morphological segments of
each word and assigning the corresponding part-of-
speech (PoS) tags to them.
Morphologically complex words may be highly
ambiguous and in order to segment them correctly
their analysis has to be disambiguated. The multiple
morphological analyses of input words may be rep-
resented via a lattice that encodes the different seg-
mentation possibilities of the entire word sequence.
One can either select a segmentation path prior to
parsing, or, as has been recently argued, one can let
the parser pick a segmentation jointly with decoding
(Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg
and Tsarfaty, 2008; Green and Manning, 2010). If
the selected segmentation is different from the gold
segmentation, the gold and parse trees are rendered
incomparable and standard evaluation metrics break
down. Evaluation scenarios restricted to gold input
are often used to bypass this problem, but, as shall be
seen shortly, they present an overly optimistic upper-
bound on parser performance.
This paper presents a full treatment of evaluation
in different parsing scenarios, using distance-based
measures defined for trees over a shared common
denominator defined in terms of a lattice structure.
We demonstrate the informativeness of our metrics
by evaluating joint segmentation and parsing perfor-
mance for the Semitic language Modern Hebrew, us-
ing the best performing systems, both constituency-
based and dependency-based (Tsarfaty, 2010; Gold-
berg, 2011a). Our experiments demonstrate that, for
all parsers, significant performance gaps between re-
alistic and non-realistic scenarios crucially depend
on the kind of information initially provided to the
parser. The tool and metrics that we provide are
completely general and can straightforwardly apply
to other languages, treebanks and different tasks.
6
(tree1) TOP
PP
IN
0B1
?in?
NP
NP
DEF
1H2
?the?
NP
NN
2CL3
?shadow?
PP
POSS
3FL4
of
PRN
4HM5
?them?
ADJP
DEF
5H6
?the?
JJ
6NEIM7
?pleasant?
(tree2) TOP
PP
IN
0B1
?in?
NP
NP
NN
1CL2
?shadow?
PP
POSS
2FL3
?of?
PRN
3HM4
?them?
VB
4HNEIM5
?made-pleasant?
Figure 1: A correct tree (tree1) and an incorrect tree (tree2) for ?BCLM HNEIM?, indexed by terminal boundaries.
Erroneous nodes in the parse hypothesis are marked in italics. Missing nodes from the hypothesis are marked in bold.
2 The Challenge: Evaluation for MRLs
In morphologically rich languages (MRLs) substan-
tial information about the grammatical relations be-
tween entities is expressed at word level using in-
flectional affixes. In particular, in MRLs such as He-
brew, Arabic, Turkish or Maltese, elements such as
determiners, definite articles and conjunction mark-
ers appear as affixes that are appended to an open-
class word. Take, for example the Hebrew word-
token BCLM,1 which means ?in their shadow?. This
word corresponds to five distinctly tagged elements:
B (?in?/IN), H (?the?/DEF), CL (?shadow?/NN), FL
(?of?/POSS), HM (?they?/PRN). Note that morpho-
logical segmentation is not the inverse of concatena-
tion. For instance, the overt definite article H and
the possessor FL show up only in the analysis.
The correct parse for the Hebrew phrase ?BCLM
HNEIM? is shown in Figure 1 (tree1), and it pre-
supposes that these segments can be identified and
assigned the correct PoS tags. However, morpholog-
ical segmentation is non-trivial due to massive word-
level ambiguity. The word BCLM, for instance, can
be segmented into the noun BCL (?onion?) and M (a
genitive suffix, ?of them?), or into the prefix B (?in?)
followed by the noun CLM (?image?).2 The multi-
tude of morphological analyses may be encoded in a
lattice structure, as illustrated in Figure 2.
1We use the Hebrew transliteration in Sima?an et al (2001).
2The complete set of analyses for this word is provided in
Goldberg and Tsarfaty (2008). Examples for similar phenom-
ena in Arabic may be found in Green and Manning (2010).
Figure 2: The morphological segmentation possibilities
of BCLM HNEIM. Double-circles are word boundaries.
In practice, a statistical component is required to
decide on the correct morphological segmentation,
that is, to pick out the correct path through the lat-
tice. This may be done based on linear local context
(Adler and Elhadad, 2006; Shacham and Wintner,
2007; Bar-haim et al, 2008; Habash and Rambow,
2005), or jointly with parsing (Tsarfaty, 2006; Gold-
berg and Tsarfaty, 2008; Green and Manning, 2010).
Either way, an incorrect morphological segmenta-
tion hypothesis introduces errors into the parse hy-
pothesis, ultimately providing a parse tree which
spans a different yield than the gold terminals. In
such cases, existing evaluation metrics break down.
To understand why, consider the trees in Figure 1.
Metrics like PARSEVAL (Black et al, 1991) cal-
culate the harmonic means of precision and recall
on labeled spans ?i, label, j? where i, j are termi-
nal boundaries. Now, the NP dominating ?shadow
of them? has been identified and labeled correctly
in tree2, but in tree1 it spans ?2,NP, 5? and in tree2
it spans ?1,NP, 4?. This node will then be counted
as an error for tree2, along with its dominated and
dominating structure, and PARSEVAL will score 0.
7
A generalized version of PARSEVAL which con-
siders i, j character-based indices instead of termi-
nal boundaries (Tsarfaty, 2006) will fail here too,
since the missing overt definite article H will cause
similar misalignments. Metrics for dependency-
based evaluation such as ATTACHMENT SCORES
(Buchholz and Marsi, 2006) suffer from similar
problems, since they assume that both trees have the
same nodes ? an assumption that breaks down in
the case of incorrect morphological segmentation.
Although great advances have been made in pars-
ing MRLs in recent years, this evaluation challenge
remained unsolved.3 In this paper we present a solu-
tion to this challenge by extending TEDEVAL (Tsar-
faty et al, 2011) for handling trees over lattices.
3 The Proposal: Distance-Based Metrics
Input and Output Spaces We view the joint task
as a structured prediction function h : X ? Y from
input space X onto output space Y . Each element
x ? X is a sequence x = w1, . . . , wn of space-
delimited words from a setW . We assume a lexicon
LEX, distinct fromW , containing pairs of segments
drawn from a set T of terminals and PoS categories
drawn from a set N of nonterminals.
LEX = {?s, p?|s ? T , p ? N}
Each word wi in the input may admit multiple
morphological analyses, constrained by a language-
specific morphological analyzer MA. The morpho-
logical analysis of an input word MA(wi) can be
represented as a lattice Li in which every arc cor-
responds to a lexicon entry ?s, p?. The morpholog-
ical analysis of an input sentence x is then a lattice
L obtained through the concatenation of the lattices
L1, . . . , Ln where MA(w1) = L1, . . . , MA(wn) =
Ln. Now, let x = w1, . . . , wn be a sentence with
a morphological analysis lattice MA(x) = L. We
define the output space YMA(x)=L for h (abbreviated
YL), as the set of linearly-ordered labeled trees such
that the yield of LEX entries ?s1, p1?,. . . ,?sk, pk? in
each tree (where si ? T and pi ? N , and possibly
k 6= n) corresponds to a path through the lattice L.
3A tool that could potentially apply here is SParseval (Roark
et al, 2006). But since it does not respect word-boundaries, it
fails to apply to such lattices. Cohen and Smith (2007) aimed to
fix this, but in their implementation syntactic nodes internal to
word boundaries may be lost without scoring.
Edit Scripts and Edit Costs We assume a
set A={ADD(c, i, j),DEL(c, i, j),ADD(?s, p?, i, j),
DEL(?s, p?, i, j)} of edit operations which can add
or delete a labeled node c ? N or an entry ?s, p? ?
LEX which spans the states i, j in the lattice L. The
operations in A are properly constrained by the lat-
tice, that is, we can only add and delete lexemes that
belong to LEX, and we can only add and delete them
where they can occur in the lattice. We assume a
function C(a) = 1 assigning a unit cost to every op-
eration a ? A, and define the cost of a sequence
?a1, . . . , am? as the sum of the costs of all opera-
tions in the sequence C(?a1, ..., am?) =
?m
i=1 C(ai).
An edit script ES(y1, y2) = ?a1, . . . , am? is a se-
quence of operations that turns y1 into y2. The tree-
edit distance is the minimum cost of any edit script
that turns y1 into y2 (Bille, 2005).
TED(y1, y2) = min
ES(y1,y2)
C(ES(y1, y2))
Distance-Based Metrics The error of a predicted
structure p with respect to a gold structure g is now
taken to be the TED cost, and we can turn it into a
score by normalizing it and subtracting from a unity:
TEDEVAL(p, g) = 1?
TED(p, g)
|p|+ |g| ? 2
The term |p| + |g| ? 2 is a normalization factor de-
fined in terms of the worst-case scenario, in which
the parser has only made incorrect decisions. We
would need to delete all lexemes and nodes in p and
add all the lexemes and nodes of g, except for roots.
An Example Both trees in Figure 1 are contained
in YL for the lattice L in Figure 2. If we re-
place terminal boundaries with lattice indices from
Figure 2, we need 6 edit operations to turn tree2
into tree1 (deleting the nodes in italic, adding the
nodes in bold) and the evaluation score will be
TEDEVAL(tree2,tree1) = 1? 614+10?2 = 0.7273.
4 Experiments
We aim to evaluate state-of-the-art parsing architec-
tures on the morphosyntactic disambiguation of He-
brew texts in three different parsing scenarios: (i)
Gold: assuming gold segmentation and PoS-tags,
(ii) Predicted: assuming only gold segmentation,
and (iii) Raw: assuming unanalyzed input text.
8
SEGEVAL PARSEVAL TEDEVAL
Gold PS U: 100.00 U: 94.35
L: 100.00 L: 88.75 L: 93.39
Predicted PS U: 100.00 U: 92.92
L: 90.85 L: 82.30 L: 86:26
Raw PS U: 96.42 U: 88.47
L: 84.54 N/A L: 80.67
Gold RR U: 100.00 U: 94.34
L: 100.00 L: 83.93 L: 92.45
Predicted RR U: 100.00 U: 92.82
L: 91.69 L: 78.93 L: 85.83
Raw RR U: 96.03 U: 87.96
L: 86.10 N/A L: 79.46
Table 1: Phrase-Structure based results for the Berke-
ley Parser trained on bare-bone trees (PS) and relational-
realizational trees (RR). We parse all sentences in the dev
set. RR extra decoration is removed prior to evaluation.
SEGEVAL ATTSCORES TEDEVAL
Gold MP 100.00 U: 83.59 U: 91.76
Predicted MP 100.00 U: 82.00 U: 91.20
Raw MP 95.07 N/A U: 87.03
Gold EF 100.00 U: 84.68 U: 92.25
Predicted EF 100.00 U: 83.97 U: 92:02
Raw EF 95.07 N/A U: 87.75
Table 2: Dependency parsing results by MaltParser (MP)
and EasyFirst (EF), trained on the treebank converted into
unlabeled dependencies, and parsing the entire dev-set.
For constituency-based parsing we use two mod-
els trained by the Berkeley parser (Petrov et al,
2006) one on phrase-structure (PS) trees and one
on relational-realizational (RR) trees (Tsarfaty and
Sima?an, 2008). In the raw scenario we let a lattice-
based parser choose its own segmentation and tags
(Goldberg, 2011b). For dependency parsing we use
MaltParser (Nivre et al, 2007b) optimized for He-
brew by Ballesteros and Nivre (2012), and the Easy-
First parser of Goldberg and Elhadad (2010) with the
features therein. Since these parsers cannot choose
their own tags, automatically predicted segments
and tags are provided by Adler and Elhadad (2006).
We use the standard split of the Hebrew tree-
bank (Sima?an et al, 2001) and its conversion into
unlabeled dependencies (Goldberg, 2011a). We
use PARSEVAL for evaluating phrase-structure trees,
ATTACHSCORES for evaluating dependency trees,
and TEDEVAL for evaluating all trees in all scenar-
ios. We implement SEGEVAL for evaluating seg-
mentation based on our TEDEVAL implementation,
replacing the tree distance and size with string terms.
Table 1 shows the constituency-based parsing re-
sults for all scenarios. All of our results confirm
that gold information leads to much higher scores.
TEDEVAL allows us to precisely quantify the drop
in accuracy from gold to predicted (as in PARSE-
VAL) and than from predicted to raw on a single
scale. TEDEVAL further allows us to scrutinize the
contribution of different sorts of information. Unla-
beled TEDEVAL shows a greater drop when moving
from predicted to raw than from gold to predicted,
and for labeled TEDEVAL it is the other way round.
This demonstrates the great importance of gold tags
which provide morphologically disambiguated in-
formation for identifying phrase content.
Table 2 shows that dependency parsing results
confirm the same trends, but we see a much smaller
drop when moving from gold to predicted. This is
due to the fact that we train the parsers for predicted
on a treebank containing predicted tags. There is
however a great drop when moving from predicted
to raw, which confirms that evaluation benchmarks
on gold input as in Nivre et al (2007a) do not pro-
vide a realistic indication of parser performance.
For all tables, TEDEVAL results are on a simi-
lar scale. However, results are not yet comparable
across parsers. RR trees are flatter than bare-bone
PS trees. PS and DEP trees have different label
sets. Cross-framework evaluation may be conducted
by combining this metric with the cross-framework
protocol of Tsarfaty et al (2012).
5 Conclusion
We presented distance-based metrics defined for
trees over lattices and applied them to evaluating
parsers on joint morphological and syntactic dis-
ambiguation. Our contribution is both technical,
providing an evaluation tool that can be straight-
forwardly applied for parsing scenarios involving
trees over lattices,4 and methodological, suggesting
to evaluate parsers in all possible scenarios in order
to get a realistic indication of parser performance.
Acknowledgements
We thank Shay Cohen, Yoav Goldberg and Spence
Green for discussion of this challenge. This work
was supported by the Swedish Science Council.
4The tool can be downloaded http://stp.ling.uu.
se/?tsarfaty/unipar/index.html
9
References
Meni Adler and Michael Elhadad. 2006. An unsuper-
vised morpheme-based HMM for Hebrew morpholog-
ical disambiguation. In Proceedings of COLING-ACL.
Miguel Ballesteros and Joakim Nivre. 2012. MaltOpti-
mizer: A system for MaltParser optimization. Istan-
bul.
Roy Bar-haim, Khalil Sima?an, and Yoad Winter. 2008.
Part-of-speech tagging of Modern Hebrew text. Natu-
ral Language Engineering, 14(2):223?251.
Philip Bille. 2005. A survey on tree-edit distance
and related. problems. Theoretical Computer Science,
337:217?239.
Ezra Black, Steven P. Abney, D. Flickenger, Claudia
Gdaniec, Ralph Grishman, P. Harrison, Donald Hin-
dle, Robert Ingria, Frederick Jelinek, Judith L. Kla-
vans, Mark Liberman, Mitchell P. Marcus, Salim
Roukos, Beatrice Santorini, and Tomek Strzalkowski.
1991. A procedure for quantitatively comparing the
syntactic coverage of English grammars. In Proceed-
ings of the DARPA Workshop on Speech and Natural
Language.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL-X, pages 149?164.
Shay B. Cohen and Noah A. Smith. 2007. Joint morpho-
logical and syntactic disambiguation. In Proceedings
of EMNLP-CoNLL, pages 208?217.
Yoav Goldberg and Michael Elhadad. 2010. Easy-first
dependency parsing of Modern Hebrew. In Proceed-
ings of NAACL/HLT workshop on Statistical Parsing
of Morphologically Rich Languages.
Yoav Goldberg and Reut Tsarfaty. 2008. A single frame-
work for joint morphological segmentation and syn-
tactic parsing. In Proceedings of ACL.
Yoav Goldberg. 2011a. Automatic Syntactic Processing
of Modern Hebrew. Ph.D. thesis, Ben-Gurion Univer-
sity of the Negev.
Yoav Goldberg. 2011b. Joint morphological segmen-
tation and syntactic parsing using a PCFGLA lattice
parser. In Proceedings of ACL.
Spence Green and Christopher D. Manning. 2010. Better
Arabic parsing: Baselines, evaluations, and analysis.
In Proceedings of COLING.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proceedings of
ACL.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007a. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 915?932.
Joakim Nivre, Jens Nilsson, Johan Hall, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007b. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(1):1?41.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of ACL.
Brian Roark, Mary Harper, Eugene Charniak, Bon-
nie Dorr C, Mark Johnson D, Jeremy G. Kahn
E, Yang Liu F, Mari Ostendorf E, John Hale
H, Anna Krasnyanskaya I, Matthew Lease D,
Izhak Shafran J, Matthew Snover C, Robin Stewart K,
and Lisa Yung J. 2006. Sparseval: Evaluation metrics
for parsing speech. In Proceesings of LREC.
Danny Shacham and Shuly Wintner. 2007. Morpholog-
ical disambiguation of Hebrew: A case study in clas-
sifier combination. In Proceedings of the 2007 Joint
Conference of EMNLP-CoNLL, pages pages 439?447.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altman,
and Noa Nativ. 2001. Building a Tree-Bank for
Modern Hebrew Text. In Traitement Automatique des
Langues.
Reut Tsarfaty and Khalil Sima?an. 2008. Relational-
Realizational parsing. In Proceedings of CoLing.
Reut Tsarfaty, Djame Seddah, Yoav Goldberg, San-
dra Kuebler, Marie Candito, Jennifer Foster, Yan-
nick Versley, Ines Rehbein, and Lamia Tounsi. 2010.
Statistical parsing for morphologically rich language
(SPMRL): What, how and whither. In Proceedings of
the first workshop on Statistical Parsing of Morpho-
logically Rich Languages (SPMRL) at NA-ACL.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2011. Evaluating dependency parsing: Robust and
heuristics-free cross-framework evaluation. In Pro-
ceedings of EMNLP.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012. Cross-framework evaluation for statistical pars-
ing. In Proceedings of EACL.
Reut Tsarfaty. 2006. Integrated morphological and syn-
tactic disambiguation for Modern Hebrew. In Pro-
ceeding of ACL-SRW.
Reut Tsarfaty. 2010. Relational-Realizational Parsing.
Ph.D. thesis, University of Amsterdam.
10
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 135?144,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Transition-Based Dependency Parser
Using a Dynamic Parsing Strategy
Francesco Sartorio
Department of
Information Engineering
University of Padua, Italy
sartorio@dei.unipd.it
Giorgio Satta
Department of
Information Engineering
University of Padua, Italy
satta@dei.unipd.it
Joakim Nivre
Department of
Linguistics and Philology
Uppsala University, Sweden
joakim.nivre@lingfil.uu.se
Abstract
We present a novel transition-based, greedy
dependency parser which implements a
flexible mix of bottom-up and top-down
strategies. The new strategy allows the
parser to postpone difficult decisions until
the relevant information becomes available.
The novel parser has a ?12% error reduc-
tion in unlabeled attachment score over an
arc-eager parser, with a slow-down factor
of 2.8.
1 Introduction
Dependency-based methods for syntactic parsing
have become increasingly popular during the last
decade or so. This development is probably due
to many factors, such as the increased availability
of dependency treebanks and the perceived use-
fulness of dependency structures as an interface
to downstream applications, but a very important
reason is also the high efficiency offered by de-
pendency parsers, enabling web-scale parsing with
high throughput. The most efficient parsers are
greedy transition-based parsers, which only explore
a single derivation for each input and relies on
a locally trained classifier for predicting the next
parser action given a compact representation of the
derivation history, as pioneered by Yamada and
Matsumoto (2003), Nivre (2003), Attardi (2006),
and others. However, while these parsers are cap-
able of processing tens of thousands of tokens per
second with the right choice of classifiers, they are
also known to perform slightly below the state-of-
the-art because of search errors and subsequent
error propagation (McDonald and Nivre, 2007),
and recent research on transition-based depend-
ency parsing has therefore explored different ways
of improving their accuracy.
The most common approach is to use beam
search instead of greedy decoding, in combination
with a globally trained model that tries to minim-
ize the loss over the entire sentence instead of a
locally trained classifier that tries to maximize the
accuracy of single decisions (given no previous er-
rors), as first proposed by Zhang and Clark (2008).
With these methods, transition-based parsers have
reached state-of-the-art accuracy for a number of
languages (Zhang and Nivre, 2011; Bohnet and
Nivre, 2012). However, the drawback with this ap-
proach is that parsing speed is proportional to the
size of the beam, which means that the most accur-
ate transition-based parsers are not nearly as fast
as the original greedy transition-based parsers. An-
other line of research tries to retain the efficiency of
greedy classifier-based parsing by instead improv-
ing the way in which classifiers are learned from
data. While the classical approach limits training
data to parser states that result from oracle predic-
tions (derived from a treebank), these novel ap-
proaches allow the classifier to explore states that
result from its own (sometimes erroneous) predic-
tions (Choi and Palmer, 2011; Goldberg and Nivre,
2012).
In this paper, we explore an orthogonal approach
to improving the accuracy of transition-based pars-
ers, without sacrificing their advantage in efficiency,
by introducing a new type of transition system.
While all previous transition systems assume a
static parsing strategy with respect to top-down
and bottom-up processing, our new system allows
a dynamic strategy for ordering parsing decisions.
This has the advantage that the parser can postpone
difficult decisions until the relevant information be-
comes available, in a way that is not possible in
existing transition systems. A second advantage of
dynamic parsing is that we can extend the feature
inventory of previous systems. Our experiments
show that these advantages lead to significant im-
provements in parsing accuracy, compared to a
baseline parser that uses the arc-eager transition
system of Nivre (2003), which is one of the most
135
widely used static transition systems.
2 Static vs. Dynamic Parsing
The notions of bottom-up and top-down parsing
strategies do not have a general mathematical defin-
ition; they are instead specified, often only inform-
ally, for individual families of grammar formal-
isms. In the context of dependency parsing, a pars-
ing strategy is called purely bottom-up if every
dependency h ? d is constructed only after all
dependencies of the form d ? i have been con-
structed. Here h? d denotes a dependency with
h the head node and d the dependent node. In con-
trast, a parsing strategy is called purely top-down
if h? d is constructed before any dependency of
the form d? i.
If we consider transition-based dependency pars-
ing (Nivre, 2008), the purely bottom-up strategy is
implemented by the arc-standard model of Nivre
(2004). After building a dependency h ? d, this
model immediately removes from its stack node d,
preventing further attachment of dependents to this
node. A second popular parser, the arc-eager model
of Nivre (2003), instead adopts a mixed strategy.
In this model, a dependency h? d is constructed
using a purely bottom-up strategy if it represents a
left-arc, that is, if the dependent d is placed to the
left of the head h in the input string. In contrast, if
h ? d represents a right-arc (defined symmetric-
ally), then this dependency is constructed before
any right-arc d ? i (top-down) but after any left-
arc d? i (bottom-up).
What is important to notice about the above
transition-based parsers is that the adopted pars-
ing strategies are static. By this we mean that each
dependency is constructed according to some fixed
criterion, depending on structural conditions such
as the fact that the dependency represents a left or a
right arc. This should be contrasted with dynamic
parsing strategies in which several parsing options
are simultaneously available for the dependencies
being constructed.
In the context of left-to-right, transition-based
parsers, dynamic strategies are attractive for sev-
eral reasons. One argument is related to the well-
known PP-attachment problem, illustrated in Fig-
ure 1. Here we have to choose whether to attach
node P as a dependent of V (arc ?2) or else as
a dependent of N1 (arc ?3). The purely bottom-
up arc-standard model has to take a decision as
soon as N1 is placed into the stack. This is so
V N1 P N2
?1
?2
?3 ?4
Figure 1: PP-attachment example, with dashed arcs
identifying two alternative choices.
because the construction of ?1 excludes ?3 from
the search space, while the alternative decision of
shifting P into the stack excludes ?2. This is bad,
because the information about the correct attach-
ment could come from the lexical content of node P.
The arc-eager model performs slightly better, since
it can delay the decision up to the point in which ?1
has been constructed and P is read from the buffer.
However, at this point it must make a commitment
and either construct ?3 or pop N1 from the stack
(implicitly committing to ?2) before N2 is read
from the buffer. In contrast with this scenario, in
the next sections we implement a dynamic parsing
strategy that allows a transition system to decide
between the attachments ?2 and ?3 after it has seen
all of the four nodes V, N1, P and N2.
Other additional advantages of dynamic parsing
strategies with respect to static strategies are re-
lated to the increase in the feature inventory that
we apply to parser states, and to the increase of
spurious ambiguity. However, these arguments are
more technical than the PP-attachment argument
above, and will be discussed later.
3 Dependency Parser
In this section we present a novel transition-based
parser for projective dependency trees, implement-
ing a dynamic parsing strategy.
3.1 Preliminaries
For non-negative integers i and j with i ? j, we
write [i, j] to denote the set {i, i+1, . . . , j}. When
i > j, [i, j] is the empty set.
We represent an input sentence as a string w =
w0 ? ? ?wn, n ? 1, where token w0 is a special
root symbol and, for each i ? [1, n], token wi =
(i, ai, ti) encodes a lexical element ai and a part-of-
speech tag ti associated with the i-th word in the
sentence.
A dependency tree for w is a directed, ordered
tree Tw = (Vw, Aw), where Vw = {wi | i ?
136
w4
w2 w5 w7
w1 w3 w6
Figure 2: A dependency tree with left spine
?w4, w2, w1? and right spine ?w4, w7?.
[0, n]} is the set of nodes, and Aw ? Vw ? Vw is
the set of arcs. Arc (wi, wj) encodes a dependency
wi ? wj . A sample dependency tree (excluding
w0) is displayed in Figure 2. If (wi, wj) ? Aw for
j < i, we say that wj is a left child of wi; a right
child is defined in a symmetrical way.
The left spine of Tw is an ordered sequence
?u1, . . . , up? with p ? 1 and ui ? Vw for i ? [1, p],
consisting of all nodes in a descending path from
the root of Tw taking the leftmost child node at
each step. More formally, u1 is the root node of Tw
and ui is the leftmost child of ui?1, for i ? [2, p].
The right spine of Tw is defined symmetrically;
see again Figure 2. Note that the left and the right
spines share the root node and no other node.
3.2 Basic Idea
Transition-based dependency parsers use a stack
data structure, where each stack element is associ-
ated with a tree spanning some (contiguous) sub-
string of the input w. The parser can combine
two trees T and T ? through attachment operations,
called left-arc or right-arc, under the condition that
T and T ? appear at the two topmost positions in
the stack. Crucially, only the roots of T and T ? are
available for attachment; see Figure 3(a).
In contrast, a stack element in our parser records
the entire left spine and right spine of the associated
tree. This allows us to extend the inventory of the
attachment operations of the parser by including
the attachment of tree T as a dependent of any node
in the left or in the right spine of a second tree T ?,
provided that this does not violate projectivity.1
See Figure 3(b) for an example.
The new parser implements a mix of bottom-up
and top-down strategies, since after any of the at-
tachments in Figure 3(b) is performed, additional
dependencies can still be created for the root of T .
Furthermore, the new parsing strategy is clearly dy-
1A dependency tree for w is projective if every subtree has
a contiguous yield in w.
T
T ?
T
T ?
(a) (b)
Figure 3: Left-arc attachment of T to T ? in case
of (a) standard transition-based parsers and (b) our
parser.
namic, due to the free choice in the timing for these
attachments. The new strategy is more powerful
than the strategy of the arc-eager model, since we
can use top-down parsing at left arcs, which is not
allowed in arc-eager parsing, and we do not have
the restrictions of parsing right arcs (h? d) before
the attachment of right dependents at node d.
To conclude this section, let us resume our dis-
cussion of the PP-attachment example in Figure 1.
We observe that the new parsing strategy allows the
construction of a tree T ? consisting of the only de-
pendency V? N1 and a tree T , placed at the right
of T ?, consisting of the only dependency P? N2.
Since the right spine of T ? consists of nodes V
and N1, we can freely choose between attachment
V? P and attachment N1? P. Note that this is
done after we have seen node N2, as desired.
3.3 Transition-based Parser
We assume the reader is familiar with the formal
framework of transition-based dependency parsing
originally introduced by Nivre (2003); see Nivre
(2008) for an introduction. To keep the notation at
a simple level, we only discuss here the unlabeled
version of our parser; however, a labeled extension
is used in ?5 for our experiments.
Our transition-based parser uses a stack data
structure to store partial parses for the input string
w. We represent the stack as an ordered sequence
? = [?d, . . . , ?1], d ? 0, of stack elements, with
the topmost element placed at the right. When d =
0, we have the empty stack ? = []. Sometimes we
use the vertical bar to denote the append operator
for ?, and write ? = ??|?1 to indicate that ?1 is the
topmost element of ?.
A stack element is a pair
?k = (?uk,1, . . . , uk,p?, ?vk,1, . . . , vk,q?)
where the ordered sequences ?uk,1, . . . , uk,p? and
137
?vk,1, . . . , vk,q? are the left and the right spines, re-
spectively, of the tree associated with ?k. Recall
that uk,1 = vk,1, since the root node of the associ-
ated tree is shared by the two spines.
The parser also uses a buffer to store the por-
tion of the input string still to be processed. We
represent the buffer as an ordered sequence ? =
[wi, . . . , wn], i ? 0, of tokens from w, with the
first element placed at the left. Note that ? always
represents a (non-necessarily proper) suffix of w.
When i > n, we have the empty buffer ? = [].
Sometimes we use the vertical bar to denote the
append operator for ?, and write ? = wi|?? to in-
dicate that wi is the first token of ?; consequently,
we have ?? = [wi+1, . . . , wn].
When processing w, the parser reaches several
states, technically called configurations. A con-
figuration of the parser relative to w is a triple
c = (?, ?,A), where ? and ? are a stack and
a buffer, respectively, and A ? Vw ? Vw is a
set of arcs. The initial configuration for w is
([], [w0, . . . , wn], ?). The set of terminal config-
urations consists of all configurations of the form
([?1], [], A), where ?1 is associated with a tree hav-
ing root w0, that is, u1,1 = v1,1 = w0, and A is any
set of arcs.
The core of a transition-based parser is the set
of its transitions. Each transition is a binary rela-
tion defined over the set of configurations of the
parser. Since the set of configurations is infinite,
a transition is infinite as well, when viewed as a
set. However, transitions can always be specified
by some finite means. Our parser uses three types
of transitions, defined in what follows.
? SHIFT, or sh for short. This transition re-
moves the first node from the buffer and
pushes into the stack a new element, consist-
ing of the left and right spines of the associ-
ated tree. More formally
(?,wi|?,A) `sh (?|(?wi?, ?wi?), ?, A)
? LEFT-ARCk, k ? 1, or lak for short. Let h
be the k-th node in the left spine of the top-
most tree in the stack, and let d be the root
node of the second topmost tree in the stack.
This transition creates a new arc h? d. Fur-
thermore, the two topmost stack elements are
replaced by a new element associated with the
tree resulting from the h? d attachment. The
transition does not advance with the reading
of the buffer. More formally
(??|?2|?1, ?, A) `lak (??|?la, ?, A ? {h? d})
where
?1 = (?u1,1, . . . , u1,p?, ?v1,1, . . . , v1,q?) ,
?2 = (?u2,1, . . . , u2,r?, ?v2,1, . . . , v2,s?) ,
?la = (?u1,1, . . . , u1,k, u2,1, . . . , u2,r?,
?v1,1, . . . , v1,q?) ,
and where we have set h = u1,k and d = u2,1.
? RIGHT-ARCk, k ? 1, or rak for short. This
transition is defined symmetrically with re-
spect to lak. We have
(??|?2|?1, ?, A) `rak (??|?ra, ?, A ? {h? d})
where ?1 and ?2 are as in the lak case,
?ra = (?u2,1, . . . , u2,r?,
?v2,1, . . . , v2,k, v1,1, . . . , v1,q?) ,
and we have set h = v2,k and d = v1,1.
Transitions lak and rak are parametric in k,
where k is bounded by the length of the input string
and not by a fixed constant (but see also the experi-
mental findings in ?5). Thus our system uses an un-
bounded number of transition relations, which has
an apparent disadvantage for learning algorithms.
We will get back to this problem in ?4.3.
A complete computation relative to w is a se-
quence of configurations c1, c2, . . . , ct, t ? 1, such
that c1 and ct are initial and final configurations,
respectively, and for each i ? [2, t], ci is produced
by the application of some transition to ci?1. It is
not difficult to see that the transition-based parser
specified above is sound, meaning that the set of
arcs constructed in any complete computation on
w is always a dependency tree for w. The parser
is also complete, meaning that every (projective)
dependency tree for w is constructed by some com-
plete computation on w. A mathematical proof of
this statement is beyond the scope of this paper,
and will not be provided here.
3.4 Deterministic Parsing Algorithm
The transition-based parser of the previous sec-
tion is a nondeterministic device, since several
transitions can be applied to a given configuration.
This might result in several complete computations
138
Algorithm 1 Parsing Algorithm
Input: string w = w0 ? ? ?wn, function score()
Output: dependency tree Tw
c = (?, ?,A)? ([], [w0, . . . , wn], ?)
while |?| > 1 ? |?| > 0 do
while |?| < 2 do
update c with sh
p? length of left spine of ?1
s? length of right spine of ?2
T ? {lak | k ? [1, p]} ?
{rak | k ? [1, s]} ? {sh}
bestT ? argmaxt?T score(t , c)
update c with bestT
return Tw = (Vw, A)
for w. We present here an algorithm that runs
the parser in pseudo-deterministic mode, greed-
ily choosing at each configuration the transition
that maximizes some score function. Algorithm 1
takes as input a string w and a scoring function
score() defined over parser transitions and parser
configurations. The scoring function will be the
subject of ?4 and is not discussed here. The output
of the parser is a dependency tree for w.
At each iteration the algorithm checks whether
there are at least two elements in the stack and, if
this is not the case, it shifts elements from the buffer
to the stack. Then the algorithm uses the function
score() to evaluate all transitions that can be ap-
plied under the current configuration c = (?, ?,A),
and it applies the transition with the highest score,
updating the current configuration.
To parse a sentence of length n (excluding the
root token w0) the algorithm applies exactly 2n+1
transitions. In the worst case, each transition ap-
plication involves 1 + p+ s transition evaluations.
We therefore conclude that the algorithm always
reaches a configuration with an empty buffer and a
stack which contains only one element. Then the al-
gorithm stops, returning the dependency tree whose
arc set is defined as in the current configuration.
4 Model and Training
In this section we introduce the adopted learning
algorithm and discuss the model parameters.
4.1 Learning Algorithm
We use a linear model for the score function in
Algorithm 1, and define score(t , c) = ~? ? ?(t , c).
Here ~? is a weight vector and function ? provides
Algorithm 2 Learning Algorithm
Input: pair (w = w0 ? ? ?wn, Ag), vector ~?
Output: vector ~?
c = (?, ?,A)? ([], [w0, . . . , wn], ?)
while |?| > 1 ? |?| > 0 do
while |?| < 2 do
update c with SHIFT
p? length of left spine of ?1
s? length of right spine of ?2
T ? {lak | k ? [1, p]} ?
{rak | k ? [1, s]} ? {sh}
bestT ? argmaxt?T score(t , c)
bestCorrectT ?
argmaxt?T ?isCorrect(t) score(t , c)
if bestT 6= bestCorrectT then
~? ? ~? ? ?(bestT , c)
+?(bestCorrectT , c)
update c with bestCorrectT
a feature vector representation for a transition t ap-
plying to a configuration c. The function ? will be
discussed at length in ?4.3. The vector ~? is trained
using the perceptron algorithm in combination with
the averaging method to avoid overfitting; see Fre-
und and Schapire (1999) and Collins and Duffy
(2002) for details.
The training data set consists of pairs (w,Ag),
where w is a sentence and Ag is the set of arcs
of the gold (desired) dependency tree for w. At
training time, each pair (w,Ag) is processed using
the learning algorithm described as Algorithm 2.
The algorithm is based on the notions of correct and
incorrect transitions, discussed at length in ?4.2.
Algorithm 2 parsesw following Algorithm 1 and
using the current ~?, until the highest score selec-
ted transition bestT is incorrect according to Ag .
When this happens, ~? is updated by decreasing the
weights of the features associated with the incorrect
bestT and by increasing the weights of the features
associated with the transition bestCorrectT having
the highest score among all possible correct trans-
itions. After each update, the learning algorithm
resumes parsing from the current configuration by
applying bestCorrectT , and moves on using the
updated weights.
4.2 Correct and Incorrect Transitions
Standard transition-based dependency parsers are
trained by associating each gold tree with a canon-
ical complete computation. This means that, for
each configuration of interest, only one transition
139
?2 ?1 b1
(a)
?2 ?1 b1
(b)
?2 ?1
? ? ?
bi
(c)
?2 ?1
? ? ?
bi
(d)
Figure 4: Graphical representation of configura-
tions; drawn arcs are in Ag but have not yet been
added to the configuration. Transition sh is incor-
rect for configuration (a) and (b); sh and ra1 are
correct for (c); sh and la1 are correct for (d).
leading to the gold tree is considered as correct. In
this paper we depart from such a methodology, and
follow Goldberg and Nivre (2012) in allowing more
than one correct transition for each configuration,
as explained in detail below.
Let (w,Ag) be a pair in the training set. In ?3.3
we have mentioned that there is always a complete
computation on w that results in the construction
of the set Ag . In general, there might be more than
one computation forAg . This means that the parser
shows spurious ambiguity.
Observe that all complete computations for Ag
share the same initial configuration cI,w and final
configuration cF,Ag . Consider now the set C(w) of
all configurations c that are reachable from cI,w,
meaning that there exists a sequence of transitions
that takes the parser from cI,w to c. A configuration
c ? C(w) is correct for Ag if cF,Ag is reachable
from c; otherwise, c is incorrect for Ag .
Let c ? C(w) be a correct configuration for Ag .
A transition t is correct for c and Ag if c `t c?
and c? is correct for Ag ; otherwise, t is incorrect
for c and Ag . The next lemma provides a charac-
terization of correct and incorrect transitions; see
Figure 4 for examples. We use this characterization
in the implementation of predicate isCorrect() in
Algorithm 2.
Lemma 1 Let (w,Ag) be a pair in the training set
and let c ? C(w) with c = (?, ?,A) be a correct
configuration for Ag . Let alo v1,k, k ? [1, q], be
the nodes in the right spine of ?1.
(i) lak and rak are incorrect for c and Ag if and
only if they create a new arc (h? d) 6? Ag ;
(ii) sh is incorrect for c and Ag if and only if the
following conditions are both satisfied:
(a) there exists an arc (h ? d) in Ag such
that h is in ? and d = v1,1;
(b) there is no arc (h? ? d?) in Ag with
h? = v1,k, k ? [1, q], and d? in ?. 2
PROOF (SKETCH) To prove part (i) we focus on
transition rak; a similar argument applies to lak.
The ?if? statement in part (i) is self-evident.
?Only if?. Assuming that transition rak creates
a new arc (h? d) ? Ag , we argue that from con-
figuration c? with c `rak c? we can still reach the
final configuration associated with Ag . We have
h = v2,k and d = u1,1. The tree fragments in ?
with roots v2,k+1 and u1,1 must be adjacent siblings
in the tree associated with Ag , since c is a correct
configuration for Ag and (v2,k ? u1,1) ? Ag .
This means that each of the nodes v2,k+1, . . . , v2,s
in the right spine in ?2 in c must have already ac-
quired all of its right dependents, since the tree is
projective. Therefore it is safe for transition rak to
eliminate the nodes v2,k+1, . . . , v2,s from the right
spine in ?2.
We now deal with part (ii). Let c `sh c?, c? =
(??, ??, A).
?If?. Assuming (ii)a and (ii)b, we argue that c? is
incorrect. Node d is the head of ??2. Arc (h? d) is
not inA, and the only way we could create (h? d)
from c? is by reaching a new configuration with d
in the topmost stack symbol, which amounts to say
that ??1 can be reduced by a correct transition. Node
h is in some ??i, i > 2, by (ii)a. Then reduction of
??1 implies that the root of ??1 is reachable from the
root of ??2, which contradicts (ii)b.
?Only if?. Assuming (ii)a is not satisfied, we
argue that sh is correct for c and Ag . There must
be an arc (h? d) not in A with d = v1,1 and h is
some token wi in ?. From stack ?? = ???|??2|??1 it
is always possible to construct (h? d) consuming
the substring of ? up to wi and ending up with
stack ???|?red , where ?red is a stack element with
root wi. From there, the parser can move on to
the final configuration cF,Ag . A similar argument
applies if we assume that (ii)b is not satisfied. 
From condition (i) in Lemma 1 and from the fact
that there are no cycles in Ag , it follows that there
is at most one correct transition among the trans-
itions of type lak or rak. From condition (ii) in the
lemma we can also see that the existence of a cor-
rect transition of type lak or rak for some configura-
tion does not imply that the sh transition is incorrect
140
for the same configuration; see Figures 4(c,d) for
examples. It follows that for a correct configuration
there might be at most 2 correct transitions. In our
training experiments for English in ?5 we observe 2
correct transitions for 42% of the reached configur-
ations. This nondeterminism is a byproduct of the
adopted dynamic parsing strategy, and eventually
leads to the spurious ambiguity of the parser.
As already mentioned, we do not impose any ca-
nonical form on complete computations that would
hardwire a preference for some correct transition
and get rid of spurious ambiguity. Following Gold-
berg and Nivre (2012), we instead regard spurious
ambiguity as an additional resource of our pars-
ing strategy. Our main goal is that the training
algorithm learns to prefer a sh transition in a con-
figuration that does not provide enough information
for the choice of the correct arc. In the context of
dependency parsing, the strategy of delaying arc
construction when the current configuration is not
informative is called the easy-first strategy, and
has been first explored by Goldberg and Elhadad
(2010).
4.3 Feature Extraction
In existing transition-based parsers a set of atomic
features is statically defined and extracted from
each configuration. These features are then com-
bined together into complex features, according to
some feature template, and joined with the avail-
able transition types. This is not possible in our
system, since the number of transitions lak and rak
is not bounded by a constant. Furthermore, it is not
meaningful to associate transitions lak and rak, for
any k ? 1, always with the same features, since
the constructed arcs impinge on nodes at differ-
ent depths in the involved spines. It seems indeed
more significant to extract information that is local
to the arc h? d being constructed by each trans-
ition, such as for instance the grandparent and the
great grandparent nodes of d. This is possible if
we introduce a higher level of abstraction than in
existing transition-based parsers. We remark here
that this abstraction also makes the feature repres-
entation more similar to the ones typically found
in graph-based parsers, which are centered on arcs
or subgraphs of the dependency tree.
We index the nodes in the stack ? relative to
the head node of the arc being constructed, in
case of the transitions lak or rak, or else relative
to the root node of ?1, in case of the transition
sh. More precisely, let c = (?, ?,A) be a con-
figuration and let t be a transition. We define
the context of c and t as the tuple C(c, t) =
(s3, s2, s1, q1, q2, gp, gg), whose components are
placeholders for word tokens in ? or in ?. All these
placeholders are specified in Table 1, for each c and
t . Figure 5 shows an example of feature extraction
for the displayed configuration c = (?, ?,A) and
the transition la2. In this case we have s3 = u3,1,
s2 = u2,1, s1 = u1,2, q1 = gp = u1,1, q2 = b1;
gg = none because the head of gp is not available
in c.
Note that in Table 1 placeholders are dynamic-
ally assigned in such a way that s1 and s2 refer to
the nodes in the constructed arc h? d, and gp, gg
refer to the grandparent and the great grandparent
nodes, respectively, of d. Furthermore, the node
assigned to s3 is the parent node of s2, if such a
node is defined; otherwise, the node assigned to
s3 is the root of the tree fragment in the stack un-
derneath ?2. Symmetrically, placeholders q1 and
q2 refer to the parent and grandparent nodes of s1,
respectively, when these nodes are defined; other-
wise, these placeholders get assigned tokens from
the buffer. See again Figure 5.
Finally, from the placeholders in C(c, t) we ex-
tract a standard set of atomic features and their
complex combinations, to define the function ?.
Our feature template is an extended version of the
feature template of Zhang and Nivre (2011), ori-
ginally developed for the arc-eager model. The
extension is obtained by adding top-down features
for left-arcs (based on placeholders gp and gg),
and by adding right child features for the first stack
element. The latter group of features is usually ex-
ploited for the arc-standard model, but is undefined
for the arc-eager model.
5 Experimental Assessment
Performance evaluation is carried out on the Penn
Treebank (Marcus et al, 1993) converted to Stan-
ford basic dependencies (De Marneffe et al, 2006).
We use sections 2-21 for training, 22 as develop-
ment set, and 23 as test set. The part-of-speech
tags are assigned by an automatic tagger with ac-
curacy 97.1%. The tagger used on the training set
is trained on the same data set by using four-way
jackknifing, while the tagger used on the develop-
ment and test sets is trained on all the training set.
We train an arc-labeled version of our parser.
In the first three lines of Table 2 we compare
141
context sh lak rak
placeholder k = 1 k = 2 k > 2 k = 1 k = 2 k > 2
s1 u1,1 = v1,1 u1,k u1,1 = v1,1
s2 u2,1 = v2,1 u2,1 = v2,1 v2,k
s3 u3,1 = v3,1 u3,1 = v3,1 u3,1 = v3,1 v2,k?1
q1 b1 b1 u1,k?1 b1
q2 b2 b2 b1 u1,k?2 b2
gp none none u1,k?1 none v2,k?1
gg none none none u1,k?2 none none v2,k?2
Table 1: Definition ofC(c, t) = (s3, s2, s1, q1, q2, gp, gg), for c = (??|?3|?2|?1, b1|b2|?,A) and t of type
sh or lak, rak, k ? 1. Symbols uj,k and vj,k are the k-th nodes in the left and right spines, respectively, of
stack element ?j , with uj,1 = vj,1 being the shared root of ?j ; none is an artificial element used when
some context?s placeholder is not available.
? ? ?
stack ?
u3,1 = v3,1
v3,2
u2,1 = v2,1
u2,2 v2,2
v2,3
u1,1 = v1,1
u1,2 v1,2
u1,3 v1,3
la2
buffer ?
b1 b2 b3 ? ? ?
context extracted for la2
s3 s2 s1 q1=gp q2
Figure 5: Extraction of atomic features for context C(c, la2) = (s3, s2, s1, q1, q2, gp, gg), c = (?, ?,A).
parser iter UAS LAS UEM
arc-standard 23 90.02 87.69 38.33
arc-eager 12 90.18 87.83 40.02
this work 30 91.33 89.16 42.38
arc-standard + easy-first 21 90.49 88.22 39.61
arc-standard + spine 27 90.44 88.23 40.27
Table 2: Accuracy on test set, excluding punc-
tuation, for unlabeled attachment score (UAS),
labeled attachment score (LAS), unlabeled exact
match (UEM).
the accuracy of our parser against our implementa-
tion of the arc-eager and arc-standard parsers. For
the arc-eager parser, we use the feature template
of Zhang and Nivre (2011). The same template is
adapted to the arc-standard parser, by removing the
top-down parent features and by adding the right
child features for the first stack element. It turns out
that our feature template, described in ?4.3, is the
exact merge of the templates used for the arc-eager
and the arc-standard parsers.
We train all parsers up to 30 iterations, and for
each parser we select the weight vector ~? from the
iteration with the best accuracy on the development
set. All our parsers attach the root node at the end
of the parsing process, following the ?None? ap-
proach discussed by Ballesteros and Nivre (2013).
Punctuation is excluded in all evaluation metrics.
Considering UAS, our parser provides an improve-
ment of 1.15 over the arc-eager parser and an im-
provement of 1.31 over the arc-standard parser, that
is an error reduction of ?12% and ?13%, respect-
ively. Considering LAS, we achieve improvements
of 1.33 and 1.47, with an error reduction of ?11%
and ?12%, over the arc-eager and the arc-standard
parsers, respectively.
We speculate that the observed improvement of
our parser can be ascribed to two distinct com-
ponents. The first component is the left-/right-
spine representation for stack elements, introduced
in ?3.3. The second component is the easy-first
strategy, implemented on the basis of the spurious
ambiguity of our parser and the definition of cor-
rect/incorrect transitions in ?4.2. In this perspective,
we observe that our parser can indeed be viewed as
an arc-standard model augmented with (i) the spine
representation, and (ii) the easy-first strategy. More
specifically, (i) generalizes the la/ra transitions to
the lak/rak transitions, introducing a top-down com-
ponent into the purely bottom-up arc-standard. On
the other hand, (ii) drops the limitation of canonical
computations for the arc-standard, and leverages
142
on the spurious ambiguity of the parser to enlarge
the search space.
The two components above are mutually inde-
pendent, meaning that we can individually imple-
ment each component on top of an arc-standard
model. More precisely, the arc-standard + spine
model uses the transitions lak/rak but retains the
definition of canonical computation, defined by ap-
plying each lak/rak transition as soon as possible.
On the other hand, the arc-standard + easy-first
model retains the original la/ra transitions but is
trained allowing any correct transition at each con-
figuration. In this case the characterization of cor-
rect and incorrect configurations in Lemma 1 has
been adapted to transitions la/ra, taking into ac-
count the bottom-up constraint.
With the purpose of incremental comparison, we
report accuracy results for the two ?incremental?
models in the last two lines of Table 2. Analyzing
these results, and comparing with the plain arc-
standard, we see that the spine representation and
the easy-first strategy individually improve accur-
acy. Moreover, their combination into our model
(third line of Table 2) works very well, with an
overall improvement larger than the sum of the
individual contributions.
We now turn to a computational analysis. At
each iteration our parser evaluates a number of
transitions bounded by ?+1, with ? the maximum
value of the sum of the lengths of the left spine in ?1
and of the right spine in ?2. Quantity ? is bounded
by the length n of the input sentence. Since the
parser applies exactly 2n + 1 transitions, worst
case running time is O(n2). We have computed
the average value of ? on our English data set,
resulting in 2.98 (variance 2.15) for training set,
and 2.95 (variance 1.96) for development set. We
conclude that, in the expected case, running time is
O(n), with a slow down constant which is rather
small, in comparison to standard transition-based
parsers. Accordingly, when running our parser
against our implementation of the arc-eager and
arc-standard models, we measured a slow-down of
2.8 and 2.2, respectively. Besides the change in
representation, this slow-down is also due to the
increase in the number of features in our system.
We have also checked the worst case value of ? in
our data set. Interestingly, we have seen that for
strings of length smaller than 40 this value linearly
grows with n, and for longer strings the growth
stops, with a maximum worst case observed value
of 22.
6 Concluding Remarks
We have presented a novel transition-based parser
using a dynamic parsing strategy, which achieves
a ?12% error reduction in unlabeled attachment
score over the static arc-eager strategy and even
more over the (equally static) arc-standard strategy,
when evaluated on English.
The idea of representing the right spine of a
tree within the stack elements of a shift-reduce
device is quite old in parsing, predating empirical
approaches. It has been mainly exploited to solve
the PP-attachment problem, motivated by psycho-
linguistic models. The same representation is also
adopted in applications of discourse parsing, where
right spines are usually called right frontiers; see
for instance Subba and Di Eugenio (2009). In
the context of transition-based dependency parsers,
right spines have also been exploited by Kitagawa
and Tanaka-Ishii (2010) to decide where to attach
the next word from the buffer. In this paper we
have generalized their approach by introducing the
symmetrical notion of left spine, and by allowing
attachment of full trees rather than attachment of a
single word.2
Since one can regard a spine as a stack in it-
self, whose elements are tree nodes, our model is
reminiscent of the embedded pushdown automata
of Schabes and Vijay-Shanker (1990), used to parse
tree adjoining grammars (Joshi and Schabes, 1997)
and exploiting a stack of stacks. However, by im-
posing projectivity, we do not use the extra-power
of the latter class.
An interesting line of future research is to com-
bine our dynamic parsing strategy with a training
method that allows the parser to explore transitions
that apply to incorrect configurations, as in Gold-
berg and Nivre (2012).
Acknowledgments
We wish to thank Liang Huang and Marco Kuhl-
mann for discussion related to the ideas reported in
this paper, and the anonymous reviewers for their
useful suggestions. The second author has been
partially supported by MIUR under project PRIN
No. 2010LYA9RH 006.
2Accuracy comparison of our work with Kitagawa and
Tanaka-Ishii (2010) is not meaningful, since these authors
have evaluated their system on the same data set but based on
gold part-of-speech tags (personal communication).
143
References
Giuseppe Attardi. 2006. Experiments with a multil-
anguage non-projective dependency parser. In Pro-
ceedings of the 10th Conference on Computational
Natural Language Learning (CoNLL), pages 166?
170.
Miguel Ballesteros and Joakim Nivre. 2013. Going
to the roots of dependency parsing. Computational
Linguistics, 39(1):5?13.
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and
labeled non-projective dependency parsing. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1455?
1465.
Jinho D. Choi and Martha Palmer. 2011. Getting the
most out of transition-based dependency parsing. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
687?692.
Michael Collins and Nigel Duffy. 2002. New ranking
algorithms for parsing and tagging: Kernels over dis-
crete structures, and the voted perceptron. In Pro-
ceedings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
263?270, Philadephia, Pennsylvania.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference
on Language Resources and Evaluation (LREC),
volume 6, pages 449?454.
Yoav Freund and Robert E. Schapire. 1999. Large
margin classification using the perceptron algorithm.
Machine Learning, 37(3):277?296, December.
Yoav Goldberg and Michael Elhadad. 2010. An ef-
ficient algorithm for easy-first non-directional de-
pendency parsing. In Proceedings of Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (NAACL), pages 742?
750, Los Angeles, USA.
Yoav Goldberg and Joakim Nivre. 2012. A dynamic or-
acle for arc-eager dependency parsing. In Proceed-
ings of the 24th International Conference on Com-
putational Linguistics (COLING), pages 959?976.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
Adjoining Grammars. In Grzegorz Rozenberg and
Arto Salomaa, editors, Handbook of Formal Lan-
guages, volume 3, pages 69?123. Springer.
Kotaro Kitagawa and Kumiko Tanaka-Ishii. 2010.
Tree-based deterministic dependency parsing ? an
application to Nivre?s method ?. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics (ACL) Short Papers, pages
189?193.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19:313?330.
Ryan McDonald and Joakim Nivre. 2007. Character-
izing the errors of data-driven dependency parsing
models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 122?131.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
Eighth International Workshop on Parsing Techno-
logies (IWPT), pages 149?160, Nancy, France.
Joakim Nivre. 2004. Incrementality in deterministic
dependency parsing. In Workshop on Incremental
Parsing: Bringing Engineering and Cognition To-
gether, pages 50?57, Barcelona, Spain.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513?553.
Yves Schabes and K. Vijay-Shanker. 1990. Determ-
inistic left to right parsing of tree adjoining lan-
guages. In Proceedings of the 28th annual meet-
ing of the Association for Computational Linguistics
(ACL), pages 276?283, Pittsburgh, Pennsylvania.
Rajen Subba and Barbara Di Eugenio. 2009. An effect-
ive discourse parser that uses rich linguistic inform-
ation. In Proceedings of Human Language Techno-
logies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 566?574.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Stat-
istical dependency analysis with support vector ma-
chines. In Proceedings of the 8th International
Workshop on Parsing Technologies (IWPT), pages
195?206.
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 562?
571.
Yue Zhang and Joakim Nivre. 2011. Transition-based
parsing with rich non-local features. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 188?193.
144
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 92?97,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Universal Dependency Annotation for Multilingual Parsing
Ryan McDonald? Joakim Nivre?? Yvonne Quirmbach-Brundage? Yoav Goldberg??
Dipanjan Das? Kuzman Ganchev? Keith Hall? Slav Petrov? Hao Zhang?
Oscar Ta?ckstro?m?? Claudia Bedini? Nu?ria Bertomeu Castello?? Jungmee Lee?
Google, Inc.? Uppsala University? Appen-Butler-Hill? Bar-Ilan University?
Contact: ryanmcd@google.com
Abstract
We present a new collection of treebanks
with homogeneous syntactic dependency
annotation for six languages: German,
English, Swedish, Spanish, French and
Korean. To show the usefulness of such a
resource, we present a case study of cross-
lingual transfer parsing with more reliable
evaluation than has been possible before.
This ?universal? treebank is made freely
available in order to facilitate research on
multilingual dependency parsing.1
1 Introduction
In recent years, syntactic representations based
on head-modifier dependency relations between
words have attracted a lot of interest (Ku?bler et
al., 2009). Research in dependency parsing ? com-
putational methods to predict such representations
? has increased dramatically, due in large part to
the availability of dependency treebanks in a num-
ber of languages. In particular, the CoNLL shared
tasks on dependency parsing have provided over
twenty data sets in a standardized format (Buch-
holz and Marsi, 2006; Nivre et al, 2007).
While these data sets are standardized in terms
of their formal representation, they are still hetero-
geneous treebanks. That is to say, despite them
all being dependency treebanks, which annotate
each sentence with a dependency tree, they sub-
scribe to different annotation schemes. This can
include superficial differences, such as the renam-
ing of common relations, as well as true diver-
gences concerning the analysis of linguistic con-
structions. Common divergences are found in the
1Downloadable at https://code.google.com/p/uni-dep-tb/.
analysis of coordination, verb groups, subordinate
clauses, and multi-word expressions (Nilsson et
al., 2007; Ku?bler et al, 2009; Zeman et al, 2012).
These data sets can be sufficient if one?s goal
is to build monolingual parsers and evaluate their
quality without reference to other languages, as
in the original CoNLL shared tasks, but there are
many cases where heterogenous treebanks are less
than adequate. First, a homogeneous represen-
tation is critical for multilingual language tech-
nologies that require consistent cross-lingual anal-
ysis for downstream components. Second, consis-
tent syntactic representations are desirable in the
evaluation of unsupervised (Klein and Manning,
2004) or cross-lingual syntactic parsers (Hwa et
al., 2005). In the cross-lingual study of McDonald
et al (2011), where delexicalized parsing models
from a number of source languages were evalu-
ated on a set of target languages, it was observed
that the best target language was frequently not the
closest typologically to the source. In one stun-
ning example, Danish was the worst source lan-
guage when parsing Swedish, solely due to greatly
divergent annotation schemes.
In order to overcome these difficulties, some
cross-lingual studies have resorted to heuristics to
homogenize treebanks (Hwa et al, 2005; Smith
and Eisner, 2009; Ganchev et al, 2009), but we
are only aware of a few systematic attempts to
create homogenous syntactic dependency anno-
tation in multiple languages. In terms of auto-
matic construction, Zeman et al (2012) attempt
to harmonize a large number of dependency tree-
banks by mapping their annotation to a version of
the Prague Dependency Treebank scheme (Hajic?
et al, 2001; Bo?hmova? et al, 2003). Addition-
ally, there have been efforts to manually or semi-
manually construct resources with common syn-
92
tactic analyses across multiple languages using al-
ternate syntactic theories as the basis for the repre-
sentation (Butt et al, 2002; Helmreich et al, 2004;
Hovy et al, 2006; Erjavec, 2012).
In order to facilitate research on multilingual
syntactic analysis, we present a collection of data
sets with uniformly analyzed sentences for six lan-
guages: German, English, French, Korean, Span-
ish and Swedish. This resource is freely avail-
able and we plan to extend it to include more data
and languages. In the context of part-of-speech
tagging, universal representations, such as that of
Petrov et al (2012), have already spurred numer-
ous examples of improved empirical cross-lingual
systems (Zhang et al, 2012; Gelling et al, 2012;
Ta?ckstro?m et al, 2013). We aim to do the same for
syntactic dependencies and present cross-lingual
parsing experiments to highlight some of the bene-
fits of cross-lingually consistent annotation. First,
results largely conform to our expectations of
which target languages should be useful for which
source languages, unlike in the study of McDon-
ald et al (2011). Second, the evaluation scores
in general are significantly higher than previous
cross-lingual studies, suggesting that most of these
studies underestimate true accuracy. Finally, un-
like all previous cross-lingual studies, we can re-
port full labeled accuracies and not just unlabeled
structural accuracies.
2 Towards A Universal Treebank
The Stanford typed dependencies for English
(De Marneffe et al, 2006; de Marneffe and Man-
ning, 2008) serve as the point of departure for our
?universal? dependency representation, together
with the tag set of Petrov et al (2012) as the under-
lying part-of-speech representation. The Stanford
scheme, partly inspired by the LFG framework,
has emerged as a de facto standard for depen-
dency annotation in English and has recently been
adapted to several languages representing different
(and typologically diverse) language groups, such
as Chinese (Sino-Tibetan) (Chang et al, 2009),
Finnish (Finno-Ugric) (Haverinen et al, 2010),
Persian (Indo-Iranian) (Seraji et al, 2012), and
Modern Hebrew (Semitic) (Tsarfaty, 2013). Its
widespread use and proven adaptability makes it a
natural choice for our endeavor, even though ad-
ditional modifications will be needed to capture
the full variety of grammatical structures in the
world?s languages.
Alexandre re?side avec sa famille a` Tinqueux .
NOUN VERB ADP DET NOUN ADP NOUN P
NSUBJ ADPMOD
ADPOBJ
POSS
ADPMOD
ADPOBJ
P
Figure 1: A sample French sentence.
We use the so-called basic dependencies (with
punctuation included), where every dependency
structure is a tree spanning all the input tokens,
because this is the kind of representation that most
available dependency parsers require. A sample
dependency tree from the French data set is shown
in Figure 1. We take two approaches to generat-
ing data. The first is traditional manual annotation,
as previously used by Helmreich et al (2004) for
multilingual syntactic treebank construction. The
second, used only for English and Swedish, is to
automatically convert existing treebanks, as in Ze-
man et al (2012).
2.1 Automatic Conversion
Since the Stanford dependencies for English are
taken as the starting point for our universal annota-
tion scheme, we begin by describing the data sets
produced by automatic conversion. For English,
we used the Stanford parser (v1.6.8) (Klein and
Manning, 2003) to convert the Wall Street Jour-
nal section of the Penn Treebank (Marcus et al,
1993) to basic dependency trees, including punc-
tuation and with the copula verb as head in cop-
ula constructions. For Swedish, we developed a
set of deterministic rules for converting the Tal-
banken part of the Swedish Treebank (Nivre and
Megyesi, 2007) to a representation as close as pos-
sible to the Stanford dependencies for English.
This mainly consisted in relabeling dependency
relations and, due to the fine-grained label set used
in the Swedish Treebank (Teleman, 1974), this
could be done with high precision. In addition,
a small number of constructions required struc-
tural conversion, notably coordination, which in
the Swedish Treebank is given a Prague style anal-
ysis (Nilsson et al, 2007). For both English and
Swedish, we mapped the language-specific part-
of-speech tags to universal tags using the map-
pings of Petrov et al (2012).
2.2 Manual Annotation
For the remaining four languages, annotators were
given three resources: 1) the English Stanford
93
guidelines; 2) a set of English sentences with Stan-
ford dependencies and universal tags (as above);
and 3) a large collection of unlabeled sentences
randomly drawn from newswire, weblogs and/or
consumer reviews, automatically tokenized with a
rule-based system. For German, French and Span-
ish, contractions were split, except in the case of
clitics. For Korean, tokenization was more coarse
and included particles within token units. Annota-
tors could correct this automatic tokenization.
The annotators were then tasked with producing
language-specific annotation guidelines with the
expressed goal of keeping the label and construc-
tion set as close as possible to the original English
set, only adding labels for phenomena that do not
exist in English. Making fine-grained label dis-
tinctions was discouraged. Once these guidelines
were fixed, annotators selected roughly an equal
amount of sentences to be annotated from each do-
main in the unlabeled data. As the sentences were
already randomly selected from a larger corpus,
annotators were told to view the sentences in or-
der and to discard a sentence only if it was 1) frag-
mented because of a sentence splitting error; 2) not
from the language of interest; 3) incomprehensible
to a native speaker; or 4) shorter than three words.
The selected sentences were pre-processed using
cross-lingual taggers (Das and Petrov, 2011) and
parsers (McDonald et al, 2011).
The annotators modified the pre-parsed trees us-
ing the TrEd2 tool. At the beginning of the annota-
tion process, double-blind annotation, followed by
manual arbitration and consensus, was used itera-
tively for small batches of data until the guidelines
were finalized. Most of the data was annotated
using single-annotation and full review: one an-
notator annotating the data and another reviewing
it, making changes in close collaboration with the
original annotator. As a final step, all annotated
data was semi-automatically checked for annota-
tion consistency.
2.3 Harmonization
After producing the two converted and four an-
notated data sets, we performed a harmonization
step, where the goal was to maximize consistency
of annotation across languages. In particular, we
wanted to eliminate cases where the same label
was used for different linguistic relations in dif-
ferent languages and, conversely, where one and
2Available at http://ufal.mff.cuni.cz/tred/.
the same relation was annotated with different la-
bels, both of which could happen accidentally be-
cause annotators were allowed to add new labels
for the language they were working on. Moreover,
we wanted to avoid, as far as possible, labels that
were only used in one or two languages.
In order to satisfy these requirements, a number
of language-specific labels were merged into more
general labels. For example, in analogy with the
nn label for (element of a) noun-noun compound,
the annotators of German added aa for compound
adjectives, and the annotators of Korean added vv
for compound verbs. In the harmonization step,
these three labels were merged into a single label
compmod for modifier in compound.
In addition to harmonizing language-specific la-
bels, we also renamed a small number of relations,
where the name would be misleading in the uni-
versal context (although quite appropriate for En-
glish). For example, the label prep (for a mod-
ifier headed by a preposition) was renamed adp-
mod, to make clear the relation to other modifier
labels and to allow postpositions as well as prepo-
sitions.3 We also eliminated a few distinctions in
the original Stanford scheme that were not anno-
tated consistently across languages (e.g., merging
complm with mark, number with num, and purpcl
with advcl).
The final set of labels is listed with explanations
in Table 1. Note that relative to the universal part-
of-speech tagset of Petrov et al (2012) our final
label set is quite rich (40 versus 12). This is due
mainly to the fact that the the former is based on
deterministic mappings from a large set of annota-
tion schemes and therefore reduced to the granu-
larity of the greatest common denominator. Such a
reduction may ultimately be necessary also in the
case of dependency relations, but since most of our
data sets were created through manual annotation,
we could afford to retain a fine-grained analysis,
knowing that it is always possible to map from
finer to coarser distinctions, but not vice versa.4
2.4 Final Data Sets
Table 2 presents the final data statistics. The num-
ber of sentences, tokens and tokens/sentence vary
3Consequently, pobj and pcomp were changed to adpobj
and adpcomp.
4The only two data sets that were created through con-
version in our case were English, for which the Stanford de-
pendencies were originally defined, and Swedish, where the
native annotation happens to have a fine-grained label set.
94
Label Description
acomp adjectival complement
adp adposition
adpcomp complement of adposition
adpmod adpositional modifier
adpobj object of adposition
advcl adverbial clause modifier
advmod adverbial modifier
amod adjectival modifier
appos appositive
attr attribute
aux auxiliary
auxpass passive auxiliary
cc conjunction
ccomp clausal complement
Label Description
compmod compound modifier
conj conjunct
cop copula
csubj clausal subject
csubjpass passive clausal subject
dep generic
det determiner
dobj direct object
expl expletive
infmod infinitival modifier
iobj indirect object
mark marker
mwe multi-word expression
neg negation
Label Description
nmod noun modifier
nsubj nominal subject
nsubjpass passive nominal subject
num numeric modifier
p punctuation
parataxis parataxis
partmod participial modifier
poss possessive
prt verb particle
rcmod relative clause modifier
rel relative
xcomp open clausal complement
Table 1: Harmonized label set based on Stanford dependencies (De Marneffe et al, 2006).
source(s) # sentences # tokens
DE N, R 4,000 59,014
EN PTB? 43,948 1,046,829
SV STB? 6,159 96,319
ES N, B, R 4,015 112,718
FR N, B, R 3,978 90,000
KO N, B 6,194 71,840
Table 2: Data set statistics. ?Automatically con-
verted WSJ section of the PTB. The data release
includes scripts to generate this data, not the data
itself. ?Automatically converted Talbanken sec-
tion of the Swedish Treebank. N=News, B=Blogs,
R=Consumer Reviews.
due to the source and tokenization. For example,
Korean has 50% more sentences than Spanish, but
?40k less tokens due to a more coarse-grained to-
kenization. In addition to the data itself, anno-
tation guidelines and harmonization rules are in-
cluded so that the data can be regenerated.
3 Experiments
One of the motivating factors in creating such a
data set was improved cross-lingual transfer eval-
uation. To test this, we use a cross-lingual transfer
parser similar to that of McDonald et al (2011).
In particular, it is a perceptron-trained shift-reduce
parser with a beam of size 8. We use the features
of Zhang and Nivre (2011), except that all lexical
identities are dropped from the templates during
training and testing, hence inducing a ?delexical-
ized? model that employs only ?universal? proper-
ties from source-side treebanks, such as part-of-
speech tags, labels, head-modifier distance, etc.
We ran a number of experiments, which can be
seen in Table 3. For these experiments we ran-
domly split each data set into training, develop-
ment and testing sets.5 The one exception is En-
glish, where we used the standard splits. Each
row in Table 3 represents a source training lan-
guage and each column a target evaluation lan-
guage. We report both unlabeled attachment score
(UAS) and labeled attachment score (LAS) (Buch-
holz and Marsi, 2006). This is likely the first re-
liable cross-lingual parsing evaluation. In partic-
ular, previous studies could not even report LAS
due to differences in treebank annotations.
We can make several interesting observations.
Most notably, for the Germanic and Romance tar-
get languages, the best source language is from
the same language group. This is in stark contrast
to the results of McDonald et al (2011), who ob-
serve that this is rarely the case with the heteroge-
nous CoNLL treebanks. Among the Germanic
languages, it is interesting to note that Swedish
is the best source language for both German and
English, which makes sense from a typological
point of view, because Swedish is intermediate be-
tween German and English in terms of word or-
der properties. For Romance languages, the cross-
lingual parser is approaching the accuracy of the
supervised setting, confirming that for these lan-
guages much of the divergence is lexical and not
structural, which is not true for the Germanic lan-
guages. Finally, Korean emerges as a very clear
outlier (both as a source and as a target language),
which again is supported by typological consider-
ations as well as by the difference in tokenization.
With respect to evaluation, it is interesting to
compare the absolute numbers to those reported
in McDonald et al (2011) for the languages com-
5These splits are included in the release of the data.
95
Source
Training
Language
Target Test Language
Unlabeled Attachment Score (UAS) Labeled Attachment Score (LAS)
Germanic Romance Germanic Romance
DE EN SV ES FR KO DE EN SV ES FR KO
DE 74.86 55.05 65.89 60.65 62.18 40.59 64.84 47.09 53.57 48.14 49.59 27.73
EN 58.50 83.33 70.56 68.07 70.14 42.37 48.11 78.54 57.04 56.86 58.20 26.65
SV 61.25 61.20 80.01 67.50 67.69 36.95 52.19 49.71 70.90 54.72 54.96 19.64
ES 55.39 58.56 66.84 78.46 75.12 30.25 45.52 47.87 53.09 70.29 63.65 16.54
FR 55.05 59.02 65.05 72.30 81.44 35.79 45.96 47.41 52.25 62.56 73.37 20.84
KO 33.04 32.20 27.62 26.91 29.35 71.22 26.36 21.81 18.12 18.63 19.52 55.85
Table 3: Cross-lingual transfer parsing results. Bolded are the best per target cross-lingual result.
mon to both studies (DE, EN, SV and ES). In that
study, UAS was in the 38?68% range, as compared
to 55?75% here. For Swedish, we can even mea-
sure the difference exactly, because the test sets
are the same, and we see an increase from 58.3%
to 70.6%. This suggests that most cross-lingual
parsing studies have underestimated accuracies.
4 Conclusion
We have released data sets for six languages with
consistent dependency annotation. After the ini-
tial release, we will continue to annotate data in
more languages as well as investigate further au-
tomatic treebank conversions. This may also lead
to modifications of the annotation scheme, which
should be regarded as preliminary at this point.
Specifically, with more typologically and morpho-
logically diverse languages being added to the col-
lection, it may be advisable to consistently en-
force the principle that content words take func-
tion words as dependents, which is currently vi-
olated in the analysis of adpositional and copula
constructions. This will ensure a consistent analy-
sis of functional elements that in some languages
are not realized as free words or are not obliga-
tory, such as adpositions which are often absent
due to case inflections in languages like Finnish. It
will also allow the inclusion of language-specific
functional or morphological markers (case mark-
ers, topic markers, classifiers, etc.) at the leaves of
the tree, where they can easily be ignored in appli-
cations that require a uniform cross-lingual repre-
sentation. Finally, this data is available on an open
source repository in the hope that the community
will commit new data and make corrections to ex-
isting annotations.
Acknowledgments
Many people played critical roles in the pro-
cess of creating the resource. At Google, Fer-
nando Pereira, Alfred Spector, Kannan Pashu-
pathy, Michael Riley and Corinna Cortes sup-
ported the project and made sure it had the re-
quired resources. Jennifer Bahk and Dave Orr
helped coordinate the necessary contracts. Andrea
Held, Supreet Chinnan, Elizabeth Hewitt, Tu Tsao
and Leigha Weinberg made the release process
smooth. Michael Ringgaard, Andy Golding, Terry
Koo, Alexander Rush and many others provided
technical advice. Hans Uszkoreit gave us per-
mission to use a subsample of sentences from the
Tiger Treebank (Brants et al, 2002), the source of
the news domain for our German data set. Anno-
tations were additionally provided by Sulki Kim,
Patrick McCrae, Laurent Alamarguy and He?ctor
Ferna?ndez Alcalde.
References
Alena Bo?hmova?, Jan Hajic?, Eva Hajic?ova?, and Barbora
Hladka?. 2003. The Prague Dependency Treebank:
A three-level annotation scenario. In Anne Abeille?,
editor, Treebanks: Building and Using Parsed Cor-
pora, pages 103?127. Kluwer.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
Treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL.
Miriam Butt, Helge Dyvik, Tracy Holloway King,
Hiroshi Masuichi, and Christian Rohrer. 2002.
The parallel grammar project. In Proceedings of
the 2002 workshop on Grammar engineering and
evaluation-Volume 15.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009. Discriminative
reordering with Chinese grammatical relations fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation (SSST-3)
at NAACL HLT 2009.
96
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of ACL-HLT.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation.
Marie-Catherine De Marneffe, Bill MacCartney, and
Chris D. Manning. 2006. Generating typed depen-
dency parses from phrase structure parses. In Pro-
ceedings of LREC.
Tomaz Erjavec. 2012. MULTEXT-East: Morphosyn-
tactic resources for Central and Eastern European
languages. Language Resources and Evaluation,
46:131?142.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction
via bitext projection constraints. In Proceedings of
ACL-IJCNLP.
Douwe Gelling, Trevor Cohn, Phil Blunsom, and Joao
Grac?a. 2012. The pascal challenge on grammar in-
duction. In Proceedings of the NAACL-HLT Work-
shop on the Induction of Linguistic Structure.
Jan Hajic?, Barbora Vidova Hladka, Jarmila Panevova?,
Eva Hajic?ova?, Petr Sgall, and Petr Pajas. 2001.
Prague Dependency Treebank 1.0. LDC, 2001T10.
Katri Haverinen, Timo Viljanen, Veronika Laippala,
Samuel Kohonen, Filip Ginter, and Tapio Salakoski.
2010. Treebanking finnish. In Proceedings of
The Ninth International Workshop on Treebanks and
Linguistic Theories (TLT9).
Stephen Helmreich, David Farwell, Bonnie Dorr, Nizar
Habash, Lori Levin, Teruko Mitamura, Florence
Reeder, Keith Miller, Eduard Hovy, Owen Rambow,
and Advaith Siddharthan. 2004. Interlingual anno-
tation of multilingual text corpora. In Proceedings
of the HLT-EACL Workshop on Frontiers in Corpus
Annotation.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Proceedings of NAACL.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(03):311?325.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of ACL.
Dan Klein and Chris D. Manning. 2004. Corpus-based
induction of syntactic structure: models of depen-
dency and constituency. In Proceedings of ACL.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Morgan and Claypool.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn treebank. Compu-
tational Linguistics, 19(2):313?330.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of EMNLP.
Jens Nilsson, Joakim Nivre, and Johan Hall. 2007.
Generalizing tree transformations for inductive de-
pendency parsing. In Proceedings of ACL.
Joakim Nivre and Bea?ta Megyesi. 2007. Bootstrap-
ping a Swedish treebank using cross-corpus harmo-
nization and annotation projection. In Proceedings
of the 6th International Workshop on Treebanks and
Linguistic Theories.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on
dependency parsing. In Proceedings of EMNLP-
CoNLL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC.
Mojgan Seraji, Bea?ta Megyesi, and Nivre Joakim.
2012. Bootstrapping a Persian dependency tree-
bank. Linguistic Issues in Language Technology,
7(18):1?10.
David A. Smith and Jason Eisner. 2009. Parser adap-
tation and projection with quasi-synchronous gram-
mar features. In Proceedings of EMNLP.
Oscar Ta?ckstro?m, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of the ACL.
Ulf Teleman. 1974. Manual fo?r grammatisk beskrivn-
ing av talad och skriven svenska. Studentlitteratur.
Reut Tsarfaty. 2013. A unified morpho-syntactic
scheme of stanford dependencies. Proceedings of
ACL.
Daniel Zeman, David Marecek, Martin Popel,
Loganathan Ramasamy, Jan S?tepa?nek, Zdene?k
Z?abokrtsky`, and Jan Hajic. 2012. Hamledt: To
parse or not to parse. In Proceedings of LREC.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL-HLT.
Yuan Zhang, Roi Reichart, Regina Barzilay, and Amir
Globerson. 2012. Learning to map into a universal
pos tagset. In Proceedings of EMNLP.
97
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 193?198,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Docent: A Document-Level Decoder for
Phrase-Based Statistical Machine Translation
Christian Hardmeier Sara Stymne J?rg Tiedemann Joakim Nivre
Uppsala University
Department of Linguistics and Philology
Box 635, 751 26 Uppsala, Sweden
firstname.lastname@lingfil.uu.se
Abstract
We describe Docent, an open-source de-
coder for statistical machine translation
that breaks with the usual sentence-by-
sentence paradigm and translates complete
documents as units. By taking transla-
tion to the document level, our decoder
can handle feature models with arbitrary
discourse-wide dependencies and consti-
tutes an essential infrastructure compon-
ent in the quest for discourse-aware SMT
models.
1 Motivation
Most of the research on statistical machine trans-
lation (SMT) that was conducted during the last
20 years treated every text as a ?bag of sentences?
and disregarded all relations between elements in
different sentences. Systematic research into ex-
plicitly discourse-related problems has only begun
very recently in the SMT community (Hardmeier,
2012) with work on topics such as pronominal
anaphora (Le Nagard and Koehn, 2010; Hard-
meier and Federico, 2010; Guillou, 2012), verb
tense (Gong et al, 2012) and discourse connect-
ives (Meyer et al, 2012).
One of the problems that hamper the develop-
ment of cross-sentence models for SMT is the fact
that the assumption of sentence independence is
at the heart of the dynamic programming (DP)
beam search algorithm most commonly used for
decoding in phrase-based SMT systems (Koehn et
al., 2003). For integrating cross-sentence features
into the decoding process, researchers had to adopt
strategies like two-pass decoding (Le Nagard and
Koehn, 2010). We have previously proposed an
algorithm for document-level phrase-based SMT
decoding (Hardmeier et al, 2012). Our decoding
algorithm is based on local search instead of dy-
namic programming and permits the integration of
document-level models with unrestricted depend-
encies, so that a model score can be conditioned on
arbitrary elements occurring anywhere in the input
document or in the translation that is being gen-
erated. In this paper, we present an open-source
implementation of this search algorithm. The de-
coder is written in C++ and follows an object-
oriented design that makes it easy to extend it with
new feature models, new search operations or dif-
ferent types of local search algorithms. The code
is released under the GNU General Public License
and published on Github1 to make it easy for other
researchers to use it in their own experiments.
2 Document-Level Decoding with Local
Search
Our decoder is based on the phrase-based SMT
model described by Koehn et al (2003) and im-
plemented, for example, in the popular Moses
decoder (Koehn et al, 2007). Translation is
performed by splitting the input sentence into
a number of contiguous word sequences, called
phrases, which are translated into the target lan-
guage through a phrase dictionary lookup and op-
tionally reordered. The choice between different
translations of an ambiguous source phrase and the
ordering of the target phrases are guided by a scor-
ing function that combines a set of scores taken
from the phrase table with scores from other mod-
els such as an n-gram language model. The actual
translation process is realised as a search for the
highest-scoring translation in the space of all the
possible translations that could be generated given
the models.
The decoding approach that is implemented in
Docent was first proposed by Hardmeier et al
(2012) and is based on local search. This means
that it has a state corresponding to a complete, if
possibly bad, translation of a document at every
1https://github.com/chardmeier/docent/wiki
193
stage of the search progress. Search proceeds by
making small changes to the current search state in
order to transform it gradually into a better trans-
lation. This differs from the DP algorithm used in
other decoders, which starts with an empty trans-
lation and expands it bit by bit. It is similar to
previous work on phrase-based SMT decoding by
Langlais et al (2007), but enables the creation of
document-level models, which was not addressed
by earlier approaches.
Docent currently implements two search al-
gorithms that are different generalisations of the
hill climbing local search algorithm by Hardmeier
et al (2012). The original hill climbing algorithm
starts with an initial state and generates possible
successor states by randomly applying simple ele-
mentary operations to the state. After each op-
eration, the new state is scored and accepted if
its score is better than that of the previous state,
else rejected. Search terminates when the decoder
cannot find an acceptable successor state after a
certain number of attempts, or when a maximum
number of steps is reached.
Simulated annealing is a stochastic variant of
hill climbing that always accepts moves towards
better states, but can also accept moves towards
lower-scoring states with a certain probability that
depends on a temperature parameter in order to
escape local maxima. Local beam search gener-
alises hill climbing in a different way by keeping
a beam of a fixed number of multiple states at any
time and randomly picking a state from the beam
to modify at each move. The original hill climb-
ing procedure can be recovered as a special case
of either one of these search algorithms, by call-
ing simulated annealing with a fixed temperature
of 0 or local beam search with a beam size of 1.
Initial states for the search process can be gen-
erated either by selecting a random segmentation
with random translations from the phrase table in
monotonic order, or by running DP beam search
with sentence-local models as a first pass. For
the second option, which generally yields better
search results, Docent is linked with the Moses
decoder and makes direct calls to the DP beam
search algorithm implemented by Moses. In addi-
tion to these state initialisation procedures, Docent
can save a search state to a disk file which can be
loaded again in a subsequent decoding pass. This
saves time especially when running repeated ex-
periments from the same starting point obtained
by DP search.
In order to explore the complete search space
of phrase-based SMT, the search operations in a
local search decoder must be able to change the
phrase translations, the order of the output phrases
and the segmentation of the source sentence into
phrases. The three operations used by Hardmeier
et al (2012), change-phrase-translation, reseg-
ment and swap-phrases, jointly meet this require-
ment and are all implemented in Docent. Addi-
tionally, Docent features three extra operations, all
of which affect the target word order: The move-
phrases operation moves a phrase to another loca-
tion in the sentence. Unlike swap-phrases, it does
not require that another phrase be moved in the
opposite direction at the same time. A pair of
operations called permute-phrases and linearise-
phrases can reorder a sequence of phrases into ran-
dom order and back into the order corresponding
to the source language.
Since the search algorithm in Docent is
stochastic, repeated runs of the decoder will gen-
erally produce different output. However, the vari-
ance of the output is usually small, especially
when initialising with a DP search pass, and it
tends to be lower than the variance introduced
by feature weight tuning (Hardmeier et al, 2012;
Stymne et al, 2013a).
3 Available Feature Models
In its current version, Docent implements a selec-
tion of sentence-local feature models that makes
it possible to build a baseline system with a con-
figuration comparable to that of a typical Moses
baseline system. The published source code
also includes prototype implementations of a few
document-level models. These models should be
considered work in progress and serve as a demon-
stration of the cross-sentence modelling capabilit-
ies of the decoder. They have not yet reached a
state of maturity that would make them suitable
for production use.
The sentence-level models provided by Docent
include the phrase table, n-gram language models
implemented with the KenLM toolkit (Heafield,
2011), an unlexicalised distortion cost model with
geometric decay (Koehn et al, 2003) and a word
penalty cost. All of these features are designed
to be compatible with the corresponding features
in Moses. From among the typical set of baseline
features in Moses, we have not implemented the
194
lexicalised distortion model, but this model could
easily be added if required. Docent uses the same
binary file format for phrase tables as Moses, so
the same training apparatus can be used.
DP-based SMT decoders have a parameter
called distortion limit that limits the difference in
word order between the input and the MT out-
put. In DP search, this is formally considered to
be a parameter of the search algorithm because it
affects the algorithmic complexity of the search
by controlling how many translation options must
be considered at each hypothesis expansion. The
stochastic search algorithm in Docent does not re-
quire this limitation, but it can still be useful be-
cause the standard models of SMT do not model
long-distance reordering well. Docent therefore
includes a separate indicator feature to indicate
a violated distortion limit. In conjunction with a
very large weight, this feature can effectively en-
sure that the distortion limit is enforced. In con-
trast with the distortion limit parameter of a DP de-
coder, the weight of our distortion limit feature can
potentially be tuned to permit occasional distor-
tion limit violations when they contribute to better
translations.
The document-level models included in Docent
include a length parity model, a semantic lan-
guage model as well as a collection of document-
level readability models. The length parity model
is a proof-of-concept model that ensures that all
sentences in a document have either consistently
odd or consistently even length. It serves mostly as
a template to demonstrate how a simple document-
level model can be implemented in the decoder.
The semantic language model was originally pro-
posed by Hardmeier et al (2012) to improve lex-
ical cohesion in a document. It is a cross-sentence
model over sequences of content words that are
scored based on their similarity in a word vector
space. The readability models serve to improve
the readability of the translation by encouraging
the selection of easier and more consistent target
words. They are described and demonstrated in
more detail in section 5.
Docent can read input files both in the NIST-
XML format commonly used to encode docu-
ments in MT shared tasks such as NIST or WMT
and in the more elaborate MMAX format (M?ller
and Strube, 2003). The MMAX format makes
it possible to include a wide range of discourse-
level corpus annotations such as coreference links.
These annotations can then be accessed by the
feature models. To allow for additional target-
language information such as morphological fea-
tures of target words, Docent can handle simple
word-level annotations that are encoded in the
phrase table in the same way as target language
factors in Moses.
In order to optimise feature weights we have
adapted the Moses tuning infrastructure to Do-
cent. In this way we can take advantage of all its
features, for instance using different optimisation
algorithms such as MERT (Och, 2003) or PRO
(Hopkins and May, 2011), and selective tuning of
a subset of features. Since document features only
give meaningful scores on the document level and
not on the sentence level, we naturally perform
optimisation on document level, which typically
means that we need more data than for the op-
timisation of sentence-based decoding. The res-
ults we obtain are relatively stable and competit-
ive with sentence-level optimisation of the same
models (Stymne et al, 2013a).
4 Implementing Feature Models
Efficiently
While translating a document, the local search de-
coder attempts to make a great number of moves.
For each move, a score must be computed and
tested against the acceptance criterion. An over-
whelming majority of the proposed moves will be
rejected. In order to achieve reasonably fast de-
coding times, efficient scoring is paramount. Re-
computing the scores of the whole document at
every step would be far too slow for the decoder
to be useful. Fortunately, score computation can
be sped up in two ways. Knowledge about how
the state to be scored was generated from its pre-
decessor helps to limit recomputations to a min-
imum, and by adopting a two-step scoring proced-
ure that just computes the scores that can be calcu-
lated with little effort at first, we need to compute
the complete score only if the new state has some
chance of being accepted.
The scores of SMT feature models can usu-
ally be decomposed in some way over parts of
the document. The traditional models borrowed
from sentence-based decoding are necessarily de-
composable at the sentence level, and in practice,
all common models are designed to meet the con-
straints of DP beam search, which ensures that
they can in fact be decomposed over even smal-
195
ler sequences of just a few words. For genuine
document-level features, this is not the case, but
even these models can often be decomposed in
some way, for instance over paragraphs, anaphoric
links or lexical chains. To take advantage of this
fact, feature models in Docent always have access
to the previous state and its score and to a list of
the state modifications that transform the previous
state into the next. The scores of the new state are
calculated by identifying the parts of a document
that are affected by the modifications, subtract-
ing the old scores of this part from the previous
score and adding the new scores. This approach
to scoring makes feature model implementation
a bit more complicated than in DP search, but it
gives the feature models full control over how they
decompose a document while still permitting effi-
cient decoding.
A feature model class in Docent implements
three methods. The initDocument method is called
once per document when decoding starts. It
straightforwardly computes the model score for
the entire document from scratch. When a state
is modified, the decoder first invokes the estim-
ateScoreUpdate method. Rather than calculating
the new score exactly, this method is only required
to return an upper bound that reflects the max-
imum score that could possibly be achieved by this
state. The search algorithm then checks this upper
bound against the acceptance criterion. Only if the
upper bound meets the criterion does it call the
updateScore method to calculate the exact score,
which is then checked against the acceptance cri-
terion again.
The motivation for this two-step procedure is
that some models can compute an upper bound ap-
proximation much more efficiently than an exact
score. For any model whose score is a log probab-
ility, a value of 0 is a loose upper bound that can
be returned instantly, but in many cases, we can do
much better. In the case of the n-gram language
model, for instance, a more accurate upper bound
can be computed cheaply by subtracting from the
old score all log-probabilities of n-grams that are
affected by the state modifications without adding
the scores of the n-grams replacing them in the
new state. This approximation can be calculated
without doing any language model lookups at all.
On the other hand, some models like the distor-
tion cost or the word penalty are very cheap to
compute, so that the estimateScoreUpdate method
can simply return the precise score as a tight up-
per bound. If a state gets rejected because of a
low score on one of the cheap models, this means
we will never have to compute the more expensive
feature scores at all.
5 Readability: A Case Study
As a case study we report initial results on how
document-wide features can be used in Docent in
order to improve the readability of texts by encour-
aging simple and consistent terminology (Stymne
et al, 2013b). This work is a first step towards
achieving joint SMT and text simplification, with
the final goal of adapting MT to user groups such
as people with reading disabilities.
Lexical consistency modelling for SMT has
been attempted before. The suggested approaches
have been limited by the use of sentence-level
decoders, however, and had to resort to proced-
ures like post processing (Carpuat, 2009), multiple
decoding runs with frozen counts from previous
runs (Ture et al, 2012), or cache-based models
(Tiedemann, 2010). In Docent, however, we al-
ways have access to a full document translation,
which makes it straightforward to include features
directly into the decoder.
We implemented four features on the document
level. The first two features are type token ra-
tio (TTR) and a reformulation of it, OVIX, which
is less sensitive to text length. These ratios have
been related to the ?idea density? of a text (M?h-
lenbock and Kokkinakis, 2009). We also wanted
to encourage consistent translations of words, for
which we used the Q-value (Del?ger et al, 2006),
which has been proposed to measure term qual-
ity. We applied it on word level (QW) and phrase
level (QP). These features need access to the full
target document, which we have in Docent. In ad-
dition, we included two sentence-level count fea-
tures for long words that have been used to meas-
ure the readability of Swedish texts (M?hlenbock
and Kokkinakis, 2009).
We tested our features on English?Swedish
translation using the Europarl corpus. For train-
ing we used 1,488,322 sentences. As test data, we
extracted 20 documents with a total of 690 sen-
tences. We used the standard set of baseline fea-
tures: 5-gram language model, translation model
with 5 weights, a word penalty and a distortion
penalty.
196
Baseline Readability features Comment
de ?rade ledam?terna (the honourable
Members)
ledam?terna (the members) / ni
(you)
+ Removal of non-essential words
p? ett s?dant s?tt att (in such a way
that)
s? att (so that) + Simplified expression
gemenskapslagstiftningen (the
community legislation)
gemenskapens lagstiftning (the
community?s legislation)
+ Shorter words by changing long
compound to genitive construction
V?rldshandelsorganisationen (World
Trade Organisation)
WTO (WTO) ? Changing long compound to
English-based abbreviation
handlingsplanen (the action plan) planen (the plan) ? Removal of important word
?gnat s?rskild uppm?rksamhet ?t (paid
particular attention to)
s?rskilt uppm?rksam p?
(particular attentive on)
? Bad grammar because of changed
part of speech and missing verb
Table 2: Example translation snippets with comments
Feature BLEU OVIX LIX
Baseline 0.243 56.88 51.17
TTR 0.243 55.25 51.04
OVIX 0.243 54.65 51.00
QW 0.242 57.16 51.16
QP 0.243 57.07 51.06
All 0.235 47.80 49.29
Table 1: Results for adding single lexical consist-
ency features to Docent
To evaluate our system we used the BLEU score
(Papineni et al, 2002) together with a set of read-
ability metrics, since readability is what we hoped
to improve by adding consistency features. Here
we used OVIX to confirm a direct impact on con-
sistency, and LIX (Bj?rnsson, 1968), which is a
common readability measure for Swedish. Unfor-
tunately we do not have access to simplified trans-
lated text, so we calculate the MT metrics against a
standard reference, which means that simple texts
will likely have worse scores than complicated
texts closer to the reference translation.
We tuned the standard features using Moses and
MERT, and then added each lexical consistency
feature with a small weight, using a grid search ap-
proach to find values with a small impact. The res-
ults are shown in Table 1. As can be seen, for in-
dividual features the translation quality was main-
tained, with small improvements in LIX, and in
OVIX for the TTR and OVIX features. For the
combination we lost a little bit on translation qual-
ity, but there was a larger effect on the readability
metrics. When we used larger weights, there was
a bigger impact on the readability metrics, with a
further decrease on MT quality.
We also investigated what types of changes the
readability features could lead to. Table 2 shows a
sample of translations where the baseline is com-
pared to systems with readability features. There
are both cases where the readability features help
and cases where they are problematic. Overall,
these examples show that our simple features can
help achieve some interesting simplifications.
There is still much work to do on how to take
best advantage of the possibilities in Docent in or-
der to achieve readable texts. This attempt shows
the feasibility of the approach. We plan to ex-
tend this work for instance by better feature op-
timisation, by integrating part-of-speech tags into
our features in order to focus on terms rather than
common words, and by using simplified texts for
evaluation and tuning.
6 Conclusions
In this paper, we have presented Docent, an open-
source document-level decoder for phrase-based
SMT released under the GNU General Public Li-
cense. Docent is the first decoder that permits the
inclusion of feature models with unrestricted de-
pendencies between arbitrary parts of the output,
even crossing sentence boundaries. A number of
research groups have recently started to investig-
ate the interplay between SMT and discourse-level
phenomena such as pronominal anaphora, verb
tense selection and the generation of discourse
connectives. We expect that the availability of a
document-level decoder will make it substantially
easier to leverage discourse information in SMT
and make SMT models explore new ground bey-
ond the next sentence boundary.
References
Carl-Hugo Bj?rnsson. 1968. L?sbarhet. Liber, Stock-
holm.
Marine Carpuat. 2009. One translation per discourse.
In Proceedings of the Workshop on Semantic Evalu-
ations: Recent Achievements and Future Directions
(SEW-2009), pages 19?27, Boulder, Colorado.
197
Louise Del?ger, Magnus Merkel, and Pierre Zweigen-
baum. 2006. Enriching medical terminologies: an
approach based on aligned corpora. In International
Congress of the European Federation for Medical
Informatics, pages 747?752, Maastricht, The Neth-
erlands.
Zhengxian Gong, Min Zhang, Chew Lim Tan, and
Guodong Zhou. 2012. N-gram-based tense models
for statistical machine translation. In Proceedings
of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning, pages 276?285,
Jeju Island, Korea.
Liane Guillou. 2012. Improving pronoun translation
for statistical machine translation. In Proceedings of
the Student Research Workshop at the 13th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 1?10, Avignon,
France.
Christian Hardmeier and Marcello Federico. 2010.
Modelling pronominal anaphora in statistical ma-
chine translation. In Proceedings of the seventh In-
ternational Workshop on Spoken Language Transla-
tion (IWSLT), pages 283?289, Paris, France.
Christian Hardmeier, Joakim Nivre, and J?rg
Tiedemann. 2012. Document-wide decoding
for phrase-based statistical machine translation.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Processing
and Computational Natural Language Learning,
pages 1179?1190, Jeju Island, Korea.
Christian Hardmeier. 2012. Discourse in statistical
machine translation: A survey and a case study. Dis-
cours, 11.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197, Edinburgh, Scotland.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1352?1362, Edinburgh, Scotland.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 conference of the North Amer-
ican chapter of the Association for Computational
Linguistics on Human Language Technology, pages
48?54, Edmonton.
Philipp Koehn, Hieu Hoang, Alexandra Birch, et al
2007. Moses: open source toolkit for Statistical Ma-
chine Translation. In Annual meeting of the Associ-
ation for Computational Linguistics: Demonstration
session, pages 177?180, Prague, Czech Republic.
Philippe Langlais, Alexandre Patry, and Fabrizio Gotti.
2007. A greedy decoder for phrase-based statist-
ical machine translation. In TMI-2007: Proceedings
of the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation,
pages 104?113, Sk?vde, Sweden.
Ronan Le Nagard and Philipp Koehn. 2010. Aiding
pronoun translation with co-reference resolution. In
Proceedings of the Joint Fifth Workshop on Statist-
ical Machine Translation and MetricsMATR, pages
252?261, Uppsala, Sweden.
Thomas Meyer, Andrei Popescu-Belis, Najeh Hajlaoui,
and Andrea Gesmundo. 2012. Machine translation
of labeled discourse connectives. In Proceedings of
the Tenth Biennial Conference of the Association for
Machine Translation in the Americas (AMTA), San
Diego, California, USA.
Katarina M?hlenbock and Sofie Johansson Kokkinakis.
2009. LIX 68 revisited ? an extended readability. In
Proceedings of the Corpus Linguistics Conference,
Liverpool, UK.
Christoph M?ller and Michael Strube. 2003. Multi-
level annotation in MMAX. In Proceedings of the
Fourth SIGdial Workshop on Discourse and Dia-
logue, pages 198?207, Sapporo, Japan.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
42nd Annual Meeting of the Association for Com-
putational Linguistics, pages 160?167, Sapporo, Ja-
pan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA.
Sara Stymne, Christian Hardmeier, J?rg Tiedemann,
and Joakim Nivre. 2013a. Feature weight optim-
ization for discourse-level SMT. In Proceedings of
the Workshop on Discourse in Machine Translation
(DiscoMT), Sofia, Bulgaria.
Sara Stymne, J?rg Tiedemann, Christian Hardmeier,
and Joakim Nivre. 2013b. Statistical machine trans-
lation with readability constraints. In Proceedings of
the 19th Nordic Conference of Computational Lin-
guistics (NODALIDA 2013), pages 375?386, Oslo,
Norway.
J?rg Tiedemann. 2010. Context adaptation in stat-
istical machine translation using models with ex-
ponentially decaying cache. In Proceedings of the
ACL 2010 Workshop on Domain Adaptation for Nat-
ural Language Processing (DANLP), pages 8?15,
Uppsala, Sweden.
Ferhan Ture, Douglas W. Oard, and Philip Resnik.
2012. Encouraging consistent translation choices.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 417?426, Montr?al, Canada.
198
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 649?655,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
On WordNet Semantic Classes and Dependency Parsing
Kepa Bengoetxea?, Eneko Agirre?, Joakim Nivre?,
Yue Zhang*, Koldo Gojenola?
?University of the Basque Country UPV/EHU / IXA NLP Group
?Uppsala University / Department of Linguistics and Philology
? Singapore University of Technology and Design
kepa.bengoetxea@ehu.es, e.agirre@ehu.es,
joakim.nivre@lingfil.uu.se, yue zhang@sutd.edu.sg,
koldo.gojenola@ehu.es
Abstract
This paper presents experiments with
WordNet semantic classes to improve de-
pendency parsing. We study the effect
of semantic classes in three dependency
parsers, using two types of constituency-
to-dependency conversions of the English
Penn Treebank. Overall, we can say that
the improvements are small and not sig-
nificant using automatic POS tags, con-
trary to previously published results using
gold POS tags (Agirre et al, 2011). In
addition, we explore parser combinations,
showing that the semantically enhanced
parsers yield a small significant gain only
on the more semantically oriented LTH
treebank conversion.
1 Introduction
This work presents a set of experiments to investi-
gate the use of lexical semantic information in de-
pendency parsing of English. Whether semantics
improve parsing is one interesting research topic
both on parsing and lexical semantics. Broadly
speaking, we can classify the methods to incor-
porate semantic information into parsers in two:
systems using static lexical semantic repositories,
such as WordNet or similar ontologies (Agirre et
al., 2008; Agirre et al, 2011; Fujita et al, 2010),
and systems using dynamic semantic clusters au-
tomatically acquired from corpora (Koo et al,
2008; Suzuki et al, 2009).
Our main objective will be to determine
whether static semantic knowledge can help pars-
ing. We will apply different types of semantic in-
formation to three dependency parsers. Specifi-
cally, we will test the following questions:
? Does semantic information in WordNet help
dependency parsing? Agirre et al (2011)
found improvements in dependency parsing
using MaltParser on gold POS tags. In this
work, we will investigate the effect of seman-
tic information using predicted POS tags.
? Is the type of semantic information related
to the type of parser? We will test three
different parsers representative of successful
paradigms in dependency parsing.
? How does the semantic information relate to
the style of dependency annotation? Most ex-
periments for English were evaluated on the
Penn2Malt conversion of the constituency-
based Penn Treebank. We will also examine
the LTH conversion, with richer structure and
an extended set of dependency labels.
? How does WordNet compare to automati-
cally obtained information? For the sake of
comparison, we will also perform the experi-
ments using syntactic/semantic clusters auto-
matically acquired from corpora.
? Does parser combination benefit from seman-
tic information? Different parsers can use se-
mantic information in diverse ways. For ex-
ample, while MaltParser can use the semantic
information in local contexts, MST can in-
corporate them in global contexts. We will
run parser combination experiments with and
without semantic information, to determine
whether it is useful in the combined parsers.
After introducing related work in section 2, sec-
tion 3 describes the treebank conversions, parsers
and semantic features. Section 4 presents the re-
sults and section 5 draws the main conclusions.
2 Related work
Broadly speaking, we can classify the attempts to
add external knowledge to a parser in two sets:
using large semantic repositories such as Word-
Net and approaches that use information automat-
ically acquired from corpora. In the first group,
Agirre et al (2008) trained two state-of-the-art
constituency-based statistical parsers (Charniak,
649
2000; Bikel, 2004) on semantically-enriched in-
put, substituting content words with their seman-
tic classes, trying to overcome the limitations of
lexicalized approaches to parsing (Collins, 2003)
where related words, like scissors and knife, can-
not be generalized. The results showed a signi-
cant improvement, giving the first results over both
WordNet and the Penn Treebank (PTB) to show
that semantics helps parsing. Later, Agirre et al
(2011) successfully introduced WordNet classes in
a dependency parser, obtaining improvements on
the full PTB using gold POS tags, trying different
combinations of semantic classes. MacKinlay et
al. (2012) investigate the addition of semantic an-
notations in the form of word sense hypernyms, in
HPSG parse ranking, reducing error rate in depen-
dency F-score by 1%, while some methods pro-
duce substantial decreases in performance. Fu-
jita et al (2010) showed that fully disambiguated
sense-based features smoothed using ontological
information are effective for parse selection.
On the second group, Koo et al (2008) pre-
sented a semisupervised method for training de-
pendency parsers, introducing features that incor-
porate word clusters automatically acquired from
a large unannotated corpus. The clusters include
strongly semantic associations like {apple, pear}
or {Apple, IBM} and also syntactic clusters like
{of, in}. They demonstrated its effectiveness in
dependency parsing experiments on the PTB and
the Prague Dependency Treebank. Suzuki et al
(2009), Sagae and Gordon (2009) and Candito
and Seddah (2010) also experiment with the same
cluster method. Recently, T?ackstr?om et al (2012)
tested the incorporation of cluster features from
unlabeled corpora in a multilingual setting, giving
an algorithm for inducing cross-lingual clusters.
3 Experimental Framework
In this section we will briefly describe the PTB-
based datasets (subsection 3.1), followed by the
data-driven parsers used for the experiments (sub-
section 3.2). Finally, we will describe the different
types of semantic representation that were used.
3.1 Treebank conversions
Penn2Malt
1
performs a simple and direct conver-
sion from the constituency-based PTB to a depen-
dency treebank. It obtains projective trees and has
been used in several works, which allows us to
1
http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html
compare our results with related experiments (Koo
et al, 2008; Suzuki et al, 2009; Koo and Collins,
2010). We extracted dependencies using standard
head rules (Yamada and Matsumoto, 2003), and a
reduced set of 12 general dependency tags.
LTH
2
(Johansson and Nugues, 2007) presents
a conversion better suited for semantic process-
ing, with a richer structure and a more fine-grained
set of dependency labels (42 different dependency
labels), including links to handle long-distance
phenomena, giving a 6.17% of nonprojective sen-
tences. The results from parsing the LTH output
are lower than those for Penn2Malt conversions.
3.2 Parsers
We have made use of three parsers representative
of successful paradigms in dependency parsing.
MaltParser (Nivre et al, 2007) is a determinis-
tic transition-based dependency parser that obtains
a dependency tree in linear-time in a single pass
over the input using a stack of partially analyzed
items and the remaining input sequence, by means
of history-based feature models. We added two
features that inspect the semantic feature at the top
of the stack and the next input token.
MST
3
represents global, exhaustive graph-
based parsing (McDonald et al, 2005; McDon-
ald et al, 2006) that finds the highest scoring di-
rected spanning tree in a graph. The learning pro-
cedure is global since model parameters are set
relative to classifying the entire dependency graph,
in contrast to the local but richer contexts used
by transition-based parsers. The system can be
trained using first or second order models. The
second order projective algorithm performed best
on both conversions, and we used it in the rest of
the evaluations. We modified the system in or-
der to add semantic features, combining them with
wordforms and POS tags, on the parent and child
nodes of each arc.
ZPar
4
(Zhang and Clark, 2008; Zhang and
Nivre, 2011) performs transition-based depen-
dency parsing with a stack of partial analysis
and a queue of remaining inputs. In contrast to
MaltParser (local model and greedy deterministic
search) ZPar applies global discriminative learn-
ing and beam search. We extend the feature set of
ZPar to include semantic features. Each set of se-
mantic information is represented by two atomic
2
http://nlp.cs.lth.se/software/treebank converter
3
http://mstparser.sourceforge.net
4
www.sourceforge.net/projects/zpar
650
Base WordNet WordNet Clusters
line SF SS
Malt 88.46 88.49 (+0.03) 88.42 (-0.04) 88.59 (+0.13)
MST 90.55 90.70 (+0.15) 90.47 (-0.08) 90.88 (+0.33)?
ZPar 91.52 91.65 (+0.13) 91.70 (+0.18)? 91.74 (+0.22)
Table 1: LAS results with several parsing algo-
rithms, Penn2Malt conversion (?: p <0.05, ?: p
<0.005). In parenthesis, difference with baseline.
feature templates, associated with the top of the
stack and the head of the queue, respectively. ZPar
was directly trained on the Penn2Malt conversion,
while we applied the pseudo-projective transfor-
mation (Nilsson et al, 2008) on LTH, in order to
deal with non-projective arcs.
3.3 Semantic information
Our aim was to experiment with different types of
WordNet-related semantic information. For com-
parison with automatically acquired information,
we will also experiment with bit clusters.
WordNet. We will experiment with the seman-
tic representations used in Agirre et al (2008) and
Agirre et al (2011), based on WordNet 2.1. Word-
Net is organized into sets of synonyms, called
synsets (SS). Each synset in turn belongs to a
unique semantic file (SF). There are a total of 45
SFs (1 for adverbs, 3 for adjectives, 15 for verbs,
and 26 for nouns), based on syntactic and seman-
tic categories. For example, noun SFs differen-
tiate nouns denoting acts or actions, and nouns
denoting animals, among others. We experiment
with both full SSs and SFs as instances of fine-
grained and coarse-grained semantic representa-
tion, respectively. As an example, knife in its
tool sense is in the EDGE TOOL USED AS A
CUTTING INSTRUMENT singleton synset, and
also in the ARTIFACT SF along with thousands
of words including cutter. These are the two ex-
tremes of semantic granularity in WordNet. For
each semantic representation, we need to deter-
mine the semantics of each occurrence of a target
word. Agirre et al (2011) used i) gold-standard
annotations from SemCor, a subset of the PTB, to
give an upper bound performance of the semantic
representation, ii) first sense, where all instances
of a word were tagged with their most frequent
sense, and iii) automatic sense ranking, predicting
the most frequent sense for each word (McCarthy
et al, 2004). As we will make use of the full PTB,
we only have access to the first sense information.
Clusters. Koo et al (2008) describe a semi-
Base WordNet WordNet Clusters
line SF SS
Malt 84.95 85.12 (+0.17) 85.08 (+0.16) 85.13 (+0.18)
MST 85.06 85.35 (+0.29)? 84.99 (-0.07) 86.18 (+1.12)?
ZPar 89.15 89.33 (+0.18) 89.19 (+0.04) 89.17 (+0.02)
Table 2: LAS results with several parsing algo-
rithms in the LTH conversion (?: p <0.05, ?: p
<0.005). In parenthesis, difference with baseline.
supervised approach that makes use of cluster fea-
tures induced from unlabeled data, providing sig-
nificant performance improvements for supervised
dependency parsers on the Penn Treebank for En-
glish and the Prague Dependency Treebank for
Czech. The process defines a hierarchical cluster-
ing of the words, which can be represented as a
binary tree where each node is associated to a bit-
string, from the more general (root of the tree) to
the more specific (leaves). Using prefixes of vari-
ous lengths, it can produce clusterings of different
granularities. It can be seen as a representation of
syntactic-semantic information acquired from cor-
pora. They use short strings of 4-6 bits to represent
parts of speech and the full strings for wordforms.
4 Results
In all the experiments we employed a baseline fea-
ture set using word forms and parts of speech, and
an enriched feature set (WordNet or clusters). We
firstly tested the addition of each individual se-
mantic feature to each parser, evaluating its contri-
bution to the parser?s performance. For the combi-
nations, instead of feature-engineering each parser
with the wide array of different possibilities for
features, as in Agirre et al (2011), we adopted
the simpler approach of combining the outputs of
the individual parsers by voting (Sagae and Lavie,
2006). We will use Labeled Attachment Score
(LAS) as our main evaluation criteria. As in pre-
vious work, we exclude punctuation marks. For
all the tests, we used a perceptron POS-tagger
(Collins, 2002), trained on WSJ sections 2?21, to
assign POS tags automatically to both the training
(using 10-way jackknifing) and test data, obtaining
a POS tagging accuracy of 97.32% on the test data.
We will make use of Bikel?s randomized parsing
evaluation comparator to test the statistical signi-
cance of the results. In all of the experiments the
parsers were trained on sections 2-21 of the PTB
and evaluated on the development set (section 22).
Finally, the best performing system was evaluated
on the test set (section 23).
651
Parsers LAS UAS
Best baseline (ZPar) 91.52 92.57
Best single parser (ZPar + Clusters) 91.74 (+0.22) 92.63
Best combination (3 baseline parsers) 91.90 (+0.38) 93.01
Best combination of 3 parsers:
3 baselines + 3 SF extensions 91.93 (+0.41) 92.95
Best combination of 3 parsers:
3 baselines + 3 SS extensions 91.87 (+0.35) 92.92
Best combination of 3 parsers:
3 baselines + 3 cluster extensions 91.90 (+0.38) 92.90
Table 3: Parser combinations on Penn2Malt.
Parsers LAS UAS
Best baseline (ZPar) 89.15 91.81
Best single parser (ZPar + SF) 89.33 (+0.15) 92.01
Best combination (3 baseline parsers) 89.15 (+0.00) 91.81
Best combination of 3 parsers:
3 baselines + 3 SF extensions 89.56 (+0.41)? 92.23
Best combination of 3 parsers:
3 baselines + 3 SS extensions 89.43 (+0.28) 93.12
Best combination of 3 parsers:
3 baselines + 3 cluster extensions 89.52 (+0.37)? 92.19
Table 4: Parser combinations on LTH (?: p <0.05,
?: p <0.005).
4.1 Single Parsers
We run a series of experiments testing each indi-
vidual semantic feature, also trying different learn-
ing configurations for each one. Regarding the
WordNet information, there were 2 different fea-
tures to experiment with (SF and SS). For the bit
clusters, there are different possibilities, depend-
ing on the number of bits used. For Malt and MST,
all the different lengths of bit strings were used.
Given the computational requirements and the pre-
vious results on Malt and MST, we only tested all
bits in ZPar. Tables 1 and 2 show the results.
Penn2Malt. Table 1 shows that the only signifi-
cant increase over the baseline is for ZPar with SS
and for MST with clusters.
LTH. Looking at table 2, we can say that the dif-
ferences in baseline parser performance are accen-
tuated when using the LTH treebank conversion,
as ZPar clearly outperforms the other two parsers
by more than 4 absolute points. We can see that
SF helps all parsers, although it is only significant
for MST. Bit clusters improve significantly MST,
with the highest increase across the table.
Overall, we see that the small improvements
do not confirm the previous results on Penn2Malt,
MaltParser and gold POS tags. We can also con-
clude that automatically acquired clusters are spe-
cially effective with the MST parser in both tree-
bank conversions, which suggests that the type of
semantic information has a direct relation to the
parsing algorithm. Section 4.3 will look at the de-
tails by each knowledge type.
4.2 Combinations
Subsection 4.1 presented the results of the base al-
gorithms and their extensions based on semantic
features. Sagae and Lavie (2006) report improve-
ments over the best single parser when combining
three transition-based models and one graph-based
model. The same technique was also used by the
winning team of the CoNLL 2007 Shared Task
(Hall et al, 2007), combining six transition-based
parsers. We used MaltBlender
5
, a tool for merging
the output of several dependency parsers, using the
Chu-Liu/Edmonds directed MST algorithm. After
several tests we noticed that weighted voting by
each parser?s labeled accuracy gave good results,
using it in the rest of the experiments. We trained
different types of combination:
? Base algorithms. This set includes the 3 base-
line algorithms, MaltParser, MST, and ZPar.
? Extended parsers, adding semantic informa-
tion to the baselines. We include the three
base algorithms and their semantic exten-
sions (SF, SS, and clusters). It is known (Sur-
deanu and Manning, 2010) that adding more
parsers to an ensemble usually improves ac-
curacy, as long as they add to the diver-
sity (and almost regardless of their accuracy
level). So, for the comparison to be fair, we
will compare ensembles of 3 parsers, taken
from sets of 6 parsers (3 baselines + 3 SF,
SS, and cluster extensions, respectively).
In each experiment, we took the best combina-
tion of individual parsers on the development set
for the final test. Tables 3 and 4 show the results.
Penn2Malt. Table 3 shows that the combina-
tion of the baselines, without any semantic infor-
mation, considerably improves the best baseline.
Adding semantics does not give a noticeable in-
crease with respect to combining the baselines.
LTH (table 4). Combining the 3 baselines does
not give an improvement over the best baseline, as
ZPar clearly outperforms the other parsers. How-
ever, adding the semantic parsers gives an increase
with respect to the best single parser (ZPar + SF),
which is small but significant for SF and clusters.
4.3 Analysis
In this section we analyze the data trying to under-
stand where and how semantic information helps
most. One of the obstacles of automatic parsers
is the presence of incorrect POS tags due to auto-
5
http://w3.msi.vxu.se/users/jni/blend/
652
LAS on sentences LAS on sentences
POS tags Parser LAS test set without POS errors with POS errors
Gold ZPar 90.45 91.68 89.14
Automatic ZPar 89.15 91.62 86.51
Automatic Best combination of 3 parsers: 89.56 (+0.41) 91.90 (+0.28) 87.06 (+0.55)
3 baselines + 3 SF extensions
Automatic Best combination of 3 parsers: 89.43 (+0.28) 91.95 (+0.33) 86.75 (+0.24)
3 baselines + 3 SS extensions
Automatic Best combination of 3 parsers: 89.52 (+0.37) 91.92 (+0.30) 86.96 (+0.45)
3 baselines + 3 cluster extensions
Table 5: Differences in LAS (LTH) for baseline and extended parsers with sentences having cor-
rect/incorrect POS tags (the parentheses show the difference w.r.t ZPar with automatic POS tags).
matic tagging. For example, ZPar?s LAS score on
the LTH conversion drops from 90.45% with gold
POS tags to 89.12% with automatic POS tags. We
will examine the influence of each type of seman-
tic information on sentences that contain or not
POS errors, and this will clarify whether the incre-
ments obtained when using semantic information
are useful for correcting the negative influence of
POS errors or they are orthogonal and constitute
a source of new information independent of POS
tags. With this objective in mind, we analyzed the
performance on the subset of the test corpus con-
taining the sentences which had POS errors (1,025
sentences and 27,300 tokens) and the subset where
the sentences had (automatically assigned) correct
POS tags (1,391 sentences and 29,386 tokens).
Table 5 presents the results of the best single
parser on the LTH conversion (ZPar) with gold
and automatic POS tags in the first two rows. The
LAS scores are particularized for sentences that
contain or not POS errors. The following three
rows present the enhanced (combined) parsers
that make use of semantic information. As the
combination of the three baseline parsers did not
give any improvement over the best single parser
(ZPar), we can hypothesize that the gain coming
from the parser combinations comes mostly from
the addition of semantic information. Table 5 sug-
gests that the improvements coming from Word-
Net?s semantic file (SF) are unevenly distributed
between the sentences that contain POS errors and
those that do not (an increase of 0.28 for sentences
without POS errors and 0.55 for those with er-
rors). This could mean that a big part of the in-
formation contained in SF helps to alleviate the
errors performed by the automatic POS tagger. On
the other hand, the increments are more evenly
distributed for SS and clusters, and this can be
due to the fact that the semantic information is
orthogonal to the POS, giving similar improve-
ments for sentences that contain or not POS errors.
We independently tested this fact for the individ-
ual parsers. For example, with MST and SF the
gains almost doubled for sentences with incorrect
POS tags (+0.37 with respect to +0.21 for sen-
tences with correct POS tags) while the gains of
adding clusters? information for sentences without
and with POS errors were similar (0.91 and 1.33,
repectively). This aspect deserves further inves-
tigation, as the improvements seem to be related
to both the type of semantic information and the
parsing algorithm.We did an initial exploration but
it did not give any clear indication of the types of
improvements that could be expected using each
parser and semantic data.
5 Conclusions
This work has tried to shed light on the contribu-
tion of semantic information to dependency pars-
ing. The experiments were thorough, testing two
treebank conversions and three parsing paradigms
on automatically predicted POS tags. Compared
to (Agirre et al, 2011), which used MaltParser on
the LTH conversion and gold POS tags, our results
can be seen as a negative outcome, as the improve-
ments are very small and non-significant in most
of the cases. For parser combination, WordNet
semantic file information does give a small sig-
nificant increment in the more fine-grained LTH
representation. In addition we show that the im-
provement of automatic clusters is also weak. For
the future, we think tdifferent parsers, eitherhat a
more elaborate scheme is needed for word classes,
requiring to explore different levels of generaliza-
tion in the WordNet (or alternative) hierarchies.
Acknowledgments
This research was supported by the the Basque
Government (IT344- 10, S PE11UN114), the Uni-
versity of the Basque Country (GIU09/19) and
the Spanish Ministry of Science and Innovation
(MICINN, TIN2010-20218).
653
References
Eneko Agirre, Timothy Baldwin, and David Martinez.
2008. Improving parsing and PP attachment per-
formance with sense information. In Proceedings
of ACL-08: HLT, pages 317?325, Columbus, Ohio,
June. Association for Computational Linguistics.
Eneko Agirre, Kepa Bengoetxea, Koldo Gojenola, and
Joakim Nivre. 2011. Improving dependency pars-
ing with semantic classes. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 699?703, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Daniel M. Bikel. 2004. Intricacies of collins? parsing
model. Computational Linguistics, 30(4):479?511.
Marie Candito and Djam?e Seddah. 2010. Pars-
ing word clusters. In Proceedings of the NAACL
HLT 2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 76?84, Los
Angeles, CA, USA, June. Association for Computa-
tional Linguistics.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st North
American chapter of the Association for Computa-
tional Linguistics conference, NAACL 2000, pages
132?139, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 1?8. Associ-
ation for Computational Linguistics, July.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Lin-
guistics, 29(4):589?637, December.
Sanae Fujita, Francis Bond, Stephan Oepen, and
Takaaki Tanaka. 2010. Exploiting semantic infor-
mation for hpsg parse selection. Research on Lan-
guage and Computation, 8(1):122.
Johan Hall, Jens Nilsson, Joakim Nivre, Glsen Eryigit,
Beta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? a study in multi-
lingual parser optimization. In Proceedings of the
CoNLL Shared Task EMNLP-CoNLL.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
English. In Proceedings of NODALIDA 2007, pages
105?112, Tartu, Estonia, May 25-26.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1?11, Uppsala, Sweden,
July. Association for Computational Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL-08: HLT, pages 595?603,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Andrew MacKinlay, Rebecca Dridan, Diana McCarthy,
and Timothy Baldwin. 2012. The effects of seman-
tic annotations on precision parse ranking. In First
Joint Conference on Lexical and Computational Se-
mantics (*SEM), page 228236, Montreal, Canada,
June. Association for Computational Linguistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses
in untagged text. In Proceedings of the 42nd Meet-
ing of the Association for Computational Linguistics
(ACL?04), Main Volume, pages 279?286, Barcelona,
Spain, July.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of ACL.
R. McDonald, K. Lerman, and F. Pereira. 2006. Mul-
tilingual dependency analysis with a two-stage dis-
criminative parser. In Proceedings of CoNLL 2006.
Jens Nilsson, Joakim Nivre, and Johan Hall. 2008.
Generalizing tree transformations for inductive de-
pendency parsing. In Proceedings of the 45th Con-
ference of the ACL.
Joakim Nivre, Johan Hall, Jens Nilsson, Chanev A.,
Glsen Eryiit, Sandra Kbler, Marinov S., and Edwin
Marsi. 2007. Maltparser: A language-independent
system for data-driven dependency parsing. Natural
Language Engineering.
Kenji Sagae and Andrew Gordon. 2009. Clustering
words by syntactic similarity improves dependency
parsing of predicate-argument structures. In Pro-
ceedings of the Eleventh International Conference
on Parsing Technologies.
Kenji Sagae and Alon Lavie. 2006. Parser com-
bination by reparsing. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the ACL.
Mihai Surdeanu and Christopher D. Manning. 2010.
Ensemble models for dependency parsing: Cheap
and good? In Proceedings of the North Ameri-
can Chapter of the Association for Computational
Linguistics Conference (NAACL-2010), Los Ange-
les, CA, June.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and
Michael Collins. 2009. An empirical study of semi-
supervised structured conditional models for depen-
dency parsing. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 551?560, Singapore, August. As-
sociation for Computational Linguistics.
654
Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 477?
487, Montr?eal, Canada, June. Association for Com-
putational Linguistics.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of the 8th IWPT, pages 195?
206. Association for Computational Linguistics.
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 562?
571, Honolulu, Hawaii, October. Association for
Computational Linguistics.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 188?193, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
655
Transactions of the Association for Computational Linguistics, 1 (2013) 1?12. Action Editor: Sharon Goldwater.
Submitted 11/2012; Revised 1/2013; Published 3/2013. c?2013 Association for Computational Linguistics.
Token and Type Constraints for Cross-Lingual Part-of-Speech Tagging
Oscar Ta?ckstro?m?? Dipanjan Das? Slav Petrov? Ryan McDonald? Joakim Nivre??
 Swedish Institute of Computer Science
?Department of Linguistics and Philology, Uppsala University
?Google Research, New York
oscar@sics.se
{dipanjand|slav|ryanmcd}@google.com
joakim.nivre@lingfil.uu.se
Abstract
We consider the construction of part-of-speech
taggers for resource-poor languages. Recently,
manually constructed tag dictionaries from
Wiktionary and dictionaries projected via bitext
have been used as type constraints to overcome
the scarcity of annotated data in this setting.
In this paper, we show that additional token
constraints can be projected from a resource-
rich source language to a resource-poor target
language via word-aligned bitext. We present
several models to this end; in particular a par-
tially observed conditional random field model,
where coupled token and type constraints pro-
vide a partial signal for training. Averaged
across eight previously studied Indo-European
languages, our model achieves a 25% relative
error reduction over the prior state of the art.
We further present successful results on seven
additional languages from different families,
empirically demonstrating the applicability of
coupled token and type constraints across a
diverse set of languages.
1 Introduction
Supervised part-of-speech (POS) taggers are avail-
able for more than twenty languages and achieve ac-
curacies of around 95% on in-domain data (Petrov et
al., 2012). Thanks to their efficiency and robustness,
supervised taggers are routinely employed in many
natural language processing applications, such as syn-
tactic and semantic parsing, named-entity recognition
and machine translation. Unfortunately, the resources
required to train supervised taggers are expensive to
create and unlikely to exist for the majority of written
?Work primarily carried out while at Google Research.
languages. The necessity of building NLP tools for
these resource-poor languages has been part of the
motivation for research on unsupervised learning of
POS taggers (Christodoulopoulos et al, 2010).
In this paper, we instead take a weakly supervised
approach towards this problem. Recently, learning
POS taggers with type-level tag dictionary constraints
has gained popularity. Tag dictionaries, noisily pro-
jected via word-aligned bitext, have bridged the gap
between purely unsupervised and fully supervised
taggers, resulting in an average accuracy of over 83%
on a benchmark of eight Indo-European languages
(Das and Petrov, 2011). Li et al (2012) further im-
proved upon this result by employing Wiktionary1 as
a tag dictionary source, resulting in the hitherto best
published result of almost 85% on the same setup.
Although the aforementioned weakly supervised
approaches have resulted in significant improvements
over fully unsupervised approaches, they have not
exploited the benefits of token-level cross-lingual
projection methods, which are possible with word-
aligned bitext between a target language of interest
and a resource-rich source language, such as English.
This is the setting we consider in this paper (?2).
While prior work has successfully considered both
token- and type-level projection across word-aligned
bitext for estimating the model parameters of genera-
tive tagging models (Yarowsky and Ngai, 2001; Xi
and Hwa, 2005, inter alia), a key observation under-
lying the present work is that token- and type-level
information offer different and complementary sig-
nals. On the one hand, high confidence token-level
projections offer precise constraints on a tag in a
particular context. On the other hand, manually cre-
1http://www.wiktionary.org/.
1
ated type-level dictionaries can have broad coverage
and do not suffer from word-alignment errors; they
can therefore be used to filter systematic as well as
random noise in token-level projections.
In order to reap these potential benefits, we pro-
pose a partially observed conditional random field
(CRF) model (Lafferty et al, 2001) that couples to-
ken and type constraints in order to guide learning
(?3). In essence, the model is given the freedom to
push probability mass towards hypotheses consistent
with both types of information. This approach is flex-
ible: we can use either noisy projected or manually
constructed dictionaries to generate type constraints;
furthermore, we can incorporate arbitrary features
over the input. In addition to standard (contextual)
lexical features and transition features, we observe
that adding features from a monolingual word cluster-
ing (Uszkoreit and Brants, 2008) can significantly im-
prove accuracy. While most of these features can also
be used in a generative feature-based hidden Markov
model (HMM) (Berg-Kirkpatrick et al, 2010), we
achieve the best accuracy with a globally normalized
discriminative CRF model.
To evaluate our approach, we present extensive
results on standard publicly available datasets for 15
languages: the eight Indo-European languages pre-
viously studied in this context by Das and Petrov
(2011) and Li et al (2012), and seven additional lan-
guages from different families, for which no compa-
rable study exists. In ?4 we compare various features,
constraints and model types. Our best model uses
type constraints derived from Wiktionary, together
with token constraints derived from high-confidence
word alignments. When averaged across the eight
languages studied by Das and Petrov (2011) and Li
et al (2012), we achieve an accuracy of 88.8%. This
is a 25% relative error reduction over the previous
state of the art. Averaged across all 15 languages,
our model obtains an accuracy of 84.5% compared to
78.5% obtained by a strong generative baseline. Fi-
nally, we provide an in depth analysis of the relative
contributions of the two types of constraints in ?5.
2 Coupling Token and Type Constraints
Type-level information has been amply used in
weakly supervised POS induction, either via pure
manually crafted tag dictionaries (Smith and Eisner,
2005; Ravi and Knight, 2009; Garrette and Baldridge,
2012), noisily projected tag dictionaries (Das and
Petrov, 2011) or through crowdsourced lexica, such
as Wiktionary (Li et al, 2012). At the other end
of the spectrum, there have been efforts that project
token-level information across word-aligned bitext
(Yarowsky and Ngai, 2001; Xi and Hwa, 2005). How-
ever, systems that combine both sources of informa-
tion in a single model have yet to be fully explored.
The following three subsections outline our overall
approach for coupling these two types of information
to build robust POS taggers that do not require any
direct supervision in the target language.
2.1 Token Constraints
For the majority of resource-poor languages, there
is at least some bitext with a resource-rich source
language; for simplicity, we choose English as our
source language in all experiments. It is then nat-
ural to consider using a supervised part-of-speech
tagger to predict part-of-speech tags for the English
side of the bitext. These predicted tags can subse-
quently be projected to the target side via automatic
word alignments. This approach was pioneered by
Yarowsky and Ngai (2001), who used the resulting
partial target annotation to estimate the parameters
of an HMM. However, due to the automatic nature
of the word alignments and the POS tags, there will
be significant noise in the projected tags. To conquer
this noise, they used very aggressive smoothing tech-
niques when training the HMM. Fossum and Abney
(2005) used similar token-level projections, but in-
stead combined projections from multiple source lan-
guages to filter out random projection noise as well
as the systematic noise arising from different source
language annotations and syntactic divergences.
2.2 Type Constraints
It is well known that given a tag dictionary, even if
it is incomplete, it is possible to learn accurate POS
taggers (Smith and Eisner, 2005; Goldberg et al,
2008; Ravi and Knight, 2009; Naseem et al, 2009).
While widely differing in the specific model struc-
ture and learning objective, all of these approaches
achieve excellent results. Unfortunately, they rely
on tag dictionaries extracted directly from the un-
derlying treebank data. Such dictionaries provide in
depth coverage of the test domain and also list all
2
	
 
     
   

 
   	
 
  	  	  

	

	
 



	



 

	    


Figure 1: Lattice representation of the inference search space Y(x) for an authentic sentence in Swedish (?The farming
products must be pure and must not contain any additives?), after pruning with Wiktionary type constraints. The
correct parts of speech are listed underneath each word. Bold nodes show projected token constraints y?. Underlined
text indicates incorrect tags. The coupled constraints lattice Y?(x, y?) consists of the bold nodes together with nodes for
words that are lacking token constraints; in this case, the coupled constraints lattice thus defines exactly one valid path.
inflected forms ? both of which are difficult to obtain
and unrealistic to expect for resource-poor languages.
In contrast, Das and Petrov (2011) automatically
create type-level tag dictionaries by aggregating over
projected token-level information extracted from bi-
text. To handle the noise in these automatic dictionar-
ies, they use label propagation on a similarity graph
to smooth (and also expand) the label distributions.
While their approach produces good results and is
applicable to resource-poor languages, it requires a
complex multi-stage training procedure including the
construction of a large distributional similarity graph.
Recently, Li et al (2012) presented a simple and
viable alternative: crowdsourced dictionaries from
Wiktionary. While noisy and sparse in nature, Wik-
tionary dictionaries are available for 170 languages.2
Furthermore, their quality and coverage is growing
continuously (Li et al, 2012). By incorporating type
constraints from Wiktionary into the feature-based
HMM of Berg-Kirkpatrick et al (2010), Li et al were
able to obtain the best published results in this setting,
surpassing the results of Das and Petrov (2011) on
eight Indo-European languages.
2.3 Coupled Constraints
Rather than relying exclusively on either token or
type constraints, we propose to complement the one
with the other during training. For each sentence in
our training set, a partially constrained lattice of tag
sequences is constructed as follows:
2http://meta.wikimedia.org/wiki/
Wiktionary ? October 2012.
1. For each token whose type is not in the tag dic-
tionary, we allow the entire tag set.
2. For each token whose type is in the tag dictio-
nary, we prune all tags not licensed by the dictio-
nary and mark the token as dictionary-pruned.
3. For each token that has a tag projected via a
high-confidence bidirectional word alignment:
if the projected tag is still present in the lattice,
then we prune every tag but the projected tag for
that token; if the projected tag is not present in
the lattice, which can only happen for dictionary-
pruned tokens, then we ignore the projected tag.
Figure 1 provides a running example. The lattice
shows tags permitted after constraining the words
to tags licensed by the dictionary (up until Step 2
from above). There is only a single token ?Jordbruk-
sprodukterna? (?the farming products?) not in the
dictionary; in this case the lattice permits the full
set of tags. With token-level projections (Step 3;
nodes with bold border in Figure 1), the lattice can
be further pruned. In most cases, the projected tag
is both correct and is in the dictionary-pruned lattice.
We thus successfully disambiguate such tokens and
shrink the search space substantially.
There are two cases we highlight in order to show
where our model can break. First, for the token
?Jordbruksprodukterna?, the erroneously projected
tag ADJ will eliminate all other tags from the lattice,
including the correct tag NOUN. Second, the token
?na?gra? (?any?) has a single dictionary entry PRON
and is missing the correct tag DET. In the case where
3
DET is the projected tag, we will not add it to the
lattice and simply ignore it. This is because we hy-
pothesize that the tag dictionary can be trusted more
than the tags projected via noisy word alignments. As
we will see in ?4, taking the union of tags performs
worse, which supports this hypothesis.
For generative models, such as HMMs (?3.1), we
need to define only one lattice. For our best gen-
erative model this is the coupled token- and type-
constrained lattice.3 At prediction time, in both the
discriminative and the generative cases, we find the
most likely label sequence using Viterbi decoding.
For discriminative models, such as CRFs (?3.2),
we need to define two lattices: one that the model
moves probability mass towards and another one
defining the overall search space (or partition func-
tion). In traditional supervised learning without a
dictionary, the former is a trivial lattice containing
the gold standard tag sequence and the latter is the
set of all possible tag sequences spanning the tokens.
With our best model, we will move mass towards the
coupled token- and type-constrained lattice, such that
the model can freely distribute mass across all paths
consistent with these constraints. The lattice defining
the partition function will be the full set of possible
tag sequences when no dictionary is used; when a
dictionary is used it will consist of all dictionary-
pruned tag sequences (sans Step 3 above; the full set
of possibilities shown in Figure 1 for our running
example).
Figures 2 and 3 provide statistics regarding the
supervision coverage and remaining ambiguity. Fig-
ure 2 shows that more than two thirds of all tokens in
our training data are in Wiktionary. However, there is
considerable variation between languages: Spanish
has the highest coverage with over 90%, while Turk-
ish, an agglutinative language with a vast number
of word forms, has less than 50% coverage. Fig-
ure 3 shows that there is substantial uncertainty left
after pruning with Wiktionary, since tokens are rarely
fully disambiguated: 1.3 tags per token are allowed
on average for types in Wiktionary.
Figure 2 further shows that high-confidence align-
ments are available for about half of the tokens for
most languages (Japanese is a notable exception with
3Other training methods exist as well, for example, con-
trastive estimation (Smith and Eisner, 2005).
0
25
50
75
100
avg bg cs da de el es fr it ja nl pt sl sv tr zh
Pe
rc
en
t o
f to
ke
ns
 c
ov
er
ed
Token
coverage Wiktionary Projected Projected+Filtered
Figure 2: Wiktionary and projection dictionary coverage.
Shown is the percentage of tokens in the target side of the
bitext that are covered by Wiktionary, that have a projected
tag, and that have a projected tag after intersecting the two.
0.0
0.5
1.0
1.5
avg bg cs da de el es fr it ja nl pt sl sv tr zh
Nu
mb
er 
of 
tag
s p
er 
tok
en
Figure 3: Average number of licensed tags per token on
the target side of the bitext, for types in Wiktionary.
less than 30% of the tokens covered). Intersecting the
Wiktionary tags and the projected tags (Step 2 and 3
above) filters out some of the potentially erroneous
tags, but preserves the majority of the projected tags;
the remaining, presumably more accurate projected
tags cover almost half of all tokens, greatly reducing
the search space that the learner needs to explore.
3 Models with Coupled Constraints
We now formally present how we couple token and
type constraints and how we use these coupled con-
straints to train probabilistic tagging models. Let
x = (x1x2 . . . x|x|) ? X denote a sentence, where
each token xi ? V is an instance of a word type from
the vocabulary V and let y = (y1y2 . . . y|x|) ? Y de-
note a tag sequence, where yi ? T is the tag assigned
to token xi and T denotes the set of all possible part-
of-speech tags. We denote the lattice of all admissible
tag sequences for the sentence x by Y(x). This is the
4
inference search space in which the tagger operates.
As we shall see, it is crucial to constrain the size of
this lattice in order to simplify learning when only
incomplete supervision is available.
A tag dictionary maps a word type xj ? V to
a set of admissible tags T (xj) ? T . For word
types not in the dictionary we allow the full set of
tags T (while possible, in this paper we do not at-
tempt to distinguish closed-class versus open-class
words). When provided with a tag dictionary, the
lattice of admissible tag sequences for a sentence x
is Y(x) = T (x1) ? T (x2) ? . . . ? T (x|x|). When
no tag dictionary is available, we simply have the full
lattice Y(x) = T |x|.
Let y? = (y?1y?2 . . . y?|x|) be the projected tags for
the sentence x. Note that {y?i} = ? for tokens without
a projected tag. Next, we define a piecewise operator
_ that couples y? and Y(x) with respect to every
sentence index, which results in a token- and type-
constrained lattice. The operator behaves as follows,
coherent with the high level description in ?2.3:
T? (xi, y?i) = y?i _ T (xi) =
{
{y?i} if y?i ? T (xi)
T (xi) otherwise .
We denote the token- and type-constrained lattice as
Y?(x, y?) = T? (x1, y?1)?T? (x2, y?2)?. . .?T? (x|x|, y?|x|).
Note that when token-level projections are not used,
the dictionary-pruned lattice and the lattice with cou-
pled constraints are identical, that is Y?(x, y?) = Y(x).
3.1 HMMs with Coupled Constraints
A first-order hidden Markov model (HMM) specifies
the joint distribution of a sentence x ? X and a
tag-sequence y ? Y(x) as:
p?(x, y) =
|x|?
i=1
p?(xi | yi)? ?? ?
emission
p?(yi | yi?1)? ?? ?
transition
.
We follow the recent trend of using a log-linear
parametrization of the emission and the transition
distributions, instead of a multinomial parametriza-
tion (Chen, 2003). This allows model parameters ?
to be shared across categorical events, which has
been shown to give superior performance (Berg-
Kirkpatrick et al, 2010). The categorical emission
and transition events are represented by feature vec-
tors ?(xi, yi) and ?(yi, yi?1). Each element of the
parameter vector ? corresponds to a particular fea-
ture; the component log-linear distributions are:
p?(xi | yi) =
exp
(
?>?(xi, yi)
)
?
x?i?V exp (?
>?(x?i, yi))
,
and
p?(yi | yi?1) =
exp
(
?>?(yi, yi?1)
)
?
y?i?T exp (?
>?(y?i, yi?1))
.
In maximum-likelihood estimation of the parameters,
we seek to maximize the likelihood of the observed
parts of the data. For this we need the joint marginal
distribution p?(x, Y?(x, y?)) of a sentence x, and its
coupled constraints lattice Y?(x, y?), which is obtained
by marginalizing over all consistent outputs:
p?(x, Y?(x, y?)) =
?
y?Y?(x,y?)
p?(x, y) .
If there are no projections and no tag dictionary, then
Y?(x, y?) = T |x|, and thus p?(x, Y?(x, y?)) = p?(x),
which reduces to fully unsupervised learning. The
`2-regularized marginal joint log-likelihood of the
constrained training data D = {(x(i), y?(i))}ni=1 is:
L(?;D) =
n?
i=1
log p?(x(i), Y?(x(i), y?(i)))?? ???22 .
(1)
We follow Berg-Kirkpatrick et al (2010) and take a
direct gradient approach for optimizing Eq. 1 with
L-BFGS (Liu and Nocedal, 1989). We set ? = 1 and
run 100 iterations of L-BFGS. One could also em-
ploy the Expectation-Maximization (EM) algorithm
(Dempster et al, 1977) to optimize this objective, al-
though the relative merits of EM versus direct gradi-
ent training for these models is still a topic of debate
(Berg-Kirkpatrick et al, 2010; Li et al, 2012).4 Note
that since the marginal likelihood is non-concave, we
are only guaranteed to find a local maximum of Eq. 1.
After estimating the model parameters ?, the tag-
sequence y? ? Y(x) for a sentence x ? X is pre-
dicted by choosing the one with maximal joint prob-
ability:
y? ? arg max
y?Y(x)
p?(x, y) .
4We trained the HMM with EM as well, but achieved better
results with direct gradient training and hence omit those results.
5
3.2 CRFs with Coupled Constraints
Whereas an HMM models the joint probability of
the input x ? X and output y ? Y(x), using locally
normalized component distributions, a conditional
random field (CRF) instead models the probability of
the output conditioned on the input as a globally nor-
malized log-linear distribution (Lafferty et al, 2001):
p?(y | x) =
exp
(
?>?(x, y)
)
?
y??Y(x) exp (?>?(x, y?))
,
where ? is a parameter vector. As for the HMM,
Y(x) is not necessarily the full space of possible
tag-sequences; specifically, for us, it is the dictionary-
pruned lattice without the token constraints.
With a first-order Markov assumption, the feature
function factors as:
?(x, y) =
|x|?
i=1
?(x, yi, yi?1) .
This model is more powerful than the HMM in that
it can use richer feature definitions, such as joint in-
put/transition features and features over a wider input
context. We model a marginal conditional probabil-
ity, given by the total probability of all tag sequences
consistent with the lattice Y?(x, y?):
p?(Y?(x, y?) | x) =
?
y?Y?(x,y?)
p?(y | x) .
The parameters of this constrained CRF are estimated
by maximizing the `2-regularized marginal condi-
tional log-likelihood of the constrained data (Riezler
et al, 2002):
L(?;D) =
n?
i=1
log p?(Y?(x(i), y?(i)) | x(i))? ????22 .
(2)
As with Eq. 1, we maximize Eq. 2 with 100 itera-
tions of L-BFGS and set ? = 1. In contrast to the
HMM, after estimating the model parameters ?, the
tag-sequence y? ? Y(x) for a sentence x ? X is
chosen as the sequence with the maximal conditional
probability:
y? ? arg max
y?Y(x)
p?(y | x) .
4 Empirical Study
We now present a detailed empirical study of the mod-
els proposed in the previous sections. In addition to
comparing with the state of the art in Das and Petrov
(2011) and Li et al (2012), we present models with
several combinations of token and type constraints,
additional features incorporating word clusters. Both
generative and discriminative models are explored.
4.1 Experimental Setup
Before delving into the experimental details, we
present our setup and datasets.
Languages. We evaluate on eight target languages
used in previous work (Das and Petrov, 2011; Li et
al., 2012) and on seven additional languages (see Ta-
ble 1). While the former eight languages all belong to
the Indo-European family, we broaden the coverage
to language families more distant from the source
language (for example, Chinese, Japanese and Turk-
ish). We use the treebanks from the CoNLL shared
tasks on dependency parsing (Buchholz and Marsi,
2006; Nivre et al, 2007) for evaluation.5 The two-
letter abbreviations from the ISO 639-1 standard are
used when referring to these languages in tables and
figures.
Tagset. In all cases, we map the language-specific
POS tags to universal POS tags using the mapping
of Petrov et al (2012).6 Since we use indirect super-
vision via projected tags or Wiktionary, the model
states induced by all models correspond directly to
POS tags, enabling us to compute tagging accuracy
without a greedy 1-to-1 or many-to-1 mapping.
Bitext. For all experiments, we use English as the
source language. Depending on availability, there
are between 1M and 5M parallel sentences for each
language. The majority of the parallel data is gath-
ered automatically from the web using the method
of Uszkoreit et al (2010). We further include data
from Europarl (Koehn, 2005) and from the UN par-
allel corpus (UN, 2006), for languages covered by
these corpora. The English side of the bitext is
POS tagged with a standard supervised CRF tagger,
trained on the Penn Treebank (Marcus et al, 1993),
with tags mapped to universal tags. The parallel sen-
5For French we use the treebank of Abeille? et al (2003).
6We use version 1.03 of the mappings available at http:
//code.google.com/p/universal-pos-tags/.
6
tences are word aligned with the aligner of DeNero
and Macherey (2011). Intersected high-confidence
alignments (confidence >0.95) are extracted and ag-
gregated into projected type-level dictionaries. For
purely practical reasons, the training data with token-
level projections is created by randomly sampling
target-side sentences with a total of 500K tokens.
Wiktionary. We use a snapshot of the Wiktionary
word definitions, and follow the heuristics of Li et
al. (2012) for creating the Wiktionary dictionary by
mapping the Wiktionary tags to universal POS tags.7
Features. For all models, we use only an identity
feature for tag-pair transitions. We use five features
that couple the current tag and the observed word
(analogous to the emission in an HMM): word iden-
tity, suffixes of up to length 3, and three indicator
features that fire when the word starts with a capital
letter, contains a hyphen or contains a digit. These are
the same features as those used by Das and Petrov
(2011). Finally, for some models we add a word
cluster feature that couples the current tag and the
word cluster identity of the word. These (monolin-
gual) word clusters are induced with the exchange
algorithm (Uszkoreit and Brants, 2008). We set the
number of clusters to 256 across all languages, as this
has previously been shown to produce robust results
for similar tasks (Turian et al, 2010; Ta?ckstro?m et
al., 2012). The clusters for each language are learned
on a large monolingual newswire corpus.
4.2 Models with Type Constraints
To examine the sole effect of type constraints, we
experiment with the HMM, drawing constraints from
three different dictionaries. Table 1 compares the per-
formance of our models with the best results of Das
and Petrov (2011, D&P) and Li et al (2012, LG&T).
As in previous work, training is done exclusively on
the training portion of each treebank, stripped of any
manual linguistic annotation.
We first use all of our parallel data to generate
projected tag dictionaries: the English POS tags are
projected across word alignments and aggregated to
tag distributions for each word type. As in Das and
Petrov (2011), the distributions are then filtered with
a threshold of 0.2 to remove noisy tags and to create
7The definitions were downloaded on August 31, 2012 from
http://toolserver.org/?enwikt/definitions/.
This snapshot is more recent than that used by Li et al
Prior work HMM with type constraints
Lang. D&P LG&T YHMMproj. YHMMwik. YHMMunion YHMMunion +C
bg ? ? 84.2 68.1 87.2 87.9
cs ? ? 75.4 70.2 75.4 79.2
da 83.2 83.3 87.7 82.0 78.4 89.5
de 82.8 85.8 86.6 85.1 80.0 88.3
el 82.5 79.2 83.3 83.8 86.0 83.2
es 84.2 86.4 83.9 83.7 88.3 87.3
fr ? ? 88.4 75.7 75.6 86.6
it 86.8 86.5 89.0 85.4 89.9 90.6
ja ? ? 45.2 76.9 74.4 73.7
nl 79.5 86.3 81.7 79.1 83.8 82.7
pt 87.9 84.5 86.7 79.0 83.8 90.4
sl ? ? 78.7 64.8 82.8 83.4
sv 80.5 86.1 80.6 85.9 85.9 86.7
tr ? ? 66.2 44.1 65.1 65.7
zh ? ? 59.2 73.9 63.2 73.0
avg (8) 83.4 84.8 84.9 83.0 84.5 87.3
avg ? ? 78.5 75.9 80.0 83.2
Table 1: Tagging accuracies for type-constrained HMM
models. D&P is the ?With LP? model in Table 2 of
Das and Petrov (2011), while LG&T is the ?SHMM-ME?
model in Table 2 of Li et al (2012). YHMMproj. , YHMMwik. and
YHMMunion are HMMs trained solely with type constraints
derived from the projected dictionary, Wiktionary and
the union of these dictionaries, respectively. YHMMunion +C is
equivalent to YHMMunion with additional cluster features. All
models are trained on the treebank of each language,
stripped of gold labels. Results are averaged over the
8 languages from Das and Petrov (2011), denoted avg (8),
as well as over the full set of 15 languages, denoted avg.
an unweighted tag dictionary. We call this model
YHMMproj. ; its average accuracy of 84.9% on the eight
languages is higher than the 83.4% of D&P and on
par with LG&T (84.8%).8 Our next model (YHMMwik. )
simply draws type constraints from Wiktionary. It
slightly underperforms LG&T (83.0%), presumably
because they used a second-order HMM. As a simple
extension to these two models, we take the union
of the projected dictionary and Wiktionary to con-
strain an HMM, which we name YHMMunion . This model
performs a little worse on the eight Indo-European
languages (84.5), but gives an improvement over the
projected dictionary when evaluated across all 15
languages (80.0% vs. 78.5%).
8Our model corresponds to the weaker, ?No LP? projection
of Das and Petrov (2011). We found that label propagation was
only beneficial when small amounts of bitext were available.
7
Token constraints HMM with coupled constraints CRF with coupled constraints
Lang. YHMMunion +C+L y?HMM+C+L y?CRF+C+L Y?HMMproj. +C+L Y?HMMwik. +C+L Y?HMMunion +C+L Y?CRFproj. +C+L Y?CRFwik. +C+L Y?CRFunion+C+L
bg 87.7 77.9 84.1 84.5 83.9 86.7 86.0 87.8 85.4
cs 78.3 65.4 74.9 74.8 81.1 76.9 74.7 80.3** 75.0
da 87.3 80.9 85.1 87.2 85.6 88.1 85.5 88.2* 86.0
de 87.7 81.4 83.3 85.0 89.3 86.7 84.4 90.5** 85.5
el 85.9 81.1 77.8 80.1 87.0 83.9 79.6 89.5** 79.7
es 89.1** 84.1 85.5 83.7 85.9 88.0 85.7 87.1 86.0
fr 88.4** 83.5 84.7 85.9 86.4 87.4 84.9 87.2 85.6
it 89.6 85.2 88.5 88.7 87.6 89.8 88.3 89.3 89.4
ja 72.8 47.6 54.2 43.2 76.1 70.5 44.9 81.0** 68.0
nl 83.1 78.4 82.4 82.3 84.2 83.2 83.1 85.9** 83.2
pt 89.1 84.7 87.0 86.6 88.7 88.0 87.9 91.0** 88.3
sl 82.4 69.8 78.2 78.5 81.8 80.1 79.7 82.3 80.0
sv 86.1 80.1 84.2 82.3 87.9 86.9 84.4 88.9** 85.5
tr 62.4 58.1 64.5 64.6 61.8 64.8 65.0 64.1** 65.2
zh 72.6 52.7 39.5 56.0 74.1 73.3 59.7 74.4** 73.4
avg (8) 87.2 82.0 84.2 84.5 87.0 86.8 84.9 88.8 85.4
avg 82.8 74.1 76.9 77.6 82.8 82.3 78.2 84.5 81.1
Table 2: Tagging accuracies for models with token constraints and coupled token and type constraints. All models use
cluster features (. . . +C) and are trained on large training sets each containing 500k tokens with (partial) token-level
projections (. . . +L). The best type-constrained model, trained on the larger datasets, YHMMunion +C+L, is included for
comparison. The remaining columns correspond to HMM and CRF models trained only with token constraints (y? . . .)
and with coupled token and type constraints (Y? . . .). The latter are trained using the projected dictionary (?proj.),
Wiktionary (?wik.) and the union of these dictionaries (?union), respectively. The search spaces of the models trained with
coupled constraints (Y? . . .) are each pruned with the respective tag dictionary used to derive the coupled constraints.
The observed difference between Y?CRFwik. +C+L and YHMMunion +C+L is statistically significant at p < 0.01 (**) and p < 0.015(*) according to a paired bootstrap test (Efron and Tibshirani, 1993). Significance was not assessed for avg or avg (8).
We next add monolingual cluster features to
the model with the union dictionary. This model,
YHMMunion +C, significantly outperforms all other type-
constrained models, demonstrating the utility of
word-cluster features.9 For further exploration, we
train the same model on the datasets containing 500K
tokens sampled from the target side of the parallel
data (YHMMunion +C+L); this is done to explore the effects
of large data during training. We find that training
on these datasets result in an average accuracy of
87.2% which is comparable to the 87.3% reported
for YHMMunion +C in Table 1. This shows that the different
source domain and amount of training data does not
influence the performance of the HMM significantly.
Finally, we train CRF models where we treat type
constraints as a partially observed lattice and use the
full unpruned lattice for computing the partition func-
9These are monolingual clusters. Bilingual clusters as intro-
duced in Ta?ckstro?m et al (2012) might bring additional benefits.
tion (?3.2). Due to space considerations, the results
of these experiments are not shown in table 1. We ob-
serve similar trends in these results, but on average,
accuracies are much lower compared to the type-
constrained HMM models; the CRF model with the
union dictionary along with cluster features achieves
an average accuracy of 79.3% when trained on same
data. This result is not unsurprising. First, the CRF?s
search space is fully unconstrained. Second, the dic-
tionary only provides a weak set of observation con-
straints, which do not provide sufficient information
to successfully train a discriminative model. How-
ever, as we will observe next, coupling the dictionary
constraints with token-level information solves this
problem.
4.3 Models with Token and Type Constraints
We now proceed to add token-level information,
focusing in particular on coupled token and type
8
constraints. Since it is not possible to generate
projected token constraints for our monolingual
treebanks, we train all models in this subsection
on the 500K-tokens datasets sampled from the bi-
text. As a baseline, we first train HMM and CRF
models that use only projected token constraints
(y?HMM+C+L and y?CRF+C+L). As shown in Table 2,
these models underperform the best type-level model
(YHMMunion +C+L),10 which confirms that projected to-
ken constraints are not reliable on their own. This
is in line with similar projection models previously
examined by Das and Petrov (2011).
We then study models with coupled token and type
constraints. These models use the same three dictio-
naries as used in ?4.2, but additionally couple the
derived type constraints with projected token con-
straints; see the caption of Table 2 for a list of these
models. Note that since we only allow projected tags
that are licensed by the dictionary (Step 3 of the trans-
fer, ?2.3), the actual token constraints used in these
models vary with the different dictionaries.
From Table 2, we see that coupled constraints are
superior to token constraints, when used both with
the HMM and the CRF. However, for the HMM, cou-
pled constraints do not provide any benefit over type
constraints alone, in particular when the projected
dictionary or the union dictionary is used to derive the
coupled constraints (Y?HMMproj. +C+L and Y?HMMunion +C+L).
We hypothesize that this is because these dictionar-
ies (in particular the former) have the same bias as
the token-level tag projections, so that the dictionary
is unable to correct the systematic errors in the pro-
jections (see ?2.1). Since the token constraints are
stronger than the type constraints in the coupled mod-
els, this bias may have a substantial impact. With
the Wiktionary dictionary, the difference between the
type-constrained and the coupled-constrained HMM
is negligible: YHMMunion +C+L and Y?HMMwik. +C+L both av-
erage at an accuracy of 82.8%.
The CRF model, on the other hand, is able to take
advantage of the complementary information in the
coupled constraints, provided that the dictionary is
able to filter out the systematic token-level errors.
With a dictionary derived from Wiktionary and pro-
jected token-level constraints, Y?CRFwik. +C+L performs
10To make the comparison fair vis-a-vis potential divergences
in training domains, we compare to the best type-constrained
model trained on the same 500K tokens training sets.
0 1 2 3
0
25
50
75
100
0 1 10 100 0 1 10 100 0 1 10 100 0 1 10 100
Number of token?level projections
Ta
gg
ing
 ac
cur
ac
y
Number of tags listed in Wiktionary
Figure 4: Relative influence of token and type constraints
on tagging accuracy in the Y?CRFwik. +C+L model. Word typesare categorized according to a) their number of Wiktionary
tags (0,1,2 or 3+ tags, with 0 representing no Wiktionary
entry; top-axis) and b) the number of times they are token-
constrained in the training set (divided into buckets of
0, 1-9, 10-99 and 100+ occurrences; x-axis). The boxes
summarize the accuracy distributions across languages
for each word type category as defined by a) and b). The
horizontal line in each box marks the median accuracy,
the top and bottom mark the first and third quantile, re-
spectively, while the whiskers mark the minimum and
maximum values of the accuracy distribution.
better than all the remaining models, with an average
accuracy of 88.8% across the eight Indo-European
languages available to D&P and LG&T. Averaged
over all 15 languages, its accuracy is 84.5%.
5 Further Analysis
In this section we provide a detailed analysis of the
impact of token versus type constraints and we study
the pruning and filtering mistakes resulting from in-
complete Wiktionary entries in detail. This analysis
is based on the training portion of each treebank.
5.1 Influence of Token and Type Constraints
The empirical success of the model trained with cou-
pled token and type constraints confirms that these
constraints indeed provide complementary signals.
Figure 4 provides a more detailed view of the rela-
tive benefits of each type of constraint. We observe
several interesting trends.
First, word types that occur with more token con-
straints during training are generally tagged more
accurately, regardless of whether these types occur
9
90.0
92.5
95.0
97.5
100.0
0 50 100 150 200 250
Number of corrected Wiktionary entries
Pr
un
ing
 ac
cur
ac
y
Figure 5: Average pruning accuracy (line) across lan-
guages (dots) as a function of the number of hypotheti-
cally corrected Wiktionary entries for the k most frequent
word types. For example, position 100 on the x-axis cor-
responds to manually correcting the entries for the 100
most frequent types, while position 0 corresponds to ex-
perimental conditions.
in Wiktionary. The most common scenario is for a
word type to have exactly one tag in Wiktionary and
to occur with this projected tag over 100 times in
the training set (facet 1, rightmost box). These com-
mon word types are typically tagged very accurately
across all languages.
Second, the word types that are ambiguous accord-
ing to Wiktionary (facets 2 and 3) are predominantly
frequent ones. The accuracy is typically lower for
these words compared to the unambiguous words.
However, as the number of projected token con-
straints is increased from zero to 100+ observations,
the ambiguous words are effectively disambiguated
by the token constraints. This shows the advantage
of intersecting token and type constraints.
Finally, projection generally helps for words that
are not in Wiktionary, although the accuracy for these
words never reach the accuracy of the words with
only one tag in Wiktionary. Interestingly, words that
occur with a projected tag constraint less than 100
times are tagged more accurately for types not in the
dictionary compared to ambiguous word types with
the same number of projected constraints. A possible
explanation for this is that the ambiguous words are
inherently more difficult to predict and that most of
the words that are not in Wiktionary are less common
words that tend to also be less ambiguous.
zh
tr
sv
sl
pt
nl
jait
fr
es
el
de
da
cs
bg
avg
0 25 50 75 100
Proportion of pruning errors
PRON
NOUN
DET
ADP
PRT
ADV
NUM
CONJ
ADJ
VERB
X
.
Figure 6: Prevalence of pruning mistakes per POS tag,
when pruning the inference search space with Wiktionary.
5.2 Wiktionary Pruning Mistakes
The error analysis by Li et al (2012) showed that the
tags licensed by Wiktionary are often valid. When
using Wiktionary to prune the search space of our
constrained models and to filter token-level projec-
tions, it is also important that correct tags are not
mistakenly pruned because they are missing from
Wiktionary. While the accuracy of filtering is more
difficult to study, due to the lack of a gold standard
tagging of the bitext, Figure 5 (position 0 on the x-
axis) shows that search space pruning errors are not
a major issue for most languages; on average the
pruning accuracy is almost 95%. However, for some
languages such as Chinese and Czech the correct tag
is pruned from the search space for nearly 10% of all
tokens. When using Wiktionary as a pruner, the upper
bound on accuracy for these languages is therefore
only around 90%. However, Figure 5 also shows that
with some manual effort we might be able to remedy
many of these errors. For example, by adding miss-
ing valid tags to the 250 most common word types in
the worst language, the minimum pruning accuracy
would rise above 95% from below 90%. If the same
was to be done for all of the studied languages, the
mean pruning accuracy would reach over 97%.
Figure 6 breaks down pruning errors resulting from
incorrect or incomplete Wiktionary entries across
the correct POS tags. From this we observe that,
for many languages, the pruning errors are highly
skewed towards specific tags. For example, for Czech
over 80% of the pruning errors are caused by mistak-
enly pruned pronouns.
10
6 Conclusions
We considered the problem of constructing multilin-
gual POS taggers for resource-poor languages. To
this end, we explored a number of different models
that combine token constraints with type constraints
from different sources. The best results were ob-
tained with a partially observed CRF model that ef-
fectively integrates these complementary constraints.
In an extensive empirical study, we showed that this
approach substantially improves on the state of the
art in this context. Our best model significantly out-
performed the second-best model on 10 out of 15
evaluated languages, when trained on identical data
sets, with an insignificant difference on 3 languages.
Compared to the prior state of the art (Li et al, 2012),
we observed a relative reduction in error by 25%,
averaged over the eight languages common to our
studies.
Acknowledgments
We thank Alexander Rush for help with the hyper-
graph framework that was used to implement our
models and Klaus Macherey for help with the bi-
text extraction. This work benefited from many dis-
cussions with Yoav Goldberg, Keith Hall, Kuzman
Ganchev and Hao Zhang. We also thank the editor
and the three anonymous reviewers for their valuable
feedback. The first author is grateful for the financial
support from the Swedish National Graduate School
of Language Technology (GSLT).
References
Anne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel.
2003. Building a Treebank for French. In A. Abeille?,
editor, Treebanks: Building and Using Parsed Corpora,
chapter 10. Kluwer.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?, John
DeNero, and Dan Klein. 2010. Painless unsupervised
learning with features. In Proceedings of NAACL-HLT.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL.
Stanley F Chen. 2003. Conditional and joint models for
grapheme-to-phoneme conversion. In Proceedings of
Eurospeech.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsupervised
POS induction: How far have we come? In Proceed-
ings of EMNLP.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of ACL-HLT.
Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin.
1977. Maximum likelihood from incomplete data via
the EM algorithm. Journal of the Royal Statistical
Society, Series B, 39.
John DeNero and Klaus Macherey. 2011. Model-based
aligner combination using dual decomposition. In Pro-
ceedings of ACL-HLT.
Brad Efron and Robert J. Tibshirani. 1993. An Introduc-
tion to the Bootstrap. Chapman & Hall, New York, NY,
USA.
Victoria Fossum and Steven Abney. 2005. Automatically
inducing a part-of-speech tagger by projecting from
multiple source languages across aligned corpora. In
Proceedings of IJCNLP.
Dan Garrette and Jason Baldridge. 2012. Type-supervised
hidden markov models for part-of-speech tagging with
incomplete tag dictionaries. In Proceedings of EMNLP-
CoNLL.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proceedings of ACL-HLT.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In
Proceedings of ICML.
Shen Li, Joa?o Grac?a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In Proceedings of
EMNLP-CoNLL.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated corpus
of English: the Penn treebank. Computational Linguis-
tics, 19(2).
Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, and
Regina Barzilay. 2009. Multilingual part-of-speech
tagging: Two unsupervised approaches. JAIR, 36.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of EMNLP-CoNLL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC.
Sujith Ravi and Kevin Knight. 2009. Minimized models
for unsupervised part-of-speech tagging. In Proceed-
ings of ACL-IJCNLP.
11
Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard
Crouch, John T. Maxwell, III, and Mark Johnson. 2002.
Parsing the wall street journal using a lexical-functional
grammar and discriminative estimation techniques. In
Proceedings of ACL.
Noah Smith and Jason Eisner. 2005. Contrastive estima-
tion: Training log-linear models on unlabeled data. In
Proceedings of ACL.
Oscar Ta?ckstro?m, Ryan McDonald, and Jakob Uszkoreit.
2012. Cross-lingual word clusters for direct transfer of
linguistic structure. In Proceedings of NAACL-HLT.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceedings
of ACL.
UN. 2006. ODS UN parallel corpus.
Jakob Uszkoreit and Thorsten Brants. 2008. Distributed
word clustering for large scale class-based language
modeling in machine translation. In Proceedings of
ACL-HLT.
Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe
Dubiner. 2010. Large scale parallel document mining
for machine translation. In Proceedings of COLING.
Chenhai Xi and Rebecca Hwa. 2005. A backoff model
for bootstrapping resources for non-English languages.
In Proceedings of HLT-EMNLP.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings of
NAACL.
12
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 159?177
Manchester, August 2008
The CoNLL-2008 Shared Task on
Joint Parsing of Syntactic and Semantic Dependencies
Mihai Surdeanu
?,?
Richard Johansson
?
Adam Meyers
?
Llu??s M
`
arquez
??
Joakim Nivre
??,??
?: Barcelona Media Innovation Center, mihai.surdeanu@barcelonamedia.org
?: Yahoo! Research Barcelona, mihais@yahoo-inc.com
?: Lund University, richard@cs.lth.se
?: New York University, meyers@cs.nyu.edu
??: Technical University of Catalonia, lluism@lsi.upc.edu
??: V?axj?o University, joakim.nivre@vxu.se
??: Uppsala University, joakim.nivre@lingfil.uu.se
Abstract
The Conference on Computational Natu-
ral Language Learning is accompanied ev-
ery year by a shared task whose purpose
is to promote natural language processing
applications and evaluate them in a stan-
dard setting. In 2008 the shared task was
dedicated to the joint parsing of syntactic
and semantic dependencies. This shared
task not only unifies the shared tasks of
the previous four years under a unique
dependency-based formalism, but also ex-
tends them significantly: this year?s syn-
tactic dependencies include more informa-
tion such as named-entity boundaries; the
semantic dependencies model roles of both
verbal and nominal predicates. In this pa-
per, we define the shared task and describe
how the data sets were created. Further-
more, we report and analyze the results and
describe the approaches of the participat-
ing systems.
1 Introduction
In 2004 and 2005 the shared tasks of the Confer-
ence on Computational Natural Language Learn-
ing (CoNLL) were dedicated to semantic role la-
beling (SRL), in a monolingual setting (English).
In 2006 and 2007 the shared tasks were devoted to
the parsing of syntactic dependencies, using cor-
pora from up to 13 languages. The CoNLL-2008
shared task
1
proposes a unified dependency-based
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1
http://www.yr-bcn.es/conll2008
formalism, which models both syntactic depen-
dencies and semantic roles. Using this formalism,
this shared task merges both the task of syntactic
dependency parsing and the task of identifying se-
mantic arguments and labeling them with semantic
roles. Conceptually, the 2008 shared task can be
divided into three subtasks: (i) parsing of syntactic
dependencies, (ii) identification and disambigua-
tion of semantic predicates, and (iii) identification
of arguments and assignment of semantic roles for
each predicate. Several objectives were addressed
in this shared task:
? SRL is performed and evaluated using a
dependency-based representation for both
syntactic and semantic dependencies. While
SRL on top of a dependency treebank has
been addressed before (Hacioglu, 2004),
our approach has several novelties: (i) our
constituent-to-dependency conversion strat-
egy transforms all annotated semantic argu-
ments in PropBank and NomBank not just a
subset; (ii) we address propositions centered
around both verbal (PropBank) and nominal
(NomBank) predicates.
? Based on the observation that a richer set
of syntactic dependencies improves seman-
tic processing (Johansson and Nugues, 2007),
the syntactic dependencies modeled are more
complex than the ones used in the previous
CoNLL shared tasks. For example, we now
include apposition links, dependencies de-
rived from named entity (NE) structures, and
better modeling of long-distance grammatical
relations.
? A practical framework is provided for the
joint learning of syntactic and semantic de-
pendencies.
159
Given the complexity of this shared task, we
limited the evaluation to a monolingual, English-
only setting. The evaluation is separated into two
different challenges: a closed challenge, where
systems have to be trained strictly with informa-
tion contained in the given training corpus, and an
open challenge, where systems can be developed
making use of any kind of external tools and re-
sources. The participants could submit results in
either one or both challenges.
This paper is organized as follows. Section 2
defines the task, including the format of the data,
the evaluation metrics, and the two challenges.
Section 3 introduces the corpora used and our
constituent-to-dependency conversion procedure.
Section 4 summarizes the results of the submit-
ted systems. Section 5 discusses the approaches
implemented by participants. Section 6 analyzes
the results using additional non-official evaluation
measures. Section 7 concludes the paper.
2 Task Definition
In this section we provide the definition of the
shared task, starting with the format of the shared
task data, followed by a description of the eval-
uation metrics used and a discussion of the two
shared task challenges, i.e., closed and open.
2.1 Data Format
The data format used in this shared task was highly
influenced by the formats used in the 2004?2007
shared tasks. The data follows these general rules:
? The files contain sentences separated by a
blank line.
? A sentence consists of one or more tokens and
the information for each token is represented
on a separate line.
? A token consists of at least 11 fields. The
fields are separated by one or more whites-
pace characters (spaces or tabs). Whitespace
characters are not allowed within fields.
Table 1 describes the fields stored for each token
in the closed-track data sets. Columns 1?3 and
5?8 are available at both training and test time.
Column 4, which contains gold-standard part-of-
speech (POS) tags, is not given at test time. The
same holds for columns 9 and above, which con-
tain the syntactic and semantic dependency struc-
tures that the systems should predict.
The PPOS and PPOSS fields were automati-
cally predicted using the SVMTool POS tagger
(Gim?enez, 2004). To predict the tags in the train-
ing set, a 5-fold cross-validation procedure was
used. The LEMMA and SPLIT LEMMA fields
were predicted using the built-in lemmatizer in
WordNet (Fellbaum, 1998) based on the most fre-
quent sense for the form and part-of-speech tag.
Since NomBank uses a sub-word anal-
ysis in some hyphenated words (such as
[finger]
ARG
-[pointing]
PRED
), the data for-
mat represents the parts in hyphenated words as
separate tokens (columns 6?8). However, the
format also represents how the parts originally fit
together before splitting (columns 2?5). Padding
characters (? ?) are used in columns 2?5 to
ensure the same number of rows for all columns
corresponding to one sentence. All syntactic and
semantic dependencies are annotated relative to
the split word forms (columns 6?8).
Table 2 shows the columns available to the sys-
tems participating in the open challenge: named-
entity labels as in the CoNLL-2003 Shared Task
(Tjong Kim San and De Meulder, 2003) and
from the BBN Wall Street Journal Entity Corpus,
2
WordNet supersense tags, and the output of an off-
the-shelf dependency parser (Nivre et al, 2007b).
Columns 1?3 were predicted using the tagger of
Ciaramita and Altun (2006). Because the BBN
corpus shares lexical content with the Penn Tree-
bank, we generated the BBN tags using a 2-fold
cross-validation procedure.
2.2 Evaluation Measures
We separate the evaluation measures into two
groups: (i) official measures, which were used for
the ranking of participating systems, and (ii) addi-
tional unofficial measures, which provide further
insight into the performance of the participating
systems.
2.2.1 Official Evaluation Measures
The official evaluation measures consist of three
different scores: (i) syntactic dependencies are
scored using the labeled attachment score (LAS),
(ii) semantic dependencies are evaluated using a
labeled F
1
score, and (iii) the overall task is scored
with a macro average of the two previous scores.
We describe all these scoring measures next.
The LAS score is defined similarly as in the pre-
vious two shared tasks, as the percentage of to-
2
LDC catalog number LDC2005T33.
160
Number Name Description
1 ID Token counter, starting at 1 for each new sentence.
2 FORM Unsplit word form or punctuation symbol.
3 LEMMA Predicted lemma of FORM.
4 GPOS Gold part-of-speech tag from the Treebank (empty at test time).
5 PPOS Predicted POS tag.
6 SPLIT FORM Tokens split at hyphens and slashes.
7 SPLIT LEMMA Predicted lemma of SPLIT FORM.
8 PPOSS Predicted POS tags of the split forms.
9 HEAD Syntactic head of the current token, which is either a value of ID or zero (0).
10 DEPREL Syntactic dependency relation to the HEAD.
11 PRED Rolesets of the semantic predicates in this sentence.
12. . . ARG Columns with argument labels for each semantic predicate following textual order.
Table 1: Column format in the closed-track data. The columns in the lower part of the table are unseen
at test time and are to be predicted by systems.
Number Name Description
1 CONLL2003 Named entity labels using the tag set from the CoNLL-2003 shared task.
2 BBN NE labels using the tag set from the BBN Wall Street Journal Entity Corpus.
3 WNSS WordNet super senses.
4 MALT HEAD Head of the syntactic dependencies generated by MaltParser.
5 MALT DEPREL Label of syntactic dependencies generated by MaltParser.
Table 2: Column format in the open-track data.
kens for which a system has predicted the correct
HEAD and DEPREL columns (see Table 1). Same
as before, our scorer also computes the unlabeled
attachment score (UAS), i.e., the percentage of to-
kens with correct HEAD, and label accuracy, i.e.,
the percentage of tokens with correct DEPREL.
The semantic propositions are evaluated by con-
verting them to semantic dependencies, i.e., we
create a semantic dependency from every predicate
to all its individual arguments. These dependen-
cies are labeled with the labels of the correspond-
ing arguments. Additionally, we create a seman-
tic dependency from each predicate to a virtual
ROOT node. The latter dependencies are labeled
with the predicate senses. This approach guaran-
tees that the semantic dependency structure con-
ceptually forms a single-rooted, connected (but not
necessarily acyclic) graph. More importantly, this
scoring strategy implies that if a system assigns
the incorrect predicate sense, it still receives some
points for the arguments correctly assigned. For
example, for the correct proposition:
verb.01: ARG0, ARG1, ARGM-TMP
the system that generates the following output for
the same argument tokens:
verb.02: ARG0, ARG1, ARGM-LOC
receives a labeled precision score of 2/4 because
two out of four semantic dependencies are incor-
rect: the dependency to ROOT is labeled 02 in-
stead of 01 and the dependency to the ARGM-TMP
is incorrectly labeled ARGM-LOC. Using this strat-
egy we compute precision, recall, and F
1
scores
for both labeled and unlabeled semantic dependen-
cies.
Finally, we combine the syntactic and semantic
measures into one global measure using macro av-
eraging. We compute macro precision and recall
scores by averaging the labeled precision and re-
call for semantic dependencies with the LAS for
syntactic dependencies:
3
LMP = W
sem
? LP
sem
+ (1?W
sem
) ? LAS (1)
LMR = W
sem
? LR
sem
+ (1?W
sem
) ? LAS (2)
where LMP is the labeled macro precision and
LP
sem
is the labeled precision for semantic depen-
dencies. Similarly, LMR is the labeled macro re-
call and LR
sem
is the labeled recall for semantic
dependencies. W
sem
is the weight assigned to the
semantic task.
4
The macro labeled F
1
score, which
was used for the ranking of the participating sys-
tems, is computed as the harmonic mean of LMP
and LMR.
3
We can do this because the LAS for syntactic dependen-
cies is a special case of precision and recall, where the pre-
dicted number of dependencies is equal to the number of gold
dependencies.
4
We assign equal weight to the two tasks, i.e., W
sem
=
0.5.
161
2.2.2 Additional Evaluation Measures
We used several additional evaluation measures
to further analyze the performance of the partici-
pating systems.
The first additional measure used is Exact
Match, which reports the percentage of sentences
that are completely correct, i.e., all the generated
syntactic dependencies are correct and all the se-
mantic propositions are present and correct. While
this score is significantly lower than any of the of-
ficial scores, it will award systems that performed
joint learning or optimization for all subtasks.
In the same spirit but focusing on the seman-
tic subtasks, we report the Perfect Proposition F
1
score, where we score entire semantic frames or
propositions. This measure is similar to the PProps
accuracy score from the 2005 shared task (Carreras
and M`arquez, 2005), with the caveat that this year
this score is implemented as an F
1
measure, be-
cause predicates are not provided in the test data.
Hence, propositions may be over or under gener-
ated at prediction time.
Lastly, we analyze systems based on the ratio
between labeled F
1
score for semantic dependen-
cies and the LAS for syntactic dependencies. In
other words, this measure normalizes the seman-
tic scores relative to the performance of the pars-
ing component. This measure estimates the true
overall performance of the semantic subtasks, in-
dependent of the syntactic parser.
5
For example,
this score addresses the situations where the se-
mantic labeled F
1
score of one system is artificially
low because the corresponding syntactic compo-
nent does not perform well.
2.3 Closed and Open Challenges
Similarly to the CoNLL-2005 shared task, this
shared task evaluation is separated into two chal-
lenges:
Closed Challenge - systems have to be built
strictly with information contained in the given
training corpus, and tuned with the development
section. In addition, the PropBank and NomBank
lexical frames can be used. These restrictions
mean that constituent-based parsers or SRL sys-
tems can not be used in this challenge because the
constituent-based annotations are not provided in
our training set. The aim of this challenge is to
5
A correct evaluation of the stand-alone SRL systems
would require the usage of gold syntactic dependencies, but
these were not provided for the testing corpora.
compare the performance of the participating sys-
tems in a fair environment.
Open Challenge - systems can be developed mak-
ing use of any kind of external tools and resources.
The only condition is that such tools or resources
must not have been developed with the annota-
tions of the test set, both for the input and out-
put annotations of the data. In this challenge,
we are interested in learning methods which make
use of any tools or resources that might improve
the performance. For example, we encourage the
use of semantic information, as provided by NE
recognition or word-sense disambiguation (WSD)
systems (such state-of-the-art annotations are pro-
vided by the organizers, see Table 2). Also, in
this challenge participants are encouraged to use
constituent-based parsers and SRL systems, as
long as these systems were trained only with the
sections of Penn Treebank used in the shared task
training corpus. To encourage the participation of
the groups that are only interested in SRL, the or-
ganizers provide also the output of a state-of-the-
art dependency parser as input in this challenge.
The comparison of different systems in this setting
may not be fair, and thus ranking of systems is not
necessarily important.
3 Data
The corpora used in the shared task evaluation
were generated through a process that merges
several input corpora and converts them from
the constituent-based formalism to dependencies.
This section starts with an introduction of the in-
put corpora used, followed by a description of
the constituent-to-dependency conversion process.
The section concludes with an overview of the
shared task corpora.
3.1 Input Corpora
Input to our merging procedures includes the Penn
Treebank, BBN?s named entity corpus, PropBank
and NomBank. In this section, we will pro-
vide brief descriptions of these annotations in
terms of both form and content. All annotations
are currently being distributed by the Linguistic
Data Consortium, with the exception of NomBank,
which is freely downloadable.
6
6
http://nlp.cs.nyu.edu/meyers/NomBank.
html
162
3.1.1 Penn Treebank 3
The Penn Treebank 3 corpus (Marcus et al,
1993) consists of hand-coded parses of the Wall
Street Journal (test, development and training) and
a small subset of the Brown corpus (W. N. Fran-
cis and H. Ku?cera, 1964) (test only). These hand
parses are notated in-line and sometimes involve
changing the strings of the input data. For ex-
ample, in file wsj 0309, the token fearlast in the
text corresponds to the two tokens fear and last
in the annotated data. In a similar way, cannot
is regularly split to can and not. It is significant
that the other annotations assume the tokeniza-
tion of the Penn Treebank, as this makes it easier
for us to merge the annotation. The Penn Tree-
bank syntactic annotation includes phrases, parts
of speech, empty category representations of vari-
ous filler/gap constructions and other phenomena,
based on a theoretical perspective similar to that
of Government and Binding Theory (Chomsky,
1981).
3.1.2 BBN Pronoun Coreference and Entity
Type Corpus
BBN?s NE annotation of the Wall Street Journal
corpus (Weischedel and Brunstein, 2005) takes the
form of SGML inline markup of text, tokenized
to be completely compatible with the Penn Tree-
bank annotation, e.g., fearlast and cannot are split
in the same ways. Named entity categories in-
clude: Person, Organization, Location, GPE, Fa-
cility, Money, Percent, Time and Date, based on
the definitions of these categories in MUC (Chin-
chor and Robinson, 1998) and ACE
7
tasks. Sub-
categories are included as well. Note however that
from this corpus we only use NE boundaries to
derive NAME dependencies between NE tokens,
e.g., we create a NAME dependency from Mary to
Smith given the NE mention Mary Smith.
3.1.3 Proposition Bank I (PropBank)
The PropBank annotation (Palmer et al, 2005)
classifies the arguments of all the main verbs in the
Penn Treebank corpus, other than be. Arguments
are numbered (ARG0, ARG1, . . .) based on lexical
entries or frame files. Different sets of arguments
are assumed for different rolesets. Dependent con-
stituents that fall into categories independent of
the lexical entries are classified as various types
7
http://projects.ldc.upenn.edu/ace/
of ARGM (TMP, ADV, etc.).
8
Rather than us-
ing PropBank directly, we used the version created
for the CoNLL-2005 shared task (Carreras and
M`arquez, 2005). PropBank?s pointers to subtrees
are converted into the list of leaves of those sub-
trees, minus the empty categories. On occasion,
arguments of verbs end up being two non-adjacent
substrings. For example, the argument of claims in
the following sentence is indicated in bold: This
sentence, Mary claims, is self-referential. The
CoNLL-2005 format handles this by marking both
strings A1 (This sentence and is self-referential),
but adding a C- prefix to the argument tag on the
second argument. Another difference between the
PropBank annotation and the CoNLL-2005 ver-
sion of it is their treatments of filler gap construc-
tions involving empty categories. PropBank an-
notation includes the whole chain of empty cate-
gories, as well as the antecedent of the empty cate-
gory (the filler of the gap). In contrast, the CoNLL-
2005 version only includes the filler of the gap and
if there is no filler, the argument is omitted, e.g.,
no ARG0 (subject) for leave would be included in
I said to leave because the subject of leave is un-
specified.
3.1.4 NomBank
NomBank annotation (Meyers et al, 2004) uses
essentially the same framework as PropBank to an-
notate arguments of nouns. Differences between
PropBank and NomBank stem from differences
between noun and verb argument structure; differ-
ences in treatment of nouns and verbs in the Penn
Treebank; and differences in the sophistication of
previous research about noun and verb argument
structure. Only the subset of nouns that take ar-
guments are annotated in NomBank and only a
subset of the non-argument siblings of nouns are
marked as ARGM. These limitations were nec-
essary to make the NomBank task consistent and
tractable. In addition, long distance dependencies
of nouns, e.g., the relation between Mary and walk
in Mary took dozens of walks is handled as fol-
lows: Mary is marked as the ARG0 of walk and
took + dozens + of is marked as a support chain
in NomBank. In contrast, verbal long distance de-
pendencies can be handled by means of empty cat-
egories in the Penn Treebank, e.g., the relation be-
8
PropBank I is used here. Later versions of PropBank
mark instances of be in addition to other verbs. PropBank?s
use of the terms roleset and ARGM correspond approximately
to sense and adjunct in common usage.
163
tween John and walked in John seemed t to walk.
Support chains are needed because nominal long
distance dependencies are not captured under the
Penn Treebank?s system of empty categories.
3.2 Conversion to Dependencies
3.2.1 Syntactic Dependencies
There exists no large-scale dependency tree-
bank for English, and we thus had to construct a
dependency-annotated corpus automatically from
the Penn Treebank (Marcus et al, 1993). Since
dependency syntax represents grammatical struc-
ture by means of labeled binary head?dependent
relations rather than phrases, the task of the con-
version procedure is to identify and label the
head?dependent pairs. The idea underpinning
constituent-to-dependency conversion algorithms
(Magerman, 1994; Collins, 1999; Yamada and
Matsumoto, 2003) is that head?dependent pairs are
created from constituents by selecting one word in
each phrase as the head and setting all other as its
dependents. The dependency labels are then in-
ferred from the phrase?subphrase or phrase?word
relations.
Our conversion procedure (Johansson and
Nugues, 2007) differs from this basic approach by
exploiting the rich structure of the constituent for-
mat used in Penn Treebank 3:
? Grammatical function labels that often can be
directly used in the dependency framework.
? Long-distance grammatical relations repre-
sented by means of empty categories and sec-
ondary edges, which can be used to create (of-
ten nonprojective) dependency links.
Of the grammatical function tags available in the
Treebank, we removed the HLN, NOM, TPC, and
TTL tags since they represent structural properties
of single phrases rather than binary relations. For
compatibility between the WSJ and Brown cor-
pora, we removed the ETC, UNF, and IMP tags
from Brown and the CLR tag from WSJ.
Algorithms 1 and 2 show the constituent-to-
dependency conversion algorithm and function la-
beling, respectively. The first steps apply structural
transformations to the constituent trees. Next, a
head word is assigned to each constituent. After
this, grammatical functions are inferred, allowing
a dependency tree to be created.
To find head children (used in
assign-heads), a system of rules is used
Algorithm 1: Pseudocode for constituent-to-
dependency conversion.
procedure constituents-to-dependencies(T )
import-glarf(T )
reattach-traces(T )
split-small-clauses(T )
assign-heads(T.root)
assign-functions(T )
return create-dependency-tree(T )
procedure import-glarf(T )
Import a GLARF surface dependency graph G
for each multi-word name N in G
for each token d in N
Set the function tag of d to NAME
for each dependency link h ?
L
d in G
if L ? { APPOSITE, A-POS, N-POS, POST-HON, Q-POS,
RED-RELATIVE, SUFFIX, T-POS, TITLE }
or if h and d are inside a split word
Set the function tag of d to L in T
if h and d are part of a larger constituent
Add an NX constituent to T that brackets h and d
procedure reattach-traces(T )
for each empty category t in T
if t is linked to a constituent C via a secondary edge label L
and L ? {
*
ICH
*
,
*
T
*
,
*
RNR
*
}
disconnect C
disconnect the secondary edge
attach C to the parent of t
procedure split-small-clauses(T )
for each verb phrase C in T
if C has a child S and the phrase label of S is S
and S is not preceded by a ?? or , tag
and S has a subject child s
disconnect s
attach s to C
set the function tag of s to OBJ
set the function tag of S to OPRD
procedure assign-heads(N)
for each child C of N
assign-heads(C)
if is-coordinated(N)
e ? index of first CC or CONJP or , or :
else
e ? index of last child of N
find head child H between 1 and e according to head rules (Table 3)
N.head ? H.head
procedure is-coordinated(N)
if N has the label UCP return True
if N has a CC or CONJP child which is not leftmost return True
if N has a , or : child c, and c is not leftmost or rightmost or
crossed by an apposition link, return True
else return False
procedure create-dependency-tree(T )
D ? {}
for each token t in T
let C be the highest constituent that t is the head of
let P be the parent of C
let L be the function tag of C
D ? D ? P.head ?
L
t
return D
(Table 3). The first column in the table indicates
the phrase type, the second is the search direction,
and the third is a priority list of phrase types to
look for. For instance, to find the head of an S
phrase, we look from right to left for a VP. If
no VP is found, look for anything with a PRD
function tag, and so on.
Moreover, since the grammatical structure in-
164
ADJP ? NNS QP NN $ ADVP JJ VBN VBG ADJP JJR NP JJS DT
FW RBR RBS SBAR RB
ADVP ? RB RBR RBS FW ADVP TO CD JJR JJ IN NP JJS NN
CONJP ? CC RB IN
FRAG ? (NN
*
| NP) W
*
SBAR (PP | IN) (ADJP | JJ) ADVP
RB
INTJ ?
*
LST ? LS :
NAC, NP, NX, WHNP ? (NN
*
| NX) NP-? JJR CD JJ JJS RB QP NP
PP, WHPP ? IN TO VBG VBN RP FW
PRN ? S
*
N
*
W
*
PP|IN ADJP|JJ
*
ADVP|RB
*
PRT ? RP
QP ? $ IN NNS NN JJ RB DT CD NCD QP JJR JJS
RRC ? VP NP ADVP ADJP PP
S ? VP
*
-PRD S SBAR ADJP UCP NP
SBAR ? S SQ SINV SBAR FRAG IN DT
SBARQ ? SQ S SINV SBARQ FRAG
SINV ? VBZ VBD VBP VB MD VP
*
-PRD S SINV ADJP NP
SQ ? VBZ VBD VBP VB MD
*
-PRD VP SQ
UCP ?
*
VP ? VBD VBN MD VBZ VB VBG VBP VP
*
-PRD ADJP NN NNS
NP
WHADJP ? CC WRB JJ ADJP
WHADVP ? CC WRB
X ?
*
Table 3: Head rules.
Algorithm 2: Pseudocode for the function la-
beling procedure.
procedure assign-functions(T )
for each constituent C in T
if C has no function tag from Penn or GLARF
L ? infer-function(C)
Set the function tag of C to L
procedure infer-function(C)
let c be the head of C, P the parent of C, and p the head of P
if C is an object return OBJ
if C is PRN return PRN
if h is punctuation return P
if C is coordinated with P return COORD
if C is PP, ADVP, or SBAR and P is VP return ADV
if C is PRT and P is VP return PRT
if C is VP and P is VP, SQ, or SINV return VC
if C is TO and P is VP return IM
if P is SBAR and p is IN return SUB
if P is VP, S, SBAR, SBARQ, SINV, or SQ and C is RB return ADV
if P is NP, NX, NAC, or WHNP return NMOD
if P is ADJP, ADVP, WHADJP, or WHADVP return AMOD
if P is PP or WHPP return PMOD
else return DEP
side noun phrases (NP) is under-specified in the
Penn Treebank, we imported dependencies in-
side NPs and hyphenated words from a version
of the Penn Treebank mapped into GLARF, the
Grammatical and Logical Argument Representa-
tion Framework (Meyers et al, 2007).
The parts of GLARF?s NP analysis that are most
relevant to this task include: (i) identifying ap-
posites (APPO, e.g., that book depends on gift in
Mary?s gift, a book about cheese; (ii) the iden-
tification of name boundaries taken from BBN?s
NE annotation, e.g., identifying that Smith de-
pends on Mary which depends on appointment
in the Mary Smith appointment; (iii) identifying
TITLE and POSTHON dependencies, e.g., deter-
mining that Ms. and III depend on Mary in Ms.
Mary Smith III. These identifications were car-
ried out by hand-coded rules that have been fine
tuned as part of GLARF, over the past several
years. For example, identifying apposition con-
structions requires identifying that both the head
and the apposite can stand alone ? proper nouns
(John Smith), plural nouns (books), and singular
common nouns with determiners (the book) are
stand-alone cases, whereas singular nouns without
determiners (green book) do not qualify.
We split Treebank tokens at a hyphen (-) or a
forward slash (/) if the segments on either side of
these delimiters are: (a) a word in a dictionary
(COMLEX Syntax or any of the dictionaries avail-
able on the NOMLEX website); (b) part of a mark-
able Named Entity;
9
or (c) a prefix from the list:
co, pre, post, un, anti, ante, ex, extra, fore, non,
over, pro, re, super, sub, tri, bi, uni, ultra. For ex-
ample, York-based was split into 3 segments: (1)
York, (2) - and (3) based.
9
The CoNLL-2008 website contains a Named Entity To-
ken gazetteer to aid in this segmentation.
165
3.2.2 Semantic Dependencies
When encoding the semantic dependencies, it
was necessary to convert the underlying con-
stituent analysis of PropBank and NomBank into
a dependency analysis. Because semantic predi-
cates are already assigned to individual tokens in
both PropBank (the version used for the CoNLL-
2005 shared task) and NomBank, constituent-to-
dependency conversion is thus necessary only for
semantic arguments. Conceptually, this conver-
sion can be handled using similar heuristics as de-
scribed in Section 3.2.1. However, in order to
avoid replicating this effort and to ensure compat-
ibility between syntactic and semantic dependen-
cies, we decided to generate semantic dependen-
cies using only argument boundaries and the syn-
tactic dependencies generated in Section 3.2.1, i.e.,
ignoring syntactic constituents. Given this input,
we identify the head of a semantic argument using
the following heuristic:
The head of a semantic argument is as-
signed to the token inside the argument
boundaries whose head is a token out-
side the argument boundaries.
This heuristic works remarkably well: over 99%
of the PropBank arguments in the training corpus
have a single token whose head is located outside
of the argument boundaries. As a simple example,
consider the following annotated text: [sold]
PRED
[1214 cars]
ARG1
[in the U.S.]
ARGM-LOC
. Us-
ing the above heuristic, the head of the ARG1 ar-
gument is set to cars, because it has an OBJ de-
pendency to sold, and the head of the ARGM-
LOC argument is set to in, because it modifies sold
through a LOC dependency.
While this heuristic processes the vast majority
of arguments, there are several cases that require
special treatment. We discuss these situations in
the remainder of this section.
Arguments with several syntactic heads
For 0.7% of the semantic arguments, the above
heuristic detects several syntactic heads for the
given boundary. For example, in the text [it]
ARG0
[expects]
PRED
[its U.S. sales to remain steady
at about 1200 cars]
ARG1
, the above heuris-
tic assigns two syntactic heads to ARG1: sales,
which modifies expects through an OBJ depen-
dency, and to, which modifies expects through a
PRD dependency. These situations are caused
by the constituent-to-dependency conversion pro-
cess described in Section 3.2.1, which in some
cases interprets syntax differently than the orig-
inal Treebank annotation, e.g., the raising phe-
nomenon for the PRD dependency in the above
example. In such cases, we split the original argu-
ment into a sequence of discontinuous arguments,
e.g., the ARG1 in the above example becomes [its
U.S. sales]
ARG1
[to remain steady at about 1200
cars]
C-ARG1
.
Merging discontinuous arguments
While in the above case we split arguments, there
are situations where we can merge arguments that
were initially discontinuous in PropBank or Nom-
Bank. This typically happens when the Prop-
Bank/NomBank predicate is infixed inside one of
its arguments. For example, in the text [Million-
dollar conferences]
ARG1
were [held]
PRED
[to
chew on subjects such as... ]
C-ARG1
, PropBank
lists multiple constituents as aggregately filling the
ARG1 slot of held. These cases are detected au-
tomatically because the least common ancestor of
the argument pieces is actually one of the argument
segments. In the above example, to chew on sub-
jects such as... depends on Million-dollar confer-
ences because to modifies conferences through a
NMOD dependency. In these situations, we treat
the least common ancestor, e.g., conferences in the
above text, as the true argument. This heuristic al-
lowed us to merge 1665 (or 0.6% of total) argu-
ments that were initially discontinuous in the Prop-
Bank training corpus.
Empty categories
PropBank and NomBank both encode chains of
empty categories. As with the 2005 shared task
(Carreras and M`arquez, 2005), we used the head
of the antecedent of empty categories as arguments
rather than empty categories. Furthermore, empty
category arguments with no antecedents were ig-
nored.
10
For example, given The man wanted t to
make a speech, we assume that the A0 of make and
speech is man, rather than the chain consisting of
the empty category represented as t and man.
Annotation disagreements
NomBank and Penn Treebank annotators some-
times disagree about constituent structure. Nom-
10
Under our approach to filler gap constructions, the filler
is a shared argument (as in Relational Grammar, most Feature
Structure and Dependency Grammar frameworks), in con-
trast with the Penn Treebank?s empty category antecedent ap-
proach (more closely resembling the various Chomskian ap-
proaches).
166
Label Freq. Description
NMOD 324834 Modifier of nominal
P 135260 Punctuation
PMOD 115988 Modifier of preposition
SBJ 89371 Subject
OBJ 66677 Object
ROOT 49178 Root
ADV 47379 General adverbial
NAME 41138 Name-internal link
VC 35250 Verb chain
COORD 31140 Coordination
DEP 29456 Unclassified
TMP 26305 Temporal adverbial or nominal modifier
CONJ 24522 Second conjunct (dependent on conjunction)
LOC 18500 Locative adverbial or nominal modifier
AMOD 17868 Modifier of adjective or adverbial
PRD 16265 Predicative complement
APPO 16163 Apposition
IM 16071 Infinitive verb (dependent on infinitive marker to)
HYPH 14073 Token part of a hyphenated word (dependent on a preceding part of the hyphenated word)
HMOD 13885 Token inside a hyphenated word (dependent on the head of the hyphenated word)
SUB 12995 Subordinated clause (dependent on subordinating conjuction)
OPRD 11707 Predicative complement of raising/control verb
SUFFIX 10548 Possessive suffix (dependent on possessor)
DIR 6145 Adverbial of direction
TITLE 5917 Title (dependent on name)
MNR 4753 Adverbial of manner
POSTHON 4377 Posthonorific modifier of nominal
PRP 4013 Adverbial of purpose or reason
PRT 3235 Particle (dependent on verb)
LGS 3115 Logical subject of a passive verb
EXT 2374 Adverbial of extent
PRN 2176 Parenthetical
EXTR 658 Extraposed element in cleft
DTV 496 Dative complement (to) in dative shift
PUT 271 Complement of the verb put
BNF 44 Benefactor complement (for) in dative shift
VOC 24 Vocative
Table 4: Statistics for atomic syntactic labels.
Bank annotators are in effect assuming that the
constituents provided form a phrase. In this case,
the constituents are adjacent to each other. For ex-
ample, consider the NP the human rights discus-
sion. In this case, the Penn Treebank would treat
each of the four words the, human, rights, discus-
sion as daughters of a single NP node. However,
NomBank would treat human rights as a single
ARG1 of discussion. Since noun noun modifica-
tion constructions are head final, we can easily de-
termine (via GLARF) that rights is the markable
dependent of discussion.
Support chains
Finally, NomBank?s encoding of support chains is
handled as chains of dependencies in the data (al-
though these are not scored). For example, given
Mary took dozens of walks, where Mary is the
ARG0 of walks, the support chain took + dozens +
of is represented as a sequence of dependencies: of
depends on Mary, dozens depends on of and took
depends on dozens. Each of these dependencies is
labeled SU.
3.3 Overview of Corpora
The syntactic dependency types are divided into
atomic types that consist of a single label, and non-
atomic types consisting of more than one label.
There are 38 atomic and 70 non-atomic labels in
the corpus. There are three types of non-atomic
labels: those consisting of a PRD or OPRD con-
catenated with an adverbial label such as LOC or
TMP; gapping labels such as GAP-SBJ; and com-
bined adverbial tags such as LOC-TMP.
Table 4 shows statistics for the atomic syntac-
tic dependencies: label type, the frequency of the
label in the complete corpus, and a description of
the label. Table 5 shows the corresponding statis-
tics for non-atomic dependencies, excluding gap-
ping dependencies. The non-atomic labels are rare,
which made it difficult to learn these relations ef-
167
Label Frequency
LOC-PRD 798
PRD-TMP 51
PRD-PRP 45
LOC-OPRD 31
DIR-PRD 4
MNR-PRD 3
LOC-TMP 2
MNR-TMP 1
LOC-MNR 1
DIR-OPRD 1
Table 5: Statistics for non-atomic syntactic labels
excluding gapping labels.
Label Frequency
GAP-SBJ 116
GAP-OBJ 102
DEP-GAP 83
GAP-TMP 69
GAP-PRD 66
GAP-LGS 44
GAP-LOC 42
DIR-GAP 37
GAP-PMOD 22
GAP-VC 20
EXT-GAP 16
ADV-GAP 15
GAP-NMOD 13
GAP-LOC-PRD 6
DTV-GAP 6
AMOD-GAP 6
GAP-MNR 5
GAP-PRP 4
EXTR-GAP 3
GAP-SUB 1
GAP-PUT 1
GAP-OPRD 1
Table 6: Statistics for non-atomic labels containing
a gapping label.
fectively. Table 6 shows the table for non-atomic
labels containing a gapping label.
A dependency link w
i
? w
j
is said to be pro-
jective if all words occurring between w
i
and w
j
in
the surface word order are dominated by w
i
(where
dominance is the transitive closure of the direct
link relation). Nonprojective links are impossible
to handle for the search procedures in many types
of dependency parsers. It has been previously ob-
served that the majority of dependencies in all lan-
guages are projective, and this is particularly true
for English ? in the complete corpus, only 4118
links (0.4%) are nonprojective. 3312 sentences, or
7.6%, contain at least one nonprojective link.
Table 7 shows statistics for different types of
nonprojective links: nonprojectivity caused by
wh-movement, such as in Where are you going?
or What have you done?; split clauses such as
Type Frequency
wh-movement 1709
Split clause 734
Split noun phrase 590
Other 1085
Table 7: Statistics for nonprojective links.
POS Frequency
NN 68477
NNS 30048
VBD 24106
VB 23650
VBN 19339
VBG 14245
VBZ 10883
VBP 6330
Other 83
Table 8: Statistics for predicates, by POS tags.
Even to make love, he says, you need experience;
split noun phrases such as hold a hearing tomor-
row on the topic; and all other types of nonprojec-
tive links.
Lastly, Tables 8 and 9 summarizes statistics for
semantic predicates and roles. Table 8 shows the
number of non-support predicates with a given
POS tag in the whole corpus (we used GPOS or
PPOSS for predicates inside hyphenated words).
The last line shows the number of predicates with
a POS tag that does not start with NN or VB. This
last table entry is generated by POS tagger mis-
takes when producing the PPOSS tags, or by errors
in our NomBank/PropBank conversion software.
11
Nevertheless, the overall picture given by the table
indicates that predicates are almost perfectly dis-
tributed between nouns and verbs: there are 98525
nominal and 98553 verbal predicates.
Table 9 shows the number of arguments with a
given role label. For brevity we list only labels that
are instantiated at least 10 times in the whole cor-
pus. The total number of arguments labeled with a
role label with frequency lower than 10 is listed
in the last line in the table. The table indicates
that, while the top three most common role labels
are ?core? labels (A1, A0, A2), modifier arguments
(AM-
*
) account for approximately 20% of the total
number of arguments. On the other hand, discon-
tinuous arguments are not common: only 0.7% of
the total number of arguments have a continuation
label (C-
*
).
11
In very few situations, we select incorrect head tokens for
multi-word predicates.
168
Label Frequency
A1 161409
A0 109437
A2 51197
AM-TMP 25913
AM-MNR 13080
AM-LOC 11409
A3 10269
AM-MOD 9986
AM-ADV 9496
AM-DIS 5369
R-A0 4432
AM-NEG 4097
A4 3281
C-A1 3118
R-A1 2565
AM-PNC 2445
AM-EXT 1428
AM-CAU 1346
AM-DIR 1318
R-AM-TMP 797
R-A2 307
R-AM-LOC 246
R-AM-MNR 155
A5 91
AM-PRD 78
C-A0 70
C-A2 65
R-AM-CAU 50
C-A3 37
R-A3 29
C-AM-MNR 24
C-AM-ADV 20
AM-REC 16
AA 14
R-AM-PNC 12
C-AM-EXT 11
C-AM-TMP 11
C-A4 11
Frequency < 10 70
Table 9: Statistics for semantic roles.
4 Submissions and Results
Nineteen groups submitted test runs in the closed
challenge and five groups participated in the open
challenge. Three of the latter groups participated
only in the open challenge, and two of these sub-
mitted results only for the semantic subtask. These
results are summarized in Tables 10 and 11.
Table 10 summarizes the official results ? i.e.,
results at evaluation deadline ? for the closed chal-
lenge. Note that several teams corrected bugs
and/or improved their systems and they submit-
ted post-evaluation scores (accounted in the shared
task website). The table indicates that most of the
top results cluster together: three systems had a
labeled macro F
1
score on the WSJ+Brown cor-
pus around 82 points (che, ciaramita, and zhao);
five systems scored around 79 labeled macro F
1
points (yuret, samuelsson, zhang, henderson, and
watanabe). Remarkably, the top-scoring system
(johansson) is in a class of its own, with scores
2?3 points higher than the next system. This is
most likely caused by the fact that Johansson and
Nugues (2008) implemented a thorough system
that addressed all facets of the task with state-of-
the-art methods: second-order parsing model, ar-
gument identification/classification models sepa-
rately tuned for PropBank and NomBank, rerank-
ing inference for the SRL task, and, finally, joint
optimization of the complete task using meta-
learning (more details in Section 5).
Table 11 lists the official results in the open chal-
lenge. The results in this challenge are lower than
in the closed challenge, but this was somewhat
to be expected considering that there were fewer
participants in this challenge and none of the top
five groups in the closed challenge submitted re-
sults in the open challenge. Only one of the sys-
tems that participated in both challenges (zhang)
improved the results submitted in the closed chal-
lenge. Zhang et al (2008) achieved this by ex-
tracting features for their semantic subtask mod-
els both from the parser used in the closed chal-
lenge and a secondary parser that was trained on
a different corpus. The improvements measured
were relatively small for the in-domain WSJ cor-
pus (0.2 labeled macro F
1
points) but larger for the
out-of-domain Brown corpus (approximately 1 la-
beled macro F
1
point).
Tables 10 and 11 indicate that in both chal-
lenges the results on the out-of-domain corpus
(Brown) are much lower than the results measured
in-domain (WSJ). The difference is around 7?8
LAS points for the syntactic subtask and 12?14 la-
beled F
1
points for semantic dependencies. Over-
all, this yields a drop of approximately 10 labeled
macro F
1
points for most systems. This perfor-
mance decrease on out-of-domain corpora is con-
sistent with the results reported in CoNLL-2005
on SRL (using the same Brown corpus). These
results indicate that domain adaptation is a prob-
lem that is far from being solved for both syntactic
and semantic analysis of text. Furthermore, as the
scores on the syntactic and semantic subtasks in-
dicate, domain adaptation becomes even harder as
the task to be solved gets more complex.
We describe the participating systems in the next
section. Then, in Section 6, we revert to result
analysis using different evaluation measures and
different views of the data.
169
Labeled Macro F
1
Labeled Attachment Score Labeled F
1
(complete task) (syntactic dependencies) (semantic dependencies)
WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown
johansson 84.86 (1) 85.95 75.95 89.32 (1) 90.13 82.81 80.37 (1) 81.75 69.06
che 82.66 (2) 83.78 73.57 86.75 (5) 87.51 80.73 78.52 (2) 80.00 66.37
ciaramita 82.06 (3) 83.25 72.46 86.60 (11) 87.47 79.67 77.50 (3) 79.00 65.24
zhao 81.44 (4) 82.62 71.78 86.66 (8) 87.52 79.83 76.16 (4) 77.67 63.69
yuret 79.84 (5) 80.97 70.55 86.62 (10) 87.39 80.46 73.06 (5) 74.54 60.62
samuelsson 79.79 (6) 80.92 70.49 86.63 (9) 87.36 80.77 72.94 (6) 74.47 60.18
zhang 79.32 (7) 80.41 70.48 87.32 (2) 88.14 80.80 71.31 (7) 72.67 60.16
henderson 79.11 (8) 80.19 70.34 86.91 (4) 87.78 80.01 70.97 (8) 72.26 60.38
watanabe 79.10 (9) 80.30 69.29 87.18 (3) 88.06 80.17 70.84 (9) 72.37 58.21
morante 78.43 (10) 79.52 69.55 86.07 (12) 86.88 79.58 70.51 (10) 71.88 59.23
li 78.35 (11) 79.38 70.01 86.69 (6) 87.42 80.8 69.95 (11) 71.27 59.17
baldridge 77.49 (12) 78.57 68.53 86.67 (7) 87.42 80.64 67.92 (14) 69.35 55.95
chen 77.00 (13) 77.95 69.23 84.47 (16) 85.20 78.58 69.45 (12) 70.62 59.81
lee 76.90 (14) 77.96 68.34 84.82 (15) 85.69 77.83 68.71 (13) 69.95 58.63
sun 76.28 (15) 77.10 69.58 85.75 (13) 86.37 80.75 66.61 (15) 67.62 58.26
choi 71.23 (16) 72.22 63.44 77.56 (17) 78.58 69.46 64.78 (16) 65.72 57.4
trandabat 63.45 (17) 64.21 57.41 85.21 (14) 85.96 79.24 40.63 (17) 41.36 34.75
lluis 63.29 (18) 63.74 59.65 71.95 (18) 72.30 69.14 54.52 (18) 55.09 49.95
neumann 19.93 (19) 20.13 18.14 16.25 (19) 16.22 16.47 22.36 (19) 22.86 17.94
Table 10: Official results in the closed challenge (post-evaluation scores are available on the shared
task website). Teams are denoted by the last name of the first author of the corresponding paper in
the proceedings or the last name of the person who registered the team if no paper was submitted.
Italics indicate that there is no corresponding paper in the proceedings. Results are sorted in descending
order of the labeled macro F
1
score on the WSJ+Brown corpus. The number in parentheses next to the
WSJ+Brown scores indicates the system rank in the corresponding task.
Labeled Macro F
1
Labeled Attachment Score Labeled F
1
(complete task) (syntactic dependencies) (semantic dependencies)
WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown
vickrey ? ? ? ? ? ? 76.17 (1) 77.38 66.23
riedel ? ? ? ? ? ? 74.59 (2) 75.72 65.38
zhang 79.61 (1) 80.61 71.45 87.32 (1) 88.14 80.80 71.89 (3) 73.08 62.11
li 77.84 (2) 78.87 69.51 86.69 (2) 87.42 80.80 68.99 (4) 70.32 58.22
wang 76.19 (3) 78.39 59.89 84.56 (3) 85.50 77.06 67.12 (5) 70.41 42.67
Table 11: Official results in the open challenge (post-evaluation scores are available on the shared task
website). Teams are denoted by the last name of the first author of the corresponding paper in the
proceedings or the last name of the person who registered the team if no paper was submitted. Italics
indicate that there is no corresponding paper in the proceedings. Results are sorted in descending order of
the labeled F
1
score for semantic dependencies on the WSJ+Brown corpus. The number in parentheses
next to the WSJ+Brown scores indicates the system rank in the corresponding task.
5 Approaches
Table 5 summarizes the properties of the sys-
tems that participated in the closed the open chal-
lenges. The second column of the table high-
lights the overall architectures. We used + to in-
dicate that the components are sequentially con-
nected. The lack of a + sign indicates that the cor-
responding tasks are performed jointly. For exam-
ple, Riedel and Meza-Ruiz (2008) perform pred-
icate and argument identification and classifica-
tion jointly, whereas Ciaramita et al (2008) im-
plemented a pipeline architecture of three compo-
nents. We use the || to indicate that several differ-
ent architectures that span multiple subtasks were
deployed in parallel.
This summary of system architectures indicates
that it is common that systems combine sev-
eral components in the semantic or syntactic sub-
tasks ? e.g., nine systems jointly performed pred-
icate/argument identification and classification ?
but only four systems combined components be-
tween the syntactic and semantic subtasks: Hen-
derson et al (2008), who implemented a generative
history-based model (Incremental Sigmoid Belief
Networks with vectors of latent variables) where
syntactic and semantic structures are separately
170
generated but using a synchronized derivation (se-
quence of actions); Samuelsson et al (2008),
who, within an ensemble-based architecture, im-
plemented a joint syntactic-semantic model using
MaltParser with labels enriched with semantic in-
formation; Llu??s and M`arquez, who used a modi-
fied version of the Eisner algorithm to jointly pre-
dict syntactic and semantic dependencies; and fi-
nally, Sun et al (2008), who integrated depen-
dency label classification and argument identifi-
cation using a maximum-entropy Markov model.
Additionally, Johansson and Nugues (2008), who
had the highest ranked system in the closed chal-
lenge, integrate syntactic and semantic analysis in
a final reranking step, which maximizes the joint
syntactic-semantic score in the top k solutions. In
the same spirit, Chen et al (2008) search in the
top k solutions for the one that maximizes a global
measure, in this case the joint probability of the
complete problem. These joint learning strategies
are summarized in the Joint Learning/Opt. col-
umn in the table. The system of Riedel and Meza-
Ruiz (2008) deserves a special mention: even
though Riedel and Meza-Ruiz did not implement
a syntactic parser, they are the only group that per-
formed the complete SRL subtask ? i.e., predicate
identification and classification, argument identifi-
cation and classification ? jointly, simultaneously
for all the predicates in a sentence. They imple-
mented a joint SRL model using Markov Logic
Networks and they selected the overall best solu-
tion using inference based on the cutting-plane al-
gorithm.
Although some of the systems that implemented
joint approaches obtained good results, the top
five systems in the closed challenge are essen-
tially systems with pipeline architectures. Further-
more, Johansson and Nugues (2008) and Riedel
and Meza-Ruiz (2008) showed that joint learn-
ing/optimization improves the overall results, but
the improvement is not large. These initial ef-
forts indicate at least that the joint modeling of this
problem is not a trivial task.
The D Arch. and D Inference columns summa-
rize the parsing architectures and the correspond-
ing inference strategies. Similar to last year?s
shared task (Nivre et al, 2007), the vast majority of
parsing models fall in two classes: transition-based
(?trans? in the table) or graph-based (?graph?)
models. By and large, transition-based models use
a greedy inference strategy, whereas graph-based
models used different Maximum Spanning Tree
(MST) algorithms: Carreras (2007) ? MST
C
, Eis-
ner (2000) ? MST
E
, or Chu-Liu/Edmonds (Mc-
Donald et al, 2005; Chu and Liu, 1965; Edmonds,
1967) ? MST
CL/E
. More interestingly, most of
the best systems used some strategy to mitigate
parsing errors. In the top three systems in the
closed challenge, two (che and ciaramita) used
parser combination through voting and/or stacking
of different models (see the D Comb. column).
Samuelsson et al (2008) perform a MST infer-
ence with the bag of all dependencies output by
the individual MALT parser variants. Johansson
and Nugues (2008) use a single parsing model, but
this model is extended with second-order features.
The PA Arch. and PA Inference columns sum-
marize the architectures and inference strategies
used for the identification and classification of
predicates and arguments. The columns indicate
that most systems modeled the SRL problem as a
token-by-token classification problem (?class? in
the table) with a corresponding greedy inference
strategy. Some systems (e.g., yuret, samuelsson,
henderson, lluis) incorporate SRL within parsing,
in which case we report the corresponding parsing
architecture and inference approach. Vickrey and
Koller (2008) simplify the sentences to be labeled
using a set of hand-crafted rules before deploying
a classification model on top of a constituent-based
representation. Unlike in the case of parsing, few
systems (yuret, samuelssson, and morante) com-
bine several PA models and the combination is lim-
ited to simple voting strategies (see the PA Comb.
column).
Finally, the ML Methods column lists the Ma-
chine Learning (ML) methods used. The column
indicates that maximum entropy (ME) was the
most popular method (12 distinct systems relied
on it). Support Vector Machines (SVM) (eight sys-
tems) and the Perceptron algorithm (three systems)
were also popular ML methods.
6 Analysis
Section 4 summarized the results in the closed
and open challenges using the official evaluation
measures. In this section, we analyze the sub-
mitted runs using different evaluation measures,
e.g., Exact Match or Perfect Proposition F
1
scores,
and different views of the data, e.g., only non-
projective dependencies or NomBank versus Prop-
Bank frames.
171
O
v
e
r
a
l
l
D
D
D
P
A
P
A
P
A
J
o
i
n
t
M
L
c
l
o
s
e
d
A
r
c
h
.
A
r
c
h
.
C
o
m
b
.
I
n
f
e
r
e
n
c
e
A
r
c
h
.
C
o
m
b
.
I
n
f
e
r
e
n
c
e
L
e
a
r
n
i
n
g
/
O
p
t
.
M
e
t
h
o
d
s
j
o
h
a
n
s
s
o
n
D
+
P
I
+
P
C
+
A
I
+
A
C
g
r
a
p
h
n
o
M
S
T
C
c
l
a
s
s
n
o
r
e
r
a
n
k
r
e
r
a
n
k
P
e
r
c
e
p
t
r
o
n
,
M
E
c
h
e
D
+
P
I
+
P
C
+
A
I
C
g
r
a
p
h
s
t
a
c
k
i
n
g
M
S
T
C
L
/
E
c
l
a
s
s
n
o
I
L
P
n
o
M
E
c
i
a
r
a
m
i
t
a
D
+
P
I
C
+
A
I
C
t
r
a
n
s
v
o
t
i
n
g
,
g
r
e
e
d
y
c
l
a
s
s
n
o
r
e
r
a
n
k
n
o
S
V
M
,
M
E
,
s
t
a
c
k
i
n
g
P
e
r
c
e
p
t
r
o
n
z
h
a
o
D
+
A
I
C
+
P
I
+
P
C
t
r
a
n
s
n
o
g
r
e
e
d
y
c
l
a
s
s
n
o
g
r
e
e
d
y
n
o
M
E
y
u
r
e
t
D
+
(
P
I
C
+
A
I
+
A
C
|
|
g
r
a
p
h
n
o
M
S
T
E
c
l
a
s
s
,
v
o
t
i
n
g
g
r
e
e
d
y
n
o
M
L
E
,
P
I
C
+
A
I
C
)
g
e
n
e
r
a
t
i
v
e
M
B
L
s
a
m
u
e
l
s
s
o
n
D
+
P
I
+
t
r
a
n
s
M
S
T
C
L
/
E
g
r
e
e
d
y
c
l
a
s
s
,
v
o
t
i
n
g
g
r
e
e
d
y
u
n
i
fi
e
d
S
V
M
(
A
I
+
A
C
|
|
D
A
I
C
)
+
P
C
b
l
e
n
d
i
n
g
t
r
a
n
s
l
a
b
e
l
s
z
h
a
n
g
D
+
P
I
+
A
I
+
A
C
+
P
C
g
r
a
p
h
,
m
e
t
a
-
M
S
T
C
L
/
E
,
c
l
a
s
s
n
o
g
r
e
e
d
y
n
o
S
V
M
,
M
E
t
r
a
n
s
l
e
a
r
n
i
n
g
g
r
e
e
d
y
h
e
n
d
e
r
s
o
n
D
P
A
I
C
+
D
g
e
n
e
r
a
t
i
v
e
,
n
o
b
e
a
m
t
r
a
n
s
n
o
b
e
a
m
s
y
n
c
h
r
o
n
i
z
e
d
I
S
B
N
t
r
a
n
s
s
e
a
r
c
h
s
e
a
r
c
h
d
e
r
i
v
a
t
i
o
n
w
a
t
a
n
a
b
e
D
I
+
D
C
+
P
I
+
P
C
+
A
I
+
A
C
r
e
l
a
t
i
v
e
p
r
e
f
e
r
e
n
c
e
n
o
g
r
e
e
d
y
t
o
u
r
n
a
m
e
n
t
c
l
a
s
s
n
o
n
o
n
o
S
V
M
,
m
o
d
e
l
m
o
d
e
l
,
V
i
t
e
r
b
i
C
R
F
,
M
B
L
m
o
r
a
n
t
e
D
+
P
I
+
A
I
+
A
C
t
r
a
n
s
n
o
g
r
e
e
d
y
c
l
a
s
s
v
o
t
i
n
g
g
r
e
e
d
y
n
o
S
V
M
,
M
B
L
l
i
D
+
P
I
C
+
A
I
C
g
r
a
p
h
n
o
M
S
T
C
L
/
E
c
l
a
s
s
v
o
t
i
n
g
g
r
e
e
d
y
n
o
M
E
c
h
e
n
D
+
P
I
+
P
C
+
A
I
C
t
r
a
n
s
n
o
p
r
o
b
c
l
a
s
s
n
o
p
r
o
b
g
l
o
b
a
l
p
r
o
b
a
b
i
l
i
t
y
M
E
o
p
t
i
m
i
z
a
t
i
o
n
l
e
e
D
+
P
I
+
A
I
C
+
P
C
t
r
a
n
s
n
o
g
r
e
e
d
y
c
l
a
s
s
n
o
p
r
o
b
n
o
S
V
M
,
M
E
s
u
n
D
I
+
P
I
+
D
C
A
I
+
A
C
g
r
a
p
h
n
o
M
S
T
E
,
g
r
a
p
h
n
o
V
i
t
e
r
b
i
,
M
E
M
M
,
M
E
V
i
t
e
r
b
i
I
L
P
V
i
t
e
r
b
i
l
l
u
i
s
D
+
P
I
+
D
A
I
C
+
P
C
g
r
a
p
h
n
o
M
S
T
E
g
r
a
p
h
n
o
M
S
T
E
M
S
T
E
P
e
r
c
e
p
t
r
o
n
,
S
V
M
n
e
u
m
a
n
n
D
+
P
I
+
P
C
+
A
I
+
A
C
t
r
a
n
s
n
o
g
r
e
e
d
y
c
l
a
s
s
n
o
n
o
n
o
M
E
o
p
e
n
v
i
c
k
r
e
y
A
I
+
A
C
+
P
I
+
P
C
?
?
?
s
e
n
t
e
n
c
e
n
o
g
r
e
e
d
y
?
M
E
s
i
m
p
l
i
fi
c
a
t
i
o
n
,
c
l
a
s
s
r
i
e
d
e
l
P
A
I
C
?
?
?
M
a
r
k
o
v
n
o
C
u
t
t
i
n
g
?
M
I
R
A
L
o
g
i
c
P
l
a
n
e
N
e
t
w
o
r
k
w
a
n
g
P
I
+
A
I
C
t
r
a
n
s
,
n
o
g
r
e
e
d
y
,
c
l
a
s
s
n
o
p
r
o
b
n
o
S
V
M
,
M
E
g
r
a
p
h
M
S
T
C
L
/
E
M
I
R
A
T
a
b
l
e
1
2
:
S
u
m
m
a
r
y
o
f
s
y
s
t
e
m
a
r
c
h
i
t
e
c
t
u
r
e
s
t
h
a
t
p
a
r
t
i
c
i
p
a
t
e
d
i
n
t
h
e
c
l
o
s
e
d
a
n
d
o
p
e
n
c
h
a
l
l
e
n
g
e
s
.
T
h
e
c
l
o
s
e
d
-
c
h
a
l
l
e
n
g
e
s
y
s
t
e
m
s
a
r
e
s
o
r
t
e
d
b
y
m
a
c
r
o
l
a
b
e
l
e
d
F
1
s
c
o
r
e
o
n
t
h
e
W
S
J
+
B
r
o
w
n
c
o
r
p
u
s
.
B
e
c
a
u
s
e
s
o
m
e
o
p
e
n
-
c
h
a
l
l
e
n
g
e
s
y
s
t
e
m
s
d
i
d
n
o
t
i
m
p
l
e
m
e
n
t
s
y
n
t
a
c
t
i
c
p
a
r
s
i
n
g
,
t
h
e
s
e
s
y
s
t
e
m
s
a
r
e
s
o
r
t
e
d
b
y
l
a
b
e
l
e
d
F
1
s
c
o
r
e
o
f
t
h
e
s
e
m
a
n
t
i
c
d
e
p
e
n
d
e
n
c
i
e
s
o
n
t
h
e
W
S
J
+
B
r
o
w
n
c
o
r
p
u
s
.
O
n
l
y
t
h
e
s
y
s
t
e
m
s
t
h
a
t
h
a
v
e
a
c
o
r
r
e
s
p
o
n
d
i
n
g
p
a
p
e
r
i
n
t
h
e
p
r
o
c
e
e
d
i
n
g
s
a
r
e
i
n
c
l
u
d
e
d
.
S
y
s
t
e
m
s
t
h
a
t
p
a
r
t
i
c
i
p
a
t
e
d
i
n
b
o
t
h
c
h
a
l
l
e
n
g
e
s
a
r
e
l
i
s
t
e
d
o
n
l
y
i
n
t
h
e
c
l
o
s
e
d
c
h
a
l
l
e
n
g
e
.
A
c
r
o
n
y
m
s
u
s
e
d
:
D
-
s
y
n
t
a
c
t
i
c
d
e
p
e
n
d
e
n
c
i
e
s
,
P
-
p
r
e
d
i
c
a
t
e
,
A
-
a
r
g
u
m
e
n
t
,
I
-
i
d
e
n
t
i
fi
c
a
t
i
o
n
,
C
-
c
l
a
s
s
i
fi
c
a
t
i
o
n
.
O
v
e
r
a
l
l
a
r
c
h
.
s
t
a
n
d
s
f
o
r
t
h
e
c
o
m
p
l
e
t
e
s
y
s
t
e
m
a
r
c
h
i
t
e
c
t
u
r
e
;
D
A
r
c
h
.
s
t
a
n
d
s
f
o
r
t
h
e
a
r
c
h
i
t
e
c
t
u
r
e
o
f
t
h
e
s
y
n
t
a
c
t
i
c
p
a
r
s
e
r
;
D
C
o
m
b
.
i
n
d
i
c
a
t
e
s
i
f
t
h
e
fi
n
a
l
p
a
r
s
e
r
o
u
t
p
u
t
w
a
s
g
e
n
e
r
a
t
e
d
u
s
i
n
g
p
a
r
s
e
r
c
o
m
b
i
n
a
t
i
o
n
;
D
I
n
f
e
r
e
n
c
e
s
t
a
n
d
s
f
o
r
t
h
e
t
y
p
e
o
f
i
n
f
e
r
e
n
c
e
u
s
e
d
f
o
r
s
y
n
t
a
c
t
i
c
p
a
r
s
i
n
g
;
P
A
A
r
c
h
.
s
t
a
n
d
s
t
h
e
t
y
p
e
o
f
a
r
c
h
i
t
e
c
t
u
r
e
u
s
e
d
f
o
r
P
A
I
C
;
P
A
C
o
m
b
.
i
n
d
i
c
a
t
e
s
i
f
t
h
e
P
A
o
u
t
p
u
t
w
a
s
g
e
n
e
r
a
t
e
d
t
h
r
o
u
g
h
s
y
s
t
e
m
c
o
m
b
i
n
a
t
i
o
n
;
P
A
I
n
f
e
r
e
n
c
e
s
t
a
n
d
s
f
o
r
t
h
e
t
h
e
t
y
p
e
o
f
i
n
f
e
r
e
n
c
e
u
s
e
d
f
o
r
P
A
I
C
;
J
o
i
n
t
L
e
a
r
n
i
n
g
/
O
p
t
.
i
n
d
i
c
a
t
e
s
i
f
s
o
m
e
f
o
r
m
o
f
j
o
i
n
t
l
e
a
r
n
i
n
g
o
r
o
p
t
i
m
i
z
a
t
i
o
n
w
a
s
i
m
p
l
e
m
e
n
t
e
d
f
o
r
t
h
e
s
y
n
t
a
c
t
i
c
+
s
e
m
a
n
t
i
c
g
l
o
b
a
l
t
a
s
k
;
M
L
m
e
t
h
o
d
s
l
i
s
t
s
t
h
e
M
L
m
e
t
h
o
d
s
u
s
e
d
t
h
r
o
u
g
h
o
u
t
t
h
e
c
o
m
p
l
e
t
e
s
y
s
t
e
m
.
172
Exact Match Perfect Proposition F
1
(complete task) (semantic dependencies)
closed WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown
johansson 12.46 (1) 12.46 12.68 54.12 (1) 56.12 36.90
che 10.37 (2) 10.21 11.50 48.05 (2) 50.15 30.90
ciaramita 9.27 (3) 9.04 10.80 46.05 (3) 48.05 28.61
zhao 9.20 (4) 9.00 10.56 43.19 (4) 45.23 26.14
henderson 8.11 (5) 7.75 10.33 39.24 (5) 40.64 27.51
watanabe 7.79 (6) 7.54 9.39 36.44 (6) 38.09 22.72
yuret 7.65 (7) 7.33 9.62 34.61 (9) 36.13 21.78
zhang 7.40 (8) 7.46 7.28 34.96 (8) 36.25 24.22
li 7.12 (9) 6.71 9.62 32.08 (10) 33.45 20.62
samuelsson 6.94 (10) 6.62 8.92 35.20 (7) 36.96 20.22
chen 6.83 (11) 6.46 9.15 31.02 (12) 32.08 22.14
lee 6.69 (12) 6.29 9.15 31.40 (11) 32.52 22.18
morante 6.44 (13) 6.04 8.92 30.41 (14) 31.97 17.49
sun 5.38 (14) 4.96 7.98 30.43 (13) 31.51 21.40
baldridge 5.24 (15) 4.92 7.28 25.35 (15) 26.57 15.26
choi 3.33 (16) 3.50 2.58 24.77 (16) 25.71 17.37
trandabat 3.26 (17) 3.08 4.46 6.59 (18) 6.81 4.76
lluis 2.55 (18) 1.96 6.10 16.07 (17) 16.46 13.00
neumann 0.11 (19) 0.12 0.23 0.30 (19) 0.31 0.20
open
vickrey ? ? ? 44.94 (1) 46.68 30.28
riedel ? ? ? 42.77 (2) 44.18 31.15
zhang 8.14 (1) 8.04 8.92 35.46 (3) 36.74 24.84
li 6.90 (2) 6.46 9.62 29.91 (4) 31.30 18.41
wang 5.17 (3) 5.12 5.63 18.63 (5) 20.31 7.09
Table 13: Exact Match and Perfect Proposition F
1
scores for runs submitted in the closed and open
challenges. The closed-challenge systems are sorted in descending order of Exact Match scores on
the WSJ+Brown corpus. Open-challenge submissions are sorted in descending order of the Perfect
Proposition F
1
score. The number in parentheses next to the WSJ+Brown scores indicates the system
rank according to the corresponding scoring measure.
6.1 Exact Match and Perfect Propositions
Table 13 lists the Exact Match and Perfect Propo-
sition F
1
scores for test runs submitted in both
challenges. Both these scores measure the capac-
ity of a system to correctly parse structures with
granularity much larger than a simple dependency,
i.e., entire sentences for Exact Match and complete
propositions for Perfect Proposition F
1
(see Sec-
tion 2.2.2 for a formal definition of these evalua-
tion measures). The table indicates that these val-
ues are much smaller than the scores previously
reported, e.g., labeled macro F
1
. This is to be
expected: the probability of an incorrectly parsed
unit (sentence or proposition) is much larger given
its granularity. However, the main purpose of this
analysis is to investigate if systems that focused
on joint learning or optimization performed bet-
ter than others with respect to these global mea-
sures. This indeed seems to be the case for at
least two systems. The system of Johansson and
Nugues (2008), which jointly optimizes the la-
beled F
1
score (for semantic dependencies) and
then the labeled macro F
1
score (for the complete
task), increases its distance from the next ranked
system: its Perfect Proposition F
1
score is over
6 points higher than the score of the second sys-
tem in Table 13. The system of Henderson et
al. (2008), which was designed for joint learning
of the complete task, improves its rank from eighth
to fifth compared to the official results (Table 10).
6.2 Nonprojectivity
Table 14 shows the unlabeled F1 scores for pre-
diction of nonprojective syntactic dependencies.
Since nonprojectivity is quite rare, many teams
chose to ignore this issue. The table shows only
those systems that submitted well-formed depen-
dency trees, and whose output contained at least
one nonprojective link. The small number of non-
projective links in the training set makes it hard to
learn to predict such links, and this is also reflected
in the figures. In general, the figures for nonpro-
jective wh-movements and split clauses are higher,
and they are also the most common types. Also,
they are detectable by fairly simple patterns, such
as the presence of a wh-word or a pair of commas.
173
System All wh-mov. SpCl SpNP
choi 25.43 49.49 45.47 8.72
lee 46.26 50.30 64.84 20.69
nugues 46.15 58.96 59.26 11.32
samuelsson 24.47 38.15 0 9.83
titov 42.32 50.56 48.71 0
zhang 13.39 5.71 12.33 7.3
Table 14: Unlabeled F1-measures for nonprojec-
tive links. Results are given for all links, wh-
movements, split clauses, and split noun phrases.
6.3 Normalized SRL Performance
Table 6.3 lists the scores for the semantic sub-
task measured as the ratio of the labeled F
1
score
and LAS. As previously mentioned, this score es-
timates the performance of the SRL component
independent of the performance of the syntactic
parser. This analysis is not a substitute for the
actual experiment where the SRL components are
evaluated using correct syntactic information but,
nevertheless, it indicates several interesting facts.
First, the ranking of the top three systems in Ta-
ble 10 changes: the system of Che et al (2008)
is now ranked first, and the system of Johansson
and Nugues (2008) is second. This shows that Che
et al have a relatively stronger SRL component,
whereas Johansson and Nugues developed a bet-
ter parser. Second, several other systems improved
their ranking compared to Table 10: e.g., chen
from position thirteenth to ninth and choi from six-
teenth to eighth. This indicates that these systems
were penalized in the official ranking mainly due
to the relative poor performance of their parsers.
Note that this experiment is relevant only for
systems that implemented pipeline architectures,
where the semantic components are in fact sep-
arated from the syntactic ones; this excludes the
systems that blended syntax with SRL: henderson,
sun, and lluis. Furthermore, systems that had sig-
nificantly lower scores in syntax will receive an un-
reasonable boost in ranking according to this mea-
sure. Fortunately, there was only one such outlier
in this evaluation (neumann), shown in gray in the
table.
6.4 PropBank versus NomBank
Table 16 lists the labeled F
1
scores for semantic
dependencies for two different views of the test-
ing data sets: for propositions centered around ver-
bal predicates, i.e., from PropBank, and for propo-
sitions centered around nominal predicates, i.e.,
from NomBank.
Labeled F
1
/ LAS
closed WSJ+Brown WSJ Brown
neumann 137.60 (1) 140.94 108.93
che 90.51 (2) 91.42 82.21
johansson 89.98 (3) 90.70 83.40
ciaramita 89.49 (4) 90.32 81.89
zhao 87.88 (5) 88.75 79.78
yuret 84.35 (6) 85.30 75.34
samuelsson 84.20 (7) 85.24 74.51
choi 83.52 (8) 83.63 82.64
chen 82.22 (9) 82.89 76.11
morante 81.92 (10) 82.73 74.43
zhang 81.67 (11) 82.45 74.46
henderson 81.66 (12) 82.32 75.47
watanabe 81.26 (13) 82.18 72.61
lee 81.01 (14) 81.63 75.33
li 80.69 (15) 81.53 73.23
baldridge 78.37 (16) 79.33 69.38
sun 77.68 (17) 78.29 72.15
lluis 75.77 (18) 76.20 72.24
trandabat 47.68 (19) 48.12 43.85
open
zhang 82.33 82.91 76.87
li 79.58 80.44 72.05
wang 79.38 82.35 55.37
Table 15: Ratio of the labeled F
1
score for seman-
tic dependencies and LAS for syntactic dependen-
cies. Systems are sorted in descending order of this
ratio score on the WSJ+Brown corpus. We only
show systems that participated in both the syntac-
tic and semantic subtasks.
The table indicates that, generally, systems per-
formed much worse on nominal predicates than
on verbal predicates. This is to be expected con-
sidering that there is significant body of previ-
ous work that analyzes the SRL problem on Prop-
Bank, but minimal work for NomBank. On aver-
age, the difference between the labeled F
1
scores
for verbal predicates and nominal predicates on the
WSJ+Brown corpus is 7.84 points. Furthermore,
the average difference between labeled F
1
scores
on the Brown corpus alone is 12.36 points. This in-
dicates that the problem of SRL for nominal predi-
cates is more sensitive to domain changes than the
equivalent problem for verbal predicates. Our con-
jecture is that, because there is very little syntac-
tic structure between nominal predicates and their
arguments, SRL models for nominal predicates se-
lect mainly lexical features, which are more brittle
than syntactic or other non-lexicalized features.
Remarkably, there is one system (baldridge)
which performed better on the WSJ+Brown for
nominal predicates than verbal predicates. Un-
fortunately, this group did not submit a system-
description paper so it is not clear what was their
approach.
174
Labeled F
1
Labeled F
1
(verbal predicates) (nominal predicates)
closed WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown
johansson 84.45 (1) 86.37 71.87 74.32 (2) 75.42 60.13
che 80.46 (2) 82.17 69.33 75.18 (1) 76.64 56.87
ciaramita 80.15 (3) 82.09 67.62 73.17 (4) 74.42 57.69
zhao 77.67 (4) 79.40 66.38 73.28 (3) 74.69 54.81
samuelsson 76.17 (5) 78.03 64.00 68.13 (7) 69.58 49.24
yuret 75.91 (6) 77.88 63.02 68.81 (5) 69.98 53.58
zhang 74.82 (7) 76.62 63.15 65.61 (11) 66.82 50.18
li 74.36 (8) 76.14 62.92 62.61 (14) 63.76 47.09
henderson 73.80 (9) 75.40 63.36 66.26 (10) 67.44 50.73
watanabe 73.06 (10) 75.02 60.34 67.15 (8) 68.37 50.92
sun 72.97 (11) 74.45 63.50 58.68 (15) 59.73 45.75
morante 72.81 (12) 74.36 62.72 66.50 (9) 67.92 47.97
lee 72.34 (13) 74.15 60.49 62.83 (13) 63.66 52.18
chen 72.02 (14) 73.49 62.46 65.02 (12) 66.14 50.48
choi 70.00 (15) 71.28 61.71 56.16 (16) 57.19 44.05
baldridge 67.02 (16) 68.64 56.50 68.57 (6) 69.78 52.96
lluis 62.42 (17) 63.49 55.49 42.15 (17) 42.81 34.22
trandabat 42.88 (18) 43.79 37.06 37.14 (18) 37.89 27.50
neumann 22.87 (19) 23.53 18.24 21.7 (19) 22.04 17.14
open
vickrey 78.41 (1) 79.75 69.57 71.86 (1) 73.29 53.25
riedel 77.13 (2) 78.72 66.75 70.25 (2) 71.03 60.17
zhang 75.00 (3) 76.62 64.44 66.76 (3) 67.79 53.76
li 73.74 (4) 75.57 62.05 61.24 (5) 62.38 46.36
wang 67.50 (5) 70.34 49.72 66.53 (4) 69.83 28.96
Table 16: Labeled F
1
scores for frames centered around verbal and nominal predicates. The number in
parentheses next to the WSJ+Brown scores indicates the system rank in the corresponding data set.
Systems can mitigate the inherent differences
between verbal and nominal predicates with dif-
ferent models for the two sub-problems. This was
indeed the approach taken by two out of the top
three systems (johansson and che). Johansson and
Nugues (2008) developed different models for ver-
bal and nominal predicates and implemented sep-
arate feature selection processes for each model.
Che et al (2008) followed the same method but
they also implemented separate domain constraints
for inference for the two models.
7 Conclusion
The previous four CoNLL shared tasks popular-
ized and, without a doubt, boosted research in se-
mantic role labeling and dependency parsing. This
year?s shared task introduces a new task that es-
sentially unifies the problems addressed in the past
four years under a unique, dependency-based for-
malism. This novel task is attractive both from
a research perspective and an application-oriented
perspective:
? We believe that the proposed dependency-
based representation is a better fit for many
applications (e.g., Information Retrieval, In-
formation Extraction) where it is often suffi-
cient to identify the dependency between the
predicate and the head of the argument con-
stituent rather than extracting the complete ar-
gument constituent.
? It was shown that the extraction of syntac-
tic and semantic dependencies can be per-
formed with state-of-the-art performance in
linear time (Ciaramita et al, 2008). This can
give a significant boost to the adoption of this
technology in real-world applications.
? We hope that this shared task will motivate
several important research directions. For ex-
ample, is the dependency-based representa-
tion better for SRL than the constituent-based
formalism? Does joint learning improve syn-
tactic and semantic analysis?
? Surface (string related patterns, syntax, etc.)
linguistic features can often be detected with
greater reliability than deep (semantic) fea-
tures. In contrast, deep features can cover
more ground because they regularize across
differences in surface strings. Machine learn-
ing systems can be more effective by using
evidence from both deep and surface features
jointly (Zhao, 2005).
175
Even though this shared task was more complex
than the previous shared tasks, 22 different teams
submitted results in at least one of the challenges.
Building on this success, we hope to expand this
effort in the future with evaluations on multiple
languages and on larger out-of-domain corpora.
Acknowledgments
We want to thank the following people who helped
us with the generation of the data sets: Jes?us
Gim?enez, for generating the predicted POS tags
with his SVMTool POS tagger, and Massimiliano
Ciaramita, for generating columns 1, 2 and 3 in the
open-challenge corpus with his semantic tagger.
We also thank the following people who helped
us with the organization of the shared task: Paola
Merlo and James Henderson for the idea and the
implementation of the Exact Match measure, Se-
bastian Riedel for his dependency visualization
software,
12
Hai Zhao, for the the idea of the F
1
ratio score, and Carlos Castillo, for help with the
shared task website. Last but not least, we thank
the organizers of the previous four shared tasks:
Sabine Buchholz, Xavier Carreras, Ryan McDon-
ald, Amit Dubey, Johan Hall, Yuval Krymolowski,
Sandra K?ubler, Erwin Marsi, Jens Nilsson, Sebas-
tian Riedel, and Deniz Yuret. This shared task
would not have been possible without their previ-
ous effort.
Mihai Surdeanu is a research fellow in the
Ram?on y Cajal program of the Spanish Ministry of
Science and Technology. Richard Johansson was
funded by the Swedish National Graduate School
of Language Technology (GSLT). Adam Meyers?
participation was supported by the National Sci-
ence Foundation, award CNS-0551615 (Towards
a Comprehensive Linguistic Annotation of Lan-
guage) and IIS-0534700 (Collaborative Research:
Structure Alignment-based Machine Translation).
Llu??s M`arquez?s participation was supported by
the Spanish Ministry of Education and Science,
through research projects Trangram (TIN2004-
07925-C03-02) and OpenMT (TIN2006-15307-
C03-02).
References
X. Carreras. 2007. Experiments with a Higher-Order
Projective Dependency Parser. In Proc. of CoNLL-
2007 Shared Task.
12
http://code.google.com/p/whatswrong/
X. Carreras and L. M`arquez. 2005. Introduction to the
CoNLL-2005 Shared Task: Semantic Role Labeling.
In Proc. of CoNLL-2005.
X. Carreras and L. M`arquez. 2004. Introduction to the
CoNLL-2004 Shared Task: Semantic Role Labeling.
In Proc. of CoNLL-2004.
W. Che, Z. Li, Y. Hu, Y. Li, B. Qin, T. Liu and S.
Li. 2008. A Cascaded Syntactic and Semantic De-
pendency Parsing System. In Proc. of CoNLL-2008
Shared Task.
E. Chen, L. Shi and D. Hu. 2008. Probabilistic Model
for Syntactic and Semantic Dependency Parsing. In
Proc. of CoNLL-2008 Shared Task.
Chinchor, N. and P. Robinson. 1998. MUC-7
Named Entity Task Definition. In Proc. of Seventh
Message Understanding Conference (MUC-7).
http://www.itl.nist.gov/iaui/894.02/related projects/
/muc/proceedings/muc 7 toc.html.
Chomsky, Noam. 1981. Lectures on Government and
Binding. Foris Publications, Dordrecht.
Y.J. Chu and T.H. Liu. 1965. On the Shortest Ar-
borescence of a Directed Graph. In Science Sinica,
14:1396-1400.
M. Ciaramita, G. Attardi, F. Dell?Orletta, and M. Sur-
deanu. 2008. DeSRL: A Linear-Time Semantic
Role Labeling System. In Proc. of CoNLL-2008
Shared Task.
M. Ciaramita and Y. Altun. 2006. Broad Coverage
Sense Disambiguation and Information Extraction
with a Supersense Sequence Tagger. In Proc. of
EMNLP.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
J. Edmonds. 1967. Optimum Branchings. In Jour-
nal of Research of the National Bureau of Standards,
71B:233?240.
J. Eisner. 2000. Bilexical Grammars and Their Cubic-
Time Parsing Algorithms. New Developments in
Parsing Algorithms, Kluwer Academic Publishers.
W. N. Francis and H. Ku?cera. 1964. Brown Corpus.
Manual of Information to accompany A Standard
Corpus of Present-Day Edited American English, for
use with Digital Computers. Revised 1971, Revised
and Amplified 1979.
C. Fellbaum, editor. 1998. WordNet: An electronic
lexical database. MIT Press.
J. Gim?enez and L. M`arquez. 2004. SVMTool: A gen-
eral POS tagger generator based on Support Vector
Machines. In Proc. of LREC.
K. Hacioglu. 2004. Semantic Role Labeling Using De-
pendency Trees. In Proc. of COLING-2004.
176
J. Henderson, P. Merlo, G. Musillo and I. Titov. 2008.
A Latent Variable Model of Synchronous Parsing for
Syntactic and Semantic Dependencies. In Proc. of
CoNLL-2008 Shared Task.
R. Johansson and P. Nugues. 2008. Dependency-
based Syntactic?Semantic Analysis with PropBank
and NomBank. In Proc. of CoNLL-2008 Shared
Task.
R. Johansson and P. Nugues. 2007. Extended
Constituent-to-Dependency Conversion for English.
In Proc. of NODALIDA.
X. Llu??s and L. M`arquez. 2008. A Joint Model for
Parsing Syntactic and Semantic Dependencies. In
Proc. of CoNLL-2008 Shared Task.
D. Magerman. 1994. Natural Language Parsing as Sta-
tistical Pattern Recognition. Ph.D. thesis, Stanford
University.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a Large Annotated Corpus of En-
glish: the Penn Treebank. Computational Linguis-
tics, 19.
R. McDonald, F. Pereira, K. Ribarov and J. Hajic.
2005. Non-Projective Dependency Parsing using
Spanning Tree Algorithms In Proc. of HLT-EMNLP.
A. Meyers, R. Grishman, M. Kosaka, and S. Zhao.
2001. Covering Treebanks with GLARF. In Proc.
of the ACL/EACL 2001 Workshop on Sharing Tools
and Resources for Research and Education.
Meyers, A., R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman. 2004.
The NomBank Project: An Interim Report. In
NAACL/HLT 2004 Workshop Frontiers in Corpus
Annotation, Boston.
J. Nivre, J. Hall, J. Nilsson and G. Eryigit. 2006. La-
beled Pseudo-Projective Dependency Parsing with
Support Vector Machines. In Proc. of CoNLL-X
Shared Task.
J. Nivre, J. Hall, S. K?ubler, R. McDonald, J. Nilsson,
S. Riedel, D. Yuret. 2007. The CoNLL 2007 Shared
Task on Dependency Parsing. In Proc. of CoNLL-
2007.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryi?git, S.
K?ubler, S. Marinov, and E. Marsi. 2007b. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language En-
gineering, 13(2):95?135.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An Annotated Corpus of Seman-
tic Roles. Computational Linguistics, 31(1).
S. Riedel and I. Meza-Ruiz. 2008. Collective Seman-
tic Role Labelling with Markov Logic. In Proc. of
CoNLL-2008 Shared Task.
Y. Samuelsson, O. T?ackstr?om, S. Velupillai, J. Eklund,
M. Fishel and M. Saers. 2008. Mixing and Blending
Syntactic and Semantic Dependencies. In Proc. of
CoNLL-2008 Shared Task.
W. Sun, H. Li and Z. Sui. 2008. The Integration of De-
pendency Relation Classification and Semantic Role
Labeling Using Bilayer Maximum Entropy Markov
Models. In Proc. of CoNLL-2008 Shared Task.
E. F. Tjong Kim San and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 Shared Task: Language-
Independent Named Entity Recognition. In Proc. of
CoNLL-2003.
D. Vickrey and D. Koller. 2008. Applying Sentence
Simplification to the CoNLL-2008 Shared Task. In
Proc. of CoNLL-2008 Shared Task.
R. Weischedel and A. Brunstein. 2005. BBN pronoun
coreference and entity type corpus. Technical report,
Linguistic Data Consortium.
H. Yamada and Y. Matsumoto. 2003. Statistical De-
pendency Analysis with Support Vector Machines.
In Proc. of IWPT.
Y. Zhang, R. Wang and H. Uszkoreit. 2008. Hybrid
Learning of Dependency Structures from Heteroge-
neous Linguistic Resources. In Proc. of CoNLL-
2008 Shared Task.
Zhao, S. 2005. Information Extraction from Multiple
Syntactic Sources. Ph.D. thesis, NYU.
177
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 94?102,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
On the Role of Morphosyntactic Features in Hindi Dependency Parsing 
 
Bharat Ram Ambati*, Samar Husain*, Joakim Nivre? and Rajeev Sangal* 
*Language Technologies Research Centre, IIIT-Hyderabad, India. 
?Department of Linguistics and Philology, Uppsala University, Sweden. 
{bharat,samar}@research.iiit.ac.in, joakim.nivre@lingfil.uu.se, san-
gal@mail.iiit.ac.in 
 
 
 
 
Abstract 
This paper analyzes the relative importance of 
different linguistic features for data-driven 
dependency parsing of Hindi, using a feature 
pool derived from two state-of-the-art parsers.  
The analysis shows that the greatest gain in 
accuracy comes from the addition of morpho-
syntactic features related to case, tense, aspect 
and modality. Combining features from the 
two parsers, we achieve a labeled attachment 
score of 76.5%, which is 2 percentage points 
better than the previous state of the art. We fi-
nally provide a detailed error analysis and 
suggest possible improvements to the parsing 
scheme. 
1 Introduction 
The dependency parsing community has since a 
few years shown considerable interest in parsing 
morphologically rich languages with flexible word 
order. This is partly due to the increasing availabil-
ity of dependency treebanks for such languages, 
but it is also motivated by the observation that the 
performance obtained for these languages has not 
been very high (Nivre et al, 2007a). Attempts at 
handling various non-configurational aspects in 
these languages have pointed towards shortcom-
ings in traditional parsing methodologies (Tsarfaty 
and Sima'an, 2008; Eryigit et al, 2008; Seddah et 
al., 2009; Husain et al, 2009; Gadde et al, 2010). 
Among other things, it has been pointed out that 
the use of language specific features may play a 
crucial role in improving the overall parsing per-
formance. Different languages tend to encode syn-
tactically relevant information in different ways, 
and it has been hypothesized that the integration of 
morphological and syntactic information could be 
a key to better accuracy. However, it has also been 
noted that incorporating these language specific 
features in parsing is not always straightforward 
and many intuitive features do not always work in 
expected ways. 
In this paper, we are concerned with Hindi, an 
Indian language with moderately rich morphology 
and relatively free word order.  There have been 
several previous attempts at parsing Hindi as well 
as other Indian languages (Bharati et al, 1995, 
Bharati et al, 2009b). Many techniques were tried 
out recently at the ICON09 dependency parsing 
tools contest (Husain, 2009). Both the best per-
forming system (Ambati et al, 2009a) and the sys-
tem in second place (Nivre, 2009b) used a 
transition-based approach to dependency parsing, 
as implemented in MaltParser (Nivre et al, 2007b). 
Other data driven parsing efforts for Indian lan-
guages in the past have been Bharati et al (2008), 
Husain et al (2009), Mannem et al (2009b) and 
Gadde et al (2010). 
In this paper, we continue to explore the transi-
tion-based approach to Hindi dependency parsing, 
building on the state-of-the-art results of Ambati et 
al. (2009a) and Nivre (2009b) and exploring the 
common pool of features used by those systems. 
Through a series of experiments we select features 
incrementally to arrive at the best parser features. 
The primary purpose of this investigation is to 
study the role of different morphosyntactic features 
in Hindi dependency parsing, but we also want to 
improve the overall parsing accuracy. Our final 
results are 76.5% labeled and 91.1% unlabeled at-
tachment score, improving previous results by 2 
and 1 percent absolute, respectively. In addition to 
this, we also provide an error analysis, isolating 
specific linguistic phenomena and/or other factors 
that impede the overall parsing performance, and 
suggest possible remedies for these problems. 
94
2 The Hindi Dependency Treebank 
Hindi is a free word order language with SOV as 
the default order. This can be seen in (1), where 
(1a) shows the constituents in the default order, 
and the remaining examples show some of the 
word order variants of (1a). 
 
(1) a. malaya  ne     sameer      ko     kitaba   dii.  
          Malay   ERG  Sameer    DAT   book    gave 
        ?Malay gave the book to Sameer? (S-IO-DO-V)1 
       b. malaya ne kitaba sameer ko dii. (S-DO-IO-V) 
       c. sameer ko malaya ne kitaba dii. (IO-S-DO-V) 
       d. sameer ko kitaba malaya ne dii. (IO-DO-S-V) 
       e. kitaba malaya ne sameer ko dii. (DO-S-IO-V) 
        f. kitaba sameer ko malaya ne dii.  (DO-IO-S-V) 
 
Hindi also has a rich case marking system, al-
though case marking is not obligatory. For exam-
ple, in (1), while the subject and indirect object are 
explicitly marked for the ergative (ERG) and da-
tive (DAT) cases, the direct object is unmarked for 
the accusative.  
The Hindi dependency treebank (Begum et al, 
2008) used for the experiment was released as part 
of the ICON09 dependency parsing tools contest 
(Husain, 2009). The dependency framework (Bha-
rati et al, 1995) used in the treebank is inspired by 
Panini?s grammar of Sanskrit. The core labels, 
called karakas, are syntactico-semantic relations 
that identify the participant in the action denoted 
by the verb. For example, in (1), ?Malay? is the 
agent, ?book? is the theme, and ?Sameer? is the be-
neficiary in the activity of ?give?. In the treebank, 
these three labels are marked as k1, k2, and k4 re-
spectively. Note, however, that the notion of kara-
ka does not capture the ?global? semantics of 
thematic roles; rather it captures the elements of 
the ?local semantics? of a verb, while also taking 
cues from the surface level morpho-syntactic in-
formation (Vaidya et al, 2009).  The syntactic re-
lational cues (such as case markers) help identify 
many of the karakas. In general, the highest availa-
ble karaka,2 if not case-marked, agrees with the 
verb in an active sentence.  In addition, the tense, 
                                                          
1 S=Subject; IO=Indirect Object; DO=Direct Object; 
V=Verb; ERG=Ergative; DAT=Dative 
2 These are the karta karaka (k1) and karma karaka (k2). k1 
and k2 can be roughly translated as ?agent? and ?theme? re-
spectively. For a complete description of the tagset and the 
dependency scheme, see Begum et al (2008) and Bharati et al 
(2009a).  
aspect and modality (TAM) marker can many a 
times control the case markers that appear on k1. 
For example, in (1) ?Malay? takes an ergative case 
because of the past perfective TAM marker (that 
appears as a suffix in this case) of the main verb 
?gave?. Many dependency relations other than ka-
rakas are purely syntactic. These include relations 
such as noun modifier (nmod), verb modifier 
(vmod), conjunct relation (ccof), etc. 
Each sentence is manually chunked and then an-
notated for dependency relations. A chunk is a mi-
nimal, non-recursive structure consisting of 
correlated groups of words (Bharati et al, 2006). A 
node in a dependency tree represents a chunk head. 
Each lexical item in a sentence is also annotated 
with its part-of-speech (POS).  For all the experi-
ments described in this paper we use gold POS and 
chunk tags. Together, a group of lexical items with 
some POS tags within a chunk can be utilized to 
automatically compute coarse grained morphosyn-
tactic information. For example, such information 
can represent the postposition/case-marking in the 
case of noun chunks, or it may represent the TAM 
information in the case of verb chunks. In the ex-
periments conducted for this paper this local in-
formation is automatically computed and 
incorporated as a feature of the head of a chunk. 
As we will see later, such information proves to be 
extremely crucial during dependency parsing. 
For all the experiments discussed in section 4, 
the training and development data size was 1500 
and 150 sentences respectively. The training and 
development data consisted of ~22k and ~1.7k 
words respectively. The test data consisted of 150 
sentences (~1.6k words). The average sentence 
length is 19.85. 
3 Transition-Based Dependency Parsing 
A transition-based dependency parser is built of 
two essential components (Nivre, 2008): 
 
? A transition system for mapping sentences to 
dependency trees 
? A classifier for predicting the next transition for 
every possible system configuration 
 
 
 
 
95
 PTAG CTAG FORM LEMMA DEPREL CTAM OTHERS 
Stack:        top 1 5 1 7  9  
Input:        next 1 5 1 7  9  
Input:        next+1 2 5 6 7    
Input:        next+2 2       
Input:        next+3 2       
Stack:       top-1 3       
String:      predecessor of top 3       
Tree:        head of top 4       
Tree:        leftmost dep of next 4 5 6     
Tree:        rightmost dep of top     8   
Tree:        left sibling of rightmost dep of top     8   
Merge:     PTAG of top and next       10 
Merge:     CTAM and DEPREL of top       10 
 
Table 1. Feature pool based on selection from Ambati et al (2009a) and Nivre (2009b). 
 
Given these two components, dependency parsing 
can be realized as deterministic search  through the 
transition system, guided by the classifier. With 
this technique, parsing can be performed in linear 
time for projective dependency trees. Like Ambati 
et al (2009a) and Nivre (2009b), we use MaltPars-
er, an open-source implementation of transition-
based dependency parsing with a variety of transi-
tion systems and customizable classifiers.3 
3.1 Transition System 
Previous work has shown that the arc-eager projec-
tive transition system first described in Nivre 
(2003) works well for Hindi (Ambati et al, 2009a; 
Nivre, 2009b). A parser configuration in this sys-
tem contains a stack holding partially processed 
tokens, an input buffer containing the remaining 
tokens, and a set of arcs representing the partially 
built dependency tree. There are four possible tran-
sitions (where top is the token on top of the stack 
and next is the next token in the input buffer): 
 
? Left-Arc(r): Add an arc labeled r from next to 
top; pop the stack. 
? Right-Arc(r): Add an arc labeled r from top to 
next; push next onto the stack. 
? Reduce: Pop the stack. 
? Shift: Push next onto the stack. 
 
Although this system can only derive projective 
dependency trees, the fact that the trees are labeled 
                                                          
3 MaltParser is available at http://maltparser.org. 
allows non-projective dependencies to be captured 
using the pseudo-projective parsing technique pro-
posed in Nivre and Nilsson (2005). 
3.2 Classifiers 
Classifiers can be induced from treebank data us-
ing a wide variety of different machine learning 
methods, but all experiments reported below use 
support vector machines with a polynomial kernel, 
as implemented in the LIBSVM package (Chang 
and Lin, 2001) included in MaltParser. The task of 
the classifier is to map a high-dimensional feature 
vector representation of a parser configuration to 
the optimal transition out of that configuration. The 
features used in our experiments represent the fol-
lowing attributes of input tokens: 
 
? PTAG: POS tag of chunk head. 
? CTAG: Chunk tag. 
? FORM: Word form of chunk head. 
? LEMMA: Lemma of chunk head. 
? DEPREL: Dependency relation of chunk. 
? CTAM: Case and TAM markers of chunk.  
 
The PTAG corresponds to the POS tag associated 
with the head of the chunk, whereas the CTAG 
represent the chunk tag. The FORM is the word 
form of the chunk head, and the LEMMA is auto-
matically computed with the help of a morphologi-
cal analyzer. CTAM gives the local 
morphosyntactic features such as case markers 
(postpositions/suffixes) for nominals and TAM 
markers for verbs (cf. Section 2). 
96
The pool of features used in the experiments are 
shown in Table 1, where rows denote tokens in a 
parser configuration ? defined relative to the stack, 
the input buffer, the partially built dependency tree 
and the input string ? and columns correspond to 
attributes. Each non-empty cell represents a fea-
ture, and features are numbered for easy reference. 
4 Feature Selection Experiments 
Starting from the union of the feature sets used by 
Ambati et al (2009a and by Nivre (2009b), we 
first used 5-fold cross-validation on the combined 
training and development sets from the ICON09 
tools contest to select the pool of features depicted 
in Table 1, keeping all features that had a positive 
effect on both labeled and unlabeled accuracy. We 
then grouped the features into 10 groups (indicated 
by numbers 1?10 in Table 1) and reran the cross-
validation, incrementally adding different feature 
groups in order to analyze their impact on parsing 
accuracy. The result is shown in Figure 1. 
30
36
42
48
54
60
66
72
78
84
90
Ex
p1
Ex
p2
Ex
p3
Ex
p4
Ex
p5
Ex
p6
Ex
p7
Ex
p8
Ex
p9
Ex
p1
0
UAS
LAS
 
Figure 1. UAS and LAS of experiments 1-10; 5-fold 
cross-validation on training and development data of the 
ICON09 tools contest. 
 
Experiment 1: Experiment 1 uses a baseline 
model with only four basic features: PTAG and 
FORM of top and next. This results in a labeled 
attachment score (LAS) of 41.7% and an unlabeled 
attachment score (UAS) of 68.2%. 
Experiments 2?3: In experiments 2 and 3, the 
PTAG of contextual words of next and top are 
added. Of all the contextual words, next+1, 
next+2, next+3, top-1 and predecessor of top were 
found to be useful.4 Adding these contextual fea-
tures gave a modest improvement to 45.7% LAS 
and 72.7% UAS. 
Experiment 4: In experiment 4, we used the 
PTAG information of nodes in the partially built 
tree, more specifically the syntactic head of top 
and the leftmost dependent of next. Using these 
features gave a large jump in accuracy to 52% 
LAS and 76.8% UAS. This is because partial in-
formation is helpful in making future decisions. 
For example, a coordinating conjunction can have 
a node of any PTAG category as its child. But all 
the children should be of same category. Knowing 
the PTAG of one child therefore helps in identify-
ing other children as well. 
Experiments 5?7: In experiments 5, 6 and 7, 
we explored the usefulness of CTAG, FORM, and 
LEMMA attributes. These features gave small in-
cremental improvements in accuracy; increasing 
LAS to 56.4% and UAS to 78.5%. It is worth not-
ing in particular that the addition of LEMMA 
attributes only had a marginal effect on accuracy, 
given that it is generally believed that this type of 
information should be beneficial for richly in-
flected languages. 
Experiment 8: In experiment 8, the DEPREL of 
nodes in the partially formed tree is used. The 
rightmost child and the left sibling of the rightmost 
child of top were found to be useful. This is be-
cause, if we know the dependency label of one of 
the children, then the search space for other child-
ren gets reduced. For example, a verb cannot have 
more than one k1 or k2. If we know that the parser 
has assigned k1 to one of its children, then it 
should use different labels for the other children. 
The overall effect on parsing accuracy is neverthe-
less very marginal, bringing LAS to 56.5% and 
UAS to 78.6%. 
Experiment 9: In experiment 9, the CTAM 
attribute of top and next is used. This gave by far 
the greatest improvement in accuracy with a huge 
jump of around 10% in LAS (to 66.3%) and 
slightly less in UAS (to 84.7%). Recall that CTAM 
consists of two important morphosyntactic fea-
tures, namely, case markers (as suffixes or postpo-
sitions) and TAM markers. These feature help 
because (a) case markers are important surface  
                                                          
4 The predecessor of top is the word occurring immediately 
before top in the input string, as opposed to top-1, which is the 
word immediately below top in the current stack. 
97
 
Figure 2. Precision and Recall of some important dependency labels. 
 
cues that help identify various dependency rela-
tions, and (b) there exists a direct mapping be-
tween many TAM labels and the nominal case 
markers because TAMs control the case markers of 
some nominals. As expected, our experiments 
show that the parsing decisions are certainly more 
accurate after using these features. In particular, (a) 
and (b) are incorporated easily in the parsing 
process.  
In a separate experiment we also added some 
other morphological features such as gender, num-
ber and person for each node. Through these fea-
tures we expected to capture the agreement in 
Hindi. The verb agrees in gender, number and per-
son with the highest available karaka. However, 
incorporating these features did not improve pars-
ing accuracy and hence these features were not 
used in the final setting. We will have more to say 
about agreement in section 5. 
Experiment 10: In experiment 10, finally, we 
added conjoined features, where the conjunction of 
POS of next and top and of CTAM and DEPREL 
of top gave slight improvements. This is because a 
child-parent pair type can only take certain labels. 
For example, if the child is a noun and the parent is 
a verb, then all the dependency labels reflecting 
noun, adverb and adjective modifications are not 
relevant. Similarly, as noted earlier, certain case-
TAM combinations demand a particular set of la-
bels only. This can be captured by the combination 
tried in this experiment. 
Experiment 10 gave the best results in the cross-
validation experiments. The settings from this ex-
periment were used to get the final performance on 
the test data. Table 2 shows the final results along 
with the results of the first and second best per-
forming systems in the ICON09 tools contest. We 
see that our system achieved an improvement of 2 
percentage points in LAS and 1 percentage point in 
UAS over the previous state of the art reported in 
Ambati et al (2009a). 
 
System LAS UAS 
Ambati et al (2009a) 74.5 90.1 
Nivre (2009b) 73.4 89.8 
Our system 76.5 91.1 
 
Table 2. Final results on the test data from the ICON09 
tools contest. 
5 Error Analysis 
In this section we provide a detailed error analysis 
on the test data and suggest possible remedies for 
problems noted. We note here that other than the 
reasons mentioned in this section, small treebank 
size could be another reason for low accuracy of 
the parser. The training data used for the experi-
ments only had ~28.5k words. With recent work on 
Hindi Treebanking (Bhatt et al, 2009) we expect 
to get more annotated data in the near future. 
Figure 2 shows the precision and recall of some 
important dependency labels in the test data. The 
labels in the treebank are syntacto-semantic in na-
ture. Morph-syntactic features such as case mark-
ers and/or TAM labels help in identifying these 
labels correctly. But lack of nominal postpositions 
can pose problems. Recall that many case mark-
ings in Hindi are optional. Also recall that the verb 
agrees with the highest available karaka. Since 
agreement features do not seem to help, if both k1 
and k2 lack case markers, k1-k2 disambiguation 
becomes difficult (considering that word order in-
formation cannot help in this disambiguation). In 
the case of k1 and k2, error rates for instances that 
lack post-position markers are 60.9% (14/23) and 
65.8% (25/38), respectively. 
 
 
98
 Correct Incorrect 
  k1 k1s k2 pof k7p k7t k7 others 
k1 184 5 3 8 3  1  3 
k1s 12 6  1 6    1 
k2 126 14  1 7 5   11 
pof 54 1 8 4      
k7p 54 3  7   1 2 3 
k7t 27 3  3 3  1  10 
k7 3 2   2 4    
 
Table 3. Confusion matrix for important labels. The 
diagonal under ?Incorrect? represents attachment errors. 
 
Table 3 shows the confusion matrix for some 
important labels in the test data. As the present 
information available for disambiguation is not 
sufficient, we can make use of some semantics to 
resolve these ambiguities. Bharati et al (2008) and 
Ambati et al (2009b) have shown that this ambi-
guity can be reduced using minimal semantics. 
They used six semantic features: human, non-
human, in-animate, time, place and abstract. Using 
these features they showed that k1-k2 and k7p-k7t 
ambiguities can be resolved to a great extent. Of 
course, automatically extracting these semantic 
features is in itself a challenging task, although 
?vrelid (2008) has shown that animacy features 
can be induced automatically from data. 
In section 4 we mentioned that a separate expe-
riment explored the effectiveness of morphological 
features like gender, number and person. Counter 
to our intuitions, these features did not improve the 
overall accuracy. Accuracies on cross-validated 
data while using these features were less than the 
best results with 66.2% LAS and 84.6% UAS. 
Agreement patterns in Hindi are not straightfor-
ward. For example, the verb agrees with k2 if the 
k1 has a post-position; it may also sometimes take 
the default features. In a passive sentence, the verb 
agrees only with k2. The agreement problem wor-
sens when there is coordination or when there is a 
complex verb. It is understandable then that the 
parser is unable to learn the selective agreement 
pattern which needs to be followed. Similar prob-
lems with agreement features have also been noted 
by Goldberg and Elhadad (2009). 
In the following sections, we analyze the errors 
due to different constructions and suggest possible 
remedies.  
 
 
5.1 Simple Sentences 
A simple sentence is one that has only one main 
verb. In these sentences, the root of the dependen-
cy tree is the main verb, which is easily identified 
by the parser. The main problem is the correct 
identification of the argument structure. Although 
the attachments are mostly correct, the dependency 
labels are error prone. Unlike in English and other 
more configurational languages, one of the main 
cues that help us identify the arguments is to be 
found in the nominal postpositions. Also, as noted 
earlier these postpositions are many times con-
trolled by the TAM labels that appear on the verb. 
There are four major reasons for label errors in 
simple sentences: (a) absence of postpositions, (b) 
ambiguous postpositions, (c) ambiguous TAMs, 
and (d) inability of the parser to exploit agreement 
features. For example in (2), raama and phala are 
arguments of the verb khaata. Neither of them has 
any explicit case marker. This makes it difficult for 
the parser to identify the correct label for these 
nodes. In (3a) and (3b) the case marker se is ambi-
guous. It signifies ?instrument? in (3b) and ?agent? 
in (3a). 
 
(2) raama    phala    khaata    hai 
     ?Ram?    ?fruit?    ?eat?       ?is? 
     ?Ram eats a fruit? 
 
(3) a. raama   se     phala  khaayaa nahi    gaya 
         ?Ram? INST ?fruit?  ?eat?      ?not?  ?PAST? 
         ?Ram could not eat the fruit? 
     b. raama  chamach   se     phala    khaata  hai 
         ?Ram?  ?spoon?   INST ?fruit?    ?eat?     ?is? 
          ?Ram eats fruit with spoon? 
5.2 Embedded Clauses 
Two major types of embedded constructions in-
volve participles and relative clause constructions. 
Participles in Hindi are identified through a set of 
TAM markers. In the case of participle embed-
dings, a sentence will have more than one verb, 
i.e., at least one participle and the matrix verb. 
Both the matrix (finite) verb and the participle can 
take their own arguments that can be identified via 
the case-TAM mapping discussed earlier. Howev-
er, there are certain syntactic constraints that limit 
the type of arguments a participle can take.  There 
99
are two sources of errors here: (a) argument shar-
ing, and (b) ambiguous attachment sites.  
Some arguments such as place/time nominals 
can be shared. Shared arguments are assigned to 
only one verb in the dependency tree. So the task 
of identifying the shared arguments, if any, and 
attaching them to the correct parent is a complex 
task. Note that the dependency labels can be identi-
fied based on the morphosyntactic features. The 
task becomes more complex if there is more than 
one participle in a sentence. 12 out of 130 in-
stances (9.23%) of shared arguments has an incor-
rect attachment.  
Many participles are ambiguous and making the 
correct attachment choice is difficult. Similar par-
ticiples, depending on the context, can behave as 
adverbials and attach to a verb, or can behave as 
adjectives and attach to a noun. Take (4) as a case 
in point.  
 
(4) maine     daurte   hue        kutte  ko   dekhaa 
     ?I?-ERG  (while) running   dog   ACC ?saw?             
 
In (4) based on how one interprets ?daurte hue?, 
one gets either the reading that ?I saw a running 
dog? or that ?I saw a dog while running?. In case of 
the adjectival participle construction (VJJ), 2 out of 
3 errors are due to wrong attachment. 
5.3 Coordination 
Coordination poses problems as it often gives rise 
to long-distance dependencies. Moreover, the tree-
bank annotation treats the coordinating conjunction 
as the head of the coordinated structure. Therefore, 
a coordinating conjunction can potentially become 
the root of the entire dependency tree. This is simi-
lar to Prague style dependency annotation (Hajico-
va, 1998). Coordinating conjunctions pose 
additional problems in such a scenario as they can 
appear as the child of different heads. A coordinat-
ing conjunction takes children of similar POS cat-
egory, but the parent of the conjunction depends on 
the type of the children.  
 
 
(5) a. raama  aur  shyaama   ne     khaana khaayaa                                    
        ?Ram? ?and? ?Shyam? ?ERG?  ?food?   ?ate?          
        ?Ram and Shyam ate the food.? 
 
 
     b. raama   ne    khaanaa  khaayaa  aur  paanii    
          ?Ram?  ?ERG? ?food?     ?ate?     ?and? ?water?  
         piyaa 
         ?drank? 
         ?Ram ate food and drank water.? 
 
In (5a), raama and shyaama are children of the 
coordinating conjunction aur, which gets attached 
to the main verb khaayaa with the label k1. In ef-
fect, syntactically aur becomes the argument of the 
main verb. In (5b), however, the verbs khaayaa 
and piyaa are the children of aur. In this case, aur 
becomes the root of the sentence. Identifying the 
nature of the conjunction and its children becomes 
a challenging task for the parser. Note that the 
number of children that a coordinating conjunction 
can take is not fixed either. The parser could iden-
tify the correct head of the conjunctions with an 
accuracy of 75.7% and the correct children with an 
accuracy of 85.7%. 
The nature of the conjunction will also affect the 
dependency relation it has with its head. For ex-
ample, if the children are nouns, then the conjunc-
tion behaves as a noun and can potentially be an 
argument of a verb. By contrast, if the children are 
finite verbs, then it behaves as a finite verb and can 
become the root of the dependency tree. Unlike 
nouns and verbs, however, conjunctions do not 
have morphological features. So a child-to-head 
feature percolation should help make a coordinat-
ing node more transparent. For example, in (5a) the 
Ergative case ne is a strong cue for the dependency 
label k1. If we copy this information from one of 
its children (here shyaama) to the conjunct, then 
the parser can possibly make use of this informa-
tion. 
5.4 Complex Predicates 
Complex predicates are formed by combining a 
noun or an adjective with a verbalizer kar or ho. 
For instance, in taariif karanaa ?to praise?, taariif 
?praise? is a noun and karanaa ?to do? is a verb. 
Together they form the main verb. Complex predi-
cates are highly productive in Hindi. Combination 
of the light verb and the noun/adjective is depen-
dent on not only syntax but also semantics and 
therefore its automatic identification is not always 
straightforward (Butt, 1995).  A noun-verb com-
plex predicate in the treebank is linked via the de-
pendency label pof. The parser makes mistakes in 
100
identifying pof or misclassifies other labels as pof. 
In particular, the confusion is with k2 and k1s 
which are object/theme and noun complements of 
k1, respectively. These labels share similar contex-
tual features like the nominal element in the verb 
complex. Table 3 includes the confusion matrix for 
pof errors.  
5.5 Non-Projectivity 
As noted earlier, MaltParser?s arc-eager parsing 
algorithm can be combined with the pseudo-
projective parsing techniques proposed in Nivre 
and Nilsson (2005), which potentially helps in 
identifying non-projective arcs. The Hindi treebank 
has ~14% non-projective arcs (Mannem et al, 
2009a). In the test set, there were a total of 11 non-
projective arcs, but the parser did not find any of 
them. This is consistent with earlier results show-
ing that pseudo-projective parsing has high preci-
sion but low recall, especially when the percentage 
of non-projective relations is small (Nilsson et al 
2007). 
Non-projectivity has proven to be one of the ma-
jor problems in dependency parsing, especially for 
free word-order languages. In Hindi, the majority 
of non-projective arcs are inter-clausal (Mannem et 
al., 2009a), involving conjunctions and relative 
clauses. There have been some attempts at han-
dling inter-clausal non-projectivity in Hindi. Hu-
sain et al (2009) proposed a two-stage approach 
that can handle some of the inter-clausal non-
projective structures. 
5.6 Long-Distance Dependencies  
Previous results on parsing other languages have 
shown that MaltParser has lower accuracy on long-
distance dependencies. Our results confirm this. 
Errors in the case of relative clauses and coordina-
tion can mainly be explained in this way. For ex-
ample, there are 8 instances of relative clauses in 
the test data. The system could identify only 2 of 
them correctly. These two are at a distance of 1 
from its parent. For the remaining 6 instances the 
distance to the parent of the relative clause ranges 
from 4 to 12. 
Figure 3 shows how parser performance de-
creases with increasing distance between the head 
and the dependent. Recently, Husain et al (2009) 
have proposed a two-stage setup to parse inter-
clausal and intra-clausal dependencies separately. 
They have shown that most long distance relations 
are inter-clausal, and therefore, using such a clause 
motivated parsing setup helps in maximizing both 
short distance and long distance dependency accu-
racy. In a similar spirit, Gadde et al (2010) showed 
that using clausal features helps in identifying long 
distance dependencies. They have shown that pro-
viding clause information in the form of clause 
boundaries and clausal heads can help a parser 
make better predictions about long distance depen-
dencies. 
0
20
40
60
80
100
0 2 4 6 8 10 12
Dependency Length
D
ep
en
de
nc
y 
Pr
ec
is
io
n
 
0
20
40
60
80
100
0 1 2 3 4 5 6 7 8 9 10 11 12
Dependency Length
D
ep
en
de
nc
y 
Re
ca
ll
 
Figure 3. Dependency arc precision/recall relative to 
dependency length, where the length of a dependency 
from wi to wj is |i-j| and roots are assumed to have dis-
tance 0 to their head. 
6 Conclusion 
In this paper we have analyzed the importance of 
different linguistic features in data-driven parsing 
of Hindi and at the same time improved the state of 
the art. Our main finding is that the combination of 
case markers on nominals with TAM markers on 
verbs is crucially important for syntactic disambig-
uation, while the inclusion of features such as per-
son, number gender that help in agreement has not 
yet resulted in any improvement. We have also 
presented a detailed error analysis and discussed 
possible techniques targeting different error 
classes. We plan to use these techniques to im-
prove our results in the near future. 
101
References  
B. R. Ambati, P. Gadde, and K. Jindal. 2009a. Experi-
ments in Indian Language Dependency Parsing. 
Proc. of ICON09 NLP Tools Contest: Indian Lan-
guage Dependency Parsing, 32-37.  
B. R. Ambati, P. Gade, C. GSK and S. Husain. 2009b. 
Effect of Minimal Semantics on Dependency Pars-
ing. Proc. of RANLP Student Research Workshop.  
R. Begum, S. Husain, A. Dhwaj, D. Sharma, L. Bai, and 
R. Sangal. 2008. Dependency annotation scheme for 
Indian languages. Proc. of IJCNLP. 
A. Bharati, V. Chaitanya and R. Sangal. 1995. Natural 
Language Processing: A Paninian Perspective, Pren-
tice-Hall of India, New Delhi. 
A. Bharati, S. Husain, B. Ambati, S. Jain, D. Sharma, 
and R. Sangal. 2008. Two semantic features make all 
the difference in parsing accuracy. Proc. of ICON. 
A. Bharati, R. Sangal, D. M. Sharma and L. Bai. 2006. 
AnnCorra: Annotating Corpora Guidelines for POS 
and Chunk Annotation for Indian Languages. Tech-
nical Report (TR-LTRC-31), LTRC, IIIT-Hyderabad. 
A. Bharati, D. M. Sharma, S. Husain, L. Bai, R. Begam 
and R. Sangal. 2009a. AnnCorra: TreeBanks for In-
dian Languages, Guidelines for Annotating Hindi 
TreeBank. 
http://ltrc.iiit.ac.in/MachineTrans/research/tb/DS-
guidelines/DS-guidelines-ver2-28-05-09.pdf 
A. Bharati, S. Husain, D. M. Sharma and R. Sangal. 
2009b. Two stage constraint based hybrid approach 
to free word order language dependency parsing. In 
Proc. of IWPT. 
R. Bhatt, B. Narasimhan, M. Palmer, O. Rambow, D. 
M. Sharma and F. Xia. 2009. Multi-Representational 
and Multi-Layered Treebank for Hindi/Urdu. Proc. 
of the Third LAW at ACL-IJCNLP, 186-189. 
M. Butt. 1995. The Structure of Complex Predicates in 
Urdu. CSLI Publications. 
G. Eryigit, J. Nivre, and K. Oflazer. 2008. Dependency 
Parsing of Turkish. Computational Linguistics 34(3), 
357-389. 
P. Gadde, K. Jindal, S. Husain, D. M. Sharma, and R. 
Sangal. 2010. Improving Data Driven Dependency 
Parsing using Clausal Information. Proc. of NAACL-
HLT. 
Y. Goldberg and M. Elhadad. 2009. Hebrew Dependen-
cy Parsing: Initial Results. Proc. of IWPT, 129-133. 
E. Hajicova. 1998. Prague Dependency Treebank: From 
Analytic to Tectogrammatical Annotation.  Proc. of 
TSD.  
J. Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi, M. 
Nilsson and M. Saers. 2007. Single Malt or Blended? 
A Study in Multilingual Parser Optimization. Proc. 
of EMNLP-CoNLL, 933-939. 
S. Husain. 2009. Dependency Parsers for Indian Lan-
guages. Proc. of ICON09 NLP Tools Contest: Indian 
Language Dependency Parsing.  
S. Husain, P. Gadde, B. Ambati, D. M. Sharma and R. 
Sangal. 2009. A modular cascaded approach to com-
plete parsing. Proc. of the COLIPS International 
Conference on Asian Language Processing.  
P. Mannem, H. Chaudhry, and A. Bharati. 2009a. In-
sights into non-projectivity in Hindi. Proc. of ACL-
IJCNLP Student Research Workshop. 
P. Mannem, A. Abhilash and A. Bharati. 2009b. LTAG-
spinal Treebank and Parser for Hindi. Proceedings of 
International Conference on NLP, Hyderabad. 2009. 
R. McDonald, K. Lerman, and F. Pereira. 2006. Multi-
lingual dependency analysis with a two-stage discri-
minative parser. Proc. of CoNLL, 216-220. 
R. McDonald and J. Nivre. 2007. Characterizing the 
errors of data-driven dependency parsing models.  
Proc. of EMNLP-CoNLL, 122-131. 
I. A. Mel'Cuk. 1988. Dependency Syntax: Theory and 
Practice, State University Press of New York. 
J. Nilsson, J. Nivre and J. Hall. 2007. Generalizing Tree 
Transformations for Inductive Dependency Parsing. 
Proc. of ACL, 968-975. 
J. Nivre. 2008. Algorithms for Deterministic Incremen-
tal Dependency Parsing. Computational Linguistics 
34(4), 513-553. 
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In 
Proceedings of ACL-HLT, pp. 950-958. 
J. Nivre. 2009a.  Non-Projective Dependency Parsing in 
Expected Linear Time. Proc. of ACL-IJCNLP, 351-
359. 
J. Nivre. 2009b. Parsing Indian Languages with Malt-
Parser. Proc. of ICON09 NLP Tools Contest: Indian 
Language Dependency Parsing, 12-18. 
J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson,  S. 
Riedel and D. Yuret. 2007a. The CoNLL 2007 
Shared Task on Dependency Parsing. Proc. of 
EMNLP/CoNLL, 915-932. 
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. 
K?bler, S. Marinov and E Marsi. 2007b. MaltParser: 
A language-independent system for data-driven de-
pendency parsing. NLE, 13(2), 95-135. 
L. ?vrelid. 2008. Argument Differentiation. Soft con-
straints and data-driven models. PhD Thesis, Uni-
versity of Gothenburg. 
D. Seddah, M. Candito and B. Crabb?. 2009. Cross 
parser evaluation: a French Treebanks study. Proc. of 
IWPT, 150-161. 
R. Tsarfaty and K. Sima'an. 2008. Relational-
Realizational Parsing. Proc. of CoLing, 889-896. 
A. Vaidya, S. Husain, P. Mannem, and D. M. Sharma. 
2009. A karaka-based dependency annotation scheme 
for English. Proc. of CICLing, 41-52. 
102
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 167?171,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Linear Inversion Transduction Grammar Alignments
as a Second Translation Path
Markus SAERS and Joakim NIVRE
Computational Linguistics Group
Dept. of Linguistics and Philology
Uppsala University
Sweden
first.last@lingfil.uu.se
Dekai WU
Human Language Technology Center
Dept. of Computer Science and Engineering
HKUST
Hong Kong
dekai@cs.ust.hk
Abstract
We explore the possibility of using
Stochastic Bracketing Linear Inversion
Transduction Grammars for a full-scale
German?English translation task, both on
their own and in conjunction with align-
ments induced with GIZA++. The ratio-
nale for transduction grammars, the details
of the system and some results are pre-
sented.
1 Introduction
Lately, there has been some interest in using In-
version Transduction Grammars (ITGs) for align-
ment purposes. The main problem with ITGs is the
time complexity, O(Gn6) doesn?t scale well. By
limiting the grammar to a bracketing ITG (BITG),
the grammar constant (G) can be eliminated, but
O(n6) is still prohibitive for large data sets.
There has been some work on approximate in-
ference of ITGs. Zhang et al (2008) present a
method for evaluating spans in the sentence pair
to determine whether they should be excluded or
not. The algorithm has a best case time com-
plexity of O(n3). Saers, Nivre & Wu (2009) in-
troduce a beam pruning scheme, which reduces
time complexity to O(bn3). They also show
that severe pruning is possible without significant
deterioration in alignment quality (as measured
by downstream translation quality). Haghighi et
al. (2009) use a simpler aligner as guidance for
pruning, which reduces the time complexity by
two orders of magnitude. Their work also par-
tially implements the phrasal ITGs for translation-
driven segmentation introduced in Wu (1997), al-
though they only allow for one-to-many align-
ments, rather than many-to-many alignments. A
more extreme approach is taken in Saers, Nivre
& Wu (2010). Not only is the search severely
pruned, but the grammar itself is limited to a lin-
earized form, getting rid of branching within a sin-
gle parse. Although a small deterioration in down-
stream translation quality is noted (compared to
harshly pruned SBITGs), the grammar can be in-
duced in linear time.
In this paper we apply SBLITGs to a full size
German?English WMT?10 translation task. We
also use differentiated translation paths to com-
bine SBLITG translation models with a standard
GIZA++ translation model.
2 Background
A transduction grammar is a grammar that gener-
ates a pair of languages. In a transduction gram-
mar, the terminal symbols consist of pairs of to-
kens where the first is taken from the vocabulary
of one of the languages, and the second from the
vocabulary of the other. Transduction grammars
have to our knowledge been restricted to trans-
duce between languages no more complex than
context-free languages (CFLs). Transduction be-
tween CFLs was first described in Lewis & Stearns
(1968), and then further explored in Aho & Ull-
man (1972). The main motivation for explor-
ing this was to build programming language com-
pilers, which essentially translate between source
code and machine code. There are two types of
transduction grammars between CFLs described in
the computer science literature: simple transduc-
tion grammars (STGs) and syntax-directed trans-
duction grammars (SDTGs). The difference be-
tween them is that STGs are monotone, whereas
SDTGs allow unlimited reordering in rule produc-
tions. Both allow the use of singletons to insert
and delete tokens from either language. A sin-
gleton is a biterminal where one of the tokens is
the empty string (). Neither STGs nor SDTGs
are intuitively useful in translating natural lan-
guages, since STGs have no way to model reorder-
ing, and SDTGs require exponential time to be in-
duced from examples (parallel corpora). Since
167
compilers in general work on well defined, manu-
ally specified programming languages, there is no
need to induce them from examples, so the expo-
nential complexity is not a problem in this setting
? SDTGs can transduce in O(n3) time, so once the
grammar is known they can be used to translate
efficiently.
In natural language translation, the grammar is
generally not known, in fact, state-of-the art trans-
lation systems rely heavily on machine learning.
For transduction grammars, this means that they
have to be induced from parallel corpora.
An inversion transduction grammar (ITG)
strikes a good balance between STGs and SDTGs,
as it allows some reordering, while requiring only
polynomial time to be induced from parallel cor-
pora. The allowed reordering is either the iden-
tity permutation of the production, or the inver-
sion permutation. Restricting the permutations in
this way ensures that an ITG can be expressed in
two-normal form, which is the key property for
avoiding exponential time complexity in biparsing
(parsing of a sentence pair).
An ITG in two-normal form (representing the
transduction between L1 and L2) is written with
identity productions in square brackets, and in-
verted productions in angle brackets. Each such
rule can be construed to represent two (one L1 and
one L2) synchronized CFG rules:
ITGL1,L2 CFGL1 CFGL2
A? [ B C ] A? B C A? B C
A? ? B C ? A? B C A? C B
A? e/f A? e A? f
Inducing an ITG from a parallel corpus is still slow,
as the time complexity is O(Gn6). Several ways
to get around this has been proposed (Zhang et al,
2008; Haghighi et al, 2009; Saers et al, 2009;
Saers et al, 2010).
Taking a closer look at the linear ITGs (Saers et
al., 2010), there are five rules in normal form. De-
composing these five rule types into monolingual
rule types reveals that the monolingual grammars
are linear grammars (LGs):
LITGL1,L2 LGL1 LGL2
A? [ e/f C ] A? e C A? f C
A? [ B e/f ] A? B e A? B f
A? ? e/f C ? A? e C A? C f
A? ? B e/f ? A? B e A? f B
A? / A?  A? 
This means that LITGs are transduction grammars
that transduce between linear languages.
There is also a nice parallel in search time com-
plexities between CFGs and ITGs on the one hand,
and LGs and LITGs on the other. Searching for
all possible parses given a sentence is O(n3) for
CFGs, and O(n2) for LGs. Searching for all possi-
ble biparses given a bisentence is O(n6) for ITGs,
and O(n4) for LITGs. This is consistent with
thinking of biparsing as finding every L2 parse for
every L1 parse. Biparsing consists of assigning a
joint structure to a sentence pair, rather than as-
signing a structure to a sentence.
In this paper, only stochastic bracketing gram-
mars (SBITGs and SBLITGs) were used. A brack-
eting grammar has only one nonterminal symbol,
denoted X . A stochastic grammar is one where
each rule is associated with a probability, such that
?X
?
?
?
?
p(X ? ?) = 1
?
?
While training a Stochastic Bracketing ITG
(SBITG) or LITG (SBLITG) with EM, expectations
of probabilities over the biparse-forest are calcu-
lated. These expectations approach the true prob-
abilities, and can be used as approximations. The
probabilities over the biparse-forest can be used
to select the one-best parse-tree, which in turn
forces an alignment over the sentence pair. The
alignments given by SBITGs and SBLITGs has been
shown to give better translation quality than bidi-
rectional IBM-models, when applied to short sen-
tence corpora (Saers and Wu, 2009; Saers et al,
2009; Saers et al, 2010). In this paper we ex-
plore whether this hold for SBLITGs on standard
sentence corpora.
3 Setup
The baseline system for the shared task was a
phrase based translation model based on bidi-
rectional IBM- (Brown et al, 1993) and HMM-
models (Vogel et al, 1996) combined with the
grow-diag-final-and heuristic. This is
computed with the GIZA++ tool (Och and Ney,
2003) and the Moses toolkit (Koehn et al, 2007).
The language model was a 5-gram SRILM (Stol-
cke, 2002). Parameters in the final translation sys-
tem were determined with Minimum Error-Rate
Training (Och, 2003), and translation quality was
assessed with the automatic measures BLEU (Pap-
ineni et al, 2002) and NIST (Doddington, 2002).
168
Corpus Type Size
German?English Europarl out of domain 1,219,343 sentence pairs
German?English news commentary in-domain 86,941 sentence pairs
English news commentary in-domain 48,653,884 sentences
German?English news commentary in-domain tuning data 2,051 sentence pairs
German?English news commentary in-domain test data 2,489 sentence pairs
Table 1: Corpora available for the German?English translation task after baseline cleaning.
System BLEU NIST
GIZA++ 17.88 5.9748
SBLITG 17.61 5.8846
SBLITG (only Europarl) 17.46 5.8491
SBLITG (only news) 15.49 5.4987
GIZA++ and SBLITG 17.66 5.9650
GIZA++ and SBLITG (only Europarl) 17.58 5.9819
GIZA++ and SBLITG (only news) 17.48 5.9693
Table 2: Results for the German?English translation task.
We chose to focus on the German?English
translation task. The corpora resources available
for that task is summarized in Table 1. We used the
entire news commentary monolingual data con-
catenated with the English side of the Europarl
bilingual data to train the language model. In ret-
rospect, this was probably a bad choice, as others
seem to prefer the use of two language models in-
stead.
We contrasted the baseline system with pure
SBLITG systems trained on different parts of the
training data, as well as combined systems, where
the SBLITG systems were combined with the base-
line system. The combination was done by adding
the SBLITG translation model as a second transla-
tion path to the base line system.
To train our SBLITG systems, we used the algo-
rithm described in Saers et al (2010). We set the
beam size parameter to 50, and ran expectation-
maximization for 10 iterations or until the log-
probability of the training corpus started deterio-
rating. After the grammar was induced we ob-
tained the one-best parse for each sentence pair,
which also dictates a word alignment over that
sentence pair, which we used instead of the word
alignments provided by GIZA++. From that point,
training did not differ from the baseline procedure.
We trained a total of three pure SBLITG system,
one with only the news commentary part of the
corpus, one with only the Europarl part, and one
with both. We also combined all three SBLITG
systems with the baseline system to see whether
the additional translation paths would help.
The system we submitted corresponds to the
?GIZA++ and SBLITG (only news)? system, but
with RandLM (Talbot and Osborne, 2007) as lan-
guage model rather than SRILM. This was because
we lacked the necessary RAM resources to calcu-
late the full SRILM model before the system sub-
mission deadline.
4 Results
The results for the development test set are sum-
marized in Table 2. The submitted system
achieved a BLEU score of 0.1759 and a NIST
score of 5.9579 for cased output on this year?s test
set (these numbers are not comparable to those
in Table 2). To our surprise, adding the addi-
tional phrases as a second translation path does
not seem to help. Instead a small deterioration
in BLEU is noted (0.22?0.40 points), whereas the
differences in NIST are mixed (-0.0098?+0.0071
points). Over all the variations were very small.
The pure SBLITG systems perform consistently
below baseline, which could indicate that the
grammar class is unable to capture the reorderings
found in longer sentence pairs adequately in one
parse. The variation between the pure SBLITG sys-
tems can be explained by the size of the training
data: more data ? better quality.
169
5 Conclusions
We tried to use SBLITGs as word aligners on full
size sentences, which has not been done to date,
and noted that the formalism seems unable to ac-
count for the full complexity of longer sentence
pairs. We also tried combining the translation
models acquired with SBLITG alignments to the
baseline system, and noted very small differences,
tending to a deterioration in quality. The fact that
SBLITGs seem unable to capture the complex re-
lationship between an English and a German sen-
tence in one parse means that we need to find ei-
ther some more complex model or some way to
use the entire parse forest to arrive at the align-
ment.
Acknowledgments
This work was funded by the Swedish Na-
tional Graduate School of Language Technol-
ogy (GSLT), the Defense Advanced Research
Projects Agency (DARPA) under GALE Con-
tract No. HR0011-06-C-0023, and the Hong
Kong Research Grants Council (RGC) under
research grants GRF621008, DAG03/04.EG09,
RGC6256/00E, and RGC6083/99E. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the Defense
Advanced Research Projects Agency. The com-
putations were performed on UPPMAX resources
under project p2007020.
References
Jeffrey D. Aho, Alfred V. Ullman. 1972. The Theory
of Parsing, Translation, and Compiling. Prentice-
Halll, Englewood Cliffs, NJ.
Peter F Brown, Stephen A Della Pietra, Vincent J Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?311.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of Human
Language Technology conference (HLT-2002), San
Diego, California.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with super-
vised ITG models. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 923?
931, Suntec, Singapore, August.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic,
June.
Philip M. Lewis and Richard E. Stearns. 1968. Syntax-
directed transduction. Journal of the Association for
Computing Machinery, 15(3):465?488.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In 41st Annual
Meeting of the Association for Computational Lin-
guistics, pages 160?167, Sapporo, Japan, Jul.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, July.
Markus Saers and Dekai Wu. 2009. Improving phrase-
based translation via word alignments from Stochas-
tic Inversion Transduction Grammars. In Proceed-
ings of the Third Workshop on Syntax and Structure
in Statistical Translation (SSST-3) at NAACL HLT
2009, pages 28?36, Boulder, Colorado, June.
Markus Saers, Joakim Nivre, and Dekai Wu. 2009.
Learning Stochastic Bracketing Inversion Transduc-
tion Grammars with a cubic time biparsing algo-
rithm. In Proceedings of the 11th International Con-
ference on Parsing Technologies (IWPT?09), pages
29?32, Paris, France, October.
Markus Saers, Joakim Nivre, and Dekai Wu. 2010.
Word alignment with Stochastic Bracketing Linear
Inversion Transduction Grammar. In Proceedings of
Human Language Technologies: The 11th Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, Los An-
geles, California, June.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In International Confer-
ence on Spoken Language Processing, Denver, Col-
orado, September.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
512?519, Prague, Czech Republic, June.
170
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical
translation. In Proceedings of the 16th conference
on Computational linguistics, pages 836?841, Mor-
ristown, New Jersey.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing.
In Proceedings of ACL-08: HLT, pages 97?105,
Columbus, Ohio, June.
171
Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 63?71,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Towards a Data-Driven Model of Eye Movement Control in Reading
Mattias Nilsson
Department of Linguistics and Philology
Uppsala University
mattias.nilsson@lingfil.uu.se
Joakim Nivre
Department of Linguistics and Philology
Uppsala University
joakim.nivre@lingfil.uu.se
Abstract
This paper presents a data-driven model
of eye movement control in reading that
builds on earlier work using machine
learning methods to model saccade behav-
ior. We extend previous work by model-
ing the time course of eye movements, in
addition to where the eyes move. In this
model, the initiation of eye movements is
delayed as a function of on-line process-
ing difficulty, and the decision of where to
move the eyes is guided by past reading
experience, approximated using machine
learning methods. In benchmarking the
model against held-out previously unseen
data, we show that it can predict gaze dura-
tions and skipping probabilities with good
accuracy.
1 Introduction
Eye movements during reading proceed as an al-
ternating series of fixations and saccades with con-
siderable variability in fixation times and saccade
lengths. This variation reflects, at least to some
extent, language-related processes during reading.
Much psycholinguistic research, therefore, relies
on measures of eye movements in reading to gain
an understanding of human sentence processing.
Eye tracking recordings are routinely used to study
how readers? eye movements respond to experi-
mental manipulation of linguistic stimuli (Clifton
et al, 2007), and corpus-based analysis of eye-
tracking data has recently emerged as a new way
to evaluate theories of human sentence process-
ing difficulty (Boston et al, 2008; Demberg and
Keller, 2008).
More detailed accounts of the workings of the
eye movement system during reading are offered
by computational models of eye movement con-
trol (see Reichle (2006b), for an overview of re-
cent models). These models receive text as in-
put and produce predictions for the placement
and duration of fixations, in approximation to hu-
man reading behavior. Because eye movements
in reading rely on a coupled cognitive-motor sys-
tem, such models provide detailed accounts for
how eye movements are controlled both by on-line
language processing and lower-level motor con-
trol. Current models such as E-Z Reader (Reichle,
2006a; Pollatsek et al, 2006; Reichle et al, 2009)
and SWIFT (Engbert et al, 2002; Engbert et al,
2005) account for numerous of the known facts
about saccade behavior in reading. This includes
word frequency and predictability effects on fixa-
tion times, word skipping rates, and preview and
spillover effects.
A recent approach to eye-movement model-
ing, less tied to psychophysiological assumptions
about the mechanisms that drive eye movements,
is to build models directly from eye-tracking data
using machine learning techniques inspired by re-
cent work in natural language processing. Thus,
Nilsson and Nivre (2009) show how a classifier
can be trained on authentic eye-tracking data and
then used to predict the saccade behavior of in-
dividual readers on new texts. Methodologically
this differs from the standard approach in compu-
tational modeling of eye movement control, where
model parameters are often fitted to data but model
predictions are not evaluated on unseen data in or-
der to assess the generalization error of these pre-
dictions. Without questioning the validity of the
standard approach, we believe that the strict sep-
aration of training data and test data assumed in
machine learning may provide additional insights
about the properties of these models.
The model of Nilsson and Nivre (2009) is based
on a simple transition system for saccadic move-
ments, a classifier that predicts where to fixate next
and a classifier-guided search algorithm to simu-
late fixation sequences over sentences.
63
One obvious limitation of the model proposed
by Nilsson and Nivre (2009) is that it does not at
all capture the temporal aspects of eye movement
behavior. Thus, for example, it says nothing about
when eye movements are initiated or when the de-
cision of where to fixate next is made during fixa-
tions. In this paper, we try to overcome this limita-
tion by placing the machine-learning approach in
a broader psychological context and detail a model
that also accounts for the timing of fixations. More
precisely, we present a model of the time course of
eye movements, where saccade timing is driven by
on-line language processing and where-decisions
are driven by the experience readers have built up
through years of reading practice.1
It is not our intention in this paper to present
a full-fledged model of eye movement control in
reading. The model is limited in scope and does
not address certain important aspects of eye move-
ment control, such as within-word fixation lo-
cations, refixations and regressions triggered by
higher-order processing. In addition, the linguistic
features influencing timing (when-decisions) and
target selection (where-decisions) are restricted to
the basic variables word length and frequency. In
this way, we hope to provide a baseline against
which richer models of language processing can
be evaluated.
The rest of this paper is structured as follows.
Section 2 provides a brief background on what is
known about the time course of eye movements
during reading. Here we introduce some com-
mon notions that will be used later on. In sec-
tion 3, we first give an overview of the model
and then describe its component processes and
how these processes interrelate. In section 4, we
present an experimental evaluation of the model
using data from the English section of the Dundee
corpus (Kennedy and Pynte, 2005). Section 5 con-
tains our conclusions and suggestions for future
research.
2 The Timing of Eye Movements
The average fixation duration in reading is about
250 ms, and most fixations last between 200-300
ms, although they may range from under 100 ms
to over 500 ms for a given reader (Rayner, 1998).
Because eye movements are a motor response re-
1This view of where-decisions being driven by experience
is similar in spirit to some earlier theories of saccade target
selection in reading, such as the probabilistic account of word
skipping proposed by Brysbaert and Vitu (1998).
quiring preparation before execution, they are ini-
tiated well before the end of the fixation. Hence,
there is a saccade latency of about 150-200 ms
from the time when a saccade is first initiated un-
til the eye movement is actually executed (Becker
and J?rgens, 1979; McPeek et al, 2000). Once
the eye movement is executed, it takes about 25-
45 ms before the eyes are fixated on a new word
again, depending on the length of the movement.
Given an average saccade latency of about 150-
200 ms, and an average fixation duration of 250
ms, it seems clear that eye movements are often
initiated within the first 100 ms of a fixation. How-
ever, as Reichle notes (Reichle et al, 2003), since
the time it takes to identify words is on the order
of 150 - 300 ms, this suggests that there is not
enough time for language processes to have any
direct on-line influence on eye movements. One
key observation to explain language influences on
eye movements, however, is the finding that read-
ers often start processing upcoming words before
they are fixated. Studies on parafoveal preview
show that the amount of time spent fixating a
word depends, among other things, on how much
parafoveal preview of the word is available prior
to the word being fixated (Balota et al, 1985; Pol-
latsek et al, 1992).
A further finding supporting the assumption that
language processes can have an early effect on
eye movements comes from the disappearing text
studies (Rayner et al, 1981; Rayner et al, 2003).
In these studies, words become masked or disap-
pear at a certain point during the fixation. De-
spite this, a word need only be on display for 50-
60 ms in order for reading to proceed quite nor-
mally. More importantly, the time the eyes re-
main fixated after a word disappears depends on
the frequency of the word. Readers remain fix-
ated on low-frequency words longer than on high-
frequency words, even though the word that was
fixated has actually disappeared. In summary,
these studies suggest that there is a robust word
frequency effect in reading as early as 60 ms after
the onset of the fixation.
3 A Model of Eye Movement Control
3.1 General Overview
The model we develop takes the basic time con-
straints associated with language processing and
motor control as a starting point. This means that
our model is driven by estimates of the time it
64
takes to process words, plan an eye movement, ex-
ecute a saccade etc. In line with cognitive con-
trol models of eye movements in reading, such
as E-Z Reader, we assume that the cognitive pro-
cessing of words is the ?engine? that drives eye
movements. That is, eye movements are initiated
in response to on-line language processing. Un-
like E-Z Reader, however, we do not presume a
two-stage lexical process where the completion of
a certain hypothesized first stage triggers an eye
movement.2 Instead, when the eyes move to a new
word, an eye movement is initiated after some de-
lay that is proportional to the amount of cognitive
work left on the word. Furthermore, in contrast
to E-Z Reader we assume that saccade initiation
is decoupled from the decision of where to move
the eyes. In E-Z Reader, the initiation of a saccade
program is in effect a decision to start program-
ming a saccade to the next word. Here, instead,
the target for the next saccade can be any of the
words in the forward perceptual span. Another re-
lated difference, with respect to previous cognitive
control models, is that we assume that the deci-
sion of where to move the eyes is not directly in-
fluenced by on-line language processing. Instead,
this decision is governed by an autonomous rou-
tine, having its own dynamics automated through
years of reading experience. This experience is
approximated using machine learning methods on
authentic eye tracking data.
The model is defined in terms of four processes
that we assume are operative during reading: lex-
ical processing (L), saccade initiation delay (D),
motor programming (M), and saccade execution
(S). These processes are defined in terms of a set
of parameters that determine their duration. Once
an ongoing process ends, a subsequent process is
initiated, for as long as reading continues. As is
commonly assumed in most models of eye move-
ment control, language-related processes and mo-
tor control processes can run in parallel. We will
use the notation wi to refer to the ith word in a text
w1, . . . , wn consisting of n words, and we will use
subscripted symbols Li, Di, Mi and Si to refer to
the lexical processing, the saccade initiation delay,
the motor programming, and the saccade execu-
tion associated with wi.
In the following four subsections, we outline
2In E-Z Reader, the first stage of lexical processing is an
early estimate of the word?s familiarity that provides the sig-
nal to the eye movement system that lexical access is immi-
nent and that a saccade should be planned.
these processes in detail and discuss the general
assumptions underlying them. We then conclude
this section by summarizing how the processes dy-
namically interact to produce eye movement be-
havior.
3.2 Lexical Processing
The time needed to process individual words in
reading is certain to depend on numerous fac-
tors related to a person?s prior reading experi-
ence, word-level properties such as length and fre-
quency, and higher-order language processes such
as syntactic and semantic processing. However,
since our goal in this paper is to validate a sim-
ple model, with as few parameters as possible, we
make the simplifying assumption that the process-
ing time of a word can be approximated by its
length (number of characters) and its frequency of
occurrence in printed text. In particular, we as-
sume that the mean time required for processing a
word wi is a linear function of its length and the
natural logarithm of its frequency:3
t(Li) = b0+b1 length(wi)?b2 ln(freq(wi)) (1)
In equation 1, b0 is the intercept representing the
base time needed to process a word while b1 and
b2 are the respective slopes for the effect of length
and frequency on the base processing time. Again,
we stress that equation 1 is by all accounts an over-
simplification. Thus, for example, it does not take
into account any higher-level top-down influence
on processing time.
Still, we believe equation 1 provides a reason-
able first approximation. A large part of the vari-
ance in measures of reading time can be accounted
for by word frequency and word length. At any
rate, our simple assumption with respect to pro-
cessing time represents a methodological decision
rather than a theoretical one. We want to keep the
model as simple as possible at this stage, and later
explore the effect of including variables related to
higher-order processing.
Once the time interval t(Li) has passed for a
given word wi, lexical processing begins on the
next word. Thus, the completion of t(Li) results
in the initiation of Li+1. Because the processing
of the next word does not start until the processing
of the current word is finished, lexical processing
3We use the logarithm of word frequency because hu-
man response times, in lexical decision tasks for instance, are
linearly related to the natural logarithm of word frequency
(Balota and Chumbley, 1984).
65
proceeds serially and no more than one word is
processed at any given time.
3.3 Saccade Initiation Delay
When the eyes move to a new word wi, a motor
program is initiated after some time. We assume
that the time when a motor program is initiated
depends on the processing difficulty of the fixated
word wi. In particular, the signal to initiate a sac-
cade is deferred in proportion to how much pro-
cessing remains on wi, or put differently, in pro-
portion to how much work remains to be done on
that word. This general routine serves to prevent
the control system from making over-hasty sac-
cades to new words. The length of the saccade ini-
tiation delay t(D) is proportional to the remaining
processing time of word wi at fixation onset:
t(Di) = d (t(Li)? t(Ei)) (2)
where d is a free parameter representing a pro-
portion, t(Li) is the lexical processing time for
the fixated word, and t(Ei) denotes the interval of
time that has elapsed since the initiation of t(Li).
More difficult words are associated with longer
processing times and thus cause later initiation of
saccade programs and therefore also longer fix-
ation durations. The free parameter d defines a
proportion taking values in the range [0, 1]. The
extremes of this range can be interpreted as fol-
lows. If d is set equal to 0, a new saccade program
is initiated immediately upon a new fixation. If
d instead is set equal to 1, the saccade program
starts only after the fixated word has been fully
processed. More generally, a change of the value
of this parameter can be understood as a change of
the amount of cognitive influence on fixation du-
rations. The higher its value, the more cognitive
work must be carried out before a new saccade
program is started. Once the time interval t(D)
has passed, the planning of a new eye movement
starts, i.e., a motor program, M , is initiated.
3.4 Motor Programming
The time needed to plan and initiate an eye move-
ment defines the saccade latency, or motor pro-
gramming time t(M). We assume that the dura-
tion of this period is given by the free parameter
m:
t(Mi) = m (3)
The following is worth noting. Some influential
research suggests that motor programming is com-
pleted in two stages (Becker and J?rgens, 1979).
The first of these being a labile stage during which
a planned saccade can be canceled, e.g., in fa-
vor of another saccade target. The second stage,
closer in time to the execution of the saccade, is
non-labile and once entered, a saccade underway
can no longer be modified or canceled. This divi-
sion between labile and non-labile stages of motor
programming is sometimes implemented in com-
putational models, for example in E-Z Reader and
SWIFT. For now, however, our model does not op-
erationalize the notion of saccade canceling and
thus makes no useful distinction between labile
and non-labile stages of motor programming. Our
only assumption with respect to these different
stages of motor programming is that their respec-
tive durations sum up to m.
An important function of motor programming
in our model, however, is to select a target for the
saccade. Before discussing how this is achieved
we should point out that we make no claim as
to how much time of motor programming is con-
sumed by target selection. It is only presupposed
that saccade target selection, in the normal course
of events, is initiated as soon as there is a decision
to make an eye movement (i.e., when motor pro-
gramming starts), and that, whatever time remains
of motor programming once a target is selected,
this time is spent on preparation of the physical
movement to the selected target. Once motor pro-
gramming is finished, a saccade S is executed to
the target.
Following Nilsson and Nivre (2009), we treat
target selection as a classification task. In prac-
tical terms, this means that we train a classifier
to predict the most likely eye movement follow-
ing any fixation. An instance to be classified con-
sists of a feature vector encoding feature informa-
tion over the current fixated word and words in
the immediate context. Given such feature rep-
resentations and training data obtained from eye-
tracking recordings, essentially any standard ma-
chine learning algorithm can be applied to the
classification task. The type of learning algorithm
that performs best on this task is, however, un-
known. Rather than speculate, we suggest that this
is a question for further research.
The remaining assumptions we make are as fol-
lows. First, because there is a sharp drop-off in
acuity of the human eye around the point of fix-
ation, the number of words that can be discrim-
inated in parafoveal vision on a given fixation is
limited to a few. Therefore, it is reasonable to as-
66
sume that the potential targets for a saccade on
any given fixation are limited to the words avail-
able within the range of effective vision. 4 This
is supported empirically by the fact that the great
majority of outgoing saccades tend to land in one
of the three words that follow the current fixation.
Moreover, we assume that for these potential tar-
gets, only rather coarse, visual information, such
as a gross appreciation of their length, can be ex-
tracted on any given fixation. The reason for this
is that target selection generally occurs relatively
early on in a fixation, at a time when only low-
level visual information can reasonably be gleaned
from the parafovea.
Secondly, we reason that target selection re-
flects an autonomous process that has been au-
tomated, through years of practice, to progress
through the text and select targets in the default
reading direction. Hence, the possible targets for
target selection, as construed here, is limited to the
targets within the forward field of effective vision.
As a consequence, words to the left of the current
fixation are not fixated as a result of target selec-
tion.
Finally, we assume that target selection by de-
fault is a mechanical routine, insensitive to ongo-
ing lexical processing. In the general case, then,
the decision of where to move eyes is made in-
dependently of processing considerations. Mo-
tor programs in general, however, may sometimes
override the default target selection mechanism
and be initiated, not in order to select a new target,
but to correct for situations where motor control
and ongoing language processing are threatening
to desynchronize. Such a corrective program may
be initiated, for instance, if a saccade is executed
to wordi but lexical processing has not yet com-
pleted on wordi?1, and so more lexical process-
ing of wordi?1 is needed before moving on. In
this case, a corrective motor program is initiated
to wordi?1, subsequently resulting in a regression
to that word. In this way, corrective motor pro-
grams serve to synchronize the eyes with the cur-
rent processing stream and for that reason they al-
ways target the word being processed. Moreover,
because corrective saccade programs are launched
with a fixed target, they do not trigger target selec-
tion during motor programming.
4The effective visual field (the perceptual span) extends
about four characters to the left and 15 characters to the right
of the fixation for normal readers of left-to-right orthogra-
phies (Rayner, 1998).
3.5 Saccade Execution
The time to execute a saccade t(S) is determined
by the free parameter s:
t(Si) = s (4)
Once a saccade has been executed, the position of
the eyes shifts to a new word and thus, in the nor-
mal course of events, a new motor program is initi-
ated after t(Di). However, sometimes a saccade is
made ahead of the current processing stream, be-
cause, as noted earlier, a word needs not be fully
processed before a saccade is executed to another
word. Likewise, a saccade may sometimes be ex-
ecuted to a word that has already been fully pro-
cessed, because target selection is an autonomous
process, not influenced by ongoing processing. In
these situations, corrective saccade programs are
initiated. Since corrective saccade programs serve
only to rapidly coordinate the eyes and the cur-
rent processing stream, we assume that they can
be initiated immediately and hence that they are
not subject to saccade initiation delay.
3.6 Eye Movement Control
Having defined the respective component pro-
cesses, we now consider how these processes are
coordinated to model eye movement control. Lex-
ical processing is always running in parallel with
the processes controlling saccade initiation delay,
motor programming and saccade execution, which
are executed in sequence. A simulation of read-
ing is started by initiating lexical processing of the
first word (L1), and the saccade initiation delay
for the first word (D1) (i.e., the first word is fix-
ated). Whenever one of the running processes ter-
minates, new processes are initiated in the follow-
ing way:
? If Li terminates, initiate Li+1.
? If Di terminates, initiate Mi and select new
fixation target wj .
? If Mi terminates, initiate Si.
? If Si terminates and the ongoing lexical pro-
cess is Lj :
? If i = j, initiate Di.
? If i 6= j, initiate Mj and set fixation tar-
get to wj
The simulation terminates when all words have
been lexically processed.
67
4 Experimental Evaluation
4.1 Experimental Setup
In order to estimate the performance of the model
described in the previous section, some experi-
ments were performed using data from the English
section of the Dundee corpus (Kennedy and Pynte,
2005).
In most evaluations of eye movement control
models, the model parameters are fitted against
one and the same corpus by searching the param-
eter space to find the set of parameter values that
best simulates the observed data. This approach
makes it somewhat hard to appreciate how well
a given model generalizes to new, previously un-
seen data. A more stringent evaluation, which af-
fords an assessment of the generalization error of
model predictions, is to set the model parameters
on some portion of the data and then test the model
on another held-out portion. The results we report
in this paper were obtained this way.
The Dundee corpus that was used in these ex-
periments contains the eye tracking records of ten
subjects reading editorials from The Independent,
a UK broadsheet newspaper. The data consist of
20 texts that were read by all subjects, and close to
2400 sentences. We divided these texts into three
sets: the first 16 for training (1911 sentences),
17-18 for model development and validation (237
sentences), and the last two texts, 19-20, for blind
testing of the model (231 sentences). Model pa-
rameters were fitted using only the training and
validation set, prior to evaluating the model on the
held-out test set.
Next we discuss how training was performed,
both in terms of the training of the classifier for
target selection and in terms of the estimation of
the model?s process parameters on the training
data. Before presenting the results, we also discuss
some standard practice in benchmarking models
of eye movement control.
4.2 Training the Classifier
We used the transition-based model outlined by
Nilsson and Nivre (2009) in combination with lo-
gistic regression for training the target selection
classifier. The classifier was trained on a restricted
number of features defined over words in the fixa-
tion context. The feature model we used for these
experiments included information about the word
length of the current fixation and upcoming words,
as well as some historical information about re-
cently made eye movements. The history of pre-
vious eye movements was represented in terms
of the saccade distance (measured in number of
words) that led up to recently made fixations (in-
cluding the current fixation). In this way, the fea-
ture model contained information about, for in-
stance, whether the saccade that led up to the cur-
rent fixation skipped a word or two.
In contrast to Nilsson and Nivre (2009) we did
not train one model for each individual subject in
the corpus. Instead, we trained a single multiple-
subject classifier on all ten readers in the training
set. The performance of this classifier was as-
sessed in terms of how well, on average, it pre-
dicted the observed saccade targets for any given
reader on the development set. Moreover, in line
with the assumption that target selection is re-
stricted to a limited number of candidate words in
the forward visual field, the classifier was trained
to select one of the three words following any fixa-
tion as the target for a saccade. This cross-subject
classifier achieved an average prediction accuracy
of 72% on the development set.
4.3 Estimating Model Parameters
Because the model?s process parameters can not
be directly estimated from eye tracking data they
need to be approximated in other ways. The val-
ues for the intercept and slope parameters for lexi-
cal processing time t(Li) were obtained by fitting
a linear regression of gaze duration on logarithmic
word frequency and word length on the training
data. The assumption that the gaze duration on
a given word reflects the time required to process
the word is necessarily an oversimplification but
is sometimes used in eye movement modeling. A
number of studies indicate that it is indeed a rea-
sonable approximation (Engbert et al, 2002; Pol-
latsek et al, 2006).
The value for the parameter d in the equation for
t(Di) was selected based on a simple parameter
search over the training data. The best fitting value
was assessed by calculating the root mean square
error between predicted and observed values for
gaze durations for different values of d ranging
from 0 to 1 in 0.1 increments, while keeping other
parameter values unchanged. To keep things sim-
ple, the parameters that determine the mean dura-
tion of motor programming, m, and saccade exe-
cution, s, were fixed at 200 ms, and 25 ms, respec-
tively. These values are in good agreement with
68
Parameter Interpretation Value
b0 Intercept: base lexical processing time (ms) 165.5
b1 Slope: effect of length on lexical processing time (ms) 13.5
b2 Slope: effect of frequency on lexical processing time (ms) 3.2
d Proportion of lexical processing time (determines saccade initiation delay) 0.5
m Mean motor programming time (ms) 200
s Mean saccade execution time (ms) 25
Table 1: Model parameters, their interpretations and values, as estimated during training.
estimated values in experimental studies. Table 1
lists the model?s six process parameters and their
values, obtained prior to testing the model.
4.4 Benchmark Evaluation
Models of eye movement control in reading are
typically benchmarked against a set of word-based
dependent eye movement measures which are av-
eraged across subjects. Two such measures are
gaze duration and probability of skipping. Gaze
duration is defined as the sum duration of all fix-
ations on a word prior to any saccade leaving
the word during first-pass reading. Probability of
skipping is simply the mean probability (across
subjects) that a given word is skipped (not fixated)
during first-pass reading.
Because word frequency effects on eye move-
ments during reading are robust and well-
documented, one common benchmark practice is
to evaluate models with respect to their capabil-
ity of reproducing word frequency effects on fix-
ation times and fixation probabilities. Typically,
averages of word-based measures are then broken
down into word-frequency classes. This is a fairly
simple way to see how well a given model can
predict observed means for measures such as gaze
duration and skipping probability for words of dif-
ferent frequency classes. The results we report are
presented this way. We used frequency estimates
based on word occurrences in the written part of
the British National Corpus (BNC). Frequencies
were normalized to occurrences per million words
and then divided into five frequency classes, as
suggested by Reichle et al (1998).
In addition to the model we have outlined so
far, we also present results for two alternative
versions. These models differ from the one we
have discussed only in positing a simpler func-
tion for lexical processing time. The alternative
versions model lexical processing time only as a
linear function of either word length or logarith-
mic word frequency. Hence, we fitted two sepa-
rate simple linear regressions of gaze duration first
on word length, and then on logarithmic word fre-
quency. The regression coefficient and slope were
estimated to 132.5 and 16 for the model based on
word length, and 284 and -11 for the model based
on frequency.
4.5 Results and Discussion
Table 2 shows the observed (empirical) and pre-
dicted (simulated) values of gaze durations and
skipping probabilities for each of the five word fre-
quency classes, both on the development set and
on the held-out test set. M1 and M2 represent the
versions of the model in which lexical processing
time is a linear function of word length, and word
frequency, respectively. M3 represents the version
of the model where lexical processing time is a
linear function of both variables.
The results show that all three models, on the
development set as well as on the test set, are
able to reproduce the most important aspect of
the observed data, namely, that mean gaze du-
rations decrease and mean skipping probabilities
increase with increasing word frequency. Over-
all, M3 performs better than the two other models
in predicting this relationship. The model based
only on word length, M1, performs worse than the
other two models. This is mainly due to the poor
performance of this model in simulating the pro-
portions of skipped words in the upper frequency
classes 4 and 5. In comparison to both M2 and M3,
M1 seriously underestimates the observed skip-
ping probability for words belonging to these fre-
quency classes, on both development and test data.
With respect to gaze duration alone, the three
models perform similarly, although M3 provides
a somewhat better fit on both data sets. The mod-
els generally predict longer gaze durations than the
observed means, except for the most low-frequent
words. In particular, gaze durations for higher-
frequency words (class 4 and 5) are prolonged
compared to the means, giving an overall nar-
69
Gaze duration Probability of skipping
Development Test Development Test
Frequency class Observed M1 M2 M3 Observed M1 M2 M3 Observed M1 M2 M3 Observed M1 M2 M3
1 290 282 280 285 286 278 280 284 0.17 0.15 0.18 0.13 0.16 0.14 0.19 0.14
2 257 271 259 272 261 273 260 275 0.19 0.18 0.20 0.16 0.19 0.15 0.22 0.17
3 229 254 252 249 235 257 254 252 0.24 0.19 0.24 0.20 0.22 0.19 0.25 0.20
4 208 240 238 237 210 244 238 237 0.52 0.23 0.36 0.43 0.53 0.24 0.34 0.40
5 198 238 236 228 195 239 237 230 0.65 0.34 0.51 0.54 0.67 0.32 0.52 0.51
Table 2: Observed and predicted values of Gaze Durations (ms) and Skipping Probabilities on de-
velopment and test set for five frequency classes of words. M1: t(Li) = b0 + b1length(wi), Root
mean square error on development set = 0.48, Root mean square error on test set = 0.52; M2:
t(Li) = b0 ? b1 ln(freq(wi)), Root mean square error on development set = 0.33, Root mean square
error on test set = 0.35; M3: t(Li) = b0 + b1length(wi) ? b2 ln(freq(wi)), Root mean square error on
development set = 0.21, Root mean square error on test set = 0.26; Frequency range: 1:1-10, 2:11-100,
3:101-1000, 4:1001-10000, 5: 10001+
rower range of mean values for the five frequency
classes.
The overall performance of each model, M1,
M2 and M3 was estimated by calculating the root
mean square error (RMSE) between the mean ob-
served and predicted gaze durations and probabil-
ities of skipping. The errors were normalized as
described in Reichle et al (1998). In comparing
the results for both development and test data, the
best overall fit is provided by M3 on the develop-
ment set, giving an RMSE of 0.21 (smaller val-
ues indicate better fit). The fit for the same model
drops to 0.26 when evaluated on the held-out test
data.
To provide some basis for comparison, the ear-
liest version of E-Z Reader (Reichle et al, 1998)
which was fitted to the same dependent measures,
had an RMSE of 0.145. It is important to point
out, however, that this result was based on fitting
the model parameters to a single sentence corpus
of 48 sentences designed for experimental pur-
poses. This corpus contained relatively short (8-
14 words) isolated sentences without any connect-
ing discourse. More generally, as noted by Re-
ichle et al (2009), RMSD values lower than 0.5
provide fits that are reasonably close to the ob-
served means. By this standard, the model M3 per-
forms rather well in simulating the observed data.
Moreover, this version of the model provides the
most realistic estimates of the time it takes to iden-
tify words. Thus, for example, the mean time to
identify the most frequent word in English, ?the?
(frequency class 5), is estimated to be 171 ms,
whereas the mean time to identify the word ?re-
populate?, which is a low-frequency (frequency
class 1) ten-letter word is estimated to be 301 ms.
These estimates are in good agreement with ex-
perimental estimates, which show that word iden-
tification latencies range between 150 and 300 ms
(Rayner and Pollatsek, 1989).
5 Conclusion
In this paper we built on previous work using ma-
chine learning methods to model saccade behavior
in reading and we extended this work by present-
ing a data-driven model of eye movement control
that provides detailed predictions for both when
and where the eyes move during reading. The most
important principles of this model are (i) the initi-
ation of eye movements is delayed as a function
of on-line processing difficulty, and (ii) the deci-
sion of where to move the eyes is driven by an
autonomous routine that has become automated
through years of practice in reading. The model
was trained on eye movements made over a large
corpus of natural text. In benchmarking the model
against held-out data we showed that it is able to
reproduce frequency effects on both gaze dura-
tion and skipping probability with good accuracy
(RMSE = 0.26).
Looking ahead, we plan to extend the model
to account for more empirical data on eye move-
ment behavior in reading. One important step
to meet this goal is to develop a more informed
model of language processing. Current models
of eye movement control in reading generally as-
sume that influences from syntactic and higher-
order processing occur too late in the process-
ing stream to directly influence eye movements.
This is, however, seemingly at odds with recent
70
findings in sentence processing research showing
an influence of syntactic processing difficulty on
both early and late measures of eye movements
in reading (Demberg and Keller, 2008; Boston et
al., 2008). Hence, it is possible that a more ac-
curate model of eye movements in reading will
need to allow for syntactic processing to influ-
ence the early decisions that control the timing of
eye movements. This and other issues will be ad-
dressed in future work.
References
David. A Balota and James. I Chumbley. 1984. Are
lexical decisions a good measure of lexical access?
the role of word frequency in the neglected decision
stage. Journal of Experimental Psychology: Human
perception and Performace, 10:340?357.
David. A. Balota, Alexander Pollatsek, and Keith
Rayner. 1985. The interaction of contextual con-
straints and parafoveal visual information in reading.
Cognitive Psychology, 17:364?390.
W Becker and R J?rgens. 1979. An analysis of the
saccadic system by means of double step stimuli. Vi-
sion Research, 19:967?983.
Marisa F. Boston, John Hale, Reinhold Kliegl, Umesh
Patil, and Shravan Vasishth. 2008. Parsing costs as
predictors of reading difficulty: An evaluation using
the potsdam sentence corpus. Journal of Eye Move-
ment Reasearch, 2:1?12.
Marc Brysbaert and Fran?oise Vitu. 1998. Word skip-
ping: implications for theories of eye movement
control in reading. In Geoffrey Underwood, edi-
tor, Eye guidance in Reading and Scene Perception,
pages 124?147. Elsevier science Ltd.
Charles Clifton, Adrian Staub, and Keith Rayner.
2007. Eye movements in reading words and sen-
tences. In Roger van Gompel, editor, Eye move-
ments: A window on mind and brain, pages 341?
372. Amsterdam: Elsevier.
Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109:193?210.
Ralf Engbert, Andr? Longtin, and Reinhold Kliegl.
2002. A dynamical model of saccade generation
in reading based on spatially distributed lexical pro-
cessing. Vision Research, 42:621?636.
Ralf Engbert, Antje Nuthmann, Eike Richter, and Rein-
hold Kliegl. 2005. SWIFT: A dynamical model of
saccade generation during reading. Psychological
Review, 112:777?813.
Alan Kennedy and Jo?l Pynte. 2005. Parafoveal-on-
foveal effects in normal reading. Vision research,
45:153?168.
R. M. McPeek, A. A. Skavenski, and K Nakayama.
2000. Concurrent processing of saccades in visual
search. Vision Research, 40:2499?2516.
Mattias Nilsson and Joakim Nivre. 2009. Learn-
ing where to look: Modeling eye movements in
reading. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), pages 93?101.
Alexander Pollatsek, Mary Lesch, Robin K. Morris,
and Keith Rayner. 1992. Phonological codes
are used in integrating information across saccades
in word identification and reading. Experimental
Psychology: Human Perception and Performance,
18:148?162.
Alexander Pollatsek, Erik Reichle, and Keith Rayner.
2006. Tests of the E-Z Reader model: Exploring
the interface between cognition and eye movements.
Cognitive Psychology, 52:1?56.
Keith Rayner and Alexander Pollatsek. 1989. The psy-
chology of reading. Englewood Cliffs, NJ: Prentice
Hall.
Keith Rayner, Albert W. Inhoff, Robert E. Morrison,
Maria L. Slowiaczek, and James H. Bertera. 1981.
Masking of foveal and parafoveal vision during
eye fixations in reading. Journal of Experimental
Psychology: Human Perception and Performance,
7:167?179.
Keith Rayner, Simon P. Liversedge, Sarah J. White, and
Dorine Vergilino-Perez. 2003. Reading disappear-
ing text: cognitive control of eye movements. Psy-
chological science, 14:385?388.
Keith Rayner. 1998. Eye movements in reading and
information processing: 20 years of research. Psy-
chological Bulletin, 124:372?422.
Erik Reichle, Alexander Pollatsek, Donald Fisher, and
Keith Rayner. 1998. Toward a model of eye move-
ment control in reading. Psychological Review,
105:125?157.
Erik Reichle, Keith Rayner, and Alexander Pollatsek.
2003. The E-Z Reader model of eye-movement con-
trol in reading: Comparisons to other models. Be-
havioral and Brain Sciences, 26:445?476.
Erik Reichle, Tessa Warren, and Kerry McConnell.
2009. Using E-Z Reader to model the effects of
higher-level language processing on eye movements
during reading. Psychonomic Bulletin & Review,
16:1?21.
Eric Reichle, editor. 2006a. Cognitive Systems Re-
search. 7:1?96. Special issue on models of eye-
movement control in reading.
Eric Reichle. 2006b. Computational models of eye
movement control in reading: Theories of the ?eye-
mind" link. Cognitive Systems Research, 7:2?3.
71
Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 10?18,
COLING 2010, Beijing, August 2010.
A Systematic Comparison between Inversion Transduction Grammar
and Linear Transduction Grammar for Word Alignment
Markus Saers and Joakim Nivre
Dept. of Linguistics & Philology
Uppsala University
first.last@lingfil.uu.se
Dekai Wu
HKUST
Human Language Technology Center
Dept. of Computer Science & Engineering
Hong Kong Univ. of Science & Technology
dekai@cs.ust.hk
Abstract
We present two contributions to gram-
mar driven translation. First, since both
Inversion Transduction Grammar and
Linear Inversion Transduction Gram-
mars have been shown to produce bet-
ter alignments then the standard word
alignment tool, we investigate how the
trade-off between speed and end-to-end
translation quality extends to the choice
of grammar formalism. Second, we
prove that Linear Transduction Gram-
mars (LTGs) generate the same transduc-
tions as Linear Inversion Transduction
Grammars, and present a scheme for ar-
riving at LTGs by bilingualizing Linear
Grammars. We also present a method for
obtaining Inversion Transduction Gram-
mars from Linear (Inversion) Transduc-
tion Grammars, which can speed up
grammar induction from parallel corpora
dramatically.
1 Introduction
In this paper we introduce Linear Transduction
Grammars (LTGs), which are the bilingual case
of Linear Grammars (LGs). We also show that
LTGs are equal to Linear Inversion Transduction
Grammars (Saers et al, 2010). To be able to in-
duce transduction grammars directly from par-
allel corpora an approximate search for parses is
needed. The trade-off between speed and end-to-
end translation quality is investigated and com-
pared to Inversion Transduction Grammars (Wu,
1997) and the standard tool for word alignment,
GIZA++ (Brown et al, 1993; Vogel et al, 1996;
Och and Ney, 2003). A heuristic for converting
stochastic bracketing LTGs into stochastic brack-
eting ITGs is presented, and fitted into the speed?
quality trade-off.
In section 3 we give an overview of transduc-
tion grammars, introduce LTGs and show that
they are equal to LITGs. In section 4 we give
a short description of the rational for the trans-
duction grammar pruning used. In section 5 we
describe a way of seeding a stochastic bracketing
ITG with the rules and probabilities of a stochas-
tic bracketing LTG. Section 6 describes the setup,
and results are given in section 7. Finally, some
conclusions are offered in section 8
2 Background
Any form of automatic translation that relies on
generalizations of observed translations needs to
align these translations on a sub-sentential level.
The standard way of doing this is by aligning
words, which works well for languages that use
white space separators between words. The stan-
dard method is a combination of the family of
IBM-models (Brown et al, 1993) and Hidden
Markov Models (Vogel et al, 1996). These
methods all arrive at a function (A) from lan-
guage 1 (F ) to language 2 (E). By running the
process in both directions, two functions can be
estimated and then combined to form an align-
ment. The simplest of these combinations are in-
tersection and union, but usually, the intersection
is heuristically extended. Transduction gram-
mars on the other hand, impose a shared struc-
ture on the sentence pairs, thus forcing a consis-
tent alignment in both directions. This method
10
has proved successful in the settings it has been
tried (Zhang et al, 2008; Saers and Wu, 2009;
Haghighi et al, 2009; Saers et al, 2009; Saers
et al, 2010). Most efforts focus on cutting down
time complexity so that larger data sets than toy-
examples can be processed.
3 Transduction Grammars
Transduction grammars were first introduced in
Lewis and Stearns (1968), and further devel-
oped in Aho and Ullman (1972). The origi-
nal notation called for regular CFG-rules in lan-
guage F with rephrased E productions, either in
curly brackets, or comma separated. The bilin-
gual version of CFGs is called Syntax-Directed
Transduction Grammars (SDTGs). To differenti-
ate identical nonterminal symbols, indices were
used (the bag of nonterminals for the two pro-
ductions are equal by definition).
A ? B(1) a B(2) {x B(1) B(2)}
= A ? B(1) a B(2), x B(1) B(2)
The semantics of the rules is that one nontermi-
nal rewrites into a bag of nonterminals that is dis-
tributed independently in the two languages, and
interspersed with any number of terminal sym-
bols in the respective languages. As with CFGs,
the terminal symbols can be factored out into
preterminals with the added twist that they are
shared between the two languages, since preter-
minals are formally nonterminals. The above
rule can thus be rephrased as
A ? B(1) Xa/x B(2), Xa/x B(1) B(2)
Xa/x ? a, x
In this way, rules producing nonterminals and
rules producing terminals can be separated.
Since only nonterminals are allowed to move,
their movement can be represented as the orig-
inal sequence of nonterminals and a permutation
vector as follows:
A ? B Xa/x B ; 1, 0, 2
Xa/x ? a, x
To keep the reordering as monotone as possible,
the terminals a and x can be produced separately,
but doing so eliminates any possibility of param-
eterizing their lexical relationship. Instead, the
individual terminals are pair up with the empty
string (?).
A ? Xx B Xa B ; 0, 1, 2, 3
Xa ? a, ?
Xx ? ?, x
Lexical rules involving the empty string are re-
ferred to as singletons. Whenever a preterminal
is used to pair up two terminal symbols, we refer
to that pair of terminals as a biterminal, which
will be written as e/f .
Any SDTG can be rephrased to contain per-
muted nonterminal productions and biterminal
productions only, and we will call this the nor-
mal form of SDTGs. Note that it is not possi-
ble to produce a two-normal form for SDTGs,
as there are some rules that are not binarizable
(Wu, 1997; Huang et al, 2009). This is an
important point to make, since efficient parsing
for CFGs is based on either restricting parsing
to only handle binary grammars (Cocke, 1969;
Kasami, 1965; Younger, 1967), or rely on on-
the-fly binarization (Earley, 1970). When trans-
lating with a grammar, parsing only has to be
done in F , which is binarizable (since it is a
CFG), and can therefor be computed in polyno-
mial time (O(n3)). Once there is a parse tree
for F , the corresponding tree for E can be eas-
ily constructed. When inducing a grammar from
examples, however, biparsing (finding an anal-
ysis that is consistent across a sentence pair) is
needed. The time complexity for biparsing with
SDTGs is O(n2n+2), which is clearly intractable.
Inversion Transduction Grammars or ITGs
(Wu, 1997) are transduction grammars that have
a two-normal form, thus guaranteeing binariz-
ability. Defining the rank of a rule as the number
of nonterminals in the production, and the rank
of a grammar as the highest ranking rule in the
rule set, ITGs are a) any SDTG of rank two, b)
any SDTG of rank three or c) any SDTG where no
rule has a permutation vector other than identity
permutation or inversion permutation. It follows
from this definition that ITGs have a two-normal
form, which is usually expressed as SDTG rules,
11
with brackets around the production to distin-
guish the different kinds of rules from each other.
A ? B C ; 0, 1 = A ? [ B C ]
A ? B C ; 1, 0 = A ? ? B C ?
A ? e/f = A ? e/f
By guaranteeing binarizability, biparsing time
complexity becomes O(n6).
There is an even more restricted version of
SDTGs called Simple Transduction Grammar
(STG), where no permutation at all is allowed,
which can also biparse a sentence pair in O(n6)
time.
A Linear Transduction Grammar (LTG) is a
bilingual version of a Linear Grammar (LG).
Definition 1. An LG in normal form is a tuple
GL = ?N,?, R, S?
Where N is a finite set of nonterminal symbols,
? is a finite set of terminal symbols, R is a finite
set of rules and S ? N is the designated start
symbol. The rule set is constrained so that
R ? N ? (? ? {?})N(? ? {?}) ? {?}
Where ? is the empty string.
To bilingualize a linear grammar, we will take
the same approach as taken when a finite-state
automaton is bilingualized into a finite-state
transducer. That is: to replace all terminal sym-
bols with biterminal symbols.
Definition 2. An LTG in normal form is a tuple
T GL = ?N,?,?, R, S?
Where N is a finite set of nonterminal symbols,
? is a finite set of terminal symbols in language
E, ? is a finite set of terminal symbols in lan-
guage F , R is a finite set of linear transduction
rules and S ? N is the designated start symbol.
The rule set is constrained so that
R ? N ??N? ? {??, ??}
Where ? = ??{?}???{?} and ? is the empty
string.
Graphically, we will represent LTG rules as pro-
duction rules with biterminals:
?A, ?x, p?B?y, q?? = A ? x/p B y/q
?A, ??, ??? = B ? ?/?
Like STGs, LTGs do not allow any reordering,
and are monotone, but because they are linear,
this has no impact on expressiveness, as we shall
see later.
Linear Inversion Transduction Grammars
(LITGs) were introduced in Saers et al (2010),
and represent ITGs that are allowed to have at
most one nonterminal symbol in each produc-
tion. These are attractive because they can bi-
parse a sentence pair in O(n4) time, which can
be further reduced to linear time by severely
pruning the search space. This makes them
tractable for large parallel corpora, and a viable
way to induce transduction grammars from large
parallel corpora.
Definition 3. An LITG in normal form is a tuple
T GLI = ?N,?,?, R, S?
Where N is a finite set of nonterminal symbols,
? is a finite set of terminal symbols from lan-
guage E, ? is a finite set of terminal symbols
from language F , R is a set of rules and S ? N
is the designated start symbol. The rule set is
constrained so that
R ? N ? {[], ??} ??N ?N? ? {??, ??}
Where [] represents identity permutation and ??
represents inversion permutation, ? = ??{?}?
? ? {?} is a possibly empty biterminal, and ? is
the empty string.
Graphically, a rule will be represented as an ITG
rule:
?A, [], B?e, f?? = A ? [ B e/f ]
?A, ??, ?e, f?B? = A ? ? e/f B ?
?A, [], ??, ??? = A ? ?/?
As with ITGs, productions with only biterminals
will be represented without their permutation, as
any such rule can be trivially rewritten into in-
verted or identity form.
12
Definition 4. An ?-free LITG is an LITG where
no rule may rewrite one nonterminal into another
nonterminal only. Formally, the rule set is con-
strained so that
R ?N ? {[], ??} ? ({??, ??}B ?B{??, ??}) = ?
The LITG presented in Saers et al (2010) is
thus an ?-free LITG in normal form, since it has
the following thirteen rule forms (of which 8 are
meaningful, 1 is only used to terminate genera-
tion and 4 are redundant):
A ? [ e/f B ]
A ? ? e/f B ?
A ? [ B e/f ]
A ? ? B e/f ?
A ? [ e/? B ] | A ? ? e/? B ?
A ? [ B e/? ] | A ? ? B e/? ?
A ? [ ?/f B ] | A ? ? B ?/f ?
A ? [ B ?/f ] | A ? ? ?/f B ?
A ? ?/?
All the singleton rules can be expressed either in
straight or inverted form, but the result of apply-
ing the two rules are the same.
Lemma 1. Any LITG in normal form can be ex-
pressed as an LTG in normal form.
Proof. The above LITG can be rewritten in LTG
form as follows:
A ? [ e/f B ] = A ? e/f B
A ? ? e/f B ? = A ? e/? B ?/f
A ? [ B e/f ] = A ? B e/f
A ? ? B e/f ? = A ? ?/f B e/?
A ? [ e/? B ] = A ? e/? B
A ? [ B e/? ] = A ? B e/?
A ? [ ?/f B ] = A ? ?/f B
A ? [ B ?/f ] = A ? B ?/f
A ? ?/? = A ? ?/?
To account for all LITGs in normal form, the fol-
lowing two non-?-free rules also needs to be ac-
counted for:
A ? [ B ] = A ? B
A ? ? B ? = A ? B
Lemma 2. Any LTG in normal form can be ex-
pressed as an LITG in normal form.
Proof. An LTG in normal form has two rules,
which can be rewritten in LITG form, either as
straight or inverted rules as follows
A ? x/p B y/q = A ? [ x/p B? ]
B? ? [ B y/q ]
= A ? ? x/q B? ?
B? ? ? B y/p ?
A ? ?/? = A ? ?/?
Theorem 1. LTGs in normal form and LITGs in
normal form express the same class of transduc-
tions.
Proof. Follows from lemmas 1 and 2.
By theorem 1 everything concerning LTGs is also
applicable to LITGs, and an LTG can be expressed
in LITG form when convenient, and vice versa.
4 Pruning the Alignment Space
The alignment space for a transduction grammar
is the combinations of the parse spaces of the
sentence pair. Let e be the E sentence, and f
be the F sentence. The parse spaces would be
O(|e|2) and O(|f |2) respectively, and the com-
bination of these spaces would be O(|e|2?|f |2),
or O(n4) if we assume n to be proportional
to the sentence lengths. In the case of LTGs,
this space is searched linearly, giving time com-
plexity O(n4), and in the case of ITGs there
is branching within both parse spaces, adding
an order of magnitude each, giving a total time
complexity of O(n6). There is, in other words,
a tight connection between the alignment space
and the time complexity of the biparsing al-
gorithm. Furthermore, most of this alignment
space is clearly useless. Consider the case where
the entire F sentence is deleted, and the entire E
sentence is simply inserted. Although it is pos-
sible that it is allowed by the grammar, it should
have a negligible probability (since it is clearly a
translation strategy that generalize poorly), and
could, for all practical reasons, be ignored.
13
Language pair Bisentences Tokens
Spanish?English 108,073 1,466,132
French?English 95,990 1,340,718
German?English 115,323 1,602,781
Table 1: Size of training data.
Saers et al (2009) present a scheme for prun-
ing away most of the points in the alignment
space. Parse items are binned according to cov-
erage (the total number of words covered), and
each bin is restricted to carry a maximum of b
items. Any items that do not fit in the bins are
excluded from further analysis. To decide which
items to keep, inside probability is used. This
pruning scheme effectively linearizes the align-
ment space, as is will be of size O(nb), regard-
less of what type grammar is used. An ITG can
thus be biparsed in cubic time, and an LTG in lin-
ear time.
5 Seeding an ITG with an LTG
Since LTGs are a subclass of ITGs, it would be
possible to convert an LTG to a ITG. This could
save a lot of time, since LTGs are much faster to
induce from corpora than ITGs.
Converting a BLTG to a BITG is fairly straight
forward. Consider the BLTG rule
X ? [ e/f X ]
To convert it to BITG in two-normal form, the
biterminal has to be factored out. Replacing
the biterminal with a temporary symbol X? , and
introducing a rule that rewrites this temporary
symbol to the replaced biterminal produces two
rules:
X ? [ X? X ]
X? ? e/f
This is no longer a bracketing grammar since
there are two nonterminals, but equating X? to X
restores this property. An analogous procedure
can be applied in the case where the nonterminal
comes before the biterminal, as well as for the
inverting cases.
When converting stochastic LTGs, the proba-
bility mass of the SLTG rule has to be distributed
to two SITG rules. The fact that the LTG rule
X ? ?/? lacks correspondence in ITGs has to
be weighted in as well. In this paper we took the
maximum entropy approach and distributed the
probability mass uniformly. This means defin-
ing the probability mass function p? for the new
SBITG from the probability mass function p of
the original SBLTG such that:
p?(X ? [ X X ]) =
?
e/f
?
???
?
p(X?[ e/f X ])
1?p(X??/?)
+?
p(X?[ X e/f ])
1?p(X??/?)
?
???
p?(X ? ? X X ?) =
?
e/f
?
???
?
p(X?? e/f X ?)
1?p(X??/?)
+?
p(X?? X e/f ?)
1?p(X??/?)
?
???
p?(X ? e/f) =
?
?????????????
?
p(X?[ e/f X ])
1?p(X??/?)
+?
p(X?[ X e/f ])
1?p(X??/?)
+?
p(X?? e/f X ?)
1?p(X??/?)
+?
p(X?? X e/f ?)
1?p(X??/?)
?
?????????????
6 Setup
The aim of this paper is to compare the align-
ments from SBITG and SBLTG to those from
GIZA++, and to study the impact of pruning
on efficiency and translation quality. Initial
grammars will be estimated by counting cooc-
currences in the training corpus, after which
expectation-maximization (EM) will be used to
refine the initial estimate. At the last iteration,
the one-best parse of each sentence will be con-
sidered as the word alignment of that sentence.
In order to keep the experiments comparable,
relatively small corpora will be used. If larger
corpora were used, it would not be possible to get
any results for unpruned SBITGs because of the
prohibitive time complexity. The Europarl cor-
pus (Koehn, 2005) was used as a starting point,
and then all sentence pairs where one of the sen-
tences were longer than 10 tokens were filtered
14
Figure 1: Trade-offs between translation quality (as measured by BLEU) and biparsing time (in
seconds plotted on a logarithmic scale) for SBLTGs, SBITGs and the combination.
Beam size
System 1 10 25 50 75 100 ?
BLEU
SBITG 0.1234 0.2608 0.2655 0.2653 0.2661 0.2671 0.2663
SBLTG 0.2574 0.2645 0.2631 0.2624 0.2625 0.2633 0.2628
GIZA++ 0.2597 0.2597 0.2597 0.2597 0.2597 0.2597 0.2597
NIST
SBITG 3.9705 6.6439 6.7312 6.7101 6.7329 6.7445 6.6793
SBLTG 6.6023 6.6800 6.6657 6.6637 6.6714 6.6863 6.6765
GIZA++ 6.6464 6.6464 6.6464 6.6464 6.6464 6.6464 6.6464
Training times
SBITG 03:10 17:00 38:00 1:20:00 2:00:00 2:40:00 3:20:00
SBLTG 35 1:49 3:40 7:33 9:44 12:13 11:59
Table 2: Results for the Spanish?English translation task.
out (see table 1). The GIZA++ system was built
according to the instructions for creating a base-
line system for the Fifth Workshop on Statistical
Machine Translation (WMT?10),1 but the above
corpora were used instead of those supplied by
the workshop. This includes word alignment
with GIZA++, a 5-gram language model built
with SRILM (Stolcke, 2002) and parameter tun-
ing with MERT (Och, 2003). To carry out the ac-
tual translations, Moses (Koehn et al, 2007) was
used. The SBITG and SBLTG systems were built
in exactly the same way, except that the align-
ments from GIZA++ were replaced by those from
the respective grammars.
In addition to trying out exhaustive biparsing
1http://www.statmt.org/wmt10/
for SBITGs and SBLTGs on three different trans-
lation tasks, several different levels of pruning
were tried (1, 10, 25, 50, 75 and 100). We also
used the grammar induced from SBLTGs with a
beam size of 25 to seed SBITGs (see section 5),
which were then run for an additional iteration
of EM, also with beam size 25.
All systems are evaluated with BLEU (Pap-
ineni et al, 2002) and NIST (Doddington, 2002).
7 Results
The results for the three different translation
tasks are presented in Tables 2, 3 and 4. It is
interesting to note that the trend they portray is
quite similar. When the beam is very narrow,
GIZA++ is better, but already at beam size 10,
both transduction grammars are superior. Con-
15
Beam size
System 1 10 25 50 75 100 ?
BLEU
SBITG 0.1268 0.2632 0.2654 0.2669 0.2668 0.2655 0.2663
SBLTG 0.2600 0.2638 0.2651 0.2668 0.2672 0.2662 0.2649
GIZA++ 0.2603 0.2603 0.2603 0.2603 0.2603 0.2603 0.2603
NIST
SBITG 4.0849 6.7136 6.7913 6.8065 6.8068 6.8088 6.8151
SBLTG 6.6814 6.7608 6.7656 6.7992 6.8020 6.7925 6.7784
GIZA++ 6.6907 6.6907 6.6907 6.6907 6.6907 6.6907 6.6907
Training times
SBITG 03:25 17:00 42:00 1:25:00 2:10:00 2:45:00 3:10:00
SBLTG 31 1:41 3:25 7:06 9:35 13:56 10:52
Table 3: Results for the French?English translation task.
Beam size
System 1 10 25 50 75 100 ?
BLEU
SBITG 0.0926 0.2050 0.2091 0.2090 0.2091 0.2094 0.2113
SBLTG 0.2015 0.2067 0.2066 0.2073 0.2080 0.2066 0.2088
GIZA++ 0.2059 0.2059 0.2059 0.2059 0.2059 0.2059 0.2059
NIST
SBITG 3.4297 5.8743 5.9292 5.8947 5.8955 5.9086 5.9380
SBLTG 5.7799 5.8819 5.8882 5.8963 5.9252 5.8757 5.9311
GIZA++ 5.8668 5.8668 5.8668 5.8668 5.8668 5.8668 5.8668
Training times
SBITG 03:20 17:00 41:00 1:25:00 2:10:00 2:45:00 3:40:00
SBLTG 38 1:58 4:52 8:08 11:42 16:05 13:32
Table 4: Results for the German?English translation task.
sistent with Saers et al (2009), SBITG has a sharp
rise in quality going from beam size 1 to 10,
and then a gentle slope up to beam size 25, af-
ter which it levels out. SBLTG, on the other hand
start out at a respectable level, and goes up a gen-
tle slope from beam size 1 to 10, after which is
level out. This is an interesting observation, as it
suggests that SBLTG reaches its optimum with a
lower beam size (although that optimum is lower
than that of SBITG). The trade-off between qual-
ity and time can now be extended beyond beam
size to include grammar choice. In Figure 1, run
times are plotted against BLEU scores to illus-
trate this trade-off. It is clear that SBLTGs are
indeed much faster than SBITGs, the only excep-
tion is when SBITGs are run with b = 1, but then
the BLEU score is so low that is is not worth con-
sidering.
The time may seem inconsistent between b =
100 and b = ? for SBLTG, but the extra time
for the tighter beam is because of beam manage-
ment, which the exhaustive search doesn?t bother
with.
In table 5 we compare the pure approaches
to one where an LTG was trained during 10 it-
erations of EM and then used to seed (see sec-
16
Translation task System BLEU NIST Total time
SBLTG 0.2631 6.6657 36:40
Spanish?English SBITG 0.2655 6.7312 6:20:00
Both 0.2660 6.7124 1:14:40
SBLTG 0.2651 6.7656 34:10
French?English SBITG 0.2654 6.7913 7:00:00
Both 0.2625 6.7609 1:16:10
SBLTG 0.2066 5.8882 48:52
German?English SBITG 0.2091 5.9292 6:50:00
Both 0.2095 5.9224 1:29:40
Table 5: Results for seeding an SBITG with an SBLTG (Both) compared to the pure approach. Total
time refers to 10 iterations of EM training for SBITG and SBLTG respectively, and 10 iterations of
SBLTG and one iteration of SBITG training for the combined system.
tion 5) an SBITG, which was then trained for
one iteration of EM. Although the differences
are fairly small, German?English and Spanish?
English seem to reach the level of SBITG,
whereas French?English is actually hurt. The
big difference is in time, since the combined sys-
tem needs about a fifth of the time the SBITG-
based system needs. This phenomenon needs to
be more thoroughly examined.
It is also worth noting that GIZA++ was beaten
by an aligner that used less than 20 minutes (less
than 2 minutes per iteration and at most 10 itera-
tions) to align the corpus.
8 Conclusions
In this paper we have introduced the bilingual
version of linear grammar: Linear Transduc-
tion Grammars, and found that they generate the
same class of transductions as Linear Inversion
Transduction Grammars. We have also com-
pared Stochastic Bracketing versions of ITGs and
LTGs to GIZA++ on three word alignment tasks.
The efficiency issues with transduction gram-
mars have been addressed by pruning, and the
conclusion is that there is a trade-off between
run time and translation quality. A part of the
trade-off is choosing which grammar framework
to use, as LTGs are faster but not as good as ITGs.
It also seems possible to take a short-cut in this
trade-off by starting out with an LTG and convert-
ing it to an ITG. We have also showed that it is
possible to beat the translation quality of GIZA++
with a quite fast transduction grammar.
Acknowledgments
This work was funded by the Swedish Na-
tional Graduate School of Language Technol-
ogy (GSLT), the Defense Advanced Research
Projects Agency (DARPA) under GALE Con-
tracts No. HR0011-06-C-0022 and No. HR0011-
06-C-0023, and the Hong Kong Research
Grants Council (RGC) under research grants
GRF621008, DAG03/04.EG09, RGC6256/00E,
and RGC6083/99E. Any opinions, findings and
conclusions or recommendations expressed in
this material are those of the authors and do not
necessarily reflect the views ofthe Defense Ad-
vanced Research Projects Agency. The computa-
tions were performed on UPPMAX resources un-
der project p2007020.
References
Aho, Alfred V. Ullman, Jeffrey D. 1972. The Theory
of Parsing, Translation, and Compiling. Prentice-
Halll, Inc., Upper Saddle River, NJ.
Brown, Peter F., Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
Cocke, John. 1969. Programming languages and
their compilers: Preliminary notes. Courant Insti-
17
tute of Mathematical Sciences, New York Univer-
sity.
Doddington, George. 2002. Automatic eval-
uation of machine translation quality using n-
gram co-occurrence statistics. In Proceedings of
Human Language Technology conference (HLT-
2002), San Diego, California.
Earley, Jay. 1970. An efficient context-free parsing
algorithm. Communications of the Association for
Comuter Machinery, 13(2):94?102.
Haghighi, Aria, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with super-
vised ITG models. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Nat-
ural Language Processing of the AFNLP, pages
923?931, Suntec, Singapore, August.
Huang, Liang, Hao Zhang, Daniel Gildea, and Kevin
Knight. 2009. Binarization of synchronous
context-free grammars. Computational Linguis-
tics, 35(4):559?595.
Kasami, Tadao. 1965. An efficient recognition
and syntax analysis algorithm for context-free lan-
guages. Technical Report AFCRL-65-00143, Air
Force Cambridge Research Laboratory.
Koehn, Philipp, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical ma-
chine translation. In Proceedings of the 45th
Annual Meeting of the Association for Computa-
tional Linguistics Companion Volume Proceedings
of the Demo and Poster Sessions, pages 177?180,
Prague, Czech Republic, June.
Koehn, Philipp. 2005. Europarl: A parallel cor-
pus for statistical machine translation. In Machine
Translation Summit X, Phuket, Thailand, Septem-
ber.
Lewis, Philip M. and Richard E. Stearns. 1968.
Syntax-directed transduction. Journal of the Asso-
ciation for Computing Machinery, 15(3):465?488.
Och, Franz Josef and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Och, Franz Josef. 2003. Minimum error rate training
in statistical machine translation. In 41st Annual
Meeting of the Association for Computational Lin-
guistics, pages 160?167, Sapporo, Japan, July.
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: a method for auto-
matic evaluation of machine translation. In Pro-
ceedings of 40th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 311?
318, Philadelphia, Pennsylvania, July.
Saers, Markus and Dekai Wu. 2009. Improving
phrase-based translation via word alignments from
Stochastic Inversion Transduction Grammars. In
Proceedings of the Third Workshop on Syntax
and Structure in Statistical Translation (SSST-3)
at NAACL HLT 2009, pages 28?36, Boulder, Col-
orado, June.
Saers, Markus, Joakim Nivre, and Dekai Wu. 2009.
Learning Stochastic Bracketing Inversion Trans-
duction Grammars with a cubic time biparsing al-
gorithm. In Proceedings of the 11th International
Conference on Parsing Technologies (IWPT?09),
pages 29?32, Paris, France, October.
Saers, Markus, Joakim Nivre, and Dekai Wu. 2010.
Word alignment with Stochastic Bracketing Linear
Inversion Transduction Grammar. In Proceedings
of Human Language Technologies: The 11th An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics,
Los Angeles, California, June.
Stolcke, Andreas. 2002. SRILM ? an extensible
language modeling toolkit. In International Con-
ference on Spoken Language Processing, Denver,
Colorado, September.
Vogel, Stephan, Hermann Ney, and Christoph Till-
mann. 1996. Hmm-based word alignment in sta-
tistical translation. In Proceedings of the 16th con-
ference on Computational linguistics, pages 836?
841, Morristown, New Jersey.
Wu, Dekai. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377?403.
Younger, Daniel H. 1967. Recognition and parsing
of context-free languages in time n3. Information
and Control, 10(2):189?208.
Zhang, Hao, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing.
In Proceedings of ACL-08: HLT, pages 97?105,
Columbus, Ohio, June.
18
Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 107?115,
Portland, Oregon, June 2011. c?2011 Association for Computational Linguistics
A Survival Analysis of Fixation Times in Reading
Mattias Nilsson
Department of Linguistics and Philology
Uppsala University
mattias.nilsson@lingfil.uu.se
Joakim Nivre
Department of Linguistics and Philology
Uppsala University
joakim.nivre@lingfil.uu.se
Abstract
Survival analysis is often used in medical and
biological studies to examine the time until
some specified event occurs, such as the time
until death of terminally ill patients. In this
paper, however, we apply survival analysis to
eye movement data in order to model the sur-
vival function of fixation time distributions in
reading. Semiparametric regression modeling
and novel evaluation methods for probabilis-
tic models of eye movements are presented.
Survival models adjusting for the influence of
linguistic and cognitive effects are shown to
reduce prediction error within a critical time
period, roughly between 150 and 250 ms fol-
lowing fixation onset.
1 Introduction
During reading, the eyes move on average four times
per second with substantial variation in individual
fixation times, reflecting, at least in part, momentary
changes in on-line language processing demands.
In psycholinguistics, it is commonly assumed that
derivative measures of fixation times, such as first
fixation duration and gaze duration, reflect cognitive
processes during reading. It is less clear, however,
how the distribution of individual fixation times in
reading is affected by on-line processing activities.
In eye movement oriented research, models that at-
tempt to model the distribution of individual fix-
ation times in reading typically assume that sac-
cadic movements are executed relatively randomly
in time, with cognition only occasionally influenc-
ing the timing of saccades (Feng, 2006; McConkie
et al, 1994; Yang and McConkie, 2001; Yang,
2006). In the model by Yang and McConkie (2001),
for example, it is assumed that cognitive control
can have a direct influence over the timing of sac-
cades only with very long fixations, after the normal
saccade has been canceled due to processing diffi-
culty. Distributional models have often made use
of the hazard function in order to analyze fixation
times in reading (Feng, 2006; Feng, 2009; Yang and
McConkie, 2001). The hazard function, in general
terms, is a function of time representing the instan-
taneous risk that an event (e.g., a saccade) will occur
at some specified time t given that it has not occurred
prior to time t.
In this paper, we model the distribution of fixa-
tion times in terms of a different but related quan-
tity, namely the survival function, which defines the
probability of being alive, i.e., the probability of the
event not having occurred, at some specified time
t. We use semiparametric regression for modeling
the influence of linguistic and cognitive effects on
the survival function, and we assess the results us-
ing survival-based time-dependent evaluation met-
rics. More specifically, our objectives are as follows.
We first estimate the survival functions for ten differ-
ent readers using the Kaplan-Meier method (Kaplan
and Meier, 1958) in order to establish the general
shape of the survival function for reading time data.
Then, we estimate adjusted survival functions using
Cox proportional hazards model (Cox, 1972) in or-
der to examine the influence of stimulus variables
on survival. Finally, we assess the adjusted survival
models both with respect to the estimated effects of
covariates and with respect to the predictive perfor-
107
mance on held out data. The experiments we report
in this paper are based on first fixation data (multi-
ple refixations discarded) from the English section
of the Dundee Corpus of eye movements in reading
(Kennedy and Pynte, 2005).
The remainder of this paper is organized as fol-
lows. Section 2 introduces survival analysis and
further motivates its use for modeling fixation du-
rations in reading. Section 3 introduces and applies
the Kaplan-Meier estimate, to compare the survival
functions for the different readers in the corpus.
Section 4 introduces the Cox proportional hazards
model and section 5 outlines two methods for assess-
ing the performance of survival models on new data.
Section 6 presents the experimental evaluation of us-
ing Cox proportional hazards to model the survival
function and summarize and discuss the results. Sec-
tion 7, finally, concludes this paper.
2 Background
Survival analysis is the study and modeling of the
time it takes for events to occur. Because methods
for survival analysis originally were developed for
studying the lifetime distributions of humans in an
epidemiological context, the prototypical event in
these studies is death and the primary variable of in-
terest thus time until death occurs. The use of sur-
vival analysis, however, reaches beyond the clinical
and medical sciences and survival methods apply to
any study with a naturally identifiable starting point
and a well-defined event of interest as end point. In
non-medical contexts, survival analysis often goes
by other names, such as failure time analysis or re-
liability analysis in engineering applications, event
history analysis in sociology, or simply duration
analysis in yet other contexts.
A defining characteristic of survival analysis is the
ability to deal with censoring in a principled manner.
Censoring is said to occur when only partial infor-
mation about the survival time of an individual (hu-
man or other) is available. The most common type
of censoring is referred to as right-censoring, which
occurs when an individual is not subject to the event
of interest during the course of the observation pe-
riod. In this case, it is only known that the individual
did not experience the event prior to the end of the
study, but may perhaps do so at a later point in time
and this piece of partial information about the cen-
sored survival time is included in the analysis.
There are, however, potentially good reasons for
using survival analysis even in time-to-event studies
that do not necessarily involve censored data, such
as when measuring the brief periods of time elaps-
ing between a stimulus appearance and a button-
press in response-time studies, or when measuring
the time between one saccade and the next during
reading using eye-tracking. Such data is usually not
normally distributed and even in the absence of cen-
soring one may take advantage of the fact that sur-
vival data is almost never assumed to be normally
distributed and the methods of survival analysis are
designed to reflect this. Furthermore, if the correct
parametric model for the data is not known, or one is
not confident enough that a given parametric model
is appropriate, the Cox proportional hazards model
provides a robust1 and widely used semiparametric
regression method for time-to-event data. With re-
spect to eye movement data, the Cox model appears
appealing because, as pointed out by Feng (2006,
2009), several different types of distributions have
been proposed as models of fixation times in reading
at one time or another, suggesting there is indeed lit-
tle agreement with respect to the correct parametric
model.
2.1 Survival and Hazard
Survival data is commonly analyzed and modeled in
terms of the survival and the hazard function.
The survival function describes the probabilistic
relationship between survival and time. Let T be
a random variable denoting an individuals? survival
time (T ? 0). The survival function, S(t), defines
the probability that the individual survives longer
than some specified time t:
S(t) = P (T > t) (1)
The survival function is a monotonically decreas-
ing function heading downward as t increases and
has the following theoretical properties: S(0) = 1,
the probability of surviving past time 0 is 1; and
S(?) = 0, eventually nobody survives and S(t)
1Cox proportional hazards model is ?robust? in the sense
that the results will be reasonably close to those obtained using
the correct parametric model.
108
falls to zero as t tends to infinity. Notice also that
if F (t) is the cumulative distribution function for T ,
the survival function, S(t), is 1? F (t).
In the present study, we let the event of interest
be the occurrence of a saccade following a fixation
period, and the most reasonable starting point for
our measurements, at least in practice, appears to be
the beginning, or the onset, of the fixation period.
We will refer to the period onset-to-saccade inter-
changeably as the fixation time or the survival time.
Thus, in this context, the survival function S(t) sim-
ply expresses the probability that a given fixation
lasts, or survives, longer than some specified time
t.
The hazard function, h(t), gives the instantaneous
potential, per unit time, for an event to occur in some
small time interval after t, given survival up to time
t:
h(t) = lim
?t?0
P (t ? T < t+ ?t | T ? t)
?t
(2)
The conditional probability in the formula for the
hazard function expresses the probability that the
survival time, T , will lie in the time interval between
t and t + ?t, given that the survival time is greater
than or equal to t, where ?t denotes an infinitesi-
mally small interval of time. As already suggested,
in this study the hazard function represents the in-
stantaneous risk, or hazard, of a saccade occurring
following a fixation at some specified time t, given
that it has not yet occurred.
3 Kaplan-Meier Survival Estimate
The survival function for time-to-event data can be
estimated from a sample of survival times, both cen-
sored and uncensored, using the Kaplan-Meier (aka
Product-Limit) method. This is a non-parametric
estimate of the survival function which orders the
survival times, from the shortest to the longest, and
adjusts, for each of the event times, the number of
cases still alive according to the number of cases
that were either subject to the event or censored in
the previous time period.
Let dj be the number of saccades that occur at
time tj , and let nj be the number of fixations for
which no saccade has yet occurred at time tj . The
Kaplan-Meier estimate of the survival function S(t)
is then given by:
S?(t) =
?
t(j)?t
(1?
dj
nj
) (3)
In the absence of censored observations, the Kaplan-
Meier estimate is equivalent to the empirical dis-
tribution, and the cumulative survival probability at
time tj reduces to the number of surviving fixations
at time tj divided by the total number of fixations in
the sample. The value of S?(t) is constant between
event times and the estimated function is therefore a
step function that changes value only at times when
one or more saccades occur.
3.1 Kaplan-Meier Survival of Reading Data
Feng (2009) estimated the hazard function for the
distribution of fixation times for the readers of the
Dundee corpus. Here, we give a complementary
account by estimating the corresponding survival
function for these readers using the Kaplan-Meier
method. Figure 1 shows the survival functions for
each reader plotted against time. Individual differ-
ences in the survival function emerge soon after 50
ms and at 100 ms we can spot different tendencies
with respect to how fast or slow the curves decline.
Overall, however, the behavior of the survival func-
tion appears similar across readers. Typically, the
survival function begins with a slow decline up until
about 150 ms and is then followed by a very rapid
decline during the next 100 ms. Thus, we can see
in figure 1 that the overall survival rates drop from
about 80% to 20% in the time interval 150-250 ms.
Thereafter, the function flattens again and at about
400 ms it appears to be converging between the read-
ers. It is worth noting, however, that the reliability
of the estimate decreases with time since the number
of surviving fixations becomes fewer and fewer.
Median survival time is the point in time when
50% of the total number of fixations has been termi-
nated by a saccade. It is thus read off the plot as the
time where the probability of survival is 0.5. Median
survival time ranges from 168 ms (reader g) to 220
ms (reader b). Mean median survival time across all
ten readers is 191.4 ms with a standard deviation of
14.9 ms.
109
0.0
0.2
0.4
0.6
0.8
1.0
Time (ms) 
Pro
bab
ility
 of 
sur
viv
al
sa   
sb   
sc   
sd   
se   
sf   
sg   
sh   
si   
sj   
0 50 100 150 200 250 300 350 400
Figure 1: Kaplan-Meier curves for fixation durations showing the cumulative survival probability, following fixation
onset, grouped by individual reader (subject a-j).
4 Cox Proportional Hazards Model
This section introduces the Cox proportional haz-
ards model. We will later apply this model in the
experimental part of the paper to obtain adjusted es-
timates of the survival function for the readers in the
Dundee corpus.
The Cox proportional hazards model is a semi-
parametric regression model for survival data relat-
ing survival time to one or more predictors or co-
variates. More precisely, the Cox model regresses
the hazard function on a set of predictors, providing
estimates of their effects in terms of hazard ratios.
The Cox proportional hazards model has the follow-
ing form:
h(t) = h0(t) exp{?1x1 + ?2x2 + . . .+ ?nxn} (4)
where h0(t) is the baseline hazard function at time t,
x1 . . . xn are the set of covariates or predictor vari-
ables, and ?1 . . . ?n are the corresponding coeffi-
cients to be estimated2. Thus, this model gives an
expression for the hazard at time t for a particular
individual with a given set of covariates.
The baseline hazard, h0(t), represents the value
of the hazard function when all covariates are equal
to zero, and in the Cox model this baseline haz-
ard is left unspecified and varies as a function of
time. Since no assumptions are made with respect
2Parameter estimates in the Cox model are obtained by max-
imizing the ?partial? likelihood, as opposed to the (full) likeli-
hood. Details of procedures for parameter estimation can be
found, for example, in Kalbfleisch and Prentice (1980).
to the form or distribution of the baseline hazard,
this can be regarded as the nonparametric part of the
Cox proportional hazards model. However, the Cox
model assumes a parametric form with respect to the
effect of the predictors on the hazard. In particu-
lar, as seen in equation 4, the predictors are assumed
to multiply hazard at any point in time. This is an
important assumption of the Cox model referred to
as the assumption of proportional hazards. It means
that the hazard functions for any two individuals at
any point in time should be proportional. In other
words, if a certain individual has a risk of the event at
some initial point in time that is twice as high as that
of another individual, then, under the proportional
hazards assumption the risk remains twice as high
also at all later times. There are a variety of different
graphical and goodness-of-fit based procedures that
can be used to evaluate the proportional hazards as-
sumption for survival data (see Kleinbaum and Klein
(2005) for an overview.).
The parameter estimates in a fitted Cox model are
commonly interpreted in terms of their hazard ra-
tios. If bi is the value of the coefficient for predictor
xi, the exponentiated coefficient, ebi , gives the esti-
mated hazard ratio for xi. For continuous variables,
the hazard ratio refers to the risk change associated
with one unit increase in xi, controlling for the effect
of the other variables. A hazard ratio above one indi-
cates a raised risk of the event occurring and the pre-
dictor is in this case thus negatively associated with
survival. Correspondingly, a value below one indi-
110
cates a decreased risk and the predictor is thus posi-
tively associated with survival. Lastly, if the hazard
ratio is equal to one, there is no indication of any
associated risk change.
5 Assessment of Survival Models
Accurate prognoses are of critical importance in
many areas where survival analysis apply, for in-
stance in medical contexts where doctors have to es-
timate the expected remaining life time for termi-
nally ill patients. Survival models are thus often as-
sessed with respect to their predictive performance
on novel data, in addition to the statistical signifi-
cance of model covariates. We now briefly review
two of the most commonly used measures for as-
sessing the quality of survival models on indepen-
dent data sets.
5.1 Prediction Error Curves
The prediction error for survival data is defined as a
function of time and can be measured by the Brier
score (Brier, 1950). Intuitively, if an individual is
alive at time t, the predicted survival probability
should be close to 1, and otherwise close to 0. The
prediction error, or Brier score, at time point t is
defined as the mean squared error between the ob-
served survival status Yi(t) for the individual i at
time t, which is equal to 1 if the individual is alive at
t, and 0 otherwise, and the predicted survival proba-
bility for i at time t:
B?S(t) =
1
n
n?
i=1
{Yi(t)? Si(t)}
2 (5)
The lower the Brier score, the lower the predic-
tion error. Various benchmark values for the Brier
score at time t exists. The values 0.25 and 0.33,
for example, correspond to a constant predicted sur-
vival probability of 50% and to a randomly pre-
dicted value between 0 and 1, respectively. Often,
however, the Kaplan-Meier estimate of the survival
function over the training sample is used. In this
case, the benchmark prediction at time point t cor-
responds to the proportion of individuals surviving
past t, thus ignoring all available covariate informa-
tion. By tracking the prediction error over time we
get the prediction error curve (Graf et al, 1999) and
a summary measure of the error for the whole ob-
servation period can be obtained by integrating over
time (the integrated Brier score).
5.2 Concordance Index
The concordance index (Harrell et al, 1982), or C-
index, estimates the probability that a given predic-
tion agrees, or concurs, with the observed outcome.
For uncensored data, the concordance index is given
by the relative frequency of concordant pairs among
all pairs of individuals. A pair is said to be concor-
dant if the individual with the shorter survival time
is also predicted by the model to have the highest
risk of the two. Useful reference values for the con-
cordance index are 0.5 which indicates that the pre-
dictions are no better than chance, and 1 which indi-
cates that the model discriminates the pairs perfectly.
6 Experimental Evaluation
In order to study the influence of cognitive and lin-
guistic effects on the survival function, the following
experiment is performed. First, the Cox proportional
hazards model is used to regress fixation times on
five different stimulus variables associated with the
current fixation, thus providing estimates of the haz-
ard ratios for the effects of each variable adjusted for
the other variables in the model. Second, we obtain
adjusted survival functions, i.e. survival curves that
adjust for the stimulus variables used as covariates,
and we assess these curves with respect to the gen-
eralization error on held-out corpus data.
It is worth pointing out that regression studies on
the Dundee Corpus of eye movements have been
carried out before (e.g., Demberg and Keller, 2008;
Pynte and Kennedy, 2006). Our experiment, how-
ever, differs from previous studies in at least three
ways: (1) our goal is to model the survival func-
tion of fixation time distributions in reading, which
means that we use the survival time of individual fix-
ations as the unit of analysis; (2) we assess the sur-
vival model not only with respect to the estimated
regression coefficients, but also with respect to the
models? predictive performance on unseen data; (3)
we use a semiparametric regression method for sur-
vival data which has not been previously applied,
as far as we know, to reading-time data. It is also
worth pointing out that although we believe that a
111
Table 1: Results of Cox proportional hazards model of fixation times in the Dundee Corpus section 01-16: hazard
ratios (HR) and significance levels (p) for all covariates in the model, and for each individual model of reader a-j.
a b c d e f g h i j
Variable HR p HR p HR p HR p HR p HR p HR p HR p HR p HR p
Word length 1.015 < .001 0.983 < .001 0.979 < .001 0.988 < .001 0.992 < .05 0.992 < .01 0.992 < .01 0.985 < .001 0.990 < .01 0.987 < .001
Word frequency 1.055 < .001 1.042 < .001 1.036 < .001 1.051 < .001 1.051 < .001 1.014 < .001 1.031 < .001 1.028 < .001 1.040 < .001 1.044 < .001
Bigram probability 1.108 < .001 1.196 < .1 1.092 < .05 1.006 < .01 1.013 < .05 1.014 < .001 0.953 1.011 < .001 1.003 1.005 < .05
Surprisal 1.001 0.986 < .001 0.994 < .01 0.984 < .001 0.998 < .01 0.991 < .05 1.002 0.994 0.993 < .05 0.996 < .01
Entropy 0.966 < .001 0.986 < .01 0.980 < .001 0.988 < .01 0.963 < .001 1.002 0.990 < .05 0.992 < .05 0.969 < .001 0.978 < .001
careful comparison of the results obtained using sur-
vival analysis to those reported for other regression
methods would be useful and interesting, it is never-
theless beyond the scope of this paper.
Most of the stimulus variables included in the
analysis have been shown to correlate with reading
times in other regression studies: the number of let-
ters in the word, the logarithm of the word?s rela-
tive frequency (based on occurrences in the British
National Corpus), the logarithm of the conditional
(bigram) probability of the word (based on occur-
rences in the Google Web 1T 5-gram corpus (Brants
and Franz, 2006)), the syntactic surprisal and en-
tropy scores3 (computed here using the probabilis-
tic PCFG parser by Roark et al (2009)). The sur-
prisal (Hale, 2001) at word wi refers to the nega-
tive log probability of wi given the preceding words,
computed using the prefix probabilities of the parser.
A number of studies have previously established a
positive relation between surprisal and word-reading
times (Boston et al, 2008; Demberg and Keller,
2008; Roark et al, 2009). The entropy, as quantified
here, approximates the structural uncertainty associ-
ated with the rest of the sentence, or what is yet to
be computed (Roark et al, 2009).
In this experiment, we use the first 16 texts in
the Dundee corpus for parameter estimation, and the
following two texts, 17 and 18 for model assessment
of the generalization error. To avoid introducing bi-
ases that may result from pooling distributional data
together, we model each of the readers in the cor-
pus separately. Prior to running the experiments,
we also validated the Cox proportional hazards as-
sumption using a goodness-of-fit approach based on
the Schoenfeld residuals (Schoenfeld, 1982). The
outcome of this test indicated a slight violation of
3To ease interpretation of the estimated hazard ratios, no in-
teraction terms were included in this model.
the proportional hazards assumption. However, it
is well-known that a slight violation may occur for
large data samples, given that p-values can be driven
by sample size (Kleinbaum and Klein, 2005).
6.1 Results
6.1.1 Hazard Ratios
Table 1 shows the results of the Cox proportional
hazards regression models. The estimated hazard ra-
tio for each covariate along with the corresponding
significance level is reported for each reader. Recall
that a hazard ratio above one indicates a worse sur-
vival prognosis, i.e. shorter fixation times, while a
hazard ratio below one indicates better survival, i.e.
longer fixation times.
Overall, the effects go in the directions expected
for these variables based on previous research.
There is a significant positive effect of word length
on survival for all but one reader. The hazard ra-
tio for the significant effects ranges between 0.979
and 0.992. Word length thus decreases the hazard by
about 1-2% for each additional letter in a word when
adjusting for the effects of the other covariates in
the model. Word frequency is significantly and neg-
atively related to survival across all readers. More
frequent words have shorter survival times. The av-
erage hazard ratio among the readers is 1.0392 and
the estimated risk of a saccade increases thus on av-
erage by a factor of 1.0392 for each unit increase
in log word frequency. Bigram probability is nega-
tively and significantly related to survival for eight
readers with an average hazard ratio of 1.0569. Sur-
prisal is significantly and positively related to sur-
vival for seven readers. Among these, the hazard
decreases by 1% for each unit increase in surprisal.
Entropy has a significant and positive effect on sur-
vival on all but one readers. The hazard ratios range
between 0.963 and 0.992, corresponding to a de-
112
Brier score t
t.100 t.150 t.200 t.250 t.300
Model Cox KM Cox KM Cox KM Cox KM Cox KM
a 0.05 0.05 0.14 0.15 0.24 0.25 0.14 0.15 0.05 0.06
b 0.05 0.05 0.12 0.13 0.23 0.25 0.21 0.23 0.12 0.13
c 0.13 0.13 0.23 0.24 0.17 0.18 0.06 0.07 0.02 0.02
d 0.07 0.07 0.17 0.18 0.23 0.25 0.15 0.16 0.06 0.06
e 0.05 0.05 0.15 0.15 0.23 0.25 0.14 0.15 0.05 0.05
f 0.09 0.09 0.21 0.21 0.22 0.23 0.12 0.12 0.06 0.06
g 0.16 0.16 0.23 0.23 0.24 0.25 0.12 0.13 0.07 0.07
h 0.07 0.07 0.15 0.15 0.24 0.25 0.20 0.20 0.12 0.12
i 0.04 0.04 0.13 0.13 0.23 0.25 0.10 0.10 0.03 0.03
j 0.06 0.06 0.18 0.19 0.23 0.25 0.12 0.12 0.05 0.05
Avg. 0.077 0.077 0.171 0.176 0.226 0.241 0.136 0.143 0.063 0.065
Table 2: Prediction error on held-out data between the observed survival status and the predicted survival probability
at different times t, for Kaplan-Meier and Cox-model adjusted survival, and for all models of readers a-j.
creased risk by 1-4% per additional unit increase,
after adjusting for the effects of the other predic-
tors. While Frank (2010) recently showed that sen-
tence entropy, i.e. non-structural entropy, accounts
for a significant fraction of the variance in reading
times, our results provide additional support for the
influence of structural sentence entropy on reading
times. Moreover, it is noteworthy that the effect of
entropy appears reliably robust in individual first fix-
ation times, suggesting that the effects of structural
processing demands can be immediate rather than
delayed in the eye movement record.
6.1.2 Adjusted Survival
We summarize the results of the evaluation of the
adjusted survival function on held-out data in table
2 and in table 3. Table 2 shows the Brier score com-
puted at different points in time in the interval 100
to 300 ms. Results are reported both for the Kaplan-
Meier estimate of the survival function and for the
fitted Cox-models. We present the results for each
individual model. The bottom row gives the results
obtained when averaging over all models at the spec-
ified time t.
Recall that the Brier score, or prediction error,
at any specified time t, is computed over all fixa-
tions in the held-out set and gives the average of the
squared distances between the actual survival status
and the predicted survival probability at time t. Al-
though the differences between the Cox-model and
the Kaplan-Meier estimate are small overall, there
are two subtle but notable results. First, the ad-
justed survival model is never underperforming the
Kaplan-Meier survival estimate. The prediction er-
ror of the Cox model is consistently lower or equal to
the Kaplan-Meier prediction error at each time point
and for each reader. Second, in comparison to the
Kaplan-Meier error, the prediction error of the ad-
justed model is systematically lower in the time win-
dow 150-250 ms, but essentially the same prior to,
and after, this time period. This is readily reflected
in the average scores, for example. One interpre-
tation of these small but systematic differences sug-
gests that there is a limited period, approximately no
earlier than 150 ms. and no later than 250 ms. on av-
erage, during which the covariates in the model are
primarily influencing the survival time. Before and
after this period, the stimulus variables of the fixated
word appear to have little or no influence on the time
when saccades are generated. In other words, we ob-
serve an improved agreement to the observed data in
the interval 150-250 ms. under the assumption that
each individual fixation has an independent survival
function whose value at time t is influenced by the
specific values for the stimulus variables of the fix-
ation. Recall that the benchmark, the Kaplan-Meier
estimate, in contrast, assumes one and the same un-
derlying survival function for all fixations, ignoring
all available covariate information. By plotting the
113
Time
Pred
iction
 error
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0 100 200 300 400 500 600
Kaplan.MeierCox
Figure 2: Prediction error curves on held-out data between the observed survival status and the predicted survival
probability, for Kaplan-Meier and Cox-model adjusted survival, for the model of reader d.
Model IBSC C-index
Kaplan-Meier 0.041 0.582
Cox 0.043 0.598
Table 3: Integrated Brier score (IBSC) and Concordance
index (C-index) on held-out data, for Kaplan-Meier and
Cox-model adjusted survival, averaged over the results
obtained for each model of reader a-j.
time-dependent prediction error, subtle differences
in survival over the time course are more easily spot-
ted. Figure 2 shows, as an example, the prediction
error curve for one of the models.
Table 3 gives the integrated brier score, i.e., the
prediction error obtained when integrating over all
event times, and the concordance index C, for
both the Kaplan-Meier estimate and the Cox model.
These results are averaged over the results of the in-
dividual models. The integrated Brier score verifies
that the Cox model fares somewhat better, although
the impact of the model variables appears limited in
time. The C-value for both the Kaplan-Meier and
the Cox model is significantly better than chance
(0.5). A C-value of 0.6 - 0.7 is a common result
for survival data.
7 Conclusion
In this paper we applied survival analysis to model
fixation times in reading. In particular, we modeled
the survival function of fixation time distributions
using the Kaplan-Meier estimate, and the Cox pro-
portional hazards model to adjust for cognitive and
linguistic effects on survival. The adjusted survival
models were assessed with respect to the effect of
covariates on hazard rates, and with respect to their
predictive performance using evaluation metrics that
are novel in the context of eye-movement and psy-
cholinguistic modeling.
The results of the analysis suggests that: (1) struc-
tural sentence entropy influences survival, i.e., in-
creasing structural uncertainty about the rest of the
sentence decreases the risk of moving the eyes; (2)
stimulus variables associated with the current fixa-
tion influence the survival of the fixation in a limited
time frame, roughly between 150 and 250 ms fol-
lowing onset; and (3) linguistic and cognitive effects
may influence the timing of saccades earlier than is
sometimes assumed.
Looking ahead, important topics to be inves-
tigated in the future include frailty models and
competing risks survival analysis. Frailty models
are survival-based regression models with random
effects, designed to account for variance due to
individual-level factors otherwise unaccounted for.
114
Competing risks survival analysis apply to situations
where a finite number of different types of events are
possible, but only one of the events can actually oc-
cur per individual, e.g., dying from either lung can-
cer or stroke. In the current study we did not dif-
ferentiate between different types of events follow-
ing a fixation. A competing risks analysis, however,
would let us differentiate between different types of
saccades and study the influence of predictors on the
survival function based on the type of the saccade
following a fixation, e.g., whether it is a forward-
directed saccade, refixation or regression. These and
other issues will be addressed.
References
Marisa F. Boston, John Hale, Reinhold Kliegl, Umesh
Patil, and Shravan Vasishth. 2008. Parsing costs as
predictors of reading difficulty: An evaluation using
the potsdam sentence corpus. Journal of Eye Move-
ment Reasearch, 2:1?12.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Version 1. Linguistic Data Consortium.
Glenn W. Brier. 1950. Verification of forecasts ex-
pressed in terms of probability. Monthly Weather Re-
view, 78:1?3.
David R. Cox. 1972. Regression models and life-
tables. Journal of the Royal Statistical Society. Series
B (Methodological), 34:187?220.
Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109:193?210.
Gary Feng. 2006. Eye movements as time-series random
variables: A stochastic model of eye movement con-
trol in reading. Cognitive Systems Research, 7:70?95.
Gary Feng. 2009. Time course and hazard function: A
distributional analysis of fixation duration in reading.
Journal of Eye Movement Research, 3:1?23.
Stefan L. Frank. 2010. Uncertainty reduction as a mea-
sure of cognitive processing effort. In Proceedings of
the ACL Workshop on Cognitive Modeling and Com-
putational Linguistics.
Erika Graf, Schmoor Claudia, Sauerbrei Will, and Schu-
macher Martin. 1999. Assessment and comparison
of prognostic classification schemes for survival data.
Statistics in Medicine, 18:2529?2545.
John Hale. 2001. A probabilistic early parser as a
psycholinguistic model. In Proceedings of the sec-
ond conference of the North American chapter of the
Association for Computational Linguistics, volume 2,
pages 159?166.
Frank E. Jr Harrell, Robert M. Califf, David B. Pryor,
Kerry L. Lee, and Rober A. Rosati. 1982. Evaluating
the yield of medical tests. Journal of the American
Medical Association, 247:2543?2546.
John D. Kalbfleisch and Ross L. Prentice. 1980. The
statistical analysis of failure time data. Wiley.
Edward L. Kaplan and Paul Meier. 1958. Nonparametric
estimation from incomplete observations. Journal of
the American statistical association, 53:457?481.
Alan Kennedy and Joel Pynte. 2005. Parafoveal-on-
foveal effects in normal reading. Vision Research,
45:153?168.
David G. Kleinbaum and Mitchell. Klein. 2005. Survival
analysis: A self-learning text. Springer.
George W. McConkie, Paul W. Kerr, and Brian P. Dyre.
1994. What are normal eye movements during read-
ing: Toward a mathematical description. In J. Ygge
and G. Lennerstrand (Eds.), editors, Eye movements
in reading: Perceptual and language processes, pages
315?327. Oxford: Elsevier.
Joel Pynte and Allan Kennedy. 2006. An influence over
eye movements in reading exerted from beyond the
level of the word: Evidence from reading english and
french. Vision Research, 46:3786?3801.
Brian Roark, Asaf Bachrach, Carlos Cardenas, and
Christophe. Pallier. 2009. Deriving lexical and syn-
tactic expectation-based measures for psycholinguistic
modeling via incremental top-down parsing. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 324?
333.
David Schoenfeld. 1982. Partial residuals for the propor-
tional hazards model. Biometrika, 69:51?55.
Shun-nan Yang and George W. McConkie. 2001. Eye
movements during reading: a theory of saccade initia-
tion times. Vision Research, 41:3567?3585.
Shun-nan Yang. 2006. A oculomotor-based model of eye
movements in reading: The competition/interaction
model. Cognitive Systems Research, 7:56?69.
115
Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 87?95,
Portland, OR, USA, 24 June 2011. c?2011 Association for Computational Linguistics
Automatic Verb Extraction from Historical Swedish Texts
Eva Pettersson
Department of Linguistics and Philology
Uppsala University
Swedish National Graduate School
of Language Technology
eva.pettersson@lingfil.uu.se
Joakim Nivre
Department of Linguistics and Philology
Uppsala University
joakim.nivre@lingfil.uu.se
Abstract
Even though historical texts reveal a lot of
interesting information on culture and social
structure in the past, information access is lim-
ited and in most cases the only way to find the
information you are looking for is to manually
go through large volumes of text, searching
for interesting text segments. In this paper we
will explore the idea of facilitating this time-
consuming manual effort, using existing natu-
ral language processing techniques. Attention
is focused on automatically identifying verbs
in early modern Swedish texts (1550?1800).
The results indicate that it is possible to iden-
tify linguistic categories such as verbs in texts
from this period with a high level of precision
and recall, using morphological tools devel-
oped for present-day Swedish, if the text is
normalised into a more modern spelling be-
fore the morphological tools are applied.
1 Introduction
Historical texts constitute a rich source of data for
researchers interested in for example culture and so-
cial structure over time. It is however a very time-
consuming task to manually search for relevant pas-
sages in the texts available. It is likely that language
technology could substantially reduce the manual
effort involved and thus the time needed to access
this information, by automatically suggesting sec-
tions that may be of interest to the task at hand. The
interesting text segments could be identified using
for example semantic features or morphological and
syntactic cues in the text.
This would however require natural language pro-
cessing tools capable of handling historical texts,
which are in many respects different from contem-
porary written language, concerning both spelling
and syntax. Ideally, one would of course like to have
tools developed specifically for the time period of in-
terest, and emerging efforts to develop resources and
tools for historical languages are therefore welcome.
Despite these efforts, however, it is unlikely that we
will have anything close to complete coverage of dif-
ferent time periods even for a single language within
the foreseeable future.
In this paper, we will therefore instead exam-
ine the possibility of improving information access
in historical texts by adapting language technology
tools developed for contemporary written language.
The work has been carried out in close cooperation
with historians who are interested in what men and
women did for a living in the early modern Swedish
society (1550?1800). We will hence focus on identi-
fying linguistic categories in Swedish texts from this
period. The encouraging results show that you may
successfully analyse historical texts using NLP tools
developed for contemporary language, if analysis is
preceded by an orthographic normalisation step.
Section 2 presents related work and character-
istics of historical Swedish texts. The extraction
method is defined in section 3. In section 4 the ex-
periments are described, while the results are pre-
sented in section 5. Section 6 describes how the verb
extraction tool is used in ongoing historical research.
Finally, conclusions are drawn in section 7.
87
2 Background
2.1 Related Work
There are still not many studies performed on natu-
ral language processing of historical texts. Pennac-
chiotti and Zanzotto (2008) used contemporary dic-
tionaries and analysis tools to analyse Italian texts
from the period 1200?1881. The results showed that
the dictionary only covered approximately 27% of
the words in the oldest text, as compared to 62.5%
of the words in a contemporary Italian newspaper
text. The morphological analyser used in the study
reached an accuracy of 0.48 (as compared to 0.91
for modern text), while the part-of-speech tagger
yielded an accuracy of 0.54 (as compared to 0.97
for modern text).
Rocio et al (1999) used a grammar of contempo-
rary Portuguese to syntactically annotate medieval
Portuguese texts. To adapt the parser to the me-
dieval language, a lexical analyser was added includ-
ing a dictionary and inflectional rules for medieval
Portuguese. This combination proved to be success-
ful for partial parsing of medieval Portuguese texts,
even though there were some problems with gram-
mar limitations, dictionary incompleteness and in-
sufficient part-of-speech tagging.
Oravecz et al (2010) tried a semi-automatic ap-
proach to create an annotated corpus of texts from
the Old Hungarian period. The annotation was per-
formed in three steps: 1) sentence segmentation
and tokenisation, 2) standardisation/normalisation,
and 3) morphological analysis and disambiguation.
They concluded that normalisation is of vital impor-
tance to the performance of the morphological anal-
yser.
For the Swedish language, Borin et al (2007)
proposed a named-entity recognition system adapted
to Swedish literature from the 19th century. The sys-
tem recognises Person Names, Locations, Organisa-
tions, Artifacts (food/wine products, vehicles etc),
Work&Art (names of novels, sculptures etc), Events
(religious, cultural etc), Measure/Numerical expres-
sions and Temporal expressions. The named en-
tity recognition system was evaluated on texts from
the Swedish Literature Bank without any adaptation,
showing problems with spelling variation, inflec-
tional differences, unknown names and structural is-
sues (such as hyphens splitting a single name into
several entities).1 Normalising the texts before ap-
plying the named entity recognition systemmade the
f-score figures increase from 78.1% to 89.5%.
All the results presented in this section indicate
that existing natural language processing tools are
not applicable to historical texts without adaptation
of the tools, or the source text.
2.2 Characteristics of Historical Swedish Texts
Texts from the early modern Swedish period (1550?
1800) differ from present-day Swedish texts both
concerning orthography and syntax. Inflectional dif-
ferences include a richer verb paradigm in historical
texts as compared to contemporary Swedish. The
Swedish language was also strongly influenced by
other languages. Evidence of this is the placement
of the finite verb at the end of relative clauses in a
German-like fashion not usually found in Swedish
texts, as in ...om man i ha?chtelse sitter as compared
to om man sitter i ha?kte (?...if you in custody are?
vs ?...if you are in custody?).
Examples of the various orthographic differences
are the duplication of long vowels in words such as
saak (sak ?thing?) and stoor (stor ?big/large?), the
use of of fv instead of v, as in o?fver (o?ver ?over?),
and gh and dh instead of the present-day g and d, as
in na?ghon (na?gon ?somebody?) and fadhren (fadern
?the father?) (Bergman, 1995).
Furthermore, the lack of spelling conventions
causes the spelling to vary highly between different
writers and text genres, and even within the same
text. There is also great language variation in texts
from different parts of the period.
3 Verb Extraction
In the following we will focus on identifying verbs
in historical Swedish texts from the period 1550?
1800. The study has been carried out in cooper-
ation with historians who are interested in finding
out what men and women did for a living in the
early modern Swedish society. One way to do this
would be to search for occupational titles occurring
in the text. This is however not sufficient since many
people, especially women, had no occupational ti-
tle. Occupational titles are also vague, and may in-
clude several subcategories of work. In the material
1http://litteraturbanken.se/
88
already (manually) analysed by the historians, oc-
cupation is often described as a verb with a direct
object. Hence, automatically extracting and display-
ing the verbs in a text could help the historians in
the process of finding relevant text segments. The
verb extraction process developed for this purpose
is performed in maximally five steps, as illustrated
in figure 1.
The first step is tokenisation. Each token is
then optionally matched against dictionaries cover-
ing historical Swedish. Words not found in the his-
torical dictionaries are normalised to a more mod-
ern spelling before being processed by the morpho-
logical analyser. Finally, the tagger disambiguates
words with several interpretations, yielding a list of
all the verb candidates in the text. In the experi-
ments, we will examine what steps are essential, and
how they are combined to yield the best results.
3.1 Tokenisation
Tokenisation is performed using an in-house stan-
dard tokeniser. The result of the tokenisation is a
text segmented into one token per line, with a blank
line marking the start of a new sentence.
3.2 Historical Dictionaries
After tokenisation, the tokens are optionally
matched against two historical dictionaries dis-
tributed by The Swedish Language Bank:2
? The Medieval Lexical Database
A dictionary describing Medieval Swedish,
containing approximately 54 000 entries from
the following three books:
? K.F. So?derwalls Ordbok O?fver svenska
medeltids-spra?ket, vol I-III (So?derwall,
1918)
? K.F. So?derwalls Ordbok O?fver svenska
medeltids-spra?ket, vol IV-V (So?derwall,
1973)
? C.J. Schlyters Ordbok till Samlingen af
Sweriges Gamla Lagar (Schlyter, 1877)
? Dalin?s Dictionary
A dictionary covering 19th Century Swedish,
created from the printed version of Ordbok
2http://spraakbanken.gu.se/
O?fver svenska spra?ket, vol I?II by Dalin
(1855). The dictionary contains approximately
64 000 entries.
The dictionaries cover medieval Swedish and 19th
century Swedish respectively. We are actually in-
terested in the time period in between these two pe-
riods, but it is assumed that these dictionaries are
close enough to cover words found in the early mod-
ern period as well. It should further be noticed that
the electronically available versions of the dictionar-
ies are still in an early stage of development. This
means that coverage varies between different word
classes, and verbs are not covered to the same ex-
tent as for example nouns. Words with an irregu-
lar inflection (which is often the case for frequently
occurring verbs) also pose a problem in the current
dictionaries.
3.3 Normalisation Rules
Since both the morphological analyser and the tag-
ger used in the experiments are developed for han-
dling modern Swedish written language, running a
text with the old Swedish spelling preserved pre-
sumably means that these tools will fail to assign
correct analyses in many cases. Therefore, the text is
optionally transformed into a more modern spelling,
before running the document through the analysis
tools.
The normalisation procedure differs slightly for
morphological analysis as compared to tagging.
There are mainly two reasons why the same set
of normalisation rules may not be optimally used
both for the morphological analyser and for the tag-
ger. First, since the tagger (unlike the morphological
analyser) is context sensitive, the normalisation rules
developed for the tagger need to be designed to also
normalise words surrounding verbs, such as nouns,
determiners, etc. For the morphological analyser,
the main focus in formulating the rules has been on
handling verb forms. Secondly, to avoid being lim-
ited to a small set of rules, an incremental normalisa-
tion procedure has been used for the morphological
analyser in order to maximise recall without sacri-
ficing precision. In this incremental process, nor-
malisation rules are applied one by one, and the less
confident rules are only applied to words not iden-
tified by the morphological analyser in the previous
89
Figure 1: Overview of the verb extraction experiment
normalisation step. The tagger on the other hand is
robust, always yielding a tag for each token, even in
cases where the word form is not present in the dic-
tionary. Thus, the idea of running the normalisation
rules in an incremental manner is not an option for
the tagger.
The total set of normalisation rules used for the
morphological analyser is 39 rules, while 29 rules
were defined for the tagger. The rules are inspired
by (but not limited to) some of the changes in
the reformed Swedish spelling introduced in 1906
(Bergman, 1995). As a complement to the rules
based on the spelling reform, a number of empiri-
cally designed rules were formulated, based on the
development corpus described in section 4.1. The
empirical rules include the rewriting of verbal end-
ings (e.g. bega?rade ? bega?rde ?requested? and
utviste ? utvisade ?deported?), transforming dou-
ble consonants into a single consonant (vetta ? veta
?know?, pro?vass ? pro?vas ?be tried?) and vice versa
(upsteg ? uppsteg ?rose/ascended?, viste ? visste
?knew?).
3.4 Morphological Analysis and Tagging
SALDO is an electronically available lexical re-
source developed for present-day written Swedish.
It is based on Svenskt AssociationsLexikon (SAL), a
semantic dictionary compiled by Lo?nngren (1992).
The first version of the SALDO dictionary was re-
leased in 2008 and comprises 72 396 lexemes. In-
flectional information conforms to the definitions
in Nationalencyklopedins ordbok (1995), Svenska
Akademiens ordlista o?ver svenska spra?ket (2006)
and Svenska Akademiens grammatik (1999). Apart
from single word entries, the SALDO dictionary
also contains approximately 2 000 multi-word units,
including 1 100 verbs, mainly particle verbs (Borin
et al, 2008). In the experiments we will use SALDO
version 2.0, released in 2010 with a number of words
added, resulting in a dictionary comprising approxi-
mately 100 000 entries.
When running the SALDO morphological anal-
yser alone, a token is always considered to be a verb
if there is a verb interpretation present in the dictio-
nary, regardless of context. For example, the word
fo?r will always be analysed both as a verb (bring)
and as a preposition (for), even though in most cases
the prepositional interpretation is the correct one.
When running the maximum five steps in the verb
extraction procedure, the tagger will disambiguate
in cases where the morphological analyser has pro-
duced both a verb interpretation and a non-verb in-
terpretation. The tagger used in this study is Hun-
POS (Hala?csy et al, 2007), a free and open source
reimplementation of the HMM-based TnT-tagger
by Brants (2000). Megyesi (2008) showed that
the HunPOS tagger trained on the Stockholm-Umea?
Corpus (Gustafson-Capkova? and Hartmann, 2006)
is one of the best performing taggers for Swedish
texts.
4 Experiments
This section describes the experimental setup in-
cluding data preparation and experiments.
90
4.1 Data Preparation
A subset of Per Larssons dombok, a selection of
court records from 1638, was used as a basis for de-
veloping the automatic verb extraction tool. This
text consists of 11 439 tokens in total, and was
printed by Edling (1937). The initial 984 to-
kens of the text were used as development data,
i.e. words used when formulating the normalisation
rules, whereas the rest of the text was used solely for
evaluation.
A gold standard for evaluation was created, by
manually annotating all the verbs in the text. For
the verb annotation to be as accurate as possible, the
same text was annotated by two persons indepen-
dently, and the results analysed and compared until
consensus was reached. The resulting gold standard
includes 2 093 verbs in total.
4.2 Experiment 1: Normalisation Rules
In the first experiment we will compare morpholog-
ical analysis results before and after applying nor-
malisation rules. To investigate what results could
optimally be expected from the morphological anal-
ysis, SALDO was also run on present-day Swedish
text, i.e. the Stockholm-Umea? Corpus (SUC). SUC
is a balanced corpus consisting of a number of dif-
ferent text types representative of the Swedish lan-
guage in the 1990s. The corpus consists of approx-
imately one million tokens, distributed among 500
texts with approximately 2 000 tokens in each text.
Each word in the corpus is manually annotated with
part of speech, lemma and a number of morpho-
logical features (Gustafson-Capkova? and Hartmann,
2006).
4.3 Experiment 2: Morphological Analysis and
Tagging
In the second experiment we will focus on the
combination of morphological analysis and tagging,
based on the following settings:
morph A token is always considered to be a verb
if the morphological analysis contains a verb
interpretation.
tag A token is always considered to be a verb if it
has been analysed as a verb by the tagger.
morph or tag A token is considered to be a verb if
there is a morphological verb analysis or if it
has been analysed as a verb by the tagger.
morph and tag A token is considered to be a verb
if there is a morphological verb analysis and it
has been tagged as a verb.
To further refine the combination of morphologi-
cal analysis and tagging, a more fine-grained dis-
ambiguation method was introduced, where the tag-
ger is only used in contexts where the morphological
analyser has failed to provide an unambiguous inter-
pretation:
morph + tag A token is considered to be a verb if
it has been unambiguously analysed as a verb
by SALDO. Likewise a token is considered not
to be a verb, if it has been given one or more
analyses from SALDO, where none of the anal-
yses is a verb interpretation. If the token has
been given both a verb analysis and a non-verb
analysis by SALDO, the tagger gets to decide.
The tagger also decides for words not found in
SALDO.
4.4 Experiment 3: Historical Dictionaries
In the third experiment, the historical dictionaries
are added, using the following combinations:
medieval A token is considered to be a verb if it has
been unambiguously analysed as a verb by the
medieval dictionary. Likewise a token is con-
sidered not to be a verb, if it has been given
one or more analyses from the medieval dic-
tionary, where none of the analyses is a verb
interpretation. If the token has been given both
a verb analysis and a non-verb analysis by the
medieval dictionary, or if the token is not found
in the dictionary, the token is processed by the
morphological analyser and the tagger as de-
scribed in setting morph + tag.
19c A token is considered to be a verb if it has been
unambiguously analysed as a verb by the 19th
century dictionary. Likewise a token is consid-
ered not to be a verb, if it has been given one
or more analyses from the 19th century dictio-
nary, where none of the analyses is a verb in-
terpretation. If the token has been given both
91
a verb analysis and a non-verb analysis by the
19th century dictionary, or if the token is not
found in the dictionary, the token is processed
by the morphological analyser and the tagger as
described in setting morph + tag.
medieval + 19c A token is considered to be a verb
if it has been unambiguously analysed as a verb
by the medieval dictionary. Likewise a token is
considered not to be a verb, if it has been given
one or more analyses from the medieval dic-
tionary, where none of the analyses is a verb
interpretation. If the token has been given both
a verb analysis and a non-verb analysis by the
medieval dictionary, or if the token is not found
in the dictionary, the token is matched against
the 19th century dictionary before being pro-
cessed by the morphological analyser and the
tagger as described in setting morph + tag.
19c + medieval A token is considered to be a verb
if it has been unambiguously analysed as a verb
by the 19th century dictionary. Likewise a to-
ken is considered not to be a verb, if it has
been given one or more analyses from the 19th
century dictionary, where none of the analyses
is a verb interpretation. If the token has been
given both a verb analysis and a non-verb anal-
ysis by the 19th century dictionary, or if the to-
ken is not found in the dictionary, the token is
matched against the medieval dictionary before
being processed by the morphological analyser
and the tagger as described in setting morph +
tag.
5 Results
5.1 Normalisation Rules
Running the SALDO morphological analyser on the
test text with the old Swedish spelling preserved,
meant that only 30% of the words were analysed
at all. Applying the normalisation rules before the
morphological analysis is performed, drastically in-
creases recall. After only 5 rules have been ap-
plied, recall is increased by 11 percentage units, and
adding another 5 rules increases recall by another
26 percentage units. All in all, recall increases from
30% for unnormalised text to 83% after all normal-
isation rules have been applied, whereas precision
increases from 54% to 66%, as illustrated in table 1.
Recall is still significantly higher for contempo-
rary Swedish texts than for the historical text (99%
as compared to 83% with the best normalisation
settings). Nevertheless, the rapid increase in re-
call when applying the normalisation rules is very
promising, and it is yet to be explored how good re-
sults it is possible to reach if including more normal-
isation rules.
Precision Recall f-score
raw data 0.54 0.30 0.39
5 rules 0.61 0.41 0.49
10 rules 0.66 0.67 0.66
15 rules 0.66 0.68 0.67
20 rules 0.67 0.73 0.70
25 rules 0.66 0.78 0.72
30 rules 0.66 0.79 0.72
35 rules 0.66 0.82 0.73
39 rules 0.66 0.83 0.74
SUC corpus 0.53 0.99 0.69
Table 1: Morphological analysis results using SALDO
version 2.0, before and after incremental application of
normalisation rules, and compared to the Stockholm-
Umea? corpus of contemporary Swedish written language.
5.2 Morphological Analysis and Tagging
Table 2 presents the results of combining the
SALDO morphological analyser and the HunPOS
tagger, using the settings described in section 4.3.
Precision Recall f-score
morph 0.66 0.83 0.74
tag 0.81 0.86 0.83
morph or tag 0.61 0.92 0.74
morph and tag 0.92 0.80 0.85
morph + tag 0.82 0.88 0.85
Table 2: Results for normalised text, combining mor-
phological analysis and tagging. morph = morphological
analysis using SALDO. tag = tagging using HunPOS.
As could be expected, the tagger yields higher
precision than the morphological anlayser, due to
the fact that the morphological analyser renders all
analyses for a word form given in the dictionary, re-
gardless of context. The results of combining the
92
morphological analyser and the tagger are also quite
expected. In the case where a token is considered
to be a verb if there is a morphological verb analy-
sis or it has been analysed as a verb by the tagger,
a very high level of recall (92%) is achieved at the
expense of low precision, whereas the opposite is
true for the case where a token is considered to be
a verb if there is a morphological verb analysis and
it has been tagged as a verb. Using the tagger for
disambiguation only in ambiguous cases yields the
best results. It should be noted that using the morph-
and-tag setting results in the same f-score as the dis-
ambiguation setting. However, the disambiguation
setting performs better in terms of recall, which is of
importance to the historians in the project at hand.
Another advantage of using the disambiguation set-
ting is that the difference between precision and re-
call is less.
5.3 Historical Dictionaries
The results of using the historical dictionaries are
presented in table 3.
Precision Recall f-score
morph + tag 0.82 0.88 0.85
medieval 0.82 0.81 0.81
19c 0.82 0.86 0.84
medieval + 19c 0.81 0.79 0.80
19c + medieval 0.81 0.79 0.80
Table 3: Results for normalised text, combining histor-
ical dictionaries and contemporary analysis tools. me-
dieval = Medieval Lexical Database. 19c = Dalin?s Dic-
tionary. morph = morphological analysis using SALDO.
tag = tagging using HunPOS.
Adding the historical dictionaries did not improve
the verb analysis results; actually the opposite is
true. Studying the results of the analyses from the
medieval dictionary, one may notice that only two
verb analyses have been found when applied to the
test text, and both of them are erroneous in this con-
text (in both cases the word lass ?load? as in the
phrase 6 lass ho?o? ?6 loads of hay?). Furthermore,
the medieval dictionary produces quite a lot of non-
verb analyses for commonly occurring verbs, for ex-
ample skola (noun: ?shool?, verb: ?should/shall?),
kunna (?can/could?), kom (?come?), finna (?find?)
and vara (noun: ?goods?, verb: ?be?). Another rea-
son for the less encouraging results seems to be that
most of the words actually found and analysed cor-
rectly are words that are correctly analysed by the
contemporary tools as well, such as i (?in?), man
(?man/you?), sin (?his/her/its?), honom (?him?) and
in (?into?).
As for the 19th century dictionary, the same prob-
lems apply. For example, a number of frequent
verb forms are analysed as non-verbs (e.g. skall
?should/shall? and ligger ?lies?). There are also
non-verbs repeatedly analysed as verbs, such as
stadgar (?regulations?) and egne (?own?). As was
the case for the medieval dictionary, most of the
words analysed correctly by the 19th century dic-
tionary are commonly occuring words that would
have been correctly analysed by the morphological
analyser and/or the tagger as well, for example och
(?and?), men (?but?) and na?r (?when?).
6 Support for Historical Research
In the ongoing Gender and Work project at the De-
partment of History, Uppsala University, historians
are interested in what men and women did for a liv-
ing in the early modern Swedish Society.3 Informa-
tion on this is registered and made available for re-
search in a database, most often in the form of a verb
and its object(s). The automatic verb extraction tool
was developed in close cooperation with the Gender
and Work participants, with the aim of reducing the
manual effort involved in finding the relevant infor-
mation to enter into the database.
The verb extraction tool was integrated in a pro-
totypical graphical user interface, enabling the his-
torians to run the system on historical texts of their
choice. The interface provides facilities for upload-
ing files, generating a list of all the verbs in the file,
displaying verb concordances for interesting verbs,
and displaying the verb in a larger context. Figure
2 illustrates the graphical user interface, displaying
concordances for the verb anklaga (?accuse?). The
historians found the interface useful and are inter-
ested in integrating the tool in the Gender and Work
database. Further development of the verb extrac-
tion tool is now partly funded by the Gender and
Work project.
3http://gaw.hist.uu.se/
93
Figure 2: Concordances displayed for the verb anklaga (?accuse?) in the graphical user interface.
7 Conclusion
Today historians and other researchers working on
older texts have to manually go through large vol-
umes of text when searching for information on
for example culture or social structure in histori-
cal times. In this paper we have shown that this
time-consuming manual effort could be significantly
reduced using contemporary natural language pro-
cessing tools to display only those text segments
that may be of interest to the researcher. We have
described the development of a tool that automati-
cally identifies verbs in historical Swedish texts us-
ing morphological analysis and tagging, and a proto-
typical graphical user interface, integrating this tool.
The results indicate that it is possible to retrieve
verbs in Swedish texts from the 17th century with
82% precision and 88% recall, using morphological
tools for contemporary Swedish, if the text is nor-
malised into a more modern spelling before the mor-
phological tools are applied (recall may be increased
to 92% if a lower precision is accepted).
Adding electronically available dictionaries cov-
ering medieval Swedish and 19th century Swedish
respectively to the verb extraction tool, did not im-
prove the results as compared to using only contem-
porary NLP tools. This seems to be partly due to
the dictionaries still being in an early stage of devel-
opment, where lexical coverage is unevenly spread
among different word classes, and frequent, irregu-
larly inflected word forms are not covered. It would
therefore be interesting to study the results of the
historical dictionary lookup, when the dictionaries
are more mature.
Since the present extraction tool has been evalu-
ated on one single text, it would also be interesting
to explore how these extraction methods should be
adapted to handle language variation in texts from
different genres and time periods. Due to the lack
of spelling conventions, it would also be interest-
ing to see how the extraction process performs on
texts from the same period and genre, but written by
different authors. Future work also includes experi-
ments on identifying linguistic categories other than
verbs.
94
References
Go?sta Bergman. 1995. Kortfattad svensk spra?khistoria.
Prisma Magnum, 5th ed., Stockholm.
Lars Borin, Markus Forsberg, and Lennart Lo?nngren.
2008. SALDO 1.0 (Svenskt associationslexikon ver-
sion 2). Spra?kbanken, University of Gothenburg.
Lars Borin, Dimitrios Kokkinakis, and Leif-Jo?ran Ols-
son. 2007. Naming the Past: Named Entity and
Anomacy Recognition in 19th Century Swedish Lit-
erature). In: Proceedings of the Workshop on Lan-
guage Technology for Cultural Heritage Data (LaT-
eCH 2007), pages 1?8. Prague, Czech Republic.
Bra Bo?cker. 1995. Nationalencyklopedins ordbok. Bra
Bo?cker, Ho?gana?s.
Thorsten Brants. 2000. TnT - A Statistical Part-of-
Speech Tagger. In: Proceedings of the 6th Applied
Natural Language Processing Conference (ANLP-00),
Seattle, Washington, USA.
Anders Fredrik Dalin. 1850?1855. Ordbok O?fver sven-
ska spra?ket. Vol I?II. Stockholm.
Nils Edling. 1937. Uppla?ndska dombo?cker. ja?mte in-
ledning, fo?rklaringar och register utgivna genom Nils
Edling. Uppsala.
Sofia Gustafson-Capkova? and Britt Hartmann. December
2006. Manual of the Stockholm Umea? Corpus version
2.0. Description of the content of the SUC 2.0 dis-
tribution, including the unfinished documentation by
Gunnel Ka?llgren.
Pe?ter Hala?csy, Andra?s Kornai, and Csaba Oravecz 2007.
HunPos - an open source trigram tagger. In: Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation for Computational Linguistics Companion Vol-
ume Proceedings of the Demo and Poster Sessions,
pages 209?212. Association for Computational Lin-
guistics, Prague, Czech Republic.
Lennart Lo?nngren. 1992. Svenskt associationslexikon,
del I?IV. Department of Linguistics and Philology,
Uppsala University.
Bea?ta B. Megyesi. 2008. The Open Source Tagger
HunPoS for Swedish. Department of Linguistics and
Philology, Uppsala University.
Csaba Oravecz, Ba?lint Sass, and Eszter Simon 2010.
Semi-automatic Normalization of Old Hungarian
Codices. In: Proceedings of the ECAI 2010 Workshop
on Language Technology for Cultural Heritage, Social
Sciences, and Humanities (LaTeCH 2010). Pages 55?
59. 16 August, 2010 Faculty of Science, University of
Lisbon Lisbon, Portugal.
Marco Pennacchiotti and Fabio Massimo Zanzotto 2008.
Natural Language Processing Across Time: An Em-
pirical Investigation on Italian. In: Aarne Ranta and
Bengt Nordstro?m (Eds.): Advances in Natural Lan-
guage Processing. GoTAL 2008, LNAI Volume 5221,
pages 371?382. Springer-Verlag Berlin Heidelberg.
Vitor Rocio, Ma?rio Amado Alves, Jose? Gabriel Lopes,
Maria Francisca Xavier, and Grac?a Vicente. 1999.
Automated Creation of a Partially Syntactically Anno-
tated Corpus of Medieval Portuguese Using Contem-
porary Portuguese Resources. In: Proceedings of the
ATALA workshop on Treebanks, Paris, France.
Carl Johan Schlyter. 1877. Ordbok till Samlingen af
Sweriges Gamla Lagar. Lund.
Svenska Akademien. 2006. Svenska Akademiens or-
dlista o?ver svenska spra?ket. Norstedts Akademiska
Fo?rlag, Stockholm.
Knut Fredrik So?derwall. 1884?1918. Ordbok O?fver
svenska medeltids-spra?ket, vol I?III. Lund.
Knut Fredrik So?derwall. 1953?1973. Ordbok O?fver
svenska medeltids-spra?ket, vol IV?V. Lund.
Ulf Teleman, Staffan Hellberg, and Erik Andersson.
1999. Svenska Akademiens grammatik. Norstedts Or-
dbok, Stockholm.
95
Proceedings of the 6th EACL Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 65?74,
Avignon, France, 24 April 2012. c?2012 Association for Computational Linguistics
Parsing the Past ? Identification of Verb Constructions in Historical Text
Eva Pettersson?, Bea?ta Megyesi and Joakim Nivre
Department of Linguistics and Philology
Uppsala University
?Swedish National Graduate School
of Language Technology
firstname.lastname@lingfil.uu.se
Abstract
Even though NLP tools are widely used for
contemporary text today, there is a lack of
tools that can handle historical documents.
Such tools could greatly facilitate the work
of researchers dealing with large volumes
of historical texts. In this paper we pro-
pose a method for extracting verbs and their
complements from historical Swedish text,
using NLP tools and dictionaries developed
for contemporary Swedish and a set of nor-
malisation rules that are applied before tag-
ging and parsing the text. When evaluated
on a sample of texts from the period 1550?
1880, this method identifies verbs with an
F-score of 77.2% and finds a partially or
completely correct set of complements for
55.6% of the verbs. Although these re-
sults are in general lower than for contem-
porary Swedish, they are strong enough to
make the approach useful for information
extraction in historical research. Moreover,
the exact match rate for complete verb con-
structions is in fact higher for historical
texts than for contemporary texts (38.7%
vs. 30.8%).
1 Introduction
Today there is an abundance of NLP tools that
can analyse contemporary language and extract
information relevant to a particular user need, but
there is a real lack of tools that can handle histor-
ical documents. Historians and other researchers
working with older texts are still mostly forced to
manually search large amounts of text in order to
find the passages of interest to their research. De-
veloping tools to facilitate this process is a great
challenge, however, as historical texts vary greatly
in both spelling and grammar between different
authors, genres and time periods, and even within
the same text, due to the lack of spelling conven-
tions. In addition to this, there is a shortage of
annotated resources that can be used for the de-
velopment and evaluation of new tools.
The work presented in this paper has been car-
ried out in cooperation with historians, who are
studying what men and women did for a living
in the Early Modern Swedish society. Currently,
the historians are manually extracting segments
describing work activities from a number of his-
torical texts, and entering them into a database,
the Gender and Work Database. Their work so far
has shown that the typical segment describing an
activity of work is a verb construction, that is, a
verb together with its complements (A?gren et al,
2011). (Examples of such segments can be found
below in Table 1.) It is very likely that the manual
effort and the time needed by the historians to find
these segments and enter them into the database
could be substantially reduced if verbs and their
complements were automatically extracted and
presented to the historian. This would give a gen-
eral overview of the content of a text, and the
task of the historian would be to select those seg-
ments that are actually describing work activities.
By linking extracted segments back to larger pas-
sages of text, historians would also be able to find
additional segments that were missed by the first
automatic extraction. The core of such a system
would be a component for identifying verb con-
structions in running text.
We propose a method for automatically iden-
tifying verbs and their complements in various
types of historical documents, produced in the
Early Modern Swedish period (1550?1800). The
method is based on using existing NLP tools for
65
contemporary Swedish, in particular a part-of-
speech tagger and a syntactic parser, and auto-
matically normalising the input text into a more
modern orthography before running it through
the tagger and parser. In order to increase the
precision of complement extraction, we use va-
lency dictionaries to filter out unlikely comple-
ments in the output of the syntactic parser. Using
this method, we are able to identify verbs with
an F-score of 77.2% and find a partially or com-
pletely correct set of complements for 55.6% of
the verbs. To our knowledge, extracting verb con-
structions from historical texts is a task that has
not been directly addressed in previous research,
which means that these results are also important
in setting benchmarks for future research.
The paper is structured as follows. Section 2
reviews related work. Section 3 describes the
method for identification of verbs and comple-
ments in more detail. Section 4 presents the data
and evaluation metrics used in our experiments,
and Section 5 discusses the results of the evalua-
tion. Finally, Section 6 concludes the paper.
2 Related Work
Using NLP tools for analysing historical texts is
still to a large extent unexplored. There is how-
ever a growing interest in this area, and there have
been attempts to analyse historical texts (1) by us-
ing contemporary NLP tools as they are, (2) by
using such tools in combination with normalisa-
tion rules and/or dictionaries covering historical
language variation, and (3) by training new tools
on annotated historical corpora.
Pennacchiotti and Zanzotto (2008) concluded
that contemporary NLP tools are not suitable as
they are for analysing historical text. They tried
to use a contemporary dictionary, morphological
analyser and part-of-speech tagger to analyse Ital-
ian texts from the period 1200?1881. In their ex-
periments, the dictionary only covered approxi-
mately 27% of the words in the oldest text, as
compared to 62.5% of the words in a contem-
porary Italian newspaper text. Consequently, the
morphological analyser based on the dictionary
reached an accuracy of only 48%, as compared
to 91% for contemporary text. Similarly, the part-
of-speech tagger used reached an accuracy of only
54%, as compared to 97% for contemporary text.
Oravecz et al (2010) included a standardis-
ation/normalisation step in their work on semi-
automatically annotating a corpus of Old Hun-
garian. Normalisation was performed using a
noisy channel model combined with morphologi-
cal analysis filtering and decision tree reranking.
Combining these methods, they reached a normal-
isation precision of 73.3%.
Rocio et al (1999) used a grammar of contem-
porary Portuguese to syntactically annotate me-
dieval Portuguese texts. A dictionary and inflec-
tional rules for medieval Portuguese were added
to the parser, to make it suitable for handling these
texts. This approach proved to be successful for
partial parsing of medieval Portuguese texts, even
though there were some problems remaining con-
cerning grammar limitations, dictionary incom-
pleteness and insufficient part-of-speech tagging.
Sa?nchez-Marco et al (2011) adapted an ex-
isting NLP tool to deal with Old Spanish. The
adapted tool had an accuracy of 94.5% in find-
ing the right part of speech, and 89.9% accuracy
in finding the complete morphological tag. The
adaptation was performed on the basis of a 20 mil-
lion token corpus of texts from the 12th to the 16th
century, and included expansion of the dictionary,
modification of tokenisation and affixation rules,
and retraining of the tagger. The retraining was
based on a gold standard of 30,000 tokens, where
the tokens were first pre-annotated with the con-
temporary tagger, and then manually corrected.
Adding new words to the dictionary had the high-
est impact on the results. This was done by au-
tomatically generating word forms through map-
ping old spelling variants to their contemporary
counterparts.
Pettersson and Nivre (2011) presented a study
on automatically extracting verbs from Swedish
17th century texts, using contemporary language
technology tools combined with normalisation
of the input text. The verb extraction process
included an iterative process of normalisation
and morphological analysis, followed by part-of-
speech tagging for disambiguation of competing
interpretations and for analysing words still un-
known to the morphological analyser after all nor-
malisation rules had been applied. Using this
method, verbs were extracted with 82% precision
and 88% recall. The study also included the re-
sults of using only the part-of-speech tagger for
verb recognition, i.e., dropping the morphologi-
cal analyser. This resulted in a small decrease in
precision to 81% and in recall to 86%.
66
3 Extraction of Verb Constructions
In this paper, we will focus on adapting existing
NLP tools by adding normalisation rules prior to
processing. We will mainly follow the method-
ology for verb extraction described in Petters-
son and Nivre (2011), but adding the extraction
of not only the verbs themselves, but also their
adherent complements. It would perhaps have
been desirable to use tools specifically trained for
analysing historical texts. This would however be
a resource-demanding task, considering the lack
of annotated data and the language variation, and
is currently not a realistic scenario.
The goal is to automatically extract verbs and
relevant complements from historical texts, in or-
der to give an overview of the contents and present
segments that are possibly describing work activ-
ities. In this context, we use the term complement
in a broad sense and do not impose a sharp dis-
tinction between complements and adjuncts, es-
pecially not for prepositional phrases. This is mo-
tivated by the fact that in the Gender and Work
Database, both segments that would traditionally
be seen as complements and phrases that would
rather be categorised as adjuncts have been con-
sidered relevant.
A closer look at the database shows that 67%
of the entered segments consist of a verb with a
direct object. Other common constructions are
verbs with a prepositional complement (11%),
verbs with both a direct object and a preposi-
tional complement (10%), and (intransitive) verbs
without complements (7%). Table 1 illustrates
the most common construction types found in
the database, which have been used to define the
rules for extracting complements from parsed sen-
tences. There were also a small number of seg-
ments (8 in total), that we were not able to cate-
gorise.
3.1 System Overview
The extraction of verbs and their complements is
basically performed in five steps:
1. Tokenisation
2. Normalisation
3. Part-of-speech tagging
4. Parsing
5. Extraction of verb constructions
Freq Comp Source Text Example
273 dobj dhe ba?rgadhe [Ho?o?]
they harvested [Hay]
47 pcomp [med een ha?st] kio?rtt
driven [with a horse]
43 dobj [det kio?pet] Han
+ [med ha?nness man] giort
pcomp [the bargain] He
made [with her husband]
30 intrans mala
to grind
5 dobj hulpit [Muremest:]
+ [inla?ggia Trappestenar]
infc helped [the Bricklayer]
[to make a Stone Stair]
3 indobj [honom] [Ja?rnet] sa?lltt
+ sold [him] [the Iron]
dobj
1 subc tillsee [att icke barnen
skulle go?ra skada]
see to it [that the children
do not do any harm]
Table 1: Segments describing work activities in the
Gender and Work Database; verbs underlined; com-
plements in brackets. Grammatical functions: dobj =
direct object, pcomp = prepositional complement, in-
trans = intransitive, indobj = indirect object, infc = in-
finitive clause, subc = subordinate clause.
Tokenisation is performed with a simple tokeniser
for Swedish that has not been adapted for histori-
cal texts.
3.2 Normalisation
After tokenisation, each word is normalised to a
more modern spelling using a set of 29 hand-
crafted rules. The rules were developed us-
ing a text sample from Per Larssons dombok,
a selection of court records from 1638 (Edling,
1937), a sample that has not been used in sub-
sequent evaluation. An example of a normalisa-
tion rule is the transformation of the letter com-
bination sch into sk, as in the old spelling schall
that is normalised to the contemporary spelling
skall (?shall/should?). Some additional rules were
also formulated based on the reformed Swedish
spelling introduced in 1906 (Bergman, 1995).
This set of rules includes the transformation of
double vowels into a single vowel, as in so?o?ka,
which is normalised into so?ka (?search?).
67
3.3 Part-of-speech Tagging
The purpose of part-of-speech tagging in our ex-
periments is both to find the verbs in the text and
to prepare for the parsing step, in which the com-
plements are identified. Part-of-speech tagging is
performed using HunPOS (Hala?csy et al, 2007),
a free and open source reimplementation of the
HMM-based TnT-tagger by Brants (2000). The
tagger is used with a pre-trained language model
based on the Stockholm-Umea? Corpus (SUC), a
balanced, manually annotated corpus of different
text types representative of the Swedish language
in the 1990s, comprising approximately one mil-
lion tokens (Gustafson-Capkova? and Hartmann,
2006). Megyesi (2009) showed that the HunPOS
tagger trained on SUC, is one of the best perform-
ing taggers for (contemporary) Swedish texts.
3.4 Parsing
The normalised and tagged input text is parsed us-
ing MaltParser version 1.6, a data-driven depen-
dency parser developed by Nivre et al (2006a).
In our experiments, the parser is run with a pre-
trained model1 for parsing contemporary Swedish
text, based on the Talbanken section of the
Swedish Treebank (Nivre et al, 2006b). The
parser produces dependency trees labeled with
grammatical functions, which can be used to iden-
tify different types of complements.
3.5 Extraction of Verb Constructions
The extraction of verb constructions from the
tagged and parsed text is performed in two steps:
1. Every word form analysed as a verb by the
tagger is treated as the head of a verb con-
struction.
2. Every phrase analysed as a dependent of the
verb by the parser is treated as a complement
provided that it has a relevant grammatical
function.
The following grammatical functions are defined
to be relevant:
1. Subject (SS)
2. Direct object (OO)
3. Indirect object (IO)
1http://maltparser.org/mco/swedish parser/swemalt.html
4. Predicative complement (SP)
5. Prepositional complement (OA)
6. Infinitive complement of object (VO)
7. Verb particle (PL)
Subjects are included only if the verb has been
analysed as a passive verb by the tagger, in which
case the subject is likely to correspond to the di-
rect object in the active voice.
In an attempt to improve precision in the com-
plement extraction phase, we also use valency
dictionaries for filtering the suggested comple-
ments. The valency frame of a verb tells us what
complements the verb is likely to occur with.
The assumption is that this information could
be useful for removing unlikely complements,
i.e., complements that are not part of the valency
frame for the verb in question. The following
example illustrates the potential usefulness of this
method:
J midler tijd kom greffuinnans gotze fougte thijtt
However, the Countess? estate bailiff came there
In this case, the parser analysed the partial noun
phrase greffuinnans gotze (?the Countess? estate?)
as a direct object connected to kom (?came?).
However, since the word kom is present in the va-
lency dictionaries, we know that it is an intransi-
tive verb that does not take a direct object. Hence,
this complement can be removed. The valency
dictionaries used for filtering are:
1. The Lexin dictionary, containing 3,550 verb
lemmas with valency information.2
2. The Parole dictionary, containing 4,308 verb
lemmas with valency information.3
3. An in-house machine translation dictionary,
containing 2,852 verb lemmas with valency
information.
4 Experimental Setup
4.1 Data
In order to evaluate the accuracy of our method,
we have used ten texts from the period 1527?
1737. The text types covered are court records
2http://spraakbanken.gu.se/lexin/valens lexikon.html
3http://spraakdata.gu.se/parole/lexikon/swedish.parole.lexikon.html
68
and documents related to the Church. In total,
there are 444,075 tokens in the corpus, distributed
as follows (number of tokens in parentheses):
Court records:
1. Per Larssons dombok (subset),1638(11,439)
2. Hammerdals tingslag, 1649?1686 (66,928)
3. Revsunds tingslag, 1649?1689 (101,020)
4. Vendels socken, 1615?45 (59,948)
5. Vendels socken, 1736?37 (55,780)
6. O?stra ha?rads i Njudung,1602?1605(34,956)
Documents related to the Church:
1. Va?stera?s recess, 1527 (12,193)
2. 1571 a?rs kyrkoordning (49,043)
3. Uppsala mo?tes beslut, 1593 (26,969)
4. 1686 a?rs kyrkolag (25,799)
A gold standard of 40 randomly selected sen-
tences from each text was compiled, i.e., in total
400 sentences. The gold standard was produced
by manually annotating the sentences regarding
verbs and complements. Because sentences are
much longer in these texts than in contemporary
texts, the 400 sentences together contain a total of
3,105 verbs. Each word form that was interpreted
as a verb was annotated with the tag VB, and com-
plements were enclosed in brackets labeled with
their grammatical function. This is illustrated in
Figure 1, which shows an annotated segment from
the test corpus.
For comparison with contemporary text, we
make use of a subset of the Stockholm-Umea? Cor-
pus of contemporary Swedish text, SUC (Ejerhed
and Ka?llgren, 1997). This subset contains those
segments in SUC that have been syntactically
annotated and manually revised in the Swedish
Treebank. In total, the subset includes approxi-
mately 20,000 tokens. Since the tagger used in the
experiments on historical texts is trained on the
whole of SUC, we had to slightly modify the ex-
traction algorithm in order not to evaluate on the
same data as the tagger has been trained. When
testing the algorithm on contemporary text, we
therefore trained a new model for the tagger, in-
cluding all tokens in SUC except for the tokens
reserved for evaluation.
Anklagadhes/VB1 Was accused/VB1
[SSvb1 [SSvb1
ryttaren the horse-rider
Hindrik Hindrik
Hyldh Hyldh
SSvb1] SSvb1]
hwilken who
[OOvb2 [OOvb2
mo?krenkningh rape
OOvb2] OOvb2]
giordt/VB2 done/VB2
medh with
en a
gienta girl
Elin Elin
Eriksdotter Eriksdotter
i in
Sika?s Sika?s
, ,
hwarfo?re why
ra?tten the Court
honnom him
tilspordhe/VB3 asked/VB3
[OOvb3 [OOvb3
om if
han he
[OOvb4 [OOvb4
dhetta this
OO vb4] OO vb4]
giordt/VB4 done/VB4
hafwer/VB5 has/VB5
OO vb3] OO vb3]
Figure 1: Annotated segment in the test corpus.
4.2 Evaluation Metrics
In order to get a more fine-grained picture of the
system?s performance, we want to evaluate three
different aspects:
1. Identification of verbs
2. Identification of complements
3. Identification of holistic verb constructions
The identification of verbs depends only on the
part-of-speech tagger and can be evaluated using
traditional precision and recall measures, compar-
ing the tokens analysed as verbs by the tagger to
the tokens analysed as verbs in the gold standard.
The identification of complements depends on
both the tagger and the parser and can also be
69
evaluated using precision and recall measures.
In this case, every complement identified by the
parser is compared to the complements annotated
in the gold standard. Precision is the number of
correct complements found by the parser divided
by the total number of complements output by
the parser, while recall is the number of correct
complements found by the parser divided by the
number of complements in the gold standard.
We do not take the labels into account when
assessing complements as correct or incorrect.
The motivation for this is that the overall aim
of the complement extraction is to present verb
expressions to historians, for them to consider
whether they are describing work activities or
not. In this context, only textual strings will
be of interest, and grammatical function labels
are ignored. For example, assume that the gold
standard is:
lefverere [IO honom] [OO Sa?dh]
deliver [IO him] [OO grain]
and that the system produces:
lefverere [OO honom]
deliver [OO him]
In this context, the complement honom (?him?)
will be regarded as correct, even though it has
been analysed as a direct object instead of an
indirect object. On the other hand, the evaluation
of complement identification is strict in that
it requires the complement found to coincide
exactly with the complement in the gold standard.
For example, assume the gold standard is:
effterfra?gat [OA om sinss manss do?dh]
asked [OA about her husband?s death]
and that the system produces:
effterfra?gat [OA om sinss manss]
asked [OA about her husband?s]
In this case, the complement will not be regarded
as correct because it does not cover exactly the
same textual string as the gold standard annota-
tion.
The identification of holistic verb construc-
tions, that is, a verb and all its complements,
depends on the identification of verbs and com-
plements, as well as the optional filtering of
complements using valency dictionaries. Here
we want to evaluate the entire text segment
extracted in a way that is relevant for the intended
application of the system. First of all, this means
that partially correct constructions should be
taken into account. Consider again the earlier
example:
effterfra?gat [OA om sinss manss do?dh]
asked [OA about her husband?s death]
and assume that the system produces:
effterfra?gat [OA om sinss manss]
asked [OA about her husband?s]
As noted above, this complement would be con-
sidered incorrect in the precision/recall evaluation
of complement extraction, even though only one
word is missing as compared to the gold standard,
and the output would probably still be valuable to
the end-user. Secondly, we should consider the to-
tal segment extracted for a verb including all com-
plements, rather than inspecting each complement
separately.
In order to reflect partially correct comple-
ments and take the total segment extracted for
each verb into account, we use a string-based
evaluation method for the identification of holis-
tic verb constructions. In this evaluation, all la-
bels and brackets are removed before comparing
the segments extracted to the segments in the text
corpus and each extracted instance is classified as
falling into one of the four following categories:
? Fully correct complement set (F)
? Partially correct complement set (P)
? Incorrect complement set (I)
? Missing complement set (M)
A complement set is regarded as fully correct if
the output string generated by the system is iden-
tical to the corresponding gold standard string.
Since labels and brackets have been removed,
these analyses will be regarded as identical:
lemnat [IO swaranden] [OO tid]
given [IO the defendant] [OO time]
70
lemnat [OO swaranden tid]
given [OO the defendant time]
A complement set is regarded as partially correct
if the output string generated by the system has a
non-empty overlap with the corresponding gold
standard string. For example, the following three
sets of analyses will be considered as partially
correct (gold standard top, system output bottom):
lefverere [IO honom] [OO Sa?dh]
deliver [IO him] [OO Grain]
lefverere [OO honom]
deliver [OO him]
effterfra?gat [OA om sinss manss do?dh]
asked [OA about her husband?s death]
effterfra?gat [OA om sinss manss]
asked [OA about her husband?s]
betale [PL a?ter] [IO ha?r Mattz] [OO Ra?gen]
pay [PL back] [IO mister Mattz] [OO the Rye]
betale [OO a?ter ha?r Mattz]
pay [OO back mister Mattz]
A (non-empty) complement set is regarded as in-
correct if the output string has no overlap with the
gold standard string. Finally, a complement set is
regarded as missing if the output string is empty
but the gold standard string is not. It is worth not-
ing that the four categories are mutually exclusive.
5 Results and Discussion
In this section, we evaluate the identification of
verbs, complements and holistic verb construc-
tions using the data and metrics described in the
previous section.
5.1 Verbs
Results on the identification of verbs using part-
of-speech tagging, with and without normalisa-
tion, are reported in Table 2. As can be seen, recall
drastically increases when normalisation rules are
applied prior to tagging, even though the normal-
isation rules used in this experiment are formu-
lated based on a subset of one single 17th cen-
tury text, and the test corpus contains samples of
various text types ranging from 1527?1737. Nor-
malisation also has a small positive effect on pre-
cision, and the best result for historical texts is
78.4% precision and 76.0% recall. This is slightly
Precision Recall F-score
Raw 75.4 60.0 66.9
Norm 78.4 76.0 77.2
SUC 99.1 99.1 99.1
Table 2: Identification of verbs by tagging. Raw = Un-
normalised input text. Norm = Normalisation of input
prior to tagging. SUC = Subset of Stockholm-Umea?
corpus of contemporary Swedish texts, as described in
section 4.1.
lower than the results presented by Pettersson and
Nivre (2011) where only 17th century text was
used for evaluation, indicating that the normalisa-
tion rules are somewhat biased towards 17th cen-
tury text, and that the results could be improved
if normalisation were adapted to specific time pe-
riods. It is also worth noting that the results are
substantially lower for historical text than the re-
sults for contemporary text, with precision and re-
call at 99.1%, but still high enough to be useful in
the intended context of application.
Tokens that are still erroneously analysed by
the tagger include the following cases:
? tokens where the old spelling is identical
to an existing, but different, word form
in contemporary language; for example,
the spelling skal would in contemporary
language be considered a noun (?shell?)
but in the old texts this spelling is used
for a word that is nowadays spelled skall
(?shall/should?) and should be regarded as a
verb;
? ambiguous words; for example, past partici-
ples are often spelled the same way as the
corresponding past tense verb, but participles
are not regarded as verb forms in our experi-
ments;4
? tokens that have not been normalised enough
and thus do not correspond to a word form
recognised by the tagger, e.g., the word
form lemnas which in contemporary lan-
guage should be spelled as la?mnas (?be
left?).
4Participles are only used adjectivally in Swedish, as the
perfect tense is formed using a distinct supine form of the
verb.
71
Precision Recall F-score
Raw 24.8 27.5 26.1
Norm 28.3 28.2 28.2
+Valency 33.1 25.5 28.8
SUC 68.2 70.7 69.5
+Valency 71.8 56.2 63.0
Table 3: Identification of complements by parsing.
Raw = Unnormalised input text. Norm = Normalisa-
tion of input prior to tagging and parsing. +Valency =
Adding valency filtering to the setting in the preced-
ing row. SUC = Subset of Stockholm-Umea? corpus of
contemporary Swedish texts, as described in section
4.1.
5.2 Complements
Recall and precision for the identification of com-
plements using parsing are presented in Table 3.
In this case, normalisation has a smaller effect
than in the case of tagging and affects precision
more than recall. Adding a filter that eliminates
unlikely complements based on the valency frame
of the verb in existing dictionaries predictably im-
proves precision at the expense of recall and re-
sults in a small F-score improvement.
Again, the best scores on the historical texts
are much lower than the corresponding results for
contemporary text, with an F-score of 28.8% in
the former case and 69.5% in the latter, but it is
worth remembering that precision and recall on
exactly matching complements is a harsh metric
that is not directly relevant for the intended appli-
cation. Finally, it is worth noting that the valency
filter has a large negative impact on recall for the
modern texts, resulting in a decrease in the F-
score, which indicates that the parser in this case
is quite successful at identifying complements (in
the wide sense) that are not covered by the va-
lency dictionaries.
5.3 Verb Constructions
As argued in section 4.2, precision and recall mea-
sures are not sufficient for evaluating the extrac-
tion of holistic verb constructions. A more rele-
vant assessment is made by counting the number
of fully correct, partially correct, incorrect and
missing complement sets for the verbs identified.
Table 4 summarises the results in accordance with
this metric.
First of all, we see that normalisation again has
a rather small effect on overall results, increas-
F P I M
Raw 32.6 20.3 29.3 17.8
Norm 34.5 19.5 25.2 20.8
+Valency 38.7 16.9 18.9 25.5
SUC 30.3 54.2 9.1 6.4
+Valency 30.8 47.9 6.8 14.6
Table 4: Identification of holistic verb constructions.
F = Fully correct, P = Partially correct, I = Incorrect,
M = Missing. Raw = Unnormalised input text. Norm
= Normalisation of input prior to tagging and parsing.
+Valency = Adding valency filtering to the setting in
the preceding row. SUC = Subset of Stockholm-Umea?
corpus of contemporary Swedish texts, as described in
section 4.1.
ing the number of fully correct constructions and
decreasing the number of incorrect constructions,
but also leading to an increase in the number of
missing complements. Adding the valency fil-
ter to remove unlikely complements has a simi-
lar effect and increases the percentage of correctly
extracted verb constructions to 38.7% while de-
creasing the share of incorrect constructions to
18.9%. However, it also increases the percent-
age of verbs with missing complement sets from
20.8% to 25.5%. This is partly due to the fact
that some of the verbs are used in a slightly dif-
ferent way in historical text as compared to con-
temporary text, meaning that the valency frames
are not as reliable. For example, the verb avsta?
(?refrain?) in the historical corpus is used with a
direct object, as in Anders Andersson afsta?dt sitt
skatte hemman (?Anders Andersson refrained his
homestead?), whereas in a contemporary context
this verb would more likely be used with a prepo-
sitional complement, avsta? fra?n na?gonting (?re-
frain from something?).
In total, 55.6% of the verbs are assigned a fully
or partially correct set of complements. This is
again lower than the result for contemporary texts
(78.7%), but the difference is smaller than for the
previous metrics, which is encouraging given that
the evaluation in this section is most relevant for
the intended application. Moreover, it is worth
noting that the difference is mostly to be found
in the category of partially correct constructions,
where the best result for modern texts is 54.2%,
to be compared to 16.9% for the historical texts.
With respect to fully correct constructions, how-
ever, the results are actually better for the histor-
72
ical texts than for the modern texts, 38.7% vs.
30.8%, a rather surprising positive result.
6 Conclusion
We have presented a method for automatically ex-
tracting verbs and their complements from histor-
ical Swedish texts, more precisely texts from the
Early Modern era (1550?1800), with the aim of
providing language technology support for histor-
ical research. We have shown that it is possible
to use existing contemporary NLP tools and dic-
tionaries for this purpose, provided that the input
text is first (automatically) normalised to a more
modern spelling. With the best configuration of
our tools, we can identify verbs with an F-score
of 77.2% and find a partially or completely cor-
rect set of complements for 55.6% of the verbs.
To the best of our knowledge, these are the first
results of their kind.
In addition to presenting a method for the iden-
tification of verb constructions, we have also pro-
posed a new evaluation framework for such meth-
ods in the context of information extraction for
historical research. As a complement to standard
precision and recall metrics for verbs and their
complements, we have evaluated the text seg-
ments extracted using the categories fully correct,
partially correct, incorrect, and missing. One
important topic for future research is to validate
this evaluation framework by correlating it to the
perceived usefulness of the system when used
by historians working on the Gender and Work
Database. Preliminary experiments using a proto-
type system indicate that this kind of support can
in fact reduce the time-consuming, manual work
that is currently carried out by historians and other
researchers working with older texts.
Another topic for future research concerns the
variation in performance across time periods and
text types. In the current evaluation, court records
and papers related to the Church ranging from
1527 to 1737 have been sampled in the gold stan-
dard. It would be interesting to explore in more
detail how the program performs on the oldest
texts as compared to the youngest texts, and on
court records as compared to the other genres.
References
Maria A?gren, Rosemarie Fiebranz, Erik Lindberg, and
Jonas Lindstro?m. 2011. Making verbs count. The
research project ?Gender and Work? and its method-
ology. Scandinavian Economic History Review,
59(3):271?291. Forthcoming.
Go?sta Bergman. 1995. Kortfattad svensk
spra?khistoria. Prisma Magnum, Stockholm, 5th
edition.
Thorsten Brants. 2000. TnT - a statistical part-of-
speech tagger. In Proceedings of the 6th Applied
Natural Language Processing Conference (ANLP),
Seattle, Washington, USA.
Nils Edling. 1937. Uppla?ndska dombo?cker. Almqvist
& Wiksells.
Eva Ejerhed and Gunnel Ka?llgren. 1997. Stockholm
Umea? Corpus. Version 1.0. Produced by Depart-
ment of Linguistics, Umea? University and Depart-
ment of Linguistics, Stockholm University. ISBN
91-7191-348-3.
Sofia Gustafson-Capkova? and Britt Hartmann. 2006.
Manual of the Stockholm Umea? Corpus version 2.0.
Technical report, December.
Pe?ter Hala?csy, Andra?s Kornai, and Csaba Oravecz.
2007. HunPos - an open source trigram tagger.
In Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics, pages
209?212, Prague, Czech Republic.
Bea?ta B. Megyesi. 2009. The open source tagger
HunPos for Swedish. In Proceedings of the 17th
Nordic Conference on Computational Linguistics
(NODALIDA).
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006a.
MaltParser: A data-driven parser-generator for de-
pendency parsing. In Proceedings of the 5th
international conference on Language Resources
and Evaluation (LREC), pages 2216?2219, Genoa,
Italy, May.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006b.
Talbanken05: A Swedish treebank with phrase
structure and dependency annotation. In Proceed-
ings of the 5th international conference on Lan-
guage Resources and Evaluation (LREC, pages 24?
26, Genoa, Italy, May.
Csaba Oravecz, Ba?lint Sass, and Eszter Simon. 2010.
Semi-automatic normalization of Old Hungarian
codices. In Proceedings of the ECAI Workshop on
Language Technology for Cultural Heritage, Social
Sciences, and Humanities (LaTeCH), pages 55?59,
Faculty of Science, University of Lisbon Lisbon,
Portugal, August.
Marco Pennacchiotti and Fabio Massimo Zanzotto.
2008. Natural language processing across time: An
empirical investigation on Italian. In Advances in
Natural Language Processing. GoTAL, LNAI, vol-
ume 5221, pages 371?382.
Eva Pettersson and Joakim Nivre. 2011. Automatic
verb extraction from historical Swedish texts. In
Proceedings of the 5th ACL-HLT Workshop on Lan-
guage Technology for Cultural Heritage, Social Sci-
ences, and Humanities, pages 87?95, Portland, OR,
73
USA, June. Association for Computational Linguis-
tics.
Vito Rocio, Ma?rio Amado Alves, Jose? Gabriel Lopes,
Maria Francisca Xavier, and Grac?a Vicente. 1999.
Automated creation of a partially syntactically an-
notated corpus of Medieval Portuguese using con-
temporary Portuguese resources. In Proceedings of
the ATALA workshop on Treebanks, Paris, France.
Cristina Sa?nchez-Marco, Gemma Boleda, and Llu??s
Padro?. 2011. Extending the tool, or how to an-
notate historical language varieties. In Proceedings
of the 5th ACL-HLT Workshop on Language Tech-
nology for Cultural Heritage, Social Sciences, and
Humanities, pages 1?9, Portland, OR, USA, June.
Association for Computational Linguistics.
74
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 109?113,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Tree Kernels for Machine Translation Quality Estimation
Christian Hardmeier and Joakim Nivre and Jo?rg Tiedemann
Uppsala University
Department of Linguistics and Philology
Box 635, 751 26 Uppsala, Sweden
firstname.lastname@lingfil.uu.se
Abstract
This paper describes Uppsala University?s
submissions to the Quality Estimation (QE)
shared task at WMT 2012. We present a QE
system based on Support Vector Machine re-
gression, using a number of explicitly defined
features extracted from the Machine Transla-
tion input, output and models in combination
with tree kernels over constituency and de-
pendency parse trees for the input and output
sentences. We confirm earlier results suggest-
ing that tree kernels can be a useful tool for
QE system construction especially in the early
stages of system design.
1 Introduction
The goal of the WMT 2012 Quality Estimation
(QE) shared task (Callison-Burch et al, 2012) was
to create automatic systems to judge the quality
of the translations produced by a Statistical Ma-
chine Translation (SMT) system given the input
text, the proposed translations and information about
the models used by the SMT system. The shared
task organisers provided a training set of 1832 sen-
tences drawn from earlier WMT Machine Transla-
tion test sets, translated from English to Spanish
with a phrase-based SMT system, along with the
models used and diagnostic output produced by the
SMT system as well as manual translation quality
annotations on a 1?5 scale for each sentence. Ad-
ditionally, a set of 17 baseline features was made
available to the participants. Systems were evalu-
ated on a test set of 422 sentences annotated in the
same way.
Uppsala University submitted two systems to this
shared task. Our systems were fairly successful and
achieved results that were outperformed by only one
competing group. They improve over the baseline
performance in two ways, building on and extend-
ing earlier work by Hardmeier (2011), on which
the system description in the following sections is
partly based: On the one hand, we enhance the set
of 17 baseline features provided by the organisers
with another 82 explicitly defined features. On the
other hand, we use syntactic tree kernels to extract
implicit features from constituency and dependency
parse trees over the input sentences and the Machine
Translation (MT) output. The experimental results
confirm the findings of our earlier work, showing
tree kernels to be a valuable tool for rapid prototyp-
ing of QE systems.
2 Features
Our QE systems used two types of features: On
the one hand, we used a set of explicit features that
were extracted from the data before running the Ma-
chine Learning (ML) component. On the other hand,
syntactic parse trees of the MT input and output
sentences provided implicit features that were com-
puted directly by the ML component using tree ker-
nels.
2.1 Explicit features
Both of the QE systems we submitted to the shared
task used the complete set of 17 baseline features
provided by the workshop organisers. Additionally,
the UU best system also contained all the features
presented by Hardmeier (2011) with the exception
109
of a few features specific to the film subtitle genre
and inapplicable to the text type of the shared task,
as well as a small number of features not included
in that work. Many of these features were modelled
on QE features described by Specia et al (2009). In
particular, the following features were included in
addition to the baseline feature set:
? number of words, length ratio (4 features)
? source and target type-token ratios (2 features)
? number of tokens matching particular patterns
(3 features each):
? numbers
? opening and closing parentheses
? strong punctuation signs
? weak punctuation signs
? ellipsis signs
? hyphens
? single and double quotes
? apostrophe-s tokens
? short alphabetic tokens (? 3 letters)
? long alphabetic tokens (? 4 letters)
? source and target language model (LM) and
log-LM scores (4 features)
? LM and log-LM scores normalised by sentence
length (4 features)
? number and percentage of out-of-vocabulary
words (2 features)
? percentage of source 1-, 2-, 3- and 4-grams oc-
curring in the source part of the training corpus
(4 features)
? percentage of source 1-, 2-, 3- and 4-grams in
each frequency quartile of the training corpus
(16 features)
? a binary feature indicating that the output con-
tains more than three times as many alphabetic
tokens as the input (1 feature)
? percentage of unaligned words and words with
1 : 1, 1 : n, n : 1 and m : n alignments (10 fea-
tures)
? average number of translations per word, un-
weighted and weighted by word frequency and
reciprocal word frequency (3 features)
? translation model entropy for the input words,
cumulatively per sentence and averaged per
word, computed based on the SMT lexical
weight model (2 features).
Whenever applicable, features were computed for
both the source and the target language, and addi-
tional features were added to represent the squared
difference of the source and target language feature
values. All feature values were scaled so that their
values ranged between 0 and 1 over the training set.
The total number of features of the UU best sys-
tem amounted to 99. It should be noted, however,
that there is considerable redundancy in the feature
set and that the 82 features of Hardmeier (2011)
overlap with the 17 baseline features to some extent.
We did not make any attempt to reduce feature over-
lap and relied on the learning algorithm for feature
selection.
2.2 Parse trees
Both the English input text and the Spanish Machine
Translations were annotated with syntactic parse
trees from which to derive implicit features. In En-
glish, we were able to produce both constituency and
dependency parses. In Spanish, we were limited to
dependency parses because of the better availability
of parsing models. English constituency parses were
produced with the Stanford parser (Klein and Man-
ning, 2003) using the model bundled with the parser.
For dependency parsing, we used MaltParser (Nivre
et al, 2006). POS tagging was done with HunPOS
(Hala?csy et al, 2007) for English and SVMTool
(Gime?nez and Ma?rquez, 2004) for Spanish, with the
models provided by the OPUS project (Tiedemann,
2009). As in previous work (Hardmeier, 2011), we
treated the parser as a black box and made no at-
tempt to handle the fact that parsing accuracy may
be decreased over malformed SMT output.
To be used with tree kernels, the output of the de-
pendency parser had to be transformed into a sin-
gle tree structure with a unique label per node and
unlabelled edges, similar to a constituency parse
tree. We followed Johansson and Moschitti (2010)
in using a tree representation which encodes part-
of-speech tags, dependency relations and words as
sequences of child nodes (see fig. 1).
110
Figure 1: Representation of the dependency tree fragment
for the words Nicole ?s dad
A tree and some of its Subset Tree Fragments
S 
N 
NP 
D N 
VP 
V Mary 
brought 
a    cat 
NP 
D N 
a    cat 
N 
   cat 
D 
a 
V 
brought 
N 
Mary 
NP 
D N 
VP 
V 
brought 
a    cat 
Fig. 1. A syntactic parse tree with its sub-
trees (STs).
NP 
D N 
a   cat 
NP 
D N 
NP 
D N 
a 
NP 
D N 
NP 
D N 
VP 
V 
brought 
a    cat 
  cat 
NP 
D N 
VP 
V 
a    cat 
NP 
D N 
VP 
V 
N 
   cat 
D 
a 
V 
brought 
N 
Mary 
? 
Fig. 2. A tree with some of its subset trees
(SSTs).
NP 
D N 
VP 
V 
brought 
a    cat 
NP 
D N 
VP 
V 
a    cat 
NP 
D N 
VP 
a    cat 
NP 
D N 
VP 
a 
NP 
D 
VP 
a 
NP 
D 
VP 
NP 
N 
VP 
NP 
N 
NP NP 
D N D 
NP 
? 
VP 
Fig. 3. A tree with some of its partial trees
(PTs).
 
 
 
 
 
is 
What offer 
an plan 
direct      stock   purchase 
Fig. 4. A dependency tree of a question.
constraint over the SSTs, we obtain a more general form of substructures that we
call partial trees (PTs). These can be generated by the application of partial
production rules of the grammar, consequently [VP [V]] and [VP [NP]] are
valid PTs. Figure 3 shows that the number of PTs derived from the same tree as
before is still higher (i.e. 30 PTs). These different substructure numbers provide
an intuitive quantification of the different information levels among the tree-
based representations.
3 Fast Tree Kernel Functions
The main idea of tree kernels is to compute the number of common substructures
between two trees T1 and T2 without explicitly considering the whole fragment
space. We have designed a general function to compute the ST, SST and PT
kernels. Our fast evaluation of the PT kernel is inspired by the efficient evaluation
of non-continuous subsequences (described in [13]). To increase the computation
speed of the above tree kernels, we also apply the pre-selection of node pairs
which have non-null kernel.
3.1 The Partial Tree Kernel
The evaluation of the common PTs rooted in nodes n1 and n2 requires the
selection of the shared child subsets of the two nodes, e.g. [S [DT JJ N]] and
[S [DT N N]] have [S [N]] (2 times) and [S [DT N]] in common. As the order
of the children is important, we can use subsequence kernels for their generation.
More in detail, let F = {f1, f2, .., f|F|} be a tree fragment space of type PTs and
let the indicator function Ii(n) be equal to 1 if the target fi is rooted at node n
and 0 otherwise, we define the PT kernel as:
A tree and some of its Partial Tree Fragments
Figure 2: Tree fragments extracted by the Subset Tree
Kernel and by the Partial Tree Kernel. Illustrations by
Moschitti (2006a).
3 M chine Learning compon nt
3.1 Overview
The QE shared task asked both for an estimate of
a 1?5 quality score for each segment in the test set
and for a ranking of t e sentences according to qual-
ity. We decided to treat score estimation as primary
and address the task as a regression problem. For
the ranking task, we simply submitted the ranking
induced by the regression output, breaking ties ran-
domly.
Our system was based on SVM regression as
implemented by the SVMlight software (Joachims,
1999) with tree kernel extensions (Moschitti,
2006b). Predicted scores less than 1 were set to 1
and predicted scores greater than 5 were set to 5
as this was known to be the range of valid scores.
Our learning algorithm had some free hyperparam-
eters. Three of them were optimised by joint grid
search with 5-fold cross-validation over the training
set: the SVM training error/margin trade-off (C pa-
rameter), one free parameter of the explicit feature
kernel and the ratio between explicit feature and tree
kernels (see below). All other parameters were left
at their default values. Before running it over the
test set, the system was retrained on the complete
training set using the parameters found with cross-
validation.
3.2 Kernels for explicit features
To select a good kernel for our explicit features,
we initially followed the advice given by Hsu et al
(2010), using a Gaussian RBF kernel and optimis-
ing the SVM C parameter and the ? parameter of the
RBF with grid search. While this gave reasonable
results, it turned out that slightly better prediction
could be achieved by using a polynomial kernel, so
we chose to use this kernel for our final submission
and used grid search to tune th degree of the poly-
nomial instead. The improvement over the Gaussian
kernel was, however, marginal.
3.3 Tree kernels
To exploit parse tree information in our Machine
Learning (ML) component, we used tree kernel
functions. Tree kernels (Collins and Duffy, 2001)
are kernel functions defined over pairs of tree struc-
tures. They measure the similarity between two trees
by counting the number of common substructures.
Implicitly, they define an infinite-dimensional fea-
ture space whose dimensions correspond to all pos-
sible tree fragments. Features are thus available to
cover different kinds of abstract node configurations
that can occ r in a tree. The important feature i-
mensions are effectively selected by the SVM train-
ing algorithm through the selection and weighting
of the support vectors. The intuition behind our
use of tree kernels is that they may help us iden-
tify constructions that are difficult to translate in the
source language, and doubtful syntactic structures in
the output language. Note that we do not currently
compare parse trees across languages; tree kernels
111
Cross-validation Test set
Features T C d ? ? MAE RMS ? ? MAE RMS
UU best 99 explicit + TK 0.05 4 2 0.506 0.566 0.550 0.692 0.56 0.62 0.64 0.79
(a) 99 explicit + TK 0.03 8 3 0.502 0.564 0.552 0.700 0.56 0.61 0.63 0.78
(b) 17 explicit + TK 0.05 4 2 0.462 0.530 0.568 0.714 0.57 0.61 0.65 0.79
UU bltk 17 explicit + TK 0.03 8 3 0.466 0.534 0.566 0.712 0.58 0.61 0.64 0.79
(c) 99 explicit 0 8 2 0.492 0.560 0.554 0.700 0.56 0.59 0.65 0.80
(d) 17 explicit 0 8 2 0.422 0.466 0.598 0.748 0.52 0.55 0.70 0.83
(e) TK only ? 4 ? 0.364 0.392 0.632 0.782 0.51 0.51 0.70 0.85
T : Tree kernel weight C: Training error/margin trade-off d: Degree of polynomial kernel
?: DeltaAvg score ?: Spearman rank correlation MAE: Mean Average Error
RMS: Root Mean Square Error TK: Tree kernels
Table 1: Experimental results
are applied to trees of the same type in the same lan-
guage only.
We used two different types of tree kernels for the
different types of parse trees (see fig. 2). The Sub-
set Tree Kernel (Collins and Duffy, 2001) consid-
ers tree fragments consisting of more than one node
with the restriction that if one child of a node is in-
cluded, then all its siblings must be included as well
so that the underlying production rule is completely
represented. This kind of kernel is well suited for
constituency parse trees and was used for the source
language constituency parses. For the dependency
trees, we used the Partial Tree Kernel (Moschitti,
2006a) instead. It extends the Subset Tree Kernel by
permitting also the extraction of tree fragments com-
prising only part of the children of any given node.
Lifting this restriction makes sense for dependency
trees since a node and its children do not correspond
to a grammatical production in a dependency tree in
the same way as they do in a constituency tree (Mos-
chitti, 2006a). It was used for the dependency trees
in the source and in the target language.
The explicit feature kernel and the three tree ker-
nels were combined additively, with a single weight
parameter to balance the sum of the tree kernels
against the explicit feature kernel. This coefficient
was optimised together with the other two hyperpa-
rameters mentioned above. It turned out that best re-
sults could be obtained with a fairly low weight for
the tree kernels, but in the cross-validation experi-
ments adding tree kernels did give an improvement
over not having them at all.
4 Experimental Results
Results for some of our experiments are shown in
table 1. The two systems we submitted to the shared
task are marked with their system identifiers. A few
other systems are included for comparison and are
numbered (a) to (e) for easier reference.
Our system using only the baseline features (d)
performs a bit worse than the reference system of
the shared task organisers. We use the same learn-
ing algorithm, so this seems to indicate that the ker-
nel and the hyperparameters they selected worked
slightly better than our choices. Using only tree
kernels with no explicit features at all (e) creates a
system that works considerably worse under cross-
validation, however we note that its performance on
the test set is very close to that of system (d).
Adding the 82 additional features of Hardmeier
(2011) to the system without tree kernels slightly im-
proves the performance both under cross-validation
and on the test set (c). Adding tree kernels has a
similar effect, which is a bit less pronounced for
the cross-validation setting, but quite comparable on
the test set (UU bltk, b). Finally, combining the
full feature set with tree kernels results in an addi-
tional gain under cross-validation, but unfortunately
the improvement does not carry over to the test set
(UU best, a).
5 Conclusions
In sum, the results confirm the findings made in our
earlier work (Hardmeier, 2011). They show that tree
kernels can be a valuable tool to boost the initial
112
performance of a Quality Estimation system without
spending much effort on feature engineering. Unfor-
tunately, it seems that the gains achieved by tree ker-
nels over simple parse trees and by the additional ex-
plicit features used in our systems do not necessarily
add up. Nevertheless, comparison with other partici-
pating systems shows that either of them is sufficient
for state-of-the-art performance.
References
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Machine
Translation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, Montreal, Canada,
June. Association for Computational Linguistics.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings of NIPS
2001, pages 625?632.
Jesu?s Gime?nez and Llu??s Ma?rquez. 2004. SVMTool: A
general POS tagger generator based on Support Vec-
tor Machines. In Proceedings of the 4th Conference
on International Language Resources and Evaluation
(LREC-2004), Lisbon.
Pe?ter Hala?csy, Andra?s Kornai, and Csaba Oravecz. 2007.
HunPos ? an open source trigram tagger. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics. Companion Volume: Pro-
ceedings of the Demo and Poster Sessions, pages 209?
212, Prague, Czech Republic, June. Association for
Computational Linguistics.
Christian Hardmeier. 2011. Improving machine transla-
tion quality prediction with syntactic tree kernels. In
Mikel L. Forcada, Heidi Depraetere, and Vincent Van-
deghinste, editors, Proceedings of the 15th conference
of the European Association for Machine Translation
(EAMT 2011), pages 233?240, Leuven, Belgium.
Chih-Wei Hsu, Chih-Chung Chang, and Chih-Jen Lin.
2010. A practical guide to support vector classifica-
tion. Technical report, Department of Computer Sci-
ence, National Taiwan University.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Scho?lkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods ? Sup-
port Vector Learning. MIT Press.
Richard Johansson and Alessandro Moschitti. 2010.
Syntactic and semantic structure for opinion expres-
sion detection. In Proceedings of the Fourteenth Con-
ference on Computational Natural Language Learn-
ing, pages 67?76, Uppsala, Sweden, July. Association
for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423?430, Sapporo, Japan, July. As-
sociation for Computational Linguistics.
Alessandro Moschitti. 2006a. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
Proceedings of the 17th European Conference on Ma-
chine Learning, Berlin.
Alessandro Moschitti. 2006b. Making tree kernels prac-
tical for natural language learning. In Proceedings of
the Eleventh International Conference of the European
Association for Computational Linguistics, Trento.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
MaltParser: A language-independent system for data-
driven dependency parsing. In Proceedings of the 5th
Conference on International Language Resources and
Evaluation (LREC-2006), pages 2216?2219, Genoa.
Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran
Wang, and John Shawe-Taylor. 2009. Improving the
confidence of Machine Translation quality estimates.
In Proceedings of MT Summit XII, Ottawa.
Jo?rg Tiedemann. 2009. News from OPUS ? a collection
of multilingual parallel corpora with tools and inter-
face. In N. Nicolov, K. Bontcheva, G. Angelova, and
R. Mitkov, editors, Recent Advances in Natural Lan-
guage Processing, pages 237?248. John Benjamins,
Amsterdam.
113
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 225?231,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Tunable Distortion Limits and Corpus Cleaning for SMT
Sara Stymne Christian Hardmeier Jo?rg Tiedemann Joakim Nivre
Uppsala University
Department of Linguistics and Philology
firstname.lastname@lingfil.uu.se
Abstract
We describe the Uppsala University sys-
tem for WMT13, for English-to-German
translation. We use the Docent decoder,
a local search decoder that translates at
the document level. We add tunable dis-
tortion limits, that is, soft constraints on
the maximum distortion allowed, to Do-
cent. We also investigate cleaning of the
noisy Common Crawl corpus. We show
that we can use alignment-based filtering
for cleaning with good results. Finally we
investigate effects of corpus selection for
recasing.
1 Introduction
In this paper we present the Uppsala University
submission to WMT 2013. We have submitted one
system, for translation from English to German.
In our submission we use the document-level de-
coder Docent (Hardmeier et al, 2012; Hardmeier
et al, 2013). In the current setup, we take advan-
tage of Docent in that we introduce tunable dis-
tortion limits, that is, modeling distortion limits as
soft constraints instead of as hard constraints. In
addition we perform experiments on corpus clean-
ing. We investigate how the noisy Common Crawl
corpus can be cleaned, and suggest an alignment-
based cleaning method, which works well. We
also investigate corpus selection for recasing.
In Section 2 we introduce our decoder, Docent,
followed by a general system description in Sec-
tion 3. In Section 4 we describe our experiments
with corpus cleaning, and in Section 5 we describe
experiments with tunable distortion limits. In Sec-
tion 6 we investigate corpus selection for recasing.
In Section 7 we compare our results with Docent
to results using Moses (Koehn et al, 2007). We
conclude in Section 8.
2 The Docent Decoder
Docent (Hardmeier et al, 2013) is a decoder for
phrase-based SMT (Koehn et al, 2003). It differs
from other publicly available decoders by its use
of a different search algorithm that imposes fewer
restrictions on the feature models that can be im-
plemented.
The most popular decoding algorithm for
phrase-based SMT is the one described by Koehn
et al (2003), which has become known as stack
decoding. It constructs output sentences bit by
bit by appending phrase translations to an initially
empty hypothesis. Complexity is kept in check,
on the one hand, by a beam search approach that
only expands the most promising hypotheses. On
the other hand, a dynamic programming technique
called hypothesis recombination exploits the lo-
cality of the standard feature models, in particu-
lar the n-gram language model, to achieve a loss-
free reduction of the search space. While this de-
coding approach delivers excellent search perfor-
mance at a very reasonable speed, it limits the
information available to the feature models to an
n-gram window similar to a language model his-
tory. In stack decoding, it is difficult to implement
models with sentence-internal long-range depen-
dencies and cross-sentence dependencies, where
the model score of a given sentence depends on
the translations generated for another sentence.
In contrast to this very popular stack decod-
ing approach, our decoder Docent implements a
search procedure based on local search (Hard-
meier et al, 2012). At any stage of the search pro-
cess, its search state consists of a complete docu-
ment translation, making it easy for feature mod-
els to access the complete document with its cur-
rent translation at any point in time. The search
algorithm is a stochastic variant of standard hill
climbing. At each step, it generates a successor
of the current search state by randomly applying
225
one of a set of state changing operations to a ran-
dom location in the document. If the new state
has a better score than the previous one, it is ac-
cepted, else search continues from the previous
state. The operations are designed in such a way
that every state in the search space can be reached
from every other state through a sequence of state
operations. In the standard setup we use three op-
erations: change-phrase-translation replaces the
translation of a single phrase with another option
from the phrase table, resegment alters the phrase
segmentation of a sequence of phrases, and swap-
phrases alters the output word order by exchang-
ing two phrases.
In contrast to stack decoding, the search algo-
rithm in Docent leaves model developers much
greater freedom in the design of their feature func-
tions because it gives them access to the transla-
tion of the complete document. On the downside,
there is an increased risk of search errors because
the document-level hill-climbing decoder cannot
make as strong assumptions about the problem
structure as the stack decoder does. In prac-
tice, this drawback can be mitigated by initializing
the hill-climber with the output of a stack decod-
ing pass using the baseline set of models without
document-level features (Hardmeier et al, 2012).
Since its inception, Docent has been used to ex-
periment with document-level semantic language
models (Hardmeier et al, 2012) and models to
enhance text readability (Stymne et al, 2013b).
Work on other discourse phenomena is ongoing.
In the present paper, we focus on sentence-internal
reordering by exploiting the fact that Docent im-
plements distortion limits as soft constraints rather
than strictly enforced limitations. We do not in-
clude any of our document-level feature functions.
3 System Setup
In this section we will describe our basic system
setup. We used all corpora made available for
English?German by the WMT13 workshop. We
always concatenated the two bilingual corpora Eu-
roparl and News Commentary, which we will call
EP-NC. We pre-processed all corpora by using
the tools provided for tokenization and we also
lower-cased all corpora. For the bilingual corpora
we also filtered sentence pairs with a length ra-
tio larger than three, or where either sentence was
longer than 60 tokens. Recasing was performed as
a post-processing step, trained using the resources
in the Moses toolkit (Koehn et al, 2007).
For the language model we trained two sepa-
rate models, one on the German side of EP-NC,
and one on the monolingual News corpus. In
both cases we trained 5-gram models. For the
large News corpus we used entropy-based prun-
ing, with 10?8 as a threshold (Stolcke, 1998). The
language models were trained using the SRILM
toolkit (Stolcke, 2002) and during decoding we
used the KenLM toolkit (Heafield, 2011).
For the translation model we also trained two
models, one with EP-NC, and one with Common
Crawl. These two models were interpolated and
used as a single model at decoding time, based on
perplexity minimization interpolation (Sennrich,
2012), see details in Section 4. The transla-
tion models were trained using the Moses toolkit
(Koehn et al, 2007), with standard settings with
5 features, phrase probabilities and lexical weight-
ing in both directions and a phrase penalty. We ap-
plied significance-based filtering (Johnson et al,
2007) to the resulting phrase tables. For decod-
ing we used the Docent decoder with random ini-
tialization and standard parameter settings (Hard-
meier et al, 2012; Hardmeier et al, 2013), which
beside translation and language model features in-
clude a word penalty and a distortion penalty.
Parameter optimization was performed using
MERT (Och, 2003) at the document-level (Stymne
et al, 2013a). In this setup we calculate both
model and metric scores on the document-level
instead of on the sentence-level. We produce k-
best lists by sampling from the decoder. In each
optimization run we run 40,000 hill-climbing it-
erations of the decoder, and sample translations
with interval 100, from iteration 10,000. This
procedure has been shown to give competitive re-
sults to standard tuning with Moses (Koehn et
al., 2007) with relatively stable results (Stymne
et al, 2013a). For tuning data we concate-
nated the tuning sets news-test 2008?2010 and
newssyscomb2009, to get a higher number of doc-
uments. In this set there are 319 documents and
7434 sentences.
To evaluate our system we use newstest2012,
which has 99 documents and 3003 sentences. In
this article we give lower-case Bleu scores (Pap-
ineni et al, 2002), except in Section 6 where we
investigate the effect of different recasing models.
226
Cleaning Sentences Reduction
None 2,399,123
Basic 2,271,912 5.3%
Langid 2,072,294 8.8%
Alignment-based 1,512,401 27.0%
Table 1: Size of Common Crawl after the different
cleaning steps and reduction in size compared to
the previous step
4 Cleaning of Common Crawl
The Common Crawl (CC) corpus was collected
from web sources, and was made available for the
WMT13 workshop. It is noisy, with many sen-
tences with the wrong language and also many
non-corresponding sentence pairs. To make better
use of this resource we investigated two methods
for cleaning it, by making use of language identi-
fication and alignment-based filtering. Before any
other cleaning we performed basic filtering where
we only kept pairs where both sentences had at
most 60 words, and with a length ratio of maxi-
mum 3. This led to a 5.3% reduction of sentences,
as shown in Table 1.
Language Identification For language identifi-
cation we used the off-the-shelf tool langid.py (Lui
and Baldwin, 2012). It is a python library, cover-
ing 97 languages, including English and German,
trained on data drawn from five different domains.
It uses a naive Bayes classifier with a multino-
mial event model, over a mixture of byte n-grams.
As for many language identification packages it
works best for longer texts, but Lui and Bald-
win (2012) also showed that it has good perfor-
mance for short microblog texts, with an accuracy
of 0.89?0.94.
We applied langid.py for each sentence in the
CC corpus, and kept only those sentence pairs
where the correct language was identified for both
sentences with a confidence of at least 0.999. The
total number of sentences was reduced by a further
8.8% based on the langid filtering.
We performed an analysis on a set of 1000 sen-
tence pairs. Among the 907 sentences that were
kept in this set we did not find any cases with
the wrong language. Table 2 shows an analysis
of the 93 sentences that were removed from this
test set. The overall accuracy of langid.py is much
higher than indicated in the table, however, since
it does not include the correctly identified English
and German sentences. We grouped the removed
sentences into four categories, cases where both
languages were correctly identified, but under the
confidence threshold of 0.999, cases where both
languages were incorrectly identified, and cases
where one language was incorrectly identified.
Overall the language identification was accurate
on 54 of the 93 removed sentences. In 18 of the
cases where it was wrong, the sentences were not
translation correspondents, which means that we
only wrongly removed 21 out of 1000 sentences.
It was also often the case when the language was
wrongly identified, that large parts of the sentence
consisted of place names, such as ?Forums about
Conil de la Frontera - Ca?diz.? ? ?Foren u?ber Conil
de la Frontera - Ca?diz.?, which were identified as
es/ht instead of en/de. Even though such sentence
pairs do correspond, they do not contain much use-
ful translation material.
Alignment-Based Cleaning For the alignment-
based cleaning, we aligned the data from the pre-
vious step using GIZA++ (Och and Ney, 2003)
in both directions, and used the intersection of
the alignments. The intersection of alignments is
more sparse than the standard SMT symmetriza-
tion heuristics, like grow-diag-final-and (Koehn et
al., 2005). Our hypothesis was that sentence pairs
with very few alignment points in the intersection
would likely not be corresponding sentences.
We used two types of filtering thresholds based
on alignment points. The first threshold is for the
ratio of the number of alignment points and the
maximum sentence length. The second threshold
is the absolute number of alignment points in a
sentence pair. In addition we used a third thresh-
old based on the length ratio of the sentences.
To find good values for the filtering thresholds,
we created a small gold standard where we man-
ually annotated 100 sentence pairs as being cor-
responding or not. In this set the sentence pairs
did not match in 33 cases. Table 3 show results for
some different values for the threshold parameters.
Overall we are able to get a very high precision
on the task of removing non-corresponding sen-
tences, which means that most sentences that are
removed based on this cleaning are actually non-
corresponding sentences. The recall is a bit lower,
indicating that there are still non-corresponding
sentences left in our data. In our translation sys-
tem we used the bold values in Table 3, since it
gave high precision with reasonable recall for the
removal of non-corresponding sentences, meaning
227
Identification Total Wrong lang. Non-corr Corr Languages identified
English and German < 0.999 15 0 7 8
Both English and German wrong 6 2 2 2 2:na/es, 2:et/et, 1: es/an, 1:es/ht
English wrong 13 1 6 6 5: es 4: fr 1: br, it, de, eo
German wrong 59 51 3 5 51: en 3: es 2:nl 1: af, la, lb
Total 93 54 18 21
Table 2: Reasons and correctness for removing sentences based on language ID for 93 sentences out of
a 1000 sentence subset, divided into wrong lang(uage), non-corr(esponding) pairs, and corr(esponding)
pairs.
Ratio align Min align Ratio length Prec. Recall F Kept
0.1 4 2 0.70 0.77 0.73 70%
0.28 4 2 0.94 0.72 0.82 57%
0.42 4 2 1.00 0.56 0.72 41%
0.28 2 2 0.91 0.73 0.81 59%
0.28 6 2 0.94 0.63 0.76 51%
0.28 4 1.5 0.94 0.65 0.77 52%
0.28 4 3 0.91 0.75 0.82 60%
Table 3: Results of alignment-based cleaning for different values of the filtering parameters, with pre-
cision, recall and F-score for the identification of erroneous sentence pairs and the percentage of kept
sentence pairs
that we kept most correctly aligned sentence pairs.
This cleaning method is more aggressive than
the other cleaning methods we described. For the
gold standard only 57% of sentences were kept,
but in the full training set it was a bit higher, 73%,
as shown in Table 1.
Phrase Table Interpolation To use the CC cor-
pus in our system we first trained a separate phrase
table which we then interpolated with the phrase
table trained on EP-NC. In this way we could al-
ways run the system with a single phrase table. For
interpolation, we used the perplexity minimization
for weighted counts method by Sennrich (2012).
Each of the four weights in the phrase table, back-
ward and forward phrase translation probabilities
and lexical weights, are optimized separately. This
method minimizes the cross-entropy based on a
held-out corpus, for which we used the concate-
nation of all available News development sets.
The cross-entropy and the contribution of CC
relative to EP-NC, are shown for phrase transla-
tion probabilities in both directions in Table 4. The
numbers for lexical weights show similar trends.
For each cleaning step the cross-entropy is re-
duced and the contribution of CC is increased. The
difference between the basic cleaning and langid is
very small, however. The alignment-based clean-
ing shows a much larger effect. After that cleaning
step the CC corpus has a similar contribution to
EP-NC. This is an indicator that the final cleaned
CC corpus fits the development set well.
p(S|T ) p(T |S)
Cleaning CE IP CE IP
Basic 3.18 0.12 3.31 0.06
Langid 3.17 0.13 3.29 0.07
Alignment-based 3.02 0.47 3.17 0.61
Table 4: Cross-entropy (CE) and relative interpo-
lation weights (IP) compared to EP-NC for the
Common Crawl corpus, with different cleaning
Results In Table 5 we show the translation re-
sults with the different types of cleaning of CC,
and without it. We show results of different corpus
combinations both during tuning and testing. We
see that we get the overall best result by both tun-
ing and testing with the alignment-based cleaning
of CC, but it is not as useful to do the extra clean-
ing if we do not tune with it as well. Overall we
get the best results when tuning is performed in-
cluding a cleaned version of CC. This setup gives
a large improvement compared to not using CC at
all, or to use it with only basic cleaning. There is
little difference in Bleu scores when testing with
either basic cleaning, or cleaning based on lan-
guage ID, with a given tuning, which is not sur-
prising given their small and similar interpolation
weights. Tuning was, however, not successful
when using CC with basic cleaning.
Overall we think that alignment-based corpus
cleaning worked well. It reduced the size of the
corpus by over 25%, improved the cross-entropy
for interpolation with the EP-NC phrase-table, and
228
Testing
Tuning not used basic langid alignment
not used 14.0 13.9 13.9 14.0
basic 14.2 14.5 14.3 14.3
langid 15.2 15.3 15.3 15.3
alignment 12.7 15.3 15.3 15.7
Table 5: Bleu scores with different types of clean-
ing and without Common Crawl
gave an improvement on the translation task. We
still think that there is potential for further improv-
ing this filtering and to annotate larger test sets to
investigate the effects in more detail.
5 Tunable Distortion Limits
The Docent decoder uses a hill-climbing search
and can perform operations anywhere in the sen-
tence. Thus, it does not need to enforce a strict
distortion limit. In the Docent implementation, the
distortion limit is actually implemented as a fea-
ture, which is normally given a very large weight,
which effectively means that it works as a hard
constraint. This could easily be relaxed, however,
and in this work we investigate the effects of using
soft distortion limits, which can be optimized dur-
ing tuning, like other features. In this way long-
distance movements can be allowed when they are
useful, instead of prohibiting them completely. A
drawback of using no or soft distortion limits is
that it increases the search space.
In this work we mostly experiment with variants
of one or two standard distortion limits, but with a
tunable weight. We also tried to use separate soft
distortion limits for left- and right-movement. Ta-
ble 6 show the results with different types of dis-
tortion limits. The system with a standard fixed
distortion limits of 6 has a somewhat lower score
than most of the systems with no or soft distortion
limits. In most cases the scores are similar, and
we see no clear affects of allowing tunable lim-
its over allowing unlimited distortion. The system
that uses two mono-directional limits of 6 and 10
has slightly higher scores than the other systems,
and is used in our final submission.
One possible reason for the lack of effect of al-
lowing more distortion could be that it rarely hap-
pens that an operator is chosen that performs such
distortion, when we use the standard Docent set-
tings. To investigate this, we varied the settings of
the parameters that guide the swap-phrases opera-
tor, and used the move-phrases operator instead of
swap-phrases. None of these changes led to any
DL type Limit Bleu
No DL ? 15.5
Hard DL 6 15.0
One soft DL 6 15.5
8 14.2
10 15.5
Two soft DLs 4,8 15.5
6,10 15.7
Bidirectional soft DLs 6,10 15.5
Table 6: Bleu scores for different distortion limit
(DL) settings
improvements, however.
While we saw no clear effects when using tun-
able distortion limits, we plan to extend this work
in the future to model movement differently based
on parts of speech. For the English?German lan-
guage pair, for instance, it would be reasonable to
allow long distance moves of verb groups with no
or little cost, but use a hard limit or a high cost for
other parts of speech.
6 Corpus Selection for Recasing
In this section we investigate the effect of using
different corpus combinations for recasing. We
lower-cased our training corpus, which means that
we need a full recasing step as post-processing.
This is performed by training a SMT system on
lower-cased and true-cased target language. We
used the Moses toolkit to train the recasing system
and to decode during recasing. We investigate the
effect of using different combinations of the avail-
able training corpora to train the recasing model.
Table 7 show case sensitive Bleu scores, which
can be compared to the previous case-insensitive
scores of 15.7. We see that there is a larger effect
of including more data in the language model than
in the translation model. There is a performance
jump both when adding CC data and when adding
News data to the language model. The results
are best when we include the News data, which
is not included in the English?German translation
model, but which is much larger than the other cor-
pora. There is no further gain by using News in
combination with other corpora compared to using
only News. When adding more data to the trans-
lation model there is only a minor effect, with the
difference between only using EP-NC and using
all available corpora is at most 0.2 Bleu points.
In our submitted system we use the monolingual
News corpus both in the LM and the TM.
There are other options for how to treat recas-
229
Language model
TM EP-NC EP-NC-CC News EP-NC-News EP-NC-CC-News
EP-NC 13.8 14.4 14.8 14.8 14.8
EP-NC-CC 13.9 14.5 14.9 14.8 14.8
News 13.9 14.5 14.9 14.9 14.9
EP-NC-News 13.9 14.5 14.9 14.9 14.9
EP-NC-CC-News 13.9 14.5 14.9 14.9 15.0
Table 7: Case-sensitive Bleu scores with different corpus combinations for the language model and
translation model (TM) for recasing
ing. It is common to train the system on true-
cased data instead of lower-cased data, which has
been shown to lead to small gains for the English?
German language pair (Koehn et al, 2008). In this
framework there is still a need to find the correct
case for the first word of each sentence, for which
a similar corpus study might be useful.
7 Comparison to Moses
So far we have only shown results using the Do-
cent decoder on its own, with a random initializa-
tion, since we wanted to submit a Docent-only sys-
tem for the shared task. In this section we also
show contrastive results with Moses, and for Do-
cent initialized with stack decoding, using Moses,
and for different type of tuning.
Previous research have shown mixed results for
the effect of initializing Docent with and with-
out stack decoding, when using the same feature
sets. In Hardmeier et al (2012) there was a drop
of about 1 Bleu point for English?French trans-
lation based on WMT11 data when random ini-
tialization was used. In Stymne et al (2013a),
on the other hand, Docent gave very similar re-
sults with both types of initialization for German?
English WMT13 data. The latter setup is similar
to ours, except that no Common Crawl data was
used.
The results with our setup are shown in Ta-
ble 8. In this case we lose around a Bleu point
when using Docent on its own, without Moses ini-
tialization. We also see that the results are lower
when using Moses with the Docent tuning method,
or when combining Moses and Docent with Do-
cent tuning. This indicates that the document-
level tuning has not given satisfactory results in
this scenario, contrary to the results in Stymne et
al. (2013a), which we plan to explore further in
future work. Overall we think it is important to
develop stronger context-sensitive models for Do-
cent, which can take advantage of the document
context.
Test system Tuning system Bleu
Docent (random) Docent 15.7
Docent (stack) Docent 15.9
Moses Docent 15.9
Docent (random) Moses 15.9
Docent (stack) Moses 16.8
Moses Moses 16.8
Table 8: Bleu scores for Docent initialized ran-
domly or with stack decoding compared to Moses.
Tuning is performed with either Moses or Docent.
For the top line we used tunable distortion lim-
its 6,10 with Docent, in the other cases a standard
hard distortion limit of 6, since Moses does not al-
low soft distortion limits.
8 Conclusion
We have presented the Uppsala University system
for WMT 2013. Our submitted system uses Do-
cent with random initialization and two tunable
distortion limits of 6 and 10. It is trained with the
Common Crawl corpus, cleaned using language
identification and alignment-based filtering. For
recasing we used the monolingual News corpora.
For corpus-cleaning, we present a novel method
for cleaning noisy corpora based on the number
and ratio of word alignment links for sentence
pairs, which leads to a large reduction of corpus
size, and to small improvements on the transla-
tion task. We also experiment with tunable dis-
tortion limits, which do not lead to any consistent
improvements at this stage.
In the current setup the search algorithm of
Docent is not strong enough to compete with
the effective search in standard decoders like
Moses. We are, however, working on developing
discourse-aware models that can take advantage of
the document-level context, which is available in
Docent. We also need to further investigate tuning
methods for Docent.
230
References
Christian Hardmeier, Joakim Nivre, and Jo?rg Tiede-
mann. 2012. Document-wide decoding for phrase-
based statistical machine translation. In Proceed-
ings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1179?1190, Jeju Island, Korea.
Christian Hardmeier, Sara Stymne, Jo?rg Tiedemann,
and Joakim Nivre. 2013. Docent: A document-level
decoder for phrase-based statistical machine transla-
tion. In Proceedings of the 51st Annual Meeting of
the ACL, Demonstration session, Sofia, Bulgaria.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197, Edinburgh, Scotland.
Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving translation qual-
ity by discarding most of the phrasetable. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 967?
975, Prague, Czech Republic.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Human Language Technology
Conference of the NAACL, pages 48?54, Edmonton,
Alberta, Canada.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system descrip-
tion for the 2005 IWSLT speech translation evalu-
ation. In Proceedings of the International Workshop
on Spoken Language Translation, Pittsburgh, Penn-
sylvania, USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL,
Demo and Poster Sessions, pages 177?180, Prague,
Czech Republic.
Philipp Koehn, Abhishek Arun, and Hieu Hoang.
2008. Towards better machine translation quality for
the German-English language pairs. In Proceedings
of the Third Workshop on Statistical Machine Trans-
lation, pages 139?142, Columbus, Ohio, USA.
Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In Pro-
ceedings of the 50th Annual Meeting of the ACL,
System Demonstrations, pages 25?30, Jeju Island,
Korea.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 42nd Annual Meeting of the ACL, pages 160?
167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the ACL, pages 311?
318, Philadelphia, Pennsylvania, USA.
Rico Sennrich. 2012. Perplexity minimization for
translation model domain adaptation in statistical
machine translation. In Proceedings of the 16th
Annual Conference of the European Association
for Machine Translation, pages 539?549, Avignon,
France.
Andreas Stolcke. 1998. Entropy-based pruning of
backoff language models. In Proceedings of the
DARPA Broadcast News Transcription and Under-
standing Workshop, pages 270?274, Landsdowne,
Virginia, USA.
Andreas Stolcke. 2002. SRILM ? an extensible
language modeling toolkit. In Proceedings of the
Seventh International Conference on Spoken Lan-
guage Processing, pages 901?904, Denver, Col-
orado, USA.
Sara Stymne, Christian Hardmeier, Jo?rg Tiedemann,
and Joakim Nivre. 2013a. Feature weight opti-
mization for discourse-level SMT. In Proceedings
of the ACL 2013 Workshop on Discourse in Machine
Translation (DiscoMT 2013), Sofia, Bulgaria.
Sara Stymne, Jo?rg Tiedemann, Christian Hardmeier,
and Joakim Nivre. 2013b. Statistical machine trans-
lation with readability constraints. In Proceedings
of the 19th Nordic Conference on Computational
Linguistics (NODALIDA?13), pages 375?386, Oslo,
Norway.
231
Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 60?69,
Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational Linguistics
Feature Weight Optimization for Discourse-Level SMT
Sara Stymne, Christian Hardmeier, Jo?rg Tiedemann and Joakim Nivre
Uppsala University
Department of Linguistics and Philology
Box 635, 751 26 Uppsala, Sweden
firstname.lastname@lingfil.uu.se
Abstract
We present an approach to feature weight
optimization for document-level decoding.
This is an essential task for enabling future
development of discourse-level statistical
machine translation, as it allows easy inte-
gration of discourse features in the decod-
ing process. We extend the framework of
sentence-level feature weight optimization
to the document-level. We show experi-
mentally that we can get competitive and
relatively stable results when using a stan-
dard set of features, and that this frame-
work also allows us to optimize document-
level features, which can be used to model
discourse phenomena.
1 Introduction
Discourse has largely been ignored in traditional
machine translation (MT). Typically each sentence
has been translated in isolation, essentially yield-
ing translations that are bags of sentences. It is
well known from translation studies, however, that
discourse is important in order to achieve good
translations of documents (Hatim and Mason,
1990). Most attempts to address discourse-level
issues for statistical machine translation (SMT)
have had to resort to solutions such as post-
processing to address lexical cohesion (Carpuat,
2009) or two-step translation to address pronoun
anaphora (Le Nagard and Koehn, 2010). Recently,
however, we presented Docent (Hardmeier et al,
2012; Hardmeier et al, 2013), a decoder based
on local search that translates full documents. So
far this decoder has not included a feature weight
optimization framework. However, feature weight
optimization, or tuning, is important for any mod-
ern SMT decoder to achieve a good translation
performance.
In previous research with Docent, we used grid
search to find weights for document-level features
while base features were optimized using stan-
dard sentence-level techniques. This approach is
impractical since many values for the extra fea-
tures have to be tried, and, more importantly, it
might not give the same level of performance as
jointly optimizing all parameters. Principled fea-
ture weight optimization is thus essential for re-
searchers that want to use document-level features
to model discourse phenomena such as anaphora,
discourse connectives, and lexical consistency. In
this paper, we therefore propose an approach that
supports discourse-wide features in document-
level decoding by adapting existing frameworks
for sentence-level optimization. Furthermore, we
include a thorough empirical investigation of this
approach.
2 Discourse-Level SMT
Traditional SMT systems translate texts sentence
by sentence, assuming independence between sen-
tences. This assumption allows efficient algo-
rithms based on dynamic programming for explor-
ing a large search space (Och et al, 2001). Be-
cause of the dynamic programming assumptions it
is hard to directly include discourse-level features
into a traditional SMT decoder. Nevertheless,
there have been several attempts to integrate inter-
sentential and long distance models for discourse-
level phenomena into standard decoders, usually
as ad-hoc additions to standard models, address-
ing a single phenomenon.
Several studies have tried to improve pro-
noun anaphora by adding information about the
antecedent, either by using two-step decoding
(Le Nagard and Koehn, 2010; Guillou, 2012) or
by extracting information from previously trans-
lated sentences (Hardmeier and Federico, 2010),
unfortunately without any convincing results. To
address the translation of discourse connectives,
source-side pre-processing has been used to anno-
tate surface forms either in the corpus or in the
60
phrase-table (Meyer and Popescu-Belis, 2012) or
by using factored decoding (Meyer et al, 2012)
to disambiguate connectives, with small improve-
ments. Lexical consistency has been addressed
by the use of post-processing (Carpuat, 2009),
multi-pass decoding (Xiao et al, 2011; Ture et al,
2012), and cache models (Tiedemann, 2010; Gong
et al, 2011). Gong et al (2012) addressed the
issue of tense selection for translation from Chi-
nese, by the use of inter-sentential tense n-grams,
exploiting information from previously translated
sentences. Another way to use a larger context
is by integrating word sense disambiguation and
SMT. This has been done by re-initializing phrase
probabilities for each sentence (Carpuat and Wu,
2007), by introducing extra features in the phrase-
table (Chan et al, 2007), or as a k-best re-ranking
task (Specia et al, 2008). Another type of ap-
proach is to integrate topic modeling into phrase
tables (Zhao and Xing, 2010; Su et al, 2012). For
a more thorough overview of discourse in SMT,
see Hardmeier (2012).
Here we instead choose to work with the re-
cent document-level SMT decoder Docent (Hard-
meier et al, 2012). Unlike in traditional decod-
ing were documents are generated sentence by
sentence, feature models in Docent always have
access to the complete discourse context, even
before decoding is finished. It implements the
phrase-based SMT approach (Koehn et al, 2003)
and is based on local search, where a state con-
sists of a full translation of a document, which is
improved by applying a series of operations to im-
prove the translation. A hill-climbing strategy is
used to find a (local) maximum. The operations
allow changing the translation of a phrase, chang-
ing the word order by swapping the positions of
two phrases, and resegmenting phrases. The initial
state can either be initialized randomly in mono-
tonic order, or be based on an initial run from a
standard sentence-based decoder. The number of
iterations in the decoder is controlled by two pa-
rameters, the maximum number of iterations and
a rejection limit, which stops the decoder if no
change was made in a certain number of iterations.
This setup is not limited by dynamic programming
constraints, and enables the use of the translated
target document to extract features. It is thus easy
to directly integrate discourse-level features into
Docent. While we use this specific decoder in our
experiments, the method proposed for document-
level feature weight optimization is not limited to
it. It can be used with any decoder that outputs
feature values at the document level.
3 Sentence-Level Tuning
Traditionally, feature weight optimization, or tun-
ing, for SMT is performed by an iterative process
where a development set is translated to produce a
k-best list. The parameters are then optimized us-
ing some procedure, generally to favor translations
in the k-best list that have a high score on some
MT metric. The translation step is then repeated
using the new weights for decoding, and optimiza-
tion is continued on a new k-best list, or on a com-
bination of all k-best lists. This is repeated until
some end condition is satisfied, for instance for a
set number of iterations, until there is only very
small changes in parameter weights, or until there
are no new translations in the k-best lists.
SMT tuning is a hard problem in general, partly
because the correct output is unreachable and
also because the translation process includes la-
tent variables, which means that many efficient
standard optimization procedures cannot be used
(Gimpel and Smith, 2012). Nevertheless, there
are a number of techniques including MERT (Och,
2003), MIRA (Chiang et al, 2008; Cherry and
Foster, 2012), PRO (Hopkins and May, 2011),
and Rampion (Gimpel and Smith, 2012). All of
these optimization methods can be plugged into
the standard optimization loop. All of the meth-
ods work relatively well in practice, even though
there are limitations, for instance that many meth-
ods are non-deterministic meaning that their re-
sults are somewhat unstable. However, there are
some important differences. MERT is based on
scores for the full test set, whereas the other meth-
ods are based on sentence-level scores. MERT
also has the drawback that it only works well for
small sets of features. In this paper we are not
concerned with the actual optimization algorithm
and its properties, though, but instead we focus
on the integration of document-level decoding into
the existing optimization frameworks.
In order to adapt sentence-level frameworks to
our needs we need to address the granularity of
scoring and the process of extracting k-best lists.
For document-level features we do not have mean-
ingful scores on the sentence level which are re-
quired in standard optimization frameworks. Fur-
thermore, the extraction of k-best lists is not as
61
Input: inputDocs, refDocs, init weights ?0, max decoder iters max, sample start ss, sample interval si,
Output: learned weights ?
1: ? ? ?0
2: Initialize empty klist
3: run? 1
4: repeat
5: Initialize empty klistrun
6: for doc? 1, inputDocs.size do Initialize decoder state randomly for inputDocs[doc]
7: for iter? 1,max do
8: Perform one hill-climbing step for inputDocs[doc]
9: if iter >= ss & iter mod si == 0 then
10: Add translation for inputDocs[doc] to klistrun
11: end if
12: end for
13: end for
14: Merge klistrun with klist
15: modelScoresdoc ? ComputeModelScores(klist)
16: metricStatsdoc ? ComputeMetricStats(klist, refDocs)
17: ?run ? ?
18: ? ? Optimize(?run,modelScoresdoc,metricStatsdoc)
19: run? run + 1
20: until Done(run, ?, ?run)
Figure 1: Document-level feature weight optimization algorithm
straightforward in our hill-climbing decoder as in
standard sentence-level decoders such as Moses
(Koehn et al, 2007) where such a list can be ap-
proximated easily from the internal beam search
strategy. Working on output lattices is another op-
tion in standard approaches (Cherry and Foster,
2012) which is also not applicable in our case.
In the following section we describe how we
can address these issues in order to adapt sentence-
level frameworks for our purposes.
4 Document-Level Tuning
To allow document-level feature weight optimiza-
tion, we make some small changes to the sentence-
level framework. Figure 1 shows the algorithm we
use. It assumes access to an optimization algo-
rithm, Optimize, and an end criterion, Done.
The changes from standard sentence-level opti-
mization is that we compute scores on the docu-
ment level, and that we sample translations instead
of using standard k-best lists.
The main challenge is that we need meaning-
ful scores which we do not have at the sentence
level in document decoding. We handle this by
simply computing all scores (model scores and
metric scores) exclusively at the document level.
Remember that all standard MT metrics based on
sentence-level comparisons with reference trans-
lations can be aggregated for a complete test set.
Here we do the same for all sentences in a given
document. This can actually be an advantage com-
pared to optimization methods that use sentence-
level scores, which are known to be unreliable
(Callison-Burch et al, 2012). Document-level
scores should thus be more stable, since they are
based on more data. A potential drawback is that
we get fewer data points with a test set of the same
size, which might mean that we need more data to
achieve as good results as with sentence-level op-
timization. We will see the ability of our approach
to optimize weights with reasonable data sets in
our experiments further down.
The second problem, the extraction of k-best
lists can be addressed in several ways. It is pos-
sible to get a k-best list from Docent by extract-
ing the results from the last k iterations. However,
since Docent operates on the document-level and
does not accept updates in each iteration, there will
be many identical and/or very similar hypotheses
with such an approach. Another option would be
to extract the translations from the k last differ-
ent iterations, which would require some small
changes to the decoder. Instead, we opt to use k-
lists, lists of translations sampled with some inter-
val, which contains k translations, but not neces-
sarily all the k best translations that could be found
by the decoder. A k-best list is of course a k-list,
which we get with a sample interval of 1.
We also choose to restart Docent randomly in
each optimization iteration, since it allows us to
explore a larger part of the search space. We
empirically found that this strategy worked better
than restarting the decoder from the previous best
state.
62
German?English English?Swedish
Type Sentences Documents Type Sentences Documents
Training
Europarl 1.9M ? Europarl 1.5M ?
News Commentary 178K ? ? ? ?
Tuning
News2009 2525 111 Europarl (Moses) 2000 ?
News2008-2010 7567 345 Europarl (Docent) 1338 100
Test News2012 3003 99 Europarl 690 20
Table 1: Domain and number of sentences and documents for the corpora
As seen in Figure 1, there are some additional
parameters in our procedure: the sample start iter-
ation and the sample interval. We also need to set
the number of decoder iterations to run. In Sec-
tion 5 we empirically investigate the effect of these
parameters.
Compared to sentence-level optimization, we
also have a smaller number of units to get scores
from, since we use documents as units, and not
sentences. The importance of this depends on the
optimization algorithm. MERT calculates metric
scores over the full tuning set, not for individual
sentences, and should not be affected too much
by the change in granularity. Many other opti-
mization algorithms, like PRO, work on the sen-
tence level, and will likely be more affected by
the reduction of units. In this work we focus on
MERT, which is the most commonly used opti-
mization procedure in the SMT community, and
which tends to work quite well with relatively few
features. However, we also show contrastive re-
sults for PRO (Hopkins and May, 2011). A fur-
ther issue is that Docent is non-deterministic, i.e.,
it can give different results with the same param-
eter weights. Since the optimization process is al-
ready somewhat unstable this is a potential issue
that needs to be explored further, which we do in
Section 5.
Implementation-wise we adapted Docent to out-
put k-lists and adapted the infrastructure available
for tuning in the Moses decoder (Koehn et al,
2007) to work with document-level scores. This
setup allows us to use the variety of optimization
procedures implemented there.
5 Experiments
In this section we report experimental results
where we investigate several issues in connec-
tion with document-level feature weight optimiza-
tion for SMT. We first describe the experimental
setup, followed by baseline results using sentence-
level optimization. We then present validation ex-
periments with standard sentence-level features,
which can be compared to standard optimization.
Finally, we report results with a set of document-
level features that have been proposed for joint
translation and text simplification (Stymne et al,
2013).
5.1 Experimental Setup
Most of our experiments are for German-to-
English news translation using data from the
WMT13 workshop.1 We also show results with
document-level features for English-to-Swedish
Europarl (Koehn, 2005). The size of the training,
tuning, and test sets are shown in Table 1. First of
all, we need to extract documents for tuning and
testing with Docent. Fortunately, the news data al-
ready contain document markup, corresponding to
individual news articles. For Europarl we define a
document as a consecutive sequence of utterances
from a single speaker. To investigate the effect of
the size of the tuning set, we used different subsets
of the available tuning data.
All our document-level experiments are car-
ried out with Docent but we also contrast with
the Moses decoder (Koehn et al, 2007). For the
purpose of comparison, we use a standard set of
sentence-level features used in Moses in most of
our experiments: five translation model features,
one language model feature, a distance-based re-
ordering penalty, and a word count feature. For
feature weight optimization we also apply the
standard settings in the Moses toolkit. We opti-
mize towards the Bleu metric, and optimization
ends either when no weights are changed by more
than 0.00001, or after 25 iterations. MERT is used
unless otherwise noted.
Except for one of our baselines, we always run
Docent with random initialization. For test we run
the document decoder for a maximum of 227 iter-
ations with a rejection limit of 100,000. In our
experiments, the decoder always stopped when
reaching the rejection limit, usually between 1?5
1http://www.statmt.org/wmt13/
translation-task.html
63
million iterations.
We show results on the Bleu (Papineni et al,
2002) and NIST (Doddington, 2002) metrics. For
German?English we show the average result and
standard deviation of three optimization runs, to
control for optimizer instability as proposed by
Clark et al (2011). For English?Swedish we re-
port results on single optimization runs, due to
time constraints.
5.2 Baselines
Most importantly, we would like to show the effec-
tiveness of the document-level tuning procedure
described above. In order to do this, we created
a baseline using sentence-level optimization with
a tuning set of 2525 sentences and the News2009
corpus for evaluation. Increasing the tuning set is
known to give only modest improvements (Turchi
et al, 2012; Koehn and Haddow, 2012).
The feature weights optimized with the stan-
dard Moses decoder can then directly be used in
our document-level decoder as we only include
sentence-level features in our baseline model. As
expected, these optimized weights also lead to
a better performance in document-level decoding
compared to an untuned model as shown in Ta-
ble 2. Note, that Docent can be initialized in
two ways, by Moses and randomly. Not surpris-
ingly, the result for the runs initialized with Moses
are identical with the pure sentence-level decoder.
Initializing randomly gives a slightly lower Bleu
score but with a larger variation than with Moses
initialization, which is also expected. Docent is
non-deterministic, and can give somewhat varying
results with the same weights. However, this vari-
ation has been shown experimentally to be very
small (Hardmeier et al, 2012).
Our goal now is to show that document-level
tuning can perform equally well in order to verify
our approach. For this, we set up a series of ex-
periments looking at varying tuning sets and dif-
ferent parameters of the decoding and optimiza-
tion procedure. With this we like to demonstrate
the stability of the document-level feature weight
optimization approach presented above. Note that
the most important baselines for comparison with
the results in the next sections are the ones with
Docent and random initialization.
5.3 Sentence-Level Features
In this section we present validation results where
we investigate different aspects of document-
System Tuning Bleu NIST
Moses None 17.7 6.25
Docent-M None 17.7 6.25
Docent-R None 15.2 (0.05) 5.88 (0.00)
Moses Moses 18.3 (0.04) 6.22 (0.01)
Docent-M Moses 18.3 (0.04) 6.22 (0.01)
Docent-R Moses 18.1 (0.13) 6.23 (0.01)
Table 2: Baseline results, where Docent-M is ini-
tialized with Moses and Docent-R randomly
Docs Sent. Min Max Bleu NIST
111 2525 3 127 18.0 (0.11) 6.19 (0.04)
345 7567 3 127 18.1 (0.14) 6.25 (0.02)
100 1921 8 40 18.0 (0.05) 6.25 (0.10)
200 3990 8 40 17.9 (0.25) 6.20 (0.09)
100 2394 8 100 18.0 (0.12) 6.27 (0.07)
200 4600 8 100 18.1 (0.29) 6.26 (0.10)
300 6852 8 100 18.2 (0.13) 6.27 (0.03)
Table 3: Results for German?English with varying
sizes of tuning set, where the number of sentences
and documents are varied, as well as the minimum
and maximum number of sentences per document
level feature weight optimization with standard
sentence-level features. In this way we can com-
pare the results directly to standard sentence-level
optimization, and to the results of Moses.
Corpus size We investigate how tuning is af-
fected by corpus size. The corpus size was var-
ied in two ways, by changing the number of docu-
ments in the tuning set, and by changing the length
of documents in the tuning sets. In this exper-
iment we run 20000 decoder iterations per opti-
mization iteration, and use a k-list of size 101,
with sample interval 100. Table 3 shows the re-
sults with varying tuning set sizes for German?
English. There is very little variation between the
scores, and no clear tendencies. All results are of
similar quality to the baseline with random initial-
ization and sentence-level tuning, and better than
not using any tuning. The top line in Table 3 is
News2009, the same tuning set as for the base-
lines. The scores are somewhat more unstable than
the baseline scores, but stability is not related to
corpus size. In the following sections we will use
the tuning set with 200 documents, size 8-40.
Number of decoder iterations and k-list sam-
pling Two issues that are relevant for feature
weight optimization with the document-level de-
coder is the number of decoder hill-climbing iter-
ations in each optimization iteration, and the set-
tings for k-list sampling. These choices affect the
64
Iterations K-list UTK Bleu NIST
20000 101 55.6 17.9 (0.25) 6.20 (0.09)
30000 201 67.2 17.9 (0.06) 6.21 (0.01)
40000 301 79.9 18.2 (0.11) 6.28 (0.09)
50000 401 86.9 18.1 (0.20) 6.22 (0.05)
75000 651 99.2 17.8 (0.15) 6.13 (0.03)
100000 901 106.8 17.9 (0.17) 6.16 (0.03)
30000 101 21.6 18.0 (0.15) 6.21 (0.02)
40000 101 12.6 17.7 (0.53) 6.12 (0.15)
50000 101 8.2 17.9 (0.24) 6.18 (0.06)
Table 4: Results for German?English with a vary-
ing number of iterations and k-list size (UTK is
the average number of unique translations per doc-
ument in the k-lists)
quality of the translations in each optimization it-
eration, and the spread in the k-list. We will report
the average number of unique translations per doc-
ument in the k-lists, UTK, during feature weight
optimization, in this section.
The top half of Table 4 shows results with a
different number of iterations, when we sample
k-lists from iteration 10000 with interval 100 for
German?English, which means that the size of the
k-lists also changes. The differences on MT met-
rics are very small. The number of new unique
translations in the k-lists decrease with the number
of decoder iterations. With 20K iterations, 55%
of the k-lists entries are unique, which could be
compared to only 12% with 100K iterations. The
majority of the unique translations are thus found
in the beginning of the decoding, which is not sur-
prising.
The bottom half of Table 4 shows results with
a different number of decoder iterations, but a set
k-list size. In this setting the number of unique
hypotheses in the k-lists obviously decreases with
the number of decoder iterations. Despite this,
there are mostly small result differences, except
for 40K iterations, which has more unstable results
than the other settings. It does not seem useful to
increase the number of decoder iterations without
also increasing the size of the k-list. An even bet-
ter strategy might be to only include unique entries
in the k-lists. We will explore this in future work.
We also ran experiments where we did not
restart the decoder with a random state in each iter-
ation, but instead saved the previous state and con-
tinued decoding with the new weights from there.
This, however, was largely unsuccessful, and gave
very low scores. We believe that the reason for this
is mainly that a much smaller part of the search
space is explored when the decoder is not restarted
Interval Start UTK Bleu NIST
1 19900 1.4 18.2 (0.07) 6.25 (0.04)
10 19000 5.2 18.1 (0.08) 6.22 (0.03)
100 10000 55.6 17.9 (0.25) 6.20 (0.09)
200 0 82.2 17.9 (0.19) 6.15 (0.05)
Table 5: Results with different k-list-sample inter-
vals for k-lists size 101 (UTK is the average num-
ber of unique translations per document in the k-
lists)
with a new seed repeatedly. The fact that a higher
overall quality can be achieved with a higher num-
ber of iterations (see Figure 2) can apparently not
compensate for this drawback.
Finally, we investigate the effect of the sam-
ple interval for the k-lists. To get k-lists of equal
size, 101, we start the sampling at different itera-
tions. Table 5 shows the results, and we can see
that with a small sample interval, the number of
unique translations decreases drastically. Despite
this, there are no large result differences. There
is actually a slight trend that a smaller sample in-
terval is better. This does not confirm our intuition
that it is important with many different translations
in the k-list. Especially for interval 1 it is surpris-
ing, since there is often only 1 unique translation
for a single document. We believe that the fact that
k-lists from different iterations are joined, can be
part of the explanation for these results. We think
more work is needed in the future, to further ex-
plore these settings, and the interaction with the
total number of decoder iteration, and the k-list
sampling.
To further shed some light to these results, we
show learning curves from the optimization. Fig-
ure 2 shows Bleu scores for the system optimized
with 100K decoder iterations after different num-
bers of iterations, for the last three iterations in
each of the three optimization runs. As shown in
Hardmeier et al (2012), the translation quality in-
creases fast at first, but start to level out at around
40K iterations. Despite this, the optimization re-
sults are good even with 20K iterations, which is
somewhat surprising. Figures 3 and 4 show the
Bleu scores after each tuning iteration for the sys-
tems in Tables 4 and 5. As is normal for SMT tun-
ing, the convergence is slow, and there are some
oscillations even late in the optimization. Over-
all systems with many iterations seem somewhat
more stable.
Overall, the results are better than the untuned
65
 
14
 
14.5 15
 
15.5 16
 
16.5 17
 
17.5
 
10
 
20
 
30
 
40
 
50
 
60
 
70
 
80
 
90 
100
Bleu
Dec
ode
r ite
ratio
ns (*
1000
)
1-23 1-24 1-25 2-23 2-24 2-25 3-23 3-24 3-25
Figure 2: Bleu scores during 100000 Docent iter-
ations during feature weight optimization
 
4
 
6
 
8
 
10
 
12
 
14
 
16
 
18  
0
 
5
 
10
 
15
 
20
 
25
Bleu
Tun
ing 
itera
tion
s20K 30K 40K 50K 75K 100
K
Figure 3: Bleu scores during feature weight opti-
mization for systems with different number of de-
coder iterations and k-list sizes.
baseline and on par with the sentence-level tuning
baselines in all settings, with a relatively modest
variation, even across settings. In fact, if we cal-
culate the total scores of all 36 systems in Tables 4
and 5, we get a Bleu score of 18.0 (0.23) and a
NIST score of 6.19 (0.07), with a variation that is
not higher than for many of the different settings.
Optimization method In this section we com-
pare the performance of the MERT optimiza-
tion algorithm with that of PRO, and a combi-
nation that starts MERT with weights initialized
with PRO (MERT+PRO), suggested by Koehn and
Haddow (2012). Here we run 30000 decoder it-
erations. Table 6 shows the results. Initializing
MERT with PRO did not affect the scores much.
The scores with only PRO, however, are slightly
lower than for MERT, and have a much larger
score variation. This could be because PRO is
 
4
 
6
 
8
 
10
 
12
 
14
 
16
 
18  
0
 
5
 
10
 
15
 
20
 
25
Bleu
Tun
ing 
itera
tion
s1 (20
K)
10 (2
0K)
100
 (20K
)
100
 (30K
)
100
 (40K
)
100
 (50K
)
200
 (20K
)
Figure 4: Bleu scores during feature weight opti-
mization for systems with different k-list sample
interval and number of decoder iterations.
Bleu NIST
MERT 17.9 (0.06) 6.21 (0.01)
PRO 17.5 (0.41) 6.15 (0.20)
MERT+PRO 18.0 (0.12) 6.18 (0.06)
Table 6: Results with different optimization algo-
rithms for German?English
likely to need more data, since it calculates met-
ric scores on individual units, sentences or docu-
ments, not across the full tuning set, like MERT.
This likely means that 200 documents are too few
for stable results with optimization methods that
depend on unit-level metric scores.
5.4 Document-Level Features
In this section we investigate the effect of opti-
mization with a number of document-level fea-
tures. We use a set of features proposed in Stymne
et al (2013), in order to promote the readability
of texts. In this scenario, however, we use these
features in a standard SMT setting, where they
can potentially improve the lexical consistency of
translations. The features are:
? Type token ratio (TTR) ? the ratio of types,
unique words, to tokens, total number of
words
? OVIX ? a reformulation of TTR that has tra-
ditionally been used for Swedish and that is
less sensible to text length than TTR, see
Eq. 1
? Q-value, phrase level (QP) - The Q-value was
developed as a measure for bilingual term
quality (Dele?ger et al, 2006), to promote
common and consistently translated terms.
See Eq. 2, where f(st) is the frequency of
66
German?English English?Swedish
System Optimization Bleu NIST Bleu NIST
Moses Sentence 18.3 (0.04) 6.22 (0.01) 24.3 6.12
Docent Sentence 18.1 (0.13) 6.23 (0.01) 24.1 6.06
Docent Document 17.9 (0.25) 6.20 (0.09) 23.4 6.01
TTR Document 18.3 (0.16) 6.33 (0.04) 23.6 6.15
OVIX Document 18.3 (0.13) 6.30 (0.03) 23.4 5.99
QW Document 18.1 (0.14) 6.22 (0.03) 24.2 6.11
QP Document 18.0 (0.10) 6.23 (0.05) 21.2 5.70
Table 7: Results when using document-level features
the phrase pair, n(s) is the number of unique
target phrases which the source phrase is
aligned to in the document, and n(t) is the
same for the target phrase. Here the Q-value
is applied on the phrase level.
? Q-value, word level (QW) - Same as above,
but here we apply the Q-value for source
words and their alignments on the target side.
OVIX =
log(count(tokens))
log
(
2?
log(count(types))
log(count(tokens))
) (1)
Q-value =
f(st)
n(s) + n(t)
(2)
We added these features one at a time to the
standard feature set. Optimization was performed
with 20000 decoder iterations, and a k-list of size
101. As shown in the previous sections, there
are slightly better settings, which could have been
used to boost the results somewhat.
The results are shown in Table 7. For German?
English, the results are generally on par with the
baselines for Bleu and slightly higher on NIST for
OVIX and TTR. For English?Swedish, we used a
smaller tuning set on the document level than on
the sentence level, see Table 1, due to time con-
straints. This is reflected in the scores, which are
generally lower than for sentence-level decoding.
Using the QW feature, however, we receive com-
petitive scores to the sentence-based baselines,
which indicates that it can be meaningful to use
document-level features with the suggested tuning
approach.
While the results do not improve much over
the baselines, these experiments still show that
we can optimize discourse-level features with
our approach. We need to identify more useful
document-level features in future work, however.
6 Conclusion
We have shown how the standard feature weight
optimization workflow for SMT can be adapted to
document-level decoding, which allows easy inte-
gration of discourse-level features into SMT. We
modified the standard framework by calculating
scores on the document-level instead of the sen-
tence level, and by using k-lists rather than k-best
lists.
Experimental results show that we can achieve
relatively stable results, on par with the results for
sentence-level optimization and better than with-
out tuning, with standard features. This is de-
spite the fact that we use the hill-climbing de-
coder without initialization by a standard decoder,
which means that it is somewhat unstable, and
is not guaranteed to find any global maximum,
even according to the model. We also show that
we can optimize document-level features success-
fully. We investigated the effect of a number of
parameters relating to tuning set size, the number
of decoder iterations, and k-list sampling. There
were generally small differences relating to these
parameters, however, indicating that the suggested
approach is robust. The interaction between pa-
rameters does need to be better explored in future
work, and we also want to explore better sampling,
without duplicate translations.
This is the first attempt of describing and exper-
imentally investigating feature weight optimiza-
tion for direct document-level decoding. While we
show the feasibility of extending sentence-level
optimization to the document level, there is still
much more work to be done. We would, for in-
stance, like to investigate other optimization pro-
cedures, especially for systems with a high num-
ber of features. Most importantly, there is a large
need for the development of useful discourse-level
features for SMT, which can now be optimized.
Acknowledgments
This work was supported by the Swedish strategic
research programme eSSENCE.
67
References
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montre?al, Canada.
Marine Carpuat and Dekai Wu. 2007. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 61?72, Prague, Czech Republic.
Marine Carpuat. 2009. One translation per discourse.
In Proceedings of the Workshop on Semantic Evalu-
ations: Recent Achievements and Future Directions
(SEW-2009), pages 19?27, Boulder, Colorado.
Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007. Word sense disambiguation improves statisti-
cal machine translation. In Proceedings of the 45th
Annual Meeting of the ACL, pages 33?40, Prague,
Czech Republic.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
427?436, Montre?al, Canada.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of Human
Language Technologies: The 2008 Annual Con-
ference of the NAACL, pages 224?233, Honolulu,
Hawaii.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for op-
timizer instability. In Proceedings of the 49th An-
nual Meeting of the ACL: Human Language Tech-
nologies, pages 176?181, Portland, Oregon, USA.
Louise Dele?ger, Magnus Merkel, and Pierre Zweigen-
baum. 2006. Enriching medical terminologies:
an approach based on aligned corpora. In Inter-
national Congress of the European Federation for
Medical Informatics, pages 747?752, Maastricht,
The Netherlands.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurence statistics. In Proceedings of the Sec-
ond International Conference on Human Language
Technology, pages 228?231, San Diego, California,
USA.
Kevin Gimpel and Noah A. Smith. 2012. Struc-
tured ramp loss minimization for machine transla-
tion. In Proceedings of the 2012 Conference of
the NAACL: Human Language Technologies, pages
221?231, Montre?al, Canada.
Zhengxian Gong, Min Zhang, and Guodong Zhou.
2011. Cache-based document-level statistical ma-
chine translation. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 909?919, Edinburgh, Scotland,
UK.
Zhengxian Gong, Min Zhang, Chew Lim Tan, and
Guodong Zhou. 2012. N-gram-based tense models
for statistical machine translation. In Proceedings
of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning, pages 276?285,
Jeju Island, Korea.
Liane Guillou. 2012. Improving pronoun translation
for statistical machine translation. In Proceedings of
the EACL 2012 Student Research Workshop, pages
1?10, Avignon, France.
Christian Hardmeier and Marcello Federico. 2010.
Modelling pronominal anaphora in statistical ma-
chine translation. In Proceedings of the Interna-
tional Workshop on Spoken Language Translation,
pages 283?289, Paris, France.
Christian Hardmeier, Joakim Nivre, and Jo?rg Tiede-
mann. 2012. Document-wide decoding for phrase-
based statistical machine translation. In Proceed-
ings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1179?1190, Jeju Island, Korea.
Christian Hardmeier, Sara Stymne, Jo?rg Tiedemann,
and Joakim Nivre. 2013. Docent: A document-level
decoder for phrase-based statistical machine transla-
tion. In Proceedings of the 51st Annual Meeting of
the ACL, Demonstration session, Sofia, Bulgaria.
Christian Hardmeier. 2012. Discourse in statistical
machine translation: A survey and a case study. Dis-
cours, 11.
Basil Hatim and Ian Mason. 1990. Discourse and the
Translator. Longman, London, UK.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland.
Philipp Koehn and Barry Haddow. 2012. Towards
effective use of training data in statistical machine
translation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 317?321,
Montre?al, Canada.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Human Language Technology
Conference of the NAACL, pages 48?54, Edmonton,
Alberta, Canada.
68
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL,
Demo and Poster Sessions, pages 177?180, Prague,
Czech Republic.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of MT Summit X, pages 79?86, Phuket, Thailand.
Ronan Le Nagard and Philipp Koehn. 2010. Aiding
pronoun translation with co-reference resolution. In
Proceedings of the Joint Fifth Workshop on Statis-
tical Machine Translation and MetricsMATR, pages
252?261, Uppsala, Sweden.
Thomas Meyer and Andrei Popescu-Belis. 2012. Us-
ing sense-labeled discourse connectives for statisti-
cal machine translation. In Proceedings of the Joint
Workshop on Exploiting Synergies between Informa-
tion Retrieval and Machine Translation (ESIRMT)
and Hybrid Approaches to Machine Translation
(HyTra), pages 129?138, Avignon, France.
Thomas Meyer, Andrei Popescu-Belis, Najeh Hajlaoui,
and Andrea Gesmundo. 2012. Machine translation
of labeled discourse connectives. In Proceedings of
the 10th Biennial Conference of the Association for
Machine Translation in the Americas, San Diego,
California, USA.
Franz Josef Och, Nicola Ueffing, and Hermann Ney.
2001. An efficient A* search algorithm for Statisti-
cal Machine Translation. In Proceedings of the ACL
2001 Workshop on Data-Driven Machine Transla-
tion, pages 55?62, Toulouse, France.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 42nd Annual Meeting of the ACL, pages 160?
167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the ACL, pages 311?
318, Philadelphia, Pennsylvania, USA.
Lucia Specia, Baskaran Sankaran, and Maria das
Grac?as Volpe Nunes. 2008. N-best reranking for
the efficient integration of word sense disambigua-
tion and statistical machine translation. In Proceed-
ings of the 9th International Conference on Intelli-
gent Text Processing and Computational Linguistics
(CICLING), pages 399?410, Haifa, Israel.
Sara Stymne, Jo?rg Tiedemann, Christian Hardmeier,
and Joakim Nivre. 2013. Statistical machine trans-
lation with readability constraints. In Proceedings
of the 19th Nordic Conference on Computational
Linguistics (NODALIDA?13), pages 375?386, Oslo,
Norway.
Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen,
Xiaodong Shi, Huailin Dong, and Qun Liu. 2012.
Translation model adaptation for statistical machine
translation with monolingual topic information. In
Proceedings of the 50th Annual Meeting of the ACL,
pages 459?468, Jeju Island, Korea.
Jo?rg Tiedemann. 2010. Context adaptation in statisti-
cal machine translation using models with exponen-
tially decaying cache. In Proceedings of the ACL
2010 Workshop on Domain Adaptation for Natural
Language Processing (DANLP), pages 8?15, Upp-
sala, Sweden.
Marco Turchi, Tijl De Bie, Cyril Goutte, and Nello
Cristianini. 2012. Learning to translate: A statis-
tical and computational analysis. Advances in Arti-
ficial Intelligence, 2012. Article ID 484580.
Ferhan Ture, Douglas W. Oard, and Philip Resnik.
2012. Encouraging consistent translation choices.
In Proceedings of the 2012 Conference of the
NAACL: Human Language Technologies, pages
417?426, Montre?al, Canada.
Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.
2011. Document-level consistency verification in
machine translation. In Proceedings of MT Summit
XIII, pages 131?138, Xiamen, China.
Bing Zhao and Eric P. Xing. 2010. HM-BiTAM: Bilin-
gual topic exploration, word alignment,and transla-
tion. In Advances in Neural Information Processing
Systems 20 (NIPS), pages 1689?1696, Cambridge,
Massachusetts, USA.
69
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 12?21,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Lithuanian Dependency Parsing with Rich Morphological Features
Jurgita Kapoc?iu?te?-Dzikiene?
Kaunas University of Technology
K. Donelaic?io 73
LT-44249 Kaunas, Lithuania
jurgita.k.dz@gmail.com
Joakim Nivre
Uppsala University
Box 635
SE-75126 Uppsala, Sweden
joakim.nivre@lingfil.uu.se
Algis Krupavic?ius
Kaunas University of Technology
K. Donelaic?io 73
LT-44249 Kaunas, Lithuania
pvai@ktu.lt
Abstract
We present the first statistical dependency
parsing results for Lithuanian, a morpholog-
ically rich language in the Baltic branch of
the Indo-European family. Using a greedy
transition-based parser, we obtain a labeled at-
tachment score of 74.7 with gold morphology
and 68.1 with predicted morphology (77.8 and
72.8 unlabeled). We investigate the usefulness
of different features and find that rich morpho-
logical features improve parsing accuracy sig-
nificantly, by 7.5 percentage points with gold
features and 5.6 points with predicted features.
As expected, CASE is the single most impor-
tant morphological feature, but virtually all
available features bring some improvement,
especially under the gold condition.
1 Introduction
During the last decades, we have seen a tremendous
increase in the number of syntactic parsers avail-
able for different languages, often enabled by the
development of syntactically annotated corpora, or
treebanks. The added linguistic diversity has high-
lighted the fact that typological differences between
languages lead to new challenges, both in parsing
technology and treebank annotation. In particu-
lar, it has been observed repeatedly that richly in-
flected languages, which often also exhibit relatively
free word order, usually obtain lower parsing accu-
racy, especially compared to English (Buchholz and
Marsi, 2006; Nivre et al, 2007). This has led to
a special interest in parsing methods for such lan-
guages (Tsarfaty et al, 2010; Tsarfaty et al, 2013).
In this paper, we contribute to the growing pool of
empirical evidence by presenting the first statistical
dependency parsing results for Lithuanian, a mor-
phologically rich Baltic language characterized as
one of the most archaic living Indo-European lan-
guages (Gimbutas, 1963).
Using the newly developed Lithuanian Treebank,
we train and evaluate a greedy transition-based
parser and in particular investigate the impact of rich
morphological features on parsing accuracy. Our
experiments show that virtually all morphological
features can be beneficial when parsing Lithuanian,
which contrasts with many previous studies that
have mainly found a positive impact for isolated fea-
tures such as CASE (Eryigit et al, 2008). Using all
available features, we achieve a labeled attachment
score of 74.7 with gold morphology (including part-
of-speech tags and lemmas) and 68.1 with predicted
morphology. The corresponding unlabeled attach-
ment scores are 77.8 and 72.8, respectively.
2 The Lithuanian Treebank
The Lithuanian Treebank was developed by the Cen-
ter of Computational Linguistics, Vytautas Magnus
University.1 The annotated texts are taken from
the newspaper domain and thus represent normative
Lithuanian language. The treebank contains 1,566
sentences and 24,265 tokens: 19,625 words (9,848
distinct) plus 4,640 punctuation marks (12 distinct).
Word tokens in the Lithuanian Treebank are mor-
1The treebank creation was one of the tasks of the project
Internet Resources: Annotated Corpus of the Lithuanian Lan-
guage and Tools of Annotation, implemented in 2007-2008 and
funded by the Lithuanian Science and Studies Foundation.
12
SBJ OBJ MODIF PRED ATTR DEP ROOT TOTAL
Abbreviation 6 457 22 485
Acronym 31 2 33
Adjectival participle 1 28 84 12 125
Adjective 1 63 1,104 157 75 1,400
Adverbial participle 37 28 3 68
Adverb 1,134 193 29 1,356
Conjunction 5 1,171 93 1,269
Infinitive 6 372 9 139 21 547
Interjection 3 6 9
Noun 775 1,097 1,314 1,712 1,415 217 6,530
Numeral 1 22 158 72 6 259
Participle 1 150 430 285 197 1,063
Particle 27 78 1 216 36 358
Preposition 253 168 630 35 1,086
Pronoun 258 170 104 558 424 21 1,535
Proper noun 15 1 22 20 1,307 60 1,425
Roman number 25 3 28
Verb 205 1,844 2,049
TOTAL 1,057 1,533 2,856 663 3,992 6,842 2,682 19,625
Table 1: Cooccurrence statistics on dependencies (columns) and PoS tags (rows) in the Lithuanian Treebank.
phologically and syntactically annotated as follows:
? Syntactic dependencies: 7 different categories
listed in Table 1 (columns).
? Part-of-Speech (PoS) tags: 18 different cate-
gories listed in Table 1 (rows). These tags sim-
ply determine PoS but do not incorporate any
additional morphological information.
? Morphological features: 12 different categories
listed with possible values in Table 2. The
number of morphological features assigned to a
word varies from 0 (for particles, conjunctions,
etc.) to 9.2
? Lemmas: base form of word, lowercase except
for proper names.
The syntactic annotation scheme only distin-
guishes 5 basic grammatical relations (SBJ, OBJ,
PRED, ATTR, MODIF) plus an additional under-
specified relation (DEP) for other dependencies be-
tween words and a special relation (ROOT) for
2For example, the participle esanti (existent) is described
by 8 feature values: CASE: Nominative, GENDER: Feminine,
NUMBER: Singular, TENSE: Present, VOICE: Active, RE-
FLEX: Non-reflexive, PRONOM: Non-pronominal, ASPECT:
Positive.
words attached to an (implicit) artificial root node.
The dependency structure always forms a tree orig-
inating from the root node, but there may be more
than one token attached to the root node. This hap-
pens when a sentence contains several clauses which
do not share any constituents. Table 1 gives statis-
tics on the different dependency relations and their
distribution over different PoS tags.
Examples of syntactically annotated sentences are
presented in Figure 1 and Figure 2. All dependency
relations are represented by arrows pointing from
the head to the dependent, the labels above indicate
the dependency type.3 For example, as we can see
in Figure 1, nerizikuoja (does not risk) is the head of
Kas (Who) and this dependency relation has the SBJ
label. The sentence in Figure 1 contains two clauses
(separated by a comma) both containing SBJ depen-
dency relations. The sentence in Figure 2 contains
the main clause Bet s?tai pro medi? praslinko nedidelis
s?es?e?lis and the subordinate clause kuriame se?de?jau
in which the subject is expressed implicitly (a pro-
noun as? (I) can be inferred from the singular 1st per-
son inflection of the verb se?de?jau (sat)). In Lithua-
nian sentences, the subject is very often omitted, and
even the verb can be expressed implicitly. For exam-
3ROOT dependencies are not shown explicitly.
13
Category Values Frequency Compatible PoS Tags
CASE Nominative 3,421 Adjective, Noun, Numeral, Participle, Pronoun, Proper noun
Genitive 4,204
Dative 445
Accusative 1,995
Instrumental 795
Locative 849
Vocative 10
GENDER Masculine 7,074 Adjective, Adverbial participle, Noun, Numeral, Participle,
Feminine 4,482 Pronoun, Proper noun
Neuter 283
Appellative 1
NUMBER Singular 8,822 Adjective, Adverbial participle, Noun, Numeral, Participle,
Plural 4,624 Pronoun, Proper noun, Verb
Dual 3
TENSE Present 1,307 Adjectival participle, Participle, Verb
Past occasion 1,352
Past 311
Past iterative 31
Future 123
MOOD Indicative 1,950 Verb
Subjunctive 87
Imperative 12
PERSON 1st 281 Verb
2nd 41
3rd 1,727
VOICE Active 456 Participle
Passive 594
Gerundive 13
REFLEX Reflexive 526 Adjectival participle, Adverbial participle, Infinitive, Noun,
Non-reflexive 3,486 Participle, Verb
DEGREE Positive 1,712 Adjective, Adverb, Numeral, Participle
Comparative 1,712
Superior 1
Superlative 94
TYPE Cardinal 145 Numeral
Ordinal 105
Multiple 9
PRONOM Pronominal 247 Adjective, Participle, Pronoun, Numeral
Non-pronominal 3,056
ASPECT Positive 6,206 Adjectival participle, Adjective, Adverbial participle, Adverb,
Negative 422 Infinitive, Noun, Participle, Particle, Preposition, Verb
Table 2: Morphological categories in the Lithuanian Treebank: possible values, frequencies and compatible PoS tags.
ple, in the sentence Jis geras z?mogus (He is a good
man), the copula verb yra (is) is omitted.
The possible values of different morphological
categories are presented with descriptive statistics
in Table 2. Given that word order in Lithuanian
sentences is relatively free, morphological informa-
tion is important to determine dependency relations.
For example, an adjective modifying a noun has
to agree in GENDER, NUMBER and CASE, as in
graz?us miestas (beautiful city), where both the ad-
jective and the noun are in masculine singular nom-
inative. Verbs agree with their subject in NUMBER
and PERSON, as in ju?s vaz?iuojate (you are going) in
second person plural. Finally, the CASE of a noun
14
Figure 1: Annotated sentence from the Lithuanian Treebank, consisting of two independent main clauses. Translation:
Who does not risk, that does not drink champagne but does not cry tearfully either.
Figure 2: Annotated sentence from the Lithuanian Treebank, consisting of a main clause and a subordinate clause.
Translation: But here through the tree in which I sat passed a small shadow.
or pronoun is an important indicator of the syntac-
tic relation to the verb, such that nominative CASE
almost always implies a SBJ relation. However, the
transparency of morphological information is lim-
ited by syncretism in CASE, NUMBER and GEN-
DER. Thus, the form mamos (mother(s)) can be ei-
ther plural nominative or singular genitive; the form
mokytojas (teacher(s)) can be either masculine sin-
gular nominative or feminine plural accusative.
3 Parsing Framework
We use the open-source system MaltParser (Nivre
et al, 2006a) for our parsing experiments with the
Lithuanian Treebank. MaltParser is a transition-
based dependency parser that performs parsing as
greedy search through a transition system, guided
by a history-based classifier for predicting the next
transition (Nivre, 2008). Although more accurate
dependency parsers exist these days, MaltParser ap-
peared suitable for our experiments for a number of
reasons. First of all, greedy transition-based parsers
have been shown to perform well with relatively
small amounts of training data (Nivre et al, 2006b).
Secondly, MaltParser implements a number of dif-
ferent transition systems and classifiers that can be
explored and also supports user-defined input for-
mats and feature specifications in a flexible way. Fi-
nally, MaltParser has already been applied to a wide
range of languages, to which the results can be com-
pared. In particular, MaltParser was used to obtain
the only published dependency parsing results for
Latvian, the language most closely related to Lithua-
nian (Pretkalnin. a and Rituma, 2013).
In our experiments, we use the latest release of
MaltParser (Version 1.7.2).4 After preliminary ex-
periments, we decided to use the arc-eager transition
system (Nivre, 2003) with pseudo-projective pars-
ing to recover non-projective dependencies (Nivre
and Nilsson, 2005) and the LIBLINEAR learning
package with multiclass SVMs (Fan et al, 2008).
Table 3 lists the options that were explored in the
preliminary experiments. We first tested all possible
combinations of learning method and parsing algo-
rithms and then performed a greedy sequential tun-
ing of the options related to covered roots, pseudo-
projective parsing, and all combinations of allow-
root and allow-reduce.
In order to use MaltParser on the Lithuanian Tree-
bank, we first converted the data to the CoNLL-X
format,5 treating all morphological feature bundles
4Available at http://maltparser.org.
5See http://ilk.uvt.nl/conll/#dataformat.
15
Option Value
Learning method (-l) liblinear
Parsing algorithm (-a) nivreeager
Covered roots (-pcr) head
Pseudo-projective parsing (-pp) head+path
Allow root (-nr) true
Allow reduce (-ne) true
Table 3: List of MaltParser options explored in prelimi-
nary experiments with best values used in all subsequent
experiments.
as a single string and putting it into the FEATS col-
umn, which means that there will be one boolean
feature for each unique set of features. However,
in order to study the influence of each individual
morphological feature, we also prepared an appro-
priate format where every morphological feature had
its own (atom-valued) column (called CASE, GEN-
DER, NUMBER, etc.), which means that there will
be one boolean feature for each unique feature value,
as specified in Table 2. In the following, we will re-
fer to these two versions as Set-FEATS and Atom-
FEATS, respectively. Another choice we had to
make was how to treat punctuation, which is not in-
tegrated into the dependency structure in the Lithua-
nian Treebank. To avoid creating spurious non-
projective dependencies by attaching them to the
root node, we simply attached all punctuation marks
to an adjacent word.6 Therefore, we also exclude
punctuation in all evaluation scores.
We use five-fold cross-validation on the entire
treebank in all our experiments. This means that
the final accuracy estimates obtained after tuning
features and other parameters may be overly opti-
mistic (in the absence of a held-out test set), but
given the very limited amount of data available this
seemed like the most reasonable approach. We
perform experiments under two conditions. In the
Gold condition, the input to the parser contains PoS
tags, lemmas and morphological features taken from
the manually annotated treebank. In the Predicted
condition, we instead use input annotations pro-
duced by the morphological analyser and lemma-
tizer Lemuoklis (Zinkevic?ius, 2000; Daudaravic?ius
et al, 2007), which also solves morphological dis-
6This is automatically handled by the covered roots option
in MaltParser; see Table 3.
Category Accuracy
POSTAG 88.1
LEMMA 91.1
Set-FEATS 78.6
Atom-FEATS
CASE 87.2
GENDER 88.3
NUMBER 86.2
TENSE 94.1
MOOD 95.9
PERSON 95.8
VOICE 90.2
REFLEX 93.3
DEGREE 90.3
TYPE 80.7
PRONOM 89.3
ASPECT 93.5
Table 4: Accuracy of the morphological analyzer and
lemmatizer used in the Predicted condition.
ambiguation problems at the sentence level. Table 4
shows the accuracy of this system for the output cat-
egories that are relevant both in the Set-FEATS and
Atom-FEATS format.
4 Parsing Experiments and Results
In our first set of experiments, we tuned two feature
models in the Gold condition:
? Baseline: Starting from the default feature
model in MaltParser, we used backward and
forward feature selection to tune a feature
model using only features over the FORM,
LEMMA, POSTAG and DEPREL fields in the
CoNLL-X format (that is, no morphological
features). Only one feature was explored at
a time, starting with FORM and going on to
LEMMA, POSTAG, DEPREL, and conjunc-
tions of POSTAG and DEPREL features. The
best templates for each feature type were re-
tained when moving on to the next feature.
? Baseline+FEATS: Starting from the Baseline
model, we used forward feature selection to
tune a feature model that additionally contains
features over the FEATS field in the Set-FEATS
16
Figure 3: The feature models Baseline and Baseline+FEATS. Rows represent address functions, columns represent
attribute functions. Gray cells represent single features, dotted lines connecting cell pairs or lines connecting cell
triplets represent conjoined features. The Baseline model contains only features that do not involve the FEATS column.
version, optionally conjoined with POSTAG
features.
The features included in these two models are
depicted schematically in Figure 3. The Base-
line+FEATS model includes all features, while the
Baseline model includes all features except those
that refer to the FEATS field. In the Gold condi-
tion, the Baseline model achieves a labeled attach-
ment score (LAS) of 67.19 and an unlabeled attach-
ment score (UAS) of 73.96, while Baseline+FEATS
gets 74.20 LAS and 77.40 UAS. In the Predicted
condition, the corresponding results are 62.47/70.30
for Baseline and 68.05/72.78 for Baseline+FEATS.
Thus, with the addition of morphological features
(all of them together) the Baseline+FEATS model
exceeds the Baseline by 7.01 percentage points for
LAS and 3.44 for UAS in the Gold condition and by
5.58 percentage points for LAS and 2.48 for UAS in
the Predicted condition. To determine whether the
differences are statistically significant we performed
McNemar?s test (McNemar, 1947) with one degree
of freedom. The test showed the differences in LAS
and UAS between Baseline and Baseline+FEATS
for both the Gold and Predicted conditions to be sta-
tistically significant with p << 0.05.
In our second set of experiments, we started from
the Baseline model and incrementally added mor-
phological features in the Atom-FEATS format, one
morphological category at a time, using the same
five feature templates (three single and two con-
joined) as for FEATS in the Baseline+FEATS model
(see Figure 3). The order of explored morpho-
logical features was random, but only features that
increased parsing accuracy when added were re-
tained when adding the next morphological feature.
The LAS results of these experiments are summa-
rized in Figure 4 (reporting results in the Gold con-
dition) and Figure 5 (in the Predicted condition).
We do not present UAS results because they show
the same trend as the LAS metric although shifted
upwards. In the Gold condition, the best feature
model is Baseline + CASE + GENDER + NUM-
BER + TENSE + DEGREE + VOICE + PERSON
+ TYPE, which achieves 74.66 LAS and 77.84
UAS and exceeds the Baseline by 7.47 percentage
points for LAS and 3.88 for UAS (MOOD, RE-
FLEX, PRONOM and ASPECT made no improve-
ments or even degraded the performance). In the
Predicted condition, the best feature model remains
Baseline+FEATS, but using the Atom-FEATS ver-
sion the best results are achieved with Baseline +
CASE + GENDER + TENSE + VOICE + PERSON
+ REFLEX, which exceeds the Baseline by 5.36 per-
centage points for LAS and 2.55 for UAS (NUM-
BER, MOOD, DEGREE, REFLEX, PRONOM and
ASPECT made no improvements or even degraded
17
the performance). All these differences are statis-
tically significant. By contrast, the differences be-
tween the best models with Atom-FEATS and Set-
FEATS are not statistically significant for any metric
or condition (with p values in the range 0.35?0.87).
5 Discussion
First of all, we may conclude that the Baseline
feature model (without morphological information)
does not perform very well for a morphologically
rich language like Lithuanian (see Figure 4 and Fig-
ure 5), despite giving high accuracy for morpholog-
ically impoverished languages like English. How-
ever, it is likely that the accuracy of the Baseline
model would be a bit higher for the Lithuanian Tree-
bank if PoS tags incorporated some morphological
information as they do, for example, in the English
Penn Treebank (Marcus et al, 1993).
It thus seems that basic PoS tags as well as lem-
mas are too general to be beneficial enough for
Lithuanian. The simple morphemic word form
could be more useful (even despite the fact that
Lithuanian is syncretic language), but the treebank
is currently too small, making the data too sparse to
create a robust model.7 Thus, the effective way of
dealing with unseen words is by incorporating mor-
phological information.
In the Predicted condition, we always see a drop
in accuracy compared to the Gold condition, al-
though our case is not exceptional. For example, the
Baseline model has a drop in LAS of 4.72 percent-
age points from Gold to Predicted, but this gap could
possibly be narrowed by retuning the feature model
for the Predicted condition instead of simply reusing
the model tuned for the Gold condition. We also
tried training the model on gold annotations for pars-
ing predicted annotations, but these produced even
worse results, confirming that it is better to make
the training condition resemble the parsing condi-
tion. Despite noisy information, morphological fea-
tures are still very beneficial compared to not using
them at all (see Figure 5). Our findings thus agree
with what has been found for Arabic by Marton et
al. (2013) but seem to contradict the results obtained
7We tried to reduce data sparseness a little bit by changing
all words into lowercase, but the drop in accuracy revealed that
orthographic information is also important for parsing.
for Hebrew by Goldberg and Elhadad (2010).
As we can see from both curves in Figure 4 and
Figure 5, the top contributors are CASE, VOICE,
and TENSE, but the CASE feature gives the biggest
contribution to accuracy. It boosts LAS by 6.51
points in the Gold condition and almost 5 points in
the Predicted condition, whereas the contribution of
all the other morphological features is less than 1
point (and not statistically significant). In a con-
trol experiment we reversed the order in which mor-
phological features are added (presented in Figure 4
and Figure 5), adding CASE at the very end. In
this case, the addition of all features except case re-
sulted in a statistically significant improvement in
the Gold condition (p = 0.001) but not in the Pre-
dicted condition (p = 0.24). However, the contribu-
tion of CASE was by far the most important again
? increasing LAS by 5.55 points in the Gold condi-
tion and by 4.68 points in the Predicted condition.
To further investigate the selection of morphologi-
cal features, we also performed a greedy selection
experiment. During this experiment CASE was se-
lected first, again proving it to be the most influential
feature. It was followed by VOICE, MOOD, NUM-
BER and DEGREE in the Gold condition and by
GENDER, TENSE, PERSON and TYPE in the Pre-
dicted condition. Overall, however, greedy selection
gave worse results than random selection, achieving
74.42 LAS and 77.60 UAS in the Gold condition and
67.83 LAS and 72.80 UAS in the Predicted condi-
tion.
To find that CASE is the most important feature
is not surprising, as CASE has been shown to be
the most helpful feature for many languages (at least
in the Gold condition). But whereas few other fea-
tures have been shown to help for other languages,
in our case the majority of features (8 out of 12 in
the Gold condition) are beneficial for Lithuanian.
The so-called agreement features (GENDER, NUM-
BER and PERSON) are beneficial for Lithuanian
(at least in the Gold condition) as well as for Ara-
bic (Marton et al, 2013), but not such languages as
Hindi (Ambati et al, 2010) and Hebrew (Goldberg
and Elhadad, 2010). In the Predicted condition, their
positive impact is marginal at best, possibly because
NUMBER is very poorly predicted by the morpho-
18
Figure 4: The contribution of individual morphological features in the Gold condition. The x axis represents feature
models incorporating different attributes; the y axis represents LAS. The horizontal line at 74.20 represents the LAS
of Baseline+FEATS.
Figure 5: The contribution of individual morphological features in the Predicted condition. The x axis represents
feature models incorporating different attributes; the y axis represents LAS. The horizontal line at 68.05 represents the
LAS of Baseline+FEATS.
19
logical analyzer.8
It is also worth noting that morphological fea-
tures have less influence on UAS than LAS, as the
gain in UAS over the Baseline is 3-4 percentage
points lower compared to LAS. This means that
morphology is more important for selecting the type
of dependency than for choosing the syntactic head.
More precisely, adding morphology improves both
recall and precision for the labels SBJ and OBJ,
which is probably due primarily to the CASE fea-
ture.
Despite the positive effect of morphological infor-
mation, the best LAS achieved is only 74.66 in the
Gold condition and 68.05 in the Predicted condition.
An error analysis shows that 38.0% of all LAS er-
rors have an incorrect syntactic head, 12.5% have an
incorrect dependency label, and 49.5% have both in-
correct. The most commonly occurring problem is
the ambiguity between DEP and ROOT dependen-
cies.
For example, in the sentence atsidu?re? Vokietijoje,
lanke? paskaitas (he got to Germany, attended lec-
tures) lanke? (attended) is the dependent of atsidu?re?
(got), because it is the consecutive action performed
by the same subject (the subject is expressed implic-
itly and can be identified according the appropriate
verb form). But in the sentence buvo puiku ir mums,
ir jam patiko (it was great for us and he enjoyed it)
patiko (enjoyed) is not a dependent of buvo (was)
but of the root node, because the sentence contains
two separate clauses with their subjects and verbs.9
Other common ambiguities are among different
types of labels that are expressed by the same mor-
phological categories and depends on the context
(and the meaning) of the sentence, for example, in
the phrase uz?z?elti augalais (to green with plants),
augalais (plants) is a dependent of uz?z?elti (to green)
with the OBJ label; in uz?siimti projektais (to en-
gage in projects) projektais (projects) is a dependent
of uz?siimti (to engage) with the MODIF label; and
in pavadinti vardais (to name with names) vardais
(names) is a dependent on pavadinti (to name) with
8The accuracy is only 86.2%, the lowest of all features.
9This type of ambiguity is somewhat artificial, since it arises
from the choice to not annotate relations between complete
clauses in the Lithuanian Treebank. We expect that parsing
accuracy would be improved if all interclausal relations were
annotated explicitly.
DEP label. The choice of dependency label in these
cases depends on the semantic role of the modifier,
corresponding to the question what in the first case,
the question how in the second case, and yet a dif-
ferent relation in the third case. In all these cases
morphology does not help to determine the particu-
lar label of the dependency relation.
Finally, we note that the results obtained for
Lithuanian are in the same range as those reported
for Latvian, another Baltic language. Using Malt-
Parser in 10-fold cross-validation on a data set of
2,500 sentences, Pretkalnin. a and Rituma (2013)
achieve an unlabeled attachment score of 74.6 in
the Gold condition and 72.2 in the Predicted condi-
tions, to be compared with 77.8 and 72.8 in our ex-
periments. It should be remembered, however, that
the results are not directly comparable due to differ-
ences in annotation schemes.
6 Conclusion
In this paper we have presented the first statisti-
cal dependency parsing results for Lithuanian. Us-
ing the transition-based system MaltParser, we have
demonstrated experimentally that the role of mor-
phology is very important for the Lithuanian lan-
guage. The addition of morphological information
resulted in a gain in attachment scores of 7.5 points
(labeled) and 3.9 points (unlabeled) with manually
validated morphology (the Gold condition) and of
5.6 points (labeled) and 2.5 points (unlabeled) with
automatically predicted morphology (the Predicted
condition). In the Gold condition, we achieved the
best results by adding each morphological feature
separately (using the Atom-FEATS representation),
but in the Predicted condition adding all features to-
gether (using the Set-FEATS representation turned
out to be better). The most important morphological
feature is CASE, followed by VOICE and TENSE.
Future work includes a more detailed error anal-
ysis for the different models, which could throw
further light on the impact of different features. It
could also be worthwhile to experiment with dif-
ferent feature templates for different morphologi-
cal categories. For example, for agreement fea-
tures it seems important to conjoin the values of two
words that are candidates for a dependency, while
this might not be necessary for features like CASE.
20
However, in order to get a major improvement in
parsing accuracy, we probably need larger amounts
of syntactically annotated data as well as more con-
sistent annotations of interclausal relations.
Acknowledgments
This research is funded by European Union Struc-
tural Funds Project ?Postdoctoral Fellowship Im-
plementation in Lithuania? (No. VP1-3.1-S?MM-01)
and was initiated when the first author was visiting
the Department of Linguistics and Philology at Up-
psala University, Sweden.
References
Bharat Ram Ambati, Samar Husain, Joakim Nivre, and
Rajeev Sangal. 2010. On the role of morphosyntactic
features in hindi dependency parsing. In Proceedings
of the NAACL HLT 2010 First Workshop on Statistical
Parsing of Morphologically-Rich Languages, SPMRL
?10, pages 94?102, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the 10th Conference on Computational
Natural Language Learning (CoNLL), pages 149?164.
Vidas Daudaravic?ius, Erika Rimkute?, and Andrius Utka.
2007. Morphological annotation of the Lithuanian
corpus. In Proceedings of the Workshop on Balto-
Slavonic Natural Language Processing: Informa-
tion Extraction and Enabling Technologies (ACL?07),
pages 94?99.
Gu?lsen Eryigit, Joakim Nivre, and Kemal Oflazer. 2008.
Dependency parsing of Turkish. Computational Lin-
guistics, 34.
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
C.-J. Lin. 2008. LIBLINEAR: A library for large lin-
ear classification. Journal of Machine Learning Re-
search, 9:1871?1874.
Marija Gimbutas. 1963. The Balts. Thames and Hudson.
Yoav Goldberg and Michael Elhadad. 2010. Easy first
dependency parsing of modern hebrew. In Proceed-
ings of the NAACL HLT 2010 First Workshop on Sta-
tistical Parsing of Morphologically-Rich Languages,
SPMRL ?10, pages 103?107, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313?330.
Yuval Marton, Nizar Habash, and Owen Rambow. 2013.
Dependency parsing of modern standard arabic with
lexical and inflectional features. Computational Lin-
guistics, 39(1):161?194, March.
Quinn Michael McNemar. 1947. Note on the sampling
error of the difference between correlated proportions
or percentages. Psychometrika, 12(2):153?157.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 99?106.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006a.
Maltparser: A data-driven parser-generator for depen-
dency parsing. In Proceedings of the 5th International
Conference on Language Resources and Evaluation
(LREC), pages 2216?2219.
Joakim Nivre, Johan Hall, Jens Nilsson, Gu?lsen Eryig?it,
and Svetoslav Marinov. 2006b. Labeled pseudo-
projective dependency parsing with support vector ma-
chines. In Proceedings of the 10th Conference on
Computational Natural Language Learning (CoNLL),
pages 221?225.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task of
EMNLP-CoNLL 2007, pages 915?932.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT), pages 149?160.
Joakim Nivre. 2008. Algorithms for deterministic incre-
mental dependency parsing. Computational Linguis-
tics, 34:513?553.
Lauma Pretkalnin. a and Laura Rituma. 2013. Statistical
syntactic parsing for Latvian. In Proceedings of the
19th Nordic Conference of Computational Linguistics
(NODALIDA 2013), pages 279?289.
Reut Tsarfaty, Djame? Seddah, Yoav Goldberg, San-
dra Kuebler, Yannick Versley, Marie Candito, Jen-
nifer Foster, Ines Rehbein, and Lamia Tounsi. 2010.
Statistical parsing of morphologically rich languages
(spmrl) what, how and whither. In Proceedings of the
NAACL HLT 2010 First Workshop on Statistical Pars-
ing of Morphologically-Rich Languages, pages 1?12.
Reut Tsarfaty, Djame? Seddah, Sandra Ku?bler, and Joakim
Nivre. 2013. Parsing morphologicall rich languages:
Introduction to the special issue. Computational Lin-
guistics, 39:15?22.
Vytautas Zinkevic?ius. 2000. Lemuoklis ? morfologinei
analizei [Morphological analysis with Lemuoklis].
Gudaitis, L. (ed.) Darbai ir dienos, 24:246?273. (in
Lithuanian).
21
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 146?182,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Overview of the SPMRL 2013 Shared Task:
Cross-Framework Evaluation of Parsing Morphologically Rich Languages?
Djam? Seddaha, Reut Tsarfatyb, Sandra K?blerc,
Marie Canditod, Jinho D. Choie, Rich?rd Farkasf , Jennifer Fosterg, Iakes Goenagah,
Koldo Gojenolai, Yoav Goldbergj , Spence Greenk, Nizar Habashl, Marco Kuhlmannm,
Wolfgang Maiern, Joakim Nivreo, Adam Przepi?rkowskip, Ryan Rothq, Wolfgang Seekerr,
Yannick Versleys, Veronika Vinczet, Marcin Wolin?skiu,
Alina Wr?blewskav, Eric Villemonte de la Cl?rgeriew
aU. Paris-Sorbonne/INRIA, bWeizman Institute, cIndiana U., dU. Paris-Diderot/INRIA, eIPsoft Inc., f,tU. of Szeged,
gDublin City U., h,iU. of the Basque Country, jBar Ilan U., kStanford U., l,qColumbia U., m,oUppsala U., nD?sseldorf U.,
p,u,vPolish Academy of Sciences, rStuttgart U., sHeidelberg U., wINRIA
Abstract
This paper reports on the first shared task on
statistical parsing of morphologically rich lan-
guages (MRLs). The task features data sets
from nine languages, each available both in
constituency and dependency annotation. We
report on the preparation of the data sets, on
the proposed parsing scenarios, and on the eval-
uation metrics for parsing MRLs given dif-
ferent representation types. We present and
analyze parsing results obtained by the task
participants, and then provide an analysis and
comparison of the parsers across languages and
frameworks, reported for gold input as well as
more realistic parsing scenarios.
1 Introduction
Syntactic parsing consists of automatically assigning
to a natural language sentence a representation of
its grammatical structure. Data-driven approaches
to this problem, both for constituency-based and
dependency-based parsing, have seen a surge of inter-
est in the last two decades. These data-driven parsing
approaches obtain state-of-the-art results on the de
facto standard Wall Street Journal data set (Marcus et
al., 1993) of English (Charniak, 2000; Collins, 2003;
Charniak and Johnson, 2005; McDonald et al, 2005;
McClosky et al, 2006; Petrov et al, 2006; Nivre et
al., 2007b; Carreras et al, 2008; Finkel et al, 2008;
?Contact authors: djame.seddah@paris-sorbonne.fr,
reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu
Huang, 2008; Huang et al, 2010; Zhang and Nivre,
2011; Bohnet and Nivre, 2012; Shindo et al, 2012),
and provide a foundation on which many tasks oper-
ating on semantic structure (e.g., recognizing textual
entailments) or even discourse structure (coreference,
summarization) crucially depend.
While progress on parsing English ? the main
language of focus for the ACL community ? has in-
spired some advances on other languages, it has not,
by itself, yielded high-quality parsing for other lan-
guages and domains. This holds in particular for mor-
phologically rich languages (MRLs), where impor-
tant information concerning the predicate-argument
structure of sentences is expressed through word for-
mation, rather than constituent-order patterns as is the
case in English and other configurational languages.
MRLs express information concerning the grammati-
cal function of a word and its grammatical relation to
other words at the word level, via phenomena such
as inflectional affixes, pronominal clitics, and so on
(Tsarfaty et al, 2012c).
The non-rigid tree structures and morphological
ambiguity of input words contribute to the challenges
of parsing MRLs. In addition, insufficient language
resources were shown to also contribute to parsing
difficulty (Tsarfaty et al, 2010; Tsarfaty et al, 2012c,
and references therein). These challenges have ini-
tially been addressed by native-speaking experts us-
ing strong in-domain knowledge of the linguistic
phenomena and annotation idiosyncrasies to improve
the accuracy and efficiency of parsing models. More
146
recently, advances in PCFG-LA parsing (Petrov et al,
2006) and language-agnostic data-driven dependency
parsing (McDonald et al, 2005; Nivre et al, 2007b)
have made it possible to reach high accuracy with
classical feature engineering techniques in addition
to, or instead of, language-specific knowledge. With
these recent advances, the time has come for estab-
lishing the state of the art, and assessing strengths
and weaknesses of parsers across different MRLs.
This paper reports on the first shared task on sta-
tistical parsing of morphologically rich languages
(the SPMRL Shared Task), organized in collabora-
tion with the 4th SPMRL meeting and co-located
with the conference on Empirical Methods in Natural
Language Processing (EMNLP). In defining and exe-
cuting this shared task, we pursue several goals. First,
we wish to provide standard training and test sets for
MRLs in different representation types and parsing
scenarios, so that researchers can exploit them for
testing existing parsers across different MRLs. Sec-
ond, we wish to standardize the evaluation protocol
and metrics on morphologically ambiguous input,
an under-studied challenge, which is also present in
English when parsing speech data or web-based non-
standard texts. Finally, we aim to raise the awareness
of the community to the challenges of parsing MRLs
and to provide a set of strong baseline results for
further improvement.
The task features data from nine, typologically di-
verse, languages. Unlike previous shared tasks on
parsing, we include data in both dependency-based
and constituency-based formats, and in addition to
the full data setup (complete training data), we pro-
vide a small setup (a training subset of 5,000 sen-
tences). We provide three parsing scenarios: one in
which gold segmentation, POS tags, and morphologi-
cal features are provided, one in which segmentation,
POS tags, and features are automatically predicted
by an external resource, and one in which we provide
a lattice of multiple possible morphological analyses
and allow for joint disambiguation of the morpholog-
ical analysis and syntactic structure. These scenarios
allow us to obtain the performance upper bound of
the systems in lab settings using gold input, as well
as the expected level of performance in realistic pars-
ing scenarios ? where the parser follows a morpho-
logical analyzer and is a part of a full-fledged NLP
pipeline.
The remainder of this paper is organized as follows.
We first survey previous work on parsing MRLs (?2)
and provide a detailed description of the present task,
parsing scenarios, and evaluation metrics (?3). We
then describe the data sets for the nine languages
(?4), present the different systems (?5), and empiri-
cal results (?6). Then, we compare the systems along
different axes (?7) in order to analyze their strengths
and weaknesses. Finally, we summarize and con-
clude with challenges to address in future shared
tasks (?8).
2 Background
2.1 A Brief History of the SPMRL Field
Statistical parsing saw initial success upon the avail-
ability of the Penn Treebank (PTB, Marcus et al,
1994). With that large set of syntactically annotated
sentences at their disposal, researchers could apply
advanced statistical modeling and machine learning
techniques in order to obtain high quality structure
prediction. The first statistical parsing models were
generative and based on treebank grammars (Char-
niak, 1997; Johnson, 1998; Klein and Manning, 2003;
Collins, 2003; Petrov et al, 2006; McClosky et al,
2006), leading to high phrase-structure accuracy.
Encouraged by the success of phrase-structure
parsers for English, treebank grammars for additional
languages have been developed, starting with Czech
(Hajic? et al, 2000) then with treebanks of Chinese
(Levy and Manning, 2003), Arabic (Maamouri et
al., 2004b), German (K?bler et al, 2006), French
(Abeill? et al, 2003), Hebrew (Sima?an et al, 2001),
Italian (Corazza et al, 2004), Spanish (Moreno et al,
2000), and more. It quickly became apparent that
applying the phrase-based treebank grammar tech-
niques is sensitive to language and annotation prop-
erties, and that these models are not easily portable
across languages and schemes. An exception to that
is the approach by Petrov (2009), who trained latent-
annotation treebank grammars and reported good
accuracy on a range of languages.
The CoNLL shared tasks on dependency parsing
(Buchholz and Marsi, 2006; Nivre et al, 2007a) high-
lighted the usefulness of an alternative linguistic for-
malism for the development of competitive parsing
models. Dependency relations are marked between
input tokens directly, and allow the annotation of
147
non-projective dependencies that are parseable effi-
ciently. Dependency syntax was applied to the de-
scription of different types of languages (Tesni?re,
1959; Mel?c?uk, 2001), which raised the hope that in
these settings, parsing MRLs will further improve.
However, the 2007 shared task organizers (Nivre
et al, 2007a) concluded that: "[Performance] classes
are more easily definable via language characteris-
tics than via characteristics of the data sets. The
split goes across training set size, original data for-
mat [...], sentence length, percentage of unknown
words, number of dependency labels, and ratio of
(C)POSTAGS and dependency labels. The class
with the highest top scores contains languages with
a rather impoverished morphology." The problems
with parsing MRLs have thus not been solved by de-
pendency parsing, but rather, the challenge has been
magnified.
The first event to focus on the particular challenges
of parsing MRLs was a dedicated panel discussion
co-located with IWPT 2009.1 Work presented on
Hebrew, Arabic, French, and German made it clear
that researchers working on non-English parsing face
the same overarching challenges: poor lexical cover-
age (due to high level of inflection), poor syntactic
coverage (due to more flexible word ordering), and,
more generally, issues of data sparseness (due to
the lack of large-scale resources). Additionally, new
questions emerged as to the evaluation of parsers in
such languages ? are the word-based metrics used
for English well-equipped to capture performance
across frameworks, or performance in the face of
morphological complexity? This event provoked ac-
tive discussions and led to the establishment of a
series of SPMRL events for the discussion of shared
challenges and cross-fertilization among researchers
working on parsing MRLs.
The body of work on MRLs that was accumulated
through the SPMRL workshops2 and hosting ACL
venues contains new results for Arabic (Attia et al,
2010; Marton et al, 2013a), Basque (Bengoetxea
and Gojenola, 2010), Croatian (Agic et al, 2013),
French (Seddah et al, 2010; Candito and Seddah,
2010; Sigogne et al, 2011), German (Rehbein, 2011),
Hebrew (Tsarfaty and Sima?an, 2010; Goldberg and
1http://alpage.inria.fr/iwpt09/panel.en.
html
2See http://www.spmrl.org/ and related workshops.
Elhadad, 2010a), Hindi (Ambati et al, 2010), Ko-
rean (Chung et al, 2010; Choi and Palmer, 2011) and
Spanish (Le Roux et al, 2012), Tamil (Green et al,
2012), amongst others. The awareness of the model-
ing challenges gave rise to new lines of work on top-
ics such as joint morpho-syntactic processing (Gold-
berg and Tsarfaty, 2008), Relational-Realizational
Parsing (Tsarfaty, 2010), EasyFirst Parsing (Gold-
berg, 2011), PLCFRS parsing (Kallmeyer and Maier,
2013), the use of factored lexica (Green et al, 2013),
the use of bilingual data (Fraser et al, 2013), and
more developments that are currently under way.
With new models and data, and with lingering in-
terest in parsing non-standard English data, questions
begin to emerge, such as: What is the realistic per-
formance of parsing MRLs using today?s methods?
How do the different models compare with one an-
other? How do different representation types deal
with parsing one particular language? Does the suc-
cess of a parsing model on a language correlate with
its representation type and learning method? How to
parse effectively in the face of resource scarcity? The
first step to answering all of these questions is pro-
viding standard sets of comparable size, streamlined
parsing scenarios, and evaluation metrics, which are
our main goals in this SPMRL shared task.
2.2 Where We Are At: The Need for
Cross-Framework, Realistic, Evaluation
Procedures
The present task serves as the first attempt to stan-
dardize the data sets, parsing scenarios, and evalu-
ation metrics for MRL parsing, for the purpose of
gaining insights into parsers? performance across lan-
guages. Ours is not the first cross-linguistic task on
statistical parsing. As mentioned earlier, two previ-
ous CoNLL shared tasks focused on cross-linguistic
dependency parsing and covered thirteen different
languages (Buchholz and Marsi, 2006; Nivre et al,
2007a). However, the settings of these tasks, e.g.,
in terms of data set sizes or parsing scenarios, made
it difficult to draw conclusions about strengths and
weaknesses of different systems on parsing MRLs.
A key aspect to consider is the relation between
input tokens and tree terminals. In the standard sta-
tistical parsing setup, every input token is assumed
to be a terminal node in the syntactic parse tree (after
deterministic tokenization of punctuation). In MRLs,
148
morphological processes may have conjoined several
words into a single token. Such tokens need to be seg-
mented and their analyses need to be disambiguated
in order to identify the nodes in the parse tree. In
previous shared tasks on statistical parsing, morpho-
logical information was assumed to be known in ad-
vance in order to make the setup comparable to that
of parsing English. In realistic scenarios, however,
morphological analyses are initially unknown and are
potentially highly ambiguous, so external resources
are used to predict them. Incorrect morphological
disambiguation sets a strict ceiling on the expected
performance of parsers in real-world scenarios. Re-
sults reported for MRLs using gold morphological
information are then, at best, optimistic.
One reason for adopting this less-than-realistic
evaluation scenario in previous tasks has been the
lack of sound metrics for the more realistic scenario.
Standard evaluation metrics assume that the number
of terminals in the parse hypothesis equals the num-
ber of terminals in the gold tree. When the predicted
morphological segmentation leads to a different num-
ber of terminals in the gold and parse trees, standard
metrics such as ParsEval (Black et al, 1991) or At-
tachment Scores (Buchholz and Marsi, 2006) fail
to produce a score. In this task, we use TedEval
(Tsarfaty et al, 2012b), a metric recently suggested
for joint morpho-syntactic evaluation, in which nor-
malized tree-edit distance (Bille, 2005) on morpho-
syntactic trees allows us to quantify the success on
the joint task in realistic parsing scenarios.
Finally, the previous tasks focused on dependency
parsing. When providing both constituency-based
and dependency-based tracks, it is interesting to com-
pare results across these frameworks so as to better
understand the differences in performance between
parsers of different types. We are now faced with
an additional question: how can we compare pars-
ing results across different frameworks? Adopting
standard metrics will not suffice as we would be com-
paring apples and oranges. In contrast, TedEval is
defined for both phrase structures and dependency
structures through the use of an intermediate repre-
sentation called function trees (Tsarfaty et al, 2011;
Tsarfaty et al, 2012a). Using TedEval thus allows us
to explore both dependency and constituency parsing
frameworks and meaningfully compare the perfor-
mance of parsers of different types.
3 Defining the Shared-Task
3.1 Input and Output
We define a parser as a structure prediction function
that maps sequences of space-delimited input tokens
(henceforth, tokens) in a language to a set of parse
trees that capture valid morpho-syntactic structures
in that language. In the case of constituency parsing,
the output structures are phrase-structure trees. In de-
pendency parsing, the output consists of dependency
trees. We use the term tree terminals to refer to the
leaves of a phrase-structure tree in the former case
and to the nodes of a dependency tree in the latter.
We assume that input sentences are represented
as sequences of tokens. In general, there may be a
many-to-many relation between input tokens and tree
terminals. Tokens may be identical to the terminals,
as is often the case in English. A token may be
mapped to multiple terminals assigned their own POS
tags (consider, e.g., the token ?isn?t?), as is the case
in some MRLs. Several tokens may be grouped into
a single (virtual) node, as is the case with multiword
expressions (MWEs) (consider ?pomme de terre? for
?potatoe?). This task covers all these cases.
In the standard setup, all tokens are tree terminals.
Here, the task of a parser is to predict a syntactic
analysis in which the tree terminals coincide with the
tokens. Disambiguating the morphological analyses
that are required for parsing corresponds to selecting
the correct POS tag and possibly a set of morpho-
logical features for each terminal. For the languages
Basque, French, German, Hungarian, Korean, Polish,
and Swedish, we assume this standard setup.
In the morphologically complex setup, every token
may be composed of multiple terminals. In this case,
the task of the parser is to predict the sequence of tree
terminals, their POS tags, and a correct tree associ-
ated with this sequence of terminals. Disambiguating
the morphological analysis therefore requires split-
ting the tokens into segments that define the terminals.
For the Semitic languages Arabic and Hebrew, we
assume this morphologically complex setup.
In the multiword expression (MWEs) setup, pro-
vided here for French only, groupings of terminals
are identified as MWEs (non-terminal nodes in con-
stituency trees, marked heads in dependency trees).
Here, the parser is required to predict how terminals
are grouped into MWEs on top of predicting the tree.
149
3.2 Data Sets
The task features nine languages from six language
families, from Germanic languages (Swedish and
German) and Romance (French) to Slavic (Polish),
Koreanic (Korean), Semitic (Arabic, Hebrew), Uralic
(Hungarian), and the language isolate Basque.
These languages cover a wide range of morpho-
logical richness, with Arabic, Basque, and Hebrew
exhibiting a high degree of inflectional and deriva-
tional morphology. The Germanic languages, Ger-
man and Swedish, have greater degrees of phrasal
ordering freedom than English. While French is not
standardly classified as an MRL, it shares MRLs char-
acteristics which pose challenges for parsing, such as
a richer inflectional system than English.
For each contributing language, we provide two
sets of annotated sentences: one annotated with la-
beled phrase-structure trees, and one annotated with
labeled dependency trees. The sentences in the two
representations are aligned at token and POS levels.
Both representations reflect the predicate-argument
structure of the same sentence, but this information
is expressed using different formal terms and thus
results in different tree structures.
Since some of our native data sets are larger than
others, we provide the training set in two sizes: Full
containing all sentences in the standard training set
of the language, and 5k containing the number of
sentences that is equivalent in size to our smallest
training set (5k sentences). For all languages, the data
has been split into sentences, and the sentences are
parsed and evaluated independently of one another.
3.3 Parsing Scenarios
In the shared task, we consider three parsing scenar-
ios, depending on how much of the morphological
information is provided. The scenarios are listed
below, in increasing order of difficulty.
? Gold: In this scenario, the parser is provided
with unambiguous gold morphological segmen-
tation, POS tags, and morphological features for
each input token.
? Predicted: In this scenario, the parser is pro-
vided with disambiguated morphological seg-
mentation. However, the POS tags and mor-
phological features for each input segment are
unknown.
Scenario Segmentation PoS+Feat. Tree
Gold X X ?
Predicted X 1-best ?
Raw (1-best) 1-best 1-best ?
Raw (all) ? ? ?
Table 1: A summary of the parsing and evaluation sce-
narios. X depicts gold information, ? depicts unknown
information, to be predicted by the system.
? Raw: In this scenario, the parser is provided
with morphologically ambiguous input. The
morphological segmentation, POS tags, and
morphological features for each input token are
unknown.
The Predicted and Raw scenarios require predict-
ing morphological analyses. This may be done using
a language-specific morphological analyzer, or it may
be done jointly with parsing. We provide inputs that
support these different scenarios:
? Predicted: Gold treebank segmentation is given
to the parser. The POS tags assignment and mor-
phological features are automatically predicted
by the parser or by an external resource.
? Raw (1-best): The 1st-best segmentation and
POS tags assignment is predicted by an external
resource and given to the parser.
? Raw (all): All possible segmentations and POS
tags are specified by an external resource. The
parser selects jointly a segmentation and a tree.
An overview of all shown in table 1. For languages
in which terminals equal tokens, only Gold and Pre-
dicted scenarios are considered. For Semitic lan-
guages we further provide input for both Raw (1-
best) and Raw (all) scenarios. 3
3.4 Evaluation Metrics
This task features nine languages, two different repre-
sentation types and three different evaluation scenar-
ios. In order to evaluate the quality of the predicted
structures in the different tracks, we use a combina-
tion of evaluation metrics that allow us to compare
the systems along different axes.
3The raw Arabic lattices were made available later than the
other data. They are now included in the shared task release.
150
In this section, we formally define the different
evaluation metrics and discuss how they support sys-
tem comparison. Throughout this paper, we will be
referring to different evaluation dimensions:
? Cross-Parser Evaluation in Gold/Predicted
Scenarios. Here, we evaluate the results of dif-
ferent parsers on a single data set in the Gold
or Predicted setting. We use standard evalu-
ation metrics for the different types of anal-
yses, that is, ParsEval (Black et al, 1991)
on phrase-structure trees, and Labeled At-
tachment Scores (LAS) (Buchholz and Marsi,
2006) for dependency trees. Since ParsEval is
known to be sensitive to the size and depth of
trees (Rehbein and van Genabith, 2007b), we
also provide the Leaf-Ancestor metric (Samp-
son and Babarczy, 2003), which is less sensitive
to the depth of the phrase-structure hierarchy. In
both scenarios we also provide metrics to evalu-
ate the prediction of MultiWord Expressions.
? Cross-Parser Evaluation in Raw Scenarios.
Here, we evaluate the results of different parsers
on a single data set in scenarios where morpho-
logical segmentation is not known in advance.
When a hypothesized segmentation is not iden-
tical to the gold segmentation, standard evalua-
tion metrics such as ParsEval and Attachment
Scores break down. Therefore, we use TedEval
(Tsarfaty et al, 2012b), which jointly assesses
the quality of the morphological and syntactic
analysis in morphologically-complex scenarios.
? Cross-Framework Evaluation. Here, we com-
pare the results obtained by a dependency parser
and a constituency parser on the same set of sen-
tences. In order to avoid comparing apples and
oranges, we use the unlabeled TedEval metric,
which converts all representation types inter-
nally into the same kind of structures, called
function trees. Here we use TedEval?s cross-
framework protocol (Tsarfaty et al, 2012a),
which accomodates annotation idiosyncrasies.
? Cross-Language Evaluation. Here, we com-
pare parsers for the same representation type
across different languages. Conducting a com-
plete and faithful evaluation across languages
would require a harmonized universal annota-
tion scheme (possibly along the lines of (de
Marneffe and Manning, 2008; McDonald et al,
2013; Tsarfaty, 2013)) or task based evaluation.
As an approximation we use unlabeled TedEval.
Since it is unlabeled, it is not sensitive to label
set size. Since it internally uses function-trees,
it is less sensitive to annotation idiosyncrasies
(e.g., head choice) (Tsarfaty et al, 2011).
The former two dimensions are evaluated on the full
sets. The latter two are evaluated on smaller, compa-
rable, test sets. For completeness, we provide below
the formal definitions and essential modifications of
the evaluation software that we used.
3.4.1 Evaluation Metrics for Phrase Structures
ParsEval The ParsEval metrics (Black et al, 1991)
are evaluation metrics for phrase-structure trees. De-
spite various shortcomings, they are the de-facto stan-
dard for system comparison on phrase-structure pars-
ing, used in many campaigns and shared tasks (e.g.,
(K?bler, 2008; Petrov and McDonald, 2012)). As-
sume that G and H are phrase-structure gold and
hypothesized trees respectively, each of which is rep-
resented by a set of tuples (i, A, j) where A is a
labeled constituent spanning from i to j. Assume
that g is the same as G except that it discards the
root, preterminal, and terminal nodes, likewise for h
and H . The ParsEval scores define the accuracy of
the hypothesis in terms of the normalized size of the
intersection of the constituent sets.
Precision(g, h) = |g?h||h|
Recall(g, h) = |g?h||g|
F1(g, h) = 2?P?RP+R
We evaluate accuracy on phrase-labels ignoring any
further decoration, as it is in standard practices.
Evalb, the standard software that implements Par-
sEval,4 takes a parameter file and ignores the labels
specified therein. As usual, we ignore root and POS
labels. Contrary to the standard practice, we do take
punctuation into account. Note that, as opposed to the
official version, we used the SANCL?2012 version5
modified to actually penalize non-parsed trees.
4http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Evalb
5Modified by Petrov and McDonald (2012) to be less sensi-
tive to punctuation errors.
151
Leaf-Ancestor The Leaf-Ancestor metric (Samp-
son and Babarczy, 2003) measures the similarity be-
tween the path from each terminal node to the root
node in the output tree and the corresponding path
in the gold tree. The path consists of a sequence of
node labels between the terminal node and the root
node, and the similarity of two paths is calculated
by using the Levenshtein distance. This distance is
normalized by path length, and the score of the tree
is an aggregated score of the values for all terminals
in the tree (xt is the leaf-ancestor path of t in tree x).
LA(h, g) =
?
t?yield(g) Lv(ht,gt)/(len(ht)+len(gt))
|yield(g)|
This metric was shown to be less sensitive to dif-
ferences between annotation schemes in (K?bler et
al., 2008), and was shown by Rehbein and van Gen-
abith (2007a) to evaluate trees more faithfully than
ParsEval in the face of certain annotation decisions.
We used the implementation of Wagner (2012).6
3.4.2 Evaluation Metrics for Dependency
Structures
Attachment Scores Labeled and Unlabeled At-
tachment scores have been proposed as evaluation
metrics for dependency parsing in the CoNLL shared
tasks (Buchholz and Marsi, 2006; Nivre et al, 2007a)
and have since assumed the role of standard metrics
in multiple shared tasks and independent studies. As-
sume that g, h are gold and hypothesized dependency
trees respectively, each of which is represented by
a set of arcs (i, A, j) where A is a labeled arc from
terminal i to terminal j. Recall that in the gold and
predicted settings, |g| = |h| (because the number of
terminals determines the number of arcs and hence it
is fixed). So Labeled Attachment Score equals preci-
sion and recall, and it is calculated as a normalized
size of the intersection between the sets of gold and
parsed arcs.7
Precision(g, h) = |g?h||g|
Recall(g, h) = |g?h||h|
LAS(g, h) = |g?h||g| =
|g?h|
|h|
6The original version is available at
http://www.grsampson.net/Resources.
html, ours at http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Leaf.
7http://ilk.uvt.nl/conll/software.html.
3.4.3 Evaluation Metrics for Morpho-Syntactic
Structures
TedEval The TedEval metrics and protocols have
been developed by Tsarfaty et al (2011), Tsarfaty
et al (2012a) and Tsarfaty et al (2012b) for coping
with non-trivial evaluation scenarios, e.g., comparing
parsing results across different frameworks, across
representation theories, and across different morpho-
logical segmentation hypotheses.8 Contrary to the
previous metrics, which view accuracy as a normal-
ized intersection over sets, TedEval computes the ac-
curacy of a parse tree based on the tree-edit distance
between complete trees. Assume a finite set of (pos-
sibly parameterized) edit operations A = {a1....an},
and a cost function c : A ? 1. An edit script is the
cost of a sequence of edit operations, and the edit dis-
tance of g, h is the minimal cost edit script that turns
g into h (and vice versa). The normalized distance
subtracted from 1 provides the level of accuracy on
the task. Formally, the TedEval score on g, h is de-
fined as follows, where ted is the tree-edit distance,
and the |x| (size in nodes) discards terminals and root
nodes.
TedEval(g, h) = 1?
ted(g, h)
|g|+ |h|
In the gold scenario, we are not allowed to manipu-
late terminal nodes, only non-terminals. In the raw
scenarios, we can add and delete both terminals and
non-terminals so as to match both the morphological
and syntactic hypotheses.
3.4.4 Evaluation Metrics for
Multiword-Expression Identification
As pointed out in section 3.1, the French data set is
provided with tree structures encoding both syntactic
information and groupings of terminals into MWEs.
A given MWE is defined as a continuous sequence of
terminals, plus a POS tag. In the constituency trees,
the POS tag of the MWE is an internal node of the
tree, dominating the sequence of pre-terminals, each
dominating a terminal. In the dependency trees, there
is no specific node for the MWE as such (the nodes
are the terminals). So, the first token of a MWE is
taken as the head of the other tokens of the same
MWE, with the same label (see section 4.4).
8http://www.tsarfaty.com/unipar/
download.html.
152
To evaluate performance on MWEs, we use the
following metrics.
? R_MWE, P_MWE, and F_MWE are recall, pre-
cision, and F-score over full MWEs, in which
a predicted MWE counts as correct if it has the
correct span (same group as in the gold data).
? R_MWE +POS, R_MWE +POS, and F_MWE
+POS are defined in the same fashion, except
that a predicted MWE counts as correct if it has
both correct span and correct POS tag.
? R_COMP, R_COMP, and F_COMP are recall,
precision and F-score over non-head compo-
nents of MWEs: a non-head component of MWE
counts as correct if it is attached to the head of
the MWE, with the specific label that indicates
that it is part of an MWE.
4 The SPMRL 2013 Data Sets
4.1 The Treebanks
We provide data from nine different languages anno-
tated with two representation types: phrase-structure
trees and dependency trees.9 Statistics about size,
average length, label set size, and other character-
istics of the treebanks and schemes are provided in
Table 2. Phrase structures are provided in an ex-
tended bracketed style, that is, Penn Treebank brack-
eted style where every labeled node may be extended
with morphological features expressed. Dependency
structures are provided in the CoNLL-X format.10
For any given language, the dependency and con-
stituency treebanks are aligned at the token and ter-
minal levels and share the same POS tagset and mor-
phological features. That is, any form in the CoNLL
format is a terminal of the respective bracketed tree.
Any CPOS label in the CoNLL format is the pre-
terminal dominating the terminal in the bracketed
tree. The FEATS in the CoNLL format are repre-
sented as dash-features decorated on the respective
pre-terminal node in the bracketed tree. See Fig-
ure 1(a)?1(b) for an illustration of this alignment.
9Additionally, we provided the data in TigerXML format
(Brants et al, 2002) for phrase structure trees containing cross-
ing branches. This allows the use of more powerful parsing
formalisms. Unfortunately, we received no submissions for this
data, hence we discard them in the rest of this overview.
10See http://ilk.uvt.nl/conll/.
For ambiguous morphological analyses, we pro-
vide the mapping of tokens to different segmentation
possibilities through lattice files. See Figure 1(c) for
an illustration, where lattice indices mark the start
and end positions of terminals.
For each of the treebanks, we provide a three-way
dev/train/set split and another train set containing the
first 5k sentences of train (5k). This section provides
the details of the original treebanks and their anno-
tations, our data-set preparation, including prepro-
cessing and data splits, cross-framework alignment,
and the prediction of morphological information in
non-gold scenarios.
4.2 The Arabic Treebanks
Arabic is a morphologically complex language which
has rich inflectional and derivational morphology. It
exhibits a high degree of morphological ambiguity
due to the absence of the diacritics and inconsistent
spelling of letters, such as Alif and Ya. As a conse-
quence, the Buckwalter Standard Arabic Morpholog-
ical Analyzer (Buckwalter, 2004; Graff et al, 2009)
produces an average of 12 analyses per word.
Data Sets The Arabic data set contains two tree-
banks derived from the LDC Penn Arabic Treebanks
(PATB) (Maamouri et al, 2004b):11 the Columbia
Arabic Treebank (CATiB) (Habash and Roth, 2009),
a dependency treebank, and the Stanford version
of the PATB (Green and Manning, 2010), a phrase-
structure treebank. We preprocessed the treebanks
to obtain strict token matching between the treebanks
and the morphological analyses. This required non-
trivial synchronization at the tree token level between
the PATB treebank, the CATiB treebank and the mor-
phologically predicted data, using the PATB source
tokens and CATiB feature word form as a dual syn-
chronized pivot.
The Columbia Arabic Treebank The Columbia
Arabic Treebank (CATiB) uses a dependency repre-
sentation that is based on traditional Arabic grammar
and that emphasizes syntactic case relations (Habash
and Roth, 2009; Habash et al, 2007). The CATiB
treebank uses the word tokenization of the PATB
11The LDC kindly provided their latest version of the Arabic
Treebanks. In particular, we used PATB 1 v4.1 (Maamouri et al,
2005), PATB 2 v3.1 (Maamouri et al, 2004a) and PATB 3 v3.3.
(Maamouri et al, 2009)
153
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
train:
#Sents 15,762 7,577 14,759 40,472 8,146 23,010 6,578
#Tokens 589,220 96,368 443,113 719,532 170,141 351,184 68,424
Lex. Size 36,906 25,136 27,470 77,222 40,782 11,1540 22,911
Avg. Length 37.38 12.71 30.02 17.77 20.88 15.26 10.40
Ratio #NT/#Tokens 0.19 0.82 0.34 0.60 0.59 0.60 0.94
Ratio #NT/#Sents 7.40 10.50 10.33 10.70 12.38 9.27 9.84
#Non Terminals 22 12 32 25 16 8 34
#POS tags 35 25 29 54 16 1,975 29
#total NTs 116,769 79,588 152,463 433,215 100,885 213,370 64,792
Dep. Label Set Size 9 31 25 43 417 22 27
train5k:
#Sents 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000
#Tokens 224,907 61,905 150,984 87,841 128,046 109,987 68,336 52,123 76,357
Lex. Size 19,433 18,405 15,480 17,421 15,975 29,009 29,715 18,632 14,110
Avg. Length 44.98 12.38 30.19 17.56 25.60 21.99 13.66 10.42 15.27
Ratio #NT/#Tokens 0.15 0.83 0.34 0.60 0.42 0.57 0.68 0.94 0.58
Ratio #NT/#Sents 7.18 10.33 10.32 10.58 10.97 12.57 9.29 9.87 8.96
#Non Terminals 22 12 29 23 60 16 8 34 8
#POS Tags 35 25 29 51 50 16 972 29 25
#total NTs 35,909 5,1691 51,627 52,945 54,856 62,889 46,484 49,381 44,845
Dep. Label Set Size 9 31 25 42 43 349 20 27 61
dev:
#Sents 1,985 948 1,235 5,000 500 1,051 2,066 821 494
#Tokens 73,932 13,851 38,820 76,704 11,301 29,989 30,480 8,600 9,341
Lex. Size 12,342 5,551 6,695 15,852 3,175 10,673 15,826 4,467 2,690
Avg. Length 37.24 14.61 31.43 15.34 22.60 28.53 14.75 10.47 18.90
Ratio #NT/#Tokens 0.19 0.74 0.33 0.63 0.47 047 0.63 0.94 0.48
Ratio #NT/#Sents 7.28 10.92 10.48 9.71 10.67 13.66 9.33 9.90 9.10
#Non Terminals 21 11 27 24 55 16 8 31 8
#POS Tags 32 23 29 50 47 16 760 29 24
#total NTs 14,452 10,356 12,951 48,560 5,338 14,366 19,283 8,132 4,496
Dep. Label Set Size 9 31 25 41 42 210 22 26 59
test:
#Sents 1959 946 2541 5000 716 1009 2287 822 666
#Tokens 73878 11457 75216 92004 16998 19908 33766 8545 10690
Lex. Size 12254 4685 10048 20149 4305 7856 16475 4336 3112
Avg. Length 37.71 12.11 29.60 18.40 23.74 19.73 14.76 10.39 16.05
Ratio #NT/#Tokens 0.19 0.83 0.34 0.60 0.47 0.62 0.61 0.95 0.57
Ratio #NT/#Sents 7.45 10.08 10.09 11.07 11.17 12.26 9.02 9.94 9.18
#Non Terminals 22 12 30 23 54 15 8 31 8
#POS Tags 33 22 30 52 46 16 809 27 25
#total NTs 14,610 9,537 25,657 55,398 8,001 12,377 20,640 8,175 6,118
Dep. Label Set Size 9 31 26 42 41 183 22 27 56
Table 2: Overview of participating languages and treebank properties. ?Sents? = number of sentences, ?Tokens? =
number of raw surface forms. ?Lex. size? and ?Avg. Length? are computed in terms of tagged terminals. ?NT? = non-
terminals in constituency treebanks, ?Dep Labels? = dependency labels on the arcs of dependency treebanks. ? A more
comprehensive table is available at http://www.spmrl.org/spmrl2013-sharedtask.html/#Prop.
154
(a) Constituency Tree
% % every line is a single tree in a bracketed Penn Treebank format
(ROOT (S (NP ( NNP-#pers=3|num=sing# John))(VP ( VB-#pers=3|num=sing# likes)(NP ( NNP-#pers=3|num=sing# Mary)))))
(b) Dependency Tree
%% every line describes a terminal: terminal-id form lemma CPOS FPOS FEATS Head Rel PHead PRel
1 John John NNP NNP pers=3|num=sing 2 sbj _ _
2 likes like VB VB pers=3|num=sing 0 root _ _
3 Mary Mary NNP NNP pers=3|num=sing 2 obj _ _
Input Lattice
0 1 2 3 4 5 6
1:AIF/NN
1:AIF/VB
1:AIF/NNT
2:LA/RB
3:NISH/VB
3:NISH/NN
4:L/PREP
4:LHSTIR/VB
4:HSTIR/VB
5:ZAT/PRP
%% every line describes a terminal: start-id end-id form lemma CPOS FPOS FEATS token-id
0 1 AIF AIF NN NN _ 1
0 1 AIF AIF NNT NNT _ 1
0 1 AIF AIF VB VB _ 1
1 2 LA LA RB RB _ 2
2 3 NISH NISH VB VB _ 3
2 3 NISH NISH NN NN _ 3
3 5 LHSTIR HSTIR VB VB _ 4
3 4 L L PREP PREP _ 4
4 5 HSTIR HSTIR VB VB _ 4
5 6 ZAT ZAT PRP PRP _ 5
Figure 1: File formats. Trees (a) and (b) are aligned constituency and dependency trees for a mockup English example.
Boxed labels are shared across the treebanks. Figure (c) shows an ambiguous lattice. The red part represents the yield
of the gold tree. For brevity, we use empty feature columns, but of course lattice arcs may carry any morphological
features, in the FEATS CoNLL format.
and employs a reduced POS tagset consisting of six
tags only: NOM (non-proper nominals including
nouns, pronouns, adjectives and adverbs), PROP
(proper nouns), VRB (active-voice verbs), VRB-
PASS (passive-voice verbs), PRT (particles such as
prepositions or conjunctions) and PNX (punctuation).
(This stands in extreme contrast with the Buckwalter
Arabic tagset (PATB official tagset) which is almost
500 tags.) To obtain these dependency trees, we used
the constituent-to-dependency tool (Habash and Roth,
2009). Additional CATiB trees were annotated di-
rectly, but we only use the portions that are converted
from phrase-structure representation, to ensure that
the constituent and dependency yields can be aligned.
The Stanford Arabic Phrase Structure Treebank
In order to stay compatible with the state of the art,
we provide the constituency data set with most of the
pre-processing steps of Green and Manning (2010),
as they were shown to improve baseline performance
on the PATB parsing considerably.12
To convert the original PATB to preprocessed
phrase-structure trees ? la Stanford, we first discard
all trees dominated by X, which indicates errors and
non-linguistic text. At the phrasal level, we collapse
unary chains with identical categories like NP? NP.
We finally remove all traces, but, unlike Green and
Manning (2010), we keep all function tags.
In the original Stanford instance, the pre-terminal
morphological analyses were mapped to the short-
ened Bies tag set provided with the treebank (where
Determiner markers, ?DT?, were added to definite
noun and adjectives, resulting in 32 POS tags). Here
we use the Kulick tagset (Kulick et al, 2006) for
12Both the corpus split and pre-processing code are available
with the Stanford parser at http://nlp.stanford.edu/
projects/arabic.shtml.
155
pre-terminal categories in the phrase-structure trees,
where the Bies tag set is included as a morphological
feature (stanpos) in our PATB instance.
Adapting the Data to the Shared Task We con-
verted the CATiB representation to the CoNLL rep-
resentation and added a ?split-from-previous? and
?split-from-next? markers as in LDC?s tree-terminal
fields.
A major difference between the CATiB treebank
and the Stanford treebank lies in the way they han-
dle paragraph annotations. The original PATB con-
tains sequences of annotated trees that belong to a
same discourse unit (e.g., paragraph). While the
CATiB conversion tool considers each sequence a
single parsing unit, the Stanford pre-processor treats
each such tree structure rooted at S, NP or Frag as
a tree spanning a single sentence. To be compati-
ble with the predicted morphology data which was
bootstrapped and trained on the CATiB interpretation,
we deterministically modified the original PATB by
adding pseudo XP root nodes, so that the Stanford
pre-proprecessor will generate the same tree yields
as the CATiB treebank.
Another important aspect of preprocessing (often-
delegated as a technicality in the Arabic parsing lit-
erature) is the normalization of token forms. Most
Arabic parsing work used transliterated text based on
the schemes proposed by Buckwalter (2002). The
transliteration schemes exhibit some small differ-
ences, but enough to increase the out-of-vocabulary
rate by a significant margin (on top of strictly un-
known morphemes). This phenomenon is evident in
the morphological analysis lattices (in the predicted
dev set there is a 6% OOV rate without normalization,
and half a point reduction after normalization is ap-
plied, see (Habash et al, 2009b; Green and Manning,
2010)). This rate is much lower for gold tokenized
predicted data (with an OOV rate of only 3.66%,
similar to French for example). In our data set, all
tokens are minimally normalized: no diacritics, no
normalization.13
Data Splits For the Arabic treebanks, we use the
data split recommended by the Columbia Arabic and
Dialect Modeling (CADiM) group (Diab et al, 2013).
13Except for the minimal normalization present in MADA?s
back-end tools. This script was provided to the participants.
The data of the LDC first three annotated Arabic Tree-
banks (ATB1, ATB2 and ATB3) were divided into
roughly a 10/80/10% dev/train/test split by word vol-
ume. When dividing the corpora, document bound-
aries were maintained. The train5k files are simply
the first 5,000 sentences of the training files.
POS Tagsets Given the richness of Arabic mor-
phology, there are multiple POS tag sets and tokeniza-
tion schemes that have been used by researchers, (see,
e.g., Marton et al (2013a)). In the shared task, we fol-
low the standard PATB tokenization which splits off
several categories of orthographic clitics, but not the
definite article Al+. On top of that, we consider three
different POS tag sets with different degrees of gran-
ularity: the Buckwalter tag set (Buckwalter, 2004),
the Kulick Reduced Tag set (Kulick et al, 2006), and
the CATiB tag set (Habash et al, 2009a), considering
that granularity of the morphological analyses may
affect syntactic processing. For more information see
Habash (2010).
Predicted Morphology To prepare input for the
Raw scenarios (?3.3), we used the MADA+TOKAN
system (Habash et al, 2009b). MADA is a system
for morphological analysis and disambiguation of
Arabic. It can predict the 1-best tokenization, POS
tags, lemmas and diacritization in one fell swoop.
The MADA output was also used to generate the
lattice files for the Raw-all scenario.
To generate input for the gold token / predicted
tag input scenario, we used Morfette (Chrupa?a et al,
2008), a joint lemmatization and POS tagging model
based on an averaged perceptron. We generated two
tagging models, one trained with the Buckwalter tag
set, and the other with the Kulick tag set. Both were
mapped back to the CATiB POS tag set such that all
predicted tags are contained in the feature field.14
4.3 The Basque Treebank
Basque is an agglutinative language with a high ca-
pacity to generate inflected wordforms, with free
constituent order of sentence elements with respect
to the main verb. Contrary to many other treebanks,
the Basque treebank was originally annotated with
dependency trees, which were later on converted to
constituency trees.
14A conversion script from the rich Buckwalter tagset to
CoNLL-like features was provided to the participants.
156
The Basque Dependency Treebank (BDT) is a
dependency treebank in its original design, due to
syntactic characteristics of Basque such as its free
word order. Before the syntactic annotation, mor-
phological analysis was performed, using the Basque
morphological analyzer of Aduriz et al (2000). In
Basque each lemma can generate thousands of word-
forms ? differing in morphological properties such
as case, number, tense, or different types of subordi-
nation for verbs. If only POS category ambiguity is
resolved, the analyses remain highly ambiguous.
For the main POS category, there is an average of
1.55 interpretations per wordform, which rises to 2.65
for the full morpho-syntactic information, resulting
in an overall 64% of ambiguous wordforms. The
correct analysis was then manually chosen.
The syntactic trees were manually assigned. Each
word contains its lemma, main POS category, POS
subcategory, morphological features, and the la-
beled dependency relation. Each form indicates mor-
phosyntactic features such as case, number and type
of subordination, which are relevant for parsing.
The first version of the Basque Dependency Tree-
bank, consisting of 3,700 sentences (Aduriz et al,
2003), was used in the CoNLL 2007 Shared Task on
Dependency Parsing (Nivre et al, 2007a). The cur-
rent shared task uses the second version of the BDT,
which is the result of an extension and redesign of the
original requirements, containing 11,225 sentences
(150,000 tokens).
The Basque Constituency Treebank (BCT) was
created as part of the CESS-ECE project, where the
main aim was to obtain syntactically annotated con-
stituency treebanks for Catalan, Spanish and Basque
using a common set of syntactic categories. BCT
was semi-automatically derived from the dependency
version (Aldezabal et al, 2008). The conversion pro-
duced complete constituency trees for 80% of the
sentences. The main bottlenecks have been sentence
connectors and non-projective dependencies which
could not be straightforwardly converted into projec-
tive tree structures, requiring a mechanism similar to
traces in the Penn English Treebank.
Adapting the Data to the Shared Task As the
BCT did not contain all of the original non-projective
dependency trees, we selected the set of 8,000 match-
ing sentences in both treebanks for the shared task.15
This implies that around 2k trees could not be gen-
erated and therefore were discarded. Furthermore,
the BCT annotation scheme does not contain attach-
ment for most of the punctuation marks, so those
were inserted into the BCT using a simple lower-left
attachment heuristic. The same goes for some con-
nectors that could not be aligned in the first phase.
Predicted Morphology In order to obtain pre-
dicted tags for the non-gold scenarios, we used the
following pipeline. First, morphological analysis as
described above was performed, followed by a dis-
ambiguation step. At that point, it is hard to obtain a
single interpretation for each wordform, as determin-
ing the correct interpretation for each wordform may
require knowledge of long-distance elements on top
of the free constituency order of the main phrasal el-
ements in Basque. The disambiguation is performed
by the module by Ezeiza et al (1998), which uses
a combination of knowledge-based disambiguation,
by means of Constraint Grammar (Karlsson et al,
1995; Aduriz et al, 1997), and a posterior statistical
disambiguation module, using an HMM.16
For the shared task data, we chose a setting that
disambiguates most word forms, and retains ? 97%
of the correct interpretations, leaving an ambiguity
level of 1.3 interpretations. For the remaining cases
of ambiguity, we chose the first interpretation, which
corresponds to the most frequent option. This leaves
open the investigation of more complex approaches
for selecting the most appropriate reading.17
4.4 The French Treebank
French is not a morphologically rich language per se,
though its inflectional system is richer than that of
English, and it also exhibits a limited amount of word
order variation occurring at different syntactic levels
including the word level (e.g. pre- or post-nominal
15We generated a 80/10/10 split, ? train/dev/test ? The first 5k
sentences of the train set were used as a basis for the train5k.
16Note that the statistical module can be parametrized accord-
ing to the level of disambiguation to trade off precision and
recall. For example, disambiguation based on the main cate-
gories (abstracting over morpho-syntactic features) maintains
most of the correct interpretations but still gives an output with
several interpretations per wordform.
17This is not an easy task. The ambiguity left is the hardest to
solve given that the knowledge-based and statistical disambigua-
tion processes have not been able to pick out a single reading.
157
adjective, pre- or post-verbal adverbs) and the phrase
level (e.g. possible alternations between post verbal
NPs and PPs). It also has a high degree of multi-
word expressions, that are often ambiguous with a
literal reading as a sequence of simple words. The
syntactic and MWE analysis shows the same kind of
interaction (though to a lesser extent) as morphologi-
cal and syntactic interaction in Semitic languages ?
MWEs help parsing, and syntactic information may
be required to disambiguate MWE identification.
The Data Set The French data sets were gener-
ated from the French Treebank (Abeill? et al, 2003),
which consists of sentences from the newspaper Le
Monde, manually annotated with phrase structures
and morphological information. Part of the treebank
trees are also annotated with grammatical function
tags for dependents of verbs. In the SPMRL shared
task release, we used only this part, consisting of
18,535 sentences,18 split into 14,759 sentences for
training, 1,235 sentences for development, and 2,541
sentences for the final evaluation.19
Adapting the Data to the Shared Task The con-
stituency trees are provided in an extended PTB
bracketed format, with morphological features at the
pre-terminal level only. They contain slight, auto-
matically performed, modifications with respect to
the original trees of the French treebank. The syntag-
matic projection of prepositions and complementiz-
ers was normalized, in order to have prepositions and
complementizers as heads in the dependency trees
(Candito et al, 2010).
The dependency representations are projective de-
pendency trees, obtained through automatic conver-
sion from the constituency trees. The conversion pro-
cedure is an enhanced version of the one described
by Candito et al (2010).
Both the constituency and the dependency repre-
sentations make use of coarse- and fine-grained POS
tags (CPOS and FPOS respectively). The CPOS are
the categories from the original treebank. The FPOS
18The process of functional annotation is still ongoing, the
objective of the FTB providers being to have all the 20000 sen-
tences annotated with functional tags.
19The first 9,981 training sentences correspond to the canoni-
cal 2007 training set. The development set is the same and the
last 1235 sentences of the test set are those of the canonical test
set.
are merged using the CPOS and specific morphologi-
cal information such as verbal mood, proper/common
noun distinction (Crabb? and Candito, 2008).
Multi-Word Expressions The main difference
with respect to previous releases of the bracketed
or dependency versions of the French treebank
lies in the representation of multi-word expressions
(MWEs). The MWEs appear in an extended format:
each MWE bears an FPOS20 and consists of a se-
quence of terminals (hereafter the ?components? of
the MWE), each having their proper CPOS, FPOS,
lemma and morphological features. Note though that
in the original treebank the only gold information
provided for a MWE component is its CPOS. Since
leaving this information blank for MWE components
would have provided a strong cue for MWE recog-
nition, we made sure to provide the same kind of
information for every terminal, whether MWE com-
ponent or not, by providing predicted morphological
features, lemma, and FPOS for MWE components
(even in the ?gold? section of the data set). This infor-
mation was predicted by the Morfette tool (Chrupa?a
et al, 2008), adapted to French (Seddah et al, 2010).
In the constituency trees, each MWE corresponds
to an internal node whose label is the MWE?s FPOS
suffixed by a +, and which dominates the component
pre-terminal nodes.
In the dependency trees, there is no ?node? for a
MWE as a whole, but one node (a terminal in the
CoNLL format) per MWE component. The first com-
ponent of a MWE is taken as the head of the MWE.
All subsequent components of the MWE depend on
the first one, with the special label dep_cpd. Further-
more, the first MWE component bears a feature mwe-
head equal to the FPOS of the MWE. For instance,
the MWE la veille (the day before) is an adverb, con-
taining a determiner component and a common noun
component. Its bracketed representation is (ADV+
(DET la) (NC veille)), and in the dependency repre-
sentation, the noun veille depends on the determiner
la, which bears the feature mwehead=ADV+.
Predicted Morphology For the predicted mor-
phology scenario, we provide data in which the
mwehead has been removed and with predicted
20In the current data, we did not carry along the lemma and
morphological features pertaining to the MWE itself, though this
information is present in the original trees.
158
FPOS, CPOS, lemma, and morphological features,
obtained by training Morfette on the whole train set.
4.5 The German Treebank
German is a fusional language with moderately free
word order, in which verbal elements are fixed in
place and non-verbal elements can be ordered freely
as long as they fulfill the ordering requirements of
the clause (H?hle, 1986).
The Data Set The German constituency data set
is based on the TiGer treebank release 2.2.21 The
original annotation scheme represents discontinuous
constituents such that all arguments of a predicate
are always grouped under a single node regardless of
whether there is intervening material between them
or not (Brants et al, 2002). Furthermore, punctua-
tion and several other elements, such as parentheses,
are not attached to the tree. In order to make the
constituency treebank usable for PCFG parsing, we
adapted this treebank as described shortly.
The conversion of TiGer into dependencies is a
variant of the one by Seeker and Kuhn (2012), which
does not contain empty nodes. It is based on the same
TiGer release as the one used for the constituency
data. Punctuation was attached as high as possible,
without creating any new non-projective edges.
Adapting the Data to the Shared Task For
the constituency version, punctuation and other
unattached elements were first attached to the tree.
As attachment target, we used roughly the respec-
tive least common ancestor node of the right and
left terminal neighbor of the unattached element (see
Maier et al (2012) for details), and subsequently, the
crossing branches were resolved.
This was done in three steps. In the first step, the
head daughters of all nodes were marked using a
simple heuristic. In case there was a daughter with
the edge label HD, this daughter was marked, i.e.,
existing head markings were honored. Otherwise, if
existing, the rightmost daughter with edge label NK
(noun kernel) was marked. Otherwise, as default, the
leftmost daughter was marked. In a second step, for
each continuous part of a discontinuous constituent,
a separate node was introduced. This corresponds
21This version is available from http://www.ims.
uni-stuttgart.de/forschung/ressourcen/
korpora/tiger.html
to the "raising" algorithm described by Boyd (2007).
In a third steps, all those newly introduced nodes
that did not cover the head daughter of the original
discontinuous node were deleted. For the second
and the third step, we used the same script as for the
Swedish constituency data.
Predicted Morphology For the predicted scenario,
a single sequence of POS tags and morphologi-
cal features has been assigned using the MATE
toolchain via a model trained on the train set via cross-
validation on the training set. The MATE toolchain
was used to provide predicted annotation for lem-
mas, POS tags, morphology, and syntax. In order to
achieve the best results for each annotation level, a
10-fold jackknifing was performed to provide realis-
tic features for the higher annotation levels. The pre-
dicted annotation of the 5k training set were copied
from the full data set.22
4.6 The Hebrew Treebank
Modern Hebrew is a Semitic language, characterized
by inflectional and derivational (templatic) morphol-
ogy and relatively free word order. The function
words for from/to/like/and/when/that/the are prefixed
to the next token, causing severe segmentation ambi-
guity for many tokens. In addition, Hebrew orthogra-
phy does not indicate vowels in modern texts, leading
to a very high level of word-form ambiguity.
The Data Set Both the constituency and the de-
pendency data sets are derived from the Hebrew
Treebank V2 (Sima?an et al, 2001; Guthmann et
al., 2009). The treebank is based on just over 6000
sentences from the daily newspaper ?Ha?aretz?, man-
ually annotated with morphological information and
phrase-structure trees and extended with head infor-
mation as described in Tsarfaty (2010, ch. 5). The
unlabeled dependency version was produced by con-
version from the constituency treebank as described
in Goldberg (2011). Both the constituency and depen-
dency trees were annotated with a set grammatical
function labels conforming to Unified Stanford De-
pendencies by Tsarfaty (2013).
22We also provided a predicted-all scenario, in which we
provided morphological analysis lattices with POS and mor-
phological information derived from the analyses of the SMOR
derivational morphology (Schmid et al, 2004). These lattices
were not used by any of the participants.
159
Adapting the Data to the Shared Task While
based on the same trees, the dependency and con-
stituency treebanks differ in their POS tag sets, as
well as in some of the morphological segmentation
decisions. The main effort towards the shared task
was unifying the two resources such that the two tree-
banks share the same lexical yields, and the same
pre-terminal labels. To this end, we took the layering
approach of Goldberg et al (2009), and included two
levels of POS tags in the constituency trees. The
lower level is lexical, conforming to the lexical re-
source used to build the lattices, and is shared by
the two treebanks. The higher level is syntactic, and
follows the tag set and annotation decisions of the
original constituency treebank.23 In addition, we uni-
fied the representation of morphological features, and
fixed inconsistencies and mistakes in the treebanks.
Data Split The Hebrew treebank is one of the
smallest in our language set, and hence it is provided
in only the small (5k) setting. For the sake of com-
parability with the 5k set of the other treebanks, we
created a comparable size of dev/test sets containing
the first and last 500 sentences respectively, where
the rest serve as the 5k training.24
Predicted Morphology The lattices encoding the
morphological ambiguity for the Raw (all) scenario
were produced by looking up the possible analyses
of each input token in the wide-coverage morpholog-
ical analyzer (lexicon) of the Knowledge Center for
Processing Hebrew (Itai and Wintner, 2008; MILA,
2008), with a simple heuristic for dealing with un-
known tokens. A small lattice encoding the possible
analyses of each token was produced separately, and
these token-lattices were concatenated to produce the
sentence lattice. The lattice for a given sentence may
not include the gold analysis in cases of incomplete
lexicon coverage.
The morphologically disambiguated input files for
the Raw (1-best) scenario were produced by run-
ning the raw text through the morphological disam-
23Note that this additional layer in the constituency treebank
adds a relatively easy set of nodes to the trees, thus ?inflating?
the evaluation scores compared to previously reported results.
To compensate, a stricter protocol than is used in this task would
strip one of the two POS layers prior to evaluation.
24This split is slightly different than the split in previous stud-
ies.
biguator (tagger) described in Adler and Elhadad
(2006; Goldberg et al (2008),Adler (2007). The
disambiguator is based on the same lexicon that is
used to produce the lattice files, but utilizes an extra
module for dealing with unknown tokens Adler et al
(2008). The core of the disambiguator is an HMM
tagger trained on about 70M unannotated tokens us-
ing EM, and being supervised by the lexicon.
As in the case of Arabic, we also provided data
for the Predicted (gold token / predicted morphol-
ogy) scenario. We used the same sequence labeler,
Morfette (Chrupa?a et al, 2008), trained on the con-
catenation of POS and morphological gold features,
leading to a model with respectable accuracy.25
4.7 The Hungarian Treebank
Hungarian is an agglutinative language, thus a lemma
can have hundreds of word forms due to derivational
or inflectional affixation (nominal declination and
verbal conjugation). Grammatical information is typ-
ically indicated by suffixes: case suffixes mark the
syntactic relationship between the head and its argu-
ments (subject, object, dative, etc.) whereas verbs
are inflected for tense, mood, person, number, and
the definiteness of the object. Hungarian is also char-
acterized by vowel harmony.26 In addition, there are
several other linguistic phenomena such as causa-
tion and modality that are syntactically expressed in
English but encoded morphologically in Hungarian.
The Data Set The Hungarian data set used in
the shared task is based on the Szeged Treebank,
the largest morpho-syntactic and syntactic corpus
manually annotated for Hungarian. This treebank
is based on newspaper texts and is available in
both constituent-based (Csendes et al, 2005) and
dependency-based (Vincze et al, 2010) versions.
Around 10k sentences of news domain texts were
made available to the shared task.27 Each word is
manually assigned all its possible morpho-syntactic
25POS+morphology prediction accuracy is 91.95% overall
(59.54% for unseen tokens). POS only prediction accuracy is
93.20% overall (71.38% for unseen tokens).
26When vowel harmony applies, most suffixes exist in two
versions ? one with a front vowel and another one with a back
vowel ? and it is the vowels within the stem that determine which
form of the suffix is selected.
27The original treebank contains 82,000 sentences, 1.2 million
words and 250,000 punctuation marks from six domains.
160
tags and lemmas and the appropriate one is selected
according to the context. Sentences were manu-
ally assigned a constituency-based syntactic struc-
ture, which includes information on phrase structure,
grammatical functions (such as subject, object, etc.),
and subcategorization information (i.e., a given NP
is subcategorized by a verb or an infinitive). The
constituency trees were later automatically converted
into dependency structures, and all sentences were
then manually corrected. Note that there exist some
differences in the grammatical functions applied to
the constituency and dependency versions of the tree-
bank, since some morpho-syntactic information was
coded both as a morphological feature and as dec-
oration on top of the grammatical function in the
constituency trees.
Adapting the Data to the Shared Task Origi-
nally, the Szeged Dependency Treebank contained
virtual nodes for elided material (ELL) and phonolog-
ically covert copulas (VAN). In the current version,
they have been deleted, their daughters have been
attached to the parent of the virtual node, and have
been given complex labels, e.g. COORD-VAN-SUBJ,
where VAN is the type of the virtual node deleted,
COORD is the label of the virtual node and SUBJ is
the label of the daughter itself. When the virtual node
was originally the root of the sentence, its daughter
with a predicative (PRED) label has been selected as
the new root of the sentence (with the label ROOT-
VAN-PRED) and all the other daughters of the deleted
virtual node have been attached to it.
Predicted Morphology In order to provide the
same POS tag set for the constituent and dependency
treebanks, we used the dependency POS tagset for
both treebank instances. Both versions of the tree-
bank are available with gold standard and automatic
morphological annotation. The automatic POS tag-
ging was carried out by a 10-fold cross-validation
on the shared task data set by magyarlanc, a natu-
ral language toolkit for processing Hungarian texts
(segmentation, morphological analysis, POS tagging,
and dependency parsing). The annotation provides
POS tags and deep morphological features for each
input token (Zsibrita et al, 2013).28
28The full data sets of both the constituency and de-
pendency versions of the Szeged Treebank are available at
4.8 The Korean Treebank
The Treebank The Korean corpus is generated by
collecting constituent trees from the KAIST Tree-
bank (Choi et al, 1994), then converting the con-
stituent trees to dependency trees using head-finding
rules and heuristics. The KAIST Treebank consists
of about 31K manually annotated constituent trees
from 97 different sources (e.g., newspapers, novels,
textbooks). After filtering out trees containing an-
notation errors, a total of 27,363 trees with 350,090
tokens are collected.
The constituent trees in the KAIST Treebank29 also
come with manually inspected morphological analy-
sis based on ?eojeol?. An eojeol contains root-forms
of word tokens agglutinated with grammatical affixes
(e.g., case particles, ending markers). An eojeol can
consist of more than one word token; for instance, a
compound noun ?bus stop? is often represented as
one eojeol in Korean, ????????????, which can be
broken into two word tokens,???? (bus) and????????
(stop). Each eojeol in the KAIST Treebank is sepa-
rated by white spaces regardless of punctuation. Fol-
lowing the Penn Korean Treebank guidelines (Han
et al, 2002), punctuation is separated as individual
tokens, and parenthetical notations surrounded by
round brackets are grouped into individual phrases
with a function tag (PRN in our corpus).
All dependency trees are automatically converted
from the constituent trees. Unlike English, which
requires complicated head-finding rules to find the
head of each phrase (Choi and Palmer, 2012), Ko-
rean is a head final language such that the rightmost
constituent in each phrase becomes the head of that
phrase. Moreover, the rightmost conjunct becomes
the head of all other conjuncts and conjunctions in
a coordination phrase, which aligns well with our
head-final strategy.
The constituent trees in the KAIST Treebank do
not consist of function tags indicating syntactic or
semantic roles, which makes it difficult to generate
dependency labels. However, it is possible to gener-
ate meaningful labels by using the rich morphology
in Korean. For instance, case particles give good
the following website: www.inf.u-szeged.hu/rgai/
SzegedTreebank, and magyarlanc is downloadable from:
www.inf.u-szeged.hu/rgai/magyarlanc.
29See Lee et al (1997) for more details about the bracketing
guidelines of the KAIST Treebank.
161
indications of what syntactic roles eojeols with such
particles should take. Given this information, 21
dependency labels were generated according to the
annotation scheme proposed by Choi (2013).
Adapting the Data to the Shared Task All details
concerning the adaptation of the KAIST treebank
to the shared task specifications are found in Choi
(2013). Importantly, the rich KAIST treebank tag set
of 1975 POS tag types has been converted to a list of
CoNLL-like feature-attribute values refining coarse
grained POS categories.
Predicted Morphology Two sets of automatic
morphological analyses are provided for this task.
One is generated by the HanNanum morphological
analyzer.30 The HanNanum morphological ana-
lyzer gives the same morphemes and POS tags as the
KAIST Treebank. The other is generated by the Se-
jong morphological analyzer.31 The Sejong morpho-
logical analyzer gives a different set of morphemes
and POS tags as described in Choi and Palmer (2011).
4.9 The Polish Treebank
The Data Set Sk?adnica is a constituency treebank
of Polish (Wolin?ski et al, 2011; S?widzin?ski and
Wolin?ski, 2010). The trees were generated with
a non-probabilistic DCG parser S?wigra and then
disambiguated and validated manually. The ana-
lyzed texts come from the one-million-token sub-
corpus of the National Corpus of Polish (NKJP,
(Przepi?rkowski et al, 2012)) manually annotated
with morpho-syntactic tags.
The dependency version of Sk?adnica is a re-
sult of an automatic conversion of manually disam-
biguated constituent trees into dependency structures
(Wr?blewska, 2012). The conversion was an entirely
automatic process. Conversion rules were based
on morpho-syntactic information, phrasal categories,
and types of phrase-structure rules encoded within
constituent trees. It was possible to extract dependen-
cies because the constituent trees contain information
about the head of the majority of constituents. For
other constituents, heuristics were defined in order to
select their heads.
30http://kldp.net/projects/hannanum
31http://www.sejong.or.kr
The version of Sk?adnica used in the shared task
comprises parse trees for 8,227 sentences.32
Predicted Morphology For the shared task Pre-
dicted scenario, an automatic morphological an-
notation was generated by the PANTERA tagger
(Acedan?ski, 2010).
4.10 The Swedish Treebank
Swedish is moderately rich in inflections, including
a case system. Word order obeys the verb second
constraint in main clauses but is SVO in subordinate
clauses. Main clause order is freer than in English
but not as free as in some other Germanic languages,
such as German. Also, subject agreement with re-
spect to person and number has been dropped in
modern Swedish.
The Data Set The Swedish data sets are taken
from the Talbanken section of the Swedish Treebank
(Nivre and Megyesi, 2007). Talbanken is a syntacti-
cally annotated corpus developed in the 1970s, orig-
inally annotated according to the MAMBA scheme
(Teleman, 1974) with a syntactic layer consisting
of flat phrase structure and grammatical functions.
The syntactic annotation was later automatically con-
verted to full phrase structure with grammatical func-
tions and from that to dependency structure, as de-
scribed by Nivre et al (2006).
Both the phrase structure and the dependency
version use the functional labels from the original
MAMBA scheme, which provides a fine-grained clas-
sification of syntactic functions with 65 different la-
bels, while the phrase structure annotation (which
had to be inferred automatically) uses a coarse set
of only 8 labels. For the release of the Swedish tree-
bank, the POS level was re-annotated to conform to
the current de facto standard for Swedish, which is
the Stockholm-Ume? tagset (Ejerhed et al, 1992)
with 25 base tags and 25 morpho-syntactic features,
which together produce over 150 complex tags.
For the shared task, we used version 1.2 of the
treebank, where a number of conversion errors in
the dependency version have been corrected. The
phrase structure version was enriched by propagating
morpho-syntactic features from preterminals (POS
32Sk?adnica is available from http://zil.ipipan.waw.
pl/Sklicense.
162
tags) to higher non-terminal nodes using a standard
head percolation table, and a version without crossing
branches was derived using the lifting strategy (Boyd,
2007).
Adapting the Data to the Shared Task Explicit
attribute names were added to the feature field and the
split was changed to match the shared task minimal
training set size.
Predicted Morphology POS tags and morpho-
syntactic features were produced using the Hun-
PoS tagger (Hal?csy et al, 2007) trained on the
Stockholm-Ume? Corpus (Ejerhed and K?llgren,
1997).
5 Overview of the Participating Systems
With 7 teams participating, more than 14 systems for
French and 10 for Arabic and German, this shared
task is on par with the latest large-scale parsing evalu-
ation campaign SANCL 2012 (Petrov and McDonald,
2012). The present shared task was extremely de-
manding on our participants. From 30 individuals or
teams who registered and obtained the data sets, we
present results for the seven teams that accomplished
successful executions on these data in the relevant
scenarios in the given the time frame.
5.1 Dependency Track
Seven teams participated in the dependency track.
Two participating systems are based on MaltParser:
MALTOPTIMIZER (Ballesteros, 2013) and AI:KU
(Cirik and S?ensoy, 2013). MALTOPTIMIZER uses
a variant of MaltOptimizer (Ballesteros and Nivre,
2012) to explore features relevant for the processing
of morphological information. AI:KU uses a combi-
nation of MaltParser and the original MaltOptimizer.
Their system development has focused on the inte-
gration of an unsupervised word clustering method
using contextual and morphological properties of the
words, to help combat sparseness.
Similarly to MaltParser ALPAGE:DYALOG
(De La Clergerie, 2013) also uses a shift-reduce
transition-based parser but its training and decoding
algorithms are based on beam search. This parser is
implemented on top of the tabular logic programming
system DyALog. To the best of our knowledge, this
is the first dependency parser capable of handling
word lattice input.
Three participating teams use the MATE parser
(Bohnet, 2010) in their systems: the BASQUETEAM
(Goenaga et al, 2013), IGM:ALPAGE (Constant et
al., 2013) and IMS:SZEGED:CIS (Bj?rkelund et al,
2013). The BASQUETEAM uses the MATE parser in
combination with MaltParser (Nivre et al, 2007b).
The system combines the parser outputs via Malt-
Blender (Hall et al, 2007). IGM:ALPAGE also uses
MATE and MaltParser, once in a pipeline architec-
ture and once in a joint model. The models are com-
bined via a re-parsing strategy based on (Sagae and
Lavie, 2006). This system mainly focuses on MWEs
in French and uses a CRF tagger in combination
with several large-scale dictionaries to handle MWEs,
which then serve as input for the two parsers.
The IMS:SZEGED:CIS team participated in both
tracks, with an ensemble system. For the depen-
dency track, the ensemble includes the MATE parser
(Bohnet, 2010), a best-first variant of the easy-first
parser by Goldberg and Elhadad (2010b), and turbo
parser (Martins et al, 2010), in combination with
a ranker that has the particularity of using features
from the constituent parsed trees. CADIM (Marton et
al., 2013b) uses their variant of the easy-first parser
combined with a feature-rich ensemble of lexical and
syntactic resources.
Four of the participating teams use exter-
nal resources in addition to the parser. The
IMS:SZEGED:CIS team uses external morpholog-
ical analyzers. CADIM uses SAMA (Graff et al,
2009) for Arabic morphology. ALPAGE:DYALOG
and IGM:ALPAGE use external lexicons for French.
IGM:ALPAGE additionally uses Morfette (Chrupa?a
et al, 2008) for morphological analysis and POS
tagging. Finally, as already mentioned, AI:KU clus-
ters words and POS tags in an unsupervised fashion
exploiting additional, un-annotated data.
5.2 Constituency Track
A single team participated in the constituency parsing
task, the IMS:SZEGED:CIS team (Bj?rkelund et al,
2013). Their phrase-structure parsing system uses a
combination of 8 PCFG-LA parsers, trained using a
product-of-grammars procedure (Petrov, 2010). The
50-best parses of this combination are then reranked
by a model based on the reranker by Charniak and
163
Johnson (2005).33
5.3 Baselines
We additionally provide the results of two baseline
systems for the nine languages, one for constituency
parsing and one for dependency parsing.
For the dependency track, our baseline system is
MaltParser in its default configuration (the arc-eager
algorithm and liblinear for training). Results marked
as BASE:MALT in the next two sections report the
results of this baseline system in different scenarios.
The constituency parsing baseline is based on the
most recent version of the PCFG-LA model of Petrov
et al (2006), used with its default settings and five
split/merge cycles, for all languages.34 We use this
parser in two configurations: a ?1-best? configura-
tion where all POS tags are provided to the parser
(predicted or gold, depending on the scenario), and
another configuration in which the parser performs
its own POS tagging. These baselines are referred to
as BASE:BKY+POS and BASE:BKY+RAW respec-
tively in the following results sections. Note that
even when BASE:BKY+POS is given gold POS tags,
the Berkeley parser sometimes fails to reach a perfect
POS accuracy. In cases when the parser cannot find a
parse with the provided POS, it falls back on its own
POS tagging for all tokens.
6 Results
The high number of submitted system variants and
evaluation scenarios in the task resulted in a large
number of evaluation scores. In the following evalu-
ation, we focus on the best run for each participant,
and we aim to provide key points on the different
dimensions of analysis resulting from our evaluation
protocol. We invite our interested readers to browse
the comprehensive representation of our results on
the official shared-task results webpages.35
33Note that a slight but necessary change in the configuration
of one of our metrics, which occurred after the system submis-
sion deadline, resulted in the IMS:SZEGED:CIS team to submit
suboptimal systems for 4 languages. Their final scores are ac-
tually slightly higher and can be found in (Bj?rkelund et al,
2013).
34For Semitic languages, we used the lattice based PCFG-LA
extension by Goldberg (2011).
35http://www.spmrl.org/
spmrl2013-sharedtask-results.html.
6.1 Gold Scenarios
This section presents the parsing results in gold sce-
narios, where the systems are evaluated on gold seg-
mented and tagged input. This means that the se-
quence of terminals, POS tags, and morphological
features are provided based on the treebank anno-
tations. This scenario was used in most previous
shared tasks on data-driven parsing (Buchholz and
Marsi, 2006; Nivre et al, 2007a; K?bler, 2008). Note
that this scenario was not mandatory. We thank our
participants for providing their results nonetheless.
We start by reviewing dependency-based parsing
results, both on the trees and on multi-word expres-
sion, and continue with the different metrics for
constituency-based parsing.
6.1.1 Dependency Parsing
Full Training Set The results for the gold parsing
scenario of dependency parsing are shown in the top
block of table 3.
Among the six systems, IMS:SZEGED:CIS
reaches the highest LAS scores, not only on aver-
age, but for every single language. This shows that
their approach of combining parsers with (re)ranking
provides robust parsing results across languages with
different morphological characteristics. The second
best system is ALPAGE:DYALOG, the third best sys-
tem is MALTOPTIMIZER. The fact that AI:KU is
ranked below the Malt baseline is due to their sub-
mission of results for 6 out of the 9 languages. Simi-
larly, CADIM only submitted results for Arabic and
ranked in the third place for this language, after the
two IMS:SZEGED:CIS runs. IGM:ALPAGE and
BASQUETEAM did not submit results for this setting.
Comparing LAS results across languages is prob-
lematic due to the differences between languages,
treebank size and annotation schemes (see section 3),
so the following discussion is necessarily tentative. If
we consider results across languages, we see that the
lowest results (around 83% for the best performing
system) are reached for Hebrew and Swedish, the
languages with the smallest data sets. The next low-
est result, around 86%, is reached for Basque. Other
languages reach similar LAS scores, around 88-92%.
German, with the largest training set, reaches the
highest LAS, 91.83%.
Interstingly, all systems have high LAS scores
on the Korean Treebank given a training set size
164
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 89.83 86.68 90.29 91.83 83.87 88.06 89.59 89.58 83.97 88.19
ALPAGE:DYALOG 85.87 80.39 87.69 88.25 80.70 79.60 88.23 86.00 79.80 84.06
MALTOPTIMIZER 87.03 82.07 85.71 86.96 80.03 83.14 89.39 80.49 77.67 83.61
BASE:MALT 82.28 69.19 79.86 79.98 76.61 72.34 88.43 77.70 75.73 78.01
AI:KU 86.39 86.98 79.42 83.67 85.16 78.87 55.61
CADIM 85.56 9.51
2) gold setting / 5k training set
IMS:SZEGED:CIS 87.35 85.69 88.73 87.70 83.87 87.21 83.38 89.16 83.97 86.34
ALPAGE:DYALOG 83.25 79.11 85.66 83.88 80.70 78.42 81.91 85.67 79.80 82.04
MALTOPTIMIZER 85.30 81.40 84.93 83.59 80.03 82.37 83.74 79.79 77.67 82.09
BASE:MALT 80.36 67.13 78.16 76.64 76.61 71.27 81.93 76.64 75.73 76.05
AI:KU 84.98 83.47 79.42 82.84 84.37 78.87 54.88
CADIM 82.67 9.19
3) predicted setting / full training set
IMS:SZEGED:CIS 86.21 85.14 85.24 89.65 80.89 86.13 86.62 87.07 82.13 85.45
ALPAGE:DYALOG 81.20 77.55 82.06 84.80 73.63 75.58 81.02 82.56 77.54 79.55
MALTOPTIMIZER 81.90 78.58 79.00 82.75 73.01 79.63 82.65 79.89 75.82 79.25
BASE:MALT 80.36 70.11 77.98 77.81 69.97 70.15 82.06 75.63 73.21 75.25
AI:KU 72.57 82.32 69.01 78.92 81.86 76.35 51.23
BASQUETEAM 84.25 84.51 88.66 84.97 80.88 47.03
IGM:ALPAGE 85.86 9.54
CADIM 83.20 9.24
4) predicted setting / 5k training set
IMS:SZEGED:CIS 83.66 83.84 83.45 85.08 80.89 85.24 80.80 86.69 82.13 83.53
MALTOPTIMIZER 79.64 77.59 77.56 79.22 73.01 79.00 75.90 79.50 75.82 77.47
ALPAGE:DYALOG 78.65 76.06 80.11 73.07 73.63 74.48 73.79 82.04 77.54 76.60
BASE:MALT 78.48 68.12 76.54 74.81 69.97 69.08 74.87 75.29 73.21 73.37
AI:KU 71.23 79.16 69.01 78.04 81.30 76.35 50.57
BASQUETEAM 83.19 82.65 84.70 84.01 80.88 46.16
IGM:ALPAGE 83.60 9.29
CADIM 80.51 8.95
Table 3: Dependency parsing: LAS scores for full and 5k training sets and for gold and predicted input. Results in bold
show the best results per language and setting.
of approximately 23,000 sentences, which is a little
over half of the German treebank. For German, on
the other hand, only the IMS:SZEGED:CIS system
reaches higher LAS scores than for Korean. This
final observation indicates that more than treebank
size is important for comparing system performance
across treebanks. This is the reason for introducing
the reduced set scenario, in which we can see how the
participating system perform on a common ground,
albeit small.
5k Training Set The results for the gold setting
on the 5k train set are shown in the second block
of Table 3. Compared with the full training, we
see that there is a drop of around 2 points in this
setting. Some parser/language pairs are more sensi-
tive to data sparseness than others. CADIM, for in-
stance, exhibit a larger drop than MALTOPTIMIZER
on Arabic, and MALTOPTIMIZER shows a smaller
drop than IMS:SZEGED:CIS on French. On average,
among all systems that covered all languages, MALT-
OPTIMIZER has the smallest drop when moving to
5k training, possibly since the automatic feature opti-
mization may differ for different data set sizes.
Since all languages have the same number of sen-
tences in the train set, these results can give us limited
insight into the parsing complexity of the different
treebanks. Here, French, Arabic, Polish, and Korean
reach the highest LAS scores while Swedish reaches
165
Team F_MWE F_COMP F_MWE+POS
1) gold setting / full training set
AI:KU 99.39 99.53 99.34
IMS:SZEGED:CIS 99.26 99.39 99.21
MALTOPTIMIZER 98.95 98.99 0
ALPAGE:DYALOG 98.32 98.81 0
BASE:MALT 68.7 72.55 68.7
2) predicted setting / full training set
IGM:ALPAGE 80.81 81.18 77.37
IMS:SZEGED:CIS 79.45 80.79 70.48
ALPAGE:DYALOG 77.91 79.25 0
BASQUE-TEAM 77.19 79.81 0
MALTOPTIMIZER 70.29 74.25 0
BASE:MALT 67.49 71.01 0
AI:KU 0 0 0
3) predicted setting / 5k training set
IGM:ALPAGE 77.66 78.68 74.04
IMS:SZEGED:CIS 77.28 78.92 70.42
ALPAGE:DYALOG 75.17 76.82 0
BASQUETEAM 73.07 76.58 0
MALTOPTIMIZER 65.76 70.42 0
BASE:MALT 62.05 66.8 0
AI:KU 0 0 0
Table 4: Dependency Parsing: MWE results
the lowest one. Treebank variance depends not only
on the language but also on annotation decisions,
such as label set (Swedish, interestingly, has a rela-
tively rich one). A more careful comparison would
then take into account the correlation of data size,
label set size and parsing accuracy. We investigate
these correlations further in section 7.1.
6.1.2 Multiword Expressions
MWE results on the gold setting are found at
the top of Table 4. All systems, with the excep-
tion of BASE:MALT, perform exceedingly well in
identifying the spans and non-head components of
MWEs given gold morphology.36 These almost per-
fect scores are the consequence of the presence of
two gold MWE features, namely MWEHEAD and
PRED=Y, which respectively indicate the node span
of the whole MWE and its dependents, which do not
have a gold feature field. The interesting scenario is,
of course, the predicted one, where these features are
not provided to the parser, as in any realistic applica-
tion.
36Note that for the labeled measure F_MWE+POS, both
MALTOPTIMIZER and ALPAGE:DYALOG have an F-score of
zero, since they do not attempt to predict the MWE label at all.
6.1.3 Constituency Parsing
In this part, we provide accuracy results for phrase-
structure trees in terms of ParsEval F-scores. Since
ParsEval is sensitive to the non-terminals-per-word
ratio in the data set (Rehbein and van Genabith,
2007a; Rehbein and van Genabith, 2007b), and given
the fact that this ratio varies greatly within our data
set (as shown in Table 2), it must be kept in mind that
ParsEval should only be used for comparing parsing
performance over treebank instances sharing the ex-
act same properties in term of annotation schemes,
sentence length and so on. When comparing F-Scores
across different treebanks and languages, it can only
provide a rough estimate of the relative difficulty or
ease of parsing these kinds of data.
Full Training Set The F-score results for the gold
scenario are provided in the first block of Table 5.
Among the two baselines, BASE:BKY+POS fares
better than BASE:BKY+RAW since the latter selects
its own POS tags and thus cannot benefit from the
gold information. The IMS:SZEGED:CIS system
clearly outperforms both baselines, with Hebrew as
an outlier.37
As in the dependency case, the results are not
strictly comparable across languages, yet we can
draw some insights from them. We see consider-
able differences between the languages, with Basque,
Hebrew, and Hungarian reaching F-scores in the low
90s for the IMS:SZEGED:CIS system, Korean and
Polish reaching above-average F-scores, and Ara-
bic, French, German, and Swedish reaching F-scores
below the average, but still in the low 80s. The per-
formance is, again, not correlated with data set sizes.
Parsing Hebrew, with one of the smallest training
sets, obtains higher accuracy many other languages,
including Swedish, which has the same training set
size as Hebrew. It may well be that gold morphologi-
cal information is more useful for combatting sparse-
ness in languages with richer morphology (though
Arabic here would be an outlier for this conjecture),
or it may be that certain treebanks and schemes are
inherently harder to parser than others, as we investi-
gate in section 7.
For German, the language with the largest training
37It might be that the easy layer of syntactic tags benefits from
the gold POS tags provided. See section 4 for further discussion
of this layer.
166
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 82.20 90.04 83.98 82.07 91.64 92.60 86.50 88.57 85.09 86.97
BASE:BKY+POS 80.76 76.24 81.76 80.34 92.20 87.64 82.95 88.13 82.89 83.66
BASE:BKY+RAW 79.14 69.78 80.38 78.99 87.32 81.44 73.28 79.51 78.94 78.75
2) gold setting / 5k training set
IMS:SZEGED:CIS 79.47 88.45 82.25 74.78 91.64 91.87 80.10 88.18 85.09 84.65
BASE:BKY+POS 77.54 74.06 78.07 71.37 92.20 86.74 72.85 87.91 82.89 80.40
BASE:BKY+RAW 75.22 67.16 75.91 68.94 87.32 79.34 60.40 78.30 78.94 74.61
3) predicted setting / full training set
IMS:SZEGED:CIS 81.32 87.86 81.83 81.27 89.46 91.85 84.27 87.55 83.99 85.49
BASE:BKY+POS 78.66 74.74 79.76 78.28 85.42 85.22 78.56 86.75 80.64 80.89
BASE:BKY+RAW 79.19 70.50 80.38 78.30 86.96 81.62 71.42 79.23 79.18 78.53
4) predicted setting / 5k training set
IMS:SZEGED:CIS 78.85 86.65 79.83 73.61 89.46 90.53 78.47 87.46 83.99 83.21
BASE:BKY+POS 74.84 72.35 76.19 69.40 85.42 83.82 67.97 87.17 80.64 77.53
BASE:BKY+RAW 74.57 66.75 75.76 68.68 86.96 79.35 58.49 78.38 79.18 74.24
Table 5: Constituent Parsing: ParsEval F-scores for full and 5k training sets and for gold and predicted input. Results in
bold show the best results per language and setting.
set and the highest scores in dependency parsing,
the F-scores are at the lower end. These low scores,
which are obtained despite the larger treebank and
only moderately free word-order, are surprising. This
may be due to case syncretism; gold morphological
information exhibits its own ambiguity and thus may
not be fully utilized.
5k Training Set Parsing results on smaller com-
parable test sets are presented in the second block
of Table 5. On average, IMS:SZEGED:CIS is less
sensitive than BASE:BKY+POS to the reduced size.
Systems are not equally sensitive to reduced training
sets, and the gaps range from 0.4% to 3%, with Ger-
man and Korean as outliers (Korean suffering a 6.4%
drop in F-score and German 7.3%). These languages
have the largest treebanks in the full setting, so it is
not surprising that they suffer the most. But this in
itself does not fully explain the cross-treebank trends.
Since ParsEval scores are known to be sensitive to
the label set sizes and the depth of trees, we provide
LeafAncestor scores in the following section.
6.1.4 Leaf-Ancestor Results
The variation across results in the previous subsec-
tion may have been due to differences across annota-
tion schemes. One way to neutralize this difference
(to some extent) is to use a different metric. We
evaluated the constituency parsing results using the
Leaf-Ancestor (LA) metric, which is less sensitive
to the number of nodes in a tree (Rehbein and van
Genabith, 2007b; K?bler et al, 2008). As shown in
Table 6, these results are on a different (higher) scale
than ParsEval, and the average gap between the full
and 5k setting is lower.
Full Training Set The LA results in gold setting
for full training sets are shown in the first block of Ta-
ble 6. The trends are similar to the ParsEval F-scores.
German and Arabic present the lowest LA scores
(in contrast to the corresponding F-scores, Arabic is
a full point below German for IMS:SZEGED:CIS).
Basque and Hungarian have the highest LA scores.
Hebrew, which had a higher F-score than Basque,
has a lower LA than Basque and is closer to French.
Korean also ranks worse in the LA analysis. The
choice of evaluation metrics thus clearly impacts sys-
tem rankings ? F-scores rank some languages suspi-
ciously high (e.g., Hebrew) due to deeper trees, and
another metric may alleviate that.
5k Training Set The results for the leaf-ancestor
(LA) scores in the gold setting for the 5k training set
are shown in the second block of Table 6. Across
167
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 88.61 94.90 92.51 89.63 92.84 95.01 91.30 94.52 91.46 92.31
BASE:BKY+POS 87.85 91.55 91.74 88.47 92.69 92.52 90.82 92.81 90.76 91.02
BASE:BKY+RAW 87.05 89.71 91.22 87.77 91.29 90.62 87.11 90.58 88.97 89.37
2) gold setting / 5k training set
IMS:SZEGED:CIS 86.68 94.21 91.56 85.74 92.84 94.79 88.87 94.17 91.46 91.15
BASE:BKY+POS 86.26 90.72 89.71 84.11 92.69 92.11 86.75 92.91 90.76 89.56
BASE:BKY+RAW 84.97 88.68 88.74 83.08 91.29 89.94 81.82 90.31 88.97 87.53
3) predicted setting / full training set
IMS:SZEGED:CIS 88.45 94.50 91.79 89.32 91.95 94.90 90.13 94.11 91.05 91.80
BASE:BKY+POS 86.60 90.90 90.96 87.46 89.66 91.72 89.10 92.56 89.51 89.83
BASE:BKY+RAW 86.97 89.91 91.11 87.46 90.77 90.50 86.68 90.48 89.16 89.23
4) predicted setting / 5k training set
IMS:SZEGED:CIS 86.69 93.85 90.76 85.20 91.95 94.05 87.99 93.99 91.05 90.61
BASE:BKY+POS 84.76 89.83 89.18 83.05 89.66 91.24 84.87 92.74 89.51 88.32
BASE:BKY+RAW 84.63 88.50 89.00 82.69 90.77 89.93 81.50 90.08 89.16 87.36
Table 6: Constituent Parsing: Leaf-Ancestor scores for full and 5k training sets and for gold and predicted input.
parsers, IMS:SZEGED:CIS again has a smaller drop
than BASE:BKY+POS on the reduced size. German
suffers the most from the reduction of the training
set, with a loss of approximately 4 points. Korean,
however, which was also severely affected in terms
of F-scores, only loses 1.17 points in the LA score.
On average, the LA seem to reflect a smaller drop
when reducing the training set ? this underscores
again the impact of the choice of metrics on system
evaluation.
6.2 Predicted Scenarios
Gold scenarios are relatively easy since syntactically
relevant morphological information is disambiguated
in advance and is provided as input. Predicted scenar-
ios are more difficult: POS tags and morphological
features have to be automatically predicted, by the
parser or by external resources.
6.2.1 Dependency Parsing
Eight participating teams submitted dependency
results for this scenario. Two teams submitted for a
single language. Four teams covered all languages.
Full Training Set The results for the predicted
scenario in full settings are shown in the third
block of Table 3. Across the board, the re-
sults are considerably lower than the gold sce-
nario. Again, IMS:SZEGED:CIS is the best per-
forming system, followed by ALPAGE:DYALOG and
MALTOPTIMIZER. The only language for which
IMS:SZEGED:CIS is outperformed is French, for
which IGM:ALPAGE reaches higher results (85.86%
vs. 85.24%). This is due to the specialized treatment
of French MWEs in the IGM:ALPAGE system, which
is thereby shown to be beneficial for parsing in the
predicted setting.
If we compare the results for the predicted set-
ting and the gold one, given the full training set,
the IMS:SZEGED:CIS system shows small differ-
ences between 1.5 and 2 percent. The only ex-
ception is French, for which the LAS drops from
90.29% to 85.24% in the predicted setting. The
other systems show somewhat larger differences than
IMS:SZEGED:CIS, with the highest drops for Ara-
bic and Korean. The AI:KU system shows a similar
problem as IMS:SZEGED:CIS for French.
5k Training Set When we consider the predicted
setting for the 5k training set, in the last block of
Table 3, we see the same trends as comparing with
the full training set or when comparing to the gold
setting. Systems suffer from not having gold stan-
dard data, and they suffer from the small training set.
Interestingly, the loss between the different training
set sizes in the predicted setting is larger than in the
168
gold setting, but only marginally so, with a differ-
ence < 0.5. In other words, the predicted setting
adds a challenge to parsing, but it only minimally
compounds data sparsity.
6.2.2 Multiword Expressions Evaluation
In the predicted setting, shown in the second
block of table 4 for the full training set and in the
third block of the same table for the 5k training set,
we see that only two systems, IGM:ALPAGE and
IMS:SZEGED:CIS can predict the MWE label when
it is not present in the training set. IGM:ALPAGE?s
approach of using a separate classifier in combination
with external dictionaries is very successful, reach-
ing an F_MWE+POS score of 77.37. This is com-
pared to the score of 70.48 by IMS:SZEGED:CIS,
which predicts this node label as a side effect of
their constituent feature enriched dependency model
(Bj?rkelund et al, 2013). AI:KU has a zero score
for all predicted settings, which results from an erro-
neous training on the gold data rather than the pre-
dicted data.38
6.2.3 Constituency Parsing
Full Training Set The results for the predicted set-
ting with the full training set are shown in the third
block of table 5. A comparison with the gold setting
shows that all systems have a lower performance in
the predicted scenario, and the differences are in the
range of 0.88 for Arabic and 2.54 for Basque. It is
interesting to see that the losses are generally smaller
than in the dependency framework: on average, the
loss across languages is 2.74 for dependencies and
1.48 for constituents. A possible explanation can be
found in the two-dimensional structure of the con-
stituent trees, where only a subset of all nodes is
affected by the quality of morphology and POS tags.
The exception to this trend is Basque, for which the
loss in constituents is a full point higher than for de-
pendencies. Another possible explanation is that all
of our constituent parsers select their own POS tags
in one way or another. Most dependency parsers ac-
cept predicted tags from an external resource, which
puts an upper-bound on their potential performance.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the bottom
38Unofficial updated results are to to be found in (Cirik and
S?ensoy, 2013)
block of table 5. They show the same trends as the
dependency ones: The results are slightly lower than
the results obtained in gold setting and the ones uti-
lizing the full training set.
6.2.4 Leaf Ancestor Metrics
Full Training Set The results for the predicted sce-
nario with a full training set are shown in the third
block of table 6. In the LA evaluation, the loss
in moving from gold morphology are considerably
smaller than in F-scores. For most languages, the
loss is less than 0.5 points. Exceptions are French
with a loss of 0.72, Hebrew with 0.89, and Korean
with 1.17. Basque, which had the highest loss in
F-scores, only shows a minor loss of 0.4 points. Also,
the average loss of 0.41 points is much smaller than
the one in the ParsEval score, 1.48.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the last
block of table 6. These results, though considerably
lower (around 3 points), exhibit the exact same trends
as observed in the gold setting.
6.3 Realistic Raw Scenarios
The previous scenarios assume that input surface to-
kens are identical to tree terminals. For languages
such as Arabic and Hebrew, this is not always the
case. In this scenario, we evaluate the capacity of a
system to predict both morphological segmentation
and syntactic parse trees given raw, unsegmented
input tokens. This may be done via a pipeline as-
suming a 1-st best morphological analysis, or jointly
with parsing, assuming an ambiguous morpholog-
ical analysis lattice as input. In this task, both of
these scenarios are possible (see section 3). Thus,
this section presents a realistic evaluation of the par-
ticipating systems, using TedEval, which takes into
account complete morpho-syntactic parses.
Tables 7 and 8 present labeled and unlabeled
TedEval results for both constituency and depen-
dency parsers, calculated only for sentence of length
<= 70.39 We firstly observe that labeled TedEval
scores are considerably lower than unlabeled Ted-
Eval scores, as expected, since unlabeled scores eval-
uate only structural differences. In the labeled setup,
39TedEval builds on algorithms for calculating edit distance
on complete trees (Bille, 2005). In these algorithms, longer
sentences take considerably longer to evaluate.
169
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 83.34 1.63 82.54 0.67 56.47 0.67 69.51 69.51
IMS:SZEGED:CIS 89.12 8.37 87.82 5.56 86.08 8.27 86.95 86.95
CADIM 87.81 6.63 86.43 4.21 - - 43.22 86.43
MALTOPTIMIZER 86.74 5.39 85.63 3.03 83.05 5.33 84.34 84.34
ALPAGE:DYALOG 86.60 5.34 85.71 3.54 82.96 6.17 41.48 82.96
ALPAGE:DYALOG (RAW) - - - - 82.82 4.35 41.41 82.82
AI:KU - - - - 78.57 3.37 39.29 78.57
Table 7: Realistic Scenario: Tedeval Labeled Accuracy and Exact Match for the Raw scenario.
The upper part refers to constituency results, the lower part refers to dependency results
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 92.06 9.49 91.29 7.13 89.30 13.60 90.30 90.30
IMS:SZEGED:CIS 91.74 9.83 90.85 7.30 89.47 16.97 90.16 90.16
ALPAGE:DYALOG 89.99 7.98 89.46 5.67 88.33 12.20 88.90 88.90
MALTOPTIMIZER 90.09 7.08 89.47 5.56 87.99 11.64 88.73 88.73
CADIM 90.75 8.48 89.89 5.67 - - 44.95 89.89
ALPAGE:DYALOG (RAW) - - - - 87.61 10.24 43.81 87.61
AI:KU - - - - 86.70 8.98 43.35 86.70
Table 8: Realistic Scenario: Tedeval Unlabeled Accuracy and Exact Match for the Raw scenario.
Top upper part refers to constituency results, the lower part refers to dependency results.
the IMS:SZEGED:CIS dependency parser are the
best for both languages and data set sizes. Table 8
shows that their unlabeled constituency results reach
a higher accuracy than the next best system, their
own dependency results. However, a quick look at
the exact match metric reveals lower scores than for
its dependency counterparts.
For the dependency-based joint scenarios, there
is obviously an upper bound on parser performance
given inaccurate segmentation. The transition-based
systems, ALPAGE:DYALOG & MALTOPTIMIZER,
perform comparably on Arabic and Hebrew, with
ALPAGE:DYALOG being slightly better on both lan-
guages. Note that ALPAGE:DYALOG reaches close
results on the 1-best and the lattice-based input set-
tings, with a slight advantage for the former. This is
partly due to the insufficient coverage of the lexical
resource we use: many lattices do not contain the
gold path, so the joint prediction can only as be high
as the lattice predicted path allows.
7 Towards In-Depth Cross-Treebank
Evaluation
Section 6 reported evaluation scores across systems
for different scenarios. However, as noted, these re-
sults are not comparable across languages, represen-
tation types and parsing scenarios due to differences
in the data size, label set size, length of sentences and
also differences in evaluation metrics.
Our following discussion in the first part of this
section highlights the kind of impact that data set
properties have on the standard metrics (label set size
on LAS, non-terminal nodes per sentence on F-score).
Then, in the second part of this section we use the
TedEval cross-experiment protocols for comparative
evaluation that is less sensitive to representation types
and annotation idiosyncrasies.
7.1 Parsing Across Languages and Treebanks
To quantify the impact of treebank characteristics on
parsing parsing accuracy we looked at correlations
of treebank properties with parsing results. The most
highly correlated combinations we have found are
shown in Figures 2, 3, and 4 for the dependency track
and the constituency track (F-score and LeafAnces-
170
21/09/13 03:00SPMRL charts
Page 3 sur 3http://pauillac.inria.fr/~seddah/updated_official.spmrl_results.html
Correlation between label set size, treebank size, and mean LAS
FrP
FrP
GeP
GeP
HuP
HuP
SwP
ArP
ArP
ArG
ArG
BaP
BaP
FrG
FrG
GeG
GeG
HeP
HeG
HuG
HuG
PoP
PoP
PoG
PoG
SwG
BaG
BaG
KoP
KoP
KoG
KoG
10 50 100 500 1 000
72
74
76
78
80
82
84
86
88
90
treebank size / #labels
L
A
S
 
(
%
)
Figure 2: The correlation between treebank size, label set size, and LAS scores. x: treebank size / #labels ; y: LAS (%)
01/10/13 00:43SPMRL charts: all sent.
Page 1 sur 5file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-S?/SPMRL_FINAL/RESULTS/OFFICIAL/official_ptb-all.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, all sent.)
(13/10/01 00:34:34
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean F1
Arabic
Basque
French
German
Hebrew
Hungarian
Korean
Polish
Swedish
ArP
ArG
BaP
BaG
FrP
FrG
GeP
GeG
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
8 9 10
72
74
76
78
80
82
84
86
88
90
92
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 3: The correlation between the non terminals per sentence ratio and F-scores. x: #non terminal/ #sentence ; y:
F1 (%)
171
tor) respectively.
Figure 2 presents the LAS against the average num-
ber of tokens relative to the number of labels. The
numbers are averaged per language over all partici-
pating systems, and the size of the ?bubbles? is pro-
portional to the number of participants for a given
language setting. We provide ?bubbles? for all lan-
guages in the predicted (-P) and gold (-G) setting,
for both training set sizes. The lower dot in terms
of parsing scores always corresponds to the reduced
training set size.
Figure 2 shows a clear correlation between data-
set complexity and parsing accuracy. The simpler
the data set is (where ?simple" here translates into
large data size with a small set of labels), the higher
the results of the participating systems. The bubbles
reflects a diagonal that indicates correlation between
these dimensions. Beyond that, we see two interest-
ing points off of the diagonal. The Korean treebank
(pink) in the gold setting and full training set can be
parsed with a high LAS relative to its size and label
set. It is also clear that the Hebrew treebank (purple)
in the predicted version is the most difficult one to
parse, relative to our expectation about its complexity.
Since the Hebrew gold scenario is a lot closer to the
diagonal again, it may be that this outlier is due to the
coverage and quality of the predicted morphology.
Figure 340 shows the correlation of data complex-
ity in terms of the average number of non-terminals
per sentence, and parsing accuracy (ParsEval F-
score). Parsing accuracy is again averaged over all
participating systems for a given language. In this
figure, we see a diagonal similar to the one in figure 2,
where Arabic (dark blue) has high complexity of the
data (here interpreted as flat trees, low number of
non terminals per sentence) and low F-scores accord-
ingly. Korean (pink), Swedish (burgundy), Polish
(light green), and Hungarian (light blue) follow, and
then Hebrew (purple) is a positive outlier, possibly
due to an additional layer of ?easy" syntactic POS
nodes which increases tree size and inflates F-scores.
French (orange), Basque (red), and German (dark
green) are negative outliers, falling off the diago-
nal. German has the lowest F-score with respect to
40This figure was created from the IMS:SZEGED:CIS
(Const.) and our own PCFG-LA baseline in POS Tagged mode
(BASE:BKY+POS) so as to avoid the noise introduced by the
parser?s own tagging step (BASE:BKY+RAW).
what would be expected for the non-terminals per
sentence ratio, which is in contrast to the LAS fig-
ure where German occurs among the less complex
data set to parse. A possible explanation may be
the crossing branches in the original treebank which
were re-attached. This creates flat and variable edges
which might be hard predict accurately.
Figure 441 presents the correlation between parsing
accuracy in terms the LeafAncestor metrics (macro
averaged) and treebank complexity in terms of the
average number of non-terminals per sentence. As
in the correlation figures, the parsing accuracy is
averaged over the participanting systems for any lan-
guage. The LeafAncestor accuracy is calculated over
phrase structure trees, and we see a similar diago-
nal to the one in Figure 3 showing that flatter tree-
banks are harder (that is, are correlated with lower
averaged scores) But, its slope is less steep than for
the F-score, which confirms the observation that the
LeafAncestor metric is less sensitive than F-score to
the non-terminals-per-sentence ratio.
Similarly to Figure 3, German is a negative outlier,
which means that this treebank is harder to parse ? it
obtains lower scores on average than we would ex-
pect. As for Hebrew, it is much closer to the diagonal.
As it turns out, the "easy" POS layer that inflates the
scores does not affect the LA ratings as much.
7.2 Evaluation Across Scenarios, Languages
and Treebanks
In this section we analyze the results in cross-
scenario, cross-annotation, and cross-framework set-
tings using the evaluation protocols discussed in
(Tsarfaty et al, 2012b; Tsarfaty et al, 2011; Tsarfaty
et al, 2012a).
As a starting point, we select comparable sections
of the parsed data, based on system runs trained on
the small train set (train5k). For those, we selected
subsets containing the first 5,000 tree terminals (re-
specting sentence boundaries) of the test set. We only
used TedEval on sentences up to 70 terminals long,
and projectivized non-projective sentences in all sets.
We use the TedEval metrics to calculate scores on
both constituency and dependency structures in all
languages and all scenarios. Since the metric de-
fines one scale for all of these different cases, we can
41This figure was created under the same condition as the
F-score correlation in figure (Figure 3).
172
04/10/13 23:05SPMRL charts:
Page 1 sur 6file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-SHAREDTASK/SPMRL_FINAL/RESULTS/TESTLEAF.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, )
(13/10/04 23:05:31
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean Leaf Accuracy
ArP
ArG
BaP
BaG
FrP
FrG
GeP
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
GeG
8 9 10
74
76
78
80
82
84
86
88
90
92
94
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 4: The correlation between the non terminals per sentence ratio and Leaf Accuracy (macro) scores. x: #non
terminal/ #sentence ; y: Acc.(%)
compare the performance across annotation schemes,
assuming that those subsets are representative of their
original source.42
Ideally, we would be using labeled TedEval scores,
as the labeled parsing task is more difficult, and la-
beled parses are far more informative than unlabeled
ones. However, most constituency-based parsers do
not provide function labels as part of the output, to
be compared with the dependency arcs. Furthermore,
as mentioned earlier, we observed a huge difference
between label set sizes for the dependency runs. Con-
sequently, labeled scores will not be as informative
across treebanks and representation types. We will
therefore only use labels across scenarios for the
same language and representation type.
42We choose this sample scheme for replicability. We first
tried sampling sentences, aiming at the same average sentence
length (20), but that seemed to create artificially difficult test sets
for languages as Polish and overly simplistic ones for French or
Arabic.
7.2.1 Cross-Scenario Evaluation: raw vs. gold
One novel aspect of this shared task is the evalu-
ation on non-gold segmentation in addition to gold
morphology. One drawback is that the scenarios are
currently not using the same metrics ? the metrics
generally applied for gold and predicted scenrios can-
not apply for raw. To assess how well state of the art
parsers perform in raw scenarios compared to gold
scenarios, we present here TedEval results comparing
raw and gold systems using the evaluation protocol
of Tsarfaty et al (2012b).
Table 9 presents the labeled and unlabeled results
for Arabic and Hebrew (in Full and 5k training set-
tings), and Table 10 presents unlabeled TedEval re-
sults (for all languages) in the gold settings. The
unlabeled TedEval results for the raw settings are
substantially lower then TedEval results on the gold
settings for both languages.
When comparing the unlabeled TedEval results for
Arabic and Hebrew on the participating systems, we
see a loss of 3-4 points between Table 9 (raw) and Ta-
ble 10 (gold). In particular we see that for the best per-
173
forming systems on Arabic (IMS:SZEGED:CIS for
both constituency and dependency), the gap between
gold and realistic scenarios is 3.4 and 4.3 points,
for the constituency and the dependency parser re-
spectively. These results are on a par with results
by Tsarfaty et al (2012b), who showed for different
settings, constituency and dependency based, that
raw scenarios are considerably more difficult to parse
than gold ones on the standard split of the Modern
Hebrew treebank.
For Hebrew, the performance gap between unla-
beled TedEval in raw (Table 9) and gold (Table 10)
is even more salient, with around 7 and 8 points of
difference between the scenarios. We can only specu-
late that such a difference may be due to the difficulty
of resolving Hebrew morpho-syntactic ambiguities
without sufficient syntactic information. Since He-
brew and Arabic now have standardized morpholog-
ically and syntactically analyzed data sets available
through this task, it will be possible to investigate
further how cross-linguistic differences in morpho-
logical ambiguity affect full-parsing accuracy in raw
scenarios.
This section compared the raw and gold parsing
results only on unlabeled TedEval metrics. Accord-
ing to what we have seen so far is expected that
for labeled TedEval metrics using the same protocol,
the gap between gold and raw scenario will be even
greater.
7.2.2 Cross-Framework Evaluation:
Dependency vs. Constituency
In this section, our focus is on comparing parsing
results across constituency and dependency parsers
based on the protocol of Tsarfaty et al (2012a) We
have only one submission from IMS:SZEGED:CIS
in the constituency track, and. from the same group,
a submission on the dependency track. We only com-
pare the IMS:SZEGED:CIS results on constituency
and dependency parsing with the two baselines we
provided. The results of the cross-framework evalua-
tion protocol are shown in Table 11.
The results comparing the two variants of the
IMS:SZEGED:CIS systems show that they are very
close for all languages, with differences ranging from
0.03 for German to 0.8 for Polish in the gold setting.
It has often been argued that dependency parsers
perform better than a constituency parser, but we
notice that when using a cross framework protocol,
such as TedEval, and assuming that our test set sam-
ple is representative, the difference between the in-
terpretation of both representation?s performance is
alleviated. Of course, here the metric is unlabeled, so
it simply tells us that both kind of parsing models are
equally able to provide similar tree structures. Said
differently, the gaps in the quality of predicting the
same underlying structure across representations for
MRLs is not as large as is sometimes assumed.
For most languages, the baseline constituency
parser performs better than the dependency base-
line one, with Basque and Korean as an exception,
and at the same time, the dependency version of
IMS:SZEGED:CIS performs slightly better than their
constituent parser for most languages, with the excep-
tion of Hebrew and Hungarian. It goes to show that,
as far as these present MRL results go, there is no
clear preference for a dependency over a constituency
parsing representation, just preferences among par-
ticular models.
More generally, we can say that even if the linguis-
tic coverage of one theory is shown to be better than
another one, it does not necessarily mean that the
statistical version of the formal theory will perform
better for structure prediction. System performance
is more tightly related to the efficacy of the learning
and search algorithms, and feature engineering on
top of the selected formalism.
7.2.3 Cross-Language Evaluation: All
Languages
We conclude with an overall outlook of the Ted-
Eval scores across all languages. The results on the
gold scenario, for the small training set and the 5k
test set are presented in Table 10. We concentrate
on gold scenarios (to avoid the variation in cover-
age of external morphological analyzers) and choose
unlabeled metrics as they are not sensitive to label
set sizes. We emphasize in bold, for each parsing
system (row in the table), the top two languages that
most accurately parsed by it (boldface) and the two
languages it performed the worse on (italics).
We see that the European languages German
and Hungarian are parsed most accurately in the
constituency-based setup, with Polish and Swedish
having an advantage in dependency parsing. Across
all systems, Korean is the hardest to parse, with Ara-
174
Arabic Hebrew AVG1 SOFT AVG Arabic Hebrew AVG2 SOFT AVG2
1) Constituency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS (Bky) 83.59 56.43 70.01 70.01 92.18 88.02 90.1 90.1
2) Dependency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS 88.61 84.74 86.68 86.68 91.41 88.58 90 90
ALPAGE:DYALOG 87.20 81.65 40.83 81.65 90.74 87.44 89.09 89.09
CADIM 87.99 - 44 87.99 91.22 - 45.61 91.22
MALTOPTIMIZER 86.62 81.74 43.31 86.62 90.26 87.00 45.13 90.26
ALPAGE:DYALOG (RAW) - 82.82 41.41 82.82 - 87.43 43.72 87.43
AI:KU - 77.8 38.9 77.8 - 85.87 42.94 85.87
Table 9: Labeled and Unlabeled TedEval Results for raw Scenarios, Trained on 5k sentences and tested on 5k terminals.
The upper part refers to constituency parsing and the lower part refers to dependency parsing.
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) Constituency Evaluation
IMS:SZEGED:CIS (Bky) 95.35 96.91 95.98 97.12 96.22 97.92 92.91 97.19 96.65
BASE:BKY+POS 95.11 94.69 95.08 97.01 95.85 97.08 90.55 96.99 96.38
BASE:BKY+RAW 94.58 94.32 94.72 96.74 95.64 96.15 87.08 95.93 95.90
2) Dependency Evaluation
IMS:SZEGED:CIS 95.76 97.63 96.59 96.88 96.29 97.56 94.62 98.01 97.22
ALPAGE:DYALOG 93.76 95.72 95.75 96.4 95.34 95.63 94.56 96.80 96.55
BASE:MALT 94.16 95.08 94.21 94.55 94.98 95.25 94.27 95.83 95.33
AI:KU - - 95.46 96.34 95.07 96.53 - 96.88 95.87
MALTOPTIMIZER 94.91 96.82 95.23 96.32 95.46 96.30 94.69 96.06 95.90
CADIM 94.66 - - - - - - - -
Table 10: Cross-Language Evaluation: Unlabeled TedEval Results in gold input scenario, On a 5k-sentences set set and
a 5k-terminals test set. The upper part refers to constituency parsing and the lower part refers to dependency parsing.
For each system we mark the two top scoring languages in bold and the two lowest scoring languages in italics.
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) gold setting
IMS:SZEGED:CIS (Bky) 95.82 97.30 96.15 97.43 96.37 98.25 94.07 97.22 96.89
IMS:SZEGED:CIS 95.87 98.06 96.61 97.46 96.31 97.93 94.62 98.04 97.24
BASE:BKY+POS 95.61 95.25 95.48 97.31 96.03 97.53 92.15 96.97 96.66
BASE:MALT 94.26 95.76 94.23 95.53 95.00 96.09 94.27 95.90 95.35
2) predicted setting
IMS:SZEGED:CIS (Bky) 95.74 97.07 96.21 97.31 96.10 98.03 94.05 96.92 96.90
IMS:SZEGED:CIS 95.18 97.67 96.15 97.09 96.22 97.63 94.43 97.50 97.02
BASE:BKY+POS 95.03 95.35 97.12 95.36 97.20 91.34 96.92 96.25
BASE:MALT 95.49 93.84 95.39 94.41 95.72 93.74 96.04 95.09
Table 11: Cross Framework Evaluation: Unlabeled TedEval on generalized gold trees in gold scenario, trained on 5k
sentences and tested on 5k terminals.
bic, Hebrew and to some extent French following. It
appears that on a typological scale, Semitic and Asian
languages are still harder to parse than a range of Eu-
ropean languages in terms of structural difficulty and
complex morpho-syntactic interaction. That said,
note that we cannot tell why certain treebanks appear
more challenging to parse then others, and it is still
unclear whether the difficulty is inherent on the lan-
guage, in the currently available models, or because
of the annotation scheme and treebank consistency.43
43The latter was shown to be an important factor orthogonal
to the morphologically-rich nature of the treebank?s language
175
8 Conclusion
This paper presents an overview of the first shared
task on parsing morphologically rich languages. The
task features nine languages, exhibiting different lin-
guistic phenomena and varied morphological com-
plexity. The shared task saw submissions from seven
teams, and results produced by more than 14 different
systems. The parsing results were obtained in dif-
ferent input scenarios (gold, predicted, and raw) and
evaluated using different protocols (cross-framework,
cross-scenario, and cross-language). In particular,
this is the first time an evaluation campaign reports
on the execution of parsers in realistic, morphologi-
cally ambiguous, setting.
The best performing systems were mostly ensem-
ble systems combining multiple parser outputs from
different frameworks or training runs, or integrat-
ing a state-of-the-art morphological analyzer on top
of a carefully designed feature set. This is con-
sistent with previous shared tasks such as ConLL
2007 or SANCL?2012. However, dealing with am-
biguous morphology is still difficult for all systems,
and a promising approach, as demonstrated by AL-
PAGE:DYALOG, is to deal with parsing and morphol-
ogy jointly by allowing lattice input to the parser. A
promising generalization of this approach would be
the full integration of all levels of analysis that are
mutually informative into a joint model.
The information to be gathered from the results of
this shared task is vast, and we only scratched the
surface with our preliminary analyses. We uncov-
ered and documented insights of strategies that make
parsing systems successful: parser combination is
empirically proven to reach a robust performance
across languages, though language-specific strategies
are still a sound avenue for obtaining high quality
parsers for that individual language. The integration
of morphological analysis into the parsing needs to
be investigated thoroughly, and new approaches that
are morphologically aware need to be developed.
Our cross-parser, cross-scenario, and cross-
framework evaluation protocols have shown that, as
expected, more data is better, and that performance
on gold morphological input is significantly higher
than that in more realistic scenarios. We have shown
that gold morphological information is more help-
(Schluter and van Genabith, 2007)
ful to some languages and parsers than others, and
that it may also interact with successful identification
of multiword expressions. We have shown that dif-
ferences between dependency and constituency are
smaller than previously assumed and that properties
of the learning model and granularity of the output
labels are more influential. Finally, we observed
that languages which are typologically farthest from
English, such as Semitic and Asian languages, are
still amongst the hardest to parse, regardless of the
parsing method used.
Our cross-treebank, in-depth analysis is still pre-
liminary, owing to the limited time between the end
of the shared task and the deadline for publication
of this overview. but we nonetheless feel that our
findings may benefit researchers who aim to develop
parsers for diverse treebanks.44
A shared task is an inspection of the state of the
art, but it may also accelerate research in an area
by providing a stable data basis as well as a set of
strong baselines. The results produced in this task
give a rich picture of the issues associated with pars-
ing MRLs and initial cues towards their resolution.
This set of results needs to be further analyzed to be
fully understood, which will in turn contribute to new
insights. We hope that this shared task will provide
inspiration for the design and evaluation of future
parsing systems for these languages.
Acknowledgments
We heartily thank Miguel Ballesteros and Corentin
Ribeire for running the dependency and constituency
baselines. We warmly thank the Linguistic Data Con-
sortium: Ilya Ahtaridis, Ann Bies, Denise DiPersio,
Seth Kulick and Mohamed Maamouri for releasing
the Arabic Penn Treebank for this shared task and
for their support all along the process. We thank
Alon Itai and MILA, the knowledge center for pro-
cessing Hebrew, for kindly making the Hebrew tree-
bank and morphological analyzer available for us,
Anne Abeill? for allowing us to use the French tree-
bank, and Key-Sun Choi for the Kaist Korean Tree-
bank. We thank Grzegorz Chrupa?a for providing
the morphological analyzer Morfette, and Joachim
44The data set will be made available as soon as possible under
the license distribution of the shared-task, with the exception
of the Arabic data, which will continue to be distributed by the
LDC.
176
Wagner for his LeafAncestor implementation. We
finally thank ?zlem ?etinog?lu, Yuval Marton, Benoit
Crabb? and Benoit Sagot who have been nothing but
supportive during all that time.
At the end of this shared task (though watch out
for further updates and analyses), what remains to be
mentioned is our deep gratitude to all people involved,
either data providers or participants. Without all of
you, this shared task would not have been possible.
References
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel.
2003. Building a treebank for French. In Anne Abeill?,
editor, Treebanks. Kluwer, Dordrecht.
Szymon Acedan?ski. 2010. A Morphosyntactic Brill Tag-
ger for Inflectional Languages. In Advances in Natural
Language Processing, volume 6233 of Lecture Notes
in Computer Science, pages 3?14. Springer-Verlag.
Meni Adler and Michael Elhadad. 2006. An unsupervised
morpheme-based HMM for Hebrew morphological dis-
ambiguation. In Proceedings COLING-ACL, pages
665?672, Sydney, Australia.
Meni Adler, Yoav Goldberg, David Gabay, and Michael
Elhadad. 2008. Unsupervised lexicon-based resolution
of unknown words for full morphological analysis. In
Proceedings of ACL-08: HLT, pages 728?736, Colum-
bus, OH.
Meni Adler. 2007. Hebrew Morphological Disambigua-
tion: An Unsupervised Stochastic Word-based Ap-
proach. Ph.D. thesis, Ben-Gurion University of the
Negev.
Itziar Aduriz, Jos? Mar?a Arriola, Xabier Artola, A D?az
de Ilarraza, et al 1997. Morphosyntactic disambigua-
tion for Basque based on the constraint grammar for-
malism. In Proceedings of RANLP, Tzigov Chark, Bul-
garia.
Itziar Aduriz, Eneko Agirre, Izaskun Aldezabal, I?aki
Alegria, Xabier Arregi, Jose Maria Arriola, Xabier Ar-
tola, Koldo Gojenola, Aitor Maritxalar, Kepa Sarasola,
et al 2000. A word-grammar based morphological
analyzer for agglutinative languages. In Proceedings
of COLING, pages 1?7, Saarbr?cken, Germany.
Itziar Aduriz, Maria Jesus Aranzabe, Jose Maria Arriola,
Aitziber Atutxa, A Diaz de Ilarraza, Aitzpea Garmen-
dia, and Maite Oronoz. 2003. Construction of a
Basque dependency treebank. In Proceedings of the
2nd Workshop on Treebanks and Linguistic Theories
(TLT), pages 201?204, V?xj?, Sweden.
Zeljko Agic, Danijela Merkler, and Dasa Berovic. 2013.
Parsing Croatian and Serbian by using Croatian depen-
dency treebanks. In Proceedings of the Fourth Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Seattle, WA.
I. Aldezabal, M.J. Aranzabe, A. Diaz de Ilarraza, and
K. Fern?ndez. 2008. From dependencies to con-
stituents in the reference corpus for the processing of
Basque. In Procesamiento del Lenguaje Natural, no
41 (2008), pages 147?154. XXIV edici?n del Congreso
Anual de la Sociedad Espa?ola para el Procesamiento
del Lenguaje Natural (SEPLN).
Bharat Ram Ambati, Samar Husain, Joakim Nivre, and
Rajeev Sangal. 2010. On the role of morphosyntactic
features in Hindi dependency parsing. In Proceedings
of the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010), Los
Angeles, CA.
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statistical
latent-variable parsing models for Arabic, English and
French. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL), Los Angeles, CA.
Miguel Ballesteros and Joakim Nivre. 2012. MaltOpti-
mizer: An optimization tool for MaltParser. In Pro-
ceedings of EACL, pages 58?62, Avignon, France.
Miguel Ballesteros. 2013. Effective morphological fea-
ture selection with MaltOptimizer at the SPMRL 2013
shared task. In Proceedings of the Fourth Workshop on
Statistical Parsing of Morphologically-Rich Languages,
pages 53?60, Seattle, WA.
Kepa Bengoetxea and Koldo Gojenola. 2010. Appli-
cation of different techniques to dependency parsing
of Basque. In Proceedings of the NAACL/HLT Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL 2010), Los Angeles, CA.
Philip Bille. 2005. A survey on tree edit distance and re-
lated problems. Theoretical Computer Science, 337(1?
3):217?239, 6.
Anders Bj?rkelund, Ozlem Cetinoglu, Rich?rd Farkas,
Thomas Mueller, and Wolfgang Seeker. 2013.
(Re)ranking meets morphosyntax: State-of-the-art re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing
of Morphologically-Rich Languages, pages 134?144,
Seattle, WA.
Ezra Black, Steven Abney, Dan Flickinger, Claudia
Gdaniec, Ralph Grishman, Philip Harrison, Donald
Hindle, Robert Ingria, Frederick Jelinek, Judith Kla-
vans, Mark Liberman, Mitchell Marcus, Salim Roukos,
Beatrice Santorini, and Tomek Strzalkowski. 1991. A
procedure for quantitatively comparing the syntactic
coverage of English grammars. In Proceedings of the
DARPA Speech and Natural Language Workshop 1991,
pages 306?311, Pacific Grove, CA.
177
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Proceed-
ings of the EMNLP-CoNLL, pages 1455?1465, Jeju,
Korea.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of COL-
ING, pages 89?97, Beijing, China.
Adriane Boyd. 2007. Discontinuity revisited: An im-
proved conversion to context-free representations. In
Proceedings of the Linguistic Annotation Workshop,
Prague, Czech Republic.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER treebank.
In Proceedings of the First Workshop on Treebanks
and Linguistic Theories (TLT), pages 24?41, Sozopol,
Bulgaria.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164, New York, NY.
Tim Buckwalter. 2002. Arabic morphological analyzer
version 1.0. Linguistic Data Consortium.
Tim Buckwalter. 2004. Arabic morphological analyzer
version 2.0. Linguistic Data Consortium.
Marie Candito and Djam? Seddah. 2010. Parsing word
clusters. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Marie Candito, Benoit Crabb?, and Pascal Denis. 2010.
Statistical French dependency parsing: Treebank con-
version and first results. In Proceedings of LREC, Val-
letta, Malta.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for ef-
ficient, feature-rich parsing. In Proceedings of CoNLL,
pages 9?16, Manchester, UK.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of ACL, pages 173?180, Barcelona,
Spain.
Eugene Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In AAAI/IAAI, pages
598?603.
Eugene Charniak. 2000. A maximum entropy inspired
parser. In Proceedings of NAACL, pages 132?139, Seat-
tle, WA.
Jinho D. Choi and Martha Palmer. 2011. Statistical de-
pendency parsing in Korean: From corpus generation
to automatic parsing. In Proceedings of Second Work-
shop on Statistical Parsing of Morphologically Rich
Languages, pages 1?11, Dublin, Ireland.
Jinho D. Choi and Martha Palmer. 2012. Guidelines
for the Clear Style Constituent to Dependency Conver-
sion. Technical Report 01-12, University of Colorado
at Boulder.
Key-sun Choi, Young S. Han, Young G. Han, and Oh W.
Kwon. 1994. KAIST Tree Bank Project for Korean:
Present and Future Development. In In Proceedings
of the International Workshop on Sharable Natural
Language Resources, pages 7?14, Nara, Japan.
Jinho D. Choi. 2013. Preparing Korean data for the
shared task on parsing morphologically rich languages.
arXiv:1309.1649.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-
abith. 2008. Learning morphology with Morfette. In
Proceedings of LREC, Marrakech, Morocco.
Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of Korean parsing. In
Proceedings of the NAACL/HLT Workshop on Sta-
tistical Parsing of Morphologically Rich Languages
(SPMRL 2010), Los Angeles, CA.
Volkan Cirik and H?sn? S?ensoy. 2013. The AI-KU
system at the SPMRL 2013 shared task: Unsuper-
vised features for dependency parsing. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 68?75, Seat-
tle, WA.
Michael Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguistics,
29(4):589?637.
Matthieu Constant, Marie Candito, and Djam? Seddah.
2013. The LIGM-Alpage architecture for the SPMRL
2013 shared task: Multiword expression analysis and
dependency parsing. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 46?52, Seattle, WA.
Anna Corazza, Alberto Lavelli, Giogio Satta, and Roberto
Zanoli. 2004. Analyzing an Italian treebank with
state-of-the-art statistical parsers. In Proceedings of
the Third Workshop on Treebanks and Linguistic Theo-
ries (TLT), T?bingen, Germany.
Benoit Crabb? and Marie Candito. 2008. Exp?riences
d?analyse syntaxique statistique du fran?ais. In Actes
de la 15?me Conf?rence sur le Traitement Automatique
des Langues Naturelles (TALN?08), pages 45?54, Avi-
gnon, France.
D?ra Csendes, J?nos Csirik, Tibor Gyim?thy, and Andr?s
Kocsor. 2005. The Szeged treebank. In Proceedings of
the 8th International Conference on Text, Speech and
Dialogue (TSD), Lecture Notes in Computer Science,
pages 123?132, Berlin / Heidelberg. Springer.
Eric De La Clergerie. 2013. Exploring beam-based
shift-reduce dependency parsing with DyALog: Re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
178
Morphologically-Rich Languages, pages 81?89, Seat-
tle, WA.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Proceedings of the workshop on Cross-
Framework and Cross-Domain Parser Evaluation.
Mona Diab, Nizar Habash, Owen Rambow, and Ryan
Roth. 2013. LDC Arabic treebanks and associated cor-
pora: Data divisions manual. Technical Report CCLS-
13-02, Center for Computational Learning Systems,
Columbia University.
Eva Ejerhed and Gunnel K?llgren. 1997. Stockholm
Ume? Corpus. Version 1.0. Department of Linguis-
tics, Ume? University and Department of Linguistics,
Stockholm University.
Eva Ejerhed, Gunnel K?llgren, Ola Wennstedt, and Mag-
nus ?str?m. 1992. The linguistic annotation system
of the Stockholm?Ume? Corpus project. Technical
Report 33, University of Ume?: Department of Linguis-
tics.
Nerea Ezeiza, I?aki Alegria, Jos? Mar?a Arriola, Rub?n
Urizar, and Itziar Aduriz. 1998. Combining stochastic
and rule-based methods for disambiguation in aggluti-
native languages. In Proceedings of COLING, pages
380?384, Montr?al, Canada.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL, pages
959?967, Columbus, OH.
Alexander Fraser, Helmut Schmid, Rich?rd Farkas, Ren-
jing Wang, and Hinrich Sch?tze. 2013. Knowledge
sources for constituent parsing of German, a morpho-
logically rich and less-configurational language. Com-
putational Linguistics, 39(1):57?85.
Iakes Goenaga, Koldo Gojenola, and Nerea Ezeiza. 2013.
Exploiting the contribution of morphological informa-
tion to parsing: the BASQUE TEAM system in the
SPRML?2013 shared task. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 61?67, Seattle, WA.
Yoav Goldberg and Michael Elhadad. 2010a. Easy-first
dependency parsing of Modern Hebrew. In Proceed-
ings of the NAACL/HLT Workshop on Statistical Pars-
ing of Morphologically Rich Languages (SPMRL 2010),
Los Angeles, CA.
Yoav Goldberg and Michael Elhadad. 2010b. An ef-
ficient algorithm for easy-first non-directional depen-
dency parsing. In Proceedings of HLT: NAACL, pages
742?750, Los Angeles, CA.
Yoav Goldberg and Reut Tsarfaty. 2008. A single frame-
work for joint morphological segmentation and syntac-
tic parsing. In Proceedings of ACL, Columbus, OH.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proc. of ACL, Columbus, OH.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and EM-HMM-based lexical probabilities. In
Proceedings of EALC, pages 327?335, Athens, Greece.
Yoav Goldberg. 2011. Automatic syntactic processing of
Modern Hebrew. Ph.D. thesis, Ben Gurion University
of the Negev.
David Graff, Mohamed Maamouri, Basma Bouziri, Son-
dos Krouna, Seth Kulick, and Tim Buckwalter. 2009.
Standard Arabic Morphological Analyzer (SAMA) ver-
sion 3.1. Linguistic Data Consortium LDC2009E73.
Spence Green and Christopher D. Manning. 2010. Better
Arabic parsing: Baselines, evaluations, and analysis.
In Proceedings of COLING, pages 394?402, Beijing,
China.
Nathan Green, Loganathan Ramasamy, and Zden?k
?abokrtsk?. 2012. Using an SVM ensemble system for
improved Tamil dependency parsing. In Proceedings
of the ACL 2012 Joint Workshop on Statistical Pars-
ing and Semantic Processing of Morphologically Rich
Languages, pages 72?77, Jeju, Korea.
Spence Green, Marie-Catherine de Marneffe, and Christo-
pher D. Manning. 2013. Parsing models for identify-
ing multiword expressions. Computational Linguistics,
39(1):195?227.
Noemie Guthmann, Yuval Krymolowski, Adi Milea, and
Yoad Winter. 2009. Automatic annotation of morpho-
syntactic dependencies in a Modern Hebrew Treebank.
In Proceedings of the Eighth International Workshop on
Treebanks and Linguistic Theories (TLT), Groningen,
The Netherlands.
Nizar Habash and Ryan Roth. 2009. CATiB: The
Columbia Arabic Treebank. In Proceedings of ACL-
IJCNLP, pages 221?224, Suntec, Singapore.
Nizar Habash, Ryan Gabbard, Owen Rambow, Seth
Kulick, and Mitch Marcus. 2007. Determining case in
Arabic: Learning complex linguistic behavior requires
complex linguistic features. In Proceedings of EMNLP-
CoNLL, pages 1084?1092, Prague, Czech Republic.
Nizar Habash, Reem Faraj, and Ryan Roth. 2009a. Syn-
tactic Annotation in the Columbia Arabic Treebank. In
Proceedings of MEDAR International Conference on
Arabic Language Resources and Tools, Cairo, Egypt.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009b.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS tag-
ging, stemming and lemmatization. In Proceedings of
the Second International Conference on Arabic Lan-
guage Resources and Tools. Cairo, Egypt.
179
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publishers.
Jan Hajic?, Alena B?hmov?, Eva Hajic?ov?, and Barbora
Vidov?-Hladk?. 2000. The Prague Dependency Tree-
bank: A three-level annotation scenario. In Anne
Abeill?, editor, Treebanks: Building and Using Parsed
Corpora. Kluwer Academic Publishers.
P?ter Hal?csy, Andr?s Kornai, and Csaba Oravecz. 2007.
HunPos ? an open source trigram tagger. In Proceed-
ings of ACL, pages 209?212, Prague, Czech Republic.
Johan Hall, Jens Nilsson, Joakim Nivre, G?ls?en Eryig?it,
Be?ta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? A study in multilingual
parser optimization. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
933?939, Prague, Czech Republic.
Chung-hye Han, Na-Rae Han, Eon-Suk Ko, Martha
Palmer, and Heejong Yi. 2002. Penn Korean Treebank:
Development and evaluation. In Proceedings of the
16th Pacific Asia Conference on Language, Information
and Computation, Jeju, Korea.
Tilman H?hle. 1986. Der Begriff "Mittelfeld", Anmerkun-
gen ?ber die Theorie der topologischen Felder. In Ak-
ten des Siebten Internationalen Germanistenkongresses
1985, pages 329?340, G?ttingen, Germany.
Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010.
Self-training with products of latent variable grammars.
In Proceedings of EMNLP, pages 12?22, Cambridge,
MA.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of ACL,
pages 586?594, Columbus, OH.
Alon Itai and Shuly Wintner. 2008. Language resources
for Hebrew. Language Resources and Evaluation,
42(1):75?98, March.
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24(4):613?
632.
Laura Kallmeyer and Wolfgang Maier. 2013. Data-driven
parsing using probabilistic linear context-free rewriting
systems. Computational Linguistics, 39(1).
Fred Karlsson, Atro Voutilainen, Juha Heikkilae, and Arto
Anttila. 1995. Constraint Grammar: a language-
independent system for parsing unrestricted text. Wal-
ter de Gruyter.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430, Sapporo, Japan.
Sandra K?bler, Erhard W. Hinrichs, and Wolfgang Maier.
2006. Is it really that difficult to parse German? In Pro-
ceedings of EMNLP, pages 111?119, Sydney, Australia,
July.
Sandra K?bler, Wolfgang Maier, Ines Rehbein, and Yan-
nick Versley. 2008. How to compare treebanks. In
Proceedings of LREC, pages 2322?2329, Marrakech,
Morocco.
Sandra K?bler. 2008. The PaGe 2008 shared task on
parsing German. In Proceedings of the Workshop on
Parsing German, pages 55?63, Columbus, OH.
Seth Kulick, Ryan Gabbard, and Mitch Marcus. 2006.
Parsing the Arabic Treebank: Analysis and Improve-
ments. In Proceedings of the Treebanks and Linguistic
Theories Conference, pages 31?42, Prague, Czech Re-
public.
Joseph Le Roux, Benoit Sagot, and Djam? Seddah. 2012.
Statistical parsing of Spanish and data driven lemmati-
zation. In Proceedings of the Joint Workshop on Statis-
tical Parsing and Semantic Processing of Morphologi-
cally Rich Languages, pages 55?61, Jeju, Korea.
Kong Joo Lee, Byung-Gyu Chang, and Gil Chang Kim.
1997. Bracketing Guidelines for Korean Syntactic Tree
Tagged Corpus. Technical Report CS/TR-97-112, De-
partment of Computer Science, KAIST.
Roger Levy and Christopher D. Manning. 2003. Is it
harder to parse Chinese, or the Chinese treebank? In
Proceedings of ACL, Sapporo, Japan.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2004a. Arabic Treebank: Part 2 v 2.0.
LDC catalog number LDC2004T02.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004b. The Penn Arabic Treebank:
Building a large-scale annotated Arabic corpus. In
NEMLAR Conference on Arabic Language Resources
and Tools, pages 102?109, Cairo, Egypt.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2005. Arabic Treebank: Part 1 v 3.0. LDC
catalog number LDC2005T02.
Mohamed Maamouri, Ann Bies, Seth Kulick, Fatma Gad-
deche, Wigdan Mekki, Sondos Krouna, and Basma
Bouziri. 2009. The Penn Arabic Treebank part 3 ver-
sion 3.1. Linguistic Data Consortium LDC2008E22.
Wolfgang Maier, Miriam Kaeshammer, and Laura
Kallmeyer. 2012. Data-driven PLCFRS parsing re-
visited: Restricting the fan-out to two. In Proceedings
of the Eleventh International Conference on Tree Ad-
joining Grammars and Related Formalisms (TAG+11),
Paris, France.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn TreeBank. Computational
Linguistics, 19(2):313?330.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference. In
Proceedings of EMNLP, pages 34?44, Cambridge, MA.
180
Yuval Marton, Nizar Habash, and Owen Rambow. 2013a.
Dependency parsing of Modern Standard Arabic with
lexical and inflectional features. Computational Lin-
guistics, 39(1):161?194.
Yuval Marton, Nizar Habash, Owen Rambow, and Sarah
Alkhulani. 2013b. SPMRL?13 shared task system:
The CADIM Arabic dependency parser. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 76?80, Seat-
tle, WA.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of HLT:NAACL, pages 152?159, New York, NY.
Ryan T. McDonald, Koby Crammer, and Fernando C. N.
Pereira. 2005. Online large-margin training of depen-
dency parsers. In Proceedings of ACL, pages 91?98,
Ann Arbor, MI.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar
Tackstrom, Claudia Bedini, Nuria Bertomeu Castello,
and Jungmee Lee. 2013. Universal dependency anno-
tation for multilingual parsing. In Proceedings of ACL,
Sofia, Bulgaria.
Igor Mel?c?uk. 2001. Communicative Organization in Nat-
ural Language: The Semantic-Communicative Struc-
ture of Sentences. J. Benjamins.
Knowledge Center for Processing Hebrew
MILA. 2008. Hebrew morphological analyzer.
http://mila.cs.technion.ac.il.
Antonio Moreno, Ralph Grishman, Susana Lopez, Fer-
nando Sanchez, and Satoshi Sekine. 2000. A treebank
of Spanish and its application to parsing. In Proceed-
ings of LREC, Athens, Greece.
Joakim Nivre and Be?ta Megyesi. 2007. Bootstrapping a
Swedish treeebank using cross-corpus harmonization
and annotation projection. In Proceedings of the 6th
International Workshop on Treebanks and Linguistic
Theories, pages 97?102, Bergen, Norway.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Tal-
banken05: A Swedish treebank with phrase structure
and dependency annotation. In Proceedings of LREC,
pages 1392?1395, Genoa, Italy.
Joakim Nivre, Johan Hall, Sandra K?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007a. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task Ses-
sion of EMNLP-CoNLL 2007, pages 915?932, Prague,
Czech Republic.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
G?ls?en Eryig?it, Sandra K?bler, Svetoslav Marinov,
and Erwin Marsi. 2007b. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Slav Petrov and Ryan McDonald. 2012. Overview of the
2012 Shared Task on Parsing the Web. In Proceedings
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL), a NAACL-HLT 2012
workshop, Montreal, Canada.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of COLING-
ACL, Sydney, Australia.
Slav Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Bekeley, Berkeley, CA.
Slav Petrov. 2010. Products of random latent variable
grammars. In Proceedings of HLT: NAACL, pages 19?
27, Los Angeles, CA.
Adam Przepi?rkowski, Miros?aw Ban?ko, Rafa? L. G?rski,
and Barbara Lewandowska-Tomaszczyk, editors. 2012.
Narodowy Korpus Jkezyka Polskiego. Wydawnictwo
Naukowe PWN, Warsaw.
Ines Rehbein and Josef van Genabith. 2007a. Eval-
uating Evaluation Measures. In Proceedings of the
16th Nordic Conference of Computational Linguistics
NODALIDA-2007, Tartu, Estonia.
Ines Rehbein and Josef van Genabith. 2007b. Treebank
annotation schemes and parser evaluation for German.
In Proceedings of EMNLP-CoNLL, Prague, Czech Re-
public.
Ines Rehbein. 2011. Data point selection for self-training.
In Proceedings of the Second Workshop on Statistical
Parsing of Morphologically Rich Languages, pages 62?
67, Dublin, Ireland.
Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In Proceedings of HLT-NAACL, pages
129?132, New York, NY.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(04):365?380.
Natalie Schluter and Josef van Genabith. 2007. Prepar-
ing, restructuring, and augmenting a French Treebank:
Lexicalised parsers or coherent treebanks? In Proc. of
PACLING 07, Melbourne, Australia.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004.
SMOR: A German computational morphology covering
derivation, composition and inflection. In Proceedings
of LREC, Lisbon, Portugal.
Djam? Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
Lemmatization and statistical lexicalized parsing of
morphologically-rich languages. In Proceedings of the
First Workshop on Statistical Parsing of Morphologi-
cally Rich Languages (SPMRL), Los Angeles, CA.
Wolfgang Seeker and Jonas Kuhn. 2012. Making el-
lipses explicit in dependency conversion for a German
181
treebank. In Proceedings of LREC, pages 3132?3139,
Istanbul, Turkey.
Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined tree
substitution grammars for syntactic parsing. In Pro-
ceedings of ACL, pages 440?448, Jeju, Korea.
Anthony Sigogne, Matthieu Constant, and Eric Laporte.
2011. French parsing enhanced with a word clustering
method based on a syntactic lexicon. In Proceedings
of the Second Workshop on Statistical Parsing of Mor-
phologically Rich Languages, pages 22?27, Dublin,
Ireland.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altmann,
and Noa Nativ. 2001. Building a tree-bank of Modern
Hebrew text. Traitement Automatique des Langues,
42:347?380.
Marek S?widzin?ski and Marcin Wolin?ski. 2010. Towards
a bank of constituent parse trees for Polish. In Pro-
ceedings of Text, Speech and Dialogue, pages 197?204,
Brno, Czech Republic.
Ulf Teleman. 1974. Manual f?r grammatisk beskrivning
av talad och skriven svenska. Studentlitteratur.
Lucien Tesni?re. 1959. ?l?ments De Syntaxe Structurale.
Klincksieck, Paris.
Reut Tsarfaty and Khalil Sima?an. 2010. Modeling mor-
phosyntactic agreement in constituency-based parsing
of Modern Hebrew. In Proceedings of the First Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Djame Seddah, Yoav Goldberg, Sandra
K?bler, Marie Candito, Jennifer Foster, Yannick Vers-
ley, Ines Rehbein, and Lamia Tounsi. 2010. Statistical
parsing for morphologically rich language (SPMRL):
What, how and whither. In Proceedings of the First
workshop on Statistical Parsing of Morphologically
Rich Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2011. Evaluating dependency parsing: Robust and
heuristics-free cross-framework evaluation. In Pro-
ceedings of EMNLP, Edinburgh, UK.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012a. Cross-framework evaluation for statistical pars-
ing. In Proceeding of EACL, Avignon, France.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012b. Joint evaluation for segmentation and parsing.
In Proceedings of ACL, Jeju, Korea.
Reut Tsarfaty, Djam? Seddah, Sandra K?bler, and Joakim
Nivre. 2012c. Parsing morphologically rich languages:
Introduction to the special issue. Computational Lin-
guistics, 39(1):15?22.
Reut Tsarfaty. 2010. Relational-Realizational Parsing.
Ph.D. thesis, University of Amsterdam.
Reut Tsarfaty. 2013. A unified morpho-syntactic scheme
of Stanford dependencies. In Proceedings of ACL,
Sofia, Bulgaria.
Veronika Vincze, D?ra Szauter, Attila Alm?si, Gy?rgy
M?ra, Zolt?n Alexin, and J?nos Csirik. 2010. Hungar-
ian Dependency Treebank. In Proceedings of LREC,
Valletta, Malta.
Joachim Wagner. 2012. Detecting Grammatical Errors
with Treebank-Induced Probabilistic Parsers. Ph.D.
thesis, Dublin City University.
Marcin Wolin?ski, Katarzyna G?owin?ska, and Marek
S?widzin?ski. 2011. A preliminary version of
Sk?adnica?a treebank of Polish. In Proceedings of
the 5th Language & Technology Conference, pages
299?303, Poznan?, Poland.
Alina Wr?blewska. 2012. Polish Dependency Bank. Lin-
guistic Issues in Language Technology, 7(1):1?15.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL:HLT, pages 188?193, Portland,
OR.
J?nos Zsibrita, Veronika Vincze, and Rich?rd Farkas.
2013. magyarlanc: A toolkit for morphological and
dependency parsing of Hungarian. In Proceedings of
RANLP, pages 763?771, Hissar, Bulgaria.
182
Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH) @ EACL 2014, pages 32?41,
Gothenburg, Sweden, April 26 2014.
c?2014 Association for Computational Linguistics
A Multilingual Evaluation of Three Spelling Normalisation
Methods for Historical Text
Eva Pettersson
1,2
, Be?ta Megyesi
1
and Joakim Nivre
1
(1) Department of Linguistics and Philology
Uppsala University
(2) Swedish National Graduate School
of Language Technology
firstname.lastname@lingfil.uu.se
Abstract
We present a multilingual evaluation of
approaches for spelling normalisation of
historical text based on data from five
languages: English, German, Hungarian,
Icelandic, and Swedish. Three different
normalisation methods are evaluated: a
simplistic filtering model, a Levenshtein-
based approach, and a character-based sta-
tistical machine translation approach. The
evaluation shows that the machine transla-
tion approach often gives the best results,
but also that all approaches improve over
the baseline and that no single method
works best for all languages.
1 Introduction
Language technology for historical text is a field
of research imposing a variety of challenges. Nev-
ertheless, there is an increasing need for natural
language processing (NLP) tools adapted to his-
torical texts, as an aid for researchers in the hu-
manities field. For example, the historians in the
Gender and Work project are studying what men
and women did for a living in the Early Mod-
ern Swedish society (?gren et al., 2011). In this
project, researchers have found that the most im-
portant words in revealing this information are
verbs such as fishing, selling etc. Instead of man-
ually going through written sources from this time
period, it is therefore assumed that an NLP tool
that automatically searches through a number of
historical documents and presents the contained
verbs (and possibly their complements), would
make the process of finding relevant text passages
more effective.
A major challenge in developing language tech-
nology for historical text is that historical language
often is under-resourced with regard to annotated
data needed for training NLP tools. This prob-
lem is further aggravated by the fact that histori-
cal texts may refer to texts from a long period of
time, during which language has changed. NLP
tools trained on 13th century texts may thus not
perform well on texts from the 18th century. Fur-
thermore, historical language usually shows a sub-
stantial variation in spelling and grammar between
different genres, different authors and even within
the same text written by the same author, due to
the lack of spelling conventions.
To deal with the limited resources and the high
degree of spelling variation, one commonly ap-
plied approach is to automatically normalise the
original spelling to a more modern spelling, be-
fore applying the NLP tools. This way, NLP tools
available for the modern language may be used
to analyse historical text. Even though there may
be structural differences as well between histor-
ical and modern language, spelling is the most
striking difference. Moreover, language technol-
ogy tools such as taggers often to some degree
rely on statistics on word form n-grams and to-
ken frequencies, implying that spelling moderni-
sation is an important step for improving the per-
formance of such tools when applied to historical
text. This paper presents an evaluation of three
approaches to spelling normalisation: 1) a filter-
ing approach based on corpus data, 2) an approach
based on Levenshtein edit distance, and 3) an
approach implementing character-based statistical
machine translation (SMT) techniques. These ap-
proaches have previously solely been evaluated in
isolation, without comparison to each other, and
for one or two languages only. We compare the
results of the different methods in a multilingual
evaluation including five languages, and we show
that all three approaches have a positive impact on
normalisation accuracy as compared to the base-
line. There is no single method that yields the
highest normalisation accuracy for all languages,
but for four out of five languages within the scope
32
of our study, the SMT-based approach gives the
best results.
2 Related Work
Spelling normalisation of historical text has pre-
viously been approached using techniques such as
dictionary lookup, edit distance calculations, and
machine translation.
Rayson et al. (2005) tried an approach based on
dictionary lookup, where a mapping scheme from
historical to modern spelling for 16th to 19th cen-
tury English texts was manually created, resulting
in the VARD tool (VARiant Detector) comprising
45,805 entries. The performance of the normal-
isation tool was evaluated on a set of 17th cen-
tury texts, and compared to the performance of
modern spell checkers on the same text. The re-
sults showed that between a third and a half of
all tokens (depending on which test text was used)
were correctly normalised by both VARD and MS
Word, whereas approximately one third of the to-
kens were correctly normalised only when using
VARD. The percentage of tokens correctly nor-
malised only by MS Word was substantially lower;
approximately 6%. VARD was later further devel-
oped into VARD2, combining the original word
list with data-driven techniques in the form of pho-
netic matching against a modern dictionary, and
letter replacement rules based on common spelling
variation patterns (Baron and Rayson, 2008).
Jurish (2008) argued that due to the lack of or-
thographic conventions, spelling generally reflects
the phonetic form of the word to a higher de-
gree in historical text. Furthermore, it is assumed
that phonetic properties are less resistant to di-
achronic change than orthography. Accordingly,
Jurish explored the idea of comparing the simi-
larity between phonetic forms rather than ortho-
graphic forms. For grapheme-to-phoneme conver-
sion, a module of the IMS German Festival text-
to-speech system (Black and Taylor, 1997) was
used, with a rule-set adapted to historical word
forms. Evaluation was performed on a corpus of
historical German verse quotations extracted from
Deutsches W?rterbuch, containing 5,491,982 to-
kens (318,383 types). Without normalisation, ap-
proximately 84% of the tokens were recognised
by a morphological analyser. After normalisa-
tion, 92% of the tokens were recognised. Adding
lemma-based heuristics, coverage increased fur-
ther to 94% of the tokens.
A Levenshtein similarity approach to normal-
isation was presented by Bollmann et al. (2011)
for Early New High German, where Levenshtein-
based normalisation rules were automatically de-
rived from a word-aligned parallel corpus consist-
ing of the Martin Luther Bible in its 1545 edi-
tion and its 1892 version, respectively. Using this
normalisation technique, the proportion of words
with a spelling identical to the modern spelling in-
creased from 65% in the original text to 91% in the
normalised text. This normalisation method was
further evaluated by Bollmann (2013), comparing
the performance of the RFTagger applied to histor-
ical text before and after normalisation. For every
evaluation text, the tagger was trained on between
100 and 1,000 manually normalised tokens, and
evaluated on the remaining tokens in the same text.
For one manuscript from the 15th century, tagging
accuracy was improved from approximately 29%
to 78% using this method.
Another Levenshtein-based approach to nor-
malisation was presented by Pettersson et al.
(2013b), using context-sensitive, weighted edit
distance calculations combined with compound
splitting. This method requires no annotated his-
torical training data, since normalisation candi-
dates are extracted by Levenshtein comparisons
between the original historical word form and
present-day dictionary entries. However, if a cor-
pus of manually normalised historical text is avail-
able, this can optionally be included for dictio-
nary lookup and weighted Levenshtein calcula-
tions, improving precision. This technique was
evaluated for Early Modern Swedish, and in the
best setting, the proportion of words in the his-
torical text with a spelling identical to the mod-
ern gold standard spelling increased from 64.6%
to 86.9%.
Pettersson et al. (2013a) treated the normalisa-
tion task as a translation problem, using character-
based SMT techniques in the spelling normalisa-
tion process. With the SMT-based approach, the
proportion of tokens in the historical text with
a spelling identical to the modern gold standard
spelling increased from 64.6% to 92.3% for Early
Modern Swedish, and from 64.8% to 83.9% for
15th century Icelandic. It was also shown that nor-
malisation had a positive effect on subsequent tag-
ging and parsing.
Language technology for historical text also has
a lot in common with adaptation of NLP tools
33
for handling present-day SMS messages and mi-
croblog text such as Twitter. In both genres there
is a high degree of spelling variation, ad hoc ab-
breviations and ungrammatical structures impos-
ing the problem of data sparseness. Similar meth-
ods for spelling normalisation may thus be used
for both tasks. Han and Baldwin (2011) pre-
sented a method for normalising SMS and Twitter
text based on morphophonemic similarity, com-
bining lexical edit distance, phonemic edit dis-
tance, prefix substring, suffix substring, and the
longest common subsequence. Context was taken
into account by means of dependency structures
generated by the Stanford Parser applied to a cor-
pus of New York Times articles. In the best set-
ting, a token-level F-score of 75.5% and 75.3%
was reported for SMS messages and Twitter texts
respectively.
3 Approaches
3.1 The Filtering Approach
The filtering approach presupposes access to a par-
allel training corpus of token pairs with historical
word forms mapped to their modernised spelling.
In the normalisation process, whenever a token is
encountered that also occurred in the training data,
the most frequent modern spelling associated with
that token in the training corpus is chosen for nor-
malisation. Other tokens are left unchanged.
3.2 The Levenshtein-based Approach
The Levenshtein-based approach was originally
presented by Pettersson et al. (2013b). In its basic
version, no historical training data is needed,
which is an important aspect considering the
common data sparseness issue, as discussed in
Section 1. Instead, a modern language dictionary
or corpus is required, from which normalisation
candidates are extracted based on edit distance
comparisons to the original historical word form.
If there is parallel data available, i.e. the same
text in its historical and its modernised spelling,
this data can be used to make more reliable Lev-
enshtein calculations by assigning weights lower
than 1 to frequently occurring edits observed in
the training data. The weights are then calculated
by comparing the frequency of each edit occurring
in the training corpus to the frequency with which
the specific source characters are left unchanged,
in accordance with the following formula:
Frequency of Unchanged
Frequency of Edit + Frequency of Unchanged
Context-sensitive weights are added to handle ed-
its affecting more than one character. The context-
sensitive weights are calculated by the same for-
mula as the single-character weights, and include
the following operations:
? double deletion: personnes? persons
? double insertion: strait? straight
? single-to-double substitution: juge? judge
? double-to-single substitution: moost? most
For all historical word forms in the training cor-
pus that are not identical in the modern spelling,
all possible single-character edits as well as multi-
character edits are counted for weighting. Hence,
the historical word form personnes, mapped to
the modern spelling persons, will yield weights
for double-to-single deletion of -ne, as illustrated
above, but also for single deletion of -n and single
deletion of -e.
Finally, a tuning corpus is used to set a
threshold for which maximum edit distance
to allow between the original word form and
its normalisation candidate(s). Based on the
average edit distance between the historical
word forms and their modern spelling in the
tuning corpus, the threshold is calculated by the
following formula (where 1.96 times the stan-
dard deviation is added to cover 95% of the cases):
avg editdistance +(1.96?standard deviation)
If several normalisation candidates have the same
edit distance as compared to the source word, the
most frequent candidate is chosen, based on mod-
ern corpus data. If none of the highest-ranked nor-
malisation candidates are present in the corpus, or
if there are several candidates with the same fre-
quency distribution, a final candidate is randomly
chosen.
3.3 The SMT-based Approach
In the SMT-based approach, originally presented
by Pettersson et al. (2013a), spelling normali-
sation is treated as a translation task. To ad-
dress changes in spelling rather than full transla-
tion of words and phrases, character-based trans-
lation (without lexical reordering) is performed,
a well-known technique for transliteration and
34
character-level translation between closely related
languages (Matthews, 2007; Vilar et al., 2007;
Nakov and Tiedemann, 2012). In character-level
SMT, phrases are modeled as character sequences
instead of word sequences, and translation models
are trained on character-aligned parallel corpora
whereas language models are trained on character
N-grams.
Since the set of possible characters in a lan-
guage is far more limited than the number of pos-
sible word forms, and the same corpus will present
a larger quantity of character instances than token
instances, only a rather small amount of parallel
data is needed for training the translation models
and the language models in character-based trans-
lation. Pettersson et al. (2013a) showed that with
a training and tuning set of only 1,000 pairs of his-
torical word forms mapped to modern spelling, a
normalisation accuracy of 76.5% was achieved for
Icelandic, as compared to 83.9% with a full-sized
training corpus of 33,888 token pairs. Their full
experiment on varying the size of the training data
is illustrated in Figure 1.
 76 77
 78 79
 80 81
 82 83
 84 85
 0  5  10  15  20  25  30  35
N
o
r
m
a
l
i
s
a
t
i
o
n
 
a
c
c
u
r
a
c
y
Size of training data (K tokens)
Normalisation accuracy for different sizes of the alignment training data
Figure 1: Normalisation accuracy when varying
the size of the alignment training data.
We use the same set of training data for the SMT
approach as for the filtering approach and for the
assignment of weights in the Levenshtein-based
approach, i.e. a set of token pairs mapping his-
torical word forms to their manually modernised
spelling. These corpora have the format of one to-
ken per line, with blank lines separating sentences.
To fully adapt this format to the format needed
for training the character-based translation mod-
els, the characters within each token are separated
by space. The SMT system will now regard each
character as a word, the full token as a sentence
and the entire sentence as a section.
The SMT engine used is Moses with all its stan-
dard components. A phrase-based model is ap-
plied, where the feature weights are trained us-
ing MERT with BLEU over character-sequences
as the objective function. The maximum size of a
phrase (sequence of characters) is set to 10.
Two different character alignment techniques
are tested: (i) the word alignment toolkit GIZA++
(Och and Ney, 2000), and (ii) a weighted finite
state transducer implemented in the m2m-aligner
(Jiampojamarn et al., 2007). GIZA is run with
standard word alignment models for character un-
igrams and bigrams, whereas the m2m aligner
implements transducer models based on context-
independent single character and multi-character
edit operations. The transducer is trained us-
ing EM on (unaligned) parallel training data, and
the final model can then be used to produce a
Viterbi alignment between given pairs of charac-
ter strings.
An example is given in Figure 2, where the Ice-
landic word forms me?r? me?ur and giallda?
galda have been aligned at a character-level using
the m2m-aligner. In this example, the  symbol
represents empty alignments, meaning insertions
or deletions. The  symbol in the source word
me?r denotes the insertion of u in the target word
me?ur. Likewise, the  symbol in the target word
galda denotes the deletion of i as compared to the
source word giallda. Furthermore, the alignment
of giallda to galda illustrates the inclusion of
multi-character edit operations, where the colon
denotes a 2:1 alignment where both letters l and d
in the source word correspond to the single letter
d in the target word.
m|e|?||r| m|e|?|u|r|
g|i|a|l|l:d|a| g||a|l|d|a|
Figure 2: m2m character-level alignment.
4 Data
In the following, we will describe the data sets
used for running the filtering approach, the Lev-
enshtein edit distance approach, and the character-
based SMT approach for historical spelling nor-
malisation applied to five languages: English, Ger-
man, Hungarian, Icelandic, and Swedish. For
convenience, we use the notions of training, tun-
35
ing and evaluation corpora, which are well-known
concepts within SMT. These data sets have been
created by extracting every 9th sentence from the
total corpus to the tuning corpus, and every 10th
sentence to the evaluation corpus, whereas the rest
of the sentences have been extracted to a training
corpus.
1
In the filtering approach, there is in fact no
distinction between training and tuning corpora,
since both data sets are combined in the dictionary
lookup process. As for the Levenshtein edit dis-
tance approach, the training corpus is used for ex-
tracting single-character and multi-character edits
by comparing the historical word forms to their
modern spelling. The edits extracted from the
training corpus are then weighted based on their
relative frequency in the tuning corpus.
The historical texts used for training and evalu-
ation are required to be available both in their orig-
inal, historical spelling and in a manually mod-
ernised and validated spelling. A modern trans-
lation of a historical text is generally not usable,
since word order and sentence structure have to re-
main the same to enable training and evaluation of
the proposed methods. The access to such data is
very limited, meaning that the data sets used in our
experiments vary in size, genres and time periods
between the languages.
4.1 English
For training, tuning and evaluation in the En-
glish experiments, we use the Innsbruck Cor-
pus of English Letters, a manually normalised
collection of letters from the period 1386?1698.
This corpus is a subset of the Innsbruck Com-
puter Archive of Machine-Readable English Texts,
ICAMET (Markus, 1999). A subset of the British
National Corpus (BNC) is used as the single mod-
ern language resource both for the Levenshtein-
based and for the SMT-based approach. Table 1
presents in more detail the data sets used in the
English experiments.
4.2 German
For training, tuning and evaluation in the German
experiments, we use a manually normalised sub-
set of the GerManC corpus of German texts from
the period 1650?1800 (Scheible et al., 2011). This
subset contains 22 texts from the period 1659?
1780, within the genres of drama, newspaper text,
1
For information on how to access the data sets used in
our experiments, please contact the authors.
Resource Data Tokens Types
Training ICAMET 148,852 18,267
Tuning ICAMET 16,461 4,391
Evaluation ICAMET 17,791 4,573
Lev. dict. BNC 2,088,680 69,153
Lev. freq. BNC 2,088,680 69,153
SMT lm BNC 2,088,680 69,153
Table 1: Language resources for English.
letters, sermons, narrative prose, humanities, sci-
ence och legal documents. The German Parole
corpus is used as the single modern language re-
source both for the Levenshtein-based and for the
SMT-based approach (Teubert (ed.), 2003). Table
2 presents in more detail the data sets used in the
German experiments.
Resource Data Tokens Types
Training GerManC 39,887 9,055
Tuning GerManC 5,418 2,056
Evaluation GerManC 5,005 1,966
Lev. dict. Parole 18,662,243 662,510
Lev. freq. Parole 18,662,243 662,510
SMT lm Parole 18,662,243 662,510
Table 2: Language resources for German.
4.3 Hungarian
For training, tuning and evaluation in the Hungar-
ian experiments, we use a collection of manually
normalised codices from the Hungarian Gener-
ative Diachronic Syntax project, HGDS (Simon,
To appear), in total 11 codices from the time pe-
riod 1440?1541. The Szeged Treebank is used
as the single modern language resource both for
the Levenshtein-based and for the SMT-based ap-
proach (Csendes et al., 2005). Table 3 presents
in more detail the data sets used in the Hungarian
experiments.
Resource Data Tokens Types
Training HGDS 137,669 45,529
Tuning HGDS 17 181 8 827
Evaluation HGDS 17,214 8,798
Lev. dict. Szeged 1,257,089 144,248
Lev. freq. Szeged 1,257,089 144,248
SMT lm Szeged 1,257,089 144,248
Table 3: Language resources for Hungarian.
36
4.4 Icelandic
For training, tuning and evaluation in the Ice-
landic experiments, we use a manually normalised
subset of the Icelandic Parsed Historical Cor-
pus (IcePaHC), a manually tagged and parsed di-
achronic corpus of texts from the time period
1150?2008 (R?gnvaldsson et al., 2012). This sub-
set contains four texts from the 15th century: three
sagas (Vilhj?lm?s saga, Jarlmann?s saga, and Ec-
tor?s saga) and one narrative-religious text (Mi?al-
da?vint?ri). As a dictionary for Levenshtein cal-
culations we use a combination of Beygingar-
l?sing ?slensks N?t?mam?ls, B?N (a database of
modern Icelandic inflectional forms (Bjarnad?t-
tir, 2012)), and all tokens occurring 100 times or
more in the Tagged Icelandic Corpus of Contem-
porary Icelandic texts, M?M (Helgad?ttir et al.,
2012).
2
The frequency-based choice of a final nor-
malisation candidate in the Levenshtein approach,
as well as the training of a language model in the
SMT approach, are done on all tokens occurring
100 times or more in the M?M corpus. Table 4
presents in more detail the data sets used in the
Icelandic experiments.
Resource Data Tokens Types
Training IcePaHC 52,440 9,748
Tuning IcePaHC 6,443 2,270
Evaluation IcePaHC 6,384 2,244
Lev. dict. B?N+M?M 27,224,798 2,820,623
Lev. freq. M?M 21,339,384 9,461
SMT lm M?M 21,339,384 9,461
Table 4: Language resources for Icelandic.
4.5 Swedish
For training, tuning and evaluation in the Swedish
experiments, we use balanced subsets of the Gen-
der and Work corpus (GaW) of court records and
church documents from the time period 1527?
1812 (?gren et al., 2011). As a dictionary for Lev-
enshtein calculations we use SALDO, a lexical re-
source developed for present-day written Swedish
(Borin et al., 2008). For frequency-based choice of
a final normalisation candidate, we use the Stock-
holm Ume? corpus (SUC) of text representative of
the Swedish language in the 1990s (Ejerhed and
K?llgren, 1997). The SUC corpus is also used
2
The B?N database alone is not sufficient for Levenshtein
calculations, since it only contains content words.
to train a language model in the SMT-based ap-
proach. Table 5 presents in more detail the data
sets used in the Swedish experiments.
Resource Data Tokens Types
Training GaW 28,237 7,925
Tuning GaW 2,590 1,260
Evaluation GaW 33,544 8,859
Lev. dict. SALDO 1,110,731 723,138
Lev. freq. SUC 1,166,593 97,670
SMT lm SUC 1,166,593 97,670
Table 5: Language resources for Swedish.
5 Results
Table 6 presents the results for different languages
and normalisation methods, given in terms of nor-
malisation accuracy, i.e. the percentage of tokens
in the normalised text with a spelling identical
to the manually modernised gold standard, and
character error rate (CER), providing a more pre-
cise estimation of the similarity between the nor-
malised token and the gold standard version at a
character level. Table 7 summarises the results in
terms of Precision (Pre), Recall (Rec) and F-score
(F) for the filtering approach, the Levenshtein-
based approach (with and without filtering), and
the best-performing SMT-based approach.
For the Levenshtein experiments, we have used
context-sensitive weights, as described in Section
3.2. In the SMT approach, we run GIZA with
standard word alignment models for character un-
igrams (un) and bigrams (bi). The m2m aligner is
implemented with single character edit operations
(1:1) and multi-character operations (2:2).
The baseline case shows the proportion of to-
kens in the original, historical text that already
have a spelling identical to the modern gold stan-
dard spelling. In the Hungarian text, only 17.1%
of the historial tokens have a modern spelling,
with a character error rate of 0.85. For German
on the other hand, accuracy is as high as 84.4%,
with a character error rate of only 0.16. At a
first glance, the historical spelling in the Hungar-
ian corpus appears to be very similar to the mod-
ern spelling. A closer look however reveals re-
current differences involving single letter substi-
tutions and/or the use of accents, as for fiayval?
fiaival, m?eghalanac?meghal?nak and hazaba?
h?z?ba.
37
English German Hungarian Icelandic Swedish
Acc CER Acc CER Acc CER Acc CER Acc CER
baseline 75.8 0.26 84.4 0.16 17.1 0.85 50.5 0.51 64.6 0.36
filter 91.7 0.20 94.6 0.26 75.0 0.30 81.7 0.25 86.2 0.27
Lev 82.9 0.19 87.3 0.13 31.7 0.71 67.3 0.35 79.4 0.22
Lev+filter 92.9 0.09 95.1 0.06 76.4 0.35 84.6 0.19 90.8 0.10
giza un 94.3 0.07 96.6 0.04 79.9 0.21 71.8 0.30 92.9 0.07
giza bi 92.4 0.09 95.5 0.05 80.1 0.21 71.5 0.30 92.5 0.08
m2m 1:1 un 90.6 0.11 96.0 0.04 79.4 0.21 71.2 0.31 92.3 0.08
m2m 1:1 bi 88.0 0.14 95.6 0.05 79.5 0.21 71.5 0.30 92.2 0.08
m2m 2:2 un 90.7 0.11 96.4 0.04 77.3 0.24 71.0 0.31 91.3 0.09
m2m 2:2 bi 87.5 0.14 95.5 0.05 79.1 0.22 71.4 0.31 92.1 0.08
Table 6: Normalisation results given in accuracy (Acc) and character error rate (CER).
English German Hungarian Icelandic Swedish
Pre Rec F Pre Rec F Pre Rec F Pre Rec F Pre Rec F
filter 93.6 97.8 95.7 95.0 99.6 97.2 77.4 96.0 85.7 89.3 90.6 89.9 87.5 98.3 92.6
Lev 92.7 88.6 90.7 91.0 95.6 93.2 68.0 37.3 48.2 85.4 76.1 80.5 90.5 86.6 88.5
Lev+filter 97.4 95.2 96.3 97.3 97.7 97.5 96.2 78.8 86.7 95.6 88.0 91.7 96.6 93.8 95.2
SMT 98.2 95.9 97.0 98.7 97.9 98.3 98.3 81.3 89.0 82.0 85.2 83.6 98.6 94.1 96.3
Table 7: Normalisation results given in precision (Pre), recall (Rec) and F-score (F).
The Icelandic corpus also has a relatively low
number of tokens with a spelling identical to the
modern spelling. Even though the Hungarian and
Icelandic texts are older than the English, German,
and Swedish texts, the rather low proportion of to-
kens with a modern spelling in the Icelandic cor-
pus is rather surprising, since the Icelandic lan-
guage is generally seen as conservative in spelling.
A closer inspection of the Icelandic corpus reveals
the same kind of subtle single letter divergences
and differences in the use of accents as for Hun-
garian, e.g. ad? a? and hun? h?n.
The simplistic filtering approach (filter), re-
lying solely on previously seen tokens in the
training data, captures frequently occurring word
forms and works surprisingly well, improving
normalisation accuracy by up to 63 percentage
units. The Levenshtein-based approach (Lev)
in its basic version, with no parallel training
data available, also improves normalisation ac-
curacy as compared to the baseline. However,
for all languages, the simplistic filtering approach
yields significantly higher normalisation accuracy
than the more sophisticated Levenshtein-based ap-
proach does. This could be partly explained by
the fact that frequently occurring word forms have
a high chance of being captured by the filter-
ing approach, whereas the Levenshtein-based ap-
proach runs the risk of consistently normalising
high-frequent word forms incorrectly. For exam-
ple, in the English Levenshtein normalisation pro-
cess, the high-frequent word form stonde has con-
sistently been normalised to stone instead of stand,
due to the larger edit distance between stonde and
stand. The even more common word form ben,
which should optimally be normalised to been, has
consistently been left unchanged as ben, since the
BNC corpus, which is used for dictionary lookup
in the English setup, contains the proper name
Ben. The issue of proper names would not be
a problem if a modern dictionary were used for
Levenshtein comparisons instead of a corpus, or if
casing was taken into account in the Levenshtein
comparisons. There would however still be cases
left like stonde being incorrectly normalised to
stone as described above, which would be disad-
vantageous to the Levenshtein-based method. The
low recall figures, especially for Hungarian, also
indicates that there may be old word forms that
are not present in modern dictionaries and thus are
out of reach for the Levenshtein-based method, as
for the previously discussed Hungarian word form
meghal?nak.
In the Lev+filter setting, the filter is used as a
first step in the normalisation process. Only to-
kens that could not be matched through dictio-
nary lookup based on the training corpus are nor-
malised by Levenshtein comparisons. The idea is
38
that combining these two techniques would per-
form better than one approach only, since high-
frequent word forms are consistently normalised
correctly by the filter, whereas previously unseen
tokens are handled through Levenshtein compar-
isons. This combination does indeed perform bet-
ter for all languages, and for Icelandic this is by far
the most successful normalisation method of all.
For the SMT-based approach, it is interesting to
note that the simple unigram models in many cases
perform better than the more advanced bigram and
multi-character models. We also tried adding the
filter to the SMT approach, so that only tokens that
could not be matched through dictionary lookup
based on the training corpus, would be considered
for normalisation by the SMT model. This did
however not have a positive effect on normalisa-
tion accuracy, probably because the training data
has already been taken care of by the SMT model,
so adding the filter only led to redundant informa-
tion and incorrect matches, deteriorating the re-
sults. For four out of five languages, the GIZA un-
igram setting yields the highest normalisation ac-
curacy of all SMT models evaluated. For Hungar-
ian, the GIZA bigram modell performs marginally
better than the unigram model.
From the presented results, it is not obvious
which normalisation approach to choose for a new
language. For Icelandic, the Levenshtein-based
approach combined with the filter leads to the
highest normalisation accuracy. For the rest of
the languages, the SMT-based approach with the
GIZA unigram or bigram setting gives the best re-
sults. Generally, the Levenshtein-based method
could be used for languages lacking access to an-
notated historical data with information on both
original and modernised spelling. If, on the other
hand, such data is available, the filtering approach,
or the combination of filtering and Levenshtein
calculations, would be likely to improve normal-
isation accuracy. Moreover, the effort of training
a character-based SMT system for normalisation
would be likely to further improve the results.
It would be interesting to also compare the re-
sults between the languages, in a language evo-
lution perspective. This is however not feasible
within the scope of this study, due to the differ-
ences in corpus size, genres and covered time pe-
riods, as discussed in Section 4.
6 Conclusion
We have performed a multilingual evaluation
of three approaches to spelling modernisation
of historical text: a simplistic filtering model,
a Levenshtein-based approach and a character-
based statistical machine translation method. The
results were evaluated on historical texts from
five languages: English, German, Hungarian, Ice-
landic and Swedish. We see that all approaches are
successful in increasing the proportion of tokens in
the historical text with a spelling identical to the
modernised gold standard spelling. We conclude
that the proposed methods have the potential of
enabling us to use modern NLP tools for analysing
historical texts. Which approach to choose is not
clear, since the results vary for the different lan-
guages in our study, even though the SMT-based
approach generally works best. If no historical
training data is available, the Levenshtein-based
approach could still be used, since only a mod-
ern dictionary is required for edit distance com-
parisons. If there is a corpus of token pairs with
historical and modern spelling available, training
an SMT model could however result in improved
normalisation accuracy. Since the SMT models
are character-based, only a rather small amount of
training data is needed for this task, as discussed
in Section 3.3.
We believe that our results would be of interest
to several research fields. From a language evolu-
tion perspective, future research would include a
thorough investigation of why certain approaches
work better for some languages but not for other
languages, and what the results would be if the
data sets for the different languages were more
similar with regard to time period, size, genre etc.
The latter could however be problematic, due to
data sparseness. For historians interested in us-
ing modern NLP tools for analysing historical text,
an extrinsic evaluation is called for, comparing
the results of tagging and parsing using modern
tools, before and after spelling normalisation. Fi-
nally, the proposed methods all treat words in iso-
lation in the normalisation process. From a lan-
guage technology perspective, it would be inter-
esting to also explore ways of handling grammat-
ical and structural differences between historical
and modern language as part of the normalisation
process. This would be particularly interesting
when evaluating subsequent tagging and parsing
performance.
39
References
Maria ?gren, Rosemarie Fiebranz, Erik Lindberg, and
Jonas Lindstr?m. 2011. Making verbs count. The
research project ?Gender and Work? and its method-
ology. Scandinavian Economic History Review,
59(3):271?291. Forthcoming.
Alistair Baron and Paul Rayson. 2008. Vard2: A tool
for dealing with spelling variation in historical cor-
pora. In Postgraduate Conference in Corpus Lin-
guistics, Aston University, Birmingham.
Krist?n Bjarnad?ttir. 2012. The Database of Modern
Icelandic Inflection. In AfLaT2012/SALTMIL joint
workshop on Language technology for normalisa-
tion of less-resourced languages, Istanbul, May.
Alan W. Black and Paul Taylor. 1997. Festival speech
synthesis system: system documentation. Technical
report, University of Edinburgh, Centre for Speech
Technology Research.
Marcel Bollmann, Florian Petran, and Stefanie Dipper.
2011. Rule-based normalization of historical texts.
In Proceedings of the Workshop on Language Tech-
nologies for Digital Humanities and Cultural Her-
itage, pages 34?42, Hissar, Bulgaria.
Marcel Bollmann. 2013. POS tagging for historical
texts with sparse training data. In Proceedings of
the 7th Linguistic Annotation Workshop & Interop-
erability with Discourse, pages 11?18, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
Lars Borin, Markus Forsberg, and Lennart L?nngren.
2008. Saldo 1.0 (svenskt associationslexikon ver-
sion 2). Spr?kbanken, University of Gothenburg.
C. Csendes, J. Csirik, T. Gyim?thy, and A. Kocsor.
2005. The Szeged Treebank. In Proceedings of
the Eighth International Conference on Text, Speech
and Dialogue (TSD 2005), Karlovy Vary, Czech Re-
public.
Eva Ejerhed and Gunnel K?llgren. 1997. Stockholm
Ume? Corpus. Version 1.0. Produced by Depart-
ment of Linguistics, Ume? University and Depart-
ment of Linguistics, Stockholm University. ISBN
91-7191-348-3.
Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a #twitter.
In Association for Computational Linguistics, edi-
tor, Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics, pages
368?378, Portland, Oregon, USA, June.
Sigr?n Helgad?ttir, ?sta Svavarsd?ttir, Eir?kur R?gn-
valdsson, Krist?n Bjarnad?ttir, and Hrafn Loftsson.
2012. The Tagged Icelandic Corpus (M?M). In
Proceedings of the Workshop on Language Tech-
nology for Normalisation of Less-Resourced Lan-
guages, pages 67?72.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and hidden markov models to letter-to-phoneme
conversion. In Proceedings of the Annual Confer-
ence of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL-HLT
2007), pages 372?379, Rochester, NY, April.
Bryan Jurish. 2008. Finding canonical forms
for historical German text. In Angelika Storrer,
Alexander Geyken, Alexander Siebert, and Kay-
Michael W?rzner, editors, Text Resources and Lex-
ical Knowledge: Selected Papers from the 9th Con-
ference on Natural Language Processing (KON-
VENS 2008), pages 27?37. Mouton de Gruyter,
Berlin.
Manfred Markus, 1999. Manual of ICAMET (Inns-
bruck Computer Archive of Machine-Readable En-
glish Texts). Leopold-Franzens-Universit?t Inns-
bruck.
David Matthews. 2007. Machine transliteration of
proper names. Master?s thesis, School of Informat-
ics.
Preslav Nakov and J?rg Tiedemann. 2012. Combin-
ing word-level and character-level models for ma-
chine translation between closely-related languages.
In Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 301?305, Jeju Island, Korea,
July. Association for Computational Linguistics.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. pages 440?447, Hongkong, China,
October.
Eva Pettersson, Be?ta Megyesi, and Tiedemann J?rg.
2013a. An SMT approach to automatic annotation
of historical text. In Proceedings of the NoDaLiDa
2013 workshop on Computational Historical Lin-
guistics, May.
Eva Pettersson, Be?ta Megyesi, and Joakim Nivre.
2013b. Normalisation of historical text using
context-sensitive weighted Levenshtein distance and
compound splitting. In Proceedings of the 19th
Nordic Conference on Computational Linguistics
(NoDaLiDa), May.
Paul Rayson, Dawn Archer, and Nicholas Smith. 2005.
VARD versus Word ? A comparison of the UCREL
variant detector and modern spell checkers on En-
glish historical corpora. In Proceedings from the
Corpus Linguistics Conference Series on-line e-
journal, volume 1, Birmingham, UK, July.
Eir?kur R?gnvaldsson, Anton Karl Ingason, Einar Freyr
Sigurdsson, and Joel Wallenberg. 2012. The Ice-
landic Parsed Historical Corpus (IcePaHC). In Pro-
ceedings of the Eight International Conference on
Language Resources and Evaluation (LREC?12), Is-
tanbul, Turkey, May. European Language Resources
Association (ELRA).
40
Silke Scheible, Richard J. Whitt, Martin Durrell, and
Paul Bennett. 2011. A Gold Standard Corpus of
Early Modern German. In Proceedings of the 5th
Linguistic Annotation Workshop, pages 124?128,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
Eszter Simon. To appear. Corpus building from Old
Hungarian codices. In Katalin ?. Kiss, editor, The
Evolution of Functional Left Peripheries in Hungar-
ian Syntax. Oxford University Press.
Wolfgang Teubert (ed.). 2003. German Parole Corpus.
Electronic resource, Oxford Text Archive.
David Vilar, Jan-Thorsten Peter, and Hermann Ney.
2007. Can we translate letters? In Proceedings of
the Second Workshop on Statistical Machine Trans-
lation, pages 33?39, Prague, Czech Republic, June.
Association for Computational Linguistics.
41
Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 72?76,
Gothenburg, Sweden, 26-27 April 2014.
c
?2014 Association for Computational Linguistics
Extraction of Nominal Multiword Expressions in French
Marie Dubremetz and Joakim Nivre
Uppsala university
Department of Linguistics and Philology
Uppsala, Sweden
Abstract
Multiword expressions (MWEs) can be
extracted automatically from large corpora
using association measures, and tools like
mwetoolkit allow researchers to generate
training data for MWE extraction given a
tagged corpus and a lexicon. We use mwe-
toolkit on a sample of the French Europarl
corpus together with the French lexicon
Dela, and use Weka to train classifiers for
MWE extraction on the generated training
data. A manual evaluation shows that the
classifiers achieve 60?75% precision and
that about half of the MWEs found are
novel and not listed in the lexicon. We also
investigate the impact of the patterns used
to generate the training data and find that
this can affect the trade-off between preci-
sion and novelty.
1 Introduction
In alphabetic languages, words are delimited by
spaces. Some words can combine to create a new
unit of meaning that we call a multiword expres-
sion (MWE). However, MWEs such as kick the
bucket must be distinguished from free combina-
tions of words such as kick the ball. A sequence of
several words is an MWE if ?at least one of its syn-
tactic, distributional or semantic properties cannot
be deduced from the properties of its component?
(Silberztein and L.A.D.L., 1990). So how can we
extract them?
Statistical association measures have long been
used for MWE extraction (Pecina, 2010), and by
training supervised classifiers that use association
measures as features we can further improve the
quality of the extraction process. However, super-
vised machine learning requires annotated data,
which creates a bottleneck in the absence of large
corpora annotated for MWEs. In order to cir-
cumvent this bottleneck, mwetoolkit (Ramisch et
al., 2010b) generates training instances by first ex-
tracting candidates that fit a certain part-of-speech
pattern, such as Noun-Noun or Noun-Adjective,
and then marking the candidates as positive or
negative instances depending on whether they can
be found in a given lexicon or not. Such a train-
ing set will presumably not contain any false pos-
itives (that is, candidates marked as positive in-
stances that are not real MWEs), but depending on
the coverage of the lexicon there will be a smaller
or larger proportion of false negatives. The ques-
tion is what quality can be obtained using such a
noisy training set. To the best of our knowledge,
we cannot find the answer for French in literature.
Indeed, Ramisch et al. (2012) compares the perfor-
mance of mwetoolkit with another toolkit on En-
glish and French corpora, but they never use the
data generated by mwetoolkit to train a model. In
contrast, Zilio et al. (2011) make a study involving
training a model but use it only on English and use
extra lexical resources to complement the machine
learning method, so their study does not focus just
on classifier evaluation.
This paper presents the first evaluation of mwe-
toolkit on French together with two resources very
commonly used by the French NLP community:
the tagger TreeTagger (Schmid, 1994) and the dic-
tionary Dela.
1
Training and test data are taken
from the French Europarl corpus (Koehn, 2005)
and classifiers are trained using the Weka machine
learning toolkit (Hall et al., 2009). The primary
goal is to evaluate what level of precision can be
achieved for nominal MWEs, using a manual eval-
uation of MWEs extracted, and to what extent the
MWEs extracted are novel and can be used to en-
rich the lexicon. In addition, we will investigate
what effect the choice of part-of-speech patterns
used to generate the training data has on precision
and novelty. Our results indicate that classifiers
1
http://www-igm.univ-mlv.fr/
?
unitex/
index.php?page=5&html=bibliography.html
72
achieve precision in the 60?75% range and that
about half of the MWEs found are novel ones. In
addition, it seems that the choice of patterns used
to generate the training data can affect the trade-
off between precision and novelty.
2 Related Work
2.1 Extraction Techniques
There is no unique definition of MWEs (Ramisch,
2012). In the literature on the subject, we no-
tice that manual MWE extraction often requires
several annotators native of the studied language.
Nevertheless, some techniques exist for selecting
automatically candidates that are more likely to be
the true ones. Candidates can be validated against
an external resource, such as a lexicon. It is pos-
sible also to check the frequency of candidates in
another corpus like the web. Villaviciencio (2005),
for example, uses number of hits on Google for
validating the likelihood of particle verbs.
However, as Ramisch (2012) states in his
introduction, MWE is an institutionalised phe-
nomenon. This means that an MWE is fre-
quently used and is part of the vocabulary of a
speaker as well as the simple words. It means
also that MWEs have specific statistical proper-
ties that have been studied. The results of those
studies are statistical measures such as dice score,
maximum likelihood estimate, pointwise mutual
information, T-score. As Islam et al. (2012) re-
mark in a study of Google Ngram, those measures
of association are language independent. And it
is demonstrated by Pecina (2008) that combining
different collocation measures using standard sta-
tistical classification methods improves over using
a single collocation measure. However, nowadays,
using only lexical association measures for extrac-
tion and validation of MWE is not considered the
most effective method. The tendency these last
years is to combine association measures with lin-
guistic features (Ramisch et al., 2010a; Pecina,
2008; Tsvetkov and Wintner, 2011).
2.2 Mwetoolkit
Among the tools developed for extracting MWEs,
mwetoolkit is one of the most recent. Developed
by Ramisch et al. (2010b) it aims not only at ex-
tracting candidates for potential MWEs, but also
at extracting their association measures. Provided
that a lexicon of MWEs is available and provided
a preprocessed corpus, mwetoolkit makes it pos-
sible to train a machine learning system with the
association measures as features with a minimum
of implementation.
Ramisch et al. (2010b) provide experiments on
Portuguese, English and Greek. Zilio et al. (2011)
provide experiments with this tool as well. In the
latter study, after having trained a machine on bi-
gram MWEs, they try to extract full n-gram ex-
pressions from the Europarl corpus. They then
reuse the model obtained on bigrams for extraction
of full n-gram MWEs. Finally, they apply a second
filter for getting back the false negatives by check-
ing every MWE annotated as False by the algo-
rithm against a online dictionary. This method gets
a very good precision (over 87%) and recall (over
84%). However, we do not really know if this re-
sult is mostly due to the coverage of the dictionary
online. What is the contribution of machine learn-
ing in itself? Another question raised by this study
is the ability of a machine trained on one kind of
pattern (e.g., Noun-Adjective) to extract correctly
another kind of MWE pattern (e.g., Noun-Noun).
That is the reason why we will run three experi-
ments close to the one of Zilio et al. (2011) but
were the only changing parameter is the pattern
that we train our classifiers on.
3 Generating Training Data
3.1 Choice of Patterns
In contrast to Zilio et al. (2011) we run our ex-
periment on French. The choice of a differ-
ent language requires an adaptation of the pat-
terns. French indeed, as a latin language, does
not show the same characteristic patterns as En-
glish. We know that there is a strong recurrence
of the pattern Noun-Adjective in bigram MWEs
in our lexicon (Silberztein and L.A.D.L., 1990,
p.82), and the next most frequent pattern is Noun-
Noun. Therefore we extract only candidates that
correspond to these patterns. And, since we have
two patterns, we will run two extra experiments
where our models will be trained only on one of
the patterns. In this way, we will discover how
sensitive the method is to the choice of pattern.
3.2 Corpus
As Ramisch et al. (2012) we work on the French
Europarl corpus. We took the three first million
words of Europarl and divided it into three equal
parts (one million words each) for running our ex-
periments. The first part will be devoted at 80% to
73
training and 20% to development test set, when
training classifiers on Noun-Adjective or Noun-
Noun patterns, or both. We use the second million
as a secondary development set that is not used in
this study. The third million is used as a final test
set and we will present results on this set.
3.3 Preprocessing
For preprocessing we used the same processes as
described in Zilio et al. (2011). First we ran the
sentence splitter and the tokenizer provided with
the Europarl corpus. Then we ran TreeTagger
(Schmid, 1994) to obtain the tags and the lemmas.
3.4 Extracting Data and Features
The mwetoolkit takes as input a preprocessed cor-
pus plus a lexicon and gives two main outputs: an
arff file which is a format adapted to the machine
learning framework Weka, and an XML file. At
the end of the process we obtain, for each candi-
date, a binary classification as an MWE (True) or
not (False) depending on whether it is contained
in the lexicon. For each candidate, we also ob-
tain the following features: maximum likelihood
estimate, pointwise mutual information, T-score,
dice coefficient, log-likelihood ratio. The machine
learning task is then to predict the class (True or
False) given the features of a candidate.
3.5 Choice of a Lexicon in French
The evaluation part of mwetoolkit is furnished
with an internal English lexicon as a gold stan-
dard for evaluating bigram MWEs, but for French
it is necessary to provide an external resource.
We used as our gold standard the French dictio-
nary Dela (Silberztein and L.A.D.L., 1990), the
MWE part of which is called Delac. It is a gen-
eral purpose dictionary for NLP and it includes
100,000 MWE expressions, which is a reasonable
size for leading an experiment on the Europarl
corpus. Also the technical documentation of the
Delac (Silberztein and L.A.D.L., 1990, p.72) says
that this dictionary has been constructed by lin-
guists with reference to several dictionaries. So it
is a manually built resource that contains MWEs
only referenced in official lexicographical books.
3.6 Processing
Thanks to mwetoolkit we extracted all the bi-
grams that correspond to the patterns Noun-
Adjective (NA), Noun-Noun (NN) and to both
Noun-Adjective and Noun-Noun (NANN) in our
three data sets and let mwetoolkit make an auto-
matic annotation by checking the presence of the
MWE candidates in the Delac. Note that the auto-
matic annotation was used only for training. The
final evaluation was done manually.
4 Training Classifiers
For finding the best model we think that we have
to favour the recall of the positive candidates. In-
deed, when an MWE candidate is annotated as
True, it means that it is listed in the Dela, which
means that it is an officially listed MWE. How-
ever, if an MWE is not in the Dela, it does not
mean that the candidate does not fulfil all the cri-
teria for being an MWE. For this reason, obtaining
a good recall is much more difficult than getting a
good precision, but it is also the most important if
we stay on a lexicographical purpose.
4.1 Training on NA
We tested several algorithms offered by Weka as
well as the training options suggested by Zilio et
al. (2011). We also tried to remove some features
and to keep only the most informative ones (MLE,
T-score and log-likelihood according to informa-
tion gain ratio) but we noticed each time a loss in
the recall. At the end with all the features kept and
for the purpose of evaluating NA MWE candidates
the best classification algorithm was the Bayesian
network.
4.2 Training on NN
When training a model on NN MWEs, our aim
was to keep as much as possible the same condi-
tion for our three experiments. However, the NN
training set has definitely not the same properties
as the NA and NANN ones. The NN training set
is twenty-four times smaller than NA training set.
Most of the algorithms offered by Weka therefore
ended up with a dummy systematic classification
to the majority class False. The only exceptions
were ibk, ib1, hyperpipes, random trees and ran-
dom forest. We kept random forest because it gave
the best recall with a very good precision. We tried
several options and obtained the optimum results
with 8 trees each constructed while considering 3
random features, one seed, and unlimited depth of
trees. As well as for NA we kept all features.
4.3 Training on NA+NN
For the training on NANN candidates we tried the
same models as for NN and for NA candidates.
74
The best result was obtained with the same algo-
rithm as for NA: Bayesian network.
5 Evaluation
The data automatically annotated by mwetoolkit
could be used for training, but to properly evalu-
ate the precision of MWE extraction on new data
and not penalize the system for ?false positives?
that are due to lack of coverage of the lexicon, we
needed to perform a manual annotation. To do so,
we randomly picked 100 candidates annotated as
True by each model (regardless if they were in the
Delac or not). We then annotated all such candi-
dates as True if they were found in Delac (without
further inspection) and otherwise classified them
manually following the definition of Silberztein
and L.A.D.L. (1990) and the intuition of a native
French speaker. The results are in Table 1.
Extracting NA NN NANN
NANN model model model
In Delac 40 ?9.4 18 ?7.2 28 ?8.6
Not in Delac 34 ?9.0 41 ?9.2 38 ?9.3
Precision 74 ?8.4 59 ?9.2 66 ?9.0
Table 1: Performance of three different models
on the same corpus of Noun-Adjective and Noun-
Noun candidates. Percentages with 95% confi-
dence intervals, sample size = 100.
As we see in Table 1, the experiment reveals a pre-
cision ranging from almost 60% up to 74%. The
results of our comparative manual annotation indi-
cate that the model trained on NN candidates has
the capacity to find more MWEs not listed in our
lexicon (41 out of 59) even if it is the least pre-
cise model. On the other hand, we notice that the
model based on Noun-Adjective patterns is more
precise but at the same time extracts fewer MWEs
that are not already in the lexicon (34 out of 74).
Our mixed model confirms these two tendencies
with a performance in between (38 new MWEs out
of 66). Thus, the method appears to be sensitive to
the patterns used for training.
We notice during evaluation different kinds of
MWEs that are successfully extracted by models
but that are not listed in the Delac. Most of them
are the MWEs specific to Europarl (e.g., ?dimen-
sion communautaire?, ?l?egislation europ?eenne?
2
).
Another category are those MWEs that became
2
?community scale?, ?European legislation?
popular in the French language after the years
2000?s and therefore could not be included in the
Delac, released in 1997. Indeed by reading the
first paragraph of the French version of Europarl
we notice that the texts have been written after
1999. Of course, they are not the majority of the
successfully extracted MWEs but we still manage
to find up to 3 of them in a sample of 100 that we
checked (?d?eveloppement durable?, ?radiophonie
num?erique?, ?site internet?
3
). Furthermore the cor-
pus in itself is already more than ten years old,
so in a text of 2014 we can expect to find even
more of them. Finally, there are MWEs that are
not in French (e.g., ?Partido popular?), these, how-
ever, did not appear systematically in our samples.
It is tricky to learn statistical properties of
MWEs when, actually, we do not have all the in-
formation necessary for extracting the MWEs in
the corpus. Indeed, for this purpose the corpus
should ideally be read and annotated by humans.
However, we still managed to train models with
decent performance, even if it is likely that a lot of
candidates pre-annotated as False in the training
data were probably perfect MWEs. This means
that the Delac has covered enough MWEs for the
features to not appear as completely meaningless
and arbitrary. The final precision would never be
as good as it is, if the coverage had been not suffi-
cient enough. This shows that the method of auto-
matic annotation offered by mwetoolkit is reliable
given a lexicon as large as Delac.
6 Conclusion
We wanted to know if the method of automatic
extraction and evaluation offered by mwetoolkit
could have a decent precision in French. We an-
notated automatically part of the Europarl corpus
given the lexical resource Dela as a gold stan-
dard and generated in this way annotated training
sets. Classifiers trained on this data using Weka
achieved a maximum precision of 74%, with about
half of the extracted MWEs being novel compared
to the lexicon. In addition, we found that the fi-
nal precision and novelty scores were sensitive to
the choice of patterns used to generate the training
data.
3
?sustainable development?, ?digital radio?,?website?
75
References
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The WEKA data mining software: an update.
SIGKDD Exploration Newsletter, 11(1):10?18.
Aminul Islam, Evangelos E Milios, and Vlado Ke-
selj. 2012. Comparing Word Relatedness Mea-
sures Based on Google n-grams. In COLING, Inter-
national Conference on Computational Linguistics
(Posters), pages 495?506, Mumbai, India.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In The Tenth
Machine Translation Summit, pages 79?86, Phuket,
Thailand.
Pavel Pecina. 2008. A Machine Learning Approach
to Multiword Expression Extraction. In Proceed-
ings of the LREC 2008 Workshop Towards a Shared
Task for Multiword Expressions, pages 54?57, Mar-
rakech, Morocco.
Pavel Pecina. 2010. Lexical association measures
and collocation extraction. Language Resources and
Evaluation, 44(1-2):137?158.
Carlos Ramisch, Helena de Medeiros Caseli, Aline
Villavicencio, Andr?e Machado, and Maria Jos?e Fi-
natto. 2010a. A Hybrid Approach for Multiword
Expression Identification. In Proceedings of the 9th
International Conference on Computational Pro-
cessing of Portuguese Language (PROPOR), pages
65?74.
Carlos Ramisch, Aline Villavicencio, and Christian
Boitet. 2010b. Multiword Expressions in the wild?
The mwetoolkit comes in handy. In COLING, Inter-
national Conference on Computational Linguistics
(Demos), pages 57?60.
Carlos Ramisch, Vitor De Araujo, and Aline Villavi-
cencio. 2012. A Broad Evaluation of Techniques for
Automatic Acquisition of Multiword Expressions.
In Proceedings of ACL 2012 Student ResearchWork-
shop, pages 1?6, Jeju Island, Korea. Association for
Computational Linguistics.
Carlos Ramisch. 2012. Une plate-forme g?en?erique et
ouverte pour l?acquisition des expressions polylexi-
cales. In Actes de la 14e Rencontres des
?
Etudiants
Chercheurs en Informatique pour le Traitement Au-
tomatique des Langues, pages 137?149, Grenoble,
France.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing, pages 44?49, Manchester, Great
Britain.
Max Silberztein and L.A.D.L. 1990. Le dictionnaire
?electronique des mots compos?es. Langue franc?aise,
87(1):71?83.
Yulia Tsvetkov and Shuly Wintner. 2011. Identifica-
tion of Multi-word Expressions by Combining Mul-
tiple Linguistic Information Sources. In Empiri-
cal Methods in Natural Language Processing, pages
836?845.
Aline Villavicencio. 2005. The availability of
verb?particle constructions in lexical resources:
How much is enough? Computer Speech & Lan-
guage, 19(4):415?432.
Leonardo Zilio, Luiz Svoboda, Luiz Henrique Longhi
Rossi, and Rafael Martins Feitosa. 2011. Automatic
extraction and evaluation of MWE. In 8th Brazilian
Symposium in Information and Human Language
Technology, pages 214?218, Cuiab?a, Brazil.
76
Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 99?103,
Gothenburg, Sweden, 26-27 April 2014.
c?2014 Association for Computational Linguistics
Paraphrasing Swedish Compound Nouns in Machine Translation
Edvin Ullman and Joakim Nivre
Department of Linguistics and Philology, Uppsala University
edvinu@stp.lingfil.uu.se joakim.nivre@lingfil.uu.se
Abstract
This paper examines the effect of para-
phrasing noun-noun compounds in statis-
tical machine translation from Swedish
to English. The paraphrases are meant
to elicit the underlying relationship that
holds between the compounding nouns,
with the use of prepositional and verb
phrases. Though some types of noun-noun
compounds are too lexicalized, or have
some other qualities that make them un-
suitable for paraphrasing, a set of roughly
two hundred noun-noun compounds are
identified, split and paraphrased to be
used in experiments on statistical machine
translation. The results indicate a slight
improvement in translation of the para-
phrased compound nouns, with a minor
loss in overall BLEU score.
1 Introduction
Swedish is a highly productive language, new
words can be constructed fairly easily by concate-
nating one word with another. This is done across
word classes, although, as can be expected, pre-
dominantly with content words. Due to this high
productivity, an exhaustive dictionary of noun
compounds in Swedish does not, and can not exist.
Instead, in this project, noun compounds are ex-
tracted from the Swedish Europarl corpus (Koehn,
2005) and a subset of Swedish Wikipedia,
1
using
a slight modification of the splitting method de-
scribed in Stymne and Holmqvist (2008), based
on previous work by Koehn and Knight (2003).
The assumption that paraphrases of noun com-
pounds can help in machine translation is sup-
1
http://sv.wikipedia.org/
ported in Nakov and Hearst (2013). Although
this study was conducted with English compound
nouns, a similar methodology is applied to the
Swedish data. The split compound nouns are para-
phrased using prepositional and verb phrases, rely-
ing on native speaker intuition for the quality and
correctness of the paraphrases. A corpus is then
paraphrased using the generated paraphrases and
used to train a statistical machine translation sys-
tem to test whether or not an improvement of qual-
ity can be observed in relation to a baseline sys-
tem trained on the unmodified corpus. The results
show a minor improvement in translation quality
for the paraphrased compounds with a minor loss
in overall BLEU score.
2 Background
Previous studies on the semantics of compound
nouns have, at least for the English language, in
general focused on finding abstract categories to
distinguish different compound nouns from each
other. Although different in form, the main idea
is that a finite set of relations hold between the
constituents of all compound nouns. Experiments
have been done to analyse such categories in Girju
et al. (2005), and applied studies on paraphrasing
compound nouns with some form of predicative
representation of these abstract categories were
performed in Nakov and Hearst (2013).
Studies on Swedish compound nouns have had
a slightly different angle. As Swedish noun com-
pounding is done in a slightly different manner
than in English, two nouns can be adjoined to
form a third, two focal points in previous studies
have been detecting compound nouns (Sj?obergh
and Kann, 2004) and splitting compound nouns
(Stymne and Holmqvist, 2008; Stymne, 2009).
Swedish nouns are compounded by concatenat-
99
Type Interfixes Example
None riskkapital
(risk + kapital)
risk capital
Additions -s -t frihetsl?angtan
(frihet + l?angtan)
longing for peace
Truncations -a -e pojkv?an
(pojke + v?an)
boyfriend
Combinations -a/-s -a/-t arbetsgrupp
-e/-s -e/-t (arbete + grupp)
working group
Table 1: Compound formation in Swedish;
adapted from Stymne and Holmqvist (2008).
ing nouns to each other, creating a single unbroken
unit. Compound nouns sometimes come with the
interfixes -s or -t, sometimes without the trailing -e
or -a from the first compounding noun, and some-
times a combination of the two. It should be noted
that this is not an exhaustive list of interfixes, there
are some other, more specific rules for noun com-
pounding, justified by for example orthographic
conventions, not included in Table 1, nor covered
by the splitting algorithm. Table 1, adapted from
Stymne and Holmqvist (2008), shows the more
common modifications and their combinations.
In Koehn and Knight (2003) an algorithm for
splitting compound nouns is described. The algo-
rithm works by iterating over potential split points
for all tokens of an input corpus. The geometri-
cal mean of the frequencies of the potential con-
stituents are then used to evaluate whether the to-
ken split actually is a compound noun or not.
3 Paraphrasing Compound Nouns
To extract candidate compound nouns for para-
phrasing, we first tagged the Swedish Europarl
corpus and a subset of Swedish Wikipedia us-
ing TnT (Brants, 2000) trained on the Stockholm-
Ume?a Corpus. The resulting corpus was used to
compile a frequency dictionary and a tag dictio-
nary, which were given as input to a modified ver-
sion of the splitting algorithm from Koehn and
Knight (2003), producing a list of nouns with pos-
sible split points and the constituents and their
tags, if any, sorted by descending frequency. The
modifications to the splitting algorithm include a
lower bound, ignoring all tokens shorter than 6
characters in the corpus. This length restriction
is added with the intention of removing noise and
lowering running time. Another constraint added
is not to consider substrings shorter than 3 char-
acters. The third and last change to the algorithm
is the addition of a length similarity bias heuristic
to decide between possible split points when there
are multiple candidates with a similar result, giv-
ing a higher score to a split point that generates
substrings which are more similar in length.
Due to the construction of the splitting algo-
rithm, not all split nouns are noun compounds,
and without any gold standard to verify against,
a set of 200 compound nouns were manually se-
lected by choosing the top 200 valid compounds
from the frequency-sorted list. The split com-
pound nouns were then paraphrased by a native
speaker of Swedish and validated by two other na-
tive speakers of Swedish. The paraphrases were
required to be exhaustive (not leave out important
semantic information), precise (not include irrel-
evant information), and standardized (not deviate
from other paraphrases in terms of structure).
Nakov and Hearst (2013) have shown that ver-
bal paraphrases are superior to the more sparse
prepositional paraphrases, but also that preposi-
tional paraphrases are more efficient for machine
translation experiments. However, when examin-
ing the compound nouns closely it becomes ob-
vious that the potential paraphrases fall in one of
the following four categories. The first category is
compound nouns that are easy to paraphrase by
a prepositional phrase only, (Examples 1a, 1b),
sometimes with several possible prepositions, as
in the latter case.
(1) a. psalmf?orfattare (hymn writer)
f?orfattare
writer
av
of
psalmer
hymns
b. j?arnv?agsstation (railway station)
station
station
{f?or,
{for,
p?a,
on,
l?angs}
along}
j?arnv?ag
railway
The second category overlaps somewhat with the
first category in that the compound nouns could be
paraphrased using only a prepositional phrase, but
some meaning is undoubtedly lost in doing so. As
such, the more suitable paraphrases contain both
prepositional and verb phrases (Examples 2a, 2b).
(2) a. barnsk?adespelare (child actor)
sk?adespelare
actor
som
who
?ar
is
barn
child
100
b. studioalbum (studio album)
album
album
inspelat
recorded
i
in
en
a
studio
studio
The third and fourth category represent noun com-
pounds that are not necessarily decomposable into
their constituents. Noun compounds in the third
category can be paraphrased with some difficulty
using prepositional phrases, verb phrases as well
as deeper knowledge of the semantics and prag-
matics of Swedish (Examples 3a, 3b).
(3) a. v?arldskrig (world war)
krig
war
som
that
drabbar
affects
hela
whole
v?arlden
world
b. l?angdskid?akning (cross-country ski-
ing)
skid?akning
skiing
p?a
on
plan
level
mark
ground
Noun compounds in the fourth category are even
harder, if not impossible to paraphrase. The mean-
ing of compound nouns that fall into this category
cannot be extracted from the constituents, or the
meaning has been obscured over time (Examples
4a, 4b). There is no use paraphrasing these com-
pound nouns, and as such they are left out.
(4) a. stadsr?attighet (city rights)
b. domkyrka (cathedral)
All compound nouns that are decomposable into
their constituents were paraphrased according to
the criteria listed above as far as possible.
4 Machine Translation Experiments
To evaluate the effect of compound paraphrasing,
a phrase-based statistical machine translation sys-
tem was trained on a subset of roughly 55,000
sentences from Swedish-English Europarl, with
the Swedish compound nouns paraphrased before
training. The system was trained using Moses
(Koehn et al., 2007) with default settings, using a
5-gram language model created from the English
side of the training corpus using SRILM (Stolcke,
2002). A test set was paraphrased in the same way
and run through the decoder. We tested two ver-
sions of the system, one where all 200 paraphrases
were used, and one where only the paraphrases in
the first two categories (transparent prepositional
and verb phrases) were used. As a baseline, we
used a system trained with the same settings on
the unmodified training corpus and applied to the
unmodified test corpus.
The systems were evaluated in two ways. First,
we computed standard BLEU scores. Secondly,
the translation of paraphrased compounds was
manually evaluated, by the author, in a random
sample of 100 sentences containing one or more of
the paraphrased compounds. Since the two para-
phrase systems used different paraphrase sets, the
manual evaluation was performed on two different
samples, in both cases comparing to the baseline
system. The results are shown in Table 2.
Looking first at the BLEU scores, we see that
there is a small drop for both paraphrase systems.
This drop in performance is most certainly a side
effect of the design of the paraphrasing script.
There is a certain crudeness in how inflections
are handled resulting in sentences that may be un-
grammatical, albeit only slightly. Inflections in the
compounding nouns is retained. However, in para-
phrases of category 2 and 3, the verbs are always
in the present tense, as deriving the tense from the
context can be hard to do with enough precision
to make it worthwhile. Consequently, the slightly
better score for the system that only uses para-
phrases of category 1 and 2 is probably just due
to the fact that fewer compounds are paraphrased
with verbal paraphrases.
Turning to the manual evaluation, we see first of
all that the baseline does a decent job translating
the compound nouns, with 88/100 correct transla-
tions in the first sample and 81/100 in the second
sample. Nevertheless, both paraphrase systems
achieve slightly higher scores. The system using
all paraphrases improves from 88 to 93, and the
system that only uses the transparent paraphrases
improves from 81 and 90. Neither of these differ-
ences is statistically significant, however. McNe-
mar?s test (McNemar, 1947) gives a p value of 0.23
for S1 and 0.11 for S2. So, even if it is likely that
the paraphrase systems can improve the quality of
compound translation, despite a drop in the overall
BLEU score, a larger sample would be needed to
fully verify this.
5 Discussion
The results from both the automatic and the man-
ual evaluation are inconclusive. On the one hand,
overall translation quality, as measured by BLEU,
is lowered, if only slightly. On the other, the
manual evaluation shows that, for the paraphrased
101
System BLEU Comp
S1 S2
Baseline 26.63 88 81
All paraphrases 26.50 93 ?
Paraphrases 1?2 26.59 ? 90
Table 2: Experimental results. Comp = translation
of compounds; S1 = sample 1; S2 = sample 2.
compound nouns, the experimental decoders per-
form better than the baseline. However, this im-
provement cannot be established to be statistically
significant. This does not necessarily mean that
paraphrasing as a general concept is flawed in
terms of translation quality, but judging from these
preliminary results, further experiments with para-
phrasing compound nouns need to address a few
issues.
The lack of quality in the paraphrases, proba-
bly attributable to how inflections are handled in
the paraphrasing scripts, might be the reason why
the first experimental system performs worse than
the second. This could indicate that there is lit-
tle to be won in paraphrasing more complex com-
pound nouns. Another possible explanation lies in
the corpus. The tone in the Europarl corpus is very
formal, and this is not necessarily the case with the
more complex paraphrases.
The number of compound nouns actually para-
phrased might also attribute to the less than stel-
lar results. If, when training the experimental
systems using the paraphrased Swedish corpora,
the number of non-paraphrased compound nouns
outweigh the number of paraphrased compound
nouns the impact of the paraphrases might actu-
ally only distort the translation models. This could
very well be the problem here, and it is hard from
these experiments to judge whether or not the so-
lution is to have more paraphrasing, or none at all.
6 Conclusion
We have reported a pilot study on using paraphras-
ing of compound nouns to improve the quality
of machine translation from Swedish to English,
building on previous work by Nakov and Hearst
(2013). The experimental results are inconclusive,
but there is at least weak evidence that this tech-
nique may improve translation quality specifically
for compounds, although it may have a negative
effect on other aspects of the translation. Further
experiments could shed some light on this.
There are a couple of routes that are interesting
to follow from here. In Nakov and Hearst (2013),
a number of verbal and prepositional paraphrases
are gathered through the means of crowd sourc-
ing, and compared to paraphrases gathered from
a simple wild card keyword search using a web
based search engine. Since the paraphrases in the
experiments described in this paper are done by
the author and verified by no more than two other
native speakers of Swedish, the paraphrases might
not be generic enough. By crowd sourcing para-
phrase candidates the impact of one individual?s
personal style and tone can be mitigated.
Another interesting topic for further research is
the one of automated compound noun detection.
The algorithm used for splitting compound nouns
returns a confidence score which is based on the
geometrical mean of the frequencies of the con-
stituents together with some heuristics based on
things such as relative length of the constituents
and whether or not the constituent was found at all
in the corpus. This confidence score could poten-
tially be used for ranking not the most frequently
occurring compound nouns, but the compounds
where the classifier is most confident.
A number of improvements on the applied sys-
tem can probably lead to a wider coverage. For
one, to alter the algorithm so as to allow for re-
cursive splitting would help in detecting and dis-
ambiguating compound nouns consisting of three
or more constituents. This might be helpful since,
as previously mentioned, Swedish is a highly pro-
ductive language, and it is quite common to see
compound nouns consisting of three or more con-
stituents. It should be noted however, that for this
to have the desired effect, the paraphrasing would
have to be done recursively as well. This could
potentially lead to very long sentences generated
from very short ones, if the sentence includes a
compound consisting of three or more parts.
Some other minor improvements or possible ex-
tensions over the current implementation includes
taking into account all orthographical irregulari-
ties to get a broader coverage, running the algo-
rithm over a more domain specific corpus to get
more relevant results, and finally, automating the
actual paraphrasing. This last step, however, is of
course far from trivial.
102
References
Thorsten Brants. 2000. TnT: A statistical part-of-
speech tagger. In Proceedings of the 6th Con-
ference on Applied Natural Language Processing,
pages 224?231.
Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel
Antohe. 2005. On the Semantics of Noun Com-
pounds. Computer Speech & Language, 19(4):479?
496.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In Proceedings of
the 10th Conference on European Chapter of the As-
sociation for Computational Linguistics, pages 187?
193.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, and
Richard Zens. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the 45th Annual Meeting of the ACL on Interactive
Poster and Demonstration Sessions, pages 177?180.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of the
10th Machine Translation Summit, pages 79?86.
Quinn McNemar. 1947. Note on the Sampling Error
of the Difference Between Correlated Proportions or
Percentages. Psychometrika, 12(2):153?157.
Preslav I. Nakov and Marti A. Hearst. 2013. Seman-
tic Interpretation of Noun Compounds Using Ver-
bal and Other Paraphrases. ACM Transactions on
Speech and Language Processing, 10(3):1?51.
Jonas Sj?obergh and Viggo Kann. 2004. Finding the
correct interpretation of swedish compounds, a sta-
tistical approach. In Proceedings of the 4th Interna-
tional Conference on Language Resources and Eval-
uation.
Andreas Stolcke. 2002. SRILM: An extensible lan-
guage modeling toolkit. In Proceedings of the 7th
International Conference on Spoken Language Pro-
cessing.
Sara Stymne and Maria Holmqvist. 2008. Process-
ing of swedish compounds for phrase-based statisti-
cal machine translation. In Proceedings of the 12th
Annual Conference of the European Association for
Machine Translation, pages 180?189.
Sara Stymne. 2009. Compound Processing
for Phrase-Based Statistical Machine Translation.
Ph.D. thesis, Department of Computer and Informa-
tion Science, Link?opings Univ.
103
Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 124?131,
Gothenburg, Sweden, 26-27 April 2014.
c?2014 Association for Computational Linguistics
Issues in Translating Verb-Particle Constructions from German to English
Nina Schottm?uller
Uppsala University
Department of Linguistics and Philology
nschottmueller@gmail.com
Joakim Nivre
Uppsala University
Department of Linguistics and Philology
joakim.nivre@lingfil.uu.se
Abstract
In this paper, we investigate difficulties
in translating verb-particle constructions
from German to English. We analyse the
structure of German VPCs and compare
them to VPCs in English. In order to find
out if and to what degree the presence of
VPCs causes problems for statistical ma-
chine translation systems, we collected a
set of 59 verb pairs, each consisting of a
German VPC and a synonymous simplex
verb. With this data, we constructed a
test suite of 236 sentences where the sim-
plex verb and VPC are completely substi-
tutable. We then translated this dataset to
English using Google Translate and Bing
Translator. Through an analysis of the re-
sulting translations we are able to show
that the quality decreases when translat-
ing sentences that contain VPCs instead
of simplex verbs. The test suite is made
freely available to the community.
1 Introduction
In this paper, we analyse and discuss German
verb-particle constructions (VPCs). VPCs are
a type of multiword expressions (MWEs) which
are defined by Sag et al. (2002) to be ?idiosyn-
cratic interpretations that cross word bounderies
(or spaces)?. Kim and Baldwin (2010) extend
this explanation in their definition of MWEs be-
ing ?lexical items consisting of multiple simplex
words that display lexical, syntactic, semantic
and/or statistical idiosyncrasies?.
VPCs are made up of a base verb and a par-
ticle. In contrast to English, where the particle is
always separated from the verb, German VPCs are
separable, meaning that the particle can either be
attached as a prefix to the verb or stand separate
from it, depending on factors such as tense and
voice, along with whether the VPC is found in a
main clause or subordinate clause.
The fact that German VPCs are separable
means that word order differences between the
source and target language can occur in statisti-
cal machine translation (SMT). It has been shown
that the translation quality of translation systems
can suffer from such differences in word order
(Holmqvist et al., 2012). Since VPCs make up for
a significant amount of verbs in English, as well
as in German, they are a likely source for transla-
tion errors. This makes it essential to analyse any
issues with VPCs that occur while translating, in
order to be able to develop possible improvements.
In our approach, we investigate if the presence
of VPCs causes translation errors. We do this by
creating and utilising a dataset of 236 sentences,
using a collection of 59 German verb pairs, each
consisting of a VPC and a synonymous simplex
verb, a test suite that is made freely available. We
discuss the English translation results generated
by the popular translation systems Google Trans-
late and Bing Translator and show that the pres-
ence of VPCs can harm translation quality.
We begin this paper by stating important related
work in the fields related to VPCs in Section 2 and
continue with a detailed analysis of VPCs in Ger-
man in Section 3. In Section 4, we describe how
the data used for evaluation was compiled, and in
Section 5, we give further details on the evalua-
tion in terms of metrics and systems tested. Sec-
tion 6 gives an overview of the results, as well as
their discussion, where we present possible rea-
sons why VPCs performed worse in the experi-
ments, which finally leads to our conclusions in
Section 7. An appendix lists all the verb pairs used
to construct the test suite.
2 Related Work
A lot of research has been done on the identifica-
tion, classification, and extraction of VPCs, with
124
the majority of work done on English. For exam-
ple, Villavicencio (2005) presents a study about
the availability of VPCs in lexical resources and
proposes an approach to use semantic classifica-
tion to identify as many VPC candidates as possi-
ble. She then validates these candidates using the
retrieved results from online search engines.
Many linguistic studies analyse VPCs in Ger-
man, or English, respectively, mostly discussing
the grammar theory that underlies the composi-
tionality of MWEs in general or presenting more
particular studies such as theories and experiments
about language acquisition. An example would be
the work of Behrens (1998), in which she con-
trasts how German, English and Dutch children
acquire complex verbs when they learn to speak,
focusing on the differences in the acquisition of
VPCs and prefix verbs. In another article in this
field by M?uller (2002), the author focuses on non-
transparent readings of German VPCs and de-
scribes the phenomenon of how particles can be
fronted.
Furthermore, there has been some research
dealing with VPCs in machine translation as well.
In a study by Chatterjee and Balyan (2011), sev-
eral rule-based solutions are proposed for how
to translate English VPCs to Hindi, using their
surrounding entities. Another paper in this field
by Collins et al. (2005) presents an approach to
clause restructuring for statistical machine trans-
lation from German to English in which one step
consists of moving the particle of a particle verb
directly in front of the verb. Moreover, even
though their work does not directly target VPCs,
Holmqvist et al. (2012) present a method for im-
proving word alignment quality by reordering the
source text according to the target word order,
where they also mention that their approach is sup-
posed to help with different word order caused by
finite verbs in German, similar to the phenomenon
of differing word order caused by VPCs.
3 German Verb-Particle Constructions
VPCs in German are made up of a base verb and
a particle. In contrast to English, German VPCs
are separable, meaning that they can occur sepa-
rated, but do not necessarily have to. This applies
only for main clauses, as VPCs can never be sep-
arated in German subordinate clauses. Depending
on the conjugation of the verb, the particle can a)
be attached to the front of the verb as prefix, ei-
ther directly or with an additional morpheme, or
b) be completely separated from the verb. The
particle is directly prefixed to the verb if it is an
infinitive construction, for example within an ac-
tive voice present tense sentence using an auxil-
iary (e.g., muss herausnehmen). It is also attached
directly to the conjugated base verb when using
a past participle form to indicate passive voice or
perfect tense (e.g., herausgenommen), or if a mor-
pheme is inserted to build an infinitive construc-
tion using zu (e.g., herauszunehmen). The parti-
cle is separated from the verb root in finite main
clauses where the particle verb is the main verb
of the sentence (e.g., nimmt heraus). The fol-
lowing examples serve to illustrate the aforemen-
tioned three forms of the non-separated case and
the one separated case.
Attached:
Du musst das herausnehmen.
You have to take this out.
Attached+perfect:
Ich habe es herausgenommen.
I have taken it out.
Attached+zu:
Es ist nicht erlaubt, das herauszunehmen.
It is not allowed to take that out.
Separated:
Ich nehme es heraus.
I take it out.
Just like simplex verbs, VPCs can be transitive
or intransitive. For the separated case, a transi-
tive VPC?s base verb and particle are always split
and the object has to be positioned between them,
despite the generally freer word order of German.
For the non-separated case, the object is found be-
tween the finite verb (normally an auxiliary) and
the VPC.
Separated transitive:
Sie nahm die Klamotten heraus.
*Sie nahm heraus die Klamotten.
She took [out] the clothes [out].
Non-separated transitive:
Sie will die Klamotten herausnehmen.
*Sie will herausnehmen die Klamotten.
She wants to take [out] the clothes [out].
Similar to English, a three-fold classification can
be applied to German VPCs. Depending on their
125
formation, they can either be classified as a) com-
positional, e.g., herausnehmen (to take out), b) id-
iomatic, e.g., ablehnen (to turn down, literally: to
lean down), or c) aspectual, e.g., aufessen (to eat
up), as proposed in Villavicencio (2005) and Deh?e
(2002).
Compositional:
Sie nahm die Klamotten heraus.
She took out the clothes.
Idiomatic:
Er lehnt das Jobangebot ab.
He turns down the job offer.
Aspectual:
Sie a? den Kuchen auf.
She ate up the cake.
There is another group of verbs in German which
look similar to VPCs. Inseparable prefix verbs
consist of a derivational prefix and a verb root. In
some cases, these prefixes and verb particles can
look the same and can only be distinguished in
spoken language. For instance, the infinitive verb
umfahren can have the following translations, de-
pending on which syllable is stressed.
VPC:
umfahren
to knock down sth./so. (in traffic)
Inseparable prefix verb:
umfahren
to drive around sth./so.
As mentioned before, there is a clear difference
between these two seemingly identical verbs in
spoken German. In written German, however, the
plain infinitive forms of the respective verbs are
the same. In most cases, context and use of finite
verb forms reveal the correct meaning.
VPC:
Sie fuhr den Mann um.
She knocked down the man (with her car).
Inseparable prefix verb:
Sie umfuhr das Hindernis.
She drove around the obstacle.
For reasons of similarity, VPCs and inseparable
prefix verbs are sometimes grouped together un-
der the term prefix verbs, in which case VPCs are
then called separable prefix verbs. However, since
Simplex VPC Total
Finite sentence 59 59 118
Auxiliary sentence 59 59 118
Total 118 118 236
Table 1: Types and number of sentences in the test
suite.
the behaviour of inseparable prefix verbs is like
that of normal verbs, they will not be treated dif-
ferently throughout this paper and will only serve
as comparison to VPCs in the same way that any
other inseparable verbs do.
4 Test Suite
In order to find out how translation quality is in-
fluenced by the presence of VPCs, we are in need
of a suitable dataset to evaluate the translation re-
sults of sentences containing both particle verbs
and synonymous simplex verbs. Since it seems
that there is no suitable dataset available for this
purpose, we decided to compile one ourselves.
With the help of several online dictionary re-
sources, we first collected a list of candidate
VPCs, based on their particle, so that as many dif-
ferent particles as possible were present in the ini-
tial set of verbs, while making sure that each par-
ticle was only sampled a handful of times. We
then checked each of the VPCs for suitable sim-
plex verb synonyms, finally resulting in a set of 59
verb pairs, each consisting of a simplex verb and a
synonymous German VPC (see Appendix A for a
full list). We allowed the two verbs of a verb pair
to be partially synonymous as long as both their
subcategorization frame and meaning was identi-
cal for some cases.
For each verb pair, we constructed two German
sentences in which the verbs were syntactically
and semantically interchangeable. The first sen-
tence for each pair had to be a finite construction,
where the respective simplex or particle verb was
the main verb, containing a direct object or any
kind of adverb to ensure that the particle of the
particle verb is properly separated from the verb
root. For the second sentence, an auxiliary with
the infinitive form of the respective verb was used
to enforce the non-separated case, where the parti-
cle is attached to the front of the verb.
Using both verbs of each verb pair, this resulted
in a test suite consisting of a total of 236 sentences
(see Table 1 for an overview). The following ex-
126
ample serves to illustrate the approach for the verb
pair kultivieren - anbauen (to grow).
Finite:
Viele Bauern in dieser Gegend kultivieren
Raps. (simplex)
Viele Bauern in dieser Gegend bauen Raps
an. (VPC)
Many farmers in this area grow rapeseed.
Auxiliary:
Kann man Steinpilze kultivieren? (simplex)
Kann man Steinpilze anbauen? (VPC)
Can you grow porcini mushrooms?
The sentences were partly taken from online texts,
or constructed by a native speaker. They were
set to be at most 12 words long and the position
of the simplex verb and VPC had to be in the
main clause to ensure comparability by avoiding
too complex constructions. Furthermore, the sen-
tences could be declarative, imperative, or inter-
rogative, as long as they conformed to the require-
ments stated above. The full test suite of 236 sen-
tences is made freely available to the community.
1
5 Evaluation
Two popular SMT systems, namely Google Trans-
late
2
and Bing Translator,
3
were utilised to per-
form German to English translation on the test
suite. The translation results were then manually
evaluated under the following criteria:
? Translation of the sentence: The translation
of the whole sentence was judged to be ei-
ther correct or incorrect. Translations were
judged to be incorrect if they contained any
kind of error, for instance grammatical mis-
takes (e.g., tense), misspellings (e.g., wrong
use of capitalisation), or translation errors
(e.g., inappropriate word choices).
? Translation of the verb: The translation of
the verb in each sentence was judged to be
correct or incorrect, depending on whether or
not the translated verb was appropriate in the
context of the sentence. It was also judged to
be incorrect if for instance only the base verb
was translated and the particle was ignored,
or if the translation did not contain a verb.
1
http://stp.lingfil.uu.se/
?
ninas/testsuite.txt
2
http://www.translate.google.com
3
http://www.bing.com/translator
? Translation of the base verb: Furthermore,
the translation of the base verb was judged
to be either correct or incorrect in order to
show if the particle of an incorrectly trans-
lated VPC was ignored, or if the verb was
translated incorrectly for any other reason.
For VPCs, this was judged to be correct if
either the VPC, or at least the base verb was
translated correctly. For simplex verbs, the
judgement for the translation of the verb and
the translation of the base verb was always
judged the same, since they do not contain
separable particles.
The evaluation was carried out by a native speaker
of German and was validated by a second German
native speaker, both proficient in English.
6 Results and Discussion
The results of the evaluation can be seen in Table
2. In this table, we merged the results for Google
and Bing to present the key results clearly. For
a more detailed overview of the results, includ-
ing the individual scores for both Google Translate
and Bing Translator, see Table 3.
In the total results, we can see that on average
48.3% of the 236 sentences were translated cor-
rectly, while a correct target translation for the
sentence?s main verb was found in 81.1% of all
cases. Moreover, 92.2% of the base verb transla-
tions were judged to be correct.
By looking at the results for VPCs and simplex
verbs separately, we are able to break down the to-
tal figures and compare them. The first thing to
note is that only 43.2% of the sentences contain-
ing VPCs were translated correctly, while the sys-
tems managed to successfully translate 53.4% of
the simplex verb sentences, showing a difference
of around 10% absolute. The results for the verb
transitions in these sentences differ even further
with 71.6% of all VPC translations being judged
to be correct and 90.7% of the simplex translations
judged to be acceptable, revealing a difference of
around 20% absolute.
Another interesting result is the translation of
the base verb, where a correct translation was
found in 93.6% of the cases for VPCs, meaning
that in 22.0% of the sentences the systems made a
mistake with a particle verb, but got the meaning
of the base verb right. This indicates that the usu-
ally different meaning of the base verb can be mis-
leading when translating a sentence that contains
127
Sentence (%) Verb (%) Base V. (%)
VPC 102 (43.2%) 169 (71.6%) 221 (93.6%)
Finite 47 (39.8%) 80 (67.8%) 114 (96.6%)
Infinitive 55 (46.6%) 89 (75.4%) 107 (90.7%)
Simplex 126 (53.4%) 214 (90.7%) 214 (90.7%)
Finite 59 (50.0%) 103 (87.3%) 103 (87.3%)
Infinitive 67 (56.8%) 111 (94.1%) 111 (94.1%)
Total 228 (48.3%) 381 (81.1%) 435 (92.2%)
Table 2: Translation results for the test suite summed over both Google Translate and Bing Translator;
absolute numbers with percentages in brackets. Sentence = correctly translated sentences, Verb = cor-
rectly translated verbs, Base V. = correctly translated base verbs, Simplex = sentences containing simplex
verbs, VPC = sentences containing VPCs, Finite = sentences where the target verb is finite, Infinitive =
sentences where the target verb is in the infinitive.
a VPC, causing a too literal translation. Interest-
ingly, many of the cases where the resulting En-
glish translation was too literal are sentences that
contain idiomatic VPCs rather than compositional
or aspectual ones, such as vorf?uhren (to demon-
strate, literally: to lead ahead/before).
In general, the sentences that contained finite
verb forms achieved worse results than the ones
containing infinitives. However, the differences
are only around 7% and seem to be constant be-
tween VPC and simplex verb sentences. Taking
into account that the sentences of each sentence
pair should not differ too much in terms of com-
plexity, this could be a hint that finite verb forms
are harder to translate than auxiliary constructions,
but no definite conclusions can be drawn from
these results.
Looking at the individual results for Google and
Bing, however, we can see that Bing?s results show
only a small difference between finite and infini-
tive verbs, whereas the scores for Google vary
much more. Even though the overall results are
still rather worse than Google?s, Bing Translator
gets a slightly better result on both finite sim-
plex verbs and VPCs, which could mean that the
system is better when it comes to identifying the
separated particle that belongs to a particle verb.
Google Translate, on the other hand, gets a notice-
ably low score on finite VPC translations, namely
59.3% compared to 86.4% for finite simplex verbs,
or to Bing?s result of 76.3%, which clearly shows
that separated VPCs are a possible cause for trans-
lation error.
The following examples serve to illustrate the
different kinds of problems that were encountered
during translation.
Ich lege manchmal Gurken ein.
Google: Sometimes I put a cucumber.
Bing: I sometimes put a cucumber.
A correct translation for einlegen would be to
pickle or to preserve. Here, both Google Trans-
late and Bing Translator seem to have used only
the base verb legen (to put, to lay) for translation
and completely ignored its particle.
Ich pflanze Chilis an.
Google: I plant to Chilis.
Bing: I plant chilies.
Here, Google Translate translated the base verb of
the VPC anpflanzen to plant, which corresponds
to the translation of pflanzen. The VPC?s particle
was apparently interpreted as the preposition to.
Furthermore, Google encountered problems trans-
lating Chilis, as this word should not be written
with a capital letter in English and the commonly
used plural form would be chillies, chilies, or chili
peppers. Bing Translator managed to translate
the noun correctly, but simply ignored the parti-
cle and only translated the base verb, providing a
much better translation than Google, even though
to grow would have been a more accurate choice
of word.
Der Lehrer f ?uhrt das Vorgehen an einem
Beispiel vor.
Google: The teacher leads the procedure be-
fore an example.
Bing: The teacher introduces the approach
with an example.
128
Google Bing
Sentence (%) Verb (%) Base V. (%) Sentence (%) Verb(%) Base V. (%)
VPC 56 (47.5%) 83 (70.3%) 112 (94.9%) 46 (39.0%) 86 (72.9%) 109 (92.4%)
Finite 24 (40.7%) 35 (59.3%) 57 (96.6%) 23 (39.0%) 45 (76.3%) 57 (96.6%)
Infinitive 32 (54.2%) 48 (81.4%) 55 (93.2%) 23 (39.0%) 41 (69.5%) 52 (88.1%)
Simplex 63 (53.4%) 108 (91.5%) 108 (91.5%) 63 (53.4%) 106 (89.8%) 106 (89.8%)
Finite 28 (47.5%) 51 (86.4%) 51 (86.4%) 32 (54.2%) 54 (91.5%) 54 (91.5%)
Infinitive 35 (59.3%) 57 (96.6%) 57 (96.6%) 31 (52.5%) 52 (88.1%) 52 (88.1%)
Total 119 (50.4%) 191 (80.9%) 220 (93.2%) 109 (46.2%) 192 (81.4%) 215 (91.1%)
Table 3: Separate results for Google Translate and Bing Translator; absolute numbers with percentages in
brackets. Sentence = correctly translated sentences, Verb = correctly translated verbs, Base V. = correctly
translated base verbs, Simplex = sentences containing simplex verbs, VPC = sentences containing VPCs,
Finite = sentences where the target verb is finite, Infinitive = sentences where the target verb is in the
infinitive.
This example shows another too literal transla-
tion of the idiomatic VPC vorf?uhren (to show, to
demonstrate). Google?s translation system trans-
lated the base verb f?uhren as to lead and the sep-
arated particle vor as the preposition before. Bing
managed to translate vorf?uhren to to introduce
which could be correct in a certain context. How-
ever, in other cases this would be an inaccurate
or even incorrect translation, for example if that
teacher demonstrated the approach for the second
time. It might be that Bing drew a connection to
the similar VPC einf?uhren which would be a suit-
able translation for to introduce.
Er macht schon wieder blau.
Google: He?s already blue.
Bing: He is again blue.
In this case, the particle of the VPC blaumachen
(to play truant, to throw a sickie) was translated
as if it were the adjective blau (blue). Since He
makes blue again is not a valid English sentence,
the language model of the translation system prob-
ably took a less probable translation of machen (to
do, to make) and translated it to the third person
singular form of to be.
These results imply that both translation sys-
tems rely too much on translating the base verb
that underlies a VPC, as well as its particle sep-
arately instead of resolving their connection first.
While this would still be a working approach for
compositional constructions such as wegrennen
(to run away), this procedure causes the transla-
tions of idiomatic VPCs like einlegen (to pickle)
to be incorrect.
7 Conclusions
This paper presented an analysis of how VPCs af-
fect translation quality in SMT. We illustrated the
similarities and differences between English and
German VPCs. In order to investigate how these
differences influence the quality of SMT systems,
we collected a set of 59 verb pairs, each consist-
ing of a German VPC and a simplex verb that are
synonyms. Then, we constructed a test suite of
118 sentences in which the simplex verb and VPC
are completely substitutable and analysed the re-
sulting English translations in Google Translate
and Bing Translator. The results showed that es-
pecially separated VPCs can affect the translation
quality of SMT systems and cause different kinds
of mistakes, such as too literal translations of id-
iomatic expressions or the omittance of particles.
The test suite that was created in the process of this
study is made accessible online, thus providing a
valuable resource for future research in this field.
This study focused on the identification and
analysis of issues in translating texts that contain
VPCs. Therefore, practical solutions to tackle
these problems were not in the scope of this
project, but would certainly be an interesting topic
for future work. For instance, the work of Collins
et al. (2005) and Holmqvist et al. (2012) could be
used as a foundation for future research on how to
avoid literal translations of VPCs by doing some
kind of reordering first, to avoid errors caused by
the translations system not being able to identify
the base verb and the particle to be connected.
Furthermore, the sentences used in this work
were rather simple and certainly did not cover all
the possible issues that can be caused by VPCs,
129
since the data was created manually by one per-
son. Therefore, it would be desirable to compile
a more realistic dataset to be able to analyse the
phenomenon of VPCs more thoroughly, as well as
employing additional people to ensure the quality
of both, the dataset and the evaluation.
Moreover, it would be important to see the influ-
ence of other grammatical alternations of VPCs as
well, as we only covered auxiliary infinitive con-
structions and finite forms in this study. Another
interesting aspect to analyse in more detail would
be if some of the errors are specifically related to
only one class of VPCs, e.g., if idiomatic VPCs
perform worse than compositional and aspectual
ones. However, this would again require a revised
dataset, where the proportion of each of the three
verb classes is about the same to ensure compara-
bility. In this study, the proportion of VPCs that
exhibited an at least slightly idiomatic meaning
was higher than for the other two verb classes.
Finally, it would be interesting to see if the
results also apply to other language pairs where
VPCs can be found, as well as to change the trans-
lation direction and investigate if it is an even
greater challenge to translate English VPCs into
German, considering that it is presumably harder
to predict the correct position of verb and particle.
References
Heike Behrens. How difficult are complex verbs? Ev-
idence from German, Dutch and English. Linguis-
tics, 36(4):679?712, 1998.
Niladri Chatterjee and Renu Balyan. Context Res-
olution of Verb Particle Constructions for English
to Hindi Translation. In Helena Hong Gao and
Minghui Dong, editors, PACLIC, pages 140?149.
Digital Enhancement of Cognitive Development,
Waseda University, 2011.
Michael Collins, Philipp Koehn, and Ivona Kucerov.
Clause restructuring for statistical machine transla-
tion. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL?05,
pages 531?540, Stroudsburg, PA, USA, 2005. Asso-
ciation for Computational Linguistics.
Nicole Deh?e. Particle Verbs in English: Syntax, Infor-
mation Structure, and Intonation. John Benjamins
Publishing Co, 2002.
Maria Holmqvist, Sara Stymne, Lars Ahrenberg, and
Magnus Merkel. Alignment-based reordering for
SMT. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Thierry Declerck, Mehmet Uur
Doan, Bente Maegaard, Joseph Mariani, Jan Odijk,
and Stelios Piperidis, editors, Proceedings of the
Eight International Conference on Language Re-
sources and Evaluation, LREC?12, Istanbul, Turkey,
2012. European Language Resources Association
(ELRA).
Su Nam Kim and Timothy Baldwin. How to Pick out
Token Instances of English Verb-Particle Construc-
tions. Language Resources and Evaluation, 44(1-
2):97?113, 2010.
Stefan M?uller. Syntax or morphology: German parti-
cle verbs revisited. In Nicole Deh?e, Ray Jackendoff,
Andrew McIntyre, and Silke Urban, editors, Verb-
Particle Explorations, volume 1 of Interface Explo-
rations, pages 119?139. Mouton de Gruyter, 2002.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. Multiword Expres-
sions: A Pain in the Neck for NLP. In Proceedings
of the Third International Conference on Compu-
tational Linguistics and Intelligent Text Processing,
CICLing?02, pages 1?15, 2002.
Aline Villavicencio. The availability of verb-particle
constructions in lexical resources: How much
is enough? Computer Speech & Language,
19(4):415?432, 2005.
Appendix A. Verb Pairs
antworten - zur?uckschreiben; bedecken - ab-
decken; befestigen - anbringen; beginnen -
anfangen; begutachten - anschauen; beruhi-
gen - abregen; bewilligen - zulassen; bitten -
einladen; demonstrieren - vorf?uhren; dulden -
zulassen; emigrieren - auswandern; entkommen
- weglaufen; entkr?aften - auslaugen; entscheiden
- festlegen; erlauben - zulassen; erschie?en -
abknallen; erw?ahnen - anf?uhren; existieren -
vorkommen; explodieren - hochgehen; fehlen
- fernbleiben; entlassen - rauswerfen; fliehen -
wegrennen; imitieren - nachahmen; immigrieren
- einwandern; inhalieren - einatmen; kapitulieren
- aufgeben; kentern - umkippen; konservieren
- einlegen; kultivieren - anbauen; lehren -
beibringen; ?offnen - aufmachen; produzieren
- herstellen; scheitern - schiefgehen; schlie?en
- ableiten; schw?anzen - blaumachen; sinken -
abnehmen; sinken - untergehen; spendieren -
ausgeben; starten - abheben; sterben - abkratzen;
st?urzen - hinfallen; subtrahieren - abziehen;
tagen - zusammenkommen; testen - ausprobieren;
?uberfahren - umfahren; ?ubergeben - aush?andigen;
?ubermitteln - durchgeben; unterscheiden - au-
seinanderhalten; verfallen - ablaufen; verjagen
- fortjagen; vermelden - mitteilen; verreisen -
wegfahren; verschenken - weggeben; verschieben
130
- aufschieben; verstehen - einsehen; wachsen
- zunehmen; wenden - umdrehen; zerlegen -
auseinandernehmen; z?uchten - anpflanzen.
URL to test suite:
http://stp.lingfil.uu.se/
?
ninas/testsuite.txt
131
Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 67?68,
Gothenburg, Sweden, April 27, 2014.
c?2014 Association for Computational Linguistics
Adventures in Multilingual Parsing
Joakim Nivre
Uppsala university
Department of Linguistics and Philology
Uppsala, Sweden
1 Introduction
The typological diversity of the world?s languages
poses important challenges for the techniques used
in machine translation, syntactic parsing and other
areas of natural language processing. Statisti-
cal models developed and tuned for English do
not necessarily perform well for richly inflected
languages, where larger morphological paradigms
and more flexible word order gives rise to data
sparseness. Since paradigms can easily be cap-
tured in rule-based descriptions, this suggests that
hybrid approaches combining statistical modeling
with linguistic descriptions might be more effec-
tive. However, in order to gain more insight into
the benefits of different techniques from a typolog-
ical perspective, we also need linguistic resources
that are comparable across languages, something
that is currently lacking to a large extent.
In this talk, I will report on two ongoing projects
that tackle these issues in different ways. In the
first part, I will describe techniques for joint mor-
phological and syntactic parsing that combines
statistical dependency parsing and rule-based mor-
phological analysis, specifically targeting the chal-
lenges posed by richly inflected languages. In the
second part, I will present the Universal Depen-
dency Treebank Project, a recent initiative seeking
to create multilingual corpora with morphosyntac-
tic annotation that is consistent across languages.
2 Morphological and Syntactic Parsing
In Bohnet et al. (2013), the goal is to improve pars-
ing accuracy for morphologically rich languages
by performing morphological and syntactic analy-
sis jointly instead of in a pipeline. In this way, we
can ideally make use of syntactic information to
disambiguate morphology, and not just vice versa.
We use a transition-based framework for depen-
dency parsing, and explore different ways of in-
tegrating morphological features into the model.
Furthermore, we investigate the use of rule-based
morphological analyzers to provide hard or soft
constraints in order to tackle the sparsity of lexi-
cal features. Evaluation on five morphologically
rich languages (Czech, Finnish, German, Hungar-
ian, and Russian) shows consistent improvements
in both morphological and syntactic accuracy for
joint prediction over a pipeline model, with further
improvements thanks to the morphological ana-
lyzers. The final results improve the state of the
art in dependency parsing for all languages.
3 Treebanks for Multilingual Parsing
In McDonald et al. (2013), we present a new col-
lection of treebanks with homogeneous syntactic
annotation for six languages: German, English,
Swedish, Spanish, French and Korean. The an-
notation is based on the Google universal part-of-
speech tags (Petrov et al., 2012) and the Stanford
dependencies (de Marneffe et al., 2006), adapted
and harmonized across languages. To show the
usefulness of such a resource, we also present a
case study of cross-lingual transfer parsing with
more reliable evaluation than has been possible be-
fore. The ?universal? treebank is made freely avail-
able in order to facilitate research on multilingual
dependency parsing.
1
A second release including
eleven languages is planned for the spring of 2014.
4 Conclusion
Although both projects reviewed in the talk may
contribute to a better understanding of how natu-
ral language processing techniques are affected by
linguistic diversity, there are still important gaps
that need to be filled. For instance, the universal
treebank annotation still fails to capture most of
the morphological categories used by the parser.
In the final part of the talk, I will try to outline
some of the challenges that lie ahead of us.
1
Downloadable at https://code.google.com/p/uni-dep-tb/.
67
References
Bernd Bohnet, Joakim Nivre, Igor Boguslavsky,
Rich?ard Farkas, Filip Ginter, and Jan Haji?c. 2013.
Joint morphological and syntactic analysis for richly
inflected languages. Transactions of the Association
for Computational Linguistics, 1:415?428.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC).
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuz-
man Ganchev, Keith Hall, Slav Petrov, Hao
Zhang, Oscar T?ackstr?om, Claudia Bedini, N?uria
Bertomeu Castell?o, and Jungmee Lee. 2013. Uni-
versal dependency annotation for multilingual pars-
ing. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 92?97.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
the 8th International Conference on Language Re-
sources and Evaluation (LREC).
68
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 130?140,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
Treebank Translation for Cross-Lingual Parser Induction
J
?
org Tiedemann
Dep. of Linguistics and Philology
Uppsala University
jorg.tiedemann@lingfil.uu.se
?
Zeljko Agi
?
c
Linguistics Department
University of Potsdam
zagic@uni-potsdam.de
Joakim Nivre
Dep. of Linguistics and Philology
Uppsala University
joakim.nivre@lingfil.uu.se
Abstract
Cross-lingual learning has become a popu-
lar approach to facilitate the development
of resources and tools for low-density lan-
guages. Its underlying idea is to make
use of existing tools and annotations in
resource-rich languages to create similar
tools and resources for resource-poor lan-
guages. Typically, this is achieved by either
projecting annotations across parallel cor-
pora, or by transferring models from one or
more source languages to a target language.
In this paper, we explore a third strategy
by using machine translation to create syn-
thetic training data from the original source-
side annotations. Specifically, we apply
this technique to dependency parsing, us-
ing a cross-lingually unified treebank for
adequate evaluation. Our approach draws
on annotation projection but avoids the use
of noisy source-side annotation of an unre-
lated parallel corpus and instead relies on
manual treebank annotation in combination
with statistical machine translation, which
makes it possible to train fully lexicalized
parsers. We show that this approach signif-
icantly outperforms delexicalized transfer
parsing.
1 Introduction
The lack of resources and tools is a serious problem
for the majority of the world?s languages (Bender,
2013). Many applications require robust tools and
the development of language-specific resources is
expensive and time consuming. Furthermore, many
tasks such as data-driven syntactic parsing require
strong supervision to achieve reasonable results
for real-world applications, since the performance
of fully unsupervised methods lags behind by a
large margin in comparison with the state of the
art. Cross-lingual learning has been proposed as
one possible solution to quickly create initial tools
for languages that lack the appropriate resources
(Ganchev and Das, 2013). By and large, there
are two main strategies that have been proposed
in the literature: annotation projection and model
transfer.
1.1 Previous Cross-Lingual Approaches
Annotation projection relies on the mapping of lin-
guistic annotation across languages using paral-
lel corpora and automatic alignment as basic re-
sources (Yarowsky et al., 2001; Hwa et al., 2005;
T?ackstr?om et al., 2013a). Tools that exist for the
source language are used to annotate the source
side of the corpus and projection heuristics are then
applied to map the annotation through word align-
ment onto the corresponding target language text.
Target language tools can then be trained on the
projected annotation assuming that the mapping is
sufficiently correct. Less frequent, but also possi-
ble, is the scenario where the source side of the cor-
pus contains manual annotation (Agi?c et al., 2012).
This addresses the problem created by projecting
noisy annotations, but it presupposes parallel cor-
pora with manual annotation, which are rarely avail-
able, and expensive and time-consuming to pro-
duce.
Model transfer instead relies on universal fea-
tures and model parameters that can be transferred
from one language to another. Abstracting away
from all language-specific parameters makes it pos-
sible to train, e.g., delexicalized parsers that ignore
lexical information. This approach has been used
with success for a variety of languages, drawing
from a harmonized POS tagset (Petrov et al., 2012)
that is used as the main source of information. One
advantage compared to annotation projection is
that no parallel data is required. In addition, train-
ing can be performed on gold standard annotation.
However, model transfer assumes a common fea-
130
ture representation across languages (McDonald et
al., 2013), which can be a strong bottleneck. Sev-
eral extensions have been proposed to make the
approach more robust. First of all, multiple source
languages can be involved to increase the statistical
basis for learning (McDonald et al., 2011; Naseem
et al., 2012), a strategy that can also be used in
the case of annotation projection. Cross-lingual
word clusters can be created to obtain additional
universal features (T?ackstr?om et al., 2012). Tech-
niques for target language adaptation can be used
to improve model transfer with multiple sources
(T?ackstr?om et al., 2013b).
1.2 The Translation Approach
In this paper, we propose a third strategy, based
on automatically translating training data to a new
language in order to create annotated resources di-
rectly from the original source. Recent advances
in statistical machine translation (SMT) combined
with the ever-growing availability of parallel cor-
pora are now making this a realistic alternative. The
relation to annotation projection is obvious as both
involve parallel data with one side being annotated.
However, the use of direct translation brings two
important advantages. First of all, using SMT, we
do not accumulate errors from two sources: the tool
? e.g., tagger or parser ? used to annotate the source
language of a bilingual corpus and the noise com-
ing from alignment and projection. Instead, we use
the gold standard annotation of the source language
which can safely be assumed to be of much higher
quality than any automatic annotation obtained by
using a tool trained on that data. Moreover, using
SMT may help in bypassing domain shift problems,
which are common when applying tools trained
(and evaluated) on one resource to text from an-
other domain. Secondly, we can assume that SMT
will produce output that is much closer to the input
than manual translations in parallel texts usually
are. Even if this may seem like a short-coming
in general, in the case of annotation projection it
should rather be an advantage, because it makes it
more straightforward and less error-prone to trans-
fer annotation from source to target. Furthermore,
the alignment between words and phrases is inher-
ently provided as an output of all common SMT
models. Hence, no additional procedures have to be
performed on top of the translated corpus. Recent
research (Zhao et al., 2009; Durrett et al., 2012)
has attempted to address synthetic data creation
for syntactic parsing via bilingual lexica. We seek
to build on this work by utilizing more advanced
translation techniques.
Further in the paper, we first describe the tools
and resources used in our experiments (?2). We
elaborate on our approach to translating treebanks
(?3) and projecting syntactic annotations (?4) for a
new language. Finally, we provide empirical evalu-
ation of the suggested approach (?5) and observe
a substantial increase in parsing accuracy over the
delexicalized parsing baselines.
2 Resources and Tools
In our experiments, we rely on standard resources
and tools for both dependency parsing and ma-
chine translation without any special enhancements.
Since we are primarily trying to provide a proof
of concept for the use of SMT-derived synthetic
training data in dependency parsing, we believe it
is more important to facilitate reproducibility than
to tweak system components to obtain maximum
accuracy.
We use the Universal Dependency Treebank v1
(McDonald et al., 2013) for annotation projection,
parser training and evaluation. It is a collection
of data sets with consistent syntactic annotation
for six languages: English, French, German, Ko-
rean, Spanish, and Swedish.
1
The annotation is
based on Stanford Typed Dependencies for English
(De Marneffe et al., 2006) but has been adapted
and harmonized to allow adequate annotation of
typologically different languages. This is the first
collection of data sets that allows reliable evalua-
tion of labeled dependency parsing accuracy across
multiple languages (McDonald et al., 2013). We
use the dedicated training and test sets from the
treebank distribution in all our experiments. As ar-
gued in (McDonald et al., 2013), most cross-lingual
dependency parsing experiments up to theirs relied
on heterogeneous treebanks such as the CoNLL
datasets for syntactic dependency parsing (Buch-
holz and Marsi, 2006; Nivre et al., 2007a), mak-
ing it difficult to address challenges like consistent
cross-lingual analysis for downstream applications
and reliable cross-lingual evaluation of syntactic
parsers. More specifically, none of the previous
research could report full labeled parsing accura-
cies, but rather just unlabeled structural accuracies
across different attachment schemes. Following
the line of McDonald et al. (2013) regarding the
1
https://code.google.com/p/uni-dep-tb/
131
emphasized importance of homogenous data and
the assignment of labels, we only report labeled
attachment scores (LAS) in all our experiments.
As it is likely the first reliable cross-lingual pars-
ing evaluation, we also choose their results as the
baseline reference point for comparison with our
experiments.
For dependency parsing, we use MaltParser
(Nivre et al., 2006a)
2
due to its efficiency in both
training and parsing, and we facilitate MaltOpti-
mizer (Ballesteros and Nivre, 2012)
3
to bypass the
tedious task of manual feature selection. Malt-
Parser is a transition-based dependency parser
that has been evaluated on a number of different
languages with competitive results (Nivre et al.,
2006b; Nivre et al., 2007b; Hall et al., 2007) and it
is widely used for benchmarking and application
development. Although more accurate dependency
parsers exist for the task of monolingual supervised
parsing, it is not clear that these differences carry
over to the cross-lingual scenario, where baselines
are lower and more complex models are more likely
to overfit. The use of a transition-based parser also
facilitates comparison with delexicalized transfer
parsing, where transition-based parsers are domi-
nant so far (McDonald et al., 2011; McDonald et
al., 2013). We leave the exploration of additional
parsing approaches for future research.
For machine translation, we select the popular
Moses toolbox (Koehn et al., 2007) and the phrase-
based translation paradigm as our basic frame-
work. Phrase-based SMT has the advantage of
being straightforward and efficient in training and
decoding, while maintaining robustness and relia-
bility for many language pairs. More details about
the setup and the translation procedures are given
in Section 3 below. The most essential ingredient
for translation performance is the parallel corpus
used for training the translation models. For our
experiments we use the freely available and widely
used Europarl corpus v7 (Koehn, 2005).
4
It is com-
monly used for training SMT models and includes
parallel data for all languages represented in the
Universal Treebank except Korean, which we will,
therefore, leave out in our experiments. For tuning
we apply the newstest 2012 data provided by the an-
nual workshop on statistical machine translation.
5
For language modeling, we use a combination of
2
http://www.maltparser.org/
3
http://nil.fdi.ucm.es/maltoptimizer/
4
http://www.statmt.org/europarl/
5
http://www.statmt.org/wmt14
DE EN ES FR SV
DE 94 M 94 M 96 M 81 M
EN 2.0 M 103 M 105 M 89 M
ES 1.9 M 2.0 M 104 M 89 M
FR 1.9 M 2.0 M 2.0 M 91 M
SV 1.8 M 1.9 M 1.8 M 1.9 M
mono 22.9 M 17.1 M 6.3 M 6.3 M 2.3 M
Table 1: Parallel data and monolingual data used
for training the SMT models. Lower-left triangle
= number of sentence pairs; upper-right triangle
= number of tokens (source and target language
together); bottom row = number of sentences in
monolingual corpora.
Europarl and News data provided from the same
source. The statistics of the corpora are given in
Table 1.
3 Translating Treebanks
The main contribution of this paper is the empirical
study of automatic treebank translation for parser
transfer. We compare three different translation
approaches in order to investigate the influence of
several parameters. All of them are based on auto-
matic word alignment and subsequent extraction of
translation equivalents as common in phrase-based
SMT. In particular, word alignment is performed us-
ing GIZA++ (Och and Ney, 2003) and IBM model
4 as the final model for creating the Viterbi word
alignments for all parallel corpora used in our ex-
periments. For the extraction of translation tables,
we use the Moses toolkit with its standard settings
to extract phrase tables with a maximum of seven
tokens per phrase from a symmetrized word align-
ment. Symmetrization is done using the grow-diag-
final-and heuristics (Koehn et al., 2003). We tune
phrase-based SMT models using minimum error
rate training (Och, 2003) and the development data
for each language pair. The language model is a
standard 5-gram model estimated from the mono-
lingual data using modified Kneser-Ney smoothing
without pruning (applying KenLM tools (Heafield
et al., 2013)).
Our first translation approach is based on a very
simple word-by-word translation model. For this,
we select the most reliable translations of single
words from the phrase translation tables extracted
from the parallel corpora as described above. We
restrict the model to tokens with alphabetic char-
acters only using pre-defined Unicode character
132
sets. The selection of translation alternatives is
based on the Dice coefficient, which combines the
two essential conditional translation probabilities
given in the phrase table. The Dice coefficient is in
fact the harmonic mean of these two probabilities
and has successfully been used for the extraction of
translation equivalents before (Smadja et al., 1996):
Dice(s, t) =
2 p(s, t)
p(s) + p(t)
= 2
(
1
p(s|t)
+
1
p(t|s)
)
?1
Other association measures would be possible as
well but Smadja et al. (1996) argue that the Dice
coefficient is more robust with respect to low fre-
quency events than other common metrics such as
pointwise mutual information, which can be a seri-
ous issue with the unsmoothed probability estima-
tions in standard phrase tables. Our first translation
model then applies the final one-to-one correspon-
dences to monotonically translate treebanks word
by word. We refer to it as the LOOKUP approach.
Note that any bilingual dictionary could have been
used to perform the same procedure.
The second translation approach (WORD-BASED
MT) is slightly more elaborate but still restricts
the translation model to one-to-one word mappings.
For this, we extract all single word translation pairs
from the phrase tables and apply the standard beam-
search decoder implemented in Moses to translate
the original treebanks to all target languages. The
motivation for this model is to investigate the im-
pact of reordering and language models while still
keeping the projection of annotated data as simple
as possible. Note that the language model may
influence not only the word order but also the lex-
ical choice as we now allow multiple translation
options in our phrase table.
The final model implements translation based
on the entire phrase table using the standard ap-
proach to PHRASE-BASED SMT. We basically run
the Moses decoder with default settings and the pa-
rameters and models trained on our parallel corpora.
Note that it is important for the annotation trans-
fer to keep track of the alignment between phrases
and words of the input and output sentences. The
Moses decoder provides both, phrase segmentation
and word alignment (if the latter is coded into the
phrase tables). This will be important as we will
see in the annotation projection discussed below.
ORIGINAL
DE EN ES FR SV
14.0 0.00 7.90 13.3 4.20
WORD-BASED MT
DE EN ES FR SV
DE ? 49.1 62.6 52.8 60.4
EN 43.3 ? 27.6 34.8 0.00
ES 54.9 25.1 ? 12.3 18.3
FR 68.2 39.6 32.8 ? 57.8
SV 34.1 5.20 21.6 33.7 ?
PHRASE-BASED MT
DE EN ES FR SV
DE ? 51.5 57.3 58.8 46.8
EN 49.3 ? 50.3 61.7 14.6
ES 65.9 66.7 ? 62.8 49.0
FR 58.0 53.7 44.7 ? 38.2
SV 43.9 43.6 49.6 57.1 ?
Table 2: Non-projectivity in synthetic treebanks.
4 Transferring Annotation
The next step in preparing synthetic training data is
to project the annotation from the original treebank
to the target language. Given the properties of a
dependency tree, where every word has exactly one
syntactic head and dependency label, the annota-
tion transfer is trivial for the two initial translation
models. All annotation can simply be copied us-
ing the dictionary LOOKUP in which we enforce
a monotonic one-to-one word mapping between
source and target language.
In the second approach, we only have to keep
track of reordering, which is reported by the de-
coder when translating with our model. Note that
the mapping is strictly one-to-one (bijective) as
phrase-based SMT does not allow deletions or in-
sertions at any point. This also ensures that we
will always maintain a tree structure even though
reordering may have a strong impact on projectiv-
ity (see Table 2). An illustration of this type of
annotation transfer is shown in the left image of
Figure 1.
The third model, full PHRASE-BASED SMT, re-
quires the most attention when transferring anno-
tation across languages. Here we have to rely on
the alignment information and projection heuris-
tics similar to the ones presented in related work
(Hwa et al., 2005). In their work, Hwa et al. (2005)
define a direct projection algorithm that transfers
automatic annotation to a target language via word
alignment. The algorithm defines a number of
133
CO
NJ
NO
UN
PR
ON
VE
RB
AD
P
NO
UN
.
Qu
e
Die
u
lui
vie
nne
en
aid
e
!
Th
at
Go
d
hel
p
him
com
e
in
!
CO
NJ
NO
UN
NO
UN
PR
ON
VE
RB
AD
P
.
exp
l ns
ub
j io
bj
roo
t ad
pm
od
adp
ob
j
p
exp
lns
ub
j
adp
ob
j
iob
j
roo
tad
pm
od p
CO
NJ
NO
UN
PR
ON
VE
RB
AD
P
NO
UN
.
Qu
e
Die
u
lui
vie
nne
en
aid
e
!
Go
d
DU
MM
Y
hel
p
DU
MM
Y
DU
MM
Y
him
!
NO
UN
CO
NJ
VE
RB
AD
P
NO
UN
PR
ON
.
exp
l n
sub
j io
bj
roo
t
adp
mo
d
adp
ob
j
p
nsu
bj
exp
l
roo
ta
dpm
od
adp
ob
j
iob
j p
CO
NJ
NO
UN
PR
ON
VE
RB
AD
PN
OU
N
.
Qu
e
Die
u
lui
vie
nne
en
aid
e
!
Go
d
hel
p
him
!
NO
UN
VE
RB
PR
ON
.
exp
l ns
ub
j io
bj
roo
t ad
pm
od
adp
ob
j
p
nsu
bj
roo
t
iob
j
p
Figure 1: Transferring annotation from French to an English translation with a WORD-BASED translation
model (left) and with a PHRASE-BASED translation model (middle and right). Annotation projection using
the Direct Projection Algorithm by Hwa et al. (2005) (middle) and our approach (right).
heuristics to handle unaligned, one-to-many, many-
to-one and many-to-many alignments. As a side ef-
fect, this approach produces several dummy-nodes
in the target language to ensure a complete pro-
jection of the source language tree (see Hwa et al.
(2005) for more details).
In our approach, we try to make use of the addi-
tional information provided by the SMT decoder to
avoid dummy-nodes and relations that may nega-
tively influence the induced target language parser.
Compared to the annotation projection approach
of Hwa et al. (2005), the situation in our PHRASE-
BASED SMT setting is slightly different. Here, we
have two types of alignments that can be considered
when relating source and target language items: (i)
the alignment between phrases (pairs of consec-
utive n-grams) and (ii) the phrase-internal word
alignment on which phrase extraction is based. The
primary information used for annotation transfer
is still the latter which has the same properties as
described by Hwa et al. (2005) (except that we have
truly many-to-many alignments in our data which
were not available in their experiments).
Note that words may be unaligned in phrase-
based SMT as the phrase extraction algorithm used
in Moses includes unaligned adjacent tokens. How-
ever, for these unaligned words, we know to which
phrase they belong and can also identify the corre-
sponding phrase in the other language using phrase
alignment information. This makes it possible to
avoid the creation of dummy-nodes altogether and
instead to link unaligned words to existing nodes
based on the given phrase segmentation.
Similarly, we define heuristics for handling one-
to-many, many-to-one and many-to-many align-
ments that avoid the creation of dummy-nodes. The
main procedure is illustrated in Figure 2.
The key feature of this projection algorithm is
that ambiguous alignments are handled by attach-
ing words to the nodes that are highest up in the
dependency tree (the procedure find highest() re-
turns the node with minimum distance to the root
of the tree). This ensures that we avoid cycles
and isolated cliques in the graph. Furthermore,
unaligned words are attached to the head of the
target phrase they belong to, which seems to be the
most appropriate place without further knowledge.
The procedures in trg phrase() and in src phrase()
make use of the phrase segmentation used in the
translation process.
One complication is the search for the corre-
sponding target head word in cases where the
source language head is not aligned or aligned to
multiple target language words. Figure 3 shows
the head alignment procedure that we define in our
projection algorithm. Procedure find aligned() re-
turns the rightmost word of all words aligned to the
given source language word s. Other heuristics or
linguistically motivated rules based on POS tags
and general language properties would be possible
here as well. If s is not aligned, we move up in
the dependency tree until we hit ROOT or find an
aligned word. If we are at the root position we
return ROOT as this does not require further map-
pings. The effect of this algorithm is illustrated by
the right-hand side image in Figure 1.
5 Parsing Across Languages
In this section, we present the results of two ex-
perimental batches. First, we establish the base-
134
Input: source tree S, target sentence T ,
word alignment A, phrase segmentation P
Output: syntactic heads head[],
word attributes attr[]
1 treeSize = max distance to root(S) ;
2 attr = [] ;
3 head = [] ;
4 for t ? T do
5 if is unaligned trg(t,A) then
6 for t? ? in trg phrase(t,P) do
7 [s
x
,..,s
y
] = aligned to(t?) ;
8 ?s = find highest([s
x
,..,s
y
],S) ;
9
?
t = find aligned(?s,S,T,A) ;
10 attr[t] = DUMMY ;
11 head[t] =
?
t ;
12 end
13 else
14 [s
x
,..,s
y
] = aligned to(t) ;
15 s = find highest([s
x
,..,s
y
],S) ;
16 attr[t] = attr(s) ;
17 ?s = head of(s,S) ;
18
?
t = find aligned(?s,S,T,A) ;
19 if
?
t == t then
20 [s
x
,..,s
y
] = in src phrase(s,P) ;
21 s* = find highest([s
x
,..,s
y
],S) ;
22 ?s = head of(s*,S) ;
23
?
t = find aligned(?s,S,T,A) ;
24 head[t] =
?
t ;
25 end
26 end
27 end
Figure 2: Annotation projection algorithm.
lines by comparing monolingual supervised pars-
ing to delexicalized transfer parsing following the
approach of McDonald et al. (2013). Second, we
present the results obtained with parsers trained
on target language treebanks produced using ma-
chine translation and annotation projection. Here,
we also look at delexicalized models trained on
translated treebanks to show the effect of machine
translation without additional lexical features.
5.1 Baseline Results
First we present the baseline parsing scores. The
baselines we explore are: (i) the monolingual base-
line, i.e., training and testing using the same lan-
guage data from the Universal Dependency Tree-
bank and (ii) the delexicalized baseline, i.e., apply-
ing delexicalized parsers across languages.
For the monolingual baseline, MaltParser mod-
els are trained on the original treebanks with uni-
versal POS labels and lexical features but leaving
out other language-specific features if they exist in
the original treebanks. The delexicalized parsers
are trained on universal POS labels only for each
language and are then applied to all other languages
Input: node s, source tree S with root ROOT,
target sentence T , word alignment A
Output: node t*
1 if s == ROOT then
2 return ROOT ;
3 end
4 while is unaligned src(s,A) do
5 s = head of(s,S) ;
6 if s == ROOT then
7 return ROOT ;
8 end
9 end
10 p = 0 ;
11 t* = undef ;
12 for t? ? aligned(s,A) do
13 if position(t?,T) > p then
14 t* = t? ;
15 p = position(t?,T) ;
16 end
17 end
18 return t* ;
Figure 3: Procedure find aligned().
without modification. For all models, features and
options are optimized using MaltOptimizer. The
accuracy is given in Table 3 as a set of labeled at-
tachment scores (LAS). We include punctuation
in our evaluation. Ignoring punctuation generally
leads to slightly higher scores as we have noted in
our experiments but we do not report those num-
bers here. Note also that the columns represent the
target languages (used for testing), while the rows
denote the source languages (used in training), as
in McDonald et al. (2013).
From the table, we can see that the baseline
scores are compatible with the ones in the orig-
inal experiments presented by (McDonald et al.,
2013), included in Table 3 for reference. The dif-
ferences are due to parser selection, as they use a
transition-based parser with beam search and per-
ceptron learning along the lines of Zhang and Nivre
(2011) whereas we rely on greedy transition-based
parsing with linear support vector machines. In the
following, we will compare results to our baseline
as we have a comparable setup in those experi-
ments. However, most improvements shown below
also apply in comparison with (McDonald et al.,
2013).
5.2 Translated Treebanks
Now we turn to the experiments on translated tree-
banks. We consider two setups. First, we look at
the effect of translation when training delexical-
ized parsers. In this way, we can perform a direct
comparison to the baseline performance presented
135
MONOLINGUAL
DE EN ES FR SV
72.13 87.50 78.54 77.51 81.28
DELEXICALIZED
DE EN ES FR SV
DE 62.71 43.20 46.09 46.09 50.64
EN 46.62 77.66 55.65 56.46 57.68
ES 44.03 46.73 68.21 57.91 53.82
FR 43.91 46.75 59.65 67.51 52.01
SV 50.69 49.13 53.62 51.97 70.22
MCDONALD ET AL. (2013)
DE EN ES FR SV
DE 64.84 47.09 48.14 49.59 53.57
EN 48.11 78.54 56.86 58.20 57.04
ES 45.52 47.87 70.29 63.65 53.09
FR 45.96 47.41 62.56 73.37 52.25
SV 52.19 49.71 54.72 54.96 70.90
Table 3: Baselines ? labeled attachment score
(LAS) for monolingual and delexicalized transfer
parsing. Delexicalized transfer parsing results of
McDonald et al. (2013) included for reference.
above. The second setup then considers fully lexi-
calized models trained on translated treebanks. The
main advantage of the translation approach is the
availability of lexical information and this final
setup represents the real power of this approach.
In it, we compare lexicalized parsers trained on
translated treebanks with their delexicalized coun-
terparts and avoid a direct comparison with the
delexicalized baselines as they involve different
types of features.
5.3 Delexicalized Parsers
Table 4 presents the scores obtained by training
delexicalized parsing models on synthetic data cre-
ated by our translation approaches presented earlier.
Feature models and training options are the same
as for the delexicalized source language models
when training and testing on the target language
data. Note that we exclude the simple dictionary
LOOKUP approach here, because this approach
leads to identical models as the basic delexicalized
models. This is because words are translated one-
to-one without any reordering which leads to ex-
actly the same annotation sequences as the source
language treebank after projecting POS labels and
dependency relations.
From the table, we can see that all but one model
improve the scores obtained by delexicalized base-
line models. The improvements are quite substan-
tial up to +6.38 LAS. The boost in performance
WORD-BASED MT
DE EN ES FR SV
DE ? 48.12
(4.92)
50.84
(4.75)
52.92
(6.83)
55.52
(4.88)
EN 49.53
(2.91)
? 57.41
(1.76)
58.53
(2.07)
57.82
(0.14)
ES 45.48
(1.45)
48.46
(1.73)
? 58.29
(0.38)
55.25
(1.43)
FR 46.59
(2.68)
47.88
(1.13)
59.72
(0.07)
? 52.31
(0.30)
SV 52.16
(1.47)
49.14
(0.01)
56.50
(2.88)
56.71
(4.74)
?
PHRASE-BASED MT
DE EN ES FR SV
DE ? 45.43
(2.23)
47.26
(1.17)
49.14
(3.05)
53.37
(2.73)
EN 49.16
(2.54)
? 57.12
(1.47)
58.23
(1.77)
58.23
(0.55)
ES 46.75
(2.72)
46.82
(0.09)
? 58.22
(0.31)
54.14
(0.32)
FR 48.02
(4.11)
49.06
(2.31)
60.23
(0.58)
? 55.24
(3.23)
SV 50.96
(0.27)
46.12
?3.01
55.95
(2.33)
54.71
(2.74)
?
Table 4: Translated treebanks: labeled attachment
score (LAS) for delexicalized parsers trained on
synthetic data created by translation. Numbers in
superscript show the absolute improvement over
our delexicalized baselines.
is especially striking for the simpleWORD-BASED
translation model considering that the only differ-
ence to the baseline model is word order. The
impact of the more complex PHRASE-BASED trans-
lation model is, however, difficult to judge. In
14 out of 20 models it actually leads to a drop in
LAS when applying phrase-based translation in-
stead of single-word translation. This is somewhat
surprising but is probably related to the additional
ambiguity in annotation projection introduced by
many-to-many alignments. The largest drop can be
seen for Swedish translated to English, which even
falls behind the baseline performance when using
the PHRASE-BASED translation model.
5.4 Lexicalized Parsers
The final experiment is concerned with lexical
parsers trained on translated treebanks. The main
objective here is to test the robustness of fully lexi-
calized models trained on noisy synthetic data cre-
ated by simple automatic translation engines. Ta-
ble 5 lists the scores obtained by our models when
trained on treebanks translated with our three ap-
proaches (dictionary LOOKUP, WORD-BASED MT
and full PHRASE-BASED translation). Again, we
use the same feature model and training options as
for the source language model when training mod-
els for the target languages. This time, of course,
this refers to the features used by the lexicalized
baseline models.
The capacity of the parsing models increases due
to the lexical information which is now included.
In order to see the effect of lexicalization, we com-
136
DE
T
DE
T
NO
UN
VE
RB
AD
P
NO
UN
CO
NJ
AD
P
DE
T
NO
UN
AD
J
.
To
us
ses
pr
od
uit
s
so
nt
de
qu
ali
te?
et
d?
un
e
fra
ich
eu
re
xe
mp
lai
res
.
Al
l
his
pr
od
uc
ts
ar
e
hig
h-
qu
ali
ty
an
d
a
co
ld
mu
lle
t
co
pie
s
.
DE
T
DE
T
NO
UN
VE
RB
NO
UN
AD
P
CO
NJ
DE
T
NO
UN
NO
UN
AD
J
.
de
t p
os
s
ns
ub
j
ro
ot
ad
pm
od
ad
po
bj
cc
co
nj
de
t
ad
po
bj
am
od
p
de
tp
os
s
ns
ub
j
ro
ot
ad
po
bj
ad
pm
od
cc
de
t
ad
po
bj a
dp
ob
j
am
od
p
Figure 4: Problematic annotation projection with ambiguous word alignment.
pare the performance now with the corresponding
delexicalized models. Note that the LOOKUP ap-
proach relates to the delexicalized baseline models
without any translation.
As we can see, all models outperform their cor-
responding delexicalized version (with one excep-
tion), which demonstrates the ability of the training
procedure to pick up valuable lexical information
from the noisy translations. Again, we can see
substantial absolute improvements of up to +7.31
LAS showing the effectiveness of the translation
approach. Note that this also means that we outper-
form the delexicalized baselines in all cases by a
large margin, even if we should not directly com-
pare these models as they draw on different fea-
ture sets. Once again, we can also see that the
very simple methods are quite successful. Even the
very basic LOOKUP approach leads to significant
improvements with one minor exception. Surpris-
ingly, no gain can be seen with the PHRASE-BASED
translation approach. The translation quality is cer-
tainly better when manually inspecting the data.
However, the increased complexity of annotation
projection seems to pull down the parsers induced
on that kind of data. A question for future work
is whether the performance of those models can
be improved by better projection algorithms and
heuristics that lead to cleaner annotations of other-
wise better translations of the original treebanks.
One possible reason for this disappointing re-
sult could be the unreliable mapping of POS labels
across many-to-many alignments. Figure 4 illus-
trates a typical case of link ambiguity that leads to
erroneous projections. For example, the mapping
of the label ADP onto the English word quality is
due to the left-to-right procedure applied in our pro-
jection algorithm and the mapping of the NOUN
label to the English adjective cold is due to the
link to fraicheur. How much these errors effect our
parsing models trained on the projected treebanks
is difficult to estimate and further investigations are
required to pinpoint these issues and to find ways
of addressing problems that may occur in various
contexts.
Nevertheless, the overall results are very positive.
The experiments clearly show the potentials of the
translation approach. Note that this paper presents
the first attempt to study the effect of translation on
cross-lingual parser induction. Further optimiza-
tion of the translation process and the connected
annotation projection procedures should lead to
further improvements over our basic models.
6 Conclusions and Future Work
In this paper, we have addressed the problem of
cross-lingual parser induction by using statistical
machine translation to create synthetic training data.
Our SMT approach avoids the noisy source-side
137
LOOKUP
DE EN ES FR SV
DE ? 48.63
(5.43)
52.66
(6.57)
52.06
(5.97)
58.78
(8.14)
EN 48.59
(1.97)
? 57.79
(2.14)
57.80
(1.34)
62.21
(4.53)
ES 47.36
(3.33)
49.13
(2.40)
? 62.24
(4.33)
57.50
(3.68)
FR 47.57
(3.66)
54.06
(7.31)
66.31
(6.66)
? 57.73
(5.72)
SV 51.88
(1.19)
48.84
(0.29)
54.74
(1.12)
52.95
(0.98)
?
WORD-BASED MT
DE EN ES FR SV
DE ? 51.86
(3.74)
55.90
(5.06)
57.77
(4.85)
61.65
(6.13)
EN 53.80
(4.27)
? 60.76
(3.35)
63.32
(4.79)
62.93
(5.11)
ES 49.94
(4.46)
49.93
(1.47)
? 65.60
(7.31)
59.22
(3.97)
FR 52.07
(5.48)
54.44
(6.56)
65.63
(5.91)
? 57.67
(5.36)
SV 53.18
(1.02)
50.91
(1.77)
60.82
(4.32)
59.14
(2.43)
?
PHRASE-BASED MT
DE EN ES FR SV
DE ? 50.89
(5.46)
52.54
(5.28)
54.99
(5.85)
59.46
(6.09)
EN 53.71
(4.55)
? 60.70
(3.58)
62.89
(4.66)
64.01
(5.78)
ES 49.59
(2.84)
48.35
(1.53)
? 64.88
(6.66)
58.99
(4.85)
FR 51.83
(3.81)
53.81
(4.75)
65.55
(5.32)
? 59.01
(3.77)
SV 53.22
(2.26)
49.06
(2.94)
58.41
(2.46)
58.04
(3.33)
?
Table 5: Translated treebanks: labeled attachment score (LAS) for lexicalized parsers trained on synthetic
data. Numbers in superscript show the absolute improvements over the delexicalized models based on the
same translation strategy.
annotations of traditional annotation projection and
makes it possible to train fully lexicalized target lan-
guage models that significantly outperform delexi-
calized transfer parsers. We have also demonstrated
that translation leads to better delexicalized models
that can directly be compared with each other as
they are based on the same feature space.
We have compared three SMT methods for syn-
thesizing training data: LOOKUP-based translation,
WORD-BASED translation and full PHRASE-BASED
translation. Our experiments show that even noisy
data sets and simple translation strategies can be
used to achieve positive results. For all three ap-
proaches, we have recorded substantial improve-
ments over the state of the art in labeled cross-
lingual parsing (McDonald et al., 2013). According
to our results, simple word-by-word translations
are often sufficient to create reasonable translations
to train lexicalized parsers on. More elaborated
phrase-based models together with advanced anno-
tation projection strategies do not necessarily lead
to any improvements.
As future work, we want to improve our model
by (i) studying the impact of other SMT properties
and improve the quality of treebank translation,
(ii) implementing more sophisticated methods for
annotation projection and (iii) using n-best lists
provided by SMT models to introduce additional
synthetic data using a single resource. We also aim
at (iv) applying our approach to transfer parsing
for closely related languages (see Agi?c et al. (2012)
and Zeman and Resnik (2008) for related work),
(v) testing it in a multi-source transfer scenario
(McDonald et al., 2011) and, finally, (vi) comparing
different dependency parsing paradigms within our
experimental framework.
Multi-source approaches are especially appeal-
ing using the translation approach. However, initial
experiments (which we omit in this presentation)
revealed that simple concatenation is not sufficient
to obtain results that improve upon the single-best
translated treebanks. A careful selection of appro-
priate training examples and their weights given
to the training procedure seems to be essential to
benefit from different sources.
7 Acknowledgements
This work was supported by the Swedish Research
Council (Vetenskapsr?adet) through the project on
Discourse-Oriented Machine Translation (2012-
916).
138
References
?
Zeljko Agi?c, Danijela Merkler, and Da?sa Berovi?c.
2012. Slovene-Croatian Treebank Transfer Using
Bilingual Lexicon Improves Croatian Dependency
Parsing. In Proceedings of IS-LTC 2012, pages 5?
9.
Miguel Ballesteros and Joakim Nivre. 2012. MaltOp-
timizer: An Optimization Tool for MaltParser. In
Proceedings of EACL 2012, pages 58?62.
Emily M. Bender. 2013. Linguistic Fundamentals for
Natural Language Processing: 100 Essentials from
Morphology and Syntax. Morgan & Claypool Pub-
lishers.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X Shared Task on Multilingual Dependency Parsing.
In Proceedings of CoNLL 2006, pages 149?164.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of LREC 2006, pages 449?454.
Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syn-
tactic Transfer Using a Bilingual Lexicon. In Pro-
ceedings of EMNLP-CoNLL 2012, pages 1?11.
Kuzman Ganchev and Dipanjan Das. 2013. Cross-
Lingual Discriminative Learning of Sequence Mod-
els with Posterior Regularization. In Proceedings of
EMNLP 2013, pages 1996?2006.
Johan Hall, Jens Nilsson, Joakim Nivre, G?ulsen Eryi?git,
Be?ata Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single Malt or Blended? A Study in Mul-
tilingual Parser Optimization. In Proceedings of
the CoNLL Shared Task Session of EMNLP-CoNLL
2007, pages 933?939.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable Mod-
ified Kneser-Ney Language Model Estimation. In
Proceedings of ACL 2013, pages 690?696.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrap-
ping Parsers via Syntactic Projection across Parallel
Texts. Natural Language Engineering, 11(3):311?
325.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase Based Translation. In Pro-
ceedings of NAACL-HLT 2003, pages 48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Christopher J. Dyer, Ond?rej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Machine
Translation. In Proceedings of ACL 2007, pages
177?180.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
MT Summit 2005, pages 79?86.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-Source Transfer of Delexicalized Dependency
Parsers. In Proceedings of EMNLP 2011, pages 62?
72.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuz-
man Ganchev, Keith Hall, Slav Petrov, Hao
Zhang, Oscar T?ackstr?om, Claudia Bedini, N?uria
Bertomeu Castell?o, and Jungmee Lee. 2013.
Universal Dependency Annotation for Multilingual
Parsing. In Proceedings of ACL 2013, pages 92?97.
Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective Sharing for Multilingual Depen-
dency Parsing. In Proceedings of ACL 2012, pages
629?637.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006a.
MaltParser: A Data-Driven Parser-Generator for De-
pendency Parsing. In Proceedings of LREC 2006,
pages 2216?2219.
Joakim Nivre, Johan Hall, Jens Nilsson, G?ulsen Eryi?git,
and Svetoslav Marinov. 2006b. Labeled Pseudo-
Projective Dependency Parsing with Support Vector
Machines. In Proceedings of CoNLL 2006, pages
221?225.
Joakim Nivre, Johan Hall, Sandra K?ubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007a. The CoNLL 2007 Shared Task on
Dependency Parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
915?932.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?uls?en Eryi?git, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007b. MaltParser: A
Language-Independent System for Data-Driven De-
pendency Parsing. Natural Language Engineering,
13(2):95?135.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?52.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of ACL 2003, pages 160?167.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A Universal Part-of-Speech Tagset. In Proceedings
of LREC 2012, pages 2089?2096.
Frank Smadja, Vasileios Hatzivassiloglou, and Kath-
leen R. McKeown. 1996. Translating Colloca-
tions for Bilingual Lexicons: A Statistical Approach.
Computational Linguistics, 22(1):1?38.
139
Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual Word Clusters for Direct
Transfer of Linguistic Structure. In Proceedings of
NAACL 2012, pages 477?487.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013a. Token and
Type Constraints for Cross-lingual Part-of-speech
Tagging. Transactions of the Association for Com-
putational Linguistics, 1:1?12.
Oscar T?ackstr?om, Ryan McDonald, and Joakim Nivre.
2013b. Target Language Adaptation of Discrimi-
native Transfer Parsers. In Proceedings of NAACL
2013, pages 1061?1071.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing Multilingual Text Analysis
Tools via Robust Projection Across Aligned Corpora.
In Proceedings of HLT 2011, pages 1?8.
Daniel Zeman and Philip Resnik. 2008. Cross-
Language Parser Adaptation between Related Lan-
guages. In Proceedings of IJCNLP 2008, pages 35?
42.
Yue Zhang and Joakim Nivre. 2011. Transition-based
Dependency Parsing with Rich Non-local Features.
In Proceedings of ACL 2011, pages 188?193.
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
2009. Cross Language Dependency Parsing Using a
Bilingual Lexicon. In Proceedings of ACL-IJCNLP
2009, pages 55?63.
140
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 122?129,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
Anaphora Models and Reordering for Phrase-Based SMT
Christian Hardmeier Sara Stymne J
?
org Tiedemann Aaron Smith Joakim Nivre
Uppsala University
Department of Linguistics and Philology
firstname.lastname@lingfil.uu.se
Abstract
We describe the Uppsala University sys-
tems for WMT14. We look at the integra-
tion of a model for translating pronomi-
nal anaphora and a syntactic dependency
projection model for English?French. Fur-
thermore, we investigate post-ordering and
tunable POS distortion models for English?
German.
1 Introduction
In this paper we describe the Uppsala University
systems for WMT14. We present three different
systems. Two of them are based on the document-
level decoder Docent (Hardmeier et al., 2012; Hard-
meier et al., 2013a). In our English?French sys-
tem we extend Docent to handle pronoun anaphora,
and in our English?German system we add part-
of-speech phrase-distortion models to Docent. For
German?English we also have a system based on
Moses (Koehn et al., 2007). Again the focus is
on word order, this time by using pre- and post-
reordering.
2 Document-Level Decoding
Traditional SMT decoders translate texts as bags
of sentences, assuming independence between sen-
tences. This assumption allows efficient algorithms
for exploring a large search space based on dy-
namic programming (Och et al., 2001). Because of
the dynamic programming assumptions it is hard to
directly include discourse-level and long-distance
features into a traditional SMT decoder.
In contrast to this very popular stack decoding
approach, our decoder Docent (Hardmeier et al.,
2012; Hardmeier et al., 2013a) implements a search
procedure based on local search. At any stage of
the search process, its search state consists of a
complete document translation, making it easy for
feature models to access the complete document
with its current translation at any point in time. The
search algorithm is a stochastic variant of standard
hill climbing. At each step, it generates a successor
of the current search state by randomly applying
one of a set of state changing operations to a ran-
dom location in the document, and accepts the new
state if it has a better score than the previous state.
The operations are to change the translation of a
phrase, to change the word order by swapping the
positions of two phrases or moving a sequence of
phrases, and to resegment phrases. The initial state
can either be initialized randomly, or be based on
an initial run from Moses. This setup is not limited
by dynamic programming constraints, and enables
the use of the full translated target document to
extract features.
3 English?French
Our English?French system is a phrase-based SMT
system with a combination of two decoders, Moses
(Koehn et al., 2007) and Docent (Hardmeier et al.,
2013a). The fundamental setup is loosely based
on the system submitted by Cho et al. (2013) to
the WMT 2013 shared task. Our phrase table is
trained on data taken from the News commentary,
Europarl, UN, Common crawl and 10
9
corpora.
The first three of these corpora were included in-
tegrally into the training set after filtering out sen-
tences of more than 80 words. The Common crawl
and 10
9
data sets were run through an additional
filtering step with an SVM classifier, closely fol-
lowing Mediani et al. (2011). The system includes
three language models, a regular 6-gram model
with modified Kneser-Ney smoothing (Chen and
Goodman, 1998) trained with KenLM (Heafield,
2011), a 4-gram bilingual language model (Niehues
et al., 2011) with Kneser-Ney smoothing trained
with KenLM and a 9-gram model over Brown clus-
ters (Brown et al., 1992) with Witten-Bell smooth-
ing (Witten and Bell, 1991) trained with SRILM
(Stolcke, 2002).
122
The latest version released in March is equipped with . . . It is sold at . . .
La derni`ere version lanc?ee en mars est dot?ee de . . . ? est vendue . . .
Figure 1: Pronominal Anaphora Model
Our baseline system achieved a cased BLEU
score of 33.2 points on the newstest2014 data set.
Since the anaphora model used in our submission
suffered from a serious bug, we do not discuss the
results of the primary submission in more detail.
3.1 Pronominal Anaphora Model
Our pronominal anaphora model is an adaptation
of the pronoun prediction model described by Hard-
meier et al. (2013b) to SMT. The model consists
of a neural network that discriminatively predicts
the translation of a source language pronoun from
a short list of possible target language pronouns us-
ing features from the context of the source language
pronouns and from the translations of possibly re-
mote antecedents. The objective of this model is to
handle situations like the one depicted in Figure 1,
where the correct choice of a target-language pro-
noun is subject to morphosyntactic agreement with
its antecedent. This problem consists of several
steps. To score a pronoun, the system must decide
if a pronoun is anaphoric and, if so, find potential
antecedents. Then, it can predict what pronouns
are likely to occur in the translation. Our pronoun
prediction model is trained on both tasks jointly,
including anaphora resolution as a set of latent vari-
ables. At test time, we split the network in two
parts. The anaphora resolution part is run sepa-
rately as a preprocessing step, whereas the pronoun
prediction part is integrated into the document-level
decoder with two additional feature models.
The features correspond to two copies of the neu-
ral network, one to handle the singular pronoun it
and one to handle the plural pronoun they. Each net-
work just predicts a binary distinction between two
cases, il and elle for the singular network and ils
and elles for the plural network. Unlike Hardmeier
et al. (2013b), we do not use an OTHER category to
capture cases that should not be translated with any
of these options. Instead, we treat all other cases in
the phrase table and activate the anaphora models
only if one of their target pronouns actually occurs
in the output.
To achieve this, we generate pronouns in two
steps. In the phrase table training corpus, we re-
place all pronouns that should be handled by the
classifier, i.e. instances of il and elle aligned to it
and instances of ils and elles aligned to they, with
special placeholders. At decoding time, if a place-
holder is encountered in a target language phrase,
the applicable pronouns are generated with equal
translation model probability, and the anaphora
model adds a score to discriminate between them.
To reduce the influence of the language model
on pronoun choice and give full control to the
anaphora model, our primary language model is
trained on text containing placeholders instead of
pronouns. Since all output pronouns can also be
generated without the interaction of the anaphora
model if they are not aligned to a source language
pronoun, we must make sure that the language
model sees training data for both placeholders and
actual pronouns. However, for the monolingual
training corpora we have no word alignments to
decide whether or not to replace a pronoun by a
placeholder. To get around this problem, we train a
6-gram placeholder language model on the target
language side of the Europarl and News commen-
tary corpora. Then, we use the Viterbi n-gram
model decoder of SRILM (Stolcke, 2002) to map
pronouns in the entire language model training set
to placeholders where appropriate. No substitu-
tions are made in the bilingual language model or
the Brown cluster language model.
3.2 Dependency Projection Model
Our English?French system also includes a depen-
dency projection model, which uses source-side
dependency structure to model target-side relations
between words. This model assigns a score to each
dependency arc in the source language by consider-
ing the target words aligned to the head and the de-
pendent. In Figure 2, for instance, there is an nsub-
jpass arc connecting dominated to production. The
head is aligned to the target word domin?ee, while
the dependent is aligned to the set {production,de}.
The score is computed by a neural network taking
as features the head and dependent words and their
part-of-speech tags in the source language, the tar-
get word sets aligned to the head and dependent,
the label of the dependency arc, the distance be-
tween the head and dependent word in the source
language as well as the shortest distance between
any pair of words in the aligned sets. The network
is a binary classifier trained to discriminate positive
examples extracted from human-made reference
123
Domestic meat production is dominated by chicken .
amod
nn
nsubjpass
auxpass prep pobj
punct
La production int?erieure de viande est domin?ee par le poulet .
Figure 2: Dependency projection model
translations from negative examples extracted from
n-best lists generated by a baseline SMT system.
4 English?German
For English?German we have two systems, one
based on Moses, and one based on Docent. In both
cases we have focused on word order, particularly
for verbs and particles.
Both our systems are trained on the same data
made available by WMT. The Common crawl data
was filtered using the method of Stymne et al.
(2013). We use factored models with POS tags
as a second output factor for German. The possi-
bility to use language models for different factors
has been added to our Docent decoder. Language
models include an in-domain news language model,
an out-of-domain model trained on the target side
of the parallel training data and a POS language
model trained on tagged news data. The LMs are
trained in the same way as for English?French.
All systems are tuned using MERT (Och, 2003).
Phrase-tables are filtered using entropy-based prun-
ing (Johnson et al., 2007) as implemented in Moses.
All BLEU scores are given for uncased data.
4.1 Pre-Ordered Alignment and
Post-Ordered Translation
The use of syntactic reordering as a separate pre-
processing step has already a long tradition in sta-
tistical MT. Handcrafted rules (Collins et al., 2005;
Popovi?c and Ney, 2006) or data-driven models (Xia
and McCord, 2004; Genzel, 2010; Rottmann and
Vogel, 2007; Niehues and Kolss, 2009) for pre-
ordering training data and system input have been
explored in numerous publications. For certain
language pairs, such as German and English, this
method can be very effective and often improves
the quality of standard SMT systems significantly.
Typically, the source language is reordered to better
match the syntax of the target language when trans-
lating between languages that exhibit consistent
word order differences, which are difficult to handle
by SMT systems with limited reordering capabil-
ities such as phrase-based models. Preordering is
often done on the entire training data as well to op-
timize translation models for the pre-ordered input.
Less common is the idea of post-ordering, which
refers to a separate step after translating source lan-
guage input to an intermediate target language with
corrupted (source-language like) word order (Na et
al., 2009; Sudoh et al., 2011).
In our experiments, we focus on the translation
from English to German. Post-ordering becomes
attractive for several reasons: One reason is the
common split of verb-particle constructions that
can lead to long distance dependencies in German
clauses. Phrase-based systems and n-gram lan-
guage models are not able to handle such relations
beyond a certain distance and it is desirable to keep
them as connected units in the phrase translation
tables. Another reason is the possible distance of
finite and infinitival verbs in German verb phrases
that can lead to the same problems described above
with verb-particle constructions. The auxiliary or
modal verb is placed at the second position but
the main verb appears at the end of the associated
verb phrase. The distances can be arbitrarily long
and long-range dependencies are quite frequent.
Similarly, negation particles and adverbials move
away from the inflected verb forms in certain con-
structions. For more details on specific phenomena
in German, we refer to (Collins et al., 2005; Go-
jun and Fraser, 2012). Pre-ordering, i.e. moving
English words into German word order does not
seem to be a good option as we loose the con-
nection between related items when moving par-
ticles and main verbs away from their associated
elements. Hence, we are interested in reordering
the target language German into English word or-
der which can be beneficial in two ways: (i) Re-
ordering the German part of the parallel training
data makes it possible to improve word alignment
(which tends to prefer monotonic mappings) and
subsequent phrase extraction which leads to better
translation models. (ii) We can explore a two-step
procedure in which we train a phrase-based SMT
model for translating English into German with
English word order first (which covers many long-
distance relations locally) and then apply a second
system that moves words into place according to
correct German syntax (which may involve long-
range distortion).
For simplicity, we base our experiments on hand-
124
crafted rules for some of the special cases discussed
above. For efficiency reasons, we define our rules
over POS tag patterns rather than on full syntac-
tic parse trees. We rely on TreeTagger and apply
rules to join verbs in discontinuous verb phrases
and to move verb-finals in subordinate clauses, to
move verb particles, adverbials and negation par-
ticles. Table 1 shows two examples of reordered
sentences together with the original sentences in
English and German. Our rules implement rough
heuristics to identify clause boundaries and word
positions. We do not properly evaluate these rules
but focus on the down-stream evaluation of the MT
system instead.
It is therefore dangerous to extrapolate from short-term trends.
Daher ist es gef?ahrlich, aus kurzfristigen Trends Prognosen abzuleiten.
Daher ist gef?ahrlich es, abzuleiten aus kurzfristigen Trends Prognosen.
The fall of Saddam ushers in the right circumstances.
Der Sturz von Saddam leitet solche richtigen Umst?ande ein.
Der Sturz von Saddam ein leitet solche richtigen Umst?ande.
Table 1: Two examples of pre-ordering outputs.
The first two lines are the original English and
German sentences and the third line shows the re-
ordered sentence.
We use three systems based on Moses to com-
pare the effect of reordering on alignment and trans-
lation. All systems are case-sensitive phrase-based
systems with lexicalized reordering trained on data
provided by WMT. Word alignment is performed
using fast align (Dyer et al., 2013). For tuning we
use newstest2011. Additionally, we also test paral-
lel data from OPUS (Tiedemann, 2012) filtered by
a method adopted from Mediani et al. (2011).
To contrast our baseline system, we trained a
phrase-based model on parallel data that has been
aligned on data pre-ordered using the reordering
rules for German, which has been restored to the
original word order after word alignment and be-
fore phrase extraction (similar to (Carpuat et al.,
2010; Stymne et al., 2010)). We expect that the
word alignment is improved by reducing crossings
and long-distance links. However, the translation
model as such has the same limitations as the base-
line system in terms of long-range distortions. The
final system is a two-step model in which we apply
translation and language models trained on pre-
ordered target language data to perform the first
step, which also includes a reordered POS language
model. The second step is also treated as a transla-
tion problem as in Sudoh et al. (2011), and in our
case we use a phrase-based model here with lexical-
ized reordering and a rather large distortion limit
of 12 words. Another possibility would be to apply
another rule set that reverts the misplaced words
to the grammatically correct positions. This, how-
ever, would require deeper syntactic information
about the target language to, for example, distin-
guish main from subordinate clauses. Instead, our
model is trained on parallel target language data
with the pre-ordered version as input and the orig-
inal version as output language. For this model,
both sides are tagged and a POS language model
is used again as one of the target language factors
in decoding. Table 2 shows the results in terms of
BLEU scores on the newstest sets from 2013 and
2014.
newstest2013 newstest2014
baseline 19.3 19.1
pre 19.4 19.3
post 18.6 18.7
baseline+OPUS 19.5 19.3
pre+OPUS 19.5 19.3
post+OPUS 19.7 18.8
Table 2: BLEU4 scores for English-German sys-
tems (w/o OPUS): Standard phrase-based (base-
line); phrase-based with pre-ordered parallel cor-
pus used for word alignment (pre); two-step phrase-
based with post-reordering (post)
The results show that pre-ordering has some ef-
fect on word alignment quality in terms of support-
ing better phrase extractions in subsequent steps.
Our experiments show a consistent but small im-
provement for models trained on data that have
been prepared in this way. In contrast, the two-step
procedure is more difficult to judge in terms of au-
tomatic metrics. On the 2013 newstest data we can
see another small improvement in the setup that
includes OPUS data but in most cases the BLEU
scores go down, even below the baseline. The
short-comings of the two-step procedure are ob-
vious. Separating translation and reordering in a
pipeline adds the risk of error propagation. Fur-
thermore, reducing the second step to single-best
translations is a strong limitation and using phrase-
based models for the final reordering procedure is
probably not the wisest decision. However, manual
inspections reveals that many interesting phenom-
ena can be handled even with this simplistic setup.
Table 3 illustrates this with a few selected out-
comes of our three systems. They show how verb-
particle constructions with long-range distortion
125
reference Schauspieler Orlando Bloom hat sich zur Trennung von seiner Frau , Topmodel Miranda Kerr , ge?au?ert .
baseline Schauspieler Orlando Bloom hat die Trennung von seiner Frau , Supermodel Miranda Kerr .
pre-ordering Schauspieler Orlando Bloom hat angek?undigt , die Trennung von seiner Frau , Supermodel Miranda Kerr .
post-ordering Schauspieler Orlando Bloom hat seine Trennung von seiner Frau angek?undigt , Supermodel Miranda Kerr .
reference Er gab bei einer fr?uheren Befragung den Kokainbesitz zu .
baseline Er gab den Besitz von Kokain in einer fr?uheren Anh?orung .
pre-ordering Er r?aumte den Besitz von Kokain in einer fr?uheren Anh?orung .
post-ordering Er r?aumte den Besitz von Kokain in einer fr?uheren Anh?orung ein .
reference Borussia Dortmund k?undigte daraufhin harte Konsequenzen an .
baseline Borussia Dortmund k?undigte an , es werde schwere Folgen .
pre-ordering Borussia Dortmund hat angek?undigt , dass es schwerwiegende Konsequenzen .
post-ordering Borussia Dortmund k?undigte an , dass es schwere Folgen geben werde .
Table 3: Selected translation examples from the newstest 2014 data; the human reference translation; the
baseline system, pre-ordering for word alignment and two-step translation with post-ordering.
such as ?r?aumte ... ein? can be created and how
discontinuous verb phrases can be handled (?hat ...
angek?undigt?) with the two-step procedure. The
model is also often better in producing verb finals
in subordinate clauses (see the final example with
?geben werde?). Note that many of these improve-
ments do not get any credit by metrics like BLEU.
For example the acceptable expression ?r?aumte ein?
which is synonymous to ?gab zu? obtains less credit
then the incomplete baseline translation. Interest-
ing is also to see the effect of pre-ordering when
used for alignment only in the second system. The
first example in Table 3, for example, includes a
correct main verb which is omitted in the baseline
translation, probably because it is not extracted as
a valid translation option.
4.2 Part-of-Speech Phrase-Distortion Models
Traditional SMT distortion models consist of two
parts. A distance-based distortion cost is based
on the position of the last word in a phrase, com-
pared to the first word in the next phrase, given the
source phrase order. A hard distortion limit blocks
translations where the distortion is too large. The
distortion limit serves to decrease the complexity
of the decoder, thus increasing its speed.
In the Docent decoder, the distortion limit is not
implemented as a hard limit, but as a feature, which
could be seen as a soft constraint. We showed in
previous work (Stymne et al., 2013) that it was
useful to relax the hard distortion limit by either
using a soft constraint, which could be tuned, or
removing the limit completely. In that work we
still used the standard parametrization of distortion,
based on the positions of the first and last words in
phrases.
Our Docent decoder, however, always provides
us with a full target translation that is step-wise im-
proved, which means that we can apply distortion
measures on the phrase-level without resorting to
heuristics, which, for instance, are needed in the
case of the lexicalized reordering models in Moses
(Koehn et al., 2005). Because of this it is possible
to use phrase-based distortion, where we calculate
distortion based on the order of phrases, not on the
order of some words. It is possible to parametrize
phrase-distortion in different ways. In this work we
use the phrase-distortion distance and a soft limit
on the distortion distance, to mimic the word-based
distortion. In our experiments we always set the
soft limit to a distance of four phrases. In addition
we use a measure based on how many crossings
a phrase order gives rise to. We thus have three
phrase-distortion features.
As captured by lexicalized reordering models,
different phrases have different tendencies to move.
To capture this to some extent, we also decided
to add part-of-speech (POS) classes to our mod-
els. POS has previously successfully been used
in pre-reordering approaches (Popovi?c and Ney,
2006; Niehues and Kolss, 2009). The word types
that are most likely to move long distances in
English?German translation are verbs and parti-
cles. Based on this observation we split phrases
into two classes, phrases that only contains verbs
and particles, and all other phrases. For these two
groups we use separate phrase-distortion features,
thus having a total of six part-of-speech phrase-
distortion features. All of these features are soft,
and are optimized during tuning.
In our system we initialize Docent by running
Moses with a standard distortion model and lexi-
calized reordering, and then continuing the search
with Docent including our part-of-speech phrase-
distortion features. Tuning was done separately for
the two components, first for the Moses component,
and then for the Docent component initialized by
126
reference Laut Dmitrij Kislow von der Organisation ?Pravo na oryzhie? kann man eine Pistole vom Typ Makarow f?ur 100 bis 300 Dollar kaufen.
baseline Laut Dmitry Kislov aus der Rechten zu Waffen, eine Makarov Gun-spiele erworben werden k?onnen f?ur 100-300 Dollar.
POS+phrase Laut Dmitry Kislov von die Rechte an Waffen, eine Pistole Makarov f?ur 100-300 Dollar erworben werden k?onnen.
reference Die Waffen gelangen ?uber mehrere Kan?ale auf den Schwarzmarkt.
baseline Der ?Schwarze? Markt der Waffen ist wieder aufgef ?ullt ?uber mehrere Kan?ale.
POS+phrase Der ?Schwarze? Markt der Waffen durch mehrere Kan?ale wieder aufgef ?ullt ist.
reference Mehr Kameras k?onnten m?oglicherweise das Problem l?osen...
baseline M?oglicherweise k?onnte das Problem l?osen, eine gro?e Anzahl von Kameras...
POS+phrase M?oglicherweise, eine gro?e Anzahl von Kameras k?onnte das Problem l?osen...
Table 4: Selected translation examples from the newstest2013 data; the human reference translation; the
baseline system (Moses with lexicalized reordering) and the system with a POS+phrase distortion model.
Moses with lexicalized reordering with its tuned
weights. We used newstest2009 for tuning. The
training data was lowercased for training and de-
coding, and recasing was performed using a sec-
ond Moses run trained on News data. As baselines
we present two Moses systems, without and with
lexicalized reordering, in addition to standard dis-
tortion features.
Table 5 shows results with our different distor-
tion models. Overall the differences are quite small.
The clearest difference is between the two Moses
baselines, where the lexicalized reordering model
leads to an improvement. With Docent, both the
word distortion and phrase distortion without POS
do not help to improve on Moses, with a small de-
crease in scores on one dataset. This is not very
surprising, since lexical distortion is currently not
supported by Docent, and the distortion models are
thus weaker than the ones implemented in Moses.
For our POS phrase distortion, however, we see a
small improvement compared to Moses, despite the
lack of lexicalized distortion. This shows that this
distortion model is actually useful, and can even
successfully replace lexicalized reordering. In fu-
ture work, we plan to combine this method with a
lexicalized reordering model, to see if the two mod-
els have complementary strengths. Our submitted
system uses the POS phrase-distortion model.
System Distortion newstest2013 newstest2014
Moses word 19.4 19.3
Moses word+LexReo 19.6 19.6
Docent word 19.5 19.6
Docent phrase 19.5 19.6
Docent POS+phrase 19.7 19.7
Table 5: BLEU4 scores for English?German sys-
tems with different distortion models.
If we inspect the translations, most of the differ-
ences between the Moses baseline and the system
with POS+phrase distortion are actually due to lex-
ical choice. Table 4 shows some examples where
there are word order differences. The result is quite
mixed with respect to the placement of verbs. In
the first example, both systems put the verbs to-
gether but in different positions, instead of splitting
them like the reference suggests. In the second
example, our system erroneously put the verbs at
the end, which would be fine if the sentence had
been a subordinate clause. In the third example,
the baseline system has the correct placement of
the auxiliary ?k?onnte?, while our system is bet-
ter at placing the main verb ?l?osen?. In general,
this indicates that our system is able to support
long-distance distortion as it is needed in certain
cases but sometimes overuses this flexibility. A
better model would certainly need to incorporate
syntactic information to distinguish main from sub-
ordinate clauses. However, this would add a lot of
complexity to the model.
5 Conclusion
We have described the three Uppsala University
systems for WMT14. In the English?French sys-
tem we extend our document-level decoder Do-
cent (Hardmeier et al., 2013a) to handle pronoun
anaphora and introduced a dependency projection
model. In our two English?German system we
explore different methods for handling reordering,
based on Docent and Moses. In particular, we look
at post-ordering as a separate step and tunable POS
phrase distortion.
Acknowledgements
This work forms part of the Swedish strategic re-
search programme eSSENCE. We also acknowl-
edge the use of the Abel cluster, owned by the
University of Oslo and the Norwegian metacenter
for High Performance Computing (NOTUR) and
operated by the Department for Research Comput-
ing at USIT, under project nn9106k. Finally, we
would also like to thank Eva Pettersson, Ali Basirat,
and Eva Martinez for help with human evaluation.
127
References
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational linguistics, 18(4):467?479.
Marine Carpuat, Yuval Marton, and Nizar Habash.
2010. Improving Arabic-to-English statistical ma-
chine translation by reordering post-verbal subjects
for alignment. In Proceedings of the ACL 2010 Con-
ference Short Papers, pages 178?183, Uppsala, Swe-
den.
Stanley F. Chen and Joshua Goodman. 1998. An
empirical study of smoothing techniques for lan-
guage modeling. Technical report, Computer Sci-
ence Group, Harvard University, Cambridge, Mas-
sachusetts, USA.
Eunah Cho, Thanh-Le Ha, Mohammed Mediani, Jan
Niehues, Teresa Herrmann, Isabel Slawik, and Alex
Waibel. 2013. The Karlsruhe Institute of Technol-
ogy translation systems for the WMT 2013. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 104?108, Sofia, Bulgaria.
Association for Computational Linguistics.
Michael Collins, Philipp Koehn, and Ivona Ku?cerov?a.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing of the ACL, pages 531?540, Ann Arbor, Michi-
gan, USA.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM model 2. In Proceedings of the 2013
Conference of the NAACL: Human Language Tech-
nologies, pages 644?648, Atlanta, Georgia, USA.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine transla-
tion. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, pages 376?
384, Beijing, China.
Anita Gojun and Alexander Fraser. 2012. Determin-
ing the placement of German verbs in English?to?
German SMT. In Proceedings of the 13th Confer-
ence of the EACL, pages 726?735, Avignon, France.
Christian Hardmeier, Joakim Nivre, and J?org Tiede-
mann. 2012. Document-wide decoding for phrase-
based statistical machine translation. In Proceed-
ings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1179?
1190, Jeju Island, Korea.
Christian Hardmeier, Sara Stymne, J?org Tiedemann,
and Joakim Nivre. 2013a. Docent: A document-
level decoder for phrase-based statistical machine
translation. In Proceedings of the 51st Annual Meet-
ing of the ACL, Demonstration session, pages 193?
198, Sofia, Bulgaria.
Christian Hardmeier, J?org Tiedemann, and Joakim
Nivre. 2013b. Latent anaphora resolution for cross-
lingual pronoun prediction. In Proceedings of the
2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 380?391, Seattle,
Washington, USA. Association for Computational
Linguistics.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187?197, Edinburgh, Scotland. Association
for Computational Linguistics.
Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving translation qual-
ity by discarding most of the phrasetable. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 967?
975, Prague, Czech Republic.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system descrip-
tion for the 2005 IWSLT speech translation evalu-
ation. In Proceedings of the International Workshop
on Spoken Language Translation, Pittsburgh, Penn-
sylvania, USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL,
Demo and Poster Sessions, pages 177?180, Prague,
Czech Republic.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The
KIT English?French translation systems for IWSLT
2011. In Proceedings of the International Workshop
on Spoken Language Translation, pages 73?78, San
Francisco, California, USA.
Hwidong Na, Jin-Ji Li, Jungi Kim, and Jong-Hyeok
Lee. 2009. Improving fluency by reordering tar-
get constituents using MST parser in English-to-
Japanese phrase-based SMT. In Proceedings of
MT Summit XII, pages 276?283, Ottawa, Ontario,
Canada.
Jan Niehues and Muntsin Kolss. 2009. A POS-based
model for long-range reorderings in SMT. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, pages 206?214, Athens, Greece.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider context by using bilin-
gual language models in machine translation. In
Proceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 198?206, Edinburgh, Scot-
land. Association for Computational Linguistics.
128
Franz Josef Och, Nicola Ueffing, and Hermann Ney.
2001. An efficient A* search algorithm for Statisti-
cal Machine Translation. In Proceedings of the ACL
2001 Workshop on Data-Driven Machine Transla-
tion, pages 55?62, Toulouse, France.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
42nd Annual Meeting of the ACL, pages 160?167,
Sapporo, Japan.
Maja Popovi?c and Hermann Ney. 2006. POS-based re-
orderings for statistical machine translation. In Pro-
ceedings of the 5th International Conference on Lan-
guage Resources and Evaluation (LREC?06), pages
1278?1283, Genoa, Italy.
Kay Rottmann and Stephan Vogel. 2007. Word re-
ordering in statistical machine translation with a
POS-based distortion model. In Proceedings of
the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation,
pages 171?180, Sk?ovde, Sweden.
Andreas Stolcke. 2002. SRILM ? an extensible
language modeling toolkit. In Proceedings of the
Seventh International Conference on Spoken Lan-
guage Processing, pages 901?904, Denver, Col-
orado, USA.
Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.
2010. Vs and OOVs: Two problems for translation
between German and English. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 183?188, Uppsala,
Sweden.
Sara Stymne, Christian Hardmeier, J?org Tiedemann,
and Joakim Nivre. 2013. Tunable distortion limits
and corpus cleaning for SMT. In Proceedings of the
Eighth Workshop on Statistical Machine Translation,
pages 225?231, Sofia, Bulgaria.
Katsuhito Sudoh, Xianchao Wu, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Post-ordering
in statistical machine translation. In Proceedings of
MT Summit XIII, pages 316?323, Xiamen. China.
J?org Tiedemann. 2012. Parallel data, tools and in-
terfaces in OPUS. In Proceedings of the 8th In-
ternational Conference on Language Resources and
Evaluation (LREC?12), pages 2214?2218, Istanbul,
Turkey.
Ian H. Witten and Timothy C. Bell. 1991. The zero-
frequency problem: Estimating the probabilities of
novel events in adaptive text compression. IEEE
Transactions on Information Theory, 37(4):1085?
1094.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proceedings of the 20th Inter-
national Conference on Computational Linguistics,
pages 508?514, Geneva, Switzerland.
129
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 275?286,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Estimating Word Alignment Quality for SMT Reordering Tasks
Sara Stymne J
?
org Tiedemann Joakim Nivre
Uppsala University
Department of Linguistics and Philology
firstname.lastname@lingfil.uu.se
Abstract
Previous studies of the effect of word
alignment on translation quality in SMT
generally explore link level metrics only
and mostly do not show any clear connec-
tions between alignment and SMT qual-
ity. In this paper, we specifically inves-
tigate the impact of word alignment on
two pre-reordering tasks in translation, us-
ing a wider range of quality indicators
than previously done. Experiments on
German?English translation show that re-
ordering may require alignment models
different from those used by the core trans-
lation system. Sparse alignments with
high precision on the link level, for trans-
lation units, and on the subset of cross-
ing links, like intersected HMM models,
are preferred. Unlike SMT performance
the desired alignment characteristics are
similar for small and large training data
for the pre-reordering tasks. Moreover,
we confirm previous research showing that
the fuzzy reordering score is a useful and
cheap proxy for performance on SMT re-
ordering tasks.
1 Introduction
Word alignment is a key component in all state-of-
the-art statistical machine translation (SMT) sys-
tems, and there has been some work exploring the
connection between word alignment quality and
translation quality (Och and Ney, 2003; Fraser and
Marcu, 2007; Lambert et al., 2012). The standard
way to evaluate word alignments in this context is
by using metrics like alignment error rate (AER)
and F-measure on the link level, and the general
conclusion appears to be that translation quality
benefits from alignments with high recall (rather
than precision), at least for large training data. Al-
though many other ways of measuring alignment
quality have been proposed, such as working on
translation units (Ahrenberg et al., 2000; Ayan and
Dorr, 2006; S?gaard and Kuhn, 2009) or using link
degree and related measures (Ahrenberg, 2010),
these methods have not been used to study the re-
lation between alignment and translation quality,
with the exception of Lambert et al. (2012).
Word alignment is also used for many other
tasks besides translation, including term bank
creation (Merkel and Foo, 2007), cross-lingual
annotation projection for part-of-speech tagging
(Yarowsky et al., 2001), semantic roles (Pado and
Lapata, 2005), pronoun anaphora (Postolache et
al., 2006), and cross-lingual clustering (T?ackstr?om
et al., 2012). Even within SMT itself, there are
tasks such as reordering that often make crucial
use of word alignments. For instance, source lan-
guage reordering commonly relies on rules learnt
automatically from word-aligned data (e.g., Xia
and McCord (2004)). As far as we know, no one
has studied the impact of alignment quality on
these additional tasks, and it seems to be tacitly
assumed that alignments that are good for transla-
tion are also good for other tasks.
In this paper we set out to explore the impact
of alignment quality on two pre-reordering tasks
for SMT. In doing so, we employ a wider range of
quality indicators than is customary, and for refer-
ence these indicators are used also to assess over-
all translation quality. To allow an in-depth explo-
ration of the connections between several aspects
of word alignment and reordering, we limit our
study to one language pair, German?English. We
think this is a suitable language pair for studying
reordering since it has both short range and long
range reorderings. Our main focus is on using rel-
atively large training data, 2M sentences, but we
also report results with small training data, 170K
sentences. The main conclusion of our study is
that alignments that are optimal for translation are
not necessarily optimal for reordering, where pre-
275
cision is of greater importance than recall. For
SMT the best alignments are different depending
on corpus size, but for the reordering tasks results
are stable across training data size.
In section 2 we discuss previous work related
to word alignment and SMT. In section 3, we in-
troduce the word alignment quality indicators we
use, and show experimental results for a number
of alignment systems on an SMT task. In sec-
tion 4, we turn to reordering for SMT and use
the same quality indicators to study the impact of
alignment quality on reordering quality. In section
5 we briefly describe results using small training
data. In section 6, we conclude and suggest direc-
tions for future work.
2 Word Alignment and SMT
Word alignment is the task of relating words
in one language to words in the translation in
another language, see an example in Figure 1.
Word alignment models can be learnt automati-
cally from large corpora of sentence aligned data.
Brown et al. (1993) proposed the so-called IBM
models, which are still widely used. These five
models estimate alignments from corpora using
the expectation-maximization algorithm, and each
model adds some complexity. Model 4 is com-
monly used in SMT systems. There have been
many later suggestions of alternatives to these
models. These are often alternatives to model 2,
such as the HMM model (Vogel et al., 1996) and
fast align (Dyer et al., 2013).
All these generative models produce directional
alignments where one word in the source can be
linked to many target words (1?m links) but not
vice versa. It is generally desirable to also allow
n?1 and n?m links, and to achieve this it is com-
mon practice to perform word alignment in both
directions and to symmetrize them using some
heuristic. A number of common symmetrization
strategies are described in Table 1 (Koehn et al.,
2005). There are also other alternatives, such as
the refined method (Och and Ney, 2003), or link
deletion from the union (Fossum et al., 2008).
There is also a wide range of alternative ap-
proaches to word alignment. For example, various
discriminative models have been proposed in the
literature (Liu et al., 2005; Moore, 2005; Taskar
et al., 2005). Their advantage is that they may
integrate a wide range of features that may lead
to improved alignment quality. However, most of
Symmetrization Description
int: intersection A
TS
?A
ST
uni: union A
TS
?A
ST
gd: grow-diag intersection plus adjacent links
from the union if both linked
words are unaligned
gdf: grow-diag-final gd with links from the union
added in a final step if either
linked word is unaligned
gdfa:
grow-diag-final-and
gd with links from the union
added in a final step if both linked
words are unaligned
Table 1: Symmetrization strategies for word align-
ments A
TS
and A
ST
in two directions
these models require external tools (for creating
linguistic features) and manually aligned training
data, which we do not have for our data sets (be-
sides the data we need for evaluation). Investigat-
ing these types of models are outside the scope of
our current work.
Word alignments are used as an important
knowledge source for training SMT systems. In
word-based SMT, the parameters of the gener-
ative word alignment models are essentially the
translation model of the system. In phrase-based
SMT (PBSMT) (Koehn et al., 2003), which is
among the state-of-the-art systems today, word
alignments are used as a basis for extracting
phrases and estimating phrase alignment probabil-
ities. Similarly, word alignments are also used for
estimating rule probabilities in various kinds of hi-
erarchical and syntactic SMT (Chiang, 2007; Ya-
mada and Knight, 2002; Galley et al., 2004).
Intrinsic evaluation of word alignment is gener-
ally based on a comparison to a gold standard of
human alignments. Based on the gold standard,
metrics like precision, recall and F-measure can
be calculated for each alignment link, see Eqs. 1?
2, where A are hypothesized alignment links and
G are gold standard links. Another common met-
ric is alignment error rate (AER) (Och and Ney,
2000), which is based on a distinction between
sure, S, and possible, P , links in the gold stan-
dard. 1?AER is identical to balanced F-measure
when the gold standard does not make a distinc-
tion between S and P.
Precision(A,G) =
|G ?A|
|A|
(1)
Recall(A,G) =
|G ?A|
|G|
(2)
AER = 1?
|P ?A|+ |S ?A|
|S|+ |A|
(3)
276
Crossing = 8
SKDT =
?
8/66 ? 0.65
6 1?1 links
3 multi links
0 null links
Figure 1: An example alignment illustrating n?1, 1?m and crossing links.
The relation between word alignment qual-
ity and PBSMT has been studied by some re-
searchers. Och and Ney (2000) looked at the im-
pact of IBM and HMM models on the alignment
template approach (Och et al., 1999) in terms of
AER. They found that AER correlates with human
evaluation of sentence level quality, but not with
word error rate. Fraser and Marcu (2007) found
that there is no correlation between AER and Bleu
(Papineni et al., 2002), especially not when the P -
set is large. They found that a balanced F-measure
is a better indicator of Bleu, but that a weighted
F-measure is even better (see Eq. 4) mostly with
a higher weight for recall than for precision. This
weight, however, needs to be optimized for each
data set, language pair, and gold standard align-
ment separately.
F(A,G, ?) =
(
?
Precision(A,G)
+
1? ?
Recall(A,G)
)
?1
(4)
Ayan and Dorr (2006) on the other hand found
some evidence for the importance of precision
over recall. However, they used much smaller
training data than Fraser and Marcu (2007). They
also suggested using a measure called consistent
phrase error-rate (CPER), but found that it was
hard to assess the impact of alignment on MT, both
with AER and CPER. Lambert et al. (2012) per-
formed a study where they investigated the effect
of word alignment on MT using a large number of
word alignment indicators. They found that there
was a difference between large and small datasets
in that alignment precision was more important
with small data sets, and recall more important
with large data sets. Overall they did not find any
indicator that was significant over two language
pairs and different corpus sizes. There were more
significant indicators for large datasets, however.
Most researchers who propose new alignment
models perform both a gold standard evalua-
tion and an SMT evaluation (Liang et al., 2006;
Ganchev et al., 2008; Junczys-Dowmunt and Sza?,
2012; Dyer et al., 2013). The relation between the
two types of evaluation is often quite weak. Sev-
eral of these studies only show AER on their gold
standard, despite its well-known shortcomings.
Even though many studies have shown some
relation between translation quality and AER or
weighted F-measure, it has rarely been investi-
gated thoroughly in its own right, and, as far as we
are aware, not for other tasks than SMT. Further-
more, most of these studies considers nothing else
but link level agreement. In this paper we take a
broader view on alignment quality and explore the
effect of other types of quality indicators as well.
3 Word Alignment Quality Indicators
We investigate four groups of quality indicators.
The first group is the classic group where met-
rics are calculated on the alignment link level,
which has been used in several studies. In our
experiments we use a gold standard that does not
make use of distinctions between sure and possible
links, as suggested by Fraser and Marcu (2007).
With this, we can calculate the standard metrics
P(recision) R(ecall) and F(-measure). We will
mainly use balanced F-measure, but occasionally
also report weighted F-measure. As noted before,
1?AER is equivalent to balanced F when only
sure links are used, and will thus not be reported
separately.
S?gaard and Kuhn (2009) and S?gaard and Wu
(2009) suggested working on the translation unit
(TU) level, instead of the link level. A translation
unit, or cept (Goutte et al., 2004), is defined as
a maximally connected subgraph of an alignment.
In Figure 1, the twelve links form nine translation
units. S?gaard and Wu (2009) suggest the metric
TUER, translation unit error rate, shown in Eq. 5,
where A
U
are hypothesized translation units, and
G
U
are gold standard translation units.
1
They use
TUER to establish lower bounds for the cover-
age of alignments from different formalisms, not
to evaluate SMT. While they only use TUER, it
1
TUER is similar to CPER (Ayan and Dorr, 2006), which
measures the error rate of extracted phrases. Due to how
phrase extraction handle null links, there are differences,
however.
277
is also possible to define Precision, Recall and F-
measure over translation units in the same way as
for alignment links. We will use these three mea-
sures to get a broader picture of TUs in alignment
evaluation. Also in this case, 1?TUER is equiva-
lent to F-measure.
TUER(A,G) = 1?
2|A
U
?G
U
|
|A
U
|+ |G
U
|
(5)
The TU metrics are quite strict, since they re-
quire exact matching of TUs. Tiedemann (2005)
suggested the MWU metrics for word alignment
evaluation, which also consider partial matches
of annotated multi-word units, which is a similar
concept to TUs. In those metrics, precision and
recall grow proportionally to the number of cor-
rectly aligned words within translation units. Pro-
posed links are in this way scored according to
their overlap with translation units in the gold stan-
dard. Precision and recall are defined in Eqs. 6?7,
where overlap(X
U
, Y ) is the number of source
and target words in X
U
that overlap with transla-
tion units in Y normalized by the size of X
U
(in
terms of source and target words). Note, that TUs
need to overlap in source and target. Otherwise,
their overlap will be counted as zero.
P
MWU
=
?
A
U
?A
overlap(A
U
, G)
|A|
(6)
R
MWU
=
?
G
U
?G
overlap(G
U
, A)
|G|
(7)
There have also been attempts at classifying
alignments in other ways, not related to a gold
standard. Ahrenberg (2010) proposed several
ways to categorize human alignments, including
link degree, reordering of links, and structural cor-
respondence. He used these indicators to profile
hand-aligned corpora from different domains. We
will not use structural correspondence, which re-
quires a dependency parser, and which we believe
is error prone when performed automatically. We
will use what we call link degree, i.e., how many
alignment links each word obtains. Ahrenberg
(2010) used a fine-grained scheme of the percent-
age for different degrees, including isomorphism
1?1, deletion 0?1, reduction m?1, and paraphrase
m?n. Similar link degree classes were used by
Lambert et al. (2012). In this work we will re-
duce these classes into three: 1?1 links, null links,
which combine the 0?1 and 1?0 cases, and multi
links where there are many words on at least one
side.
Ahrenberg (2010) also proposed to measure re-
orderings. He does this by calculating the percent-
age of links with crossings of different lengths. To
define this he only considers adjacent links in the
source using the distance between corresponding
target words, which means that his metric becomes
a directional measure. Reorderings of alignments
was also used by Genzel (2010), who used cross-
ing score, the number of crossing links, to rank
reordering rules. This is non-directional and sim-
pler to calculate than Ahrenberg (2010)?s metrics,
and implicitly covers length since a long distance
reordering leads to a higher number of pairwise
crossing links. Birch and Osborne (2011) sug-
gest using squared Kendall ? distance (SKTD), see
Eq. 8, where n is the number of links, as a basis
of LR-score, an MT metric that takes reordering
into account. They found that squaring ? better
explained reordering, than using only ? . In this
study we will use both, crossing score and SKTD.
Figure 1 shows these scores for an example sen-
tence. These two measures only tell us how much
reordering there is. To quantify this relative to the
gold standard we also report the absolute differ-
ence between the number of gold standard cross-
ings and system crossings, which we call Crossd-
iff. To account for the quality of crossings, to some
extent, we will also report precision, recall, and F-
measure for the subset of translation units that are
involved in a crossing.
SKTD =
?
|crossing link pairs|
(n
2
? n)/2
(8)
3.1 Alignment Experiments
We perform all our experiments for German?
English. The alignment indicators are calculated
on a corpus of 987 hand aligned sentences (Pado
and Lapata, 2005). The gold standard contains
explicit null links, which the symmetrized auto-
matic alignments do not. To allow a straightfor-
ward comparison we consistently remove all null
links when comparing system alignments to the
gold standard.
For creating the automatic alignments we used
GIZA++ (Och and Ney, 2003) to compute direc-
tional alignments for model 2?4 and the HMM
model, and fast align (fa) (Dyer et al., 2013) as
newer alternatives to model 2. These models re-
quire large amounts of data to be estimated reli-
ably. To achieve this we concatenated the gold
standard with the large SMT training data (see
278
A
l
i
g
n
m
e
n
t
l
i
n
k
s
T
r
a
n
s
l
a
t
i
o
n
u
n
i
t
s
M
W
U
L
i
n
k
d
e
g
r
e
e
L
i
n
k
c
r
o
s
s
i
n
g
s
T
o
t
a
l
P
R
F
T
o
t
a
l
P
R
F
P
R
F
1
-
1
n
u
l
l
m
u
l
t
i
T
o
t
a
l
S
K
T
D
P
R
F
C
r
o
s
s
d
i
f
f
g
o
l
d
2
2
6
2
9
?
?
?
1
7
0
6
8
?
?
?
?
?
?
.
5
4
2
.
3
2
8
.
1
3
0
3
0
1
6
3
.
2
9
2
?
?
?
0
2
-
i
n
t
1
5
3
6
2
.
8
5
0
.
5
7
7
.
6
8
7
1
5
3
6
2
.
7
0
1
.
6
3
1
.
6
6
4
.
8
4
9
.
7
1
2
.
7
7
4
.
5
0
0
.
5
0
0
.
0
0
0
1
0
0
6
4
.
2
6
7
.
5
5
1
.
4
6
3
.
5
0
3
2
0
0
9
9
3
-
i
n
t
1
6
5
7
3
.
8
6
0
.
6
3
0
.
7
2
7
1
6
5
7
3
.
7
0
7
.
6
8
6
.
6
9
7
.
8
5
7
.
7
7
6
.
8
1
4
.
5
6
1
.
4
3
9
.
0
0
0
1
2
6
8
2
.
2
7
4
.
5
5
3
.
5
2
1
.
5
3
7
1
7
4
8
1
4
-
i
n
t
1
6
5
2
9
.
9
0
3
.
6
6
0
.
7
6
3
1
6
5
2
9
.
7
4
3
.
7
2
0
.
7
3
1
.
9
0
1
.
8
1
3
.
8
5
5
.
5
5
9
.
4
4
1
.
0
0
0
1
1
2
2
9
.
2
5
1
.
6
6
3
.
5
2
2
.
5
8
4
1
8
9
3
4
H
M
M
-
i
n
t
1
4
8
7
1
.
9
2
2
.
6
0
6
.
7
3
1
1
4
8
7
1
.
7
6
8
.
6
6
9
.
7
1
5
.
9
2
0
.
7
5
0
.
8
2
7
.
4
7
6
.
5
2
4
.
0
0
0
8
0
7
7
.
2
2
1
.
7
0
9
.
4
1
7
.
5
2
5
2
2
0
8
6
f
a
-
i
n
t
1
5
9
9
7
.
8
5
7
.
6
0
6
.
7
1
0
1
5
9
9
7
.
6
9
6
.
6
5
2
.
6
7
3
.
8
5
4
.
7
4
2
.
7
9
4
.
5
3
1
.
4
6
9
.
0
0
0
9
7
2
4
.
2
4
6
.
5
6
8
.
4
7
1
.
5
1
5
2
0
4
3
9
2
-
g
d
2
2
8
8
2
.
7
0
2
.
7
1
0
.
7
0
6
1
6
5
1
1
.
5
9
9
.
5
7
9
.
5
8
9
.
8
0
6
.
8
2
7
.
8
1
6
.
5
2
4
.
2
8
9
.
1
8
6
2
1
8
2
3
.
2
7
0
.
4
4
6
.
4
4
4
.
4
4
5
8
3
4
0
3
-
g
d
2
1
9
6
1
.
7
5
7
.
7
3
4
.
7
4
5
1
7
6
4
4
.
6
5
0
.
6
7
2
.
6
6
1
.
8
1
7
.
8
5
5
.
8
3
6
.
6
0
8
.
2
7
0
.
1
2
2
2
1
8
8
6
.
2
7
8
.
4
9
2
.
5
2
3
.
5
0
7
8
2
7
7
4
-
g
d
2
2
7
5
4
.
7
6
8
.
7
7
2
.
7
7
0
1
7
6
1
1
.
6
7
0
.
6
9
2
.
6
8
1
.
8
3
9
.
8
8
6
.
8
6
2
.
6
0
5
.
2
4
7
.
1
4
8
2
1
9
6
6
.
2
5
9
.
5
8
3
.
5
1
7
.
5
4
8
8
1
9
7
H
M
M
-
g
d
1
9
4
3
0
.
8
1
2
.
6
9
8
.
7
5
1
1
5
8
3
1
.
7
0
9
.
6
5
8
.
6
8
2
.
8
7
8
.
8
2
0
.
8
4
8
.
4
9
9
.
4
0
7
.
0
9
4
1
4
3
3
4
.
2
3
1
.
6
2
1
.
4
1
1
.
4
9
5
1
5
8
2
9
f
a
-
g
d
2
3
1
4
8
.
7
0
2
.
7
1
9
.
7
1
0
1
7
0
4
3
.
5
8
9
.
5
8
8
.
5
8
8
.
8
0
2
.
8
3
9
.
8
2
0
.
5
4
8
.
2
5
8
.
1
9
4
1
8
5
7
8
.
2
4
2
.
4
5
4
.
4
4
7
.
4
5
0
1
1
5
8
5
2
-
g
d
f
a
2
3
8
4
0
.
6
8
7
.
7
2
4
.
7
0
5
1
7
4
6
9
.
5
7
5
.
5
8
8
.
5
8
2
.
7
8
0
.
8
4
1
.
8
0
9
.
5
9
0
.
2
1
6
.
1
9
4
2
5
6
1
6
.
2
7
9
.
4
1
9
.
4
7
3
.
4
4
4
6
7
1
8
3
-
g
d
f
a
2
3
0
4
9
.
7
3
6
.
7
4
9
.
7
4
2
1
8
7
3
2
.
6
2
1
.
6
8
1
.
6
5
0
.
7
8
6
.
8
7
0
.
8
2
6
.
6
8
4
.
1
8
8
.
1
2
8
2
7
1
1
9
.
2
9
4
.
4
5
1
.
5
6
1
.
5
0
0
4
5
4
7
4
-
g
d
f
a
2
3
7
0
4
.
7
5
1
.
7
8
7
.
7
6
9
1
8
5
6
1
.
6
4
5
.
7
0
1
.
6
7
2
.
8
1
3
.
9
0
1
.
8
5
5
.
6
7
3
.
1
7
2
.
1
5
4
2
6
9
7
7
.
2
7
5
.
5
2
9
.
5
6
2
.
5
4
5
3
0
4
4
H
M
M
-
g
d
f
a
2
0
5
5
4
.
7
9
9
.
7
2
6
.
7
6
1
1
6
9
5
5
.
6
8
5
.
6
8
1
.
6
8
3
.
8
5
7
.
8
5
1
.
8
5
4
.
5
6
5
.
3
3
7
.
0
9
8
1
7
3
9
9
.
2
4
6
.
5
8
4
.
4
7
5
.
5
2
4
1
2
7
6
4
f
a
-
g
d
f
a
2
3
7
1
7
.
6
9
3
.
7
2
6
.
7
1
0
1
7
6
1
2
.
5
7
5
.
5
9
4
.
5
8
4
.
7
8
5
.
8
4
6
.
8
1
5
.
5
8
7
.
2
1
4
.
1
9
9
2
0
3
8
4
.
2
4
7
.
4
3
9
.
4
6
5
.
4
5
2
9
7
7
9
2
-
g
d
f
2
9
0
5
0
.
5
9
1
.
7
5
8
.
6
6
4
1
7
0
8
9
.
5
1
1
.
5
1
2
.
5
1
2
.
7
6
1
.
8
7
6
.
8
1
4
.
6
2
5
.
0
0
2
.
3
7
3
5
9
5
9
2
.
3
3
8
.
3
2
1
.
4
3
8
.
3
7
0
2
9
4
2
9
3
-
g
d
f
2
6
5
7
5
.
6
6
0
.
7
7
5
.
7
1
3
1
8
3
5
4
.
5
8
8
.
6
3
2
.
6
0
9
.
7
7
8
.
8
9
1
.
8
3
1
.
7
1
2
.
0
6
4
.
2
2
5
5
0
8
3
4
.
3
4
4
.
3
8
7
.
5
5
2
.
4
5
5
2
0
6
7
1
4
-
g
d
f
2
6
5
2
9
.
6
9
3
.
8
1
2
.
7
4
8
1
8
2
6
9
.
6
2
8
.
6
7
3
.
6
5
0
.
8
1
0
.
9
2
2
.
8
6
2
.
7
0
6
.
0
7
0
.
2
2
3
4
7
2
1
6
.
3
2
2
.
4
5
9
.
5
8
5
.
5
1
4
1
7
0
5
3
H
M
M
-
g
d
f
2
3
8
8
6
.
7
2
5
.
7
6
5
.
7
4
4
1
6
6
6
0
.
6
5
1
.
6
3
5
.
6
4
3
.
8
5
1
.
8
8
7
.
8
6
9
.
5
7
9
.
2
5
1
.
1
6
9
3
6
8
8
1
.
3
0
9
.
4
7
3
.
4
9
9
.
4
8
6
6
7
1
8
f
a
-
g
d
f
2
6
7
2
4
.
6
3
3
.
7
4
8
.
6
8
6
1
7
4
5
4
.
5
2
4
.
5
3
6
.
5
3
0
.
7
6
9
.
8
6
5
.
8
1
4
.
5
8
9
.
1
0
1
.
3
1
0
3
4
3
0
9
.
3
7
9
.
3
5
1
.
4
4
5
.
3
9
2
4
1
4
6
2
-
u
n
i
3
0
7
1
2
.
5
6
6
.
7
6
9
.
6
5
2
1
5
8
6
4
.
5
0
3
.
4
6
8
.
4
8
5
.
7
7
4
.
8
6
9
.
8
1
8
.
5
8
4
.
0
0
2
.
4
1
3
7
1
2
2
3
.
3
4
9
.
3
0
5
.
3
9
6
.
3
4
5
4
1
0
6
0
3
-
u
n
i
2
8
0
9
3
.
6
3
6
.
7
8
9
.
7
0
4
1
7
3
9
1
.
5
9
2
.
6
0
3
.
5
9
7
.
7
9
1
.
8
8
9
.
8
3
7
.
6
8
4
.
0
6
7
.
2
4
9
6
1
8
2
3
.
3
5
5
.
3
8
1
.
5
2
3
.
4
4
1
3
1
6
6
0
4
-
u
n
i
2
7
9
2
0
.
6
7
0
.
8
2
7
.
7
4
0
1
7
4
1
1
.
6
3
6
.
6
4
9
.
6
4
2
.
8
2
6
.
9
2
1
.
8
7
1
.
6
8
2
.
0
7
4
.
2
4
4
5
7
4
0
8
.
3
3
3
.
4
5
6
.
5
6
4
.
5
0
4
2
7
2
4
5
H
M
M
-
u
n
i
2
4
7
1
2
.
7
0
7
.
7
7
2
.
7
3
8
1
5
9
8
0
.
6
4
9
.
6
0
8
.
6
2
8
.
8
5
7
.
8
8
1
.
8
6
9
.
5
6
1
.
2
6
0
.
1
8
0
4
2
2
6
4
.
3
1
9
.
4
5
9
.
4
7
5
.
4
6
7
1
2
1
0
1
f
a
-
u
n
i
2
7
9
5
1
.
6
1
2
.
7
5
6
.
6
7
6
1
6
3
8
5
.
5
1
2
.
4
9
1
.
5
0
4
.
7
8
1
.
8
6
7
.
8
2
2
.
5
4
8
.
1
1
1
.
3
4
6
3
8
2
8
5
.
3
9
6
.
3
3
6
.
4
0
7
.
3
6
8
8
1
2
2
T
a
b
l
e
2
:
V
a
l
u
e
s
f
o
r
a
l
i
g
n
m
e
n
t
q
u
a
l
i
t
y
i
n
d
i
c
a
t
o
r
s
f
o
r
t
h
e
d
i
f
f
e
r
e
n
t
a
l
i
g
n
m
e
n
t
s
,
w
h
e
r
e
2
?
4
,
H
M
M
,
a
n
d
f
a
a
r
e
a
l
i
g
n
m
e
n
t
m
o
d
e
l
s
,
a
n
d
s
y
m
m
e
t
r
i
z
a
t
i
o
n
s
t
r
a
t
e
g
i
e
s
r
e
f
e
r
t
o
T
a
b
l
e
1
279
Section 3.2) of 2M sentences during alignment.
For symmetrization we used all methods in Table
1, as implemented in the Moses toolkit (Koehn et
al., 2007) and in fast align (Dyer et al., 2013).
Based on the automatically aligned gold stan-
dard, we calculated all alignment indicators for all
settings. The complete results can be found in
Table 2, where we have ordered the symmetriza-
tion methods with the most sparse, intersection, on
top. Overall we can see that while several of the
alignment methods create a much higher number
of alignment links than the gold standard, they do
not produce many more translation units. This is
very interesting and indicates why link level statis-
tics may not be accurate enough to predict the per-
formance of certain downstream applications. As
expected, the metric scores for translation units
are lower than for link level metrics. This is
partly due to the fact that these measures do not
count any partially correct links; the MWU met-
rics which considers partial matches often have
higher scores than link level metrics. Another
finding is that the number of crossings vary a lot
with more than twice as many as the reference for
model2+union, and less than three times as many
for HMM+intersection. The HMM and fa models
have fewer reorderings than the IBM models.
We are now interested in the relation between
alignment evaluation on the link level and on the
translation unit level, which has not been thor-
oughly investigated before. Table 3 shows the cor-
relations between the various metrics. Both preci-
sion and F-measure at the link level have signifi-
cant correlations to all TU metrics. Link level re-
call, on the other hand, is significantly negatively
correlated with TU precision, but not significantly
correlated to any other TU metric, not even TU re-
call. Link level precision is thus highly important
for matching translation units. We can also note
here that while there is a trade-off between preci-
sion and recall on link level, this is not the case for
translation units, which can have both high pre-
cision and high recall. The same is not true for
MWU, that allows partial matching, where we also
see at least some precision/recall trade-off.
3.2 SMT Experiments
For reference, we first study the impact of align-
ment on SMT performance. Our SMT system
is a standard PBSMT system trained on WMT13
Translation unit
Link level ? P R F
P .95 .77 .90
R ?.57 ?.22 ?.42
F .70 .90 .83
Table 3: Pearson correlations between gold stan-
dard word alignment evaluation on the link level
and on translation unit level. Significant correla-
tions are marked with bold (< 0.01).
data.
2
We trained a German?English system on
2M sentences from Europarl and News Commen-
tary. We used the target side of the parallel corpus
and the SRILM toolkit (Stolcke, 2002) to train a 5-
gram language model. For training the translation
model and for decoding we used the Moses toolkit
(Koehn et al., 2007). We applied a standard feature
set consisting of a language model feature, four
translation model features, word penalty, phrase
penalty, and distortion cost. For tuning we used
minimum error-rate training (Och, 2003). In or-
der to minimize the risk of tuning influencing the
results, we used a fixed set of weights for each
experiment, tuned on a model 4+gdfa alignment.
3
For tuning we used newstest2009 with 2525 sen-
tences, and for testing we used newstest2013 with
3000 sentences. Evaluation was performed using
the Bleu metric (Papineni et al., 2002). The same
system setup was used for the SMT systems with
reordering.
Table 4 shows the results on the SMT task.
Model 3 and 4 with gd/gdfa symmetrization yield
the highest scores. There is a larger difference be-
tween systems with different symmetrization than
between systems with different alignment models.
The sparse intersection symmetrization gives the
poorest results. The top row in Table 5 shows
correlations between Bleu and all word alignment
quality indicators. There are significant correla-
tions with link level recall. A weighted link level
F-measure with ? = 0.3 gives a significant corre-
lation of .72, which confirms the results of Fraser
and Marcu (2007). There are no significant corre-
lations with the TU metrics but a positive correla-
tion with the number of TUs. For the MWU met-
rics the correlations are similar to the link level,
2
http://www.statmt.org/wmt13/
translation-task.html
3
This could have disfavored the other alignments, so we
also performed control experiments where we ran separate
tunings for each alignment. While the absolute results varied
somewhat, the correlations with alignment indicators were
stable.
280
m2 m3 m4 HMM fa
inter 18.1 19.1 19.3 18.8 18.9
gd 20.4 20.9 20.9 20.5 20.6
gdfa 20.4 20.7 20.8 20.5 20.5
gdf 19.4 19.7 20.1 19.9 20.0
union 19.2 19.6 19.8 19.7 20.0
Table 4: Baseline Bleu scores for different sym-
metrization heuristics
suggesting that they measure similar things. Intu-
itively it seems important for SMT to match full
translation units, but it might be the case that the
phrase extraction strategy is robust as long as there
are partial matches. There are no significant cor-
relations with link degree or link crossings, ex-
cept a negative correlation with Crossdiff, which
means that it is good to have a similar number of
crossings as the baseline. These results confirm
results from previous studies that link level mea-
sures, especially recall and weighted F-measure
show some correlation with SMT quality whereas
precision does not.
4 Reordering Tasks for SMT
Reordering is an important part of any SMT sys-
tem. One way to address it is to add reorder-
ing models to standard PBSMT systems, for in-
stance lexicalized reordering models (Koehn et al.,
2005), or to directly model reordering in hierarchi-
cal (Chiang, 2007) or syntactic translation models
(Yamada and Knight, 2002). Another type of ap-
proach is preordering, where the source side is re-
ordered to mimic the target side before translation.
There have also been approaches where reordering
is modeled as part of the evaluation of MT systems
(Birch and Osborne, 2011).
We can distinguish two main types of ap-
proaches to preordering in SMT, either by using
hand-written rules, which often operate on syn-
tactic trees (Collins et al., 2005), or by reordering
rules that are learnt automatically based on a word
aligned corpus (Xia and McCord, 2004). The lat-
ter approach is of interest to us, since it is based
on word alignments.
There has been much work on automatic learn-
ing of reordering rules, which can be based on dif-
ferent levels of annotation, such as part-of-speech
tags (Rottmann and Vogel, 2007; Niehues and
Kolss, 2009; Genzel, 2010), chunks (Zhang et
al., 2007) or parse trees (Xia and McCord, 2004).
In general, all these approaches lead to improve-
ments of translation quality. The reordering is
always applied on the translation input. It can
also be applied on the source side of the train-
ing corpora, which sometimes improves the results
(Rottmann and Vogel, 2007), but sometimes does
not make a difference (Stymne, 2012). When pre-
ordering is performed on the translation input, it
can be presented to the decoder as a 1-best reorder-
ing (Xia and McCord, 2004), as an n-best list (Li
et al., 2007), or as a lattice of possible reorderings
(Rottmann and Vogel, 2007; Zhang et al., 2007).
In the preordering studies cited above it is often
not even stated which alignment model was used.
A few authors mention the alignment tool that has
been applied but no comparison between different
alignment models is performed in any of the pa-
pers we are aware of. Li et al. (2007), for exam-
ple, simply state that they used GIZA++ and gdf
symmetrization and that they removed less proba-
ble multi links. Lerner and Petrov (2013) use the
intersection of HMM alignments and claims that
model 4 did not add much value. Genzel (2010)
did mention that using a standard model 4 was
not successful for his rule learning approach. In-
stead he used filtered model-1-alignments, which
he claims was more successful. However, there
are no further analyses or comparisons between
the alignments reported in any of these papers.
Another type of approach to reordering is to
only reorder the data in order to improve word
alignments, and to restore the original word or-
der before training the SMT system. This type
of approach has the advantage that no modifica-
tions are needed for the translation input. This ap-
proach has also been used both with hand-written
rules (Carpuat et al., 2010; Stymne et al., 2010)
and with rules based on initial word alignments on
non-reordered texts (Holmqvist et al., 2009). For
the latter approach a small study of the effect of gd
and gdfa symmetrizations was presented, which
only showed small variations in quality scores
(Holmqvist et al., 2012).
Below we present the two tasks that we study
in this paper: part-of-speech-based reordering for
creating input lattices for SMT and alignment-
based reordering for improving phrase-tables. We
evaluate the performance of these tasks in rela-
tion to the use of different alignment models and
symmetrization heuristics. For these tasks we are
mainly interested in the full translation task, for
which we report Bleu scores. In addition we also
show fuzzy reordering score (FRS), which focuses
281
Alignment links Translation units MWU
Total P R F Total P R F P R F
SMT, Bleu .33 ?.25 .56 .46 .65 ?.20 .16 ?.02 ?.29 .59 .44
POSReo, FRS ?.80 .87 ?.49 .75 ?.23 .90 .81 .89 .82 ?.45 .22
POSReo, Bleu ?.64 .74 ?.27 .85 .05 .80 .80 .86 .67 ?.23 .35
AlignReo, FRS ?.77 .88 ?.43 .84 ?.11 .90 .88 .92 .81 ?.37 .31
AlignReo, Bleu ?.81 .83 ?.58 .61 ?.24 .75 .64 .72 .71 ?.53 .04
Link degree Link crossings
1-1 null multi Total SKTD P R F Crossdiff
SMT, Bleu .33 ?.30 .21 ?.05 ?.14 ?.09 .25 .07 ?.63
POSReo, FRS ?.41 .84 ?.89 ?.81 ?.70 .90 .21 .86 ?.41
POSReo, Bleu ?.17 .66 ?.80 ?.71 ?.60 .79 .42 .89 ?.49
AlignReo, FRS ?.32 .77 ?.86 ?.80 ?.73 .94 .27 .92 ?.38
AlignReo, Bleu ?.57 .83 ?.79 ?.93 ?.91 .86 ?.07 .69 ?.52
Table 5: Pearson correlations between different alignment characteristics and scores for the translation
and reordering tasks. Significant correlations are marked with bold (< 0.01).
only on the reordering component (Talbot et al.,
2011). It compares a system reordering to a refer-
ence reordering, by measuring how many chunks
that have to be moved to get an identical word or-
der, see Eq. 9, where C is the number of con-
tiguously aligned chunks, and M the number of
words. To find the reference ordering we apply
the method of Holmqvist et al. (2009), described
in Section 4.2, to the gold standard alignment.
FRS = 1?
C ? 1
M ? 1
(9)
4.1 Part-of-Speech-Based Reordering
Our first reordering task is a part-of-speech-based
preordering method described by Rottmann and
Vogel (2007) and Niehues and Kolss (2009),
which was successfully used for German?English
translation. Rules are learnt from a word aligned
POS-tagged corpus. Based on the alignments, tag
patterns are identified that give rise to specific re-
orderings. These patterns are then scored based
on relative frequency.
4
The rules are then applied
to the translation input to create a reordering lat-
tice, with normalized edge scores based on rule
scores. In our experiments we only use rules with
a score higher than 0.2, to limit the size of the lat-
tices. For calculating FRS, we pick the highest
scoring 1-best word order from the lattices.
We learn rules from our entire SMT training
corpus varying alignment models and symmetriza-
tion. To investigate only the effect of word align-
ment for creating reordering rules, we do not
4
Note that we do not use words (Rottmann and Vogel,
2007) or wild cards (Niehues and Kolss, 2009) in our rules.
m2 m3 m4 HMM fa
inter .577 .575 .581 .596 .567
gd .555 .559 .570 .589 .546
gdfa .540 .540 .559 .579 .539
gdf .439 .499 .542 .560 .495
union .442 .492 .544 .563 .486
Table 6: Fuzzy reordering scores for part-of-
speech-based reordering for different alignments
m2 m3 m4 HMM fa
inter 21.4 21.6 21.8 21.6 21.6
gd 21.5 21.6 21.6 21.7 21.5
gdfa 21.4 21.5 21.7 21.7 21.4
gdf 20.3 21.0 21.4 21.5 21.0
union 20.3 21.5 21.6 21.5 20.8
Table 7: Bleu scores for part-of-speech-based re-
ordering for different alignments
change the SMT system, which is trained based
on model 4+gdfa alignments. The only thing that
varies for the translation task is thus the input lat-
tice given to this SMT system.
The results are shown in Tables 6 and 7. Most
Bleu scores are better than using the same SMT
system without preordering, with a Bleu score of
20.8. The results on FRS and Bleu are highly cor-
related at .94, despite the fact that we use a lattice
as SMT input, and the 1-best order for FRS. For
both metrics sparse symmetrization like intersec-
tion and gd performs best. Model 4 and HMM
perform best with similar Bleu scores, but FRS is
better for the HMM model.
Table 5 shows the correlations with the word
alignment indicators, in the rows labeled POSReo.
There are strong correlations with all TU metrics,
contrary to the SMT task. There are also signifi-
cant correlations with link level precision and bal-
282
anced F-measure. The correlation with weighted
link level F-measure is even higher, .91 for ? =
0.6. This is an indication that this algorithm is
more sensitive to precision than the SMT task. As
for the SMT task, the correlation patterns are simi-
lar for the MWU metrics as for link level. For link
degree, null alignments are correlated, but there is
a negative correlation for multi links. The correla-
tions with the number of crossings and SKTD are
negative, which means that it is better to have a
low number of crossings. This may seem counter-
intuitive, but note in Table 1 that many alignments
have a much higher number of crossings than the
baseline. The precision of the crossing links is
highly correlated with performance on this task,
while the recall is not. This tells us that it is impor-
tant that the crossings we find in the alignment are
good, but that it is less important that we find all
crossings. This makes sense since the rule learner
can then learn at least a subset of all existing cross-
ings well.
4.2 Reordering for Alignment
In our second reordering task we investigate
alignment-based reordering for improving phrase-
tables (Holmqvist et al., 2009; Holmqvist et al.,
2012). This strategy first performs a word align-
ment, based on which the source text is reordered
to remove all crossings. A second alignment is
trained on the reordered data, which is then re-
stored to the original order before training the
full SMT system. In Holmqvist et al. (2012) it
was shown that this strategy leads to improve-
ments in link level recall and F-measure as well
as small translation improvements for English?
Swedish. It also led to small improvements for
German?English translation.
Similar to the previous experiments, we now
vary alignment models and symmetrization that
are used for reordering during the first step. The
second step is kept the same using model 4+gdfa
in order to focus on the reordering step in our com-
parisons. Tables 8 and 9 show the results of these
experiments. In this case the reordering strat-
egy was not successful, always producing lower
Bleu scores than the baseline of 20.8. However,
there are some interesting differences in these out-
comes. On this task as well, FRS and Bleu scores
are highly correlated at .89, which was expected,
since this method directly uses the reordered data
to train phrase tables. For the best systems, the
m2 m3 m4 HMM fa
inter .583 .604 .669 .654 .598
gd .548 .583 .646 .642 .561
gdfa .532 .564 .633 .645 .553
gdf .422 .482 .571 .574 .474
union .395 .455 .552 .545 .452
Table 8: Fuzzy reordering scores for alignment-
based reordering for different alignments
m2 m3 m4 HMM fa
inter 19.5 19.5 19.9 20.2 19.4
gd 19.3 19.5 19.8 20.2 19.3
gdfa 19.1 19.2 19.6 20.0 19.2
gdf 18.3 18.2 18.6 19.0 18.9
union 17.4 17.8 18.4 18.8 18.8
Table 9: Bleu scores for alignment-based reorder-
ing for different alignments
FRS scores are higher than for the previous task,
see Table 6, which shows that reordering directly
based on alignments is easier than learning and ap-
plying rules based on them, given suitable align-
ments. On this task, again, the sparser alignments
are the most successful on both tasks. Here, how-
ever, the HMM model gives the best Bleu scores,
and similar FRS scores to model 4.
Table 5 shows the correlations with the word
alignment indicators, in the rows labeled Align-
Reo. The correlation patterns are very similar
to the previous task. A few more indicators are
significantly negatively correlated with alignment-
based reordering than with the other reordering
tasks and metrics. The performance on our two
reordering tasks are significantly correlated at .76.
Again alignments with good scores on TU met-
rics, link level precision and crossing link preci-
sion are preferable. For this task, the best correla-
tion with weighted link level F-measure is .86 for
? = 0.8. Again, we thus see that sparse align-
ments with high precision on all measures includ-
ing the crossing subset, are important.
5 Small Training Data
Since previous work has suggested that training
data size influences the relation between align-
ment and SMT quality for small and large training
data (Lambert et al., 2012), we investigated this is-
sue also for our reordering tasks. We repeated all
our experiments on a small dataset, only the News
Commentary data from WMT13, with 170K sen-
tences. Due to space constraints we cannot show
all results in the paper, but the main findings are
283
summarized in this section.
To acquire alignment results we realigned the
gold standard concatenated with the smaller data,
to reflect the actual quality of alignment with a
small dataset. As expected the quality scores tend
to be lower with less data. Overall the same sys-
tems tend to perform good on each metric with the
small and large data, even though there is some
variation in the ranking between systems. On the
SMT task as well, the Bleu scores are lower, as
expected. In this case fast align is doing best fol-
lowed by model 4 and 3. The best symmetrization
is again gd and gdfa. There are also some differ-
ences in the correlation profile. Link recall and
number of translation units are no longer signifi-
cantly correlated, whereas the number of crossings
and SKTD are. The highest correlation for link
level F-measure is .60 for balanced F-measure,
showing that precision is equally important to re-
call with less data.
For the reordering tasks the scores are again
lower. The POS-based reorderings again help over
the baseline SMT, whereas the alignment-based
reordering leads to slightly lower scores. The cor-
relation profile look exactly the same for Bleu
for POS-based reordering. FRS for both tasks
and Bleu for alignment-based reordering have the
same correlation profiles as Bleu for alignment-
based reordering on large data. There are thus
very small differences in the word alignment qual-
ity indicators that are relevant with large and small
training data, while there are some differences on
the SMT task. For weighted link level F-measure,
the highest correlations are found with ? = 0.6?
0.7 on the different metrics, again showing that
precision is more important than recall. For FRS
on both tasks and Bleu for alignment-based re-
ordering, model4 and HMM with intersection and
gd still perform best. For Bleu for POS-based re-
ordering, gdfa and model 3 also give good results.
6 Conclusion and Future Work
We have shown that the best combination of align-
ment and symmetrization models for SMT are not
the best models for reordering tasks in our ex-
perimental setting. For SMT, high recall is more
important than precision with large training data,
while precision and recall are of equal impor-
tance with small training data. This finding sup-
ports previous research (Fraser and Marcu, 2007;
Lambert et al., 2012). Translation unit metrics
are not predictive of SMT performance. For the
large data condition model 3 and 4 with gd and
gdfa symmetrization gave the best results, whereas
fast align with gd and gdfa was best with small
training data.
For the two preordering tasks we investigated,
however, link level weighted F-measure that gave
more weight to precision was important, as well as
all TU metrics. It was also important to have high
precision for the crossing subset of TUs. Hence,
it is more important to reliably find some cross-
ings than to find all crossings. This make sense
since the extracted rules or performed reorderings
are likely good in such cases, even if we are not
able to find all possible reorderings. In conclu-
sion, based on this study, we recommend intersec-
tion symmetrization with model 4 and HMM for
SMT reordering tasks.
We have studied two relatively different re-
ordering tasks with two training data sizes, but
found that they to a large extent prefer the same
types of alignments. Moreover, the results on
these two reordering tasks correlates strongly with
FRS, which is much cheaper to calculate than
SMT metrics that may even require retraining of
full SMT systems. This is consistent with Tal-
bot et al. (2011) who suggested FRS for preorder-
ing tasks. We thus would encourage developers
of alignment methods to not only give results for
SMT, but also for FRS, as a proxy for reordering
tasks. Furthermore, it is also useful to give results
on TU metrics in addition to link level metrics to
complement the evaluation.
In this paper, we have looked at existing genera-
tive alignment and symmetrization models. In fu-
ture work, we would also like to investigate other
models, including the removal of low-confidence
links, which has previously been proposed for pre-
reordering (Li et al., 2007; Genzel, 2010). Given
the results, it also seems motivated to develop
or adapt the existing models in general, to bet-
ter fit the properties of specific auxiliary tasks.
Furthermore, we need to validate our findings on
other language pairs, especially for non-related
languages with even more diverse word order.
Acknowledgments
This work was supported by the Swedish strategic
research programme eSSENCE.
284
References
Lars Ahrenberg, Magnus Merkel, Anna S?agvall Hein,
and J?org Tiedemann. 2000. Evaluation of word
alignment systems. In Proceedings of LREC, vol-
ume III, pages 1255?1261, Athens, Greece.
Lars Ahrenberg. 2010. Alignment-based profiling of
Europarl data in an English-Swedish parallel corpus.
In Proceedings of LREC, pages 3398?3404, Valetta,
Malta.
Necip Fazil Ayan and Bonnie J. Dorr. 2006. Going
beyond AER: An extensive analysis of word align-
ments and their impact on MT. In Proceedings of
Coling and ACL, pages 9?16, Sydney, Australia.
Alexandra Birch and Miles Osborne. 2011. Reorder-
ing metrics for MT. In Proceedings of ACL, pages
1027?1035, Portland, Oregon, USA.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263?
311.
Marine Carpuat, Yuval Marton, and Nizar Habash.
2010. Improving Arabic-to-English statistical ma-
chine translation by reordering post-verbal subjects
for alignment. In Proceedings of ACL, Short Papers,
pages 178?183, Uppsala, Sweden.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):202?228.
Michael Collins, Philipp Koehn, and Ivona Ku?cerov?a.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL, pages 531?540,
Ann Arbor, Michigan, USA.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM model 2. In Proceedings of NAACL,
pages 644?648, Atlanta, Georgia, USA.
Victoria Fossum, Kevin Knight, and Steven Abney.
2008. Using syntax to improve word alignment pre-
cision for syntax-based machine translation. In Pro-
ceedings of WMT, pages 44?52, Columbus, Ohio.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine trans-
lation. Computational Linguistics, 33(3):293?303.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of NAACL, pages 273?280, Boston,
Massachusetts, USA.
Kuzman Ganchev, Jo?ao V. Grac?a, and Ben Taskar.
2008. Better alignments = better translations? In
Proceedings of ACL, pages 986?993, Columbus,
Ohio, USA.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine trans-
lation. In Proceedings of Coling, pages 376?384,
Beijing, China.
Cyril Goutte, Kenji Yamada, and Eric Gaussier. 2004.
Aligning words using matrix factorisation. In Pro-
ceedings of ACL, pages 502?509, Barcelona, Spain.
Maria Holmqvist, Sara Stymne, Jody Foo, and Lars
Ahrenberg. 2009. Improving alignment for SMT
by reordering and augmenting the training corpus.
In Proceedings of WMT, pages 120?124, Athens,
Greece.
Maria Holmqvist, Sara Stymne, Lars Ahrenberg, and
Magnus Merkel. 2012. Alignment-based reordering
for SMT. In Proceedings of LREC, Istanbul, Turkey.
Marcin Junczys-Dowmunt and Arkadiusz Sza?. 2012.
SyMGiza++: Symmetrized word alignment models
for statistical machine translation. In International
Joint Conference of Security and Intelligent Infor-
mation Systems, pages 379?390, Warsaw, Poland.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of NAACL, pages 48?54, Edmonton, Al-
berta, Canada.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system descrip-
tion for the 2005 IWSLT speech translation evalu-
ation. In Proceedings of the International Workshop
on Spoken Language Translation, Pittsburgh, Penn-
sylvania, USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of ACL, Demonstration Session, pages
177?180, Prague, Czech Republic.
Patrik Lambert, Simon Petitrenaud, Yanjun Ma, and
Andy Way. 2012. What types of word alignment
improve statistical machine translation? Machine
Translation, 26(4):289?323.
Uri Lerner and Slav Petrov. 2013. Source-side clas-
sifier preordering for machine translation. In Pro-
ceedings of EMNLP, pages 513?523, Seattle, Wash-
ington, USA.
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li,
Ming Zhou, and Yi Guan. 2007. A probabilistic ap-
proach to syntax-based reordering for statistical ma-
chine translation. In Proceedings of the 45th Annual
Meeting of the ACL, pages 720?727, Prague, Czech
Republic.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of NAACL,
pages 104?111, New York City, New York, USA.
285
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In Proceedings of
ACL, pages 459?466, Ann Arbor, Michigan, USA.
Magnus Merkel and Jody Foo. 2007. Terminology
extraction and term ranking for standardizing term
banks. In Proceedings of the 16th Nordic Confer-
ence on Computational Linguistics, pages 349?354,
Tartu, Estonia.
Robert C. Moore. 2005. A discriminative framework
for bilingual word alignment. In Proceedings of
HLT and EMNLP, pages 81?88, Vancouver, British
Columbia, Canada.
Jan Niehues and Muntsin Kolss. 2009. A POS-based
model for long-range reorderings in SMT. In Pro-
ceedings of WMT, pages 206?214, Athens, Greece.
Franz Josef Och and Hermann Ney. 2000. A com-
parison of alignment models for statistical machine
translation. In Proceedings of Coling, pages 1086?
1090, Saarbr?ucken, Germany.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och, Christoph Tillmann, and Hermann
Ney. 1999. Improved alignment models for sta-
tistical machine translation. In Proceedings of the
Joint Conference of EMNLP and Very Large Cor-
pora, pages 20?28, College Park, Maryland, USA.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160?167, Sapporo, Japan.
Sebastian Pado and Mirella Lapata. 2005. Cross-
linguistic projection of role-semantic information.
In Proceedings of HLT and EMNLP, pages 859?866,
Vancouver, British Columbia, Canada.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318, Philadelphia, Pennsylvania,
USA.
Oana Postolache, Dan Cristea, and Constantin Or?asan.
a. 2006. Transferring coreference chains through
word alignment. In Proceedings of LREC, pages
889?892, Genoa, Italy.
Kay Rottmann and Stephan Vogel. 2007. Word re-
ordering in statistical machine translation with a
POS-based distortion model. In Proceedings of
the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation,
pages 171?180, Sk?ovde, Sweden.
Anders S?gaard and Jonas Kuhn. 2009. Empirical
lower bounds on alignment error rates in syntax-
based machine translation. In Proceedings of the
Third Workshop on Syntax and Structure in Statis-
tical Translation, pages 19?27, Boulder, Colorado,
USA.
Anders S?gaard and Dekai Wu. 2009. Empirical lower
bounds on translation unit error rate for the full class
of inversion transduction grammars. In Proceedings
of 11th International Conference on Parsing Tech-
nologies, pages 33?36, Paris, France.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP,
pages 901?904, Denver, Colorado, USA.
Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.
2010. Vs and OOVs: Two problems for translation
between German and English. In Proceedings of
WMT and MetricsMATR, pages 183?188, Uppsala,
Sweden.
Sara Stymne. 2012. Clustered word classes for pre-
ordering in statistical machine translation. In Pro-
ceedings of ROBUS-UNSUP 2012: Joint Workshop
on Unsupervised and Semi-Supervised Learning in
NLP, pages 28?34, Avignon, France.
Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proceedings of
NAACL, pages 477?487, Montr?eal, Quebec, Canada.
David Talbot, Hideto Kazawa, Hiroshi Ichikawa, Jason
Katz-Brown, Masakazu Seno, and Franz Och. 2011.
A lightweight evaluation framework for machine
translation reordering. In Proceedings of WMT,
pages 12?21, Edinburgh, Scotland.
Ben Taskar, Lacoste-Julien Simon, and Dan Klein.
2005. A discriminative matching approach to word
alignment. In Proceedings of HLT and EMNLP,
pages 73?80, Vancouver, British Columbia, Canada.
J?org Tiedemann. 2005. Optimisation of word
alignment clues. Natural Language Engineering,
11(03):279?293. Special Issue on Parallel Texts.
Stephan Vogel, Hermann Ney, and Christoph Tillman.
1996. HMM-based word alignment in statistical
translation. In Proceedings of Coling, pages 836?
841, Copenhagen, Denmark.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proceedings of Coling, pages
508?514, Geneva, Switzerland.
Kenji Yamada and Kevin Knight. 2002. A decoder for
syntax-based statistical MT. In Proceedings of ACL,
pages 303?310, Philadelphia, Pennsylvania, USA.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In Proceedings of the First International Conference
on Human Language Technology, pages 1?8, San
Diego, California, USA.
Yuqi Zhang, Richard Zens, and Hermann Ney. 2007.
Improved chunk-level reordering for statistical ma-
chine translation. In Proceedings of the Interna-
tional Workshop on Spoken Language Translation,
pages 21?28, Trento, Italy.
286
