Left-To-Right Parsing 
and Bilexical Context-Free Grammars 
Mark- Jan  Nederhof  
DFK I  
Stuhlsatzenhausweg 3 
D-66123 Saarbrficken 
Germany 
nederhofOdfk?,  de 
Giorgio Sat ta  
Dipart imento di E lettronica e Informat ica 
Universit~ di Padova 
via Gradenigo,  6 /A  
1-35131 Padova, I ta ly  
satta~dei, unipd, it 
Abst rac t  
We compare the asymptotic time complexity of 
left-to-right and bidirectional parsing techniques for 
bilexical context-free grammars, a grammar formal- 
ism that is an abstraction oflanguage models used in 
several state-of-the-art real-world parsers. We pro- 
vide evidence that left-to-right parsing cannot be re- 
alised within acceptable time-bounds if the so called 
correct-prefix property is to be ensured. Our evi- 
dence is based on complexity results for the repre- 
sentation of regular languages. 
1 Introduction 
Traditionally, algorithms for natural language pars- 
ing process the input string strictly from left to right. 
In contrast, several algorithms have been proposed 
in the literature that process the input in a bidi- 
rectional fashion; see (van Noord, 1997; Satta and 
Stock, 1994) and references therein. The issue of 
parsing efficiency for left-to-right vs. bidirectional 
methods has longly been debated. On the basis of 
experimental results, it has been argued that the 
choice of the most favourable strategy should depend 
on the grammar at hand. With respect o grammar 
formalisms based upon context-free grammars, and 
when the rules of these formalisms strongly depend 
on lexical information, (van Noord, 1997) shows that 
bidirectional strategies are more efficient than left- 
to-right strategies. This is because bidirectional 
strategies are most effective in reducing the parsing 
search space, by activating as early as possible the 
maximum number of lexical constraints available in 
the grammar. 
In this paper we present mathematical arguments 
in support of the above empirically motivated the- 
sis. We investigate a class of lexicalized grammars 
that, in their probabilistic versions, have been widely 
adopted as language models in state-of-the-art real- 
world parsers. The size of these grammars usually 
grows with the square of the size of the working lex- 
icon, and thus can be very large. In these cases, the 
primary goal in the design of a parsing algorithm 
is to achieve asymptotic time performance sublinear 
in the size of the working grammar and indepen- 
dent of the size of the lexicon. These desiderata are 
met by existing bidirectional algorithms (Alshawi, 
1996; Eisner, 1997; Eisner and Satta, 1999). In con- 
trast, we show the following two main results for 
the asymptotic time performance of left-to-right al- 
gorithms atisfying the so called correct-prefix prop- 
erty. 
? In case off-line compilation of the working ram- 
mar is not allowed, left-to-right parsing cannot 
be realised within time bounds independent of
the size of the lexicon. 
? In case polynomial-time, off-line compilation of 
the working grammar is allowed, left-to-right 
parsing cannot be realised in polynomial time, 
and independently of the size of the lexicon, un- 
less a strong conjecture based on complexity re- 
sults for the representation f regular languages 
is falsified. 
The first result implies that the well known Earley 
algorithm and related standard parsing techniques 
that do not require grammar precompilation can- 
not be directly extended to process the above men- 
tioned grammars (resp. language models) within an 
acceptable time bound. The second result provides 
evidence that well known parsing techniques as left- 
corner parsing, requiring polynomial-time prepro- 
cessing of the grammar, also cannot be directly ex- 
tended to process these formalisms within an accept- 
able time bound. 
The grammar formalisms we investigate are based 
upon context-free grammars and are called bilex- 
ical context-free grammars. Bilexical context-free 
grammars have been presented in (Eisner and Satta, 
1999) as an abstraction oflanguage models that have 
been adopted in several recent real-world parsers, 
improving state-of-the-art parsing accuracy (A1- 
shawl, 1996; Eisner, 1996; Charniak, 1997; Collins, 
1997). Our results directly transfer to all these lan- 
guage models. In a bilexical context-free grammar, 
possible arguments of a word are always specified 
along with possible head words for those arguments. 
Therefore abilexical grammar requires the grammar 
writer to make stipulations about the compatibil- 
272 
ity of particular pairs of words in particular oles, 
something that was not necessarily true of general 
context-free grammars. 
The remainder of this paper is organized as fol- 
lows. We introduce bilexical context-free grammars 
in Section 2, and discuss parsing with the correct- 
prefix property in Section 3. Our results for parsing 
with on-line and off-line grammar compilation are 
presented in Sections 4 and 5, respectively. To com- 
plete the presentation, Appendix A shows that left- 
to-right parsing in time independent of the size of 
the lexicon is indeed possible when an off-line com- 
pilation of the working grammar is allowed that has 
an exponential time complexity. 
2 B i lex ica l  context - f ree  grammars  
In this section we introduce the grammar formalism 
we investigate in this paper. This formalism, origi- 
nally presented in (Eisner and Satta, 1999), is an ab- 
straction of the language models adopted by several 
state-of-the-art eal-world parsers (see Section 1). 
We specify a non-stochastic version of the formal- 
ism, noting that probabilities may be attached to 
the rewrite rules exactly as in stochastic CFG (Gon- 
zales and Thomason, 1978; Wetherell, 1980). We 
assume that the reader is familiar with context-free 
grammars. Here we follow the notation of (Harrison, 
1978; Hopcroft and Ullman, 1979). 
A context-free grammar (CFG) is a tuple G = 
(VN, VT, P, S), where VN and VT are finite, disjoint 
sets of nonterminal and terminal symbols, respec- 
tively, S E VN is the start symbol, and P is a finite 
set of productions having the form A -+ a, where 
A E VN and a E (VN t3 VT)*. A "derives" relation, 
written ::~, is associated with a CFG as usual. We 
use the reflexive and transitive closure of =~, writ- 
ten =~*, and define L(G) accordingly. The size of a 
CFG G is defined as IGI = ~(A--+a)EP \]Aal. If every 
production in P has the form A --+ BC or A --+ a, 
for A,B ,C  E VN,a E VT, then G is said to be in 
Chomsky Normal Form (CNF). 
A CFG G = (VN, VT, P, S\[$\]) in CNF is called a 
bilexical context - f ree  grammar  if there exists a 
set VD, called the set of delexical ized nontermi -  
nals, such that nonterminals from VN are of the form 
A\[a\], consisting of A E VD and a E VT, and every 
production in P has one of the following two forms: 
(i) A\[a\] ~ B\[b\] C\[c\], a E {b, c); 
(ii) A\[a\] ~ a. 
A nonterminal A\[a\] is said to have terminal symbol 
a as its lexlcal head. Note that in a parse tree for 
G, the lexical head of a nonterminal is always "in- 
herited" from some daughter symbol (i.e., from some 
symbol in the right-hand side of a production). In 
the sequel, we also refer to the set VT as the lexicon 
of the grammar. 
A bilexical CFG can encode lexically specific pref- 
erences in the form of binary relations on lexi- 
cal items. For instance, one might specify P as 
to contain the production VP\[solve\] -+ V\[solve\] 
NP\[puzzles\] but not the production VP\[eat\] --4 
V\[eat\] NP\[puzzles\]. This will allow derivation of 
some VP constituents such as "solve two puzzles", 
while forbidding "eat two puzzles". See (Eisner and 
Satta, 1999) for further discussion. 
The cost of this expressiveness is a very large 
grammar. Indeed, we have IGI = O(\]VD\[ 3" IVT\[2), 
and in practical applications \]VTI >> IVDI > 1. Thus, 
the grammar size is dominated in its growth by the 
square of the size of the working lexicon. Even if we 
conveniently group lexical items with distributional 
similarities into the same category, in practical ap- 
plications the resulting rammar might have several 
thousand productions. Parsing strategies that can- 
not work in sublinear time with respect o the size of 
the lexicon and with respect o the size of the whole 
input grammar are very inefficient in these cases. 
3 Cor rect -pre f ix  p roper ty  
So called left-to-right strategies are standa~dly 
adopted in algorithms for natural language pars- 
ing. Although intuitive, the notion of left-to-right 
parsing is a concept with no precise mathematical 
meaning. Note that in fact, in a pathological way, 
one could read the input string from left-to-right, 
storing it into some data structure, and then per- 
form syntactic analysis with a non-left-to-right strat- 
egy. In this paper we focus on a precise definition 
of left-to-right parsing, known in the literature as 
correct-prefix property parsing (Sippu and Soisalon- 
Soininen, 1990). Several algorithms commonly used 
in natural anguage parsing satisfy this property, as 
for instance Earley's algor!thm (Earley, 1970), tab- 
ular left-corner and PLR parsing (Nederhof, 1994) 
and tabular LR parsing (Tomita, 1986). 
Let VT be some alphabet. A generic string over VT 
is denoted as w = al " -an ,  with n _> 0 and ai E VT 
(1 < i < n); in case n = 0, w equals the empty 
string e. For integers i and j with 1 < i < j < n, we 
write w\[i,j\] to denote string aiai+l"  .aj; if i > j,  
we define w\[i, j\] = c. 
Let G -- (VN,VT,P,S) be a CFG and let w = 
al ... an with n _> 0 be some string over VT. A rec- 
ognizer  for the CFG class is an algorithm R that, 
on input (G,w), decides whether w E L(G). We 
say that R satisfies the cor rect -pre f ix  p roper ty  
(CPP) if the following condition holds. Algorithm 
R processes the input string from left-to-right, "con- 
suming" one symbol ai at a time. If for some i, 
0 < i < n, the set of derivations in G having the 
form S ~*  w\[1, i\]7, 7 E (VN U VT)*, is empty, then 
R rejects and halts, and it does so before consuming 
symbol ai+l, if i < n. In this case, we say that R 
273 
has detected an error at position i in w. Note that 
the above property forces the recognizer to do rele- 
vant computation for each terminal symbol that is 
consumed. 
We say that w\[1,i\] is a cor rect -pre f ix  for a lan- 
guage L if there exists a string z such that w\[1,i\]z E 
L. In the natural language parsing literature, the 
CPP is sometimes defined with the following condi- 
tion in place of the above. If for some i, 0 < i < n, 
w\[1, if is not a correct prefix for L(G), then R rejects 
and halts, and it does so before consuming symbol 
ai+i,~ if i < n. Note that the latter definition asks 
for a stronger condition, and the two definitions are 
equivalent only in case the input grammar G is re- 
duced, i While the above mentioned parsing algo- 
rithms satisfy the former definition of CPP, they do 
not satisfy the latter. Actually, we are not aware of 
any practically used parsing algorithm that satisfies 
the latter definition of CPP. 
One needs to distinguish CPP parsing from Some 
well known parsing algorithms in the literature that 
process ymbols in the right-hand sides of each gram- 
mar production from left to right, but that do not 
exhibit any left-to-right dependency between differ- 
ent productions. In particular, processing of the 
right-hand side of some production may be initi- 
ated at some input position without consultation of 
productions or parts of productions that may have 
been found to cover parts of the input to the left 
of that position. These algorithms may also consult 
input symbols from left to right, but the processing 
that takes place to the right of some position i does 
not strictly depend on the processing that has taken 
place to the left of i. Examples are pure bottom-up 
methods, such as left-corner parsing without top- 
down filtering (Wiren, 1987). 
Algorithms that do satisfy the CPP make use of 
some form of top-down prediction. Top-down pre- 
diction can be implemented at parse-time as in the 
case of Earley's algorithm by means of the "predic- 
tor" step, or can be precompiled, as in the case of 
left-corner parsing (Rosenkrantz and Lewis, 1970), 
by means of the left-corner relation, or as in the case 
of LR parsers (Sippu and Soisalon-Soininen, 1990), 
through the closure function used in the construc- 
tion of LR states. 
4 Recognit ion without 
precompilat ion 
In this section we consider recognition algorithms 
that do not require off-line compilation of the input 
grammar. Among algorithms that satisfy the CPP, 
the most popular example of a recognizer that does 
i A context-free grammar G is reduced if every nonterminal 
of G can be part of at least one derivation that rewrites the 
start symbol into some string of terminal symbols. 
not require grammar precompilation is perhaps Ear- 
ley's algorithm (Earley, 1970). We show here that 
methods in this family cannot be extended to work 
in time independent of the size of the lexicon, in 
contrast with bidirectional recognition algorithms. 
The result presented below rests on the follow- 
ing, quite obvious, assumption. There exists a con- 
stant c, depending on the underlying computation 
model, such that in k > 0 elementary computation 
steps any recognizer can only read up to c.  k pro- 
ductions from set P. In what follows, and without 
any loss of generality, we assume c = 1. Apart from 
this assumption, no other restriction is imposed on 
the representation f the input grammar or on the 
access to the elements of sets VN, VT and P. 
Theorem 1 Let f be any function of two variables 
defined on natural numbers. No recognizer for bilexi- 
cal context-free grammars that satisfies the CPP can 
run on input (G, wl in an amount of time bounded 
by f(\[VDI, \[W\[), where VD is the set of delexicatized 
nonterminals of G. 
Proof. Assume the existence of a recognizer R sat- 
isfying the CPP and running in I(IVDI, IwL) steps or 
less. We show how to derive a contradiction. 
Let q >_ 1 be an integer. Define a bilexical CFG 
G a = (Vr~ , V~, Pq, d\[bi\]) where V~ contains q + 2 
distinct symbols {bi,. . . ,bq+2} and 
V~ = {A\[b,\]l l<i<q+l}u{T\[b\]lbeV~}, 
and where set pa contains all and only the following 
productions: 
(i) A\[b,\] --+ A\[b,+i\] T\[b,\], 1 < i < q; 
(ii) A\[bq+i\] -+ T\[bq+2\] T\[bq+l\]; 
(iii) T\[b\] -+ b, b E V~. 
Productions in (i) are called bridging productions. 
Note that there are q bridging productions in Gq. 
Also, note that V~ = {A,T} does not depend on 
the choice of q. Thus, we will simply write VD. 
Choose q > max{f(IVD\[,2), l  }. On input 
(Gq, bq+2bq+i), R does not detect any error at posi- 
tion 1, that is after having read the first symbol bq+2 
of the input string. This is because A\[bl\] ~* bq+2~/ 
with 3' =- T\[ba+i\]T\[ba\]T\[bq- i \ ] '"T\[bi \]  is a valid 
derivation in G. Since R executes no more than 
f(IVD\] ,2) steps, from our assumption that reading 
a production takes unit time it follows that there 
must be an integer k, 1 < k < q, such that bridging 
production A\[bk\] --+ A\[bk+i\] T\[bk\] is not read from 
Gq. Construct hen a new grammar GI~ by replacing 
in Gq the production A\[bk\] --+ A\[bk+l\] T\[bk\] with 
the new production A\[bk\] --+ T\[bk\] A\[bk+i\], leaving 
everything else unchanged. It follows that, on in- 
put (G~, ba+2ba+i), R behaves exactly as before and 
does not detect any error at position 1. But this is 
274 
a contradiction, since there is no derivation in G~ of 
the form A\[bl\] =~* bq+2"Y, 7 E (VN U VT)*, as can be 
easily verified. ? 
We can use the above result in the comparison 
of left-to-right and bidirectional recognizers. The 
recognition of bilexical context-free languages can 
be carried out by existing bidirectional algorithms 
in time independent of the size of the lexicon and 
without any precompilation of the input bilexical 
grammar. For instance, the algorithms presented 
in (Eisner and Satta, 1999) allow recognition i  time 
O(IVDI 3 IwI4). 2 Theorem 1 states that this time 
bound cannot be met if we require the CPP and if 
the input grammar is not precompiled. In the next 
section, we will consider the possibility that the in- 
put grammar is in a precompiled form. 
5 Recogn i t ion  w i th  precompi la t ion  
In this section we consider recognition algorithms 
that satisfy the CPP and allow off-line, polynomial- 
time compilation of the working grammar. We focus 
on a class of bilexical context-free grammars where 
recognition requires the stacking of a number of un- 
resolved lexical dependencies that is proportional to 
the length of the input string. We provide evidence 
that the above class of recognizers perform much less 
efficiently for these grammars than existing bidirec- 
tional recognizers. 
We assume that the reader is familiar with the 
notions of deterministic and nondeterministic finite 
automata. We follow here the notation in (Hopcroft 
and Ullman, 1979). A nondeterministic finite au- 
tomaton (FA) is a tuple M = (Q, E, 5, q0, F), where 
Q and P. are finite, disjoint sets of state and alphabet 
symbols, respectively, qo E Q and F _C Q are the ini- 
tial state and the set of final states, respectively, and 
is a total function mapping Q x ~ to 2 Q, the power- 
set of Q. Function 5 represents he transitions of the 
automaton. Given a string w = al " "an ,  n > O, an 
accept ing  computat ion  in M for w is a sequence 
qo, a l ,q l ,a2,q2 . . . .  ,an,q,, such that qi E 5(q i - l ,a i )  
for 1 < i  < n, and q~ E F. The languageL(M) is 
the set of all strings in E* that admit at least one 
accepting computation in M. The size of M is de- 
fined as \]M\] = ~qeQ,ae~ I~(q,a)l. The automaton 
M is deterministic f, for every q E Q and a E ~, we 
have IS(q, a)\] = 1. 
We call quas i -determin izer  any algorithm A 
that satisfies the following two conditions: 
1. A takes as input a nondeterministic FA M --= 
(Q, ~, 5, qo, F) and produces as output a device 
DM that, when given a string w as input, de- 
cides whether w E L(M); and 
2More precisely, the running time for these algorithms is 
O(IVDI 3 Iw\[3min{\[VT\[, \[w\[}). In cases of practical interest, 
we always have Iw\[ < IVT\[. 
2. there exists a polynomial PA such that every 
DM runs in an amount of time bounded by 
PA(Iwl). 
We remark that, given a nondeterministic FA M 
specified as above, known algorithms allow simula- 
tion of M on an input string w in time O( IM I IwI) 
(see for instance (Aho et al, 1974, Thin. 9.5) 
or (Sippu and Soisalon-Soininen, 1988, Thm. 3.38)). 
In contrast, a quasi-determinizer produces a device 
that simulates M in an amount of time independent 
of the size of M itself. 
A standard example of a quasi-determinizer is the 
so called power-set construction, used to convert 
a nondeterministic FA into a language-equivalent 
deterministic FA (see for instance (Hopcroft and 
Ullman, 1979, Thin. 2.1) or (Sippu and Soisalon- 
Soininen, 1988, Thm. 3.30)). In fact, there exist 
constants c and d such that any deterministic FA 
can be simulated on input string w in an amount of 
time bounded by c \]w I + d. This requires function 
to be stored as a IQ\] x \]El, 2-dimensional rray with 
values in Q. This is a standard representation for 
automata-like structures; see (Gusfield, 1997, :Sect. 
6.5) for discussion. 
We now pose the question of the time efficiency 
of a quasi-determinizer, and consider the amount of 
time needed in the construction of DM. In (Meyer 
and Fisher, 1971; Stearns and Hunt, 1981) it is 
shown that there exist (infinitely many) nonde- 
terministic FAs with state set Q, such that any 
language-equivalent deterministic FA must have at 
least 2 IQ} states. This means that the power-set con- 
struction cannot work in polynomial time in the size 
of the input FA. Despite of much effort, no algo- 
rithm has been found, up to the authors' knowledge, 
that can simulate a nondeterministic FA on an input 
string w in linear time in' Iwl and independently of
IMI, if only polynomial-time precompilation of M 
is allowed. Even in case we relax the linear-time re- 
striction and consider ecognition of w in polynomial 
time, for some fixed polynomial, it seems unlikely 
that the problem can be solved if only polynomial- 
time precompilation of M is allowed. Furthermore, 
if we consider precompilation of nondeterministic 
FAs into "partially determinized" FAs that would 
allow recognition in polynomial (or even exponen- 
tial) time in Iw\], it seems unlikely that the analysis 
required for this precompilation could consider less 
than exponentially many combinations ofstates that 
may be active at the same time for the original non- 
deterministic FA. Finally, although more powerful 
formalisms have been shown to represent some regu- 
lar languages much more succinctly than FAs (Meyer 
and Fisher, 1971), while allowing polynomial-time 
parsing, it seem unlikely that this could hold for reg- 
ular languages in general. 
275 
Conjecture There is no quasi-determinizer that 
works in polynomial time in the size of the input 
automaton. 
Before turning to our main result, we need to 
develop some additional machinery. Let M = 
(Q,E,6, qo, F) be a nondeterministic FA and let 
w = a l . . -an  E L(M),  where n > 0. Let 
qo, al , ql , . . . , an, qn be an accepting computation for 
w in M, and choose some symbol $ ? E. We can 
now encode the accepting computation as 
($, q0)(al, ql) ? ? ? (an, qa) 
where we pair alphabet symbols to states, prepend- 
ing $ to make up for the difference in the number 
of alphabet symbols and states. We now provide 
a construction that associates M with a bilexical 
CFG GM. Strings in L(GM) are obtained by pair- 
ing strings in L(M)  with encodings of their accepting 
computations ( ee below for an example). 
Def in i t ion 1 Let M = (Q,E, J ,  qo,F) be a nonde- 
terministic FA. Choose two symbols $, # ~ E, and 
let A = {(a,q) I a e EU{$},  q ? O}. A bilexi- 
cat CFG GM -- (VN, VT, e~ C\[($, qo)\]) is specified as 
follows: 
(i) vN = {TIff I ~ ? VT} U {C\[~\],C'\[.\] I ~ ? a} ;  
(ii) V T = A U ~ U {}; 
(iii) P contains all and only the following produc- 
tions: 
(a) for each a ? VT, 
T\[a\] - ,  a; 
(b) for each (a,q),(a',q*) ? A such that q* ? 
5(q,a'), 
C\[(a, q)\] ~ C'\[(a', 4)\] T\[(a, q)\]; 
(C) for each (a,q) ? A, 
C'\[(a,q)\] ~ T\[a\] C\[(a,q)\]; 
(d) .for each (a, q) ? A such that q ? F, 
C\[(a, q)\] - ,  T\[#\] T\[(a, q)\]. 
We give an example of the above construc- 
tion. Consider an automaton M and a string 
w = ala2a3 such that w ? L(M).  Let 
($,qo)(al,ql)(a2,q2)(a3,q3) be the encoding of an 
accepting computation in M for w. Then the 
string ala2a3~(a3, q3)(a2, q2)(al, ql)($, qo) belongs 
to L(GM). The tree depicted in Figure I represents 
a derivation in GM of such a string. 
The following fact will be used below. 
Lemma 1 For each w ? E*, w# is a correct-prefix 
for L(GM) if and only if w ? L (M) .  
Outline of the proof. We claim the following 
fact. For each k > 0, a l ,a2 , . . . ,ak  ? ~ and 
qo, ql , . . . , qk ? Q we have 
qi ? 6(qi- l ,ai),  for all i (1 < i < k), 
c\[($,q0)l 
C'\[(al, ql)\] T\[($, q0)\] 
T\[a,\] C\[(al,ql)\] ($, q0) 
al C'\[(a2, q2)\] r \ [ (a l ,  ql)l 
T\[a2\] C\[(a2, q2)\] (al, qz ) 
a~ c'\[(a~,q~)\] T\[(a~,q2)\] 
T\[a3\] C\[(a3, q3)\] (a2, (12) 
a~ T\[#1 T\[(a~,q3)\] 
I I 
# (a3, q3) 
Figure I: A derivation in GM for string 
ala2a3#(a3, qs)(a2, q2)(ax, ql)($, q0). 
ff and only ff 
C\[($, qo)\] ~*  
a l ' . "  akg\[Cak, qk)\](at?-l, qk-x) ? ? ? ($, qo). 
The claim can be proved by induction on k, using 
productions (a) to (c) from Definition 1. 
Let R denote the reverse operator on strings. 3
From the above claim and using production (d) from 
Definition 1, one can easily show that 
L(GM) = {w#u \[ w E L (M) ,  u R encodes an 
accepting computation for w}. 
The lemma directly follows from this relation. ? 
We can now provide the main result of this sec- 
tion. To this end, we refine the definition of rec- 
ognizer presented in Section 3. A recognizer for the 
CFG class is an algorithm R that has random access 
to some data structure C(G) obtained by means of 
some off-line precompilation of a CFG G. On in- 
put w, which is a string on the terminal symbols of 
G, R decides whether w E L(G). The definition of 
the CPP extends in the obvious way to recognizers 
working with precompiled grammars. 
Theorem 2 Let p be any polynomial in two vari- 
ables. 1\] the conjecture about quasi-determinizers 
holds true, then no recognizer exists that 
3Note that R does not affect individual symbols in a string. 
Thus (a, q)R = (a, q). 
276 
(i) has random access to data structure C(G) pre- 
compiled from a bilexical CFG G in polynomial 
time in IGI, 
(ii) runs in an amount of time bounded by 
p(IVDI, Iwl), where VD is the set of delexicalized 
nonterminals of G and w is the input string, 
and 
(iii) satisfies the CPP. 
Proo/. Assume there exists a recognizer R that sat- 
isfies conditions (i) to (iii) in the statement of the 
theorem. We show how this entails that the conjec- 
ture about quasi-determinizers is false. 
We use algorithm R to specify a quasi- 
determinizer A. Given a nondeterministic FA M, 
A goes through the following steps. 
1. A constructs grammar GM as in Definition 1. 
2. A precompiles GM as required by R, producing 
data structure C(GM). 
3. A returns a device DM specified as follows. 
Given a string w as input, DM runs R on string 
w~. If R detects an error at any position i, 
0 < i < Iw#l, then DM rejects and halts, oth- 
erwise DM accepts and halts. 
From Lemma 1 we have that DM accepts w if and 
only if w E L(M). Since R runs in time P(\]VDI, Iwl) 
and since GM has a set of delexicalized nonterminals 
independent of M, we have that there exists a poly- 
nomial PA such that every DM works in an amount 
of time bounded by pA(IWl). We therefore conclude 
that A is a quasi-determinizer. 
It remains to be shown that A works in polyno- 
mial time in IMI. Step 1 can be carried out in time 
O(IM\[). The compilation at Step 2 takes polynomial 
time in IGM\], following our hypotheses on R, and 
hence polynomial time in IMI, since IGMI = O(IMI). 
Finally, the construction of DM at Step 3 can easily 
be carried out in time O(IMI) as well. ? 
In addition to Theorem 1, Theorem 2 states that, 
even in case the input grammar is compiled off- 
line and in polynomial time, we cannot perform 
CPP recognition for bilexical context-free grammars 
in time polynomial in the grammar and the input 
string but independent of the lexicon size. This 
is true with at least the same evidence that sup- 
ports the conjecture on quasi-determinizers. Again, 
this should be contrasted with the time performance 
of existing bidirectional algorithms, allowing recog- 
nition for bilexical context-free grammars in time 
O(IVDI 3 Iwl ). 
In order to complete our investigation of the above 
problem, in Appendix A we show that, when we drop 
the polynomial-time r striction on the grammar pre- 
compilation, it is indeed possible to get rid of any 
IVT\] factor from the running time of the recognizer. 
6 Conc lus ion  
Empirical results presented in the literature show 
that bidirectional parsing strategies can be more 
time efficient in cases of grammar formalisms whose 
rules are specialized for one or more lexical items. 
In this paper we have provided an original mathe- 
matical argument in favour of this thesis. Our re- 
sults hold for bilexical context-free grammars and 
directly transfer to several language models that can 
be seen as stochastic versions of this formalism (see 
Section 1). We perceive that these results can be ex- 
tended to other language models that properly em- 
bed bilexical context-free grammars, as for instance 
the more general history-based models used in (Rat- 
naparkhi, 1997) and (Chelba and Jelinek, 1998). We 
leave this for future work. 
Acknowledgements 
We would like to thank Jason Eisner and Mehryar 
Mohri for fruitful discussions. The first author is 
supported by the German Federal Ministry of Edu- 
cation, Science, Research and Technology (BMBF) 
in the framework of the VERBMOBIL Project under 
Grant 01 IV 701 V0, and was employed at AT&T 
Shannon Laboratory during a part of the period this 
paper was written. The second author is supported 
by MURST under project PRIN." BioInformatica e
Ricerca Genomica and by University of Padua, un- 
der project Sviluppo di Sistemi ad Addestramento 
Automatico per l'Analisi del Linguaggio Naturale. 
References 
A. V. Aho, J. E. Hopcroft, and J. D. Ullman. 1974. 
The Design and Analysis of Computer Algorithms. 
Addison-Wesley, Reading, MA. 
H. Alshawi. 1996. Head automata and bilingual 
tiling: Translation with minimal representations. 
In Proc. of the 3~ th ACL, pages 167-176, Santa 
Cruz, CA. 
E. Charniak. 1997. Statistical parsing with a 
context-free grammar and word statistics. In 
Proc. of AAAI-97, Menlo Park, CA. 
C. Chelba and F. Jelinek. 1998. Exploiting syntactic 
structure for language modeling. In Proc. o\] the 
36 th A CL, Montreal, Canada. 
M. Collins. 1997. Three generative, lexicalised mod- 
els for statistical parsing. In Proc. of the 35 th 
ACL, Madrid, Spain. 
J. Earley. 1970. An efficient context-free parsing al- 
gorithm. Communications of the Association for 
Computing Machinery, 13(2):94-102. 
J. Eisner and G. Satta. 1999. Efficient parsing for 
bilexical context-free grammars and head automa- 
ton grammars. In Proc. of the 37 th A CL, pages 
457-464, College Park, Maryland. 
277 
J. Eisner. 1996. An empirical comparison of proba- 
bility models for dependency grammar. Technical 
Report IRCS-96-11, IRCS, Univ. of Pennsylvania. 
J. Eisner. 1997. Bilexical grammars and a cubic- 
time probabilistic parser. In Proceedings of the 
~th Int. Workshop on Parsing Technologies, MIT, 
Cambridge, MA, September. 
R. C. Gonzales and M. G. Thomason. 1978. Syntac- 
tic Pattern Recognition. Addison-Wesley, Read- 
ing, MA. 
D. Gusfield. 1997. Algorithms on Strings, Trees and 
Sequences. Cambridge University Press, Cam- 
bridge, UK. 
M. A. Harrison. 1978. Introduction to Formal Lan- 
guage Theory. Addison-Wesley, Reading, MA. 
J. E. Hopcroft and J. D. Ullman. 1979. Introduc- 
tion to Automata Theory, Languages and Compu- 
tation. Addison-Wesley, Reading, MA. 
A. R. Meyer and M. J. Fisher. 1971. Economy of de- 
scription by automata, grammars and formal sys- 
tems. In 12th Annual Symp. on Switching and Au- 
tomata Theory, pages 188-190, New York. IEEE. 
M.-J. Nederhof and G. Satta. 1996. Efficient abular 
LR parsing. In Proc. of the 3~ th A CL, pages 239- 
246, Santa Cruz, CA. 
M.-J. Nederhof. 1994. An optimal tabular parsing 
algorithm. In Proc. of the 32 ~ ACL, pages 117- 
124, Las Cruces, New Mexico. 
A. Ratnaparkhi. 1997. A linear observed time sta- 
tistical parser based on maximum entropy mod- 
els. In Second Conference on Empirical Methods 
in Natural Language Processing, Brown Univer- 
sity, Providence, Rhode Island. 
D. J. Rosenkrantz and P. M. Lewis. 1970. Determin- 
istic left corner parsing. In IEEE Conf. Record 
11 th Annual Symposium on Switching and Au- 
tomata Theory, pages 139-152. 
G. Satta and O. Stock. 1994. Bidirectional context- 
free grammar parsing for natural anguage pro- 
cessing. Artificial Intelligence, 69:123-164. 
S. Sippu and E. Soisalon-Soininen. 1988. Pars- 
ing Theory: Languages and Parsing, volume 1. 
Springer-Verlag, Berlin, Germany. 
S. Sippu and E. Soisalon-Soininen. 1990. Parsing 
Theory: LR(k) and LL(k) Parsing, volume 2. 
Springer-Verlag, Berlin, Germany. 
R. E. Stearns and H. B. Hunt. 1981. On the equiva- 
lence and containment problem for unambiguous 
regular expressions, grammars, and automata. In 
22nd Annual Syrup. on Foundations of Computer 
Science, pages 74-81, New York. IEEE. 
M. Tomita. 1986. Ej~icient Parsing \]or Natural Lan- 
guage. Kluwer, Boston, Mass. 
G. van Noord. 1997. An efficient implementation f 
the head-corner parser. Computational Linguis- 
tics, 23(3):425-456. 
C. S. Wetherell. 1980. Probabilistic languages: A 
review and some open questions. Computing Sur- 
veys, 12(4):361-379. 
M. Wiren. 1987. A comparison of rule-invocation 
strategies in parsing. In Proc. of the 3 ~d EACL, 
pages 226-233, Copenhagen, Denmark. 
A Recogn i t ion  in  t ime independent  
o f  the  lex icon  
In Section 5 we have shown that it is unlikely that 
correct-prefix property parsing for a bilexical CFG 
can be carried out in polynomial time and indepen- 
dently of the lexicon size, when only polynomial- 
time off-line compilation of the grammar is allowed. 
To complete our presentation, we show here that 
correct-prefix property parsing in time independent 
of the lexicon size is indeed possible if we spend ex- 
ponential time on grammar precompilation. 
We first consider tabular LR parsing (Tomita, 
1986), a technique which satisfies the correct-prefix 
property, and apply it to bilexical CFGs. Our pre- 
sentation relies on definitions from (Nederhof and 
Satta, 1996). Let w E V~ be some input string. A 
property of LR parsing is that any state that can be 
reached after reading prefix w\[1,j\], j < \]w\], must be 
of the form 
goto(goto(. . (goto( q~n, X1),...), Xm-1), Xm) 
where q~ is the initial LR state, and X I , . . . ,  X,~ are 
terminals or nonterminals such that X I ' . 'Xm o*  
w\[1, if. For a bilexical CFG, each X~ is of the form b~ 
or of the form B~\[b~\], where bl , . . . ,  bm is some subse- 
quence of wIl , j  \]. This means that there are at most 
(2+ IVDI)" distinct states that can be reached by the 
recognizer, apart from qin. In the algorithm, the tab- 
ulation prevents repeated manipulation ofstates for 
a triple of input positions, leading to a time complex- 
ity of O(n 3 IvDIn), where n = Iwl. Hence, when we 
apply precompilation f the grammar, we can carry 
out recognition i  time exponential in the length of 
the input string, yet independent ofthe lexicon size. 
Note however that the precompilation for LR pars- 
ing takes exponential time. 
The second algorithm with the CPP we will con- 
sider can be derived from Earley's algorithm (Ear- 
ley, 1970). For this new recognizer, we achieve a 
time complexity completely independent of the size 
of the whole grammar, not merely independent of
the size of the lexicon as in the case of tabular LR 
parsing. Furthermore, the input grammar can be 
any general CFG, not necessarily a bilexical one. In 
terms of the length of the input, the complexity is
polynomial rather than exponential. 
Earley's algorithm is outlined in what follows, 
with minor modifications with respect o its origi- 
nal presentation. An i tem is an object of the form 
278 
\[A -+ a ,, j3\], where A -~ a~ is a production from 
the grammar. The recognition algorithm consists in 
an incremental construction of a (n + 1) x (n + 1), 
2-dimensional table T, where n is the length of the 
input string. At each stage, each entry T\[i,j\] in 
the table contains a set of items, which is initially 
the empty set. After an initial item is added to en- 
try T\[0, 0\] in the table, other items in other entries 
are derived from it, directly or indirectly, using three 
steps called predictor, scanner and completer. When 
no more new items can be derived, the presence of 
a final item in entry T\[0, n\] indicates whether the 
input is recognized. 
The recognition process can be precompiled, 
based on the observation that for any grammar the 
set of all possible items is finite, and thereby all po- 
tential contents of T's entries can be enumerated. 
Furthermore, the dependence of entries on one an- 
other is not cyclic; one item in T\[i, j\] may be derived 
from a second item in the same entry, but it is not 
possible that, for example, an item in T\[i,j\] is de- 
rived from an item in T\[i',j'\], with (i,j) ~ (i',j'), 
which is in turn derived from an item in T\[i,j\]. 
A consequence is that entries can be computed 
in a strict order, and an operation that involves the 
combination of, say, the items from two entries T\[i, j\] 
and T\[j, k\] by means of the completer step can be 
implemented by a simple table lookup. More pre- 
cisely, each set of items is represented by an atomic 
state, and combining two sets of items according 
to the completer step is implemented by indexing 
a 2-dimensional rray by the two states representing 
those two sets, yielding a third state representing 
the resulting set of items. Similarly, the scanner 
and predictor steps and the union operation on sets 
of items can all be implemented by table lookup. 
The time complexity of recognition can straight- 
forwardly be shown to be (9(n3), independent of 
the size of the grammar. However, massive pre- 
compilation is involved in enumerating all possi- 
ble sets of items and precomputing the operations 
on them. The motivation for discussing this algo- 
rithm is therefore purely theoretical: it illustrates 
the unfavourable complexity properties that The- 
orem 2, together with the conjecture about quasi- 
determinizers, attributes to the recognition problem 
if the correct-prefix property is to be ensured. 
279 
Practical Experiments with Regular 
Approximation of Context-Free Languages 
Mark- Jan  Nederhof "  
German Research Center 
Intelligence 
for Artificial 
Several methods are discussed that construct afinite automaton given a context-free grammar, 
including both methods that lead to subsets and those that lead to supersets of the original 
context-free language. Some of these methods of regular approximation are new, and some others 
are presented here in a more refined form with respect to existing literature. Practical experiments 
with the different methods of regular approximation are performed for spoken-language input: 
hypotheses from a speech recognizer are filtered through afinite automaton. 
1. Introduction 
Several methods of regular approximation of context-free languages have been pro- 
posed in the literature. For some, the regular language is a superset of the context-free 
language, and for others it is a subset. We have implemented a large number of meth- 
ods, and where necessary, refined them with an analysis of the grammar. We also 
propose a number of new methods. 
The analysis of the grammar is based on a sufficient condition for context-free 
grammars to generate regular languages. For an arbitrary grammar, this analysis iden- 
tifies sets of rules that need to be processed in a special way in order to obtain a regular 
language. The nature of this processing differs for the respective approximation meth- 
ods. For other parts of the grammar, no special treatment is needed and the grammar 
rules are translated to the states and transitions of a finite automaton without affecting 
the language. 
Few of the published articles on regular approximation have discussed the appli- 
cation in practice. In particular, little attention has been given to the following two 
questions: First, what happens when a context-free grammar grows in size? What is 
then the increase of the sizes of the intermediate r sults and the obtained minimal de- 
terministic automaton? Second, how "precise" are the approximations? That is, how 
much larger than the original context-free language is the language obtained by a 
superset approximation, and how much smaller is the language obtained by a subset 
approximation? (How we measure the "sizes" of languages in a practical setting will 
become clear in what follows.) 
Some considerations with regard to theoretical upper bounds on the sizes of the 
intermediate r sults and the finite automata have already been discussed in Nederhof 
(1997). In this article we will try to answer the above two questions in a practical set- 
ring, using practical linguistic grammars and sentences taken from a spoken-language 
corpus. 
? DFKI, Stuhlsatzenhausweg 3, D-66123 Saarbriicken, Germany. E-mail: nederhof@dfki.de 
? 2000 Association for Computational Linguistics 
Computational Linguistics Volume 26, Number 1 
The structure of this paper is as follows: In Section 2 we recall some standard 
definitions from language theory. Section 3 investigates a sufficient condition for a 
context-free grammar to generate a regular language. We also present he construction 
of a finite automaton from such a grammar. In Section 4, we discuss several meth- 
ods to approximate the language generated by a grammar if the sufficient condition 
mentioned above is not satisfied. These methods can be enhanced by a grammar trans- 
formation presented in Section 5. Section 6 compares the respective methods, which 
leads to conclusions in Section 7. 
2. Preliminaries 
Throughout this paper we use standard formal language notation (see, for example, 
Harrison \[1978\]). In this section we review some basic definitions. 
A context-free grammar G is a 4-tuple (G ,N,P ,S) ,  where G and N are two finite 
disjoint sets of terminals and nonterminals, respectively, S E N is the start symbol, and 
P is a finite set of rules. Each rule has the form A ~ ~ with A E N and ~ E V*, where 
V denotes N U ~. The relation ~ on N x V* is extended to a relation on V* x V* as 
usual. The transitive and reflexive closure of 4 is denoted by 4* .  
The language generated by G is given by the set {w E G* I S 4"  w}. By definition, 
such a set is a context-free language. By reduction of a grammar we mean the elimi- 
nation from P of all rules A 4 ,7  such that S --+* c~Afl --* a"/fl 7"  w does not hold for 
any~, f lEV*andwEG* .  
We generally use symbols A, B, C . . . .  to range over N, symbols a, b, c , . . .  to range 
over ~, symbols X, Y, Z to range over V, symbols a, fl,"7 . . . .  to range over V* and 
symbols v, w, x . . . .  to range over G* We write ? to denote the empty string. 
A rule of the form A --, B is called a unit rule. 
A (nondeterministic) finite automaton .T is a 5-tuple (K, G, A, s, F), where K is a 
finite set of states, of which s is the initial state and those in F c K are the final states, 
is the input alphabet, and the transition relation A is a finite subset of K x Z* x K. 
We define a configuration to be an element of K x G*. We define the binary relation 
t- between configurations as: (q, vw) F- (q', w) if and only if (q, v, q') E A. The transitive 
and reflexive closure of ~- is denoted by F-*. 
Some input v is recognized if (s, v) t-* (q, c), for some q E F. The language accepted 
by .T is defined to be the set of all strings v that are recognized. By definition, a 
language accepted by a finite automaton is called a regular language. 
3. Finite Automata in the Absence of Self-Embedding 
We define a spine in a parse tree to be a path that runs from the root down to some 
leaf. Our main interest in spines lies in the sequences of grammar symbols at nodes 
bordering on spines. 
A simple example is the set of parse trees such as the one in Figure 1, for a 
grammar of palindromes. It is intuitively clear that the language is not regular: the 
grammar symbols to the left of the spine from the root to E "communicate" with those 
to the right of the spine. More precisely, the prefix of the input up to the point where 
it meets the final node c of the spine determines the suffix after that point, in such 
a way that an unbounded quantity of symbols from the prefix need to be taken into 
account. 
A formal explanation for why the grammar may not generate a regular language 
relies on the following definition (Chomsky 1959b): 
18 
Nederhof Experiments with Regular Approximation 
S--' ,a S a 
S-->b S b 
S---~ ~ 
S 
a S a 
Y\  
b S b 
Figure 1 
Grammar of palindromes, and a parse tree. 
Definition 
A grammar is sel f -embedding if there is some A E N such that A --+* c~Afl, for some 
a?eandf l?e .  
If a grammar is not self-embedding, this means that when a section of a spine in 
a parse tree repeats itself, then either no grammar symbols occur to the left of that 
section of the spine, or no grammar symbols occur to the right. This prevents the 
"unbounded communication" between the two sides of the spine exemplified by the 
palindrome grammar. 
We now prove that grammars that are not self-embedding generate regular lan- 
guages. For an arbitrary grammar, we define the set of reeursive nonterminals as: 
B 
N = {A E N I  Ag\]} 
m 
We determine the partition N" of N consisting of subsets N1, N2 . . . .  , Nk, for some k > 0, 
of mutual ly recursive nonterminals: 
H = {N1,N2 . . . .  ,Nk} 
NIUN2U. . .UNk=N 
Vi\[Ni 7L O\] 
Vi, j\[i  j =~ Ni N Nj = 0\] 
and for all A, B E N: 
3i\[A E Ni AB  @ Nil - ~oQ, fll, O~2,fl2\[a ---~* alBfll AB  ---+* c?2Afl2\], 
We now define the function recursive from N" to the set {left, right, self, cyclic}. For 
l< iKk :  
recursive(Ni) -- left, if ~LeftGenerating(Ni) 
= right, if LeftGenerating(Ni) 
-- self, if LeftGenerating(Ni) 
= cyclic, if -,LeftGenerating(Ni) 
/x RightGenerating(Ni) 
/x ~RightGenerating(Ni) 
/x RightGenerating(Ni) 
/x ~RightGenerating( Ni ) 
where 
LeftGenerating(Ni) = 3(A --* aBfl) E P\[A E Ni A B E Ni /X ~ 7~ e\] 
RightGenerating(Ni) = 3(A --* aBfl) E P\[A E Ni /x B E Ni /~ fl  ?\] 
19 
Computational Linguistics Volume 26, Number 1 
When recursive(Ni) = left, Ni consists of only left-recursive nonterminals, which does 
not mean it cannot also contain right-recursive nonterminals, but in that case right 
recursion amounts to application of unit rules. When recursive(Ni) = cyclic, it is only 
such unit rules that take part in the recursion. 
That recursive(Ni) = self, for some i, is a sufficient and necessary condition for the 
grammar to be self-embedding. Therefore, we have to prove that if recursive(Ni) E 
{left, right, cyclic}, for all i, then the grammar generates a regular language. Our proof 
differs from an existing proof (Chomsky 1959a) in that it is fully constructive: Fig- 
ure 2 presents an algorithm for creating a finite automaton that accepts the language 
generated by the grammar. 
The process is initiated at the start symbol, and from there the process descends 
the grammar in all ways until terminals are encountered, and then transitions are 
created labeled with those terminals. Descending the grammar is straightforward in 
the case of rules of which the left-hand side is not a recursive nonterminal: the sub- 
automata found recursively for members in the right-hand side will be connected. 
In the case of recursive nonterminals, the process depends on whether the nontermi- 
nals in the corresponding set from H are mutually left-recursive or right-recursive; 
if they are both, which means they are cyclic, then either subprocess can be ap- 
plied; in the code in Figure 2 cyclic and right-recursive subsets Ni are treated uni- 
formly. 
We discuss the case in which the nonterminals are left-recursive. One new state is 
created for each nonterminal in the set. The transitions that are created for terminals 
and nonterminals not in Ni are connected in a way that is reminiscent of the con- 
struction of left-corner parsers (Rosenkrantz and Lewis 1970), and specifically of one 
construction that focuses on sets of mutually recursive nonterminals (Nederhof 1994, 
Section 5.8). 
An example is given in Figure 3. Four states have been labeled according to the 
names they are given in procedure make~fa. There are two states that are labeled qB. 
This can be explained by the fact that nonterminal B can be reached by descending 
the grammar from S in two essentially distinct ways. 
The code in Figure 2 differs from the actual implementation in that sometimes, for a 
nonterminal, a separate finite automaton is constructed, namely, for those nonterminals 
that occur as A in the code. A transition in such a subautomaton may be labeled by 
another nonterminal B, which then represents he subautomaton corresponding to B. 
The resulting representation is similar to extended context-free grammars (Purdom 
and Brown 1981), with the exception that in our case recursion cannot occur, by virtue 
of the construction. 
The representation forthe running example is indicated by Figure 4, which shows 
two subautomata, l beled S and B. The one labeled S is the automaton on the top level, 
and contains two transitions labeled B, which refer to the other subautomaton. Note 
that this representation is more compact han that of Figure 3, since the transitions 
that are involved in representing the sublanguage of strings generated by nonterminal 
B are included only once. 
The compact representation consisting of subautomata c n be turned into a sin- 
gle finite automaton by substituting subautomata A for transitions labeled A in other 
automata. This comes down to regular substitution i the sense of Berstel (1979). The 
advantage of this way of obtaining a finite automaton over a direct construction of a 
nondeterministic automaton is that subautomata may be determinized and minimized 
before they are substituted into larger subautomata. Since in many cases determinized 
and minimized automata re much smaller, this process avoids much of the combina- 
20 
Nederhof Experiments with Regular Approximation 
let K = O, A = O, s = fresh_state, f = fresh_state, F = {f}; 
make_fa( s, S, f) .  
procedure  makeffa(qo, a, ql): 
i f  a= e 
then  let A = A U {(q0,e, ql)} 
elsei f  a = a, some a E ,U 
then  let A = A U {(q0, a, ql)} 
elsei f  a = Xfl, some X E V, fl C V* such  that  IflI > 0 
then  let q = fresh_state; 
makeffa(qo, X, q); 
makeffa( q, t ,  ql ) 
else let A = a; (* a must consist of a single nonterminal *) 
i f  there  ex ists  i such  that  A C Ni 
then  for  each  B E Ni do let qB = fresh_state nd;  
if recursive(Ni) = left 
then  for  each  (C -+ X I ' . 'Xm)  E P such  that  CENi  AX1, . . . ,Xm~Ni  
do make_fa(qo, XI  " . Xm, qc ) 
end;  
for  each  (C --+ DX1 ... X,~) C P such  that  
C,D ~ Ni A X1 , . . . ,Xm ~ Ni 
do make ffa( qD , X I " " X,~ , qc ) 
end;  
let A = A U {(qA, e, ql)} 
else (* recursive(g,) C {right, cyclic} *) 
for  each  (C -+ X1 . . .Xm)  E P such  that  CENi  A X1 , . . . ,Xm~Ni  
do make_fa(qc, X1 . . .  Xm, ql) 
end;  
for  each  (C --~ XI ".. XmD) E P such  that  
C, D E Ni AX I , . . . ,Xm ~ Ni 
do makc_fa(qc, XI ".. Xm, qD) 
end;  
let A = A U {(qo, e, qa)} 
end 
else for  each  (A -+ fl) C P do make_fa(qo,fl, ql) end (* A is not recursive *) 
end  
end 
end.  
p rocedure  fresh_state(): 
create some object q such that q ~ K;  
let K=KU{q};  
re turn  q 
end.  
Figure 2 
Transformation from a grammar G = (E, N,P, S) that is not self-embedding into an equivalent 
finite automaton 3 v = (K, E, A, s, F). 
21 
Computational Linguistics Volume 26, Number 1 
S --* Aa 
A --* SB 
A ~ Bb 
B --* Bc 
B ---* d 
c 
qB 
Figure 3 
N = {S,A,B} 
\]kf : {N1, N2} 
N1 = {S,A} recursive(N1) = left 
N2 -- {B} recursive(N2) = left 
__qA a d 
Application of the code from Figure 2 on a small grammar. 
S 
Figure 4 
B 
. . . . . . . . . . . . . . . . . . . . . . . . . . . .  1 c 
i 
i 
i 
I 
, , .  d ~ (~) , 
f w  - w  = i 
qB I 
The automaton from Figure 3 in a compact representation. 
torial explosion that takes place upon naive construction of a single nondeterministic 
finite automaton. 1 
Assume we have a list of subautomata A1 . . . . .  Am that is ordered from lower-level 
to higher-level automata; i.e., if an automaton Ap occurs as the label of a transition 
of automaton Aq, then p < q; Am must be the start symbol  S. This order is a natural 
result of the way that subautomata are constructed uring our depth-first raversal of 
the grammar, which is actually postorder in the sense that a subautomaton is output 
after all subautomata occurring at its transitions have been output. 
Our implementat ion constructs a minimal deterministic automaton by repeating 
the following for p = 1 , . . . ,m:  
. 
. 
Make a copy of Ap. Determinize and minimize the copy. If it has fewer 
transitions labeled by nonterminals than the original, then replace Ap by 
its copy. 
Replace each transition in Ap of the form (q, Ar, q') by (a copy of) 
automaton Ar in a straightforward way. This means that new e-transitions 
connect q to the start state of Ar and the final states of Ar to qt. 
1 The representation in Mohri and Pereira (1998) is even more compact than ours for grammars that are 
not self-embedding. However, in this paper we use our representation as an intermediate r sult in 
approximating an unrestricted context-free grammar, with the final objective of obtaining a single 
minimal deterministic automaton. For this purpose, Mohri and Pereira's representation ffers little 
advantage. 
22 
Nederhof Experiments with Regular Approximation 
3. Again determinize and minimize Ap and store it for later reference. 
The automaton obtained for Am after step 3 is the desired result. 
4. Methods of Regular Approximation 
This section describes a number of methods for approximating a context-free gram- 
mar by means of a finite automaton. Some published methods did not mention self- 
embedding explicitly as the source of nonregularity for the language, and suggested 
that approximations should be applied globally for the complete grammar. Where 
this is the case, we adapt the method so that it is more selective and deals with 
self-embedding locally. 
The approximations are integrated into the construction of the finite automaton 
from the grammar, which was described in the previous ection. A separate incarnation 
of the approximation process is activated upon finding a nonterminal A such that 
A E Ni and recursive(Ni) = self, for some i. This incarnation then only pertains to 
the set of rules of the form B --* c~, where B E Ni. In other words, nonterminals not 
in Ni are treated by this incarnation of the approximation process as if they were 
terminals. 
4.1 Superset Approximation Based on RTNs 
The following approximation was proposed in Nederhof (1997). The presentation 
here, however, differs substantially from the earlier publication, which treated the ap- 
proximation process entirely on the level of context-free grammars: a self-embedding 
grammar was transformed in such a way that it was no longer self-embedding. A 
finite automaton was then obtained from the grammar by the algorithm discussed 
above. 
The presentation here is based on recursive transition networks (RTNs) (Woods 
1970). We can see a context-free grammar as an RTN as follows: We introduce two 
states qA and q~ for each nonterminal A, and m + 1 states q0 . . . . .  qm for each rule 
A --* X1 .. ? Xm. The  states for a rule A ~ X 1 . . . X m are connected with each other and 
to the states for the left-hand side A by one transition (qA, c, q0), a transition (qi-1, Xi, qi) 
for each i such that 1 < i < m, and one transition (qm, e,q~A). (Actually, some epsilon 
transitions are avoided in our implementation, but we will not be concerned with such 
optimizations here.) 
In this way, we obtain a finite automaton with initial state qA and final state q~ for 
each nonterminal A and its defining rules A --* X1 ? ? ? Xm. This automaton can be seen 
as one component of the RTN. The complete RTN is obtained by the collection of all 
such finite automata for different nonterminals. 
An approximation now results if we join all the components in one big automaton, 
and if we approximate the usual mechanism of recursion by replacing each transition 
(q, A, q') by two transitions (q, c, qA) and (q~, e, q'). The construction is illustrated in 
Figure 5. 
In terms of the original grammar, this approximation can be informally explained 
as follows: Suppose we have three rules B --* c~Afl, B I ~ c~IAfl ~, and A ~ % Top-down, 
left-to-right parsing would proceed, for example, by recognizing a in the first rule; 
it would then descend into rule A ~ % and recognize "y; it would then return to 
the first rule and subsequently process ft. In the approximation, however, the finite 
automaton "forgets" which rule it came from when it starts to recognize % so that it 
may subsequently recognize fl' in the second rule. 
23 
Computational Linguistics Volume 26, Number 1 
(b) 
a B b 
(a) 
d A e 
qB Y '~ ; i  = i ~  'B A---~ a B b >t~ q 
A---~c A " ' "~"~ f 
B- - -~dA e 
B--~ f
(c) 
a b 
~ ~ t / 
Figure 5 
Application of the RTN method for the grammar in (a). The RTN is given in (b), and (c) 
presents the approximating finite automaton. We assume A is the start symbol and therefore 
qA becomes the initial state and q~ becomes the final state in the approximating automaton. 
For the sake of presentational convenience, the above describes a construction 
working on the complete grammar. However, our implementation applies the con- 
struction separately for each nonterminal in a set Ni such that recursive(Ni) = self, 
which leads to a separate subautomaton of the compact representation (Section 3). 
See Nederhof (1998) for a variant of this approximation that constructs finite trans- 
ducers rather than finite automata. 
We have further implemented a parameterized version of the RTN approximation. 
A state of the nondeterministic automaton is now also associated to a list H of length 
IHI strictly smaller than a number d, which is the parameter to the method. This list 
represents a history of rule positions that were encountered in the computation leading 
to the present state. 
More precisely, we define an item to be an object of the form \[A ~ a ? fl\], 
where A ~ aft is a rule from the grammar. These are the same objects as the "dot- 
ted" productions of Earley (1970). The dot indicates a position in the right-hand 
side. 
The unparameterized RTN method had one state qI for each i tem/,  and two states 
qA and q~ for each nonterminal A. The parameterized RTN method has one state qrH 
for each item I and each list of items H that represents a valid history for reaching 
I, and two states qaH and q~H for each nonterminal A and each list of items H that 
represents a valid history for reaching A. Such a valid history is defined to be a list 
24 
Nederhof Experiments with Regular Approximation 
H with 0 < \[HI < d that represents a series of positions in rules that could have been 
invoked before reaching I or A, respectively. More precisely, if we set H =/1 .. .  In, then 
each Im (1 < m < n) should be of the form \[Am ~ olin ? Bmflm\] and for 1 < m < n we 
should have Am -- Bm+l. Furthermore, for a state qiH with I = \[A --* a ? fl\] we demand 
A = B1 if n > 0. For a state qAH we demand A -- B1 if n > 0. (Strictly speaking, states 
qAH and qrH, with \[HI < d - 1 and I = \[A --+ a ? fl\], will only be needed if AIH \] is the 
start symbol in the case IH\[ > 0, or if A is the start symbol in the case H = c.) 
The transitions of the automaton that pertain to terminals in right-hand sides 
of rules are very similar to those in the case of the unparameterized method: For a 
state qIH with I of the form \[A ~ a ? aft\], we create a transition (q~H, a, qi,H), with 
I' = \[A ~ aa ? fl\]. 
Similarly, we create epsilon transitions that connect left-hand sides and right-hand 
sides of rules: For each state qAa there is a transition (qAH, e, qIH) for each item I = 
\[A --* ? a\], for some a, and for each state of the form qI,u, with I' = \[A ~ a ?\], there 
is a transition (qFa, c, q~H). 
For transitions that pertain to nonterminals in the right-hand sides of rules, we 
need to manipulate the histories. For a state qIH with I of the form \[A ~ a ? Bfl\], we 
create two epsilon transitions. One is (qIH, c, qBn,), where H' is defined to be IH  if 
\[IH\[ < d, and to be the first d - 1 items of IH ,  otherwise. Informally, we extend the 
history by the item I representing the rule position that we have just come from, but 
the oldest information in the history is discarded if the history becomes too long. The 
second transition is (q'BH,, ~, q~'H), with I' = \[A --* aB ? fl\]. 
If the start symbol is S, the initial state is qs and the final state is q~ (after the 
symbol S in the subscripts we find empty lists of items). Note that the parameterized 
method with d -- 1 concurs with the unparameterized method, since the lists of items 
then remain empty. 
An example with parameter d -- 2 is given in Figure 6. For the unparameterized 
method, each I = \[A --* a ? fl\] corresponded to one state (Figure 5). Since reaching A 
can have three different histories of length shorter than 2 (the empty history, since A is 
the start symbol; the history of coming from the rule position given by item \[A -~ c ? A\]; 
and the history of coming from the rule position given by item \[B ~ d ? Ae\]), in Figure 6 
we now have three states of the form qI~ for each I -- \[A ~ a ? fl\], as well as three 
states of the form qA~r and q~H" 
The higher we choose d, the more precise the approximation is, since the histories 
allow the automaton to simulate part of the mechanism of recursion from the original 
grammar, and the maximum length of the histories corresponds to the number of 
levels of recursion that can be simulated accurately. 
4.2 Refinement of RTN Superset Approximation 
We rephrase the method of Grimley-Evans (1997) as follows: First, we construct he 
approximating finite automaton according to the unparameterized RTN method above. 
Then an additional mechanism is introduced that ensures for each rule A --~ X1 ? .. Xm 
separately that the list of visits to the states qo,. .  ? ? qm satisfies ome reasonable criteria: 
a visit to qi, with 0 < i < m, should be followed by one to qi+l or q0. The latter option 
amounts to a nested incarnation of the rule. There is a complementary condition for 
what should precede a visit to qi, with 0 < i < m. 
Since only pairs of consecutive visits to states from the set {q0 . . . . .  qm} are consid- 
ered, finite-state techniques suffice to implement such conditions. This can be realized 
by attaching histories to the states as in the case of the parameterized RTN method 
above, but now each history is a set rather than a list, and can contain at most one 
item \[A --* a ? fl\] for each rule A ---* aft.  As reported by Grimley-Evans (1997) and con- 
25 
Computational Linguistics Volume 26, Number 1 
A~a B b 
A~c A 
B---,'d A e 
B--->f 
Figure 6 
a 
c 
/ 
a 
H = \[A----> c .A l  qA~_  . . . . .  g ~', 
x I I  , a ,, 
H= \[B-->d.A el qA e',, 
b 
, E 
b 
i , 
, ,, . . . .  
b " 
,,'" \ -qA .  
d " ,,, , Z e _ 
qB Q~___ H = \[A --~ a .B  b\] - -  5 . .  "'L qBH 
Application of the parameterized RTN method with d = 2. We again assume A is the start 
symbol. States qm have not been labeled in order to avoid cluttering the picture. 
f irmed by our own experiments, the nondeterministic finite automata resulting from 
this method may be quite large, even for small grammars. The explanation is that the 
number of such histories is exponential in the number of rules. 
We have refined the method with respect o the original publication by applying 
the construction separately for each nonterminal in a set Ni such that recursive(Ni) = 
self. 
4.3 Subset Approximation by Transforming the Grammar 
Putting restrictions on spines is another way to obtain a regular language. Several 
methods can be defined. The first method we present investigates spines in a very 
detailed way. It eliminates from the language only those sentences for which a sub- 
derivation is required of the form B --~* aBfl, for some a ~ ? and fl ~ e. The motivation 
is that such sentences do not occur frequently in practice, since these subderivations 
make them difficult for people to comprehend (Resnik 1992). Their exclusion will 
therefore not lead to much loss of coverage of typical sentences, especially for simple 
application domains. 
We express the method in terms of a grammar transformation i Figure 7. The 
effect of this transformation is that a nonterminal A is tagged with a set of pairs 
(B, Q), where B is a nonterminal occurring higher in the spine; for any given B, at 
most one such pair (B, Q) can be contained in the set. The set Q may contain the 
element l to indicate that something to the left of the part of the spine from B to A 
26 
Nederhof Experiments with Regular Approximation 
We are given a grammar G = (E,N, P, S). The following is to be performed for each 
set Ni EAf  such that recursive(Ni) = self. 
. For each A E Ni and each F E 2 (Nix2~l''}), add the following nonterminal 
to N. 
? A F . 
2. For each A E Ni, add the following rule to P. 
? A---~A 0. 
. For each (A --* o~0A1o~1A2... C~m-lAmCrm) E P such that A, A1 . . . .  ,Am E Ni 
and no symbols from c~0 . . . .  , am are members of Ni, and each F such that 
(A, (l, r}) ~ F, add the following rule to P. 
a F F1 Fm o~0A 1 oq. . .  A m O~m, where, for 1 G j _< m, 
- -  F j= {(B, QUC~U~F) I (B,Q) E F'}; 
F' = FU {(A, 0)} if -~3Q\[(A,Q) E F\], and F' = F 
otherwise; 
- -   = 0 if c~0AlC~l...Aj-I~j-1 = c, and ~ = {l} otherwise; 
- -  QJr = 0 if o/.jaj+lOLj+l...AmOL m = ?,  and QJr = {r} 
otherwise. 
4. Remove from P the old rules of the form A --* c~, where A E Ni. 
5. Reduce the grammar. 
Figure 7 
Subset approximation by transforming the grammar. 
was generated. Similarly, r E Q indicates that something to the right was generated. If 
Q = {l, r}, then we have obtained a derivation B --** c~Afl, for some c~ ~ c and fl ~ ~, 
and further occurrences of B below A should be blocked in order to avoid a derivation 
with self-embedding. 
An example is given in Figure 8. The original grammar is implicit in the depicted 
parse tree on the left, and contains at least the rules S --+ A a, A --, b B, B -* C, and 
C --* S. This grammar is self-embedding, since we have a subderivation S --~* bSa. 
We explain how FB is obtained from FA in the rule A ~ --* b B r'. We first construct 
F' = {(S, {r}), (A, 0)} from FA = {(S, (r})} by adding (A, 0), since no other pair of the 
form (A, Q) was already present. To the left of the occurrence of B in the original rule 
A --* b B we find a nonempty string b. This means that we have to add l to all second 
components of pairs in F', which gives us FB = {(S, (l, r}), (A, {l})}. 
In the transformed grammar, the lower occurrence of S in the tree is tagged with 
the set {(S, {I, r}), (A, {l}), (B, 0), (C, 0)}. The meaning is that higher up in the spine, we 
will find the nonterminals S, A, B, and C. The pair (A, (1}) indicates that since we saw 
A on the spine, something to the left has been generated, namely, b. The pair (B, 0) 
indicates that nothing either to the left or to the right has been generated since we 
saw B. The pair (S, {1, r}) indicates that both to the left and to the right something has 
been generated (namely, b on the left and a on the right). Since this indicates that an 
27 
Computational Linguistics Volume 26, Number 1 
s 
(a) s (b) s Fs Fs  = 
A a a / \  / \  
FB b B b B 'B 
I 
C ~F c Fc  = 
5 '  ' - -  s 
X 
0 
{(S, {l, r}), (A, {/})} 
{(S, {l, r}), (A, {/}), (B, 0)} 
{(S, {l, r}), (A, {/}), (B, 0), (C, 0)} 
Figure 8 
A parse tree m a self-embedding grammar (a), and the corresponding parse tree in the 
transformed grammar (b), for the transformation from Figure 7. For the moment we ignore 
step 5 of Figure 7, i.e., reduction of the transformed grammar. 
offending subderivation S --** c~Sfl has been found, further completion of the parse 
tree is blocked: the transformed grammar will not have any rules with left-hand side 
S {(S'{I'r})'(A'{I})'(B'O)'(C'O)}. In fact, after the grammar is reduced, any parse tree that is 
constructed can no longer even contain a node labeled by S {(s'U'r})'(a'{O)'(B'?)'(c'?)}, or 
any nodes with labels of the form A r such that (A, {l,r}) c F. 
One could generalize this approximation i such a way that not all self-embedding 
is blocked, but only self-embedding occurring, say, twice in a row, in the sense of a 
subderivation of the form A --** a lA f l l  --+* oqol2Afl2fll. We will not do so here, because 
already for the basic case above, the transformed grammar can be huge due to the 
high number of nonterminals of the form A F that may result; the number of such 
nonterminals i exponential in the size of Ni. 
We therefore present, in Figure 9, an alternative approximation that has a lower 
complexity. By parameter d, it restricts the number of rules along a spine that may 
generate something to the left and to the right. We do not, however, restrict pure left 
recursion and pure right recursion. Between two occurrences of an arbitrary rule, we 
allow left recursion followed by right recursion (which leads to tag r followed by tag 
rl), or right recursion followed by left recursion (which leads to tag l followed by 
tag lr). 
An example is given in Figure 10. As before, the rules of the grammar are implicit 
in the depicted parse tree. At the top of the derivation we find S. In the transformed 
grammar, we first have to apply S --* S -r'?. The derivation starts with a rule S --* A a, 
which generates a string (a) to the right of a nonterminal (A). Before we can apply zero 
or more of such rules, we first have to apply a unit rule S T,? --* S r,? in the transformed 
grammar. For zero or more rules that subsequently generate something on the left, 
such as A ~ b B, we have to obtain a superscript containing rl, and in the example 
this is done by applying A r,? ~ A rl,?. Now we are finished with pure left recursion and 
pure right recursion, and apply B rl,O ---+ B ?,0. This allows us to apply one unconstrained 
rule, which appears in the transformed grammar as B ?,? ---* c S T'I d. 
28 
Nederhof Experiments with Regular Approximation 
We are given a grammar G = (G, N, P, S). The following is to be performed for each 
set Ni C .IV" such that recursive(Ni) = self. The value d stands for the maximum number 
of unconstrained rules along a spine, possibly alternated with a series of left-recursive 
rules followed by a series of right-recursive rules, or vice versa. 
1. For each A c Ni, each Q E { T, l, r, It, rl, 3_ }, and each f such that 
0 < f < d, add the following nonterminals to N. 
? AQ,f. 
2. For each A E Ni, add the following rule to P. 
? A ---+ A T'0. 
3. For each A E Ni and f such that 0 G f G d, add the following rules to P. 
? AT, f  ___+ Al,f. 
? ATd: __+ Ar,f. 
? Aid ---+ Alr,f. 
? Ar,f ---, A~l,/. 
? Atr,f __+ A?,d .  
? Arl,f ___+ A?,f.  
4. For each (A -+ Ba) ~ P such that A, B c Ni and no symbols from ~ are 
members of Ni, eachf  such that 0 <f  G d, and each Q E {r, lr}, add the 
following rule to P. 
? AQd ~ BQ/a. 
5. For each (A --+ c~B) E P such that A, B E Ni and no symbols from c~ are 
members of Ni, eachf  such that 0 Gf  < d, and each Q c {l, rl}, add the 
following rule to P. 
? Aqd ~ c~BQ,f. 
6. For each (A -~ o~0AloqA2... O~m-lAmC~m) C P such that A, A1 . . . . .  Am E Ni 
and no symbols from s0 . . . . .  C~m are members of Ni, and each f such that 
0 < f G d, add the following rule to P, provided m = 0 v f  < d. 
? A?/  c~0Alq-d+lc~l AT,f+1 - - -4  . . .~ l  m ' OLm . 
7. Remove from P the old rules of the form A ~ c~, where A E Ni. 
8. Reduce the grammar. 
Figure 9 
A simpler subset approximation by transforming the grammar. 
Now the counter f has been increased from 0 at the start of the subderivation to 
1 at the end. Depending on the value d that we choose, we cannot build derivations 
by repeating subderivation S --+* b c S d a an unlimited number of times: at some 
point the counter will exceed d. If we choose d = 0, then already the derivation at 
29 
Computational Linguistics Volume 26, Number 1 
S 
S !T,O 
(a) /~  (b)!r,O 
So\ A a ' a 
rl, O 
b B b B rl'O 
:o 
t t 
t t 
t t 
t t Figure 10 
A parse tree in a self-embedding grammar (a), and the corresponding parse tree in the 
transformed grammar (b), for the simple subset approximation from Figure 9. 
Figure 10 (b) is no longer possible, since no nonterminal in the transformed grammar 
would contain 1 in its superscript. 
Because of the demonstrated increase of the counter f ,  this transformation is guar- 
anteed to remove self-embedding from the grammar. However, it is not as selective as 
the transformation we saw before, in the sense that it may also block subderivations 
that are not of the form A --** ~Afl. Consider for example the subderivation from 
Figure 10, but replacing the lower occurrence of S by any other nonterminal C that is 
mutually recursive with S, A, and B. Such a subderivation S ---** b c C d a would also 
be blocked by choosing d = 0. In general, increasing d allows more of such derivations 
that are not of the form A ~"  o~Afl but also allows more derivations that are of that 
form. 
The reason for considering this transformation rather than any other that elim- 
inates self-embedding is purely pragmatic: of the many variants we have tried that 
yield nontrivial subset approximations, this transformation has the lowest complex- 
ity in terms of the sizes of intermediate structures and of the resulting finite au- 
tomata. 
In the actual implementation, wehave integrated the grammar transformation a d 
the construction of the finite automaton, which avoids reanalysis of the grammar to 
determine the partition of mutually recursive nonterminals after transformation. This 
integration makes use, for example, of the fact that for fixed Ni and fixed f,  the set of 
nonterminals of the form A,f ,  with A c Ni, is (potentially) mutually right-recursive. 
A set of such nonterminals can therefore be treated as the corresponding case from 
Figure 2, assuming the value right. 
The full formulation of the integrated grammar transformation a d construction 
of the finite automaton is rather long and is therefore not given here. A very similar 
formulation, for another grammar transformation, is given in Nederhof (1998). 
30 
Nederhof Experiments with Regular Approximation 
4.4 Superset Approximation through Pushdown Automata 
The distinction between context-free languages and regular languages can be seen in 
terms of the distinction between pushdown automata and finite automata. Pushdown 
automata maintain a stack that is potentially unbounded in height, which allows more 
complex languages to be recognized than in the case of finite automata. Regular ap- 
proximation can be achieved by restricting the height of the stack, as we will see in 
Section 4.5, or by ignoring the distinction between several stacks when they become 
too high. 
More specifically, the method proposed by Pereira and Wright (1997) first con- 
structs an LR automaton, which is a special case of a pushdown automaton. Then, 
stacks that may be constructed in the course of recognition of a string are computed 
one by one. However, stacks that contain two occurrences of a stack symbol are iden- 
tified with the shorter stack that results by removing the part of the stack between the 
two occurrences, including one of the two occurrences. This process defines a congru- 
ence relation on stacks, with a finite number of congruence classes. This congruence 
relation directly defines a finite automaton: each class is translated to a unique state of 
the nondeterministic finite automaton, shift actions are translated to transitions labeled 
with terminals, and reduce actions are translated to epsilon transitions. 
The method has a high complexity. First, construction of an LR automaton, of 
which the size is exponential in the size of the grammar, may be a prohibitively ex- 
pensive task (Nederhof and Satta 1996). This is, however, only a fraction of the effort 
needed to compute the congruence classes, of which the number is in turn exponen- 
tial in the size of the LR automaton. If the resulting nondeterministic automaton is
determinized, we obtain a third source of exponential behavior. The time and space 
complexity of the method are thereby bounded by a triple exponential function in the 
size of the grammar. This theoretical nalysis eems to be in keeping with the high 
costs of applying this method in practice, as will be shown later in this article. 
As proposed by Pereira and Wright (1997), our implementation applies the ap- 
proximation separately for each nonterminal occurring in a set Ni that reveals self- 
embedding. 
A different superset approximation based on LR automata was proposed by Baker 
(1981) and rediscovered by Heckert (1994). Each individual stack symbol is now trans- 
lated to one state of the nondeterministic finite automaton. It can be argued theoret- 
ically that this approximation differs from the unparameterized RTN approximation 
from Section 4.1 only under certain conditions that are not likely to occur very often 
in practice. This consideration is confirmed by our experiments obe discussed later. 
Our implementation differs from the original algorithm in that the approximation is 
applied separately for each nonterminal in a set Ni that reveals elf-embedding. 
A generalization f this method was suggested by Bermudez and Schimpf (1990). 
For a fixed number d > 0 we investigate sequences of d top-most elements of stacks 
that may arise in the LR automaton, and we translate these to states of the finite 
automaton. More precisely, we define another congruence r lation on stacks, such that 
we have one congruence class for each sequence of d stack symbols and this class 
contains all stacks that have that sequence as d top-most elements; we have a separate 
class for each stack that contains fewer than d elements. As before, each congruence 
class is translated to one state of the nondeterministic finite automaton. Note that the 
case d = 1 is equivalent to the approximation i  Baker (1981). 
If we replace the LR automaton by a certain type of automaton that performs top- 
down recognition, then the method in Bermudez and Schimpf (1990) amounts to the 
parameterized RTN method from Section 4.1; note that the histories from Section 4.1 
in fact function as stacks, the items being the stack symbols. 
31 
Computational Linguistics Volume 26, Number 1 
4.5 Subset Approximation through Pushdown Automata 
By restricting the height of the stack of a pushdown automaton, one obstructs recogni- 
tion of a set of strings in the context-free language, and therefore a subset approxima- 
tion results. This idea was proposed by Krauwer and des Tombe (1981), Langendoen 
and Langsam (1987), and Pulman (1986), and was rediscovered by Black (1989) and 
recently by Johnson (1998). Since the latest publication in this area is more explicit in 
its presentation, we will base our treatment on this, instead of going to the historical 
roots of the method. 
One first constructs a modified left-corner ecognizer from the grammar, in the 
form of a pushdown automaton. The stack height is bounded by a low number; 
Johnson (1998) claims a suitable number would be 5. The motivation for using the 
left-corner strategy is that the height of the stack maintained by a left-corner parser 
is already bounded by a constant in the absence of self-embedding. If the artificial 
bound imposed by the approximation method is chosen to be larger than or equal to 
this natural bound, then the approximation may be exact. 
Our own implementation is more refined than the published algorithms mentioned 
above, in that it defines a separate left-corner recognizer for each nonterminal A such 
that A E Ni and recursive(Ni) = self, some i. In the construction of one such recognizer, 
nonterminals that do not belong to Ni are treated as terminals, as in all other methods 
discussed here. 
4.6 Superset Approximation by N-grams 
An approximation from Seyfarth and Bermudez (1995) can be explained as follows. 
Define the set of all terminals reachable from nonterminal A to be ~A = {a I 3c~, iliA --** 
o~afl\]}. We now approximate the set of strings derivable from A by G~, which is the 
set of strings consisting of terminals from GA. Our implementation is made slightly 
more sophisticated by taking ~A to be {X \] 3B, c~,fl\[B E Ni A B ~ oLXfl A X ~ Ni\]}, for 
each A such that A E Ni and recursive(Ni) = self, for some i. That is, each X E ~A is 
a terminal, or a nonterminal not in the same set Ni as A, but immediately reachable 
from set Ni, through B E Ni. 
This method can be generalized, inspired by Stolcke and Segal (1994), who derive 
N-gram probabilities from stochastic ontext-free grammars. By ignoring the probabil- 
ities, each N = 1, 2, 3 . . . .  gives rise to a superset approximation that can be described 
as follows: The set of strings derivable from a nonterminal A is approximated by the 
set of strings al .. .  an such that 
? for each substring v = ai+l . . .  ai+N (0 < i < n -- N) we have A --+* wvy, for 
some w and y, 
? for each prefix v = al . . .  ai (0 < i < n) such that i < N we have A -** vy, 
for some y, and 
? for each suffix v = ai+l ...  an (0 < i < n) such that n - i < N we have 
a ---~* wv, for some w. 
(Again, the algorithms that we actually implemented are more refined and take into 
account he sets Ni.) 
The approximation from Seyfarth and Bermudez (1995) can be seen as the case N = 
1, which will henceforth be called the unigram method. We have also experimented 
with the cases N = 2 and N = 3, which will be called the bigram and trigram methods. 
32 
Nederhof Experiments with Regular Approximation 
5. Increasing the Precision 
The methods of approximation described above take as input the parts of the grammar 
that pertain to self-embedding. It is only for those parts that the language is affected. 
This leads us to a way to increase the precision: before applying any of the above 
methods of regular approximation, we first transform the grammar. 
This grammar transformation copies grammar ules containing recursive nonter- 
minals and, in the copies, it replaces these nonterminals by new nonrecursive nonter- 
minals. The new rules take over part of the roles of the old rules, but since the new 
rules do not contain recursion and therefore do not pertain to self-embedding, they 
remain unaffected by the approximation process. 
Consider for example the palindrome grammar from Figure 1. The RTN method 
will yield a rather crude approximation, amely, the language {a, b}*. We transform 
this grammar in order to keep the approximation process away from the first three 
levels of recursion. We achieve this by introducing three new nonterminals S\[1\], S\[2\] 
and S\[3\], and by adding modified copies of the original grammar ules, so that we 
obtain: 
S\[1\] 
S\[2\] 
S\[3\] 
S 
The new start symbol is S\[1\]. 
aS\[2\]a \] bS\[2\] b I ? 
aS\[3\]a \] bS\[3\] b I c 
aSa l bSb  i c 
aSa  i bSb  i e 
The new grammar generates the same language as before, but the approximation 
process leaves unaffected the nonterminals S\[1\], S\[2\], and S\[3\] and the rules defining 
them, since these nonterminals are not recursive. These nonterminals amount o the 
upper three levels of the parse trees, and therefore the effect of the approximation 
on the language is limited to lower levels. If we apply the RTN method then we 
obtain the language that consists of (grammatical) palindromes of the form ww R, where 
w E {?, a, b} U {a, b} 2 U {a, b} 3, plus (possibly ungrammatical) strings of the form wvw R, 
where w E {a, b} 3 and v E {a, b}*. (w R indicates the mirror image of w.) 
The grammar transformation i its full generality is given by the following, which 
is to be applied for fixed integer j > 0, which is a parameter of the transformation, 
and for each Ni such that recursive(Ni) = self. 
For each nonterminal A E Ni we introduce j new nonterminals All\] . . . . .  A~\]. For 
each A --, X1 . . .Xm in P such that A E Ni, and h such that 1 ~ h < j, we add 
A\[h\] --* X ' I . . .  X"  to P, where for 1 < k < m: 
X~k = Xk\[h + 1\] if X k E Ni /X h < j 
= Xk otherwise 
Further, we replace all rules A --* X1 . . .  Xm such that A ~ Ni by A --* X~ ... X~m, where 
for 1 < k < m: 
X~ -- Xk\[1\] i fXkENi  
= Xk otherwise 
If the start symbol S was in Ni, we let S\[1\] be the new start symbol. 
A second transformation, which shares some characteristics with the one above, 
was presented in Nederhof (1997). One of the earliest papers uggesting such transfor- 
mations as a way to increase the precision of approximation is due to ~ulik and Cohen 
(1973), who only discuss examples, however; no general algorithms were defined. 
33 
Computational Linguistics Volume 26, Number 1 
550 
500 
450 
? 400 -5 
350 
._N_ 300 
250 
E 200 
E 150 
100 
50 
0 
0 
I I I I I I 
50 100 150 200 250 300 350 
corpus size (# sentences) 
E 
180 
160 
140 
120 
100 
80 
60 
40 
20 
0 
5 10 15 20 25 30 
length (# words) 
Figure 11 
The test material. The left-hand curve refers to the construction of the grammar f om 332 
sentences, the right-hand curve refers to the corpus of 1,000 sentences used as input to the 
finite automata. 
6. Empirical Results 
In this section we investigate empirically how the respective approximation methods 
behave on grammars of different sizes and how much the approximated languages 
differ from the original context-free languages. This last question is difficult o answer 
precisely. Both an original context-free language and an approximating regular lan- 
guage generally consist of an infinite number of strings, and the number of strings 
that are introduced in a superset approximation or that are excluded in a subset ap- 
proximation may also be infinite. This makes it difficult to attach numbers to the 
"quality" of approximations. 
We have opted for a pragmatic approach, which does not require investigation of
the entire infinite languages of the grammar and the finite automata, but looks at a 
certain finite set of strings taken from a corpus, as discussed below. For this finite set 
of strings, we measure the percentage that overlaps with the investigated languages. 
For the experiments, we took context-free grammars for German, generated auto- 
matically from an HPSG and a spoken-language corpus of 332 sentences. This corpus 
consists of sentences possessing grammatical phenomena ofinterest, manually selected 
from a larger corpus of actual dialogues. An HPSG parser was applied on these sen- 
tences, and a form of context-free backbone was selected from the first derivation that 
was found. (To take the first derivation is as good as any other strategy, given that we 
have at present no mechanisms for relative ranking of derivations.) The label occur- 
ring at a node together with the sequence of labels at the daughter nodes was then 
taken to be a context-free rule. The collection of such rules for the complete corpus 
forms a context-free grammar. Due to the incremental nature of this construction of 
the grammar, we can consider the subgrammars obtained after processing the first p 
sentences, where p = 1, 2, 3 . . . . .  332. See Figure 11 (left) for the relation between p and 
the number of rules of the grammar. The construction is such that rules have at most 
two members in the right-hand side. 
As input, we considered a set of 1,000 sentences, obtained independently from the 
332 sentences mentioned above. These 1,000 sentences were found by having a speech 
recognizer provide a single hypothesis for each utterance, where utterances come from 
actual dialogues. Figure 11 (right) shows how many sentences of different lengths the 
corpus contains, up to length 30. Above length 25, this number quickly declines, but 
still a fair quantity of longer strings can be found, e.g., 11 strings of a length between 
34 
Nederhof Experiments with Regular Approximation 
51 and 60 words. In most cases however such long strings are in fact composed of a 
number of shorter sentences. 
Each of the 1,000 sentences were input in their entirety to the automata, lthough 
in practical spoken-language systems, often one is not interested in the grammaticality 
of complete utterances, but tries to find substrings that form certain phrases bearing 
information relevant to the understanding of the utterance. We will not be concerned 
here with the exact way such recognition of substrings could be realized by means of 
finite automata, since this is outside the scope of this paper. 
For the respective methods of approximation, we measured the size of the com- 
pact representation f the nondeterministic automaton, the number of states and the 
number of transitions of the minimal deterministic automaton, and the percentage 
of sentences that were recognized, in comparison to the percentage of grammatical 
sentences. For the compact representation, we counted the number of lines, which is 
roughly the sum of the numbers of transitions from all subautomata, not considering 
about three additional lines per subautomaton for overhead. 
We investigated the size of the compact representation because it is reasonably 
implementation independent, barring optimizations of the approximation algorithms 
themselves that affect he sizes of the subautomata. For some methods, we show that 
there is a sharp increase in the size of the compact representation for a small increase 
in the size of the grammar, which gives us a strong indication of how difficult it 
would be to apply the method to much larger grammars. Note that the size of the 
compact representation is a (very) rough indication of how much effort is involved in 
determinization, minimization, and substitution of the subautomata into each other. 
For determinization a d minimization of automata, we have applied programs from 
the FSM library described in Mohri, Pereira, and Riley (1998). This library is considered 
to be competitive with respect o other tools for processing of finite-state machines. 
When these programs cannot determinize or minimize in reasonable time and space 
some subautomata constructed by a particular method of approximation, then this can 
be regarded as an indication of the impracticality of the method. 
We were not able to compute the compact representation for all the methods 
and all the grammars. The refined RTN approximation from Section 4.2 proved to be 
quite problematic. We were not able to compute the compact representation for any 
of the automatically obtained grammars in our collection that were self-embedding. 
We therefore liminated individual rules by hand, starting from the smallest self- 
embedding rammar in our collection, eventually finding grammars small enough to 
be handled by this method. The results are given in Table 1. Note that the size of the 
compact representation increases ignificantly for each additional grammar rule. The 
sizes of the finite automata, fter determinization a d minimization, remain relatively 
small. 
Also problematic was the first approximation from Section 4.4, which was based 
on LR parsing following Pereira and Wright (1997). Even for the grammar of 50 rules, 
we were not able to determinize and minimize one of the subautomata according 
to step 1 of Section 3: we stopped the process after it had reached a size of over 600 
megabytes. Results, as far as we could obtain them, are given in Table 2. Note the sharp 
increases in the size of the compact representation, resulting from small increases, from 
44 to 47 and from 47 to 50, in the number of rules, and note an accompanying sharp 
increase in the size of the finite automaton. For this method, we see no possibility 
of accomplishing the complete approximation process, including determinization a d 
minimization, for grammars in our collection that are substantially larger than 50 rules. 
Since no grammars of interest could be handled by them, the above two methods 
will be left out of further consideration. 
35 
Computational Linguistics Volume 26, Number 1 
Table 1 
Size of the compact representation a d number of states and transitions, 
for the refined RTN approximation (Grimley-Evans 1997). 
Grammar Size Compact Representation # of States # of Transitions 
10 133 11 14 
12 427 17 26 
13 1,139 17 34 
14 4,895 17 36 
15 16,297 17 40 
16 51,493 19 52 
17 208,350 19 52 
18 409,348 21 59 
19 1,326,256 21 61 
Table 2 
Size of the compact representation a d number of states and transitions, 
for the superset approximation based on LR automata following Pereira 
and Wright (1997). 
Grammar Size Compact Representation # of States # of Transitions 
35 15,921 350 2,125 
44 24,651 499 4,352 
47 151,226 5,112 35,754 
50 646,419 ? ? 
Below, we refer to the unparameterized and parameterized approximations based 
on RTNs (Section 4.1) as RTN and RTNd, respectively, for d = 2,3; to the subset 
approximation from Figure 9 as Subd, for d = 1, 2, 3; and to the second and third 
methods from Section 4.4, which were based on LR parsing following Baker (1981) 
and Bermudez and Schimpf (1990), as LR and LRd, respectively, for d = 2, 3. We refer 
to the subset approximation based on left-corner parsing from Section 4.5 as LCd, for 
the maximal stack height of d = 2, 3, 4; and to the methods discussed in Section 4.6 as 
Unigram, Bigram, and Trigram. 
We first discuss the compact representation f the nondeterministic automata. In 
Figure 12 we use two different scales to be able to represent the large variety of values. 
For the method Subd, the compact representation is of purely theoretical interest for 
grammars larger than 156 rules in the case of Sub1, for those larger than 62 rules 
in the case of Sub2, and for those larger than 35 rules in the case of Sub3, since 
the minimal deterministic automata could thereafter no longer be computed with a 
reasonable bound on resources; we stopped the processes after they had consumed 
over 400 megabytes. For LC3, LC4, RTN3, LR2, and LR3, this was also the case for 
grammars larger than 139, 62, 156, 217, and 156 rules, respectively. The sizes of the 
compact representation seem to grow moderately for LR and Bigram, in the upper 
panel, yet the sizes are much larger than those for RTN and Unigram, which are 
indicated in the lower panel. 
The numbers of states for the respective methods are given in Figure 13, again 
using two very different scales. As in the case of the grammars, the terminals of our 
finite automata re parts of speech rather than words. This means that in general there 
will be nondeterminism during application of an automaton on an input sentence due 
to lexical ambiguity. This nondeterminism can be handled efficiently using tabular 
36 
Nederhof Experiments with Regular Approximation 
r'~ 
E O o 
700000 
600000 
500000 
400000 
300000 
200000 
100000 
0 
0 
i i \] ; ; / i i 
, / ! /' LC4 
! / / ," LR3--x---. 
i \[ / ., RTN3 -~ 
i I i ," LC3 
i \[ i " LR2 . . . . .  
! / j  Trigram -~'--- 
, / ; LC2 -e--- 
i / i RTN2 
i / / LR -+ -- 
:t \] / Bigram-E3--- 
~: ~ ;' 
~ /' 
! /" / /  4- 
/ ," ,4- ...... + .... 
" / '  / '~  4-"""  ,' / . - "  _ . .+  . . . . . . . .  4- ..... 
' ~ - ~  "" - - - -E3-  . . . . . . . . . . . .  E}  . . . . . . .  E} - - - -n  . . . .  - - ' -  
- - . . . . . . . . .  1 . . . . . . . .  I I I I I I 
50 100 150 200 250 300 350 400 450 500 550 
grammar size 
20000 
15000 
"3 
& 
10000 
o 
5000 
IJ :/ 
/" 
/ /" 
/ 
/.' ,.' 
,; ,.: ," 
i ... ,' 
,,..,' 
...." ,,,' 
/ 
,,: ,,, . . , . .  
.: .... 
z~ ,'" . .. "~  
/ ... ,' .- . ......' 
.....- 
, , ?  y~..- 
" ' " " ' '  
, , ,~  
::'~ ..... 
0 50 100 150 200 
Figure 12 
Size of the compact representation. 
RTN2 .-a-- 
LC2 -e - -  
LR -+ --' 
Bigram -~--- 
Sub3 -x . . . .  
Sub2 "a ..... 
Sub1 -~ ..... 
RTN 
Unigram -~,--- 
.... 
.,0. 
I I I I I I 
250 300 350 400 450 500 550 
grammar size 
techniques, provided the number of states is not too high. This consideration favors 
methods that produce low numbers of states, such as Trigram, LR, RTN, Bigram, and 
Unigram. 
37 
Computational Linguistics Volume 26, Number 1 
10000 
9000 
8000 
7000 
6000 
5000 
4000 
3000 
2000 
1000 
0 
Sub2 ..A ..... 
LC4 -8 - -  
Sub1 --~ ..... 
LC3 -x - -  
RTN3 
LC2 -e - -  
f 
/ j 
/ 
\[ 
/ 
/ 
~~~i  
100 
?~- '~-~ ~ 
0 200 300 400 500 600 
grammar size 
100 , , , , , 
LC3 
RTN3 
LR3 --x- -- 
LC2 -e - -  
x RTN2 
80 ,,,,,," TrigramLR2 -.~--~---- 
LR -+ -- 
RTN -B~ 
Bigram -a-- 
~- -~-  Unigram -~--- 
60 .~?" 
/ 
40 
: ~ ~  d~3._-.D .D  ---\[\] ' ---   
. . . . . . . . .  ~E\]" * . . . . . .  \ [ \ ]  . . . . . . . . . . .  - 
20 " ~" --e-- "<> 
. .e -  . . . . . . . . .  e . . . . .  -e . . . . . . .  ~ . . . . . .  
0 i i i i i 
0 100 200 300 400 500 600 
grammar size 
Figure 13 
Number of states of the deterrninized and minimized automata. 
Note that the numbers of states for LR and RTN differ very little. In fact, for 
some of the smallest and for some of the largest grammars  in our collection, the 
resulting automata were identical. Note, however, that the intermediate results for LR 
38 
Nederhof Experiments with Regular Approximation 
(Figure 12) are much larger. It should therefore be concluded that the "sophistication" 
of LR parsing is here merely an avoidable source of inefficiency. 
The numbers of transitions for the respective methods are given in Figure 14. 
Again, note the different scales used in the two panels. The numbers of transitions 
roughly correspond to the storage requirements for the automata. It can be seen that, 
again, Trigram, LR, RTN, Bigram, and Unigram perform well. 
The precision of the respective approximations i  measured in terms of the per- 
centage of sentences in the corpus that are recognized by the automata, in comparison 
to the percentage of sentences that are generated by the grammar, as presented by Fig- 
ure 15. The lower panel represents an enlargement of a section from the upper panel. 
Methods that could only be applied for the smaller grammars are only presented in 
the lower panel; LC4 and Sub2 have been omitted entirely. 
The curve labeled G represents he percentage of sentences generated by the gram- 
mar. Note that since all approximation methods compute ither supersets or subsets, a
particular automaton cannot both recognize some ungrammatical sentences and reject 
some grammatical sentences. 
Unigram and Bigram recognize very high percentages of ungrammatical sentences. 
Much better results were obtained for RTN. The curve for LR would not be distin- 
guishable from that for RTN in the figure, and is therefore omitted. (For only two of 
the investigated grammars was there any difference, the largest difference occurring 
for grammar size 217, where 34.1 versus 34.5 percent of sentences were recognized 
in the cases of LR and RTN, respectively.) Trigram remains very close to RTN (and 
LR); for some grammars a lower percentage is recognized, for others a higher per- 
centage is recognized. LR2 seems to improve slightly over RTN and Trigram, but data 
is available only for small grammars, due to the difficulty of applying the method to 
larger grammars. A more substantial improvement is found for RTN2. Even smaller 
percentages are recognized by LR3 and RTN3, but again, data is available only for 
small grammars. 
The subset approximations LC3 and Sub1 remain very close to G, but here again 
only data for small grammars is available, since these two methods could not be 
applied on larger grammars. Although application of LC2 on larger grammars required 
relatively few resources, the approximation is very crude: only a small percentage of 
the grammatical sentences are recognized. 
We also performed experiments with the grammar transformation from Section 5, 
in combination with the RTN method. We found that for increasing j, the interme- 
diate automata soon became too large to be determinized and minimized, with a 
bound on the memory consumption of 400 megabytes. The sizes of the automata that 
we were able to compute are given in Figure 16. RTN+j, for j = 1, 2, 3,4, 5, repre- 
sents the (unparameterized) RTN method in combination with the grammar transfor- 
mation with parameter j. This is not to be confused with the parameterized RTNd 
method. 
Figure 17 indicates the number of sentences in the corpus that are recognized by 
an automaton divided by the number of sentences in the corpus that are generated 
by the grammar. For comparison, the figure also includes curves for RTNd, where 
d = 2, 3 (cf. Figure 15). We see that j = 1, 2 has little effect. For j = 3,4, 5, however, 
the approximating language becomes ubstantially smaller than that in the case of 
RTN, but at the expense of large automata. In particular, if we compare the sizes of 
the automata for RTN+j in Figure 16 with those for RTNd in Figures 13 and 14, then 
Figure 17 suggests the large sizes of the automata for RTN+j are not compensated 
adequately by a reduction of the percentage of sentences that are recognized. RTNd 
seems therefore preferable to RTN+j. 
39 
Computational Linguistics Volume 26, Number 1 
90000 
80000 
70000 
60000 
50000 o 
40000 
30000 
20000 
100OO 
0 
0 
z~ 
100 200 300  400  500  
grammar  size 
i 
Sub2 --~ ...... 
LC4 
Sub1 --~ ..... 
LC3 
LC2 
RTN2 
I 
600 
5000 
4000 - 
0000 / /! ' 2000 - / 
/ -  
. / / ' / '  /';'" .13 - - - _  D - 
/ ";Y" _ . . . . . .  ~-  . . . . .  43"- 
1000 I I .:..i~ ..- . . . .  (3 . . . .  
.D -''0 
LC3 -x---  
RTN3 
LR3 --x- -- 
LC2  -e~ 
RTN2 
Tr ig ram -~--- 
LR2  -~,- - 
LR -? -- 
RTN 
B igram -B - -  
Un igram -~- -- 
0 100 200  300  400  500  
grammar  size 
Figure 14 
Number of transitions of the determinized and minimized automata. 
600 
7. Conc lus ions  
If we apply the finite automata with the intention of filtering out incorrect sentences, 
for example from the output from a speech recognizer, then it is al lowed that a 
40 
Nederhof Experiments with Regular Approximation 
100 
80 
-o 6O 
& 
oo 
40 
0 
0 
20 
Unigram -4- -- 
Bigram -\[\]-- 
,e - - -~  Trigram -x--.- 
,,' RTN(LR)  -~- -  
e- . . . . . .  ?, -- --e- -- -~- . . . . .  '~ RTN2G -~---~a-- 
. . '  LC2 -e - -  
? 13 -* -?3  
,.O . . . . . . . .  ~" ~3 . . . . .  (3" 
"'" 13 . . . . . .  E l - - - - \ [ \ ] ' - ' -  
\ [ \ ]  . . . . . . .  Ey ? 
/ , - _ .  lq___  + 
/' ,' 4 . . -  I 
6 ,~'  0 0 . _ . - - -e - -~O 0 
? C\] / /  
i" I I 
100 200 300 400 500 600 
grammar size 
5' 
,\[:: 
4 
"E 
0 
3 
i , ' i  \[\] 
/ , -  
? j:/ 
,D / / i -  , 
/ /  . .- 
/ '  
0 i I I I I I I 
40 60 80 100 120 140 160 
grammar size 
Figure 15 
Percentage of sentences that are recognized. 
/ i 
/ Bigram -D-- 
RTN(LR)  
Trigram -x-.- 
LR2 -,~- - 
RTN2 -~- -  
LR3 --x- -- 
RTN3 -+- - -  
G - - t - - .  
LC3 -x - -  
Sub1 --~ ..... 
LC2 -e - -  
certain percentage of ungrammatical input is recognized. Recognizing ungrammat- 
ical input merely makes filtering less effective; it does not affect the functionality 
of the system as a whole, provided we assume that the grammar specifies exactly 
the set of sentences that can be successfully handled by a subsequent phase of pro- 
41 
Computational Linguistics Volume 26, Number 1 
10000 
8000 
6000 
4000 
2000 
: ! !  
i i i i 
RTN+5 -~-- 
RTN+4 -0-- 
RTN+3 -+-- 
RTN+2 -o-- 
RTN+I ..... 
a %0 
, / ~ , ,  ,, 
; /  .4 -  -" 
50 100 150 200 250 300 350 400 450 500 550 
grammar size 
250000 
200000 
o 
150000 
100000 
50000 
i i i , i i  
:i 
a ii 
j 
} 
! o 
i i i i i 
RTN+5 -z~-- 
RTN+4 -o-- 
RTN+3 -+-- 
RTN+2 -ra-- 
RTN+I "~'" 
,+" ~-E\],\[3 
^ _ ~ /  ,, 4z ' '+  
0 :"~'~"" .. . . . . . . . . . . .  
0 50 100 150 200 250 300 350 400 450 500 550 
grammar size 
Figure 16 
Number of states and number of transitions of the determinized and minimized automata. 
1.6 
1.5 
1.4 
1.3 
s 
1.2 
1.1 
i i 
RTN 
RTN2 -a--- 
RTN3 
RTN+I -~<-- 
RTN+2 -D-- 
RTN+3 -+-- 
RTN+4 -o-- 
RTN+5 -a-- 
1 
50 100 150 200 250 300 350 400 
grammar size 
Figure 17 
Number of recognized sentences divided by number of grammatical sentences. 
cessing. Also allowed is that "pathological" grammatical sentences are rejected that 
seldom occur in practice; an example are sentences requiring multiple levels of self- 
embedding. 
Of the methods we considered that may lead to rejection of grammatical sen- 
tences, i.e., the subset approximations, none seems of much practical value. The most 
serious problem is the complexity of the construction of automata from the compact 
representation for large grammars. Since the tools we used for obtaining the minimal 
42 
Nederhof Experiments with Regular Approximation 
deterministic automata re considered to be of high quality, it seems unlikely that 
alternative implementations could succeed on much larger grammars, especially con- 
sidering the sharp increases in the sizes of the automata for small increases in the size 
of the grammar. Only LC2 could be applied with relatively few resources, but this is a 
very crude approximation, which leads to rejection of many more sentences than just 
those requiring self-embedding. 
Similarly, some of the superset approximations are not applicable to large gram- 
mars because of the high costs of obtaining the minimal deterministic automata. Some 
others provide rather large languages, and therefore do not allow very effective ill- 
tering of ungrammatical input. One method, however, seems to be excellently suited 
for large grammars, namely, the RTN method, considering both the unparameterized 
version and the parameterized version with d = 2. In both cases, the size of the au- 
tomaton grows moderately in the grammar size. For the unparameterized version, the 
compact representation also grows moderately. Furthermore, the percentage of recog- 
nized sentences remains close to the percentage of grammatical sentences. It seems 
therefore that, under the conditions of our experiments, this method is the most suit- 
able regular approximation that is presently available. 
Acknowledgments 
This paper could not have been written 
without he wonderful help of Hans-Ulrich 
Krieger, who created the series of grammars 
that are used in the experiments. I also owe 
to him many thanks for countless 
discussions and for allowing me to pursue 
this work. I am very grateful to the 
anonymous referees for their inspiring 
suggestions. 
This work was funded by the German 
Federal Ministry of Education, Science, 
Research and Technology (BMBF) in the 
framework of the VERBMOBIL Project under 
Grant 01 IV 701 V0. 
References 
Baker, Theodore P. 1981. Extending 
lookahead for LR parsers. Journal of 
Computer and System Sciences, 22:243-259. 
Bermudez, Manuel E. and Karl M. Schimpf. 
1990. Practical arbitrary lookahead LR 
parsing. Journal of Computer and System 
Sciences, 41:230-250. 
Berstel, Jean. 1979. Transductions and 
Context-Free Languages. B. G. Teubner, 
Stuttgart. 
Black, Alan W. 1989. Finite state machines 
from feature grammars. In International 
Workshop on Parsing Technologies, pages 
277-285, Pittsburgh, PA. 
Chomsky, Noam. 1959a. A note on phrase 
structure grammars. Information and 
Control, 2:393-395. 
Chomsky, Noam. 1959b. On certain formal 
properties of grammars. Information and 
Control, 2:137-167. 
Culik, Karel II and Rina Cohen. 1973. 
LR-regular grammars--An extension of 
LR(k) grammars. Journal of Computer and 
System Sciences, 7:66-96. 
Earley, Jay. 1970. An efficient context-free 
parsing algorithm. Communications of the 
ACM, 13(2):94-102, February. 
Grimley-Evans, Edmund. 1997. 
Approximating context-free grammars 
with a finite-state calculus. In Proceedings 
of the 35th Annual Meeting of the Association 
for Computational Linguistics an 8th 
Conference ofthe European Chapter of the 
Association for Computational Linguistics, 
pages 452-459, Madrid, Spain. 
Harrison, Michael A. 1978. Introduction to 
Formal Language Theory. Addison-Wesley. 
Heckert, Erik. 1994. Behandlung von 
Syntaxfehlern fiir LR-Sprachen ohne 
Korrekturversuche. Ph.D. thesis, 
Ruhr-Universit/it Bochum. 
Johnson, Mark. 1998. Finite-state 
approximation of constraint-based 
grammars using left-comer grammar 
transforms. In COLING-ACL "98: 36th 
Annual Meeting of the Association for 
Computational Linguistics and 17th 
International Conference on Computational 
Linguistics, volume 1, pages 619-623, 
Montreal, Quebec, Canada. 
Krauwer, Steven and Louis des Tombe. 1981. 
Transducers and grammars as theories of 
language. Theoretical Linguistics, 8:173-202. 
Langendoen, D. Terence and Yedidyah 
Langsam. 1987. On the design of finite 
transducers for parsing phrase-structure 
languages. In Alexis Manaster-Ramer, 
editor, Mathematics of Language. John 
Benjamins, Amsterdam, pages 191-235. 
Mohri, Mehryar and Fernando C. N. 
43 
Computational Linguistics Volume 26, Number 1 
Pereira. 1998. Dynamic ompilation of 
weighted context-free grammars. In 
COLING-ACL "98: 36th Annual Meeting of 
the Association for Computational Linguistics 
and 17th International Conference on 
Computational Linguistics, volume 2, pages 
891-897, Montreal, Quebec, Canada. 
Mohri, Mehryar, Femando C. N. Pereira, 
and Michael Riley. 1998. A rational design 
for a weighted finite-state transducer 
library. In Derick Wood and Sheng Yu, 
editors, Automata Implementation. Lecture 
Notes in Computer Science, Number 1436. 
Springer Verlag, pages 144-158. 
Nederhof, Mark-Jan. 1994. Linguistic Parsing 
and Program Transformations. Ph.D. thesis, 
University of Nijmegen. 
Nederhof, Mark-Jan. 1997. Regular 
approximations of CFLs: A grammatical 
view. In Proceedings ofthe International 
Workshop on Parsing Technologies, 
pages 159-170, Massachusetts Institute of 
Technology. 
Nederhof, Mark-Jan. 1998. Context-free 
parsing through regular approximation. 
In Proceedings ofthe International Workshop 
on Finite State Methods in Natural Language 
Processing, pages 13-24, Ankara, Turkey. 
Nederhof, Mark-Jan and Giorgio Satta. 1996. 
Efficient abular LR parsing. In Proceedings 
of the 34th Annual Meeting, pages 239-246, 
Santa Cruz, CA. Association for 
Computational Linguistics. 
Pereira, Fernando C. N. and Rebecca N. 
Wright. 1997. Finite-state approximation 
of phrase-structure grammars. In 
Emmanuel Roche and Yves Schabes, 
editors, Finite-State Language Processing. 
MIT Press, pages 149-173. 
Pulman, S. G. 1986. Grammars, parsers, and 
memory limitations. Language and 
Cognitive Processes, 1(3):197-225. 
Purdom, Paul Walton, Jr. and Cynthia A. 
Brown. 1981. Parsing extended LR(k) 
grammars. Acta Informatica, 15:115-127. 
Resnik, Philip. 1992. Left-corner parsing and 
psychological p ausibility. In COLING '92: 
Papers presented tothe Fifteenth \[sic\] 
International Conference on Computational 
Linguistics, pages 191-197, Nantes, France. 
Rosenkrantz, D. J. and P. M. Lewis, II. 1970. 
Deterministic left comer parsing. In IEEE 
Conference Record of the 11th Annual 
Symposium on Switching and Automata 
Theory, pages 139-152. 
Seyfarth, Benjamin R. and Manuel E. 
Bermudez. 1995. Suffix languages in LR 
parsing. International Journal of Computer 
Mathematics, 55:135-153. 
Stolcke, Andreas and Jonathan Segal. 1994. 
Precise N-gram probabilities from 
stochastic context-free grammars. In 
Proceedings ofthe 32nd Annual Meeting, 
pages 74-79, Las Cruces, NM. Association 
for Computational Linguistics. 
Woods, W. A. 1970. Transition etwork 
grammars for natural language analysis. 
Communications of the ACM, 
13(10):591-606. 
44 
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 343?350,
New York, June 2006. c?2006 Association for Computational Linguistics
Estimation of Consistent
Probabilistic Context-free Grammars
Mark-Jan Nederhof
Max Planck Institute
for Psycholinguistics
P.O. Box 310
NL-6500 AH Nijmegen
The Netherlands
MarkJan.Nederhof@mpi.nl
Giorgio Satta
Dept. of Information Engineering
University of Padua
via Gradenigo, 6/A
I-35131 Padova
Italy
satta@dei.unipd.it
Abstract
We consider several empirical estimators
for probabilistic context-free grammars,
and show that the estimated grammars
have the so-called consistency property,
under the most general conditions. Our
estimators include the widely applied ex-
pectation maximization method, used to
estimate probabilistic context-free gram-
mars on the basis of unannotated corpora.
This solves a problem left open in the lit-
erature, since for this method the consis-
tency property has been shown only under
restrictive assumptions on the rules of the
source grammar.
1 Introduction
Probabilistic context-free grammars are one of the
most widely used formalisms in current work in sta-
tistical natural language parsing and stochastic lan-
guage modeling. An important property for a proba-
bilistic context-free grammar is that it be consistent,
that is, the grammar should assign probability of one
to the set of all finite strings or parse trees that it
generates. In other words, the grammar should not
lose probability mass with strings or trees of infinite
length.
Several methods for the empirical estimation of
probabilistic context-free grammars have been pro-
posed in the literature, based on the optimization of
some function on the probabilities of the observed
data, such as the maximization of the likelihood of
a tree bank or a corpus of unannotated sentences. It
has been conjectured in (Wetherell, 1980) that these
methods always provide probabilistic context-free
grammars with the consistency property. A first re-
sult in this direction was presented in (Chaudhuri et
al., 1983), by showing that a probabilistic context-
free grammar estimated by maximizing the likeli-
hood of a sample of parse trees is always consistent.
In later work by (Sa?nchez and Bened??, 1997)
and (Chi and Geman, 1998), the result was in-
dependently extended to expectation maximization,
which is an unsupervised method exploited to es-
timate probabilistic context-free grammars by find-
ing local maxima of the likelihood of a sample of
unannotated sentences. The proof in (Sa?nchez and
Bened??, 1997) makes use of spectral analysis of ex-
pectation matrices, while the proof in (Chi and Ge-
man, 1998) is based on a simpler counting argument.
Both these proofs assume restrictions on the un-
derlying context-free grammars. More specifically,
in (Chi and Geman, 1998) empty rules and unary
rules are not allowed, thus excluding infinite ambi-
guity, that is, the possibility that some string in the
input sample has an infinite number of derivations in
the grammar. The treatment of general form context-
free grammars has been an open problem so far.
In this paper we consider several estimation meth-
ods for probabilistic context-free grammars, and we
show that the resulting grammars have the consis-
tency property. Our proofs are applicable under
the most general conditions, and our results also
include the expectation maximization method, thus
solving the open problem discussed above. We use
an alternative proof technique with respect to pre-
343
vious work, based on an already known renormal-
ization construction for probabilistic context-free
grammars, which has been used in the context of
language modeling.
The structure of this paper is as follows. We pro-
vide some preliminary definitions in Section 2, fol-
lowed in Section 3 by a brief overview of the esti-
mation methods we investigate in this paper. In Sec-
tion 4 we prove some properties of a renormaliza-
tion technique for probabilistic context-free gram-
mars, and use this property to show our main results
in Section 5. Section 6 closes with some concluding
remarks.
2 Preliminaries
In this paper we use mostly standard notation, as for
instance in (Hopcroft and Ullman, 1979) and (Booth
and Thompson, 1973), which we summarize below.
A context-free grammar (CFG) is a 4-tupleG =
(N,?, S,R) where N and ? are finite disjoint sets
of nonterminal and terminal symbols, respectively,
S ? N is the start symbol and R is a finite set of
rules. Each rule has the form A ? ?, where A ? N
and ? ? (? ?N)?. We write V for set ? ?N .
Each CFG G is associated with a left-most de-
rive relation ?, defined on triples consisting of two
strings ?, ? ? V ? and a rule pi ? R. We write ? pi? ?
if and only if ? = uA?? and ? = u???, for some
u ? ??, ?? ? V ?, and pi = (A ? ?). A left-
most derivation for G is a string d = pi1 ? ? ?pim,
m ? 0, such that ?0 pi1? ?1 pi2? ? ? ? pim? ?m, for
some ?0, . . . , ?m ? V ?; d = ? (where ? denotes
the empty string) is also a left-most derivation. In
the remainder of this paper, we will let the term
derivation always refer to left-most derivation. If
?0
pi1? ? ? ? pim? ?m for some ?0, . . . , ?m ? V ?, then
we say that d = pi1 ? ? ?pim derives ?m from ?0 and
we write ?0 d? ?m; d = ? derives any ?0 ? V ? from
itself.
A (left-most) derivation d such that S d? w,
w ? ??, is called a complete derivation. If d is
a complete derivation, we write y(d) to denote the
(unique) string w ? ?? such that S d? w. We
define D(G) to be the set of all complete deriva-
tions for G. The language generated by G is the set
of all strings derived by complete derivations, i.e.,
L(G) = {y(d) | d ? D(G)}. It is well-known that
there is a one-to-one correspondence between com-
plete derivations and parse trees for strings in L(G).
For X ? V and ? ? V ?, we write f(X,?) to
denote the number of occurrences of X in ?. For
(A ? ?) ? R and a derivation d, f(A ? ?, d)
denotes the number of occurrences of A ? ? in d.
We let f(A, d) =?? f(A ? ?, d).
A probabilistic CFG (PCFG) is a pair G =
(G, pG), where G is a CFG and pG is a function
from R to real numbers in the interval [0, 1]. We
say that G is proper if, for every A ? N , we have
?
A??
pG(A ? ?) = 1. (1)
Function pG can be used to assign probabilities to
derivations of the underlying CFG G, in the follow-
ing way. For d = pi1 ? ? ?pim ? R?,m ? 0, we define
pG(d) =
m
?
i=1
pG(pii). (2)
Note that pG(?) = 1. The probability of a string
w ? ?? is defined as
pG(w) =
?
y(d)=w
pG(d). (3)
A PCFG is consistent if
?
w
pG(w) = 1. (4)
Consistency implies that the PCFG defines a proba-
bility distribution over both sets D(G) and L(G).
If a PCFG is proper, then consistency means that
no probability mass is lost in derivations of infinite
length. All PCFGs in this paper are implicitly as-
sumed to be proper, unless otherwise stated.
3 Estimation of PCFGs
In this section we give a brief overview of some esti-
mation methods for PCFGs. These methods will be
later investigated to show that they always provide
consistent PCFGs.
In natural language processing applications, esti-
mation of a PCFG is usually carried out on the ba-
sis of a tree bank, which in this paper we assume to
be a sample, that is, a finite multiset, of complete
derivations. Let D be such a sample, and let D be
344
the underlying set of derivations. For d ? D, we
let f(d,D) be the multiplicity of d in D, that is, the
number of occurrences of d in D. We define
f(A ? ?,D) =
?
d?D
f(d,D) ? f(A ? ?, d), (5)
and let f(A,D) =?? f(A ? ?,D).
Consider a CFG G = (N,?,R, S) defined by
all and only the nonterminals, terminals and rules
observed in D. The criterion of maximum likeli-
hood estimation (MLE) prescribes the construction
of a PCFG G = (G, pG) such that pG maximizes the
likelihood of D, defined as
pG(D) =
?
d?D
pG(d)f(d,D), (6)
subject to the properness conditions
?
? pG(A ?
?) = 1 for eachA ? N . The maximization problem
above has a unique solution, provided by the estima-
tor (see for instance (Chi and Geman, 1998))
pG(A ? ?) =
f(A ? ?,D)
f(A,D) . (7)
We refer to this as the supervised MLE method.
In applications in which a tree bank is not avail-
able, one might still use the MLE criterion to train
a PCFG in an unsupervised way, on the basis of a
sample of unannotated sentences, also called a cor-
pus. Let us call C such a sample and C the underly-
ing set of sentences. For w ? C, we let f(w, C) be
the multiplicity of w in C.
Assume a CFG G = (N,?,R, S) that is able
to generate all of the sentences in C, and possibly
more. The MLE criterion prescribes the construc-
tion of a PCFG G = (G, pG) such that pG maxi-
mizes the likelihood of C, defined as
pG(C) =
?
w?C
pG(w)f(w,C), (8)
subject to the properness conditions as in the super-
vised case above. The above maximization prob-
lem provides a system of |R| nonlinear equations
(see (Chi and Geman, 1998))
pG(A ? ?) =
?
w?C f(w, C) ? EpG(d |w) f(A ? ?, d)
?
w?C f(w, C) ? EpG(d |w) f(A, d)
, (9)
where Ep denotes an expectation computed under
distribution p, and pG(d |w) is the probability of
derivation d conditioned by sentence w (so that
pG(d |w) > 0 only if y(d) = w). The solution to
the above system is not unique, because of the non-
linearity. Furthermore, each solution of (9) identi-
fies a point where the curve in (8) has partial deriva-
tives of zero, but this does not necessarily corre-
spond to a local maximum, let alne an absolute
maximum. (A point with partial derivatives of zero
that is not a local maximum could be a local min-
imum or even a so-called saddle point.) In prac-
tice, this system is typically solved by means of an
iterative algorithm called inside/outside (Charniak,
1993), which implements the expectation maximiza-
tion (EM) method (Dempster et al, 1977). Starting
with an initial function pG that probabilistically ex-
tends G, a so-called growth transformation is com-
puted, defined as
pG(A ? ?) =
?
w?C f(w, C)?
?
y(d)=w
pG(d)
pG(w) ?f(A ? ?, d)
?
w?C f(w, C)?
?
y(d)=w
pG(d)
pG(w) ?f(A, d)
. (10)
Following (Baum, 1972), one can show that
pG(C) ? pG(C). Thus, by iterating the growth trans-
formation above, we are guaranteed to reach a local
maximum for (8), or possibly a saddle point. We
refer to this as the unsupervised MLE method.
We now discuss a third estimation method for
PCFGs, which was proposed in (Corazza and Satta,
2006). This method can be viewed as a general-
ization of the supervised MLE method to probabil-
ity distributions defined over infinite sets of com-
plete derivations. Let D be an infinite set of com-
plete derivations using nonterminal symbols in N ,
start symbol S ? N and terminal symbols in ?.
We assume that the set of rules that are observed
in D is drawn from some finite set R. Let pD be
a probability distribution defined over D, that is,
a function from set D to interval [0, 1] such that
?
d?D pD(d) = 1.
Consider the CFG G = (N,?,R, S). Note that
D ? D(G). We wish to extend G to some PCFG
G = (G, pG) in such a way that pD is approxi-
mated by pG (viewed as a distribution over complete
derivations) as well as possible according to some
criterion. One possible criterion is minimization of
345
the cross-entropy between pD and pG, defined as
the expectation, under distribution pD, of the infor-
mation of the derivations in D computed under dis-
tribution pG, that is
H(pD || pG) = EpD log
1
pG(d)
= ?
?
d?D
pD(d) ? log pG(d). (11)
We thus want to assign to the parameters pG(A ?
?), A ? ? ? R, the values that minimize (11), sub-
ject to the conditions
?
? pG(A ? ?) = 1 for each
A ? N . Note that minimization of the cross-entropy
above is equivalent to minimization of the Kullback-
Leibler distance between pD and pG. Also note that
the likelihood of an infinite set of derivations would
always be zero and therefore cannot be considered
here.
The solution to the above minimization problem
provides the estimator
pG(A ? ?) =
EpD f(A ? ?, d)
EpD f(A, d)
. (12)
A proof of this result appears in (Corazza and Satta,
2006), and is briefly summarized in Appendix A,
in order to make this paper self-contained. We call
the above estimator the cross-entropy minimization
method.
The cross-entropy minimization method can be
viewed as a generalization of the supervised MLE
method in (7), as shown in what follows. Let D and
D be defined as for the supervisedMLEmethod. We
define a distribution over D as
pD(d) =
f(d,D)
|D| . (13)
Distribution pD is usually called the empirical dis-
tribution associated withD. Applying the estimator
in (12) to pD, we obtain
pG(A ? ?) =
=
?
d?D pD(d) ? f(A ? ?, d)
?
d?D pD(d) ? f(A, d)
=
?
d?D
f(d,D)
|D| ? f(A ? ?, d)
?
d?D
f(d,D)
|D| ? f(A, d)
=
?
d?D f(d,D) ? f(A ? ?, d)
?
d?D f(d,D) ? f(A, d)
. (14)
This is the supervised MLE estimator in (7). This re-
minds us of the well-known fact that maximizing the
likelihood of a (finite) sample through a PCFG dis-
tribution amounts to minimizing the cross-entropy
between the empirical distribution of the sample and
the PCFG distribution itself.
4 Renormalization
In this section we recall a renormalization technique
for PCFGs that was used before in (Abney et al,
1999), (Chi, 1999) and (Nederhof and Satta, 2003)
for different purposes, and is exploited in the next
section to prove our main results. In the remainder
of this section, we assume a fixed, not necessarily
proper PCFG G = (G, pG), with G = (N,?, S,R).
We define the renormalization of G as the PCFG
R(G) = (G, pR) with pR specified by
pR(A ? ?) =
pG(A ? ?) ?
?
d,w pG(?
d? w)
?
d,w pG(A
d? w)
. (15)
It is not difficult to see that R(G) is a proper PCFG.
We now show an important property of R(G), dis-
cussed before in (Nederhof and Satta, 2003) in the
context of so-called weighted context-free gram-
mars.
Lemma 1 For each derivation d with A d? w, A ?
N and w ? ??, we have
pR(A d? w) =
pG(A d? w)
?
d?,w? pG(A
d?? w?)
. (16)
Proof. The proof is by induction on the length of d,
written |d|. If |d| = 1 we must have d = (A ? w),
and thus pR(d) = pR(A ? w). In this case, the
statement of the lemma directly follows from (15).
Assume now |d| > 1 and let pi = (A ? ?)
be the first rule used in d. Note that there must
be at least one nonterminal symbol in ?. We can
then write ? as u0A1u1A2 ? ? ?uq?1Aquq, for q ? 1,
Ai ? N , 1 ? i ? q, and uj ? ??, 0 ?
j ? q. In words, A1, . . . , Aq are all of the occur-
rences of nonterminals in ?, as they appear from
left to right. Consequently, we can write d in the
form d = pi ? d1 ? ? ? dq for some derivations di,
1 ? i ? q, with Ai di? wi, |di| ? 1 and with
346
w = u0w1u1w2 ? ? ?uq?1wquq. Below we use the
fact that pR(uj ?? uj) = pG(uj ?? uj) = 1 for
each j with 0 ? j ? q, and further using the def-
inition of pR and the inductive hypothesis, we can
write
pR(A d? w) =
= pR(A ? ?) ?
q
?
i=1
pR(Ai di? wi)
= pG(A ? ?) ?
?
d?,w? pG(?
d?? w?)
?
d?,w? pG(A
d?? w?)
?
?
q
?
i=1
pR(Ai di? wi)
= pG(A ? ?) ?
?
d?,w? pG(?
d?? w?)
?
d?,w? pG(A
d?? w?)
?
?
q
?
i=1
pG(Ai di? wi)
?
d?,w? pG(Ai
d?? w?)
= pG(A ? ?) ?
?
d?,w? pG(?
d?? w?)
?
d?,w? pG(A
d?? w?)
?
?
?q
i=1 pG(Ai
di? wi)
?q
i=1
?
d?,w? pG(Ai
d?? w?)
= pG(A ? ?) ?
?
d?,w? pG(?
d?? w?)
?
d?,w? pG(A
d?? w?)
?
?
?q
i=1 pG(Ai
di? wi)
?
d?,w? pG(?
d?? w?)
= pG(A ? ?) ?
?q
i=1 pG(Ai
di? wi)
?
d?,w? pG(A
d?? w?)
?
= pG(A
d? w)
?
d?,w? pG(A
d?? w?)
. (17)
As an easy corollary of Lemma 1, we have that
R(G) is a consistent PCFG, as we can write
?
d,w
pR(S d? w) =
=
?
d,w
pG(S d? w)
?
d?,w? pG(S
d?? w?)
=
?
d,w pG(S
d? w)
?
d?,w? pG(S
d?? w?)
= 1. (18)
5 Consistency
In this section we prove the main results of this
paper, namely that all of the estimation methods
discussed in Section 3 always provide consistent
PCFGs. We start with a technical lemma, central
to our results, showing that a PCFG that minimizes
the cross-entropy with a distribution over any set of
derivations must be consistent.
Lemma 2 Let G = (G, pG) be a proper PCFG
and let pD be a probability distribution defined over
some set D ? D(G). If G minimizes function
H(pD || pG), then G is consistent.
Proof. LetG = (N,?, S,R), and assume that G is
not consistent. We establish a contradiction. Since G
is not consistent, we must have
?
d,w pG(S
d? w) <
1. Let then R(G) = (G, pR) be the renormalization
of G, defined as in (15). For any derivation S d? w,
w ? ??, with d in D, we can use Lemma 1 and
write
pR(S d? w) =
= 1
?
d?,w? pG(S
d?? w?)
? pG(S d? w)
> pG(S d? w). (19)
In words, every complete derivation d in D has a
probability in R(G) that is strictly greater than in
G. But this means H(pD || pR) < H(pD || pG),
against our hypothesis. Therefore, G is consistent
and pG is a probability distribution over set D(G).
Thus function H(pD || pG) can be interpreted as the
cross-entropy. (Observe that in the statement of the
lemma we have avoided the term ?cross-entropy?,
since cross-entropies are only defined for probability
distributions.)
Lemma 2 directly implies that the cross-entropy
minimization method in (12) always provides a con-
sistent PCFG, since it minimizes cross-entropy for a
distribution defined over a subset of D(G). We have
already seen in Section 3 that the supervised MLE
method is a special case of the cross-entropy min-
imization method. Thus we can also conclude that
a PCFG trained with the supervised MLE method is
347
always consistent. This provides an alternative proof
of a property that was first shown in (Chaudhuri et
al., 1983), as discussed in Section 1.
We now prove the same result for the unsuper-
vised MLE method, without any restrictive assump-
tion on the rules of our CFGs. This solves a problem
that was left open in the literature (Chi and Geman,
1998); see again Section 1 for discussion. Let C and
C be defined as in Section 3. We define the empiri-
cal distribution of C as
pC(w) =
f(w, C)
|C| . (20)
Let G = (N,?, S,R) be a CFG such that C ?
L(G). Let D(C) be the set of all complete deriva-
tions for G that generate sentences in C, that is,
D(C) = {d | d ? D(G), y(d) ? C}.
Further, assume some probabilistic extension G =
(G, pG) of G, such that pG(d) > 0 for every d ?
D(C). We define a distribution over D(C) by
pD(C)(d) = pC(y(d)) ?
pG(d)
pG(y(d))
. (21)
It is not difficult to verify that
?
d?D(C)
pD(C)(d) = 1. (22)
We now apply to G the estimator in (12), in order
to obtain a new PCFG G? = (G, p?G) that minimizes
the cross-entropy between pD(C) and p?G. According
to Lemma 2, we have that G? is a consistent PCFG.
Distribution p?G is specified by
p?G(A ? ?) =
=
?
d?D(C) pD(C)(d)?f(A ? ?, d)
?
d?D(C) pD(C)(d)?f(A, d)
=
?
d?D(C)
f(y(d),C)
|C| ?
pG(d)
pG(y(d)) ?f(A ? ?, d)
?
d?D(C)
f(y(d),C)
|C| ?
pG(d)
pG(y(d)) ?f(A, d)
=
?
w?C f(w, C)?
?
y(d)=w
pG(d)
pG(w) ?f(A ? ?, d)
?
w?C f(w, C)?
?
y(d)=w
pG(d)
pG(w) ?f(A, d)
=
?
w?C f(w, C)?EpG(d |w)f(A ? ?, d)
?
w?C f(w, C)?EpG(d |w)f(A, d)
. (23)
Since distribution pG was arbitrarily chosen, sub-
ject to the only restriction that pG(d) > 0 for ev-
ery d ? D(C), we have that (23) is the growth
estimator (10) already discussed in Section 3. In
fact, for each w ? L(G) and d ? D(G), we have
pG(d |w) = pG(d)pG(w) . We conclude with the desired
result, namely that a general form PCFG obtained at
any iteration of the EM method for the unsupervised
MLE is always consistent.
6 Conclusions
In this paper we have investigated a number of
methods for the empirical estimation of probabilis-
tic context-free grammars, and have shown that the
resulting grammars have the so-called consistency
property. This property guarantees that all the prob-
ability mass of the grammar is used for the finite
strings it derives. Thus if the grammar is used in
combination with other probabilistic models, as for
instance in a speech processing system, consistency
allows us to combine or compare scores from differ-
ent modules in a sound way.
To obtain our results, we have used a novel proof
technique that exploits an already known construc-
tion for the renormalization of probabilistic context-
free grammars. Our proof technique seems more
intuitive than arguments previously used in the lit-
erature to prove the consistency property, based on
counting arguments or on spectral analysis. It is
not difficult to see that our proof technique can
also be used with probabilistic rewriting formalisms
whose underlying derivations can be characterized
by means of context-free rewriting. This is for
instance the case with probabilistic tree-adjoining
grammars (Schabes, 1992; Sarkar, 1998), for which
consistency results have not yet been shown in the
literature.
A Cross-entropy minimization
In order to make this paper self-contained, we sketch
a proof of the claim in Section 3 that the estimator
in (12) minimizes the cross entropy in (11). A full
proof appears in (Corazza and Satta, 2006).
Let D, pD and G = (N,?,R, S) be defined as
in Section 3. We want to find a proper PCFG G =
(G, pG) such that the cross-entropy H(pD || pG) is
minimal. We use Lagrange multipliers ?A for each
A ? N and define the form
? =
?
A?N
?A ? (
?
?
pG(A ? ?)? 1) +
348
?
?
d?D
pD(d) ? log pG(d). (24)
We now consider all the partial derivatives of?. For
each A ? N we have
??
??A
=
?
?
pG(A ? ?)? 1. (25)
For each (A ? ?) ? R we have
??
?pG(A ? ?)
=
= ?A ?
?
?pG(A ? ?)
?
d?D
pD(d) ? log pG(d)
= ?A ?
?
d?D
pD(d) ?
?
?pG(A ? ?)
log pG(d)
= ?A ?
?
d?D
pD(d) ?
?
?pG(A ? ?)
log
?
(B??)?R
pG(B ? ?)f(B??,d)
= ?A ?
?
d?D
pD(d) ?
?
?pG(A ? ?)
?
(B??)?R
f(B ? ?, d) ? log pG(B ? ?)
= ?A ?
?
d?D
pD(d) ?
?
(B??)?R
f(B ? ?, d) ?
?
?pG(A ? ?)
log pG(B ? ?)
= ?A ?
?
d?D
pD(d) ? f(A ? ?, d) ?
? 1ln(2) ?
1
pG(A ? ?)
= ?A ?
1
ln(2) ?
1
pG(A ? ?)
?
?
?
d?D
pD(d) ? f(A ? ?, d)
= ?A ?
1
ln(2) ?
1
pG(A ? ?)
?
? EpD f(A ? ?, d). (26)
By setting to zero all of the above partial derivatives,
we obtain a system of |N |+|R| equations, which we
must solve. From ???pG(A??) = 0 we obtain
?A ? ln(2) ? pG(A ? ?) =
EpDf(A ? ?, d). (27)
We sum over all strings ? such that (A ? ?) ? R,
deriving
?A ? ln(2) ?
?
?
pG(A ? ?) =
=
?
?
EpD f(A ? ?, d)
=
?
?
?
d?D
pD(d) ? f(A ? ?, d)
=
?
d?D
pD(d) ?
?
?
f(A ? ?, d)
=
?
d?D
pD(d) ? f(A, d)
= EpD f(A, d). (28)
From each equation ????A = 0 we obtain
?
? pG(A ? ?) = 1 for each A ? N (our original
constraints). Combining this with (28) we obtain
?A ? ln(2) = EpD f(A, d). (29)
Replacing (29) into (27) we obtain, for every rule
(A ? ?) ? R,
pG(A ? ?) =
EpD f(A ? ?, d)
EpD f(A, d)
. (30)
This is the estimator introduced in Section 3.
References
S. Abney, D. McAllester, and F. Pereira. 1999. Relating
probabilistic grammars and automata. In 37th Annual
Meeting of the Association for Computational Linguis-
tics, Proceedings of the Conference, pages 542?549,
Maryland, USA, June.
L. E. Baum. 1972. An inequality and associated max-
imization technique in statistical estimations of prob-
abilistic functions of Markov processes. Inequalities,
3:1?8.
T.L. Booth and R.A. Thompson. 1973. Applying prob-
abilistic measures to abstract languages. IEEE Trans-
actions on Computers, C-22(5):442?450, May.
E. Charniak. 1993. Statistical Language Learning. MIT
Press.
R. Chaudhuri, S. Pham, and O. N. Garcia. 1983. Solution
of an open problem on probabilistic grammars. IEEE
Transactions on Computers, 32(8):748?750.
Z. Chi and S. Geman. 1998. Estimation of probabilis-
tic context-free grammars. Computational Linguistics,
24(2):299?305.
349
Z. Chi. 1999. Statistical properties of probabilistic
context-free grammars. Computational Linguistics,
25(1):131?160.
A. Corazza and G. Satta. 2006. Cross-entropy and es-
timation of probabilistic context-free grammars. In
Proc. of HLT/NAACL 2006 Conference (this volume),
New York.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society, B,
39:1?38.
J.E. Hopcroft and J.D. Ullman. 1979. Introduction
to Automata Theory, Languages, and Computation.
Addison-Wesley.
M.-J. Nederhof and G. Satta. 2003. Probabilistic pars-
ing as intersection. In 8th International Workshop on
Parsing Technologies, pages 137?148, LORIA, Nancy,
France, April.
J.-A. Sa?nchez and J.-M. Bened??. 1997. Consistency
of stochastic context-free grammars from probabilis-
tic estimation based on growth transformations. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 19(9):1052?1055, September.
A. Sarkar. 1998. Conditions on consistency of proba-
bilistic tree adjoining grammars. In Proc. of the 36th
ACL, pages 1164?1170, Montreal, Canada.
Y. Schabes. 1992. Stochastic lexicalized tree-adjoining
grammars. In Proc. of the 14th COLING, pages 426?
432, Nantes, France.
C. S. Wetherell. 1980. Probabilistic languages: A re-
view and some open questions. Computing Surveys,
12(4):361?379.
350
Kullback-Leibler Distance
between Probabilistic Context-Free Grammars
and Probabilistic Finite Automata
Mark-Jan Nederhof
Faculty of Arts
University of Groningen
P.O. Box 716
NL-9700 AS Groningen, The Netherlands
markjan@let.rug.nl
Giorgio Satta
Department of Information Engineering
University of Padua
via Gradenigo, 6/A
I-35131 Padova, Italy
satta@dei.unipd.it
Abstract
We consider the problem of computing the
Kullback-Leibler distance, also called the
relative entropy, between a probabilistic
context-free grammar and a probabilistic fi-
nite automaton. We show that there is
a closed-form (analytical) solution for one
part of the Kullback-Leibler distance, viz.
the cross-entropy. We discuss several ap-
plications of the result to the problem of
distributional approximation of probabilis-
tic context-free grammars by means of prob-
abilistic finite automata.
1 Introduction
Among the many formalisms used for descrip-
tion and analysis of syntactic structure of natu-
ral language, the class of context-free grammars
(CFGs) is by far the best understood and most
widely used. Many formalisms with greater gen-
erative power, in particular the different types
of unification grammars, are ultimately based
on CFGs.
Regular expressions, with their procedural
counter-part of finite automata (FAs), are not
able to describe hierarchical, tree-shaped struc-
ture, and thereby seem less suitable than CFGs
for full analysis of syntactic structure. How-
ever, there are many applications where only
partial or approximated analysis of structure is
needed, and where full context-free processing
could be prohibitively expensive. Such appli-
cations can for example be found in real-time
speech recognition systems: of the many hy-
potheses returned by a speech recognizer, shal-
low syntactic analysis may be used to select a
small subset of those that seem most promis-
ing for full syntactic processing in a next phase,
thereby avoiding further computational costs
for the less promising hypotheses.
As FAs cannot describe structure as such, it
is impractical to write the automata required
for such applications by hand, and even diffi-
cult to derive them automatically by training.
For this reason, the used FAs are often derived
from CFGs, by means of some form of approx-
imation. An overview of different methods of
approximating CFGs by FAs, along with an ex-
perimental comparison, was given by (Nederhof,
2000).
The next step is to assign probabilities to the
transitions of the approximating FA, as the ap-
plication outlined above requires a qualitative
distinction between hypotheses rather than the
purely boolean distinction of language member-
ship. Under certain circumstances, this may be
done by carrying over the probabilities from an
input probabilistic CFG (PCFG), as shown for
the special case of n-grams by (Rimon and Herz,
1991; Stolcke and Segal, 1994), or by training
of the FA on a corpus generated by the PCFG
(Jurafsky et al, 1994). See also (Mohri and
Nederhof, 2001) for discussion of related ideas.
An obvious question to ask is then how
well the resulting PFA approximates the input
PCFG, possibly for different methods of deter-
mining an FA and different ways of attaching
probabilities to the transitions. Until now, any
direct way of measuring the distance between
a PCFG and a PFA has been lacking. As we
will argue in this paper, the natural distance
measure between probability distributions, the
Kullback-Leibler (KL) distance, is difficult to
compute. (The KL distance is also called rela-
tive entropy.) We can however derive a closed-
form (analytical) solution for the cross entropy
of a PCFG and a PFA, provided the FA under-
lying the PFA is deterministic. The difference
between the cross-entropy and the KL distance
is the entropy of the PCFG, which does not rely
on the PFA. This means that if we are interested
in the relative quality of different approximat-
ing PFAs with respect to a single input PCFG,
the cross-entropy may be used instead of the
KL distance. The constraint of determinism is
not a problem in practice, as any FA can be
determinized, and FAs derived by approxima-
tion algorithms are normally determinized (and
minimized).
As a second possible application, we now look
more closely into the matter of determinization
of finite-state models. Not all PFAs can be de-
terminized, as discussed by (Mohri, 1997). This
is unfortunate, as deterministic (P)FAs process
input with time and space costs independent
of the size of the automaton, whereas these
costs are linear in the size of the automaton
in the nondeterministic case, which may be too
high for some real-time applications. Instead
of distribution-preserving determinization, we
may therefore approximate a nondeterministic
PFA by a deterministic PFA whose probability
distribution is close to, but not necessarily iden-
tical to, that of the first PFA. Again, an impor-
tant question is how close the two models are to
each other. It was argued before by (Juang and
Rabiner, 1985; Falkhausen et al, 1995; Vihola
et al, 2002) that the KL distance between finite-
state models is difficult to compute in general.
The theory developed in this paper shows how-
ever that the cross-entropy between the input
PFA and the approximating deterministic PFA
can be expressed in closed form, relying on the
fact that a PFA can be seen as a special case of
a PCFG. Thereby, different approximating de-
terministic PFAs can be compared for closeness
to the input PFA. We can even compute the
KL distance between two unambiguous PFAs,
in closed form. (It is not difficult to see that
ambiguity is a decidable property for FAs.)
The structure of this paper is as follows.
We provide some preliminary definitions in Sec-
tion 2. Section 3 discusses the expected fre-
quency of a rule in derivations allowed by a
PCFG, and explains how such values can be ef-
fectively computed. The KL distance between
a PCFG and a PFA is closely related to the
entropy of the PCFG, which we discuss in Sec-
tion 4. Essential to our approach is the inter-
section of PCFGs and PFAs, to be discussed in
Section 5. As we show in Section 6, the part
of the KL distance expressing the cross-entropy
can be computed in closed form, based on this
intersection. Section 7 concludes this paper.
2 Preliminaries
Throughout the paper we use mostly stan-
dard formal language notation, as for instance
in (Hopcroft and Ullman, 1979; Booth and
Thompson, 1973), which we summarize below.
A context-free grammar (CFG) is a 4-tuple
G = (?,N, S,R) where ? and N are finite dis-
joint sets of terminals and nonterminals, respec-
tively, S ? N is the start symbol and R is a fi-
nite set of rules. Each rule has the form A? ?,
where A ? N and ? ? (? ?N)?.
The ?derives? relation ? associated with G
is defined on triples consisting of two strings
?, ? ? (? ? N)? and a rule pi ? R. We write
? pi? ? if and only if ? is of the form uA?
and ? is of the form u??, for some u ? ??,
? ? (? ? N)?, and pi = (A ? ?). A left-most
derivation (for G) is a string d = pi1 ? ? ?pim,
m ? 0, such that ?0
pi1? ?1
pi2? ? ? ?
pim? ?m, for
some ?0, . . . , ?m ? (? ? N)?; d =  (where 
denotes the empty string) is also a left-most
derivation. In the remainder of this paper,
we will let the term ?derivation? refer to ?left-
most derivation?, unless specified otherwise. If
?0
pi1? ? ? ?
pim? ?m for some ?0, . . . , ?m ? (??N)?,
then we say that d = pi1 ? ? ?pim derives ?m from
?0 and we write ?0
d? ?m; d =  derives any
?0 ? (? ?N)? from itself.
A (left-most) derivation d such that S d? w,
w ? ??, is called a complete derivation. If d is
a complete derivation, we write y(d) to denote
the (unique) string w ? ?? such that S d? w.
The language generated by G is the set of all
strings y(d) derived by complete derivations,
i.e., L(G) = {w |S d? w, d ? R?, w ? ??}.
It is well-known that there is a one-to-one cor-
respondence between complete derivations and
parse trees for strings in L(G).
A probabilistic CFG (PCFG) is a pair Gp =
(G, pG), where G is a CFG and pG is a function
from R to real numbers in the interval [0, 1].
A PCFG is proper if
?
pi=(A??) pG(pi) = 1 for
all A ? N . Function pG can be used to as-
sociate probabilities to derivations of the un-
derlying CFG G, in the following way. For
d = pi1 ? ? ?pim ? R?, m ? 0, we define pG(d) =
?m
i=1 pG(pii) if S
d? w for some w ? ??, and
pG(d) = 0 otherwise. The probability of a string
w ? ?? is defined as pG(w) =
?
d:y(d)=w pG(d).
A PCFG is consistent if
?
w pG(w) = 1. Con-
sistency implies that the PCFG defines a proba-
bility distribution on the set of terminal strings
as well as on the set of grammar derivations. If
a PCFG is proper, then consistency means that
no probability mass is lost in ?infinite? deriva-
tions.
A finite automaton (FA) is a 5-tuple M = (?,
Q, q0, Qf , T ), where ? and Q are two finite sets
of terminals and states, respectively, q0 is the
initial state, Qf ? Q is the set of final states,
and T is a finite set of transitions, each of the
form s a7? t, where s, t ? Q and a ? ?. A
probabilistic finite automaton (PFA) is a pair
Mp = (M,pM ), where M is an FA and pM is a
function from T to real numbers in the interval
[0, 1].1
For a fixed (P)FA M , we define a configu-
ration to be an element of Q ? ??, and we
define the relation ` on triples consisting of
two configurations and a transition ? ? T by:
(s, w)
?
` (t, w?) if and only if w is of the form aw?,
for some a ? ?, and ? = (s a7? t). A complete
computation is a string c = ?1 ? ? ? ?m, m ? 0,
such that (s0, w0)
?1
` (s1, w1)
?2
` ? ? ?
?m
` (sm, wm),
for some (s0, w0), . . . , (sm, wm) ? Q???, with
s0 = q0, sm ? Qf and wm = , and we write
(s0, w0)
c
` (sm, wm). The language accepted by
M is L(M) = {w ? ?? | (q?, w)
c
` (s, ), c ?
T ?, s ? Qf}.
For a PFA Mp = (M,pM ), and c = ?1 ? ? ? ?m ?
T ?, m ? 0, we define pM (c) =
?m
i=1 pM (?i) if
c is a complete computation, and pM (c) = 0
otherwise. A PFA is consistent if
?
c pM (c) = 1.
We say M is unambiguous if for each w ? ??,
?s?Qf [(q0, w)
c
` (s, )] for at most one c ? T ?.
We say M is deterministic if for each s and a,
there is at most one transition s a7? t. Deter-
minism implies unambiguity. It can be more
readily checked whether an FA is determinis-
tic than whether it is unambiguous. Further-
more, any FA can be effectively turned into a
deterministic FA accepting the same language.
Therefore, this paper will assume that FAs are
deterministic, although technically, unambigu-
ity is sufficient for our constructions to apply.
3 Expectation of rule frequency
Here we discuss how we can compute the ex-
pectation of the frequency of a rule or a non-
terminal over all derivations of a probabilistic
context-free grammar. These quantities will be
used later by our algorithms.
1Our definition of PFAs amounts to a slight loss of
generality with respect to standard definitions, in that
there are no epsilon transitions and no probability func-
tion on states being final. We want to avoid these con-
cepts as they would cause some technical complications
later in this article. There is no loss of generality how-
ever if we may assume an end-of-sentence marker, which
is often the case in practice.
Let (A ? ?) ? R be a rule of PCFG Gp,
and let d ? R? be a complete derivation in Gp.
We define f(A? ?; d) as the number of occur-
rences, or frequency , of A ? ? in d. Similarly,
the frequency of nonterminal A in d is defined
as f(A; d) =
?
? f(A? ?; d). We consider the
following related quantities
EpG f(A? ?; d) =
?
d
pG(d) ? f(A? ?; d),
EpG f(A; d) =
?
d
pG(d) ? f(A; d)
=
?
?
EpG f(A? ?; d).
A method for the computation of these quan-
tities is reported in (Hutchins, 1972), based on
the so-called momentum matrix. We propose
an alternative method here, based on an idea
related to the inside-outside algorithm (Baker,
1979; Lari and Young, 1990; Lari and Young,
1991). We observe that we can factorize a
derivation d at each occurrence of rule A ? ?
into an ?innermost? part d2 and two ?outermost?
parts d1 and d3. We can then write
EpG f(A? ?; d) =
?
d=pi1???pim,m1,m2,w,?,v,x:
S
d1?wA?, with d1=pi1???pim1?1,
(A??)=pim1 ,
?
d2?v, with d2=pim1+1???pim2 ,
?
d3?x, with d3=pim2+1???pim
m?
i=1
pG(pii).
Next we group together all of the innermost and
all of the outermost derivations and write
EpG f(A? ?; d) =
outGp(A) ? pG(A? ?) ? inGp(?)
where
outGp(A) =
?
d=pi1???pim,d?=pi?1???pi
?
m?
,w,?,x:
S
d
?wA?, ?
d?
?x
m?
i=1
pG(pii) ?
m??
i=1
pG(pi
?
i)
and
inGp(?) =
?
d=pi1???pim,v:
?
d
?v
m?
i=1
pG(pii).
Both outGp(A) and inGp(?) can be described in
terms of recursive equations, of which the least
fixed-points are the required values. If Gp is
proper and consistent, then inGp(?) = 1 for
each ? ? (? ? N)?. Quantities outGp(A) for
every A can all be (exactly) calculated by solv-
ing a linear system, requiring an amount of time
proportional to the cube of the size of Gp; see
for instance (Corazza et al, 1991).
On the basis of all the above quantities, a
number of useful statistical properties of Gp can
be easily computed, such as the expected length
of derivations, denoted EDL(Gp) and the ex-
pected length of sentences, denoted EWL(Gp),
discussed before by (Wetherell, 1980). These
quantities satisfy the relations
EDL(Gp) = EpG |d| =
?
A??
outGp(A) ? pG(A? ?) ? inGp(?),
EWL(Gp) = EpG |y(d)| =
?
A??
outGp(A) ? pG(A? ?) ? inGp(?) ? |?|? ,
where for a string ? ? (N ? ?)? we write |?|?
to denote the number of occurrences of terminal
symbols in ?.
4 Entropy of PCFGs
In this section we introduce the notion of deriva-
tional entropy of a PCFG, and discuss an algo-
rithm for its computation.
Let Gp = (G, pG) be a PCFG. For a nonter-
minal A of G, let us define the entropy of A as
the entropy of the distribution pG on all rules
of the form A? ?, i.e.,
H(A) = EpG log
1
pG(A? ?)
=
?
?
pG(A? ?) ? log
1
pG(A? ?)
.
The derivational entropy of Gp is defined as
the expectation of the information of the com-
plete derivations generated by Gp, i.e.,
Hd(Gp) = EpG log
1
pG(d)
=
?
d
pG(d) ? log
1
pG(d)
. (1)
We now characterize derivational entropy using
expected rule frequencies as
Hd(Gp) =
?
d
pG(d) ? log
1
pG(d)
=
?
d
pG(d) ? log
?
A??
(
1
pG(A? ?)
)f(A??;d)
=
?
d
pG(d) ?
?
A??
f(A? ?; d) ? log
1
pG(A? ?)
=
?
A??
log
1
pG(A? ?)
?
?
d
pG(d) ? f(A? ?; d) =
?
A??
log
1
pG(A? ?)
? EpG f(A? ?; d) =
?
A
?
?
log
1
pG(A? ?)
? outGp(A) ? pG(A? ?)?
inGp(?) =
?
A
outGp(A) ?
?
?
pG(A? ?) ? log
1
pG(A? ?)
?
inGp(?).
As already discussed, under the assumption
that Gp is proper and consistent we have
inGp(?) = 1 for every ?. Thus we can write
Hd(Gp) =
?
A
outGp(A) ?H(A). (2)
The computation of outGp(A) was discussed
in Section 3, and also H(A) can easily be calcu-
lated.
Under the restrictive assumption that a
PCFG is proper and consistent, the characteri-
zation in (2) was already known from (Grenan-
der, 1976, Theorem 10.7, pp. 90?92). The proof
reported in that work is different from ours and
uses a momentum matrix (Section 3). Our char-
acterization above is more general and uses sim-
pler notation than the one in (Grenander, 1976).
The sentential entropy , or entropy for short,
of Gp is defined as the expectation of the infor-
mation of the strings generated by Gp, i.e.,
H(Gp) = EpG log
1
pG(w)
=
?
w
pG(w) ? log
1
pG(w)
, (3)
assuming 0 ? log 10 = 0, for strings w not gen-
erated by Gp. It is not difficult to see that
H(Gp) ? Hd(Gp) and equality holds if and only
if G is unambiguous (Soule, 1974, Theorem 2.2).
As ambiguity of CFGs is undecidable, it follows
that we cannot hope to obtain a closed-form
solution for H(Gp) for which equality to (2) is
decidable. We will return to this issue in Sec-
tion 6.
5 Weighted intersection
In order to compute the cross-entropy defined
in the next section, we need to derive a sin-
gle probabilistic model that simultaneously ac-
counts for both the computations of an under-
lying FA and the derivations of an underlying
PCFG. We start from a construction originally
presented in (Bar-Hillel et al, 1964), that com-
putes the intersection of a context-free language
and a regular language. The input consists of a
CFG G = (?, N, S, R) and an FA M = (?, Q,
q0, Qf , T ); note that we assume, without loss
of generality, that G and M share the same set
of terminals ?.
The output of the construction is CFG G? =
(?, N?, S?, R?), where N? = Q ? (? ? N) ?
Q ? {S?}, and R? consists of the set of rules
that is obtained as follows.
? For each s ? Qf , let S? ? (q0, S, s) be a
rule of G?.
? For each rule A ? X1 ? ? ?Xm of G
and each sequence of states s0, . . . , sm
of M , with m ? 0, let (s0, A, sm) ?
(s0, X1, s1) ? ? ? (sm?1, Xm, sm) be a rule of
G?; form = 0, G? has a rule (s0, A, s0)? 
for each state s0.
? For each transition s a7? t of M , let
(s, a, t)? a be a rule of G?.
Note that for each rule (s0, A, sm) ?
(s0, X1, s1) ? ? ? (sm?1, Xm, sm) there is a unique
rule A ? X1 ? ? ?Xm from which it has been
constructed by the above. Similarly, each rule
(s, a, t) ? a uniquely identifies a transition
s a7? t. This means that if we take a complete
derivation d? in G?, we can extract a sequence
h1(d?) of rules from G and a sequence h2(d?) of
transitions from M , where h1 and h2 are string
homomorphisms that we define point-wise as
? h1(pi?) = , if pi? is S? ? (q0, S, s);
h1(pi?) = pi, if pi? is (s0, A, sm) ?
(s0, X1, s1) ? ? ? (sm?1, Xm, sm) and pi is
(A? X1 ? ? ?Xm);
h1(pi?) = , if pi? is (s, a, t)? a;
? h2(pi?) = , if pi? is S? ? (q0, S, s);
h2(pi?) = ? , if pi? is (s, a, t) ? a and ? is
s a7? t;
h2(pi?) = , if pi? is (s0, A, sm) ?
(s0, X1, s1) ? ? ? (sm?1, Xm, sm).
We define h(d?) = (h1(d?), h2(d?)). It can be
easily shown that if S?
d?? w and h(d?) = (d, c),
then for the same w we have S d? w and
(q0, w)
c
` (s, ), some s ? Qf . Conversely,
if for some w, d and c we have S d? w and
(q0, w)
c
` (s, ), some s ? Qf , then there is pre-
cisely one derivation d? such that h(d?) = (d, c)
and S?
d?? w.
As noted before by (Nederhof and Satta,
2003), this construction can be extended to ap-
ply to a PCFG Gp = (G, pG) and an FA M . The
output is a PCFG G?,p = (G?, pG?), where G?
is defined as above and pG? is defined by:
? pG?(S? ? (q0, S, s)) = 1;
? pG?((s0, A, sm) ? (s0, X1, s1) ? ? ?
(sm?1, Xm, sm)) = pG(A? X1 ? ? ?Xm);
? pG?((s, a, t)? a) = 1.
Note that G?,p is non-proper. More specifically,
probabilities of rules with left-hand side S? or
(s0, A, sm) might not sum to one. This is not
a problem for the algorithms presented in this
paper, as we have never assumed properness for
our PCFGs. What is most important here is the
following property of G?,p. If d?, d and c are
such that h(d?) = (d, c), then pG?(d?) = pG(d).
Let us now assume that M is deterministic.
(In fact, the weaker condition of M being unam-
biguous is sufficient for our purposes, but unam-
biguity is not a very practical condition.) Given
a string w and a transition s a7? t of M we define
f(s a7? t;w) as the frequency (number of occur-
rences) of s a7? t in the unique computation of
M , if it exists, that accepts w; this frequency is
0 if w is not accepted by M . On the basis of the
above construction of G?,p and of Section 3, we
find
EpG f(s
a7? t; y(d)) =
?
d
pG(d) ? f(s
a7? t; y(d)) =
outG?,p((s, a, t)) ? pG?((s, a, t)?a) ? inG?,p(a) =
outG?,p((s, a, t)) (4)
6 Kullback-Leibler distance
In this section we consider the Kullback-Leibler
distance between a PCFGs and a PFA, and
present a method for its optimization under cer-
tain assumptions. Let Gp = (G, pG) be a consis-
tent PCFG and let Mp = (M,pM ) be a consis-
tent PFA. We demand that M be deterministic
(or more generally, unambiguous). Let us first
assume that L(G) ? L(M); we will later drop
this constraint.
The cross-entropy of Gp and Mp is defined as
usual for probabilistic models, viz. as the expec-
tation under distribution pG of the information
of the strings generated by M , i.e.,
H(Gp ||Mp) = EpG log
1
pM (w)
=
?
w
pG(w) ? log
1
pM (w)
.
The Kullback-Leibler distance of Gp and Mp is
defined as
D(Gp ||Mp) = EpG log
pG(w)
pM (w)
=
?
w
pG(w) ? log
pG(w)
pM (w)
.
Quantity D(Gp ||Mp) can also be expressed as
the difference between the cross-entropy of Gp
and Mp and the entropy of Gp, i.e.,
D(Gp ||Mp) = H(Gp ||Mp)?H(Gp). (5)
Let G?,p be the PCFG obtained by intersecting
Gp with the non-probabilistic FA M underlying
Mp, as in Section 5. Using (4) the cross-entropy
of Gp and Mp can be expressed as
H(Gp ||Mp) =
?
w
pG(w) ? log
1
pM (w)
=
?
d
pG(d) ? log
1
pM (y(d))
=
?
d
pG(d) ? log
?
s
a
7?t
(
1
pM (s
a7? t)
)f(s
a
7?t;y(d))
=
?
d
pG(d) ?
?
s
a
7?t
f(s a7? t; y(d)) ? log
1
pM (s
a7? t)
=
?
s
a
7?t
log
1
pM (s
a7? t)
?
?
d
pG(d) ? f(s
a7? t; y(d)) =
?
s
a
7?t
log
1
pM (s
a7? t)
? EpG f(s
a7? t; y(d)) =
?
s
a
7?t
log
1
pM (s
a7? t)
? outG?,p((s, a, t)).
We can combine the above with (5) to obtain
D(Gp ||Mp) =
?
s
a
7?t
outG?,p((s, a, t)) ? log
1
pM (s
a7? t)
?H(Gp).
The values of outG?,p can be calculated eas-
ily, as discussed in Section 3. Computation of
H(Gp) in closed-form is problematic, as already
pointed out in Section 4. However, for many
purposes computation of H(Gp) is not needed.
For example, assume that the non-
probabilistic FA M underlying Mp is given, and
our goal is to measure the distance between Gp
and Mp, for different choices of pM . Then the
choice that minimizes H(Gp ||Mp) determines
the choice that minimizes D(Gp ||Mp), irre-
spective of H(Gp). Formally, we can use the
above characterization to compute
p?M = argmaxpM
D(Gp ||Mp)
= argmax
pM
H(Gp ||Mp).
When L(G) ? L(M) is non-empty, both
D(Gp ||Mp) and H(Gp ||Mp) are undefined, as
their definitions imply a division by pM (w) = 0
for w ? L(G)? L(M). In cases where the non-
probabilistic FA M is given, and our goal is to
compare the relative distances between Gp and
Mp for different choices of pM , it makes sense
to ignore strings in L(G) ? L(M), and define
D(Gp ||Mp), H(Gp ||Mp) and H(Gp) on the do-
main L(G) ? L(M). Our equations above then
still hold. Note that strings in L(M)?L(G) can
be ignored since they do not contribute non-zero
values to D(Gp ||Mp) and H(Gp ||Mp).
7 Conclusions
We have discussed the computation of the
KL distance between PCFGs and deterministic
PFAs. We have argued that exact computation
is difficult in general, but for determining the
relative qualities of different PFAs, with respect
to their closeness to an input PCFG, it suffices
to compute the cross-entropy. We have shown
that the cross-entropy between a PCFG and a
deterministic PFA can be computed exactly.
These results can also be used for comparing
a pair of PFAs, one of which is deterministic.
Generalization of PCFGs to probabilistic tree-
adjoining grammars (PTAGs) is also possible,
by means of the intersection of a PTAG and a
PFA, along the lines of (Lang, 1994).
Acknowledgements
Helpful comments from Zhiyi Chi are gratefully
acknowledged. The first author is supported by
the PIONIER Project Algorithms for Linguis-
tic Processing , funded by NWO (Dutch Orga-
nization for Scientific Research). The second
author is partially supported by MIUR (Italian
Ministry of Education) under project PRIN No.
2003091149 005.
References
J.K. Baker. 1979. Trainable grammars for
speech recognition. In J.J. Wolf and D.H.
Klatt, editors, Speech Communication Papers
Presented at the 97th Meeting of the Acousti-
cal Society of America, pages 547?550.
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964.
On formal properties of simple phrase struc-
ture grammars. In Y. Bar-Hillel, editor,
Language and Information: Selected Essays
on their Theory and Application, chapter 9,
pages 116?150. Addison-Wesley.
T.L. Booth and R.A. Thompson. 1973. Ap-
plying probabilistic measures to abstract lan-
guages. IEEE Transactions on Computers,
C-22(5):442?450, May.
A. Corazza, R. De Mori, R. Gretter, and
G. Satta. 1991. Computation of probabilities
for an island-driven parser. IEEE Transac-
tions on Pattern Analysis and Machine In-
telligence, 13(9):936?950.
M. Falkhausen, H. Reininger, and D. Wolf.
1995. Calculation of distance measures be-
tween Hidden Markov Models. In Proceedings
of Eurospeech ?95, pages 1487?1490, Madrid.
U. Grenander. 1976. Lectures in Pattern The-
ory, Vol. I: Pattern Synthesis. Springer-
Verlag.
J.E. Hopcroft and J.D. Ullman. 1979. Intro-
duction to Automata Theory, Languages, and
Computation. Addison-Wesley.
S.E. Hutchins. 1972. Moments of strings and
derivation lengths of stochastic context-free
ggrammars. Information Sciences, 4:179?
191.
B.-H. Juang and L.R. Rabiner. 1985. A prob-
abilistic distance measure for hidden Markov
models. AT&T Technical Journal, 64(2):391?
408.
D. Jurafsky, C. Wooters, G. Tajchman, J. Se-
gal, A. Stolcke, E. Fosler, and N. Morgan.
1994. The Berkeley Restaurant Project. In
Proceedings of the International Conference
on Spoken Language Processing (ICSLP-94),
pages 2139?2142, Yokohama, Japan.
B. Lang. 1994. Recognition can be harder
than parsing. Computational Intelligence,
10(4):486?494.
K. Lari and S.J. Young. 1990. The estimation
of stochastic context-free grammars using the
Inside-Outside algorithm. Computer Speech
and Language, 4:35?56.
K. Lari and S.J. Young. 1991. Applications of
stochastic context-free grammars using the
Inside-Outside algorithm. Computer Speech
and Language, 5:237?257.
M. Mohri and M.-J. Nederhof. 2001. Regu-
lar approximation of context-free grammars
through transformation. In J.-C. Junqua and
G. van Noord, editors, Robustness in Lan-
guage and Speech Technology, pages 153?163.
Kluwer Academic Publishers.
M. Mohri. 1997. Finite-state transducers in
language and speech processing. Computa-
tional Linguistics, 23(2):269?311.
M.-J. Nederhof and G. Satta. 2003. Proba-
bilistic parsing as intersection. In 8th Inter-
national Workshop on Parsing Technologies,
pages 137?148, LORIA, Nancy, France, April.
M.-J. Nederhof. 2000. Practical experi-
ments with regular approximation of context-
free languages. Computational Linguistics,
26(1):17?44.
M. Rimon and J. Herz. 1991. The recogni-
tion capacity of local syntactic constraints.
In Fifth Conference of the European Chap-
ter of the Association for Computational Lin-
guistics, Proceedings of the Conference, pages
155?160, Berlin, Germany, April.
S. Soule. 1974. Entropies of probabilistic gram-
mars. Information and Control, 25:57?74.
A. Stolcke and J. Segal. 1994. Precise N -
gram probabilities from stochastic context-
free grammars. In 32nd Annual Meeting of
the Association for Computational Linguis-
tics, Proceedings of the Conference, pages 74?
79, Las Cruces, New Mexico, USA, June.
M. Vihola, M. Harju, P. Salmela, J. Suon-
tausta, and J. Savela. 2002. Two dissimilar-
ity measures for HMMs and their application
in phoneme model clustering. In ICASSP
2002, volume I, pages 933?936.
C.S. Wetherell. 1980. Probabilistic languages:
A review and some open questions. Comput-
ing Surveys, 12(4):361?379, December.
c? 2003 Association for Computational Linguistics
Squibs and Discussions
Weighted Deductive Parsing and Knuth?s
Algorithm
Mark-Jan Nederhof?
University of Groningen
We discuss weighted deductive parsing and consider the problem of finding the derivation with
the lowest weight. We show that Knuth?s generalization of Dijkstra?s algorithm for the shortest-
path problem offers a general method to solve this problem. Our approach is modular in the sense
that Knuth?s algorithm is formulated independently from the weighted deduction system.
1. Introduction
As for algorithms in general, there are significant advantages to specifying parsing
algorithms in a modular way (i.e., as the combination of subalgorithms). First, modular
specifications often allow simpler implementations. Secondly, if otherwise seemingly
distinct types of parser are described in a modular way, the common parts can often
be more readily identified, which helps to classify and analyze parsing algorithms.
In this article we discuss a modular design for weighted deductive parsing by
distinguishing between a weighted deduction system, on the one hand, which per-
tains to the choice of grammatical formalism and parsing strategy, and the algorithm
that finds the derivation with the lowest weight, on the other. The latter is Dijkstra?s
algorithm for the shortest-path problem (Dijkstra 1959) as generalized by Knuth (1977)
for a problem on grammars. It has been argued by, for example, Backhouse (2001),
that this algorithm can be used to solve a wide range of problems on context-free
grammars. A brief presentation of a very similar algorithm for weighted deductive
parsing has been given before by Eisner (2000, Figure 3.5e).
Our presentation contrasts with that of Klein and Manning (2001), who offer an in-
divisible specification for a small collection of parsing strategies for weighted context-
free grammars only, referring to a generalization of Dijkstra?s algorithm to hypergraphs
by Gallo et al (1993). This article also addresses the efficiency of Knuth?s algorithm
for weighted deductive parsing, relative to the more commonly used algorithm by
Viterbi.
2. Weighted Deductive Parsing
The use of deduction systems for specifying parsers has been proposed by Shieber,
Schabes, and Pereira (1995) and Sikkel (1997). As already remarked by Goodman
(1999), deduction systems can also be extended to manipulate weights.1 Here we de-
? Faculty of Arts, Humanities Computing, University of Groningen, P.O. Box 716, NL-9700 AS
Groningen, The Netherlands. E-mail: markjan@let.rug.nl. Secondary affiliation is the German Research
Center for Artificial Intelligence (DFKI).
1 Weighted deduction is closely related to probabilistic logic, although the problem considered in this
article (viz., finding derivations with lowest weights) is different from typical problems in probabilistic
logic. For example, Frisch and Haddawy (1994) propose inference rules that manipulate logical
formulas attached to intervals of probabilities, and the objective of deduction is to determine intervals
that are as narrow as possible.
136
Computational Linguistics Volume 29, Number 1
Initializer:
y : [B ? ? ?, j, j]
{
(y : B ? ?) ? P
0 ? j ? n
Scanner:
x1 : [A ? ? ? a?, i, j]
x1 : [A ? ?a ? ?, i, j + 1]
?
?
?
(y1 : A ? ?a?) ? P
0 ? i ? j < n
aj+1 = a
Completer:
x1 : [A ? ? ? B?, i, j]
x2 : [B ? ? ?, j, k]
x1 + x2 : [A ? ?B ? ?, i, k]
?
?
?
(y1 : A ? ?B?) ? P
(y2 : B ? ?) ? P
0 ? i ? j ? k ? n
Goal items: [S ? ? ?, 0, n] for any (y : S ? ?) ? P, where S is the start symbol
Figure 1
Weighted deduction system for bottom-up parsing.
fine such a weighted deduction system for parsing as consisting of a finite set of
inference rules of the form:
x1 : I1
x2 : I2
...
xm : Im
f (x1, x2, . . . , xm) : I0
?
?
?
?
?
c1
...
cp
where m ? 0 and p ? 0, and I0, I1, . . . , Im are items, of which I0 is the consequent
and I1, . . . , Im are the antecedents, and c1, . . . , cp is a list of side conditions linking
the inference rule to the grammar and the input string.2 We assign unique variables
x1, . . . , xm to each of the antecedents, and a weight function f , with x1, . . . , xm as argu-
ments, to the consequent. This allows us to assign a weight to each occurrence of an
(instantiated) item that we derive by an inference rule, by means of a function on the
weights of the (instantiated) antecedents of that rule.
A weighted deduction system furthermore contains a set of goal items; like the
inference rules, this set is parameterized by the grammar and the input. The objective
of weighted deductive parsing is to find the derivation of a goal item with the lowest
weight. In this article we assume that, for a given grammar and input string, each
inference rule can be instantiated in a finite number of ways, which ensures that this
problem can be solved under the constraints on the weight functions to be discussed
in Sections 4 and 5.
Our examples will be restricted to context-free parsing and include the deduction
system for weighted bottom-up parsing in Figure 1 and that for weighted top-down
parsing in Figure 2. The latter is very close to an extension of Earley?s algorithm
described by Lyon (1974). The side conditions refer to an input string w = a1 ? ? ? an and
to a weighted context-free grammar with a set of productions P, each of which has the
form (y: A ? ?), where y is a non-negative real-valued weight, A is a nonterminal,
2 Note that we have no need for (explicit) axioms, since we allow inference rules to have zero
antecedents.
137
Nederhof Weighted Deductive Parsing
Starter:
y : [S ? ? ?, 0, 0]
{
(y : S ? ?) ? P, where S is the start symbol
Predictor:
x1 : [A ? ? ? B?, i, j]
y2 : [B ? ? ?, j, j]
?
?
?
(y1 : A ? ?B?) ? P
(y2 : B ? ?) ? P
0 ? i ? j ? n
Scanner, completer and set of goal items are as in Figure 1.
Figure 2
Weighted deduction system for top-down parsing.
Starter:
(y, y) : [S ? ? ?, 0, 0]
{
(y : S ? ?) ? P, where S is the start symbol
Scanner:
(z1, x1) : [A ? ? ? a?, i, j]
(z1, x1) : [A ? ?a ? ?, i, j + 1]
?
?
?
(y1 : A ? ?a?) ? P
0 ? i ? j < n
aj+1 = a
Predictor:
(z1, x1) : [A ? ? ? B?, i, j]
(z1 + y2, y2) : [B ? ? ?, j, j]
?
?
?
(y1 : A ? ?B?) ? P
(y2 : B ? ?) ? P
0 ? i ? j ? n
Completer:
(z1, x1) : [A ? ? ? B?, i, j]
(z2, x2) : [B ? ? ?, j, k]
(z1 + x2, x1 + x2) : [A ? ?B ? ?, i, k]
?
?
?
(y1 : A ? ?B?) ? P
(y2 : B ? ?) ? P
0 ? i ? j ? k ? n
Set of goal items is as in Figure 1.
Figure 3
Alternative weighted deduction system for top-down parsing.
and ? is a list of zero or more terminals or nonterminals. We assume the weight
of a grammar derivation is given by the sum of the weights of the occurrences of
productions therein.
Weights may be atomic entities, as in the deduction systems discussed above,
where they are real-valued, but they may also be composed entities. For example,
Figure 3 presents an alternative form of weighted top-down parsing using pairs of
values, following Stolcke (1995). The first value is the forward weight, that is, the sum
of weights of all productions that were encountered in the lowest-weighted derivation
in the deduction system of an item [A ? ? ? ?, i, j]. The second is the inner weight;
that is, it considers the weight only of the current production A ? ?? plus the weights
of productions in lowest-weighted grammar derivations for nonterminals in ?. These
inner weights are the same values as the weights in Figures 1 and 2. In fact, if we omit
the forward weights, we obtain the deduction system in Figure 2.
Since forward weights pertain to larger parts of grammar derivations than the
inner weights, they may be better suited to direct the search for the lowest-weighted
complete grammar derivation. We assume a pair (z1, x1) is smaller than (z2, x2) if and
138
Computational Linguistics Volume 29, Number 1
only if z1 < z2 or z1 = z2 ? x1 < x2. (Tendeau [1997] has shown the general idea can
also be applied to left-corner parsing.)
In order to link (weighted) deduction systems to literature to be discussed in
Section 3, we point out that a deduction system having a grammar G in a certain
formalism F and input string w in the side conditions can be seen as a construction c of
a context-free grammar c(G, w) out of grammar G and input w. The set of productions
of c(G, w) is obtained by instantiating the inference rules in all possible ways using
productions from G and input positions pertaining to w. The consequent of such
an instantiated inference rule then acts as the left-hand side of a production, and
the (possibly empty) list of antecedents acts as its right-hand side. In the case of
a weighted deduction system, the productions are associated with weight functions
computing the weight of the left-hand side from the weights of the right-hand side
nonterminals.
For example, if the input is w = a1a2a3, and if there are two productions in the
weighted context-free grammar G of the form (y1: A ? C B D), (y2: B ? E) ? P,
then from the completer in Figure 1 we may obtain, among others, a production
[A ? C B ? D, 0, 2] ? [A ? C ? B D, 0, 1] [B ? E ?, 1, 2], with associated weight function
f (x1, x2) = x1 + x2, which states that if the production is used in a derivation, then the
weights of the two subderivations should be added. The number of productions in
c(G, w) is determined by the number of ways we can instantiate inference rules, which
in the case of Figure 1 is O(|G|2 ? n3), where |G| is the size of G in terms of the total
number of occurrences of terminals and nonterminals in productions.
If we assume, without loss of generality, that there is only one goal item, then this
goal item becomes the start symbol of c(G, w).3 Since there are no terminals in c(G, w),
either the grammar generates the language {}, containing only the empty string , or
it generates the empty language; in the latter case, this indicates that w is not in the
language generated by G.
Note that for all three examples above, the derivation with the lowest weight
allowed by c(G, w) encodes the derivation with the lowest weight allowed by G for
w. Together with the dynamic programming algorithm to be discussed in the next
section that finds the derivation with the lowest weight on the basis of c(G, w), we
obtain a modular approach to describing weighted parsers: One part of the description
specifies how to construct grammar c(G, w) out of grammar G and input w, and the
second part specifies the dynamic programming algorithm to investigate c(G, w).
Such a modular way of describing parsers in the unweighted case has already
been fully developed in work by Lang (1974) and Billot and Lang (1989). Instead
of a deduction system, they use a pushdown transducer to express a parsing strat-
egy such as top-down parsing, left-corner parsing or LR parsing. Such a pushdown
transducer can in the context of their work be regarded as specifying a context-free
grammar c(G, w), given a context-free grammar G and an input string w. The second
part of the description of the parser is a dynamic programming algorithm for actually
constructing c(G, w) in polynomial time in the length of w.
This modular approach to describing parsing algorithms is also applicable to for-
malisms F other than context-free grammars. For example, it was shown by Vijay-
Shanker and Weir (1993) that tree-adjoining parsing can be realized by constructing a
context-free grammar c(G, w) out of a tree-adjoining grammar G and an input string
w. This can straightforwardly be generalized to weighted (in particular, stochastic)
tree-adjoining grammars (Schabes 1992).
3 If there is more than one goal item, then a new symbol needs to be introduced as the start symbol.
139
Nederhof Weighted Deductive Parsing
It was shown by Boullier (2000) that F may furthermore be the formalism of range
concatenation grammars. Since the class of range concatenation grammars generates
exactly PTIME, this demonstrates the generality of the approach.4
Instead of string input, one may also consider input consisting of a finite au-
tomaton, along the lines of Bar-Hillel, Perles, and Shamir (1964); this can be trivially
extended to the weighted case. That we restrict ourselves to string input in this article
is motivated by presentational considerations.
3. Knuth?s Algorithm
The algorithm by Dijkstra (1959) effectively finds the shortest path from a distin-
guished source node in a weighted, directed graph to a distinguished target node.
The underlying idea of the algorithm is that it suffices to investigate only the shortest
paths from the source node to other nodes, since longer paths can never be extended
to become shorter paths (weights of edges are assumed to be non-negative).
Knuth (1977) generalizes this algorithm to the problem of finding lowest-weighted
derivations allowed by a context-free grammar with weight functions, similar to those
we have seen in the previous section. (The restrictions Knuth imposes on the weight
functions will be discussed in the next section.) Again, the underlying idea of the
algorithm is that it suffices to investigate only the lowest-weighted derivations of
nonterminals.
The algorithm by Knuth is presented in Figure 4. We have taken the liberty of
making some small changes to Knuth?s formulation. The largest difference between
Knuth?s formulation and ours is that we have assumed that the context-free grammar
with weight functions on which the algorithm is applied has the form c(G, w), obtained
by instantiating the inference rules of a weighted deduction system for given grammar
G and input w. Note, however, that c(G, w) is not fully constructed before applying
Knuth?s algorithm, and the algorithm accesses only as much of it as is needed in its
search for the lowest-weighted goal item.
In the algorithm, the set D contains items I for which the lowest overall weight
has been found; this weight is given by ?(I). The set E contains items I0 that can be
derived in one step from items in D, but for which the lowest weight ?(I0) found thus
far may still exceed the lowest overall weight for I0. In each iteration, it is established
that the lowest weight ?(I) for an item I in E is the lowest overall weight for I, which
justifies transferring I to D. The algorithm can be extended to output the derivation
corresponding to the goal item with the lowest weight; this is fairly trivial and will
not be discussed here.
A few remarks about the implementation of Knuth?s algorithm are in order. First,
instead of constructing E and ? anew at step 2 for each iteration, it may be more
efficient to construct them only once and revise them every time a new item I is added
to D. This revision consists in removing I from E and combining it with existing items
in D, as antecedents of inference rules, in order to find new items to be added to
E and/or to update ? to assign lower values to items in E. Typically, E would be
organized as a priority queue.
Second, practical implementations would maintain appropriate tables for indexing
the items in such a way that when a new item I is added to D, the lists of existing items
in D together with which it matches the lists of antecedents of inference rules can be
4 One may even consider formalisms F that generate languages beyond PTIME, but such applications of
the approach would not necessarily be of practical value.
140
Computational Linguistics Volume 29, Number 1
1. Let D be the empty set ?.
2. Determine the set E and the function ? as follows:
? E is the set of items I0 /? D such that there is at least one
inference rule from the deduction system that can be
instantiated to a production of the form I0 ? I1 ? ? ? Im, for some
m ? 0, with weight function f , where I1, . . . , Im ? D.
? For each such I0 ? E, let ?(I0) be the minimal weight
f (?(I1), . . . ,?(Im)) for all such instantiated inference rules.
3. If E is empty, then report failure and halt.
4. Choose an item I ? E such that ?(I) is minimal.
5. Add I to D, and let ?(I) = ?(I).
6. If I is a goal item, then output ?(I) and halt.
7. Repeat from step 2.
Figure 4
Knuth?s generalization of Dijkstra?s algorithm. Implicit are a weighted deduction system, a
grammar G and an input w. For conditions on correctness, see Section 4.
efficiently found. Since techniques for such kinds of indexing are well-established in
the computer science literature, no further discussion is warranted here.
4. Conditions on the Weight Functions
A sufficient condition for Knuth?s algorithm to correctly compute the derivations with
the lowest weights is that the weight functions f are all superior, which means that they
are monotone nondecreasing in each variable and that f (x1, . . . , xm) ? max(x1, . . . , xm)
for all possible values of x1, . . . , xm. For this case, Knuth (1977) provides a short and
elegant proof of correctness. Note that the weight functions in Figure 1 are all superior,
so that correctness is guaranteed.
In the case of the top-down strategy from Figure 2, however, the weight functions
are not all superior, since we have constant weight functions for the predictor, which
may yield weights that are less than their arguments. It is not difficult, however, to
show that Knuth?s algorithm still correctly computes the derivations with the lowest
weights, given that we have already established the correctness for the bottom-up
case.
First, note that items of the form [B ? ? ?, j, j], which are introduced by the
initializer in the bottom-up case, can be introduced by the starter or the predictor in
the top-down case; in the top-down case, these items are generally introduced later
than in the bottom-up case. Second, note that such items can contribute to finding a
goal item only if from [B ? ? ?, j, j] we succeed in deriving an item [B ? ? ?, j, k] that is
either such that B = S, j = 0, and k = n, or such that there is an item [A ? ? ? B?, i, j].
In either case, the item [B ? ? ?, j, j] can be introduced by the starter or predictor
so that [B ? ? ?, j, k] will be available to the algorithm if and when it is needed to
determine the derivation with the lowest weight for [S ? ? ?, 0, n] or [A ? ?B ? ?, i, k],
respectively, which will then have a weight greater than or equal to that of [B ? ? ?, j, j].
141
Nederhof Weighted Deductive Parsing
For the alternative top-down strategy from Figure 3, the proof of correctness is
similar, but now the proof depends for a large part on the additional forward weights,
the first values in the pairs (z, x); note that the second values are the inner weights
(i.e., the weights we already considered in Figures 1 and 2). An important observation
is that if there are two derivations for the same item with weights (z1, x1) and (z2, x2),
respectively, such that z1 < z2 and x1 > x2, then there must be a third derivation of that
item with weight (z1, x2). This shows that no relevant inner weights are overlooked
because of the ordering we imposed on pairs (z, x).
Since Figures 1 through 3 are merely examples to illustrate the possibilities of
deduction systems and Knuth?s algorithm, we do not provide full proofs of correctness.
5. Viterbi?s Algorithm
This section places Knuth?s algorithm in the context of a more commonly used alter-
native. This algorithm is applicable on a weighted deduction system if a simple partial
order on items exists that is such that the antecedents of an inference rule are always
strictly smaller than the consequent. When this is the case, we may treat items from
small to large to compute their lowest weights. There are no constraints on the weight
functions other than that they should be monotone nondecreasing.
The algorithm by Viterbi (1967) may be the earliest that operates according to this
principle. The partial order is based on the linear order given by a string of input
symbols. In this article we will let the term ?Viterbi?s algorithm? refer to the general
type of algorithm to search for the derivation with the lowest weight given a deduction
system, a grammar, an input string, and a partial order on items consistent with the
inference rules in the sense given above.5
Another example of an algorithm that can be seen as an instance of Viterbi?s algo-
rithm was presented by Jelinek, Lafferty, and Mercer (1992). This algorithm is essen-
tially CYK parsing (Aho and Ullman 1972) extended to handle weights (in particular,
probabilities). The partial order on items is based on the sizes of their spans (i.e., the
number of input symbols that the items cover). Weights of items with smaller spans
are computed before the weights of those with larger spans. In cases in which a simple
a priori order on items is not available but derivations are guaranteed to be acyclic,
one may first determine a topological sorting of the complete set of derivable items
and then compute the weights based on that order, following Martelli and Montanari
(1978).
A special situation arises when a deduction system is such that inference rules
allow cyclic dependencies within certain subsets of items, but dependencies between
these subsets represent a partial order. One may then combine the two algorithms:
Knuth?s (or Dijkstra?s) algorithm is used within each subset and Viterbi?s algorithm
is used to relate items in distinct subsets. This is exemplified by Bouloutas, Hart, and
Schwartz (1991).
In cases in which both Knuth?s algorithm and Viterbi?s algorithm are applicable,
the main difference between the two is that Knuth?s algorithm may halt as soon as
the lowest weight for a goal item is found, and no items with larger weights than that
goal item need to be treated, whereas Viterbi?s algorithm treats all derivable items. This
suggests that Knuth?s algorithm may be more efficient than Viterbi?s. The worst-case
time complexity of Knuth?s algorithm, however, involves an additional factor because
5 Note that some authors let the term ?Viterbi algorithm? refer to any algorithm that computes the
?Viterbi parse,? that is, the parse with the lowest weight or highest probability.
142
Computational Linguistics Volume 29, Number 1
of the maintenance of the priority queue. Following Cormen, Leiserson, and Rivest
(1990), this factor is O(log(?c(G, w)?)), where ?c(G, w)? is the number of nonterminals
in c(G, w), which is an upper bound on the number of elements on the priority queue
at any given time. Furthermore, there are observations by, for example, Chitrao and
Grishman (1990), Tjong Kim Sang (1998, Sections 3.1 and 3.4), and van Noord et al
(1999, Section 3.9), that suggest that the apparent advantage of Knuth?s algorithm does
not necessarily lead to significantly lower time costs in practice.
In particular, consider deduction systems with items associated with spans like, for
example, that in Figure 1, in which the span of the consequent of an inference rule is
the concatenation of the spans of the antecedents. If weights of individual productions
in G differ only slightly, as is often the case in practice, then different derivations for an
item have only slightly different weights, and the lowest such weight for a certain item
is roughly proportional to the size of its span. This suggests that Knuth?s algorithm
treats most items with smaller spans before any item with a larger span is treated, and
since goal items typically have the maximal span, covering the complete input, there
are few derivable items at all that are not treated before any goal item is found.
6. Conclusions
We have shown how a general weighted parser can be specified in two parts, the
first being a weighted deduction system, and the second being Knuth?s algorithm (or
possibly Viterbi?s algorithm, where applicable). Such modular specifications have clear
theoretical and practical advantages over indivisible specifications. For example, we
may identify common aspects of otherwise seemingly distinct types of parser. Further-
more, modular specifications allow simpler implementations. We have also identified
close connections between our approach to specifying weighted parsers and well-
established theory of grammars and parsing. How the efficiency of Knuth?s algorithm
relates to that of Viterbi?s algorithm in a practical setting is still to be investigated.
Acknowledgments
The author is supported by the Royal
Netherlands Academy of Arts and Sciences.
I am grateful to Gertjan van Noord, Giorgio
Satta, Khalil Sima?an, Frederic Tendeau, and
the anonymous referees for valuable
comments.
References
Aho, Alfred V. and Jeffrey D. Ullman. 1972.
Parsing, volume 1 of The Theory of Parsing,
Translation and Compiling. Prentice-Hall,
Englewood Cliffs, New Jersey.
Backhouse, Roland. 2001. Fusion on
languages. In 10th European Symposium on
Programming, volume 2028 of Lecture Notes
in Computer Science, pages 107?121.
Springer-Verlag, Berlin, April.
Bar-Hillel, Y., M. Perles, and E. Shamir.
1964. On formal properties of simple
phrase structure grammars. In
Y. Bar-Hillel, editor, Language and
Information: Selected Essays on Their Theory
and Application. Addison-Wesley, Reading,
Massachusetts, pages 116?150.
Billot, Sylvie and Bernard Lang. 1989. The
structure of shared forests in ambiguous
parsing. In 27th Annual Meeting of the
Association for Computational Linguistics,
Proceedings of the Conference,
pages 143?151, Vancouver, British
Columbia, Canada, June.
Boullier, Pierre. 2000. Range concatenation
grammars. In Proceedings of the Sixth
International Workshop on Parsing
Technologies, pages 53?64, Trento, Italy,
February.
Bouloutas, A., G. W. Hart, and M. Schwartz.
1991. Two extensions of the Viterbi
algorithm. IEEE Transactions on Information
Theory, 37(2):430?436.
Chitrao, Mahesh V. and Ralph Grishman.
1990. Statistical parsing of messages. In
Speech and Natural Language Proceedings,
pages 263?266, Hidden Valley,
Pennsylvania, June.
Cormen, Thomas H., Charles E. Leiserson,
and Ronald L. Rivest. 1990. Introduction to
Algorithms. MIT Press, Cambridge.
Dijkstra, E. W. 1959. A note on two
143
Nederhof Weighted Deductive Parsing
problems in connexion with graphs.
Numerische Mathematik, 1:269?271.
Eisner, Jason. 2000. Bilexical grammars and
their cubic-time parsing algorithms. In
H. Bunt and A. Nijholt, editors, Advances
in Probabilistic and Other Parsing
Technologies. Kluwer Academic Publishers,
Dordrecht, The Netherlands, pages 29?61.
Frisch, Alan M. and Peter Haddawy. 1994.
Anytime deduction for probabilistic logic.
Artificial Intelligence, 69:93?122.
Gallo, Giorgio, Giustino Longo, Stefano
Pallottino, and Sang Nguyen. 1993.
Directed hypergraphs and applications.
Discrete Applied Mathematics, 42:177?201.
Goodman, Joshua. 1999. Semiring parsing.
Computational Linguistics, 25(4):573?605.
Jelinek, F., J. D. Lafferty, and R. L. Mercer.
1992. Basic methods of probabilistic
context free grammars. In P. Laface and
R. De Mori, editors, Speech Recognition and
Understanding?Recent Advances, Trends and
Applications. Springer-Verlag, Berlin,
pages 345?360.
Klein, Dan and Christopher D. Manning.
2001. Parsing and hypergraphs. In
Proceedings of the Seventh International
Workshop on Parsing Technologies,
pages 123?134, Beijing, October.
Knuth, Donald E. 1977. A generalization of
Dijkstra?s algorithm. Information Processing
Letters, 6(1):1?5.
Lang, Bernard. 1974. Deterministic
techniques for efficient non-deterministic
parsers. In Automata, Languages and
Programming, 2nd Colloquium, volume 14
of Lecture Notes in Computer Science,
pages 255?269, Saarbru?cken.
Springer-Verlag, Berlin.
Lyon, Gordon. 1974. Syntax-directed
least-errors analysis for context-free
languages: A practical approach.
Communications of the ACM, 17(1):3?14.
Martelli, Alberto and Ugo Montanari. 1978.
Optimizing decision trees through
heuristically guided search.
Communications of the ACM,
21(12):1025?1039.
Schabes, Yves. 1992. Stochastic lexicalized
tree-adjoining grammars. In Proceedings of
the 15th International Conference on
Computational Linguistics, volume 2,
pages 426?432, Nantes, August.
Shieber, Stuart M., Yves Schabes, and
Fernando C. N. Pereira. 1995. Principles
and implementation of deductive parsing.
Journal of Logic Programming, 24:3?36.
Sikkel, Klaas. 1997. Parsing Schemata.
Springer-Verlag, Berlin.
Stolcke, Andreas. 1995. An efficient
probabilistic context-free parsing
algorithm that computes prefix
probabilities. Computational Linguistics,
21(2):167?201.
Tendeau, Fre?de?ric. 1997. Analyse syntaxique et
se?mantique avec e?valuation d?attributs dans un
demi-anneau. Ph.D. thesis, University of
Orle?ans.
Tjong Kim Sang, Erik F. 1998. Machine
Learning of Phonotactics. Ph.D. thesis,
University of Groningen.
van Noord, Gertjan, Gosse Bouma, Rob
Koeling, and Mark-Jan Nederhof. 1999.
Robust grammatical analysis for spoken
dialogue systems. Natural Language
Engineering, 5(1):45?93.
Vijay-Shanker, K. and David J. Weir. 1993.
The use of shared forests in tree adjoining
grammar parsing. In Sixth Conference of the
European Chapter of the Association for
Computational Linguistics, Proceedings of the
Conference, pages 384?393, Utrecht, The
Netherlands, April.
Viterbi, Andrew J. 1967. Error bounds for
convolutional codes and an
asymptotically optimum decoding
algorithm. IEEE Transactions on Information
Theory, IT-13(2):260?269.
A General Technique to Train Language
Models on Language Models
Mark-Jan Nederhof?
University of Groningen
We show that under certain conditions, a language model can be trained on the basis of a
second language model. The main instance of the technique trains a finite automaton on the
basis of a probabilistic context-free grammar, such that the Kullback-Leibler distance between
grammar and trained automaton is provably minimal. This is a substantial generalization of
an existing algorithm to train an n-gram model on the basis of a probabilistic context-free
grammar.
1. Introduction
In this article, the term language model is used to refer to any description that assigns
probabilities to strings over a certain alphabet. Language models have important
applications in natural language processing, and in particular, in speech recognition
systems (Manning and Schu?tze 1999).
Language models often consist of a symbolic description of a language, such as
a finite automaton (FA) or a context-free grammar (CFG), extended by a probability
assignment to, for example, the transitions of the FA or the rules of the CFG, by which
we obtain a probabilistic finite automaton (PFA) or probabilistic context-free grammar
(PCFG), respectively. For certain applications, one may first determine the symbolic part
of the automaton or grammar and in a second phase try to find reliable probability
estimates for the transitions or rules. The current article is involved with the second
problem, that of extending FAs or CFGs to become PFAs or PCFGs. We refer to this
process as training.
Training is often done on the basis of a corpus of actual language use in a certain
domain. If each sentence in this corpus is annotated by a list of transitions of an
FA recognizing the sentence or a parse tree for a CFG generating the sentence, then
training may consist simply in relative frequency estimation. This means that we estimate
probabilities of transitions or rules by counting their frequencies in the corpus, relative
to the frequencies of the start states of transitions or to the frequencies of the left-hand
side nonterminals of rules, respectively. By this estimation, the likelihood of the corpus
is maximized.
The technique we introduce in this article is different in that training is done on
the basis not of a finite corpus, but of an input language model. Our goal is to find
estimations for the probabilities of transitions or rules of the input FA or CFG such that
? Faculty of Arts, Humanities Computing, P.O. Box 716, NL-9700 AS Groningen, The Netherlands.
E-mail: markjan@let.rug.nl.
Submission received: 20th January 2004; Revised submission received: 5th August 2004; Accepted for
publication: 19th September 2004
? 2005 Association for Computational Linguistics
Computational Linguistics Volume 31, Number 2
the resulting PFA or PCFG approximates the input language model as well as possible,
or more specifically, such that the Kullback-Leibler (KL) distance (or relative entropy)
between the input model and the trained model is minimized. The input FA or CFG to
be trained may be structurally unrelated to the input language model.
This technique has several applications. One is an extension with probabilities
of existing work on approximation of CFGs by means of FAs (Nederhof 2000). The
motivation for this work was that application of FAs is generally less costly than
application of CFGs, which is an important benefit when the input is very large, as
is often the case in, for example, speech recognition systems. The practical relevance of
this work was limited, however, by the fact that in practice one is more interested in
the probabilities of sentences than in a purely Boolean distinction between grammatical
and ungrammatical sentences.
Several approaches were discussed by Mohri and Nederhof (2001) to extend this
work to approximation of PCFGs by means of PFAs. A first approach is to directly map
rules with attached probabilities to transitions with attached probabilities. Although
this is computationally the easiest approach, the resulting PFA may be a very inaccurate
approximation of the probability distribution described by the input PCFG. In particu-
lar, there may be assignments of probabilities to the transitions of the same FA that lead
to more accurate approximating language models.
A second approach is to train the approximating FA by means of a corpus. If
the input PCFG was itself obtained by training on a corpus, then we already possess
training material. However, this may not always be the case, and no training material
may be available. Furthermore, as a determinized approximating FA may be much
larger than the input PCFG, the sparse-data problem may be more severe for the
automaton than it was for the grammar.1 Hence, even if sufficient material was available
to train the CFG, it may not be sufficient to accurately train the FA.
A third approach is to construct a training corpus from the PCFG by means of
a (pseudo)random generator of sentences, such that sentences that are more likely
according to the PCFG are generated with greater likelihood. This has been proposed
by Jurafsky et al (1994), for the special case of bigrams, extending a nonprobabilistic
technique by Zue et al (1991). It is not clear, however, whether this idea is feasible
for training of finite-state models that are larger than bigrams. The reason is that
very large corpora would have to be generated in order to obtain accurate probability
estimates for the PFA. Note that the number of parameters of a bigram model is
bounded by the square of the size of the lexicon; such a bound does not exist for
general PFAs.
The current article discusses a fourth approach. In the limit, it is equivalent to the
third approach above, as if an infinite corpus were constructed on which the PFA is
trained, but we have found a way to avoid considering sentences individually. The key
idea that allows us to handle an infinite set of strings generated by the PCFG is that we
construct a new grammar that represents the intersection of the languages described by
the input PCFG and the FA. Within this new grammar, we can compute the expected
frequencies of transitions of the FA, using a fairly standard analysis of PCFGs. These
expected frequencies then allow us to determine the assignment of probabilities to
transitions of the FA that minimizes the KL distance between the PCFG and the resulting
PFA.
1 In Nederhof (2000), several methods of approximation were discussed that lead to determinized
approximating FAs that can be much larger than the input CFGs.
174
Nederhof Training Models on Models
The only requirement is that the FA to be trained be unambiguous, by which we
mean that each input string can be recognized by at most one computation of the FA.
The special case of n-grams has already been formulated by Stolcke and Segal (1994),
realizing an idea previously envisioned by Rimon and Herz (1991). An n-gram model is
here seen as a (P)FA that contains exactly one state for each possible history of the n ? 1
previously read symbols. It is clear that such an FA is unambiguous (even deterministic)
and that our technique therefore properly subsumes the technique by Stolcke and Segal
(1994), although the way that the two techniques are formulated is rather different. Also
note that the FA underlying an n-gram model accepts any input string over the alphabet,
which does not hold for general (unambiguous) FAs.
Another application of our work involves determinization and minimization of
PFAs. As shown by Mohri (1997), PFAs cannot always be determinized, and no practical
algorithms are known to minimize arbitrary nondeterministic (P)FAs. This can be a
problem when deterministic or small PFAs are required. We can, however, always
compute a minimal deterministic FA equivalent to an input FA. The new results in this
article offer a way to extend this determinized FA to a PFA such that it approximates
the probability distribution described by the input PFA as well as possible, in terms of
the KL distance.
Although the proposed technique has some limitations, in particular, that the model
to be trained is unambiguous, it is by no means restricted to language models based on
finite automata or context-free grammars, as several other probabilistic grammatical
formalisms can be treated in a similar manner.
The structure of this article is as follows. We provide some preliminary definitions
in Section 2. Section 3 discusses how the expected frequency of a rule in a PCFG can be
computed. This is an auxiliary step in the algorithms to be discussed below. Section 4
defines a way to combine a PFA and a PCFG into a new PCFG that extends a well-known
representation of the intersection of a regular and a context-free language. Thereby
we merge the input model and the model to be trained into a single structure. This
structure is the foundation for a number of algorithms, presented in section 5, which
allow, respectively, training of an unambiguous FA on the basis of a PCFG (section 5.1),
training of an unambiguous CFG on the basis of a PFA (section 5.2), and training of an
unambiguous FA on the basis of a PFA (section 5.3).
2. Preliminaries
Many of the definitions on probabilistic context-free grammars are based on Santos
(1972) and Booth and Thompson (1973), and the definitions on probabilistic finite
automata are based on Paz (1971) and Starke (1972).
A context-free grammar G is a 4-tuple (?, N, S, R), where ? and N are two finite
disjoint sets of terminals and nonterminals, respectively, S ? N is the start symbol, and
R is a finite set of rules, each of the form A ? ?, where A ? N and ? ? (? ? N)?. A
probabilistic context-free grammar G is a 5-tuple (?, N, S, R, pG ), where ?, N, S and R
are as above, and pG is a function from rules in R to probabilities.
In what follows, symbol a ranges over the set ?, symbols w, v range over the
set ??, symbols A, B range over the set N, symbol X ranges over the set ? ? N,
symbols ?,?,? range over the set (? ? N)?, symbol ? ranges over the set R, and
symbols d, e range over the set R?. With slight abuse of notation, we treat a rule
? = (A ? ?) ? R as an atomic symbol when it occurs within a string d?e ? R?.
The symbol  denotes the empty string. String concatenation is represented by
operator ? or by empty space.
175
Computational Linguistics Volume 31, Number 2
For a fixed (P)CFG G, we define the relation ? on triples consisting of two strings
?,? ? (? ? N)? and a rule ? ? R by ? ?? ?, if and only if ? is of the form wA? and ?
is of the form w??, for some w ? ?? and ? ? (? ? N)?, and ? = (A ? ?). A leftmost
derivation (in G) is a string d = ?1 ? ? ??m, m ? 0, such that ?0
?1? ?1
?2? ? ? ? ?m? ?m, for
some ?0, . . . ,?m ? (? ? N)?; d =  is always a leftmost derivation. In the remainder
of this article, we let the term derivation refer to leftmost derivation, unless spec-
ified otherwise. If ?0
?1? ? ? ? ?m? ?m for some ?0, . . . ,?m ? (? ? N)?, then we say that
d = ?1 ? ? ??m derives ?m from ?0, and we write ?0 d? ?m;  derives any ?0 ? (? ? N)?
from itself. A derivation d such that S d? w, for some w ? ??, is called a complete
derivation. We say that G is unambiguous if for each w ? ??, S d? w for at most
one d ? R?.
Let G be a fixed PCFG (?, N, S, R, pG ). For ?,? ? (? ? N)? and d = ?1 ? ? ??m ? R?,
m ? 0, we define pG (? d? ?) =
?m
i=1 pG (?i) if ?
d? ?, and pG (?
d? ?) = 0 otherwise. The
probability pG (w) of a string w ? ?? is defined to be
?
d pG (S
d? w).
PCFG G is said to be proper if
?
?,? pG (A
?? ?) = 1 for all A ? N, that is, if the
probabilities of all rules ? = (A ? ?) with left-hand side A sum to one. PCFG G is said to
be consistent if
?
w pG (w) = 1. Consistency implies that the PCFG defines a probability
distribution on the set of terminal strings. There is a practical sufficient condition for
consistency that is decidable (Booth and Thompson 1973).
A PCFG is said to be reduced if for each nonterminal A, there are d1, d2 ? R?,
w1, w2 ? ??, and ? ? (? ? N)? such that pG (S
d1? w1A?) ? pG (w1A?
d2? w1w2) > 0. In
words, if a PCFG is reduced, then for each nonterminal A, there is at least one derivation
d1d2 with nonzero probability that derives a string w1w2 from S and that includes
some rule with left-hand side A. A PCFG G that is not reduced can be turned into
one that is reduced and that describes the same probability distribution, provided that
?
w pG (w) > 0. This reduction consists in removing from the grammar any nonterminal
A for which the above conditions do not hold, together with any rule that contains
such a nonterminal; see Aho and Ullman (1972) for reduction of CFGs, which is very
similar.
A finite automaton M is a 5-tuple (?, Q, q0, qf , T), where ? and Q are two
finite sets of terminals and states, respectively, q0, qf ? Q are the initial and final
states, respectively, and T is a finite set of transitions, each of the form r a? s, where
r ? Q ? {qf}, s ? Q, and a ? ?.2 A probabilistic finite automaton M is a 6-tuple (?, Q,
q0, qf , T, pM), where?, Q, q0, qf , and T are as above, and pM is a function from transitions
in T to probabilities.
In what follows, symbols q, r, s range over the set Q, symbol ? ranges over the set T,
and symbol c ranges over the set T?.
For a fixed (P)FA M, we define a configuration to be an element of Q ? ??, and we
define the relation  on triples consisting of two configurations and a transition ? ? T
by (r, w)
?
 (s, w?) if and only if w is of the form aw?, for some a ? ?, and ? = (r a? s).
A computation (in M) is a string c = ?1 ? ? ? ?m, m ? 0, such that (r0, w0)
?1
 (r1, w1)?2
 ? ? ? ?m (rm, wm), for some (r0, w0), . . . , (rm, wm) ? Q ? ??; c =  is always a compu-
tation. If (r0, w0)
?1
 ? ? ? ?m (rm, wm) for some (r0, w0), . . . , (rm, wm) ? Q ? ?? and c = ?1 ? ? ?
?m ? T?, then we write (r0, w0)
c
 (rm, wm). We say that c recognizes w if (q0, w)
c
 (qf ,).
2 That we only allow one final state is not a serious restriction with regard to the set of strings we can
process; only when the empty string is to be recognized could this lead to difficulties. Lifting the
restriction would encumber the presentation with treatment of additional cases without affecting,
however, the validity of the main results.
176
Nederhof Training Models on Models
Let M be a fixed FA (?, Q, q0, qf , T). The language L(M) accepted by M is
defined to be {w ? ?? | ?c[(q?, w)
c
 (qf ,)]}. We say M is unambiguous if for each
w ? ??, (q0, w)
c
 (qf ,) for at most one c ? T?. We say M is deterministic if for each
(r, w) ? Q ? ??, there is at most one combination of ? ? T and (s, w?) ? Q ? ?? such
that (r, w)
?
 (s, w?). Turning a given FA into one that is deterministic and accepts the
same language is called determinization. All FAs can be determinized. Turning a given
(deterministic) FA into the smallest (deterministic) FA that accepts the same language
is called minimization. There are effective algorithms for minimization of deterministic
FAs.
Let M be a fixed PFA (?, Q, q0, qf , T, pM). For (r, w), (s, v) ? Q ? ?? and
c = ?1 ? ? ? ?m ? T?, we define pM((r, w)
c
 (s, v)) =
?m
i=1 pM(?i) if (r, w)
c
 (s, v), and
pM((r, w)
c
 (s, v)) = 0 otherwise. The probability pM(w) of a string w ? ?? is defined
to be
?
c pM((q0, w)
c
 (qf ,)).
PFA M is said to be proper if
?
?,a,s: ?=(r a?s)?T pM(?) = 1 for all r ? Q ? {qf}.
3. Expected Frequencies of Rules
Let G be a PCFG (?, N, S, R, pG ). We assume without loss of generality that S does not
occur in the right-hand side of any rule from R. For each rule ?, we define
E(?) =
?
d,d?,w
pG (S
d?d?? w) (1)
If G is proper and consistent, (1) is the expected frequency of ? in a complete derivation.
Each complete derivation d?d? can be written as d?d??d???, with d? = d??d???, where
S d? w?A?, A ?? ?,? d
??
? w??,? d
???
? w??? (2)
for some A, ?, ?, w?, w??, and w???. Therefore
E(?) = outer(A) ? pG (?) ? inner(?) (3)
where we define
outer(A) =
?
d,w?,?,d???,w???
pG (S
d? w?A?) ? pG (?
d???? w???) (4)
inner(?) =
?
d??,w??
pG (?
d??? w??) (5)
for each A ? N and ? ? (? ? N)?. From the definition of inner, we can easily derive the
following equations:
inner(a) = 1 (6)
inner(A) =
?
?,?:
?=(A??)
pG (?) ? inner(?) (7)
inner(X?) = inner(X) ? inner(?) (8)
177
Computational Linguistics Volume 31, Number 2
This can be taken as a recursive definition of inner, assuming ? =  in (8). Similarly, we
can derive a recursive definition of outer:
outer(S) = 1 (9)
outer(A) =
?
?,B,?,?:
?=(B??A?)
outer(B) ? pG (?) ? inner(?) ? inner(?) (10)
for A = S.
In general, there may be cyclic dependencies in the equations for inner and outer;
that is, for certain nonterminals A, inner(A) and outer(A) may be defined in terms
of themselves. There may even be no closed-form expression for inner(A). However,
one may approximate the solutions to arbitrary precision by means of fixed-point
iteration.
4. Intersection of Context-Free and Regular Languages
We recall a construction from Bar-Hillel, Perles, and Shamir (1964) that computes the
intersection of a context-free language and a regular language. The input consists of a
CFG G = (?, N, S, R) and an FA M= (?, Q, q0, qf , T); note that we assume, without loss
of generality, that G and M share the same set of terminals ?.
The output of the construction is CFG G? = (?, N?, S?, R?), where N? = Q ?
(? ? N) ? Q, S? = (q0, S, qf ), and R? consists of the set of rules that is obtained as
follows:
 For each rule ? = (A ? X1 ? ? ?Xm) ? R, m ? 0, and each sequence of states
r0, . . . , rm ? Q, let the rule ?? = ((r0, A, rm) ? (r0, X1, r1) ? ? ? (rm?1, Xm, rm))
be in R?; for m = 0, R? contains a rule ?? = ((r0, A, r0) ? ) for each
state r0.
 For each transition ? = (r a? s) ? T, let the rule ?? = ((r, a, s) ? a) be
in R?.
Note that for each rule (r0, A, rm) ? (r0, X1, r1) ? ? ? (rm?1, Xm, rm) from R?, there is a
unique rule A ? X1 ? ? ?Xm from R from which it has been constructed by the above.
Similarly, each rule (r, a, s) ? a uniquely identifies a transition r a? s. This means that if
we take a derivation d? in G?, we can extract a sequence h1(d?) of rules from G and a
sequence h2(d?) of transitions from M, where h1 and h2 are string homomorphisms that
we define pointwise as
h1(??) = ? if ?? = ((r0, A, rm) ? (r0, X1, r1) ? ? ? (rm?1, Xm, rm))
and ? = (A ? X1 ? ? ?Xm)
(11)
 if ?? = ((r, a, s) ? a) (12)
h2(??) = ? if ?? = ((r, a, s) ? a) and ? = (r
a? s) (13)
 if ?? = ((r0, A, rm) ? (r0, X1, r1) ? ? ? (rm?1, Xm, rm)) (14)
178
Nederhof Training Models on Models
We define h(d?) = (h1(d?), h2(d?)). It can be easily shown that if h(d?) = (d, c) and
S?
d?? w, then for the same w, we have S d? w and (q0, w)
c
 (qf ,). Conversely, if for some
w, d, and c we have S d? w and (q0, w)
c
 (qf ,), then there is precisely one derivation d?
such that h(d?) = (d, c) and S?
d?? w.
It was observed by Lang (1994) that G? can be seen as a parse forest, that is, a
compact representation of all parse trees according to G that derive strings recognized
by M. The construction can be generalized to, for example, tree-adjoining grammars
(Vijay-Shanker and Weir 1993) and range concatenation grammars (Boullier 2000;
Bertsch and Nederhof 2001). The construction for the latter also has implications for
linear context-free rewriting systems (Seki et al 1991).
The construction has been extended by Nederhof and Satta (2003) to apply to a
PCFG G = (?, N, S, R, pG ) and a PFA M = (?, Q, q0, qf , T, pM). The output is a
PCFG G? = (?, N?, S?, R?, p?), where N?, S?, and R? are as before, and p? is
defined by
p?((r0, A, rm) ? (r0, X1, r1) ? ? ? (rm?1, Xm, rm)) = pG (A ? X1 ? ? ?Xm) (15)
p?((r, a, s) ? a) = pM(r
a? s) (16)
If d?, d, and c are such that h(d?) = (d, c), then clearly p?(d?) = pG (d) ? pM(c).
5. Training Models on Models
We restrict ourselves to a few cases of the general technique of training a model on the
basis of another model.
5.1 Training a PFA on a PCFG
Let us assume we have a proper and consistent PCFG G = (?, N, S, R, pG ) and an FA
M = (?, Q, q0, qf , T) that is unambiguous. This FA may have resulted from (nonprob-
abilistic) approximation of CFG (?, N, S, R), but it may also be totally unrelated to G.
Note that an FA is guaranteed to be unambiguous if it is deterministic; any FA can be
determinized. Our goal is now to assign probabilities to the transitions from FA M to
obtain a proper PFA that approximates the probability distribution described by G as
well as possible.
Let us define 1 as the function that maps each transition from T to one. This means
that for each r, w, c and s, 1((r, w)
c
 (s,)) = 1 if (r, w)
c
 (s,), and 1((r, w)
c
 (s,)) = 0
otherwise.
Of the set of strings generated by G, a subset is recognized by computations of M;
note again that there can be at most one such computation for each string. The expected
frequency of a transition ? in such computations is given by
E(?) =
?
w,c,c?
pG (w) ? 1((q0, w)
c?c?
 (qf ,)) (17)
Now we construct the PCFG G? as explained in section 4 from the PCFG G and the
PFA (?, Q, q0, qf , T, 1). Let ? = (r
a? s) ? T and ? = ((r, a, s) ? a). On the basis of the
179
Computational Linguistics Volume 31, Number 2
properties of function h, we can now rewrite E(?) as
E(?) =
?
d,w,c,c?
pG (S
d? w) ? 1((q0, w)
c?c?
 (qf ,))
=
?
e,d,w,c,c? :
h(e)=(d,c?c? )
pG (S
d? w) ? 1((q0, w)
c?c?
 (qf ,))
=
?
e,e?,w
p?(S?
e?e?? w)
= E(?) (18)
Hereby we have expressed the expected frequency of a transition ? = (r a? s) in
terms of the expected frequency of rule ? = ((r, a, s) ? a) in derivations in PCFG G?.
It was explained in section 3 how such a value can be computed. Note that since
by definition 1(?) = 1, also p?(?) = 1. Furthermore, for the right-hand side a of ?,
inner(a) = 1. Therefore,
E(?) = outer((r, a, s)) ? p?(?) ? inner(a)
= outer((r, a, s)) (19)
To obtain the required PFA (?, Q, q0, qf , T, pM), we now define the probability
function pM for each ? = (r
a? s) ? T as
pM(?) =
outer((r, a, s))
?
a?,s?:(r a
?
?s? )?T
outer((r, a?, s?))
(20)
That such a relative frequency estimator pM minimizes the KL distance between pG and
pM on the domain L(M) is proven in the appendix.
An example with finite languages is given in Figure 1. We have, for example,
pM(q0
a? q1) =
outer((q0, a, q1))
outer((q0, a, q1)) + outer((q0, c, q1))
=
1
3
1
3 +
2
3
= 13 (21)
5.2 Training a PCFG on a PFA
Similarly to section 5.1, we now assume we have a proper PFA M = (?, Q, q0,
qf , T, pM) and a CFG G = (?, N, S, R) that is unambiguous. Our goal is to find a
function pG that lets proper and consistent PCFG (?, N, S, R, pG ) approximate M as
well as possible. Although CFGs used for natural language processing are usually
ambiguous, there may be cases in other fields in which we may assume grammars are
unambiguous.
180
Nederhof Training Models on Models
Figure 1
Example of input PCFG G, with rule probabilities between square brackets, input FA M, the
reduced PCFG G?, and the resulting trained PFA.
Let us define 1 as the function that maps each rule from R to one. Of the set of
strings recognized by M, a subset can be derived in G. The expected frequency of a rule
? in those derivations is given by
E(?) =
?
d,d?,w
pM(w) ? 1(S
d?d?? w) (22)
Now we construct the PCFG G? from the PCFG G = (?, N, S, R, 1) and the
PFA M as explained in section 4. Analogously to section 5.1, we obtain for each
? = (A ? X1 ? ? ?Xm)
E(?) =
?
r0,r1,...,rm
E((r0, A, rm) ? (r0, X1, r1) ? ? ? (rm?1, Xm, rm))
=
?
r0,r1,...,rm
outer((r0, A, rm)) ? inner((r0, X1, r1) ? ? ? (rm?1, Xm, rm)) (23)
To obtain the required PCFG (?, N, S, R, pG ), we now define the probability function
pG for each ? = (A ? ?) as
pG (?) =
E(?)
?
??=(A??? )?R E(??)
(24)
The proof that this relative frequency estimator pG minimizes the KL distance between
pM and pG on the domain L(G) is almost identical to the proof in the appendix for a
similar claim from section 5.1.
5.3 Training a PFA on a PFA
We now assume we have a proper PFA M1 = (?, Q1, q0,1, qf,1, T1, p1) and an FA
M2 = (?, Q2, q0,2, qf,2, T2) that is unambiguous. Our goal is to find a function p2 so that
181
Computational Linguistics Volume 31, Number 2
proper PFA (?, Q2, q0,2, qf,2, T2, p2) approximates M1 as well as possible, minimizing
the KL distance between p1 and p2 on the domain L(M2).
One way to solve this problem is to map M2 to an equivalent right-linear CFG G and
then to apply the algorithm from section 5.2. The obtained probability function pG can
be translated back to an appropriate function p2. For this special case, the construction
from section 4 can be simplified to the ?cross-product? construction of finite automata
(see, e.g., Aho and Ullman 1972). The simplified forms of the functions inner and outer
from section 3 are commonly called forward and backward, respectively, and they are
defined by systems of linear equations. As a result, we can compute exact solutions, as
opposed to approximate solutions by iteration.
Appendix
We now prove that the choice of pM in section 5.1 is such that it minimizes the Kullback-
Leibler distance between pG and pM, restricted to the domain L(M). Without this
restriction, the KL distance is given by
D(pG?pM) =
?
w
pG (w) ? log
pG (w)
pM(w)
(25)
This can be used for many applications mentioned in section 1. For example, an FA M
approximating a CFG G is guaranteed to be such that L(M) ? L(G) in the case of most
practical approximation algorithms. However, if there are strings w such that w /? L(M)
and pG (w) > 0, then (25) is infinite, regardless of the choice of pM. We therefore restrict
pG to the domain L(M) and normalize it to obtain
pG|M(w) =
pG (w)
Z , if w ? L(M) (26)
0, otherwise (27)
where Z =
?
w:w?L(M) pG (w). Note that pG|M = pG if L(M) ? L(G). Our goal is now to
show that our choice of pM minimizes
D(pG|M?pM) =
?
w:w?L(M)
pG|M(w) ? log
pG|M(w)
pM(w)
= log 1Z +
1
Z
?
w:w?L(M)
pG (w) ? log
pG (w)
pM(w)
(28)
As Z is independent of pM, it is sufficient to show that our choice of pM minimizes
?
w:w?L(M)
pG (w) ? log
pG (w)
pM(w)
(29)
Now consider the expression
?
?
pM(?)E(?) (30)
182
Nederhof Training Models on Models
By the usual proof technique with Lagrange multipliers, it is easy to show that our
choice of pM in section 5.1, given by
pM(?) =
E(?)
?
??,a?,s?:??=(r a
?
?s? )?T
E(??)
(31)
for each ? = (r a? s) ? T, is such that it maximizes (30), under the constraint of
properness.
For ? ? T and w ? ??, we define #?(w) to be zero, if w /? L(M), and otherwise to be
the number of occurrences of ? in the (unique) computation that recognizes w. Formally,
#?(w) =
?
c,c? 1((q0, w)
c?c?
 (qf ,)). We rewrite (30) as
?
?
pM(?)E(?) =
?
?
pM(?)
?
w pG (w)?#?(w)
=
?
w
?
?
pM(?)pG (w)?#?(w)
=
?
w
(
?
?
pM(?)#?(w)
)pG (w)
=
?
w:pM(w)>0
pM(w)pG (w)
=
?
w:pM(w)>0
2pG (w)?log pM(w)
=
?
w:pM(w)>0
2pG (w)?log pM(w)?pG (w)?log pG (w)+pG (w)?log pG (w)
=
?
w:pM(w)>0
2?pG (w)?log
pG (w)
pM (w)
+pG (w)?log pG (w)
= 2?
?
w:pM (w)>0 pG (w)?log
pG (w)
pM (w) ? 2
?
w:pM (w)>0 pG (w)?log pG (w) (32)
We have already seen that the choice of pM that maximizes (30) is given by (31), and
(31) implies pM(w) > 0 for all w such that w ? L(M) and pG (w) > 0. Since pM(w) > 0 is
impossible for w /? L(M), the value of
2
?
w:pM (w)>0 pG (w)?log pG (w) (33)
is determined solely by pG and by the condition that pM(w) > 0 for all w such that
w ? L(M) and pG (w) > 0. This implies that (30) is maximized by choosing pM such
that
2?
?
w:pM (w)>0 pG (w)?log
pG (w)
pM (w) (34)
183
Computational Linguistics Volume 31, Number 2
is maximized, or alternatively that
?
w:pM(w)>0
pG (w) ? log
pG (w)
pM(w)
(35)
is minimized, under the constraint that pM(w) > 0 for all w such that w ? L(M) and
pG (w) > 0. For this choice of pM, (29) equals (35).
Conversely, if a choice of pM minimizes (29), we may assume that pM(w) > 0 for
all w such that w ? L(M) and pG (w) > 0, since otherwise (29) is infinite. Again, for this
choice of pM, (29) equals (35). It follows that the choice of pM that minimizes (29) concurs
with the choice of pM that maximizes (30), which concludes our proof.
Acknowledgments
Comments by Khalil Sima?an, Giorgio Satta,
Yuval Krymolowski, and anonymous
reviewers are gratefully acknowledged. The
author is supported by the PIONIER Project
Algorithms for Linguistic Processing, funded
by NWO (Dutch Organization for Scientific
Research).
References
Aho, Alfred V. and Jeffrey D. Ullman. 1972.
Parsing, volume 1 of The Theory of Parsing,
Translation and Compiling. Prentice Hall,
Englewood Cliffs, NJ.
Bar-Hillel, Yehoshua, M. Perles, and
E. Shamir. 1964. On formal properties of
simple phrase structure grammars. In
Yehoshua Bar-Hillel, editor, Language and
Information: Selected Essays on Their Theory
and Application. Addison-Wesley, Reading,
MA, pages 116?150.
Bertsch, Eberhard and Mark-Jan Nederhof.
2001. On the complexity of some
extensions of RCG parsing. In Proceedings
of the Seventh International Workshop on
Parsing Technologies, pages 66?77, Beijing,
October.
Booth, Taylor L. and Richard A. Thompson.
1973. Applying probabilistic measures to
abstract languages. IEEE Transactions on
Computers, C-22(5):442?450.
Boullier, Pierre. 2000. Range concatenation
grammars. In Proceedings of the Sixth
International Workshop on Parsing
Technologies, pages 53?64, Trento, Italy,
February.
Jurafsky, Daniel, Chuck Wooters, Gary
Tajchman, Jonathan Segal, Andreas
Stolcke, Eric Fosler, and Nelson Morgan.
1994. The Berkeley Restaurant Project. In
Proceedings of the International Conference on
Spoken Language Processing (ICSLP-94),
pages 2139?2142, Yokohama, Japan.
Lang, Bernard. 1994. Recognition can be
harder than parsing. Computational
Intelligence, 10(4):486?494.
Manning, Christopher D. and Hinrich
Schu?tze. 1999. Foundations of Statistical
Natural Language Processing. MIT Press,
Cambridge, MA.
Mohri, Mehryar. 1997. Finite-state
transducers in language and speech
processing. Computational Linguistics,
23(2):269?311.
Mohri, Mehryar and Mark-Jan Nederhof.
2001. Regular approximation of
context-free grammars through
transformation. In J.-C. Junqua and G. van
Noord, editors, Robustness in Language and
Speech Technology. Kluwer Academic,
pages 153?163.
Nederhof, Mark-Jan. 2000. Practical
experiments with regular approximation
of context-free languages. Computational
Linguistics, 26(1):17?44.
Nederhof, Mark-Jan and Giorgio Satta. 2003.
Probabilistic parsing as intersection. In
Proceedings of the Eighth International
Workshop on Parsing Technologies, pages
137?148, Laboratoire Lorrain de recherche
en informatique et ses applications
(LORIA), Nancy, France, April.
Paz, Azaria. 1971. Introduction to Probabilistic
Automata. Academic Press, New York.
Rimon, Mori and J. Herz. 1991. The
recognition capacity of local syntactic
constraints. In Proceedings of the Fifth
Conference of the European Chapter of the
ACL, pages 155?160, Berlin, April.
Santos, Eugene S. 1972. Probabilistic
grammars and automata. Information and
Control, 21:27?47.
Seki, Hiroyuki, Takashi Matsumura,
Mamoru Fujii, and Tadao Kasami.
1991. On multiple context-free grammars.
Theoretical Computer Science,
88:191?229.
184
Nederhof Training Models on Models
Starke, Peter H. 1972. Abstract Automata.
North-Holland, Amsterdam.
Stolcke, Andreas and Jonathan Segal. 1994.
Precise N-gram probabilities from
stochastic context-free grammars. In
Proceedings of the 32nd Annual Meeting
of the ACL, pages 74?79, Las Cruces,
NM, June.
Vijay-Shanker, K. and David J. Weir.
1993. The use of shared forests in
tree adjoining grammar parsing. In
Proceedings of the Sixth Conference of the
European Chapter of the ACL, pages 384?393,
Utrecht, The Netherlands, April.
Zue, Victor, James Glass, David Goodine,
Hong Leung, Michael Phillips, Joseph
Polifroni, and Stephanie Seneff. 1991.
Integration of speech recognition and
natural language processing in the MIT
Voyager system. In Proceedings of the
ICASSP-91, Toronto, volume 1, pages
713?716.
185

Parsing Non-Recursive Context-Free Grammars
Mark-Jan Nederhof  
Faculty of Arts
University of Groningen
P.O. Box 716
NL-9700 AS Groningen, The Netherlands
markjan@let.rug.nl
Giorgio Satta
Dip. di Elettronica e Informatica
Universita` di Padova
via Gradenigo, 6/A
I-35131 Padova, Italy
satta@dei.unipd.it
Abstract
We consider the problem of parsing
non-recursive context-free grammars, i.e.,
context-free grammars that generate finite
languages. In natural language process-
ing, this problem arises in several areas
of application, including natural language
generation, speech recognition and ma-
chine translation. We present two tabu-
lar algorithms for parsing of non-recursive
context-free grammars, and show that they
perform well in practical settings, despite
the fact that this problem is PSPACE-
complete.
1 Introduction
Several applications in natural language processing
require ?parsing? of a large but finite set of candidate
strings. Here parsing means some computation that
selects those strings out of the finite set that are well-
formed according to some grammar, or that are most
likely according to some language model. In these
applications, the finite set is typically encoded in a
compact way as a context-free grammar (CFG) that
is non-recursive. This is motivated by the fact that
non-recursive CFGs allow very compact represen-
tations for finite languages, since the strings deriv-
able from single nonterminals may be substrings of
many different strings in the language. Unfolding
such a grammar and parsing the generated strings

Secondary affiliation is the German Research Center for
Artificial Intelligence (DFKI).
one by one then leads to an unnecessary duplica-
tion of subcomputations, since each occurrence of
a repeated substring has to be independently parsed.
As this approach may be prohibitively expensive, it
is preferable to find a parsing algorithm that shares
subcomputations among different strings by work-
ing directly on the nonterminals and the rules of the
non-recursive CFG. In this way, ?parsing? a nonter-
minal of the grammar amounts to shared parsing of
all the substrings encoded by that nonterminal.
To give a few examples, in some natural lan-
guage generation systems (Langkilde, 2000) non-
recursive CFGs are used to encode very large sets
of candidate sentences realizing some input con-
ceptual representation (Langkilde calls such gram-
mars forests). Each CFG is later ?parsed? using a
language model, in order to rank the sentences in
the set according to their likelyhood. Similarly, in
some approaches to automatic speech understand-
ing (Corazza and Lavelli, 1994) the  -best sen-
tences obtained from the speech recognition module
are ?compressed? into a non-recursive CFG gram-
mar, which is later provided as input to a parser. Fi-
nally, in some machine translation applications re-
lated techniques are exploited to obtain sentences
that simultaneously realize two different conceptual
representations (Knight and Langkilde, 2000). This
is done in order to produce translations that preserve
syntactic or semantic ambiguity in cases where the
ambiguity could not be resolved when processing
the source sentence.
To be able to describe the above applications in an
abstract way, let us first fix some terminology. The
term ?recognition? refers to the process of deciding
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 112-119.
                         Proceedings of the 40th Annual Meeting of the Association for
whether an input string is in the language described
by a grammar, the parsing grammar  . We will
generalize this notion in a natural way to input rep-
resenting a set of strings, and here the goal of recog-
nition is to decide whether at least one of the strings
in the set is in the language described by  . If the
input is itself given in the form of a grammar, the
input grammar  , then recognition amounts to de-
termining whether the intersection of the languages
described by  and 	 is non-empty. In this paper
we use the term parsing as synonymous to recog-
nition, since the recognition algorithms we present
can be easily extended to yield parse trees (with as-
sociated probabilities if either 
 or 	 or both are
probabilistic).
In what follows we consider the case where both
	 and  are CFGs. General CFGs have un-
favourable computational properties with respect to
intersection. In particular, the problem of deciding
whether the intersection of two CFGs is non-empty
is undecidable (Harrison, 1978). Following the ter-
minology adopted above, this means that parsing
a context-free input grammar  on the basis of a
context-free parsing grammar  is not possible in
general.
One way to make the parsing problem decidable
is to place some additional restrictions on 
 or
	 . This direction is taken by Langkilde (2000),
where   is a non-recursive CFG and   repre-
sents a regular language, more precisely an

-gram
model. In this way the problem can be solved us-
ing a stochastic variant of an algorithm presented
by Bar-Hillel et al (1964), where it is shown that the
intersection of a general context-free language and a
regular language is still context-free.
In the present paper we leave the theoretical
framework of Bar-Hillel et al (1964), and consider
parsing grammars  that are unrestricted CFGs,
and input grammars  that are non-recursive
context-free grammars. In this case the parsing (in-
tersection) problem becomes PSPACE-complete.1
Despite of this unfavourable theoretical result, algo-
rithms for the problem at hand have been proposed
in the literature and are currently used in practical
applications. In (Knight and Langkilde, 2000) 
 is
1The PSPACE-hardness result has been shown by Harry B.
Hunt III and Dan Rosenkrantz (Harry B. Hunt III, p.c.). Mem-
bership in PSPACE is shown by Nederhof and Satta (2002).
unfolded into a lattice (acyclic finite automaton) and
later parsed with  using an algorithm close to the
one by Bar-Hillel et al (1964). The algorithm pro-
posed by Corazza and Lavelli (1994) involves copy-
ing of charts, and this makes it very similar in be-
haviour to the former approach. Thus in both al-
gorithms parts of the input grammar  are copied
where a nonterminal occurs more than once, which
destroys the compactness of the representation. In
this paper we propose two alternative tabular algo-
rithms that exploit the compactness of 
 as much
as possible. Although a limited amount of copying
is also done by our algorithms, this never happens in
cases where the resulting structure is ungrammatical
with respect to the parsing grammar   .
The structure of this paper is as follows. In Sec-
tion 2 we introduce some preliminary definitions,
followed in Section 3 by a first algorithm based on
CKY parsing. A more sophisticated algorithm, sat-
isfying the equivalent of the correct-prefix property
and based on Earley?s algorithm, is presented in Sec-
tion 4. Section 5 presents our experimental results
and Section 6 closes with some discussion.
2 Preliminaries
In this section we briefly recall some standard no-
tions from formal language theory. For more details
we refer the reader to textbooks such as (Harrison,
1978).
A context-free grammar is a 4-tuple 

, where  is a finite set of terminals, called the
alphabet,  is a finite set of nonterminals, including
the start symbol  , and  is a finite set of rules hav-
ing the form Probabilistic Parsing Strategies
Mark-Jan Nederhof
Faculty of Arts
University of Groningen
P.O. Box 716
NL-9700 AS Groningen
The Netherlands
markjan@let.rug.nl
Giorgio Satta
Dept. of Information Engineering
University of Padua
via Gradenigo, 6/A
I-35131 Padova
Italy
satta@dei.unipd.it
Abstract
We present new results on the relation between
context-free parsing strategies and their probabilis-
tic counter-parts. We provide a necessary condition
and a sufficient condition for the probabilistic exten-
sion of parsing strategies. These results generalize
existing results in the literature that were obtained
by considering parsing strategies in isolation.
1 Introduction
Context-free grammars (CFGs) are standardly used
in computational linguistics as formal models of the
syntax of natural language, associating sentences
with all their possible derivations. Other computa-
tional models with the same generative capacity as
CFGs are also adopted, as for instance push-down
automata (PDAs). One of the advantages of the use
of PDAs is that these devices provide an operational
specification that determines which steps must be
performed when parsing an input string, something
that is not offered by CFGs. In other words, PDAs
can be associated to parsing strategies for context-
free languages. More precisely, parsing strategies
are traditionally specified as constructions that map
CFGs to language-equivalent PDAs. Popular ex-
amples of parsing strategies are the standard con-
structions of top-down PDAs (Harrison, 1978), left-
corner PDAs (Rosenkrantz and Lewis II, 1970),
shift-reduce PDAs (Aho and Ullman, 1972) and LR
PDAs (Sippu and Soisalon-Soininen, 1990).
CFGs and PDAs have probabilistic counterparts,
called probabilistic CFGs (PCFGs) and probabilis-
tic PDAs (PPDAs). These models are very popular
in natural language processing applications, where
they are used to define a probability distribution
function on the domain of all derivations for sen-
tences in the language of interest. In PCFGs and
PPDAs, probabilities are assigned to rules or tran-
sitions, respectively. However, these probabilities
cannot be chosen entirely arbitrarily. For example,
for a given nonterminalA in a PCFG, the sum of the
probabilities of all rules rewritingAmust be 1. This
means that, out of a total of saym rules rewritingA,
only m? 1 rules represent ?free? parameters.
Depending on the choice of the parsing strategy,
the constructed PDA may allow different probabil-
ity distributions than the underlying CFG, since the
set of free parameters may differ between the CFG
and the PDA, both quantitatively and qualitatively.
For example, (Sornlertlamvanich et al, 1999) and
(Roark and Johnson, 1999) have shown that a prob-
ability distribution that can be obtained by training
the probabilities of a CFG on the basis of a corpus
can be less accurate than the probability distribution
obtained by training the probabilities of a PDA con-
structed by a particular parsing strategy, on the basis
of the same corpus. Also the results from (Chitrao
and Grishman, 1990), (Charniak and Carroll, 1994)
and (Manning and Carpenter, 2000) could be seen
in this light.
The question arises of whether parsing strate-
gies can be extended probabilistically, i.e., whether
a given construction of PDAs from CFGs can be
?augmented? with a function defining the probabili-
ties for the target PDA, given the probabilities asso-
ciated with the input CFG, in such a way that the ob-
tained probabilistic distributions on the CFG deriva-
tions and the corresponding PDA computations are
equivalent. Some first results on this issue have been
presented by (Tendeau, 1995), who shows that the
already mentioned left-corner parsing strategy can
be extended probabilistically, and later by (Abney et
al., 1999) who show that the pure top-down parsing
strategy and a specific type of shift-reduce parsing
strategy can be probabilistically extended.
One might think that any ?practical? parsing
strategy can be probabilistically extended, but this
turns out not to be the case. We briefly discuss
here a counter-example, in order to motivate the ap-
proach we have taken in this paper. Probabilistic
LR parsing has been investigated in the literature
(Wright and Wrigley, 1991; Briscoe and Carroll,
1993; Inui et al, 2000) under the assumption that
it would allow more fine-grained probability distri-
butions than the underlying PCFGs. However, this
is not the case in general. Consider a PCFG with
rule/probability pairs:
S ? AB , 1 B ? bC , 23
A? aC , 13 B ? bD ,
1
3
A? aD , 23 C ? xc, 1
D ? xd , 1
There are two key transitions in the associated LR
automaton, which represent shift actions over c and
d (we denote LR states by their sets of kernel items
and encode these states into stack symbols):
?c : {C ? x ? c,D ? x ? d}
c7?
{C ? x ? c,D ? x ? d} {C ? xc ?}
?d : {C ? x ? c,D ? x ? d}
d7?
{C ? x ? c,D ? x ? d} {D ? xd ?}
Assume a proper assignment of probabilities to the
transitions of the LR automaton, i.e., the sum of
transition probabilities for a given LR state is 1. It
can be easily seen that we must assign probabil-
ity 1 to all transitions except ?c and ?d, since this
is the only pair of distinct transitions that can be ap-
plied for one and the same top-of-stack symbol, viz.
{C ? x ? c,D ? x ? d}. However, in the PCFG
model we have
Pr(axcbxd)
Pr(axdbxc) =
Pr(A?aC )?Pr(B?bD)
Pr(A?aD)?Pr(B?bC ) =
1
3 ?
1
3
2
3 ?
2
3
= 14
whereas in the LR PPDA model we have
Pr(axcbxd)
Pr(axdbxc) =
Pr(?c)?Pr(?d)
Pr(?d)?Pr(?c)
= 1 6= 14 .
Thus we conclude that there is no proper assignment
of probabilities to the transitions of the LR automa-
ton that would result in a distribution on the gener-
ated language that is equivalent to the one induced
by the source PCFG. Therefore the LR strategy does
not allow probabilistic extension.
One may seemingly solve this problem by drop-
ping the constraint of properness, letting each tran-
sition that outputs a rule have the same probability
as that rule in the PCFG, and letting other transitions
have probability 1. However, the properness condi-
tion for PDAs has been heavily exploited in pars-
ing applications, in doing incremental left-to-right
probability computation for beam search (Roark
and Johnson, 1999; Manning and Carpenter, 2000),
and more generally in integration with other lin-
ear probabilistic models. Furthermore, commonly
used training algorithms for PCFGS/PPDAs always
produce proper probability assignments, and many
desired mathematical properties of these methods
are based on such an assumption (Chi and Geman,
1998; Sa?nchez and Bened??, 1997). We may there-
fore discard non-proper probability assignments in
the current study.
However, such probability assignments are out-
side the reach of the usual training algorithms for
PDAs, which always produce proper PDAs. There-
fore, we may discard such assignments in the cur-
rent study, which investigates aspects of the poten-
tial of training algorithms for CFGs and PDAs.
What has been lacking in the literature is a theo-
retical framework to relate the parameter space of a
CFG to that of a PDA constructed from the CFG by
a particular parsing strategy, in terms of the set of
allowable probability distributions over derivations.
Note that the number of free parameters alone is
not a satisfactory characterization of the parameter
space. In fact, if the ?nature? of the parameters is
ill-chosen, then an increase in the number of param-
eters may lead to a deterioration of the accuracy of
the model, due to sparseness of data.
In this paper we extend previous results, where
only a few specific parsing strategies were consid-
ered in isolation, and provide some general char-
acterization of parsing strategies that can be prob-
abilistically extended. Our main contribution can
be stated as follows.
? We define a theoretical framework to relate the
parameter space defined by a CFG and that de-
fined by a PDA constructed from the CFG by a
particular parsing strategy.
? We provide a necessary condition and a suffi-
cient condition for the probabilistic extension
of parsing strategies.
We use the above findings to establish new results
about probabilistic extensions of parsing strategies
that are used in standard practice in computational
linguistics, as well as to provide simpler proofs of
already known results.
We introduce our framework in Section 3 and re-
port our main results in Sections 4 and 5. We discuss
applications of our results in Section 6.
2 Preliminaries
In this paper we assume some familiarity with def-
initions of (P)CFGs and (P)PDAs. We refer the
reader to standard textbooks and publications as for
instance (Harrison, 1978; Booth and Thompson,
1973; Santos, 1972).
A CFG G is a tuple (?, N, S, R), with ? and N
the sets of terminals and nonterminals, respectively,
S the start symbol and R the set of rules. In this
paper we only consider left-most derivations, repre-
sented as strings d ? R? and simply called deriva-
tions. For ?, ? ? (? ?N)?, we write ??d ? with
the usual meaning. If ? = S and ? = w ? ??, we
call d a complete derivation of w. We say a CFG is
reduced if each rule in R occurs in some complete
derivation.
A PCFG is a pair (G, p) consisting of a CFG G
and a probability function p from R to real num-
bers in the interval [0, 1]. A PCFG is proper
if
?
pi=(A??)?R p(pi) = 1 for each A ? N .
The probability of a (left-most) derivation d =
pi1 ? ? ?pim, pii ? R for 1 ? i ? m, is p(d) =?m
i=1 p(pii). The probability of a string w ? ??
is p(w) =
?
S?dw p(d). A PCFG is consistent if
?w??? p(w) = 1. A PCFG (G, p) is reduced if G is
reduced.
In this paper we will mainly consider push-down
transducers rather than push-down automata. Push-
down transducers not only compute derivations of
the grammar while processing an input string, but
they also explicitly produce output strings from
which these derivations can be obtained. We use
transducers for two reasons. First, constraints on
the output strings allow us to restrict our attention
to ?reasonable? parsing strategies. Those strategies
that cannot be formalized within these constraints
are unlikely to be of practical interest. Secondly,
mappings from input strings to derivations, such as
those realized by push-down transducers, turn out
to be a very powerful abstraction and allow direct
proofs of several general results.
Contrary to many textbooks, our push-down de-
vices do not possess states next to stack symbols.
This is without loss of generality, since states can
be encoded into the stack symbols, given the types
of transitions that we allow. Thus, a PDT A is a
6-tuple (??, ??, Q, Xin, Xfin, ?), with ?? and
?? the input and output alphabets, respectively, Q
the set of stack symbols, including the initial and fi-
nal stack symbols Xin and Xfin, respectively, and
? the set of transitions. Each transition has one
of the following three forms: X 7? XY , called a
push transition, YX 7? Z, called a pop transition,
or X
x,y
7? Y , called a swap transition; here X , Y ,
Z ? Q, x ? ?? ? {?} is the input read by the tran-
sition and y ? ??? is the written output. Note that
in our notation, stacks grow from left to right, i.e.,
the top-most stack symbol will be found at the right
end. A configuration of a PDT is a triple (?,w, v),
where ? ? Q? is a stack, w ? ??? is the remain-
ing input, and v ? ??? is the output generated so
far. Computations are represented as strings c ?
??. For configurations (?,w, v) and (?,w?, v?), we
write (?,w, v) `c (?,w?, v?) with the usual mean-
ing, and write (?,w, v) `? (?,w?, v?) when c is of
no importance. If (Xin, w, ?) `c (Xfin, ?, v), then
c is a complete computation of w, and the output
string v is denoted out(c). A PDT is reduced if
each transition in ? occurs in some complete com-
putation.
Without loss of generality, we assume that com-
binations of different types of transitions are not al-
lowed for a given stack symbol. More precisely,
for each stack symbol X 6= Xfin, the PDA can
only take transitions of a single type (push, pop or
swap). A PDT can easily be brought in this form
by introducing for each X three new stack symbols
Xpush , Xpop and Xswap and new swap transitions
X
?,?
7? Xpush , X
?,?
7? Xpop and X
?,?
7? Xswap . In
each existing transition that operates on top-of-stack
X , we then replace X by one from Xpush , Xpop or
Xswap , depending on the type of that transition. We
also assume that Xfin does not occur in the left-
hand side of a transition, again without loss of gen-
erality.
A PPDT is a pair (A, p) consisting of a PDT A
and a probability function p from? to real numbers
in the interval [0, 1]. A PPDT is proper if
? ??=(X 7?XY )?? p(?) = 1 for each X ? Q
such that there is at least one transition X 7?
XY , Y ? Q;
? ?
?=(X
x,y
7?Y )??
p(?) = 1 for each X ? Q such
that there is at least one transition X x,y7? Y ,
x ? ?? ? {?}, y ? ??? , Y ? Q; and
? ??=(Y X 7?Z)?? p(?) = 1, for each X,Y ? Q
such that there is at least one transition Y X 7?
Z, Z ? Q.
The probability of a computation c = ?1 ? ? ? ?m,
?i ? ? for 1 ? i ? m, is p(c) =?m
i=1 p(?i). The probability of a stringw is p(w) =?
(Xin,w,?)`c(Xfin,?,v) p(c). A PPDT is consistent
if ?w??? p(w) = 1. A PPDT (A, p) is reduced if
A is reduced.
3 Parsing Strategies
The term ?parsing strategy? is often used informally
to refer to a class of parsing algorithms that behave
similarly in some way. In this paper, we assign a
formal meaning to this term, relying on the obser-
vation by (Lang, 1974) and (Billot and Lang, 1989)
that many parsing algorithms for CFGs can be de-
scribed in two steps. The first is a construction of
push-down devices from CFGs, and the second is
a method for handling nondeterminism (e.g. back-
tracking or dynamic programming). Parsing algo-
rithms that handle nondeterminism in different ways
but apply the same construction of push-down de-
vices from CFGs are seen as realizations of the same
parsing strategy.
Thus, we define a parsing strategy to be a func-
tion S that maps a reduced CFG G = (??, N, S,
R) to a pair S(G) = (A, f) consisting of a reduced
PDT A = (??, ??, Q, Xin, Xfin, ?), and a func-
tion f that maps a subset of ??? to a subset of R?,
with the following properties:
? R ? ??.
? For each string w ? ??? and each complete
computation c on w, f(out(c)) = d is a (left-
most) derivation of w. Furthermore, each sym-
bol from R occurs as often in out(c) as it oc-
curs in d.
? Conversely, for each string w ? ??? and
each derivation d of w, there is precisely
one complete computation c on w such that
f(out(c)) = d.
If c is a complete computation, we will write f(c)
to denote f(out(c)). The conditions above then im-
ply that f is a bijection from complete computations
to complete derivations. Note that output strings of
(complete) computations may contain symbols that
are not in R, and the symbols that are in R may
occur in a different order in v than in f(v) = d.
The purpose of the symbols in ?? ? R is to help
this process of reordering of symbols from R in v,
as needed for instance in the case of the left-corner
parsing strategy (see (Nijholt, 1980, pp. 22?23) for
discussion).
A probabilistic parsing strategy is defined to
be a function S that maps a reduced, proper and
consistent PCFG (G, pG) to a triple S(G, pG) =
(A, pA, f), where (A, pA) is a reduced, proper and
consistent PPDT, with the same properties as a
(non-probabilistic) parsing strategy, and in addition:
? For each complete derivation d and each com-
plete computation c such that f(c) = d, pG(d)
equals pA(c).
In other words, a complete computation has the
same probability as the complete derivation that it
is mapped to by function f . An implication of
this property is that for each string w ? ??? , the
probabilities assigned to that string by (G, pG) and
(A, pA) are equal.
We say that probabilistic parsing strategy S ? is an
extension of parsing strategy S if for each reduced
CFG G and probability function pG we have S(G) =
(A, f) if and only if S ?(G, pG) = (A, pA, f) for
some pA.
4 Correct-Prefix Property
In this section we present a necessary condition for
the probabilistic extension of a parsing strategy. For
a given PDT, we say a computation c is dead if
(Xin, w1, ?) `c (?, ?, v1), for some ? ? Q?, w1 ?
??? and v1 ? ??? , and there are no w2 ? ??? and
v2 ? ??? such that (?,w2, ?) `? (Xfin, ?, v2). In-
formally, a dead computation is a computation that
cannot be continued to become a complete compu-
tation. We say that a PDT has the correct-prefix
property (CPP) if it does not allow any dead com-
putations. We also say that a parsing strategy has
the CPP if it maps each reduced CFG to a PDT that
has the CPP.
Lemma 1 For each reduced CFG G, there is a
probability function pG such that PCFG (G, pG) is
proper and consistent, and pG(d) > 0 for all com-
plete derivations d.
Proof. Since G is reduced, there is a finite set D
consisting of complete derivations d, such that for
each rule pi in G there is at least one d ? D in
which pi occurs. Let npi,d be the number of occur-
rences of rule pi in derivation d ? D, and let npi be
?d?D npi,d, the total number of occurrences of pi in
D. Let nA be the sum of npi for all rules pi withA in
the left-hand side. A probability function pG can be
defined through ?maximum-likelihood estimation?
such that pG(pi) = npinA for each rule pi = A? ?.
For all nonterminals A, ?pi=A?? pG(pi) =
?pi=A?? npinA =
nA
nA
= 1, which means that the
PCFG (G, pG) is proper. Furthermore, it has been
shown in (Chi and Geman, 1998; Sa?nchez and
Bened??, 1997) that a PCFG (G, pG) is consistent if
pG was obtained by maximum-likelihood estimation
using a set of derivations. Finally, since npi > 0 for
each pi, also pG(pi) > 0 for each pi, and pG(d) > 0
for all complete derivations d.
We say a computation is a shortest dead compu-
tation if it is dead and none of its proper prefixes is
dead. Note that each dead computation has a unique
prefix that is a shortest dead computation. For a
PDT A, let TA be the union of the set of all com-
plete computations and the set of all shortest dead
computations.
Lemma 2 For each proper PPDT (A, pA),
?c?TA pA(c) ? 1.
Proof. The proof is a trivial variant of the proof
that for a proper PCFG (G, pG), the sum of pG(d) for
all derivations d cannot exceed 1, which is shown by
(Booth and Thompson, 1973).
From this, the main result of this section follows.
Theorem 3 A parsing strategy that lacks the CPP
cannot be extended to become a probabilistic pars-
ing strategy.
Proof. Take a parsing strategy S that does not have
the CPP. Then there is a reduced CFG G = (??, N,
S, R), with S(G) = (A, f) for some A and f , and
a shortest dead computation c allowed by A.
It follows from Lemma 1 that there is a proba-
bility function pG such that (G, pG) is a proper and
consistent PCFG and pG(d) > 0 for all complete
derivations d. Assume we also have a probability
function pA such that (A, pA) is a proper and con-
sistent PPDT and pA(c?) = pG(f(c?)) for each com-
plete computation c?. SinceA is reduced, each tran-
sition ? must occur in some complete computation
c?. Furthermore, for each complete computation c?
there is a complete derivation d such that f(c?) = d,
and pA(c?) = pG(d) > 0. Therefore, pA(?) > 0
for each transition ? , and pA(c) > 0, where c is the
above-mentioned dead computation.
Due to Lemma 2, 1 ? ?c??TA pA(c?) ?
?w???? pA(w) + pA(c) > ?w???? pA(w) =
?w???? pG(w). This is in contradiction with the con-
sistency of (G, pG). Hence, a probability function
pA with the properties we required above cannot ex-
ist, and therefore S cannot be extended to become a
probabilistic parsing strategy.
5 Strong Predictiveness
In this section we present our main result, which is a
sufficient condition allowing the probabilistic exten-
sion of a parsing strategy. We start with a technical
result that was proven in (Abney et al, 1999; Chi,
1999; Nederhof and Satta, 2003).
Lemma 4 Given a non-proper PCFG (G, pG), G =
(?,N, S,R), there is a probability function p?G such
that PCFG (G, p?G) is proper and, for every com-
plete derivation d, p?G(d) = 1C ? pG(d), where C =?
S?d?w,w??? pG(d
?).
Note that if PCFG (G, pG) in the above lemma is
consistent, then C = 1 and (G, p?G) and (G, pG) de-
fine the same distribution on derivations. The nor-
malization procedure underlying Lemma 4 makes
use of quantities
?
A?dw,w??? pG(d) for each A ?
N . These quantities can be computed to any degree
of precision, as discussed for instance in (Booth and
Thompson, 1973) and (Stolcke, 1995). Thus nor-
malization of a PCFG can be effectively computed.
For a fixed PDT, we define the binary relation ;
on stack symbols by: Y ; Y ? if and only if
(Y,w, ?) `? (Y ?, ?, v) for some w ? ??? and
v ? ??? . In words, some subcomputation of the
PDT may start with stack Y and end with stack Y ?.
Note that all stacks that occur in such a subcompu-
tation must have height of 1 or more. We say that a
(P)PDA or a (P)PDT has the strong predictiveness
property (SPP) if the existence of three transitions
X 7? XY , XY1 7? Z1 and XY2 7? Z2 such that
Y ; Y1 and Y ; Y2 implies Z1 = Z2. Infor-
mally, this means that when a subcomputation starts
with some stack ? and some push transition ? , then
solely on the basis of ? we can uniquely determine
what stack symbol Z1 = Z2 will be on top of the
stack in the firstly reached configuration with stack
height equal to |?|. Another way of looking at it is
that no information may flow from higher stack el-
ements to lower stack elements that was not already
predicted before these higher stack elements came
into being, hence the term ?strong predictiveness?.
We say that a parsing strategy has the SPP if it maps
each reduced CFG to a PDT with the SPP.
Theorem 5 Any parsing strategy that has the CPP
and the SPP can be extended to become a proba-
bilistic parsing strategy.
Proof. Consider a parsing strategy S that has the
CPP and the SPP, and a proper, consistent and re-
duced PCFG (G, pG), G = (??, N, S, R). Let
S(G) = (A, f), A = (??, ??, Q, Xin, Xfin, ?).
We will show that there is a probability function pA
such that (A, pA) is a proper and consistent PPDT,
and pA(c) = pG(f(c)) for all complete computa-
tions c.
We first construct a PPDT (A, p?A) as follows.
For each scan transition ? = X x,y7? Y in ?, let
p?A(?) = pG(y) in case y ? R, and p?A(?) = 1
otherwise. For all remaining transitions ? ? ?, let
p?A(?) = 1. Note that (A, p?A) may be non-proper.
Still, from the definition of f it follows that, for each
complete computation c, we have
p?A(c) = pG(f(c)), (1)
and so our PPDT is consistent.
We now map (A, p?A) to a language-equivalent
PCFG (G?, pG?), G? = (??, Q,Xin, R?), where R?
contains the following rules with the specified asso-
ciated probabilities:
? X ? YZ with pG?(X ? YZ ) = p ?A(X 7?
XY ), for each X 7? XY ? ? with Z the
unique stack symbol such that there is at least
one transition XY ? 7? Z with Y ; Y ?;
? X ? xY with pG?(X ? xY ) = p ?A(X
x7?
Y ), for each transition X x7? Y ? ?;
? Y ? ? with pG?(X ? ?) = 1, for each stack
symbol Y such that there is at least one transi-
tion XY 7? Z ? ? or such that Y = Xfin.
It is not difficult to see that there exists a bijection
f ? from complete computations of A to complete
derivations of G?, and that we have
pG?(f
?(c)) = p?A(c), (2)
for each complete computation c. Thus (G?, pG?)
is consistent. However, note that (G?, pG?) is not
proper.
By Lemma 4, we can construct a new PCFG
(G?, p?G?) that is proper and consistent, and such that
pG?(d) = p?G?(d), for each complete derivation d of
G?. Thus, for each complete computation c ofA, we
have
p?G?(f
?(c)) = pG?(f
?(c)). (3)
We now transfer back the probabilities of rules of
(G?, p?G?) to the transitions ofA. Formally, we define
a new probability function pA such that, for each
? ? ?, pA(?) = p?G?(pi), where pi is the rule in R?
that has been constructed from ? as specified above.
It is easy to see that PPDT (A, pA) is now proper.
Furthermore, for each complete computation c ofA
we have
pA(c) = p
?
G?(f
?(c)), (4)
and so (A, pA) is also consistent. By combining
equations (1) to (4) we conclude that, for each com-
plete computation c of A, pA(c) = p?G?(f ?(c)) =
pG?(f ?(c)) = p?A(c) = pG(f(c)). Thus our parsing
strategy S can be probabilistically extended.
Note that the construction in the proof above can
be effectively computed (see discussion in Section 4
for effective computation of normalized PCFGs).
The definition of p?A in the proof of Theorem 5
relies on the strings output by A. This is the main
reason why we needed to consider PDTs rather
than PDAs. Now assume an appropriate probabil-
ity function pA has been computed, such that the
source PCFG and (A, pA) define equivalent dis-
tributions on derivations/computations. Then the
probabilities assigned to strings over the input al-
phabet are also equal. We may subsequently ignore
the output strings if the application at hand merely
requires probabilistic recognition rather than proba-
bilistic transduction, or in other words, we may sim-
plify PDTs to PDAs.
The proof of Theorem 5 also leads to the obser-
vation that parsing strategies with the CPP and the
SPP as well as their probabilistic extensions can be
described as grammar transformations, as follows.
A given (P)CFG is mapped to an equivalent (P)PDT
by a (probabilistic) parsing strategy. By ignoring
the output components of swap transitions we ob-
tain a (P)PDA, which can be mapped to an equiva-
lent (P)CFG as shown above. This observation gives
rise to an extension with probabilities of the work on
covers by (Nijholt, 1980; Leermakers, 1989).
6 Applications
Many well-known parsing strategies with the CPP
also have the SPP. This is for instance the case
for top-down parsing and left-corner parsing. As
discussed in the introduction, it has already been
shown that for any PCFG G, there are equiva-
lent PPDTs implementing these strategies, as re-
ported in (Abney et al, 1999) and (Tendeau, 1995),
respectively. Those results more simply follow
now from our general characterization. Further-
more, PLR parsing (Soisalon-Soininen and Ukko-
nen, 1979; Nederhof, 1994) can be expressed in our
framework as a parsing strategy with the CPP and
the SPP, and thus we obtain as a new result that this
strategy allows probabilistic extension.
The above strategies are in contrast to the LR
parsing strategy, which has the CPP but lacks the
SPP, and therefore falls outside our sufficient condi-
tion. As we have already seen in the introduction, it
turns out that LR parsing cannot be extended to be-
come a probabilistic parsing strategy. Related to LR
parsing is ELR parsing (Purdom and Brown, 1981;
Nederhof, 1994), which also lacks the SPP. By an
argument similar to the one provided for LR, we can
show that also ELR parsing cannot be extended to
become a probabilistic parsing strategy. (See (Ten-
deau, 1997) for earlier observations related to this.)
These two cases might suggest that the sufficient
condition in Theorem 5 is tight in practice.
Decidability of the CPP and the SPP obviously
depends on how a parsing strategy is specified. As
far as we know, in all practical cases of parsing
strategies these properties can be easily decided.
Also, observe that our results do not depend on the
general behaviour of a parsing strategy S, but just
on its ?point-wise? behaviour on each input CFG.
Specifically, if S does not have the CPP and the
SPP, but for some fixed CFG G of interest we ob-
tain a PDT A that has the CPP and the SPP, then
we can still apply the construction in Theorem 5.
In this way, any probability function pG associated
with G can be converted into a probability function
pA, such that the resulting PCFG and PPDT induce
equivalent distributions. We point out that decid-
ability of the CPP and the SPP for a fixed PDT can
be efficiently decided using dynamic programming.
One more consequence of our results is this. As
discussed in the introduction, the properness condi-
tion reduces the number of parameters of a PPDT.
However, our results show that if the PPDT has the
CPP and the SPP then the properness assumption is
not restrictive, i.e., by lifting properness we do not
gain new distributions with respect to those induced
by the underlying PCFG.
7 Conclusions
We have formalized the notion of CFG parsing strat-
egy as a mapping from CFGs to PDTs, and have in-
vestigated the extension to probabilities. We have
shown that the question of which parsing strategies
can be extended to become probabilistic heavily re-
lies on two properties, the correct-prefix property
and the strong predictiveness property. As far as we
know, this is the first general characterization that
has been provided in the literature for probabilistic
extension of CFG parsing strategies. We have also
shown that there is at least one strategy of practical
interest with the CPP but without the SPP, namely
LR parsing, that cannot be extended to become a
probabilistic parsing strategy.
Acknowledgements
The first author is supported by the PIO-
NIER Project Algorithms for Linguistic Process-
ing, funded by NWO (Dutch Organization for
Scientific Research). The second author is par-
tially supported by MIUR under project PRIN No.
2003091149 005.
References
S. Abney, D. McAllester, and F. Pereira. 1999. Re-
lating probabilistic grammars and automata. In
37th Annual Meeting of the Association for Com-
putational Linguistics, Proceedings of the Con-
ference, pages 542?549, Maryland, USA, June.
A.V. Aho and J.D. Ullman. 1972. Parsing, vol-
ume 1 of The Theory of Parsing, Translation and
Compiling. Prentice-Hall.
S. Billot and B. Lang. 1989. The structure of
shared forests in ambiguous parsing. In 27th
Annual Meeting of the Association for Com-
putational Linguistics, Proceedings of the Con-
ference, pages 143?151, Vancouver, British
Columbia, Canada, June.
T.L. Booth and R.A. Thompson. 1973. Apply-
ing probabilistic measures to abstract languages.
IEEE Transactions on Computers, C-22(5):442?
450, May.
T. Briscoe and J. Carroll. 1993. Generalized prob-
abilistic LR parsing of natural language (cor-
pora) with unification-based grammars. Compu-
tational Linguistics, 19(1):25?59.
E. Charniak and G. Carroll. 1994. Context-
sensitive statistics for improved grammatical lan-
guage models. In Proceedings Twelfth National
Conference on Artificial Intelligence, volume 1,
pages 728?733, Seattle, Washington.
Z. Chi and S. Geman. 1998. Estimation of prob-
abilistic context-free grammars. Computational
Linguistics, 24(2):299?305.
Z. Chi. 1999. Statistical properties of probabilistic
context-free grammars. Computational Linguis-
tics, 25(1):131?160.
M.V. Chitrao and R. Grishman. 1990. Statistical
parsing of messages. In Speech and Natural Lan-
guage, Proceedings, pages 263?266, Hidden Val-
ley, Pennsylvania, June.
M.A. Harrison. 1978. Introduction to Formal Lan-
guage Theory. Addison-Wesley.
K. Inui, V. Sornlertlamvanich, H. Tanaka, and
T. Tokunaga. 2000. Probabilistic GLR parsing.
In H. Bunt and A. Nijholt, editors, Advances
in Probabilistic and other Parsing Technologies,
chapter 5, pages 85?104. Kluwer Academic Pub-
lishers.
B. Lang. 1974. Deterministic techniques for ef-
ficient non-deterministic parsers. In Automata,
Languages and Programming, 2nd Colloquium,
volume 14 of Lecture Notes in Computer Science,
pages 255?269, Saarbru?cken. Springer-Verlag.
R. Leermakers. 1989. How to cover a grammar.
In 27th Annual Meeting of the Association for
Computational Linguistics, Proceedings of the
Conference, pages 135?142, Vancouver, British
Columbia, Canada, June.
C.D. Manning and B. Carpenter. 2000. Proba-
bilistic parsing using left corner language mod-
els. In H. Bunt and A. Nijholt, editors, Ad-
vances in Probabilistic and other Parsing Tech-
nologies, chapter 6, pages 105?124. Kluwer Aca-
demic Publishers.
M.-J. Nederhof and G. Satta. 2003. Probabilis-
tic parsing as intersection. In 8th International
Workshop on Parsing Technologies, pages 137?
148, LORIA, Nancy, France, April.
M.-J. Nederhof. 1994. An optimal tabular parsing
algorithm. In 32nd Annual Meeting of the Associ-
ation for Computational Linguistics, Proceedings
of the Conference, pages 117?124, Las Cruces,
New Mexico, USA, June.
A. Nijholt. 1980. Context-Free Grammars: Cov-
ers, Normal Forms, and Parsing, volume 93 of
Lecture Notes in Computer Science. Springer-
Verlag.
P.W. Purdom, Jr. and C.A. Brown. 1981. Pars-
ing extended LR(k) grammars. Acta Informatica,
15:115?127.
B. Roark and M. Johnson. 1999. Efficient proba-
bilistic top-down and left-corner parsing. In 37th
Annual Meeting of the Association for Compu-
tational Linguistics, Proceedings of the Confer-
ence, pages 421?428, Maryland, USA, June.
D.J. Rosenkrantz and P.M. Lewis II. 1970. Deter-
ministic left corner parsing. In IEEE Conference
Record of the 11th Annual Symposium on Switch-
ing and Automata Theory, pages 139?152.
J.-A. Sa?nchez and J.-M. Bened??. 1997. Consis-
tency of stochastic context-free grammars from
probabilistic estimation based on growth trans-
formations. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 19(9):1052?1055,
September.
E.S. Santos. 1972. Probabilistic grammars and au-
tomata. Information and Control, 21:27?47.
S. Sippu and E. Soisalon-Soininen. 1990. Parsing
Theory, Vol. II: LR(k) and LL(k) Parsing, vol-
ume 20 of EATCS Monographs on Theoretical
Computer Science. Springer-Verlag.
E. Soisalon-Soininen and E. Ukkonen. 1979. A
method for transforming grammars into LL(k)
form. Acta Informatica, 12:339?369.
V. Sornlertlamvanich, K. Inui, H. Tanaka, T. Toku-
naga, and T. Takezawa. 1999. Empirical sup-
port for new probabilistic generalized LR pars-
ing. Journal of Natural Language Processing,
6(3):3?22.
A. Stolcke. 1995. An efficient probabilistic
context-free parsing algorithm that computes
prefix probabilities. Computational Linguistics,
21(2):167?201.
F. Tendeau. 1995. Stochastic parse-tree recognition
by a pushdown automaton. In Fourth Interna-
tional Workshop on Parsing Technologies, pages
234?249, Prague and Karlovy Vary, Czech Re-
public, September.
F. Tendeau. 1997. Analyse syntaxique et
se?mantique avec e?valuation d?attributs dans
un demi-anneau. Ph.D. thesis, University of
Orle?ans.
J.H. Wright and E.N. Wrigley. 1991. GLR pars-
ing with probability. In M. Tomita, editor, Gen-
eralized LR Parsing, chapter 8, pages 113?128.
Kluwer Academic Publishers.
An alternative method of training probabilistic LR parsers
Mark-Jan Nederhof
Faculty of Arts
University of Groningen
P.O. Box 716
NL-9700 AS Groningen
The Netherlands
markjan@let.rug.nl
Giorgio Satta
Dept. of Information Engineering
University of Padua
via Gradenigo, 6/A
I-35131 Padova
Italy
satta@dei.unipd.it
Abstract
We discuss existing approaches to train LR parsers,
which have been used for statistical resolution of
structural ambiguity. These approaches are non-
optimal, in the sense that a collection of probability
distributions cannot be obtained. In particular, some
probability distributions expressible in terms of a
context-free grammar cannot be expressed in terms
of the LR parser constructed from that grammar,
under the restrictions of the existing approaches to
training of LR parsers. We present an alternative
way of training that is provably optimal, and that al-
lows all probability distributions expressible in the
context-free grammar to be carried over to the LR
parser. We also demonstrate empirically that this
kind of training can be effectively applied on a large
treebank.
1 Introduction
The LR parsing strategy was originally devised
for programming languages (Sippu and Soisalon-
Soininen, 1990), but has been used in a wide range
of other areas as well, such as for natural language
processing (Lavie and Tomita, 1993; Briscoe and
Carroll, 1993; Ruland, 2000). The main difference
between the application to programming languages
and the application to natural languages is that in
the latter case the parsers should be nondetermin-
istic, in order to deal with ambiguous context-free
grammars (CFGs). Nondeterminism can be han-
dled in a number of ways, but the most efficient
is tabulation, which allows processing in polyno-
mial time. Tabular LR parsing is known from the
work by (Tomita, 1986), but can also be achieved
by the generic tabulation technique due to (Lang,
1974; Billot and Lang, 1989), which assumes an in-
put pushdown transducer (PDT). In this context, the
LR parsing strategy can be seen as a particular map-
ping from context-free grammars to PDTs.
The acronym ?LR? stands for ?Left-to-right pro-
cessing of the input, producing a Right-most deriva-
tion (in reverse)?. When we construct a PDTA from
a CFG G by the LR parsing strategy and apply it on
an input sentence, then the set of output strings ofA
represents the set of all right-most derivations that G
allows for that sentence. Such an output string enu-
merates the rules (or labels that identify the rules
uniquely) that occur in the corresponding right-most
derivation, in reversed order.
If LR parsers do not use lookahead to decide be-
tween alternative transitions, they are called LR(0)
parsers. More generally, if LR parsers look ahead k
symbols, they are called LR(k) parsers; some sim-
plified LR parsing models that use lookahead are
called SLR(k) and LALR(k) parsing (Sippu and
Soisalon-Soininen, 1990). In order to simplify the
discussion, we abstain from using lookahead in this
article, and ?LR parsing? can further be read as
?LR(0) parsing?. We would like to point out how-
ever that our observations carry over to LR parsing
with lookahead.
The theory of probabilistic pushdown automata
(Santos, 1972) can be easily applied to LR parsing.
A probability is then assigned to each transition, by
a function that we will call the probability function
pA, and the probability of an accepting computa-
tion of A is the product of the probabilities of the
applied transitions. As each accepting computation
produces a right-most derivation as output string, a
probabilistic LR parser defines a probability distri-
bution on the set of parses, and thereby also a prob-
ability distribution on the set of sentences generated
by grammar G. Disambiguation of an ambiguous
sentence can be achieved on the basis of a compari-
son between the probabilities assigned to the respec-
tive parses by the probabilistic LR model.
The probability function can be obtained on the
basis of a treebank, as proposed by (Briscoe and
Carroll, 1993) (see also (Su et al, 1991)). The
model by (Briscoe and Carroll, 1993) however in-
corporated a mistake involving lookahead, which
was corrected by (Inui et al, 2000). As we will not
discuss lookahead here, this matter does not play a
significant role in the current study. Noteworthy is
that (Sornlertlamvanich et al, 1999) showed empir-
ically that an LR parser may be more accurate than
the original CFG, if both are trained on the basis
of the same treebank. In other words, the resulting
probability function pA on transitions of the PDT
allows better disambiguation than the correspond-
ing function pG on rules of the original grammar.
A plausible explanation of this is that stack sym-
bols of an LR parser encode some amount of left
context, i.e. information on rules applied earlier, so
that the probability function on transitions may en-
code dependencies between rules that cannot be en-
coded in terms of the original CFG extended with
rule probabilities. The explicit use of left con-
text in probabilistic context-free models was inves-
tigated by e.g. (Chitrao and Grishman, 1990; John-
son, 1998), who also demonstrated that this may
significantly improve accuracy. Note that the prob-
ability distributions of language may be beyond the
reach of a given context-free grammar, as pointed
out by e.g. (Collins, 2001). Therefore, the use of left
context, and the resulting increase in the number of
parameters of the model, may narrow the gap be-
tween the given grammar and ill-understood mech-
anisms underlying actual language.
One important assumption that is made by
(Briscoe and Carroll, 1993) and (Inui et al, 2000)
is that trained probabilistic LR parsers should be
proper, i.e. if several transitions are applicable for
a given stack, then the sum of probabilities as-
signed to those transitions by probability function
pA should be 1. This assumption may be moti-
vated by pragmatic considerations, as such a proper
model is easy to train by relative frequency estima-
tion: count the number of times a transition is ap-
plied with respect to a treebank, and divide it by
the number of times the relevant stack symbol (or
pair of stack symbols) occurs at the top of the stack.
Let us call the resulting probability function prfe .
This function is provably optimal in the sense that
the likelihood it assigns to the training corpus is
maximal among all probability functions pA that are
proper in the above sense.
However, properness restricts the space of prob-
ability distributions that a PDT allows. This means
that a (consistent) probability function pA may ex-
ist that is not proper and that assigns a higher like-
lihood to the training corpus than prfe does. (By
?consistent? we mean that the probabilities of all
strings that are accepted sum to 1.) It may even
be the case that a (proper and consistent) probabil-
ity function pG on the rules of the input grammar G
exists that assigns a higher likelihood to the corpus
than prfe , and therefore it is not guaranteed that LR
parsers allow better probability estimates than the
CFGs from which they were constructed, if we con-
strain probability functions pA to be proper. In this
respect, LR parsing differs from at least one other
well-known parsing strategy, viz. left-corner pars-
ing. See (Nederhof and Satta, 2004) for a discus-
sion of a property that is shared by left-corner pars-
ing but not by LR parsing, and which explains the
above difference.
As main contribution of this paper we establish
that this restriction on expressible probability dis-
tributions can be dispensed with, without losing the
ability to perform training by relative frequency es-
timation. What comes in place of properness is
reverse-properness, which can be seen as proper-
ness of the reversed pushdown automaton that pro-
cesses input from right to left instead of from left to
right, interpreting the transitions of A backwards.
As we will show, reverse-properness does not re-
strict the space of probability distributions express-
ible by an LR automaton. More precisely, assume
some probability distribution on the set of deriva-
tions is specified by a probability function pA on
transitions of PDT A that realizes the LR strat-
egy for a given grammar G. Then the same prob-
ability distribution can be specified by an alterna-
tive such function p?A that is reverse-proper. In ad-
dition, for each probability distribution on deriva-
tions expressible by a probability function pG for G,
there is a reverse-proper probability function pA for
A that expresses the same probability distribution.
Thereby we ensure that LR parsers become at least
as powerful as the original CFGs in terms of allow-
able probability distributions.
This article is organized as follows. In Sec-
tion 2 we outline our formalization of LR pars-
ing as a construction of PDTs from CFGs, making
some superficial changes with respect to standard
formulations. Properness and reverse-properness
are discussed in Section 3, where we will show
that reverse-properness does not restrict the space
of probability distributions. Section 4 reports on ex-
periments, and Section 5 concludes this article.
2 LR parsing
As LR parsing has been extensively treated in exist-
ing literature, we merely recapitulate the main defi-
nitions here. For more explanation, the reader is re-
ferred to standard literature such as (Harrison, 1978;
Sippu and Soisalon-Soininen, 1990).
An LR parser is constructed on the basis of a CFG
that is augmented with an additional rule S? ?` S,
where S is the former start symbol, and the new
nonterminal S? becomes the start symbol of the
augmented grammar. The new terminal ` acts as
an imaginary start-of-sentence marker. We denote
the set of terminals by ? and the set of nontermi-
nals by N . We assume each rule has a unique label
r.
As explained before, we construct LR parsers as
pushdown transducers. The main stack symbols
of these automata are sets of dotted rules, which
consist of rules from the augmented grammar with
a distinguished position in the right-hand side in-
dicated by a dot ???. The initial stack symbol is
pinit = {S? ? ` ? S}.
We define the closure of a set p of dotted rules as
the smallest set closure(p) such that:
1. p ? closure(p); and
2. for (B ? ? ? A?) ? closure(p) and A ?
? a rule in the grammar, also (A ? ? ?) ?
closure(p).
We define the operation goto on a set p of dotted
rules and a grammar symbol X ? ? ?N as:
goto(p,X) = {A? ?X ? ? |
(A? ? ? X?) ? closure(p)}
The set of LR states is the smallest set such that:
1. pinit is an LR state; and
2. if p is an LR state and goto(p,X) = q 6= ?, for
some X ? ? ?N , then q is an LR state.
We will assume that PDTs consist of three types
of transitions, of the form P a,b7? P Q (a push tran-
sition), of the form P a,b7? Q (a swap transition), and
of the form P Q a,b7? R (a pop transition). Here P , Q
and R are stack symbols, a is one input terminal or
is the empty string ?, and b is one output terminal or
is the empty string ?. In our notation, stacks grow
from left to right, so that P a,b7? P Q means that Q is
pushed on top of P . We do not have internal states
next to stack symbols.
For the PDT that implements the LR strategy, the
stack symbols are the LR states, plus symbols of the
form [p;X], where p is an LR state andX is a gram-
mar symbol, and symbols of the form (p,A,m),
where p is an LR state, A is the left-hand side of
some rule, and m is the length of some prefix of the
right-hand side of that rule. More explanation on
these additional stack symbols will be given below.
The stack symbols and transitions are simultane-
ously defined in Figure 1. The final stack symbol
is pfinal = (pinit , S?, 0). This means that an input
a1 ? ? ? an is accepted if and only if it is entirely read
by a sequence of transitions that take the stack con-
sisting only of pinit to the stack consisting only of
pfinal . The computed output consists of the string of
terminals b1 ? ? ? bn? from the output components of
the applied transitions. For the PDTs that we will
use, this output string will consist of a sequence of
rule labels expressing a right-most derivation of the
input. On the basis of the original grammar, the cor-
responding parse tree can be constructed from such
an output string.
There are a few superficial differences with LR
parsing as it is commonly found in the literature.
The most obvious difference is that we divide re-
ductions into ?binary? steps. The main reason is that
this allows tabular interpretation with a time com-
plexity cubic in the length of the input. Otherwise,
the time complexity would be O(nm+1), where m
is the length of the longest right-hand side of a rule
in the CFG. This observation was made before by
(Kipps, 1991), who proposed a solution similar to
ours, albeit formulated differently. See also a related
formulation of tabular LR parsing by (Nederhof and
Satta, 1996).
To be more specific, instead of one step of the
PDT taking stack:
?p0p1 ? ? ? pm
immediately to stack:
?p0q
where (A ? X1 ? ? ?Xm ?) ? pm, ? is a string
of stack symbols and goto(p0, A) = q, we have
a number of smaller steps leading to a series of
stacks:
?p0p1 ? ? ? pm?1pm
?p0p1 ? ? ? pm?1(A,m?1)
?p0p1 ? ? ? (A,m?2)
.
.
.
?p0(A, 0)
?p0q
There are two additional differences. First, we
want to avoid steps of the form:
?p0(A, 0)
?p0q
by transitions p0 (A, 0)
?,?
7? p0 q, as such transitions
complicate the generic definition of ?properness?
for PDTs, to be discussed in the following section.
For this reason, we use stack symbols of the form
[p;X] next to p, and split up p0 (A, 0)
?,?
7? p0 q into
pop [p0;X0] (A, 0)
?,?
7? [p0;A] and push [p0;A]
?,?
7?
[p0;A] q. This is a harmless modification, which in-
creases the number of steps in any computation by
at most a factor 2.
Secondly, we use stack symbols of the form
(p,A,m) instead of (A,m). This concerns the con-
ditions of reverse-properness to be discussed in the
? For LR state p and a ? ? such that goto(p, a) 6= ?:
p
a,?
7? [p; a] (1)
? For LR state p and (A? ?) ? p, where A? ? has label r:
p
?,r
7? [p;A] (2)
? For LR state p and (A? ? ?) ? p, where |?| = m > 0 and A? ? has label r:
p
?,r
7? (p,A,m? 1) (3)
? For LR state p and (A? ? ? X?) ? p, where |?| = m > 0, such that goto(p,X) = q 6= ?:
[p;X] (q, A,m)
?,?
7? (p,A,m? 1) (4)
? For LR state p and (A? ? X?) ? p, such that goto(p,X) = q 6= ?:
[p;X] (q, A, 0)
?,?
7? [p;A] (5)
? For LR state p and X ? ? ?N such that goto(p,X) = q 6= ?:
[p;X]
?,?
7? [p;X] q (6)
Figure 1: The transitions of a PDT implementing LR(0) parsing.
following section. By this condition, we consider
LR parsing as being performed from right to left, so
backwards with regard to the normal processing or-
der. If we were to omit the first components p from
stack symbols (p,A,m), we may obtain ?dead ends?
in the computation. We know that such dead ends
make a (reverse-)proper PDT inconsistent, as proba-
bility mass lost in dead ends causes the sum of prob-
abilities of all computations to be strictly smaller
than 1. (See also (Nederhof and Satta, 2004).) It
is interesting to note that the addition of the compo-
nents p to stack symbols (p,A,m) does not increase
the number of transitions, and the nature of LR pars-
ing in the normal processing order from left to right
is preserved.
With all these changes together, reductions
are implemented by transitions resulting in the
following sequence of stacks:
??[p0;X0][p1;X1] ? ? ? [pm?1;Xm?1]pm
??[p0;X0][p1;X1] ? ? ? [pm?1;Xm?1](pm, A,m?1)
??[p0;X0][p1;X1] ? ? ? (pm?1, A,m?2)
.
.
.
??[p0;X0](p1, A, 0)
??[p0;A]
??[p0;A]q
Please note that transitions of the form
[p;X] (q, A,m)
?,?
7? (p,A,m? 1) may corre-
spond to several dotted rules (A ? ? ? X?) ? p,
with different ? of length m and different ?. If we
were to multiply such transitions for different ? and
?, the PDT would become prohibitively large.
3 Properness and reverse-properness
If a PDT is regarded to process input from left to
right, starting with a stack consisting only of pinit ,
and ending in a stack consisting only of pfinal , then
it seems reasonable to cast this process into a prob-
abilistic framework in such a way that the sum of
probabilities of all choices that are possible at any
given moment is 1. This is similar to how the notion
of ?properness? is defined for probabilistic context-
free grammars (PCFGs); we say a PCFG is proper if
for each nonterminalA, the probabilities of all rules
with left-hand side A sum to 1.
Properness for PCFGs does not restrict the space
of probability distributions on the set of parse trees.
In other words, if a probability distribution can be
defined by attaching probabilities to rules, then we
may reassign the probabilities such that that PCFG
becomes proper, while preserving the probability
distribution. This even holds if the input grammar
is non-tight, meaning that probability mass is lost
in ?infinite derivations? (Sa?nchez and Bened??, 1997;
Chi and Geman, 1998; Chi, 1999; Nederhof and
Satta, 2003).
Although CFGs and PDTs are weakly equiva-
lent, they behave very differently when they are ex-
tended with probabilities. In particular, there seems
to be no notion similar to PCFG properness that
can be imposed on all types of PDTs without los-
ing generality. Below we will discuss two con-
straints, which we will call properness and reverse-
properness. Neither of these is suitable for all types
of PDTs, but as we will show, the second is more
suitable for probabilistic LR parsing than the first.
This is surprising, as only properness has been de-
scribed in existing literature on probabilistic PDTs
(PPDTs). In particular, all existing approaches to
probabilistic LR parsing have assumed properness
rather than anything related to reverse-properness.
For properness we have to assume that for each
stack symbol P , we either have one or more tran-
sitions of the form P a,b7? P Q or P a,b7? Q, or one
or more transitions of the form Q P a,b7? R, but no
combination thereof. In the first case, properness
demands that the sum of probabilities of all transi-
tions P a,b7? P Q and P a,b7? Q is 1, and in the second
case properness demands that the sum of probabili-
ties of all transitions Q P a,b7? R is 1 for each Q.
Note that our assumption above is without loss
of generality, as we may introduce swap transitions
P
?,?
7? P1 and P
?,?
7? P2, where P1 and P2 are new
stack symbols, and replace transitions P a,b7? P Q
and P a,b7? Q by P1
a,b
7? P1 Q and P1
a,b
7? Q, and
replace transitions Q P a,b7? R by Q P2
a,b
7? R.
The notion of properness underlies the normal
training process for PDTs, as follows. We assume
a corpus of PDT computations. In these computa-
tions, we count the number of occurrences for each
transition. For each P we sum the total number of
all occurrences of transitions P a,b7? P Q or P a,b7? Q.
The probability of, say, a transition P a,b7? P Q is
now estimated by dividing the number of occur-
rences thereof in the corpus by the above total num-
ber of occurrences of transitions with P in the left-
hand side. Similarly, for each pair (Q,P ) we sum
the total number of occurrences of all transitions of
the formQ P a,b7? R, and thereby estimate the proba-
bility of a particular transitionQ P a,b7? R by relative
frequency estimation. The resulting PPDT is proper.
It has been shown that imposing properness is
without loss of generality in the case of PDTs
constructed by a wide range of parsing strategies,
among which are top-down parsing and left-corner
parsing. This does not hold for PDTs constructed by
the LR parsing strategy however, and in fact, proper-
ness for such automata may reduce the expressive
power in terms of available probability distributions
to strictly less than that offered by the original CFG.
This was formally proven by (Nederhof and Satta,
2004), after (Ng and Tomita, 1991) and (Wright and
Wrigley, 1991) had already suggested that creating
a probabilistic LR parser that is equivalent to an in-
put PCFG is difficult in general. The same difficulty
for ELR parsing was suggested by (Tendeau, 1997).
For this reason, we investigate a practical alter-
native, viz. reverse-properness. Now we have to as-
sume that for each stack symbol R, we either have
one or more transitions of the form P a,b7? R or
Q P
a,b
7? R, or one or more transitions of the form
P
a,b
7? P R, but no combination thereof. In the first
case, reverse-properness demands that the sum of
probabilities of all transitions P a,b7? R or Q P a,b7? R
is 1, and in the second case reverse-properness de-
mands that the sum of probabilities of transitions
P
a,b
7? P R is 1 for each P . Again, our assumption
above is without loss of generality.
In order to apply relative frequency estimation,
we now sum the total number of occurrences of tran-
sitions P a,b7? R or Q P a,b7? R for each R, and we
sum the total number of occurrences of transitions
P
a,b
7? P R for each pair (P,R).
We now prove that reverse-properness does not
restrict the space of probability distributions, by
means of the construction of a ?cover? grammar
from an input CFG, as reported in Figure 2. This
cover CFG has almost the same structure as the PDT
resulting from Figure 1. Rules and transitions al-
most stand in a one-to-one relation. The only note-
worthy difference is between transitions of type (6)
and rules of type (12). The right-hand sides of those
rules can be ? because the corresponding transitions
are deterministic if seen from right to left. Now it
becomes clear why we needed the components p in
stack symbols of the form (p,A,m). Without it, one
could obtain an LR state q that does not match the
underlying [p;X] in a reversed computation.
We may assume without loss of generality that
rules of type (12) are assigned probability 1, as a
probability other than 1 could be moved to corre-
sponding rules of types (10) or (11) where state
q was introduced. In the same way, we may as-
sume that transitions of type (6) are assigned prob-
ability 1. After making these assumptions, we ob-
tain a bijection between probability functions pA for
the PDT and probability functions pG for the cover
CFG. As was shown by e.g. (Chi, 1999) and (Neder-
hof and Satta, 2003), properness for CFGs does not
restrict the space of probability distributions, and
thereby the same holds for reverse-properness for
PDTs that implement the LR parsing strategy.
It is now also clear that a reverse-proper LR
parser can describe any probability distribution that
the original CFG can. The proof is as follows.
Given a probability function pG for the input CFG,
we define a probability function pA for the LR
parser, by letting transitions of types (2) and (3)
? For LR state p and a ? ? such that goto(p, a) 6= ?:
[p; a]? p (7)
? For LR state p and (A? ?) ? p, where A? ? has label r:
[p;A]? p r (8)
? For LR state p and (A? ? ?) ? p, where |?| = m > 0 and A? ? has label r:
(p,A,m? 1)? p r (9)
? For LR state p and (A? ? ? X?) ? p, where |?| = m > 0, such that goto(p,X) = q 6= ?:
(p,A,m? 1)? [p;X] (q, A,m) (10)
? For LR state p and (A? ? X?) ? p, such that goto(p,X) = q 6= ?:
[p;A]? [p;X] (q, A, 0) (11)
? For LR state q:
q ? ? (12)
Figure 2: A grammar that describes the set of computations of the LR(0) parser. Start symbol is pfinal =
(pinit , S?, 0). Terminals are rule labels. Generated language consists of right-most derivations in reverse.
have probability pG(r), and letting all other transi-
tions have probability 1. This gives us the required
probability distribution in terms of a PPDT that is
not reverse-proper in general. This PPDT can now
be recast into reverse-proper form, as proven by the
above.
4 Experiments
We have implemented both the traditional training
method for LR parsing and the novel one, and have
compared their performance, with two concrete ob-
jectives:
1. We show that the number of free parameters
is significantly larger with the new training
method. (The number of free parameters is
the number of probabilities of transitions that
can be freely chosen within the constraints of
properness or reverse-properness.)
2. The larger number of free parameters does not
make the problem of sparse data any worse,
and precision and recall are at least compara-
ble to, if not better than, what we would obtain
with the established method.
The experiments were performed on the Wall
Street Journal (WSJ) corpus, from the Penn Tree-
bank, version II. Training was done on sections 02-
21, i.e., first a context-free grammar was derived
from the ?stubs? of the combined trees, taking parts
of speech as leaves of the trees, omitting all af-
fixes from the nonterminal names, and removing ?-
generating subtrees. Such preprocessing of the WSJ
corpus is consistent with earlier attempts to derive
CFGs from that corpus, as e.g. by (Johnson, 1998).
The obtained CFG has 10,035 rules. The dimen-
sions of the LR parser constructed from this gram-
mar are given in Table 1.
The PDT was then trained on the trees from the
same sections 02-21, to determine the number of
times that transitions are used. At first sight it is not
clear how to determine this on the basis of the tree-
bank, as the structure of LR parsers is very differ-
ent from the structure of the grammars from which
they are constructed. The solution is to construct a
second PDT from the PDT to be trained, replacing
each transition ? a,b7? ? with label r by transition
?
b,r
7? ?. By this second PDT we parse the tree-
bank, encoded as a series of right-most derivations
in reverse.1 For each input string, there is exactly
one parse, of which the output is the list of used
transitions. The same method can be used for other
parsing strategies as well, such as left-corner pars-
ing, replacing right-most derivations by a suitable
alternative representation of parse trees.
By the counts of occurrences of transitions, we
may then perform maximum likelihood estimation
to obtain probabilities for transitions. This can
be done under the constraints of properness or of
reverse-properness, as explained in the previous
section. We have not applied any form of smooth-
1We have observed an enormous gain in computational ef-
ficiency when we also incorporate the ?shifts? next to ?reduc-
tions? in these right-most derivations, as this eliminates a con-
siderable amount of nondeterminism.
total # transitions 8,340,315
# push transitions 753,224
# swap transitions 589,811
# pop transitions 6,997,280
Table 1: Dimensions of PDT implementing LR
strategy for CFG derived from WSJ, sect. 02-21.
proper rev.-prop.
# free parameters 577,650 6,589,716
# non-zero probabilities 137,134 137,134
labelled precision 0.772 0.777
labelled recall 0.747 0.749
Table 2: The two methods of training, based on
properness and reverse-properness.
ing or back-off, as this could obscure properties in-
herent in the difference between the two discussed
training methods. (Back-off for probabilistic LR
parsing has been proposed by (Ruland, 2000).) All
transitions that were not seen during training were
given probability 0.
The results are outlined in Table 2. Note that the
number of free parameters in the case of reverse-
properness is much larger than in the case of normal
properness. Despite of this, the number of transi-
tions that actually receive non-zero probabilities is
(predictably) identical in both cases, viz. 137,134.
However, the potential for fine-grained probability
estimates and for smoothing and parameter-tying
techniques is clearly greater in the case of reverse-
properness.
That in both cases the number of non-zero prob-
abilities is lower than the total number of parame-
ters can be explained as follows. First, the treebank
contains many rules that occur a small number of
times. Secondly, the LR automaton is much larger
than the CFG; in general, the size of an LR automa-
ton is bounded by a function that is exponential in
the size of the input CFG. Therefore, if we use the
same treebank to estimate the probability function,
then many transitions are never visited and obtain a
zero probability.
We have applied the two trained LR automata
on section 22 of the WSJ corpus, measuring la-
belled precision and recall, as done by e.g. (John-
son, 1998).2 We observe that in the case of reverse-
properness, precision and recall are slightly better.
2We excluded all sentences with more than 30 words how-
ever, as some required prohibitive amounts of memory. Only
one of the remaining 1441 sentences was not accepted by the
parser.
The most important conclusion that can be drawn
from this is that the substantially larger space of
obtainable probability distributions offered by the
reverse-properness method does not come at the ex-
pense of a degradation of accuracy for large gram-
mars such as those derived from the WSJ. For com-
parison, with a standard PCFG we obtain labelled
precision and recall of 0.725 and 0.670, respec-
tively.3
We would like to stress that our experiments
did not have as main objective the improvement of
state-of-the-art parsers, which can certainly not be
done without much additional fine-tuning and the
incorporation of some form of lexicalization. Our
main objectives concerned the relation between our
newly proposed training method for LR parsers and
the traditional one.
5 Conclusions
We have presented a novel way of assigning proba-
bilities to transitions of an LR automaton. Theoreti-
cal analysis and empirical data reveal the following.
? The efficiency of LR parsing remains unaf-
fected. Although a right-to-left order of read-
ing input underlies the novel training method,
we may continue to apply the parser from left
to right, and benefit from the favourable com-
putational properties of LR parsing.
? The available space of probability distributions
is significantly larger than in the case of the
methods published before. In terms of the
number of free parameters, the difference that
we found empirically exceeds one order of
magnitude. By the same criteria, we can now
guarantee that LR parsers are at least as pow-
erful as the CFGs from which they are con-
structed.
? Despite the larger number of free parameters,
no increase of sparse data problems was ob-
served, and in fact there was a small increase
in accuracy.
Acknowledgements
Helpful comments from John Carroll and anony-
mous reviewers are gratefully acknowledged. The
first author is supported by the PIONIER Project
Algorithms for Linguistic Processing, funded by
NWO (Dutch Organization for Scientific Research).
The second author is partially supported by MIUR
under project PRIN No. 2003091149 005.
3In this case, all 1441 sentences were accepted.
References
S. Billot and B. Lang. 1989. The structure of shared
forests in ambiguous parsing. In 27th Annual
Meeting of the Association for Computational
Linguistics, pages 143?151, Vancouver, British
Columbia, Canada, June.
T. Briscoe and J. Carroll. 1993. Generalized prob-
abilistic LR parsing of natural language (cor-
pora) with unification-based grammars. Compu-
tational Linguistics, 19(1):25?59.
Z. Chi and S. Geman. 1998. Estimation of prob-
abilistic context-free grammars. Computational
Linguistics, 24(2):299?305.
Z. Chi. 1999. Statistical properties of probabilistic
context-free grammars. Computational Linguis-
tics, 25(1):131?160.
M.V. Chitrao and R. Grishman. 1990. Statistical
parsing of messages. In Speech and Natural Lan-
guage, Proceedings, pages 263?266, Hidden Val-
ley, Pennsylvania, June.
M. Collins. 2001. Parameter estimation for sta-
tistical parsing models: Theory and practice of
distribution-free methods. In Proceedings of the
Seventh International Workshop on Parsing Tech-
nologies, Beijing, China, October.
M.A. Harrison. 1978. Introduction to Formal Lan-
guage Theory. Addison-Wesley.
K. Inui, V. Sornlertlamvanich, H. Tanaka, and
T. Tokunaga. 2000. Probabilistic GLR parsing.
In H. Bunt and A. Nijholt, editors, Advances
in Probabilistic and other Parsing Technologies,
chapter 5, pages 85?104. Kluwer Academic Pub-
lishers.
M. Johnson. 1998. PCFG models of linguistic
tree representations. Computational Linguistics,
24(4):613?632.
J.R. Kipps. 1991. GLR parsing in time O(n3). In
M. Tomita, editor, Generalized LR Parsing, chap-
ter 4, pages 43?59. Kluwer Academic Publishers.
B. Lang. 1974. Deterministic techniques for ef-
ficient non-deterministic parsers. In Automata,
Languages and Programming, 2nd Colloquium,
volume 14 of Lecture Notes in Computer Science,
pages 255?269, Saarbru?cken. Springer-Verlag.
A. Lavie and M. Tomita. 1993. GLR? ? an efficient
noise-skipping parsing algorithm for context free
grammars. In Third International Workshop on
Parsing Technologies, pages 123?134, Tilburg
(The Netherlands) and Durbuy (Belgium), Au-
gust.
M.-J. Nederhof and G. Satta. 1996. Efficient tab-
ular LR parsing. In 34th Annual Meeting of the
Association for Computational Linguistics, pages
239?246, Santa Cruz, California, USA, June.
M.-J. Nederhof and G. Satta. 2003. Probabilis-
tic parsing as intersection. In 8th International
Workshop on Parsing Technologies, pages 137?
148, LORIA, Nancy, France, April.
M.-J. Nederhof and G. Satta. 2004. Probabilis-
tic parsing strategies. In 42nd Annual Meeting
of the Association for Computational Linguistics,
Barcelona, Spain, July.
S.-K. Ng and M. Tomita. 1991. Probabilistic LR
parsing for general context-free grammars. In
Proc. of the Second International Workshop on
Parsing Technologies, pages 154?163, Cancun,
Mexico, February.
T. Ruland. 2000. A context-sensitive model for
probabilistic LR parsing of spoken language
with transformation-based postprocessing. In
The 18th International Conference on Compu-
tational Linguistics, volume 2, pages 677?683,
Saarbru?cken, Germany, July?August.
J.-A. Sa?nchez and J.-M. Bened??. 1997. Consis-
tency of stochastic context-free grammars from
probabilistic estimation based on growth trans-
formations. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 19(9):1052?1055,
September.
E.S. Santos. 1972. Probabilistic grammars and au-
tomata. Information and Control, 21:27?47.
S. Sippu and E. Soisalon-Soininen. 1990. Parsing
Theory, Vol. II: LR(k) and LL(k) Parsing, vol-
ume 20 of EATCS Monographs on Theoretical
Computer Science. Springer-Verlag.
V. Sornlertlamvanich, K. Inui, H. Tanaka, T. Toku-
naga, and T. Takezawa. 1999. Empirical sup-
port for new probabilistic generalized LR pars-
ing. Journal of Natural Language Processing,
6(3):3?22.
K.-Y. Su, J.-N. Wang, M.-H. Su, and J.-S. Chang.
1991. GLR parsing with scoring. In M. Tomita,
editor, Generalized LR Parsing, chapter 7, pages
93?112. Kluwer Academic Publishers.
F. Tendeau. 1997. Analyse syntaxique et
se?mantique avec e?valuation d?attributs dans
un demi-anneau. Ph.D. thesis, University of
Orle?ans.
M. Tomita. 1986. Efficient Parsing for Natural
Language. Kluwer Academic Publishers.
J.H. Wright and E.N. Wrigley. 1991. GLR pars-
ing with probability. In M. Tomita, editor, Gen-
eralized LR Parsing, chapter 8, pages 113?128.
Kluwer Academic Publishers.
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 13?24,
Paris, October 2009. c?2009 Association for Computational Linguistics
Weighted parsing of trees
Mark-Jan Nederhof
School of Computer Science, University of St Andrews
North Haugh, St Andrews, KY16 9SX, Scotland
Abstract
We show how parsing of trees can be for-
malized in terms of the intersection of two
tree languages. The focus is on weighted
regular tree grammars and weighted tree
adjoining grammars. Potential applica-
tions are discussed, such as parameter es-
timation across formalisms.
1 Introduction
In parsing theory, strings and trees traditionally
have had a very different status. Whereas strings
in general receive the central focus, the trees in-
volved in derivations of strings are often seen as
auxiliary concepts at best. Theorems tend to be
about the power of grammatical formalisms to
produce strings (weak generative power) rather
than trees (strong generative power).
This can be explained by looking at typical
applications of parsing. In compiler construc-
tion for example, one distinguishes between parse
trees and (abstract) syntax trees, the former being
shaped according to a grammar that is massaged
to make it satisfy relatively artificial constraints,
e.g. that of LALR(1), which is required by many
compiler generators (Aho et al, 2007). The form
of syntax trees is often chosen to simplify phases
of semantic processing that follow parsing. As
the machinery used in such processing is generally
powerful, this offers much flexibility in the choice
of the exact shape and labelling of syntax trees, as
intermediate form between parsing and semantic
analysis.
In the study of natural languages, parse trees
have played a more important role. Whereas lin-
guistic utterances are directly observable and trees
deriving them are not, there are nevertheless tradi-
tions within linguistics that would see one struc-
tural analysis of a sentence as strongly preferred
over another. Furthermore, within computational
linguistics there are empirical arguments to claim
certain parses are correct and others are incorrect.
For example, a question answering systems may
verifiably give the wrong answer if the question
is parsed incorrectly. See (Jurafsky and Martin,
2000) for general discussion on the role of parsing
in NLP.
Despite the relative importance of strong gen-
erative power in computational linguistics, there
is still much freedom in how exactly parse trees
are shaped and how vertices are labelled, due to
the power of semantic analysis that typically fol-
lows parsing. This has affected much of the the-
oretical investigations into the power of linguistic
formalisms, and where strong equivalence is con-
sidered at all, it is often ?modulo relabelling? or
allowing minor structural changes.
With the advent of syntax-based machine trans-
lation, trees have however gained much impor-
tance, and are even considered as the main ob-
jects of study. This is because many MT mod-
ules have trees both as input and output, which
means the computational strength of such mod-
ules can be measured only in terms of the tree lan-
guages they accept and the transductions between
tree languages that they implement. See for exam-
ple (Knight, 2007).
In contrast, trees have always been the central
issue in an important and well-established subfield
of formal language theory that studies tree lan-
guages, tree automata and tree transducers (Gc-
seg and Steinby, 1997). The string languages gen-
erated by the relevant formalisms in this context
are mostly taken to be of secondary importance, if
they are considered at all.
This paper focuses on tree languages, but in-
volves a technique that was devised for string lan-
guages, and shows how the technique carries over
to tree languages. The original technique can be
seen as the most fundamental idea in the field of
context-free parsing, as it captures the essence of
13
finding hierarchical structure in a linear sequence.
The generalization also finds structure in a lin-
ear sequence, but now the sequence corresponds
to paths in trees each leading down from a vertex
to a leaf. This means that the proposed type of
parsing is orthogonal to the conventional parsing
of strings.
The insights this offers have the potential to cre-
ate new avenues of research into the relation be-
tween formalisms that were until now considered
only in isolation. We seek credence to this claim
by investigating how probability distributions can
be carried over from tree adjoining grammars to
regular tree grammars, and vice versa.
The implication that the class of tree languages
of tree adjoining grammars (TAGs) is closed under
intersection with regular tree languages is not sur-
prising, as the linear context-free tree languages
(LCFTLs) are closed under intersection with reg-
ular tree languages (Kepser and Mo?nnich, 2006).
The tree languages of TAGs form a subclass of the
LCFTLs, and the main construction in the proof
of the closure result for the latter can be suitably
restricted to the former.
The structure of this paper is as follows. The
main grammatical formalisms considered in this
paper are summarized in Section 2 and Sec-
tion 3 discusses a number of analyses of these for-
malisms that will be used in later sections. Sec-
tion 4 starts by explaining how parsing of a string
can be seen as the construction of a grammar that
generates the intersection of two languages, and
then moves on to a type of parsing involving in-
tersection of tree languages in place of string lan-
guages.
In order to illustrate the implications of the the-
ory, we consider how it can be used to solve a prac-
tical problem, in Section 5. A number of possible
extensions are outlined in Section 6.
2 Formalisms
In this section, we recall the formalisms of
weighted regular tree grammars and weighted tree
adjoining grammars. We use similar notation and
terminology for both, in order to prepare for Sec-
tion 4, where we investigate the combination of
these formalisms through intersection. As a conse-
quence of the required unified notation, we deviate
to some degree from standard definitions, without
affecting generative power however.
For common definitions of weighted regular
tree grammars, the reader is referred to (Graehl
and Knight, 2004). Weighted tree adjoining gram-
mars are a straightforward generalization of prob-
abilistic (or stochastic) tree adjoining grammars,
as introduced by (Resnik, 1992) and (Schabes,
1992).
For both regular tree grammars (RTGs) and tree
adjoining grammars (TAGs), we will write a la-
beled and ordered tree as A(?). where A is the la-
bel of the root node, and ? is a sequence of expres-
sions of the same form that each represent an im-
mediate subtree. In our presentation, labels do not
have explicit ranks, that is, the number of children
of a node is not determined by its label. This al-
lows an interesting generalization, to be discussed
in Section 6.2.
Where we are interested in the string language
generated by a tree-generating grammar, we may
distinguish between two kinds of labels, the ter-
minal labels, which may occur only at leaves, and
the nonterminal labels, which may occur at any
node. It is customary to write terminal leaves as
a instead of a(). The yield of a tree is the string
of occurrences of terminal labels in it, from left to
right. Note that also nonterminal labels may occur
at the leaves, but they will not be included in the
yield; cf. epsilon rules in context-free grammars.
2.1 Weighted regular tree grammars
A weighted regular tree grammar (WRTG) is a 4-
tuple G = (S,L,R, s`), where S and L are two
finite sets of states and labels, respectively, s` ? S
is the initial state, and R is a finite set of rules.
Each rule has the form:
s0 ? A(s1 ? ? ? sm) ?w?,
where s0, s1, . . . , sm are states (0 ? m), A is a
label and w is a weight.
Rewriting starts with a string containing only
the initial state s`. This string is repeatedly rewrit-
ten by replacing the left-hand side state of a rule by
the right-hand side of the same rule, until no state
remains. It may be convenient to assume a canoni-
cal order of rewriting, for example in terms of left-
most derivations (Hopcroft and Ullman, 1979).
Although alternative semirings can be consid-
ered, here we always assume that the weights
of rules are non-negative real numbers, and the
weight of a derivation of a tree is the product of
the weights of the rule occurrences. If several
(left-most) derivations result in the same tree, then
14
the weight of that tree is given by the sum of the
weights of those derivations. Where we are inter-
ested in the string language, the weights of trees
with the same yield are added to obtain the weight
of that yield.
A (weighted) context-free grammar can be seen
as a special case of a (weighted) regular tree gram-
mar, where the set of states equals the set of labels,
and rules have the form:
A ? A(B1 ? ? ?Bm).
Also the class of (weighted) tree substitution
grammars (Sima?an, 1997) can be seen as a spe-
cial case of (weighted) regular tree grammars, by
letting the set of labels overlap with the set of
states, and imposing two constraints on the allow-
able rules. The first constraint is that for each la-
bel that is also a state, all defining rules are of the
form:
A ? A(s1 ? ? ? sm).
The second constraint is that for each state that is
not a label, there is exactly one rule with that state
in the left-hand side. This means that exactly one
subtree (or elementary tree) can be built top-down
out of such states, down to a level where we again
encounter states that are also labels. If desired, we
can exclude infinite elementary trees by imposing
an additional constraint on allowed sets of rules
(no cycles composed of states that are not labels);
alternatively, we can demand that the grammar
does not contain any useless rules, which automat-
ically excludes such infinite elementary trees.
2.2 Weighted linear indexed grammars
Although we are mainly interested in the tree lan-
guages of tree adjoining grammars, we will use
an equivalent representation in terms of linear in-
dexed grammars, in order to obtain a uniform no-
tation with regard to regular tree grammars.
Thus, a weighted linear indexed grammar
(WLIG) is a 5-tuple G = (S, I, L,R, s`), where
S, I and L are three finite sets of states, indices
and labels, respectively, s` ? S is the initial state,
and R is a finite set of rules. Each rule has one of
the following four forms:
1. s0[??] ? A( s1[ ] ? ? ?
sj?1[ ] sj [??] sj+1[ ] ? ? ?
sm[ ] ) ?w?,
where s0, s1, . . . , sm are states (1 ? j ? m),
A is a label and w is a weight;
2. s[ ] ? A() ?w?;
3. s[??] ? s?[???] ?w?, where ? is an index;
4. s[???] ? s?[??] ?w?.
The expression ?? may be thought of as a vari-
able denoting a string of indices on a stack, and
this variable is to be consistently substituted in
the left-hand and the right-hand sides of rules
upon application during rewriting. In other words,
stacks are copied from the left-hand side of a rule
to at most one member in the right-hand side,
which we will call the head of that rule. The ex-
pression [ ] stands for the empty stack and [???] de-
notes a stack with top element ?. Thereby, rules of
the third type implement a stack push and rules of
the fourth type implement a pop. Rewriting starts
from s`[ ]. The four subsets of R containing rules
of the respective four forms above will be referred
to as R1, R2, R3 and R4.
In terms of tree adjoining grammars, which as-
sume a finite number of elementary trees, the in-
tuition behind the four types of rules is as fol-
lows. Rules of the first type correspond to con-
tinued construction of the same elementary tree.
Rules of the third type correspond to the initiation
of a newly adjoined auxiliary tree and rules of the
fourth type correspond to its completion at a foot
node, returning to an embedding elementary tree
that is encoded in the index that is popped. Rules
of the second type correspond to construction of
leaves, as in the case of regular tree grammars.
See further (Vijay-Shanker and Weir, 1994) for the
equivalence of linear indexed grammars and tree
adjoining grammars.
Note that regular tree grammars can be seen as
special cases of linear indexed grammars, by ex-
cluding rules of the third and fourth types, which
means that stacks of indices always remain empty
(Joshi and Schabes, 1997).
2.3 Probabilistic grammars
A weighted regular tree grammar, or weighted lin-
ear indexed grammar, respectively, is called prob-
abilistic if the weights are probabilities, that is,
values between 0 and 1. A probabilistic regular
tree grammar (PRTG) is proper if for each state
s, the probabilities of all rules that have left-hand
side s sum to one.
Properness for a probabilistic linear indexed
grammar (PLIG) is more difficult to define, due
to the possible overlap of applicability between
15
the four types of rules, listed in the section above.
However, if we encode a given TAG as a LIG in a
reasonable way, then a state s may occur both in
left-hand sides of rules from R1 and in left-hand
sides of rules from R3, but all other such overlap
between the four types is precluded.
Intuitively, a state may represent an internal
node of an elementary tree, in which case rules
from both R1 and R3 may apply, or it may rep-
resent a non-foot leaf node, in which case a rule
from R2 may apply, or it may be a foot node, in
which case a rule from R4 may apply.
With this assumption that the only overlap in ap-
plicability is between R1 and R3, properness can
be defined as follows.
? For each state s, either there are no rules in
R1 or R3 with s in the left-hand side, or the
sum of probabilities of all such rules equals
one.
? For each state s, either there are no rules in
R2 with s in the left-hand side, or the sum of
probabilities of all such rules equals one.
? For each state s and index ?, either there
are no rules in R4 with left-hand side s[???],
or the sum of probabilities of all such rules
equals one.
We say a weighted regular tree grammar, or
weighted linear indexed grammar, respectively, is
consistent if the sum of weights of all (left-most)
derivations is one. This is equivalent to saying that
the sum of weights of all trees is one, and to saying
that the sum of weights of all strings is one.
For each consistent WRTG (WLIG, respec-
tively), there is an equivalent proper and consistent
PRTG (PLIG, respectively). The proof lies in nor-
malization. For WRTGs this is a trivial extension
of normalization of weighted context-free gram-
mars, as described for example by (Nederhof and
Satta, 2003). For WLIGs (and weighted TAGs),
the problem of normalization also becomes very
similar once we consider that the set of derivation
trees of tree adjoining grammars can be described
with context-free grammars, and that this carries
over to weighted derivation trees. See also (Sarkar,
1998).
WLIGs seemingly incur an extra complication,
if a state may occur in combination with an index
on top of the associated stack such that no rules are
applicable. However, for LIGs that encode TAGs,
the problem does not arise as, informally, one may
always resume construction of the embedding el-
ementary tree below the foot node of an adjoined
auxiliary tree.
We say a LIG is in TAG-normal form if (a) at
least one rule is applicable for each combination
of state s and index ? such that s[???] is deriv-
able from s`[ ], and (b) the only overlap in ap-
plicability of the four types of rules is between
R1 and R3. Statements in what follows involv-
ing WLIGs (or PLIGs) in TAG-normal form also
hold for weighted (or probabilistic) TAGs.
3 Analysis of grammars
We call a grammar rule useless if it cannot be part
of any derivation of a tree (or of a string, in the
case of grammars with an emphasis on string lan-
guages). We say a grammar is reduced if it does
not contain useless rules.
Whereas most grammars written by hand or in-
duced by a corpus or treebank are reduced, there
are practical operations that turn reduced gram-
mars into grammars with useless rules; we will
see an example in the next section, where gram-
mars are constructed that generate the intersection
of two given languages. In order to determine
whether the intersection is non-empty, it suffices to
identify useless rules in the intersection grammar.
If and only if all rules are useless, the generated
language is empty.
In the case of context-free grammars (see for ex-
ample (Sippu and Soisalon-Soininen, 1988)), the
analysis to identify useless rules can be split into
two phases:
1. a bottom-up phase to identify the grammar
symbols that generate substrings, which may
include the start symbol if the generated lan-
guage is non-empty; and
2. a top-down phase to identify the grammar
symbols that are reachable from the start
symbol.
The intersection of the generating symbols and the
reachable symbols gives the set of useful symbols.
One can then identify useless rules as those that
contain one or more symbols that are not useful.
The procedure for linear indexed grammars is
similarly split into two phases, of which the first
is given in Figure 1 in the form of a deduction
system. The inference rules simultaneously derive
16
(s, s)
{
s ? S (a)
s
{
s[ ] ? A() (b)
s1 ? ? ? sj?1 (sj , s) sj+1 ? ? ? sm
(s0, s)
{
s0[??] ? A(s1[ ] ? ? ? sj [??] ? ? ? sm[ ]) (c)
s1 ? ? ? sm
s0
{
s0[??] ? A(s1[ ] ? ? ? sj [??] ? ? ? sm[ ]) (d)
(s1, s2)
(s3, s4)
(s0, s4)
{
s0[??] ? s1[???]
s2[???] ? s3[??] (e)
(s1, s2)
s3
s0
{
s0[??] ? s1[???]
s2[???] ? s3[??] (f)
Figure 1: Simultaneous analysis of two kinds of subderivations in a LIG. Items (s, s?) represent existence
of one or more subderivations s[ ] ?? ?(s?[ ]), where ? is a tree with a gap in the form of an unresolved
state s? associated with an empty stack. Furthermore, s and s? are connected through propagation of a
stack of indices, or in other words, the occurrence of s? is the head of a rule, of which the left-hand side
state is the head of another rule, etc., up to s. In the inference rules, items s represent existence of one or
more subderivations s[ ] ?? ?, where ? is a complete tree (without any unresolved states).
two types of item. The generated language is non-
empty if the item s` can be derived.
We will explain inference rule (f), which is the
most involved of the six rules. The two items
in the antecedent indicate the existence of deriva-
tions s1[ ] ?? ?(s2[ ]) and s3[ ] ?? ?. Note
that s1[ ] ?? ?(s2[ ]) implies s1[?] ?? ?(s2[?]),
because an additional element in the bottom of
a stack would not block an existing derivation.
Hence s0[ ] ? s1[?] ?? ?(s2[?]) ? ?(s3[ ]) ??
?(?), which justifies the item s0 in the consequent
of the rule.
After determining which items can be derived
through the deduction system, it is straightforward
to identify those rules that are useful, by applying
the inference rules in reverse, from consequent to
antecedents, starting with s`.
The running time of the analysis is determined
by how often each of the inference rules can be
applied, which is bounded by the number of ways
each can be instantiated with states and rules from
the grammar. The six inference rules together give
us O(|S| + |R2| + |R1| ? |S| + |R1| + |R3| ? |R4| ?
|S| + |R3| ? |R4|) = O(|S| + |R1| ? |S| + |R2|
+ |R3| ? |R4| ? |S|) = |G|3, where we assume a
reasonable measure for the size |G| of a LIG G, for
example, the total number of occurrences of states,
labels and indices in the rules.
It is not difficult to see that there is exactly one
deduction of s` in the deduction system for each
complete derivation in the grammar. We leave the
full proof to the interested reader, but provide the
hint that items (s, s?) can only play a role in a
complete deduction provided s? is rewritten by a
rule that pops an index from the stack. Because
of this, derivations in the grammar of the form
s[ ] ?? ?(s?[ ]) or of the form s[ ] ?? ? can be
divided in a unique way into subderivations repre-
sentable by our items.
The above deduction system is conceptually
very close to a system of equations that expresses
the sum of weights of all derivations in the gram-
mar, or in(s`), in terms of similar values of the
form in(s), which is the sum of weights of all
subderivations s[ ] ?? ?, and in(s, s?), which is
the sum of weights of all subderivations s[ ] ??
?(s?[ ]). The equations are given in Figure 2.
Although the expressions look unwieldy, they
17
in(s0) =
?
s0[ ] ? A() ?w?
w +
?
s0[??] ? A(s1[ ] ? ? ? sj [??] ? ? ? sm[ ]) ?w?
w ? in(s1) ? . . . ? in(sm) +
?
s0[??] ? s1[???] ?w?
s2[???] ? s3[??] ?v?
w ? v ? in(s1, s2) ? in(s3)
in(s0, s) = ?(s0 = s) + ?
s0[??] ? A(s1[ ] ? ? ? sj [??] ? ? ? sm[ ]) ?w?
w ? in(s1) ? . . . ? in(sj , s) ? . . . ? in(sm) +
?
s0[??] ? s1[???] ?w?
s2[???] ? s3[??] ?v?
w ? v ? in(s1, s2) ? in(s3, s)
Figure 2: The sum of weights of all derivations in a WLIG, or in(s`), is defined by the smallest non-
negative solution to a system of equations. The function ? with a boolean argument evaluates to 1 if the
condition is true and to 0 otherwise.
express exactly the ?inside? value of the weighted
context-free grammar that we can extract out of
the deduction system from Figure 1, by instanti-
ating the inference rules in all possible ways, and
then taking the consequent as the left-hand side of
a rule, and the antecedent as the right-hand side.
The weight is the product of weights of rules that
appear in the side conditions. It is possible to ef-
fectively solve the system of equations, as shown
by (Wojtczak and Etessami, 2007).
In the same vein we can compute ?outside?
values for weighted linear indexed grammars, as
straightforward analogues of the outside values of
weighted and probabilistic context-free grammars.
The outside value is the sum of weights of partial
derivations that may lie ?outside? a subderivation
s[ ] ?? ? in the case of out(s), or a subderivation
s[ ] ?? ?(s?[ ]) in the case of out(s, s?). The equa-
tions in Figure 3 again follow trivially from the
view of Figure 1 as weighted context-free gram-
mar and the usual definition of outside values.
The functions in and out are particularly useful
for PLIGs in TAG-normal form, as they allow the
expected number of occurrences of state s to be
expressed as:
E(s) = in(s) ? out(s)
Similarly, the expected number of subderivations
of the form s[ ] ?? ?(s?[ ]) is:
E(s, s?) = in(s, s?) ? out(s, s?)
We will return to this issue in Section 5.
4 Weighted intersection
Before we discuss intersection on the level of
trees, we first show how a well-established type of
intersection on the level of strings, with weighted
context-free grammars and weighted finite au-
tomata (WFAs), can be trivially extended to re-
place CFGs with RTGs or LIGs. The intersec-
tion paradigm is originally due to (Bar-Hillel et
al., 1964). Extension to tree adjoining grammars
and linear indexed grammars was proposed before
by (Lang, 1994) and (Vijay-Shanker and Weir,
1993b).
4.1 Intersection of string languages
Let us assume aWLIG G with terminal and nonter-
minal labels. Furthermore, we assume a weighted
finite automaton A, with an input alphabet equal
to the set of terminal labels of G. The transitions
of A are of the form:
q a7? q? ?w?,
where q and q? are states, a is a terminal symbol,
and w is a weight. To simplify the presentation,
18
out(s?) = ?(s? = s`) + ?
s0[??] ? A(s1[ ] ? ? ? sj [??] ? ? ? sm[ ]) ?w?
k ? {1, . . . , sj?1, sj+1, . . . , sm} s.t. s? = sk
w ? out(s0, s) ? in(sj , s) ?
?
p /? {j, k}
in(sp) +
?
s0[??] ? A(s1[ ] ? ? ? sj [??] ? ? ? sm[ ]) ?w?
k ? {1, . . . , sm} s.t. s? = sk
w ? out(s0) ?
?
p 6= k
in(sp) +
?
s0[??] ? s1[???] ?w?
s2[???] ? s?[??] ?v?
w ? v ? out(s0) ? in(s1, s2)
out(s?, s) = ?
s0[??] ? A(s1[ ] ? ? ? sj [??] ? ? ? sm[ ]) ?w?
s? = sj
w ? out(s0, s) ?
?
p 6= j
in(sp) +
?
s0[??] ? s?[???] ?w?
s[???] ? s3[??] ?v?
w ? v ? out(s0, s4) ? in(s3, s4) +
?
s0[??] ? s1[???] ?w?
s2[???] ? s?[??] ?v?
w ? v ? out(s0, s) ? in(s1, s2) +
?
s0[??] ? s?[???] ?w?
s[???] ? s3[??] ?v?
w ? v ? out(s0) ? in(s3)
Figure 3: The outside values in a WLIG.
we ignore epsilon transitions, and assume there is
a unique initial state q` and a unique final state qa.
We can construct a new WLIG G? whose gen-
erated language is the intersection of the language
generated by G and the language accepted by A.
The rules of G? are:
1. (q0, s0, qm)[??] ?
A( (q0, s1, q1)[ ] ? ? ?
(qj?2, sj?1, qj?1)[ ]
(qj?1, sj , qj)[??]
(qj , sj+1, qj+1)[ ] ? ? ?
(qm?1, sm, qm)[ ] ) ?w?,
for each rule s0[??] ? A(s1[ ] ? ? ? sj?1[ ]
sj [??] sj+1[ ] ? ? ? sm[ ]) ?w? from G and se-
quence q0, . . . , qm of states from A;
2. (q, s, q)[ ] ? A() ?w?, for each rule s[ ] ?
A() ?w? from G and state q from A;
3. (q, s, q?)[ ] ? a ?w ? v?, for each rule s[ ] ?
a ?w? from G and transition q a7? q? ?v? from
A;
4. (q, s, q?)[??] ? (q, s?, q?)[???] ?w?, for each
rule s[??] ? s?[???] ?w? from G and states
q, q? from A;
5. (q, s, q?)[???] ? (q, s?, q?)[??] ?w?, for each
rule s[???] ? s?[??] ?w? from G and states
q, q? from A.
The new states (q, s, q?) give (left-most) deriva-
tions in G? that each simultaneously represent one
(left-most) derivation in G of a certain substring,
starting from state s, and one sequence of transi-
tions taking the automaton A from state q to state
q? while scanning the same substring. The initial
state of G? is naturally (q`, s`, qa), which derives
strings in the intersection of the original two lan-
guages.
Further note that each derivation in G? has a
weight that is the product of the weight of the cor-
19
responding derivation in G and the weight of the
corresponding sequence of transitions in A. This
allows a range of useful applications. For exam-
ple, if A is deterministic (the minimum require-
ment is in fact absence of ambiguity) and if it as-
signs the weight one to all transitions, then G? gen-
erates a set of trees that is exactly the subset of
trees generated by G whose yields are accepted by
A. Furthermore, the weights of those derivations
are preserved. If G is a consistent PLIG in TAG-
normal form, and if A accepts the language of all
strings containing a fixed substring x, then the sum
of probabilities of all derivations in G? gives the
substring probability of x. The effective computa-
tion of this probability was addressed in Section 3.
An even more restricted, but perhaps more fa-
miliar case is if A is a linear structure that accepts
a single input string y of length n. Then G? gen-
erates exactly the set of trees generated by G that
have y as yield. In other words, the string y is
thereby parsed.
If G is binary, i.e. all rules have at most two
states in the right-hand side, then G? has a size
that is cubic in n. This may seem surprising, in
the light of the awareness that practical parsing al-
gorithms for tree adjoining grammars have a time
complexity of no less thanO(n6). However, in or-
der to solve the recognition problem, an analysis
is needed to determine whether G? allows at least
one derivation.
The analysis from Figure 1 requires O(|S?| +
|R?1| ? |S?| + |R?2| + |R?3| ? |R?4| ? |S?|) steps, where
|S?| = O(n2) is the number of states of G?, and
|R?1| = O(n3), |R?2| = |R?3| = |R?4| = O(n2) are
the numbers of rules of G?, divided into the four
main types. This leads to an overall time com-
plexity of O(n6), as expected.
The observation that recognition can be harder
than parsing was made before by (Lang, 1994).
The central new insight this provided was that the
notion of ?parsing? is ill-defined in the literature.
One may choose a form in which to capture all
parses of an input allowed by a grammar, but dif-
ferent such forms may incur different costs of ex-
tracting individual parse trees.
In Section 6.2 we will consider the complexity
of parsing and recognition if G is not binary.
4.2 Intersection of tree languages
We now shift our attention from strings to trees,
and consider the intersection of the tree language
generated by a weighted linear indexed grammar
G1 and the tree language generated by a weighted
regular tree grammar G2. This intersection is gen-
erated by another weighted linear indexed gram-
mar G, which has the following rules:
1. (s0, q0)[??] ? A( (s1, q1)[ ] ? ? ?
(sj?1, qj?1)[ ]
(sj , qj)[??]
(sj+1, qj+1)[ ] ? ? ?
(sm, qm)[ ] ) ?w ? v?,
for each rule s0[??] ? A(s1[ ] ? ? ? sj?1[ ]
sj [??] sj+1[ ] ? ? ? sm[ ]) ?w? from G1 and each
rule q0 ? A(q1 ? ? ? qm) ?v? from G2;
2. (s, q)[ ] ? A() ?w ? v?, for each rule s[ ] ?
A() ?w? from G1 and each rule q ? A() ?v?
from G2;
3. (s, q)[??] ? (s?, q)[???] ?w?, for each rule
s[??] ? s?[???] ?w? from G1 and state q from
G2;
4. (s, q)[???] ? (s?, q)[??] ?w?, for each rule
s[???] ? s?[??] ?w? from G1 and state q from
G2.
Much as in the previous section, each (left-
most) derivation in G corresponds to one (left-
most) derivation in G1 and one in G2. Further-
more, these three derivations derive the same la-
belled tree, and a derivation in G has a weight that
is the product of the weights of the corresponding
derivations in G1 and G2.
It can be instructive to look at special cases.
Suppose that G2 is an unambiguous regular tree
grammar of size O(n) generating a single tree t
with n vertices, assigning weight one to all its
rules. Then the above construction can be seen
as parsing of that tree t. The sum of weights of
derivations in G then gives the weight of the tree
in G1. See Section 3 once more for a general way
to compute this weight, as the inside value of the
initial state of G, which is naturally (s`, q`).
In order to do recognition of t, or in other words,
to determine whether G allows at least one deriva-
tion, the analysis from Figure 1 can be used, which
has time complexity O(|S| + |R1| ? |S| + |R2|
+ |R3| ? |R4| ? |S|), where |S| = O(n) is the
number of states of G, and the numbers of rules
are |R1| = O(n), |R2| = |R3| = |R4| = O(n).
Note that |R1| = O(n) because we have assumed
that G2 allows only one derivation of one tree t,
20
hence q0 uniquely determines q1, . . . , qm. Over-
all, we obtain O(n3) steps, which concurs with a
known result about the complexity of TAG parsing
of trees, as opposed to strings (Poller and Becker,
1998).
Another special case is if WLIG G1 simplifies
to a WRTG (i.e. the stacks of indices remain al-
ways empty), which means we compute the inter-
section of two weighted regular tree grammars G1
and G2. For recognition, or in other words to de-
cide non-emptiness of the intersection, we can still
use Figure 1, although now only inference rules
(b) and (d) are applicable (with a small refinement
to the algorithm we can block spurious application
of (a) where no rules exist that pop indices.) The
complexity is determined by (d), which requires
O(|G1| ? |G2|) steps.
5 Parameter estimation
PLIGs allow finer description of probability distri-
butions than PRTG, both over string languages and
over tree languages. However, the (string) pars-
ing complexity of regular tree grammars is O(n3)
and that of LIGs is O(n6). It may therefore be
preferable for reasons of performance to do pars-
ing with a PRTG even when a PTAGs or PLIG is
available with accurately trained probabilities. Al-
ternatively, one may do both, with a PRTG used
in a first phase to heuristically reduce the search
space.
This section outlines how a suitable PRTG G2
can be extracted out of a PLIG G1, assuming the
underlying RTG G?2 without weights is already
given. The tree language generated by G?2 may be
an approximation of that generated by G1. The ob-
jective is to make G2 as close as possible to G1 in
terms of probability distributions over trees. We
assume that G?2 is unambiguous, that is, for each
tree it generates, there is at most one derivation.
The procedure is a variant of the one described
by (Nederhof, 2005). The idea is that derivations
in G1 are mapped to those in G?2, via the trees in the
intersection of the two tree languages. The proba-
bility distribution of states and rules in G2 is esti-
mated based on the expected frequencies of states
and rules from G?2 in the intersection.
Concretely, we turn the RTG G?2 into a PRTG
G??2 that is obtained simply be assigning weight
one to all rules. We then compute the intersec-
tion grammar G as in Section 4.2. Subsequently,
the inside and outside values are computed for G,
as explained in Section 3. The expected number of
occurrences of a rule in G of the form:
(s0, q0)[??] ? A( (s1, q1)[ ] ? ? ?
(sj?1, qj?1)[ ]
(sj , qj)[??]
(sj+1, qj+1)[ ] ? ? ?
(sm, qm)[ ] ) ?w ? v?,
is given by multiplying the outside and inside
probabilities and the rule probability, as usual.
We get two terms however that we need to sum.
The intuition is that we must count both rule oc-
currences used for building initial TAG trees and
those used for building auxiliary TAG trees. This
gives:
w ? v ? out((s0, q0)) ?
?
k
in((sk, qk)) +
w ? v ?
?
s,q
out((s0, q0), (s, q)) ?
in((sj , qj), (s, q)) ?
?
k 6=j
in((sk, qk))
By summing these expected numbers for different
rules s0[??] ? A(s1[ ] ? ? ? sj?1[ ] sj [??] sj+1[ ]
? ? ? sm[ ]), we obtain the expected number of oc-
currences of q0 ? A(q1 ? ? ? qm), Let us denote
this sum by E(q0 ? A(q1 ? ? ? qm)). By summing
these for fixed q0, we obtain the expected number
of occurrences of q0, which we denote by E(q0).
The probability of q0 ? A(q1 ? ? ? qm) in G2 is then
set to be the ratio of E(q0 ? A(q1 ? ? ? qm)) and
E(q0).
By this procedure, the Kullback-Leibler dis-
tance between G1 and G2 is minimized. Although
the present paper deals with very different for-
malisms, the proof of correctness is identical to
that in (Nederhof, 2005). The reason is that in both
cases the mathematical analysis must focus on the
objects in the intersection (strings or trees) which
may correspond to multiple derivations in the orig-
inal model (here G1) but to a single derivation in
the unambiguous model to be trained (here G2),
and each derivation is composed of rules, whose
probabilities are to be multiplied.
6 Extensions
6.1 Transduction
For various formalisms describing (string or tree)
languages, there are straightforward generaliza-
tions that describe a relation between two or more
21
languages, which is known as a transduction. The
idea is that the underlying control mechanism,
such as the states in regular tree grammars or lin-
ear indexed grammars, is now coupled to two or
more surface forms that are synchronously pro-
duced. For example, a rule in a weighted syn-
chronous regular tree grammar (WSRTG) has the
form:
s0 ? A(s1 ? ? ? sm), B(spi(1) ? ? ? spi(m)) ?w?,
where pi is a permutation of 1, . . . ,m. We can gen-
eralize this to having a third label C and a second
permutation pi?, in order to describe simultaneous
relations between three tree languages, etc. In this
section we will restrict ourselves to binary rela-
tions however, and call the first surface form the
input and the second surface form the output. For
synchronous tree adjoining grammars, see for ex-
ample (Shieber, 1994).
If we apply intersection on the input or on the
output of a synchronous grammar formalism, then
this is best seen as composition. This is well-
known in the case of finite-state transducers and
some forms of context-free transduction (Berstel,
1979), and application to a wider range of for-
malisms is gaining interest in the area of machine
translation (Knight, 2007).
With the intersection from Section 4.2 trivially
extended to composition, we can now implement
composition of the form:
?1 ? . . . ? ?k,
where the different ?j are transducers, of which
k ? 1 are (W)SRTGs and at most one is a
(weighted) synchronous LIG ((W)SLIG). The re-
sult of the composition is another (W)SLIG. It
should be noted that a (W)RTS (or (W)LIG) can
be seen as a (W)SRTG (or (W)SLIG, respectively)
that represents the identity relation on its tree lan-
guage.
6.2 Binarization
In the discussion of complexity in Section 4.1, we
assumed that rules are binary, that is, that they
have at most two states in each right-hand side.
However, whereas any context-free grammar can
be transformed into a binary form (e.g. Chomsky
normal form), the grammars as we have defined
them cannot be. We will show that this is to a large
extent a consequence of our definitions, which
were motivated by presentational ease, rather than
by generality.
The main problem is formed by rules of the
form s0 ? A(s1 ? ? ? sm), where m > 2. Such
long rules cannot be broken up into shorter rules
of the same form, as this would require an addi-
tional labelled vertex, changing the tree language.
An apparent solution lies in allowing branching
rules without any label, for example s1 ? s2 s3.
Regrettably this could create substantial computa-
tional problems for intersection of the described
tree languages. As labels provide the mechanism
through which to intersect tree languages, rules
of the above form are somewhat similar to unit
rules or epsilon rules in context-free grammars, in
that they are not bound to observable elements.
Branching rules furthermore have the potential
to generate context-free languages, and therefore
they are more pernicious to intersection, consider-
ing that emptiness of intersection of context-free
languages is undecidable.
It therefore seems better to restrict branching
rules s1 ? s2 s3 to finite-state power, for exam-
ple by making these rules exclusively left-linear
or right-linear. A more elegant but equivalent way
of looking at this may be to have rules of the form:
s0 ? A(R),
where R is a regular language over states. In the
case of linear indexed grammars, we would have
rules of the form:
s[??] ? A(L s?[??] R)
where L and R are regular languages over expres-
sions of the form s[ ]. Appropriate weighted fi-
nite automata can be used to assign weights to se-
quences of such expressions in L and R. With
these extended types of rules, our construction
from Section 4.2 still works. The key observation
here is that regular languages are closed under in-
tersection.
One of the implications of the above extended
definitions is that labels appear not only with-
out fixed ranks, as we have assumed from the
start in Section 2, but even without a bound on
the rank. Concretely, a vertex may appear with
any number of children in a tree. Whereas this
may be unconventional in certain areas of formal
language theory, it is a well-accepted practice in
the parsing of natural language to make the num-
ber of constituents of syntactic categories flexi-
ble and conceptually unbounded; see for example
22
(Collins, 1997). Also the literature on unranked
tree automata is very relevant; see for example
(Schwentick, 2007). Binarization for LIGs was
considered before by (Vijay-Shanker and Weir,
1993a).
6.3 Beyond TAGs
In the light of results by (Kepser and Mo?nnich,
2006) it is relatively straightforward to consider
larger classes of linear context-free tree grammars
in place of tree-adjoining grammars, in order to
generalize the construction in Section 4.2.
The generalization described in what follows
seems less straightforward. Context-free lan-
guages can be characterized in terms of parse trees
in which path sets (sets of strings of labels on
paths from the root to a leaf) are regular. In the
case of tree adjoining languages, the path sets are
context-free. There is a hierarchy of classes of lan-
guages in which the third step is to consider path
sets that are tree adjoining languages (Weir, 1992).
In this paper, we have considered the parsing-as-
intersection paradigm for the first two members of
the hierarchy. It may be possible that the paradigm
is also applicable to the third and following mem-
bers. This avenue is yet to be pursued.
7 Conclusions
This paper has extended the parsing-as-
intersection paradigm from string languages
to tree languages. Probabilities, or weights in
general, were incorporated in this framework in a
natural way. We have discussed one particular ap-
plication involving a special case of the extended
paradigm.
Acknowledgements
Helpful comments by anonymous reviewers are
gratefully acknowledged. The basic result from
Section 4.1 as it pertains to RTGs as subclass of
LIGs was discussed with Heiko Vogler, who pro-
posed two alternative proofs. Sylvain Schmitz
pointed out to me the relevance of literature on lin-
ear context-free tree languages.
References
A.V. Aho, M.S. Lam, R. Sethi, and J.D. Ullman.
2007. Compilers: Principles, Techniques, & Tools.
Addison-Wesley.
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On
formal properties of simple phrase structure gram-
mars. In Y. Bar-Hillel, editor, Language and Infor-
mation: Selected Essays on their Theory and Appli-
cation, chapter 9, pages 116?150. Addison-Wesley,
Reading, Massachusetts.
J. Berstel. 1979. Transductions and Context-Free Lan-
guages. B.G. Teubner, Stuttgart.
M. Collins. 1997. Three generative, lexicalised mod-
els for statistical parsing. In 35th Annual Meeting of
the Association for Computational Linguistics, Pro-
ceedings of the Conference, pages 16?23, Madrid,
Spain, July.
J. Graehl and K. Knight. 2004. Training tree transduc-
ers. In HLT-NAACL 2004, Proceedings of the Main
Conference, Boston, Massachusetts, USA, May.
F. Gcseg and M. Steinby. 1997. Tree languages. In
G. Rozenberg and A. Salomaa, editors, Handbook
of Formal Languages, Vol. 3, chapter 1, pages 1?68.
Springer, Berlin.
J.E. Hopcroft and J.D. Ullman. 1979. Introduction
to Automata Theory, Languages, and Computation.
Addison-Wesley.
A.K. Joshi and Y. Schabes. 1997. Tree-adjoining
grammars. In G. Rozenberg and A. Salomaa, edi-
tors, Handbook of Formal Languages. Vol 3: Beyond
Words, chapter 2, pages 69?123. Springer-Verlag,
Berlin/Heidelberg/New York.
D. Jurafsky and J.H. Martin. 2000. Speech and Lan-
guage Processing. Prentice-Hall.
S. Kepser and U. Mo?nnich. 2006. Closure properties
of linear context-free tree languages with an appli-
cation to optimality theory. Theoretical Computer
Science, 354:82?97.
K. Knight. 2007. Capturing practical natural language
transformations. Machine Translation, 21:121?133.
B. Lang. 1994. Recognition can be harder than pars-
ing. Computational Intelligence, 10(4):486?494.
M.-J. Nederhof and G. Satta. 2003. Probabilistic pars-
ing as intersection. In 8th International Workshop
on Parsing Technologies, pages 137?148, LORIA,
Nancy, France, April.
M.-J. Nederhof. 2005. A general technique to train
language models on language models. Computa-
tional Linguistics, 31(2):173?185.
P. Poller and T. Becker. 1998. Two-step TAG pars-
ing revisited. In Fourth International Workshop on
Tree Adjoining Grammars and Related Frameworks,
pages 143?146. Institute for Research in Cognitive
Science, University of Pennsylvania, August.
23
P. Resnik. 1992. Probabilistic tree-adjoining grammar
as a framework for statistical natural language pro-
cessing. In Proc. of the fifteenth International Con-
ference on Computational Linguistics, pages 418?
424. Nantes, August.
A. Sarkar. 1998. Conditions on consistency of prob-
abilistic tree adjoining grammars. In 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics, volume 2, pages 1164?1170,
Montreal, Quebec, Canada, August.
Y. Schabes. 1992. Stochastic lexicalized tree-
adjoining grammars. In Proc. of the fifteenth Inter-
national Conference on Computational Linguistics,
pages 426?432. Nantes, August.
Thomas Schwentick. 2007. Automata for XML?a
survey. Journal of Computer and System Sciences,
73:289?315.
S.M. Shieber. 1994. Restricting the weak-generative
capacity of synchronous tree-adjoining grammars.
Computational Intelligence, 10(4):371?385.
K. Sima?an. 1997. Efficient disambiguation by means
of stochastic tree substitution grammars. In D. Jones
and H. Somers, editors, New Methods in Language
Processing. UCL Press, UK.
S. Sippu and E. Soisalon-Soininen. 1988. Parsing The-
ory, Vol. I: Languages and Parsing, volume 15 of
EATCS Monographs on Theoretical Computer Sci-
ence. Springer-Verlag.
K. Vijay-Shanker and D.J. Weir. 1993a. Parsing some
constrained grammar formalisms. Computational
Linguistics, 19(4):591?636.
K. Vijay-Shanker and D.J. Weir. 1993b. The use of
shared forests in tree adjoining grammar parsing. In
Sixth Conference of the European Chapter of the As-
sociation for Computational Linguistics, Proceed-
ings of the Conference, pages 384?393, Utrecht, The
Netherlands, April.
K. Vijay-Shanker and D.J. Weir. 1994. The equiva-
lence of four extensions of context-free grammars.
Mathematical Systems Theory, 27:511?546.
D.J. Weir. 1992. A geometric hierarchy beyond
context-free languages. Theoretical Computer Sci-
ence, 104:235?261.
D. Wojtczak and K. Etessami. 2007. PReMo: an an-
alyzer for Probabilistic Recursive Models. In Tools
and Algorithms for the Construction and Analysis
of Systems, 13th International Conference, volume
4424 of Lecture Notes in Computer Science, pages
66?71, Braga, Portugal. Springer-Verlag.
24
Approximating Context-Free by Rational Transduction for
Example-Based MT
Mark-Jan Nederhof  
AT&T Labs-Research, 180 Park Avenue, Florham Park, NJ 07932
and
Alfa Informatica (RUG), P.O. Box 716, NL-9700 AS Groningen, The Netherlands
Abstract
Existing studies show that a weighted
context-free transduction of reasonable
quality can be effectively learned from
examples. This paper investigates the
approximation of such transduction by
means of weighted rational transduc-
tion. The advantage is increased pro-
cessing speed, which benefits real-
time applications involving spoken lan-
guage.
1 Introduction
Several studies have investigated automatic or
partly automatic learning of transductions for ma-
chine translation. Some of these studies have con-
centrated on finite-state or extended finite-state
machinery, such as (Vilar and others, 1999), oth-
ers have chosen models closer to context-free
grammars and context-free transduction, such as
(Alshawi et al, 2000; Watanabe et al, 2000; Ya-
mamoto and Matsumoto, 2000), and yet other
studies cannot be comfortably assigned to either
of these two frameworks, such as (Brown and oth-
ers, 1990) and (Tillmann and Ney, 2000).
In this paper we will investigate both context-
free and finite-state models. The basis for our
study is context-free transduction since that is a
powerful model of translation, which can in many
cases adequately describe the changes of word

The second address is the current contact address; sup-
ported by the Royal Netherlands Academy of Arts and Sci-
ences; current secondary affiliation is the German Research
Center for Artificial Intelligence (DFKI).
order between two languages, and the selection
of appropriate lexical items. Furthermore, for
limited domains, automatic learning of weighted
context-free transductions from examples seems
to be reasonably successful.
However, practical algorithms for computing
the most likely context-free derivation have a cu-
bic time complexity, in terms of the length of
the input string, or in the case of a graph out-
put by a speech recognizer, in terms of the num-
ber of nodes in the graph. For certain lexicalized
context-free models we even obtain higher time
complexities when the size of the grammar is not
to be considered as a parameter (Eisner and Satta,
1999). This may pose problems, especially for
real-time speech systems.
Therefore, we have investigated approximation
of weighted context-free transduction by means
of weighted rational transduction. The finite-state
machinery for implementing the latter kind of
transduction in general allows faster processing.
We can also more easily obtain robustness. We
hope the approximating model is able to preserve
some of the accuracy of the context-free model.
In the next section, we discuss preliminary def-
initions, adapted from existing literature, mak-
ing no more than small changes in presentation.
In Section 3 we explain how context-free trans-
duction grammars can be represented by ordinary
context-free grammars, plus a phase of postpro-
cessing. The approximation is discussed in Sec-
tion 4. As shown in Section 5, we may easily
process input in a robust way, ensuring we always
obtain output. Section 6 discusses empirical re-
sults, and we end the paper with conclusions.
2 Preliminaries
2.1 hierarchical alignment
The input to our algorithm is a corpus consisting
of pairs of sentences related by an hierarchical
alignment (Alshawi et al, 2000). In what follows,
the formalization of this concept has been slightly
changed with respect to the above reference, to
suit our purposes in the remainder of this article.
The hierarchically aligned sentence pairs in the
corpus are 5-tuples
	


satisfying
the following. The first two components,

and


, are strings, called the source string and the
target string, respectively, the lengths of which
are denoted by 
   
and 

Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1370?1381, Dublin, Ireland, August 23-29 2014.
Hybrid Grammars for Discontinuous Parsing
Mark-Jan Nederhof
School of Computer Science
University of St Andrews
KY16 9SX, UK
Heiko Vogler
Department of Computer Science
Technische Universit?at Dresden
D-01062 Dresden, Germany
Abstract
We introduce the concept of hybrid grammars, which are extensions of synchronous grammars,
obtained by coupling of lexical elements. One part of a hybrid grammar generates linear struc-
tures, another generates hierarchical structures, and together they generate discontinuous struc-
tures. This formalizes and generalizes some existing mechanisms for dealing with discontinuous
phrase structures and non-projective dependency structures. Moreover, it allows us to separate
the degree of discontinuity from the time complexity of parsing.
1 Introduction
Discontinuous phrases occur frequently in languages with relatively free word order, and adequate de-
scription of their structure requires special care (Kathol and Pollard, 1995; M?uller, 2004). Even for
languages such as English, with a relatively rigid word order, there is a clear need for discontinuous
structures (McCawley, 1982; Stucky, 1987).
Early treebanks for English (Marcus et al., 1993) have often represented discontinuity in a way that
makes it tempting to ignore it altogether, certainly for the purposes of parsing, whereas recent approaches
tend to represent discontinuity in a more overt form, sometimes after transformation of existing treebanks
(Choi and Palmer, 2010; Evang and Kallmeyer, 2011). In many modern treebanks, discontinuous struc-
tures have been given a prominent status (B?ohmov?a et al., 2000).
Classes of trees without discontinuity can be specified as the sets of parse trees of context-free gram-
mars (CFGs). Somewhat larger classes can be specified by tree substitution grammars (Sima?an et al.,
1994) and regular tree grammars (Brainerd, 1969; G?ecseg and Steinby, 1997). Practical parsers for these
three formalisms have running time O(n
3
), where n is the length of the input sentence. Discontinuous
structures go beyond their strong generative capacity however. Similarly, non-projective dependency
structures cannot be obtained by traditional dependency grammars. See (Rambow, 2010) for discussion
of the relation between constituent and dependency structures and see (Maier and Lichte, 2009) for a
comparison of discontinuity and non-projectivity.
One way to solve the above problems has been referred to as pseudo-projectivity, i.e. a parser produces
a projective structure, which in a second phase is transformed into a non-projective structure (Kahane
et al., 1998; McDonald and Pereira, 2006; Nivre and Nilsson, 2005). In particular, this may involve
lifting, whereby one end point of a dependency link moves across a path of nodes. A related idea for
discontinuous phrase structure is the reversible splitting conversion of (Boyd, 2007). See also (Johnson,
2002; Campbell, 2004; Gabbard et al., 2006).
As shown by (Nivre, 2009), the second phase of pseudo-projective dependency parsing can be inter-
leaved with the first, by replacing the usual one-way input tape by an additional stack, or buffer. Where
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1370
non-topmost positions from the parsing stack are moved back to the buffer, input positions are effectively
swapped and non-projective dependency structures arise.
Tree adjoining grammars (TAGs) can describe strictly larger classes of word order phenomena than
CFGs (Rambow and Joshi, 1997). TAG parsers have a time complexity of O(n
6
) (Vijay-Shankar and
Joshi, 1985). However, the derived trees they generate are still continuous. Although their derivation
trees may be argued to be discontinuous, these by themselves are not normally the desired syntactic
structures. It was argued by (Becker et al., 1991) that further additions to TAGs are needed to obtain
adequate descriptions of scrambling phenomena.
An alternative is proposed by (Kallmeyer and Kuhlmann, 2012): a transformation is added that turns a
derivation tree of a (lexicalized) TAG into a non-projective dependency structure. A very similar mech-
anism is used to obtain non-projective dependency structures using linear context-free rewriting systems
(LCFRSs) (Kuhlmann, 2013) that are lexicalized. In a LCFRS the synthesis of strings is normally spec-
ified by yield functions associated with rules. By an additional interpretation of the templates of these
yield functions in the algebra of dependency trees (with the overt lexical items as roots), the LCFRS
generates both strings and (possibly non-projective) dependency structures.
However, the running time of LCFRS parsers is generally very high, still polynomial in the sentence
length, but with a degree determined by properties of the grammar; difficulties involved in running
LCFRS parsers for natural languages are described by (Kallmeyer and Maier, 2013).
It follows from the above that there is considerable freedom in the design of parsers that produce
discontinuous structures for given input sentences. One can distinguish between two main issues. The
first is the formalism that guides the parsing of the input. This determines a class of input (string)
languages, which can be that of the context-free languages, or tree adjoining languages, etc. We assume
parsing with any of these formalisms results in derivations of some sort. The second main issue is the
mechanism that translates such derivations into discontinuous structures.
This leads to a number of open questions that are all related. First, what is, or should be, the division
of labor between the parser producing the derivations and the mechanism turning those derivations into
discontinuous structures? If we want to achieve high degrees of discontinuity in the output structures,
should the formalism for the input language be much more powerful than, say, context-free? Or can
highly discontinuous structures be obtained equally well through ordinary CFGs in combination with an
advanced mechanism producing discontinuous structures out of derivations?
Second, how should one approach the problem of finding the grammar (and grammar class) for the
input language and the mapping from derivations to structures if the only thing that is given is a treebank?
A third question is which formalisms are suitable to formally describe mappings from derivations to
discontinuous structures. Lastly, can we characterize the classes of output (tree-)languages for various
combinations of input grammars and derivation-to-structure mappings?
In this paper we provide one possible answer to these questions by a new type of formalism, which we
call hybrid grammars. Such a grammar consists of a string grammar and a tree grammar. Derivations are
coupled so as to achieve synchronous rewriting. The input string language and the output tree language
are thereby straightforwardly defined. Different from synchronous grammars (Shieber and Schabes,
1990; Satta and Peserico, 2005) is that occurrences of terminal symbols are also coupled. Thereby
the linear order of the symbols in a derived string imposes an order on the coupled symbols in the
synchronously derived tree; this allows a straightforward specification of a discontinuous structure.
One can define a hybrid grammar consisting of a simple macro grammar (Fischer, 1968) and a simple
context-free tree grammar (Rounds, 1970), but various other combinations of a string grammar and a tree
grammar are possible as well. Due to lack of space we will here concentrate on only one kind of hybrid
grammar, namely that consisting of a LCFRS as string grammar and a form of definite clause program as
tree grammar. We will show that hybrid grammars that induce (finite) sets of hybrid trees can always be
constructed, even if the allowable derivations are severely restricted, and we discuss experiments. Lastly,
a negative result will be given, which shows that a certain linguistic phenomenon cannot be handled if
the string grammar is too restricted.
We cast our definitions in terms of hybrid trees, of which discontinuous phrase structures and non-
1371
projective dependency structures are special cases.
1
Thereby the generality of the framework is demon-
strated.
2 Preliminaries
Let N = {0, 1, 2, . . .} and N
+
= N \ {0}. For each n ? N
+
, we let [n] stand for the set {1, . . . , n}, and
we let [0] stand for ?. We write [n]
0
to denote [n] ? {0}. We fix an infinite list x
1
, x
2
, . . . of pairwise
distinct variables. We let X = {x
1
, x
2
, x
3
, . . .} and X
k
= {x
1
, . . . , x
k
} for each k ? N.
A ranked set ? is a set of symbols, associated with a rank function assigning a number rk
?
(?) ? N
to each symbol ? ? ?. A ranked alphabet is a ranked set with a finite number of symbols. We let ?
(k)
denote {? ? ? | rk
?
(?) = k}.
The following definitions were inspired by (Seki and Kato, 2008). The sets of terms and sequence-
terms (s-terms) over ranked set ?, with variables in some set Y ? X , are denoted by T
?
(Y ) and T
?
?
(Y ),
respectively, and defined inductively as follows:
(i) Y ? T
?
(Y ),
(ii) if k ? N, ? ? ?
(k)
and s
i
? T
?
?
(Y ) for each i ? [k], then ?(s
1
, . . . , s
k
) ? T
?
(Y ), and
(iii) if n ? N and t
i
? T
?
(Y ) for each i ? [n], then ?t
1
, . . . , t
n
? ? T
?
?
(Y ).
We let T
?
?
and T
?
stand for T
?
?
(?) and T
?
(?) respectively. Throughout this paper, we use variables such
as s and s
i
for s-terms and variables such as t and t
i
for terms. The justification for using s-terms as
defined here is that they provide the required flexibility for dealing with both strings (? = ?
(0)
) and
unranked trees (? = ?
(1)
), in combination with derivational nonterminals.
Concatenation of s-terms is given by ?t
1
, . . . , t
n
? ? ?t
n+1
, . . . , t
n+m
? = ?t
1
, . . . , t
n+m
?. Sequences
such as s
1
, . . . , s
k
or x
1
, . . . , x
k
will typically be abbreviated to s
1,k
or x
1,k
, respectively. For ? ? ?
(0)
we sometimes abbreviate ?() to ?.
In examples we also abbreviate ?t
1
, . . . , t
n
? to t
1
? ? ? t
n
, that is, omitting the angle brackets and com-
mas. Moreover, we sometimes abbreviate ?(??) to ?. Whether ? then stands for ?(??) or for ?() depends
on whether ? ? ?
(1)
or ? ? ?
(0)
, which will be clear from the context.
Subterms in terms or s-terms are identified by positions; these can be formalized by a suitable refine-
ment of the familiar notion of Gorn address. The set of all positions in term t or in s-term s is denoted
by pos(t) or pos(s), respectively. The subset of pos(t) consisting of all positions where the label is in
some set ? ? ? is denoted by pos
?
(t).
3 Hybrid trees
The purpose of this section is to unify existing notions of non-projective dependency structures and
discontinuous phrase structures, formalized using s-terms.
We fix an alphabet ? = ?
(1)
and a subset ? ? ?. A hybrid tree over (?,?) is a pair h = (s,?
s
),
where s ? T
?
?
and?
s
is a total order on pos
?
(s). In words, a hybrid tree combines hierarchical structure,
in the form of an s-term over the full alphabet ?, with a linear structure, which can be seen as a string
over ? ? ?. This string will be denoted by str(h).
For discontinuous phrase structures, the elements of ? would typically represent lexical items, and
the elements of ? \ ? would typically represent syntactic categories. For non-projective dependency
structures, ? would be equal to ?. Simple examples of discontinuous phrase structures are presented in
Figures 1 and 2.
4 Basic grammatical formalisms
The concept of hybrid grammars is illustrated in Section 5, by coupling a class of string grammars and a
class of tree grammars.
1
Moreover, we need to avoid any confusion with the term ?discontinuous tree? from (Bunt, 1996), which is characterized
by the notion of ?context daughter?, which is absent from our framework. The term ?hybrid tree? was used before by (Lu et
al., 2008), also for a mixture of a tree structure and a linear structure, generated by a probabilistic model. However, the linear
?surface? structure was obtained by a simple left-to-right tree traversal, whereas a meaning representation was obtained by a
slightly more flexible traversal of the same tree. The emphasis in the current paper is rather on separating the linear structure
from the tree structure.
1372
VP
V
hat gearbeitet
ADV
schnell
hat schnell gearbeitet
Figure 1: Hybrid tree for German ?[...]
hat schnell gearbeitet? (?[...] has worked
quickly?), after (Seifert and Fischer, 2004).
The bottom line indicates the word order in
German. (Alternative analyses exist that do not
require discontinuity; we make no claim the
structure above is the most adequate.)
S
a S
a S
a b
b
b
aaa bb b
Figure 2: Abstract representation of cross-
serial dependencies in Dutch (Bresnan et al.,
1982).
4.1 Linear context-free rewriting systems
Much as in (Vijay-Shanker et al., 1987), we define a linear context-free rewriting system (LCFRS) as a
tuple G = (N,S,?, P ), where N is a ranked alphabet of nonterminals, S ? N
(1)
is the start symbol,
? = ?
(0)
is a ranked alphabet of terminals (? ?N = ?), and P is a finite set of rules, each of the form:
A
0
(s
1,k
0
)? ?A
1
(x
1,m
1
), A
2
(x
m
1
+1,m
2
), . . . , A
n
(x
m
n?1
+1,m
n
)? (1)
where n ? N, A
i
? N
(k
i
)
for each i ? [n]
0
, and m
i
=
?
j:1?j?i
k
j
for each i ? [n], and s
j
? T
?
?
(X
m
n
)
for each j ? [k
0
]. In words, the right-hand side is an s-term consisting of nonterminals A
i
(i ? [n]), with
distinct variables as arguments; there are m
n
variables altogether, which is the sum of the ranks k
i
of all
A
i
(i ? [n]). The left-hand side is an occurrence of A
0
with each argument being a string of variables
and terminals. Furthermore, we demand that each x
j
(j ? [m
n
]) occurs exactly once in the left-hand
side. The largest rank of any nonterminal is called the fanout of the grammar.
A rule instance is obtained by choosing a rule of the above form, and consistently substituting variables
with s-terms in T
?
?
(which are strings due to the terminals having rank 0). The language induced is the
set of s-terms s such that ?S(s)? ?
?
G
??, where?
G
is the ?derives? relation that uses rule instances. For
given s, the set of all LCFRS derivations ?S(s)? ?
?
G
?? (in compact tabular form) can be obtained in
polynomial time in the length of s (Seki et al., 1991).
Example 1
An example of a LCFRS is presented on the S(x
1
x
3
x
2
x
4
) ? A(x
1
, x
2
) B(x
3
, x
4
)
A(ax
1
,bx
2
) ? A(x
1
, x
2
)
A(??, ??) ? ??
B(cx
1
,dx
2
) ? B(x
1
, x
2
)
B(??, ??) ? ??
right. Terminals are lower case bold letters and
nonterminals are upper-case italic letters. All
derived strings are of the form a
m
c
n
b
m
d
n
with
m,n ? N. The linguistic relevance lies in cross-
serial dependencies in Swiss German (Shieber,
1985).
4.2 Definite clause programs
In this section we describe a particular kind of definite clause programs. Our definition is inspired by
(Deransart and Ma?uszynski, 1985), which investigated the relation between logic programs and attribute
grammars, together with the ?syntactic single use requirement? from (Giegerich, 1988). The values
produced are trees (or to be more precise s-terms).
1373
A simple definite clause program (sDCP) is a tuple G = (N,S,?, P ), where N is a ranked alphabet
of nonterminals and ? = ?
(1)
is a ranked alphabet of terminals.
2
Moreover, each nonterminal A ? N
has a fixed i-rank (the number of inherited arguments) and a fixed s-rank (the number of synthesized
arguments), denoted by i-rk(A) and s-rk(A), respectively, satisfying i-rk(A) + s-rk(A) = rk
N
(A). In
our notation, the inherited arguments precede the synthesized arguments. The start symbol S has only
one argument, which is synthesized, i.e. rk
N
(S) = s-rk(S) = 1 and i-rk(S) = 0.
A rule is of the form:
A
0
(x
(0)
1,k
0
, s
(0)
1,k
?
0
)? ?A
1
(s
(1)
1,k
1
, x
(1)
1,k
?
1
), . . . , A
n
(s
(n)
1,k
n
, x
(n)
1,k
?
n
)? (2)
where n ? N, k
i
= i-rk(A
i
) and k
?
i
= s-rk(A
i
), for i ? [n]
0
. The set of variables occurring in the lists
x
(0)
1,k
0
and x
(i)
1,k
?
i
(i ? [n]) equals X
m
, where m = k
0
+
?
i?[n]
k
?
i
. In other words, every variable from X
m
occurs exactly once in all these lists together. This is where values ?enter? the rule. Further, the s-terms
in s
(0)
1,k
?
0
and s
(i)
1,k
i
(i ? [n]) are in T
?
?
(X
m
) and together contain each variable in X
m
exactly once. This
is where values are combined and ?exit? the rule.
The ?derives? relation?
G
and other relevant notation are defined as for LCFRSs (where the s-terms
in arguments are now trees due to the terminals having rank 1). If the rules in a derivation are given, then
the relevant rule instances are uniquely determined, and can be computed in linear time in the size of
the derivation, provided the sDCP contains no cycles. The existence of cycles is decidable, as we know
from the literature on attribute grammars. There are sufficient conditions for absence of cycles, such as
the grammar being L-attributed (Bochmann, 1976). In this article, we will assume that sDCPs contain
no cycles.
Example 2
An example of a sDCP is presented S(x
2
) ? A(x
1
) B(x
1
, x
2
)
A(a A(x
1
) b) ? A(x
1
)
A(??) ? ??
B(x
1
, c B(x
2
) d) ? B(x
1
, x
2
)
B(x
1
, x
1
) ? ??
on the right, where the first argument of
B is inherited and all other arguments
are synthesized. A derived s-term is
e.g. c B(c B(a A(??) b) d) d.
5 Hybrid grammars
We couple derivations in two grammars in a way similar to how this is commonly done for synchronous
grammars, namely by indexed symbols. However, we apply the mechanism not only to derivational
nonterminals but also to terminals.
Let ? be a ranked alphabet. We define the ranked set I(?) = {?
u
| ? ? ?, u ? N
+
}, with rk
I(?)
(?
u
)
= rk
?
(?). Let ? be another ranked alphabet (? ? ? = ?) and Y ? X , with X as in Section 2. We let
I
?
?,?
(Y ) be the set of all s-terms s ? T
?
I(?)??
(Y ) in which each index occurs at most once.
For an s-term s, let ind(s) be the set of all indices occurring in s. The deindexing function D removes
all indices from an s-term s ? I
?
?,?
(Y ) to obtain D(s) ? T
?
???
(Y ). The set I
?,?
(Y ) ? T
I(?)??
(Y ) of
terms with indexed symbols is defined much as above. We let I
?
?,?
= I
?
?,?
(?) and I
?,?
= I
?,?
(?).
A LCFRS/sDCP hybrid grammar (HG) is a tuple G = ((N
1
, S
1
,?), (N
2
, S
2
,?), P ), subject to the
following restrictions. The objects ? and ? are ranked alphabets with ? = ?
(0)
and ? = ?
(1)
. As mere
sets of symbols, we demand ? ? ? but the rank functions associated with ? and ? differ. Let ? be the
ranked alphabet ? \ ?, with rk
?
(?) = 1 for ? ? ?.
The hybrid rules in P are of the form [?
1
, ?
2
] where ?
1
has the form in Equation (1) of an LCFRS
rule except that s
i
? I
?
?,?
(X
m
n
) (i ? [k
0
]) and A
i
? I(N
1
) (i ? [n]) and each index in ?
1
occurs
exactly once, and ?
2
has the form in Equation (2) of a sDCP rule except that the s-terms in s
(0)
1,k
?
0
and
s
(i)
1,k
i
(i ? [n]) are in I
?
?,?
(X
m
) and A
i
? I(N
2
) (i ? [n]) and each index in ?
2
occurs exactly once. We
require that ind(?
1
) = ind(?
2
) and each index either couples a pair of identical terminals or couples a
pair of (possibly distinct) nonterminals.
2
The term ?simple? here has a more restrictive meaning than the term with the same name in (Deransart and Ma?uszynski,
1985).
1374
Let P
1
and P
2
be the sets of all D(?
1
) and D(?
2
), respectively, of some hybrid rule [?
1
, ?
2
]. Then we
refer to the LCFRS (N
1
, S
1
,?, P
1
) and the sDCP (N
2
, S
2
,?, P
2
) as the first and second components,
respectively, of G.
In order to define the ?derives? relation?
G
, we need rule instantiation as before, in combination with
reindexing, which is a common notion for synchronous grammars. This allows specification of a set of
pairs [s
1
, s
2
] ? I
?
?,?
? I
?
?,?
which are such that [?S
1
1
(s
1
)?, ?S
1
2
(s
2
)?] ?
?
G
[??, ??]. For each such pair
we can construct a hybrid tree (s,?
s
) over (?,?), where s = D(s
2
), and ?
s
is defined as follows. If
there is a combination of positions p
1
, p
?
1
, p
2
, p
?
2
such that at p
1
in s
1
we find the same label as at p
2
in
s
2
(this label must then be in I(?)), and at p
?
1
in s
1
we find the same label as at p
?
2
in s
2
, and p
1
occurs
to the left of p
?
1
, then p
2
?
s
p
?
2
. The language induced by G is defined as the set of all such hybrid trees.
Given an input string, the desired hybrid trees can be effectively enumerated. To be exact, after
construction of the parse table by a LCFRS parser, which takes polynomial time in the length of the
string, synchronous derivations can be enumerated. Extracting a single derivation from the table requires
linear time in the size of that derivation. Given a derivation, an s-term can be constructed in linear time
in the size of that derivation, applying sDCP rules in the second component. This s-term, in combination
with the input string and the indices linking the two is then easily extended to a hybrid tree as outlined
above.
Example 3
The hybrid tree
[VP(x
1
x
2
x
3
)? V
1
(x
1
, x
3
) ADV
2
(x
2
),VP(VP(x
1
x
2
))? V
1
(x
1
) ADV
2
(x
2
)]
[V(h
1
, g
2
)? ??,V(V(h
1
g
2
))? ??]
[ADV(s
1
)? ??,ADV(ADV(s
1
))? ??]
in Figure 1 is ob-
tained by the HG
on the right. (All
arguments in the
second component are synthesized.) We derive:
[VP
1
(h
2
s
3
g
4
),VP
1
(VP(V(h
2
g
4
) ADV(s
3
)))]?
[V
1
(h
2
, g
4
) ADV
5
(s
3
),V
1
(V(h
2
g
4
)) ADV
5
(ADV(s
3
))]?
[ADV
5
(s
3
), ADV
5
(ADV(s
3
))]? [??, ??]
Note that in the LCFRS that
[VP(x
1
)? V
1
(x
1
), VP(VP(x
1
))? V
1
(x
1
)]
[V(h
1
x
1
g
2
)? ADV
3
(x
1
), V(V(h
1
g
2
) x
1
)? ADV
3
(x
1
)]
[ADV(s
1
)? ??, ADV(ADV(s
1
))? ??]
is the first component of the HG
above, nonterminal V has rank 2.
On the right is an alternative HG
deriving the same hybrid tree, but
now with all LCFRS nonterminals having rank 1, by which we obtain a syntactic variant of a CFG. Yet
another HG for the same hybrid tree will be discussed in the next section, where we will see that the first
and second components can be disconnected even further, departing from the traditional way of LCFRS
parsing.
Example 4
Hybrid trees as in Figure 2
[A(x
1
x
2
)? S
1
(x
1
, x
2
), A(x
1
)? S
1
(x
1
)]
[S(a
1
x
1
,b
2
x
2
)? S
3
(x
1
, x
2
), S(S(a
1
x
1
b
2
)? S
3
(x
1
)]
[S(??, ??)? ??, S(??)? ??]
can be obtained by the HG on
the right.
6 Grammar induction
We define a recursive partitioning of a string s = ?
1
? ? ??
n
as a tree whose nodes are labeled with
subsets of [n]. The root is labeled with [n]. Each leaf is labeled with a single element of [n]. Each
internal node is labeled with the union of the labels of its children, which furthermore must be disjoint.
We say a subset of [n] has fanout k if k is the smallest number such that it can be written as the union of
k sets of consecutive numbers.
1375
A derivation of an LCFRS relates straightforwardly to a recursive partitioning. Consider for example
the derivation of string h s g by the LCFRS that is the first component of the first HG in Example 3.
The root would be labeled {1, 2, 3}, with children labeled {1, 3} and {2}. The node labeled {1, 3} has
children labeled {1} and {3}. The fanout of {1, 3} is 2, whereas it is 1 for all other node labels. One
may also extract a recursive partitioning directly from a hybrid tree, by associating each node with the
set of positions of terminals that it dominates. For example, Figure 1 gives rise to the same recursive
partitioning as the one mentioned above.
One central observation of this paper is that for any hybrid tree h = (s,?
s
) and any recursive par-
titioning of str(h), not necessarily extracted from h, we can construct a hybrid grammar G allowing a
derivation of h, and moreover, the first (LCFRS) component of that derivation parses str(h) according to
the given recursive partitioning. This observation holds for both dependency structures and constituent
structures. The proof for dependency structures is quite technical however, and requires that the second
(sDCP) component of a hybrid grammar has rules with inherited arguments. For lack of space, we can
only give an outline for constituent structures, or in other words, we consider only input hybrid trees over
(?,?) where labels from ? occur exclusively at the leaves. In the resulting hybrid grammars, all sDCP
rules will have only synthesized arguments.
The intuition is the following. For each node of the given recursive partitioning, the numbers in its
label correspond to leaves of s, for the given hybrid tree h = (s,?
s
). There is a smallest number of
maximal disjoint subtrees in s that together contain all those leaves and no others. If we now relate a
parent node of the recursive partitioning to its child nodes, then we see that the relevant disjoint subtrees
in s for the children can be combined to give the relevant disjoint subtrees for the parent, possibly adding
further internal nodes. This process can be expressed in terms of a hybrid rule. Each pair consisting of
a hybrid tree and a recursive partitioning gives rise to a number of hybrid rules. For a collection of such
pairs, we can combine all the rules into a hybrid grammar.
Example 5 Consider again the hybrid tree in Figure 1, in combination with a recursive partitioning
whose root has children labeled {1, 2} and {3}. The relevant disjoint subtrees for {1, 2} are hat and
ADV(schnell) and for {3} there is the subtree gearbeitet. (In a real-world grammar we would have
parts of speech occurring above all the words.) An appropriate hybrid rule that both respects the recursive
partitioning (by the first component LCFRS rule) and puts together relevant parts of the hybrid tree (by
the second component sDCP rule) would be of the form:
[A(x
1
x
2
)? B
1
(x
1
) C
2
(x
2
), A(VP(V(x
1
x
3
)x
2
))? B
1
(x
1
, x
2
) C
2
(x
3
)]
Here A, B and C should to be chosen to be consistent with neighboring nodes in the recursive partition-
ing, to be discussed next. An alternative recursive partitioning whose root has children labeled {1, 3}
and {2} leads to the first hybrid rule in Example 3 (apart from nonterminal names).
We have experimented with two ways of naming nonterminals in the derived hybrid rules. The first
encodes the list of labels of the roots of the relevant disjoint subtrees. In the above example, we would
have a name such as ?hat,ADV? for A. For fanout greater than 1, the locations of the ?gaps? are ex-
plicitly indicated. For example, we might have ?hat, gap, gearbeitet?. We will call this strict labeling.
The second, and less precise, way is to replace lists of labels of siblings by a single name of the form
children-of(X), where X is the label of the parent. We will call this child labeling.
Because our construction of hybrid grammars works for all recursive partitionings, there is no need to
limit ourselves to those extracted directly from the hybrid trees. Moreover, a given recursive partitioning
can be transformed into a similar but different one in which fanout is restricted to some given value
k ? 1. One possible procedure is to start at the root. If the label J of the present node is a singleton,
then we stop. Otherwise, we search breadth-first through the subtree of the present node to identify a
descendant such that both its label J
?
and J \J
?
have fanout not exceeding k. (It is easy to see such a node
always exists: ultimately breadth-first search will reach the leaves, which are labeled with singletons.)
The present node is now given two children, the first is the node labeled J
?
that we identified above, and
the second is a copy of the present subtree, but with J
?
subtracted from the label of every node. (Nodes
1376
labeled with the empty set are removed, and if a node has the same label as its parent then the two are
collapsed.) We repeat the procedure for both children recursively. Note that with k = 1, we can induce
a ?CFG/sDCP? hybrid grammar, that is, with the first component having fanout 1.
Example 6
The recursive partition-
{1, 2, 3, 5, 6, 7}
{1, 3, 6, 7}
{1, 6}
{1} {6}
{3, 7}
{3} {7}
{2, 5}
{2} {5}
=?
{1, 2, 3, 5, 6, 7}
{3, 7}
{3} {7}
{1, 2, 5, 6}
{1, 6}
{1} {6}
{2, 5}
{2} {5}
Figure 3: Transformation of recursive partitioning to restrict fanout to 2.
ing in the left half of Fig-
ure 3 has a node labeled
{1, 3, 6, 7}, with fanout 3.
With J = {1, 2, 3, 5, 6, 7}
and k = 2, one possible
choice for J
?
is {3, 7}, as
then both J
?
and J \ J
?
=
{1, 2, 5, 6} have fanout not
exceeding 2. This leads to
the partitioning in the right
half of the figure. Because now all node labels have fanout not exceeding 2, recursive traversal will make
no further changes. Other valid choices for J
?
would be {2} and {5}. Not a valid choice for J
?
would be
{1, 6}, as J \ {1, 6} = {2, 3, 5, 7}, which has fanout 3.
Our procedure ensures that subsequent grammar induction leads to binary grammars. Note that this
contrasts with binarization algorithms (G?omez-Rodr??guez and Satta, 2009; G?omez-Rodr??guez et al.,
2009) that are applied after a grammar is obtained. Unlike (van Cranenburgh, 2012), our objective is
not to obtain a ?coarse? grammar for the purpose of coarse-to-fine parsing.
In experiments we also considered the right-branching partitioning, whose internal node labels are
{m,m+ 1, . . . , n}, with children labeled {m} and {m+ 1, . . . , n}. Similarly, there is a left-branching
recursive partitioning. In this way, we can induce a ?FA/sDCP? hybrid grammar, with the first component
having finite-state power, which means we can parse in linear time.
7 Experiments
The theory developed above shows that hybrid grammars allow considerable flexibility in the first com-
ponent, leading to a wide range of different time complexities of parsing while, at least potentially, the
same kinds of discontinuous structures can be obtained. We have run experiments to measure what
impact different choices of the first component have on recall/precision and the degree of discontinuity.
The training data consisted of the first 7000 trees of the TIGER treebank (Brants et al., 2004). From
these, recursive partitionings were straightforwardly obtained, and transformed for different values of k.
Also the left-branching and right-branching recursive partitionings were considered. Hybrid grammars
were then extracted using strict or child labeling. Probabilities of rules were determined by relative
frequency estimation, without any smoothing techniques.
Test sentences were taken from the next 500 trees, excluding sentences of length greater than 20 and
those where a single tree did not span the entire sentence, leaving 324 sentences. Parsing was on (gold
standard) parts of speech rather than words. All punctuation was ignored. Labeled recall, precision and
F-measure were computed on objects each consisting of the label of a node and a sequence of pairs of
input positions delimiting substrings covered by that node. The algorithms were implemented in Python
and the experiments were carried out on a desktop with four 3.1GHz Intel Core i5 CPUs.
Results are reported in Table 1. The choice of k = 1 can be seen as a baseline, the first component
then being restricted to context-free power. Note that k = 1, 2, 3 imply parsing complexities O(n
3
),
O(n
6
), O(n
9
), respectively.
In the case of strict labeling, the change from k = 1 to k = 2 leads to significant changes in running
time, but that from k = 2 to k = 3 less so, which can be explained by the smaller number of constituents
that have two gaps, compared to those with zero or one gap. There was no significant change, neither in
running time nor in F-measure, for values of k greater than 4, and therefore these values were omitted
1377
here. Note that for k = ? one would obtain the conventional technique of discontinuous parsing using
LCFRSs. For the right-branching recursive partitionings, the running time is significantly higher than
that for the left-branching ones, although it is linear-time in both cases. This is due to the directional bias
of the implemented parsing strategy. In order to allow a straightforward comparison we have taken the
same parsing strategy in all cases. Note the large number of parse failures for the right-branching and
left-branching partitionings, which is explained by the large number of very specific nonterminals.
Child labeling leads to much smaller
fail R P F1 # gaps secs
strict labeling
k = 1 16 73.0 70.4 71.2 0.0075 442
k = 2 12 73.1 70.7 71.4 0.0111 2,580
k = 3 12 73.1 70.7 71.4 0.0121 2,942
k = 4 12 73.1 70.7 71.4 0.0127 2,828
r-branch 151 65.6 62.4 63.2 0.0118 775
l-branch 266 82.0 78.9 79.5 0.0124 24
child labeling
k = 1 4 74.3 74.2 73.9 0.0120 939
k = 2 4 75.0 75.1 74.7 0.0125 58,164
r-branch 15 73.1 73.0 72.6 0.0117 319
l-branch 56 75.7 76.6 75.7 0.0114 183
Table 1: Number of parse failures, recall, precision, F-
measure, average number of gaps per constituent, and run-
ning time.
numbers of nonterminals, and thereby
also to more ambiguity, and as a re-
sult the increase from time complexity
O(n
3
) to O(n
6
) is more noticeable in
terms of the actual running time. There-
fore carrying out the experiment for k ?
3 was outside our reach. Surprisingly,
the right-branching partitioning performed
very well in this case, with a relatively low
number of parse failures, F-measure com-
peting with k = 1, 2, 3, 4 and strict label-
ing, although it is clearly worse than that
with k = 1, 2 and child labeling, and run-
ning time smaller than in the case of any of
the hybrid grammars where the first com-
ponent has power beyond that of finite au-
tomata.
Child labeling generally gave better F-measure than strict labeling (ignoring strict labeling and left-
branching partitioning, where the many parse failures distort the recall and precision). This seems to be
due to the more accurate parameter estimation that was possible for the smaller numbers of rules obtained
with child labeling.
The differences in F-measure are relatively small for varying k. This can be explained by the relatively
small portion of discontinuous structures in the test set. We have looked closer at discontinuity in the
test set in two ways. First, we measured the average number of gaps per constituent, which in the gold
standard was 0.0171. None of the hybrid grammars came close to achieving this, but we do observe
that more discontinuity is obtained for higher values of k. Secondly, we reran the experiments for only
the 75 sentences out of the aforementioned 324 where the gold structure had at least one discontinuous
phrase. For this smaller set, F1 increases from 59.5 (k = 1) to 61.9 (k = 2, 3, 4) for strict labeling, and
it increases from 64.4 (k = 1) to 66.5 (k = 2) for child labeling. This suggests that with higher k, the
additional discontinuous structures found have at least some overlap with those of the gold standard. Note
again that there is no a priori bound on the fanout of produced hybrid trees, even when the first component
has finite-state power, but the ability to abstract away from discontinuous structures in the training set
seems to be enhanced if the first component is more powerful. This is consistent with observations made
by (van Cranenburgh, 2012).
8 Limitations
The theory from Section 6 does not necessarily mean that any language of hybrid trees can be induced
by a HG whose first-component LCFRS has arbitrarily low fanout. We illustrate this by means of the
language of hybrid trees generated by the HG of Example 4, in which the LCFRS has fanout 2. No
CFG/sDCP grammar in fact exists for the same language, or in other words, the fanout of the first-
component LCFRS cannot be reduced to 1, regardless of how we choose the second-component sDCP.
For a proof, assume that a CFG/sDCP grammar does exist. Letm be the maximum number of members
in the right-hand side of any CFG rule. Let k be the maximum rank of any nonterminal in the second-
component sDCP. Now consider a CFG/sDCP derivation for a hybrid tree with yield a
n
b
n
, where n ?
1378
2 ? k ?m. In a top-down traversal, identify the first CFG nonterminal occurrence that covers a substring
of the input string that has a length smaller than or equal to n/2 and greater than k. This substring
may contain occurrences of a and of b, but because its length is at most n/2, there will not be any pair
consisting of an occurrence of a and an occurrence of b that are both part of that substring, and that
have a common parent labeled S in the hybrid tree. This means that more than k tree fragments or tree
nodes with missing child nodes are involved, which translate to more than k synthesized or inherited
arguments, contradicting the assumptions.
9 Conclusions
We have presented hybrid grammars as a novel framework for describing languages of discontinuous
syntactic structures. This framework sheds light on the relation between various existing techniques, but
it also offers potential for development of novel techniques. Much of what we have shown is merely
an illustration of particular instances of this framework. For example, next to the hybrid grammars
discussed here, we can consider those with macro grammars as first component, or simple context-
free tree grammars as second component. Many variations exist on the illustrated grammar induction
technique. For example, next to our strict labeling and child labeling, one can consider approaches using
latent variables, combined with expectation-maximization.
Acknowledgments
We thank the anonymous reviewers for many helpful comments.
References
T. Becker, A.K. Joshi, and O. Rambow. 1991. Long-distance scrambling and Tree Adjoining Grammars. In Fifth
EACL, pages 21?26.
G.V. Bochmann. 1976. Semantic evaluation from left to right. Communications of the ACM, 19(2):55?62.
A. B?ohmov?a, J. Haji?c, E. Haji?cov?a, and B. Hladk?a. 2000. The Prague dependency treebank: A tree-level anno-
tation scenario. In A. Abeill?e, editor, Treebanks: Building and using syntactically annotated corpora, pages
103?127. Kluwer, Dordrecht.
A. Boyd. 2007. Discontinuity revisited: An improved conversion to context-free representations. In Linguistic
Annotation Workshop, at ACL 2007, pages 41?44.
W.S. Brainerd. 1969. Tree generating regular systems. Information and Control, 14:217?231.
S. Brants, S. Dipper, P. Eisenberg, S. Hansen-Schirra, E. K?onig, W. Lezius, C. Rohrer, G. Smith, and H. Uszkoreit.
2004. TIGER: Linguistic interpretation of a German corpus. Research on Language and Computation, 2:597?
620.
J. Bresnan, R.M. Kaplan, S. Peters, and A. Zaenen. 1982. Cross-serial dependencies in Dutch. Linguistic Inquiry,
13(4):613?635.
H. Bunt. 1996. Formal tools for describing and processing discontinuous constituency structure. In H. Bunt and
A. van Horck, editors, Discontinuous Constituency, pages 63?84. Mouton de Gruyter.
R. Campbell. 2004. Using linguistic principles to recover empty categories. In 42nd Annual Meeting of the ACL,
pages 645?652.
J.D. Choi and M. Palmer. 2010. Robust constituent-to-dependency conversion for English. In Ninth International
Workshop on Treebanks and Linguistic Theories, pages 55?66.
P. Deransart and J. Ma?uszynski. 1985. Relating logic programs and attribute grammars. Journal of Logic Pro-
gramming, 2:119?155.
K. Evang and L. Kallmeyer. 2011. PLCFRS parsing of English discontinuous constituents. In 12th International
Conference on Parsing Technologies, pages 104?116.
1379
M.J. Fischer. 1968. Grammars with macro-like productions. In IEEE Conference Record of 9th Annual Sympo-
sium on Switching and Automata Theory, pages 131?142.
R. Gabbard, S. Kulick, and M. Marcus. 2006. Fully parsing the Penn Treebank. In Human Language Technology
Conference of the NAACL, Main Conference, pages 184?191.
F. G?ecseg and M. Steinby. 1997. Tree languages. In G. Rozenberg and A. Salomaa, editors, Handbook of Formal
Languages, Vol. 3, chapter 1, pages 1?68. Springer, Berlin.
R. Giegerich. 1988. Composition and evaluation of attribute coupled grammars. Acta Informatica, 25:355?423.
C. G?omez-Rodr??guez and G. Satta. 2009. An optimal-time binarization algorithm for linear context-free rewriting
systems with fan-out two. In 47th ACL and 4th International Joint Conference on Natural Language Processing
of the AFNLP, pages 985?993.
C. G?omez-Rodr??guez, M. Kuhlmann, G. Satta, and D. Weir. 2009. Optimal reduction of rule length in linear
context-free rewriting systems. In Human Language Technologies: The 2009 Annual Conference of the North
American Chapter of the ACL, pages 539?547.
M. Johnson. 2002. A simple pattern-matching algorithm for recovering empty nodes and their antecedents. In
40th ACL, pages 136?143.
S. Kahane, A. Nasr, and O. Rambow. 1998. Pseudo-projectivity, a polynomially parsable non-projective depen-
dency grammar. In 36th ACL and 17th International Conference on Computational Linguistics, volume 1, pages
646?652.
K. Kallmeyer and M. Kuhlmann. 2012. A formal model for plausible dependencies in lexicalized tree adjoining
grammar. In Eleventh International Workshop on Tree Adjoining Grammar and Related Formalisms, pages
108?116.
L. Kallmeyer and W. Maier. 2013. Data-driven parsing using probabilistic linear context-free rewriting systems.
Computational Linguistics, 39(1):87?119.
A. Kathol and C. Pollard. 1995. Extraposition via complex domain formation. In 33rd ACL, pages 174?180.
M. Kuhlmann. 2013. Mildly non-projective dependency grammar. Computational Linguistics, 39(2):355?387.
W. Lu, H.T. Ng, W.S. Lee, and L.S. Zettlemoyer. 2008. A generative model for parsing natural language to
meaning representations. In Conference on Empirical Methods in Natural Language Processing, pages 783?
792.
W. Maier and T. Lichte. 2009. Characterizing discontinuity in constituent treebanks. In P. de Groote, M. Egg,
and L. Kallmeyer, editors, 14th Conference on Formal Grammar, volume 5591 of Lecture Notes in Artificial
Intelligence, Bordeaux, France.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The
Penn treebank. Computational Linguistics, 19(2):313?330.
J.D. McCawley. 1982. Parentheticals and discontinuous constituent structure. Linguistic Inquiry, 13(1):91?106.
R. McDonald and F. Pereira. 2006. Online learning of approximate dependency parsing algorithms. In 11th
EACL, pages 81?88.
S. M?uller. 2004. Continuous or discontinuous constituents? a comparison between syntactic analyses for con-
stituent order and their processing systems. Research on Language and Computation, 2:209?257.
J. Nivre and J. Nilsson. 2005. Pseudo-projective dependency parsing. In 43rd ACL, pages 99?106.
J. Nivre. 2009. Non-projective dependency parsing in expected linear time. In Joint Conference of the 47th ACL
and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 351?359.
O. Rambow and A.K. Joshi. 1997. A formal look at dependency grammars and phrase structure grammars with
special consideration of word-order phenomena. In L. Wenner, editor, Recent Trends in Meaning-Text Theory.
John Benjamin.
O. Rambow. 2010. The simple truth about dependency and phrase structure representations: An opinion piece.
In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL,
Main Conference, pages 337?340.
1380
W.C. Rounds. 1970. Mappings and grammars on trees. Mathematical Systems Theory, 4:257?287.
G. Satta and E. Peserico. 2005. Some computational complexity results for synchronous context-free grammars.
In Human Language Technology Conference and Conference on Empirical Methods in Natural Language Pro-
cessing, pages 803?810.
S. Seifert and I. Fischer. 2004. Parsing string generating hypergraph grammars. In H. Ehrig, G. Engels, F. Parisi-
Presicce, and G. Rozenberg, editors, 2nd International Conference on Graph Transformations, volume 3256 of
Lecture Notes in Computer Science, pages 352?267. Springer-Verlag.
H. Seki and Y. Kato. 2008. On the generative power of multiple context-free grammars and macro grammars.
IEICE Transactions on Information and Systems, E91-D:209?221.
H. Seki, T. Matsumura, M. Fujii, and T. Kasami. 1991. On multiple context-free grammars. Theoretical Computer
Science, 88:191?229.
S.M. Shieber and Y. Schabes. 1990. Synchronous tree-adjoining grammars. In Papers presented to the 13th
International Conference on Computational Linguistics, volume 3, pages 253?258.
S.M. Shieber. 1985. Evidence against the context-freeness of natural language. Linguistics and Philosophy,
8(3):333?343.
K. Sima?an, R. Bod, S. Krauwer, and R. Scha. 1994. Efficient disambiguation by means of stochastic tree
substitution grammars. In International Conference on New Methods in Language Processing, pages 50?58.
S. Stucky. 1987. Configurational variation in English. In G.J. Huck and A.E. Ojeda, editors, Discontinuous
Constituency, volume 20 of Syntax and Semantics, pages 377?404. Academic Press.
A. van Cranenburgh. 2012. Efficient parsing with linear context-free rewriting systems. In 13th EACL, pages
460?470.
K. Vijay-Shankar and A.K. Joshi. 1985. Some computational properties of tree adjoining grammars. In 23rd ACL,
pages 82?93.
K. Vijay-Shanker, D.J. Weir, and A.K. Joshi. 1987. Characterizing structural descriptions produced by various
grammatical formalisms. In 25th ACL, pages 104?111.
1381
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1213?1221,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Computation of Infix Probabilities
for Probabilistic Context-Free Grammars
Mark-Jan Nederhof
School of Computer Science
University of St Andrews
United Kingdom
markjan.nederhof@gmail.com
Giorgio Satta
Dept. of Information Engineering
University of Padua
Italy
satta@dei.unipd.it
Abstract
The notion of infix probability has been intro-
duced in the literature as a generalization of
the notion of prefix (or initial substring) prob-
ability, motivated by applications in speech
recognition and word error correction. For the
case where a probabilistic context-free gram-
mar is used as language model, methods for
the computation of infix probabilities have
been presented in the literature, based on vari-
ous simplifying assumptions. Here we present
a solution that applies to the problem in its full
generality.
1 Introduction
Probabilistic context-free grammars (PCFGs for
short) are a statistical model widely used in natural
language processing. Several computational prob-
lems related to PCFGs have been investigated in
the literature, motivated by applications in model-
ing of natural language syntax. One such problem is
the computation of prefix probabilities for PCFGs,
where we are given as input a PCFG G and a string
w, and we are asked to compute the probability that
a sentence generated by G starts with w, that is, has
w as a prefix. This quantity is defined as the possi-
bly infinite sum of the probabilities of all strings of
the form wx, for any string x over the alphabet of G.
The problem of computation of prefix probabili-
ties for PCFGs was first formulated by Persoon and
Fu (1975). Efficient algorithms for its solution have
been proposed by Jelinek and Lafferty (1991) and
Stolcke (1995). Prefix probabilities can be used to
compute probability distributions for the next word
or part-of-speech, when a prefix of the input has al-
ready been processed, as discussed by Jelinek and
Lafferty (1991). Such distributions are useful for
speech recognition, where the result of the acous-
tic processor is represented as a lattice, and local
choices must be made for a next transition. In ad-
dition, distributions for the next word are also useful
for applications of word error correction, when one
is processing ?noisy? text and the parser recognizes
an error that must be recovered by operations of in-
sertion, replacement or deletion.
Motivated by the above applications, the problem
of the computation of infix probabilities for PCFGs
has been introduced in the literature as a generaliza-
tion of the prefix probability problem. We are now
given a PCFG G and a string w, and we are asked
to compute the probability that a sentence generated
by G has w as an infix. This probability is defined
as the possibly infinite sum of the probabilities of
all strings of the form xwy, for any pair of strings x
and y over the alphabet of G. Besides applications
in computation of the probability distribution for the
next word token and in word error correction, in-
fix probabilities can also be exploited in speech un-
derstanding systems to score partial hypotheses in
algorithms based on beam search, as discussed by
Corazza et al (1991).
Corazza et al (1991) have pointed out that the
computation of infix probabilities is more difficult
than the computation of prefix probabilities, due to
the added ambiguity that several occurrences of the
given infix can be found in a single string generated
by the PCFG. The authors developed solutions for
the case where some distribution can be defined on
1213
the distance of the infix from the sentence bound-
aries, which is a simplifying assumption. The prob-
lem is also considered by Fred (2000), which pro-
vides algorithms for the case where the language
model is a probabilistic regular grammar. However,
the algorithm in (Fred, 2000) does not apply to cases
with multiple occurrences of the given infix within
a string in the language, which is what was pointed
out to be the problematic case.
In this paper we adopt a novel approach to the
problem of computation of infix probabilities, by re-
moving the ambiguity that would be caused by mul-
tiple occurrences of the given infix. Although our
result is obtained by a combination of well-known
techniques from the literature on PCFG parsing and
pattern matching, as far as we know this is the first
algorithm for the computation of infix probabilities
that works for general PCFG models without any re-
strictive assumption.
The remainder of this paper is structured as fol-
lows. In Section 2 we explain how the sum of the
probabilities of all trees generated by a PCFG can
be computed as the least fixed-point solution of a
non-linear system of equations. In Section 3 we re-
call the construction of a new PCFG out of a given
PCFG and a given finite automaton, such that the
language generated by the new grammar is the in-
tersection of the languages generated by the given
PCFG and the automaton, and the probabilities of
the generated strings are preserved. In Section 4
we show how one can efficiently construct an un-
ambiguous finite automaton that accepts all strings
with a given infix. The material from these three
sections is combined into a new algorithm in Sec-
tion 5, which allows computation of the infix prob-
ability for PCFGs. This is the main result of this
paper. Several extensions of the basic technique are
discussed in Section 6. Section 7 discusses imple-
mentation and some experiments.
2 Sum of probabilities of all derivations
Assume a probabilistic context-free grammar G, rep-
resented by a 5-tuple (?, N, S, R, p), where ? and
N are two finite disjoint sets of terminals and non-
terminals, respectively, S ? N is the start symbol,
R is a finite set of rules, each of the form A ? ?,
whereA ? N and ? ? (??N)?, and p is a function
from rules in R to real numbers in the interval [0, 1].
The concept of left-most derivation in one step is
represented by the notation ? pi?G ?, which means
that the left-most occurrence of any nonterminal in
? ? (? ? N)? is rewritten by means of some rule
pi ? R. If the rewritten nonterminal is A, then pi
must be of the form (A ? ?) and ? is the result
of replacing the occurrence of A in ? by ?. A left-
most derivation with any number of steps, using a
sequence d of rules, is denoted as ? d?G ?. We omit
the subscript G when the PCFG is understood. We
also write ? ?? ? when the involved sequence of
rules is of no relevance. Henceforth, all derivations
we discuss are implicitly left-most.
A complete derivation is either the empty se-
quence of rules, or a sequence d = pi1 ? ? ?pim, m ?
1, of rules such that A d? w for some A ? N and
w ? ??. In the latter case, we say the complete
derivation starts with A, and in the former case, with
d an empty sequence of rules, we assume the com-
plete derivation starts and ends with a single termi-
nal, which is left unspecified. It is well-known that
there exists a bijective correspondence between left-
most complete derivations starting with nonterminal
A and parse trees derived by the grammar with root
A and a yield composed of terminal symbols only.
The depth of a complete derivation d is the length
of the longest path from the root to a leaf in the parse
tree associated with d. The length of a path is de-
fined as the number of nodes it visits. Thus if d = pi
for some rule pi = (A ? w) with w ? ??, then the
depth of d is 2.
The probability p(d) of a complete derivation d =
pi1 ? ? ?pim, m ? 1, is:
p(d) =
m?
i=1
p(pii).
We also assume that p(d) = 1 when d is an empty
sequence of rules. The probability p(w) of a string
w is the sum of all complete derivations deriving that
string from the start symbol:
p(w) =
?
d: S d?w
p(d).
With this notation, consistency of a PCFG is de-
1214
fined as the condition:
?
d,w: S d?w
p(d) = 1.
In other words, a PCFG is consistent if the sum
of probabilities of all complete derivations starting
with S is 1. An equivalent definition of consistency
considers the sum of probabilities of all strings:
?
w
p(w) = 1.
See (Booth and Thompson, 1973) for further discus-
sion.
In practice, PCFGs are often required to satisfy
the additional condition:
?
pi=(A??)
p(pi) = 1,
for each A ? N . This condition is called proper-
ness. PCFGs that naturally arise by parameter es-
timation from corpora are generally consistent; see
(Sa?nchez and Bened??, 1997; Chi and Geman, 1998).
However, in what follows, neither properness nor
consistency is guaranteed.
We define the partition function of G as the func-
tion Z that assigns to each A ? N the value
Z(A) =
?
d,w
p(A d? w). (1)
Note that Z(S) = 1 means that G is consistent.
More generally, in later sections we will need to
compute the partition function for non-consistent
PCFGs.
We can characterize the partition function of a
PCFG as a solution of a specific system of equa-
tions. Following the approach in (Harris, 1963; Chi,
1999), we introduce generating functions associated
with the nonterminals of the grammar. For A ? N
and ? ? (N ? ?)?, we write f(A,?) to denote the
number of occurrences of symbol A within string ?.
Let N = {A1, A2, . . . , A|N |}. For each Ak ? N , let
mk be the number of rules in R with left-hand side
Ak, and assume some fixed order for these rules. For
each i with 1 ? i ? mk, let Ak ? ?k,i be the i-th
rule with left-hand side Ak.
For each k with 1 ? k ? |N |, the generating
function associated with Ak is defined as
gAk(z1, z2, . . . , z|N |) =
mk?
i=1
(
p(Ak ? ?k,i) ?
|N |?
j=1
zf(Aj ,?k,i)j
)
. (2)
Furthermore, for each i ? 1 we recursively define
functions g(i)Ak(z1, z2, . . . , z|N |) by
g(1)Ak (z1, z2, . . . , z|N |) = gAk(z1, z2, . . . , z|N |), (3)
and, for i ? 2, by
g(i)Ak(z1, z2, . . . , z|N |) = (4)
gAk( g
(i?1)
A1 (z1, z2, . . . , z|N |),
g(i?1)A2 (z1, z2, . . . , z|N |), . . . ,
g(i?1)A|N| (z1, z2, . . . , z|N |) ).
Using induction it is not difficult to show that, for
each k and i as above, g(i)Ak(0, 0, . . . , 0) is the sum ofthe probabilities of all complete derivations fromAk
having depth not exceeding i. This implies that, for
i = 0, 1, 2, . . ., the sequence of the g(i)Ak(0, 0, . . . , 0)monotonically converges to Z(Ak).
For each k with 1 ? k ? |N | we can now write
Z(Ak) =
= lim
i??
g(i)Ak(0, . . . , 0)
= lim
i??
gAk( g
(i?1)
A1 (0, 0, . . . , 0), . . . ,
g(i?1)A|N| (0, 0, . . . , 0) )
= gAk( limi?? g
(i?1)
A1 (0, 0, . . . , 0), . . . ,
limi?? g(i?1)A|N| (0, 0, . . . , 0) )
= gAk(Z(A1), . . . , Z(A|N |)).
The above shows that the values of the partition
function provide a solution to the system of the fol-
lowing equations, for 1 ? k ? |N |:
zk = gAk(z1, z2, . . . , z|N |). (5)
In the case of a general PCFG, the above equa-
tions are non-linear polynomials with positive (real)
coefficients. We can represent the resulting system
in vector form and write X = g(X). These systems
1215
are called monotone systems of polynomial equa-
tions and have been investigated by Etessami and
Yannakakis (2009) and Kiefer et al (2007). The
sought solution, that is, the partition function, is the
least fixed point solution of X = g(X).
For practical reasons, the set of nonterminals of
a grammar is usually divided into maximal subsets
of mutually recursive nonterminals, that is, for each
A and B in such a subset, we have A ?? uB?
and B ?? vA?, for some u, v, ?, ?. This corre-
sponds to a strongly connected component if we
see the connection between the left-hand side of a
rule and a nonterminal member in its right-hand side
as an edge in a directed graph. For each strongly
connected component, there is a separate system of
equations of the formX = g(X). Such systems can
be solved one by one, in a bottom-up order. That
is, if one strongly connected component contains
nonterminalA, and another contains nonterminalB,
where A ?? uB? for some u, ?, then the system for
the latter component must be solved first.
The solution for a system of equations such as
those described above can be irrational and non-
expressible by radicals, even if we assume that all
the probabilities of the rules in the input PCFG are
rational numbers, as observed by Etessami and Yan-
nakakis (2009). Nonetheless, the partition function
can still be approximated to any degree of preci-
sion by iterative computation of the relation in (4),
as done for instance by Stolcke (1995) and by Ab-
ney et al (1999). This corresponds to the so-called
fixed-point iteration method, which is well-known
in the numerical calculus literature and is frequently
applied to systems of non-linear equations because
it can be easily implemented.
When a number of standard conditions are met,
each iteration of (4) adds a fixed number of bits
to the precision of the solution; see Kelley (1995,
Chapter 4). Since each iteration can easily be im-
plemented to run in polynomial time, this means
that we can approximate the partition function of a
PCFG in polynomial time in the size of the PCFG
itself and in the number of bits of the desired preci-
sion.
In practical applications where large PCFGs are
empirically estimated from data sets, the standard
conditions mentioned above for the polynomial time
approximation of the partition function are usually
met. However, there are some degenerate cases for
which these standard conditions do not hold, result-
ing in exponential time behaviour of the fixed-point
iteration method. This has been firstly observed
in (Etessami and Yannakakis, 2005).
An alternative iterative algorithm for the approx-
imation of the partition function has been proposed
by Etessami and Yannakakis (2009), based on New-
ton?s method for the solution of non-linear systems
of equations. From a theoretical perspective, Kiefer
et al (2007) have shown that, after a certain number
of initial iterations, Newton?s method adds a fixed
number of bits to the precision of the approximated
solution, even in the above mentioned cases in which
the fixed-point iteration method shows exponential
time behaviour. However, these authors also show
that, in some degenerate cases, the number of itera-
tions needed to compute the first bit of the solution
can be at least exponential in the size of the system.
Experiments with Newton?s method for the ap-
proximation of the partition functions of PCFGs
have been carried out in several application-oriented
settings, by Wojtczak and Etessami (2007) and by
Nederhof and Satta (2008), showing considerable
improvements over the fixed-point iteration method.
3 Intersection of PCFG and FA
It was shown by Bar-Hillel et al (1964) that context-
free languages are closed under intersection with
regular languages. Their proof relied on the con-
struction of a new CFG out of an input CFG and
an input finite automaton. Here we extend that con-
struction by letting the input grammar be a proba-
bilistic CFG. We refer the reader to (Nederhof and
Satta, 2003) for more details.
To avoid a number of technical complications, we
assume the finite automaton has no epsilon transi-
tions, and has only one final state. In the context
of our use of this construction in the following sec-
tions, these restrictions are without loss of general-
ity. Thus, a finite automaton (FA) M is represented
by a 5-tuple (?, Q, q0, qf , ?), where ? and Q are
two finite sets of terminals and states, respectively,
q0 is the initial state, qf is the final state, and ? is
a finite set of transitions, each of the form s a7? t,
where s, t ? Q and a ? ?.
A complete computation of M accepting string
1216
w = a1 ? ? ? an is a sequence c = ?1 ? ? ? ?n of tran-
sitions such that ?i = (si?1 ai7? si) for each i (1 ?
i ? n), for some s0, s1, . . . , sn with s0 = q0 and
sn = qf . The language of all strings accepted byM
is denoted by L(M). A FA is unambiguous if at
most one complete computation exists for each ac-
cepted string. A FA is deterministic if there is at
most one transition s a7? t for each s and a.
For a FAM as above and a PCFG G = (?, N, S,
R, p) with the same set of terminals, we construct
a new PCFG G? = (?, N ?, S?, R?, p?), where N ? =
Q? (??N)?Q, S? = (q0, S, qf ), and R? is the set
of rules that is obtained as follows.
? For each A ? X1 ? ? ?Xm in R and each se-
quence s0, . . . , sm with si ? Q, 0 ? i ? m,
and m ? 0, let (s0, A, sm) ? (s0, X1, s1) ? ? ?
(sm?1, Xm, sm) be in R?; if m = 0, the new
rule is of the form (s0, A, s0) ? . Function p?
assigns the same probability to the new rule as
p assigned to the original rule.
? For each s a7? t in ?, let (s, a, t) ? a be in R?.
Function p? assigns probability 1 to this rule.
Intuitively, a rule of G? is either constructed out of
a rule of G or out of a transition of M. On the basis
of this correspondence between rules and transitions
of G?, G and M, it is not difficult to see that each
derivation d? in G? deriving string w corresponds to a
unique derivation d in G deriving the same string and
a unique computation c in M accepting the same
string. Conversely, if there is a derivation d in G
deriving string w, and some computation c in M
accepting the same string, then the pair of d and c
corresponds to a unique derivation d? in G? deriving
the same string w. Furthermore, the probabilities of
d and d? are equal, by definition of p?.
Let us now assume that each string w is accepted
by at most one computation, i.e. M is unambigu-
ous. If a string w is accepted by M, then there are
as many derivations deriving w in G? as there are in
G. If w is not accepted by M, then there are zero
derivations deriving w in G?. Consequently:
?
d?,w:
S? d
?
?G?w
p?(d?) =
?
d,w:
S d?Gw?w?L(M)
p(d),
or more succinctly:
?
w
p?(w) =
?
w?L(M)
p(w).
Note that the above construction of G? is exponen-
tial in the largest value of m in any rule from G. For
this reason, G is usually brought in binary form be-
fore the intersection, i.e. the input grammar is trans-
formed to let each right-hand side have at most two
members. Such a transformation can be realized in
linear time in the size of the grammar. We will return
to this issue in Section 7.
4 Obtaining unambiguous FAs
In the previous section, we explained that unambigu-
ous finite automata have special properties with re-
spect to the grammar G? that we may construct out
of a FA M and a PCFG G. In this section we dis-
cuss how unambiguity can be obtained for the spe-
cial case of finite automata accepting the language
of all strings with given infix w ? ??:
Linfix (w) = {xwy | x, y ? ??}.
Any deterministic automaton is also unambigu-
ous. Furthermore, there seem to be no practical al-
gorithms that turn FAs into equivalent unambiguous
FAs other than the algorithms that also determinize
them. Therefore, we will henceforth concentrate on
deterministic rather than unambiguous automata.
Given a string w = a1 ? ? ? an, a finite automaton
accepting Linfix (w) can be straightforwardly con-
structed. This automaton has states s0, . . . , sn, tran-
sitions s0 a7? s0 and sn a7? sn for each a ? ?, and
transition si?1 ai7? si for each i (1 ? i ? n). The
initial state is s0 and the final state is sn. Clearly,
there is nondeterminism in state s0.
One way to make this automaton deterministic is
to apply the general algorithm of determinization of
finite automata; see e.g. (Aho and Ullman, 1972).
This algorithm is exponential for general FAs. An
alternative approach is to construct a deterministic
finite automaton directly from w, in line with the
Knuth-Morris-Pratt algorithm (Knuth et al, 1977;
Gusfield, 1997). Both approaches result in the same
deterministic FA, which we denote by Iw. However,
the latter approach is easier to implement in such a
1217
way that the time complexity of constructing the au-
tomaton is linear in |w|.
The automaton Iw is described as follows. There
are n + 1 states t0, . . . , tn, where as before n is
the length of w. The initial state is t0 and the final
state is tn. The intuition is that Iw reads a string
x = b1 ? ? ? bm from left to right, and when it has
read the prefix b1 ? ? ? bj (0 ? j ? m), it is in state
ti (0 ? i < n) if and only if a1 ? ? ? ai is the longest
prefix of w that is also a suffix of b1 ? ? ? bj . If the
automaton is in state tn, then this means that w is an
infix of b1 ? ? ? bj .
In more detail, for each i (1 ? i ? n) and each
a ? ?, there is a transition ti?1 a7? tj , where j is
the length of the longest string that is both a prefix
of w and a suffix of a1 ? ? ? ai?1a. If a = ai, then
clearly j = i, and otherwise j < i. To ensure that we
remain in the final state once an occurrence of infix
w has been found, we also add transitions tn a7? tn
for each a ? ?. This construction is illustrated in
Figure 1.
5 Infix probability
With the material developed in the previous sections,
the problem of computing the infix probabilities can
be effectively solved. Our goal is to compute for
given infix w ? ?? and PCFG G = (?, N, S, R,
p):
?infix (w,G) =
?
z?Linfix (w)
p(z).
In Section 4 we have shown the construction of finite
automaton Iw accepting Linfix (w), by which we ob-
tain:
?infix (w,G) =
?
z?L(Iw)
p(z).
As Iw is deterministic and therefore unambiguous,
the results from Section 3 apply and if G? = (?, N ?,
S?, R?, p?) is the PCFG constructed out of G and Iw
then:
?infix (w,G) =
?
z
p?(z).
Finally, we can compute the above sum by applying
the iterative method discussed in Section 2.
6 Extensions
The approach discussed above allows for a number
of generalizations. First, we can replace the infix w
by a sequence of infixes w1, . . . , wm, which have to
occur in the given order, one strictly after the other,
with arbitrary infixes in between:
?island (w1, . . . , wm,G) =?
x0,...,xm???
p(x0w1x1 ? ? ?wmxm).
This problem was discussed before by (Corazza et
al., 1991), who mentioned applications in speech
recognition. Further applications are found in com-
putational biology, but their discussion is beyond the
scope of this paper; see for instance (Apostolico et
al., 2005) and references therein. In order to solve
the problem, we only need a small addition to the
procedures we discussed before. First we construct
separate automata Iwj (1 ? j ? m) as explained in
Section 4. These automata are then composed into
a single automaton I(w1,...,wm). In this composition,
the outgoing transitions of the final state of Iwj , for
each j (1 ? j < m), are removed and that final state
is merged with the initial state of the next automaton
Iwj+1 . The initial state of the composed automaton
is the initial state of Iw1 , and the final state is the
final state of Iwm . The time costs of constructing
I(w1,...,wm) are linear in the sum of the lengths of the
strings wj .
Another way to generalize the problem is to re-
place w by a finite set L = {w1, . . . , wm}. The ob-
jective is to compute:
?infix (L,G) =
?
w?L,x,y???
p(xwy)
Again, this can be solved by first constructing a de-
terministic FA, which is then intersected with G.
This FA can be obtained by determinizing a straight-
forward nondeterministic FA accepting L, or by di-
rectly constructing a deterministic FA along the lines
of the Aho-Corasick algorithm (Aho and Corasick,
1975). Construction of the automaton with the latter
approach takes linear time.
Further straightforward generalizations involve
formalisms such as probabilistic tree adjoining
grammars (Schabes, 1992; Resnik, 1992). The tech-
nique from Section 3 is also applicable in this case,
1218
t0 t1 t2 t3 t4a b a c
b, c a a, b, cc
b, c a
b
Figure 1: Deterministic automaton that accepts all strings over alphabet {a, b, c} with infix abac.
as the construction from Bar-Hillel et al (1964) car-
ries over from context-free grammars to tree ad-
joining grammars, and more generally to the linear
context-free rewriting systems of Vijay-Shanker et
al. (1987).
7 Implementation
We have conducted experiments with the computa-
tion of infix probabilities. The objective was to iden-
tify parts of the computation that have a high time
or space demand, and that might be improved. The
experiments were run on a desktop with a 3.0 GHz
Pentium 4 processor. The implementation language
is C++.
The set-up of the experiments is similar to that in
(Nederhof and Satta, 2008). A probabilistic context-
free grammar was extracted from sections 2-21 of
the Penn Treebank version II. Subtrees that gener-
ated the empty string were systematically removed.
The result was a CFG with 10,035 rules, 28 nonter-
minals and 36 parts-of-speech. The rule probabili-
ties were determined by maximum likelihood esti-
mation. The grammar was subsequently binarized,
to avoid exponential behaviour, as explained in Sec-
tion 3.
We have considered 10 strings of length 7, ran-
domly generated, assuming each of the parts-of-
speech has the same probability. For all prefixes of
those strings from length 2 to length 7, we then com-
puted the infix probability. The duration of the full
computation, averaged over the 10 strings of length
7, is given in the first row of Table 1.
In order to solve the non-linear systems of equa-
tions, we used Broyden?s method. It can be seen
as an approximation of Newton?s method. It re-
quires more iterations, but seems to be faster over-
all, and more scalable to large problem sizes, due to
the avoidance of matrix inversion, which sometimes
makes Newton?s method prohibitively expensive. In
our experiments, Broyden?s method was generally
faster than Newton?s method and much faster than
the simple iteration method by the relation in (4).
For further details on Broyden?s method, we refer
the reader to (Kelley, 1995).
The main obstacle to computation for infixes sub-
stantially longer than 7 symbols is the memory con-
sumption rather than the running time. This is due
to the required square matrices, the dimension of
which is the number of nonterminals. The number
of nonterminals (of the intersection grammar) natu-
rally grows as the infix becomes longer.
As explained in Section 2, the problem is divided
into smaller problems by isolating disjoint sets of
mutually recursive nonterminals, or strongly con-
nected components. We found that for the applica-
tion to the automata discussed in Section 4, there
were exactly three strongly connected components
that contained more than one element, throughout
the experiments. For an infix of length n, these com-
ponents are:
? C1, which consists of nonterminals of the form
(ti, A, tj), where i < n and j < n,
? C2, which consists of nonterminals of the form
(ti, A, tj), where i = j = n, and
? C3, which consists of nonterminals of the form
(ti, A, tj), where i < j = n.
This can be easily explained by looking at the struc-
ture of our automata. See for example Figure 1, with
cycles running through states t0, . . . , tn?1, and cy-
cles through state tn. Furthermore, the grammar ex-
tracted from the Penn Treebank is heavily recursive,
1219
infix length 2 3 4 5 6 7
total running time 1.07 1.95 5.84 11.38 23.93 45.91
Broyden?s method for C1 0.46 0.90 3.42 6.63 12.91 24.38
Broyden?s method for C2 0.08 0.04 0.07 0.04 0.03 0.09
Broyden?s method for C3 0.20 0.36 0.81 1.74 5.30 9.02
Table 1: Running time for infixes from length 2 to length 7. The infixes are prefixes of 10 random strings of length 7,
and reported CPU times (in seconds) are averaged over the 10 strings.
so that almost every nonterminal can directly or in-
directly call any other.
The strongly connected component C2 is always
the same, consisting of 2402 nonterminals, for each
infix of any length. (Note that binarization of the
grammar introduced artificial nonterminals.) The
last three rows of Table 1 present the time costs of
Broyden?s method, for the three strongly connected
components.
The strongly connected componentC3 happens to
correspond to a linear system of equations. This is
because a rule in the intersection grammar with a
left-hand side (ti, A, tj), where i < j = n, must
have a right-hand side of the form (ti, A?, tj), or of
the form (ti, A1, tk) (tk, A2, tj), with k ? n. If k <
n, then only the second member can be in C3. If
k = n, only first member can be in C3. Hence,
such a rule corresponds to a linear equation within
the system of equations for the entire grammar.
A linear system of equations can be solved an-
alytically, for example by Gaussian elimination,
rather than approximated through Newton?s method
or Broyden?s method. This means that the running
times in the last row of Table 1 can be reduced by
treating C3 differently from the other strongly con-
nected components. However, the running time for
C1 dominates the total time consumption.
The above investigations were motivated by two
questions, namely whether any part of the computa-
tion can be precomputed, and second, whether infix
probabilities can be computed incrementally, for in-
fixes that are extended to the left or to the right. The
first question can be answered affirmatively for C2,
as it is always the same. However, as we can see in
Table 1, the computation of C2 amounts to a small
portion of the total time consumption.
The second question can be rephrased more pre-
cisely as follows. Suppose we have computed the
infix probability of a string w and have kept inter-
mediate results in memory. Can the computation of
the infix probability of a string of the form aw orwa,
a ? ?, be computed by relying on the existing re-
sults, so that the computation is substantially faster
than if the computation were done from scratch?
Our investigations so far have not found a posi-
tive answer to this second question. In particular,
the systems of equations for C1 and C3 change fun-
damentally if the infix is extended by one more sym-
bol, which seems to at least make incremental com-
putation very difficult, if not impossible. Note that
the algorithms for the computation of prefix prob-
abilities by Jelinek and Lafferty (1991) and Stolcke
(1995) do allow incrementality, which contributes to
their practical usefulness for speech recognition.
8 Conclusions
We have shown that the problem of infix probabili-
ties for PCFGs can be solved by a construction that
intersects a context-free language with a regular lan-
guage. An important constraint is that the finite
automaton that is input to this construction be un-
ambiguous. We have shown that such an automa-
ton can be efficiently constructed. Once the input
probabilistic PCFG and the FA have been combined
into a new probabilistic CFG, the infix probability
can be straightforwardly solved by iterative algo-
rithms. Such algorithms include Newton?s method,
and Broyden?s method, which was used in our exper-
iments. Our discussion ended with an open question
about the possibility of incremental computation of
infix probabilities.
References
S. Abney, D. McAllester, and F. Pereira. 1999. Relating
probabilistic grammars and automata. In 37th Annual
1220
Meeting of the Association for Computational Linguis-
tics, Proceedings of the Conference, pages 542?549,
Maryland, USA, June.
A.V. Aho and M.J. Corasick. 1975. Efficient string
matching: an aid to bibliographic search. Communi-
cations of the ACM, 18(6):333?340, June.
A.V. Aho and J.D. Ullman. 1972. Parsing, volume 1
of The Theory of Parsing, Translation and Compiling.
Prentice-Hall, Englewood Cliffs, N.J.
A. Apostolico, M. Comin, and L. Parida. 2005. Con-
servative extraction of overrepresented extensible mo-
tifs. In Proceedings of Intelligent Systems for Molecu-
lar Biology (ISMB05).
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On formal
properties of simple phrase structure grammars. In
Y. Bar-Hillel, editor, Language and Information: Se-
lected Essays on their Theory and Application, chap-
ter 9, pages 116?150. Addison-Wesley, Reading, Mas-
sachusetts.
T.L. Booth and R.A. Thompson. 1973. Applying prob-
abilistic measures to abstract languages. IEEE Trans-
actions on Computers, C-22:442?450.
Z. Chi and S. Geman. 1998. Estimation of probabilis-
tic context-free grammars. Computational Linguistics,
24(2):299?305.
Z. Chi. 1999. Statistical properties of probabilistic
context-free grammars. Computational Linguistics,
25(1):131?160.
A. Corazza, R. De Mori, R. Gretter, and G. Satta.
1991. Computation of probabilities for an island-
driven parser. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 13(9):936?950.
K. Etessami and M. Yannakakis. 2005. Recursive
Markov chains, stochastic grammars, and monotone
systems of nonlinear equations. In 22nd International
Symposium on Theoretical Aspects of Computer Sci-
ence, volume 3404 of Lecture Notes in Computer Sci-
ence, pages 340?352, Stuttgart, Germany. Springer-
Verlag.
K. Etessami and M. Yannakakis. 2009. Recursive
Markov chains, stochastic grammars, and monotone
systems of nonlinear equations. Journal of the ACM,
56(1):1?66.
A.L.N. Fred. 2000. Computation of substring proba-
bilities in stochastic grammars. In A. Oliveira, edi-
tor, Grammatical Inference: Algorithms and Applica-
tions, volume 1891 of Lecture Notes in Artificial Intel-
ligence, pages 103?114. Springer-Verlag.
D. Gusfield. 1997. Algorithms on Strings, Trees, and
Sequences. Cambridge University Press, Cambridge.
T.E. Harris. 1963. The Theory of Branching Processes.
Springer-Verlag, Berlin, Germany.
F. Jelinek and J.D. Lafferty. 1991. Computation of the
probability of initial substring generation by stochas-
tic context-free grammars. Computational Linguistics,
17(3):315?323.
C.T. Kelley. 1995. Iterative Methods for Linear and
Nonlinear Equations. Society for Industrial and Ap-
plied Mathematics, Philadelphia, PA.
S. Kiefer, M. Luttenberger, and J. Esparza. 2007. On the
convergence of Newton?s method for monotone sys-
tems of polynomial equations. In Proceedings of the
39th ACM Symposium on Theory of Computing, pages
217?266.
D.E. Knuth, J.H. Morris, Jr., and V.R. Pratt. 1977. Fast
pattern matching in strings. SIAM Journal on Comput-
ing, 6:323?350.
M.-J. Nederhof and G. Satta. 2003. Probabilistic pars-
ing as intersection. In 8th International Workshop on
Parsing Technologies, pages 137?148, LORIA, Nancy,
France, April.
M.-J. Nederhof and G. Satta. 2008. Computing parti-
tion functions of PCFGs. Research on Language and
Computation, 6(2):139?162.
E. Persoon and K.S. Fu. 1975. Sequential classification
of strings generated by SCFG?s. International Journal
of Computer and Information Sciences, 4(3):205?217.
P. Resnik. 1992. Probabilistic tree-adjoining grammar as
a framework for statistical natural language process-
ing. In Proc. of the fifteenth International Conference
on Computational Linguistics, Nantes, August, pages
418?424.
J.-A. Sa?nchez and J.-M. Bened??. 1997. Consistency
of stochastic context-free grammars from probabilis-
tic estimation based on growth transformations. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 19(9):1052?1055, September.
Y. Schabes. 1992. Stochastic lexicalized tree-adjoining
grammars. In Proc. of the fifteenth International Con-
ference on Computational Linguistics, Nantes, Au-
gust, pages 426?432.
A. Stolcke. 1995. An efficient probabilistic context-free
parsing algorithm that computes prefix probabilities.
Computational Linguistics, 21(2):167?201.
K. Vijay-Shanker, D.J. Weir, and A.K. Joshi. 1987.
Characterizing structural descriptions produced by
various grammatical formalisms. In 25th Annual
Meeting of the Association for Computational Linguis-
tics, Proceedings of the Conference, pages 104?111,
Stanford, California, USA, July.
D. Wojtczak and K. Etessami. 2007. PReMo: an an-
alyzer for Probabilistic Recursive Models. In Tools
and Algorithms for the Construction and Analysis of
Systems, 13th International Conference, volume 4424
of Lecture Notes in Computer Science, pages 66?71,
Braga, Portugal. Springer-Verlag.
1221
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 338?347,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Deterministic Parsing using PCFGs
Mark-Jan Nederhof and Martin McCaffery
School of Computer Science
University of St Andrews, UK
Abstract
We propose the design of deterministic
constituent parsers that choose parser ac-
tions according to the probabilities of
parses of a given probabilistic context-free
grammar. Several variants are presented.
One of these deterministically constructs a
parse structure while postponing commit-
ment to labels. We investigate theoretical
time complexities and report experiments.
1 Introduction
Transition-based dependency parsing (Yamada
and Matsumoto, 2003; Nivre, 2008) has attracted
considerable attention, not only due to its high ac-
curacy but also due to its small running time. The
latter is often realized through determinism, i.e.
for each configuration a unique next action is cho-
sen. The action may be a shift of the next word
onto the stack, or it may be the addition of a de-
pendency link between words.
Because of the determinism, the running time
is often linear or close to linear; most of the time
and space resources are spent on deciding the next
parser action. Generalizations that allow nonde-
terminism, while maintaining polynomial running
time, were proposed by (Huang and Sagae, 2010;
Kuhlmann et al., 2011).
This work has influenced, and has been in-
fluenced by, similar developments in constituent
parsing. The challenge here is to deterministi-
cally choose a shift or reduce action. As in the
case of dependency parsing, solutions to this prob-
lem are often expressed in terms of classifiers of
some kind. Common approaches involve maxi-
mum entropy (Ratnaparkhi, 1997; Tsuruoka and
Tsujii, 2005), decision trees (Wong and Wu, 1999;
Kalt, 2004), and support vector machines (Sagae
and Lavie, 2005).
The programming-languages community rec-
ognized early on that large classes of gram-
mars allow deterministic, i.e. linear-time, pars-
ing, provided parsing decisions are postponed as
long as possible. This has led to (deterministic)
LR(k) parsing (Knuth, 1965; Sippu and Soisalon-
Soininen, 1990), which is a form of shift-reduce
parsing. Here the parser needs to commit to a
grammar rule only after all input covered by the
right-hand side of that rule has been processed,
while it may consult the next k symbols (the
lookahead). LR is the optimal, i.e. most determin-
istic, parsing strategy that has this property. De-
terministic LR parsing has also been considered
relevant to psycholinguistics (Shieber, 1983).
Nondeterministic variants of LR(k) parsing, for
use in natural language processing, have been
proposed as well, some using tabulation to en-
sure polynomial running time in the length of
the input string (Tomita, 1988; Billot and Lang,
1989). However, nondeterministic LR(k) pars-
ing is potentially as expensive as, and possibly
more expensive than, traditional tabular parsing
algorithms such as CKY parsing (Younger, 1967;
Aho and Ullman, 1972), as shown by for exam-
ple (Shann, 1991); greater values of k make mat-
ters worse (Lankhorst, 1991). For this reason, LR
parsing is sometimes enhanced by attaching prob-
abilities to transitions (Briscoe and Carroll, 1993),
which allows pruning of the search space (Lavie
and Tomita, 1993). This by itself is not uncon-
troversial, for several reasons. First, the space of
probability distributions expressible by a LR au-
tomaton is incomparable to that expressible by a
CFG (Nederhof and Satta, 2004). Second, because
an LR automaton may have many more transitions
than rules, more training data may be needed to
accurately estimate all parameters.
The approach we propose here retains some im-
portant properties of the above work on LR pars-
ing. First, parser actions are delayed as long as
338
possible, under the constraint that a rule is com-
mitted to no later than when the input covered by
its right-hand side has been processed. Second, the
parser action that is performed at each step is the
most likely one, given the left context, the looka-
head, and a probability distribution over parses
given by a PCFG.
There are two differences with traditional LR
parsing however. First, there is no explicit repre-
sentation of LR states, and second, probabilities of
actions are computed dynamically from a PCFG
rather than retrieved as part of static transitions.
In particular, this is unlike some other early ap-
proaches to probabilistic LR parsing such as (Ng
and Tomita, 1991).
The mathematical framework is reminiscent of
that used to compute prefix probabilities (Jelinek
and Lafferty, 1991; Stolcke, 1995). One major dif-
ference is that instead of a prefix string, we now
have a stack, which does not need to be parsed. In
the first instance, this seems to make our problem
easier. For our purposes however, we need to add
new mechanisms in order to take lookahead into
consideration.
It is known, e.g. from (Cer et al., 2010; Candito
et al., 2010), that constituent parsing can be used
effectively to achieve dependency parsing. It is
therefore to be expected that our algorithms can be
used for dependency parsing as well. The parsing
steps of shift-reduce parsing with a binary gram-
mar are in fact very close to those of many depen-
dency parsing models. The major difference is,
again, that instead of general-purpose classifiers to
determine the next step, we would rely directly on
a PCFG.
The emphasis of this paper is on deriving the
necessary equations to build several variants of
deterministic shift-reduce parsers, all guided by a
PCFG. We also offer experimental results.
2 Shift-reduce parsing
In this section, we summarize the theory of LR
parsing. As usual, a context-free grammar (CFG)
is represented by a 4-tuple (?, N, S, P ), where
? and N are two disjoint finite sets of terminals
and nonterminals, respectively, S ? N is the start
symbol, and P is a finite set of rules, each of the
form A ? ?, where A ? N and ? ? (? ? N)
?
.
By grammar symbol we mean a terminal or non-
terminal. We use symbols A,B,C, . . . for non-
terminals, a, b, c, . . . for terminals, v, w, x, . . . for
strings of terminals, X for grammar symbols, and
?, ?, ?, . . . for strings of grammar symbols. For
technical reasons, a CFG is often augmented by
an additional rule S
?
? S$, where S
?
/? N and
$ /? ?. The symbol $ acts as an end-of-sentence
marker.
As usual, we have a (right-most) ?derives? re-
lation ?
rm
, ?
?
rm
denotes derivation in zero or
more steps, and ?
+
rm
denotes derivation in one
or more steps. If d is a string of rules pi
1
? ? ?pi
k
,
then ?
d
?
rm
? means that ? can be derived from
? by applying this list of rules in right-most order.
A string ? such that S ?
?
rm
? is called a right-
sentential form.
The last rule A ? ? used in a derivation
S ?
+
rm
? together with the position of (the rel-
evant occurrence of) ? in ? we call the han-
dle of the derivation. In more detail, such a
derivation can be written as S = A
0
?
rm
?
1
A
1
?
1
?
?
rm
?
1
A
1
v
1
?
rm
?
1
?
2
A
2
?
2
v
2
?
?
rm
. . . ?
?
rm
?
1
? ? ??
k?1
A
k?1
v
k?1
? ? ? v
1
?
rm
?
1
? ? ??
k?1
?v
k?1
? ? ? v
1
, where k ? 1, and
A
i?1
? ?
i
A
i
?
i
(1 ? i < k) andA
k?1
? ? are in
P . The underlined symbols are those that are (re-
cursively) rewritten to terminal strings within the
following relation?
rm
or?
?
rm
. The handle here
is A
k?1
? ?, together with the position of ? in
the right-sentential form, just after ?
1
? ? ??
k?1
. A
prefix of ?
1
? ? ??
k?1
? is called a viable prefix in
the derivation.
Given an input string w, a shift-reduce parser
finds a right-most derivation of w, but in reverse
order, identifying the last rules first. It manipulates
configurations of the form (?, v$), where ? is a
viable prefix (in at least one derivation) and v is
a suffix of w. The initial configuration is (?, w$),
where ? is the empty string. The two allowable
steps are (?, av$) ` (?a, v$), which is called a
shift, and (??, v$) ` (?A, v$) where A? ? is in
P , which is called a reduce. Acceptance happens
upon reaching a configuration (S, $).
A 1-item has the form [A ? ? ? ?, a], where
A ? ?? is a rule. The bullet separates the right-
hand side into two parts, the first of which has been
matched to processed input. The symbol a ? ? ?
{$} is called the follower.
In order to decide whether to apply a
shift or reduce after reaching a configuration
(X
1
? ? ?X
k
, w), one may construct the sets I
0
, . . . ,
I
k
, inductively defined as follows, with 0 ? i ? k:
? if S ? ? in P , then [S ? ? ?, $] ? I
0
,
339
? if [A ? ? ? B?, a] ? I
i
, B ? ? in P , and
? ?
?
rm
x, then [B ? ? ?, b] ? I
i
, where
b = 1 : xa,
? if [A ? ? ? X
i
?, a] ? I
i?1
then [A ?
?X
i
? ?, a] ? I
i
.
(The expression 1 : y denotes a if y = az, for
some a and z; we leave it undefined for y = ?.)
Exhaustive application of the second clause above
will be referred to as the closure of a set of items.
It is not difficult to show that if [A? ? ?, a] ?
I
k
, then ? is of the form X
j+1
? ? ?X
k
, some j,
and A ? ? at position j + 1 is the handle of at
least one derivation S ?
?
rm
X
1
? ? ?X
k
ax, some
x. If furthermore a = 1 : w, where 1 : w is
called the lookahead of the current configuration
(X
1
? ? ?X
k
, w), then this justifies a reduce with
A ? ?, as a step that potentially leads to a com-
plete derivation; this is only ?potentially? because
the actual remaining input w may be unlike ax,
apart from the matching one-symbol lookahead.
Similarly, if [A ? ? ? a?, b] ? I
k
, then
? = X
j+1
? ? ?X
k
, some j, and if furthermore
a = 1 : w, then a shift of symbol a is a justifiable
step. Potentially, if a is followed by some x such
that ? ?
?
rm
x, then we may eventually obtain a
stack X
1
? ? ?X
j
?a?, which is a prefix of a right-
sentential form, with the handle being A ? ?a?
at position j + 1.
For a fixed grammar, the collection of all pos-
sible sets of 1-items that may arise in processing
any viable prefix is a finite set. The technique
of LR(1) parsing relies on a precomputation of all
such sets of items, each of which is turned into a
state of the LR(1) automaton. The initial state con-
sists of closure({[S ? ? ?, $] | S ? ? ? P}).
The automaton has a transition labeled X from
I to J if goto(I,X) = J , where goto(I,X)
= closure({[A ? ?X ? ?, a] | [A ? ? ?
X?, a] ? I}). In the present study, we do not pre-
compute all possible states of the LR(1) automa-
ton, as this would require prohibitive amounts of
time and memory. Instead, our parsers are best
understood as computing LR states dynamically,
while furthermore attaching probabilities to indi-
vidual items.
In the sequel we will assume that all rules either
have the (lexical) form A ? a, the (binary) form
A ? BC, or the (unary) form A ? B. This
means that A ?
?
rm
? is not possible for any A.
The end-of-sentence marker is now introduced by
two augmented rules S
?
? SS
$
and S
$
? $.
3 Probabilistic shift-reduce parsing
A probabilistic CFG (PCFG) is a 5-tuple (?, N,
S, P , p), where the extra element p maps rules
to probabilities. The probability of a derivation
?
d
?
rm
?, with d = pi
1
? ? ?pi
k
, is defined to be
p(d) =
?
i
p(pi
i
). The probability p(w) of a string
w is defined to be the sum of p(d) for all d with
S
d
?
rm
w.
We assume properness, i.e.
?
pi=A??
p(pi) =
1 for all A, and consistency, i.e.
?
w
p(w) = 1.
Properness and consistency together imply that for
each nonterminal A, the sum of p(d) for all d with
?
w
A
d
?
rm
w equals 1. We will further assume an
augmented PCFG with extra rules S
?
? SS
$
and
S
$
? $ both having probability 1.
Consider a viable prefix A
1
? ? ?A
k
on the stack
of a shift-reduce parser, and lookahead a. Each
right-most derivation in which the handle is A ?
A
k?1
A
k
at position k ? 1 must be of the form
sketched in Figure 1.
Because of properness and consistency, we may
assume that all possible subderivations generat-
ing strings entirely to the right of the lookahead
have probabilities summing to 1. To compactly
express the remaining probabilities, we need addi-
tional notation. First we define:
V(C,D) =
?
d : ?
w
C
d
?
rm
Dw
p(d)
for any pair of nonterminals C and D. This will
be used later to ?factor out? a common term in a
(potentially infinite) sum of probabilities of sub-
derivations; the w in the expression above corre-
sponds to a substring of the unknown input beyond
the lookahead. In order to compute such values,
we fix an ordering of the nonterminals by N =
{C
1
, . . . , C
r
}, with r = |N |. We then construct
a matrix M , such that M
i,j
=
?
pi=C
i
?C
j
?
p(pi).
In words, we sum the probabilities of all rules that
have left-hand sideC
i
and a right-hand side begin-
ning with C
j
.
A downward path in a parse tree from an oc-
currence of C to an occurrence of D, restricted
to following always the first child, can be of any
length n, including n = 0 if C = D. This means
we need to obtain the matrixM
?
=
?
0?n
M
n
, and
V(C
i
, C
j
) =M
?
i,j
for all i and j. Fortunately,M
?
i,j
can be effectively computed as (I ?M)
?1
, where
I is the identity matrix of size r and the superscript
denotes matrix inversion.
340
We further define:
U(C,D) =
?
d : C
d
?
rm
D
p(d)
much as above, but restricting attention to unit
rules.
The expected number of times a handle A ?
A
k?1
A
k
at position k ? 1 occurs in a right-most
derivation with viable prefix A
1
? ? ?A
k
and looka-
head a is now given by:
E(A
1
? ? ?A
k
, a, A? A
k?1
A
k
) =
?
S
?
= E
0
, . . . , E
k?2
, F
1
, . . . , F
k?1
= A,
F,E,B,B
?
,m : 0 ? m < k ? 1
?
i: 1?i?m
V(E
i?1
, F
i
) ? p(F
i
? A
i
E
i
) ?
V(E
m
, F ) ? p(F ? EB) ? U(E,F
m+1
) ?
?
i: m<i<k?1
p(F
i
? A
i
E
i
) ? U(E
i
, F
i+1
) ?
p(F
k?1
? A
k?1
A
k
) ? V(B,B
?
) ? p(B
?
? a)
Note that the value above is not a probability and
may exceed 1. This is because the same viable
prefix may occur several times in a single right-
most derivation.
At first sight, the computation of E seems to re-
quire an exponential number of steps in k. How-
ever, we can use an idea similar to that commonly
used for computation of forward probabilities for
HMMs (Rabiner, 1989). We first define F :
F(?, E) =
{
1 if E = S
?
0 otherwise
F(?A,E) =
?
E
?
,pi=F?AE
F(?,E
?
) ? V(E
?
, F ) ? p(pi)
This corresponds to the part of the definition
of E involving A
1
, . . . , A
m
, E
0
, . . . , E
m
and
F
1
, . . . , F
m
. We build on this by defining:
G(?,E,B) =
?
E
?
,pi=F?EB
F(?,E
?
) ? V(E
?
, F ) ? p(pi)
One more recursive function is needed for
what was A
m+1
, . . . , A
k?2
, E
m+1
, . . . , E
k?2
and
F
m+1
, . . . , F
k?2
in the earlier definition of E :
H(?, E,B)=G(?, E,B)
H(?A,E,B)=
?
E
?
,pi=F?AE
H(?,E
?
, B) ? U(E
?
, F ) ? p(pi)
+ G(?A,E,B)
E
0
F
1
A
1
E
1
E
m?1
F
m
A
m
E
m
F
E
F
m+1
A
m+1
E
m+1
E
k?2
F
k?1
A
k?1
A
k
B
B
?
a
Figure 1: Right-most derivation leading to
F
k?1
? A
k?1
A
k
in viable prefix A
1
? ? ?A
k
with
lookahead a.
Finally, we can express E in terms of these re-
cursive functions, considering the more general
case of any rule pi = F ? ?:
E(??, a, F ? ?) =
?
E,B
H(?,E,B) ? U(E,F ) ? p(pi) ? L(B, a)
E(?, a, F ? ?) = 0 if ??
?
? = ??
where:
L(B, a) =
?
pi=B
?
?a
V(B,B
?
) ? p(pi)
The expected number of times the handle is to
be found to the right of ?, with the stack being ?
and the lookahead symbol being a, is:
E(?, a, shift) =
?
B
F(?,B) ? L(B, a)
The expected number of times we see a stack ?
with lookahead a is:
E(?, a) = E(?, a, shift) +
?
pi
E(?, a, pi)
341
The probability that a reduce with rule pi is the
correct action when the stack is ? and the looka-
head is a is naturally E(?, a, pi)/E(?, a) and the
probability that a shift is the correct action is
E(?, a, shift)/E(?, a). For determining the most
likely action we do not need to compute E(?, a);
it suffices to identify the maximum value among
E(?, a, shift) and E(?, a, pi) for each rule pi.
A deterministic shift-reduce parser can now be
constructed that always chooses the most likely
next action. For a given input string, the number
of actions performed by this parser is linear in the
input length.
A call of E may lead to a number of recursive
calls of F and H that is linear in the stack size
and thereby in the input length. Note however that
by remembering the values returned by these func-
tion between parser actions, one can ensure that
each additional element pushed on the stack re-
quires a bounded number of additional calls of the
auxiliary functions. Because only linearly many
elements are pushed on the stack, the time com-
plexity becomes linear in the input length.
Complexity analysis seems less favorable if we
consider the number of nonterminals. The defi-
nitions of G and H each involve four nontermi-
nals excluding the stack symbol A, so that the
time complexity is O(|w| ? |N |
4
), where |w| is
the length of the input w. A finer analysis gives
O(|w| ? (|N | ? |P |+ |N |
2
? ?P?)), where ?P? is
the maximum for all A of the number of rules
of the form F ? AE. By splitting up G and
H into smaller functions, we obtain complexity
O(|w| ? |N |
3
), which can still be prohibitive.
Therefore we have implemented an alternative
that has a time complexity that is only quadratic
in the size of the grammar, at the expense of a
quadratic complexity in the length of the input
string, as detailed in Appendix A. This is still
better in practice if the number of nonterminals is
much greater than the length of the input string, as
in the case of the grammars we investigated.
4 Structural determinism
We have assumed so far that a deterministic shift-
reduce parser chooses a unique next action in each
configuration, an action being a shift or reduce.
Implicit in this was that if the next action is a re-
duce, then also a unique rule is chosen. However,
if we assume for now that all non-lexical rules
are binary, then we can easily generalize the pars-
ing algorithm to consider all possible rules whose
right-hand sides match the top-most two stack el-
ements, and postpone commitment to any of the
nonterminals in the left-hand sides. This requires
that stack elements now contain sets of grammar
symbols. Each of these is associated with the
probability of the most likely subderivation con-
sistent with the relevant substring of the input.
Each reduce with a binary rule is implicitly fol-
lowed by zero or more reduces with unary rules.
Similarly, each shift is implicitly followed by a re-
duce with a lexical rule and zero or more reduces
with unary rules; see also (Graham et al., 1980).
This uses a precompiled table similar to U , but us-
ing maximization in place of summation, defined
by:
U
max
(C,D) = max
d : C
d
?
rm
D
p(d)
More concretely, configurations have the form
(Z
1
. . . Z
k
, v$), k ? 0, where each Z
i
(1 ? i ? k)
is a set of pairs (A, p), where A is a nonterminal
and p is a (non-zero) probability; each A occurs
at most once in Z
i
. A shift turns (?, av$) into
(?Z, v$), where Z consists of all pairs (E, p) such
that p = max
F
U
max
(E,F ) ? p(F ? a). A gen-
eralized binary reduce now turns (?Z
1
Z
2
, v$) into
(?Z, v$), where Z consists of all pairs (E, p) such
that:
p = max
pi = F ? A
1
A
2
,
(A
1
, p
1
) ? Z
1
, (A
2
, p
2
) ? Z
2
U
max
(E,F ) ? p(pi) ? p
1
? p
2
We characterize this parsing procedure as struc-
turally deterministic, as an unlabeled structure is
built deterministically in the first instance. The
exact choices of rules can be postponed until af-
ter reaching the end of the sentence. Then follows
a straightforward process of ?backtracing?, which
builds the derivation that led to the computed prob-
ability associated with the start symbol.
The time complexity is now O(|w| ? |N |
5
) in
the most straightforward implementation, but we
can reduce this to quadratic in the size of the gram-
mar provided we allow an additional factor |w| as
before. For more details see Appendix B.
5 Other variants
One way to improve accuracy is to increase the
size of the lookahead, beyond the current 1, com-
parable to the generalization from LR(1) to LR(k)
parsing. The formulas are given in Appendix C.
342
Yet another variant investigates only the top-
most n stack symbols when choosing the next
parser action. In combination with Appendix A,
this brings the time complexity down again to lin-
ear time in the length of the input string. The re-
quired changes to the formulas are given in Ap-
pendix D. There is a slight similarity to (Schuler,
2009), in that no stack elements beyond a bounded
depth are considered at each parsing step, but in
our case the stack can still have arbitrary height.
Whereas we have concentrated on determinism
in this paper, one can also introduce a limited de-
gree of nondeterminism and allow some of the
most promising configurations at each input posi-
tion to compete, applying techniques such as beam
search (Roark, 2001; Zhang and Clark, 2009; Zhu
et al., 2013), best-first search (Sagae and Lavie,
2006), or A
?
search (Klein and Manning, 2003)
in order to keep the running time low. For com-
paring different configurations, one would need to
multiply the values E(?, a) as in Section 3 by the
probabilities of the subderivations associated with
occurrences of grammar symbols in stack ?.
Further variants are obtained by replacing the
parsing strategy. One obvious candidate is left-
corner parsing (Rosenkrantz and Lewis II, 1970),
which is considerably simpler than LR parsing.
The resulting algorithm would be very different
from the left-corner models of e.g. (Henderson,
2003), which rely on neural networks instead of
PCFGs.
6 Experiments
We used the WSJ treebank from OntoNotes 4.0
(Hovy et al., 2006), with Sections 2-21 for train-
ing and the 2228 sentences of up to 40 words from
Section 23 for testing. Grammars with different
sizes, and in the required binary form, were ex-
tracted by using the tools from the Berkeley parser
(Petrov et al., 2006), with between 1 and 6 split-
merge cycles. These tools offer a framework for
handling unknown words, which we have adopted.
The implementation of the parsing algorithms
is in C++, running on a desktop with four 3.1GHz
Intel Core i5 CPUs. The main algorithm is that of
Appendix C, with lookahead k between 1 and 3,
also in combination with structural determinism
(Appendix B), which is indicated here by sd. The
variant that consults the stack down to bounded
depth n (Appendix D) will only be reported for
k = 1 and n = 5.
Bracketing recall, precision and F-measure, are
computed using evalb, with settings as in (Collins,
1997), except that punctuation was deleted.
1
Ta-
ble 1 reports results.
A nonterminal B in the stack may occur in a
small number of rules of the form A ? BC. The
C of one such rule is needed next in order to al-
low a reduction. If future input does not deliver
this C, then parsing may fail. This problem be-
comes more severe as nonterminals become more
specific, which is what happens with an increase of
the number of split-merge cycles. Even more fail-
ures are introduced by removing the ability to con-
sult the complete stack, which explains the poor
results in the case of k = 1, n = 5; lower values
of n lead to even more failures, and higher values
further increase the running time. That the running
time exceeds that of k = 1 is explained by the fact
that with the variant from Appendix D, every pop
or push requires a complete recomputation of all
function values.
Parse failures can be almost completely elimi-
nated however by choosing higher values of k and
by using structural determinism. A combination
thereof leads to high accuracy, not far below that
of the Viterbi parses. Note that one cannot expect
the accuracy of our deterministic parsers to exceed
that of Viterbi parses. Both rely on the same model
(a PCFG), but the first is forced to make local deci-
sions without access to the input string that follows
the bounded lookahead.
7 Conclusions
We have shown that deterministic parsers can be
constructed from a given PCFG. Much of the ac-
curacy of the grammar can be retained by choosing
a large lookahead in combination with ?structural
determinism?, which postpones commitment to
nonterminals until the end of the input is reached.
Parsers of this nature potentially run in linear
time in the length of the input, but our parsers are
better implemented to run in quadratic time. In
terms of the grammar size, the experiments sug-
gest that the number of rules is the dominating fac-
tor. The size of the lookahead strongly affects run-
ning time. The extra time costs of structural deter-
minism are compensated by an increase in accu-
racy and a sharp decrease of the parse failures.
1
Evalb otherwise stumbles over e.g. a part of speech con-
sisting of two single quotes in the parsed file, against a part
of speech ?POS? in the gold file, for an input token consisting
of a single quote.
343
Table 1: Total time required (seconds), number of parse failures, recall, precision, F-measure, for deter-
ministic parsing, compared to the Viterbi parses as computed with the Berkeley parser.
time fail R P F1
1-split-merge (12,059 rules)
k = 1 43 11 67.20 66.67 66.94
k = 2 99 0 70.74 71.01 70.88
k = 3 199 0 71.41 71.85 71.63
k = 1, sd 62 0 68.12 68.52 68.32
k = 2, sd 135 0 70.98 71.72 71.35
k = 3, sd 253 0 71.31 72.50 71.90
k = 1, n = 5 56 170 66.19 65.67 65.93
Viterbi 0 72.45 74.55 73.49
2-split-merge (32,994 rules)
k = 1 120 33 72.65 70.50 71.56
k = 2 275 1 78.44 77.26 77.84
k = 3 568 0 79.81 79.27 79.54
k = 1, sd 196 0 74.78 74.96 74.87
k = 2, sd 439 0 79.96 80.40 80.18
k = 3, sd 770 0 80.49 81.20 80.85
k = 1, n = 5 146 247 72.27 70.34 71.29
Viterbi 0 82.16 82.69 82.43
3-split-merge (95,647 rules)
k = 1 305 75 74.39 72.33 73.35
k = 2 770 3 81.32 80.35 80.83
k = 3 1,596 0 82.78 82.35 82.56
k = 1, sd 757 0 78.11 78.37 78.24
k = 2, sd 1,531 0 82.85 83.39 83.12
k = 3, sd 2,595 0 83.66 84.25 83.96
k = 1, n = 5 404 401 74.52 72.39 73.44
Viterbi 0 85.38 86.03 85.71
time fail R P F1
4-split-merge (269,162 rules)
k = 1 870 115 75.69 73.30 74.48
k = 2 2,257 1 83.48 82.35 82.91
k = 3 4,380 1 84.95 84.06 84.51
k = 1, sd 2,336 1 80.82 80.65 80.74
k = 2, sd 4,747 0 85.52 85.64 85.58
k = 3, sd 7,728 0 86.62 86.82 86.72
k = 1, n = 5 1,152 508 76.21 73.92 75.05
Viterbi 0 87.95 88.10 88.02
5-split-merge (716,575 rules)
k = 1 3,166 172 76.17 73.44 74.78
k = 2 7,476 2 84.14 82.80 83.46
k = 3 14,231 1 86.05 85.24 85.64
k = 1, sd 7,427 1 81.99 81.44 81.72
k = 2, sd 14,587 0 86.89 87.00 86.95
k = 3, sd 24,553 0 87.67 87.82 87.74
k = 1, n = 5 4,572 559 77.65 75.13 76.37
Viterbi 0 88.65 89.00 88.83
6-split-merge (1,947,915 rules)
k = 1 7,741 274 76.60 74.08 75.32
k = 2 19,440 5 84.60 83.17 83.88
k = 3 35,712 0 86.02 85.07 85.54
k = 1, sd 19,530 1 82.64 81.95 82.29
k = 2, sd 39,615 0 87.36 87.20 87.28
k = 3, sd 64,906 0 88.16 88.26 88.21
k = 1, n = 5 10,897 652 77.89 75.57 76.71
Viterbi 0 88.69 88.99 88.84
There are many advantages over other ap-
proaches to deterministic parsing that rely on
general-purpose classifiers. First, some state-of-
the-art language models are readily available as
PCFGs. Second, most classifiers require tree-
banks, whereas our algorithms are also applicable
to PCFGs that were obtained in any other way, for
example through intersection of language models.
Lastly, our algorithms fit within well understood
automata theory.
Acknowledgments We thank the reviewers.
A Formulas for quadratic time
complexity
The following are the formulas that correspond
to the first implemented variant. Relative to Sec-
tion 3, some auxiliary functions are broken up, and
associating the lookahead a with an appropriate
nonterminal B is now done in G:
F(?, E) =
{
1 if E = S
?
0 otherwise
F(?A,E) =
?
pi=F?AE
F
?
(?, F ) ? p(pi)
F
?
(?, F ) =
?
E
F(?,E) ? V(E,F )
G(?,E, a) =
?
F
F
?
(?, F ) ? G
?
(F,E, a)
G
?
(F,E, a) =
?
pi=F?EB
p(pi) ? L(B, a)
H(?, E, a) = G(?, E, a)
H(?A,E, a) =
?
pi=F?AE
H
?
(?, F, a) ? p(pi)
+ G(?A,E, a)
H
?
(?, F, a) =
?
E
H(?,E, a) ? U(E,F )
344
E(??, a, F ? ?) = H
?
(?, F, a) ? p(F ? ?)
E(?, a, F ? ?) = 0 if ??
?
? = ??
E(?A, a, shift) = G(?,A, a)
E(?, a, shift) = L(S
?
, a)
These equations correspond to a time complex-
ity of O(|w|
2
? |N |
2
+ |w| ? |P |). Each definition
except that of G
?
involves one stack (of linear size)
and, at most, one terminal plus two arbitrary non-
terminals. The full grammar is only considered
once for every input position, in the definition of
G
?
.
The values are stored as vectors and matrices.
For example, for each distinct lookahead symbol
a, there is a (sparse) matrix containing the value of
G
?
(F,E, a) at a row and a column uniquely iden-
tified by F and E, respectively.
B Formulas for structural determinism
For the variant from Section 4, we need to change
only two definitions of auxiliary functions:
F(?Z,E) =
?
(A,p)?Z,pi=F?AE
F
?
(?, F ) ? p(pi) ? p
H(?Z,E, a) =
?
(A,p)?Z,pi=F?AE
H
?
(?, F, a) ? p(pi) ? p
+ G(?Z,E, a)
The only actions are shift and generalized bi-
nary reduce red . The definition of E becomes:
E(?Z
1
Z
2
, a, red)=
?
(A
1
,p
1
)?Z
1
,(A
2
,p
2
)?Z
2
pi=F?A
1
A
2
H
?
(?, F, a) ? p(pi) ? p
1
? p
2
E(?Z, a, shift) =
?
(A,p)?Z
G(?,A, a) ? p
The time complexity now increases to
O(|w|
2
? (|N |
2
+ |P |)) due to the newH.
C Formulas for larger lookahead
In order to handle k symbols of lookahead (Sec-
tion 5) some technical problems are best avoided
by having k copies of the end-of-sentence marker
appended behind the input string, with a corre-
sponding augmentation of the grammar. We gen-
eralize L(B, v) to be the sum of p(d) for all d
such that B
d
?
rm
vx, some x. We let I(B, v)
be the sum of p(d) for all d such that B
d
?
rm
v.
If I is given for all prefixes of a fixed lookahead
string of length k (this requires cubic time in k),
we can compute L in linear time for all suffixes of
the same string:
L(B, v) =
?
B
?
V(B,B
?
) ? L
?
(B
?
, v)
L
?
(B, v) =
?
pi=B?B
1
B
2
,v
1
,v
2
:
v=v
1
v
2
,1?|v
1
|,1?|v
2
|
p(pi) ? I(B
1
, v
1
) ? L(B
2
, v
2
)
if |v| > 1
L
?
(B, a) =
?
pi=B?a
p(pi)
The function H is generalized straightforwardly
by letting it pass on a string v (1 ? |v| ? k) in-
stead of a single terminal a. The same holds for E .
The function G requires a slightly bigger modifica-
tion, leading back to H if not all of the lookahead
has been matched yet:
G(?,E, v) =
?
F
F
?
(?, F ) ? G
?
(F,E, v) +
?
F,v
1
,v
2
:v=v
1
v
2
,|v
2
|>0
H
?
(?, F, v
2
) ? G
??
(F,E, v
1
)
G
?
(F,E, v) =
?
pi=F?EB
p(pi) ? L(B, v)
G
??
(F,E, v) =
?
pi=F?EB
p(pi) ? I(B, v)
The time complexity is nowO(k ? |w|
2
? |N |
2
+
k
3
? |w| ? |P |).
D Investigation of top-most n stack
symbols only
As discussed in Section 5, we want to predict the
next parser action without consulting any symbols
in ?, when the current stack is ??, with |?| =
n. This is achieved by approximating F(?,E) by
the outside value of E, that is, the sum of p(d)
for all d such that ?
?,w
S
d
?
rm
?Ew. Similarly,
H
?
(?, F, v) is approximated by
?
E
G(?,E, v) ?
W(E,F ) where:
W(C,D) =
?
d : ?
?
C
d
?
rm
?D
p(d)
The time complexity (with lookahead k) is now
O(k ? n ? |w| ? |N |
2
+ k
3
? |w| ? |P |).
345
References
A.V. Aho and J.D. Ullman. 1972. Parsing, volume 1 of
The Theory of Parsing, Translation and Compiling.
Prentice-Hall, Englewood Cliffs, N.J.
S. Billot and B. Lang. 1989. The structure of
shared forests in ambiguous parsing. In 27th An-
nual Meeting of the ACL, Proceedings of the Confer-
ence, pages 143?151, Vancouver, British Columbia,
Canada, June.
T. Briscoe and J. Carroll. 1993. Generalized prob-
abilistic LR parsing of natural language (corpora)
with unification-based grammars. Computational
Linguistics, 19(1):25?59.
M. Candito, J. Nivre, P. Denis, and E. Henestroza An-
guiano. 2010. Benchmarking of statistical de-
pendency parsers for French. In The 23rd Inter-
national Conference on Computational Linguistics,
pages 108?116, Beijing, China, August.
D. Cer, M.-C. de Marneffe, D. Jurafsky, and C. Man-
ning. 2010. Parsing to Stanford dependen-
cies: Trade-offs between speed and accuracy. In
LREC 2010: Seventh International Conference on
Language Resources and Evaluation, Proceedings,
pages 1628?1632, Valletta , Malta, May.
M. Collins. 1997. Three generative, lexicalised models
for statistical parsing. In 35th Annual Meeting of the
ACL, Proceedings of the Conference, pages 16?23,
Madrid, Spain, July.
S.L. Graham, M.A. Harrison, and W.L. Ruzzo. 1980.
An improved context-free recognizer. ACM Trans-
actions on Programming Languages and Systems,
2:415?462.
J. Henderson. 2003. Generative versus discrimina-
tive models for statistical left-corner parsing. In
8th International Workshop on Parsing Technolo-
gies, pages 115?126, LORIA, Nancy, France, April.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. OntoNotes: The 90% solu-
tion. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, Main Conference,
pages 57?60, New York, USA, June.
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proceedings
of the 48th Annual Meeting of the ACL, pages 1077?
1086, Uppsala, Sweden, July.
F. Jelinek and J.D. Lafferty. 1991. Computation
of the probability of initial substring generation by
stochastic context-free grammars. Computational
Linguistics, 17(3):315?323.
T. Kalt. 2004. Induction of greedy controllers for de-
terministic treebank parsers. In Conference on Em-
pirical Methods in Natural Language Processing,
pages 17?24, Barcelona, Spain, July.
D. Klein and C.D. Manning. 2003. A
?
parsing: Fast
exact Viterbi parse selection. In Proceedings of the
2003 Human Language Technology Conference of
the North American Chapter of the ACL, pages 40?
47, Edmonton, Canada, May?June.
D.E. Knuth. 1965. On the translation of languages
from left to right. Information and Control, 8:607?
639.
M. Kuhlmann, C. G?omez-Rodr??guez, and G. Satta.
2011. Dynamic programming algorithms for
transition-based dependency parsers. In 49th An-
nual Meeting of the ACL, Proceedings of the Con-
ference, pages 673?682, Portland, Oregon, June.
M. Lankhorst. 1991. An empirical comparison of gen-
eralized LR tables. In R. Heemels, A. Nijholt, and
K. Sikkel, editors, Tomita?s Algorithm: Extensions
and Applications, Proc. of the first Twente Work-
shop on Language Technology, pages 87?93. Uni-
versity of Twente, September.
A. Lavie and M. Tomita. 1993. GLR
?
? an effi-
cient noise-skipping parsing algorithm for context
free grammars. In Third International Workshop on
Parsing Technologies, pages 123?134, Tilburg (The
Netherlands) and Durbuy (Belgium), August.
M.-J. Nederhof and G. Satta. 2004. An alternative
method of training probabilistic LR parsers. In 42nd
Annual Meeting of the ACL, Proceedings of the Con-
ference, pages 551?558, Barcelona, Spain, July.
S.-K. Ng and M. Tomita. 1991. Probabilistic LR pars-
ing for general context-free grammars. In Proc. of
the Second International Workshop on Parsing Tech-
nologies, pages 154?163, Cancun, Mexico, Febru-
ary.
J. Nivre. 2008. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34(4):513?553.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree
annotation. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the ACL, pages 433?440, Sydney,
Australia, July.
L.R. Rabiner. 1989. A tutorial on hidden Markov mod-
els and selected applications in speech recognition.
Proceedings of the IEEE, 77(2):257?286, February.
A. Ratnaparkhi. 1997. A linear observed time statis-
tical parser based on maximum entropy models. In
Proceedings of the Second Conference on Empirical
Methods in Natural Language Processing, pages 1?
10, Providence, Rhode Island, USA, August.
B. Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
346
D.J. Rosenkrantz and P.M. Lewis II. 1970. Determin-
istic left corner parsing. In IEEE Conference Record
of the 11th Annual Symposium on Switching and Au-
tomata Theory, pages 139?152.
K. Sagae and A. Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of the Ninth International Workshop on Parsing
Technologies, pages 125?132, Vancouver, British
Columbia, Canada, October.
K. Sagae and A. Lavie. 2006. A best-first probabilistic
shift-reduce parser. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and 44th Annual Meeting of the ACL, pages 691?
698, Sydney, Australia, July.
W. Schuler. 2009. Positive results for parsing with
a bounded stack using a model-based right-corner
transform. In Proceedings of Human Language
Technologies: The 2009 Annual Conference of the
North American Chapter of the ACL, pages 344?
352, Boulder, Colorado, May?June.
P. Shann. 1991. Experiments with GLR and chart pars-
ing. In M. Tomita, editor, Generalized LR Parsing,
chapter 2, pages 17?34. Kluwer Academic Publish-
ers.
S.M. Shieber. 1983. Sentence disambiguation by
a shift-reduce parsing technique. In 21st Annual
Meeting of the ACL, Proceedings of the Conference,
pages 113?118, Cambridge, Massachusetts, July.
S. Sippu and E. Soisalon-Soininen. 1990. Parsing The-
ory, Vol. II: LR(k) and LL(k) Parsing, volume 20 of
EATCS Monographs on Theoretical Computer Sci-
ence. Springer-Verlag.
A. Stolcke. 1995. An efficient probabilistic context-
free parsing algorithm that computes prefix proba-
bilities. Computational Linguistics, 21(2):167?201.
M. Tomita. 1988. Graph-structured stack and natu-
ral language parsing. In 26th Annual Meeting of
the ACL, Proceedings of the Conference, pages 249?
257, Buffalo, New York, June.
Y. Tsuruoka and J. Tsujii. 2005. Chunk parsing re-
visited. In Proceedings of the Ninth International
Workshop on Parsing Technologies, pages 133?140,
Vancouver, British Columbia, Canada, October.
A. Wong and D. Wu. 1999. Learning a lightweight
robust deterministic parser. In Sixth European Con-
ference on Speech Communication and Technology,
pages 2047?2050.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
8th International Workshop on Parsing Technolo-
gies, pages 195?206, LORIA, Nancy, France, April.
D.H. Younger. 1967. Recognition and parsing of
context-free languages in time n
3
. Information and
Control, 10:189?208.
Y. Zhang and S. Clark. 2009. Transition-based pars-
ing of the Chinese treebank using a global discrimi-
native model. In Proceedings of the 11th Interna-
tional Conference on Parsing Technologies, pages
162?171, Paris, France, October.
M. Zhu, Y. Zhang, W. Chen, M. Zhang, and J. Zhu.
2013. Fast and accurate shift-reduce constituent
parsing. In 51st Annual Meeting of the ACL, Pro-
ceedings of the Conference, volume 1, pages 434?
443, Sofia, Bulgaria, August.
347
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 460?469,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Prefix Probability
for Probabilistic Synchronous Context-Free Grammars
Mark-Jan Nederhof
School of Computer Science
University of St Andrews
North Haugh, St Andrews, Fife
KY16 9SX
United Kingdom
markjan.nederhof@googlemail.com
Giorgio Satta
Dept. of Information Engineering
University of Padua
via Gradenigo, 6/A
I-35131 Padova
Italy
satta@dei.unipd.it
Abstract
We present a method for the computation of
prefix probabilities for synchronous context-
free grammars. Our framework is fairly gen-
eral and relies on the combination of a sim-
ple, novel grammar transformation and stan-
dard techniques to bring grammars into nor-
mal forms.
1 Introduction
Within the area of statistical machine translation,
there has been a growing interest in so-called syntax-
based translation models, that is, models that de-
fine mappings between languages through hierar-
chical sentence structures. Several such statistical
models that have been investigated in the literature
are based on synchronous rewriting or tree transduc-
tion. Probabilistic synchronous context-free gram-
mars (PSCFGs) are one among the most popular ex-
amples of such models. PSCFGs subsume several
syntax-based statistical translation models, as for in-
stance the stochastic inversion transduction gram-
mars of Wu (1997), the statistical model used by the
Hiero system of Chiang (2007), and systems which
extract rules from parsed text, as in Galley et al
(2004).
Despite the widespread usage of models related to
PSCFGs, our theoretical understanding of this class
is quite limited. In contrast to the closely related
class of probabilistic context-free grammars, a syn-
tax model for which several interesting mathemati-
cal and statistical properties have been investigated,
as for instance by Chi (1999), many theoretical prob-
lems are still unsolved for the class of PSCFGs.
This paper considers a parsing problem that is
well understood for probabilistic context-free gram-
mars but that has never been investigated in the con-
text of PSCFGs, viz. the computation of prefix prob-
abilities. In the case of a probabilistic context-free
grammar, this problem is defined as follows. We
are asked to compute the probability that a sentence
generated by our model starts with a prefix string v
given as input. This quantity is defined as the (pos-
sibly infinite) sum of the probabilities of all strings
of the form vw, for any string w over the alphabet
of the model. This problem has been studied by
Jelinek and Lafferty (1991) and by Stolcke (1995).
Prefix probabilities can be used to compute probabil-
ity distributions for the next word or part-of-speech.
This has applications in incremental processing of
text or speech from left to right; see again (Jelinek
and Lafferty, 1991). Prefix probabilities can also be
exploited in speech understanding systems to score
partial hypotheses in beam search (Corazza et al,
1991).
This paper investigates the problem of computing
prefix probabilities for PSCFGs. In this context, a
pair of strings v1 and v2 is given as input, and we are
asked to compute the probability that any string in
the source language starting with prefix v1 is trans-
lated into any string in the target language starting
with prefix v2. This probability is more precisely
defined as the sum of the probabilities of translation
pairs of the form [v1w1, v2w2], for any strings w1
and w2.
A special case of prefix probability for PSCFGs
is the right prefix probability. This is defined as the
probability that some (complete) input string w in
the source language is translated into a string in the
target language starting with an input prefix v.
460
Prefix probabilities and right prefix probabilities
for PSCFGs can be exploited to compute probabil-
ity distributions for the next word or part-of-speech
in left-to-right incremental translation, essentially in
the same way as described by Jelinek and Lafferty
(1991) for probabilistic context-free grammars, as
discussed later in this paper.
Our solution to the problem of computing prefix
probabilities is formulated in quite different terms
from the solutions by Jelinek and Lafferty (1991)
and by Stolcke (1995) for probabilistic context-free
grammars. In this paper we reduce the computation
of prefix probabilities for PSCFGs to the computa-
tion of inside probabilities under the same model.
Computation of inside probabilities for PSCFGs is
a well-known problem that can be solved using off-
the-shelf algorithms that extend basic parsing algo-
rithms. Our reduction is a novel grammar trans-
formation, and the proof of correctness proceeds
by fairly conventional techniques from formal lan-
guage theory, relying on the correctness of standard
methods for the computation of inside probabilities
for PSCFG. This contrasts with the techniques pro-
posed by Jelinek and Lafferty (1991) and by Stolcke
(1995), which are extensions of parsing algorithms
for probabilistic context-free grammars, and require
considerably more involved proofs of correctness.
Our method for computing the prefix probabili-
ties for PSCFGs runs in exponential time, since that
is the running time of existing methods for comput-
ing the inside probabilities for PSCFGs. It is un-
likely this can be improved, because the recogni-
tion problem for PSCFG is NP-complete, as estab-
lished by Satta and Peserico (2005), and there is a
straightforward reduction from the recognition prob-
lem for PSCFGs to the problem of computing the
prefix probabilities for PSCFGs.
2 Definitions
In this section we introduce basic definitions re-
lated to synchronous context-free grammars and
their probabilistic extension; our notation follows
Satta and Peserico (2005).
Let N and ? be sets of nonterminal and terminal
symbols, respectively. In what follows we need to
represent bijections between the occurrences of non-
terminals in two strings over N ??. This is realized
by annotating nonterminals with indices from an in-
finite set. We define I(N) = {A t | A ? N, t ?
N} and VI = I(N) ? ?. For a string ? ? V ?I , we
write index(?) to denote the set of all indices that
appear in symbols in ?.
Two strings ?1, ?2 ? V ?I are synchronous if each
index from N occurs at most once in ?1 and at most
once in ?2, and index(?1) = index(?2). Therefore
?1, ?2 have the general form:
?1 = u10A
t1
11 u11A
t2
12 u12 ? ? ?u1r?1A
tr
1r u1r
?2 = u20A
tpi(1)
21 u21A
tpi(2)
22 u22 ? ? ?u2r?1A
tpi(r)
2r u2r
where r ? 0, u1i, u2i ? ??, A
ti
1i , A
tpi(i)
2i ? I(N),
ti 6= tj for i 6= j, and pi is a permutation of the set
{1, . . . , r}.
A synchronous context-free grammar (SCFG)
is a tuple G = (N,?,P, S), where N and ? are fi-
nite, disjoint sets of nonterminal and terminal sym-
bols, respectively, S ? N is the start symbol and
P is a finite set of synchronous rules. Each syn-
chronous rule has the form s : [A1 ? ?1, A2 ?
?2], where A1, A2 ? N and where ?1, ?2 ? V ?I are
synchronous strings. The symbol s is the label of
the rule, and each rule is uniquely identified by its
label. For technical reasons, we allow the existence
of multiple rules that are identical apart from their
labels. We refer to A1 ? ?1 and A2 ? ?2, respec-
tively, as the left and right components of rule s.
Example 1 The following synchronous rules im-
plicitly define a SCFG:
s1 : [S ? A
1 B 2 , S ? B 2 A 1 ]
s2 : [A ? aA
1 b, A ? bA 1 a]
s3 : [A ? ab, A ? ba]
s4 : [B ? cB
1 d, B ? dB 1 c]
s5 : [B ? cd, B ? dc] 2
In each step of the derivation process of a SCFG
G, two nonterminals with the same index in a pair of
synchronous strings are rewritten by a synchronous
rule. This is done in such a way that the result is once
more a pair of synchronous strings. An auxiliary
notion is that of reindexing, which is an injective
function f fromN toN. We extend f to VI by letting
f(A t ) = A f(t) for A t ? I(N) and f(a) = a
for a ? ?. We also extend f to strings in V ?I by
461
letting f(?) = ? and f(X?) = f(X)f(?), for each
X ? VI and ? ? V ?I .
Let ?1, ?2 be synchronous strings in V ?I . The de-
rive relation [?1, ?2] ?G [?1, ?2] holds whenever
there exist an index t in index(?1) = index(?2), a
synchronous rule s : [A1 ? ?1, A2 ? ?2] in P
and some reindexing f such that:
(i) index(f(?1)) ? (index(?1) \ {t}) = ?;
(ii) ?1 = ??1A
t
1 ?
??
1 , ?2 = ?
?
2A
t
2 ?
??
2 ; and
(iii) ?1 = ??1f(?1)?
??
1 , ?2 = ?
?
2f(?2)?
??
2 .
We also write [?1, ?2] ?sG [?1, ?2] to explicitly
indicate that the derive relation holds through rule s.
Note that ?1, ?2 above are guaranteed to be syn-
chronous strings, because ?1 and ?2 are syn-
chronous strings and because of (i) above. Note
also that, for a given pair [?1, ?2] of synchronous
strings, an index t and a rule s, there may be in-
finitely many choices of reindexing f such that the
above constraints are satisfied. In this paper we will
not further specify the choice of f .
We say the pair [A1, A2] of nonterminals is linked
(in G) if there is a rule of the form s : [A1 ?
?1, A2 ? ?2]. The set of linked nonterminal pairs
is denoted by N [2].
A derivation is a sequence ? = s1s2 ? ? ? sd of syn-
chronous rules si ? P with d ? 0 (? = ? for
d = 0) such that [?1i?1, ?2i?1] ?
si
G [?1i, ?2i] for
every i with 1 ? i ? d and synchronous strings
[?1i, ?2i] with 0 ? i ? d. Throughout this paper,
we always implicitly assume some canonical form
for derivations in G, by demanding for instance that
each step rewrites a pair of nonterminal occurrences
of which the first is leftmost in the left component.
When we want to focus on the specific synchronous
strings being derived, we also write derivations in
the form [?10, ?20] ??G [?1d, ?2d], and we write
[?10, ?20] ??G [?1d, ?2d] when ? is not further
specified. The translation generated by a SCFG G
is defined as:
T (G) = {[w1, w2] | [S
1 , S 1 ] ??G [w1, w2],
w1, w2 ? ?
?}
For w1, w2 ? ??, we write D(G, [w1, w2]) to de-
note the set of all (canonical) derivations ? such that
[S 1 , S 1 ] ??G [w1, w2].
Analogously to standard terminology for context-
free grammars, we call a SCFG reduced if ev-
ery rule occurs in at least one derivation ? ?
D(G, [w1, w2]), for some w1, w2 ? ??. We as-
sume without loss of generality that the start sym-
bol S does not occur in the right-hand side of either
component of any rule.
Example 2 Consider the SCFG G from example 1.
The following is a canonical derivation in G, since it
is always the leftmost nonterminal occurrence in the
left component that is involved in a derivation step:
[S 1 , S 1 ] ?G [A
1 B 2 , B 2 A 1 ]
?G [aA
3 bB 2 , B 2 bA 3 a]
?G [aaA
4 bbB 2 , B 2 bbA 4 aa]
?G [aaabbbB
2 , B 2 bbbaaa]
?G [aaabbbcB
5 d, dB 5 cbbbaaa]
?G [aaabbbccdd, ddccbbbaaa]
It is not difficult to see that the generated translation
is T (G) = {[apbpcqdq, dqcqbpap] | p, q ? 1}. 2
The size of a synchronous rule s : [A1 ? ?1,
A2 ? ?2], is defined as |s| = |A1?1A2?2|. The
size of G is defined as |G| =
?
s?P |s|.
A probabilistic SCFG (PSCFG) is a pair G =
(G, pG) where G = (N,?,P, S) is a SCFG and pG
is a function from P to real numbers in [0, 1]. We
say that G is proper if for each pair [A1, A2] ? N [2]
we have:
?
s:[A1??1, A2??2]
pG(s) = 1
Intuitively, properness ensures that where a pair
of nonterminals in two synchronous strings can be
rewritten, there is a probability distribution over the
applicable rules.
For a (canonical) derivation ? = s1s2 ? ? ? sd, we
define pG(?) =
?d
i=1 pG(si). For w1, w2 ? ?
?,
we also define:
pG([w1, w2]) =
?
??D(G,[w1,w2])
pG(?) (1)
We say a PSCFG is consistent if pG defines a prob-
ability distribution over the translation, or formally:
?
w1,w2
pG([w1, w2]) = 1
462
If the grammar is reduced, proper and consistent,
then also:
?
w1,w2???, ??P ?
s.t. [A 11 , A
1
2 ]?
?
G[w1, w2]
pG(?) = 1
for every pair [A1, A2] ? N [2]. The proof is identi-
cal to that of the corresponding fact for probabilistic
context-free grammars.
3 Effective PSCFG parsing
If w = a1 ? ? ? an then the expression w[i, j], with
0 ? i ? j ? n, denotes the substring ai+1 ? ? ? aj (if
i = j then w[i, j] = ?). In this section, we assume
the input is the pair [w1, w2] of terminal strings.
The task of a recognizer for SCFG G is to decide
whether [w1, w2] ? T (G).
We present a general algorithm for solving the
above problem in terms of the specification of a de-
duction system, following Shieber et al (1995). The
items that are constructed by the system have the
form [m1, A1,m?1; m2, A2,m
?
2], where [A1, A2] ?
N [2] and where m1, m?1, m2, m
?
2 are non-negative
integers such that 0 ? m1 ? m?1 ? |w1| and
0 ? m2 ? m?2 ? |w2|. Such an item can be de-
rived by the deduction system if and only if:
[A 11 , A
1
2 ] ?
?
G [w1[m1,m
?
1], w2[m2,m
?
2]]
The deduction system has one inference rule,
shown in figure 1. One of its side conditions has
a synchronous rule in P of the form:
s : [A1 ? u10A
t1
11 u11 ? ? ?u1r?1A
tr
1r u1r,
A2 ? u20A
tpi(1)
21 u21 ? ? ?u2r?1A
tpi(r)
2r u2r] (2)
Observe that, in the right-hand side of the two rule
components above, nonterminals A1i and A2pi?1(i),
1 ? i ? r, have both the same index. More pre-
cisely, A1i has index ti and A2pi?1(i) has index ti?
with i? = pi(pi?1(i)) = i. Thus the nonterminals in
each antecedent item in figure 1 form a linked pair.
We now turn to a computational analysis of the
above algorithm. In the inference rule in figure 1
there are 2(r + 1) variables that can be bound to
positions in w1, and as many that can be bound to
positions in w2. However, the side conditions imply
m?ij = mij + |uij |, for i ? {1, 2} and 0 ? j ? r,
and therefore the number of free variables is only
r + 1 for each component. By standard complex-
ity analysis of deduction systems, for example fol-
lowing McAllester (2002), the time complexity of
a straightforward implementation of the recogni-
tion algorithm is O(|P | ? |w1|
rmax+1 ? |w2|
rmax+1),
where rmax is the maximum number of right-hand
side nonterminals in either component of a syn-
chronous rule. The algorithm therefore runs in ex-
ponential time, when the grammar G is considered
as part of the input. Such computational behavior
seems unavoidable, since the recognition problem
for SCFG is NP-complete, as reported by Satta and
Peserico (2005). See also Gildea and Stefankovic
(2007) and Hopkins and Langmead (2010) for fur-
ther analysis of the upper bound above.
The recognition algorithm above can easily be
turned into a parsing algorithm by letting an imple-
mentation keep track of which items were derived
from which other items, as instantiations of the con-
sequent and the antecedents, respectively, of the in-
ference rule in figure 1.
A probabilistic parsing algorithm that computes
pG([w1, w2]), defined in (1), can also be obtained
from the recognition algorithm above, by associat-
ing each item with a probability. To explain the ba-
sic idea, let us first assume that each item can be
inferred in finitely many ways by the inference rule
in figure 1. Each instantiation of the inference rule
should be associated with a term that is computed
by multiplying the probability of the involved rule
s and the product of all probabilities previously as-
sociated with the instantiations of the antecedents.
The probability associated with an item is then
computed as the sum of each term resulting from
some instantiation of an inference rule deriving that
item. This is a generalization to PSCFG of the in-
side algorithm defined for probabilistic context-free
grammars (Manning and Schu?tze, 1999), and we
can show that the probability associated with item
[0, S, |w1| ; 0, S, |w2|] provides the desired value
pG([w1, w2]). We refer to the procedure sketched
above as the inside algorithm for PSCFGs.
However, this simple procedure fails if there are
cyclic dependencies, whereby the derivation of an
item involves a proper subderivation of the same
item. Cyclic dependencies can be excluded if it can
463
[m?10, A11,m11; m
?
2pi?1(1)?1, A2pi?1(1),m2pi?1(1)]
...
[m?1r?1, A1r,m1r; m
?
2pi?1(r)?1, A2pi?1(r),m2pi?1(r)]
[m10, A1,m?1r; m20, A2,m
?
2r]
?
???????
???????
s:[A1 ? u10A
t1
11 u11 ? ? ?u1r?1A
tr
1r u1r,
A2 ? u20A
tpi(1)
21 u21 ? ? ?u2r?1A
tpi(r)
2r u2r] ? P,
w1[m10,m?10] = u10,
...
w1[m1r,m?1r] = u1r,
w2[m20,m?20] = u20,
...
w2[m2r,m?2r] = u2r
Figure 1: SCFG recognition, by a deduction system consisting of a single inference rule.
be guaranteed that, in figure 1, m?1r ?m10 is greater
than m1j ? m?1j?1 for each j (1 ? j ? r), or
m?2r ? m20 is greater than m2j ? m
?
2j?1 for each
j (1 ? j ? r).
Consider again a synchronous rule s of the form
in (2). We say s is an epsilon rule if r = 0 and
u10 = u20 = . We say s is a unit rule if r = 1
and u10 = u11 = u20 = u21 = . Similarly to
context-free grammars, absence of epsilon rules and
unit rules guarantees that there are no cyclic depen-
dencies between items and in this case the inside al-
gorithm correctly computes pG([w1, w2]).
Epsilon rules can be eliminated from PSCFGs
by a grammar transformation that is very similar
to the transformation eliminating epsilon rules from
a probabilistic context-free grammar (Abney et al,
1999). This is sketched in what follows. We first
compute the set of all nullable linked pairs of non-
terminals of the underlying SCFG, that is, the set of
all [A1, A2] ? N [2] such that [A
1
1 , A
1
2 ] ?
?
G [?, ?].
This can be done in linear time O(|G|) using essen-
tially the same algorithm that identifies nullable non-
terminals in a context-free grammar, as presented for
instance by Sippu and Soisalon-Soininen (1988).
Next, we identify all occurrences of nullable pairs
[A1, A2] in the right-hand side components of a rule
s, such that A1 and A2 have the same index. For
every possible choice of a subset U of these occur-
rences, we add to our grammar a new rule sU con-
structed by omitting all of the nullable occurrences
in U . The probability of sU is computed as the prob-
ability of s multiplied by terms of the form:
?
? s.t. [A 11 ,A
1
2 ]?
?
G[?, ?]
pG(?) (3)
for every pair [A1, A2] in U . After adding these extra
rules, which in effect circumvents the use of epsilon-
generating subderivations, we can safely remove all
epsilon rules, with the only exception of a possible
rule of the form [S ? , S ? ]. The translation and
the associated probability distribution in the result-
ing grammar will be the same as those in the source
grammar.
One problem with the above construction is that
we have to create new synchronous rules sU for each
possible choice of subset U . In the worst case, this
may result in an exponential blow-up of the source
grammar. In the case of context-free grammars, this
is usually circumvented by casting the rules in bi-
nary form prior to epsilon rule elimination. How-
ever, this is not possible in our case, since SCFGs
do not allow normal forms with a constant bound
on the length of the right-hand side of each compo-
nent. This follows from a result due to Aho and Ull-
man (1969) for a formalism called syntax directed
translation schemata, which is a syntactic variant of
SCFGs.
An additional complication with our construction
is that finding any of the values in (3) may involve
solving a system of non-linear equations, similarly
to the case of probabilistic context-free grammars;
see again Abney et al (1999), and Stolcke (1995).
Approximate solution of such systems might take
exponential time, as pointed out by Kiefer et al
(2007).
Notwithstanding the worst cases mentioned
above, there is a special case that can be easily dealt
with. Assume that, for each nullable pair [A1, A2] in
G we have that [A 11 , A
1
2 ] ?
?
G [w1, w2] does not
hold for any w1 and w2 with w1 6= ? or w2 6= ?.
Then each of the values in (3) is guaranteed to be 1,
and furthermore we can remove the instances of the
nullable pairs in the source rule s all at the same
time. This means that the overall construction of
464
elimination of nullable rules from G can be imple-
mented in linear time |G|. It is this special case that
we will encounter in section 4.
After elimination of epsilon rules, one can elimi-
nate unit rules. We define Cunit([A1, A2], [B1, B2])
as the sum of the probabilities of all derivations de-
riving [B1, B2] from [A1, A2] with arbitrary indices,
or more precisely:
?
??P ? s.t. ?t?N,
[A 11 , A
1
2 ]?
?
G[B
t
1 , B
t
2 ]
pG(?)
Note that [A1, A2] may be equal to [B1, B2] and ?
may be ?, in which case Cunit([A1, A2], [B1, B2]) is
at least 1, but it may be larger if there are unit rules.
Therefore Cunit([A1, A2], [B1, B2]) should not be
seen as a probability.
Consider a pair [A1, A2] ? N [2] and let al unit
rules with left-hand sides A1 and A2 be:
s1 : [A1, A2] ? [A
t1
11 , A
t1
21 ]
...
sm : [A1, A2] ? [A
tm
1m , A
tm
2m ]
The values ofCunit(?, ?) are related by the following:
Cunit([A1, A2], [B1, B2]) =
?([A1, A2] = [B1, B2]) +
?
i
pG(si) ? C
unit([A1i, A2i], [B1, B2])
where ?([A1, A2] = [B1, B2]) is defined to be 1 if
[A1, A2] = [B1, B2] and 0 otherwise. This forms a
system of linear equations in the unknown variables
Cunit(?, ?). Such a system can be solved in polyno-
mial time in the number of variables, for example
using Gaussian elimination.
The elimination of unit rules starts with adding
a rule s? : [A1 ? ?1, A2 ? ?2] for each non-
unit rule s : [B1 ? ?1, B2 ? ?2] and pair
[A1, A2] such that Cunit([A1, A2], [B1, B2]) > 0.
We assign to the new rule s? the probability pG(s) ?
Cunit([A1, A2], [B1, B2]). The unit rules can now
be removed from the grammar. Again, in the re-
sulting grammar the translation and the associated
probability distribution will be the same as those in
the source grammar. The new grammar has size
O(|G|2), where G is the input grammar. The time
complexity is dominated by the computation of the
solution of the linear system of equations. This com-
putation takes cubic time in the number of variables.
The number of variables in this case is O(|G|2),
which makes the running time O(|G|6).
4 Prefix probabilities
The joint prefix probability pprefixG ([v1, v2]) of a
pair [v1, v2] of terminal strings is the sum of the
probabilities of all pairs of strings that have v1 and
v2, respectively, as their prefixes. Formally:
pprefixG ([v1, v2]) =
?
w1,w2???
pG([v1w1, v2w2])
At first sight, it is not clear this quantity can be ef-
fectively computed, as it involves a sum over in-
finitely many choices of w1 and w2. However, anal-
ogously to the case of context-free prefix probabili-
ties (Jelinek and Lafferty, 1991), we can isolate two
parts in the computation. One part involves infinite
sums, which are independent of the input strings v1
and v2, and can be precomputed by solving a sys-
tem of linear equations. The second part does rely
on v1 and v2, and involves the actual evaluation of
pprefixG ([v1, v2]). This second part can be realized
effectively, on the basis of the precomputed values
from the first part.
In order to keep the presentation simple, and
to allow for simple proofs of correctness, we
solve the problem in a modular fashion. First,
we present a transformation from a PSCFG
G = (G, pG), with G = (N,?,P, S), to a
PSCFG Gprefix = (Gprefix, pGprefix), with Gprefix =
(Nprefix, ?, Pprefix, S?). The latter grammar derives
all possible pairs [v1, v2] such that [v1w1, v2w2] can
be derived from G, for some w1 and w2. Moreover,
pGprefix([v1, v2]) = p
prefix
G ([v1, v2]), as will be veri-
fied later.
Computing pGprefix([v1, v2]) directly using a
generic probabilistic parsing algorithm for PSCFGs
is difficult, due to the presence of epsilon rules and
unit rules. The next step will be to transform Gprefix
into a third grammar G?prefix by eliminating epsilon
rules and unit rules from the underlying SCFG,
and preserving the probability distribution over pairs
of strings. Using G?prefix one can then effectively
465
apply generic probabilistic parsing algorithms for
PSCFGs, such as the inside algorithm discussed in
section 3, in order to compute the desired prefix
probabilities for the source PSCFG G.
For each nonterminal A in the source SCFG G,
the grammar Gprefix contains three nonterminals,
namely A itself, A? and A?. The meaning of A re-
mains unchanged, whereas A? is intended to gen-
erate a string that is a suffix of a known prefix v1 or
v2. Nonterminals A? generate only the empty string,
and are used to simulate the generation by G of in-
fixes of the unknown suffix w1 or w2. The two left-
hand sides of a synchronous rule in Gprefix can con-
tain different combinations of nonterminals of the
forms A, A?, or A?. The start symbol of Gprefix is
S?. The structure of the rules from the source gram-
mar is largely retained, except that some terminal
symbols are omitted in order to obtain the intended
interpretation of A? and A?.
In more detail, let us consider a synchronous rule
s : [A1 ? ?1, A2 ? ?2] from the source gram-
mar, where for i ? {1, 2} we have:
?i = ui0A
ti1
i1 ui1 ? ? ?uir?1A
tir
ir uir
The transformed grammar then contains a large
number of rules, each of which is of the form s? :
[B1 ? ?1, B2 ? ?2], where Bi ? ?i is of
one of three forms, namely Ai ? ?i, A
?
i ? ?
?
i
or A?i ? ?
?
i , where ?
?
i and ?
?
i are explained below.
The choices for i = 1 and for i = 2 are independent,
so that we can have 3 ? 3 = 9 kinds of synchronous
rules, to be further subdivided in what follows. A
unique label s? is produced for each new rule, and
the probability of each new rule equals that of s.
The right-hand side ??i is constructed by omitting
all terminals and propagating downwards the ? su-
perscript, resulting in:
??i = A
? ti1
i1 ? ? ?A
? tir
ir
It is more difficult to define ??i . In fact, there can
be a number of choices for ??i and, for each choice,
the transformed grammar contains an instance of the
synchronous rule s? : [B1 ? ?1, B2 ? ?2] as de-
fined above. The reason why different choices need
to be considered is because the boundary between
the known prefix vi and the unknown suffix wi can
occur at different positions, either within a terminal
string uij or else further down in a subderivation in-
volving Aij . In the first case, we have for some j
(0 ? j ? r):
??i = ui0A
ti1
i1 ui1A
ti2
i2 ? ? ?
uij?1A
tij
ij u
?
ijA
? tij+1
ij+1 A
? tij+2
ij+2 ? ? ?A
? tir
ir
where u?ij is a choice of a prefix of uij . In words,
the known prefix ends after u?ij and, thereafter, no
more terminals are generated. We demand that u?ij
must not be the empty string, unless Ai = S and
j = 0. The reason for this restriction is that we want
to avoid an overlap with the second case. In this
second case, we have for some j (1 ? j ? r):
??i = ui0A
ti1
i1 ui1A
ti2
i2 ? ? ?
uij?1A
? tij
ij A
? tij+1
ij+1 A
? tij+2
ij+2 ? ? ?A
? tir
ir
Here the known prefix of the input ends within a sub-
derivation involving Aij , and further to the right no
more terminals are generated.
Example 3 Consider the synchronous rule s :
[A ? aB 1 bc C 2 d,D ? ef E 2 F 1 ]. The first
component of a synchronous rule derived from this
can be one of the following eight:
A? ? B? 1 C? 2
A? ? aB? 1 C? 2
A? ? aB? 1 C? 2
A? ? aB 1 b C? 2
A? ? aB 1 bc C? 2
A? ? aB 1 bc C? 2
A? ? aB 1 bc C 2 d
A ? aB 1 bc C 2 d
The second component can be one of the following
six:
D? ? E? 2 F ? 1
D? ? eE? 2 F ? 1
D? ? ef E? 2 F ? 1
D? ? ef E? 2 F ? 1
D? ? ef E 2 F ? 1
D ? ef E 2 F 1
466
In total, the transformed grammar will contain 8 ?
6 = 48 synchronous rules derived from s. 2
For each synchronous rule s, the above gram-
mar transformation produces O(|s|) left rule com-
ponents and as many right rule components. This
means the number of new synchronous rules is
O(|s|2), and the size of each such rule is O(|s|). If
we sum O(|s|3) for every rule s we obtain a time
and space complexity of O(|G|3).
We now investigate formal properties of our
grammar transformation, in order to relate it to pre-
fix probabilities. We define the relation ` between P
and Pprefix such that s ` s? if and only if s? was ob-
tained from s by the transformation described above.
This is extended in a natural way to derivations, such
that s1 ? ? ? sd ` s?1 ? ? ? s
?
d? if and only if d = d
? and
si ` s?i for each i (1 ? i ? d).
The formal relation between G and Gprefix is re-
vealed by the following two lemmas.
Lemma 1 For each v1, v2, w1, w2 ? ?? and
? ? P ? such that [S, S] ??G [v1w1, v2w2], there
is a unique ?? ? P ?prefix such that [S
?, S?] ??
?
Gprefix
[v1, v2] and ? ` ??. 2
Lemma 2 For each v1, v2 ? ?? and derivation
?? ? P ?prefix such that [S
?, S?] ??
?
Gprefix
[v1, v2],
there is a unique ? ? P ? and unique w1, w2 ? ??
such that [S, S] ??G [v1w1, v2w2] and ? ` ?
?. 2
The only non-trivial issue in the proof of Lemma 1
is the uniqueness of ??. This follows from the obser-
vation that the length of v1 in v1w1 uniquely deter-
mines how occurrences of left components of rules
in P found in ? are mapped to occurrences of left
components of rules in Pprefix found in ??. The same
applies to the length of v2 in v2w2 and the right com-
ponents.
Lemma 2 is easy to prove as the structure of the
transformation ensures that the terminals that are in
rules from P but not in the corresponding rules from
Pprefix occur at the end of a string v1 (and v2) to form
the longer string v1w1 (and v2w2, respectively).
The transformation also ensures that s ` s? im-
plies pG(s) = pGprefix(s
?). Therefore ? ` ?? implies
pG(?) = pGprefix(?
?). By this and Lemmas 1 and 2
we may conclude:
Theorem 1 pGprefix([v1, v2]) = p
prefix
G ([v1, v2]). 2
Because of the introduction of rules with left-hand
sides of the formA? in both the left and right compo-
nents of synchronous rules, it is not straightforward
to do effective probabilistic parsing with the gram-
mar Gprefix. We can however apply the transforma-
tions from section 3 to eliminate epsilon rules and
thereafter eliminate unit rules, in a way that leaves
the derived string pairs and their probabilities un-
changed.
The simplest case is when the source grammar G
is reduced, proper and consistent, and has no epsilon
rules. The only nullable pairs of nonterminals in
Gprefix will then be of the form [A?1, A
?
2]. Consider
such a pair [A?1, A
?
2]. Because of reduction, proper-
ness and consistency of G we have:
?
w1,w2???, ??P ? s.t.
[A 11 , A
1
2 ]?
?
G[w1, w2]
pG(?) = 1
Because of the structure of the grammar transforma-
tion by which Gprefix was obtained from G, we also
have:
?
??P ? s.t.
[A? 11 , A
? 1
2 ]?
?
Gprefix
[?, ?]
pGprefix(?) = 1
Therefore pairs of occurrences of A?1 and A
?
2 with
the same index in synchronous rules of Gprefix
can be systematically removed without affecting the
probability of the resulting rule, as outlined in sec-
tion 3. Thereafter, unit rules can be removed to allow
parsing by the inside algorithm for PSCFGs.
Following the computational analyses for all of
the constructions presented in section 3, and for the
grammar transformation discussed in this section,
we can conclude that the running time of the pro-
posed algorithm for the computation of prefix prob-
abilities is dominated by the running time of the in-
side algorithm, which in the worst case is exponen-
tial in |G|. This result is not unexpected, as already
pointed out in the introduction, since the recogni-
tion problem for PSCFGs is NP-complete, as estab-
lished by Satta and Peserico (2005), and there is a
straightforward reduction from the recognition prob-
lem for PSCFGs to the problem of computing the
prefix probabilities for PSCFGs.
467
One should add that, in real world machine trans-
lation applications, it has been observed that recog-
nition (and computation of inside probabilities) for
SCFGs can typically be carried out in low-degree
polynomial time, and the worst cases mentioned
above are not observed with real data. Further dis-
cussion on this issue is due to Zhang et al (2006).
5 Discussion
We have shown that the computation of joint prefix
probabilities for PSCFGs can be reduced to the com-
putation of inside probabilities for the same model.
Our reduction relies on a novel grammar transfor-
mation, followed by elimination of epsilon rules and
unit rules.
Next to the joint prefix probability, we can also
consider the right prefix probability, which is de-
fined by:
pr?prefixG ([v1, v2]) =
?
w
pG([v1, v2w])
In words, the entire left string is given, along with a
prefix of the right string, and the task is to sum the
probabilities of all string pairs for different suffixes
following the given right prefix. This can be com-
puted as a special case of the joint prefix probability.
Concretely, one can extend the input and the gram-
mar by introducing an end-of-sentence marker $.
Let G? be the underlying SCFG grammar after the
extension. Then:
pr?prefixG ([v1, v2]) = p
prefix
G? ([v1$, v2])
Prefix probabilities and right prefix probabilities
for PSCFGs can be exploited to compute probability
distributions for the next word or part-of-speech in
left-to-right incremental translation of speech, or al-
ternatively as a predictive tool in applications of in-
teractive machine translation, of the kind described
by Foster et al (2002). We provide some technical
details here, generalizing to PSCFGs the approach
by Jelinek and Lafferty (1991).
Let G = (G, pG) be a PSCFG, with ? the alpha-
bet of terminal symbols. We are interested in the
probability that the next terminal in the target trans-
lation is a ? ?, after having processed a prefix v1 of
the source sentence and having produced a prefix v2
of the target translation. This can be computed as:
pr?wordG (a | [v1, v2]) =
pprefixG ([v1, v2a])
pprefixG ([v1, v2])
Two considerations are relevant when applying
the above formula in practice. First, the computa-
tion of pprefixG ([v1, v2a]) need not be computed from
scratch if pprefixG ([v1, v2]) has been computed al-
ready. Because of the tabular nature of the inside al-
gorithm, one can extend the table for pprefixG ([v1, v2])
by adding new entries to obtain the table for
pprefixG ([v1, v2a]). The same holds for the compu-
tation of pprefixG ([v1b, v2]).
Secondly, the computation of pprefixG ([v1, v2a]) for
all possible a ? ? may be impractical. However,
one may also compute the probability that the next
part-of-speech in the target translation isA. This can
be realised by adding a rule s? : [B ? b, A ? cA]
for each rule s : [B ? b, A ? a] from the source
grammar, where A is a nonterminal representing a
part-of-speech and cA is a (pre-)terminal specific to
A. The probability of s? is the same as that of s. If
G? is the underlying SCFG after adding such rules,
then the required value is pprefixG? ([v1, v2 cA]).
One variant of the definitions presented in this pa-
per is the notion of infix probability, which is use-
ful in island-driven speech translation. Here we are
interested in the probability that any string in the
source language with infix v1 is translated into any
string in the target language with infix v2. However,
just as infix probabilities are difficult to compute
for probabilistic context-free grammars (Corazza et
al., 1991; Nederhof and Satta, 2008) so (joint) infix
probabilities are difficult to compute for PSCFGs.
The problem lies in the possibility that a given in-
fix may occur more than once in a string in the lan-
guage. The computation of infix probabilities can
be reduced to that of solving non-linear systems of
equations, which can be approximated using for in-
stance Newton?s algorithm. However, such a system
of equations is built from the input strings, which en-
tails that the computational effort of solving the sys-
tem primarily affects parse time rather than parser-
generation time.
468
References
S. Abney, D. McAllester, and F. Pereira. 1999. Relating
probabilistic grammars and automata. In 37th Annual
Meeting of the Association for Computational Linguis-
tics, Proceedings of the Conference, pages 542?549,
Maryland, USA, June.
A.V. Aho and J.D. Ullman. 1969. Syntax directed trans-
lations and the pushdown assembler. Journal of Com-
puter and System Sciences, 3:37?56.
Z. Chi. 1999. Statistical properties of probabilistic
context-free grammars. Computational Linguistics,
25(1):131?160.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
A. Corazza, R. De Mori, R. Gretter, and G. Satta.
1991. Computation of probabilities for an island-
driven parser. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 13(9):936?950.
G. Foster, P. Langlais, and G. Lapalme. 2002. User-
friendly text prediction for translators. In Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 148?155, University of Pennsylvania,
Philadelphia, PA, USA, July.
M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004.
What?s in a translation rule? In HLT-NAACL 2004,
Proceedings of the Main Conference, Boston, Mas-
sachusetts, USA, May.
D. Gildea and D. Stefankovic. 2007. Worst-case syn-
chronous grammar rules. In Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Proceedings of the Main Conference, pages 147?
154, Rochester, New York, USA, April.
M. Hopkins and G. Langmead. 2010. SCFG decod-
ing without binarization. In Conference on Empirical
Methods in Natural Language Processing, Proceed-
ings of the Conference, pages 646?655, October.
F. Jelinek and J.D. Lafferty. 1991. Computation of the
probability of initial substring generation by stochas-
tic context-free grammars. Computational Linguistics,
17(3):315?323.
S. Kiefer, M. Luttenberger, and J. Esparza. 2007. On the
convergence of Newton?s method for monotone sys-
tems of polynomial equations. In Proceedings of the
39th ACM Symposium on Theory of Computing, pages
217?266.
C.D. Manning and H. Schu?tze. 1999. Foundations of
Statistical Natural Language Processing. MIT Press.
D. McAllester. 2002. On the complexity analysis of
static analyses. Journal of the ACM, 49(4):512?537.
M.-J. Nederhof and G. Satta. 2008. Computing parti-
tion functions of PCFGs. Research on Language and
Computation, 6(2):139?162.
G. Satta and E. Peserico. 2005. Some computational
complexity results for synchronous context-free gram-
mars. In Human Language Technology Conference
and Conference on Empirical Methods in Natural Lan-
guage Processing, pages 803?810.
S.M. Shieber, Y. Schabes, and F.C.N. Pereira. 1995.
Principles and implementation of deductive parsing.
Journal of Logic Programming, 24:3?36.
S. Sippu and E. Soisalon-Soininen. 1988. Parsing
Theory, Vol. I: Languages and Parsing, volume 15
of EATCS Monographs on Theoretical Computer Sci-
ence. Springer-Verlag.
A. Stolcke. 1995. An efficient probabilistic context-free
parsing algorithm that computes prefix probabilities.
Computational Linguistics, 21(2):167?201.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?404.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proceedings of the Human Language
Technology Conference of the NAACL, Main Confer-
ence, pages 256?263, New York, USA, June.
469
Proceedings of the 2010 Workshop on Applications of Tree Automata in Natural Language Processing, ACL 2010, pages 37?45,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Transforming Lexica as Trees
Mark-Jan Nederhof
University of St Andrews
North Haugh, St Andrews
KY16 9SX
Scotland
Abstract
We investigate the problem of structurally
changing lexica, while preserving the in-
formation. We present a type of lexicon
transformation that is complete on an in-
teresting class of lexica. Our work is mo-
tivated by the problem of merging one or
more lexica into one lexicon. Lexica, lexi-
con schemas, and lexicon transformations
are all seen as particular kinds of trees.
1 Introduction
A standard for lexical resources, called Lexical
Markup Framework (LMF), has been developed
under the auspices of ISO (Francopoulo et al,
2006). At its core is the understanding that most
information represented in a lexicon is hierarchi-
cal in nature, so that it can be represented as a
tree. Although LMF also includes relations be-
tween nodes orthogonal to the tree structure, we
will in this paper simplify the presentation by
treating only purely tree-shaped lexica.
There is a high demand for tools supporting the
merger of a number of lexica. A few examples
of papers that express this demand are Chan Ka-
Leung and Wu (1999), Jing et al (2000), Mona-
chini et al (2004) and Ruimy (2006). A typical
scenario is the following. The ultimate goal of
a project is the creation of a single lexicon for a
given language. In order to obtain the necessary
data, several field linguists independently gather
lexical resources. Despite efforts to come to agree-
ments before the start of the field work, there will
generally be overlap in the scope of the respec-
tive resources and there are frequently inconsis-
tencies both in the lexical information itself and
in the form in which information is represented.
In the latter case, the information needs to be re-
structured as part of the process of creating a sin-
gle lexicon.
We have developed a model of the merging pro-
cess, and experiments with an implementation are
underway. The actions performed by the tool are
guided by a linguist, but portions of the work may
also be done purely mechanically, if this is so
specified by the user. The purpose of the present
paper is to study one aspect of the adequacy of
the model, namely the restructuring of informa-
tion, with one input lexicon and one output lexi-
con. This corresponds to a special use of our tool,
which may in general produce one output lexicon
out of any number of input lexica.
As our lexica are trees, the use of well-
established techniques such as term unification
(Lloyd, 1984) and tree transduction (Fu?lo?p and
Vogler, 1998) seem obvious candidates for so-
lutions to our problem. Also technologies such
as XSL (Harold and Means, 2004) and XQuery
(Walmsley, 2007) spring to mind. We have chosen
a different approach however, which, as we will
show, has favourable theoretical properties.
The structure of this paper is as follows. The
type of lexicon that we consider is formalized in
Section 2, and lexicon transformations are dis-
cussed in Section 3. Section 4 shows that the pro-
posed type of lexicon transformation suffices to
map all ?reasonable? lexica to one another, as long
as they contain the same information. Conditions
under which transformations preserve information
are discussed in Section 5. A brief overview of an
implementation is given in Section 6.
2 Lexica and their structures
In this section, we formalize the notions of lexica,
lexicon structures, and their meanings, abstracting
37
away from details that are irrelevant to the discus-
sion that follows.
A lexicon schema S is a tuple (A,C, T ), where
A is a finite set of attributes, C is a finite set of
components (A ? C = ?), and T is a labelled,
unordered tree such that:
? each leaf node is labelled by an element from
A,
? each non-leaf node is labelled by an element
from C , and
? each element from A?C occurs exactly once.
A lexicon L is a tuple (A,V,C, t), where A is
as above, V is a set of values, C is as above, and t
is a labelled, unordered tree such that:
? each leaf node is labelled by an element from
A? V ,
? each non-leaf node is labelled by an element
from C ,
? if a leaf node with a label of the form (a, v1)
has a parent labelled c, then each leaf node
with a label of the form (a, v2) has a parent
labelled c, and
? if a non-leaf node labelled c1 has a parent la-
belled c2, then each non-leaf node labelled c1
has a parent labelled c2.
Due to the last two constraints, we may compare
lexica and lexicon schemata. In order to simplify
this comparison, we will assume that in a lexicon,
A and C only contain elements that occur in t.
This is without loss of generality, as unused ele-
ments of A and C can be omitted. We will also
assume that t contains at least two nodes, so that
the root is not a leaf.
We say a lexicon L = (AL, V, CL, t) is an in-
stance of lexicon schema S = (AS , CS , T ) if
AL ? AS , CL ? CS , and furthermore:
? the label of the root of t equals the label of
the root of T ,
? if a leaf node of t with a label of the form
(a, v1) has a parent labelled c, then the leaf
node of T labelled a has a parent labelled c,
and
? if a non-leaf node of t labelled c1 has a par-
ent labelled c2, then the non-leaf node of T
labelled c1 has a parent labelled c2.
Lexicon
lang Entry
Key
lemma pos
Meaning
gloss example
Figure 1: A lexicon schema S.
Examples of a lexicon schema and a lexicon are
given in Figures 1 and 2. For the sake of succinct-
ness, an attribute-value pair such as (example, ?Er
ist mit dem Zug gefahren?) is commonly separated
by =, and where it is required for graphical rea-
sons, the value may be drawn beneath the attribute,
stretched out vertically.
On a number of occasions in the constructions
and proofs that follow, it is convenient to assume
that the root node of a lexicon schema has exactly
one child. If this does not hold, as in the run-
ning example, we may introduce an artificial root
node labelled by an artificial component, denoted
by ?$?, which has the conceptual root node as only
child. We will refer to the lexicon schema that
results as an extended lexicon schema. (Cf. the
theory of context-free grammars, which are often
extended with a new start symbol.) As a conse-
quence, a lexicon that is an instance of an extended
lexicon schema may, in pathological cases, have
several nodes that are labelled by the conceptual
root component of the schema.
The components in lexicon schemata and lexica
provide a means of structuring sets of attributes, or
sets of attribute-value pairs respectively, into tree-
shaped forms. The discussion that follows will
treat components and structure as secondary, and
will take attributes and attribute-value pairs as the
primary carriers of information.
A lexicon base B is a tuple (A,V, I), where A
and V are as above, and I is a finite non-empty set
of items, each of which is a partial function from
A to V , defined on at least one attribute. Such
partial functions will also be represented as non-
empty sets of attribute-value pairs, in which each
attribute occurs at most once.
38
Lexicon
lang=German Entry
Key
lemma
=
fahren
pos
=
V
Meaning
gloss
=
drive
example
=
Ein
Fahrrad
fahren
Meaning
gloss
=
go
example
=
Er
ist
mit
dem
Zug
gefahren
Entry
Key
lemma
=
Fahrrad
pos
=
N
Meaning
gloss
=
bicycle
example
=
Ein
Fahrrad
fahren
example
=
Mein
Fahrrad
hat
einen
Platten
Figure 2: A lexicon L that is an instance of S from Figure 1.
Let L = (A,V,C, t) be a lexicon, where r is the
root of t. Its base, denoted by B(L), is (A,V, I)
with I = I(r), where the function I on nodes n of
the lexicon is defined as follows.
? For a leaf node n labelled by the attribute-
value pair (a, v), I(n) = {{(a, v)}}. In
words, the set I(n) contains only one item,
which is a partial function mapping attribute
a to value v.
? For a non-leaf node n, assume that m differ-
ent components or attributes d1, . . . , dm oc-
cur among the children. (Each element d is
either a component or an attribute.) Let Nj
(1 ? j ? m) be the set of children of n
labelled by dj if dj is a component or by
(dj , v), some value v, if dj is an attribute.
Then:
I(n) =
{?1 ? ? ? ? ? ?m | n1 ? N1, . . . , nm ? Nm,
?1 ? I(n1), . . . , ?m ? I(nm)}.
Note that by the definition of lexica and of N1, . . . ,
Nm, no attribute may occur both in ?i and in ?j if
i 6= j. This means that ?1 ? ? ? ? ? ?m is a partial
function as required.
For the lexicon of the running example, the base
is:
{ {lang=German, lemma=fahren, pos=V,
gloss=drive,
example=Ein Fahrrad fahren},
{lang=German, lemma=fahren, pos=V,
gloss=go,
example=Er ist mit dem Zug gefahren},
{lang=German, lemma=Fahrrad, pos=N,
gloss=bicycle,
example=Ein Fahrrad fahren},
{lang=German, lemma=Fahrrad, pos=N,
gloss=bicycle,
example=Mein Fahrrad hat einen Platten} }.
There are many different lexica however that share
the same base. This is illustrated by Figure 3. We
see that the information is presented in an entirely
different fashion, with a focus on the examples.
In a lexicon such as that in Figure 2, there may
be nodes labelled ?Meaning? without any children
corresponding to attribute ?example?. This means
that there would be items ? in B(L) such that
?(example) is undefined. For some of the con-
structions and proofs below, it is convenient to cir-
cumvent this complication, by assuming special
?null? values for absent leaf nodes for attributes.
As a result, we may treat an item as a complete
function rather than as a partial function on the do-
main A.
There is a certain resemblance between the base
of a lexicon and the disjunctive normal form of a
logical expression, the attribute-value pairs taking
the place of propositional variables, and the items
39
Phrasebook
lang=German Phrase
example
=
Ein
Fahrrad
fahren
Word
lemma
=
fahren
pos
=
V
gloss
=
drive
Word
lemma
=
Fahrrad
pos
=
N
gloss
=
bicycle
Phrase
example
=
Er
ist
mit
dem
Zug
gefahren
Word
lemma
=
fahren
pos
=
V
gloss
=
go
Phrase
example
=
Mein
Fahrrad
hat
einen
Platten
Word
lemma
=
Fahrrad
pos
=
N
gloss
=
bicycle
Figure 3: A lexicon L? with the same base as the one in Figure 2.
taking the place of conjunctions. Thus our seman-
tic interpretation of lexica is such that two siblings
in the tree are regarded as alternatives (disjunc-
tion) if their labels contain the same attribute or
component, and they are regarded as joint infor-
mation (conjunction) if their labels contain distinct
attributes or components.
Theorem 1 For each lexicon base B =
(AB , V, I) and for each lexicon schema S =
(AS , C, T ) with AB ? AS , there is a lexicon L
that is an instance of S and whose base is B.
Proof Assume the root r of T has only one child
r?. (Otherwise, make S extended first.) Let T ? be
the subtree of T at r?. For each item ? ? I , create
a copy of T ?, denoted by t?. At each leaf node of
t?, supplement the label a with the corresponding
value from ? if any; if a does not occur in ?, then
remove the leaf node from t?. (If the parent of a
removed leaf node has no other children, then also
remove the parent, etc.) Create a root node, with
the same label as r, the children of which are the
roots of the respective t?. Let the resulting tree be
called t. The requirements of the theorem are now
satisfied by L = (AB , V, C, t).
3 Lexicon transformations
As we have seen, the information contained in one
lexicon base may be rendered in different struc-
tural forms, in terms of lexica. The structure of a
lexicon is isolated from its content by means of a
lexicon schema. In this section we will address the
question how we may formalize transformations
from one lexicon schema S1 to lexicon schema S2,
or more precisely, from one class of lexica that are
instances of S1 to another class of lexica that are
instances of S2. In fact, for the sake of the defini-
tions below, we assume that the input to a transfor-
mation is not a lexicon but its base, which contains
all the necessary information. (That the actual im-
plementation mentioned in Section 1 may often
avoid expansion to the base need not concern us
here.)
A lexicon transformation R is a tuple (A,C, ?),
where A is a finite set of attributes as before, C
is a finite set of components as before, and ? is a
labelled, unordered tree such that:
? each leaf node is labelled by an element from
A,
? the root node is labelled by an element from
C ,
? each internal node is either labelled by an el-
ement from C , or by a subset of A,
? each element from A?C occurs exactly once
as a label by itself,
? each element from A occurs exactly once in
a label that is a subset of A, and
? each node ? labelled by a set {a1, . . . , ak}
? A has exactly one child, which is labelled
40
by an element from A ? C , and the leaves
labelled a1, . . . , ak are each descendants of
?.
A lexicon transformation is very similar to a lex-
icon schema, except for the extra nodes labelled
by sets A? ? A of attributes, which we refer to
as restrictors. Such a node indicates that for the
purpose of the subtree, one should commit to par-
ticular subsets of the input lexicon base. Each such
subset is determined by a choice of a fixed value
for each attribute in A?.
As an example, consider the lexicon transfor-
mations in Figure 4(a) and (b). If we omit the
nodes labelled by restrictors, then we obtain a lex-
icon schema. In the case of (b), this is the lexi-
con schema in Figure 1. In Figure 4(a), the node
labelled {example} means that the transforma-
tion takes one non-empty subset of the base for
each possible value of attribute ?example?. For
each subset, one node labelled ?Phrase? is gener-
ated in the target lexicon. At the node labelled
{lemma,pos}, the subset of the base is further re-
stricted, and for each combination of a value of
?lemma? and a value of ?pos? in the current sub-
set of items, a node labelled ?Word? is generated.
If the base contains several glosses for one choice
of ?example?, ?lemma? and ?pos?, each such gloss
leads to a separate leaf node.
The meaning of a lexicon transformation is for-
mally expressed in Figure 5. A call lexicon(?, I ?),
where ? is a node of ? and I ? is a subset of I from
the input lexicon base B = (A,V, I), returns a set
of nodes. The function is recursive, in that the
value of lexicon(?, I ?) is expressed in terms of val-
ues of lexicon(? ?, I ??) for child nodes ? ? of ? and
subsets I ?? of I ?. The main purpose is the computa-
tion of lexicon(?, I), where ? is the root of ? . As ?
is labelled by an element from C , lexicon(?, I) is
by definition a singleton set {r}, with r becoming
the root of the resulting lexicon.
Note that the placement of restrictors is criti-
cal. For example, if we were to move up the re-
strictor {gloss} in Figure 4(b) to merge with the
restrictor {lemma,pos}, this would result in one
entry for each combination of ?lemma?, ?pos? and
?gloss?, and in each entry there would be at most
one meaning. It is not apparent that such a choice
would be less appropriate than the choice we made
in Figures 2 and 4(b). However, if we were to
move down the node labelled {gloss} to become a
child of the node labelled ?Meaning? and a parent
Phrasebook
{lang}
lang
{example}
Phrase
example {lemma, pos}
Word
lemma pos {gloss}
gloss
(a)
Lexicon
{lang}
lang
{lemma, pos}
Entry
Key
lemma pos
{gloss}
Meaning
gloss {example}
example
(b)
Figure 4: Two lexicon transformations: (a) is ca-
pable of mapping the base of lexicon L (Figure 2)
to lexicon L? (Figure 3), and (b) is capable of the
reverse mapping.
of the leaf node labelled ?gloss?, then we would
lose the coupling between glosses and examples,
which seems undesirable. This observation under-
lies much of the development in Section 5.
4 Completeness
Next, we investigate whether the lexicon transfor-
mations as we defined them are powerful enough
to produce ?reasonable? lexica starting from a lex-
icon base. As unreasonable, we reject those lexica
that contain information that cannot be expressed
in terms of a base. This concerns siblings in the
tree with the same component label. How many
siblings with the same component should be gen-
erated can be deduced from the base, provided we
may assume that there is a combination of attribute
values that distinguishes one sibling from another.
41
lexicon(?, I ?) :
if the label of ? is a ? A
let v be the (only) value such that ?? ? I ?[?(a) = v]
create a new node n with label (a, v)
return {n}
else if the label of ? is c ? C
let the children of ? be ?1, . . . , ?m
create a new node n with label c and children
?
1?i?m
lexicon(?i, I ?)
return {n}
else if the label of ? is A? = {a1, . . . , ak} ? A
let the only child of ? be ? ?
let I be the set of all I ?? such that there is a combination of
v1, . . . , vk ? V with I ?? = {? ? I ? | ?(a1) = v1, . . . , ?(ak) = vk} 6= ?
return
?
I???I lexicon(? ?, I ??)
Figure 5: The meaning of a lexicon transformation, as a recursive function. The return value is a set of
nodes that are created. The main application is lexicon(?, I), where ? is the root of ? and I is taken from
the input lexicon base.
We call such a combination of attributes a key.
Formally, a key mapping for a lexicon schema
(A,C, T ) is a function f that maps each compo-
nent from C to a subset of A, subject to the fol-
lowing restrictions. Let c be a component and let
n be the node of T that is labelled by c. Then for
each attribute a in key f(c), the leaf node of T that
is labelled by a should be a descendant of n. The
component that occurs as label of the root of T is
always mapped to the empty set of attributes, and
may be ignored in the following discussion.
Let lexicon L = (AL, V, CL, t) be an instance of
schema S = (AS , CS , T ). We say that L satisfies
the key mapping f for S if:
1. among the leaves, there is no pair of distinct
siblings in t with identical labels, and
2. for each maximal set {n1, . . . , nm} of sib-
lings in t labelled by the same component c,
with f(c) = {a1, . . . , ak}, we have that for
each i (1 ? i ? m), there is a distinct combi-
nation of values v1, . . . , vk ? V such that:
I(ni) = {? ?
?
1?j?m
I(nj) | ?(a1) = v1, . . . ,
?(ak) = vk}.
The second condition states that the total set of
items coming from all siblings with the same label
c is partitioned on the basis of distinct combina-
tions of values for attributes from the key, and the
subsets of the partition come from the respective
siblings.
Returning to the running example, the lexicon L
in Figure 2 satisfies the key mapping f given by:
f(Lexicon) = ?
f(Entry) = {lemma,pos}
f(Key) = ?
f(Meaning) = {gloss}
A different key mapping exists for the lexicon L?
in Figure 3.
If n1 and n2 are two distinct nodes in the tree
T of schema S, with labels c1 and c2, respec-
tively, then we may assume that f(c1) and f(c2)
are disjoint, for the following reason. Suppose that
the intersection of f(c1) and f(c2) includes an at-
tribute a, then n1 must be a descendant of n2 or
vice versa, because the leaf labelled a must be a
descendant of both n1 and n2. Assume that n1 is a
descendant of n2. As the base is already restricted
at n1 to items ? with ?(a) = v, for certain v, a
may be omitted from f(c2) without changing the
semantics of the key mapping. This observation is
used in the construction in the proof of the follow-
ing.
Theorem 2 Let lexicon L = (AL, V, CL, t) be an
instance of schema S = (AS , CS , T ), satisfying
key mapping f . Then there is a lexicon transfor-
mation that maps B(L) to L.
Proof The required lexicon transformation is
constructed out of T and f . We insert an ad-
ditional restrictor node just above each non-leaf
node labelled c, and as the restrictor we take f(c).
42
(If f(c) = ?, we may abstain from adding a restric-
tor node.) If an attribute a does not occur in f(c),
for any c ? CS , then we add a restrictor node with
set {a} just above the leaf node labelled a. The
result is the tree ? of a lexicon transformation R =
(AS , CS , ?).
It is now straightforward to prove that R maps
B(L) to L, by induction on the height of T , on
the basis of the close similarity between the struc-
ture of T and the structure of ? , and the close link
between the chosen restrictors and the keys from
which they were constructed.
For the running example, the construction in the
proof above leads to the transformation in Fig-
ure 4(b).
Theorem 2 reveals the conditions under which
the structure of a lexicon can be retrieved from
its base, by means of a transformation. Simulta-
neously, it shows the completeness of the type of
lexicon transformation that we proposed. If a lexi-
con L is given, and if an alternative lexicon L? with
B(L?) = B(L) exists that is an instance of some
schema S and that is ?reasonable? in the sense that
it satisfies a key mapping for S, then L? can be ef-
fectively constructed from L by the derived trans-
formation.
5 Consistency
We now investigate the conditions under which
a lexicon transformation preserves the base. The
starting point is the observation at the end of Sec-
tion 3, where we argued that if a restrictor is cho-
sen too low in the tree ? relative to other restric-
tors, then some necessary dependence between at-
tribute values is lost. Note that the proof of Theo-
rem 1 suggests that having only one restrictor with
all attributes at the root of the tree always pre-
serves the base, but the result would be unsatis-
factory in practice.
For a set A of attributes, we define an indepen-
dence system D as a set of triples (A1, A2, A3)
where A1, A2, A3 ? A and A1 ? A2 = ?. We
pronounce (A1, A2, A3) ? D as ?A1 and A2 are
independent under A3?. It should be noted that A3
may overlap with A1 and with A2.
We say a lexicon base (A,V, I) satisfies D if
for each (A1, A2, A3) ? D with A1 = {a1,1,
. . . a1,k1}, A2 = {a2,1, . . . a2,k2}, A3 = {a3,1,
. . . a3,k3}, and for each combination of values v1,1,
. . . , v1,k1 , v2,1, . . . , v2,k2 , v3,1, . . . , v3,k3 , we have:
?? ? I[?(a1,1) = v1,1 ? . . . ? ?(a1,k1) = v1,k1 ?
?(a3,1) = v3,1 ? . . . ? ?(a3,k3) = v3,k3] ?
?? ? I[?(a2,1) = v2,1 ? . . . ? ?(a2,k2) = v2,k2 ?
?(a3,1) = v3,1 ? . . . ? ?(a3,k3) = v3,k3]
=?
?? ? I[?(a1,1) = v1,1 ? . . . ? ?(a1,k1) = v1,k1 ?
?(a2,1) = v2,1 ? . . . ? ?(a2,k2) = v2,k2 ?
?(a3,1) = v3,1 ? . . . ? ?(a3,k3) = v3,k3].
The intuition is that as long as the values for A3 are
fixed, allowable combinations of values for A1 ?
A2 in I can be found by looking at A1 and A2
individually.
We say that a lexicon transformation R =
(A,C, ?) is allowed by an independence system
D if the following condition is satisfied for each
node ? in ? that is labelled by a component c and
a node ? ? that is its child: Let A1 be the set of at-
tributes at leaves that are descendants of ? ?, and
let A2 be the set of attributes at leaves that are de-
scendants of the other children of ?. Let A3 be
the union of the restrictors at ancestors of ?. Now
(A1, A2, A3) should be in D.
Theorem 3 If a lexicon base B = (A,V, I) satis-
fies an independence system D, if a lexicon trans-
formation R is allowed by D, and if R maps B to
lexicon L, then B(L) = B.
The proof by induction on the height of ? is
fairly straightforward but tedious.
In the running example, there are a num-
ber of triples in D but most are trivial, such
as (?, {gloss, example}, {lemma,pos}).
Another triple in D is ({lang},
{lemma,pos, gloss, example}, ?), but only
because we assume in this example that one
lexicon is designed for one language only. In
general, there will be more interesting indepen-
dency, typically if a lexical entry consists of a
number of unconnected units, for example one
explaining syntactic usage of a word, another
explaining semantic usage, and another presenting
information on etymology.
The implication of Theorem 3 is that transfor-
mations between lexica preserve the information
that they represent, as long as the transforma-
tions respect the dependencies between sets of at-
tributes. Within these bounds, an attribute a may
be located in a restrictor in ? anywhere between
the root node and the leaf node labelled a.
43
6 Implementation
The mathematical framework in this paper mod-
els a restricted case of merging and restructuring
a number of input lexica. An implementation was
developed as a potential new module of LEXUS,
which is a web-based tool for manipulating lexi-
cal resources, as described by Kemps-Snijders et
al. (2006).
The restriction considered here involves only
one input lexicon, and we have abstracted away
from a large number of features present in the ac-
tual implementation, among which are provisions
to interact with the user, to access external linguis-
tic functions (e.g. morphological operations), and
to rename attributes. These simplifications have
allowed us to isolate one essential and difficult
problem of lexicon merging, namely how to carry
over the underlying information from one lexicon
to another, in spite of possible significant differ-
ences in structure.
The framework considered here assumes that
during construction of the target lexicon, the infor-
mation present in the source lexicon is repeatedly
narrowed down by restrictors, as explained in Sec-
tion 3. Each restrictor amounts to a loop over all
combinations of the relevant attribute values from
the currently considered part of the source lexicon.
Let us consider a path from the root of the lexi-
con transformation to a leaf, which may comprise
several restrictors. The number of combinations of
attribute values considered is bounded by an expo-
nential function on the total number of attributes
contained in those restrictors. Motivated by this
consideration, we have chosen to regard a lexicon
transformation as if its input were an expanded
form of the source lexicon, or in other words, a
lexicon base.
However, in terms of the actual implementation,
the discussed form of restrictors must be seen as a
worst case, which is able to realize some of the
most invasive types of restructuring. Next to re-
strictors that select combinations of attribute val-
ues, our lexicon transformations also allow prim-
itives that each represent a loop over all nodes of
the presently considered part of the source lexi-
con that are labelled by a chosen component or
attribute. By using only such primitives, the time
complexity remains polynomial in the size of the
input lexicon and the size of the input lexicon
transformation. This requires an implementation
that does not expand the information contained in
a source lexicon in terms of a lexicon base. A
full description of the implementation would go
beyond the context of this paper.
7 Conclusions
We have introduced a class of lexicon transfor-
mations, and have shown interesting completeness
and consistency properties.
The restrictors in our lexicon transformations
are able to repeatedly narrow down the informa-
tion contained in the source lexicon based on at-
tribute values, while constructing the target lexi-
con from the top down. Existing types of tree ma-
nipulations, such as tree transducers, do not pos-
sess the ability to repeatedly narrow down a set
of considered nodes scattered throughout a source
structure, and therefore seem to be incapable of
expressing types of lexicon transformations allow-
ing the completeness results we have seen in this
paper.
One could in principle implement our lexicon
transformations in terms of technologies such as
XQuery and XSLT, but only in the sense that
these formalisms are Turing complete. Our restric-
tors do not have a direct equivalent in these for-
malisms, which would make our type of lexicon
transformation cumbersome to express in XQuery
or XSLT. At the same time, their Turing complete-
ness makes XQuery and XSLT too powerful to
be of practical use for the specification of lexicon
transformations.
A tentative conclusion seems to be that our class
of lexicon transformations has useful properties
not shared by a number of existing theories involv-
ing tree manipulations. This justifies further study.
Acknowledgements
This work was done while the author was em-
ployed at the Max Planck Institute for Psycholin-
guistics. The work was motivated by suggestions
from Peter Wittenburg and Marc Kemps-Snijders,
whose input is gratefully acknowledged.
References
D. Chan Ka-Leung and D. Wu. 1999. Automati-
cally merging lexicons that have incompatible part-
of-speech categories. In Joint SIGDAT Conference
on Empirical Methods in Natural Language Pro-
cessing and Very Large Corpora, pages 247?257,
University of Maryland, USA, June.
44
G. Francopoulo, N. Bel, M. George, N. Calzolari,
M. Monachini, M. Pet, and C. Soria. 2006. Lexi-
cal markup framework (LMF) for NLP multilingual
resources. In Proceedings of the Workshop on Mul-
tilingual Language Resources and Interoperability,
pages 1?8, Sydney, Australia, July.
Z. Fu?lo?p and H. Vogler. 1998. Syntax-Directed Se-
mantics: Formal Models Based on Tree Transduc-
ers. Springer, Berlin.
E.R. Harold and W.S. Means. 2004. XML in a Nut-
shell. O?Reilly.
H. Jing, Y. Dahan Netzer, M. Elhadad, and K.R. McK-
eown. 2000. Integrating a large-scale, reusable lex-
icon with a natural language generator. In Proceed-
ings of the First International Conference on Nat-
ural Language Generation, pages 209?216, Mitzpe
Ramon, Israel, June.
M. Kemps-Snijders, M.-J. Nederhof, and P. Witten-
burg. 2006. LEXUS, a web-based tool for manip-
ulating lexical resources. In LREC 2006: Fifth In-
ternational Conference on Language Resources and
Evaluation, Proceedings, pages 1862?1865.
J.W. Lloyd. 1984. Foundations of Logic Programming.
Springer-Verlag.
M. Monachini, F. Calzolari, M. Mammini, S. Rossi,
and M. Ulivieri. 2004. Unifying lexicons in view
of a phonological and morphological lexical DB. In
LREC 2004: Fourth International Conference on
Language Resources and Evaluation, pages 1107?
1110, Lisbon, Portugal, May.
N. Ruimy. 2006. Merging two ontology-based lexi-
cal resources. In LREC 2006: Fifth International
Conference on Language Resources and Evaluation,
Proceedings, pages 1716?1721.
P. Walmsley. 2007. XQuery. O?Reilly.
45
