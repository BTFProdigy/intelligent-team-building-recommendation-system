Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 19?26,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Freshmen?s CL curriculum: the benefits of redundancy ?
Heike Zinsmeister
Department of Linguistics
University of Konstanz
78457 Konstanz, Germany
Heike.Zinsmeister@uni-konstanz.de
Abstract
In the course of the European Bologna ac-
cord on higher education, German universities
have been reorganizing their traditional ?Mag-
ister? and ?Diplom? studies into modularized
bachelor?s and master?s programs. This re-
vision provides a chance to update the pro-
grams. In this paper we introduce the curricu-
lum of a first semester B.A. program in Com-
putational Linguistics which was taught for
the first time last semester. In addition, we an-
alyze the syllabi of four mandatory courses of
the first semester to identify overlapping con-
tent which led to redundancies. We suggest for
future semesters to reorganize the schedules in
a way that students encounter recurring topics
iteratively in a constructive way.
1 Introduction
We present the first semester curriculum of a newly
revised bachelor?s program in Computational Lin-
guistics at the University of Heidelberg, Germany,
which was taught for the first time at the Department
of Computational Linguistics last winter semester.
Four courses are mandatory for the students in
the first semester: a comprehensive Introduction to
Computational Linguistics, backed up with a course
on Formal Foundations that emphasizes mathemat-
ical topics, and a general introduction to linguistic
core modules in Foundations of Linguistic Analysis,
the set up is completed by an Introduction to Pro-
?This paper is about the curriculum taught at the Depart-
ment of Computational Linguistics at the University of Heidel-
berg, where the author used to work.
gramming that introduces core concepts of program-
ming employing the programming language Python.
The parallel design leads to a situation in which
related topics are introduced in the same semester
in parallel fashion. Redundant duplication per se is
to be avoided given that lecture time is always too
sparse and should be used most efficiently such that
there is enough room for examples, short in-course
exercises, questions and discussions.
We analyzed the syllabi for common topics and
plotted these topics to see whether they are dealt
with in a constructive way across the curricu-
lum. For future semesters we suggest some re-
organization to optimize the courses? interactions.
Since all courses are taught in the department of
Computational Linguistics, decisions on both the
courses? subtopics as well as their temporal sequenc-
ing is in full control of the local department.
We think that it is reasonable to keep the com-
mon topics and even the redundancy of introducing
them in more than one course only. Iterative re-
introduction could be helpful for the students if it
is accompanied by a reference to the earlier men-
tion as well as a motivation of the specific relevance
for the course at hand. We expect that such an iter-
ative approach reinforces understanding since it al-
lows the students to build upon their prior knowl-
edge and, furthermore, to approach the very same
concept from different perspectives. This iterative
method is inspired by the idea of spiral learning in
the sense of Jerome S. Bruner (Bruner, 1960) which
builds on a constructivist view on learning. It as-
sumes that learning is an active process in which
learners construct new ideas or concepts based upon
19
their prior knowledge. A curriculum can support this
process if it revisits its basic ideas repeatedly: ?the
spiral curriculum [..] turns back on itself at higher
levels? (Bruner, 1960, p.53).
The rest of this paper is organized as follows.
First, we briefly sketch the Bologna Process, an ef-
fort of harmonizing higher education in Europe and
also the special situation in Heidelberg being the
background against which the bachelor?s program
described is created. Then, we introduce the bach-
elor?s program of Computational Linguistics at the
University of Heidelberg in Germany and describe
its four mandatory courses of the first semester. We
analyze the syllabi for common topics, and, finally,
present a re-organized schedule for future semesters
which is inspired by an iterative learning approach.
2 Background
The European Bologna Process is an effort of Eu-
ropean countries to establish a common higher ed-
ucation area by the year 2010. Its central element
is the introduction of a two-cycle study system con-
sisting of bachelor?s and master?s degrees with com-
parable qualifications throughout Europe based on
a common credit transfer system which allows for
comparing the workload of individual courses.1
In the course of this international harmonizing ef-
fort, German universities are reorganizing their pro-
grams from traditional ?Magister? or ?Diplom? pro-
grams to modular bachelor?s and master?s programs.
Previously ?Magister? or ?Diplom? was first degree
in Germany, i.e. a bachelor?s degree did not exist.
A characteristic of the traditional programs was the
freedom of choice they offered to their students,
more pronounced in the ?Magister? programs than
in the ?Diplom? programs the latter of which were
traditionally realized in more technically oriented
disciplines and the former in the humanities. Both
type of programs were set up with a standard period
of study of nine to ten semesters but the average stu-
dent required more than this. European bachelor?s
programs predetermine a highly structured curricu-
1One European Credit Transfer System point cor-
responds to 25-30 hours workload on the student cf.
http://www.uni-heidelberg.de/studium/bologna/
materialien/diploma/ECTSUsersGuide05.pdf. For the
Bologna Process in general see http://ec.europa.eu/
education/policies/educ/bologna/bologna_en.html.
lum and offer a first university degree after six or
seven semester of study.
The Computational Linguistics department in
Heidelberg was pioneering with an early bachelor?s
program devised by Peter Hellwig at the beginning
of the Bologna Process. Adaptions of the orig-
inal bachelor?s program became necessary due to
general developments in the international Bologna
policy and finally the need for a revised program
emerged. This was realized in 2007 by Anette Frank
who had filled the by then vacant chair in Compu-
tational Linguistics. The change of the departmen-
tal head brought a change from a more vocationally
oriented program that prepared students to take jobs
in the local language technology industry to a more
academically oriented one, which is reflected in the
revised syllabus. We will point to differences be-
tween the original program and the revised program
where relevant.
3 The Study of Computational Linguistics
in Heidelberg
Computational linguistics (CL) is a
discipline between linguistics and com-
puter science which is concerned with the
computational aspects of the human lan-
guage faculty. [...] The applied compo-
nent of CL is more interested in the practi-
cal outcome of modeling human language
use. The goal is to create software prod-
ucts that have some knowledge of human
language. [...] Theoretical CL takes up
issues in formal theories. [...] Compu-
tational linguists develop formal models
simulating aspects of the human language
faculty and implement them as computer
programs. (www.aclweb.org/nlpfaq.txt,
credited to Hans Uszkoreit)
This quote from Hans Uszkoreit outlines the
knowledge and skills that a study of CL should equip
its students with: programming skills, handling of
formal models, algorithmic thinking and last but not
least an explicit knowledge of linguistic analysis.
All four areas are covered in our freshmen?s
classes which are introduced in more detail in subse-
quent subsections after the presentation of the over-
all program.
20
In Heidelberg, B.A. students have to collect 180
credit points to complete their study. They nor-
mally enroll in two or three subjects which means
that they take Computational Linguistics as main
subject (in which it provides 75% of the overall
workload), secondary main subject (50%) or mi-
nor subject (25%) 2 in combination with comple-
mentary subjects in the areas of computer science3,
humanities, psychology, economics, or law. Table
1 gives an overview of the courses in a 75% B.A.
The first semester requirements are the same in all
B.A. options involving Computational Linguistics.4
In addition to the courses depicted in Table 1 stu-
dents need to gain credits in Generic Competences
(?u?bergreifende Kompetenzen? aka soft skills and
courses from other departments of the faculty).5
3.1 The Curriculum
We thought it relevant for the students to get ac-
quainted with Computational Linguistics proper as
early as the first semester. Therefore, in addition to
an introduction to formal foundations and program-
ming a comprehensive introduction to algorithms
and analysis in computational linguistics is manda-
tory. It was the first time that this combination of
courses was taught. Before that, the Introduction
to Computational Linguistics also introduced stu-
dents to core linguistic topics which were spread
across the whole course. The motivation for an inde-
pendent introduction to linguistics was that students
should get a profound background knowledge in lin-
guistic analysis such that further courses could build
on them. Before that, even basic concepts such as
morpheme had to be reintroduced. Introduction to
Programming and Formal Foundations used to be in
complementary distribution due to the fact that they
used to be taught by the one and the same person.
An additional lecturer position in the department al-
lowed us to to offer both courses in parallel.
The Freshmen?s curriculum consists of four
2The minor subject option had to be introduced due to for-
mal requirements. It is likely to be dispensed with in the future.
3Computer science can only be taken as minor subject.
4In the 25% B.A. the workload on students is reduced. They
only need to attend one of the two courses on formal founda-
tions either Mathematical Foundations in the first semester or
Logical Foundations in the second one.
5In the 75% B.A. students need to collect 20 credit points in
Generic Competences during their three-year study.
mandatory courses which are described in the fol-
lowing.
3.1.1 Introduction to Computational
Linguistics
The core lecture of the first semester is the Intro-
duction to Computational Linguistics. It is held four
hours a week and is worth six credit points. It in-
troduces the foundations of Computational Linguis-
tics, its research objectives and research methods. It
provides an overall survey of the field: the levels
of language description, formal-mathematical and
logical models as well as algorithmic approaches
for processing such formal models. Specific top-
ics are: dealing with ambiguities, approximation of
linguistic regularities, and the relation of language
and knowledge; some applications of Computational
Linguistics are also introduced. Mandatory read-
ings are selected sections from Jurafsky & Martin
(2000), complemented by chapters from Carstensen
et al (2004) and Bird et al (forthcoming).
This course is seen as the backbone of the first
semester curriculum. We therefore list the lectures
in detail. The content of the other three courses is
only briefly described below and will be discussed
in Section 4.
The first part of the schedule was strongly inspired
by Jurafsky & Martin (2000):
? Sub-token level (3 lectures): computing mor-
phology by means of regular expressions, au-
tomata, and transducers.
? Token level and context (4 lectures): identify-
ing tokens and computing them by means of
tokenizing, edit distance, n-grams, and part-of-
speech tagging.
? Syntactic level (6 lectures): syntactic analysis
in terms of constituency, dependency, phrase
structure grammars and probabilistic context
free grammars; formal grammar types: compu-
tation of syntactic structure by means of pars-
ing strategies and parsing algorithms, and syn-
tactic resources in terms of treebanks.
The second part of the schedule built more on
Carstensen et al (2004). It mainly dealt with se-
mantic issues in term of analysis, computation, and
resources.
21
Computational Linguistics Linguistic Computational
Semester Modules Modules Modules
6 BA-Thesis, Oral Exam
5 Advanced Studies (Computational Linguistics Core Studies in Software
or Formal Linguistics) Theoretical Project
4 Core Studies in Computational Linguistics or Applied
Computer
3 Statistical Methods Algorithmic CL Formal Semantics Science
for CL
2 Logical Formal Syntax Advanced Programming
Foundations
1 Introduction Mathematical Foundations of Introduction to
to CL Foundations Linguistic Analysis Programming
Table 1: Modules in B.A. Computational Linguistics (75%)
? predicate logic (2 lectures)
? propositional logic and inferences (2 lectures)
? compositional semantics and Lambda calculus
(1 lecture)
? lexical semantics including resources (2 lec-
tures)
? discourse semantics / pragmatics (1 lecture)
The schedule was rounded off by two lectures
on applications, in particular information extraction
and machine translation.
There were eight assessments during the semester
of which students had to pass 60%. Most of them
dealt with theoretical comprehension, two more
practical assessments involved an introduction to ba-
sic UNIX tools, and (probabilistic) parsing with the
NLTK tools (Bird et al, forthcoming). We decided
to split the written exam into two sub-exams, the first
one took place in half time the second one in the fi-
nal week of the semester. Thus students could better
focus on the topics at hand.
3.1.2 Formal Foundations part 1:
Mathematical Foundations
Formal Foundations is held two hours a week and
is worth six credit points. The theory of formal
languages is a prerequisite for e.g. model-theoretic
semantics and parsing approaches. This lecture in
particular deals with mathematical foundations, for-
mal languages and formal grammars, regular expres-
sions and finite automata, context-free languages,
context-sensitive languages and Type-0 languages,
Turing machines, and computability theory. The
recommended reading includes Scho?ning (2001),
Klabunde (1998), Partee et al (1990), as well as
Hopcroft and Ullman (1979).
There were eight graded assessments and the stu-
dents had to pass 50% of the overall tasks .
3.1.3 Foundations of Linguistic Analysis
The introduction to linguistics is also held two
hours a week and is worth four credit points. Lin-
guistic knowledge is a distinctive property of com-
putational linguistics. In this lecture students get a
thorough introduction to the core modules of the lan-
guage faculty: phonetics and phonology, morphol-
ogy, syntax, semantics, and pragmatics with a spe-
cial emphasis on linguistic phenomena of German.
The core reading was Meibauer et al (2002).
There were ten small assessments of which the
students had to pass eight.
3.1.4 Introduction to Programming
The fourth mandatory course is held four hours
a week and is worth six credit points. In this lec-
ture, students learn to devise algorithmic solutions
and implementations for problems related to Natu-
ral Language Processing. Moreover, the course in-
troduces basic principles of software engineering in
22
order to equip the students with skills to develop cor-
rect and maintainable programs. These capabilities
are further facilitated in the Advanced Programming
course during the second semester and a comprehen-
sive hands-on software project during the advanced
phase of undergraduate studies.
Recommended reading is Demleitner (unpub-
lished), Lutz and Ascher (2007), Martelli (2006),
as well as the official Python documentation (van
Rossum, 2008).
There were ten programming assessments of
which the students had to hand in eight and earn half
of the points to be permitted to take the final exam.
3.2 Local Conditions
3.2.1 Students
Students require higher education entrance quali-
fication and no other prerequisites. Language of in-
struction is German but students come from various
countries and speak a diversity of native languages,
including Bulgarian, Chinese, English, French, Ital-
ian, Japanese, Kurdish, Polish, Russian, Spanish,
Turkish, Turkmen and Ukrainian. About 40 stu-
dents enrolled in Computational Linguistics, about
two third of which classified themselves as program-
ming beginners. In general about 20% of the first
semester students failed at least one of the courses
first time.
3.2.2 Realization of Courses
Three of the four courses under examination are
taught by faculty members holding a PhD (or a
comparable doctoral degree) and one by a member
of the faculty still completing his doctorate. The
courses are taught as lectures which are accompa-
nied by optional tutorial sessions. These tutorials
were coached by undergraduate student tutors who
mainly corrected and discussed the students? assess-
ments. The students had to hand in assessments on a
regular basis which could either be solved as a group
or individually depending on the course. Passing a
substantial portion of the exercises was a prerequi-
site for being permitted to take the courses? exams.
Each course provided its own wiki platform for the
students to communicate easily among themselves
as well as with student tutors and lecturers. The
wikis were also a common platform for publishing
example solutions by the tutors and keeping records
of answers to students? questions.
4 Analysis of the Syllabi
The individual courses were planned in accordance
with the sequence of topics in standard textbooks
such as Jurafsky and Martin (2000) and Carstensen
et al (2004) for Introduction to Computational Lin-
guistics, Scho?ning (2001) for Formal Foundations,
and Meibauer et al (2002) for Foundations of Lin-
guistic Analysis. In Introduction to Programming
we used a hands-on manuscript (Demleitner, unpub-
lished).
The following list summarizes the main topics
that are dealt with in more than one syllabus. Com-
mon topics include:
? modules of linguistics: ICL, FLA
? regular expressions: ICL, FF, IP
? automata: ICL, FF
? grammar types: ICL, FF
? morphology: ICL, FLA
? segmentation, tokenization: ICL, FLA, IP
? n-grams: ICL, IP
? phrase-structure grammars: ICL, FF, FLA
? parsing: ICL, FF, IP
? lexical semantics: ICL, FLA
? model in semantics: ICL, FF
? discourse semantics, pragmatics: ICL, FLA
Before the semester started, the group of lectur-
ers met and arranged the general schedules of the
courses. During the semester, the lecturers happened
to lose track of the progression of other courses. In
some cases explicit cross-references were given, for
example in the case of lexical semantics, but most
of the time, concepts were (re-)introduced in each
course independently. Sometimes lecturers asked
students whether they were already familiar with
a newly introduced topic from other courses; then
there was a short discussion in class and students
23
were reminded of previous mentions of that topic. In
general, the didactics of the individual courses were
not adapted to take account of such recurrence of
topics across the curriculum.
Nevertheless, the parallel fashion of the four
courses at hand seemed to be reasonable even in
this form. Students deemed the interdependence be-
tween the courses as appropriate in the final evalua-
tion of the courses. They gave it an average score of
2.052 with a standard deviation of 1.05 on a scale of
1 (very appropriate) to 6 (non-existent).
Our conclusion is that a slight rescheduling of
the courses would improve teaching efficiency in
the sense that lecturers could count on already in-
troduced materials and students could benefit from
recurring topics by exploring them in the context of
different disciplines. Table 2 depicts our proposed
schedule.
An important and easily realizable change that we
suggest is to ensure that all linguistic modules are
dealt with first in Foundation of Linguistic Anal-
ysis (FLA) before they are set into a more formal
and also computational setting in the Introduction to
Computational Linguistics (ICL). This could be re-
alized by starting FLA with morphology right from
the beginning, instead of introducing the linguistic
modules first which was also part of the introduc-
tion in ICL. FLA also entered the areas of lexicogra-
phy and psycho linguistics (aka the mental lexicon)
which could be skipped in future semesters. Lec-
tures on phonetics and phonology which were taught
after morphology could be rescheduled to the end
of the semester. Both topics are relevant for appli-
cations which were introduced in the final sessions
of ICL and also for subsequent optional seminars
in speech generation or speech synthesis in higher
semesters.
In Formal Foundations (FF) lectures on gram-
mars, the Chomsky hierarchy, and decision theory
took place in lectures 5 and 6. They could be post-
poned and lectures on automata moved forward in-
stead. This would ensure that both of these topics
are dealt with in FF after they have been introduced
in ICL. Formal Foundations provides a more formal
and deepened insight into these topics and should,
therefore, be encountered last.
In Introduction to Programming (IP) issues of al-
gorithms and analysis are a means to an end: they
are used in programming examples and assessments.
Therefore, such topics should be referred to in IP
only after they have been introduced in ICL. The
coordination of this already worked out well with
respect to n-grams and phrase structure grammars.
Lectures on segmentation and regular expressions
took place in the last third of the semester and could
be moved forward to have them closer to their intro-
duction in the other courses.
From a student?s perspective these changes would
result in a kind of spiral curriculum. For example,
the first encounter with constituency and syntactic
phrase structure would be in FLA, the course which
is least formal and relates most to secondary school
knowledge. Their second involvement with phrase
structure would be in ICL and was more formal
and also involved computational aspects of syntactic
analysis. Then, they would learn more on the formal
characteristics of grammars in FF, and finally, they
perceived it as an application in an IP programming
task. If these lectures are seen as stages on a com-
mon pathway of learning then they conform to the
idea of spiral learning: in course of time the students
return to the same concepts each time on a more ad-
vanced level.
Table 2 gives a contrastive overview of the four
course curricula and shows how the individual topics
could temporally related to one another to support an
iterative leaning approach.
The first column counts the semester?s teaching
units in the average winter semester (which includes
some public holidays). Introduction to Computa-
tional Linguistics (ICL) and Introduction to Pro-
gramming (IP) took place twice a week, Foun-
dations of Linguistic Analysis (FLA) and Formal
Foundations (FF) only once. The 25th session is fol-
lowed by another week of revision and final exams,
which is not included here.
5 Conclusion
We proposed an enhanced curriculum for teaching
parallel freshman?s courses in Computational Lin-
guistics, in the spirit of the newly revised bache-
lor?s program in Computational Linguistics at the
University of Heidelberg. In particular, we exam-
ined the first semester curriculum of four mandatory
courses: Introduction to Computational Linguis-
24
Introduction to Formal Foundations of Introduction to
# Computational Linguistics Foundations Linguistic Analysis Programming
1 sets, introduction
iterations, relations
2 introduction to morphology: morphemes data types
Computational Linguistics inflection, derivation
and linguistic modules
3 regular expression equivalence relation functions and
and automata function, induction methods
formal languages
4 morphology and syntax: PoS, strings, data structures,
finite automata topological fields control structures
5 morphology and automata: sequences
finite transducers DFAs and NFAs
6 tokenizer and NFAs, regular grammars data structures:
spelling editor regular expression dictionaries
7 tokenizing syntax: phrases encodings
and n-grams chunks, X-bar schema
8 tagging: rule-based, Pumping lemma, modules,
HMMs, Brill minimizing of automata packages, tests
9 tagging syntax: valency, semantic modules
roles, gram. functions
10 syntax and CFGs closures exercise: n-grams
constituency, dependency
11 grammar types, syntax: sentential level regular expressions
parsing CP/IP structures
12 parsing: bottom up, grammars, left-right regular expressions
top down derivation, Chomsky hierarchy
13 parsing: Earley semantics: meaning, PS grammar,
algorithm lexical semantics recursion
14 midterm exam decision theory file handling
15 treebanks and PCFCs parsing: CYK algorithm tuple, list
comprehensions
16 treebanks: semantics: compositional object-oriented
resources semantics programming: basics
17 semantics: predicate logic pushdown automata oo programming:
Turing machines, techniques
computability theory
18 Christmas puzzle: pragmatics: deixis, Christmas lecture
predicate logic and anaphora, information
model theory structure
19 semantics: propositional revision: oo programming:
logic and inferences Pumping lemma techniques
20 semantics: propositional pragmatics: speech acts exercise:
logic and inference conversational maxims, segmentation
presuppositions
21 semantics: compositional a simple grammar factory functions
semantics and ?-calculus for English
22 semantics: lexical phonetics blocks and visibility
semantics
23 semantics: lexical exceptions
semantics revision
24 semantics: discourse phonology object
semantics customization
25 applications exam revision exam
Table 2: Re-organized curriculum of first semester courses
25
tics, Formal Foundations, Foundations of Linguis-
tic Analysis, and Introduction to Programming, and
identified common topics. When the four courses
were first held in parallel last semester, it happened
that recurring topics were introduced independently
without taking into account their previous mention
in other courses. For future semesters we suggest a
better alignment of recurring topics and sketch re-
arrangements of the courses? schedules. Instead of
pruning recurrent topics, we think that from the per-
spective of the psychology of learning it is useful
for the students if the same concepts and ideas are
approached from different angles iteratively.
Acknowledgments
We are indebted to our co-instructors in Heidel-
berg: Anette Frank, teaching the Introduction to
Computational Linguistics, Philipp Cimiano teach-
ing Formal Foundations, as well as Matthias Har-
tung and Wolodja Wentland, co-teaching Introduc-
tion to Programming, for sharing their experiences
and commenting on versions of this paper. We
would also like to thank Anke Holler for valuable
input on the history of the Heidelberg B.A. program,
Karin Thumser-Dauth for pointing us to the work of
Jerome Bruner, Piklu Gupta for commenting on a
pre-final version and also for help with the English.
A special thank goes to three anonymous reviewers
for their very detailed and constructive comments.
References
Steven Bird, Ewan Klein, and Edward Loper. forthcom-
ing. Natural Language Processing in Python.
Jerome S. Bruner. 1960. The Process of Education. Har-
vard University Press, Cambridge, Mass.
Kai-Uwe Carstensen, Christian Ebert, Cornelia Endriss,
Susanne Jekat, Ralf Klabunde, Hagen Langer. eds.
2004. Computerlinguistik und Sprachtechnologie.
Eine Einfu?hrung. Spektrum, Akademischer Verlag,
Heidelberg.
Markus Demleitner. unpublished. Programmieren I.
www.cl.uni-heidelberg.de/kurs/skripte/prog1/
html/
John E. Hopcroft and Jeffrey D. Ullman. 1979. Introduc-
tion to Automata Theory, Languages and Computation
Addison Wesley.
Daniel Jurafsky and James H. Martin. 2000. Speech
and Language Processing. An Introduction to Natural
Language Processing, Computational Linguistics, and
Speech Recognition. Prentice Hall Series in Artificial
Intelligence. Prentice Hall.
Ralf Klabunde. 1998. Formale Grundlagen der Linguis-
tik Narr, Tu?bingen.
Mark Lutz and David Ascher. 2007. Learning Python.
O?Reilly, 2nd Edition.
Alex Martelli. 2006. Python in a Nutshell. A Desktop
Quick Reference. O?Reilly, 2nd Edition.
Jo?rg Meibauer et al eds. 2007. Einfu?hrung in die ger-
manistische Linguistik. Metzler, Stuttgart.
Barbara Partee et al. 1990. Mathematical Methods in
Linguistics. Kluwer, Dordrecht.
Guido van Rossum. 2008. Python Tutorial. Python Soft-
ware Foundation. docs.python.org/tut/tut.html
Uwe Scho?ning. 2001. Theoretische Informatik kurzge-
fasst. Spektrum Akademischer Verlag in Elsevier.
26
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 166?169,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Annotating Discourse Anaphora
Stefanie Dipper
Institute of Linguistics
Bochum University
dipper@linguistics.rub.de
Heike Zinsmeister
Institute of Linguistics
Konstanz University
Heike.Zinsmeister@uni-konstanz.de
Abstract
In this paper, we present preliminary work on
corpus-based anaphora resolution of discourse
deixis in German. Our annotation guidelines
provide linguistic tests for locating the antecedent,
and for determining the semantic types of both the
antecedent and the anaphor. The corpus consists of
selected speaker turns from the Europarl corpus.
1 Introduction
An important component of text understanding is
anaphora resolution, i.e. to determine the refer-
ence of constituents whose interpretation depends
on (the reference of) other textual elements. The
majority of anaphora are instances of noun phrase
anaphora, which relate a noun phrase anaphor to
a nominal antecedent. Grammatical restrictions
(gender, number agreement) and saliency (gram-
matical function, recency) guide the resolution
process in these cases. In addition to pronouns,
definite noun phrases can be viewed as anaphoric
in that they may corefer to some other NP in the
given context. To solve the latter type of anaphora,
lexical semantic knowledge is required, as pro-
vided by an ontology or a database like WordNet.
Another type of anaphora is discourse deixis
(Webber 1988; 1991), which relates a noun phrase
anaphor to a verbal or (multi-)clausal antecedent.
The discourse entities that are introduced by
antecedents of discourse deictic pronouns are
called ?abstract objects? since they refer to prop-
erties and propositional entities (Asher, 1993).
Grammatical restrictions cannot apply since the
antecedent is non-nominal and the anaphor?
commonly in the form of a personal or demonstra-
tive pronoun?is usually in neuter singular. We
assume that in addition to saliency the resolution
process needs to take semantic restrictions into ac-
count (cf. Hegarty et al (2002)).
The automatic procedure of our research effort
can be envisaged as follows: Given some text we
first locate discourse anaphors. Next, the semantic
(= abstract) type of each anaphor is determined,
based on contextual features that are derived from
annotated corpus data. The anaphor?s semantic
type restricts the semantic type of the antecedent,
and thus narrows down the search space. Finally,
the antecedent is located with the help of these se-
mantic restrictions and, again, with contextual fea-
tures derived from the corpus.
2 Related Work
Corpus-based studies have shown that abstract ob-
jects are less salient than other discourse referents,
which has an effect on the choice of the anaphoric
element (Hegarty et al, 2002). The abstract type
of the antecedent and that of the anaphor do not
necessarily coincide. The data suggests that refer-
ence to other types (referred to in the literature as
coercion) is possible only in accordance to an ab-
stractness hierarchy (Hegarty, 2003; Consten and
Knees, 2005; Consten et al, 2007). The hierarchy
starts with events as the most concrete type, which
are anchored in spatial-temporal dimensions, and
ends with propositions as the most abstract types.
Anaphoric reference is possible to antecedents that
are of the same type or less abstract than the
anaphor (Consten and Knees, 2005).
Most works concerning the annotation of
anaphora resolution do not make reference to ab-
stract entities. OntoNotes, for example, only an-
notates reference to verbs (Pradhan et al, 2007).
Annotation research efforts on discourse deixis in-
clude: Eckert and Strube (2000), Byron (2002),
Poesio and Modjeska (2005), Poesio and Artstein
(2008), and Mu?ller (2007) for English; Navarretta
(2000) for Danish; and Recasens (2008) for Span-
ish/Catalan. To our knowledge, there has been no
attempt to systematically annotate such a corpus
of German.
166
Test: Die Zusammenfu?hrung der nationalen und europa?ischen Ebene ist sehr wohl notwendig , obwohl natu?rlich die
Hauptta?tigkeit in den Mitgliedstaaten stattfinden sollte und nur dann auf europa?ischer Ebene eingegriffen werden sollte ,
wenn dies ? na?mlich auf europa?ischer Ebene einzugreifen ? unbedingt notwendig ist .
Anno: Die Zusammenfu?hrung der nationalen und europa?ischen Ebene ist sehr wohl notwendig , obwohl natu?rlich die
Hauptta?tigkeit in den Mitgliedstaaten stattfinden sollte und nur dann [auf europa?ischer Ebene eingegriffen]prop werden
sollte , wenn [dies]prop unbedingt notwendig ist .
Engl: ?It is indeed necessary to bring the national and European levels together, even though, of course, the main work should
be done in the Member States, with the European level intervening only when this is absolutely necessary.?
Figure 1: Paraphrase test to determine the extension of the antecedent.
3 The Corpus
Our corpus consists of texts from the Europarl cor-
pus (Koehn, 2005). As our basis, we selected all
contributions whose original language is German
(including Austrian German).
For the annotation task, we isolated medium-
sized turns, consisting of 15?20 sentences. This
was done to guarantee that the turns were not
too lengthy but still provided enough information
for the annotators to understand the broader con-
text of discussion, so that they could resolve the
anaphors without comprehension problems. From
these turns, we selected those that contained the
anaphor dies ?this?. This is the only anaphor in
German which unambiguously refers to discourse
units.
4 The Guidelines
Our guidelines are based on theoretical research
on discourse semantics as well as work on anno-
tating discourse phenomena.
Given some discourse anaphor (i.e., anaphoric
das, dies, was, es ?that, this, which, it?), the guide-
lines define (i) how to locate the antecedent, (ii)
how to determine the semantic type of the an-
tecedent, and (iii) how to determine the seman-
tic type of the anaphor. For each of these tasks,
the guidelines provide linguistic tests (Dipper and
Zinsmeister, 2009).
4.1 Locating the antecedent
To determine the antecedent of the anaphoric re-
lation, a ?paraphrase test? is applied: The anno-
tator supplements the anaphor by a paraphrase in
the form of na?mlich . . . ?namely . . . ?. The part
that fills the . . . corresponds to the antecedent that
we are looking for, cf. Fig. 1.1 Antecedents can
1The Test line displays the sentence with the anaphor
(marked in bold-face) followed by the inserted paraphrase
(in bold-face and italics). The Anno line shows the same ex-
consist of VPs, (fragments of) main or subordinate
clauses, or multiple sentences.2
4.2 The semantic type of the antecedent
We distinguish 10 types of propositional enti-
ties. Many verbs prototypically denote one type
of propositional entity; gewinnen ?win?, for in-
stance, usually expresses an event. Often, how-
ever, the type of entity that is denoted depends on
the context and usage of the verb; Hans hat A?pfel
gegessen (?Hans ate apples?) denotes a process,
whereas Hans hat zwei A?pfel gegessen (?Hans ate
two apples?) denotes an event because the action
has an end (when both apples are eaten)?i.e., the
action is telic. The semantic types are defined in
terms of the following features: world-dependent,
time-dependent, dynamic, telic, and modal (with
subtypes deontic and epistemic, generic, subjec-
tive) (see e.g., Vendler (1967), Asher (1993)). Ta-
ble 1 displays the different types of propositional
entities and their defining features. It also lists the
labels used for annotating these entities. The en-
tity types are ordered according to their degree of
abstractness.
The entity type ?deict? (deictic) does not fit in
the abstractness hierarchy of the table. It refers
to extra-linguistic entities, such as the external sit-
uation, or an issue that is currently the focus of
attention in parliament, etc.
ample with the identified antecedent underlined. Both the
antecedent and the anaphor are labeled with their seman-
tic types (see below). The Engl line presents an English
translation that is based on the original translations from Eu-
roparl. We used the tool OPUS (http://urd.let.rug.
nl/tiedeman/OPUS) to retrieve the English translations.
2E.g., the anaphor dies alles ?all this? often refers to
an antecedent consisting of multiple sentences. The ac-
tual antecedent can diverge from the one constructed by
the paraphrase test in minor aspects, such as active-passive-
alternations, or bare infinitive vs. zu-infinitive vs. participle.
In some cases, the divergences are more important and could
involve, for instance, the insertion or modification of the main
verb. In such cases, annotators were asked to note and record
the differences.
167
Prop. Entity Label Defining Features Replacement Test
W T Dyn Tel Mod
1. Event ev + + + + - Ereignis (?event?)
2. Process proc + + + - - Vorgang (?process?)
3. State state + + - (-) - Zustand (?state?)
4. Circumstance circ + + - - - Umstand (?circumstance?)
5. Modal (deontic
+ epistemic)
mod + + - - mod Notwendigkeit, Mo?glichkeit, Chance, . . . (?ne-
cessity, possibility, opportunity, . . . ?)
6. Opinion, claim op + + - - subj Meinung, Ansicht, Behauptung, Einscha?tzung,
Forderung, . . . (?opinion, view, claim, assess-
ment, request, . . . ?)
7. Generic gen + +/- - - gen wohlbekannte, allgemeingu?ltige Tatsache (?the
well-known, universal fact?)
8. Fact fact + +/- +/- +/- - Tatsache (?fact?)
9. Proposition prop - - +/- +/- - (Art von) Aktivita?t, Aktion, Eigenschaft, . . .
?(kind of) activity, action, property, . . . ?)
Table 1: Semantic types and their defining features: W(orld), T(ime), Dyn(amic), (Tel)ic, Mod(al)
4.3 The semantic type of the anaphor
To determine the type of anaphors, we defined a
?replacement test?. With this test, the demonstra-
tive anaphor dies, das, etc. is replaced by a suitable
NP, such as dieses Ereignis, dieser Vorgang. The
head noun indicates the type of the propositional
entity (e.g., event, process).3 Table 1 lists the dif-
ferent types of propositional entities and suitable
replacement nouns. The annotators are asked to
choose the most concrete, suitable noun.
5 Results
As a first pilot study on the reliability of our an-
notation guidelines, two student annotators anno-
tated 32 texts that included 48 instances of the
demonstrative pronoun dies ?this?. The pronouns
were marked in bold face, and the annotation was
performed on paper. After annotating 17 texts, the
annotators discussed their intermediate results.
Locating the antecedent: In one case, one of
the annotators decided on a deictic reading and did
not mark an antecedent at all. 40 out of 47 an-
tecedents (85%) were marked with identical spans.
In four cases they chose differing but adjacent
spans and in one case one of the annotators chose
a longer string than the other.
The semantic type of the antecedent: The
type of the antecedents coincided in 28 out of 47
cases (60%, ?=0.52).4 Agreement improved af-
3We use the term ?semantic type of the anaphor? in a
somewhat sloppy way. Put more precisely, the ?semantic type
of the anaphor? indicates the way that the anaphor refers to
(parts of) the propositional discourse referent that is denoted
by the antecedent.
4We computed ? according to www.asc.upenn.edu/
usr/krippendorff/webreliability.doc.
ter the discussion period: 11/17 cases matched
(?=0.60).
The semantic type of the anaphor: The results
with respect to the semantic type of the anaphor
seemed more disappointing: the annotators agreed
in only 22 out of 48 instances (46%, ?=0.37).
However, after the discussion period, agreement
leveled that of the type of the antecedent: 12 out
of 17 cases coincided (?=0.66). In addition to the
semantic type, we annotated the grammatical role
of the anaphor, which occurred as the subject in
79% of cases and as objects elsewhere.
Annotators agreed most often on the four most
concrete types (?ev, proc, state, circ?) and least of-
ten on the three most abstract types (?gen, fact,
prop?). This might be due to the fact that the most
abstract types are applicable in many cases, but an-
notators are advised to choose the most concrete
type that is available. In the majority of the cases
(73%), the anaphor?s type was identical with or
more abstract than the antecedent?s type.
6 Conclusion
In this paper, we presented a corpus-driven ap-
proach to discourse deictic anaphora in German.
We introduced annotation guidelines that provide
linguistic tests for locating the antecedent, and
for determining the semantic types of both the
antecedent and the anaphor. Further work will
include exploitation of contextual information in
combination with the semantic types to confine the
set of potential antecedents.
Our corpus consists of selected speaker turns
from the Europarl corpus. In this study, 32 texts
(providing 48 instances of discourse deixis) were
168
annotated according to these guidelines, and first
results concerning inter-annotator agreement are
promising (with an agreement of 85% on the ex-
tension of the antecedent, 60% on the antecedent
type, and 46% on the type of the anaphor). The
pilot study indicates that the paraphrase test helps
the annotators in determining on the extension of
the abstract antecedent.5 It also shows that the lin-
guistic tests for the semantic types have to be re-
fined.
In the next steps, we will switch from paper-
and-pencil annotation to annotation based on the
tool MMAX26. In addition to manually determin-
ing the semantic types of anaphors, we will in-
vestigate robust, fully-automatic approaches to the
derivation of contextual features for anaphora res-
olution. For instance, we plan to take into account
anaphors of the form dieses Ereignis, dieser Um-
stand, etc. (?this event, this circumstance?), which
explicitly name the semantic type of the anaphor.
In a later step other, more ambiguous, types of
anaphors will be included in the investigation.
Acknowledgments
We would like to thank the anonymous review-
ers for their helpful comments, and our student
annotators: Doris Aimer, Iris Bra?uning, Christine
Enzinger, Stefanie Konetzka, Barbara Mrzyglod.
The work was in part supported by Europa?ischer
Sozialfonds in Baden-Wu?rttemberg.
References
Nicholas Asher. 1993. Reference to Abstract Objects
in Discourse. Kluwer Academic Publishers, Boston
MA.
Donna K. Byron. 2002. Resolving pronominal refer-
ence to abstract entities. In Proceedings of the ACL-
02 conference, pages 80?87.
Manfred Consten and Mareile Knees. 2005. Complex
anaphors ? ontology and resolution. In P. Dekker,
editor, Proceedings of the 15th Amsterdam Collo-
quium, Amsterdam: University.
Manfred Consten, Mareile Knees, and Monika
Schwarz-Friesel. 2007. The function of com-
plex anaphors in texts: Evidence from corpus stud-
ies and ontological considerations. In Anaphors
in Text, pages 81?102. John Benjamins, Amster-
dam/Philadephia.
5The study was restricted to instances of the unambiguous
anaphor dies ?this?, which might have simplified the task of
selecting an antecedent.
6MMAX2: http://mmax2.sourceforge.net/.
Stefanie Dipper and Heike Zinsmeister. 2009. An-
notation guidelines ?Discourse-Deictic Anaphora?.
Draft. Universities of Bochum and Konstanz.
Miriam Eckert and Michael Strube. 2000. Dialogue
acts, synchronising units and anaphora resolution.
Journal of Semantics, 17(1):51?89.
Michael Hegarty, Jeanette K. Gundel, and Kaja
Borthen. 2002. Information structure and the ac-
cessibility of clausally introduced referents. Theo-
retical Linguistics, 27(2-3):163?186.
Michael Hegarty. 2003. Type shifting of Entities in
Discourse. Presentation at the First International
Workshop on Current Research in the Semantics-
Pragmatics Interface, Michigan State University.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of MT Summit.
Christoph Mu?ller. 2007. Resolving it, this, and that
in unrestricted multi-party dialog. In Proceedings of
the 45th Annual Meeting of the ACL, pages 816?823,
Prague, Czech Republic, June.
Costanza Navarretta. 2000. Abstract Anaphora Res-
olution in Danish. In 1st SIGdial Workshop on
Discourse and Dialogue, pages 56?65, Hong Kong,
China, October.
Massimo Poesio and Ron Artstein. 2008. Anaphoric
annotation in the ARRAU corpus. In Proceedings
of the LREC 2008, Marrakech, Morocco.
Massimo Poesio and Natalia N. Modjeska. 2005. Fo-
cus, activation, and this-noun phrases: An empirical
study. In Anto?nio Branco, Tony McEnery, and Rus-
lan Mitkov, editors, Anaphora Processing, volume
263 of Current Issues in Linguistic Theory, pages
429?442. John Benjamins.
Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007.
Unrestricted coreference: Identifying entities and
events in OntoNotes. In Proceedings of the IEEE
International Conference on Semantic Computing
(ICSC), Irvine, CA.
Marta Recasens. 2008. Discourse deixis and coref-
erence: Evidence from AnCora. In Proceedings of
the SecondWorkshop on Anaphora Resolution (WAR
II), pages 73?82.
Zeno Vendler, 1967. Linguistics in Philosophy, chapter
Verbs and Time, pages 97?121. Cornell University
Press, Ithaca.
Bonnie L. Webber. 1988. Discourse deixis: Reference
to discourse segments. In Proceedings of the ACL-
88 conference, pages 113?122.
Bonnie L. Webber. 1991. Structure and ostention in
the interpretation of discourse deixis. Language and
Cognitive Processes, 6:107?135.
169
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 300?310,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Interpreting Anaphoric Shell Nouns using Antecedents of
Cataphoric Shell Nouns as Training Data
Varada Kolhatkar
Department of Computer Science
University of Toronto
varada@cs.toronto.edu
Heike Zinsmeister
Institut fu?r Germanistik
Universita?t Hamburg
heike.zinsmeister@uni-hamburg.de
Graeme Hirst
Department of Computer Science
University of Toronto
gh@cs.toronto.edu
Abstract
Interpreting anaphoric shell nouns
(ASNs) such as this issue and this fact
is essential to understanding virtually
any substantial natural language text.
One obstacle in developing methods
for automatically interpreting ASNs is
the lack of annotated data. We tackle
this challenge by exploiting cataphoric
shell nouns (CSNs) whose construction
makes them particularly easy to interpret
(e.g., the fact that X). We propose an ap-
proach that uses automatically extracted
antecedents of CSNs as training data to
interpret ASNs. We achieve precisions
in the range of 0.35 (baseline = 0.21) to
0.72 (baseline = 0.44), depending upon
the shell noun.
1 Introduction
Anaphors such as this fact and this issue encapsulate
complex abstract entities such as propositions, facts,
and events. An example is shown below.
(1) Here is another bit of advice: Environmental
Defense, a national advocacy group, notes that
?Mowing the lawn with a gas mower produces
as much pollution in half an hour as driving a
car 172 miles.? This fact may help to explain
the recent surge in the sales of the good old-
fashioned push mowers or the battery-powered
mowers.
Here, the anaphor this fact is interpreted with the
help of the clausal antecedent marked in bold. The
antecedent here is complex because it involves a
number of entities and events (e.g., mowing the
lawn, a gas mower) and relationships between them,
and is abstract because the antecedent itself is not a
purely physical entity.
The distinguishing property of these anaphors is
that they contain semantically rich abstract nouns
(e.g., fact in (1)) which characterize and label their
corresponding antecedents. Linguists and philoso-
phers have studied such abstract nouns for decades
(Vendler, 1968; Halliday and Hasan, 1976; Francis,
1986; Ivanic, 1991; Asher, 1993). Our work is in-
spired by one such study, namely that of Schmid
(2000). Following Schmid, we refer to these abstract
nouns as shell nouns, as they serve as conceptual
shells for complex chunks of information. Accord-
ingly, we refer to the anaphoric occurrences of shell
nouns (e.g., this fact in (1)) as anaphoric shell nouns
(ASNs).
An important reason for studying ASNs is their
ubiquity in all kinds of text. Schmid (2000) ob-
served that shell nouns such as fact, idea, point, and
problem were among the 100 most frequently oc-
curring nouns in a corpus of 225 million words of
British English. Moreover, ASNs can play several
roles in organizing a discourse such as encapsulation
of complex information, cohesion, and topic bound-
ary marking. So correct interpretation of ASNs can
be an important step for correct interpretation of a
discourse, and in a number of NLP applications such
as text summarization, information extraction, and
non-factoid question answering.
Despite their importance, ASNs have not received
much attention in Computational Linguistics, and
research in this field remains in its earliest stages. At
300
present, the major obstacle is that there is very little
annotated data available that could be used to train a
supervised machine learning system for robustly in-
terpreting these anaphors, and manual annotation is
an expensive and time-consuming task.
We tackle this challenge by exploiting a category
of examples, as shown in (2), whose construction is
particularly easy to interpret.
(2) Congress has focused almost solely on the fact
that special education is expensive ? and that
it takes away money from regular education.
Here, in contrast with (1), the fact is not anaphoric
in the traditional sense, but is an easy case of a
forward-looking anaphor ? a cataphor. While the
resolution process of this fact in (1) is quite chal-
lenging as it requires the use of semantics and world
knowledge, it is fairly easy to interpret the fact in
(2) based on the syntactic structure alone. We refer
to these easy-to-interpret cataphoric occurrences of
shell nouns as cataphoric shell nouns (CSNs). The
interpretation of both ASNs and CSNs will be re-
ferred to as antecedent.1 The antecedent of the fact
in (2) is given in the post-nominal that clause. We
use the term shell concept to refer to the general no-
tion of a shell noun, i.e., the semantic type of the
antecedent. For example, the notion of an issue is an
important problem which requires a solution.
In this work, we propose an approach to interpret
ASNs that exploits unlabelled but easy-to-interpret
CSN examples to extract characteristic features as-
sociated with the antecedent of different shell con-
cepts. We evaluate our approach using crowdsourc-
ing. Our results show that these unlabelled CSN ex-
amples provide useful linguistic properties that help
in interpreting ASNs.
2 Related work
The resolution of anaphors to non-nominal an-
tecedents has been well analyzed taking discourse
structure and semantic types into account (Web-
ber, 1991; Passonneau, 1989; Asher, 1993). Most
work in machine anaphora resolution, however,
is restricted to anaphora that involve nominal an-
tecedents only (Poesio et al, 2011).
1Sadly, the more-logical term for the interpretation of a
CSN, succedent, does not actually exist.
There are some notable exceptions which have
tackled the challenge of interpreting non-nominal
antecedents (Eckert and Strube, 2000; Strube and
Mu?ller, 2003; Byron, 2004; Mu?ller, 2008). These
approaches are limited as they either rely heavily
on domain-specific syntactic and semantic annota-
tion or prepossessing, or mark only verbal proxies
for non-nominal antecedents.
Recently, Kolhatkar and Hirst (2012) presented
a machine-learning based resolution system for this
issue anaphora, identifying full syntactic phrases as
antecedents. Although they achieved promising re-
sults, their approach was limited in two respects.
First, it focused on only one type of shell noun
anaphora (issue anaphora). Second, their training
data was restricted to MEDLINE abstracts in which
this issue is used in a rather systematic way. Further-
more, their work is based on manually labelled ASN
antecedents, whereas we use automatically identi-
fied CSN antecedents, which we interpret as explic-
itly expressed antecedents in comparison to the more
implicitly expressed ASN antecedents.
Using explicitly expressed structure in the text
to identify implicit structure is not new. The same
idea has been applied before in computational lin-
guistics. Marcu and Echihabi (2002) identified im-
plicit discourse relations using explicit ones. Mark-
ert and Nissim (2005) used Hearst?s (1992) explicit
patterns to learn lexical semantic relations for NP-
coreference and other-anaphora resolution from the
web. Although our work focuses on a different topic,
the methodology is in the same vein.
3 Hypothesis of this work
The hypothesis of this work is that CSN antecedents
and ASN antecedents share some linguistic proper-
ties and hence linguistic knowledge encoded in CSN
antecedents will help in interpreting ASNs. Accord-
ingly, we examine which features present in CSN
antecedents are relevant in interpreting ASNs.
The motivation and intuition behind this hypoth-
esis is as follows. The antecedents of both ASNs
and CSNs represent the corresponding shell con-
cept. So are there any characteristic features asso-
ciated with this shell concept? Do speakers of En-
glish follow certain patterns of syntactic shape or
words, for instance, when they state facts, decisions,
301
The CSN corpus  (211,722 instances) The ASN corpus  (2,323 instances) 
Training 
Training data 
CSN antecedent models  
CSN antecedent extractor Ranked ASN antecedent candidates 
Crowdsourcing evaluation 
Testing 
Figure 1: Overview of our approach
or issues? There is an abundance of data for CSN
antecedents and if we are able to capture particu-
lar linguistic characteristic features associated with
a shell concept using this data, we can use this in-
formation to interpret ASNs. For instance, exam-
ple (2) demonstrates characteristic properties of an-
tecedents of the shell noun fact including that (a)
they are propositions and are generally expressed
with clauses or sentences rather than noun phrases,
and (b) they are generally expressed in the present
tense. Observe that these properties also hold for
the antecedent of this fact in example (1).
We test our hypothesis by building machine learn-
ing models that are trained on automatically ex-
tracted CSN antecedents and then applying these
models to recover ASN antecedents. Figure 1 shows
an overview of our methodology.
4 Background
Formal definition Shell-nounhood is a functional
notion; it is defined by the use of an abstract noun
rather than the inherent properties of the noun itself
(Schmid, 2000). An abstract noun is a shell noun
when the speaker decides to use it as a shell noun.
Shell noun categorization Schmid (2000) gives
a list of 670 English nouns which are frequently
used as shell nouns. He divides them into six
broad semantic classes: factual, linguistic, mental,
modal, eventive, and circumstantial. Table 1 shows
this classification, along with example shell nouns
for each category. For this work, we selected six
frequently occurring shell nouns covering four of
Schmid?s six classes: fact and reason from factual,
issue and decision from mental, question from lin-
guistic, and possibility from modal. These shell
nouns tend to have antecedents that lie within a sin-
gle sentence. We excluded eventive and circumstan-
Class Description Examples
factual states of affairs fact, reason
linguistic linguistic acts question
mental ideas issue, decision
modal judgements possibility
eventive events act, reaction
circumstantial situations situation, way
Table 1: Schmid?s classification of shell nouns. The
nouns given in the Example column tend to occur fre-
quently with the respective class. The shell nouns used in
this work are shown in boldface.
Pattern Example
N-to Several people at the group said the deci-
sion to write the letters was not controver-
sial internally.
N-be-to The principal reason is to create a repre-
sentative government rather than to select
the most talented person.
N-that Mr. Shoval left open the possibility that
Israel would move into other West Bank
cities.
N-be-that The simple and reassuring fact is that a
future generation of leaders is seeking new
challenges during challenging times.
N-wh There is now some question whether the
country was ever really in a recession.
N-be-wh Of course, the central, and probably in-
soluble, issue is whether animal testing is
cruel.
Table 2: Easy-to-interpret CSN patterns given by Schmid
(2000). In the Example column, the patterns are marked
in boldface and the antecedents are marked in italics.
tial classes because the shell nouns in these classes
tend to have rather unclear and long antecedents.2
Shell noun patterns Schmid (2000) also provides
a number of lexico-grammatical patterns for shell
nouns. In Section 1, we noted two such patterns:
this-N (this fact in example (1)) and N-that (fact that
in example (2)). We also noted that CSNs with pat-
tern N-that are fairly easy to interpret compared to
the ASN pattern this-N. Table 2 shows some other
easy-to-interpret CSN patterns given by Schmid.
Generally, for all these patterns, the antecedent is
2These observations are based on an exploratory pilot anno-
tation we carried out on sample data of 150 ASN instances.
302
quite easy to extract with a few predefined rules.
Shell antecedent properties Antecedents of
CSNs and ASNs share some properties while they
are distinguished by others. The distinguishing
property is that CSNs, by their construction, have
their antecedents in the same sentence, as shown
in example (2). On the other hand, ASNs can
have long-distance as well as short-distance an-
tecedents.3 The common properties are as follows.
First, antecedents of both ASNs and CSNs represent
the corresponding shell concept, e.g., the notion
of a fact or an issue. Second, in both cases, the
antecedents are complex abstract entities, which
involve a number of entities and relationships
between them. Finally, in both cases, there is no
one-to-one correspondence between the syntactic
type of an antecedent and semantic type of its
referent (Webber, 1991). For instance, a semantic
type such as fact can be expressed with different
syntactic shapes such as a clause, a verb phrase, or
a complex sentence. Conversely, a syntactic shape,
such as a clause, can function as several semantic
types, including fact, proposition, and event.
5 Training phase
As shown in Figure 1, the goal of the training phase
is to build training data from CSNs and their an-
tecedents and train models which can be used for
resolving ASNs.
5.1 The CSN corpus
We automatically constructed a corpus, a subset of
the New York times (NYT) corpus4, which contains
211,722 sentences following CSN patterns from Ta-
ble 2. We considered part-of-speech information5
while looking for the patterns. For instance, in-
stead of the pattern N-that, we actually looked for
{shell noun NN that IN}.
3In our annotated sample data, we observed ASN an-
tecedents as close as the same sentence and as far as 7 sentences
away from the anaphor.
4http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2008T19
5http://nlp.stanford.edu/software/tagger.
shtml
5.2 Antecedent extractor for CSNs
The goal of the antecedent extractor is to create auto-
matically labelled CSN antecedent data. Recall that
antecedents of CSNs can be extracted using simple
predefined rules that are based on the syntactic struc-
ture alone. For instance, the antecedent extraction
rule for example (2) would be: if the example fol-
lows the pattern fact-that, extract the post-nominal
that clause as the antecedent. To come up with a list
of such extraction rules, we systematically analyzed
a sample of examples (about 20 examples) of each
pattern for each shell noun. Table 3 summarizes the
resulting antecedent extraction rules.
The actual antecedent extraction works as fol-
lows. First, we parsed the examples from the CSN
corpus using the Stanford parser.6 Then for each ex-
ample, we applied rules from Table 3 depending on
the shell noun and the pattern it follows to extract
an appropriate syntactic constituent as the CSN an-
tecedent. For instance, for the noun fact following
the N-that pattern, as in example (2), we first looked
for the NP constituent containing the shell noun fact,
and then extracted the sentential constituent follow-
ing the NP constituent as the CSN antecedent. Al-
though, in most of the cases, the antecedent is given
in the post-nominal wh, that, or infinitive clauses,
sometimes it is not present in the immediately fol-
lowing clause but is given only as a predicate, as
shown in (3).
(3) The primary reason that the archdiocese cannot
pay teachers more is that its students cannot af-
ford higher tuition.
In such cases, we looked for the pattern (VP (VB
be verb) X) in the right sibling of the NP contain-
ing the pattern shell noun-that and extracted X as
the CSN antecedent.
Two contradictory goals need to be achieved
while extracting antecedents of CSNs. The first re-
quires only considering CSNs with high-confidence
patterns, whereas the second requires considering as
many patterns as possible to allow a wide variety of
antecedent examples with different linguistic prop-
erties (e.g., syntactic shape). Our antecedent extrac-
tor tries to find a balance between the two goals.
6http://nlp.stanford.edu/software/lex-parser.
shtml
303
fact reason issue decision question possibility
N-to ? ? ? inf clause predicate inf clause
N-be-to ? inf clause inf clause inf clause inf /wh clause inf clause
N-that that clause predicate predicate ? predicate that clause
N-be-that that clause that clause that clause that clause that clause that clause
N-wh ? predicate wh clause wh clause wh clause ?
N-be-wh wh clause wh clause wh clause wh clause wh clause wh clause
Table 3: Content extraction patterns for CSNs. Patterns in boldface are the prominent patterns for the respective shell
noun. inf clause = infinitive clause. Discarded patterns are denoted by ?.
To address the first goal, we filter examples fol-
lowing noisy patterns, i.e., the patterns that do not
unambiguously encode antecedents of that CSN. For
instance, the pattern N-to is a highly preferred pat-
tern for decision, as shown in (4). The antecedent
extraction rule here is relatively simple: if the exam-
ple follows the pattern decision-to, extract the post-
nominal infinitive clause as the correct antecedent.
(4) President Jacques Chirac?s arrogant decision to
defy the world and go ahead with two nuclear
bomb tests in Polynesia deserves contempt.
But the same pattern is noisy for reason. In (5), for
example, the actual reason is not given anywhere in
the sentence. So we discard the examples following
the pattern N-to for reason.
(5) Investors have had reason to worry about stocks.
We also discard examples with negative determiners,
as in (6), because in such cases, the extraction rules
do not precisely give the antecedent of the given
CSN.
(6) He was careful to repeat anew that he had made
no decision to go to war.
For the N-wh pattern, we exclude certain wh
words for certain nouns. For example, we exclude
the wh word which for question as the Penn Tree-
bank tagset7 does not distinguish between which as a
relative pronoun and as a question. We are interested
in the latter but not the former. Other discarded wh
words include which and when for fact; all wh words
except when and why for reason, all wh words except
how and whether for issue; which, whom, when, and
why for decision; which and when for question; and
all wh words for possibility.
7The Stanford tagger we employ uses the Penn Treebank
tagset (Marcus et al, 1993).
To address the second goal of allowing a wide
variety of antecedent examples, we try to include
as many patterns as possible for each shell noun,
as shown in Table 3. For instance, the patterns
question-to and question-be-to will have infinitive
clauses as antecedents (marked as VP or S+VP
by the parser), whereas for the examples follow-
ing patterns question-wh and question-be-wh the
antecedent will be in the wh clauses (marked as
SBAR). For the pattern question-that, the antecedent
will be in the predicate (similar to example (3)),
which can be a prepositional phrase, a noun phrase
or a clause.
5.3 Models for CSN antecedents
The antecedent extractor gives labels for each in-
stance in the CSN corpus. Using this labelled data,
we train machine learning ranking models for dif-
ferent shell concepts that capture the characteristic
features associated with that shell concept. The fol-
lowing sections describe each step of our ranking
models in detail.
5.3.1 Candidate extraction
The first step is to extract the set of eligible an-
tecedent candidates C = {C1,C2, ...,Ck} for the CSN
instance ai. To train a machine learning model we
need positive and negative examples. We already
have positive examples for antecedent candidates ?
the true antecedents given by the antecedent extrac-
tor. But we also need negative examples of an-
tecedent candidates. By their construction, CSNs
have their antecedents in the same sentence. So
we extract all syntactic constituents of this sentence,
given by the Stanford parser. All the syntactic con-
stituents, except the true antecedent, are considered
as negative examples. With this candidate extraction
method, we end up with many more negative exam-
304
ples than positive examples, but that is exactly what
we expect with ASN antecedent candidates, i.e., the
test data on which we will be applying our models.
5.3.2 Features
Although our problem is similar to anaphora res-
olution, we cannot make use of the usual anaphora
or coreference resolution features such as agreement
or string matching (Soon et al, 2001) because of the
nature of ASN and CSN antecedents. We came up
with a set of features based on the properties that
were common in both ASN and CSN antecedents,
according to our judgement.
Syntactic type of the candidate (S) We observed
that each shell noun prefers specific CSN patterns
and each pattern involves a particular syntactic type.
For instance, decision prefers the pattern N-to and
consequently realizes as its antecedents more verb
phrases than, for example, noun phrases. We employ
two versions of syntactic type: fine-grained syntac-
tic type given by the Stanford parser (e.g., NP-TMP,
RRC) and coarse-grained syntactic type (e.g., NP,
VP, S, PP) in which we consider ten basic syntactic
categories and map all fine-grained syntactic types
to these categories.
Context features (C) Context features allow our
models to learn about the contextual clues that signal
the antecedent. This class contains two features: (a)
coarse-grained syntactic type of left and right sib-
lings of the candidate, and (b) part-of-speech tag of
the preceding and following words of the candidate.
Embedding level features (E) These features
(Mu?ller, 2008) encode the embedding level of the
candidate within its sentence. We consider two em-
bedding level features: top embedding level and im-
mediate embedding level. Top embedding level is
the level of embedding of the given candidate with
respect to its top clause (the root node), and immedi-
ate embedding level is the level of embedding with
respect to its immediate clause (the closest ancestor
of type S or SBAR). The intuition behind this fea-
ture is that if the candidate is deep in the parse tree,
it is possibly not salient enough to be an antecedent.
As we consider all syntactic constituents as poten-
tial candidates, there are many that clearly cannot be
antecedents. This feature will allow us to get rid of
this noise.
Subordinating conjunctions (SC) As we can see
in Table 2, subordinating conjunctions are common
with CSN and ASN antecedents. Vendler (1968)
points out that the shell noun fact prefers a that-
clause, and question and issue prefer a wh-question
clause. We observed that the pattern because X is
common with reason. The subordinating conjunc-
tion feature encodes these preferences for different
shell nouns. The feature checks whether the candi-
date follows the pattern SBAR ? (IN sconj) (S ...),
where sconj is a subordinating conjunction.
Verb features (V) A prominent property of CSN
and ASN antecedents is that they tend to contain
verbs. All examples from Table 2, for example, con-
tain verbs. Moreover, certain shell nouns have tense
and aspect preferences. For instance, for shell noun
fact, lexical verbs in past and present tenses predom-
inate (Schmid, 2000), whereas modal forms are ex-
tremely common for possibility. We use three verb
features that capture this idea: (a) presence of verbs
in general, (b) whether the main verb is finite or non-
finite, and (c) presence of modals.
Length features (L) The intuition behind these
features is that CSN and ASN antecedents tend to be
long, especially for nouns such as fact. We consider
two length features: (a) length of the candidate in
words, and (b) relative length of the candidate with
respect to the sentence containing the antecedent.
Lexical features (LX) Our extractor gives us a
large number of antecedent examples for each shell
noun. A natural question is whether certain words
tend to occur more frequently in the antecedent than
non-antecedent parts of the sentence. To deal with
this question, we extracted all antecedent unigrams
(i.e., unigrams occurring in antecedent part of the
sentence) and non-antecedent unigrams (i.e., uni-
grams occurring in non-antecedent parts of the sen-
tence) for each shell noun. Then for all antecedent
unigrams for a particular shell noun, we computed
term goodness in terms of information gain (Yang
and Pedersen, 1997) and considered the first 50
highly ranked unigrams as the lexical features for
that noun. Note that, in contrast with the other fea-
tures, these lexical features are tailored for each shell
noun and are extracted a priori.
305
5.3.3 Candidate ranking models
Now that we have the set of candidate antecedents
and a set of features, we are ready to train CSN an-
tecedent models. We follow the candidate-ranking
models proposed by Denis and Baldridge (2008) be-
cause they allow us to evaluate how good an an-
tecedent candidate is relative to all other candidates.
For every shell noun, we gather automatically ex-
tracted antecedent data given by the extractor for all
instances of that shell noun. Then for each instance
in this data, we extract the set C as explained in
Section 5.3.1. For each candidate Ci ? C, we ex-
tract a feature vector to create a corresponding set of
feature vectors, C f = {C f 1,C f 2, ...,C f k}. For every
CSN ai and a set of feature vectors corresponding
to its eligible candidates C f = {C f 1,C f 2, ...,C f k},
we create training examples (ai,C f i,rank),?C f i ?
C f . The rank is 1 if Ci is same as the true an-
tecedent, i.e., the automatically extracted antecedent
for that CSN, otherwise the rank is 2. We use the
svm rank learn call of SVMrank (Joachims, 2002)
for training the candidate-ranking models.
6 Testing phase
In this phase, we use the learned candidate ranking
models to identify the antecedents of ASNs.
6.1 The ASN corpus
We started with about 450 instances for each of the
six selected shell nouns (2,700 total instances), con-
taining the pattern {this shell noun}. The instances
were extracted from the NYT. Each instance con-
tains three paragraphs from the corresponding NYT
article: the paragraph containing the ASN and two
preceding paragraphs as context. After automati-
cally removing duplicates and ASNs with a non-
abstract sense (e.g., this issue with a publication-
related sense), we were left with 2,323 instances.
6.2 Antecedent identification
Candidate extraction The search space of ASN
antecedents is quite large for two reasons: ASNs
tend to have long-distance as well as short-distance
antecedents, and there is no clear restriction on the
syntactic type of the antecedents. In the ASN cor-
pus, each sentence on average had 49.5 distinct syn-
tactic constituents given by the Stanford parser. If
we consider n preceding sentences, the sentence
containing the anaphor, and one following sentence8
as sources for antecedents, then the average num-
ber of antecedent candidates will be 49.5? (n+ 2).
This is large compared to the search space of ordi-
nary nominal anaphora. In our previous work (Kol-
hatkar et al, 2013), we have developed methods that
identify the sentence containing the antecedent of
the ASN before identifying the precise antecedent.
In brief, given a set of a fixed number of sentences
around the sentence containing an ASN, these meth-
ods reliably identify the sentence containing the an-
tecedent. In this paper, we treat these methods as a
black box.
Given the sentence containing the antecedent,
we extract all syntactic constituents given by the
Stanford parser from that sentence as potential an-
tecedent candidates as for the training phase. In
the training phase, the antecedent is always con-
tained in the set of syntactic constituents given by
the Stanford parser because the extractor obtains the
appropriate antecedent using the syntactic informa-
tion. But in the testing phase, we cannot guarantee
that the true antecedent occurs in the extracted syn-
tactic constituents due to the parser?s errors. So for
robust candidate extraction, we extract all distinct
constituents from the 30-best parses instead of only
considering the best parse, which increases the aver-
age number of candidates from 49.5 to 55.2.
Feature extraction and candidate ranking
Given the antecedent candidates, feature extraction
and candidate ranking are essentially the same as
for the training phase, except of course we do not
know the true antecedent. Once we have the feature
vectors for each antecedent candidate, the appro-
priate trained model, i.e., the model trained for the
corresponding shell noun, is invoked and the can-
didates are ranked using the svm rank classify
call of SVMrank.
7 Evaluation
We evaluate the ranked candidates of ASN instances
using crowdsourcing.
8The ASN corpus contains a few cataphoric examples that
do not follow the standard patterns of the CSNs shown in Table
2, but actually refer to an antecedent in the following sentence
(e.g., Mr. Dukakis put this question to him: X).
306
Interface We chose to use CrowdFlower9 as our
crowdsourcing interface because of its integrated
quality-control mechanism. For instance, it throws
gold questions randomly at the workers and the
workers who do not answer them correctly are not
allowed to continue.
We presented to the crowd evaluators the ASN
instances from the ASN corpus. Recall that each
ASN instance is made up of the paragraph contain-
ing the ASN and two preceding paragraphs as con-
text. We displayed the first 10 highly-ranked candi-
dates (ordered randomly) given by our testing phase
and asked the evaluators to choose the best answer
that represents the ASN antecedent. We encouraged
the evaluators to select None when they did not agree
with any of the displayed answers. We also asked
them how satisfied they were with the displayed an-
swers. We provided them with three options: unsat-
isfied, satisfied, and partially satisfied.
Our job contained 2,323 evaluation units. We
asked for 8 judgements per instance and paid 6
cents per evaluation unit. As we were interested
in the verdict of native speakers of English, we
limited the allowed demographic region to English-
speaking countries.
Results Among the 2,323 ASN instances, 96% of
them were labelled as satisfied, 3% as partially satis-
fied and 1% as unsatisfied. Only 2% of the instances
were labelled as None. As expected, evaluators were
unsatisfied or partially satisfied with the options of
these instances. These results suggest that our res-
olution models trained on automatically extracted
antecedents of CSNs bring the relevant candidates
of ASN antecedents to the top, i.e., within first 10
highly-ranked candidates. This itself is a positive re-
sult given the large search space of ASN antecedent
candidates (more than 55 candidates on average).
Among the evaluation units, more than half of the
evaluators agreed on an answer for 1,810 units. We
used these instances for further analysis.
To examine which CSN antecedent features are
relevant in identifying ASN antecedents, we carried
out ablation experiments with all feature class com-
binations. We compared the rankings given by our
ranker to the crowd?s answer using precision at n
9http://crowdflower.com/
(P@n).10 More specifically, we count the number of
instances where the crowd?s answers occur within
our ranker?s first n choices. P@n then is this count
divided by the total number of instances. Note that
P@1 is equivalent to the standard precision.
We compared our results against two baselines:
preceding sentence and chance. The preceding sen-
tence baseline chooses the previous sentence as the
correct antecedent. The chance baseline chooses a
candidate from a uniform random distribution over
the set of 10 top-ranked candidates.
The results are shown in Table 4. Although dif-
ferent feature combinations gave the best results for
different shell nouns, the features that occur fre-
quently in many best-performing combinations were
embedding level (E), lexical (LX), and subordinat-
ing conjunction (SC) features. The SC features were
particularly effective for issue and question, where
we expected patterns such as whether X.
Surprisingly, the syntactic type features (S) did
not show up very often in the best-performing fea-
ture combinations, suggesting that the ASN an-
tecedents had a greater variety of syntactic types
than what was available in our CSN training data.
The context features (C) did not appear in any of
the best-performing feature combinations. In fact,
they resulted in a sharp decline in the precision. For
instance, for question, adding the context features
to the best-performing combination {E,SC,V,L,LX}
resulted in a drop of 16 percentage points. This
result was not surprising because although the an-
tecedents of ASNs and CSNs share similar proper-
ties such as common words, we know that their con-
text is generally different.
We did not observe specific features associated
with Schmid?s semantic categories. An exception
was the E features which were particularly effective
for the factual nouns fact and reason: the results
with them alone gave high precision (0.68 for fact
and 0.72 for reason). That said, the E features were
present in most of the best-performing combinations
even for the shell nouns in other semantic categories.
10CrowdFlower gives us a unique answer for each instance,
which we take to be the crowd?s answer. During annotation, ev-
ery annotator is presented with a few gold questions randomly
and each annotator is assigned a trust score based on her per-
formance on these gold questions. The unique answer for an
instance is the answer with the highest sum of trusts.
307
fact (43,000 train and 472 test instances)
Features P@1 P@2 P@3 P@4
{E,L,LX} .70? .85 .91 .94
{E,V,L,LX} .68? .86 .92 .95
{E,SC,L,LX} .66? .83 .92 .95
PSbaseline .40 ? ? ?
reason (4,520 train and 443 test instances)
Features P@1 P@2 P@3 P@4
{E,V,L} .72? .86 .90 .93
{E,V} .72? .85 .90 .92
{E,SC,LX} .69? .84 .90 .94
PSbaseline .44 ? ? ?
issue (3,000 train and 303 test instances)
Features P@1 P@2 P@3 P@4
{SC,L} .47? .59 .71 .78
{SC,L,LX} .46? .60 .70 .81
{S,E,SC,L,LX}.40? .61 .72 .81
PSbaseline .30 ? ? ?
decision (42,332 train and 390 test instances)
Features P@1 P@2 P@3 P@4
{E,LX} .35? .53 .67 .76
{E,SC,LX} .30? .48 .65 .75
{E,SC,V,L,LX}.27 .44 .57 .69
PSbaseline .21 ? ? ?
question (9,336 train and 440 test instances)
Features P@1 P@2 P@3 P@4
{E,SC,V,L,LX} .70? .82 .87 .90
{E,SC,LX} .68? .83 .88 .91
{E,SC,V,LX} .69? .80 .87 .91
PSbaseline .25 ? ? ?
possibility (11,735 train and 278 test instances)
Features P@1 P@2 P@3 P@4
{SC,L,LX} .56? .75 .87 .92
{E,SC} .56? .76 .87 .91
{E,L,LX} .54? .76 .86 .91
PSbaseline .34 ? ? ?
Table 4: Evaluation of our ranker for antecedents of six ASNs. For each noun we show the three best-performing
feature combinations. P@n is the precision at rank n (P@1 = standard precision). Boldface indicates the best in the
column. PSbaseline = preceding sentence baseline. The P@1 results significantly higher than PSbaseline are marked
with ?(two-sample ?2 test: p < 0.05). The chance baseline results were 0.1, 0.2, 0.3, and 0.4 for P@1, P@2, P@3,
and P@4, respectively.
The only previous work with which our results
could be compared is that of Kolhatkar and Hirst
(2012). The work reports precision in the range
of 0.41 to 0.61 in resolving this issue anaphora in
the Medline domain. In our case, for this issue in-
stances from the NYT corpus, we achieved precision
in the range of 0.40 to 0.47. Furthermore, we ap-
plied our models to resolve this issue instances from
Kolhatkar and Hirst?s (2012) work.11 Even with
models trained on automatically labelled CSN an-
tecedents, we achieved similar results to Kolhatkar
and Hirst?s results: P@1 of 0.45, P@2 of 0.59, P@3
of 0.65, and P@4 of 0.67. These results show the
domain robustness of our methods with respect to
the shell noun issue. Recall that Kolhatkar and Hirst
(2012) looked at only very specific cases of this is-
sue and used manually annotated data (Section 2),
as opposed to the automatically extracted CSN an-
tecedent data we use.
11We thank an anonymous reviewer for suggesting this to us.
8 Discussion and conclusion
The goal of this paper was to examine to what ex-
tent CSNs help in interpreting ASNs. Based on the
evaluators? satisfaction level and very few None re-
sponses, we conclude that our models trained on
CSN antecedents were able to bring the relevant
ASN antecedent candidates into the top 10 candi-
dates.
When we applied the models trained on CSN an-
tecedents to interpret ASNs, we achieved precision
in the range of 0.35 to 0.72. The precision results as
high as 0.72 for reason and 0.70 for fact and ques-
tion support our hypothesis that the linguistic knowl-
edge provided by CSN antecedents helps in identify-
ing the antecedents of ASNs. We observed different
behaviour for different nouns. The mental nouns is-
sue and decision in general were harder to interpret
than other shell nouns. The models trained on CSNs
achieved precisions of 0.35 for decision and 0.47 for
issue. So there is still much room for improvement.
That said, for the same nouns, the antecedents were
in the first four ranks about 76% to 81% of the times,
308
suggesting that in future research, our models can be
used as base models to reduce the large search space
of ASN antecedent candidates.
We observed a wide range of performance for dif-
ferent shell nouns. One reason is that the size of the
training data was different for different shell nouns.
After excluding the noisy examples (Section 5.2),
there were about 43,000 training examples for fact,
but only about 3,000 for issue. In addition, a par-
ticular shell concept itself can be difficult, e.g., the
very idea of what counts as an issue is more fuzzy
than what counts as a fact.
One limitation of our approach is that it only
learns the properties that are present in CSN an-
tecedents. However, ASN antecedents have addi-
tional properties which are not always captured by
CSN antecedents. For instance, for the shell noun
decision, most of the training examples were infini-
tive phrases of the form to X. But antecedents of the
ASN decision were mostly court decisions and were
expressed with full sentences.
Moreover, although the models trained on CSN
antecedents are able to encode characteristic fea-
tures associated with the general shell concept, they
are unable to distinguish between two competing
candidates both containing the characteristic fea-
tures of that shell concept. For instance, our ap-
proach will not be able to handle the constructed ex-
amples in (7).
(7) The teacher erased the solutions before John had
time to copy them out, as he had momentarily
been distracted by a band playing outside.
a) This fact infuriated him, as the teacher al-
ways erased the board quickly and John sus-
pected it was just to punish anyone who was
lost in thought, even for a moment.
b) This fact infuriated the teacher, who had al-
ready told John several times to focus on
class work.
Here, both propositions possess properties of the
shell concept fact. Understanding the context of the
anaphor itself is crucial in correctly identifying the
fact in each case, which cannot be learnt from CSN
antecedents due to their specific context patterns.
A number of extensions are planned for this work.
First, we plan to use both kinds of data, CSN and
ASN antecedent data, which will give us a basis
for developing a better performing ASN resolver.
We also plan to incorporate contextual features (e.g.,
right-frontier rule (Webber, 1991) and context rank-
ing (Eckert and Strube, 2000)). Finally, we will ex-
amine whether a model trained for one shell noun
can be generalized to other shell nouns from the
same semantic category.
Acknowledgements
We thank the anonymous reviewers for their
constructive comments. This material is based
upon work supported by the United States Air
Force and the Defense Advanced Research Projects
Agency under Contract No. FA8650-09-C-0179,
Ontario/Baden-Wu?rttemberg Faculty Research Ex-
change, and the University of Toronto.
References
Nicholas Asher. 1993. Reference to Abstract Objects in
Discourse. Kluwer Academic Publishers, Dordrecht,
Netherlands.
Donna K. Byron. 2004. Resolving pronominal refer-
ence to abstract entities. Ph.D. thesis, Rochester, New
York: University of Rochester.
Pascal Denis and Jason Baldridge. 2008. Specialized
models and ranking for coreference resolution. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 660?669,
Honolulu, Hawaii, October. Association for Computa-
tional Linguistics.
Miriam Eckert and Michael Strube. 2000. Dialogue acts,
synchronizing units, and anaphora resolution. Journal
of Semantics, 17:51?89.
Gill Francis. 1986. Anaphoric Nouns. Discourse Anal-
ysis Monographs 11, Birmingham: English Language
Research, University of Birmingham.
M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. Longman Pub Group.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In In Proceedings of
the 14th International Conference on Computational
Linguistics, pages 539?545, Nantes, France. Associa-
tion for Computational Linguistics.
Roz Ivanic. 1991. Nouns in search of a context: A study
of nouns with both open- and closed-system character-
istics. International Review of Applied Linguistics in
Language Teaching, 29:93?114.
Thorsten Joachims. 2002. Optimizing search engines us-
ing clickthrough data. In ACM SIGKDD Conference
309
on Knowledge Discovery and Data Mining (KDD),
pages 133?142.
Varada Kolhatkar and Graeme Hirst. 2012. Resolv-
ing ?this-issue? anaphora. In Proceedings of the
2012 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning, pages 1255?1265, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
Varada Kolhatkar, Heike Zinsmeister, and Graeme Hirst.
2013. Annotating anaphoric shell nouns with their an-
tecedents. In Proceedings of the 7th Linguistic Anno-
tation Workshop and Interoperability with Discourse,
pages 112?121, Sofia, Bulgaria, August. Association
for Computational Linguistics.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse re-
lations. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics,
pages 368?375, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313?330.
Christoph Mu?ller. 2008. Fully Automatic Resolution of
It, This and That in Unrestricted Multi-Party Dialog.
Ph.D. thesis, Universita?t Tu?bingen.
Rebecca J. Passonneau. 1989. Getting at discourse refer-
ents. In Proceedings of the 27th Annual Meeting of the
Association for Computational Linguistics, pages 51?
59, Vancouver, British Columbia, Canada. Association
for Computational Linguistics.
Massimo Poesio, Simone Ponzetto, and Yannick Versley.
2011. Computational models of anaphora resolution:
A survey. Unpublished.
Hans-Jo?rg Schmid. 2000. English Abstract Nouns As
Conceptual Shells: From Corpus to Cognition. Topics
in English Linguistics 34. Mouton de Gruyter, Berlin.
Wee Meng Soon, Hwee Tou Ng, and Chung Yong Lim.
2001. A machine learning approach to coreference
resolution of noun phrases. Computational Linguis-
tics, 27(4):521?544.
Michael Strube and Christoph Mu?ller. 2003. A machine
learning approach to pronoun resolution in spoken di-
alogue. In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics, pages
168?175, Sapporo, Japan, July. Association for Com-
putational Linguistics.
Zeno Vendler. 1968. Adjectives and Nominalizations.
Mouton de Gruyter, The Hague.
Bonnie Lynn Webber. 1991. Structure and ostension
in the interpretation of discourse deixis. In Language
and Cognitive Processes, pages 107?135.
Yiming Yang and Jan O. Pedersen. 1997. A comparative
study on feature selection in text categorization. In
Proceedings of the 14th International Conference on
Machine Learning, pages 412?420, Nashville, TN.
310
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 112?121,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Annotating Anaphoric Shell Nouns with their Antecedents
Varada Kolhatkar
Department of Computer Science
University of Toronto
varada@cs.toronto.edu
Heike Zinsmeister
Institut fu?r Maschinelle Sprachverarbeitung
Universita?t Stuttgart
zinsmeis@ims.stuttgart-uni.de
Graeme Hirst
Department of Computer Science
University of Toronto
gh@cs.toronto.edu
Abstract
Anaphoric shell nouns such as this is-
sue and this fact conceptually encapsulate
complex pieces of information (Schmid,
2000). We examine the feasibility of anno-
tating such anaphoric nouns using crowd-
sourcing. In particular, we present our
methodology for reliably annotating an-
tecedents of such anaphoric nouns and the
challenges we faced in doing so. We also
evaluated the quality of crowd annotation
using experts. The results suggest that
most of the crowd annotations were good
enough to use as training data for resolv-
ing such anaphoric nouns.
1 Introduction
Anaphoric shell nouns (ASNs) such as this fact,
this possibility, and this issue are common in all
kinds of text. They are called shell nouns be-
cause they provide nominal conceptual shells for
complex chunks of information representing ab-
stract concepts such as fact, proposition, and event
(Schmid, 2000). An example is shown in (1).
(1) Despite decades of education and widespread course
offerings, the survival rate for out-of-hospital car-
diac arrest remains a dismal 6 percent or less
worldwide.
This fact prompted the American Heart Association
last November to simplify the steps of CPR to make
it easier for lay people to remember and to encour-
age even those who have not been formally trained
to try it when needed.
Here, the ASN this fact encapsulates the clause
marked in bold from the preceding paragraph.
ASNs play an important role in organizing a dis-
course. First, they are used metadiscursively to
talk about the current discourse. In (1), the au-
thor characterizes the information presented in the
context by referring to it as a fact ? a thing that
is indisputably the case. Second, they are used as
cohesive devices in a discourse. In (1), for exam-
ple, this fact on the one hand refers to the propo-
sition marked in bold, and on the other, faces for-
ward and serves as the starting point of the follow-
ing paragraph. Finally, as Schmid (2000) points
out, like conjunctions so and however, ASNs
may function as topic boundary markers and topic
change markers.
Despite their importance, ASNs have not re-
ceived much attention in Computational Linguis-
tics. Although there has been some effort to anno-
tate certain anaphors with similar properties, i.e.,
demonstratives and the pronoun it (Byron, 2003;
Artstein and Poesio, 2006), in contrast to ordi-
nary nominal anaphora, there are not many anno-
tated corpora available that could be used to study
ASNs. Indeed, many questions of annotation of
ASNs must still be answered. For example, the
extent to which native speakers themselves agree
on the resolution of such anaphors, i.e., on the pre-
cise antecedents, remains unclear.
An essential first step in this field of research
is therefore to clearly establish the extent of inter-
annotator agreement on antecedents of ASNs as
a measure of feasibility of the task. In this pa-
per, we describe our methodology for annotating
ASNs using crowdsourcing, a cheap and fast way
of obtaining annotation. We also describe how we
evaluated the feasibility of the task and the quality
of the annotation, and the challenges we faced in
doing so, both with regard to the task itself and the
crowdsourcing platform we use. The results sug-
gest that most of the crowd-annotations were good
enough to use as training data for ASN resolution.
112
2 Related work
There exist only few annotated corpora of
anaphora with non-nominal antecedents (Dipper
and Zinsmeister, 2011). The largest one of these,
the ARRAU corpus (Poesio and Artstein, 2008),
contains 455 anaphors pointing to non-nominal
antecedents, but only a few instances are ASNs.
Kolhatkar and Hirst (2012) annotated antecedents
of the same type as we do, but restricted their ef-
forts to the ASN this issue.1 In addition, there are
corpora annotated with event anaphora in which
verbal instances are identified as proxies for non-
nominal antecedents (Pradhan et al, 2007; Chen
et al, 2011; Lee et al, 2012).
For the task of identifying non-nominal an-
tecedents as free spans of text, there is no standard
way of reporting inter-annotator agreement. Some
studies report only observed percentage agree-
ment with results in the range of about 0.40?
0.55 (Vieira et al, 2002; Dipper and Zinsmeis-
ter, 2011). The studies differed with respect to
number of annotators, types of anaphors, and lan-
guage of the corpora. Artstein and Poesio (2006)
discuss Krippendorff?s alpha for chance-corrected
agreement. They considered antecedent strings as
bags of words and computed the degree of differ-
ence between them by different distance measures
(e.g. Jaccard, Dice). The bag-of-words approach
is rather optimistic in the sense that even two non-
overlapping strings are very likely to share at least
a few words. Kolhatkar and Hirst (2012) followed
a different approach by using Krippendorff?s uni-
tizing alpha (u?) which considers the longest com-
mon subsequence of different antecedent options
(Krippendorff, 2013). They reported high chance-
corrected u? of 0.86 for two annotators but in a
very restricted domain.
There has been some prior effort to annotate
anaphora and coreference using Games with a
Purpose as a method of crowdsourcing (Chamber-
lain et al, 2009; Hladka? et al, 2009). Another, less
time-consuming approach of crowdsourcing is us-
ing platforms such as Amazon Mechanical Turk2.
It has been shown that crowdsourced data can suc-
cessfully be used as training data for NLP tasks
(Hsueh et al, 2009).
1Another data set reported in the literature could have
been relevant for us: Botley?s (2006) corpus contained about
462 ASN instances signaled by shell nouns; but this data is
no longer available (S. Botley, p.c.).
2https://mturk.com/mturk/
Class Description Examples
factual states of affairs fact, reason
linguistic linguistic acts question, report
mental thoughts and ideas issue, decision
modal subjective judgements possibility, truth
eventive events act, reaction
circumstantial situations situation, way
Table 1: Schmids categorization of shell nouns.
The nouns in boldface are used in this research.
3 The Anaphoric Shell Noun Corpus
Our goal is to obtain annotated data for ASN an-
tecedents that could be used to train a supervised
machine learning system to resolve ASNs. For
that, we created the Anaphoric Shell Noun (ASN)
corpus.
Schmid (2000) provides a list of 670 English
nouns which are frequently used as shell nouns.
He divides them into six broad semantic classes:
factual, mental, linguistic, modal, circumstantial,
and eventive. Table 1 shows this classification,
along with example shell nouns for each category.
To begin with, we considered articles contain-
ing occurrences of these 670 shell nouns from the
New York Times (NYT) corpus (about 711,046
occurrences).3 To create a corpus of a manage-
able size for annotation, we considered first 10
highly frequent shell nouns distributed across each
of Schmid?s shell noun categories from Table 1
and extracted ASN instances by searching for the
pattern {this shell noun} in these articles.4
To examine the feasibility of the annotation, we
systematically annotated sample data ourselves,
which contained about 15 examples of each of
these 10 highly frequent shell nouns. The anno-
tation process revealed that not all ASN instances
are easy to resolve. The instances with shell nouns
from the circumstantial and eventive categories, in
particular, had very long and unclear antecedents.
So we excluded these categories in this research
and work with six shell nouns from the other four
categories: fact, reason, issue, decision, question,
and possibility. To create the ASN corpus, we
extracted about 500 instances for each of these
six shell nouns. After removing duplicates and
instances with a non-abstract sense (e.g., this is-
3http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2008T19
4Schmid (2000) provides patterns for anaphoric shell
nouns, and this-NP is the most prominent pattern among
them.
113
sue with a publication-related sense), we were left
with 2,822 ASN instances.
4 ASN Annotation Challenges
ASN antecedent annotation is a complex task, as
it involves deeply understanding the discourse and
interpreting it. Here we point out two main chal-
lenges associated with the task.
What to annotate? The question of ?what to an-
notate? as mentioned by Fort et al (2012) is not
straightforward for ASN antecedents, as the no-
tion of markables is complex compared to ordi-
nary nominal anaphora: the units on which the
annotation work should focus are heterogeneous.5
Moreover, due to this heterogeneous nature of an-
notation units, there is a huge number of mark-
ables (e.g., all syntactic constituents given by a
syntactic parse tree). So there are many options
to choose from, while only a few units are actu-
ally to be annotated. Moreover, there is no one-
to-one correspondence between the syntactic type
of an antecedent and the semantic type of its refer-
ent (Webber, 1991). For instance, a semantic type
such as fact can be expressed with different syn-
tactic shapes such as a clause, a verb phrase, or a
complex sentence. Conversely, a syntactic shape,
such as a clause, can function as several semantic
types, including fact, proposition, and event.
Lack of the notion of the right answer It is not
obvious how to define clear and detailed annota-
tion guidelines to create a gold-standard corpus
for ASN antecedent annotation due to our limited
understanding of the nature and interpretation of
such antecedents. The notion of the right answer
is not well-defined for ASN antecedents. Indeed
most people will be hard-pressed to say whether
or not to include the clause Despite decades of
education and widespread course offerings in the
antecedent of this fact in example (1). The main
challenge is to identify the conditions when two
different candidates for annotation should be con-
sidered as representing essentially the same con-
cept, which raises deep philosophical issues that
we do not propose to solve in this paper. For our
purposes, we believe, this challenge could only
be possibly tackled by the requirements of down-
stream applications of ASN resolution.
5Occasionally, ASN antecedents are non-contiguous
spans of text, but in this work, we ignore them for simplicity.
5 Annotation Methodology
Considering the difficulties of ASN annotation
discussed above, there were two main challenges
involved in the annotation process: first, to find an-
notators who can annotate data reliably with min-
imal guidelines, and second, to design simple an-
notation tasks that will elicit data useful for our
purposes. Now we discuss how we dealt with
these challenges.
Crowdsourcing We wanted to examine to what
extent non-expert native speakers of English with
minimal annotation guidelines would agree on
ASN antecedents. We explored the possibility of
using crowdsourcing, which is an effective way to
obtain annotations for natural language research
(Snow et al, 2008). In particular, we explored the
use of CrowdFlower6, a crowdsourcing platform
that in turn uses various worker channels such as
Amazon Mechanical Turk. CrowdFlower offers a
number of features.
First, it offers a number of integrated quality-
control mechanisms. For instance, it throws gold
questions randomly at the annotators, and anno-
tators who do not answer them correctly are not
allowed to continue. To further minimize spam-
mers, it also offers a training phase before the ac-
tual annotation. In this phase, every annotator is
presented with a few gold questions. Only those
annotators who get the gold questions right get ad-
mittance to do the actual annotation.
Second, CrowdFlower chooses a unique answer
for each annotation unit based on the majority vote
of the trusted annotators. For each annotator, it
assigns a trust level based on how she performs
on the gold examples. The unique answer is com-
puted by adding together the trust scores of an-
notators, and then picking the answer with the
highest sum of trusts (CrowdFlower team, p.c.).
It also assigns a confidence score (denoted as c
henceforth) for each answer, which is a normal-
ized score of the summation of the trusts. For ex-
ample, suppose annotators A, B, and C with trust
levels 0.75, 0.75, and 1.0 give answers no, yes, yes
respectively for a particular instance. Then the an-
swer yes will score 1.75 and answer no will score
0.75 and yes will be chosen as the crowd?s answer
with c = 0.7 (i.e., 1.75/(1.75 + 0.75)). We use
these confidence scores in our analysis of inter-
annotator agreement below.
6http://crowdflower.com/
114
Finally, CrowdFlower also provides detailed an-
notation results including demographic informa-
tion and trustworthiness of each annotator.
Design of the annotation tasks With the help of
well-designed gold examples, CrowdFlower can
get rid of spammers and ensures that only reliable
annotators perform the annotation task. But the
annotation task must be well-designed in the first
place to get a good quality annotation. Following
the claim in the literature that with crowdsourc-
ing platforms simple tasks do best (Madnani et al,
2010; Wang et al, 2012), we split our annotation
task into two relatively simple sequential annota-
tion tasks. First, identifying the broad region of the
antecedent, i.e., not the precise antecedent but the
region where the antecedent lies, and second, iden-
tifying the precise antecedent, given the broad re-
gion of the antecedent. Now we will discuss each
of our annotation tasks in detail.
5.1 CrowdFlower experiment 1
The first annotation task was about identifying the
broad region of ASN antecedents without actu-
ally pinpointing the precise antecedents. We de-
fined the broad region as the sentence containing
the ASN antecedent, as the shell nouns we have
chosen tend to have antecedents that lie within
a single sentence. We designed a CrowdFlower
experiment where we presented to the annotators
ASNs from the ASN corpus with three preceding
paragraphs as context. Sentences in the vicinity
of ASNs were each labelled: four sentences pre-
ceding the anaphor, the sentence containing the
anaphor, and two sentences following the anaphor.
This choice was based on our pilot annotation:
the antecedents very rarely occur more than four
sentences away from the anaphor. The annota-
tion task was to pinpoint the sentence in the pre-
sented text that contained the antecedent for the
ASN and selecting the appropriate sentence label
as the correct answer. If no labelled sentence in the
presented text contained the antecedent, we sug-
gested to the annotators to select None. If the an-
tecedent spanned more than one sentence, then we
suggested to them to select Combination. We also
provided a link to the complete article from which
the text was drawn in case the annotators wanted
to have a look at it.
Settings We asked for 8 judgements per instance
and paid 8 cents per annotation unit. Our job
contained in total 2,822 annotation units with 168
gold units. As we were interested in the ver-
dict of native speakers of English, we limited the
allowed demographic region to English-speaking
countries.
5.2 CrowdFlower experiment 2
This annotation task was about pinpointing the
exact antecedent text of the ASN instances. We
designed a CrowdFlower experiment, where we
presented to the annotators ASN instances from
the ASN corpus with highlighted ASNs and the
sentences containing the antecedents, the output
of experiment 1. One way to pinpoint the ex-
act antecedent string is to ask the annotators to
mark free spans of text within the antecedent sen-
tence, similar to Byron (2003) and Artstein and
Poesio (2006). However, CrowdFlower quality-
control mechanisms require multiple-choice an-
notation labels. So we decided to display a set
of labelled candidates to the annotators and ask
them to choose the answer that best represents
the ASN antecedent. A practical requirement of
this approach is that the number of options to be
displayed be only a handful in order to make it
a feasible task for online annotation. But as we
noted in Section 4, the number of markables for
ASN antecedents is large. If, for example, we de-
fine markables as all syntactic constituents given
by the Stanford parser7, there are on average 49.5
such candidates per sentence in the ASN corpus. It
is not practical to display all these candidates and
to ask CrowdFlower annotators to choose one an-
swer from this many options. Also, some potential
candidates are clearly not appropriate candidates
for a particular shell noun. For instance, the NP
constituent the survival rate in example (1) is not
an appropriate candidate for the shell noun fact as
generally facts are propositions. So the question is
whether it is possible to restrict this set of candi-
dates by discarding unlikely ones.
To deal with this question, we used super-
vised machine learning methods trained on easy,
non-anaphoric unlabelled examples of shell nouns
(e.g., the fact that X). In this paper, we will focus
on the annotation and will treat these methods as a
black box. In brief, the methods reduce the large
search space of ASN antecedent candidates to a
size that is manageable for crowdsourcing anno-
tation, without eliminating the most likely candi-
7http://nlp.stanford.edu/software/
lex-parser.shtml
115
dates. We displayed the 10 most-likely candidates
given by these methods. In addition, we made sure
not to display two candidates with only a negli-
gible difference. For example, given two candi-
dates, X and that X, which differ only with respect
to the introductory that, we chose to display only
the longer candidate that X.
In a controlled annotation, with detailed guide-
lines, such difficulties of selecting between minor
variations could be avoided. However, such de-
tailed annotation guidelines still have to be devel-
oped.
Settings As in experiment 1, we asked for 8
judgements per instance and paid 6 cents per anno-
tation unit. But for this experiment we considered
only 2,323 annotation units with 151 gold units,
only high-confidence units (c ? 0.5) from experi-
ment 1. This task turned out to be a suitable task
for crowdsourcing as it offered a limited number
of options to choose from, instead of asking the
annotators to mark arbitrary spans of text.
6 Agreement
Our annotation tasks pose difficulties in measur-
ing inter-annotator agreement both in terms of the
task itself and the platform used for annotation. In
this section, we describe our attempt to compute
agreement for each of our annotation tasks and the
challenges we faced in doing so.
6.1 CrowdFlower experiment 1
Recall that in this experiment, annotators identify
the sentence containing the antecedent and select
the appropriate sentence label as their answer. We
know from our pilot annotation that the distribu-
tion of such labels is skewed: most of the ASN an-
tecedents lie in the sentence preceding the anaphor
sentence. We observed the same trend in the re-
sults of this experiment. In the ASN corpus, the
crowd chose the preceding sentence 64% of the
time, the same sentence 13% of the time, and long-
distance sentences 23% of the time.8 Consider-
ing the skewed distribution of labels, if we use tra-
ditional agreement coefficients, such as Cohen?s
? (1960) or Krippendorff?s ? (2013), expected
agreement is very high, which in turn results in a
low reliability coefficient (in our case ? = 0.61)
that does not necessarily reflect the true reliability
of the annotation (Artstein and Poesio, 2008).
8This confirms Passonneau?s (1989) observation that non-
nominal antecedents tend to be close to the anaphors.
F R I D Q P all
c < .5 8 8 36 21 13 7 16
.5? c < .6 6 6 13 8 7 5 8
.6? c < .8 24 25 31 31 22 27 27
.8? c < 1. 22 23 11 14 19 25 18
c = 1. 40 38 9 26 39 36 31
Average c .83 .82 .61 .72 .80 .83 .76
Table 2: CrowdFlower confidence distribution for
CrowdFlower experiment 1. Each column shows
the distribution in percentages for confidence of
annotating antecedents of that shell noun. The fi-
nal row shows the average confidence of the dis-
tribution. Number of ASN instances = 2,822. F
= fact, R = reason, I = issue, D = decision, Q =
question, P = possibility.
One way to measure the reliability of the data,
without taking chance correction into account, is
to consider the distribution of the ASN instances
with different levels of CrowdFlower confidence.
Table 2 shows the percentages of instances in dif-
ferent confidence level bands for each shell noun
as well as for all instances. For example, for the
shell noun fact, 8% of the total number of this fact
instances were annotated with c < 0.5. As we
can see, most of the instances of the shell nouns
fact, reason, question, and possibility were anno-
tated with high confidence. In addition, most of
them occurred in the band 0.8 ? c ? 1. There
are relatively few instances with low confidence
for these nouns, suggesting the feasibility of re-
liable antecedent annotation for these nouns. By
contrast, the mental nouns issue and decision had
a large number of low-confidence (c < 0.5) in-
stances, bringing in the question of reliability of
antecedent annotation of these nouns.
Given these results with different confidence
levels, the primary question is what confidence
level should be considered acceptable? For our
task, we required that at least four trusted anno-
tators out of eight annotators should agree on an
answer for it to be acceptable.9 We will talk about
acceptability later in Section 7.
6.2 CrowdFlower experiment 2
Recall that this experiment was about identifying
the precise antecedent text segment given the sen-
tence containing the antecedent. It is not clear
what the best way to measure the amount of such
9We chose this threshold after systematically examining
instances with different confidence levels.
116
Jaccard Dice
Do De ? Do De ?
A&P .53 .95 .45 .43 .94 .55
Our results .47 .96 .51 .36 .92 .61
Table 3: Agreement using Krippendorff?s ? for
CrowdFlower experiment 2. A&P = Artstein and
Poesio (2006).
agreement is. Agreement coefficients such as Co-
hen?s ? underestimate the degree of agreement for
such annotation, suggesting disagreement even be-
tween two very similar annotated units (e.g., two
text segments that differ in just a word or two).
We present the agreement results in three different
ways: Krippendorff?s ? with distance metrics Jac-
card and Dice (Artstein and Poesio, 2006), Krip-
pendorff?s unitizing alpha (Krippendorff, 2013),
and CrowdFlower confidence values.
Krippendorff?s ? using Jaccard and Dice To
compare our agreement results with previous ef-
forts to annotate such antecedents, following Art-
stein and Poesio (2006), we computed Krippen-
dorff?s ? using distance metrics Jaccard and Dice.
The general form of coefficient ? is:
? = 1? Do
De
(1)
where Do and De are observed and expected dis-
agreements respectively. ? = 1 indicates perfect
reliability and u? = 0 indicates the absence of re-
liability. When u? < 0, either the sample size
is very small or the disagreement is systematic.
Table 3 shows the agreement results. Our agree-
ment results are comparable to Artstein and Poe-
sio?s agreement results. They had 20 annotators
annotating 16 anaphor instances with segment an-
tecedents, whereas we had 8 annotators annotat-
ing 2,323 ASN instances. As Artstein and Poesio
point out, expected disagreement in case of such
antecedent annotation is close to maximal, as there
is little overlap between segment antecedents of
different anaphors and therefore ? pretty much re-
flects the observed agreement.
Krippendorff?s unitizing ? (u?) Following
Kolhatkar and Hirst (2012), we use u? for measur-
ing reliability of the ASN antecedent annotation
task. This coefficient is appropriate when the an-
notators work on the same text, identify the units
in the text that are relevant to the given research
F R I D Q P all
c < .5 11 17 32 31 14 28 21
.5? c < .6 12 12 19 23 9 19 15
.6? c < .8 36 33 34 32 30 36 33
.8? c < 1. 24 22 10 10 21 13 18
c = 1. 17 16 5 3 26 4 13
Average c .74 .71 .60 .59 .77 .62 .68
Table 4: CrowdFlower confidence distribution for
CrowdFlower experiment 2. Each column shows
the distribution in percentages for confidence of
annotating antecedents of that shell noun. The fi-
nal row shows the average confidence of the dis-
tribution. Number of ASN instances = 2,323. F
= fact, R = reason, I = issue, D = decision, Q =
question, P = possibility.
question, and then label the identified units (Krip-
pendorff, p.c.). The general form of coefficient
u? is the same as in equation 1. In our context,
the annotators work on the same text, the ASN in-
stances. We define an elementary annotation unit
(the smallest separately judged unit) to be a word
token. The annotators identify and locate ASN
antecedents for the given anaphor in terms of se-
quences of elementary annotation units.
u? incorporates the notion of distance between
strings by using a distance function which is de-
fined as the square of the distance between the
non-overlapping tokens in our case. The distance
is 0 when the annotated units are exactly the same,
and is the summation of the squares of the un-
matched parts if they are different. We compute
observed and expected disagreement as explained
by Krippendorff (2013, Section 12.4). For our
data, u? was 0.54.10 u? was lower for the men-
tal nouns issue and decision and the modal noun
possibility compared to other shell nouns.
CrowdFlower confidence results We also ex-
amined different confidence levels for ASN an-
tecedent annotation. Table 4 gives confidence re-
sults for all instances and for each noun. In con-
trast with Table 2, the instances are more evenly
distributed here. As in experiment 1, the men-
tal nouns issue and decision had many low con-
fidence instances. For the modal noun possibility,
it was easy to identify the sentence containing the
antecedent, but pinpointing the precise antecedent
10Note that u? reported here is just an approximation of
the actual agreement as in our case the annotators chose an
option from a set of predefined options instead of marking
free spans of text.
117
turned out to be difficult.
Now we discuss the nature of disagreement in
ASN annotation.
Disagreement in experiment 1 There were two
primary sources of disagreement in experiment 1.
First, the annotators had problems agreeing on the
answer None. We instructed them to choose None
when the sentence containing the antecedent was
not labelled. Nonetheless, some annotators chose
sentences that did not precisely contain the actual
antecedent but just hinted at it. Second, sometimes
it was hard to identify the precise antecedent sen-
tence as the antecedent was either present in the
blend of all labelled sentences or there were multi-
ple possible answers, as shown in example (2).
(2) Any biography of Thomas More has to answer one
fundamental question. Why? Why, out of all the
many ambitious politicians of early Tudor England,
did only one refuse to acquiesce to a simple piece
of religious and political opportunism? What was it
about More that set him apart and doomed him to a
spectacularly avoidable execution?
The innovation of Peter Ackroyd?s new biography of
More is that he places the answer to this question
outside of More himself.
Here, the author formulates the question in a num-
ber of ways and any question mentioned in the
preceding text can serve as the antecedent of the
anaphor this question.
Hard instances Low agreement can indicate
different problems: unclear guidelines, poor-
quality annotators, or difficult instances (e.g., not
well understood linguistic phenomena) (Artstein
and Poesio, 2006). We can rule out the pos-
sibility of poor-quality annotators for two rea-
sons. First, we consider 8 diverse annotators
who work independently. Second, we use Crowd-
Flower?s quality-control mechanisms and hence
allow only trustworthy annotators to annotate our
texts. Regarding instructions, we take inter-
annotator agreement as a measure for feasibility of
the task, and hence we keep the annotation instruc-
tion as simple as possible. This could be a source
of low agreement. The third possibility is hard in-
stances. Our results show that the mental nouns
issue and decision had many low-confidence in-
stances, suggesting the difficulty associated with
the interpretation of these nouns (e.g., the very
idea of what counts as an issue is fuzzy). The shell
noun decision was harder because most of its in-
stances were court-decision related articles, which
were in general hard to understand.
Different strings representing similar concepts
As noted in Section 4, the main challenge with the
ASN annotation task is that different antecedent
candidates might represent the same concept and
it is not trivial to incorporate this idea in the anno-
tation process. When five trusted annotators iden-
tify the antecedent as but X and three trusted anno-
tators identify it as merely X, since CrowdFlower
will consider these two answers to be two com-
pletely different answers, it will give the answer
but X a confidence of only about 0.6. u? or ? with
Jaccard and Dice will not consider this as a com-
plete disagreement; however, the coefficients will
register it as a difference. In other words, the dif-
ference functions used with these coefficients do
not respond to semantics, paraphrases, and other
similarities that humans might judge as inconse-
quential. One way to deal with this problem would
be clustering the options that reflect essentially the
same concepts before measuring the agreement.
Some of these problems could also be avoided by
formulating instructions for marking antecedents
so that these differences do not occur in the iden-
tified antecedents. However, crowdsourcing plat-
forms require annotation guidelines to be clear and
minimal, which makes it difficult to control the an-
notation variations.
7 Evaluation of Crowd Annotation
CrowdFlower experiment 2 resulted in 1,810 ASN
instances with c > 0.5. The question is how good
are these annotations from experts? point of view.
To examine the quality of the crowd annotation
we asked two judges A and B to evaluate the ac-
ceptability of the crowd?s answers. The judges
were highly-qualified academic editors: A, a re-
searcher in Linguistics and B, a translator with a
Ph.D. in History and Philosophy of Science. From
the crowd-annotated ASN antecedent data, we
randomly selected 300 instances, 50 instances per
shell noun. We made sure to choose instances with
borderline confidence (0.5 ? c < 0.6), medium
confidence (0.6 ? c < 0.8), and high confidence
(0.8 ? c ? 1.0). We asked the judges to rate
the acceptability of the crowd-answers based on
the extent to which they provided interpretation of
the corresponding anaphor. We gave them four
options: perfectly (the crowd?s answer is perfect
and the judge would have chosen the same an-
tecedent), reasonably (the crowd?s answer is ac-
ceptable and is close to their answer),
118
Judge B
P R I N Total
Judge A
P 171 44 11 7 233
R 12 27 7 4 50
I 2 4 6 1 13
N 1 2 0 1 4
Total 186 77 24 13 300
Table 5: Evaluation of ASN antecedent annota-
tion. P = perfectly, R = reasonably, I = implicitly,
N = not at all
implicitly (the crowd?s answer only implicitly
contains the actual antecedent), and not at all (the
crowd?s answer is not in any way related to the
actual antecedent).11 Moreover, if they did not
mark perfectly, we asked them to provide their an-
tecedent string. The two judges worked on the task
independently and they were completely unaware
of how the annotation data was collected.
Table 5 shows the confusion matrix of the rat-
ings of the two judges. Judge B was stricter than
Judge A. Given the nature of the task, it was
encouraging that most of the crowd-antecedents
were rated as perfectly by both judges (72% by
A and 62% by B). Note that perfectly is rather a
strong evaluation for ASN antecedent annotation,
considering the nature of ASN antecedents them-
selves. If we weaken the acceptability criteria and
consider the antecedents rated as reasonably to be
also acceptable antecedents, 84.6% of the total in-
stances were acceptable according to both judges.
Regarding the instances marked implicitly, most
of the times the crowd?s answer was the closest
textual string of the judges? answer. So we again
might consider instances marked implicitly as ac-
ceptable answers.
For a very few instances (only about 5%) either
of the judges marked not at all. This was a posi-
tive result and suggests success of different steps
of our annotation procedure: identifying broad re-
gion, identifying the set of most likely candidates,
and identifying precise antecedent. As we can see
in Table 5, there were 7 instances where the judge
A rated perfectly while the judge B rated not at all,
i.e., completely contradictory judgements. When
we looked at these examples, they were rather hard
and ambiguous cases. An example is shown in (3).
The whether clause marked in the preceding sen-
11Before starting the actual annotation, we carried out a
training phase with 30 instances, which gave an opportunity
to the judges to ask questions about the task.
tence is the crowd?s answer. One of our judges
rated this answer as perfectly, while the other rated
it as not at all. According to her the correct an-
tecedent is whether Catholics who vote for Mr.
Kerry would have to go to confession.
(3) Several Vatican officials said, however, that any such
talk has little meaning because the church does not
take sides in elections. But the statements by several
American bishops that Catholics who vote for Mr.
Kerry would have to go to confession have raised
the question in many corners about whether this is
an official church position.
The church has not addressed this question publicly
and, in fact, seems reluctant to be dragged into the
fight...?
There was no notable relation between the an-
notator?s rating and the confidence level: many in-
stances with borderline confidence were marked
perfectly or reasonably, suggesting that instances
with c ? 0.5 were reasonably annotated instances,
to be used as training data for ASN resolution.
8 Conclusion
In this paper, we addressed the fundamental ques-
tion about feasibility of ASN antecedent annota-
tion, which is a necessary step before developing
computational approaches to resolve ASNs. We
carried out crowdsourcing experiments to get na-
tive speaker judgements on ASN antecedents. Our
results show that among 8 diverse annotators who
worked independently with a minimal set of an-
notation instructions, usually at least 4 annotators
converged on a single ASN antecedent. The re-
sult is quite encouraging considering the nature of
such antecedents.
We asked two highly-qualified judges to in-
dependently examine the quality of a sample of
crowd-annotated ASN antecedents. According to
both judges, about 95% of the crowd-annotations
were acceptable. We plan to use this crowd-
annotated data (1,810 instances) as training data
for an ASN resolver. We also plan to distribute the
annotations at a later date.
Acknowledgements
We thank the CrowdFlower team for their respon-
siveness and Hans-Jo?rg Schmid for helpful dis-
cussions. This material is based upon work sup-
ported by the United States Air Force and the De-
fense Advanced Research Projects Agency under
Contract No. FA8650-09-C-0179, Ontario/Baden-
Wu?rttemberg Faculty Research Exchange, and the
University of Toronto.
119
References
Ron Artstein and Massimo Poesio. 2006. Identify-
ing reference to abstract objects in dialogue. In
Proceedings of the 10th Workshop on the Semantics
and Pragmatics of Dialogue, pages 56?63, Potsdam,
Germany.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596, December.
Simon Philip Botley. 2006. Indirect anaphora: Testing
the limits of corpus-based linguistics. International
Journal of Corpus Linguistics, 11(1):73?112.
Donna K. Byron. 2003. Annotation of pronouns and
their antecedents: A comparison of two domains.
Technical report, University of Rochester. Computer
Science Department.
Jon Chamberlain, Udo Kruschwitz, and Massimo Poe-
sio. 2009. Constructing an anaphorically anno-
tated corpus with non-experts: Assessing the quality
of collaborative annotations. In Proceedings of the
2009 Workshop on The People?s Web Meets NLP:
Collaboratively Constructed Semantic Resources,
pages 57?62, Suntec, Singapore, August. Associa-
tion for Computational Linguistics.
Bin Chen, Jian Su, Sinno Jialin Pan, and Chew Lim
Tan. 2011. A unified event coreference resolu-
tion by integrating multiple resolvers. In Proceed-
ings of 5th International Joint Conference on Nat-
ural Language Processing, pages 102?110, Chiang
Mai, Thailand, November.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20(1):37.
Stefanie Dipper and Heike Zinsmeister. 2011. Anno-
tating abstract anaphora. Language Resources and
Evaluation, 69:1?16.
Kare?n Fort, Adeline Nazarenko, and Sophie Rosset.
2012. Modeling the complexity of manual anno-
tation tasks: a grid of analysis. In 24th Inter-
national Conference on Computational Linguistics,
pages 895?910.
Barbora Hladka?, Jir??? M??rovsky?, and Pavel Schlesinger.
2009. Play the language: Play coreference. In Pro-
ceedings of the Association of Computational Lin-
guistics and International Joint Conference on Nat-
ural Language Processing 2009 Conference Short
Papers, pages 209?212, Suntec, Singapore, August.
Association for Computational Linguistics.
Pei-Yun Hsueh, Prem Melville, and Vikas Sindhwani.
2009. Data quality from crowdsourcing: A study of
annotation selection criteria. In Proceedings of the
NAACL HLT 2009 Workshop on Active Learning for
Natural Language Processing, pages 27?35, Boul-
der, Colorado, June. Association for Computational
Linguistics.
Varada Kolhatkar and Graeme Hirst. 2012. Resolving
?this-issue? anaphora. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 1255?1265, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
Klaus Krippendorff. 2013. Content Analysis: An
Introduction to Its Methodology. Sage, Thousand
Oaks, CA, third edition.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity and
event coreference resolution across documents. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
489?500, Jeju Island, Korea, July. Association for
Computational Linguistics.
Nitin Madnani, Jordan Boyd-Graber, and Philip
Resnik. 2010. Measuring transitivity using un-
trained annotators. In Proceedings of the NAACL
HLT 2010 Workshop on Creating Speech and Lan-
guage Data with Amazon?s Mechanical Turk, pages
188?194, Los Angeles, June. Association for Com-
putational Linguistics.
Rebecca J. Passonneau. 1989. Getting at discourse ref-
erents. In Proceedings of the 27th Annual Meeting
of the Association for Computational Linguistics,
pages 51?59, Vancouver, British Columbia, Canada,
June. Association for Computational Linguistics.
Massimo Poesio and Ron Artstein. 2008. Anaphoric
annotation in the ARRAU corpus. In Proeedings
of the Sixth International Conference on Language
Resources and Evaluation (LREC?08), Marrakech,
Morocco, May. European Language Resources As-
sociation (ELRA).
Sameer S. Pradhan, Lance A. Ramshaw, Ralph M.
Weischedel, Jessica MacBride, and Linnea Micci-
ulla. 2007. Unrestricted coreference: Identifying
entities and events in OntoNotes. In Proceedings of
the International Conference on Semantic Comput-
ing, pages 446?453, September.
Hans-Jo?rg Schmid. 2000. English Abstract Nouns As
Conceptual Shells: From Corpus to Cognition. Top-
ics in English Linguistics 34. De Gruyter Mouton,
Berlin.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and fast ? but is it
good? evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 254?263, Honolulu, Hawaii, Oc-
tober. Association for Computational Linguistics.
Renata Vieira, Susanne Salmon-Alt, Caroline
Gasperin, Emmanuel Schang, and Gabriel Othero.
2002. Coreference and anaphoric relations of
120
demonstrative noun phrases in multilingual corpus.
In Proceedings of the 4th Discourse Anaphora and
Anaphor Resolution Conference (DAARC 2002),
pages 385?427, Lisbon, Portugal, September.
Aobo Wang, Cong Duy Vu Hoang, and Min-Yen Kan.
2012. Perspectives on crowdsourcing annotations
for natural language processing. In Language Re-
sources and Evaluation, volume in press, pages 1?
23. Springer.
Bonnie Lynn Webber. 1991. Structure and ostension in
the interpretation of discourse deixis. In Language
and Cognitive Processes, pages 107?135.
121
LAW VIII - The 8th Linguistic Annotation Workshop, pages 99?104,
Dublin, Ireland, August 23-24 2014.
Annotating descriptively incomplete language phenomena
Fabian Barteld, Sarah Ihden, Ingrid Schr?der, and Heike Zinsmeister
Institut f?r Germanistik
Universit?t Hamburg
Von-Melle-Park 6
20146 Hamburg, Germany
{ fabian.barteld, sarah.ihden, ingrid.schroeder, heike.zinsmeister }
@uni-hamburg.de
Abstract
When annotating non-standard languages, descriptively incomplete language phenomena (EA-
GLES, 1996) are often encountered. In this paper, we present examples of ambiguous forms
taken from a historical corpus and offer a classification of such descriptively incomplete lan-
guage phenomena and its rationale. We then discuss various approaches to the annotation of
these phenomena, arguing that multiple annotations provide the most appropriate encoding strat-
egy for the annotator. Finally, we show how multiple annotations can be encoded in existing
standards such as PAULA and GrAF.
1 Introduction
In grammatical annotations, a lack of ambiguity is of great benefit: The more distinctive the relationship
between a token and its morphological and syntactic attributes, the more successful and reliable the an-
notation. However, especially in corpora of non-standard language varieties annotators are confronted
with a significant number of cases of doubt and ambiguity. This problem has been more relevant in se-
mantic and syntactic analyses than in PoS tagging and morphological annotation, and consequently has
already been addressed in the former processes (Kountz et al., 2008; Bunt, 2007; Spranger and Kountz,
2007; Regneri et al., 2008) and incorporated into tools such as SALTO (Burchardt et al., 2006). With
respect to corpora of non-standard languages, ambiguous forms must be taken into consideration in mor-
phosyntactic tagging as well. This has been confirmed by current corpus projects of historical varieties
of German ? for example, the ?MERCURIUS Corpus of Early New High German? (ENHG1) (Pauly et
al., 2012) and the ?Historical Tagset? (HiTS) (Dipper et al., 2013), which provide different options for
dealing with ambiguities at the level of part of speech. Below we will discuss examples of ambiguities
at the morphological level.
Within the extensive field of non-standard language annotations, we have concentrated on historical
linguistics, showcasing the kinds of ambiguities that historical corpus linguists must confront and how
they can be managed. Historical corpus linguistics based on annotation necessarily faces the challenge of
avoiding circular argumentation. The description of a historic language must be based on the annotated
texts of the corpus, since they are the only sources of linguistic material in historical grammatography.
However, no annotation of the material can be accomplished without a basic knowledge of the language
and its structure. Thus, an annotator confronted with a dubious case cannot know whether it is actu-
ally a case of ambiguity in the language system or whether the grammatical categories adopted for the
annotation do not fit the grammatical system of the non-standard language. Transferring the annotation
standards developed for a standardized language such as written New High German (NHG) to a historical
corpus might at first seem tempting, but this process would conceal the actual grammatical characteristics
of the language to be described.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1All language abbreviations in this article correspond to ISO 639.
99
Masc Neut Fem
Sg Nom h?, h? it, et s?, s?, s??
Gen is, es, s?n, s?ner is, es ere, er erer, ?rerDat en, eme, ?me en, em, eme, ?m, ?me
Acc en, ene, ?n, ?ne it, et s?, s?, s??
Pl Nom s?, s?
Gen ere, er, erer, ?rer
Dat en, em, ?m, j?m
Acc s?, s?
Table 1
GML pronouns - 3rd person; freely based on Lasch (1974)
Type of True Annotator Token
phenomenon analysis
Uncertainty Dat Dat?Acc? en
Underspecification Obj Dat?Acc? en
Ambiguity {Dat,Acc} Dat?Acc? en
Table 2
Types of descriptively incomplete language phenomena
2 Cases of descriptively incomplete phenomena
The project ?Reference Corpus Middle Low German/ Low Rhenish (1200?1650)?2 transliterates and
grammatically annotates the Middle Low German (GML) texts from which we take our examples. Be-
cause GML is a non-standardized language that is not well described, ambiguous forms occur frequently,
and accurately interpreting them is a matter of high priority for any annotation. First, with regard to nouns
and pronouns, GML?s case syncretism3 should be mentioned. For personal pronouns, in particular the
syncretism of the dative and accusative forms in the first- and second-person singular and plural leads to
problems in annotation. However, in this section, we concentrate on the third person.
Table 1 illustrates the many identical forms of third person personal pronouns that are used for sev-
eral morphological feature values. Moreover, it reveals the distribution of case syncretism across the
three different genders of the third-person singular.4 While the neuter paradigm shows syncretism in
the nominative and accusative forms, for the feminine pronouns there are ambiguous forms not only for
nominative and accusative but also for genitive and dative. The masculine paradigm includes a partial
syncretism of dative and accusative for the pronoun en (?him?).
In addition, there is syncretism in the dative forms of the third-person singular masculine and neuter
and in the third-person plural. Hence, in example (1),5 the word en could be either masculine or neuter if
there is no context providing reliable information on the gender of the referent, or it could even be plural
(where there is syncretism between the three genders). If en is plural or neuter, it can only be a dative
form, but if it is masculine, it could be either dative or accusative.
(1) vppe
upon
dat
that
god-es
god-M.GEN.PL
sone
son-M.NOM.SG
ge-ere-t
PTCP-honour-PTCP
werd-e
will-3SG.PRS.SBJV
dor
through
en
EN
?so that god?s son would be honoured through EN?
(BuxtehEv, Joh 11,4)
Even where the context provides additional information, often not all ambiguities can be resolved. In
example (1), the antecedent of en provides information on gender (masculine) and number (singular), but
the ambiguity with respect to case can only be resolved in a local context ? here, the prepositional phrase.
The problem is that in GML the preposition dor (?through?) can govern different cases. Consequently,
the case ambiguity in (1) cannot be resolved.
There are many other examples of ambiguous forms, for instance, the gender of nouns or the inflection
paradigm of verbs. For all these cases of ambiguity the annotation should provide as much grammatical
information on a given form as possible.
2The ?Referenzkorpus Mittelniederdeutsch/ Niederrheinisch (1200?1650)? (?Reference Corpus Middle Low German/ Low
Rhenish?, or ?ReN?), supported by the German Research Foundation (DFG)) and in development since February/ March
2013 at the universities of Hamburg and M?nster, is part of the ?Corpus of Historical German Texts?, together with the
corpora ?Altdeutsch? (Old German), ?Mittelhochdeutsch? (Middle High German), and ?Fr?hneuhochdeutsch? (Early New
High German). More information on the structure of ReN can be found in Nagel and Peters (In print) and on the website
www.referenzkorpus-mnd-nrh.de. For information on the annotation used in ReN and possible grammatical analyses, see
Schr?der (In print).
3Baerman (2006) asserts that ?syncretism refers to the situation when a single inflectional form corresponds to multiple
morphosyntactic feature values? (363). With respect to the feature case, this means that identical forms are used for different
cases, e.g., for dative and accusative.
4The order of the pronouns was chosen for presentational reasons. The example en that we refer to in this paper is shown in
bold italics.
5This glossing is based on the Leipzig Glossing Rules (http://www.eva.mpg.de/lingua/pdf/LGR08.02.05.pdf).
100
3 Types of descriptively incomplete language phenomena
In cases of descriptively incomplete language phenomena such as those described above, the annotator
(which could be a tool or a human) is unable to unambiguously assign an analysis to the language data.
This inability can have various causes. Consequently, EAGLES (1996) distinguishes between two types
of ?descriptively incomplete phenomena?: underspecification and ambiguity. In the first case, the inabil-
ity arises because ?the distinction between the different values of an attribute is not relevant?. The second
case is characterized as ?the phenomenon of lack of information, where there is uncertainty between two
or more alternative descriptions?. For both of these types, EAGLES provides subtypes; however, in the
case of ambiguity, these subtypes also differ with respect to the reason for the uncertainty. In one subtype,
the apparent ambiguity could be resolved given more information. In the other, the uncertainty results
from a real ambiguity in the language or the given text and therefore cannot be resolved. Consequently, we
propose a differentiation between three types of descriptively incomplete language phenomena that can
occur during annotation: (i) uncertainty, i.e., incomplete information due to infrequent occurrence in the
training material (automatic annotation), incomplete treatment in annotation guidelines, or an incomplete
understanding of the language system (manual annotation); (ii) underspecification, i.e., incomplete in-
formation due to an undistinguished feature of the language system; and (iii) ambiguity, i.e., incomplete
information due to an ambiguity in the language data.
Returning to example (1), further analyses could provide evidence that the preposition dor (?through?)
unambiguously takes the accusative case, such that this would represent a case of uncertainty. In English
personal pronouns, there is no distinction made between dative and accusative, both of which are repre-
sented by the objective case (Obj) (Quirk et al., 1985). If this were also true for GML, the example would
be a case of underspecification. However, it could also represent a true case of ambiguity. As long as
this categorization is unclear, the types cannot be distinguished.
Table 2 summarizes the distinction between these three types. Although all of them result in the same
situation for the annotator (machine or human), they differ with respect to the true analysis, which is
unknown to the annotator; it is therefore impossible for him or her to definitively assign a tag to the
token, as exemplified in Table 2. In situations of uncertainty or underspecification, an unambiguous,
true analysis exists. In the case of uncertainty, it is a matter of redefining the annotation guidelines to
help the annotating system to find this true analysis. In the case of underspecification, the tagset is too
fine-grained to provide the true analysis. Only by adjusting the tagset would the annotator be able to
determine the true analysis. Adjustments to the annotation guidelines and the tagset during the process of
annotation can be accomplished through the use of an annotation development cycle such as theMATTER
methodology (Pustejovsky and Stubbs, 2012, 23?32). In the case of ambiguity, however, both analyses
are true. They should be retrievable for further interpretation and thus should both be assigned to the
token.
Optimally, the different types of incomplete information ?should be distinguishable by different mark-
up? (EAGLES, 1996). But as we have argued, when annotating historical languages (or less-studied
languages in general), it is not always possible to decide at the time of annotation whether there is an
ambiguity, an underspecification, or an uncertainty, as all three result in the same problem for the an-
notator. Thus, in many cases, the annotator can only distinguish between the three types (if at all) after
the annotation has been completed and the quantitative results based on the annotated data have become
available. The three types must therefore be dealt with similarly during the annotation process, and the
possible interpretations should be retrievable from the annotations. Consequently, the annotator should
have the possibility to assign any number of annotations to every possible feature. This would require
special tools to create and retrieve these annotations, but existing standards to encode annotations are
already flexible enough to allow annotations. Some examples are shown in the next section.
4 Encoding multiple annotations in markup standards
This section presents three formats for encoding multiple annotations of descriptively incomplete struc-
tures in XML markup. We return to the ambiguous GML pronoun en ?him/ it? introduced in example (1)
in Section 2.
101
Our first option is T?PP-D/Z DTD (Ule, 2004), an inline-XML specification that was designed to
represent a ranked list of multiple competing tagger outputs resulting from ensemble tagging. Using the
same kind of structure, all possible interpretations of the pronoun en could be encoded and made available
for further analysis and disambiguation.
The other two options are generic XML-standoff formats that represent annotations as directed acyclic
graphs: PAULA (Dipper, 2005; Chiarcos et al., 2008), derived from early drafts of the Linguistic Anno-
tation Framework (LAF) (Ide and Romary, 2004), and GrAF (Ide and Suderman, 2007), a more recent
specification of the LAF. Each level of annotation is represented separately, such that features are related
to annotation objects (?markables?) only by links. Markables themselves are defined on the basis of text
tokens or other markables. Multiple markables can be related to the same token, as each markable is
uniquely identified by its ID. These options also allow us to encode all interpretations of en.6
In certain cases, there are dependencies between multiple ambiguous features. Concerning ?en?, if the
gender is Neut, the case is not ambiguous, but if the gender is Masc, the case could be either Dat or Acc
(cf. Table 1). The above strategies do not allow us to encode these dependencies. However, the generic
LAF-derived standoff formats can be employed to do this because they also allow us to define labels for
edges, such that they can be annotated and typed. Kountz et al. (2008) propose an extension to GrAF in
which such dependencies are explicitly modeled. As depicted in Figure 1, we make use of this property to
combine a choice structure with a collect structure. In this way, each token correlates with one MorphSet
object that can be instantiated by a set of MorphInst objects, thereby explicitly encoding the dependencies
between the multiple ambiguous features of gender and case.
Token
 ...      ...     
Token
 ?en?
  Case   has_feature
  Gender 
has_feature
Token
 ...      ...     
Acc
choice
Datchoice
Mascchoice
Neut
choice
MorphInst
 mi1
has_featureInst
MorphInst
 mi2
has_featureInst
MorphInst
 mi3
has_featureInst
has_featureInst
has_featureInst
has_featureInst
MorphSet
 ms1
collect
collect
collect
Figure 1: Representation of an encoding of the ambiguous GML pronoun en ?him/it? with typed edges
5 Conclusion and Outlook
In order to avoid circular argumentation and to reveal the actual grammatical characteristics of the lan-
guage under investigation, historical corpus linguistics must go beyond simply adapting the rules of a
standardized language, both by disambiguating ambiguous forms but also by encoding ambiguities. By
means of data taken from the ?ReN? corpus, we have demonstrated that in historical language corpora, an-
notators must deal with descriptively incomplete language phenomena. Furthermore, they need to decide
what type of phenomena these are, i.e., real ambiguities, underspecifications or uncertainties. Often this
decision is impossible at the time of the annotation, since all three types result in the same problem for the
annotator, as discussed in Section 3. In Section 4, we have shown that in markup formats such PAULA or
GrAF, the straightforward encoding of multiple annotations and their dependencies is possible. Neverthe-
less, linguists still lack sufficient tools to create, query, and visualize the multiple annotations represented
in the underlying data structure. For these reasons, corpus projects such as ?ReN? are currently unable to
use multiple annotations, even though this is the most appropriate encoding strategy for the grammatical
annotation of historical languages.
6In addition, PAULA offers a multiFeat structure (Zeldes et al., 2013, 14f.) for linking sets of fully-specified features to one
markable. However, each piece of information must be unambiguous.
102
Acknowledgements
Wewould like to thank Kerstin Eckart for very helpful discussion and suggestions, and also Claire Bacher
for improving our English. All remaining errors are ours. Figure 1 was created with GraphvizFiddle
(https://stamm-wilbrandt.de/GraphvizFiddle/) an online editor for Graphviz (http://www.
graphviz.org/). Part of this work was funded by the German Research Foundation (DFG).
Sources of Attested Examples
BuxtehEv Qvator Evangeliorum versio Saxonica. A GML handwritten gospel from the fifteenth century.
Transliterated by the DFG-funded project ?ReN?. For further information, see Pettke and Schr?der (1992).
References
Matthew Baerman. 2006. Syncretism. In Keith Brown, editor, Encyclopedia of Language and Linguistics., vol-
ume 12, pages 363?366. Elsevier, Amsterdam [a.o.], 2nd edition.
Harry Bunt. 2007. Semantic underspecification: Which technique for what purpose? In Harry Bunt and Rein-
hard Muskens, editors, Computing Meaning, volume 83 of Studies in Linguistics and Philosophy, pages 55?85.
Springer.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea Kowalski, and Sebastian Pado. 2006. SALTO: A versatile
multi-level annotation tool. In Proceedings of LREC 2006, pages 517?520.
Christian Chiarcos, Stefanie Dipper, Michael G?tze, Ulf Leser, Anke L?deling, Julia Ritz, and Manfred Stede.
2008. A flexible framework for integrating annotations from different tools and tagsets. Traitement Automatique
des Langues, 49(2):271?293.
Stefanie Dipper, Karin Donhauser, Thomas Klein, Sonja Linde, Stefan M?ller, and Klaus-Peter Wegera. 2013.
HiTS: ein Tagset f?r historische Sprachstufen des Deutschen. [HiTS: A tagset for historical varieties of German].
JLCL, 28(1):1?53.
Stefanie Dipper. 2005. XML-based stand-off representation and exploitation of multi-level linguistic annotation
schema. In Proceedings of Berliner XML Tage 2005 (BXML 2005), pages 39?50, Berlin.
EAGLES. 1996. Recommendations for the morphosyntactic annotation of corpora. EAGLES document EAG-
TCWG-MAC/R. Technical report.
Nancy Ide and Laurent Romary. 2004. International standard for a linguistic annotation framework. Journal of
Natural Language Engineering, 10(3-4):211?225.
Nancy Ide and Keith Suderman. 2007. GrAF: A graph-based format for linguistic annotation. In Proceedings of
the Linguistic Annotation Workshop (LAW), pages 1?8, Prague, Czech Republic. Association for Computational
Linguistics.
Manuel Kountz, Ulrich Heid, and Kerstin Eckart. 2008. A LAF/GrAF-based encoding scheme for underspecified
representations of dependency structures. In Proceedings of LREC-2008, Linguistic Resources and Evaluation
Conference, Marrakesh.
Agathe Lasch. 1974. Mittelniederdeutsche Grammatik. [Middle Low German Grammar]. Sammlung kurzer
Grammatiken germanischer Dialekte. A. Hauptreihe, 9. Niemeyer, T?bingen, 2nd edition.
Norbert Nagel and Robert Peters. In print. Das digitale ?Referenzkorpus Mittelniederdeutsch/ Niederrheinisch
(ReN)?. [The digital Reference Corpus ofMiddle LowGerman/ LowRhenish (ReN)]. In Jahrbuch f?r germanis-
tische Sprachgeschichte 5. De Gruyter.
Dennis Pauly, Ulyana Senyuk, and Ulrike Demske. 2012. Strukturelle Mehrdeutigkeit in fr?hneuhochdeutschen
Texten. [Structural ambiguities in Early New High German texts]. JLCL, 27(2):65?82.
Sabine Pettke and Ingrid Schr?der. 1992. Eine Buxtehuder Evangelienhandschrift. Die vier Evangelien in einer
mittelniederdeutschen ?bersetzung des 15. Jahrhunderts aus dem Alten Kloster. [A Buxtehude handwritten
gospel. A GML translation of the four gospels from the fifteenth century]. In Bernd Uterm?hlen, editor,
Qvatuor Evangeliorum versio Saxonica. Eine mittelniederdeutsche Evangelienhandschrift aus dem 15. Jahrhun-
dert. Textedition, Buxtehuder Notizen Nr. 5, pages 99?266. Stadt Buxtehude, Buxtehude.
103
James Pustejovsky and Amber Stubbs. 2012. Natural language annotation for machine learning. O?Reilly,
Beijing [a.o.].
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech, and Jan Svartvik. 1985. A Comprehensive Grammar of the
English Language. Longman, London.
Michaela Regneri, Markus Egg, and Alexander Koller. 2008. Efficient processing of underspecified discourse
representations. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics
on Human Language Technologies: Short Papers, pages 245?248. Association for Computational Linguistics.
Ingrid Schr?der. In print. Das Referenzkorpus: Neue Perspektiven f?r die mittelniederdeutsche Grammatikogra-
phie. [The reference corpus: New perspectives for GML grammatography]. In Jahrbuch f?r germanistische
Sprachgeschichte 5. De Gruyter.
Kristina Spranger and Manuel Kountz. 2007. Efficient ambiguity-handling using underspecified representations.
In Georg Rehm, Andreas Witt, and Lothar Lemnitzer, editors, Data Structures for Linguistic Resources and
Applications. Gunter Narr Verlag, T?bingen.
Tylman Ule. 2004. Markup manual for the T?bingen Partially Parsed Corpus of Written German (T?PP-D/Z).
Technical report, University of T?bingen.
Amir Zeldes, Florian Zipser, and Arne Neumann. 2013. PAULA XML Documentation: Format version 1.1.
Technical Report Version: P1.1.2013.1.21a, Humboldt-Universit?t zu Berlin, Berlin.
104
