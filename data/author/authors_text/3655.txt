Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 89?96, Vancouver, October 2005. c?2005 Association for Computational Linguistics
A Maximum Entropy Word Aligner for Arabic-English Machine
Translation
Abraham Ittycheriah and Salim Roukos
IBM T.J. Watson Research Center
1101 Kitchawan Road
Yorktown Heights, NY 10598
{abei,roukos}@us.ibm.com
Abstract
This paper presents a maximum entropy
word alignment algorithm for Arabic-
English based on supervised training data.
We demonstrate that it is feasible to cre-
ate training material for problems in ma-
chine translation and that a mixture of su-
pervised and unsupervised methods yields
superior performance. The probabilistic
model used in the alignment directly mod-
els the link decisions. Significant improve-
ment over traditional word alignment tech-
niques is shown as well as improvement on
several machine translation tests. Perfor-
mance of the algorithm is contrasted with
human annotation performance.
1 Introduction
Machine translation takes a source sequence,
S = [s1 s2 . . . sK ]
and generates a target sequence,
T = [t1 t2 . . . tM ]
that renders the meaning of the source sequence into
the target sequence. Typically, algorithms operate
on sentences. In the most general setup, one or more
source words can generate 0, 1 or more target words.
Current state of the art machine translation systems
(Och, 2003) use phrasal (n-gram) features extracted
automatically from parallel corpora. These phrases
are extracted using word alignment algorithms that
are trained on parallel corpora. Phrases, or phrasal
features, represent a mapping of source sequences
into a target sequences which are typically a few
words long.
In this paper, we investigate the feasibility of train-
ing alignment algorithms based on supervised align-
ment data. Although there is a modest cost associ-
ated with annotating data, we show that a reduction
of 40% relative in alignment error (AER) is possible
over the GIZA++ aligner (Och and Ney, 2003).
Although there are a number of other applications
for word alignment, for example in creating bilingual
dictionaries, the primary application continues to be
as a component in a machine translation system. We
test our aligner on several machine translation tests
and show encouraging improvements.
2 Related Work
Most of the prior work on word alignments has been
done on parallel corpora where the alignment at the
sentence level is also done automatically. The IBM
models 1-5 (Brown et al, 1993) produce word align-
ments with increasing algorithmic complexity and
performance. These IBM models and more recent
refinements (Moore, 2004) as well as algorithms that
bootstrap from these models like the HMM algo-
rithm described in (Vogel et al, 1996) are unsuper-
vised algorithms.
The relative success of these automatic techniques
together with the human annotation cost has delayed
the collection of supervised word-aligned corpora for
more than a decade.
(Cherry and Lin, 2003) recently proposed a di-
rect alignment formulation and state that it would
be straightforward to estimate the parameters given
a supervised alignment corpus. In this paper, we ex-
tend their work and show that with a small amount
of annotated data, together with a modeling strat-
egy and search algorithm yield significant gains in
alignment F-measure.
89
show
vAny +pAl#
AlvAnyp
secondWords
WordNet
the
2nd
2d
pointed
+pwvyqAl#+tA$Arw#
wA$Art AlwvyqpWords
Segm.
to
Aly
Aly
Source
Target
papers
document
indicate
point
Figure 1: Alignment example.
3 Algorithm
In order to describe the algorithm, we will need to
first describe the direct link model. Figure 1 shows
two sequences where the top sequence is considered
the source sequence and the bottom sequence the
target sequence. Each sequence can have auxilliary
information such as Arabic segmentation or English
WordNet (Miller, 1990) information as shown. Given
the source and target sequences, there are a number
of different ways to link each target word to a source
word. Each target word has a link li which indi-
cates which source position it links to. The range
of li is from 0 to K and there are M of these links.
The source word position 0 is used to indicate NULL
which we imagine gives rise to unaligned English
words. In this paper, we refer to these words as be-
ing spontaneous. A valid link configuration has M
links. Define L to be the set of all possible valid link
configurations, and L to be a member of that set.
We seek to maximize the alignment probability by
finding the optimum link configuration Lopt,
p(Lopt|S, T ) = argmax
L?L
p(L|S, T )
= p(lMi |tM1 , sK1 )
=
M
?
i=0
p(li|tM1 , sK1 , li?11 ).
We factor this into a transition model and an obser-
vation model,
p(L|S, T ) = 1Z
M
?
i=0
p(li|li?1)?p(li|tM1 , sK1 , li?11 )1??.
where Z is the normalizing constant.
We factor the model as above so that the tran-
sition model computation, which uses information
available on the search hypotheses, is reduced during
the search process. In the aligner presented here, ?
is always set to 0.5. Next we will describe the tran-
sition model, then the observation model and finally
the experiments in alignment and machine transla-
tion.
In the IBM Model 1 aligner, the choice of the lan-
guage to serve as states of the search algorithm is not
prescribed, but practically the choice is important as
it affects performance. To see this, note that in gen-
erative models an input word can only be aligned to
a single state in the search. In our current situa-
tion, we are interested in aligning unsegmented Ara-
bic words and typical words have a few affixes to
indicate for example pronouns, definiteness, prepo-
sitions and conjunctions. In English these are sepa-
rate words, and therefore to maximize performance
the unsegmented Arabic words serve as states in the
search algorithm and we align English words to these
states.
3.1 Transition Model
The transition model tends to keep the alignments
close together and penalizes alignments in which ad-
jacent words in the target language come from very
distant words in the source language. Also, we would
like to penalize many English words coming from the
same Arabic state; we call this the state visit penalty
and will be described later. In this paper, we use a
parametric form for the transition model,
p(li|li?1) =
1
Z(li?1)
[ 1
dist(li, li?1)
+ 1ns(li)
]
(1)
90
where ns(i) represents the state visit penalty for
state i, Z(li?1) is the normalization constant and
dist(li, li?1) = min(|li ? li?1|, |li ? fi|) + a.
Here a is a penalty for a zero distance transition and
is set to 1 in the experiments below. The min op-
erator chooses the lowest cost transition distance ei-
ther from the previous state or the frontier state, fi,
which is the right most state that has been visited
(even though Arabic is normally displayed right to
left, we make our Arabic state graphs from left to
right). This is a language specific criteria and in-
tended to model the adjective noun reversal between
English and Arabic. Once the current noun phrase
is completed, the next word often aligns to the state
just beyond frontier state. As an example, in Fig-
ure 1, the verb ?pointed? aligns to the first Arabic
word ?wA$Art?, and aligning the ?to? to its Arabic
counterpart ?Aly? would incur normally a distance of
3 but with the frontier notion it incurs only a penalty
of 1 on the hypothesis that aligns the word ?second?
to ?AlvAnyp?. In this alignment with the frontier no-
tion, there are only distance 1 transitions, whereas
the traditional shapes would incur a penalty of 2 for
alignment of ?pointed? and a penalty of 3 for the word
?to?.
The state visit penalty, ns(i) is the distance be-
tween the English words aligned to this state times
the number of state visits1. This penalty controls
the fertility of the Arabic words. To determine the
English words that aligned to the Arabic position,
the search path is traced back for each hypothe-
sis and a sufficiently large beam is maintained so
that alignments in the future can correct past align-
ment decisions. This penalty allows English deter-
miners and prepositions to align to the Arabic con-
tent word while penalizing distant words from align-
ing to the state. In terms of alignment F-measure
to be described below, the state visit penalty, if re-
moved makes the performance degrade from F=87.8
to F=84.0 compared to removing the frontier notion
which only degrades performance to F=86.9.
3.2 Observation Model
The observation model measures the linkage of the
source and target using a set of feature functions
defined on the words and their context. In Figure 1,
an event is a single link from an English word to
an Arabic state and the event space is the sentence
pair. We use the maximum entropy formulation (e.g.
(Berger et al, 1996)),
1We are overloading the word ?state? to mean Arabic
word position.
f = ?(li)
h =
[
ti?11 , sK1
]
p(f |h) = 1Z(h) exp
?
i
?i?i(h, f),
where Z(h) is the normalizing constant,
Z(h) =
?
f
exp
?
i
?i?i(h, f).
and ?i(h, f) are binary valued feature functions. The
function ? selects the Arabic word at the position
being linked or in the case of segmentation features,
one of the segmentations of that position. We re-
strict the history context to select from the current
English word and words to the left as well as the
current word?s WordNet (Miller, 1990) synset as re-
quired by the features defined below. As in (Cherry
and Lin, 2003), the above functions simplify the con-
ditioning portion, h by utilizing only the words and
context involved in the link li. Training is done us-
ing the IIS technique (Della Pietra et al, 1995) and
convergence often occurs in 3-10 iterations. The five
types of features which are utilized in the system are
described below.
Phrase to phrase (for example, idiomatic phrases)
alignments are intepreted as each English word com-
ing from each of the Arabic words.
3.2.1 Lexical Features
The lexical features are similar to the translation
matrix of the IBM Model 1. However, there is a sign-
ficant out of vocabulary (OOV) issue in the model
since training data is limited. All words that have
a corpus frequency of 1 are left out of the model
and classed into an unknown word class in order to
explicitly model connecting unknown words. From
the training data we obtain 50K lexical features, and
applying the Arabic segmenter obtain another 17K
lexical features of the form ?(English content word,
Arabic stem).
3.2.2 Arabic Segmentation Features
An Arabic segmenter similar to (Lee et al, 2003)
provides the segmentation features. A small dictio-
nary is used (with 71 rules) to restrict the set of Ara-
bic segments that can align to English stopwords, for
example that ?the? aligns to ?Al#? and that ?for?, ?in?
and ?to? align to ?b#? and ?her? aligns with the suf-
fix ?+hA?. Segmentation features also help align un-
known words, as stems might be seen in the training
corpus with other prefixes or suffixes. Additionally,
the ability to align the prefix and suffix accurately,
tends to ?drag? the unknown stem to its English tar-
get.
91
3.2.3 WordNet Features
WordNet features provide normalization on the
English words. The feature is instantiated for nouns,
adjectives, adverbs and verbs following their defini-
tions in WordNet. If the Arabic word has a seg-
mentation then the feature is ?(WordNet synset id,
Arabic stem), otherwise it is ?(WordNet synset id,
Arabic word). The feature ties together English syn-
onyms and helps improve recall of the aligner.
3.2.4 Spelling Feature
The spelling feature is applied only on unknown
words and is used to measure the string kernel dis-
tance(Lodhi et al, 2000) between romanized Arabic
and English words. The feature is designed primar-
ily to link unknown names. For example, ?Clinton?
is written as ?klyntwn? in one of its romanized Ara-
bic versions. In a sentence, measuring the string ker-
nel distance shows a correlation between these names
even though there is not much overlap between the
characters. The feature has four possible values: no-
match, somematch, goodmatch, and exact.
3.2.5 Dynamic Features
Dynamic features are defined on the lattice of the
search algorithm. These features fire when the pre-
vious source and target word pair are linked. For
example, one such feature is ?b# in? and if on the
hypothesis we have just linked this pair and the next
English word is being aligned to the stem of the Ara-
bic word where this prefix occurs, this feature fires
and boosts the probability that the next words are
aligned. The basic intuition behind this feature is
that words inside prepositional phrases tend to align,
which is similar to the dependency structure feature
of (Cherry and Lin, 2003).
At training time, the lattice reduces to the sin-
gle path provided by the annotation. Since this fea-
ture tends to suffer from the drag of function words,
we insist that the next words that are being linked
have at least one feature that applies. All word pairs
linked in the training data have lexical features as de-
scribed above, and if both source and target words
are unknown they have a single feature for their link.
Applying dynamic features on words that have at
least one other feature prevents words which are com-
pletely unrelated from being linked because of a fea-
ture about the context of the words.
Two types of dynamic features are distinguished:
(a) English word with Arabic prefix/suffix and (b)
English word with Arabic stem.
4 Smoothing the Observation Model
Since the annotated training data for word alignment
is limited and a much larger parallel corpus is avail-
able for other aligners, we smooth the observation
Anno. 1 Anno. 1? Anno. 2
Correction
Anno. 1 96.5 92.4 91.7
Anno. 1? 95.2 ? 93.2
Table 1: F-measure for human performance on word
alignment for Arabic-English.
probability with an IBM Model 1 estimate,
p(li|tM1 , sK1 ) =
1
Z pME(li|t
M
1 , sK1 )?pM1(s|ti)1?? .
where ? is set to 0.9 in the experiments below. In
the equation above, the s represents the Arabic word
that is being linked from the English word ti.
When ? is set to 1.0 there is no smoothing per-
formed and performance degrades to F=84.0 from
the best system performance (F=87.8). When ? is
set to 0, the model uses only the IBM Model 1 distri-
bution and the resulting aligner is similar to an HMM
aligner with the transition shape discussed above and
yields performance of F=73.2.
5 Search Algorithm
A beam search algorithm is utilized with the English
words consumed in sequence and the Arabic word
positions serving as states in the search process. In
order to take advantage of the transition model de-
scribed above, a large beam must be maintained. To
see this, note that English words often repeat in a
sentence and the models will tend to link the word
to all Arabic positions which have the same Ara-
bic content. In traditional algorithms, the Markov
assumption is made and hypothesis are merged if
they have the same history in the previous time step.
However, here we maintain all hypotheses and merge
only if the paths are same for 30 words which is the
average sentence length.
6 Experimental Data
We have word aligned a portion of the Arabic Tree-
bank (4300 sentences) and material from the LDC
news sources (LDC, 2005) to obtain a total of 10.3K
sentence pairs for training. As a test of alignment,
we use the first 50 sentences of the MT03 Evaluation
test set which has 1313 Arabic words and 1528 En-
glish words 2. In terms of annotation guidelines, we
use the following instructions: (a) Align determiners
to their head nouns, (b) Alignments are done word
by word unless the phrase is idiomatic in which case
the entire phrase to phrase alignment was marked,
(c) spontaneous words are marked as being part of a
2The test data is available by contacting the authors.
92
1K 3K 5K 7K 9K 10.3K
# of features 15510 32111 47962 63140 73650 80321
English % OOV 15.9 8.2 5.5 4.4 4.05 3.6
Arabic % OOV 31 19.6 15.6 13.2 10.8 10.3
F-measure 83.2 85.4 86.5 87.4 87.5 87.8
Table 2: Varying Training data size.
phrase wherever possible but left unaligned if there
is no evidence to link the word.
In order to measure alignment performance, we
use the standard AER measure (Och and Ney, 2000)
but consider all links as sure. This measure is then
related to the F-measure which can be defined in
terms of precision and recall as
Precision The number of correct word links over
the total number of proposed links.
Recall The number of correct word links over the
total number of links in the reference.
and the usual definition of the F-measure,
F = 2PR(R+ P )
and define the alignment error as AER = 1 ? F .
In this paper, we report our results in terms of F-
measure over aligned links. Note that links to the
NULL state (unaligned English words) are not in-
cluded in the F-measure. Systems are compared rel-
ative to the reduction in AER.
6.1 Annotator Agreement
We measure intra/inter-annotator agreement on the
test set in order to determine the feasibility of hu-
man annotation of word links. These are shown in
Table 1. In the table, the column for ?Annotator 1
Correction? is the first annotator correcting his own
word alignments after a span of a year. After two
weeks, the annotator (Annotator 1?) was given the
same material with all the links removed and asked
to realign and we see that there is more discrepancy
in resulting alignments. The differences are largely
on the head concept where determiners are attached
and the alignment of spontaneous words. The perfor-
mance with a second annotator is in the same range
as the reannotation by a single annotator.
7 Experiments
In order to evaluate the performance of the algo-
rithm, we investigate the effect due to: (a) increasing
the training data size, (b) additional feature types,
and (c) comparable algorithms.
7.1 Training Data Size
We varied the training data size from 1K sentences to
the complete set in Table 2. Each batch re-estimates
the unknown word class by creating a vocabulary
on the training set. The trend indicates a reasonable
progression of performance and more data is required
to determine the saturation point.
7.2 Feature Types
The results obtained by different feature sets are
shown in Table 3. Each feature type was added incre-
mentally (Add Feature column) to the line above to
determine the effect of the individual feature types
and then removed incrementally from the full sys-
tem (Subtract Feature column) in order to see the
final effect. The results indicate that lexical features
are the most important type of feature; segmenta-
tion features further reduce the AER by 15.8%. The
other features add small gains in performance which,
although are not statistically significant for the align-
ment F-measure, are important in terms of feature
extraction. Segmentation features discussed above
result in both suffix and prefix features as well as
stem features. In the Subtract column, for the seg-
mentation feature, only the suffix and prefix features
were removed. This result indicates that most of the
alignment improvement from the segmentation fea-
ture comes in the form of new lexical features to link
Arabic stems and English words.
7.3 Comparison to other alignment
algorithms
In order to gauge the performance of the algorithm
with respect to other alignment strategies, we pro-
vide results using GIZA++ and an HMM Max Poste-
rior Algorithm (Ge, 2004). These algorithms, as well
as the Model 1 smoothing for the MaxEnt aligner,
are all trained on a corpus of 500K sentence pairs
from the UN parallel corpus and the LDC news cor-
pora released for 2005 (LDC, 2005). Note that these
algorithms are unsupervised by design but we utilize
them to have a baseline for comparing the perfor-
mance of this supervised approach.
7.3.1 HMM Max Posterior Aligner
The maximum-posterior word alignments are ob-
tained by finding the link configuration that maxi-
93
System # of Add Subtract
feats Feature Feature
Word pairs 50070 85.03 76.3
Spelling 4 85.11 87.7
Segmentation 70 87.39 87.5(*)
WordNet 13789 87.54 87.5
Dynamic-Words 1952 87.80 87.1
Dynamic-Segmentation 42 87.84 87.8
Table 3: Alignment performance in terms of the feature types utilized.
F-Measure
GIZA++ 79.5
HMM 76.3
MaxEnt 87.8
Table 4: Alignment performance
mizes the posterior state probability. In contrast, in
performing a Viterbi alignment, we compute the best
state sequence given the observation. The maximum
posterior computes the best state one at a time and
iterates over all possible combinations. Once we find
the maximum in the posterior probability matrix,
we also know the corresponding state and observa-
tion which is nothing but the word pair (sj , ti). We
will then align the pair and continue to find the next
posterior maximum and align the resulting pair. At
each iteration of the process, a word pair is aligned.
The process is repeated until either every word in one
(or both) language is aligned or no more maximum
can be found, whichever happens first.
7.3.2 GIZA Alignment
In order to contrast our algorithm, we ran
GIZA++ in the standard configuration which im-
plies 5 iterations of IBM Model 1, HMM, Model 3
and Model 4. All parameters are left to their default
values.
The results using the three different aligners is
shown in Table 4. The reduction in AER over the
GIZA++ system is 40.5% and over the HMM sys-
tem is 48.5%. The Wilcoxon signed-rank test yields
a probability of 0.39 for rejecting the GIZA++ align-
ment over the HMM alignment, whereas the MaxEnt
algorithm should be rejected with a probability of
1.7e-6 over the HMM algorithm and similarly Max-
Ent should be rejected with a probability of 0.9e-
6 over the GIZA++ algorithm. These significance
tests indicate that the MaxEnt algorithm presented
above is significantly better than either GIZA++ or
HMM.
Figure 2: An alignment showing a split link from an
Arabic word.
8 Phrase Extraction
Once an alignment is obtained, phrases which sat-
isfy the inverse projection constraint are extracted
(although earlier this constraint was called consis-
tent alignments (Och et al, 1999)). This constraint
enforces that a sequence of source words align to a
sequence of target words as defined by the lowest and
highest target index, and when the target words are
projected back to the source language through the
alignment, the original source sequence is retrieved.
Examination of the hand alignment training data
showed that this criteria is often violated for Ara-
bic and English. Prepositional phrases with adjec-
tives often require a split? for example, the align-
ment shown in Figure 2 has ?of its relations? aligned
to a word in Arabic and ?tense? aligned to the next
word. The inverse projection constraint fails in this
case, and in the experiments below, we relax this con-
straint and generate features for single source words
as long as the target phrase has a gap less than 2
English words. This relaxation allows a pair of ad-
jectives to modify the head noun. In future work we
explore the use of features with variables to be filled
at decode time.
9 Translation Experiments
The experiments in machine translation are carried
out on a phrase based decoder similar to the one de-
94
MT03 MT04 MT05
GIZA++ 0.454 ? ?
HMM 0.459 0.419 0.456
MaxEnt 0.468 0.433 0.451
Combined 0.479 0.437 0.465
Significance 0.017 0.020 ?
Table 5: Machine Translation Performance using the
NIST 2005 Bleu scorer
scribed in (Tillmann and Ney, 2003). In order to con-
trast the performance of the extracted features, we
compare the translation performance to (a) a system
built from alignments proposed by an HMM Max
Posterior Aligner, and (b) a system built from GIZA
alignments. All other parameters of the decoder re-
main constant and only the feature set is changed for
these experiments. As training data, we use the UN
parallel corpus and the LDC news corpora released
in 2005. Comparison should therefore be only made
across systems reported here and not to earlier eval-
uations or other systems. The results are shown in
Table 5.
Combination of the phrasal features from the
HMM and MaxEnt alignments results in the ?Com-
bined? system. The Combined system performs bet-
ter in all cases; in MT03 and MT04 the MaxEnt
derived features perform better than the HMM sys-
tem. In MT05, there is a slight degradation which is
not significant and the combination system still re-
sults in an improvement over either system. Since
the MaxEnt aligner has access to a unique resource,
every attempt was made to make that resource avail-
able to the other systems. Although GIZA++ and
HMM can not directly utilize word aligned data, the
training data for MaxEnt was converted to paral-
lel sentences where each sentence has only the pair
of linked words. The resulting numbers make both
HMM and GIZA much closer in performance to the
MaxEnt aligner but the results are better for com-
paring alignment methods.
10 Error Analysis and Discussion
The alignment errors made by the system can be
attributed to
? English words that require multi-word Arabic
states, for example (a) dates which are written
in Arabic in more than one form ?kAnwn Al-
vAny / ynAyr? for ?january?, and (b) compound
words like ?rAm Allh? in English is ?Ramallah?.
? Rare translation of a common Arabic word as
well as a common English word used as the
translation for a rare Arabic word.
? Parallel corpora mismatch: training material for
translation is processed at a document level and
yet systems often operate at a sentence level.
Human translators often use pronouns for ear-
lier mentioned names although in the source lan-
guage the name is repeated. Information which
is sometimes repeated in the source in an ear-
lier sentence is dropped in future sentences of
the document. Document level features are re-
quired to allow the system to have information
to leave these words unaligned.
Figure 3 shows a human alignment on the left and
a machine output on the right. The columns next
to the words indicate whether the alignments are
?good? or ?extra? which indicates that these words
are aligned to the special NULL state. There are two
examples of multi-word Arabic states shown: (a) for
?january?, and (b) the English word ?agenda?. The
system aligns ?the? before committee and it seems
in this case its an annotation error. In this exam-
ple the Arabic words lnAHyp, AltnZym, wAlAEdAd
and Allwjsty are all unknown words in the vocabu-
lary yet the system managed to link 3 out 4 words
correctly.
While significant gains have been made in align-
ment performance, these gains have not directly
translated to machine translation improvements. In
fact, although the GIZA system is better than the
HMM system at alignment, the machine translation
result on MT03 indicates a slight degradation (al-
though it is not statistically significant). The prime
reason for this is that features extracted from the
alignments are aggregated over the training corpus
and this process helps good alignments to have signif-
icantly better counts than errors in alignment. Align-
ing rare words correctly should help performance but
since their count is low it is not reflected in bleu
scores.
11 Conclusion and Future Work
This paper presented a word aligner trained on anno-
tated data. While the performance of the aligner is
shown to be significantly better than other unsuper-
vised algorithms, the utility of these alignments in
machine translation is still an open subject although
gains are shown in two of the test sets. Since features
are extracted from a parallel corpus, most of the in-
formation relating to the specific sentence alignment
is lost in the aggregation of features across sentences.
Improvements in capturing sentence context could
allow the machine translation system to use a rare
but correct link appropriately.
Another significant result is that a small amount
(5K sentences) of word-aligned data is sufficient for
this algorithm since a provision is made to handle
95
Figure 3: An example sentence with human output on the left and system output on the right.
unknown words appropriately.
12 Acknowledgements
This work was partially supported by the Defense
Advanced Research Projects Agency and monitored
by SPAWAR under contract No. N66001-99-2-8916.
The views and findings contained in this material are
those of the authors and do not necessarily reflect
the position or policy of the U.S. government and no
official endorsement should be inferred. This paper
owes much to the collaboration of the Statistical MT
group at IBM.
References
Adam L. Berger, Vincent Della Pietra, and Stephen Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguistics,
22(1):39?71.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The Mathematics of Statistical Machine Translation:
Parameter Estimation. Computational Linguistics,
19(2):263?311.
Colin Cherry and Dekang Lin. 2003. A probability model
to improve word alignment. In 41st Annual Meeting of
the Association for Computational Linguistics, pages
88?95, Sapporo, Japan.
Stephen Della Pietra, Vincent Della Pietra, and John
Lafferty. 1995. Inducing features of random fields.
Technical Report, Department of Computer Science,
Carnegie-Mellon University, CMU-CS-95-144, May.
Niyu Ge. 2004. Improvement in Word Alignments. Pre-
sentation given at DARPA/TIDES MT workshop.
LDC. 2005. http://ldc.upenn.edu/projects/tides/
mt2005ar.htm.
Young-Suk Lee, Kishore Papineni, and Salim Roukos.
2003. Language model based arabic word segmenta-
tion. In 41st Annual Meeting of the Association for
Computational Linguistics, pages 399?406, Sapporo,
Japan.
Huma Lodhi, John Shawe-Taylor, Nello Cristianini, and
Christopher J. C. H. Watkins. 2000. Text classification
using string kernels. In NIPS, pages 563?569.
G. Miller. 1990. Wordnet: An on-line lexical database.
International Journal of Lexicography, 3(4):235?244.
Robert C. Moore. 2004. Improving IBM Word-Alignment
Model 1. In 42nd Annual Meeting of the Associ-
ation for Computational Linguistics, pages 518?525,
Barcelona, Spain.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In 38th Annual Meeting of
the Association for Computational Linguistics, pages
440?447, Hong Kong, China.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Joint Conf. of Empirical Methods
in Natural Language Processing and Very Large Cor-
pora, pages 20?28, College Park, Maryland.
Franz Josef Och. 2003. Minimum error rate training in
Statistical Machine Translation. In 41st Annual Meet-
ing of the Association for Computational Linguistics,
pages 160?167, Sapporo, Japan.
Christoph Tillmann and Hermann Ney. 2003. Word re-
ordering and a dynamic programming beam search al-
gorithm for Statistical Machine Translation. 29(1):97?
133.
Stefan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM BasedWord Alignment in Statistical Ma-
chine Translation. In Proc. of the 16th Int. Conf.
on Computational Linguistics (COLING 1996), pages
836?841, Copenhagen, Denmark, August.
96
 
		In Question Answering, Two Heads Are Better Than One
Jennifer Chu-Carroll Krzysztof Czuba John Prager Abraham Ittycheriah
IBM T.J. Watson Research Center
P.O. Box 704
Yorktown Heights, NY 10598, U.S.A.
jencc,kczuba,jprager,abei@us.ibm.com
Abstract
Motivated by the success of ensemble methods
in machine learning and other areas of natu-
ral language processing, we developed a multi-
strategy and multi-source approach to question
answering which is based on combining the re-
sults from different answering agents searching
for answers in multiple corpora. The answer-
ing agents adopt fundamentally different strate-
gies, one utilizing primarily knowledge-based
mechanisms and the other adopting statistical
techniques. We present our multi-level answer
resolution algorithm that combines results from
the answering agents at the question, passage,
and/or answer levels. Experiments evaluating
the effectiveness of our answer resolution algo-
rithm show a 35.0% relative improvement over
our baseline system in the number of questions
correctly answered, and a 32.8% improvement
according to the average precision metric.
1 Introduction
Traditional question answering (QA) systems typically
employ a pipeline approach, consisting roughly of ques-
tion analysis, document/passage retrieval, and answer se-
lection (see e.g., (Prager et al, 2000; Moldovan et al,
2000; Hovy et al, 2001; Clarke et al, 2001)). Although a
typical QA system classifies questions based on expected
answer types, it adopts the same strategy for locating po-
tential answers from the same corpus regardless of the
question classification. In our own earlier work, we de-
veloped a specialized mechanism called Virtual Annota-
tion for handling definition questions (e.g., ?Who was
Galileo?? and ?What are antibiotics??) that consults,
in addition to the standard reference corpus, a structured
knowledge source (WordNet) for answering such ques-
tions (Prager et al, 2001). We have shown that better
performance is achieved by applying Virtual Annotation
and our general purpose QA strategy in parallel. In this
paper, we investigate the impact of adopting such a multi-
strategy and multi-source approach to QA in a more gen-
eral fashion.
Our approach to question answering is additionally
motivated by the success of ensemble methods in ma-
chine learning, where multiple classifiers are employed
and their results are combined to produce the final output
of the ensemble (for an overview, see (Dietterich, 1997)).
Such ensemble methods have recently been adopted in
question answering (Chu-Carroll et al, 2003b; Burger
et al, 2003). In our question answering system, PI-
QUANT, we utilize in parallel multiple answering agents
that adopt different processing strategies and consult dif-
ferent knowledge sources in identifying answers to given
questions, and we employ resolution mechanisms to com-
bine the results produced by the individual answering
agents.
We call our approach multi-strategy since we com-
bine the results from a number of independent agents im-
plementing different answer finding strategies. We also
call it multi-source since the different agents can search
for answers in multiple knowledge sources. In this pa-
per, we focus on two answering agents that adopt fun-
damentally different strategies: one agent uses predomi-
nantly knowledge-based mechanisms, whereas the other
agent is based on statistical methods. Our multi-level
resolution algorithm enables combination of results from
each answering agent at the question, passage, and/or an-
swer levels. Our experiments show that in most cases
our multi-level resolution algorithm outperforms its com-
ponents, supporting a tightly-coupled design for multi-
agent QA systems. Experimental results show signifi-
cant performance improvement over our single-strategy,
single-source baselines, with the best performing multi-
level resolution algorithm achieving a 35.0% relative im-
provement in the number of correct answers and a 32.8%
improvement in average precision, on a previously un-
seen test set.
                                                               Edmonton, May-June 2003
                                                               Main Papers , pp. 24-31
                                                         Proceedings of HLT-NAACL 2003
Answering Agents
KSP
SemanticSearch
KeywordSearch
Question
WordNet
Answer
Cyc
QFrame
QuestionAnalysis QGoals
Knowledge-BasedAnswering Agent
StatisticalAnswering Agent
Aquaintcorpus
TRECcorpus
EB
AnswerResolution
Definition QAnswering Agent
KSP-BasedAnswering Agent
Knowledge Sources
Figure 1: PIQUANT?s Architecture
2 A Multi-Agent QA Architecture
In order to enable a multi-source and multi-strategy ap-
proach to question answering, we developed a modu-
lar and extensible QA architecture as shown in Figure 1
(Chu-Carroll et al, 2003a; Chu-Carroll et al, 2003b).
With a consistent interface defined for each component,
this architecture allows for easy plug-and-play of individ-
ual components for experimental purposes.
In our architecture, a question is first processed by the
question analysis component. The analysis results are
represented as a QFrame, which minimally includes a set
of question features that help activate one or more an-
swering agents. Each answering agent takes the QFrame
and generates its own set of requests to a variety of
knowledge sources. This may include performing search
against a text corpus and extracting answers from the re-
sulting passages, or performing a query against a struc-
tured knowledge source, such as WordNet (Miller, 1995)
or Cyc (Lenat, 1995). The (intermediate) results from
the individual answering agents are then passed on to the
answer resolution component, which combines and re-
solves the set of results, and either produces the system?s
final answers or feeds the intermediate results back to the
answering agents for further processing.
We have developed multiple answering agents, some
general purpose and others tailored for specific ques-
tion types. Figure 1 shows the answering agents cur-
rently available in PIQUANT. The knowledge-based and
statistical answering agents are general-purpose agents
that adopt different processing strategies and consult a
number of different text resources. The definition-Q
agent targets definition questions (e.g., ?What is peni-
cillin?? and ?Who is Picasso??) with a technique called
Virtual Annotation using the external knowledge source
WordNet (Prager et al, 2001). The KSP-based answer-
ing agent focuses on a subset of factoid questions with
specific logical forms, such as capital(?COUNTRY) and
state tree(?STATE). The answering agent sends requests
to the KSP (Knowledge Sources Portal), which returns, if
possible, an answer from a structured knowledge source
(Chu-Carroll et al, 2003a).
In the rest of this paper, we briefly describe our two
general-purpose answering agents. We then focus on a
multi-level answer resolution algorithm, applicable at dif-
ferent points in the QA process of these two answering
agents. Finally, we discuss experiments conducted to dis-
cover effective methods for combining results from mul-
tiple answering agents.
3 Component Answering Agents
We focus on two end-to-end answering agents designed
to answer short, fact-seeking questions from a collection
of text documents, as motivated by the requirements of
the TREC QA track (Voorhees, 2003). Both answer-
ing agents adopt the classic pipeline architecture, con-
sisting roughly of question analysis, passage retrieval,
and answer selection components. Although the answer-
ing agents adopt fundamentally different strategies in
their individual components, they have performed quite
comparably in past TREC QA tracks (Voorhees, 2001;
Voorhees, 2002).
3.1 Knowledge-Based Answering Agent
Our first answering agent utilizes a primarily knowledge-
driven approach, based on Predictive Annotation (Prager
et al, 2000). A key characteristic of this approach is that
potential answers, such as person names, locations, and
dates, in the corpus are predictively annotated. In other
words, the corpus is indexed not only with keywords, as
is typical for most search engines, but also with the se-
mantic classes of these pre-identified potential answers.
During the question analysis phase, a rule-based mech-
anism is employed to select one or more expected an-
swer types, from a set of about 80 classes used in the
predictive annotation process, along with a set of ques-
tion keywords. A weighted search engine query is then
constructed from the keywords, their morphological vari-
ations, synonyms, and the answer type(s). The search en-
gine returns a hit list of typically 10 passages, each con-
sisting of 1-3 sentences. The candidate answers in these
passages are identified and ranked based on three criteria:
1) match in semantic type between candidate answer and
expected answer, 2) match in weighted grammatical rela-
tionships between question and answer passages, and 3)
frequency of answer in candidate passages (redundancy).
The answering agent returns the top n ranked candidate
answers along with a confidence score for each answer.
3.2 Statistical Answering Agent
The second answering agent takes a statistical approach
to question answering (Ittycheriah, 2001; Ittycheriah et
al., 2001). It models the distribution p(c|q, a), which
measures the ?correctness? (c) of an answer (a) to a ques-
tion (q), by introducing a hidden variable representing the
answer type (e) as follows:
p(c|q, a) =
?
e p(c, e|q, a)
=
?
e p(c|e, q, a)p(e|q, a)
p(e|q, a) is the answer type model which predicts, from
the question and a proposed answer, the answer type they
both satisfy. p(c|e, q, a) is the answer selection model.
Given a question, an answer, and the predicted answer
type, it seeks to model the correctness of this configura-
tion. These distributions are modeled using a maximum
entropy formulation (Berger et al, 1996), using training
data which consists of human judgments of question an-
swer pairs. For the answer type model, 13K questions
were annotated with 31 categories. For the answer selec-
tion model, 892 questions from the TREC 8 and TREC 9
QA tracks were used, along with 4K trivia questions.
During runtime, the question is first analyzed by the
answer type model, which selects one out of a set of 31
types for use by the answer selection model. Simultane-
ously, the question is expanded using local context anal-
ysis (Xu and Croft, 1996) with an encyclopedia, and the
top 1000 documents are retrieved by the search engine.
From these documents, the top 100 passages are chosen
that 1) maximize the question word match, 2) have the
desired answer type, 3) minimize the dispersion of ques-
tion words, and 4) have similar syntactic structures as the
question. From these passages, candidate answers are ex-
tracted and ranked using the answer selection model. The
top n candidate answers are then returned, each with an
associated confidence score.
4 Answer Resolution
Given two answering agents with the same pipeline archi-
tecture, there are multiple points in the process at which
(intermediate) results can be combined, as illustrated in
Figure 2. More specifically, it is possible for one answer-
ing agent to provide input to the other after the question
analysis, passage retrieval, and answer selection phases.
In PIQUANT, the knowledge based agent may accept in-
put from the statistical agent after each of these three
phases.1 The contributions from the statistical agent are
taken into consideration by the knowledge based answer-
ing agent in a phase-dependent fashion. The rest of this
section details our combination strategies for each phase.
4.1 Question-Level Combination
One of the key tasks of the question analysis component
is to determine the expected answer type, such as PERSON
for ?Who discovered America?? and DATE for ?When
did World War II end?? This information is taken into ac-
count by most existing QA systems when ranking candi-
date answers, and can also be used in the passage retrieval
process to increase the precision of candidate passages.
We seek to improve the knowledge-based agent?s
performance in passage retrieval and answer selection
through better answer type identification by consulting
the statistical agent?s expected answer type. This task,
however, is complicated by the fact that QA systems em-
ploy different sets of answer types, often with different
granularities and/or with overlapping types. For instance,
while one system may generate ROYALTY for the ques-
tion ?Who was the King of France in 1702??, another
system may produce PERSON as the most specific an-
swer type in its repertoire. This is quite a serious problem
for us as the knowledge based agent uses over 80 answer
types while the statistical agent adopts only 31 categories.
In order to distinguish actual answer type discrepan-
cies from those due to granularity differences, we first
manually created a mapping between the two sets of an-
swer types. This mapping specifies, for each answer type
used by the statistical agent, a set of possible correspond-
ing types used by the knowledge-based agent. For exam-
ple, the GEOLOGICALOBJ class is mapped to a set of finer
grained classes: RIVER, MOUNTAIN, LAKE, and OCEAN.
At processing time, the statistical agent?s answer type
is mapped to the knowledge-based agent?s classes (SA-
1Although it is possible for the statistical agent to receive
input from the knowledge based agent as well, we have not pur-
sued that option because of implementation issues.
Question
Analysis 1
Passage
Retrieval 1
Answer
Selection 1
passages answers
Question
Analysis 2
Passage
Retrieval 2
Answer
Selection 2
Agent 1 (Knowledge-Based)
Agent 2 (Statistical)
question
typeQFrame
Figure 2: Answer Resolution Strategies
types), which are then merged with the answer type(s) se-
lected by the knowledge-based agent itself (KBA-types)
as follows:
1. If the intersection of KBA-types and SA-types is
non-null, i.e., the two agents produced consistent an-
swer types, then the merged set is KBA-types.
2. Otherwise, the two sets of answer types are truly
in disagreement, and the merged set is the union of
KBA-types and SA-types.
The merged answer types are then used by the
knowledge-based agent in further processing.
4.2 Passage-Level Combination
The passage retrieval component selects, from a large text
corpus, a small number of short passages from which an-
swers are identified. Oftentimes, multiple passages that
answer a question are retrieved. Some of these passages
may be better suited than others for the answer selection
algorithm employed downstream. For example, consider
?When was Benjamin Disraeli prime minister??, whose
answer can be found in both passages below:
1. Benjamin Disraeli, who had become prime minister
in 1868, was born into Judaism but was baptized a
Christian at the age of 12.
2. France had a Jewish prime minister in 1936, Eng-
land in 1868, and Spain, of all countries, in 1835,
but none of them, Leon Blum, Benjamin Disraeli or
Juan Alvarez Mendizabel, were devoutly observant,
as Lieberman is.
Although the correct answer, 1868, is present in both
passages, it is substantially easier to identify the answer
from the first passage, where it is directly stated, than
from the second passage, where recognition of parallel
constructs is needed to identify the correct answer.
Because of strategic differences in question analysis
and passage retrieval, our two answering agents often re-
trieve different passages for the same question. Thus, we
perform passage-level combination to make a wider va-
riety of passages available to the answer selection com-
ponent, as shown in Figure 2. The potential advantages
are threefold. First, passages from agent 2 may contain
answers absent in passages retrieved by agent 1. Sec-
ond, agent 2 may have retrieved passages better suited for
the downstream answer selection algorithm than those re-
trieved by agent 1. Third, passages from agent 2 may con-
tain additional occurrences of the correct answer, which
boosts the system?s confidence in the answer through the
redundancy measure.2
Our passage-level combination algorithm adds to the
passages extracted by the knowledge-based agent the top-
ranked passages from the statistical agent that contain
candidate answers of the right type. More specifically,
the statistical agent?s passages are semantically annotated
and the top 10 passages containing at least one candidate
of the expected answer type(s) are selected.3
4.3 Answer-Level Combination
The answer selection component identifies, from a set
of passages, the top n answers for the given question,
with their associated confidence scores. An answer-level
combination algorithm takes the top answer(s) from the
individual answering agents and determines the overall
best answer(s). Of our three combination algorithms, this
most closely resembles traditional ensemble methods, as
voting takes place among the end results of individual an-
2On the other hand, such redundancy may result in error
compounding, as discussed in Section 5.3.
3We selected the top 10 passages so that the same number
of passages are considered from both answering agents.
swering agents to determine the final output of the ensem-
ble.
We developed two answer-level combination algo-
rithms, both utilizing a simple confidence-based voting
mechanism, based on the premise that answers selected
by both agents with high confidence are more likely to
be correct than those identified by only one agent.4 In
both algorithms, named entity normalization is first per-
formed on all candidate answers considered. In the first
algorithm, only the top answer from each agent is taken
into account. If the two top answers are equivalent, the
answer is selected with the combined confidence from
both agents; otherwise, the more confident answer is se-
lected.5 In the second algorithm, the top 5 answers from
each agent are allowed to participate in the voting pro-
cess. Each instance of an answer votes with a weight
equal to its confidence value and the weights of equiv-
alent answers are again summed. The answer with the
highest weight, or confidence value, is selected as the
system?s final answer. Since in our evaluation, the second
algorithm uniformly outperforms the first, it is adopted as
our answer-level combination algorithm in the rest of the
paper.
5 Performance Evaluation
5.1 Experimental Setup
To assess the effectiveness of our multi-level answer res-
olution algorithm, we devised experiments to evaluate the
impact of the question, passage, and answer-level combi-
nation algorithms described in the previous section.
The baseline systems are the knowledge-based and sta-
tistical agents performing individually against a single
reference corpus. In addition, our earlier experiments
showed that when employing a single answer finding
strategy, consulting multiple text corpora yielded better
performance than using a single corpus. We thus con-
figured a version of our knowledge-based agent to make
use of three available text corpora,6 the AQUAINT cor-
pus (news articles from 1998-2000), the TREC corpus
(news articles from 1988-1994),7 and a subset of the En-
cyclopedia Britannica. This multi-source version of the
knowledge-based agent will be used in all answer resolu-
tion experiments in conjunction with the statistical agent.
We configured multiple versions of PIQUANT to eval-
uate our question, passage, and answer-level combination
4In future work we will be investigating weighted voting
schemes based on question features.
5The confidence values from both answering agents are nor-
malized to be between 0 and 1.
6The statistical agent is currently unable to consult multiple
corpora.
7Both the AQUAINT and TREC corpora are available from
the Linguistics Data Consortium, http://www.ldc.org.
algorithms individually and cumulatively. For cumula-
tive effects, we 1) combined the algorithms pair-wise,
and 2) employed all three algorithms together. The two
test sets were selected from the TREC 10 and 11 QA
track questions (Voorhees, 2002; Voorhees, 2003). For
both test sets, we eliminated those questions that did not
have known answers in the reference corpus. Further-
more, from the TREC 10 test set, we discarded all defini-
tion questions,8 since the knowledge-based agent adopts
a specialized strategy for handling definition questions
which greatly reduces potential contributions from other
answering agents. This results in a TREC 10 test set of
313 questions and a TREC 11 test set of 453 questions.
5.2 Experimental Results
We ran each of the baseline and combined systems on the
two test sets. For each run, the system outputs its top
answer and its confidence score for each question. All
answers for a run are then sorted in descending order of
the confidence scores. Two established TREC QA eval-
uation metrics are adopted to assess the results for each
run as follows:
1. % Correct: Percentage of correct answers.
2. Average Precision: A confidence-weighted score
that rewards systems with high confidence in cor-
rect answers as follows, where N is the number of
questions:
1
N
N?
i=1
# correct up to question i/i
Table 1 shows our experimental results. The top sec-
tion shows the comparable baseline results from the sta-
tistical agent (SA-SS) and the single-source knowledge-
based agent (KBA-SS). It also includes results for the
multi-source knowledge-based agent (KBA-MS), which
improve upon those for its single-source counterpart
(KBA-SS).
The middle section of the table shows the answer
resolution results, including applying the question, pas-
sage, and answer-level combination algorithms individu-
ally (Q, P, and A, respectively), applying them pair-wise
(Q+P, P+A, and Q+A), and employing all three algo-
rithms (Q+P+A). Finally, the last row of the table shows
the relative improvement by comparing the best perform-
ing system configuration (highlighted in boldface) with
the better performing single-source, single-strategy base-
line system (SA-SS or KBA-SS, in italics).
Overall, PIQUANT?s multi-strategy and multi-source
approach achieved a 35.0% relative improvement in the
8Definition questions were intentionally excluded by the
track coordinator in the TREC 11 test set.
TREC 10 (313) TREC 11 (453)
% Corr Avg Prec % Corr Avg Prec
SA-SS 36.7% 0.569 32.9% 0.534
KBA-SS 39.6% 0.595 32.5% 0.531
KBA-MS 43.8% 0.641 38.2% 0.622
Q 44.7% 0.647 38.9% 0.632
P 49.5% 0.661 40.0% 0.627
A 49.5% 0.712 43.5% 0.704
Q+P 48.9% 0.656 41.1% 0.640
P+A 51.1% 0.711 44.2% 0.686
Q+A 49.8% 0.716 43.9% 0.709
Q+P+A 50.8% 0.706 44.4% 0.690
rel. improv. 29.0% 20.3% 35.0% 32.8%
Table 1: Experimental Results
number of correct answers and a 32.8% improvement in
average precision on the TREC 11 data set. Of the com-
bined improvement, approximately half was achieved by
the multi-source aspect of PIQUANT, while the other half
was obtained by PIQUANT?s multi-strategy feature. Al-
though the absolute average precision values are com-
parable on both test sets and the absolute percentage of
correct answers is lower on the TREC 11 data, the im-
provement is greater on TREC 11 in both cases. This
is because the TREC 10 questions were taken into ac-
count for manual rule refinement in the knowledge-based
agent, resulting in higher baselines on the TREC 10 test
set. We believe that the larger improvement on the previ-
ously unseen TREC 11 data is a more reliable estimate of
PIQUANT?s performance on future test sets.
We applied an earlier version of our combination algo-
rithms, which performed between our current P and P+A
algorithms, in our submission to the TREC 11 QA track.
Using the average precision metric, that version of PI-
QUANT was among the top 5 best performing systems
out of 67 runs submitted by 34 groups.
5.3 Discussion and Analysis
A cursory examination of the results in Table 1 allows
us to draw two general conclusions about PIQUANT?s
performance. First, all three combination algorithms ap-
plied individually improved upon the baseline using both
evaluation metrics on both test sets. In addition, overall
performance is generally better the later in the process
the combination occurs, i.e., the answer-level combina-
tion algorithm outperformed the passage-level combina-
tion algorithm, which in turn outperformed the question-
level combination algorithm. Second, the cumulative im-
provement from multiple combination algorithms is in
general greater than that from the components. For in-
stance, the Q+A algorithm uniformly outperformed the Q
and A algorithms alone. Note, however, that the Q+P+A
algorithm achieved the highest performance only on the
TREC 11 test set using the % correct metric. We believe
KBA
TREC 10 (313) TREC 11 (453)
+ - + -
SA + 185 43 254 58
- 24 61 41 100
Table 2: Passage Retrieval Analysis
that this is because of compounding errors that occurred
during the multiple combination process.
In ensemble methods, the individual components must
make different mistakes in order for the combined sys-
tem to potentially perform better than the component sys-
tems (Dietterich, 1997). We examined the differences
in results between the two answering agents from their
question analysis, passage retrieval, and answer selection
components. We focused our analysis on the potential
gain/loss from incorporating contributions from the sta-
tistical agent, and how the potential was realized as actual
performance gain/loss in our end-to-end system.
At the question level, we examined those questions
for which the two agents proposed incompatible answer
types. On the TREC 10 test set, the statistical agent in-
troduced correct answer types in 6 cases and incorrect
answer types in 9 cases. As a result, in some cases the
question-level combination algorithm improved system
performance (comparing A and Q+A) and in others it
degraded performance (comparing P and Q+P). On the
other hand, on the TREC 11 test set, the statistical agent
introduced correct and incorrect answer types in 15 and
6 cases, respectively. As a result, in most cases perfor-
mance improved when the question-level combination al-
gorithm was invoked. The difference in question analysis
performance again reflects the fact that TREC 10 ques-
tions were used in question analysis rule refinement in
the knowledge-based agent.
At the passage level, we examined, for each ques-
tion, whether the candidate passages contained the cor-
rect answer. Table 2 shows the distribution of ques-
tions for which correct answers were (+) and were not
(-) present in the passages for both agents. The bold-
faced cells represent questions for which the statistical
agent retrieved passages with correct answers while the
knowledge-based agent did not. There were 43 and 58
such questions in the TREC 10 and TREC 11 test sets, re-
spectively, and employing the passage-level combination
algorithm resulted only in an additional 18 and 8 correct
answers on each test set. This is because the statistical
agent?s proposes in its 10 passages, on average, 29 candi-
date answers, most of which are incorrect, of the proper
semantic type per question. As the downstream answer
selection component takes redundancy into account in an-
swer ranking, incorrect answers may reinforce one an-
other and become top ranked answers. This suggests that
KBA
TREC 10 (313) TREC 11 (453)
1st 2-5th none 1st 2-5th none
SA 1st 66 22 26 93 21 35
2-5th 26 9 13 29 19 22
none 45 14 92 51 21 162
Table 3: Answer Voting Analysis
the relative contributions of our answer selection features
may not be optimally tuned for our multi-agent approach
to QA. We plan to investigate this issue in future work.
At the answer level, we analyzed each agent?s top 5
answers, used in the combination algorithm?s voting pro-
cess. Table 3 shows the distribution of questions for
which an answer was found in 1st place, in 2nd-5th place,
and not found in top 5. Since we employ a linear vot-
ing strategy based on confidence scores, we classify the
cells in Table 3 as follows based on the perceived likeli-
hood that the correct answers for questions in each cell
wins in the voting process. The boldfaced and underlined
cells contain highly likely candidates, since a correct an-
swer was found in 1st place by both agents.9 The bold-
faced cells consist of likely candidates, since a 1st place
correct answer was supported by a 2nd-5th place answer.
The italicized and underlined cells contain possible can-
didates, while the rest of the cells cannot produce correct
1st place answers using our current voting algorithm. On
TREC 10 data, 194 questions fall into the highly likely,
likely, and possible categories, out of which the voting al-
gorithm successfully selected 155 correct answers in 1st
place. On TREC 11 data, 197 correct answers were se-
lected out of 248 questions that fall into these categories.
These results represent success rates of 79.9% and 79.4%
for our answer-level combination algorithm on the two
test sets.
6 Related Work
There has been much work in employing ensemble meth-
ods to increase system performance in machine learning.
In NLP, such methods have been applied to tasks such
as POS tagging (Brill and Wu, 1998), word sense dis-
ambiguation (Pedersen, 2000), parsing (Henderson and
Brill, 1999), and machine translation (Frederking and
Nirenburg, 1994).
In question answering, a number of researchers have
investigated federated systems for identifying answers to
questions. For example, (Clarke et al, 2003) and (Lin et
al., 2003) employ techniques for utilizing both unstruc-
9These cells are not marked as definite because in a small
number of cases, the two answers are not equivalent. For exam-
ple, for the TREC 9 question, ?Who is the emperor of Japan??,
Hirohito, Akihito, and Taisho are all considered correct answers
based on the reference corpus.
tured text and structured databases for question answer-
ing. However, the approaches taken by both these sys-
tems differ from ours in that they enforce an order be-
tween the two strategies by attempting to locate answers
in structured databases first for select question types and
falling back to unstructured text when the former fails,
while we explore both options in parallel and combine
the results from multiple answering agents.
The multi-agent approach to question answering most
similar to ours is that by Burger et al (2003). They
applied ensemble methods to combine the 67 runs sub-
mitted to the TREC 11 QA track, using an unweighted
centroid method for selecting among the 67 proposed an-
swers for each question. However, their combined sys-
tem did not outperform the top scoring system(s). Fur-
thermore, their approach differs from ours in that they fo-
cused on combining the end results of a large number of
systems, while we investigated a tightly-coupled design
for combining two answering agents.
7 Conclusions
In this paper, we introduced a multi-strategy and multi-
source approach to question answering that enables com-
bination of answering agents adopting different strategies
and consulting multiple knowledge sources. In partic-
ular, we focused on two answering agents, one adopt-
ing a knowledge-based approach and one using statistical
methods. We discussed our answer resolution component
which employs a multi-level combination algorithm that
allows for resolution at the question, passage, and answer
levels. Best performance using the % correct metric was
achieved by the three-level algorithm that combines af-
ter each stage, while highest average precision was ob-
tained by a two-level algorithm merging at the question
and answer levels, supporting a tightly-coupled design
for multi-agent question answering. Our experiments
showed that our best performing algorithms achieved a
35.0% relative improvement in the number of correct an-
swers and a 32.8% improvement in average precision on
a previously unseen test set.
Acknowledgments
We would like to thank Dave Ferrucci, Chris Welty, and
Salim Roukos for helpful discussions, Diane Litman and
the anonymous reviewers for their comments on an ear-
lier draft of this paper. This work was supported in
part by the Advanced Research and Development Ac-
tivity (ARDA)?s Advanced Question Answering for In-
telligence (AQUAINT) Program under contract number
MDA904-01-C-0988.
References
Adam L. Berger, Vincent Della Pietra, and Stephen Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguistics,
22(1):39?71.
Eric Brill and Jun Wu. 1998. Classifier combination for
improved lexical disambiguation. In Proceedings of
the 36th Annual Meeting of the Association for Com-
putational Linguistics, pages 191?195.
John D. Burger, Lisa Ferro, Warren Greiff, John Hender-
son, Marc Light, and Scott Mardis. 2003. MITRE?s
Qanda at TREC-11. In Proceedings of the Eleventh
Text Retrieval Conference. To appear.
Jennifer Chu-Carroll, David Ferrucci, John Prager, and
Christopher Welty. 2003a. Hybridization in ques-
tion answering systems. In Working Notes of the AAAI
Spring Symposium on New Directions in Question An-
swering, pages 116?121.
Jennifer Chu-Carroll, John Prager, Christopher Welty,
Krzysztof Czuba, and David Ferrucci. 2003b. A
multi-strategy and multi-source approach to question
answering. In Proceedings of the Eleventh Text Re-
trieval Conference. To appear.
Charles Clarke, Gordon Cormack, and Thomas Lynam.
2001. Exploiting redundancy in question answering.
In Proceedings of the 24th SIGIR Conference, pages
358?365.
C.L.A. Clarke, G.V. Cormack, G. Kemkes, M. Laszlo,
T.R. Lynam, E.L. Terra, and P.L. Tilker. 2003. Statis-
tical selection of exact answers. In Proceedings of the
Eleventh Text Retrieval Conference. To appear.
Thomas G. Dietterich. 1997. Machine learning research:
Four current directions. AI Magazine, 18(4):97?136.
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proceedings of the Fourth
Conference on Applied Natural Language Processing.
John C. Henderson and Eric Brill. 1999. Exploiting
diversity in natural language processing: Combining
parsers. In Proceedings of the 4th Conference on Em-
pirical Methods in Natural Language Processing.
Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Michael
Junk, and Chin-Yew Lin. 2001. Question answering
in Webclopedia. In Proceedings of the Ninth Text RE-
trieval Conference, pages 655?664.
Abraham Ittycheriah, Martin Franz, Wei-Jing Zhu, and
Adwait Ratnaparkhi. 2001. Question answering using
maximum entropy components. In Proceedings of the
2nd Conference of the North American Chapter of the
Association for Computational Linguistics, pages 33?
39.
Abraham Ittycheriah. 2001. Trainable Question Answer-
ing Systems. Ph.D. thesis, Rutgers - The State Univer-
sity of New Jersey.
Douglas B. Lenat. 1995. Cyc: A large-scale investment
in knowledge infrastructure. Communications of the
ACM, 38(11).
Jimmy Lin, Aaron Fernandes, Boris Katz, Gregory Mar-
ton, and Stefanie Tellex. 2003. Extracting an-
swers from the web using knowledge annotation and
knowledge mining techniques. In Proceedings of the
Eleventh Text Retrieval Conference. To appear.
George Miller. 1995. Wordnet: A lexical database for
English. Communications of the ACM, 38(11).
Dan Moldovan, Sanda Harabagiu, Marius Pasca, Rada
Mihalcea, Roxana Girju, Richard Goodrum, and Vasile
Rus. 2000. The structure and performance of an open-
domain question answering system. In Proceedings of
the 39th Annual Meeting of the Association for Com-
putational Linguistics, pages 563?570.
Ted Pedersen. 2000. A simple approach to building en-
sembles of naive Bayesian classifiers for word sense
disambiguation. In Proceedings of the 1st Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 63?69.
John Prager, Eric Brown, Anni Coden, and Dragomir
Radev. 2000. Question-answering by predictive anno-
tation. In Proceedings of the 23rd SIGIR Conference,
pages 184?191.
John Prager, Dragomir Radev, and Krzysztof Czuba.
2001. Answering what-is questions by virtual anno-
tation. In Proceedings of Human Language Technolo-
gies Conference, pages 26?30.
Ellen M. Voorhees. 2001. Overview of the TREC-9
question answering track. In Proceedings of the 9th
Text Retrieval Conference, pages 71?80.
Ellen M. Voorhees. 2002. Overview of the TREC 2001
question answering track. In Proceedings of the 10th
Text Retrieval Conference, pages 42?51.
Ellen M. Voorhees. 2003. Overview of the TREC
2002 question answering track. In Proceedings of the
Eleventh Text Retrieval Conference. To appear.
Jinxi Xu and W. Bruce Croft. 1996. Query expansion
using local and global document analysis. In Proceed-
ings of the 19th SIGIR Conference, pages 4?11.



Automatic Derivation of Surface Text Patterns for a Maximum Entropy
Based Question Answering System
Deepak Ravichandran
 
USC Information Sciences Institute
4676, Admiralty Way
Marina del Rey, CA, 90292
ravichan@isi.edu
Abraham Ittycheriah and Salim Roukos
IBM TJ Watson Research Center
Yorktown Heights, NY, 10598

abei,roukos  @us.ibm.com
Abstract
In this paper we investigate the use of surface
text patterns for a Maximum Entropy based
Question Answering (QA) system. These text
patterns are collected automatically in an un-
supervised fashion using a collection of trivia
question and answer pairs as seeds. These pat-
terns are used to generate features for a statis-
tical question answering system. We report our
results on the TREC-10 question set.
1 Introduction
Several QA systems have investigated the use of text pat-
terns for QA (Soubbotin and Soubbotin, 2001), (Soub-
botin and Soubbotin, 2002), (Ravichandran and Hovy,
2002). For example, for questions like ?When was
Gandhi born??, typical answers are ?Gandhi was born in
1869? and ?Gandhi (1869-1948)?. These examples sug-
gest that the text patterns such as ?  NAME  was born in
 BIRTHDATE  ? and ?  NAME  (  BIRTHDATE  -
 DEATHYEAR  )? when formulated as regular expres-
sions, can be used to select the answer phrase to ques-
tions. Another approach to a QA system is learning cor-
respondences between question and answer pairs. IBM?s
Statistical QA (Ittycheriah et al, 2001a) system uses a
probabilistic model trainable from Question-Answer sen-
tence pairs. The training is performed under a Maximum
Entropy model, using bag of words, syntactic and name
entity features. This QA system does not employ the use
of patterns. In this paper, we explore the inclusion of
surface text patterns into the framework of a statistical
question answering system.
2 KM Corpus
A corpus of question-answer pairs was obtained from
Knowledge Master (1999). We refer to this corpus as the
1Work done while the author was an intern at IBM TJ Wat-
son Research Center during Summer 2002.
KM database. Each of the pairs in KM represents a trivia
question and its corresponding answer, such as the ones
used in the trivia card game. The question-answer pairs
in KM were filtered to retain only questions that look
similar to the ones presented in the TREC task2. Some
examples of QA pairs in KM:
1. Which country was invaded by the Libyan troops in
1983? - Chad
2. Who led the 1930 Salt March in India? - Mohandas
Gandhi
3 Unsupervised Construction of Training
Set for Pattern Extraction
We use an unsupervised technique that uses the QA in
KM as seeds to learn patterns. This method was first de-
scribed in Ravichandran and Hovy (2002). However, in
this work we have enriched the pattern format by induc-
ing specific semantic types of QTerms, and have learned
many more patterns using the KM.
3.1 Algorithm for sentence construction
1. For every question, we run a Named Entity Tagger
HMMNE 3 and identify chunks of words, that sig-
nify entities. Each such entity obtained from the
Question is defined as a Question term (QTerm).
The Answer Term (ATerm) is the Answer given by
the KM corpus.
2. Each of the question-answer pairs is submitted as
query to a popular Internet search engine4. We use
the top 50 relevant documents after stripping off the
HTML tags. The text is then tokenized to smoothen
white space variations and chopped to individual
sentences.
3. For every sentence obtained from Step (3) apply
2This was done by retaining only those questions that had
10 words or less, and were not multiple choice.
3In these experiments we use HMMNE, a named entity tag-
ger similar to the BBN?s Identifinder HMM Tagger (Bikel et al,
1999).
4Alta Vista http://www.altavista.com
HMMNE and retain only those sentences that con-
tains at least one of the QTerms plus the ATerm.
For example, we obtain the following sentences for the
QA pair ?Which country was invaded by the Libyan
troops in 1983? - Chad?:
1. More than 7,000 Libyan troops entered Chad.
2. An OUA peacekeeping force of 3,500 troops replaced the
Libyan forces in the remainder of Chad.
3. In the summer of 1983, GUNT forces launched an offen-
sive against government positions in northern and eastern
Chad.
The underlined words indicate the QTerms and the
ATerms that helped to select the sentence as a potential
way of answering the Question. The algorithm described
above was applied to each of the 16,228 QA pairs in our
KM database. A total of more than 250K sentences was
obtained.
3.2 Sentence Canonicalization
Every sentence obtained from the sentence construction
algorithm is canonicalized. Canonicalization of a sen-
tence is performed on the basis of the information pro-
vided by HMMNE, the QTerms and the ATerm. Canon-
icalization in this context may be defined as the general-
ization of a sentence based on the following process:
1. Apply HMMNE to each sentence obtained from the
sentence construction algorithm.
2. Identify the QTerms and ATerm in the answer sen-
tence.
3. Replace the ATerm by the tag ?  ANSWER  ?.
4. Replace each identified Named Entity by the class
of entity it represents.
5. If a given Named Entity is also a QTerm, indicate it
by the tag ?QT?.
The following example illustrates canonicalization.
Consider the sentence:
More than 7,000 Libyan troops entered Chad.
The application of HMMNE results in:
More than  NUMEX TYPE=CARDINAL  7,000
 /NUMEX   HUMAN TYPE=PEOPLE  Libyan
 /HUMAN  troops entered  ENAMEX TYPE=
COUNTRY  Chad  /ENAMEX  .
The canonicalization step gives the sentence:
More than  CARDINAL  PEOPLE QT  troops en-
tered  ANSWER  .
3.3 Pattern Extraction
Pattern extraction algorithm.
1. Every sentence obtained from sentence canon-
icalization algorithm is delimited by the tags
?  START  ? and ?  END  ? and then passed
through a Suffix Tree. The Suffix Tree algorithm
obtains the counts of all sub-strings of the sentence.
2. From the Suffix Tree we obtain only those sub-
strings that are at least a trigram, contain both the
?  ANSWER  ? and the ?  QT  ? tag and have at
least a count of 3 occurrences.
Source Number of Questions
Trec8 200
Trec9 500
KM 4200
Table 1: Training source and sizes.
Some examples of patterns obtained from the Suffix Tree
algorithm are as follows:
1. son of  PERSON QT  and  ANSWER 
2. of the  ANSWER  DISEASE QT 
3. of  ANSWER  at  LOCATION QT 
4.  ANSWER  was the  ORDINAL 	 OCCUPATION
QT  to
5.  ANSWER  was elected  OCCUPATION QT  of the
 LOCATION QT 
6.  ANSWER  was a prolific  OCCUPATION QT 
7.  LOCATION QT  ,  ANSWER 
8.  ANSWER  ,  LOCATION QT 
9.  START 
 ANSWER  served as  OCCUPATION
QT  from  DATE 
10.  START  ANSWER  is the  PEOPLE QT 
name for
A set of 22,353 such patterns were obtained by the ap-
plication of the pattern extraction algorithm from more
than 250,000 sentences. Some patterns are very general
and applicable to many questions, such as the ones in ex-
amples (7) and (8) while others are more specific to a
few questions, such as examples (9) and (10). Having
obtained these patterns we now can learn the appropriate
?weights? to use these patterns in a Question Answering
System.
4 Maximum Entropy Training
For these experiments we use the Maximum Entropy for-
mulation (Della Pietra et al, 1995) and model the distri-
bution (Ittycheriah, 2001b),
 ffA Statistical Model for Multilingual Entity Detection and Tracking
R. Florian, H. Hassan   , A. Ittycheriah, H. Jing
N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos
I.B.M. T.J. Watson Research Center
Yorktown Heights, NY 10598
{raduf,abei,hjing,nanda,xiaoluo, nicolas,roukos}@us.ibm.com

hanyh@eg.ibm.com
Abstract
Entity detection and tracking is a relatively new
addition to the repertoire of natural language
tasks. In this paper, we present a statistical
language-independent framework for identify-
ing and tracking named, nominal and pronom-
inal references to entities within unrestricted
text documents, and chaining them into clusters
corresponding to each logical entity present in
the text. Both the mention detection model
and the novel entity tracking model can use
arbitrary feature types, being able to integrate
a wide array of lexical, syntactic and seman-
tic features. In addition, the mention detec-
tion model crucially uses feature streams de-
rived from different named entity classifiers.
The proposed framework is evaluated with sev-
eral experiments run in Arabic, Chinese and
English texts; a system based on the approach
described here and submitted to the latest Au-
tomatic Content Extraction (ACE) evaluation
achieved top-tier results in all three evaluation
languages.
1 Introduction
Detecting entities, whether named, nominal or pronom-
inal, in unrestricted text is a crucial step toward under-
standing the text, as it identifies the important concep-
tual objects in a discourse. It is also a necessary step for
identifying the relations present in the text and populating
a knowledge database. This task has applications in in-
formation extraction and summarization, information re-
trieval (one can get al hits for Washington/person and not
the ones for Washington/state or Washington/city), data
mining and question answering.
The Entity Detection and Tracking task (EDT hence-
forth) has close ties to the named entity recognition
(NER) and coreference resolution tasks, which have been
the focus of attention of much investigation in the recent
past (Bikel et al, 1997; Borthwick et al, 1998; Mikheev
et al, 1999; Miller et al, 1998; Aberdeen et al, 1995;
Ng and Cardie, 2002; Soon et al, 2001), and have been
at the center of several evaluations: MUC-6, MUC-7,
CoNLL?02 and CoNLL?03 shared tasks. Usually, in com-
putational linguistic literature, a named entity represents
an instance of a name, either a location, a person, an or-
ganization, and the NER task consists of identifying each
individual occurrence of such an entity. We will instead
adopt the nomenclature of the Automatic Content Extrac-
tion program1 (NIST, 2003a): we will call the instances
of textual references to objects or abstractions mentions,
which can be either named (e.g. John Mayor), nominal
(e.g. the president) or pronominal (e.g. she, it). An entity
consists of all the mentions (of any level) which refer to
one conceptual entity. For instance, in the sentence
President John Smith said he has no comments.
there are two mentions: John Smith and he (in the order
of appearance, their levels are named and pronominal),
but one entity, formed by the set {John Smith, he}.
In this paper, we present a general statistical frame-
work for entity detection and tracking in unrestricted text.
The framework is not language specific, as proved by ap-
plying it to three radically different languages: Arabic,
Chinese and English. We separate the EDT task into a
mention detection part ? the task of finding all mentions
in the text ? and an entity tracking part ? the task of com-
bining the detected mentions into groups of references to
the same object.
The work presented here is motivated by the ACE eval-
uation framework, which has the more general goal of
building multilingual systems which detect not only enti-
ties, but also relations among them and, more recently,
events in which they participate. The EDT task is ar-
guably harder than traditional named entity recognition,
because of the additional complexity involved in extract-
ing non-named mentions (nominals and pronouns) and
the requirement of grouping mentions into entities.
We present and evaluate empirically statistical mod-
els for both mention detection and entity tracking prob-
lems. For mention detection we use approaches based on
Maximum Entropy (MaxEnt henceforth) (Berger et al,
1996) and Robust Risk Minimization (RRM henceforth)
1For a description of the ACE program see
http://www.nist.gov/speech/tests/ace/.
(Zhang et al, 2002). The task is transformed into a se-
quence classification problem. We investigate a wide ar-
ray of lexical, syntactic and semantic features to perform
the mention detection and classification task including,
for all three languages, features based on pre-existing sta-
tistical semantic taggers, even though these taggers have
been trained on different corpora and use different seman-
tic categories. Moreover, the presented approach implic-
itly learns the correlation between these different seman-
tic types and the desired output types.
We propose a novel MaxEnt-based model for predict-
ing whether a mention should or should not be linked to
an existing entity, and show how this model can be used
to build entity chains. The effectiveness of the approach
is tested by applying it on data from the above mentioned
languages ? Arabic, Chinese, English.
The framework presented in this paper is language-
universal ? the classification method does not make any
assumption about the type of input. Most of the fea-
ture types are shared across the languages, but there are a
small number of useful feature types which are language-
specific, especially for the mention detection task.
The paper is organized as follows: Section 2 describes
the algorithms and feature types used for mention detec-
tion. Section 3 presents our approach to entity tracking.
Section 4 describes the experimental framework and the
systems? results for Arabic, Chinese and English on the
data from the latest ACE evaluation (September 2003), an
investigation of the effect of using different feature types,
as well as a discussion of the results.
2 Mention Detection
The mention detection system identifies the named, nom-
inal and pronominal mentions introduced in the previous
section. Similarly to classical NLP tasks such as base
noun phrase chunking (Ramshaw and Marcus, 1994), text
chunking (Ramshaw and Marcus, 1995) or named entity
recognition (Tjong Kim Sang, 2002), we formulate the
mention detection problem as a classification problem,
by assigning to each token in the text a label, indicating
whether it starts a specific mention, is inside a specific
mention, or is outside any mentions.
2.1 The Statistical Classifiers
Good performance in many natural language process-
ing tasks, such as part-of-speech tagging, shallow pars-
ing and named entity recognition, has been shown to de-
pend heavily on integrating many sources of information
(Zhang et al, 2002; Jing et al, 2003; Ittycheriah et al,
2003). Given the stated focus of integrating many feature
types, we are interested in algorithms that can easily in-
tegrate and make effective use of diverse input types. We
selected two methods which satisfy these criteria: a linear
classifier ? the Robust Risk Minimization classifier ? and
a log-linear classifier ? the Maximum Entropy classifier.
Both methods can integrate arbitrary types of informa-
tion and make a classification decision by aggregating all
information available for a given classification.
Before formally describing the methods2, we introduce
some notations: let
 	



be the set of pre-
dicted classes,  be the example space and 

be the feature space. Each example Proceedings of NAACL HLT 2007, pages 57?64,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Direct Translation Model 2
Abraham Ittycheriah and Salim Roukos
IBM T.J. Watson Research Center
1101 Kitchawan Road
Yorktown Heights, NY 10598
{abei,roukos}@us.ibm.com
Abstract
This paper presents a maximum entropy ma-
chine translation system using a minimal set
of translation blocks (phrase-pairs). While
recent phrase-based statistical machine trans-
lation (SMT) systems achieve significant im-
provement over the original source-channel sta-
tistical translation models, they 1) use a large
inventory of blocks which have significant over-
lap and 2) limit the use of training to just a
few parameters (on the order of ten). In con-
trast, we show that our proposed minimalist
system (DTM2) achieves equal or better per-
formance by 1) recasting the translation prob-
lem in the traditional statistical modeling ap-
proach using blocks with no overlap and 2) re-
lying on training most system parameters (on
the order of millions or larger). The new model
is a direct translation model (DTM) formu-
lation which allows easy integration of addi-
tional/alternative views of both source and tar-
get sentences such as segmentation for a source
language such as Arabic, part-of-speech of both
source and target, etc. We show improvements
over a state-of-the-art phrase-based decoder in
Arabic-English translation.
1 Introduction
Statistical machine translation takes a source se-
quence, S = [s1 s2 . . . sK ], and generates a target
sequence, T ? = [t1 t2 . . . tL], by finding the most
likely translation given by:
T ? = argmax
T
p(T |S).
1.1 Block selection
Recent statistical machine translation (SMT) al-
gorithms generate such a translation by incorpo-
rating an inventory of bilingual phrases (Och and
Ney, 2000). A m-n phrase-pair, or block, is a se-
quence of m source words paired with a sequence
of n target words. The inventory of blocks in cur-
rent systems is highly redundant. We illustrate the
redundancy using the example in Table 1 which
lljnp
Almrkzyp
llHzb
Al$ywEy
AlSyny
the
Politburo
of
the
Central
Committee
of
the
Chinese
Communist
Party
Almktb
AlsyAsy
Figure 1: Example of Arabic snipet and alignment
to its English translation.
shows a set of phrases that cover the two-word
Arabic fragment ?lljnp Almrkzyp? whose align-
ment and translation is shown in Figure 1. One
notices the significant overlap between the vari-
ous blocks including the fact the output target se-
quence ?of the central committee? can be pro-
duced in at least two different ways: 1) as 2-4 block
?lljnp Almrkzyp | of the central committee? cov-
ering the two Arabic words, or 2) by using the 1-
3 block ?Almrkzyp | of the central? followed by
covering the first Arabic word with the 1-1 block
?lljnp | committee?. In addition, if one adds one
more word to the Arabic fragment in the third posi-
tion such as the block ?AlSyny | chinese? the over-
lap increases significantly and more alternate possi-
bilities are available to produce an output such as
the ?of the central chinese committee.?
In this work, we propose to only use 1-n blocks and
avoid completely the redundancy obtained by the use
of m-n blocks for m > 1 in current phrase-based sys-
tems. We discuss later how by defining appropriate
features in the translation model, we capture the im-
portant dependencies required for producing n-long
fragments for an m-word input sequence including
the reordering required to produce more fluent out-
put. So in Table 1 only the blocks corresponding to
a single Arabic word are in the block inventory. To
differentiate this work from previous approaches in
57
lljnp Almrkzyp
committee central
of the commission the central
commission of the central
of the committee of central
the committee and the central
of the commission on and central
the commission , central
committee of ?s central
. . . . . .
of the central committee(11)
of the central committee of (11)
the central committee of (8)
central committee(7)
committee central (2)
central committee , (2)
. . .
Table 1: Example Arabic-English blocks showing
possible 1-n and 2-n blocks ranked by frequency.
Block count is given in () for 2-n blocks.
direct modeling for machine translation, we call our
current approach DTM2 (Direct Translation Model
2).
1.2 Statistical modeling for translation
Earlier work in statistical machine translation
(Brown et al, 1993) is based on the ?noisy-channel?
formulation where
T ? = arg max
T
p(T |S) = argmax
T
p(T )p(S|T ) (1)
where the target language model p(T ) is further de-
composed as
p(T ) ?
?
i
p(ti|ti?1, . . . , ti?k+1)
where k is the order of the language model and the
translation model p(S|T ) has been modeled by a
sequence of five models with increasing complexity
(Brown et al, 1993). The parameters of each of the
two components are estimated using Maximum Like-
lihood Estimation (MLE). The LM is estimated by
counting n-grams and using smoothing techniques.
The translation model is estimated via the EM algo-
rithm or approximations that are bootstrapped from
the previous model in the sequence as introduced in
(Brown et al, 1993). As is well known, improved
results are achieved by modifying the Bayes factor-
ization in Equation 1 above by weighing each distri-
bution differently as in:
p(T |S) ? p?(T )p1??(S|T ) (2)
This is the simplest MaxEnt1 model that uses two
feature functions. The parameter ? is tuned on a
development set (usually to improve an error met-
ric instead of MLE). This model is a special case
of the Direct Translation Model proposed in (Pap-
ineni et al, 1997; Papineni et al, 1998) for language
understanding; (Foster, 2000) demostrated perplex-
ity reductions by using direct models; and (Och and
Ney, 2002) employed it very successfully for language
translation by using about ten feature functions:
p(T |S) = 1Z exp
?
i
?i?i(S, T )
Many of the feature functions used for translation are
MLE models (or smoothed variants). For example,
if one uses ?1 = log(p(T )) and ?2 = log(p(S|T )) we
get the model described in Equation 2. Most phrase-
based systems, including the baseline decoder used
in this work use feature functions:
? a target word n-gram model (e.g., n = 5),
? a target part-of-speech n-gram model (n ? 5),
? various translation models such as a block in-
ventory with the following three varieties: 1) the
unigram block count, 2) a model 1 score p(si|ti)
on the phrase-pair, and 3)a model 1 score for
the other direction p(ti|si),
? a target word count penalty feature |T |,
? a phrase count feature,
? a distortion model (Al-Onaizan and Papineni,
2006).
The weight vector ? is estimated by tuning on a
rather small (as compared to the training set used to
define the feature functions) development set using
the BLEU metric (or other translation error met-
rics). Unlike MaxEnt training, the method (Och,
2003) used for estimating the weight vector for BLEU
maximization are not computationally scalable for a
large number of feature functions.
2 Related Work
Most recent state-of-the-art machine translation de-
coders have the following aspects that we improve
upon in this work: 1) block style, and 2) model pa-
rameterization and parameter estimation. We dis-
cuss each item next.
1The subfields of log-linear models, exponential fam-
ily, and MaxEnt describe the equivalent techniques from
different perspectives.
58
2.1 Block style
In order to extract phrases from alignments available
in one or both directions, most SMT approaches use
a heuristic such as union, intersection, inverse pro-
jection constraint, etc. As discussed earlier, these
approaches result in a large overlap between the ex-
tracted blocks (longer blocks overlap with all the
shorter subcomponents blocks). Also, slightly re-
stating the advantages of phrase-pairs identified in
(Quirk and Menezes, 2006), these blocks are effec-
tive at capturing context including the encoding of
non-compositional phrase pairs, and capturing local
reordering, but they lack variables (e.g. embedding
between ne . . . pas in French), have sparsity prob-
lems, and lack a strategy for global reordering. More
recently, (Chiang, 2005) extended phrase-pairs (or
blocks) to hierarchical phrase-pairs where a grammar
with a single non-terminal allows the embedding of
phrases-pairs, to allow for arbitrary embedding and
capture global reordering though this approach still
has the high overlap problem. However, in (Quirk
and Menezes, 2006), the authors investigate mini-
mum translation units (MTU) which is a refinement
over a similar approach by (Banchs et al, 2005)
to eliminate the overlap issue. The MTU approach
picks all the minimal blocks subject to the condition
that no word alignment link crosses distinct blocks.
They do not have the notion of a block with a vari-
able (a special case of the hierarchical phrase-pairs)
that we employ in this work. They also have a weak-
ness in the parameter estimation method; they rely
on an n-gram language model on blocks which inher-
ently requires a large bilingual training data set.
2.2 Estimating Model Parameters
Most recent SMT systems use blocks (i.e. phrase-
pairs) with a few real valued ?informative? features
which can be viewed as an indicator of how proba-
ble the current translation is. As discussed in Sec-
tion 1.2, these features are typically MLE models
(e.g. block translation, Model 1, language model,
etc.) whose scores are log-linearly combined using
a weight vector, ?f where f is a particular feature.
The ?f are trained using a held-out corpus using
maximum BLEU training (Och, 2003). This method
is only practical for a small number of features; typ-
ically, the number of features is on the order of 10 to
20.
Recently, there have been several discriminative
approaches at training large parameter sets includ-
ing (Tillmann and Zhang, 2006) and (Liang et al,
2006). In (Tillmann and Zhang, 2006) the model
is optimized to produce a block orientation and the
target sentence is used only for computing a sentence
level BLEU. (Liang et al, 2006) demonstrates a dis-
criminatively trained system for machine translation
that has the following characteristics: 1) requires a
varying update strategy (local vs. bold) depending
on whether the reference sentence is ?reachable? or
not, 2) uses sentence level BLEU as a criterion for se-
lecting which output to update towards, and 3) only
trains on limited length (5-15 words) sentences.
So both methods fundamentally rely on a prior
decoder to produce an ?N-best? list that is used to
find a target (using max BLEU) for the training al-
gorithm. The methods to produce an ?N-best? list
tend to be not very effective since most alternative
translations are minor differences from the highest
scoring translation and do not typically include the
reference translation (particularly when the system
makes a large error).
In this paper, the algorithm trains on all sentences
in the test-specific corpus and crucially, the algo-
rithm directly uses the target translation to update
the model parameters. This latter point is a critical
difference that contrasts to the major weakness of the
work of (Liang et al, 2006) which uses a top-N list of
translations to select the maximum BLEU sentence
as a target for training (so called local update).
3 A Categorization of Block Styles
In (Brown et al, 1993), multi-word ?cepts? (which
are realized in our block concept) are discussed and
the authors state that when a target sequence is
sufficiently different from a word by word transla-
tion, only then should the target sequence should
be promoted to a cept. This is in direct opposition
to phrase-based decoders which utilize all possible
phrase-pairs and limit the number of phrases only
due to practical considerations. Following the per-
spective of (Brown et al, 1993), a minimal set of
phrase blocks with lengths (m, n) where either m or
n must be greater than zero results in the following
types of blocks:
1. n = 0, source word producing nothing in the
target language (deletion block),
2. m = 0, spontaneous target word (insertion
block),
3. m = 1 and n ? 1, a source word producing n
target words including the possibility of a vari-
able (denoted by X) which is to be filled with
other blocks from the sentence (the latter case
called a discontiguous block)
4. m ? 1 and n = 1, a sequence of source words
producing a single target words including the
possibility of a variable on the source side (as in
the French ne...pas translating into not, called
multi-word singletons) in the source sequence
59
5. m > 1 and n > 1, a non-compositional phrase
translation
In this paper, we restrict the blocks to Types 1 and 3.
From the example in Figure 1, the following blocks
are extracted:
? lljnp ? of the X Committee
? Almrkzyp ? Central
? llHzb ? of the X Party
? Al$ywEy ? Communist
? AlSyny ? Chinese.
These blocks can now be considered more ?general?
and can be used to generate more phrases compared
to the blocks shown in Table 1. These blocks when
utilized independently of the remainder of the model
perform very poorly as all the advantages of blocks
are absent. These advantages are obtained using the
features to be described below. Also, we store with a
block additional information such as: (a) alignment
information, and (b) source and target analysis. The
target analysis includes part of speech and for each
target string a list of part of speech sequences are
stored along with their corpus frequencies.
The first alignment shown in Figure 1 is an exam-
ple of a Type 5 non-compositional block; although
this is not currently addressed by the decoder, we
plan to handle such blocks in the future.
4 Algorithm
A classification problem can be considered as a map-
ping from a set of histories, S, into a set of futures,
T . Traditional classification problems deal with a
small finite set of futures usually no more than a few
thousands of classes.
Machine translation can be cast into the same
framework with a much larger future space. In con-
trast to the current global models, we decompose the
process into a sequence of steps. The process begins
at the left edge of a sentence and for practical rea-
sons considers a window of source words that could
be translated. The first action is to jump a distance,
j to a source position and to produce a target string,
t corresponding to the source word at that position.
The process then marks the source position as hav-
ing been visited and iterates till all source words have
been visited. The only wrinkle in this relatively sim-
ple process is the presence of a variable in the tar-
get sequence. In the case of a variable, the source
position is marked as having been partially visited.
When a partially visited source position is visited
again, the target string to the right of the variable is
output and the process is iterated. The distortion or
jump from the previously translated source word, j
in training can vary widely due to automatic sentence
alignment that is used to create the parallel corpus.
To limit the sparseness created by these longer jumps
we cap the jump to a window of source words (-5 to 5
words) around the last translated source word; jumps
outside the window are treated as being to the edge
of the window.
We combine the above translation model with a
n-gram language model as in
p(T, j|S) =
?
i
p(ti, j|si)
?
?
i
?LMp(ti|ti?1, . . . , ti?n)+
?TMp(ti, j|si)
This mixing allows the use of language model built
from a very large monolingual corpus to be used with
a translation model which is built from a smaller
parallel corpus. In the rest of this paper, we are
concerned only with the translation model.
The minimum requirements for the algorithm are
(a) parallel corpus of source and target languages
and (b) word-alignments. While one can use the
EM algorithm to train this hidden alignment model
(the jump step), we use Viterbi training, i.e. we use
the most likely alignment between target and source
words in the training corpus to estimate this model.
We assume that each sentence pair in the training
corpus is word-aligned (e.g. using a MaxEnt aligner
(Ittycheriah and Roukos, 2005) or an HMM aligner
(Ge, 2004)). The algorithm performs the following
steps in order to train the maximum entropy model:
(a) block extraction, (b) feature extraction, and (c)
parameter estimation. Each of the first two steps
requires a pass over the training data and param-
eter estimation requires typically 5-10 passes over
the data. (Della Pietra et al, 1995) documents the
Improved Iterative Scaling (IIS) algorithm for train-
ing maximum entropy models. When the system is
restricted to 1-N type blocks, the future space in-
cludes all the source word positions that are within
the skip window and all their corresponding blocks.
The training algorithm at the parameter estimation
step can be concisely stated as:
1. For each sentence pair in the parallel corpus,
walk the alignment in source word order.
2. At each source word, the alignment identifies the
?true? block.
3. Form a window of source words and allow all
blocks at source words to generate at this gen-
eration point.
60
4. Apply the features relevant to each block and
compute the probability of each block.
5. Form the MaxEnt polynomials(Della Pietra et
al., 1995) and solve to find the update for each
feature.
We will next discuss the prior distribution used in
the maximum entropy model, the block extraction
method and the feature generation method and dis-
cuss differences with a standard phrase based de-
coder.
4.1 Prior Distribution
Maximum entropy models are of the form,
p(t, j|s) = p0(t, j|s)Z exp
?
i
?i?i(t, j, s)
where p0 is a prior distribution, Z is a normalizing
term, and ?i(t, j, s) are the features of the model.
The prior distribution can contain any information
we know about our future and in this work we utilize
the normalized phrase count as our prior. Strictly,
the prior has to be uniform on the set of futures to
be a ?maximum? entropy algorithm and choices of
other priors result in minimum divergence models.
We refer to both as a maximum entropy models.
The practical benefit of using normalized phrase
count as the prior distribution is for rare transla-
tions of a common source words. Such a translation
block may not have a feature due to restrictions in
the number of features in the model. Utilizing the
normalized phrase count prior, the model is still able
to penalize such translations. In the best case, a fea-
ture is present in the model and the model has the
freedom to either boost the translation probability
or to further reduce the prior.
4.2 Block Extraction
Similar to phrase decoders, a single pass is made
through the parallel corpus and for each source word,
the target sequence derived from the alignments
is extracted. The ?Inverse Projection Constraint?,
which requires that the target sequence be aligned
only to the source word or phrase in question, is then
checked to ensure that the phrase pair is consistent.
A slight relaxation is made to the traditional target
sequence in that variables are allowed if the length of
their span is 3 words or less. The length restriction is
imposed to reduce the effect of alignment errors. An
example of blocks extracted for the romanized ara-
bic words ?lljnp? and ?Almrkzyp? are shown Figure 2,
where on the left side are shown the unsegmented
Arabic words, the segmented Arabic stream and the
corresponding Arabic part-of-speech. On the right,
the target sequences are shown with the most fre-
quently occuring part-of-speech and the corpus count
of this block.
The extracted blocks are pruned in order to min-
imize alignment problems as well as optimize the
speed during decoding. Blocks are pruned if their
corpus count is a factor of 30 times smaller than the
most frequent target sequence for the same source
word. This results in about 1.6 million blocks from
an original size of 3.2 million blocks (note this is
much smaller than the 50 million blocks or so that
are derived in current phrase-based systems).
4.3 Features
The features investigated in this work are binary
questions about the lexical context both in the source
and target streams. These features can be classi-
fied into the following categories: (a) block internal
features, and (b) block context features. Features
can be designed that are specific to a block. Such
features are modeling the unigram phrase count of
the block, which is information already present in
the prior distribution as discussed above. Features
which are less specific are tied across many transla-
tions of the word. For example in Figure 2, the pri-
mary translation for ?lljnp? is ?committee? and occurs
920 times across all blocks extracted from the corpus;
the final block shown which is ?of the X committee?
occurs only 37 times but employs a lexical feature
?lljnp committee? which fires 920 times.
4.3.1 Lexical Features
Lexical features are block internal features which
examine a source word, a target word and the jump
from the previously translated source word. As dis-
cussed above, these are shared across blocks.
4.3.2 Lexical Context Features
Context features encode the context surrounding
a block by examining the previous and next source
word and the previous two target words. Unlike a
traditional phrase pair, which encodes all the infor-
mation lexically, in this approach we define in Ta-
ble 2, individual feature types to examine a por-
tion of the context. One or more of these features
may apply in each instance where a block is relevant.
The previous source word is defined as the previously
translated source word, but the next source word is
always the next word in the source string. At train-
ing time, the previously translated source word is
found by finding the previous target word and utiliz-
ing the alignment to find the previous source word.
If the previous target word is unaligned, no context
feature is applied.
61
committee/NN (613)
of the commission/IN DT NN (169)
the committee/DT NN (136)
commission/NN (135)
of the committee/IN DT NN (134)
the commission/DT NN (106)
of the HOLE committee/IN DT -1 NN(37)
central/NNP (731)
the central/DT JJ (504)
of the central/IN DT NNP(64)
the cia/DT NNP (58)
Almrkzyp
Al# mrkzy +p
DET ADJ NSUFF_FEM_SG
lljnp
l# ljn +p
PREP NOUN NSUFF_FEM_SG
Figure 2: Extracted blocks for ?lljnp? and ?Almrkzyp?.
Feature Name Feature variables
SRC LEFT source left, source word,
target word
SRC RIGHT source right, source word,
target word
SRC TGT LEFT source left, target left,
source word, target word
SRC TGT LEFT 2 source left, target left,
target left 2, source word,
target word
Table 2: Context Feature Types
4.3.3 Arabic Segmentation Features
An Arabic segmenter produces morphemes; in
Arabic, prefixes and suffixes are used as prepositions,
pronouns, gender and case markers. This produces a
segmentation view of the arabic source words (Lee et
al., 2003). The features used in the model are formed
from the Cartesian product of all segmentation to-
kens with the English target sequence produced by
this source word or words. However, prefixes and
suffixes which are specific in translation are limited
to their English translations. For example the pre-
fix ?Al#? is only allowed to participate in a feature
with the English word ?the? and similarly ?the? is not
allowed to participate in a feature with the stem of
the Arabic word. These restrictions limit the num-
ber of features and also reduce the over fitting by the
model.
4.3.4 Part-of-speech Features
Part-of-speech taggers were run on each language:
the English part of speech tagger is a MaxEnt tag-
ger built on the WSJ corpus and on the WSJ test
set achieves an accuracy of 96.8%; the Arabic part
of speech tagger is a similar tagger built on the Ara-
bic tree bank and achieves an accuracy of 95.7% on
automatically segmented data. The part of speech
feature type examines the source and target as well
as the previous target and the corresponding previ-
ous source part of speech. A separate feature type
examines the part of speech of the next source word
when the target sequence has a variable.
4.3.5 Coverage Features
These features examine the coverage status of the
source word to the left and the source word to the
right. During training, the coverage is determined
by examining the alignments; the source word to the
left is uncovered if its target sequence is to the right
of the current target sequence. Since the model em-
ploys binary questions and predominantly the source
word to the left is already covered and the right
source word is uncovered, these features fire only if
the left is open or if the right is closed in order to
minimize the number of features in the model.
5 Translation Decoder
A beam search decoder similar to phrase-based sys-
tems (Tillmann and Ney, 2003) is used to translate
the Arabic sentence into English. These decoders
have two parameters that control their search strat-
egy: (a) the skip length (how many positions are al-
lowed to be untranslated) and (b) the window width,
which controls how many words are allowed to be
considered for translation. Since the majority of the
blocks employed in this work do not encode local re-
ordering explicitly, the current DTM2 decoder uses
a large skip (4 source words for Arabic) and tries
all possible reorderings. The primary difference be-
tween a DTM2 decoder and standard phrase based
decoders is that the maximum entropy model pro-
vides a cost estimate of producing this translation
using the features described in previous sections. An-
other difference is that the DTM2 decoder handles
blocks with variables. When such a block is pro-
posed, the initial target sequence is first output and
the source word position is marked as being partially
visited and an index into which segment was gener-
ated is kept for completing the visit at a later time.
Subsequent extensions of this path can either com-
plete this visit or visit other source words. On a
search path, we make a further assumption that only
62
one source position can be in a partially visited state
at any point. This greatly reduces the search task
and suffices to handle the type of blocks encountered
in Arabic to English translation.
6 Experiments
The UN parallel corpus and the LDC news corpora
released as training data for the NIST MT06 eval-
uation are used for all evaluations presented in this
paper. A variety of test corpora are now available
and we use MT03 as development test data, and
test results are presented on MT05. Results obtained
on MT06 are from a blind evaluation. For Arabic-
English, the NIST MT06 training data contains 3.7M
sentence pairs from the UN from 1993-2002 and 100K
sentences pairs from news sources. This represents
the universe of training data, but for each test set
we sample this corpus to train efficiently while also
observing slight gains in performance. The training
universe is time sorted and the most recent corpora
are sampled first. Then for a given test set, we obtain
the first 20 instances of n-grams from the test that
occur in the training universe and the resulting sam-
pled sentences then form the training sample. The
contribution of the sampling technique is to produce
a smaller training corpus which reduces the compu-
tational load; however, the sampling of the universe
of sentences can be viewed as test set domain adapta-
tion which improves performance and is not strictly
done due to computational limitations2. The 5-gram
language model is trained from the English Gigaword
corpus and the English portion of the parallel corpus
used in the translation model training.
The baseline decoder is a phrase-based decoder
that employs n-m blocks and uses the same test set
specific training corpus described above.
6.1 Feature Type Experiments
There are 15 individual feature types utilized in the
system, but in order to be brief we present the re-
sults by feature groups (see Table 3): (a) lexical, (b)
lexical context, (c) segmentation, (d) part-of-speech,
and (e) coverage features. The results show im-
provements with the addition of each feature set, but
the part-of-speech features and coverage features are
not statistically significant improvements. The more
complex features based on Arabic segmentation and
English part-of-speech yield a small improvement of
0.5 BLEU points over the model with only lexical
context.
2Recent results indicate that test set adaptation by
test set sampling of the training corpus achieves a cased
Bleu of 53.26 on MT03 whereas a general system trained
on all data achieves only 51.02
Verb Placement 3
Missing Word 5
Extra Word 5
Word Choice 26
Word Order 3
Other error 1
Total 43
Table 4: Errors on last 25 sentences of MT-03.
7 Error Analysis and Discussion
We analyzed the errors in the last 25 sentences of the
MT-03 development data using the broad categories
shown in Table 4. These error types are not indepen-
dent of each other; indeed, incorrect verb placement
is just a special case of the word order error type
but for this error analysis for each error we take the
first category available in this list. Word choice er-
rors can be a result of (a) rare words with few, or
incorrect, or no translation blocks (4 times) or (b)
model weakness3 (22 times). In order to address the
model weakness type of errors, we plan on investigat-
ing feature selection using a language model prior.
As an example, consider an arabic word which pro-
duces both ?the? (due to alignment errors) and ?the
conduct?. An n-gram LM has very low cost for the
word ?the? but a rather high cost for content words
such as ?conduct?. Incorporating the LM model as a
prior should help the maximum entropy model focus
its weighting on the content word to overcome the
prior information.
8 Conclusion and Future Work
We have presented a complete direct translation
model with training of millions of parameters based
on a set of minimalist blocks and demonstrated the
ability to retain good performance relative to phrase
based decoders. Tied features minimize the num-
ber of parameters and help avoid the sparsity prob-
lems associated with phrase based decoders. Uti-
lizing language analysis of both the source and tar-
get languages adds 0.8 BLEU points on MT-03, and
0.4 BLEU points on MT-05. The DTM2 decoder
achieved a 1.7 BLEU point improvement over the
phrase based decoder on MT-06. In this work, we
have restricted the block types to only single source
word blocks. Many city names and dates in Ara-
bic can not be handled by such blocks and in future
work we intend to investigate the utilization of more
complex blocks as necessary. Also, the DTM2 de-
coder utilized the LM component independently of
3The word occurred with the correct translation in
the phrase library with a count more than 10 and yet the
system used an incorrect translation.
63
Feature Types # of feats MT-03 MT-05 MT-06
(MT03)
Training Size
Num. of Sentences 197K 267K 279K
Phrase-based Decoder 51.20 49.06 36.92
DTM2 Decoder
Lex Feats a 439,582 49.70 48.37
+Lex Context b 2,455,394 50.45 49.61
+Seg Feats c 2,563,338 50.97 49.96
+POS Feats d 2,608,352 51.27 49.93
+Cov Feats e 2,783,813 51.19 50.00 38.61
Table 3: Bleu scores on MT03-MT06.
the translation model; however, in future work we
intend to investigate feature selection using the lan-
guage model as a prior which should result in much
smaller systems.
9 Acknowledgements
This work was partially supported by the Department of
the Interior, National Business Center under contract No.
NBCH2030001 and Defense Advanced Research Projects
Agency under contract No. HR0011-06-2-0001. The
views and findings contained in this material are those
of the authors and do not necessarily reflect the position
or policy of the U.S. government and no official endorse-
ment should be inferred. This paper owes much to the
collaboration of the Statistical MT group at IBM.
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Distortion
models for statistical machine translation. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and 44th Annual Meeting of the
ACL, pages 529?536, Sydney, Australia.
Rafael Banchs, Josep M. Crego, Adria` de Gispert, Pa-
trik Lambert, and Jose? B. Marino. 2005. Statistical
machine translation of euparl data by using bilingual
n-grams. In Proc. of the ACL Workshop on Building
and Using Parallel Texts, pages 133?136, Ann Arbor,
Michigan, USA.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The Mathematics of Statistical Machine Translation:
Parameter Estimation. Computational Linguistics,
19(2):263?311.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the ACL, pages 263?270,
Ann Arbor, Michigan, June.
Stephen Della Pietra, Vincent Della Pietra, and John
Lafferty. 1995. Inducing features of random fields.
Technical Report, Department of Computer Science,
Carnegie-Mellon University, CMU-CS-95-144.
George Foster. 2000. A maximum entropy/minimum
divergence translation model. In 38th Annual Meeting
of the ACL, pages 45?52, Hong Kong.
Niyu Ge. 2004. Improvement in Word Alignments. Pre-
sentation given at DARPA/TIDES MT workshop.
Abraham Ittycheriah and Salim Roukos. 2005. A maxi-
mum entropy word aligner for arabic-english machine
translation. In HLT ?05: Proceedings of the HLT and
EMNLP, pages 89?96.
Young-Suk Lee, Kishore Papineni, and Salim Roukos.
2003. Language model based arabic word segmenta-
tion. In 41st Annual Meeting of the ACL, pages 399?
406, Sapporo, Japan.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the ACL, pages
761?768, Sydney, Australia.
Franz Josef Och and Hermann Ney. 2000. Statistical
machine translation. In EAMT Workshop, pages 39?
46, Ljubljana, Slovenia.
Franz-Josef Och and Hermann Ney. 2002. Discriminative
Training and Maximum Entropy Models for Statistical
Machine Translations. In 40th Annual Meeting of the
ACL, pages 295?302, Philadelphia, PA, July.
Franz Josef Och. 2003. Minimum error rate training in
Statistical Machine Translation. In 41st Annual Meet-
ing of the ACL, pages 160?167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, and R. T. Ward.
1997. Feature-based language understanding. In EU-
ROSPEECH, pages 1435?1438, Rhodes,Greece.
Kishore Papineni, Salim Roukos, and R. T. Ward. 1998.
Maximum likelihood and discriminative training of di-
rect translation models. In International Conf. on
Acoustics, Speech and Signal Processing, pages 189?
192, Seattle, WA.
Chris Quirk and Arul Menezes. 2006. Do we need
phrases? challenging the conventional wisdom in sta-
tistical machine translation. In Proceedings of the Hu-
man Language Technology Conference of the NAACL,
pages 9?16, New York, NY, USA.
Christoph Tillmann and Hermann Ney. 2003. Word re-
ordering and a dynamic programming beam search al-
gorithm for Statistical Machine Translation. 29(1):97?
133.
Christoph Tillmann and Tong Zhang. 2006. A discrimi-
native global training algorithm for statistical mt. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meeting
of the ACL, pages 721?728, Sydney, Australia.
64
tRuEcasIng
Lucian Vlad Lita ?
Carnegie Mellon
llita@cs.cmu.edu
Abe Ittycheriah
IBM T.J. Watson
abei@us.ibm.com
Salim Roukos
IBM T.J. Watson
roukos@us.ibm.com
Nanda Kambhatla
IBM T.J. Watson
nanda@us.ibm.com
Abstract
Truecasing is the process of restoring
case information to badly-cased or non-
cased text. This paper explores truecas-
ing issues and proposes a statistical, lan-
guage modeling based truecaser which
achieves an accuracy of ?98% on news
articles. Task based evaluation shows a
26% F-measure improvement in named
entity recognition when using truecasing.
In the context of automatic content ex-
traction, mention detection on automatic
speech recognition text is also improved
by a factor of 8. Truecasing also en-
hances machine translation output legibil-
ity and yields a BLEU score improvement
of 80.2%. This paper argues for the use of
truecasing as a valuable component in text
processing applications.
1 Introduction
While it is true that large, high quality text corpora
are becoming a reality, it is also true that the digital
world is flooded with enormous collections of low
quality natural language text. Transcripts from var-
ious audio sources, automatic speech recognition,
optical character recognition, online messaging and
gaming, email, and the web are just a few exam-
ples of raw text sources with content often produced
in a hurry, containing misspellings, insertions, dele-
tions, grammatical errors, neologisms, jargon terms
? Work done at IBM TJ Watson Research Center
etc. We want to enhance the quality of such sources
in order to produce better rule-based systems and
sharper statistical models.
This paper focuses on truecasing, which is the
process of restoring case information to raw text.
Besides text rEaDaBILiTY, truecasing enhances the
quality of case-carrying data, brings into the pic-
ture new corpora originally considered too noisy for
various NLP tasks, and performs case normalization
across styles, sources, and genres.
Consider the following mildly ambiguous sen-
tence ?us rep. james pond showed up riding an it
and going to a now meeting?. The case-carrying al-
ternative ?US Rep. James Pond showed up riding an
IT and going to a NOW meeting? is arguably better
fit to be subjected to further processing.
Broadcast news transcripts contain casing errors
which reduce the performance of tasks such as
named entity tagging. Automatic speech recognition
produces non-cased text. Headlines, teasers, section
headers - which carry high information content - are
not properly cased for tasks such as question answer-
ing. Truecasing is an essential step in transforming
these types of data into cleaner sources to be used by
NLP applications.
?the president? and ?the President? are two viable
surface forms that correctly convey the same infor-
mation in the same context. Such discrepancies are
usually due to differences in news source, authors,
and stylistic choices. Truecasing can be used as a
normalization tool across corpora in order to pro-
duce consistent, context sensitive, case information;
it consistently reduces expressions to their statistical
canonical form.
In this paper, we attempt to show the benefits of
truecasing in general as a valuable building block
for NLP applications rather than promoting a spe-
cific implementation. We explore several truecasing
issues and propose a statistical, language modeling
based truecaser, showing its performance on news
articles. Then, we present a straight forward appli-
cation of truecasing on machine translation output.
Finally, we demonstrate the considerable benefits of
truecasing through task based evaluations on named
entity tagging and automatic content extraction.
1.1 Related Work
Truecasing can be viewed in a lexical ambiguity res-
olution framework (Yarowsky, 1994) as discriminat-
ing among several versions of a word, which hap-
pen to have different surface forms (casings). Word-
sense disambiguation is a broad scope problem that
has been tackled with fairly good results generally
due to the fact that context is a very good pre-
dictor when choosing the sense of a word. (Gale
et al, 1994) mention good results on limited case
restoration experiments on toy problems with 100
words. They also observe that real world problems
generally exhibit around 90% case restoration accu-
racy. (Mikheev, 1999) also approaches casing dis-
ambiguation but models only instances when capi-
talization is expected: first word in a sentence, after
a period, and after quotes. (Chieu and Ng, 2002)
attempted to extract named entities from non-cased
text by using a weaker classifier but without focus-
ing on regular text or case restoration.
Accents can be viewed as additional surface forms
or alternate word casings. From this perspective, ei-
ther accent identification can be extended to truecas-
ing or truecasing can be extended to incorporate ac-
cent restoration. (Yarowsky, 1994) reports good re-
sults with statistical methods for Spanish and French
accent restoration.
Truecasing is also a specialized method for
spelling correction by relaxing the notion of casing
to spelling variations. There is a vast literature on
spelling correction (Jones and Martin, 1997; Gold-
ing and Roth, 1996) using both linguistic and statis-
tical approaches. Also, (Brill and Moore, 2000) ap-
ply a noisy channel model, based on generic string
to string edits, to spelling correction.
2 Approach
In this paper we take a statistical approach to true-
casing. First we present the baseline: a simple,
straight forward unigram model which performs rea-
sonably well in most cases. Then, we propose a bet-
ter, more flexible statistical truecaser based on lan-
guage modeling.
From a truecasing perspective we observe four
general classes of words: all lowercase (LC), first
letter uppercase (UC), all letters uppercase (CA), and
mixed case word MC). The MC class could be fur-
ther refined into meaningful subclasses but for the
purpose of this paper it is sufficient to correctly iden-
tify specific true MC forms for each MC instance.
We are interested in correctly assigning case la-
bels to words (tokens) in natural language text. This
represents the ability to discriminate between class
labels for the same lexical item, taking into account
the surrounding words. We are interested in casing
word combinations observed during training as well
as new phrases. The model requires the ability to
generalize in order to recognize that even though the
possibly misspelled token ?lenon? has never been
seen before, words in the same context usually take
the UC form.
2.1 Baseline: The Unigram Model
The goal of this paper is to show the benefits of true-
casing in general. The unigram baseline (presented
below) is introduced in order to put task based eval-
uations in perspective and not to be used as a straw-
man baseline.
The vast majority of vocabulary items have only
one surface form. Hence, it is only natural to adopt
the unigram model as a baseline for truecasing. In
most situations, the unigram model is a simple and
efficient model for surface form restoration. This
method associates with each surface form a score
based on the frequency of occurrence. The decoding
is very simple: the true case of a token is predicted
by the most likely case of that token.
The unigram model?s upper bound on truecasing
performance is given by the percentage of tokens
that occur during decoding under their most frequent
case. Approximately 12% of the vocabulary items
have been observed under more than one surface
form. Hence it is inevitable for the unigram model
to fail on tokens such as ?new?. Due to the over-
whelming frequency of its LC form, ?new? will take
this particular form regardless of what token follows
it. For both ?information? and ?york? as subsequent
words, ?new? will be labeled as LC. For the latter
case, ?new? occurs under one of its less frequent sur-
face forms.
2.2 Truecaser
The truecasing strategy that we are proposing seeks
to capture local context and bootstrap it across a
sentence. The case of a token will depend on the
most likely meaning of the sentence - where local
meaning is approximated by n-grams observed dur-
ing training. However, the local context of a few
words alone is not enough for case disambiguation.
Our proposed method employs sentence level con-
text as well.
We capture local context through a trigram lan-
guage model, but the case label is decided at a sen-
tence level. A reasonable improvement over the un-
igram model would have been to decide the word
casing given the previous two lexical items and their
corresponding case content. However, this greedy
approach still disregards global cues. Our goal is
to maximize the probability of a larger text segment
(i.e. a sentence) occurring under a certain surface
form. Towards this goal, we first build a language
model that can provide local context statistics.
2.2.1 Building a Language Model
Language modeling provides features for a label-
ing scheme. These features are based on the prob-
ability of a lexical item and a case content condi-
tioned on the history of previous two lexical items
and their corresponding case content:
Pmodel(w3|w2, w1) = ?trigramP (w3|w2, w1)
+ ?bigramP (w3|w2)
+ ?unigramP (w3)
+ ?uniformP0 (1)
where trigram, bigram, unigram, and uniform prob-
abilities are scaled by individual ?is which are
learned by observing training examples. wi repre-
sents a word with a case tag treated as a unit for
probability estimation.
2.2.2 Sentence Level Decoding
Using the language model probabilities we de-
code the case information at a sentence level. We
construct a trellis (figure 1) which incorporates all
the sentence surface forms as well as the features
computed during training. A node in this trellis con-
sists of a lexical item, a position in the sentence, a
possible casing, as well as a history of the previous
two lexical items and their corresponding case con-
tent. Hence, for each token, all surface forms will
appear as nodes carrying additional context infor-
mation. In the trellis, thicker arrows indicate higher
transition probabilities.
Figure 1: Given individual histories, the decodings
delay and DeLay, are most probable - perhaps in the
context of ?time delay? and respectively ?Senator
Tom DeLay?
The trellis can be viewed as a Hidden Markov
Model (HMM) computing the state sequence
which best explains the observations. The states
(q1, q2, ? ? ? , qn) of the HMM are combinations of
case and context information, the transition proba-
bilities are the language model (?) based features,
and the observations (O1O2 ? ? ?Ot) are lexical items.
During decoding, the Viterbi algorithm (Rabiner,
1989) is used to compute the highest probability
state sequence (q?? at sentence level) that yields the
desired case information:
q?? = argmaxqi1qi2???qitP (qi1qi2 ? ? ? qit|O1O2 ? ? ?Ot, ?)
(2)
where P (qi1qi2 ? ? ? qit|O1O2 ? ? ?Ot, ?) is the proba-
bility of a given sequence conditioned on the obser-
vation sequence and the model parameters. A more
sophisticated approach could be envisioned, where
either the observations or the states are more expres-
sive. These alternate design choices are not explored
in this paper.
Testing speed depends on the width and length of
the trellis and the overall decoding complexity is:
Cdecoding = O(SMH+1) where S is the sentence
size, M is the number of surface forms we are will-
ing to consider for each word, and H is the history
size (H = 3 in the trigram case).
2.3 Unknown Words
In order for truecasing to be generalizable it must
deal with unknown words ? words not seen during
training. For large training sets, an extreme assump-
tion is that most words and corresponding casings
possible in a language have been observed during
training. Hence, most new tokens seen during de-
coding are going to be either proper nouns or mis-
spellings. The simplest strategy is to consider all
unknown words as being of the UC form (i.e. peo-
ple?s names, places, organizations).
Another approach is to replace the less frequent
vocabulary items with case-carrying special tokens.
During training, the word mispeling is replaced with
by UNKNOWN LC and the word Lenon with UN-
KNOWN UC. This transformation is based on the
observation that similar types of infrequent words
will occur during decoding. This transformation cre-
ates the precedent of unknown words of a particular
format being observed in a certain context. When a
truly unknown word will be seen in the same con-
text, the most appropriate casing will be applied.
This was the method used in our experiments. A
similar method is to apply the case-carrying special
token transformation only to a small random sam-
ple of all tokens, thus capturing context regardless
of frequency of occurrence.
2.4 Mixed Casing
A reasonable truecasing strategy is to focus on to-
ken classification into three categories: LC, UC, and
CA. In most text corpora mixed case tokens such as
McCartney, CoOl, and TheBeatles occur with mod-
erate frequency. Some NLP tasks might prefer map-
ping MC tokens starting with an uppercase letter into
the UC surface form. This technique will reduce the
feature space and allow for sharper models. How-
ever, the decoding process can be generalized to in-
clude mixed cases in order to find a closer fit to the
true sentence. In a clean version of the AQUAINT
(ARDA) news stories corpus, ? 90% of the tokens
occurred under the most frequent surface form (fig-
ure 2).
Figure 2: News domain casing distribution
The expensive brute force approach will consider
all possible casings of a word. Even with the full
casing space covered, some mixed cases will not be
seen during training and the language model prob-
abilities for n-grams containing certain words will
back off to an unknown word strategy. A more fea-
sible method is to account only for the mixed case
items observed during training, relying on a large
enough training corpus. A variable beam decod-
ing will assign non-zero probabilities to all known
casings of each word. An n-best approximation is
somewhat faster and easier to implement and is the
approach employed in our experiments. During the
sentence-level decoding only the n-most-frequent
mixed casings seen during training are considered.
If the true capitalization is not among these n-best
versions, the decoding is not correct. Additional lex-
ical and morphological features might be needed if
identifying MC instances is critical.
2.5 First Word in the Sentence
The first word in a sentence is generally under the
UC form. This sentence-begin indicator is some-
times ambiguous even when paired with sentence-
end indicators such as the period. While sentence
splitting is not within the scope of this paper, we
want to emphasize the fact that many NLP tasks
would benefit from knowing the true case of the first
word in the sentence, thus avoiding having to learn
the fact that beginning of sentences are artificially
important. Since it is uneventful to convert the first
letter of a sentence to uppercase, a more interest-
ing problem from a truecasing perspective is to learn
how to predict the correct case of the first word in a
sentence (i.e. not always UC).
If the language model is built on clean sentences
accounting for sentence boundaries, the decoding
will most likely uppercase the first letter of any sen-
tence. On the other hand, if the language model
is trained on clean sentences disregarding sentence
boundaries, the model will be less accurate since dif-
ferent casings will be presented for the same context
and artificial n-grams will be seen when transition-
ing between sentences. One way to obtain the de-
sired effect is to discard the first n tokens in the train-
ing sentences in order to escape the sentence-begin
effect. The language model is then built on smoother
context. A similar effect can be obtained by initial-
izing the decoding with n-gram state probabilities so
that the boundary information is masked.
3 Evaluation
Both the unigram model and the language model
based truecaser were trained on the AQUAINT
(ARDA) and TREC (NIST) corpora, each consist-
ing of 500M token news stories from various news
agencies. The truecaser was built using IBM?s
ViaVoiceTMlanguage modeling tools. These tools
implement trigram language models using deleted
interpolation for backing off if the trigram is not
found in the training data. The resulting model?s
perplexity is 108.
Since there is no absolute truth when truecasing a
sentence, the experiments need to be built with some
reference in mind. Our assumption is that profes-
sionally written news articles are very close to an
intangible absolute truth in terms of casing. Fur-
thermore, we ignore the impact of diverging stylistic
forms, assuming the differences are minor.
Based on the above assumptions we judge the
truecasing methods on four different test sets. The
first test set (APR) consists of the August 25,
2002 ? top 20 news stories from Associated Press
and Reuters excluding titles, headlines, and sec-
tion headers which together form the second test set
(APR+). The third test set (ACE) consists of ear-
?Randomly chosen test date
Figure 3: LM truecaser vs. unigram baseline.
lier news stories from AP and New York Times be-
longing to the ACE dataset. The last test set (MT)
includes a set of machine translation references (i.e.
human translations) of news articles from the Xin-
hua agency. The sizes of the data sets are as follows:
APR - 12k tokens, ACE - 90k tokens, and MT - 63k
tokens. For both truecasing methods, we computed
the agreement with the original news story consid-
ered to be the ground truth.
3.1 Results
The language model based truecaser consistently
displayed a significant error reduction in case
restoration over the unigram model (figure 3). On
current news stories, the truecaser agreement with
the original articles is ? 98%.
Titles and headlines usually have a higher con-
centration of named entities than normal text. This
also means that they need a more complex model to
assign case information more accurately. The LM
based truecaser performs better in this environment
while the unigram model misses named entity com-
ponents which happen to have a less frequent surface
form.
3.2 Qualitative Analysis
The original reference articles are assumed to have
the absolute true form. However, differences from
these original articles and the truecased articles are
not always casing errors. The truecaser tends to
modify the first word in a quotation if it is not
proper name: ?There has been? becomes ?there has
been?. It also makes changes which could be con-
sidered a correction of the original article: ?Xinhua
BLEU Breakdown
System BLEU 1gr Precision 2gr Precision 3gr Precision 4gr Precision
all lowercase 0.1306 0.6016 0.2294 0.1040 0.0528
rule based 0.1466 0.6176 0.2479 0.1169 0.0627
1gr truecasing 0.2206 0.6948 0.3328 0.1722 0.0988
1gr truecasing+ 0.2261 0.6963 0.3372 0.1734 0.0997
lm truecasing 0.2596 0.7102 0.3635 0.2066 0.1303
lm truecasing+ 0.2642 0.7107 0.3667 0.2066 0.1302
Table 1: BLEU score for several truecasing strategies. (truecasing+ methods additionally employ the ?first
sentence letter uppercased? rule adjustment).
Baseline With Truecasing
Class Recall Precision F Recall Precision F
ENAMEX 48.46 36.04 41.34 59.02 52.65 55.66 (+34.64%)
NUMEX 64.61 72.02 68.11 70.37 79.51 74.66 (+9.62%)
TIMEX 47.68 52.26 49.87 61.98 75.99 68.27 (+36.90%)
Overall 52.50 44.84 48.37 62.01 60.42 61.20 (+26.52%)
Table 2: Named Entity Recognition performance with truecasing and without (baseline).
news agency? becomes ?Xinhua News Agency? and
?northern alliance? is truecased as ?Northern Al-
liance?. In more ambiguous cases both the original
version and the truecased fragment represent differ-
ent stylistic forms: ?prime minister Hekmatyar? be-
comes ?Prime Minister Hekmatyar?.
There are also cases where the truecaser described
in this paper makes errors. New movie names are
sometimes miss-cased: ?my big fat greek wedding?
or ?signs?. In conducive contexts, person names
are correctly cased: ?DeLay said in?. However, in
ambiguous, adverse contexts they are considered to
be common nouns: ?pond? or ?to delay that?. Un-
seen organization names which make perfectly nor-
mal phrases are erroneously cased as well: ?interna-
tional security assistance force?.
3.3 Application: Machine Translation
Post-Processing
We have applied truecasing as a post-processing step
to a state of the art machine translation system in or-
der to improve readability. For translation between
Chinese and English, or Japanese and English, there
is no transfer of case information. In these situations
the translation output has no case information and it
is beneficial to apply truecasing as a post-processing
step. This makes the output more legible and the
system performance increases if case information is
required.
We have applied truecasing to Chinese-to-English
translation output. The data source consists of news
stories (2500 sentences) from the Xinhua News
Agency. The news stories are first translated, then
subjected to truecasing. The translation output is
evaluated with BLEU (Papineni et al, 2001), which
is a robust, language independent automatic ma-
chine translation evaluation method. BLEU scores
are highly correlated to human judges scores, pro-
viding a way to perform frequent and accurate au-
tomated evaluations. BLEU uses a modified n-gram
precision metric and a weighting scheme that places
more emphasis on longer n-grams.
In table 1, both truecasing methods are applied to
machine translation output with and without upper-
casing the first letter in each sentence. The truecas-
ing methods are compared against the all letters low-
ercased version of the articles as well as against an
existing rule-based system which is aware of a lim-
ited number of entity casings such as dates, cities,
and countries. The LM based truecaser is very ef-
fective in increasing the readability of articles and
captures an important aspect that the BLEU score is
sensitive to. Truecasig the translation output yields
Baseline With Truecasing
Source Recall Precision F Recall Precision F
BNEWS ASR 23 3 5 56 39 46 (+820.00%)
BNEWS HUMAN 77 66 71 77 68 72 (+1.41%)
XINHUA 76 71 73 79 72 75 (+2.74%)
Table 3: Results of ACE mention detection with and without truecasing.
an improvement ? of 80.2% in BLEU score over the
existing rule base system.
3.4 Task Based Evaluation
Case restoration and normalization can be employed
for more complex tasks. We have successfully lever-
aged truecasing in improving named entity recogni-
tion and automatic content extraction.
3.4.1 Named Entity Tagging
In order to evaluate the effect of truecasing on ex-
tracting named entity labels, we tested an existing
named entity system on a test set that has signif-
icant case mismatch to the training of the system.
The base system is an HMM based tagger, similar
to (Bikel et al, 1997). The system has 31 semantic
categories which are extensions on the MUC cate-
gories. The tagger creates a lattice of decisions cor-
responding to tokenized words in the input stream.
When tagging a word wi in a sentence of words
w0...wN , two possibilities. If a tag begins:
p(tN1 |wN1 )i = p(ti|ti?1, wi?1)p?(wi|ti, wi?1)
If a tag continues:
p(tN1 |wN1 )i = p(wi|ti, wi?1)
The ? indicates that the distribution is formed from
words that are the first words of entities. The p? dis-
tribution predicts the probability of seeing that word
given the tag and the previous word instead of the
tag and previous tag. Each word has a set of fea-
tures, some of which indicate the casing and embed-
ded punctuation. These models have several levels
of back-off when the exact trigram has not been seen
in training. A trellis spanning the 31 futures is built
for each word in a sentence and the best path is de-
rived using the Viterbi algorithm.
?Truecasing improves legibility, not the translation itself
The performance of the system shown in table 2
indicate an overall 26.52% F-measure improvement
when using truecasing. The alternative to truecas-
ing text is to destroy case information in the train-
ing material 	 SNORIFY procedure in (Bikel et al,
1997). Case is an important feature in detecting
most named entities but particularly so for the title
of a work, an organization, or an ambiguous word
with two frequent cases. Truecasing the sentence is
essential in detecting that ?To Kill a Mockingbird? is
the name of a book, especially if the quotation marks
are left off.
3.4.2 Automatic Content Extraction
Automatic Content Extraction (ACE) is task fo-
cusing on the extraction of mentions of entities and
relations between them from textual data. The tex-
tual documents are from newswire, broadcast news
with text derived from automatic speech recognition
(ASR), and newspaper with text derived from optical
character recognition (OCR) sources. The mention
detection task (ace, 2001) comprises the extraction
of named (e.g. ?Mr. Isaac Asimov?), nominal (e.g.
?the complete author?), and pronominal (e.g. ?him?)
mentions of Persons, Organizations, Locations, Fa-
cilities, and Geo-Political Entities.
The automatically transcribed (using ASR) broad-
cast news documents and the translated Xinhua
News Agency (XINHUA) documents in the ACE
corpus do not contain any case information, while
human transcribed broadcast news documents con-
tain casing errors (e.g. ?George bush?). This prob-
lem occurs especially when the data source is noisy
or the articles are poorly written.
For all documents from broadcast news (human
transcribed and automatically transcribed) and XIN-
HUA sources, we extracted mentions before and af-
ter applying truecasing. The ASR transcribed broad-
cast news data comprised 86 documents containing
a total of 15,535 words, the human transcribed ver-
sion contained 15,131 words. There were only two
XINHUA documents in the ACE test set containing
a total of 601 words. None of this data or any ACE
data was used for training the truecasing models.
Table 3 shows the result of running our ACE par-
ticipating maximum entropy mention detection sys-
tem on the raw text, as well as on truecased text. For
ASR transcribed documents, we obtained an eight
fold improvement in mention detection from 5% F-
measure to 46% F-measure. The low baseline score
is mostly due to the fact that our system has been
trained on newswire stories available from previous
ACE evaluations, while the latest test data included
ASR output. It is very likely that the improvement
due to truecasing will be more modest for the next
ACE evaluation when our system will be trained on
ASR output as well.
4 Possible Improvements & Future Work
Although the statistical model we have considered
performs very well, further improvements must go
beyond language modeling, enhancing how expres-
sive the model is. Additional features are needed
during decoding to capture context outside of the
current lexical item, medium range context, as well
as discontinuous context. Another potentially help-
ful feature to consider would provide a distribu-
tion over similar lexical items, perhaps using an
edit/phonetic distance.
Truecasing can be extended to cover a more gen-
eral notion surface form to include accents. De-
pending on the context, words might take different
surface forms. Since punctuation is a notion exten-
sion to surface form, shallow punctuation restora-
tion (e.g. word followed by comma) can also be ad-
dressed through truecasing.
5 Conclusions
We have discussed truecasing, the process of restor-
ing case information to badly-cased or non-cased
text, and we have proposed a statistical, language
modeling based truecaser which has an agreement
of ?98% with professionally written news articles.
Although its most direct impact is improving legibil-
ity, truecasing is useful in case normalization across
styles, genres, and sources. Truecasing is a valu-
able component in further natural language process-
ing. Task based evaluation shows a 26% F-measure
improvement in named entity recognition when us-
ing truecasing. In the context of automatic content
extraction, mention detection on automatic speech
recognition text is improved by a factor of 8. True-
casing also enhances machine translation output leg-
ibility and yields a BLEU score improvement of
80.2% over the original system.
References
2001. Entity detection and tracking. ACE Pilot Study
Task Definition.
D. Bikel, S. Miller, R. Schwartz, and R. Weischedel.
1997. Nymble: A high-performance learning name
finder. pages 194?201.
E. Brill and R. C. Moore. 2000. An improved error
model for noisy channel spelling correction. ACL.
H.L. Chieu and H.T. Ng. 2002. Teaching a weaker clas-
sifier: Named entity recognition on upper case text.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1994. Discrimination decisions for
100,000-dimensional spaces. Current Issues in Com-
putational Linguistics, pages 429?450.
Andrew R. Golding and Dan Roth. 1996. Applying win-
now to context-sensitive spelling correction. ICML.
M. P. Jones and J. H. Martin. 1997. Contextual spelling
correction using latent semantic analysis. ANLP.
A. Mikheev. 1999. A knowledge-free method for capi-
talized word disambiguation.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. IBM Research Re-
port.
L. R. Rabiner. 1989. A tutorial on hidden markov models
and selected applications in speech recognition. Read-
ings in Speech Recognition, pages 267?295.
David Yarowsky. 1994. Decision lists for ambiguity res-
olution: Application to accent restoration in spanish
and french. ACL, pages 88?95.
A Mention-Synchronous Coreference Resolution Algorithm Based on the
Bell Tree
Xiaoqiang Luo and Abe Ittycheriah
Hongyan Jing and Nanda Kambhatla and Salim Roukos
1101 Kitchawan Road
Yorktown Heights, NY 10598, U.S.A.
{xiaoluo,abei,hjing,nanda,roukos}@us.ibm.com
Abstract
This paper proposes a new approach for
coreference resolution which uses the Bell
tree to represent the search space and casts
the coreference resolution problem as finding
the best path from the root of the Bell tree to
the leaf nodes. A Maximum Entropy model
is used to rank these paths. The coreference
performance on the 2002 and 2003 Auto-
matic Content Extraction (ACE) data will be
reported. We also train a coreference system
using the MUC6 data and competitive results
are obtained.
1 Introduction
In this paper, we will adopt the terminologies used in
the Automatic Content Extraction (ACE) task (NIST,
2003). Coreference resolution in this context is defined
as partitioning mentions into entities. A mention is an
instance of reference to an object, and the collection
of mentions referring to the same object in a document
form an entity. For example, in the following sentence,
mentions are underlined:
?The American Medical Association voted
yesterday to install the heir apparent as its
president-elect, rejecting a strong, upstart
challenge by a District doctor who argued
that the nation?s largest physicians? group
needs stronger ethics and new leadership.?
?American Medical Association?, ?its? and ?group?
belong to the same entity as they refer to the same ob-
ject.
Early work of anaphora resolution focuses on find-
ing antecedents of pronouns (Hobbs, 1976; Ge et al,
1998; Mitkov, 1998), while recent advances (Soon et
al., 2001; Yang et al, 2003; Ng and Cardie, 2002; Itty-
cheriah et al, 2003) employ statistical machine learn-
ing methods and try to resolve reference among all
kinds of noun phrases (NP), be it a name, nominal, or
pronominal phrase ? which is the scope of this paper
as well. One common strategy shared by (Soon et al,
2001; Ng and Cardie, 2002; Ittycheriah et al, 2003) is
that a statistical model is trained to measure how likely
a pair of mentions corefer; then a greedy procedure is
followed to group mentions into entities. While this ap-
proach has yielded encouraging results, the way men-
tions are linked is arguably suboptimal in that an instant
decision is made when considering whether two men-
tions are linked or not.
In this paper, we propose to use the Bell tree to rep-
resent the process of forming entities from mentions.
The Bell tree represents the search space of the coref-
erence resolution problem ? each leaf node corresponds
to a possible coreference outcome. We choose to model
the process from mentions to entities represented in the
Bell tree, and the problem of coreference resolution is
cast as finding the ?best? path from the root node to
leaves. A binary maximum entropy model is trained to
compute the linking probability between a partial entity
and a mention.
The rest of the paper is organized as follows. In
Section 2, we present how the Bell tree can be used
to represent the process of creating entities from men-
tions and the search space. We use a maximum en-
tropy model to rank paths in the Bell tree, which is dis-
cussed in Section 3. After presenting the search strat-
egy in Section 4, we show the experimental results on
the ACE 2002 and 2003 data, and the Message Under-
standing Conference (MUC) (MUC, 1995) data in Sec-
tion 5. We compare our approach with some recent
work in Section 6.
2 Bell Tree: From Mention to Entity
Let us consider traversing mentions in a document from
beginning (left) to end (right). The process of form-
ing entities from mentions can be represented by a tree
structure. The root node is the initial state of the pro-
cess, which consists of a partial entity containing the
first mention of a document. The second mention is
[1][2] 3*
[1][2][3]
[1] [23]
[13][2]
[123]
[12][3]
[1] 2* 3
[1]
[12]
[1]
[2]
(c1)
(c5)
(b1)
(c2)
(c3)
(c4)
(a) [12] 3*
(b2)
Figure 1: Bell tree representation for three mentions:
numbers in [] denote a partial entity. In-focus entities
are marked on the solid arrows, and active mentions
are marked by *. Solid arrows signify that a mention
is linked with an in-focus partial entity while dashed
arrows indicate starting of a new entity.
added in the next step by either linking to the exist-
ing entity, or starting a new entity. A second layer
of nodes are created to represent the two possible out-
comes. Subsequent mentions are added to the tree in
the same manner. The process is mention-synchronous
in that each layer of tree nodes are created by adding
one mention at a time. Since the number of tree leaves
is the number of possible coreference outcomes and it
equals the Bell Number (Bell, 1934), the tree is called
the Bell tree. The Bell Number
 
is the num-
ber of ways of partitioning

distinguishable objects
(i.e., mentions) into non-empty disjoint subsets (i.e.,
entities). The Bell Number has a ?closed? formula
 
	


and it increases rapidly as

in-
creases:
 Named Entity Recognition through Classifier Combination
Radu Florian and Abe Ittycheriah and Hongyan Jing and Tong Zhang
IBM T.J. Watson Research Center
1101 Kitchawan Rd, Yorktown Heights, NY 10598, USA
{raduf,abei,hjing,tzhang}@us.ibm.com
Abstract
This paper presents a classifier-combination
experimental framework for named entity
recognition in which four diverse classi-
fiers (robust linear classifier, maximum en-
tropy, transformation-based learning, and hid-
den Markov model) are combined under differ-
ent conditions. When no gazetteer or other ad-
ditional training resources are used, the com-
bined system attains a performance of 91.6F
on the English development data; integrat-
ing name, location and person gazetteers, and
named entity systems trained on additional,
more general, data reduces the F-measure error
by a factor of 15 to 21% on the English data.
1 Introduction
This paper investigates the combination of a set of di-
verse statistical named entity classifiers, including a
rule-based classifier ? the transformation-based learning
classifier (Brill, 1995; Florian and Ngai, 2001, hence-
forth fnTBL) with the forward-backward extension de-
scribed in Florian (2002a), a hidden Markov model clas-
sifier (henceforth HMM), similar to the one described
in Bikel et al (1999), a robust risk minimization classi-
fier, based on a regularized winnow method (Zhang et al,
2002) (henceforth RRM) and a maximum entropy clas-
sifier (Darroch and Ratcliff, 1972; Berger et al, 1996;
Borthwick, 1999) (henceforth MaxEnt). This particular
set of classifiers is diverse across multiple dimensions,
making it suitable for combination:
? fnTBL is a discriminant classifier ? it bases its clas-
sification decision only on the few most discriminant
features active on an example ? while HMM, RRM
and MaxEnt are agglomerative classifiers ? their de-
cision is based on the combination of all features ac-
tive for the particular example.
? In dealing with the data sparseness problem, fnTBL,
MaxEnt and RRM investigate and integrate in their
decision arbitrary feature types, while HMM is de-
pendent on a prespecified back-off path.
? The search methods employed by each classifier are
different: the HMM, MaxEnt and RRM classifiers
construct a model for each example and then rely
on a sequence search such as the Viterbi algorithm
(Viterbi, 1967) to identify the best overall sequence,
while fnTBL starts with most frequent classification
(usually per token), and then dynamically models
the interaction between classifications, effectively
performing the search at training time.
? The classifiers also differ in their output: fnTBL
and RRM return a single classification per exam-
ple1, while the MaxEnt and HMM classifiers return
a probability distribution.
The remainder of the paper is organized as follows: Sec-
tion 2 describes the features used by the classifiers, Sec-
tion 3 briefly describes the algorithms used by each clas-
sifier, and Section 4 analyzes in detail the results obtained
by each classifier and their combination.
2 The Classification Method and Features
Used
All algorithms described in this paper identify the named
entities in the text by labeling each word with a tag
corresponding to its position relative to a named entity:
whether it starts/continues/ends a specific named entity,
or does not belong to any entity. RRM, MaxEnt, and
fnTBL treat the problem entirely as a tagging task, while
the HMM algorithm used here is constraining the transi-
tions between the various phases, similar to the method
described in (Bikel et al, 1999).
Feature design and integration is of utmost importance
in the overall classifier design ? a rich feature space is the
key to good performance. Often, high performing classi-
fiers operating in an impoverished space are surpassed by
a lower performing classifier when the latter has access
to enhanced feature spaces (Zhang et al, 2002; Florian,
1 However, both classifiers? algorithms can be modified such
that a class probability distribution is returned instead.
2002a). In accordance with this observation, the clas-
sifiers used in this research can access a diverse set of
features when examining a word in context, including:
? words and their lemmas in a 5-word-window sur-
rounding the current word
? the part-of-speech tags of the current and surround-
ing words
? the text chunks in a -1..1 window
? the prefixes and suffixes of length up to 4 of the cur-
rent and the surrounding words
? a word feature flag for each word, similar to the flag
described in (Bikel et al, 1999); examples of such
assigned flags are firstCap, 2digit and allCaps.
? gazetteer information, in the form of a list of 50,000
cities, 80,000 proper names and 3500 organizations
? the output of other two named entity classifiers,
trained on a richer tagset data (32 named categories),
used in the IBM question answering system (Itty-
cheriah et al, 2001)
In addition, a ngram-based capitalization restoration al-
gorithm has been applied on the sentences that appear in
all caps2, for the English task.
3 The Algorithms
This section describes only briefly the classifiers used in
combination in Section 4; a full description of the algo-
rithms and their properties is beyond the scope of this pa-
per ? the reader is instead referred to the original articles.
3.1 The Robust Risk Minimization Classifier
This classifier is described in detail in (Zhang and John-
son, 2003, this volume), along with a comprehensive
evaluation of its performance, and therefore is not pre-
sented here.
3.2 The Maximum Entropy Classifier
The MaxEnt classifier computes the posterior class prob-
ability of an example by evaluating the normalized prod-
uct of the weights active for the particular example. The
model weights are trained using the improved iterative
scaling algorithm (Berger et al, 1996). To avoid running
in severe over-training problems, a feature cutoff of 4 is
applied before the model weights are learned. At decod-
ing time, the best sequence of classifications is identified
with the Viterbi algorithm.
3.3 The Transformation-Based Learning Classifier
Transformation-based learning is an error-driven algo-
rithm which has two major steps: it starts by assigning
some classification to each example, and then automat-
ically proposing, evaluating and selecting the classifica-
tion changes that maximally decrease the number of er-
rors.
2 Usually, document titles, but also table headers, etc.
English German
(a) (b) (a) (b)
HMM 82.0 74.6 - -
TBL 88.1 81.2 69.5 68.6
MaxEnt 90.8 85.6 68.0 67.3
RRM 92.1 85.5 70.7 71.3
Tab. 1: Individual classifier results on the two test sets.
TBL has some attractive qualities that make it suitable
for the language-related tasks: it can automatically in-
tegrate heterogeneous types of knowledge, without the
need for explicit modeling, it is error?driven, and has an
inherently dynamic behavior.
The particular setup in which fnTBL is used in this
work is described in Florian (2002a): in a first phase,
TBL is used to identify the entity boundaries, followed by
a sequence classification stage, where the entities identi-
fied at the first step are classified using internal and exter-
nal clues3.
3.4 The Hidden Markov Model Classifier
The HMM classifier used in the experiments in Section
4 follows the system description in (Bikel et al, 1999),
and it performs sequence classification by assigning each
word either one of the named entity types or the label
NOT-A-NAME to represent "not a named entity". The
states in the HMM are organized into regions, one re-
gion for each type of named entity plus one for NOT-
A-NAME. Within each of the regions, a statistical bi-
gram language model is used to compute the likelihood of
words occurring within that region (named entity type).
The transition probabilities are computed by deleted in-
terpolation (Jelinek, 1997), and the decoding is done
through the Viterbi algorithm. The particular implemen-
tation we used underperformed consistently all the other
classifiers on German, and is not included.
4 Combination Methodology and
Experimental Results
The results obtained by each individual classifier, bro-
ken down by entity type, are presented in Table 1. Out
of the four classifiers, the MaxEnt and RRM classifiers
are the best performers, followed by the modified fnTBL
classifier and the HMM classifier. The error-based clas-
sifiers (RRM and fnTBL) tend to obtain balanced preci-
sion/recall numbers, while the other two tend to be more
precise at the expense of recall. To facilitate comparison
with other classifiers for this task, most reported results
3 The method of retaining only the boundaries and reclas-
sifying the entities was shown to improve the performance of
11 of the 12 systems participating in the CoNLL-2002 shared
tasks, in both languages (Florian, 2002b).
are obtained by using features exclusively extracted from
the training data.
In general, given n classifiers, one can interpret the
classifier combination framework as combining probabil-
ity distributions:
P (C|w,Cn1 ) = f ((Pi (C|w,C
n
1 ))i=1...n) (1)
where Ci is the classifier i?s classification output, f is
a combination function. A widely used combination
scheme is through linear interpolation of the classifiers?
class probability distribution
P (C|w,Cn1 ) =
n?
i=1
P (C|w, i, Ci) ? P (i|w)
=
n?
i=1
Pi (C|w,Ci) ? ?i (w) (2)
The weights ?i (w) encode the importance given to clas-
sifier i in combination, for the context of word w, and
Pi (C|w,Ci) is an estimation of the probability that the
correct classification is C, given that the output of the
classifier i on word w is Ci.
To estimate the parameters in Equation (2), the pro-
vided training data was split into 5 equal parts, and each
classifier was trained, in a round-robin fashion, on 4 fifths
of the data and applied on the remaining fifth. This
way, the entire training data can be used to estimate the
weight parameters ?i (w) and Pi (C|w,Ci) but, at de-
coding time, the individual classifier outputs Ci are com-
puted by using the entire training data.
Table 2 presents the combination results, for differ-
ent ways of estimating the interpolation parameters. A
simple combination method is the equal voting method
(van Halteren et al, 2001; Tjong Kim Sang et al, 2000),
where the parameters are computed as ?i (w) = 1n and
Pi (C|w,Ci) = ? (C,Ci), where ? is the Kronecker op-
erator (? (x, y) := (x = y?1 : 0)) ? each of the classi-
fiers votes with equal weight for the class that is most
likely under its model, and the class receiving the largest
number of votes wins. However, this procedure may lead
to ties, where some classes receive the same number of
votes ? one usually resorts to randomly selecting one of
the tied candidates in this case ? Table 2 presents the av-
erage results obtained by this method, together with the
variance obtained over 30 trials. To make the decision de-
terministically, the weights associated with the classifiers
can be chosen as ?i (w) = Pi (error). In this method,
presented in Table 2 as weighted voting, better perform-
ing classifiers will have a higher impact in the final clas-
sification.
In the voting methods, each classifier gave its entire
vote to one class ? its own output. However, Equation
(2) allows for classifiers to give partial credit to alterna-
tive classifications, through the probability Pi (C|w,Ci).
Method Precision Recall Fmeasure
Best Classifier 91.37% 88.56% 89.94
Equal voting 91.5?0.13 91.0?0.06 91.23?0.08
Weighted voting 92.13% 91.00% 91.56
Model 1 90.99% 90.81% 90.9
Model 2 92.43% 90.86% 91.64
RRM (Combo) 92.01% 91.25% 91.63
Tab. 2: Classifier combination results on English devset
data (no gazetteers of any kind)
Development Test
Language Unique Corpus Unique Corpus
English 33.4% 8.0% 40.3% 11.7%
German 52% 16.2% 48.6% 14.2%
Tab. 3: Word statistics (percent unknown words)
In our experiments, this value is computed through 5-
fold cross-validation on the training data. The space
of possible choices for C, w and Ci is large enough
to make the estimation unreliable, so we use two ap-
proximations, named Model 1 and Model 2 in Table 2:
Pi (C|w,Ci) = Pi (C|w)and Pi (C|w,Ci) = Pi (C|Ci),
respectively. On the development data, the former esti-
mation type obtains a lower performance than the latter.
In a last experiment using only features extracted from
the training data, we use the RRM method to compute
the function f in Equation (1), allowing the system to
select a good performing combination of features. At
training time, the system was fed the output of each clas-
sifier on the cross-classified data, the part-of-speech and
chunk boundary tags. At test time, the system was fed the
classifications of each system trained on the entire train-
ing data, and the corresponding POS and chunk bound-
ary tags. The result obtained rivals the one obtained by
model 2, both displaying a 17% reduction in F-measure
error4, indicating that maybe all sources of information
have been explored and incorporated.
The RRM method is showing its combining power
when additional information sources are used. Specifi-
cally, the system was fed additional feature streams from
a list of gazetteers and the output of two other named en-
tity systems trained on 1.7M words annotated with 32
name categories. The RRM system alone obtains an F-
measure of 92.1, and can effectively integrate these in-
formation streams with the output of the four classifiers,
gazetteers and the two additional classifiers into obtaining
93.9 F-measure, as detailed in Table 4, a 21% reduction
in F-measure error. In contrast, combination model 2 ob-
tains only a performance of 92.4, showing its limitations
4 Measured as 100? F .
in combining diverse sources of information.
German poses a completely different problem for
named entity recognition: the data is considerably
sparser. Table 3 shows the relative distribution of un-
known words in the development and test corpora. We
note that the numbers are roughly twice as large for the
development data in German as they are for English.
Since the unknown words are classed by most classifiers,
this results in few data points to estimate classifier com-
binations. Also, specifically for the German data, tradi-
tional approaches which utilize capitalization do not work
as well as in English, because all nouns are capitalized in
German.
For German, in addition to the entity lists provided, we
also used a small gazetteer of names (4500 first and last
names, 4800 locations in Germany and 190 countries),
which was collected by browsing web pages in about two
person-hours. The average classifier performance gain by
using these features is about 1.5F for the testa data and
about .6F for the testb data.
5 Conclusion
In conclusion, we have shown results on a set of both
well-established and novel classifier techniques which
improve the overall performance, when compared with
the best performing classifier, by 17-21% on the English
task. For the German task, the improvement yielded by
classifier combination is smaller. As a machine learning
method, the RRM algorithm seems especially suited to
handle additional feature streams, and therefore is a good
candidate for classifier combination.
References
A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A maxi-
mum entropy approach to natural language processing. Com-
putational Linguistics, 22(1):39?71.
Daniel M. Bikel, Richard L. Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what?s in a
name. Machine Learning, 34(1-3):211?231.
A. Borthwick. 1999. A Maximum Entropy Approach to Named
Entity Recognition. Ph.D. thesis, New York University.
E. Brill. 1995. Transformation-based error-driven learning and
natural language processing: A case study in part of speech
tagging. Computational Linguistics, 21(4):543?565.
J. N. Darroch and D. Ratcliff. 1972. Generalized iterative
scaling for log-linear models. The Annals of Mathematical
Statistics, 43(5):1470?1480.
R. Florian and G. Ngai, 2001. Fast Transformation-
Based Learning Toolkit. Johns Hopkins University,
http://nlp.cs.jhu.edu/?rflorian/fntbl/documentation.html.
R. Florian. 2002a. Named entity recognition as a house of
cards: Classifier stacking. In Proceedings of CoNLL-2002,
pages 175?178.
R. Florian. 2002b. Transformation Based Learning and Data-
Driven Lexical Disambiguation: Syntactic and Semantic
Ambiguity Resolution. Ph.D. thesis, Johns Hopkins Univer-
sity. Chapter 5.3, pages 135?142.
English devel. Precision Recall F?=1
LOC 96.59% 95.65% 96.12
MISC 90.77% 87.42% 89.06
ORG 90.85% 89.63% 90.24
PER 96.08% 97.12% 96.60
overall 94.26% 93.47% 93.87
English test Precision Recall F?=1
LOC 90.59% 91.73% 91.15
MISC 83.46% 77.64% 80.44
ORG 85.93% 83.44% 84.67
PER 92.49% 95.24% 93.85
overall 88.99% 88.54% 88.76
German devel. Precision Recall F?=1
LOC 83.19% 72.90% 77.71
MISC 83.20% 42.18% 55.98
ORG 83.64% 61.80% 71.08
PER 87.43% 67.02% 75.88
overall 84.60% 61.93% 71.51
German test Precision Recall F?=1
LOC 80.19% 71.59% 75.65
MISC 77.87% 41.49% 54.14
ORG 79.43% 54.46% 64.62
PER 91.93% 75.31% 82.80
overall 83.87% 63.71% 72.41
Tab. 4: Results on the development and test sets in En-
glish and German
Abraham Ittycheriah, Martin Franz, and Salim Roukos. 2001.
IBM?s statistical question answering system ? trec-10.
TREC-10 Proceedings, pages 258?264.
F. Jelinek. 1997. Statistical Methods for Speech Recognition.
MIT Press.
E. F. Tjong Kim Sang, W. Daelemans, H. Dejean, R. Koeling,
Y. Krymolowsky, V. Punyakanok, and D. Roth. 2000. Ap-
plying system combination to base noun phrase identifica-
tion. In Proceedings of COLING 2000, pages 857?863.
H. van Halteren, J. Zavrel, and W. Daelemans. 2001. Improv-
ing accuracy in word class tagging through the combination
fo machine learning systems. Computational Linguistics,
27(2):199?230.
A. J. Viterbi. 1967. Error bounds for convolutional codes and an
asymptotically optimum decoding algorithm. IEEE Transac-
tions on Information Theory, IT-13:260?267.
T. Zhang and D. Johnson. 2003. A robust risk minimization
based named entity recognition system. In Proceedings of
CoNLL-2003.
T. Zhang, F. Damerau, and D. Johnson. 2002. Text chunking
based on a generalization of winnow. Journal of Machine
Learning Research, 2:615?637, March.
HowtogetaChineseName(Entity): Segmentation and Combination Issues
Hongyan Jing Radu Florian Xiaoqiang Luo
Tong Zhang Abraham Ittycheriah
IBM T.J. Watson Research Center
Yorktown Heights, NY 10598
 
hjing,raduf,xiaoluo,tzhang,abei  @us.ibm.com
Abstract
When building a Chinese named entity
recognition system, one must deal with
certain language-specific issues such as
whether the model should be based on
characters or words. While there is no
unique answer to this question, we discuss
in detail advantages and disadvantages of
each model, identify problems in segmen-
tation and suggest possible solutions, pre-
senting our observations, analysis, and
experimental results. The second topic
of this paper is classifier combination.
We present and describe four classifiers
for Chinese named entity recognition and
describe various methods for combining
their outputs. The results demonstrate that
classifier combination is an effective tech-
nique of improving system performance:
experiments over a large annotated corpus
of fine-grained entity types exhibit a 10%
relative reduction in F-measure error.
1 Introduction
Named entity (NE) recognition has drawn much at-
tention in recent years. It was a designated task
in a number of conferences, including the Mes-
sage Understanding Conferences (MUC-6, 1995;
MUC-7, 1997), the Information Retrieval and Ex-
traction Conference (IREX, 1999), the Conferences
on Natural Language Learning (Tjong Kim Sang,
2002; Tjong Kim Sang and De Meulder, 2003),
and the recent Automatic Content Extraction Con-
ference (ACE, 2002).
A variety of algorithms have been proposed for
NE recognition. Many of these algorithms are, in
principle, language-independent. However, when
applying these algorithms to languages such as
Chinese and Japanese, we must deal with cer-
tain language-specific issues: for example, should
we build a character-based model or a word-based
model? how do word segmentation errors affect NE
recognition? how should word segmentation and NE
recognition interact with each other? Besides word
segmentation related issues, Chinese does not have
capitalization, which is a very useful feature in iden-
tifying NEs in languages such as English, Spanish,
or Dutch. How does the lack of features such as cap-
italization affect the performance?
In the first part of this paper, we discuss these
language-specific issues in Chinese NE recogni-
tion. In particular, we use a hidden Markov model
(HMM) system as an example, and discuss various
issues related to applying the HMM classifier to Chi-
nese. The HMM classifier is similar to the one de-
scribed in (Bikel et al, 1999).
In the second part of this paper, we investigate
the combination of a set of diverse NE recognition
classifiers. Four statistical classifiers are combined
in the experiments, including the above-mentioned
hidden Markov model classifier, a transformation-
based learning classifier (Brill, 1995; Florian and
Ngai, 2001), a maximum entropy classifier (Ratna-
parkhi, 1999), and a robust risk minimization classi-
fier (Zhang et al, 2002).
The remainder of this paper is organized as fol-
lows: Section 2 describes the experiment data, Sec-
tion 3 discusses specific issues related to Chinese
NE recognition, Section 4 presents the four classi-
fiers and approaches to combining these classifiers.
2 Data
We used three annotated Chinese corpora in our ex-
periments.
The IBM-FBIS Corpus
The Foreign Broadcast Information Service (FBIS)
offers an extensive collection of translations and
transcriptions of open source information monitored
worldwide on diverse topics such as military af-
fairs, politics, economics, and science and technol-
ogy. The IBM-FBIS corpus consists of approxi-
mately 3,000 Chinese articles obtained from FBIS
(about 3.2 million Chinese characters in total). This
corpus was tagged by a native Chinese speaker with
32 NE categories, such as person, location, organi-
zation, country, people, date, time, percentage, car-
dinal, ordinal, product, substance, and salutation.
There are approximately 300,000 NEs in the entire
corpus, 16% of which are labeled as person, 16% as
organization, and 11% as location.
The IBM-CT Corpus
The Chinese Treebank (Xia et al, 2000), avail-
able from Linguistic Data Consortium, consists of
a 100,000 word (approximately 160,000 characters)
corpus annotated with word segmentation, part-of-
speech tags, and syntactic bracketing. It includes
325 articles from Xinhua newswire between 1994
and 1998. The same Chinese annotator who worked
on the above IBM-FBIS data also annotated the Chi-
nese Treebank data with NE information, henceforth
the IBM-CT corpus, using the same 32 categories as
mentioned above.
The IEER data
The National Institute of Standard and Technol-
ogy organized the Information Extraction ? Entity
Recognition (IEER) evaluation, which involves en-
tity recognition from textual information sources in
both English and Mandarin. The Mandarin training
data consists of approximately 10 hours of broad-
cast news transcripts comprised of approximately
390 stories. The test data also contains transcripts of
broadcast news1. The training data includes approx-
imately 170,000 characters and the test data includes
approximately 6,500 characters. Ten categories of
NEs were annotated, such as person, location, orga-
nization, date, duration, and measure.
1Other types of test data were also used in IEER evaluation,
including newswire text and real automatic speech recognition
transcripts, but we did not use them in our experiments.
3 Language-Specific Issues in Chinese NE
Recognition
Chinese does not have delimiters between words,
so a key design issue in Chinese NE recognition
is whether to build a character-based model or a
word-based model. In this section, we use a hid-
den Markov model NE recognition system as an ex-
ample to discuss language-specific issues in Chinese
NE recognition.
3.1 The Hidden Markov Model Classifier
NE recognition can be formulated as a classification
task, where the goal is to label each token with a
tag indicating whether it belongs to a specific NE
or is not part of any NE. The HMM classifier used
in our experiments follows the algorithm described
in (Bikel et al, 1999). It performs sequence clas-
sification by assigning each token either one of the
NE types or the label ?O? to represent ?outside any
NE?. The states in the HMM are organized into re-
gions, one region for each type of NE plus one for
?O?. Within each of the regions, a statistical lan-
guage model is used to compute the likelihood of
words occurring within that region. The transition
probabilities are smoothed by deleted interpolation,
and the decoding is performed using the Viterbi al-
gorithm.
3.2 Character-Based, Word-Based, and
Class-Based Models
To build a model for identifying Chinese NEs, we
need to determine the basic unit of the model: char-
acter or word. On one hand, the word-based model
is attractive since it allows the system to inspect a
larger window of text, which may lead to more in-
formative decisions. On the other hand, a word seg-
menter is not error-prone and these errors may prop-
agate and result in errors in NE recognition.
Two systems, a character-based HMM model and
a word-based HMM model, were built for compar-
ison. The word segmenter used in our experiments
relies on dictionaries and surrounding words in lo-
cal context to determine the word boundaries. Dur-
ing training, the NE boundaries were provided to the
word segmenter; the latter is restricted to enforce
word boundaries at each entity boundary. Therefore,
at training time, the word boundaries are consistent
with the entity boundaries. At test time, however,
the segmenter could create words which do not agree
with the gold-standard entity boundaries.
Corpus Model Prec Rec 
Character 74.36% 80.24% 77.19
IBM- Word 72.46% 75.97% 74.17
FBIS Class 72.74% 76.20% 74.43
Character 74.57% 78.01% 76.25
IEER Word 77.51% 65.22% 70.83
Class 77.21% 64.36% 70.20
Table 1: Performance of the character-based HMM
model, the word-based HMM model, and the class-
based HMM model. (The precision, recall, and F-
measure presented in this table and throughout this
paper are based on correct identification of all the
attributes of an NE, including boundary, content,
and type.)
The performance of the character-based model
and the word-based model are shown in Table 1. The
two corpora used in the evaluation, the IBM-FBIS
corpus and the IEER corpus, differ greatly in data
size and the number of NE types. The IBM-FBIS
training data consists of 3.1 million characters and
the corresponding test data has 270,000 characters.
As we can see from the table, for both corpora, the
character-based model outperforms the word-based
model, with a lead of 3 to 5.5 in F-measure. The per-
formance gap between two models is larger for the
IEER data than for the IBM-FBIS data.
We also built a class-based NE model. After word
segmentation, class tags such as number, chinese-
name, foreign-name, date, and percent are used to
replace words belonging to these classes. Whether
a word belongs to a specific class is identified by
a rule-based normalizer. The performance of the
class-based HMM model is also shown in Table 1.
For the IBM-FBIS corpus, the class-based model
outperforms the word-based model; for the IEER
corpus, the class-based model is worse than the
word-based model. In both cases, the performance
difference between the word-based model and the
class-based model is very small. The character-
based model outperforms the class-based model in
both tests.
A more careful analysis indicates that although
the word-based model performs worse than the
character-based model overall in our evaluation, it
performs better for certain NE types. For instance,
the word-based model has a better performance
for the organization category than the character-
based model in both tests. While the character-
based model has an F-measure of 65.07 (IBM-FBIS)
and 64.76 (IEER) for the organization category,
the word-based model achieves F-measure scores of
69.14 (IBM-FBIS) and 72.38 (IEER) respectively.
One reason may be that organization names tend to
contain many characters, and since the word-based
model allows the system to analyze a larger window
of text, it is more likely to make a correct guess.
We can integrate the character-based model and the
word-based model by combining the decisions from
the two models. For instance, if we use the de-
cisions of the word-based model for the organiza-
tion category, but use the decisions of the character-
based model for all the other categories, the over-
all F-measure goes up to 76.91 for the IEER data,
higher than using either the character-based or word-
based model alone. Another way to integrate the
two models is to use a hybrid model ? starting with
a word-based model and backing off to character-
based model if the word is unknown.
3.3 Granularity of Word Segmentation
We believe that one main reason for the lower per-
formance of the word-based model is that the word
granularity defined by the word segmenter is not
suitable for the HMM model to perform the NE
recognition task. What exactly constitutes a Chinese
word has been a topic of major debate. We are in-
terested in what is the best word granularity for our
particular task.
To illustrate the word granularity problem for NE
tagging, we take person names as an example. Our
word segmenter marks a person?s name as one word,
consistent with the convention used by the Chinese
treebank and many other word segmentation sys-
tems. While this may be useful in other applications,
it is certainly not a good choice for our NE model.
Chinese names typically contain two or three char-
acters, with family name preceding first name. Only
a limited set of characters are used as family names,
while the first name can be any character(s). There-
fore, the family name is a very important and use-
ful feature in identifying an NE in the person cate-
gory. By combining the family name and the first
name into one word, this important feature is lost to
the word-based model. In our tests, the word-based
model performs much worse for the person category
than the character-based model. We believe that, for
the purpose of NE recognition, it is better to separate
the family name from the first name in word segmen-
tation, although this is not the convention used in the
Chinese treebank.
Other examples include the segmentation of
words indicating dates, countries, locations, percent-
ages, measures, and ordinals. For instance, ?July
4th? is expressed by four characters ?7th month 4th
day? in Chinese. The word segmenter marks the
four characters as a single word; however, the sec-
ond and the last character are actually good features
for indicating date, since the dates are usually ex-
pressed using the same structure (e.g., ?March 25th?
is expressed by ?3rd month 25th day? in Chinese).
For reasons similar to the above, we believe that it
is better to separate characters representing ?month?
and ?day?, rather than combining the four charac-
ters into one word. A similar problem can be ob-
served in English with tokens such as ?61-year-old
man? if one is interested in identifying a person?s
age, in which case ?year? and ?old? are good features
for predication.
The above analysis suggests that a better way to
apply a word segmenter in an NE system is to first
adapt the segmenter so that the segmentation granu-
larity is more appropriate to the particular task and
model. As a guideline, characters that are good fea-
tures for identifying NEs should not be combined
with other characters into word. Additional ex-
amples include characters expressing ?percent? and
characters representing monetary measures .
3.4 The Effect of Segmentation Errors
Word segmentation errors can lead to mistakes in
NE recognition. Suppose an NE consists of four
characters 	
  
  
  
 , if the word segmen-
tation merges 
  with a character preceding it,
then this NE cannot be correctly identified by the
word-based model since the boundary will be incor-
rect. Besides inducing NE boundary errors, incor-
rect word segmentation also leads to wrong match-
ings between training examples and testing exam-
ples, which may result in mistakes in identifying en-
tities.
We computed the upper bound for the word-based
model for the IBM-FBIS test presented in Table 1.
The upper bound of performance is computed by
dividing the total number of NEs whose bound-
aries are also recognized as boundaries by the word
segmenter by the total number of NEs in the cor-
pus, which is the precision, recall, and also the F-
measure. For the IBM-FBIS test data in Table 1,
the upper bound of the word-based model is 95.7 F-
measure.
We also did the following experiment to measure
the effect of word segmentation errors: we gave
the boundaries of NEs in the test data to the word
segmenter and forced it to mark entity boundaries
as word boundaries. This eliminates the word seg-
mentation errors that inevitably result in NE bound-
ary errors. For the IBM-FBIS data, the word-based
HMM model achieves 76.60 F-measure when the
entity boundaries in the test data are given, and the
class-based model achieves 77.77 F-measure, higher
than the 77.19 F-measure by the character-based
model in Table 1. For the IEER data, the F-measure
of the word-based model improves from 70.83 to
73.74 when the entity boundaries are given, and the
class-based model improves from 70.20 to 72.47.
This suggests that with the improvement in Chi-
nese word segmentation, the word-based model may
achieve comparable or better performance than the
character-based model.
3.5 Lexical Features
Capitalization in English gives good evidence of
names. Our HMM classifier for English uses a set
of word-features to indicate whether a word con-
tains all capitalized letters, only digits, or capital-
ized letters and period, as described in (Bikel et al,
1999). However, Chinese does not have capitaliza-
tion. When we applied the HMM system to Chinese,
we retained such features since Chinese text also in-
clude digits and roman words (such as in product
or company names). In an attempt to investigate the
usefulness of such features for Chinese, we removed
them from the system and observed very little dif-
ference in overall performance (0.4 difference in F-
measure).
3.6 Sensitivity to Corpus and Training Size
Variation
To test the robustness of the model, we trained the
system on the 100,000 word IBM-CT data and tested
on the same IBM-FBIS data. The character-based
model achieves 61.36 F-measure and the word-
based model achieves 58.40 F-measure, compared
to 77.19 and 74.17, respectively, using the 20 times
larger IBM-FBIS training set. This represents an ap-
proximately 20% relative reduction in performance
when trained on a related yet different and consider-
ably smaller training set. We plan to investigate fur-
ther the relation between corpus type and size and
performance.
4 Classifier Combination
This section investigates the combination of a set of
classifiers for NE recognition. We first introduce the
classifiers used in our experiments and then describe
the combination methods.
4.1 The Classifiers
Besides the HMM classifier mentioned in the previ-
ous section, the following three classifiers were used
in the experiments.
4.1.1 The Transformation-Based Learning
(fnTBL) Classifier
Transformation-based learning is an error-driven
algorithm which has two major steps: it starts by
assigning some classification to each example, and
then automatically proposing, evaluating and select-
ing the classification changes that maximally de-
crease the number of errors.
TBL has some attractive qualities that make it
suitable for the language-related tasks: it can au-
tomatically integrate heterogeneous types of knowl-
edge, without the need for explicit modeling (similar
to Snow (Dagan et al, 1997), Maximum Entropy,
decision trees, etc); it is error?driven, thus directly
minimizes the ultimate evaluation measure: the er-
ror rate. The TBL toolkit used in this experiment is
described in (Florian and Ngai, 2001).
4.1.2 The Maximum Entropy Classifier
(MaxEnt)
The model used here is based on the maxi-
mum entropy model used for shallow parsing (Rat-
naparkhi, 1999). A sentence with NE tags is
converted into a shallow tree: tokens not in any
NE are assigned an ?O? tag, while tokens within
an NE are represented as constituents whose la-
bel is the same as the NE type. For exam-
ple, the annotated sentence ?I will fly to (LO-
CATION New York) (DATEREF tomorrow)? is
represented as a tree ?(S I/O will/O fly/O to/O
(LOCATION New/LOCATION York/LOCATION)
(DATEREF tomorrow/DATEREF) )?. Once an NE
is represented as a shallow tree, NE recognition can
be realized by performing shallow parsing.
We use the tagging and chunking model described
in (Ratnaparkhi, 1999) for shallow parsing. In the
tagging model, the context consists of a window of
five tokens (including the token being tagged and
two tokens to its left and two tokens to its right) and
two tags to the left of the current token. Five groups
of feature templates are used: token unigram, token
bigram, token trigram, tag unigram and tag bigram
(all within the context window). In the chunking
model, the context is limited to a window of three
subtrees: the previous, current and next subtree. Un-
igram and bigram chunk (or tag) labels are used as
features.
4.1.3 The Robust Risk Minimization (RRM)
Classifier
This system is a variant of the text chunking
system described in Zhang et al (2002), where the
NE recognition problem is regarded as a sequential
token-based tagging problem. We denote by Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 61?68,
ACL-08: HLT, Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
 
Multiple Reorderings in Phrase-based Machine Translation 
 
Niyu Ge,  Abe Ittycheriah 
IBM T.J.Watson Research 
1101 Kitchawan Rd.  
Yorktown Heights, NY 10598 
(niyuge, abei)@us.ibm.com 
 
Kishore Papineni 
Yahoo! Research 
45 West 18th St. 
New York, NY 10011 
kpapi@yahoo-inc.com 
 
 
Abstract  
This paper presents a method to integrate 
multiple reordering strategies in 
phrase-based statistical machine 
translation.  Recently there has been much 
research effort in reordering problems in 
machine translation.   State-of-the-art 
decoders incorporate sophisticated local 
reordering strategies, but there is little 
research on a unified approach to 
incorporate various kinds of reordering 
methods.  We present a phrase-based 
decoder which easily allows multiple 
reordering schemes.  We show how to use 
this framework to perform distance-based 
reordering and HIERO-style (Chiang 
2005) hierarchical reordering.  We also 
present two novel syntax-based reordering 
methods, one built on part-of-speech tags 
and the other based on parse trees.  We will 
give experimental results using these 
relatively easy to implement methods on 
standard tests.    
1 Introduction and Previous Work 
Given an input source sentence and guided by a 
translation model, language model, distortion 
model, etc., a machine translation decoder 
searches for a target sentence that is the best 
translation of the source.  There are usually two 
aspects of the search.  One tries to find target 
words for a given source segment.  The other 
searches for the order in which the source 
segments are to be translated.   A source segment 
here means a contiguous part of the source 
sentence. The former is largely controlled by 
language models and translation models and the 
latter by language models and distortion models.  
It is, in most cases, the latter, the search for the 
correct word order (which source segment to be 
translated next) that results in a large 
combinatorial search space.  State-of-the-art 
decoders use dynamic programming based 
beam-search with local reordering (Och 1999, 
Tillmann 2000).  Although local reordering to 
some degree is implicit in phrase-based 
decoding, the kind of reordering is very limited.   
The simplest distance-based reordering, from the 
current source position i, tries to defer the 
translation of the next n words (1 ? n ? N, N the 
maximum number of words to be delayed).  N is 
bounded by the computational requirements.    
 
Recent work on reordering has been on trying to 
find ?smart? ways to decide word order, using 
syntactic features such as POS tags (Lee and Ge 
2005) , parse trees (Zhang et.al, 2007, Wang et.al. 
2007,  Collins et.al. 2005, Yamada and Knight 
2001) to name just a few, and synchronized CFG 
(Wu 1997, Chiang 2005), again to name just a 
few.  These efforts have shown promising 
improvements in translation quality.  However, 
to use these features during decoding requires 
either a separate decoder to be written or some 
ad-hoc mechanisms to be invented to incorporate 
them into an existing decoder, or in some cases 
(Wang et. al. 2007) the input source is 
pre-ordered to be decoded monotonically.   
 
(Kanthak et. al. 2005) described a framework  in 
which different reordering methods are 
represented as search constraints to a finite state 
automata.   It is able to compute distance-based 
and ITG-style reordering automata.   We differ 
from that approach in a couple of ways.  One is 
that in (Kanthak et. al. 2005), an on-demand 
61
 reordering graph is pre-computed which is then 
taken as a input for monotonic decoding.    We 
compute the reordering as the sentence is being 
decoded.  The second is that it is not clear how to 
generate the permutation graphs under, say 
HIERO-type hierarchical constraints,  or other 
syntax-inspired reorderings such as those based 
on part-of-speech patterns.  Our approach differs 
in that we allow greater flexibility in capturing a 
wider range of reordering strategies. 
 
We will first give an overview of  the framework 
(?2).  We then describe how to implement four 
reordering methods in a single decoder in ?3.  ?4 
presents some Chinese-English results on the 
NIST MT test sets.  It also shows results on web 
log and broadcast news data.    
2 Reordering in Decoding 
2.1 Hypothesis 
The process of MT decoding can be thought of as 
a process of hypothesizing target translations.  
Given an input source sentence of length L, the 
decoding is done segment by segment.  A 
segment is simply an n-word source chunk, 
where 1 ? n ? L.  Decoding finishes when all 
source chunks are translated (some source words 
that have no target translations can be thought of 
as being translated into a special token NULL).   
The decoder at this point outputs its best 
hypothesis. 
 
2.2 Hypothesis with reorderings 
In order to facilitate various search strategies, a 
separation of duty is called for.    The decoder is 
composed of two major modules, a reordering 
module and a production module.  The reordering 
module decides which source segment to be 
translated next.   The production module 
produces the actual translations for a given 
segment.  Although most of the start-of-the-art 
decoders have these two modules, they are 
nevertheless tightly coupled.  Here they are 
separated.  This separation does not compromise 
the search space of the decoder.  Hypotheses that 
are explored in the traditional way are still 
explored in this framework.  This separation is 
essential if one were to design a decoder that 
incorporates phrase-based, syntax-based, and 
other types of decoding in a unified and 
disciplined way.   In the decoder, each hypothesis 
carries with it a sequence of source segments to 
be decoded at the current time step.   After the 
production module translates these segments and 
after beam pruning is applied to all the 
hypotheses produced at this time step, the 
hypotheses go back to the reordering module 
which determines the next source segments to be 
translated.  This process continues until all source 
words are translated.   
 
One can think of the reordering module as a black 
box whose sole responsibility is to determine the 
next sequence of source segments to be translated.   
Given this separation, the reordering module can 
be implemented in whichever way and the 
changes in it do not require changes to any other 
modules in the decoder.  There can be a suite of 
such modules, each exploring different features 
and implementing different search schemes.  A 
reordering module that implement basic 
distance-based reordering will take two 
parameters, the number of source words to be 
skipped and the window size that determines 
when the skipped words must be translated.  A 
reordering module that is based on HIERO rules 
will take the library of HIERO rules and select 
the subset that fire on a given input sentence.  The 
module will use this subset of rules to determine 
the source translation order.  A parse-inspired 
reordering module will take an input parse tree 
and based on either a trained model or 
hand-written rules  decide the next source 
sequence to be translated.  As long as all the 
reordering modules are written to a common 
interface,  they can be separately written and 
maintained.   
Figure 1 shows an example of how three 
reordering modules can be incorporated into a 
single decoder.  The input source is S1?Sn.   
Module
skip = 2
window = 3
S1 S2 X ?> T1 T2 X
S1
S2 S3
Sn?1 Sn
.....
Distance?based
Reordering
S1 X Sn ?> Tn X T1 HIERO?based
Reordering
Parse?based
Reordering
S1
S2
S3
Sn
S1
S1
S2
Sn?1
Production
 
Figure 1.  Reordering module example 
62
 Each reordering module has its own resources 
and parameters which are shown on the left side.  
Each reordering module produces a vector of 
next source positions.  The production module 
takes these positions and produces translations 
for them.  
3  Reordering Modules 
In this section, we describe four reordering 
modules implementing different reordering 
strategies.  The framework is not limited to these 
four methods.  We present these four to 
demonstrate the ability of the framework to 
incorporate a wide variety of reordering methods. 
 
3.1 Distance-based Skip Reordering 
 
This is the type of reordering first presented by 
(Brown et.al. 1993) and was briefly alluded to in 
the above Introduction section.  This method is 
controlled by 2 parameters: 
Skip = number of words whose 
translations are to be delayed.  Let us call these 
words skipped words. 
WindowWidth (ww) = maximum 
number of words allowed to be translated before 
translating the skipped words. 
 
This reordering module outputs all the possible 
next source words to be translated according to 
these two parameters.  For illustration purposes, 
let us use a bit vector B to represent which source 
words have been translated.  Thus those that have 
been translated have value 1 in the bit vector, and 
those un-translated have 0.   As an example, let 
skip = 2 and ww = 3, and an input sentence of 
length = 10.  Initially, all 10 entries of B are 0.  At 
the first time step, only the following are possible 
next positions: 
a) 1 0 0 0 0 0 0 0 0 0 :  translate 1st word 
b) 0 1 0 0 0 0 0 0 0 0 :  skip 1st word 
c) 0 0 1 0 0 0 0 0 0 0 :  skip 1st and 2nd words 
 
At the next time step,  if  we want to continue the 
path of c),  we have these choices: 
1) we can leave the first 2 words open and 
continue until we reach 3 words (because ww=3) 
c1) 0 0 1 1 0 0 0 0 0 0  
c2) 0 0 1 1 1 0 0 0 0 0 
2) or we can go back and translate either of the 
first 2 skipped words: 
 c3) 1 0 1 0 0 0 0 0 0 0 
 c4) 0 1 1 0 0 0 0 0 0 0 
 
It is clear that the search space easily blows up 
with large skip and window-width values.  
Therefore, a beam pruning step is performed after 
partial hypotheses are produced at every time 
step.   
 
3.2 HIERO Hierarchical Reordering 
 
In this section we show an example of how the 
Hiero decoding method (Chiang 2005) can be 
implemented as a reordering module in this 
framework.  This is not meant to show that our 
MT decoder is a synchronous CFG parser.  This 
is a conceptual demonstration of how the Hiero 
rules can be used in a reordering module to 
decide the source translation order and thus used 
in a traditional phrase-based decoder.  This 
module uses the Hiero rules to determine the next 
source segment to be translated.  The example is 
Chinese-English translation. Consider the 
following Chinese sentence (word position and 
English gloss are shown in parentheses): 
 
(1.Australia) (2. is)  (3. with) 
(4. North Korea) 	(5. have)  
(6. diplomatic 
relation)  (7. NULL)  (8. few)  (9. 
country) (10. one of) 
 
Suppose we have two following Hiero rules: 
 X ? Australia X  (1) 
 X  ? is one of X   (2) 
 
The left-hand-side of Hiero rules are source 
phrases and the right-hand-side is their English 
translation and the Xs are the non-terminals 
whose extent is determined by the source input 
against which the rules are tested for matching.  
A rule fires if its left-hand-side matches certain 
segments of the input. 
 
Given the above Chinese input and the two Hiero 
rules, the Hiero decoder as described in (Chiang 
2005) will produce a partial hypothesis 
?Australia is one of? by firing the two rules 
during parsing (see Chiang 2005 for decoding 
details).  We will show how to decode in the 
Hiero paradigm using the framework. 
63
  
The reordering module first decides a source 
segment based on rule (1).  Rule (1) generates a 
sequence of source segments in term of source 
ranges: <[1,1],[2,10]>.  This means the source 
segment spanning range [1,1] (word 1, 
/Australia) is to be translated first, and then the 
remaining segment spanning range [2,10] is to be 
translated next.  This is exactly what rule (1) 
dictates where  corresponds to source 
[1,1] in the reordering module?s output and the X 
is [2,10].  The range [1,1], after being given to the 
production module,  results in the production of a 
partial hypothesis where the target ?Australia? is 
produced.  The task now is to translate the next 
source range [2,10].  At this point,  the reordering 
module generates another source segment 
according to rule (2) where the left-hand-side ? 
X ? is matched against the input and three 
corresponding source ranges are found which are 
[2,2] (/is), [4,9] (X), and [10,10] (/one of).  
According to rule (2), this source sequence is to 
be translated in the order of [2,2] (is), [10,10] 
(one of), and then [4,9] (X).  Therefore the output 
of the reordering module at this stage is 
<[2,2],[10,10],[4,9]>.  This would then go on to 
be translated and results in a partial hypothesis to 
?Australia is one of?.  Thus ?Australia is one of? 
is a partial production which covers source 
segments [1,1] [2,2] and [10,10] in that order.  
Note that the source segments decoded so far are 
not contiguous and this is the effect of long-range 
reordering imposed by rule (2).  The next stage is 
<[4,9]> which is what the X in rule (2) 
corresponds to.  From here onwards, other rules 
will fire and the decoding sequence these rules 
dictate will be realized by the reordering module 
in the form of source ranges.  This process can 
also be viewed hierarchically in Figure 2. 
 
In Figure 2 the ranges (the bracketed numbers) 
are source segments and the leaves are English 
productions.  Initially we have the whole input 
sentence as one range [1,10].  According to rule 
(1), this initial range is refined to be 
<[1,1],[2,10]>,  the 2nd level in Figure 2.  The 
[2,10] is further refined by rule (2)  to generate  
the 3rd level ranges <[2,2],[10,10],[4,9]> and the 
process goes on.  Ranges that cannot be further 
refined go into the production module which 
 ...
[1,10]
[1,1]
Australia
[2,10]
[2,2] [10,10] [4,9]
is one of
 
 
Figure 2. Hiero-style decoding  
 
generates partial hypotheses which are the leaves 
in the figure.  In other words, the partial 
hypotheses are generated by traversing the tree in 
Figure 2 in a left-to-right depth-first fashion. 
 
3.3 Generalized Part-Of-Speech-based 
Reordering 
 
The aim of a generalized part-of-speech-based 
reordering method is to tackle the problem of 
long-range word movement.  Chinese is a 
pre-modification language in which the modifiers 
precede the head.  The following is an example 
with English gloss   in parentheses.  The 
prepositional modifier ?on the table'' follows the 
head ?the book'' in English (3.3b), but precedes it 
in Chinese (3.3a).  When the modifiers are long, 
word-based local reordering is inadequate to 
handle the movement. 
3.3a.  (table)  (on)  (NULL) (book) 
 3.3b.  the book on the table 
 
There have been several approaches to the 
problem some of which are mentioned in ?1.  
Compared to these methods, this approach is 
lightweight in that it requires only part-of-speech  
(POS) tagging on the source side. The idea is to 
capture general long-distance distortion 
phenomena by extracting reordering patterns 
using a mixture of words and part-of-speech tags 
on the source side.  The reordering patterns are 
extracted for every contiguously aligned source 
segment in the following form:  
source  sequence ? target sequence 
 
 
Both the source sequence and the target  
sequence are expressed using a combination of 
source words and POS tags.  The patterns are 
?generalized? not only because POS tags are used 
but also because variables or place-holders are 
64
 allowed.  Given a pair of source and target 
training sentences, their word alignments and 
POS tags on the source, we look for any 
contiguously aligned source segment and extract 
word reordering patterns around it.  Figure 3 
shows an example.   
 
Shown in Figure 3 are a pair of Chinese and 
English sentence, the Chinese POS tags and the 
word alignment indicated by the lines.  When 
multiple English words  are aligned to a single 
Chinese word, they are grouped by a rectangle for 
easy viewing.  Here we have a contiguously 
aligned source segment from position 3 to 8. 
Using the range notation, we say that source 
range [3,8] is aligned to target range [6, 14].  Let 
X denote the source  segment [3,8].   The source 
verb phrase (at positions 9 and 10) occur after X 
whereas the corresponding target verb phrase 
(target words 2,3, and 4) occur before the 
translation of X (which is target [6,14]). We thus 
extract the following pattern: 
  X V N ? V N  X       (1) 
 
where the left-hand side ? X V N? is the source 
word sequence and the right-hand side ?V N  X? 
is the target word sequence.   The X  in the pattern 
is meant to represent a variable, to be matched by 
a sequence of source words in the test data when 
this pattern fires during decoding.  Note that the 
pattern is a mixture of words and POS tags.  
Specifically, the word identity of the preposition 
 (position 2) is retained whereas the content 
words (the verb and the noun) are substituted by 
their POS tags.  This is because in general, for the 
reordering purpose the POS tags are good class 
representations for content words whereas 
different prepositions may have different word 
order patterns so that mapping them all to a single 
POS P masks the difference. Examples of 
patterns are shown in Table 1. 
 
In Chinese-English translation, the majority of 
the reorderings occur around verb modifiers 
(prepositions) and noun modifiers (usually 
around the Chinese part-of-speech DEG as in 
position 6).   Therefore we choose to extract only 
these 2 kinds of patterns that involve a 
preposition and/or a DEG.  In the example above, 
there are only 2 such patterns: 
    X V N ? V N  X              (1) 
X1 DEG X2 ? X2 DEG X1           (2)     
 
Figure 3. Chinese/English Alignment Example 
 
 
 
 Source Seq. Target Seq. Count P(tseq 
|sseq) 
1 X DEG NN X DEG NN 861 0.198 
2 X DEG NN X NN DEG 1322 0.305 
3 X DEG NN NN DEG X 2070 0.477 
4 X DEG NN NN X DEG 10 0.002 
5 X DEG NN DEG NN X 52 0.012 
6 X DEG NN DEG X NN 22 0.005 
7  X VV  X VV 15 0.118 
8  X VV VV  X 112 0.882 
9 X VV VV  X 2 0.041 
10 X VV X VV 47 0.959 
 
Table 1. Pattern examples 
 
 
In the table, we see that when the preposition is 
  (rows 7 and 8, translation: by), then the 
swapping is more likely (0.882 in row 8).  When 
the preposition is  (rows 9 and 10 translation: 
because), then the target most often stays the 
same order as the source (prob 0.959, last row). 
 
3.4 Parse-based Lexicalized Reordering 
  
Part-of-speech reordering patterns as described in 
?3.3 are crude approximation to the structure of 
the source sentence.  For example, in the source 
pattern ?X DEG NN?, the variable X can match a 
source segment of arbitrary length which is 
followed by ?DEG NN?.  Although it does 
capture very long range movement as a result of 
SrcPOS  Source              Target 
 
1.NNP Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 889?898,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Correction Model for Word Alignments
J. Scott McCarley, Abraham Ittycheriah, Salim Roukos, Bing Xiang, Jian-ming Xu
IBM T.J. Watson Research Center
1101 Kitchawan Road, Rt. 134
Yorktown Heights, NY 10598
{jsmc,abei,roukos,bxiang,jianxu}@us.ibm.com
Abstract
Models of word alignment built as sequences
of links have limited expressive power, but are
easy to decode. Word aligners that model the
alignment matrix can express arbitrary align-
ments, but are difficult to decode. We pro-
pose an alignment matrix model as a cor-
rection algorithm to an underlying sequence-
based aligner. Then a greedy decoding al-
gorithm enables the full expressive power of
the alignment matrix formulation. Improved
alignment performance is shown for all nine
language pairs tested. The improved align-
ments also improved translation quality from
Chinese to English and English to Italian.
1 Introduction
Word-level alignments of parallel text are crucial for
enabling machine learning algorithms to fully uti-
lize parallel corpora as training data. Word align-
ments appear as hidden variables in IBM Models 1-
5 (Brown et al, 1993) in order to bridge a gap be-
tween the sentence-level granularity that is explicit
in the training data, and the implicit word-level cor-
respondence that is needed to statistically model lex-
ical ambiguity and word order rearrangements that
are inherent in the translation process. Other no-
table applications of word alignments include cross-
language projection of linguistic analyzers (such as
POS taggers and named entity detectors,) a subject
which continues to be of interest. (Yarowsky et al,
2001), (Benajiba and Zitouni, 2010)
The structure of the alignment model is tightly
linked to the task of finding the optimal alignment.
Many alignment models are factorized in order to
use dynamic programming and beam search for ef-
ficient marginalization and search. Such a factoriza-
tion encourages - but does not require - a sequential
(often left-to-right) decoding order. If left-to-right
decoding is adopted (and exact dynamic program-
ming is intractable) important right context may ex-
ist beyond the search window. For example, the link-
age of an English determiner may be considered be-
fore the linkage of a distant head noun.
An alignment model that jointly models all of the
links in the entire sentence does not motivate a par-
ticular decoding order. It simply assigns comparable
scores to the alignment of the entire sentence, and
may be used to rescore the top-N hypotheses of an-
other aligner, or to decide whether heuristic pertur-
bations to the output of an existing aligner constitute
an improvement. Both the training and decoding of
full-sentence models have presented difficulties in
the past, and approximations are necessary.
In this paper, we will show that by using an ex-
isting alignment as a starting point, we can make a
significant improvement to the alignment by propos-
ing a series of heuristic perturbations. In effect, we
train a model to fix the errors of the existing aligner.
From any initial alignment configuration, these per-
turbations define a multitude of paths to the refer-
ence (gold) alignment. Our model learns alignment
moves that modify an initial alignment into the ref-
erence alignment. Furthermore, the resulting model
assigns a score to the alignment and thus could be
used in numerous rescoring algorithms, such as top-
N rescorers.
In particular, we use the maximum entropy frame-
889
work to choose alignment moves. The model is sym-
metric: source and target languages are interchange-
able. The alignment moves are sufficiently rich to
reach arbitrary phrase to phrase alignments. Since
most of the features in the model are not language-
specific, we are able to test the correction model
easily on nine language pairs; our corrections im-
proved the alignment quality compared to the input
alignments in all nine. We also tested the impact on
translation and found a 0.48 BLEU improvement on
Chinese to English and a 1.26 BLEU improvement
on English to Italian translation.
2 Alignment sequence models
Sequence models are the traditional workhorse for
word alignment, appearing, for instance, in IBM
Models 1-5. This type of alignment model is not
symmetric; interchanging source and target lan-
guages results in a different aligner. This parameter-
ization does not allow a target word to be linked to
more than one source word, so some phrasal align-
ments are simply not considered. Often the choice of
directionality is motivated by this restriction, and the
choice of tokenization style may be designed (Lee,
2004) to reduce this problem. Nevertheless, aligners
that use this parameterization internally often incor-
porate various heuristics in order to augment their
output with the disallowed alignments - for example,
swapping source and target languages to obtain a
second alignment (Koehn et al, 2007) with different
limitations. Training both directions jointly (Liang
et al, 2006) and using posterior probabilities dur-
ing alignment prediction even allows the model to
see limited right context. Another alignment combi-
nation strategy (Deng and Zhou, 2009) directly op-
timizes the size of the phrase table of a target MT
system.
Generative models (such as Models 1-5, and the
HMM model (Vogel et al, 1996)) motivate a narra-
tive where alignments are selected left-to-right and
target words are then generated conditioned upon
the alignment and the source words. Generative
models are typically trained unsupervised, from par-
allel corpora without manually annotated word-level
alignments.
Discriminative models of alignment incorporate
source and target words, as well as more linguisti-
cally motivated features into the prediction of align-
ment. These models are trained from annotated
word alignments. Examples include the maximum
entropy model of (Ittycheriah and Roukos, 2005) or
the conditional random field jointly normalized over
the entire sequence of alignments of (Blunsom and
Cohn, 2006).
3 Joint Models
An alternate parameterization of alignment is the
alignment matrix (Niehues and Vogel, 2008). For a
source sentence F consisting of words f1...fm, and
a target sentence E = e1...el, the alignment matrix
A = {?ij} is an l ? m matrix of binary variables.
If ?ij = 1, then ei is said to be linked to fj . If ei
is unlinked then ?ij = 0 for all j. There is no con-
straint limiting the number of source tokens to which
a target word is linked either; thus the binary ma-
trix allows some alignments that cannot be modeled
by the sequence parameterization. All 2lm binary
matrices are potentially allowed in alignment matrix
models. For typical l and m, 2lm  (m + 1)l, the
number of alignments described by a comparable se-
quence model. This parameterization is symmetric -
if source and target are interchanged, then the align-
ment matrix is transposed.
A straightforward approach to the alignment ma-
trix is to build a log linear model (Liu et al, 2005)
for the probability of the alignment A. (We continue
to refer to ?source? and ?target? words only for con-
sistency of notation - alignment models such as this
are indifferent to the actual direction of translation.)
The log linear model for the alignment (Liu et al,
2005) is
p(A|E,F ) = exp (
?
i ?i?i(A,E, F ))
Z(E,F ) (1)
where the partition function (normalization) is given
by
Z(E,F ) =
?
A
exp
(?
i
?i?i(A,E, F )
)
. (2)
Here the ?i(A,E, F ) are feature functions. The
model is parameterized by a set of weights ?i, one
for each feature function. Feature functions are often
binary, but are not required to be. Feature functions
890
may depend upon any number of components ?ij of
the alignment matrix A.
The sum over all alignments of a sentence pair
(2lm terms) in the partition function is computa-
tionally impractical except for very short sentences,
and is rarely amenable to dynamic programming.
Thus the partition function is replaced by an ap-
proximation. For example, the sum over all align-
ments may be restricted to a sum over the n-best
list from other aligners (Liu et al, 2005). This ap-
proximation was found to be inconsistent for small
n unless the merged results of several aligners were
used. Alternately, loopy belief propagation tech-
niques were used in (Niehues and Vogel, 2008).
Loopy belief propagation is not guaranteed to con-
verge, and feature design is influenced by consider-
ation of the loops created by the features. Outside
of the maximum entropy framework, similar models
have been trained using maximum weighted bipar-
tite graph matching (Taskar et al, 2005), averaged
perceptron (Moore, 2005), (Moore et al, 2006), and
transformation-based learning (Ayan et al, 2005).
4 Alignment Correction Model
In this section we describe a novel approach to word
alignment, in which we train a log linear (maximum
entropy) model of alignment by viewing it as correc-
tion model that fixes the errors of an existing aligner.
We assume a priori that the aligner will start from
an existing alignment of reasonable quality, and will
attempt to apply a series of small changes to that
alignment in order to correct it. The aligner naturally
consists of a move generator and a move selector.
The move generator perturbs an existing align-
ment A in order to create a set of candidate align-
mentsMt(A), all of which are nearby to A in the
space of alignments. We index the set of moves by
the decoding step t to indicate that we generate en-
tirely different (even non-overlapping) sets of moves
at different steps t of the alignment prediction. Typ-
ically the moves affect linkages local to a particular
word, e.g. the t?th source word.
The move selector then chooses one of the align-
ments At+1 ? Mt(At), and proceeds iteratively:
At+2 ? Mt+1(At+1), etc. until suitable termina-
tion criteria are reached. Pseudocode is depicted in
Fig. (1.) In practice, one move for each source and
Input: sentence pair E1 .. El, F1 .. Fm
Input: alignment A
Output: improved alignment Afinal
for t = 1? l do
generate moves:Mt(At)
select move:
At+1 ? argmaxA?Mt(At)p(A|At, E, F )
Afinal ? Al+1
{repeat for source words}
Figure 1: pseudocode for alignment correction
target word is sufficient.
4.1 Move generation
Many different types of alignment perturbations are
possible. Here we restrict ourselves to a very sim-
ple move generator that changes the linkage of ex-
actly one source word at a time, or exactly one target
word at a time. Many of our corrections are simi-
lar to those of (Setiawan et al, 2010), although our
motivation is perhaps closer to (Brown et al, 1993),
who used similar perturbations to approximate in-
tractable sums that arise when estimating the param-
eters of the generative models Models 3-5, and ap-
proach refined in (Och and Ney, 2003). We note that
our corrections are designed to improve even a high-
quality starting alignment; in contrast the model of
(Fossum et al, 2008) considers deletion of links
from an initial alignment (union of aligners) that is
likely to overproduce links.
From the point of view of the alignment ma-
trix, we consider changes to one row or one col-
umn (generically, one slice) of the alignment matrix.
At each step t, the move setMt(At) is formed by
choosing a slice of the current alignment matrix At,
and generating all possible alignments from a few
families of moves. Then the move generator picks
another slice and repeats. The m + l slices are cy-
cled in a fixed order: the first m slices correspond to
source words (ordered according to a heuristic top-
down traversal of the dependency parse tree if avail-
able), and the remaining l slices correspond to target
words, similarly parse-ordered. For each slice we
consider the following families of moves, illustrated
by rows.
? add link to row i - for one j such that ?ij = 0,
891
make ?ij = 1 (shown here for row i = 1.)
? ? ?
a ? ? ?
b ? ? ?
c ? ? ?
=?
? ? ?
a ? ? ?
b ? ? ?
c ? ? ?
? remove one or more links from row i - for some
j such that ?ij = 1, make ?ij = 0 (shown here
for i = 3.)
? ? ?
a ? ? ?
b ? ? ?
c ? ? ?
=?
? ? ?
a ? ? ?
b ? ? ?
c ? ? ?
? move a link in row i - for one j and one j? such
that ?ij = 1 and ?ij? = 0, make ?ij = 0 and
?ij? = 1 (shown here for i = 1.)
? ? ?
a ? ? ?
b ? ? ?
c ? ? ?
=?
? ? ?
a ? ? ?
b ? ? ?
c ? ? ?
? leave row i unchanged
Similar families of moves apply to column slices
(source words.) In practice, perturbations are re-
stricted by a window (typically ?5 from existing
links.) If the given source word is unlinked, we
consider adding a link to each target word in a win-
dow (?5 from nearby links.) The window size re-
strictions mean that some reference alignments are
not reachable from the starting point. However, this
is unlikely to limit performance - an oracle aligner
achieves 97.6%F -measure on the Arabic-English
training set.
4.2 Move selection
A log linear model for the selection of the candidate
alignment at t+1 from the set of alignmentsMt(At)
generated by the move generator at step t takes the
form:
p(At+1|E,F,Mt(At)) =
e
P
i ?i?i(At+1,E,F )
Z(E,F,Mt(At))
(3)
where the partition function is now given by
Z(E,F,M) =
?
A?M
e
P
i ?i?i(A,E,F ) (4)
and At+1 ? Mt(At) is required for correct normal-
ization. This equation is notationally very similar
to equation (1), except that the predictions of the
model are restricted to a small set of nearby align-
ments. For the move generator considered in this pa-
per, the summation in Eq.(4) is similarly restricted,
and hence training the model is tractable. The set
of candidate alignmentsMt(At) typically does not
contain the reference (gold) alignment; we model
the best alignment among a finite set of alternatives,
rather than the correct alignment from among all
possible alignments. This is a key difference be-
tween our model and (Liu et al, 2005).
Note that if we extended our definition of pertur-
bation to the limiting case that the alignment set in-
cluded all possible alignments then we would clearly
recover the standard log linear model of alignment.
4.3 Training
Since the model is designed to predict perturbation
to an alignment, it is trained from a collection of
errorful alignments and corresponding reference se-
quences of aligner moves that reach the reference
(gold) alignment. We construct a training set from
a collection of sentence pairs and reference align-
ments for training (A?n, En, Fn)Nn=1, as well as col-
lections of corresponding ?first pass? alignments An1
produced by another aligner. For each n, we form a
number of candidate alignment sets Mt(Ant ), one
for each source and target word. For training pur-
poses, the true alignment from the set is taken to be
the one identical withA?n in the slice targeted by the
move generator at the current step. (A small number
of move sets do not have an exact match and are dis-
carded.) Then we form an objective function from
the log likelihood of reference alignment, smoothed
with a gaussian prior
L =
?
n
Ln +
?
i
(?i/?)2 (5)
892
where the likelihood of each training sample is
Ln =
?
?
log p1(A0n|E,Fn;M(f?, A0n, E, Fn))
+
?
?
log p1(A0n|E,Fn;M(e?, A0n, E, Fn)) (6)
The likelihood has a term for each sentence pair
and for each decoder step. The model is trained
by gradient ascent using the l-BFGS method (Liu
and Nocedal, 1989), which has been successfully
used for training log linear models (Blunsom and
Cohn, 2006) in many natural language tasks, includ-
ing alignment.
5 Features
A wide variety of features were used in the model.
We group the features in three broad categories:
link-based, geometrical, and parse-based.
Link-based features are those which decompose
into a (linear) sum of alignment matrix elements ?ij .
An example link-based feature is one that fires if a
source language noun is linked to a target language
determiner. Note that this feature may fire more than
once in a given sentence pair: as with most fea-
tures in our model, it is an integer-valued feature
that counts the number of times a structure appears
in a sentence pair. These features do not capture any
correlation between different ?ij . Among the link-
based features are those based on Model 1 transla-
tion matrix parameters ?(ei|fj) and ?(fj |ei). We
bin the model 1 parameters, and form integer-valued
features for each bin that count the number of links
with ?0 < ?(ei|fj) < ?1.
Geometrical features are those which capture cor-
relation between different ?ij based on adjacency or
nearness. They capture the idea that nearby words
in one language link to nearby words in the other
language - the motivation of HMM-based models
of alignment. An example is a feature that counts
the number of times that the next word in the source
language is linked to the next word in the target lan-
guage:
?(A,E, F ) =
?
ij
?ij?i+1,j+1 (7)
Parse-based features are those which capture cor-
relation between different ?ij , but use parsing to de-
termine links which are correlated - for example, if a
determiner links to the same word as its head noun.
As an example, if ei is the headword of ei? , and fj is
the headword of fj? , then
?(A,E, F ) =
?
ij
?ij?i?j? (8)
counts the number of times that a dependency rela-
tion in one language is preserved by alignment in the
other language. This feature can also be decorated,
either lexically, or with part-of-speech tags (as many
features in all three categories are.)
5.1 Unsupervised Adaptation
We constructed a heuristic phrase dictionary for un-
supervised adapatation. After aligning a large unan-
notated parallel corpus with our aligner, we enumer-
ate fully lexicalized geometrical features that can be
extracted from the resulting alignments - these are
entries in a phrase dictionary. These features are
tied, and treated as a single real-valued feature that
fires during training and decoding phases if a set of
hypothesized links matches the geometrical feature
extracted from the unannotated data. The value of
this real-valued feature is the log of the number of
occurrences of the identical (lexicalized) geometri-
cal feature in the aligned unannotated corpus.
6 Results
We design our experiments to validate that a cor-
rection model using simple features, mostly non-
language-specific, can improve the alignment accu-
racy of a variety of existing aligners for a variety of
language pairs; we do not attempt to exactly match
features between comparison aligners - this is un-
likely to lead to a robust correction model.
6.1 Arabic-English alignment results
We trained the Arabic-English alignment system
on 5125 sentences from Arabic-English treebanks
(LDC2008E61, LDC2008E22) that had been an-
notated for word alignment. Reference parses
were used during the training. Results are mea-
sured on a 500 sentence test set, sampled from
a wide variety of parallel corpora, including vari-
ous genres. During alignment, only automatically-
generated parses (based on the parser of (Rat-
naparkhi, 1999)) were available. Alignments on
893
initial align correction model R (%) P (%) F (%) ?F
GIZA++ 76 76 76
corr(GIZA++) 86 94 90 14?
corr(ME-seq) 88 92 90 14?
HMM 73 73 73
corr(HMM) 87 92 89 16?
corr(ME-seq) 87 93 90 17?
ME-seq 82 84 83
corr(HMM) 88 92 90 7?
corr(GIZA++) 87 94 91 8?
corr(ME-seq) 89 94 91 8?
Table 1: Alignment accuracy for Arabic-English systems in percentage recall (R), precision(P), and F -measure. ?
denotes statistical significance (see text.)
lang method R (%) P(%) F (%) ?F
ZH?EN GIZA++ 55 67 61
ME-seq 66 72 69
corr(ME-seq) 74 76 75 6?
Table 2: Alignment accuracy for Chinese(ZH)-English(EN) systems. ? denotes statistical significance
lang aligner R(%) P(%) F (%) ?F
IT? EN ME-seq 74 87 80
corr(ME-seq) 84 92 88 8?
EN?IT ME-seq 75 86 80
corr(ME-seq) 84 92 88 8?
PT?EN ME-seq 77 83 80
corr(ME-seq) 87 91 89 9?
EN?PT ME-seq 79 87 83
corr(ME-seq) 88 90 89 6?
JA?EN ME-seq 72 78 75
corr(ME-seq) 77 83 80 5?
RU?EN ME-seq 81 85 83
corr(ME-seq) 82 92 87 4?
DE?EN ME-seq 77 82 79
corr(ME-seq) 78 87 82 3?
ES?EN ME-seq 93 86 90
corr(ME-seq) 92 88 90 0.6
FR?EN ME-seq 89 91 90
corr(ME-seq) 88 92 90 0.1
Table 3: Alignment accuracy for additional languages. ? denotes statistical significance; ? statistical significance not
available. IT=Italian, PT=Portuguese, JA=Japanese, RU=Russian, DE=German, ES=Spanish, FR=French
894
the training and test sets were decoded with three
other aligners, so that the robustness of the cor-
rection model to different input alignments could
be validated. The three aligners were GIZA++
(Och and Ney, 2003) (with the MOSES (Koehn
et al, 2007) postprocessing option -alignment
grow-diag-final-and) the posterior HMM
aligner of (Ge, 2004), a maximum entropy sequen-
tial model (ME-seq) (Ittycheriah and Roukos, 2005).
ME-seq is our primary point of comparison: it is
discriminatively trained (on the same training data,)
uses a rich set of features, and provides the best
alignments of the three. Three correction models
were trained: corr(GIZA++) is trained to correct
the alignments produced by GIZA++, corr(HMM)
is trained to correct the alignments produced by the
HMM aligner, and corr(ME-seq) is trained to correct
the alignments produced by the ME-seq model.
In Table (1) we show results for our system cor-
recting each of the aligners as measured in the usual
recall, precision, and F -measure.1 The resulting
improvements in F -measure of the alignments pro-
duced by our models over their corresponding base-
lines is statistically significant (p < 10?4, indicated
by a ?.) Statistical significance is tested by a Monte
Carlo bootstrap (Efron and Tibshirani, 1986) - sam-
pling with replacement the difference in F -measure
of the two system?s alignments of the same sentence
pair. Both recall and precision are improved, but the
improvement in precision is somewhat larger. We
also show cross-condition results in which a correc-
tion model trained to correct HMM alignments is ap-
plied to correct ME-seq alignments. These results
show that our correction model is robust to different
starting aligners.
6.2 Chinese-English alignment results
Table (2) presents results for Chinese-English word
alignments. The training set for the corr(ME-
seq) model consisted of approximately 8000 hand-
aligned sentences sampled from LDC2006E93 and
LDC2008E57. The model was trained to correct
the output of the ME-seq aligner, and tested on
the same condition. For this language pair, refer-
ence parses were not available in our training set, so
1We do not distinguish sure and possible links in our anno-
tations - under this circumstance, alignment error rate(Och and
Ney, 2003) is 1? F .
automatically-generated parses were used for both
training and test sets. Results are measured on a 512
sentence test set, sampled from a wide variety of par-
allel corpora of various genres. We compare perfor-
mance with GIZA++, and with the ME-seq aligner.
Again the resulting improvement over the ME-seq
aligner is statistically significant. However, here the
improvement in recall is somewhat larger than the
improvement in precision.
6.3 Additional language pairs
Table (3) presents alignment results for seven other
language pairs. Separate alignment corrector mod-
els were trained for both directions of Italian ?
English and Portuguese ? English. The training
and test data vary by language, and are sampled
uniformly from a diverse set of corpora of various
genres, including newswire, and technical manuals.
Manual alignments for training and test data were
annotated. We compare performance with the ME-
seq aligner trained on the same training data. As
with the Chinese results above, customization and
feature development for the language pairs was min-
imal. In general, machine parses were always avail-
able for the English half of the pair. Machine parses
were also available for French and Spanish. Ma-
chine part of speech tags were available for all lan-
guage (although character-based heuristic was sub-
stituted for Japanese.) Large amounts (up to 10 mil-
lion sentence pairs) of unaligned parallel text was
available for model 1 type features. Our model ob-
tained improved alignment F -measure in all lan-
guage pairs, although the improvements were small
for ES?EN and FR?EN, the language pairs for
which the baseline accuracy was the highest.
6.4 Analysis
Some of the improvement can be attributed to ?look-
ahead? during the decoding. For example, the
English word ?the?, which (during Arabic-English
alignment) should often be aligned to the same Ara-
bic words to which its headword is linked. The num-
ber of errors associated with ?the? dropped from 383
(186 false alarms, 197 misses) in the ME-seq model
to 137 (60 false alarms and 77 misses) in the current
model.
In table 5, we show contributions to performance
resulting from various classes of features. The
895
Zh-En Ar-En
method correct miss fa correct miss fa
hmm 147 256 300
GIZA++ 139 677 396 132 271 370
ME-seq 71 745 133 127 276 191
corr(ME-seq) 358 458 231 264 139 114
Table 4: Analysis of 2?1 alignments errors (misses and false alarms) for Zh-En and Ar-En aligners
largest contribution is noted by removing features
based on the Model 1 translation matrices. These
features contain a wealth of lexical information
learned from approximately 7 ? 106 parallel sen-
tences - information that cannot be learned from
a relatively small amount of word-aligned train-
ing data. Geometrical features contribute more
than parse-based features, but the contribution from
parse-based features is important, and these are
more difficult to incorporate into sequential mod-
els. We note that all of the comparison aligners had
equivalent lexical information.
We show a small improvement from the unsuper-
vised adaptation - learning phrases from the parallel
corpus that are not captured by the lexical features
based on model 1. The final row in the table shows
the result of running the correction model on its own
output. The improvement is not statistically signif-
icant, but it is important to note the performance is
stable - a further indication that the model is robust
to a wide variety of input alignments, and that our
decoding scheme is a reasonable approach to find-
ing the best alignment.
In table 4, we characterize the errors based on the
fertility of the source and target words. We focus
on the case that exactly one target word is linked to
exactly two source words. These are the links that
feature R(%) P(%) F (%) Nexact
base 89 94 91 136
base-M1 82 88 85 89
base-geometric 83 90 86 92
base-parse 87 93 90 116
base+un.adapt 89 94 92 141
+iter2 90 94 92 141
Table 5: Importance of feature classes - ablation experi-
ments
corpus-level p90
alignment TER BLEU TER BLEU
ME-seq 56.06 32.65 64.20 21.31
corr(Me-seq) 56.25 33.10 63.47 22.02
both 56.07 33.13 63.41 22.14
Table 6: Translation results, Zh to En. BLEU=BLEUr4n4
alignment TER BLEUr1n4
ME-seq 35.02 69.94
corr(Me-seq ) 33.10 71.20
Table 7: Translation results, En to It
are poorly suited for the HMM and ME-seq mod-
els used in this comparison because of the chosen
directionality: the source (Arabic, Chinese) words
are the states and the target (English) words are the
observation. The HMM is able to produce these
links only by the use of posterior probabilities, rather
than viterbi decoding. The ME-seq model only pro-
duces these links because of language-specific post-
processing. GIZA++ has an underlying sequential
model, but uses both directionalities. The correc-
tion model improved performance across all three of
these links structures. The single exception is that
the number of 2?1 false alarms increased (Zh-En
alignments) but in this case, the first pass ME-seq
alignment produced few false alarms because it sim-
ply proposed few links of this form. It is also notable
that 1?2 links are more numerous than 2?1 links,
in both language pairs. This is consequence of the
choice of directionality and tokenization style.
6.5 Translation Impact
We tested the impact of improved alignments on
the performance of a phrase-based translation sys-
tem (Ittycheriah and Roukos, 2007) for three lan-
896
guage pairs. Our alignment did not improve the
performance of a mature Arabic to English trans-
lation system, but two notable successes were ob-
tained: Chinese to English, and English to Italian.
It is well known that improved alignment perfor-
mance does not always improve translation perfor-
mance (Fraser and Marcu, 2007). A mature machine
translation system may incorporate alignments ob-
tained from multiple aligners, or from both direc-
tions of an asymmetric aligner. Furthermore, with
large amounts of training data (the Gale Phase 4
Arabic English corpus consisting of 8 ? 106 sen-
tences,) a machine translation system is subject to
a saturation effect: correcting an alignment may
not yield a significant improvement because the the
phrases learned from the correct alignment have al-
ready been acquired in other contexts.
For the Chinese to English translation system (ta-
ble 6) the training corpus consisted of 11? 106 sen-
tence pairs, subsampled to 106. The test set was
NIST MT08 Newswire, consisting of 691 sentences
and 4 reference translations. Corpus-level perfor-
mance (columns 2 and 3) improved when measured
by BLEU, but not by TER. Performance on the
most difficult sentences (near the 90th percentile,
columns 4 and 5) improved on both BLEU and TER
(Snover et al, 2006), and the improvement in BLEU
was larger for the more difficult sentences than it
was overall. Translation performance further im-
proved, by a smaller amount, using bothME-seq and
corr(ME-seq) alignments during the training.
The improved alignments impacted the transla-
tion performance of the English to Italian transla-
tion system (table 7) even more strongly. Here the
training corpus consisted of 9.4?106 sentence pairs,
subsampled to 387000 pairs. The test set consisted
of 7899 sentences. Overall performance improved
as measured by both TER and BLEU (1.26 points.)
7 Conclusions
A log linear model for the alignment matrix is used
to guide systematic improvements to an existing
aligner. Our system models arbitrary alignment ma-
trices and allows features that incorporate such in-
formation as correlations based on parse trees in
both languages. We train models to correct the er-
rors of several existing aligners; we find the resulting
models are robust to using different aligners as start-
ing points. Improvements in alignment F -measure,
often significant improvements, show that our model
successfully corrects input alignments from existing
models in all nine language pairs tested. The result-
ing Chinese-English and English-Italian word align-
ments also improved translation performance, espe-
cially on the English-Italian test, and notably on the
particularly difficult subset of the Chinese sentences.
Future work will assess its impact on translation for
the other language pairs, as well as its impact on
other tasks, such as named entity projection.
8 Acknowledgements
We would like to acknowledge the support of
DARPA under Grant HR0011-08-C-0110 for fund-
ing part of this work. The views, opinions, and/or
findings contained in this article/presentation are
those of the author/presenter and should not be in-
terpreted as representing the official views or poli-
cies, either expressed or implied, of the Defense Ad-
vanced Research Projects Agency or the Department
of Defense.
References
Necip Fazil Ayan, Bonnie J. Dorr, and Christof Monz.
2005. Alignment link projection using transformation-
based learning. In Proceedings of the conference on
Human Language Technology and Empirical Methods
in Natural Language Processing, HLT ?05, pages 185?
192, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Yassine Benajiba and Imed Zitouni. 2010. Enhanc-
ing mention detection using projection via aligned
corpora. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?10, pages 993?1001. Association for Com-
putational Linguistics.
Phil Blunsom and Trevor Cohn. 2006. Discriminative
word alignment with conditional random fields. In In
Proc. of ACL-2006, pages 65?72.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematic
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Yonggang Deng and Bowen Zhou. 2009. Optimizing
word alignment combination for phrase table training.
In Proceedings of the ACL-IJCNLP 2009 Conference
897
Short Papers, ACLShort ?09, pages 229?232, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
B. Efron and R. Tibshirani. 1986. Bootstrap meth-
ods for standard errors, confidence intervals, and other
measures of statistical accuracy. Statistical Science,
1(1):pp. 54?75.
Victoria Fossum, Kevin Knight, and Steven Abney. 2008.
Using syntax to improve word alignment precision for
syntax-based machine translation. In Proceedings of
the Third Workshop on Statistical Machine Transla-
tion, StatMT ?08, pages 44?52. Association for Com-
putational Linguistics.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine transla-
tion. Comput. Linguist., 33(3):293?303.
Niyu Ge. 2004. Improvement in word alignments. In
DARPA/TIDES MT workshop.
Abraham Ittycheriah and Salim Roukos. 2005. A maxi-
mum entropy word aligner for arabic-english machine
translation. In HLT-EMNLP, pages 89?96.
Abraham Ittycheriah and Salim Roukos. 2007. Direct
translation model 2. In Human Language Technolo-
gies 2007: The Conference of the NA-ACL, pages 57?
64, Rochester, New York, April. Association for Com-
putational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL.
Young-Suk Lee. 2004. Morphological analysis for sta-
tistical machine translation. In Proceedings of HLT-
NAACL 2004: Short Papers on XX, HLT-NAACL ?04,
pages 57?60. Association for Computational Linguis-
tics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the main con-
ference on Human Language Technology Conference
of the North American Chapter of the Association of
Computational Linguistics, pages 104?111. Associa-
tion for Computational Linguistics.
Dong C. Liu and Jorge Nocedal. 1989. On the lim-
ited memory bfgs method for large scale optimization.
Mathematical Programming, 45:503?528.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-linear
models for word alignment. In ACL ?05: Proceedings
of the 43rd Annual Meeting on Association for Com-
putational Linguistics, pages 459?466. Association for
Computational Linguistics.
Robert C. Moore, Wen-tau Yih, and Andreas Bode. 2006.
Improved discriminative bilingual word alignment. In
ACL-44: Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 513?520. Association for Computa-
tional Linguistics.
Robert C. Moore. 2005. A discriminative framework for
bilingual word alignment. In In Proceedings of HLT-
EMNLP, pages 81?88.
Jan Niehues and Stephan Vogel. 2008. Discrimina-
tive word alignment via alignment matrix modeling.
In Proceedings of the Third Workshop on Statistical
Machine Translation, pages 18?25, Columbus, Ohio,
June. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Adwait Ratnaparkhi. 1999. Learning to parse natu-
ral language with maximum entropy models. Mach.
Learn., 34:151?175, February.
Hendra Setiawan, Chris Dyer, and Philip Resnik. 2010.
Discriminative word alignment with a function word
reordering model. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?10, pages 534?544. Association for
Computational Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In
Proceedings of Association for Machine Translation in
the Americas.
Ben Taskar, Simon Lacoste-julien, and Dan Klein. 2005.
A discriminative matching approach to word align-
ment. In In Proceedings of HLT-EMNLP, pages 73?
80.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the 16th conference on Com-
putational linguistics, pages 836?841.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001. Inducing multilingual text analysis tools via ro-
bust projection across aligned corpora. In Proceedings
of the first international conference on Human lan-
guage technology research, HLT ?01, pages 1?8. As-
sociation for Computational Linguistics.
898
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 424?428,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Discriminative Feature-Tied Mixture Modeling for Statistical Machine
Translation
Bing Xiang and Abraham Ittycheriah
IBM T. J. Watson Research Center
Yorktown Heights, NY 10598
{bxiang,abei}@us.ibm.com
Abstract
In this paper we present a novel discrimi-
native mixture model for statistical machine
translation (SMT). We model the feature space
with a log-linear combination of multiple mix-
ture components. Each component contains a
large set of features trained in a maximum-
entropy framework. All features within the
same mixture component are tied and share
the same mixture weights, where the mixture
weights are trained discriminatively to max-
imize the translation performance. This ap-
proach aims at bridging the gap between the
maximum-likelihood training and the discrim-
inative training for SMT. It is shown that the
feature space can be partitioned in a vari-
ety of ways, such as based on feature types,
word alignments, or domains, for various ap-
plications. The proposed approach improves
the translation performance significantly on a
large-scale Arabic-to-English MT task.
1 Introduction
Significant progress has been made in statisti-
cal machine translation (SMT) in recent years.
Among all the proposed approaches, the phrase-
based method (Koehn et al, 2003) has become the
widely adopted one in SMT due to its capability
of capturing local context information from adja-
cent words. There exists significant amount of work
focused on the improvement of translation perfor-
mance with better features. The feature set could be
either small (at the order of 10), or large (up to mil-
lions). For example, the system described in (Koehn
et al, 2003) is a widely known one using small num-
ber of features in a maximum-entropy (log-linear)
model (Och and Ney, 2002). The features include
phrase translation probabilities, lexical probabilities,
number of phrases, and language model scores, etc.
The feature weights are usually optimized with min-
imum error rate training (MERT) as in (Och, 2003).
Besides the MERT-based feature weight opti-
mization, there exist other alternative discriminative
training methods for MT, such as in (Tillmann and
Zhang, 2006; Liang et al, 2006; Blunsom et al,
2008). However, scalability is a challenge for these
approaches, where all possible translations of each
training example need to be searched, which is com-
putationally expensive.
In (Chiang et al, 2009), there are 11K syntac-
tic features proposed for a hierarchical phrase-based
system. The feature weights are trained with the
Margin Infused Relaxed Algorithm (MIRA) effi-
ciently on a forest of translations from a develop-
ment set. Even though significant improvement has
been obtained compared to the baseline that has
small number of features, it is hard to apply the
same approach to millions of features due to the data
sparseness issue, since the development set is usu-
ally small.
In (Ittycheriah and Roukos, 2007), a maximum
entropy (ME) model is proposed, which utilizes mil-
lions of features. All the feature weights are trained
with a maximum-likelihood (ML) approach on the
full training corpus. It achieves significantly bet-
ter performance than a normal phrase-based system.
However, the estimation of feature weights has no
direct connection with the final translation perfor-
424
mance.
In this paper, we propose a hybrid framework, a
discriminative mixture model, to bridge the gap be-
tween the ML training and the discriminative train-
ing for SMT. In Section 2, we briefly review the ME
baseline of this work. In Section 3, we introduce the
discriminative mixture model that combines various
types of features. In Section 4, we present experi-
mental results on a large-scale Arabic-English MT
task with focuses on feature combination, alignment
combination, and domain adaptation, respectively.
Section 5 concludes the paper.
2 Maximum-Entropy Model for MT
In this section we give a brief review of a special
maximum-entropy (ME) model as introduced in (It-
tycheriah and Roukos, 2007). The model has the
following form,
p(t, j|s) = p0(t, j|s)
Z(s)
exp
?
i
?i?i(t, j, s), (1)
where s is a source phrase, and t is a target phrase.
j is the jump distance from the previously translated
source word to the current source word. During
training j can vary widely due to automatic word
alignment in the parallel corpus. To limit the sparse-
ness created by long jumps, j is capped to a win-
dow of source words (-5 to 5 words) around the last
translated source word. Jumps outside the window
are treated as being to the edge of the window. In
Eq. (1), p0 is a prior distribution, Z is a normal-
izing term, and ?i(t, j, s) are the features of the
model, each being a binary question asked about the
source, distortion, and target information. The fea-
ture weights ?i can be estimated with the Improved
Iterative Scaling (IIS) algorithm (Della Pietra et al,
1997), a maximum-likelihood-based approach.
3 Discriminative Mixture Model
3.1 Mixture Model
Now we introduce the discriminative mixture model.
Suppose we partition the feature space into multiple
clusters (details in Section 3.2). Let the probabil-
ity of target phrase and jump given certain source
phrase for cluster k be
pk(t, j|s) =
1
Zk(s)
exp
?
i
?ki?ki(t, j, s), (2)
where Zk is a normalizing factor for cluster k.
We propose a log-linear mixture model as shown
in Eq. (3).
p(t, j|s) = p0(t, j|s)
Z(s)
?
k
pk(t, j|s)wk . (3)
It can be rewritten in the log domain as
log p(t, j|s) = log p0(t, j|s)
Z(s)
+
?
k
wk log pk(t, j|s)
= log
p0(t, j|s)
Z(s)
?
?
k
wk log Zk(s)
+
?
k
wk
?
i
?ki?ki(t, j, s). (4)
The individual feature weights ?ki for the i-th
feature in cluster k are estimated in the maximum-
entropy framework as in the baseline model. How-
ever, the mixture weights wk can be optimized di-
rectly towards the translation evaluation metric, such
as BLEU (Papineni et al, 2002), along with other
usual costs (e.g. language model scores) on a devel-
opment set. Note that the number of mixture com-
ponents is relatively small (less than 10) compared
to millions of features in baseline. Hence the opti-
mization can be conducted easily to generate reliable
mixture weights for decoding with MERT (Och,
2003) or other optimization algorithms, such as
the Simplex Armijo Downhill algorithm proposed
in (Zhao and Chen, 2009).
3.2 Partition of Feature Space
Given the proposed mixture model, how to split the
feature space into multiple regions becomes crucial.
In order to surpass the baseline model, where all
features can be viewed as existing in a single mix-
ture component, the separated mixture components
should be complementary to each other. In this
work, we explore three different ways of partitions,
based on either feature types, word alignment types,
or the domain of training data.
In the feature-type-based partition, we split the
ME features into 8 categories:
? F1: Lexical features that examine source word,
target word and jump;
425
? F2: Lexical context features that examine
source word, target word, the previous source
word, the next source word and jump;
? F3: Lexical context features that examine
source word, target word, the previous source
word, the previous target word and jump;
? F4: Lexical context features that examine
source word, target word, the previous or next
source word and jump;
? F5: Segmentation features based on mor-
phological analysis that examine source mor-
phemes, target word and jump;
? F6: Part-of-speech (POS) features that examine
the source and target POS tags and their neigh-
bors, along with target word and jump;
? F7: Source parse tree features that collect the
information from the parse labels of the source
words and their siblings in the parse trees,
along with target word and jump;
? F8: Coverage features that examine the cover-
age status of the source words to the left and
to the right. They fire only if the left source
is open (untranslated) or the right source is
closed.
All the features falling in the same feature cate-
gory/cluster are tied to each other to share the same
mixture weights at the upper level as in Eq. (3).
Besides the feature-type-based clustering, we can
also divide the feature space based on word align-
ment types, such as supervised alignment versus un-
supervised alignment (to be described in the exper-
iment section). For each type of word alignment,
we build a mixture component with millions of ME
features. On the task of domain adaptation, we
can also split the training data based on their do-
main/resources, with each mixture component rep-
resenting a specific domain.
4 Experiments
4.1 Data and Baseline
We conduct a set of experiments on an Arabic-to-
English MT task. The training data includes the UN
parallel corpus and LDC-released parallel corpora,
with about 10M sentence pairs and 300M words in
total (counted at the English side). For each sentence
in the training, three types of word alignments are
created: maximum entropy alignment (Ittycheriah
and Roukos, 2005), GIZA++ alignment (Och and
Ney, 2000), and HMM alignment (Vogel et al,
1996). Our tuning and test sets are extracted from
the GALE DEV10 Newswire set, with no overlap
between tuning and test. There are 1063 sentences
(168 documents) in the tuning set, and 1089 sen-
tences (168 documents) in the test set. Both sets
have one reference translation for each sentence. In-
stead of using all the training data, we sample the
training corpus based on the tuning/test set to train
the systems more efficiently. In the end, about 1.5M
sentence pairs are selected for the sampled training.
A 5-gram language model is trained from the En-
glish Gigaword corpus and the English portion of the
parallel corpus used in the translation model train-
ing. In this work, the decoding weights for both
the baseline and the mixture model are tuned with
the Simplex Armijo Downhill algorithm (Zhao and
Chen, 2009) towards the maximum BLEU.
System Features BLEU
F1 685K 37.11
F2 5516K 38.43
F3 4457K 37.75
F4 3884K 37.56
F5 103K 36.03
F6 325K 37.89
F7 1584K 38.56
F8 1605K 37.49
Baseline 18159K 39.36
Mixture 18159K 39.97
Table 1: MT results with individual mixture component
(F1 to F8), baseline, or mixture model.
4.2 Feature Combination
We first experiment with the feature-type-based
clustering as described in Section 3.2. The trans-
lation results on the test set from the baseline and
the mixture model are listed in Table 1. The MT
performance is measured with the widely adopted
BLEU metric. We also evaluate the systems that uti-
lize only one of the mixture components (F1 to F8).
The number of features used in each system is also
426
listed in the table. As we can see, when using all
18M features in the baseline model, without mixture
weighting, the baseline achieved 3.3 points higher
BLEU score than F5 (the worst component), and 0.8
higher BLEU score than F7 (the best component).
With the log-linear mixture model, we obtained 0.6
gain compared to the baseline. Since there are ex-
actly the same number of features in the baseline
and mixture model, the better performance is due
to two facts: separate training of the feature weights
? within each mixture component; the discrimina-
tive training of mixture weights w. The first one al-
lows better parameter estimation given the number
of features in each mixture component is much less
than that in the baseline. The second factor connects
the mixture weighting to the final translation perfor-
mance directly. In the baseline, all feature weights
are trained together solely under the maximum like-
lihood criterion, with no differentiation of the vari-
ous types of features in terms of their contribution to
the translation performance.
System Features BLEU
ME 5687K 39.04
GIZA 5716K 38.75
HMM 5589K 38.65
Baseline 18159K 39.36
Mixture 16992K 39.86
Table 2: MT results with different alignments, baseline,
or mixture model.
4.3 Alignment Combination
In the baseline mentioned above, three types of word
alignments are used (via corpus concatenation) for
phrase extraction and feature training. Given the
mixture model structure, we can apply it to an align-
ment combination problem. With the phrase table
extracted from all the alignments, we train three
feature mixture components, each on one type of
alignments. Each mixture component contains mil-
lions of features from all feature types described in
Section 3.2. Again, the mixture weights are op-
timized towards the maximum BLEU. The results
are shown in Table 2. The baseline system only
achieved 0.3 minor gain compared to extracting fea-
tures from ME alignment only (note that phrases are
from all the alignments). With the mixture model,
we can achieve another 0.5 gain compared to the
baseline, especially with less number of features.
This presents a new way of doing alignment com-
bination in the feature space instead of in the usual
phrase space.
System Features BLEU
Newswire 8898K 38.82
Weblog 1990K 38.20
UN 4700K 38.21
Baseline 18159K 39.36
Mixture 15588K 39.81
Table 3: MT results with different training sub-corpora,
baseline, or mixture model.
4.4 Domain Adaptation
Another popular task in SMT is domain adapta-
tion (Foster et al, 2010). It tries to take advantage of
any out-of-domain training data by combining them
with the in-domain data in an appropriate way. In
our sub-sampled training corpus, there exist three
subsets: newswire (1M sentences), weblog (200K),
and UN data (300K). We train three mixture com-
ponents, each on one of the training subsets. All re-
sults are compared in Table 3. The baseline that was
trained on all the data achieved 0.5 gain compared to
using the newswire training data alone (understand-
ably it is the best component given the newswire test
data). Note that since the baseline is trained on sub-
sampled training data, there is already certain do-
main adaptation effect involved. On top of that, the
mixture model results in another 0.45 gain in BLEU.
All the improvements in the mixture models above
against the baseline are statistically significant with
p-value < 0.0001 by using the confidence tool de-
scribed in (Zhang and Vogel, 2004).
5 Conclusion
In this paper we presented a novel discriminative
mixture model for bridging the gap between the
maximum-likelihood training and the discriminative
training in SMT. We partition the feature space into
multiple regions. The features in each region are tied
together to share the same mixture weights that are
optimized towards the maximum BLEU scores. It
was shown that the same model structure can be ef-
427
fectively applied to feature combination, alignment
combination and domain adaptation. We also point
out that it is straightforward to combine any of these
three. For example, we can cluster the features based
on both feature types and alignments. Further im-
provement may be achieved with other feature space
partition approaches in the future.
Acknowledgments
We would like to acknowledge the support of
DARPA under Grant HR0011-08-C-0110 for fund-
ing part of this work. The views, opinions, and/or
findings contained in this article/presentation are
those of the author/presenter and should not be in-
terpreted as representing the official views or poli-
cies, either expressed or implied, of the Defense Ad-
vanced Research Projects Agency or the Department
of Defense.
References
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proceedings of ACL-08:HLT.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In Proceedings of NAACL-HLT.
Stephen Della Pietra, Vincent Della Pietra, and John Laf-
ferty. 1997. Inducing features of random fields. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in satistical machine translation. In Proceedings
of EMNLP.
Abraham Ittycheriah and Salim Roukos. 2005. A maxi-
mum entropy word aligner for arabic-english machine
translation. In Proceedings of HLT/EMNLP, pages
89?96, October.
Abraham Ittycheriah and Salim Roukos. 2007. Di-
rect translation model 2. In Proceedings HLT/NAACL,
pages 57?64, April.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
NAACL/HLT.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative
approach to machine translation. In Proceedings of
ACL/COLING, pages 761?768, Sydney, Australia.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of ACL,
pages 440?447, Hong Kong, China, October.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translations. In Proceedings of ACL,
pages 295?302, Philadelphia, PA, July.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL,
pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
pages 311?318.
Christoph Tillmann and Tong Zhang. 2006. A discrim-
inative global training algorithm for statistical mt. In
Proceedings of ACL/COLING, pages 721?728, Syd-
ney, Australia.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical trans-
lation. In Proceedings of COLING, pages 836?841.
Ying Zhang and Stephan Vogel. 2004. Measuring con-
fidence intervals for the machine translation evalua-
tion metrics. In Proceedings of The 10th International
Conference on Theoretical and Methodological Issues
in Machine Translation.
Bing Zhao and Shengyuan Chen. 2009. A simplex
armijo downhill algorithm for optimizing statistical
machine translation decoding parameters. In Proceed-
ings of NAACL-HLT.
428
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 861?870,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Adaptive HTER Estimation for Document-Specific MT Post-Editing
Fei Huang
?
Facebook Inc.
Menlo Park, CA
feihuang@fb.com
Jian-Ming Xu Abraham Ittycheriah
IBM T.J. Watson Research Center
Yorktown Heights, NY
{jianxu, abei, roukos}@us.ibm.com
Salim Roukos
Abstract
We present an adaptive translation qual-
ity estimation (QE) method to predict
the human-targeted translation error rate
(HTER) for a document-specific machine
translation model. We first introduce fea-
tures derived internal to the translation de-
coding process as well as externally from
the source sentence analysis. We show
the effectiveness of such features in both
classification and regression of MT qual-
ity. By dynamically training the QE model
for the document-specific MT model, we
are able to achieve consistency and pre-
diction quality across multiple documents,
demonstrated by the higher correlation co-
efficient and F-scores in finding Good sen-
tences. Additionally, the proposed method
is applied to IBM English-to-Japanese MT
post editing field study and we observe
strong correlation with human preference,
with a 10% increase in human translators?
productivity.
1 Introduction
Machine translation (MT) systems suffer from an
inconsistent and unstable translation quality. De-
pending on the difficulty of the input sentences
(sentence length, OOV words, complex sentence
structures and the coverage of the MT system?s
training data), some translation outputs can be per-
fect, while others are ungrammatical, missing im-
portant words or even totally garbled. As a result,
users do not know whether they can trust the trans-
lation output unless they spend time to analyze
?
This work was done when the author was with IBM Re-
search.
the MT output. This shortcoming is one of the
main obstacles for the adoption of MT systems,
especially in machine assisted human translation:
MT post-editing, where human translators have
an option to edit MT proposals or translate from
scratch. It has been observed that human trans-
lators often discard MT proposals even if some
are very accurate. If MT proposals are used prop-
erly, post-editing can increase translators produc-
tivity and lead to significant cost savings. There-
fore, it is beneficial to provide MT confidence es-
timation, to help the translators to decide whether
to accept MT proposals, making minor modifica-
tions on MT proposals when the quality is high
or translating from scratching when the quality is
low. This will save the time of reading and parsing
low quality MT and improve user experience.
In this paper we propose an adaptive qual-
ity estimation that predicts sentence-level human-
targeted translation error rate (HTER) (Snover et
al., 2006) for a document-specific MT post-editing
system. HTER is an ideal quality measurement
for MT post editing since the reference is ob-
tained from human correction of the MT output.
Document-specific MT model is an MT model that
is specifically built for the given input document.
It is demonstrated in (Roukos et al, 2012) that
document-specific MT models significantly im-
prove the translation quality. However, this raises
two issues for quality estimation. First, existing
approaches to MT quality estimation rely on lex-
ical and syntactical features defined over parallel
sentence pairs, which includes source sentences,
MT outputs and references, and translation models
(Blatz et al, 2004; Ueffing and Ney, 2007; Spe-
cia et al, 2009a; Xiong et al, 2010; Soricut and
Echihabi, 2010a; Bach et al, 2011). Therefore,
when the MT quality estimation model is trained,
861
it can not be adapted to provide accurate estimates
on the outputs of document-specific MT models.
Second, the MT quality estimation might be in-
consistent across different document-specific MT
models, thus the confidence score is unreliable and
not very helpful to users.
In contrast to traditional static MT quality es-
timation methods, our approach not only trains
the MT quality estimator dynamically for each
document-specific MT model to obtain higher pre-
diction accuracy, but also achieves consistency
over different document-specific MT models. The
experiments show that our MT quality estima-
tion is highly correlated with human judgment
and helps translators to increase the MT proposal
adoption rate in post-editing.
We will review related work on MT quality es-
timation in section 2. In section 3 we will intro-
duce the document-specific MT system built for
post-editing. We describe the static quality estima-
tion method in section 4, and propose the adaptive
quality estimation method in section 5. In section
6 we demonstrate the improvement of MT quality
estimation with our method, followed by discus-
sion and conclusion in section 7.
2 Related Work
There has been a long history of study in con-
fidence estimation of machine translation. The
work of (Blatz et al, 2004) is among the best
known study of sentence and word level features
for translation error prediction. Along this line of
research, improvements can be obtained by incor-
porating more features as shown in (Quirk, 2004;
Sanchis et al, 2007; Raybaud et al, 2009; Specia
et al, 2009b). Soricut and Echihabi (2010b) pro-
posed various regression models to predict the ex-
pected BLEU score of a given sentence translation
hypothesis. Ueffing and Hey (2007) introduced
word posterior probabilities (WPP) features and
applied them in the n-best list reranking. Target
part-of-speech and null dependency link are ex-
ploited in a MaxEnt classifier to improve the MT
quality estimation (Xiong et al, 2010).
Quality estimation focusing on MT post-editing
has been an active research topic, especially after
the WMT 2012 (Callison-Burch et al, 2012) and
WMT2013 (Bojar et al, 2013) workshops with
the ?Quality Estimation? shared task. Bic?ici et
al. (2013) proposes a number of features mea-
suring the similarity of the source sentence to the
source side of the MT training corpus, which,
combined with features from translation output,
achieved significantly superior performance in the
MT QE evaluation. Felice and Specia (2012) in-
vestigates the impact of a large set of linguisti-
cally inspired features on quality estimation accu-
racy, which are not able to outperform the shal-
lower features based on word statistics. Gonz?alez-
Rubio et al (2013) proposed a principled method
for performing regression for quality estimation
using dimensionality reduction techniques based
on partial least squares regression. Given the fea-
ture redundancy in MT QE, their approach is able
to improve prediction accuracy while significantly
reducing the size of the feature sets.
3 Document-specific MT System
In our MT post-editing setup, we are given docu-
ments in the domain of software manuals, techni-
cal outlook or customer support materials. Each
translation request comes as a document with sev-
eral thousand sentences, focusing on a specific
topic, such as the user manual of some software.
The input documents are automatically seg-
mented into sentences, which are also called seg-
ments. Thus in the rest of the paper we will use
sentences and segments interchangeably. Our par-
allel corpora includes tens of millions of sentence
pairs covering a wide range of topics. Building
a general MT system using all the parallel data
not only produces a huge translation model (unless
with very aggressive pruning), the performance on
the given input document is suboptimal due to the
unwanted dominance of out-of-domain data. Past
research suggests using weighted sentences or cor-
pora for domain adaptation (Lu et al, 2007; Mat-
soukas et al, 2009; Foster et al, 2010). Here
we adopt the same strategy, building a document-
specific translation model for each input docu-
ment.
The document-specific system is built based on
sub-sampling: from the parallel corpora we se-
lect sentence pairs that are the most similar to
the sentences from the input document, then build
the MT system with the sub-sampled sentence
pairs. The similarity is defined as the number of
n-grams that appear in both source sentences, di-
vided by the input sentence?s length, with higher
weights assigned to longer n-grams. From the
extracted sentence pairs, we utilize the standard
pipeline in SMT system building: word align-
862
Figure 1: Adaptive QE for document-specific MT system.
ment (HMM (Vogel et al, 1996) and MaxEnt (It-
tycheriah and Roukos, 2005) alignment models,
phrase pair extraction, MT model training (Itty-
cheriah and Roukos, 2007) and LM model train-
ing. The top region within the dashed line in Fig-
ure 1 shows the overall system built pipeline.
3.1 MT Decoder
The MT decoder (Ittycheriah and Roukos, 2007)
employed in our study extracts various features
(source words, morphemes and POS tags, target
words and POS tags, etc.) with their weights
trained in a maximum entropy framework. These
features are combined with other features used in
a typical phrase-based translation system. Alto-
gether the decoder incorporates 17 features with
weights estimated by PRO (Hopkins and May,
2011) in the decoding process, and achieves
state-of-the-art translation performance in vari-
ous Arabic-English translation evaluations (NIST
MT2008, GALE and BOLT projects).
4 Static MT Quality Estimation
MT quality estimation is typically formulated as
a prediction problem: estimating the confidence
score or translation error rate of the translated sen-
tences or documents based on a set of features. In
this work, we adopt HTER in (Snover et al, 2006)
as our prediction output. HTER measures the per-
centage of insertions, deletions, substitutions and
shifts needed to correct the MT outputs. In the
rest of the paper, we use TER and HTER inter-
changably.
In this section we will first introduce the set of
features, and then discuss MT QE problem from
classification and regression point of views.
4.1 Features for MT QE
The features for quality estimation should reflect
the complexity of the source sentence and the de-
coding process. Therefore we conduct syntactic
analysis on the source sentences, extract features
from the decoding process and select the follow-
ing 26 features:
? 17 decoding features, including phrase
translation probabilities (source-to-target and
target-to-source), word translation probabil-
ities (also in both directions), maxent prob-
abilities
1
, word count, phrase count, distor-
1
The maxent probability is the translation probability
863
tion probabilities, as well as a set of language
model scores.
? Sentence length, i.e., the number of words in
the source sentence.
? Source sentence syntactic features, including
the number of noun phrases, verb phrases,
adjective phrases, adverb phrases, as in-
spired by (Green et al, 2013).
? The length of verb phrases, because verbs are
typically the roots in dependency structure
and they have more varieties during transla-
tion.
? The maximum length of source phrases in
the final translation, since longer matching
source phrase indicates better coverage of the
input sentence with possibly better transla-
tions.
? The number of phrase pairs with high fuzzy
match (FM) score. The high FM phrases are
selected from sentence pairs which are clos-
est in terms of n-gram overlap to the input
sentence. These sentences are often found in
previous translations of the software manual,
and thus are very helpful for translating the
current sentence.
? The average translation probability of the
phrase translation pairs in the final transla-
tion, which provides the overall translation
quality on the phrase level.
The first 17 features come from the decod-
ing process, which are called ?decoding features?.
The remaining 9 features not related to the de-
coder are called ?external features?. To evaluate
the effectiveness of the proposed features, we train
various classifiers with different feature configura-
tions to predict whether a translation output is use-
ful (with lower TER) as described in the following
section.
4.2 MT QE as Classification
Predicting TER with various input features can
be treated as a regression problem. However for
the post-editing task, we argue that it could also
be cast as a classification problem: MT system
derived from a Maximum Entropy translation model (Itty-
cheriah and Roukos, 2005).
Configuration Training set Test set
Baseline (All negative) 80% 77%
17 decoding features only 89% 79%
9 external features only 85% 81%
total 26 features 92% 83%
Table 1: QE classification accuracy with different
feature configurations
users (including the translators) are often inter-
ested to know whether a given translation is rea-
sonably good or not. If useful, they can quickly
look through the translation and make minor mod-
ifications. On the other hand, they will just skip
reading and parsing the bad translation, and prefer
to translate by themselves from scratch. Therefore
we also develop algorithms that classify the trans-
lation at different levels, depending on whether the
TER is less than a given threshold. In our experi-
ments, we set TER=0.1 as the threshold.
We randomly select one input document with
2067 sentences for the experiment. We build
a document-specific MT system to translate this
document, then ask human translator to correct
the translation output. We compute TER for each
sentence using the human correction as the refer-
ence. The TER of the whole document is 0.31,
which means about 30% errors should be cor-
rected. In the classification task, our goal is to pre-
dict whether a sentence is a Good translation (with
TER ? 0.1), and label them for human correction.
We adopt a decision tree-based classifier, experi-
menting with different feature configurations. We
select the top 1867 sentences for training and the
bottom 200 sentences for test. In the test set, there
are 46 sentences with TER ? 0.1. Table 1 shows
the classification accuracy.
First we can see that as the overall TER is
around 0.3, predicting all the sentences being neg-
ative already has a strong baseline: 77%. How-
ever this is not helpful for the human translators,
because that means they have to translate every
sentence from scratch, and consequently there is
no productivity gain from MT post-editing. If we
only use the 17 decoding features, it improves the
classification accuracy by 9% on the training set,
but only 2% on the test set. This is probably due to
the overfitting when training the decision tree clas-
sifier. While using the 7 external features, the gain
on training set is less but the gain on the test set
864
is greater (4% improvement), because the trans-
lation output is generated based on the log-linear
combination of these decoding features, which are
biased towards the final translations. The exter-
nal features capture the syntactic structure of the
source sentence, as well as the coverage of the
training data with regard to the input sentence,
which are good indicators of the translation qual-
ity. Combining both the decoding features and the
external features, we observed the best accuracy
on both the training and test set. We will use the
combined 26 features in the following work.
4.3 MT QE as Regression
For the QE regression task, we predict the TER for
each sentence translation using the above 26 fea-
tures. We experiment with several classifiers: lin-
ear regression model, decision tree based regres-
sion model and SVM model. With the same train-
ing and test data set up, we predict the TER for
each sentence in the test set, and compute the cor-
relation coefficient (r) and root mean square error
(RMSE). Our experiments show that the decision
tree-based regression model obtains the highest
correlation coefficients (0.53) and lowest RMSE
(0.23) in both the training and test sets. We will
use this model for the adaptive MT QE in the fol-
lowing work.
5 Adaptive MT Quality Estimation
The above QE regression model is trained on a
portion of the sentences from the input document,
and evaluated on the remaining sentences from the
same document. One would like to know whether
the trained model can achieve consistent TER pre-
diction accuracy on other documents. When we
use the cross-document models for prediction, the
correlation is significantly worse (the details are
discussed in section 6.1). Therefore it is neces-
sary to build a QE regression model that?s robust
to different document-specific translation models.
To deal with this problem, we propose this adap-
tive MT QE method described below.
Our proposed method is as follows: we select a
fixed set of sentence pairs (S
q
, R
q
) to train the QE
model. The source side of the QE training data
S
q
is combined with the input document S
d
for
MT system training data subsampling. Once the
document-specific MT system is trained, we use it
to translate both the input document and the source
QE training data, obtaining the translation T
d
and
Figure 2: Correlation coefficient r between pre-
dicted TER (x-axis) and true TER (y-axis) for QE
models trained from the same document (top fig-
ure) or different document (bottom figure).
T
q
. We compute the TER of T
q
using R
q
as the
reference, and train a QE regression model with
the 26 features proposed in section 4.1. Then we
use this document-specific QE model to predict the
TER of the document translation T
d
. As the QE
model is adaptively re-trained for each document-
specific MT system, its prediction is more accurate
and consistent. Figure 1 shows the flow of our MT
system with the adaptive QE training integrated as
part of the built.
6 Experiments
In this section, we first discuss experiments that
compare adaptive QE method and static QE
method on a few documents, and then present
results we obtained after deploying the adaptive
QE method in an English-to-Japanese MT Post-
Editing project. As mentioned before, the main
motivation for us to develop MT QE classification
scheme is that translators often discard good MT
proposals and translate the segments from scratch.
We would like to provide translators with some
guidance on reasonably good MT proposals?the
sentences with low TERs?to help them increase
the leverage on MT proposals to achieve improved
productivity.
865
6.1 Evaluation on Test Set
Our experiment and evaluation is conducted over
three documents, each with about 2000 segments.
We first build document-specific MT model for
each document, then ask human translators to cor-
rect the MT outputs and obtain the reference trans-
lation. In a typical MT QE scenario, the QE model
is pre-trained and applied to various MT outputs,
even though the QE training data and MT out-
puts are generated from different translation mod-
els. To evaluate whether such model mismatch
matters, we compare the cross-model QE with the
same-model QE, where the QE training data and
the MT outputs are generated from the same MT
model.
We select one document LZA with 2067 sen-
tences. We use the first 1867 sentences to train the
static QE model and the remaining 200 sentences
are used as test set for TER prediction. We com-
pute the correlation coefficient (r) between each
predicted TER and true TER, as shown in Figure
2. We find that the TER predictions are reason-
ably correct when the training and test sentences
are from the same MT model (the top figure), with
correlation coefficients around 0.5. For the cross-
model QE, we train a static QE model with 1867
sentences from another document RTW, and use it
to predict the TER of the same 200 sentences from
document LZA (the bottom figure). We observe
significant degradation of correlation coefficient,
dropping from 0.5 to 0.1. This degradation and
unstable nature is the prime motivation to develop
a more robust MT quality estimation model.
We select 1700 sentences from multiple pre-
viously translated documents as the QE training
data, which are independent of the test documents.
We train the static QE model with this training set,
including the source sentences, references and MT
outputs (from multiple translation models). To
train the adaptive QE model for each test docu-
ment, we build a translation model whose subsam-
pling data includes source sentences from both the
test document and the QE training data. We trans-
late the QE source sentences with this newly built
MT model, and the translation output is used to
train the QE model specific to each test document.
We compare these two QE models on three doc-
uments, LZA, RTW and WC7, measuring r and
RMSE for each QE model. The result is shown
in Table 2. We find that the adaptive QE model
demonstrates higher r and lower RMSE than the
static QE model for all the test documents.
Besides the general correlation with human
judgment, we particularly focus on those reason-
ably good translations, i.e., the sentences with low
TERs which can help improve the translator?s pro-
ductivity most. Here we report the precision, re-
call and F-score of finding such ?Good? sentences
(with TER ? 0.1) on the three documents in Ta-
ble 3. Again, the adaptive QE model produces
higher recall, mostly higher precision, and signif-
icantly improved F-score. The overall F-score of
the adaptive QE model is 0.28
2
. Compared with
the static QE model?s 0.17 F-score, this is rela-
tively 64% improvement.
In the adaptive QE model, the source side QE
training data is included in the subsampling pro-
cess to build the document-specific MT model. It
would be interesting to know whether this process
will negatively affect the MT quality. We evaluate
the TER of MT outputs with and without the adap-
tive QE training on the same three documents. As
seen in Table 4, we do not notice translation qual-
ity degradation. Instead, we observe slightly im-
provement on two document, with TERs reduction
by 0.1-0.4 pt. As our MT model training data in-
clude proprietary data, the MT performance is sig-
nificantly better than publicly available MT soft-
ware.
6.2 Impact on Human Translators
We apply the proposed adaptive QE model to
large scale English-to-Japanese MT Post-Editing
project on 36 documents with 562K words. Each
English sentence can be categorized into 3 classes:
? Exact Match (EM): the source sentence is
completely covered in the bilingual training
corpora thus the corresponding target sen-
tence is returned as the translation;
? Fuzzy Match (FM): the source sentence is
similar to some sentence in the training data
(similarity measured by string editing dis-
tance), the corresponding fuzzy match target
sentence (FM proposal) as well as the MT
translation output (MT proposal) are returned
for human translators to select and correct;
? No Proposal (NP): there is no close match
source sentences in the training data (the FM
2
The adaptive QE model obtains much higher F-score
(80%) on the rest of the sentences (with TER > 0.1).
866
Document LZA RTW WC7
Num. of Sents 2067 2003 2405
r ? RMSE ? r ? RMSE ? r ? RMSE ?
Static QE 0.10 0.38 0.40 0.32 0.13 0.36
Adaptive QE 0.58 0.23 0.61 0.22 0.47 0.20
Table 2: QE regression with static and adaptive models
Document LZA RTW WC7
Num. of Sents 2067 2003 2405
P/R/F-score P/R/F-score P/R/F-score
Static QE 0.73/0.08/0.14 0.69/ 0.11/ 0.19 0.74/ 0.10/ 0.18
Adaptive QE 0.69/0.14/0.24 0.84/ 0.16/ 0.26 0.80/ 0.23/ 0.35
Table 3: Performance on predicting Good sentences with static and adaptive models
similarity score of 70% is used as the thresh-
old), therefore only the MT output is re-
turned.
EM sentences are excluded from the study be-
cause in general they do not require editing. We
focus on the FM and NP sentences
3
. In Table 5
we present the precision, recall and F-score of the
?Good? sentences in the FM and NP categories,
similar to those shown in Table 3. We consistently
observe higher performance on the FM sentences,
in terms of precision, recall and F-score. This is
expected because these sentences are well covered
in the training data. The overall F-score is in line
with the test set results shown in Table 3.
We are also interested to know whether the pro-
posed adaptive QE method is helpful to human
translators in the MT post-editing task. Based on
the TERs predicted by the adaptive QE model, we
assign each MT proposal with a confidence label:
High (0 ? TER ? 0.2), Medium (0.2 < TER ?
0.3), or Low (TER > 0.3). We present the MT pro-
posals with confidence labels to human translators,
then measure the percentage of sentences whose
MT proposals are used. From Table 6 and 7,
we can see that sentences with High and Medium
confidence labels are more frequently used by the
translators than those with Low labels, for both the
FM and NP categories. The MT usage for the FM
category is less than that for the NP category be-
cause translators can choose FM proposals instead
of the MT proposals for correction.
We also measure the translator?s productivity
gain for MT proposals with different confidence
3
The word count distribution of EM, FM and NP is 21%,
38% and 41%, respectively.
Document LZA RTW WC7
TER-Baseline 30.81 30.74 29.96
TER-with Adaptive QE 30.69 30.78 29.56
Table 4: MT Quality with and without Adaptive
QE measured by TER
labels. The productivity of a translator is defined
as the number of source words translated per unit
time. The post editing tool, IBM TranslationMan-
ager, records the time that a translator spends on
a segment and computes the number of characters
that a translator types on the segment so that we
can compute how many words the translator has
finished in a given time.
We choose the overall productivity of NP0 as
the base unit 1, where there is no proposal presents
and the translator has to translate the segments
from scratch. Measured with this unit, for exam-
ple, the overall productivity of FM0 being 1.14
implies a relative gain of 14% over that of NP0,
which demonstrates the effectiveness of FM pro-
posals.
Table 6 and 7 also show the productivity gain
on sentences with High, Medium and Low labels
from FM and NP categories. Again, the produc-
tivity gain is consistent with the confidence labels
from the adaptive QE model?s prediction. The
overall productivity gain with confidence-labeled
MT proposals is about 10% (comparing FM1 vs.
FM0 and NP1 vs. NP0). These results clearly
demonstrate the effectiveness of the adaptive QE
model in aiding the translators to make use of MT
proposals and improve productivity.
867
Category Class FM usage MT usage Productivity
High 33% 34% 1.35
FM1 Medium 47% 18% 1.21
Low 60% 8% 1.20
Overall 45% 21% 1.26
High 53% - 1.12
FM0 Medium 64% - 1.14
Low 67% - 1.16
Overall 59% - 1.14
Table 6: MT proposal usage and productivity gain in FM category.
In FM1, both Fuzzy Match and MT proposals present. In control class FM0, only Fuzzy Match proposals
present, and therefore, MT usage is not available for FM0. Strong correlation is observed between
predicted ?High? , ?Medium? and ?Low? sentences with MT usage and post editing productivity.
Category Class MT usage Productivity
High 50% 1.25
NP1 Medium 42% 1.08
Low 27% 1.00
Overall 38% 1.09
High - 1.08
NP0 Medium - 1.00
Low - 0.96
Overall - 1.00
Table 7: MT proposal usage and productivity gain in NP category.
In NP1, MT is the only proposal available, while in control NP0, there presents no proposal at all and
the translator has to translate from scratch. Strong correlation is observed between predicted ?High? ,
?Medium? and ?Low? sentences with MT usage and post editing productivity
868
Type Precision Recall F-score
FM 0.71 0.23 0.35
NP 0.67 0.18 0.29
Overall 0.69 0.21 0.32
Table 5: Performance on predicting Good sen-
tences (TER ? 0.1) by adaptive QE model
7 Discussion and Conclusion
In this paper we proposed a method to adaptively
train a quality estimation model for document-
specific MT post editing. With the 26 pro-
posed features derived from decoding process and
source sentence syntactic analysis, the proposed
QE model achieved better TER prediction, higher
correlation with human correction of MT output
and higher F-score in finding good translations.
The proposed adaptive QE model is deployed to
a large scale English-to-Japanese MT post edit-
ing project, showing strong correlation with hu-
man preference and leading to about 10% gain in
human translator productivity.
The training data for QE model can be selected
independent of the input document. With such
fixed QE training data, it is possible to measure the
consistency of the trained QE models, and to al-
low the sanity check of the document-specific MT
models. However, adding such data in the sub-
sampling process extracts more bilingual data for
building the MT models, which slightly increase
the model building time but increased the transla-
tion quality. Another option is to select the sen-
tence pairs from the MT system subsampled train-
ing data, which is more similar to the input docu-
ment thus the trained QE model could be a better
match to the input document. However, the QE
model training data is no longer constant. The
model consistency is no longer guaranteed, and
the QE training data must be removed from the
MT system training data to avoid data contamina-
tion.
References
Nguyen Bach, Fei Huang, and Yaser Al-Onaizan.
2011. Goodness: A method for measuring machine
translation confidence. In ACL, pages 211?219.
Ergun Bic?ici, Declan Groves, and Josef van Genabith.
2013. Predicting sentence translation quality using
extrinsic and language independent features. Ma-
chine Translation.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2004. Confidence es-
timation for machine translation. In Proceedings of
the 20th international conference on Computational
Linguistics, COLING ?04, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Ondrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut,
and Lucia Specia. 2013. Findings of the 2013
Workshop on Statistical Machine Translation. In
Eighth Workshop on Statistical Machine Transla-
tion, WMT-2013, pages 1?44, Sofia, Bulgaria.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Seventh Workshop on Statis-
tical Machine Translation, pages 10?51, Montr?eal,
Canada.
Mariano Felice and Lucia Specia. 2012. Linguistic
features for quality estimation. In Seventh Workshop
on Statistical Machine Translation, pages 96?103,
Montr?eal, Canada.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In Proceed-
ings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?10,
pages 451?459, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Jes?us Gonz?alez-Rubio, Jose Ram?on Navarro-Cerd?an,
and Francisco Casacuberta. 2013. Dimensionality
reduction methods for machine translation quality
estimation. Machine Translation, 27(3-4):281?301.
Spence Green, Jeffrey Heer, and Christopher D. Man-
ning. 2013. The efficacy of human post-editing for
language translation. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Sys-
tems, CHI ?13, pages 439?448, New York, NY,
USA. ACM.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Abraham Ittycheriah and Salim Roukos. 2005. A
maximum entropy word aligner for arabic-english
machine translation. In In Proceedings of HLT-
EMNLP, pages 89?96.
Abraham Ittycheriah and Salim Roukos. 2007. Direct
translation model 2. In In HLT-NAACL 2007: Main
Conference, pages 57?64.
Yajuan Lu, Jin Huang, and Qun Liu. 2007. Improv-
ing statistical machine translation performance by
869
training data selection and optimization. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 343?350, Prague, Czech Republic,
June. Association for Computational Linguistics.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight es-
timation for machine translation. In Proceedings
of the 2009 Conference on Empirical Methods in
Natural Language Processing: Volume 2 - Volume
2, EMNLP ?09, pages 708?717, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Christopher B. Quirk. 2004. Training a sentence-level
machine translation confidence measure. In In Pro-
ceedings of LREC.
Sylvain Raybaud, Caroline Lavecchia, David Langlois,
and Kamel Sma??li. 2009. New confidence mea-
sures for statistical machine translation. CoRR,
abs/0902.1033.
Salim Roukos, Abraham Ittycheriah, and Jian-Ming
Xu. 2012. Document-specific statistical machine
translation for improving human translation produc-
tivity. In Proceedings of the 13th international con-
ference on Computational Linguistics and Intelli-
gent Text Processing - Volume Part II, CICLing?12,
pages 25?39, Berlin, Heidelberg. Springer-Verlag.
Alberto Sanchis, Alfons Juan, Enrique Vidal, and De-
partament De Sistemes Informtics. 2007. Estima-
tion of confidence measures for machine translation.
In In Procedings of Machine Translation Summit XI.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In In Proceedings of Association for Machine
Translation in the Americas, pages 223?231.
Radu Soricut and Abdessamad Echihabi. 2010a.
Trustrank: inducing trust in automatic translations
via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, ACL ?10, pages 612?621, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Radu Soricut and Abdessamad Echihabi. 2010b.
Trustrank: Inducing trust in automatic translations
via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 612?621. Association for Computa-
tional Linguistics.
Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran
Wang, and John Shawe-taylor. 2009a. Improving
the confidence of machine translation quality esti-
mates. In In Proceedings of MT Summit XII.
Lucia Specia, Marco Turchi, Zhuoran Wang, John
Shawe-Taylor, and Craig Saunders. 2009b. Improv-
ing the confidence of machine translation quality es-
timates.
Nicola Ueffing and Hermann Ney. 2007. Word-
level confidence estimation for machine translation.
Computational Linguistics, 33(1):9?40.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical
translation. In Proceedings of the 16th Conference
on Computational Linguistics - Volume 2, COLING
?96, pages 836?841, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Er-
ror detection for statistical machine translation using
linguistic features. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 604?611, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
870
Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 61?69,
ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational Linguistics
Improving Reordering for Statistical Machine Translation with Smoothed
Priors and Syntactic Features
Bing Xiang, Niyu Ge, and Abraham Ittycheriah
IBM T. J. Watson Research Center
Yorktown Heights, NY 10598
{bxiang,niyuge,abei}@us.ibm.com
Abstract
In this paper we propose several novel ap-
proaches to improve phrase reordering for
statistical machine translation in the frame-
work of maximum-entropy-based modeling.
A smoothed prior probability is introduced to
take into account the distortion effect in the
priors. In addition to that we propose multi-
ple novel distortion features based on syntac-
tic parsing. A new metric is also introduced to
measure the effect of distortion in the transla-
tion hypotheses. We show that both smoothed
priors and syntax-based features help to sig-
nificantly improve the reordering and hence
the translation performance on a large-scale
Chinese-to-English machine translation task.
1 Introduction
Over the past decade, statistical machine translation
(SMT) has evolved into an attractive area in natural
language processing. SMT takes a source sequence,
S = [s1 s2 . . . sK ] from the source language, and
generates a target sequence, T ? = [t1 t2 . . . tL], by
finding the most likely translation given by:
T ? = arg max
T
p(T |S) (1)
In most of the existing approaches, following
(Brown et al, 1993), Eq. (1) is factored using the
source-channel model into
T ? = arg max
T
p(S|T )p?(T ), (2)
where the two models, the translation model,
p(S|T ), and the language model (LM), p(T ), are es-
timated separately: the former using a parallel cor-
pus and a hidden alignment model and the latter us-
ing a typically much larger monolingual corpus. The
weighting factor ? is typically tuned on a develop-
ment test set by optimizing a translation accuracy
criterion such as BLEU (Papineni et al, 2002).
In recent years, among all the proposed ap-
proaches, the phrase-based method has become
the widely adopted one in SMT due to its capa-
bility of capturing local context information from
adjacent words. Word order in the translation
output relies on how the phrases are reordered
based on both language model scores and distor-
tion cost/penalty (Koehn et al, 2003), among all
the features utilized in a maximum-entropy (log-
linear) model (Och and Ney, 2002). The distor-
tion cost utilized during the decoding is usually a
penalty linearly proportional to the number of words
in the source sentence that are skipped in a transla-
tion path.
In this paper, we propose several novel ap-
proaches to improve reordering in the phrase-based
translation with a maximum-entropy model. In Sec-
tion 2, we review the previous work that focused on
the distortion and phrase reordering in SMT. In Sec-
tion 3, we briefly review the baseline of this work.
In Section 4, we introduce a smoothed prior prob-
ability by taking into account the distortions in the
priors. In Section 5, we present multiple novel dis-
tortion features based on syntactic parsing. A new
distortion evaluation metric is proposed in Section
6 and experimental results on a large-scale Chinese-
English machine translation task are reported in Sec-
tion 7. Section 8 concludes the paper.
61
2 Previous Work
Significant amount of research has been conducted
in the past on the word reordering problem in SMT.
In (Brown et al, 1993) IBM Models 3 through 5
model reordering based on the surface word infor-
mation. For example, Model 4 attempts to assign
target-language positions to source-language words
by modeling d(j|i,K,L) where j is the target-
language position, i is the source-language position,
K and L are respectively source and target sentence
lengths. These models are not effective in modeling
reordering because they do not have enough context
and lack structural information.
Phrase-based SMT systems such as (Koehn et al,
2003) move from using words as translation units
to using phrases. One of the advantages of phrase-
based SMT systems is that the local reordering is in-
herent in the phrase translations. However, phrase-
based SMT systems capture reordering instances
and not reordering phenomena. It has trouble to pro-
duce the right translation order if the training data
does not contain the specific phrase pairs. For ex-
ample, phrases do not capture the phenomenon that
Arabic adjectives and nouns need to be reordered.
Instead of directly modeling the distance of word
movement, some phrase-level reordering models in-
dicate how to move phrases, also called orientations.
Orientations typically apply to the adjacent phrases.
Two adjacent phrases can be either placed mono-
tonically (sometimes called straight) or swapped
(non-monotonically or inverted). In (Och and Ney,
2004; Tillmann, 2004; Kumar and Byrne, 2005; Al-
Onaizan and Papineni, 2006; Xiong et al, 2006;
Zens and Ney, 2006; Ni et al, 2009), people pre-
sented models that use lexical features from the
phrases to predict their orientations. These models
are very powerful in predicting local phrase place-
ments. In (Galley and Manning, 2008) a hierar-
chical orientation model is introduced that captures
some non-local phrase reordering by a shift reduce
algorithm. Because of the heavy use of lexical fea-
tures, these models tend to suffer from data sparse-
ness problems.
Syntax information has been used for reordering,
such as in (Xia and McCord, 2004; Collins et al,
2005; Wang et al, 2007; Li et al, 2007; Chang et
al., 2009). More recently, in (Ge, 2010) a proba-
bilistic reordering model is presented to model di-
rectly the source translation sequence and explicitly
assign probabilities to the reordering of the source
input with no restrictions on gap, length or adja-
cency. The reordering model is used to generate a re-
ordering lattice which encodes many reordering and
their costs (negative log probability). Another recent
work is (Green et al, 2010), which estimates future
linear distortion cost and presents a discriminative
distortion model that predicts word movement dur-
ing translation based on multiple features.
This work differentiates itself from all the previ-
ous work on the phrase reordering as the following.
Firstly, we propose a smoothed distortion prior prob-
ability in the maximum-entropy-based MT frame-
work. It not only takes into account the distortion
in the prior, but also alleviates the data sparseness
problem. Secondly, we propose multiple syntactic
features based on the source-side parse tree to cap-
ture the reordering phenomena between two differ-
ent languages. The correct reordering patterns will
be automatically favored during the decoding, due to
the higher weights obtained through the maximum
entropy training on the parallel data. Finally, we
also introduce a new metric to quantify the effect on
the distortions in different systems. The experiments
on a Chinese-English MT task show that these pro-
posed approaches additively improve both the dis-
tortion and translation performance significantly.
3 Maximum-Entropy Model for MT
In this section we give a brief review of a special
maximum-entropy (ME) model as introduced in (It-
tycheriah and Roukos, 2007). The model has the
following form,
p(t, j|s) = p0(t, j|s)
Z
exp
?
i
?i?i(t, j, s), (3)
where s is a source phrase, and t is a target phrase.
j is the jump distance from the previously translated
source word to the current source word. During
training j can vary widely due to automatic word
alignment in the parallel corpus. To limit the sparse-
ness created by long jumps, j is capped to a win-
dow of source words (-5 to 5 words) around the last
translated source word. Jumps outside the window
are treated as being to the edge of the window. In
62
Eq. (3), p0 is a prior distribution, Z is a normalizing
term, and ?i(t, j, s) are the features of the model,
each being a binary question asked about the source
and target streams. The feature weights ?i can be
estimated with the Improved Iterative Scaling (IIS)
algorithm.
Several categories of features have been pro-
posed:
? Lexical features that examine source word, tar-
get word and jump;
? Lexical context features that examine the pre-
vious and next source words, and also the pre-
vious two target words;
? Segmentation features based on morphological
analysis;
? Part-of-speech (POS) features that collect the
syntactic information from the source and tar-
get words;
? Coverage features that examine the coverage
status of the source words to the left and to the
right. They fire only if the left source is open
(untranslated) or the right source is closed.
 
 
                <=-5          -4             -3             -2            -1            1              2             3              4           >=5 
                                                                                         jump 
Figure 1: Counts of jumps for words with POS NN.
4 Distortion Priors
Generally the prior distribution in Eq. (3) can con-
tain any information we know about the future.
 
 
                <=-5          -4             -3             -2            -1            1              2             3              4           >=5 
                                                                                        jump 
Figure 2: Counts of jumps for words with POS NT.
In (Ittycheriah and Roukos, 2007), the normalized
phrase count is utilized as the prior, i.e.
p0(t, j|s) ?
1
l
p0(t|s) =
C(s, t)
l ? C(s) (4)
where l is the jump window size (a constant), C(s, t)
is the co-ocurrence count of phrase pair (s, t), and
C(s) is the source phrase count of s. It can be seen
that distortion j is not taken into account in Eq. (4).
The contribution of distortion solely comes from the
features. In this work, we estimate the prior proba-
bility with distortion included,
p0(t, j|s) = p0(t|s)p(j|s, t) (5)
where p(j|s, t) is the distortion probability for a
given phrase pair (s, t).
Due to the sparseness issue in the estimation of
p(j|s, t), we choose to smooth it with the global dis-
tortion probability through
p(j|s, t) = ?pl(j|s, t) + (1 ? ?)pg(j), (6)
where pl is the local distortion probability estimated
based on the counts of jumps for each phrase pair
in the training, pg is the global distortion probability
estimated on all the training data, and ? is the inter-
polation weight. In this work, pg is estimated based
on either source POS (if it?s a single-word source
phrase) or source phrase size (if it?s more than one
word long), as shown below.
pg(j) =
{
Pg(j|POS), if |s| = 1
Pg(j||s|), if |s| > 1
(7)
63
In this way, the system can differentiate the distor-
tion distributions for single source words with differ-
ent POS tags, such as adjectives versus nouns. And
in the meantime, we also differentiate the distortion
distribution with different source phrase lengths. We
show several examples of the jump distributions in
Fig. 1 and 2 collected from 1M sentence pairs in
a Chinese-to-English parallel corpus with automatic
parsing and word alignment. Fig. 1 shows the count
histogram for single-word phrases with POS tag as
NN. The distortion with j = 1, i.e. monotone, domi-
nates the distribution with the highest count. The re-
ordering with j = ?1 has the second highest count.
Such pattern is shared by most of the other POS tags.
However, Fig. 2 shows that the distribution of jumps
for NT is quite different from NN. The jump with
j = ?1 is actually the most dominant, with higher
counts than monotone translation. This is due to the
different order in English when translating Chinese
temporal nouns.
5 Distortion Features
Although the maximum entropy translation model
has an explicit indicator of distortion, j, built into
the features, we discuss in this section some novel
features that try to capture the distortion phenomena
of translation. These features are questions about the
parse tree of the source language and in particular
about the local parse node neighborhood of the cur-
rent source word being translated. Figure 3 shows an
example sentence from the Chinese-English Parallel
Treebank (LDC2009E83) and the source language
parse is displayed on the left. The features below
can be viewed as either being within a parse node
or asking about the coverage status of neighborhood
nodes.
Since these features are asking about the current
coverage, they are specific to a path in the search lat-
tice during the decoding phase of translation. Train-
ing these features is done by evaluating on the path
defined by the automatic word alignment of the par-
allel corpus sentence.
5.1 Parse Tree Modifications
The ?de? construction in Chinese is by now famous.
In order to ask more coherent questions about the
parse neighborhood, we modify the parse structures
to ?raise? the ?de? structure. The parse trees anno-
tated by the LDC have a structure as shown in Fig.
4. After raising the ?de? structure we obtain the tree
in Fig. 5.
NP-OBJ
CP
IP
...
DEC
de
QP
...
NP
NN
Figure 4: Original parse tree from LDC.
DNP
CP
IP
...
DEC
de
QP
...
NP
NN
Figure 5: The parse tree after transformation.
The transformation has been applied to the exam-
ple shown in Figure 3. The resulting flat structure
facilitates the parse sibling feature discussed below.
5.2 Parse Coverage Feature
The first set of new features we will introduce is the
source parse coverage feature. This feature is in-
terior to a source parse node and asks if the leaves
under this parse node are covered (translated) or not
so far. The feature has the following components:
?i(SourceWord, TargetWord, SourceParseParent,
jump, Coverage).
Unary parents in the source parse tree are ex-
cluded since the feature has no ambiguity in cover-
age. In Figure 3, the ?PP? node above position 5 has
two children, P, NP. When translating source posi-
tion 6, this feature indicates that the PP node has a
leaf that is already covered.
5.3 Parse Sibling Feature
The second set of new features is the source parse
sibling feature. This feature asks whether the neigh-
64
 Figure 3: Chinese-English example.
boring parse node has been covered or not. The fea-
ture includes two types:
?i(SourceWord, TargetWord, SourceParseSibling,
jump, SiblingCoverage, SiblingOrientation)
and
?i(SourcePOS, TargetPOS, SourceParseSibling,
jump, SiblingCoverage, SiblingOrientation).
Some example features for the first type are
shown in Table 1, where ?i = e?i . The coverage
status (Cov) of the parse sibling node indicates if the
node is covered completely (1), partially (2) or not
covered (0). In order to capture the relationship of
the neighborhood node, we indicate the orientation
which can be either of {left (-1), right (1)}. Given
the example shown in Figure 3, at source position
10, the system can now ask about the ?CP? structure
to the left and the ?QP? and ?NP? structures to the
right. An ?i of greater than 1.0 (meaning ?i > 0)
indicates that the feature increases the probability of
the related target block. From these examples, it?s
clear that the system prefers to produce an empty
translation for the Chinese word ?de? when the ?QP?
and ?NP? nodes to the right of it are already covered
(the first two features in Table 1) and when the ?CP?
node to left is still uncovered (the third feature). The
last feature in the table shows ?i for the case when
?CP? has already been covered.
These features are able to capture neighborhoods
that are much larger than the original baseline model
which only asked questions about the immediate
lexical neighborhood of the current source word.
Cnt ?i Tgt Src Parse Cov Orien-
Node tation
18065 2.06 e0 de QP 1 1
366153 1.99 e0 de NP 1 1
143433 3.41 e0 de CP 0 -1
99297 1.05 e0 de CP 1 -1
Table 1: Parse Sibling Word Features (e0 represents
empty target).
6 A New Distortion Evaluation Metric
MT performance is usually measured by such met-
ric as BLEU which measures the MT output as a
whole including word choice and reordering. It is
useful to measure these components separately. Un-
igram BLEU (BLEUn1) measures the precision of
word choice. We need a metric for measuring re-
ordering accuracy. The naive way of counting accu-
racy at every source position does not account for the
case of the phrasal movement. If a phrase is moved
to the wrong place, every source word in the phrase
would be penalized whereas a more reasonable met-
ric would penalize the phrase movement only once
if the phrase boundary is correct.
We propose the following pair-wise distortion
metric. From an MT output, we first extract the
source visit sequence:
Hyp:{h1,h2, . . . hn}
where hi are the visit order of the source sentence.
From the reference, we extract the true visit se-
quence:
65
Ref:{r1,r2, . . . rn}
The Pair-wise Distortion metric PDscore can be
computed as follows:
PDscore(
??
H ) =
n
?
i=1
I(hi = rj ? hi?1 = rj?1)
n
(8)
It measures how often the translation output gets
the pair-wise source visit order correct. We notice
that an MT metric named LRscore was proposed in
(Birch and Osborne, 2010). It computes the distance
between two word order sequences, which is differ-
ent from the metric we proposed here.
7 Experiments
7.1 Data and Baseline
We conduct a set of experiments on a Chinese-to-
English MT task. The training data includes the UN
parallel corpus and LDC-released parallel corpora,
with about 11M sentence pairs, 320M words in to-
tal (counted at the English side). To evaluate the
smoothed distortion priors and different features, we
use an internal data set as the development set and
the NIST MT08 evaluation set as the test set, which
includes 76 documents (691 sentences) in newswire
and 33 documents (666 sentences) in weblog, both
with 4 sets of references for each sentence. Instead
of using all the training data, we sample the training
corpus based on the dev/test set to train the system
more efficiently. The most recent and good-quality
corpora are sampled first. For the given test set, we
obtain the first 20 instances of n-grams (length from
1 to 15) from the test that occur in the training uni-
verse and the resulting sentences then form the train-
ing sample. In the end, 1M sentence pairs are se-
lected for the sampled training for each genre of the
MT08 test set.
A 5-gram language model is trained from the En-
glish Gigaword corpus and the English portion of
the parallel corpus used in the translation model
training. The Chinese parse trees are produced
by a maximum entropy based parser (Ratnaparkhi,
1997). The baseline decoder is a phrase-based de-
coder that employs both normal phrases and also
non-contiguous phrases. The value of maximum
skip is set to 9 in all the experiments. The smoothing
parameter ? for distortion prior is set to 0.9 empiri-
cally based on the results on the development set.
7.2 Distortion Evaluation
We evaluate the MT distortion using the metric in
Eq. (8) on two hand-aligned test sets. Test-278 in-
cludes 278 held-out sentences. Test-52 contains the
first 52 sentences from the MT08 Newswire set, with
the Chinese input sentences manually aligned to the
first set of reference translations. From the hand
alignment, we extract the true source visit sequence
and this is the reference.
The evaluation results are in Table 2. It is shown
that the smoothed distortion prior, parse coverage
feature and parse sibling feature each provides im-
provement on the PDscore on Test-278 and Test-52.
The final system scores are 2 to 3 points absolute
higher than the baseline scores. The state visit se-
quence in the final system is closer to the true visit
sequence than that of the baseline. This indicates
the advantage of using both parse-based syntactic
features and also the smoothed prior that takes into
account of the distortion effect. We also provide
an upper-bound in the last row by computing the
PDscore between the first and second set of refer-
ences for Test-52. The number shows the agreement
between two human translators in terms of PDscore
is around 71%.
System Test-278 Test-52
ME Baseline 44.58 48.96
+Prior 45.12 49.22
+COV 45.00 49.03
+SIB 45.43 49.20
+COV+SIB 46.16 49.45
+Prior+COV+SIB 47.68 51.04
Ref1 vs. Ref2 - 70.99
Table 2: Distortion accuracy PDscore (Prior:smoothed
distortion prior; COV:parse coverage feature; SIB:parse
sibling feature).
7.3 Translation Results
Translation results on the MT08 Newswire set and
MT08 Weblog set are listed in Table 3 and Table 4
respectively. The MT performance is measured with
the widely adopted BLEU and TER (Snover et al,
2006) metrics. We also compare the results from
different configurations with a normal phrase-based
66
System Number of Features BLEU TER
PBT n/a 29.71 59.40
ME 9,008,382 32.12 56.78
+Prior 9,008,382 32.46 56.41
+COV 9,202,431 32.48 56.50
+SIB 10,088,487 32.73 56.26
+COV+SIB 10,282,536 32.94 55.97
+Prior+COV+SIB 10,282,536 33.15 55.62
Table 3: MT results on MT08 Newswire set (PBT:normal phrase-based MT; ME:Maximum-entropy baseline;
Prior:smoothed distortion prior; COV:parse coverage feature; SIB:parse sibling feature).
System Number of Features BLEU TER
PBT n/a 20.07 62.90
ME 9,192,617 22.42 60.36
+Prior 9,192,617 22.70 60.11
+COV 9,306,967 22.69 60.14
+SIB 9,847,445 22.91 59.92
+COV+SIB 9,961,795 23.04 59.78
+Prior+COV+SIB 9,961,795 23.25 59.56
Table 4: MT results on MT08 Weblog set (PBT:normal phrase-based MT; ME:Maximum-entropy baseline;
Prior:smoothed distortion prior; COV:parse coverage feature; SIB:parse sibling feature).
SMT system (Koehn et al, 2003) that is trained on
the same training data. The number of features used
in the systems are listed in the tables.
We start from the maximum-entropy baseline, a
system implemented similarly as in (Ittycheriah
and Roukos, 2007). It utilizes multiple features as
listed in Section 3, including lexical reordering fea-
tures, and produces an already significantly better
performance than the normal phrase-based MT sys-
tem (PBT). It is around 2.5 points better in both
BLEU and TER than the PBT baseline. By adding
smoothed priors, parse coverage features or parse
sibling features each separately, the MT perfor-
mance is improved by 0.3 to 0.6. The parse sibling
feature alone provides the largest individual contri-
bution. When adding both types of new features,
the improvement is around 0.6 to 0.8 on two gen-
res. Finally, applying all three results in the best
performance (the last row). On the Newswire set,
the final system is more than 3 points better than the
PBT baseline and 1 point better than the ME base-
line. On the Weblog set, it is more than 3 points
better than PBT and 0.8 better than the ME baseline.
All the MT results above are statistically significant
with p-value < 0.0001 by using the tool described in
(Zhang and Vogel, 2004).
7.4 Analysis
To better understand the distortion and translation
results, we take a closer look at the parse-based fea-
tures. In Table 5, we list the most frequent parse sib-
ling features that are related to the Chinese phrases
with ?PP VV? structures. It is known that in Chi-
nese usually the preposition phrases (?PP?) are writ-
ten/spoken before the verbs (?VV?), with a different
order from English. Table 5 shows how such re-
ordering phenomenon is captured by the parse sib-
ling features. Recall that when ?i is greater than 1,
the system prefers the reordering with that feature
fired. When ?i is smaller than 1, the system will
penalize the corresponding translation order during
the decoding search. When the coverage is equal to
1, it means ?PP? has been translated before translat-
ing current ?VV?. As shown in the table, those fea-
tures with coverage equal to 1 have ?i lower than 1,
which will result in penalties on incorrect translation
orders.
In Fig. 6, we show the comparison between the
67
Count ?i j TgtPOS SrcPOS ParseSib Cov Orien-
Node tation
3052 1.10 5 VBD VV PP 0 -1
2662 1.10 -1 VBD VV PP 0 -1
2134 1.25 4 VBD VV PP 0 -1
50 0.73 5 VBD VV PP 1 -1
39 0.84 -5 VBD VV PP 1 -1
18 0.95 -2 VBD VV PP 1 -1
Table 5: Parse Sibling Word Features related to Chinese ?PP VV?.
 
Src1 
 
 

  

 	


 

 

 

 , 1850  2005 

 , 
 
 

 1800    (were) (at) (annual) 3%  