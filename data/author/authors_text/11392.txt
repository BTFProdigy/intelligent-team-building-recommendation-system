Proceedings of the 12th Conference of the European Chapter of the ACL, pages 380?388,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Rule Filtering by Pattern for Efficient Hierarchical Translation
Gonzalo Iglesias? Adria` de Gispert?
? University of Vigo. Dept. of Signal Processing and Communications. Vigo, Spain
{giglesia,erbanga}@gts.tsc.uvigo.es
? University of Cambridge. Dept. of Engineering. CB2 1PZ Cambridge, U.K.
{ad465,wjb31}@eng.cam.ac.uk
Eduardo R. Banga? William Byrne?
Abstract
We describe refinements to hierarchical
translation search procedures intended to
reduce both search errors and memory us-
age through modifications to hypothesis
expansion in cube pruning and reductions
in the size of the rule sets used in transla-
tion. Rules are put into syntactic classes
based on the number of non-terminals and
the pattern, and various filtering strate-
gies are then applied to assess the impact
on translation speed and quality. Results
are reported on the 2008 NIST Arabic-to-
English evaluation task.
1 Introduction
Hierarchical phrase-based translation (Chiang,
2005) has emerged as one of the dominant cur-
rent approaches to statistical machine translation.
Hiero translation systems incorporate many of
the strengths of phrase-based translation systems,
such as feature-based translation and strong tar-
get language models, while also allowing flexi-
ble translation and movement based on hierarchi-
cal rules extracted from aligned parallel text. The
approach has been widely adopted and reported to
be competitive with other large-scale data driven
approaches, e.g. (Zollmann et al, 2008).
Large-scale hierarchical SMT involves auto-
matic rule extraction from aligned parallel text,
model parameter estimation, and the use of cube
pruning k-best list generation in hierarchical trans-
lation. The number of hierarchical rules extracted
far exceeds the number of phrase translations typ-
ically found in aligned text. While this may lead
to improved translation quality, there is also the
risk of lengthened translation times and increased
memory usage, along with possible search errors
due to the pruning procedures needed in search.
We describe several techniques to reduce mem-
ory usage and search errors in hierarchical trans-
lation. Memory usage can be reduced in cube
pruning (Chiang, 2007) through smart memoiza-
tion, and spreading neighborhood exploration can
be used to reduce search errors. However, search
errors can still remain even when implementing
simple phrase-based translation. We describe a
?shallow? search through hierarchical rules which
greatly speeds translation without any effect on
quality. We then describe techniques to analyze
and reduce the set of hierarchical rules. We do
this based on the structural properties of rules and
develop strategies to identify and remove redun-
dant or harmful rules. We identify groupings of
rules based on non-terminals and their patterns and
assess the impact on translation quality and com-
putational requirements for each given rule group.
We find that with appropriate filtering strategies
rule sets can be greatly reduced in size without im-
pact on translation performance.
1.1 Related Work
The search and rule pruning techniques described
in the following sections add to a growing lit-
erature of refinements to the hierarchical phrase-
based SMT systems originally described by Chi-
ang (2005; 2007). Subsequent work has addressed
improvements and extensions to the search proce-
dure itself, the extraction of the hierarchical rules
needed for translation, and has also reported con-
trastive experiments with other SMT architectures.
Hiero Search Refinements Huang and Chiang
(2007) offer several refinements to cube pruning
to improve translation speed. Venugopal et al
(2007) introduce a Hiero variant with relaxed con-
straints for hypothesis recombination during pars-
ing; speed and results are comparable to those of
cube pruning, as described by Chiang (2007). Li
and Khudanpur (2008) report significant improve-
ments in translation speed by taking unseen n-
grams into account within cube pruning to mini-
mize language model requests. Dyer et al (2008)
380
extend the translation of source sentences to trans-
lation of input lattices following Chappelier et al
(1999).
Extensions to Hiero Blunsom et al (2008)
discuss procedures to combine discriminative la-
tent models with hierarchical SMT. The Syntax-
Augmented Machine Translation system (Zoll-
mann and Venugopal, 2006) incorporates target
language syntactic constituents in addition to the
synchronous grammars used in translation. Shen
at al. (2008) make use of target dependency trees
and a target dependency language model during
decoding. Marton and Resnik (2008) exploit shal-
low correspondences of hierarchical rules with
source syntactic constituents extracted from par-
allel text, an approach also investigated by Chiang
(2005). Zhang and Gildea (2006) propose bina-
rization for synchronous grammars as a means to
control search complexity arising from more com-
plex, syntactic, hierarchical rules sets.
Hierarchical rule extraction Zhang et al (2008)
describe a linear algorithm, a modified version of
shift-reduce, to extract phrase pairs organized into
a tree from which hierarchical rules can be directly
extracted. Lopez (2007) extracts rules on-the-fly
from the training bitext during decoding, search-
ing efficiently for rule patterns using suffix arrays.
Analysis and Contrastive Experiments Zollman
et al (2008) compare phrase-based, hierarchical
and syntax-augmented decoders for translation of
Arabic, Chinese, and Urdu into English, and they
find that attempts to expedite translation by simple
schemes which discard rules also degrade transla-
tion performance. Lopez (2008) explores whether
lexical reordering or the phrase discontiguity in-
herent in hierarchical rules explains improvements
over phrase-based systems. Hierarchical transla-
tion has also been used to great effect in combina-
tion with other translation architectures (e.g. (Sim
et al, 2007; Rosti et al, 2007)).
1.2 Outline
The paper proceeds as follows. Section 2 de-
scribes memoization and spreading neighborhood
exploration in cube pruning intended to reduce
memory usage and search errors, respectively. A
detailed comparison with a simple phrase-based
system is presented. Section 3 describes pattern-
based rule filtering and various procedures to se-
lect rule sets for use in translation with an aim
to improving translation quality while minimizing
rule set size. Finally, Section 4 concludes.
2 Two Refinements in Cube Pruning
Chiang (2007) introduced cube pruning to apply
language models in pruning during the generation
of k-best translation hypotheses via the application
of hierarchical rules in the CYK algorithm. In the
implementation of Hiero described here, there is
the parser itself, for which we use a variant of the
CYK algorithm closely related to CYK+ (Chap-
pelier and Rajman, 1998); it employs hypothesis
recombination, without pruning, while maintain-
ing back pointers. Before k-best list generation
with cube pruning, we apply a smart memoiza-
tion procedure intended to reduce memory con-
sumption during k-best list expansion. Within the
cube pruning algorithm we use spreading neigh-
borhood exploration to improve robustness in the
face of search errors.
2.1 Smart Memoization
Each cell in the chart built by the CYK algorithm
contains all possible derivations of a span of the
source sentence being translated. After the parsing
stage is completed, it is possible to make a very ef-
ficient sweep through the backpointers of the CYK
grid to count how many times each cell will be ac-
cessed by the k-best generation algorithm. When
k-best list generation is running, the number of
times each cell is visited is logged so that, as each
cell is visited for the last time, the k-best list as-
sociated with each cell is deleted. This continues
until the one k-best list remaining at the top of the
chart spans the entire sentence. Memory reduc-
tions are substantial for longer sentences: for the
longest sentence in the tuning set described later
(105 words in length), smart memoization reduces
memory usage during the cube pruning stage from
2.1GB to 0.7GB. For average length sentences of
approx. 30 words, memory reductions of 30% are
typical.
2.2 Spreading Neighborhood Exploration
In generation of a k-best list of translations for
a source sentence span, every derivation is trans-
formed into a cube containing the possible trans-
lations arising from that derivation, along with
their translation and language model scores (Chi-
ang, 2007). These derivations may contain non-
terminals which must be expanded based on hy-
potheses generated by lower cells, which them-
381
HIERO MJ1 HIERO HIERO SHALLOW
X ? ?V2V1,V1V2? X ? ??,?? X ? ??s,?s?
X ? ?V ,V ? ?, ? ? ({X} ?T)+ X ? ?V ,V ?
V ? ?s,t? V ? ?s,t?
s, t ? T+ s, t ? T+; ?s, ?s ? ({V } ? T)+
Table 1: Hierarchical grammars (not including glue rules). T is the set of terminals.
selves may contain non-terminals. For efficiency
each cube maintains a queue of hypotheses, called
here the frontier queue, ranked by translation and
language model score; it is from these frontier
queues that hypotheses are removed to create the
k-best list for each cell. When a hypothesis is ex-
tracted from a frontier queue, that queue is updated
by searching through the neighborhood of the ex-
tracted item to find novel hypotheses to add; if no
novel hypotheses are found, that queue necessar-
ily shrinks. This shrinkage can lead to search er-
rors. We therefore require that, when a hypothe-
sis is removed, new candidates must be added by
exploring a neighborhood which spreads from the
last extracted hypothesis. Each axis of the cube
is searched (here, to a depth of 20) until a novel
hypothesis is found. In this way, up to three new
candidates are added for each entry extracted from
a frontier queue.
Chiang (2007) describes an initialization pro-
cedure in which these frontier queues are seeded
with a single candidate per axis; we initialize each
frontier queue to a depth of bNnt+1, where Nnt is
the number of non-terminals in the derivation and
b is a search parameter set throughout to 10. By
starting with deep frontier queues and by forcing
them to grow during search we attempt to avoid
search errors by ensuring that the universe of items
within the frontier queues does not decrease as the
k-best lists are filled.
2.3 A Study of Hiero Search Errors in
Phrase-Based Translation
Experiments reported in this paper are based
on the NIST MT08 Arabic-to-English transla-
tion task. Alignments are generated over all al-
lowed parallel data, (?150M words per language).
Features extracted from the alignments and used
in translation are in common use: target lan-
guage model, source-to-target and target-to-source
phrase translation models, word and rule penalties,
number of usages of the glue rule, source-to-target
and target-to-source lexical models, and three rule
Figure 1: Spreading neighborhood exploration
within a cube, just before and after extraction
of the item C. Grey squares represent the fron-
tier queue; black squares are candidates already
extracted. Chiang (2007) would only consider
adding items X to the frontier queue, so the queue
would shrink. Spreading neighborhood explo-
ration adds candidates S to the frontier queue.
count features inspired by Bender et al (2007).
MET (Och, 2003) iterative parameter estimation
under IBM BLEU is performed on the develop-
ment set. The English language used model is a
4-gram estimated over the parallel text and a 965
million word subset of monolingual data from the
English Gigaword Third Edition. In addition to the
MT08 set itself, we use a development set mt02-
05-tune formed from the odd numbered sentences
of the NIST MT02 through MT05 evaluation sets;
the even numbered sentences form the validation
set mt02-05-test. The mt02-05-tune set has 2,075
sentences.
We first compare the cube pruning decoder to
the TTM (Kumar et al, 2006), a phrase-based
SMT system implemented with Weighted Finite-
State Tansducers (Allauzen et al, 2007). The sys-
tem implements either a monotone phrase order
translation, or an MJ1 (maximum phrase jump of
1) reordering model (Kumar and Byrne, 2005).
Relative to the complex movement and translation
allowed by Hiero and other models, MJ1 is clearly
inferior (Dreyer et al, 2007); MJ1 was developed
with efficiency in mind so as to run with a mini-
mum of search errors in translation and to be eas-
ily and exactly realized via WFSTs. Even for the
382
large models used in an evaluation task, the TTM
system is reported to run largely without pruning
(Blackwood et al, 2008).
The Hiero decoder can easily be made to
implement MJ1 reordering by allowing only a
restricted set of reordering rules in addition to
the usual glue rule, as shown in left-hand column
of Table 1, where T is the set of terminals.
Constraining Hiero in this way makes it possible
to compare its performance to the exact WFST
TTM implementation and to identify any search
errors made by Hiero.
Table 2 shows the lowercased IBM BLEU
scores obtained by the systems for mt02-05-tune
with monotone and reordered search, and with
MET-optimised parameters for MJ1 reordering.
For Hiero, an N-best list depth of 10,000 is used
throughout. In the monotone case, all phrase-
based systems perform similarly although Hiero
does make search errors. For simple MJ1 re-
ordering, the basic Hiero search procedure makes
many search errors and these lead to degradations
in BLEU. Spreading neighborhood expansion re-
duces the search errors and improves BLEU score
significantly but search errors remain a problem.
Search errors are even more apparent after MET.
This is not surprising, given that mt02-05-tune is
the set over which MET is run: MET drives up the
likelihood of good hypotheses at the expense of
poor hypotheses, but search errors often increase
due to the expanded dynamic range of the hypoth-
esis scores.
Our aim in these experiments was to demon-
strate that spreading neighborhood exploration can
aid in avoiding search errors. We emphasize that
we are not proposing that Hiero should be used to
implement reordering models such as MJ1 which
were created for completely different search pro-
cedures (e.g. WFST composition). However these
experiments do suggest that search errors may be
an issue, particularly as the search space grows
to include the complex long-range movement al-
lowed by the hierarchical rules. We next study
various filtering procedures to reduce hierarchi-
cal rule sets to find a balance between translation
speed, memory usage, and performance.
3 Rule Filtering by Pattern
Hierarchical rules X ? ??,?? are composed of
sequences of terminals and non-terminals, which
Monotone MJ1 MJ1+MET
BLEU SE BLEU SE BLEU SE
a 44.7 - 47.2 - 49.1 -
b 44.5 342 46.7 555 48.4 822
c 44.7 77 47.1 191 48.9 360
Table 2: Phrase-based TTM and Hiero perfor-
mance on mt02-05-tune for TTM (a), Hiero (b),
Hiero with spreading neighborhood exploration
(c). SE is the number of Hiero hypotheses with
search errors.
we call elements. In the source, a maximum of
two non-adjacent non-terminals is allowed (Chi-
ang, 2007). Leaving aside rules without non-
terminals (i.e. phrase pairs as used in phrase-
based translation), rules can be classed by their
number of non-terminals, Nnt, and their number
of elements, Ne. There are 5 possible classes:
Nnt.Ne= 1.2, 1.3, 2.3, 2.4, 2.5.
During rule extraction we search each class sep-
arately to control memory usage. Furthermore, we
extract from alignments only those rules which are
relevant to our given test set; for computation of
backward translation probabilities we log general
counts of target-side rules but discard unneeded
rules. Even with this restriction, our initial ruleset
for mt02-05-tune exceeds 175M rules, of which
only 0.62M are simple phrase pairs.
The question is whether all these rules are
needed for translation. If the rule set can be re-
duced without reducing translation quality, both
memory efficiency and translation speed can be
increased. Previously published approaches to re-
ducing the rule set include: enforcing a mini-
mum span of two words per non-terminal (Lopez,
2008), which would reduce our set to 115M rules;
or a minimum count (mincount) threshold (Zoll-
mann et al, 2008), which would reduce our set
to 78M (mincount=2) or 57M (mincount=3) rules.
Shen et al (2008) describe the result of filter-
ing rules by insisting that target-side rules are
well-formed dependency trees. This reduces their
rule set from 140M to 26M rules. This filtering
leads to a degradation in translation performance
(see Table 2 of Shen et al (2008)), which they
counter by adding a dependency LM in translation.
As another reference point, Chiang (2007) reports
Chinese-to-English translation experiments based
on 5.5M rules.
Zollmann et al (2008) report that filtering rules
383
en masse leads to degradation in translation per-
formance. Rather than apply a coarse filtering,
such as a mincount for all rules, we follow a more
syntactic approach and further classify our rules
according to their pattern and apply different fil-
ters to each pattern depending on its value in trans-
lation. The premise is that some patterns are more
important than others.
3.1 Rule Patterns
Class Rule Pattern
Nnt.Ne ?source , target? Types
?wX1 , wX1? 1185028
1.2 ?wX1 , wX1w? 153130
?wX1 , X1w? 97889
1.3 ?wX1w , wX1w? 32903522
?wX1w , wX1? 989540
2.3 ?X1wX2 , X1wX2? 1554656
?X2wX1 , X1wX2? 39163
?wX1wX2 , wX1wX2? 26901823
?X1wX2w , X1wX2w? 26053969
2.4 ?wX1wX2 , wX1wX2w? 2534510
?wX2wX1 , wX1wX2? 349176
?X2wX1w , X1wX2w? 259459
?wX1wX2w , wX1wX2w? 61704299
?wX1wX2w , wX1X2w? 3149516
2.5 ?wX1wX2w , X1wX2w? 2330797
?wX2wX1w , wX1wX2w? 275810
?wX2wX1w , wX1X2w? 205801
Table 3: Hierarchical rule patterns classed by
number of non-terminals, Nnt, number of ele-
ments Ne, source and target patterns, and types in
the rule set extracted for mt02-05-tune.
Given a rule set, we define source patterns and
target patterns by replacing every sequence of
non-terminals by a single symbol ?w? (indicating
word, i.e. terminal string, w ? T+). Each hierar-
chical rule has a unique source and target pattern
which together define the rule pattern.
By ignoring the identity and the number of ad-
jacent terminals, the rule pattern represents a nat-
ural generalization of any rule, capturing its struc-
ture and the type of reordering it encodes. In to-
tal, there are 66 possible rule patterns. Table 3
presents a few examples extracted for mt02-05-
tune, showing that some patterns are much more
diverse than others. For example, patterns with
two non-terminals (Nnt=2) are richer than pat-
terns with Nnt=1, as they cover many more dis-
tinct rules. Additionally, patterns with two non-
terminals which also have a monotonic relation-
ship between source and target non-terminals are
much more diverse than their reordered counter-
parts.
Some examples of extracted rules and their cor-
responding pattern follow, where Arabic is shown
in Buckwalter encoding.
Pattern ?wX1 , wX1w? :
?w+ qAl X1 , the X1said?
Pattern ?wX1w , wX1? :
?fy X1kAnwn Al>wl , on december X1?
Pattern ?wX1wX2 , wX1wX2w? :
?Hl X1lAzmp X2 , a X1solution to the X2crisis?
3.2 Building an Initial Rule Set
We describe a greedy approach to building a rule
set in which rules belonging to a pattern are added
to the rule set guided by the improvements they
yield on mt02-05-tune relative to the monotone
Hiero system described in the previous section.
We find that certain patterns seem not to con-
tribute to any improvement. This is particularly
significant as these patterns often encompass large
numbers of rules, as with patterns with match-
ing source and target patterns. For instance, we
found no improvement when adding the pattern
?X1w,X1w?, of which there were 1.2M instances
(Table 3). Since concatenation is already possible
under the general glue rule, rules with this pattern
are redundant. By contrast, the much less frequent
reordered counterpart, i.e. the ?wX1,X1w? pat-
tern (0.01M instances), provides substantial gains.
The situation is analogous for rules with two non-
terminals (Nnt=2).
Based on exploratory analyses (not reported
here, for space) an initial rule set was built by
excluding patterns reported in Table 4. In to-
tal, 171.5M rules are excluded, for a remaining
set of 4.2M rules, 3.5M of which are hierarchi-
cal. We acknowledge that adding rules in this way,
by greedy search, is less than ideal and inevitably
raises questions with respect to generality and re-
peatability. However in our experience this is a
robust approach, mainly because the initial trans-
lation system runs very fast; it is possible to run
many exploratory experiments in a short time.
384
Excluded Rules Types
a ?X1w,X1w? , ?wX1,wX1? 2332604
b ?X1wX2,?? 2121594
?X1wX2w,X1wX2w? ,c ?wX1wX2,wX1wX2?
52955792
d ?wX1wX2w,?? 69437146
e Nnt.Ne= 1.3 w mincount=5 32394578
f Nnt.Ne= 2.3 w mincount=5 166969
g Nnt.Ne= 2.4 w mincount=10 11465410
h Nnt.Ne= 2.5 w mincount=5 688804
Table 4: Rules excluded from the initial rule set.
3.3 Shallow versus Fully Hierarchical
Translation
In measuring the effectiveness of rules in transla-
tion, we also investigate whether a ?fully hierarchi-
cal? search is needed or whether a shallow search
is also effective. In constrast to full Hiero, in the
shallow search, only phrases are allowed to be sub-
stituted into non-terminals. The rules used in each
case can be expressed as shown in the 2nd and 3rd
columns of Table 1. Shallow search can be con-
sidered (loosely) to be a form of rule filtering.
As can be seen in Table 5 there is no impact on
BLEU, while translation speed increases by a fac-
tor of 7. Of course, these results are specific to this
Arabic-to-English translation task, and need not
be expected to carry over to other language pairs,
such as Chinese-to-English translation. However,
the impact of this search simplification is easy to
measure, and the gains can be significant enough,
that it may be worth investigation even for lan-
guages with complex long distance movement.
mt02-05- -tune -test
System Time BLEU BLEU
HIERO 14.0 52.1 51.5
HIERO - shallow 2.0 52.1 51.4
Table 5: Translation performance and time (in sec-
onds per word) for full vs. shallow Hiero.
3.4 Individual Rule Filters
We now filter rules individually (not by class) ac-
cording to their number of translations. For each
fixed ? /? T+ (i.e. with at least 1 non-terminal),
we define the following filters over rules X ?
??,??:
? Number of translations (NT). We keep the
NT most frequent ?, i.e. each ? is allowed to
have at most NT rules.
? Number of reordered translations (NRT).
We keep the NRT most frequent ? with
monotonic non-terminals and the NRT most
frequent ? with reordered non-terminals.
? Count percentage (CP). We keep the most
frequent ? until their aggregated number of
counts reaches a certain percentage CP of the
total counts of X ? ??,??. Some ??s are al-
lowed to have more ??s than others, depend-
ing on their count distribution.
Results applying these filters with various
thresholds are given in Table 6, including num-
ber of rules and decoding time. As shown, all
filters achieve at least a 50% speed-up in decod-
ing time by discarding 15% to 25% of the base-
line rules. Remarkably, performance is unaffected
when applying the simple NT and NRT filters
with a threshold of 20 translations. Finally, the
CM filter behaves slightly worse for thresholds of
90% for the same decoding time. For this reason,
we select NRT=20 as our general filter.
mt02-05- -tune -test
Filter Time Rules BLEU BLEU
baseline 2.0 4.20 52.1 51.4
NT=10 0.8 3.25 52.0 51.3
NT=15 0.8 3.43 52.0 51.3
NT=20 0.8 3.56 52.1 51.4
NRT=10 0.9 3.29 52.0 51.3
NRT=15 1.0 3.48 52.0 51.4
NRT=20 1.0 3.59 52.1 51.4
CP=50 0.7 2.56 51.4 50.9
CP=90 1.0 3.60 52.0 51.3
Table 6: Impact of general rule filters on transla-
tion (IBM BLEU), time (in seconds per word) and
number of rules (in millions).
3.5 Pattern-based Rule Filters
In this section we first reconsider whether reintro-
ducing the monotonic rules (originally excluded as
described in rows ?b?, ?c?, ?d? in Table 4) affects
performance. Results are given in the upper rows
of Table 7. For all classes, we find that reintroduc-
ing these rules increases the total number of rules
385
mt02-05- -tune -test
Nnt.Ne Filter Time Rules BLEU BLEU
baseline NRT=20 1.0 3.59 52.1 51.4
2.3 +monotone 1.1 4.08 51.5 51.1
2.4 +monotone 2.0 11.52 51.6 51.0
2.5 +monotone 1.8 6.66 51.7 51.2
1.3 mincount=3 1.0 5.61 52.1 51.3
2.3 mincount=1 1.2 3.70 52.1 51.4
2.4 mincount=5 1.8 4.62 52.0 51.3
2.4 mincount=15 1.0 3.37 52.0 51.4
2.5 mincount=1 1.1 4.27 52.2 51.5
1.2 mincount=5 1.0 3.51 51.8 51.3
1.2 mincount=10 1.0 3.50 51.7 51.2
Table 7: Effect of pattern-based rule filters. Time in seconds per word. Rules in millions.
substantially, despite the NRT=20 filter, but leads
to degradation in translation performance.
We next reconsider the mincount threshold val-
ues for Nnt.Ne classes 1.3, 2.3, 2.4 and 2.5 origi-
nally described in Table 4 (rows ?e? to ?h?). Results
under various mincount cutoffs for each class are
given in Table 7 (middle five rows). For classes
2.3 and 2.5, the mincount cutoff can be reduced
to 1 (i.e. all rules are kept) with slight translation
improvements. In contrast, reducing the cutoff for
classes 1.3 and 2.4 to 3 and 5, respectively, adds
many more rules with no increase in performance.
We also find that increasing the cutoff to 15 for
class 2.4 yields the same results with a smaller rule
set. Finally, we consider further filtering applied to
class 1.2 with mincount 5 and 10 (final two rows
in Table 7). The number of rules is largely un-
changed, but translation performance drops con-
sistently as more rules are removed.
Based on these experiments, we conclude that it
is better to apply separate mincount thresholds to
the classes to obtain optimal performance with a
minimum size rule set.
3.6 Large Language Models and Evaluation
Finally, in this section we report results of our
shallow hierarchical system with the 2.5 min-
count=1 configuration from Table 7, after includ-
ing the following N-best list rescoring steps.
? Large-LM rescoring. We build sentence-
specific zero-cutoff stupid-backoff (Brants et
al., 2007) 5-gram language models, estimated
using ?4.7B words of English newswire text,
and apply them to rescore each 10000-best
list.
? Minimum Bayes Risk (MBR). We then rescore
the first 1000-best hypotheses with MBR,
taking the negative sentence level BLEU
score as the loss function to minimise (Ku-
mar and Byrne, 2004).
Table 8 shows results for mt02-05-tune, mt02-
05-test, the NIST subsets from the MT06 evalu-
ation (mt06-nist-nw for newswire data and mt06-
nist-ng for newsgroup) and mt08, as measured by
lowercased IBM BLEU and TER (Snover et al,
2006). Mixed case NIST BLEU for this system on
mt08 is 42.5. This is directly comparable to offi-
cial MT08 evaluation results1.
4 Conclusions
This paper focuses on efficient large-scale hierar-
chical translation while maintaining good trans-
lation quality. Smart memoization and spreading
neighborhood exploration during cube pruning are
described and shown to reduce memory consump-
tion and Hiero search errors using a simple phrase-
based system as a contrast.
We then define a general classification of hi-
erarchical rules, based on their number of non-
terminals, elements and their patterns, for refined
extraction and filtering.
For a large-scale Arabic-to-English task, we
show that shallow hierarchical decoding is as good
1Full MT08 results are available at
http://www.nist.gov/speech/tests/mt/2008/. It is worth
noting that many of the top entries make use of system
combination; the results reported here are for single system
translation.
386
mt02-05-tune mt02-05-test mt06-nist-nw mt06-nist-ng mt08
HIERO+MET 52.2 / 41.6 51.5 / 42.2 48.4 / 43.6 35.3 / 53.2 42.5 / 48.6
+rescoring 53.2 / 40.8 52.6 / 41.4 49.4 / 42.9 36.6 / 53.5 43.4 / 48.1
Table 8: Arabic-to-English translation results (lower-cased IBM BLEU / TER) with large language mod-
els and MBR decoding.
as fully hierarchical search and that decoding time
is dramatically decreased. In addition, we describe
individual rule filters based on the distribution of
translations with further time reductions at no cost
in translation scores. This is in direct contrast
to recent reported results in which other filtering
strategies lead to degraded performance (Shen et
al., 2008; Zollmann et al, 2008).
We find that certain patterns are of much greater
value in translation than others and that separate
minimum count filters should be applied accord-
ingly. Some patterns were found to be redundant
or harmful, in particular those with two monotonic
non-terminals. Moreover, we show that the value
of a pattern is not directly related to the number of
rules it encompasses, which can lead to discarding
large numbers of rules as well as to dramatic speed
improvements.
Although reported experiments are only for
Arabic-to-English translation, we believe the ap-
proach will prove to be general. Pattern relevance
will vary for other language pairs, but we expect
filtering strategies to be equally worth pursuing.
Acknowledgments
This work was supported in part by the GALE pro-
gram of the Defense Advanced Research Projects
Agency, Contract No. HR0011- 06-C-0022. G.
Iglesias supported by Spanish Government re-
search grant BES-2007-15956 (project TEC2006-
13694-C03-03).
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of CIAA, pages 11?23.
Oliver Bender, Evgeny Matusov, Stefan Hahn, Sasa
Hasan, Shahram Khadivi, and Hermann Ney. 2007.
The RWTH Arabic-to-English spoken language
translation system. In Proceedings of ASRU, pages
396?401.
Graeme Blackwood, Adria` de Gispert, Jamie Brunning,
and William Byrne. 2008. Large-scale statistical
machine translation with weighted finite state trans-
ducers. In Proceedings of FSMNLP, pages 27?35.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proceedings of ACL-HLT,
pages 200?208.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
EMNLP-ACL, pages 858?867.
Jean-Ce?dric Chappelier and Martin Rajman. 1998. A
generalized CYK algorithm for parsing stochastic
CFG. In Proceedings of TAPD, pages 133?137.
Jean-Ce?dric Chappelier, Martin Rajman, Ramo?n
Aragu?e?s, and Antoine Rozenknop. 1999. Lattice
parsing for speech recognition. In Proceedings of
TALN, pages 95?104.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Markus Dreyer, Keith Hall, and Sanjeev Khudanpur.
2007. Comparing reordering constraints for SMT
using efficient BLEU oracle computation. In Pro-
ceedings of SSST, NAACL-HLT 2007 / AMTA Work-
shop on Syntax and Structure in Statistical Transla-
tion.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice translation.
In Proceedings of ACL-HLT, pages 1012?1020.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of ACL, pages 144?151.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT-NAACL, pages 169?
176.
Shankar Kumar and William Byrne. 2005. Lo-
cal phrase reordering models for statistical machine
translation. In Proceedings of HLT-EMNLP, pages
161?168.
Shankar Kumar, Yonggang Deng, and William Byrne.
2006. A weighted finite state transducer translation
template model for statistical machine translation.
Natural Language Engineering, 12(1):35?75.
387
Zhifei Li and Sanjeev Khudanpur. 2008. A scal-
able decoder for parsing-based machine translation
with equivalent language model state maintenance.
In Proceedings of the ACL-HLT Second Workshop
on Syntax and Structure in Statistical Translation,
pages 10?18.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of EMNLP-
CONLL, pages 976?985.
Adam Lopez. 2008. Tera-scale translation models
via pattern matching. In Proceedings of COLING,
pages 505?512.
Yuval Marton and Philip Resnik. 2008. Soft syntac-
tic constraints for hierarchical phrased-based trans-
lation. In Proceedings of ACL-HLT, pages 1003?
1011.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of
ACL, pages 160?167.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
Dorr. 2007. Combining outputs from multiple ma-
chine translation systems. In Proceedings of HLT-
NAACL, pages 228?235.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-HLT, pages 577?585.
Khe Chai Sim, William Byrne, Mark Gales, Hichem
Sahbi, and Phil Woodland. 2007. Consensus net-
work decoding for statistical machine translation
system combination. In Proceedings of ICASSP,
volume 4, pages 105?108.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. 2006. A
study of translation edit rate with targeted human an-
notation. In Proceedings of AMTA, pages 223?231.
Ashish Venugopal, Andreas Zollmann, and Vogel
Stephan. 2007. An efficient two-pass approach to
synchronous-CFG driven statistical MT. In Pro-
ceedings of HLT-NAACL, pages 500?507.
Hao Zhang and Daniel Gildea. 2006. Synchronous
binarization for machine translation. In Proceedings
of HLT-NAACL, pages 256?263.
Hao Zhang, Daniel Gildea, and David Chiang. 2008.
Extracting synchronous grammar rules from word-
level alignments in linear time. In Proceedings of
COLING, pages 1081?1088.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings of NAACL Workshop on Statistical
Machine Translation, pages 138?141.
Andreas Zollmann, Ashish Venugopal, Franz Och,
and Jay Ponte. 2008. A systematic comparison
of phrase-based, hierarchical and syntax-augmented
statistical MT. In Proceedings of COLING, pages
1145?1152.
388
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 433?441,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Hierarchical Phrase-Based Translation with
Weighted Finite State Transducers
Gonzalo Iglesias? Adria` de Gispert?
? University of Vigo. Dept. of Signal Processing and Communications. Vigo, Spain
{giglesia,erbanga}@gts.tsc.uvigo.es
? University of Cambridge. Dept. of Engineering. CB2 1PZ Cambridge, U.K.
{ad465,wjb31}@eng.cam.ac.uk
Eduardo R. Banga? William Byrne?
Abstract
This paper describes a lattice-based decoder
for hierarchical phrase-based translation. The
decoder is implemented with standard WFST
operations as an alternative to the well-known
cube pruning procedure. We find that the
use of WFSTs rather than k-best lists requires
less pruning in translation search, resulting
in fewer search errors, direct generation of
translation lattices in the target language,
better parameter optimization, and improved
translation performance when rescoring with
long-span language models and MBR decod-
ing. We report translation experiments for
the Arabic-to-English and Chinese-to-English
NIST translation tasks and contrast the WFST-
based hierarchical decoder with hierarchical
translation under cube pruning.
1 Introduction
Hierarchical phrase-based translation generates
translation hypotheses via the application of hierar-
chical rules in CYK parsing (Chiang, 2005). Cube
pruning is used to apply language models at each
cell of the CYK grid as part of the search for a
k-best list of translation candidates (Chiang, 2005;
Chiang, 2007). While this approach is very effective
and has been shown to produce very good quality
translation, the reliance on k-best lists is a limita-
tion. We take an alternative approach and describe a
lattice-based hierarchical decoder implemented with
Weighted Finite State Transducers (WFSTs). In ev-
ery CYK cell we build a single, minimal word lattice
containing all possible translations of the source sen-
tence span covered by that cell. When derivations
contain non-terminals, we use pointers to lower-
level lattices for memory efficiency. The pointers
are only expanded to the actual translations if prun-
ing is required during search; expansion is otherwise
only carried out at the upper-most cell, after the full
CYK grid has been traversed.
We describe how this decoder can be easily im-
plemented with WFSTs. For this we employ the
OpenFST libraries (Allauzen et al, 2007). Using
standard FST operations such as composition, ep-
silon removal, determinization, minimization and
shortest-path, we find this search procedure to be
simpler to implement than cube pruning. The main
modeling advantages are a significant reduction in
search errors, a simpler implementation, direct gen-
eration of target language word lattices, and better
integration with other statistical MT procedures. We
report translation results in Arabic-to-English and
Chinese-to-English translation and contrast the per-
formance of lattice-based and cube pruning hierar-
chical decoding.
1.1 Related Work
Hierarchical phrase-based translation has emerged
as one of the dominant current approaches to statis-
tical machine translation. Hiero translation systems
incorporate many of the strengths of phrase-based
translation systems, such as feature-based transla-
tion and strong target language models, while also
allowing flexible translation and movement based
on hierarchical rules extracted from aligned paral-
lel text. We summarize some extensions to the basic
approach to put our work in context.
433
Hiero Search Refinements Huang and Chiang
(2007) offer several refinements to cube pruning to
improve translation speed. Venugopal et al (2007)
introduce a Hiero variant with relaxed constraints
for hypothesis recombination during parsing; speed
and results are comparable to those of cube prun-
ing, as described by Chiang (2007). Li and Khudan-
pur (2008) report significant improvements in trans-
lation speed by taking unseen n-grams into account
within cube pruning to minimize language model re-
quests. Dyer et al (2008) extend the translation of
source sentences to translation of input lattices fol-
lowing Chappelier et al (1999).
Extensions to Hiero Several authors describe ex-
tensions to Hiero, to incorporate additional syntactic
information (Zollmann and Venugopal, 2006; Zhang
and Gildea, 2006; Shen et al, 2008; Marton and
Resnik, 2008), or to combine it with discriminative
latent models (Blunsom et al, 2008).
Analysis and Contrastive Experiments Zollman et
al. (2008) compare phrase-based, hierarchical and
syntax-augmented decoders for translation of Ara-
bic, Chinese, and Urdu into English. Lopez (2008)
explores whether lexical reordering or the phrase
discontiguity inherent in hierarchical rules explains
improvements over phrase-based systems. Hierar-
chical translation has also been used to great effect
in combination with other translation architectures,
e.g. (Sim et al, 2007; Rosti et al, 2007).
WFSTs for Translation There is extensive work in
using Weighted Finite State Transducer for machine
translation (Bangalore and Riccardi, 2001; Casacu-
berta, 2001; Kumar and Byrne, 2005; Mathias and
Byrne, 2006; Graehl et al, 2008).
To our knowledge, this paper presents the first de-
scription of hierarchical phrase-based translation in
terms of lattices rather than k-best lists. The next
section describes hierarchical phrase-based transla-
tion with WFSTs, including the lattice construction
over the CYK grid and pruning strategies. Sec-
tion 3 reports translation experiments for Arabic-to-
English and Chinese-to-English, and Section 4 con-
cludes.
2 Hierarchical Translation with WFSTs
The translation system is based on a variant of the
CYK algorithm closely related to CYK+ (Chappe-
lier and Rajman, 1998). Parsing follows the de-
scription of Chiang (2005; 2007), maintaining back-
pointers and employing hypothesis recombination
without pruning. The underlying model is a syn-
chronous context-free grammar consisting of a set
R = {Rr} of rules Rr : N ? ??r,?r? / pr, with
?glue? rules, S ? ?X,X? and S ? ?S X,S X?. If a
rule has probability pr, it is transformed to a cost cr;
here we use the tropical semiring, so cr = ? log pr.
N denotes a non-terminal; in this paper, N can be
either S, X, or V (see section 3.2). T denotes the
terminals (words), and the grammar builds parses
based on strings ?, ? ? {{S,X, V } ? T}+. Each
cell in the CYK grid is specified by a non-terminal
symbol and position in the CYK grid: (N,x, y),
which spans sx+y?1x on the source sentence.
In effect, the source language sentence is parsed
using a context-free grammar with rules N ? ?.
The generation of translations is a second step that
follows parsing. For this second step, we describe
a method to construct word lattices with all possible
translations that can be produced by the hierarchical
rules. Construction proceeds by traversing the CYK
grid along the backpointers established in parsing.
In each cell (N,x, y) in the CYK grid, we build a
target language word lattice L(N,x, y). This lat-
tice contains every translation of sx+y?1x from every
derivation headed by N . These lattices also contain
the translation scores on their arc weights.
The ultimate objective is the word lattice
L(S, 1, J) which corresponds to all the analyses that
cover the source sentence sJ1 . Once this is built,
we can apply a target language model to L(S, 1, J)
to obtain the final target language translation lattice
(Allauzen et al, 2003).
We use the approach of Mohri (2002) in applying
WFSTs to statistical NLP. This fits well with the use
of the OpenFST toolkit (Allauzen et al, 2007) to
implement our decoder.
2.1 Lattice Construction Over the CYK Grid
In each cell (N,x, y), the set of rule indices used
by the parser is denoted R(N,x, y), i.e. for r ?
R(N,x, y), N ? ??r,?r? was used in at least one
derivation involving that cell.
For each rule Rr, r ? R(N,x, y), we build a lat-
tice L(N,x, y, r). This lattice is derived from the
target side of the rule ?r by concatenating lattices
434
R1: X ? ?s1 s2 s3,t1 t2?
R2: X ? ?s1 s2,t7 t8?
R3: X ? ?s3,t9?
R4: S ? ?X,X?
R5: S ? ?S X,S X?
L(S, 1, 3) = L(S, 1, 3, 4) ? L(S, 1, 3, 5)
L(S, 1, 3, 4) = L(X, 1, 3) = L(X, 1, 3, 1) =
= A(t1)?A(t2)
L(S, 1, 3, 5) = L(S, 1, 2)? L(X, 3, 1)
L(S, 1, 2) = L(S, 1, 2, 4) = L(X, 1, 2) =
= L(X, 1, 2, 2) = A(t7)?A(t8)
L(X, 3, 1) = L(X, 3, 1, 3) = A(t9)
L(S, 1, 3, 5) = A(t7)?A(t8)?A(t9)
L(S, 1, 3) = (A(t1)?A(t2))? (A(t7)?A(t8)?A(t9))
Figure 1: Production of target lattice L(S, 1, 3) using translation rules within CYK grid for sentence s1s2s3. The grid
is represented here in two dimensions (x, y). In practice only the first column accepts both non-terminals (S,X). For
this reason it is divided in two subcolumns.
corresponding to the elements of ?r = ?r1...?r|?r |.
If an ?ri is a terminal, creating its lattice is straight-
forward. If ?ri is a non-terminal, it refers to a cell
(N ?, x?, y?) lower in the grid identified by the back-
pointer BP (N,x, y, r, i); in this case, the lattice
used is L(N ?, x?, y?). Taken together,
L(N,x, y, r) = ?
i=1..|?r|
L(N,x, y, r, i) (1)
L(N,x, y, r, i) =
{
A(?i) if ?i ? T
L(N ?, x?, y?) else
(2)
where A(t), t ? T returns a single-arc accep-
tor which accepts only the symbol t. The lattice
L(N,x, y) is then built as the union of lattices cor-
responding to the rules in R(N,x, y):
L(N,x, y) = ?
r?R(N,x,y)
L(N,x, y, r) (3)
Lattice union and concatenation are performed
using the ? and ? WFST operations respectively, as
described by Allauzen et al(2007). If a rule Rr has
a cost cr, it is applied to the exit state of the lattice
L(N,x, y, r) prior to the operation of Equation 3.
2.1.1 An Example of Phrase-based Translation
Figure 1 illustrates this process for a three word
source sentence s1s2s3 under monotone phrase-
based translation. The left-hand side shows the state
of the CYK grid after parsing using the rules R1 to
R5. These include 3 rules with only terminals (R1,
R2, R3) and the glue rules (R4, R5). Arrows repre-
sent backpointers to lower-level cells. We are inter-
ested in the upper-most S cell (S, 1, 3), as it repre-
sents the search space of translation hypotheses cov-
ering the whole source sentence. Two rules (R4, R5)
are in this cell, so the lattice L(S, 1, 3) will be ob-
tained by the union of the two lattices found by the
backpointers of these two rules. This process is ex-
plicitly derived in the right-hand side of Figure 1.
2.1.2 An Example of Hierarchical Translation
Figure 2 shows a hierarchical scenario for the
same sentence. Three rules, R6, R7, R8, are added
to the example of Figure 1, thus providing two ad-
ditional derivations. This makes use of sublattices
already produced in the creation of L(S, 1, 3, 5) and
L(X, 1, 3, 1) in Figure 1; these are within {}.
2.2 A Procedure for Lattice Construction
Figure 3 presents an algorithm to build the lattice
for every cell. The algorithm uses memoization: if
a lattice for a requested cell already exists, it is re-
turned (line 2); otherwise it is constructed via equa-
tions 1,2,3. For every rule, each element of the tar-
get side (lines 3,4) is checked as terminal or non-
terminal (equation 2). If it is a terminal element
(line 5), a simple acceptor is built. If it is a non-
terminal (line 6), the lattice associated to its back-
pointer is returned (lines 7 and 8). The complete
lattice L(N,x, y, r) for each rule is built by equa-
tion 1 (line 9). The lattice L(N,x, y) for this cell
is then found by union of all the component rules
(line 10, equation 3); this lattice is then reduced by
435
R6: X ? ?s1,t20?
R7: X ? ?X1 s2 X2,X1 t10 X2?
R8: X ? ?X1 s2 X2,X2 t10 X1?
L(S, 1, 3) = L(S, 1, 3, 4) ?{L(S, 1, 3, 5)}
L(S, 1, 3, 4) = L(X, 1, 3) =
={L(X, 1, 3, 1)} ?L(X, 1, 3, 7)? L(X, 1, 3, 8)
L(X, 1, 3, 7) = L(X, 1, 1, 6)?A(t10)?L(X, 3, 1, 3) =
= A(t20)?A(t10)?A(t9)
L(X, 1, 3, 8) = A(t9)?A(t10)?A(t20)
L(S, 1, 3) = {(A(t1)?A(t2))} ?
?(A(t20)?A(t10)?A(t9))? (A(t9)?A(t10)?A(t20))?
?{(A(t7)?A(t8)?A(t9))}
Figure 2: Translation as in Figure 1 but with additional rules R6,R7,R8. Lattices previously derived appear within {}.
standard WFST operations (lines 11,12,13). It is
important at this point to remove any epsilon arcs
which may have been introduced by the various
WFST union, concatenation, and replacement oper-
ations (Allauzen et al, 2007).
1 function buildFst(N,x,y)
2 if ? L(N,x, y) return L(N,x, y)
3 for r ? R(N,x, y), Rr : N ? ??,??
4 for i = 1...|?|
5 if ?i ? T, L(N,x, y, r, i) = A(?i)
6 else
7 (N ?, x?, y?) = BP (?i)
8 L(N,x, y, r, i) = buildFst(N ?, x?, y?)
9 L(N,x, y, r)=?i=1..|?| L(N,x, y, r, i)
10 L(N,x, y) =?r?R(N,x,y) L(N,x, y, r)
11 fstRmEpsilon L(N,x, y)
12 fstDeterminize L(N,x, y)
13 fstMinimize L(N,x, y)
14 return L(N,x, y)
Figure 3: Recursive Lattice Construction.
2.3 Delayed Translation
Equation 2 leads to the recursive construction of lat-
tices in upper-levels of the grid through the union
and concatenation of lattices from lower levels. If
equations 1 and 3 are actually carried out over fully
expanded word lattices, the memory required by the
upper lattices will increase exponentially.
To avoid this, we use special arcs that serve as
pointers to the low-level lattices. This effectively
builds a skeleton of the desired lattice and delays
the creation of the final word lattice until a single
replacement operation is carried out in the top cell
(S, 1, J). To make this exact, we define a function
g(N,x, y) which returns a unique tag for each lattice
in each cell, and use it to redefine equation 2. With
the backpointer (N ?, x?, y?) = BP (N,x, y, r, i),
these special arcs are introduced as:
L(N,x, y, r, i) =
{
A(?i) if ?i ? T
A(g(N ?, x?, y?)) else
(4)
The resulting lattices L(N,x, y) are a mix of tar-
get language words and lattice pointers (Figure 4,
top). However each still represents the entire search
space of all translation hypotheses covering the
span. Importantly, operations on these lattices ?
such as lossless size reduction via determinization
and minimization ? can still be performed. Owing
to the existence of multiple hierarchical rules which
share the same low-level dependencies, these opera-
tions can greatly reduce the size of the skeleton lat-
tice; Figure 4 shows the effect on the translation ex-
ample. This process is carried out for the lattice at
every cell, even at the lowest level where there are
only sequences of word terminals. As stated, size
reductions can be significant. However not all redu-
dancy is removed, since duplicate paths may arise
through the concatenation and union of sublattices
with different spans.
At the upper-most cell, the lattice L(S, 1, J) con-
tains pointers to lower-level lattices. A single FST
replace operation (Allauzen et al, 2007) recursively
substitutes all pointers by their lower-level lattices
until no pointers are left, thus producing the com-
plete target word lattice for the whole source sen-
tence. The use of the lattice pointer arc was in-
spired by the ?lazy evaluation? techniques developed
by Mohri et al(2000). Its implementation uses the
infrastructure provided by the OpenFST libraries for
436
01
t1
2g(X,1,2)
3
g(X,1,1)
5
g(X,3,1)
7
t2
g(X,3,1)
4
t10
6t10
g(X,3,1)
g(X,1,1)
0
3g(X,1,1)
2g(X,1,2)
1
t1
4
g(X,3,1)
t10
6
g(X,3,1)
t2
5
t10
g(X,1,1)
Figure 4: Delayed translation WFST with derivations
from Figure 1 and Figure 2 before [t] and after minimiza-
tion [b].
delayed composition, etc.
2.4 Pruning in Lattice Construction
The final translation lattice L(S, 1, J) can grow very
large after the pointer arcs are expanded. We there-
fore apply a word-based language model, via WFST
composition, and perform likelihood-based prun-
ing (Allauzen et al, 2007) based on the combined
translation and language model scores.
Pruning can also be performed on sublattices
during search. One simple strategy is to monitor
the number of states in the determinized lattices
L(N,x, y). If this number is above a threshold, we
expand any pointer arcs and apply a word-based lan-
guage model via composition. The resulting lattice
is then reduced by likelihood-based pruning, after
which the LM scores are removed. This search prun-
ing can be very selective. For example, the pruning
threshold can depend on the height of the cell in the
grid. In this way the risk of search errors can be
controlled.
3 Translation Experiments
We report experiments on the NIST MT08 Arabic-
to-English and Chinese-to-English translation tasks.
We contrast two hierarchical phrase-based decoders.
The first decoder, Hiero Cube Pruning (HCP), is a k-
best decoder using cube pruning implemented as de-
scribed by Chiang (2007). In our implementation, k-
best lists contain unique hypotheses. The second de-
coder, Hiero FST (HiFST), is a lattice-based decoder
implemented with Weighted Finite State Transduc-
ers as described in the previous section. Hypotheses
are generated after determinization under the trop-
ical semiring so that scores assigned to hypotheses
arise from single minimum cost / maximum likeli-
hood derivations. We also use a variant of the k-best
decoder which works in alignment mode: given an
input k-best list, it outputs the feature scores of each
hypothesis in the list without applying any pruning.
This is used for Minimum Error Training (MET)
with the HiFST system.
These two language pairs pose very different
translation challenges. For example, Chinese-
to-English translation requires much greater word
movement than Arabic-to-English. In the frame-
work of hierarchical translation systems, we have
found that shallow decoding (see section 3.2) is
as good as full hierarchical decoding in Arabic-
to-English (Iglesias et al, 2009). In Chinese-to-
English, we have not found this to be the case.
Therefore, we contrast the performance of HiFST
and HCP under shallow hierarchical decoding for
Arabic-to-English, while for Chinese-to-English we
perform full hierarchical decoding.
Both hierarchical translation systems share a
common architecture. For both language pairs,
alignments are generated over the parallel data. The
following features are extracted and used in trans-
lation: target language model, source-to-target and
target-to-source phrase translation models, word and
rule penalties, number of usages of the glue rule,
source-to-target and target-to-source lexical models,
and three rule count features inspired by Bender et
al. (2007). The initial English language model is
a 4-gram estimated over the parallel text and a 965
million word subset of monolingual data from the
English Gigaword Third Edition. Details of the par-
allel corpus and development sets used for each lan-
guage pair are given in their respective section.
Standard MET (Och, 2003) iterative parameter
estimation under IBM BLEU (Papineni et al, 2001)
is performed on the corresponding development set.
For the HCP system, MET is done following Chi-
ang (2007). For the HiFST system, we obtain a k-
437
best list from the translation lattice and extract each
feature score with the aligner variant of the k-best
decoder. After translation with optimized feature
weights, we carry out the two following rescoring
steps.
? Large-LM rescoring. We build sentence-
specific zero-cutoff stupid-backoff (Brants et
al., 2007) 5-gram language models, estimated
using ?4.7B words of English newswire text,
and apply them to rescore either 10000-best
lists generated by HCP or word lattices gener-
ated by HiFST. Lattices provide a vast search
space relative to k-best lists, with translation
lattice sizes of 1081 hypotheses reported in the
literature (Tromble et al, 2008).
? Minimum Bayes Risk (MBR). We rescore the
first 1000-best hypotheses with MBR, taking
the negative sentence level BLEU score as the
loss function (Kumar and Byrne, 2004).
3.1 Building the Rule Sets
We extract hierarchical phrases from word align-
ments, applying the same restrictions as introduced
by Chiang (2005). Additionally, following Iglesias
et al (2009) we carry out two rule filtering strate-
gies:
? we exclude rules with two non-terminals with
the same order on the source and target side
? we consider only the 20 most frequent transla-
tions for each rule
For each development set, this produces approx-
imately 4.3M rules in Arabic-to-English and 2.0M
rules in Chinese-to-English.
3.2 Arabic-to-English Translation
We translate Arabic-to-English with shallow hierar-
chical decoding, i.e. only phrases are allowed to be
substituted into non-terminals. The rules used in this
case are, in addition to the glue rules:
X ? ??s,?s?
X ? ?V ,V ?
V ? ?s,t?
s, t ? T+; ?s, ?s ? ({V } ?T)+
For translation model training, we use all allowed
parallel corpora in the NIST MT08 Arabic track
(?150M words per language). In addition to the
MT08 set itself, we use a development set mt02-05-
tune formed from the odd numbered sentences of the
NIST MT02 through MT05 evaluation sets; the even
numbered sentences form the validation set mt02-
05-test. The mt02-05-tune set has 2,075 sentences.
The cube pruning decoder, HCP, employs k-best
lists of depth k=10000 (unique). Using deeper lists
results in excessive memory and time requirements.
In contrast, the WFST-based decoder, HiFST, re-
quires no local pruning during lattice construction
for this task and the language model is not applied
until the lattice is fully built at the upper-most cell of
the CYK grid.
Table 1 shows results for mt02-05-tune, mt02-
05-test and mt08, as measured by lowercased IBM
BLEU and TER (Snover et al, 2006). MET param-
eters are optimized for the HCP decoder. As shown
in rows ?a? and ?b?, results after MET are compara-
ble.
Search Errors Since both decoders use exactly the
same features, we can measure their search errors on
a sentence-by-sentence basis. A search error is as-
signed to one of the decoders if the other has found
a hypothesis with lower cost. For mt02-05-tune, we
find that in 18.5% of the sentences HiFST finds a hy-
pothesis with lower cost than HCP. In contrast, HCP
never finds any hypothesis with lower cost for any
sentence. This is as expected: the HiFST decoder
requires no pruning prior to applying the language
model, so search is exact.
Lattice/k-best Quality Rescoring results are dif-
ferent for cube pruning and WFST-based decoders.
Whereas HCP improves by 0.9 BLEU, HiFST im-
proves over 1.5 BLEU. Clearly, search errors in HCP
not only affect the 1-best output but also the quality
of the resulting k-best lists. For HCP, this limits the
possible gain from subsequent rescoring steps such
as large LMs and MBR.
Translation Speed HCP requires an average of 1.1
seconds per input word. HiFST cuts this time by
half, producing output at a rate of 0.5 seconds per
word. It proves much more efficient to process com-
pact lattices contaning many hypotheses rather than
to independently processing each one of them in k-
best form.
438
decoder mt02-05-tune mt02-05-test mt08
BLEU TER BLEU TER BLEU TER
a HCP 52.2 41.6 51.5 42.2 42.5 48.6
+5gram 53.1 41.0 52.5 41.5 43.3 48.3
+MBR 53.2 40.8 52.6 41.4 43.4 48.1
b HiFST 52.2 41.5 51.6 42.1 42.4 48.7
+5gram 53.3 40.6 52.7 41.3 43.7 48.1
+MBR 53.7 40.4 53.3 40.9 44.0 48.0
Decoding time in secs/word: 1.1 for HCP; 0.5 for HiFST.
Table 1: Constrative Arabic-to-English translation results (lower-cased IBM BLEU | TER) after MET and subsequent
rescoring steps. Decoding time reported for mt02-05-tune.
The mixed case NIST BLEU-4 for the HiFST sys-
tem on mt08 is 42.9. This is directly comparable to
the official MT08 Constrained Training Track eval-
uation results1.
3.3 Chinese-to-English Translation
We translate Chinese-to-English with full hierarchi-
cal decoding, i.e. hierarchical rules are allowed to be
substituted into non-terminals. We consider a maxi-
mum span of 10 words for the application of hierar-
chical rules and only glue rules are allowed at upper
levels of the CYK grid.
For translation model training, we use all avail-
able data for the GALE 2008 evaluation2, approx.
250M words per language. In addition to the MT08
set itself, we use a development set tune-nw and
a validation set test-nw. These contain a mix of
the newswire portions of MT02 through MT05 and
additional developments sets created by translation
within the GALE program. The tune-nw set has
1,755 sentences.
Again, the HCP decoder employs k-best lists of
depth k=10000. The HiFST decoder applies prun-
ing in search as described in Section 2.4, so that any
lattice in the CYK grid is pruned if it covers at least
3 source words and contains more than 10k states.
The likelihood pruning threshold relative to the best
path in the lattice is 9. This is a very broad threshold
so that very few paths are discarded.
1Full MT08 results are available at http://www.nist.gov/
speech/tests/mt/2008/doc/mt08 official results v0.html. It is
worth noting that many of the top entries make use of system
combination; the results reported here are for single system
translation.
2See http://projects.ldc.upenn.edu/gale/data/catalog.html.
Improved Optimization Table 2 shows results for
tune-nw, test-nw and mt08, as measured by lower-
cased IBM BLEU and TER. The first two rows show
results for HCP when using MET parameters opti-
mized over k-best lists produced by HCP (row ?a?)
and by HiFST (row ?b?). We find that using the k-
best list obtained by the HiFST decoder yields bet-
ter parameters during optimization. Tuning on the
HiFST k-best lists improves the HCP BLEU score,
as well. We find consistent improvements in BLEU;
TER also improves overall, although less consis-
tently.
Search Errors Measured over the tune-nw devel-
opment set, HiFST finds a hypothesis with lower
cost in 48.4% of the sentences. In contrast, HCP
never finds any hypothesis with a lower cost for any
sentence, indicating that the described pruning strat-
egy for HiFST is much broader than that of HCP.
Note that HCP search errors are more frequent for
this language pair. This is due to the larger search
space required in fully hierarchical translation; the
larger the search space, the more search errors will
be produced by the cube pruning k-best implemen-
tation.
Lattice/k-best Quality The lattices produced by
HiFST yield greater gains in LM rescoring than the
k-best lists produced by HCP. Including the subse-
quent MBR rescoring, translation improves as much
as 1.2 BLEU, compared to 0.7 BLEU with HCP.
The mixed case NIST BLEU-4 for the HiFST sys-
tem on mt08 is 27.8, comparable to official results
in the UnConstrained Training Track of the NIST
2008 evaluation.
439
decoder MET k-best tune-nw test-nw mt08
BLEU TER BLEU TER BLEU TER
a HCP HCP 31.6 59.7 31.9 59.7 ? ?
b HCP 31.7 60.0 32.2 59.9 27.2 60.2
+5gram HiFST 32.2 59.3 32.6 59.4 27.8 59.3
+MBR 32.4 59.2 32.7 59.4 28.1 59.3
c HiFST 32.0 60.1 32.2 60.0 27.1 60.5
+5gram HiFST 32.7 58.3 33.1 58.4 28.1 59.1
+MBR 32.9 58.4 33.4 58.5 28.9 58.9
Table 2: Contrastive Chinese-to-English translation results (lower-cased IBM BLEU|TER) after MET and subsequent
rescoring steps. The MET k-best column indicates which decoder generated the k-best lists used in MET optimization.
4 Conclusions
The lattice-based decoder for hierarchical phrase-
based translation described in this paper can be eas-
ily implemented using Weighted Finite State Trans-
ducers. We find many benefits in this approach
to translation. From a practical perspective, the
computational operations required can be easily car-
ried out using standard operations already imple-
mented in general purpose libraries. From a model-
ing perspective, the compact representation of mul-
tiple translation hypotheses in lattice form requires
less pruning in hierarchical search. The result is
fewer search errors and reduced overall memory use
relative to cube pruning over k-best lists. We also
find improved performance of subsequent rescor-
ing procedures which rely on the translation scores.
In direct comparison to k-best lists generated un-
der cube pruning, we find that MET parameter opti-
mization, rescoring with large language models, and
MBR decoding, are all improved when applied to
translations generated by the lattice-based hierarchi-
cal decoder.
Acknowledgments
This work was supported in part by the GALE pro-
gram of the Defense Advanced Research Projects
Agency, Contract No. HR0011- 06-C-0022. G.
Iglesias supported by Spanish Government research
grant BES-2007-15956 (project TEC2006-13694-
C03-03).
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003.
Generalized algorithms for constructing statistical lan-
guage models. In Proceedings of ACL, pages 557?
564.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of CIAA, pages 11?23.
Srinivas Bangalore and Giuseppe Riccardi. 2001. A
finite-state approach to machine translation. In Pro-
ceedings of NAACL.
Oliver Bender, Evgeny Matusov, Stefan Hahn, Sasa
Hasan, Shahram Khadivi, and Hermann Ney. 2007.
The RWTH Arabic-to-English spoken language trans-
lation system. In Proceedings of ASRU, pages 396?
401.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proceedings of ACL-HLT,
pages 200?208.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of EMNLP-ACL,
pages 858?867.
Francisco Casacuberta. 2001. Finite-state transducers
for speech-input translation. In Proceedings of ASRU.
Jean-Ce?dric Chappelier and Martin Rajman. 1998.
A generalized CYK algorithm for parsing stochastic
CFG. In Proceedings of TAPD, pages 133?137.
Jean-Ce?dric Chappelier, Martin Rajman, Ramo?n
Aragu?e?s, and Antoine Rozenknop. 1999. Lattice
parsing for speech recognition. In Proceedings of
TALN, pages 95?104.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL, pages 263?270.
440
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In Pro-
ceedings of ACL-HLT, pages 1012?1020.
Jonathan Graehl, Kevin Knight, and Jonathan May. 2008.
Training tree transducers. Computational Linguistics,
34(3):391?427.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of ACL, pages 144?151.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern
for efficient hierarchical translation. In Proceedings of
EACL.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT-NAACL, pages 169?176.
Shankar Kumar and William Byrne. 2005. Local phrase
reordering models for statistical machine translation.
In Proceedings of HLT-EMNLP, pages 161?168.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In Pro-
ceedings of the ACL-HLT Second Workshop on Syntax
and Structure in Statistical Translation, pages 10?18.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proceedings of COLING, pages
505?512.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proceedings of ACL-HLT, pages 1003?1011.
Lambert Mathias and William Byrne. 2006. Statisti-
cal phrase-based speech translation. In Proceedings
of ICASSP.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
2000. The design principles of a weighted finite-
state transducer library. Theoretical Computer Sci-
ence, 231:17?32.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
2002. Weighted finite-state transducers in speech
recognition. In Computer Speech and Language, vol-
ume 16, pages 69?88.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL,
pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of ACL,
pages 311?318.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-
ros Matsoukas, Richard Schwartz, and Bonnie Dorr.
2007. Combining outputs from multiple machine
translation systems. In Proceedings of HLT-NAACL,
pages 228?235.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-HLT, pages 577?585.
Khe Chai Sim, William Byrne, Mark Gales, Hichem
Sahbi, and Phil Woodland. 2007. Consensus net-
work decoding for statistical machine translation sys-
tem combination. In Proceedings of ICASSP, vol-
ume 4, pages 105?108.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of AMTA, pages 223?231.
Roy Tromble, Shankar Kumar, Franz J. Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-Risk
decoding for statistical machine translation. In Pro-
ceedings of EMNLP, pages 620?629.
Ashish Venugopal, Andreas Zollmann, and Vogel
Stephan. 2007. An efficient two-pass approach to
synchronous-CFG driven statistical MT. In Proceed-
ings of HLT-NAACL, pages 500?507.
Hao Zhang and Daniel Gildea. 2006. Synchronous bi-
narization for machine translation. In Proceedings of
HLT-NAACL, pages 256?263.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of NAACL Workshop on Statistical Ma-
chine Translation, pages 138?141.
Andreas Zollmann, Ashish Venugopal, Franz Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
MT. In Proceedings of COLING, pages 1145?1152.
441
Hierarchical Phrase-Based Translation with
Weighted Finite-State Transducers and
Shallow-n Grammars
Adria` de Gispert?
University of Cambridge
Gonzalo Iglesias??
University of Vigo
Graeme Blackwood?
University of Cambridge
Eduardo R. Banga??
University of Vigo
William Byrne?
University of Cambridge
In this article we describe HiFST, a lattice-based decoder for hierarchical phrase-based translation
and alignment. The decoder is implemented with standard Weighted Finite-State Transducer
(WFST) operations as an alternative to the well-known cube pruning procedure. We find that
the use of WFSTs rather than k-best lists requires less pruning in translation search, resulting in
fewer search errors, better parameter optimization, and improved translation performance. The
direct generation of translation lattices in the target language can improve subsequent rescoring
procedures, yielding further gains when applying long-span language models and Minimum
Bayes Risk decoding. We also provide insights as to how to control the size of the search space
defined by hierarchical rules. We show that shallow-n grammars, low-level rule catenation,
and other search constraints can help to match the power of the translation system to specific
language pairs.
1. Introduction
Hierarchical phrase-based translation (Chiang 2005) is one of the current promising
approaches to statistical machine translation (SMT). Hiero SMT systems are based
on probabilistic synchronous context-free grammars (SCFGs) whose translation rules
? University of Cambridge, Department of Engineering. CB2 1PZ Cambridge, U.K.
E-mail: {ad465,gwb24,wjb31}@eng.cam.ac.uk.
?? University of Vigo, Department of Signal Processing and Communications, 36310 Vigo, Spain.
E-mail: {giglesia,erbanga}@gts.tsc.uvigo.es.
Submission received: 30 October 2009; revised submission received: 24 February 2010; accepted for
publication: 10 April 2010.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 3
can be extracted automatically from word-aligned parallel text. These grammars can
produce a very rich space of candidate translations and, relative to simpler phrase-
based systems (Koehn, Och, and Marcu 2003), the power of Hiero is most evident in
translation between dissimilar languages, such as English and Chinese (Chiang 2005,
2007). Hiero is able to learn and apply complex patterns in movement and translation
that are not possible with simpler systems. Hiero can also be used to good effect on
?simpler? problems, such as translation between English and Spanish (Iglesias et al
2009c), even though there is not the same need for the full complexity of movement and
translation. If gains in using Hiero are small, however, the computational and modeling
complexity involved are difficult to justify. Such concerns would vanish if there were
reliable methods to match Hiero complexity for specific translation problems. Loosely
put, it would be a good thing if the complexity of a system was somehow proportional
to the improvement in translation quality the system delivers.
Another notable current trend in SMT is system combination. Minimum Bayes
Risk decoding is widely used to rescore and improve hypotheses produced by indi-
vidual systems (Kumar and Byrne 2004; Tromble et al 2008; de Gispert et al 2009),
and more aggressive system combination techniques which synthesize entirely new
hypotheses from those of contributing systems can give even greater translation im-
provements (Rosti et al 2007; Sim et al 2007). It is now commonplace to note that even
the best available individual SMT system can be significantly improved upon by such
techniques. This puts a burden on the underlying SMT systems which is somewhat
unusual in NLP. The requirement is not merely to produce a single hypothesis that
is as good as possible. Ideally, the SMT systems should generate large collections of
candidate hypotheses that are simultaneously diverse and of good quality.
Relative to these concerns, previously published descriptions of Hiero have noted
certain limitations. Spurious ambiguity (Chiang 2005) was described as
a situation where the decoder produces many derivations that are distinct yet have the
same model feature vectors and give the same translation. This can result in n-best lists
with very few different translations which is problematic for the minimum-error-rate
training algorithm ...
This is due in part to the cube pruning procedure (Chiang 2007), which enumerates all
distinct hypotheses to a fixed depth by means of k-best hypothesis lists. If enumeration
was not necessary, or if the lists could be arbitrarily deep, there might still be many
duplicate derivations, but at least the hypothesis space would not be impoverished.
Spurious ambiguity is also related to overgeneration (Varile and Lau 1988; Wu
1997; Setiawan et al 2009). For our purposes we say that overgeneration occurs when
different derivations based on the same set of rules give rise to different translations.
An example is given in Figure 1.
This process is not necessarily a bad thing in that it allows new translations to be
synthesized from rules extracted from training data; a strong target language model,
such as a high order n-gram, is typically relied upon to discard unsuitable hypotheses.
Overgeneration does complicate translation, however, in that many hypotheses are
introduced only to be subsequently discarded. The situation is further complicated by
search errors. Any search procedure which relies on pruning during search is at risk of
search errors and the risk is made worse if the grammars tend to introduce many similar
scoring hypotheses. In particular we have found that cube pruning is very prone to
search errors, that is, the hypotheses produced by cube pruning are not the top scoring
hypotheses which should be found under the Hiero grammar (Iglesias et al 2009b).
506
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Figure 1
Example of multiple translation sequences from a simple grammar fragment showing variability
in reordering in translation of the source sequence abc.
These limitations are clearly related to each other. Moreover, they become more
problematic as the amount of parallel text grows. As the number of rules in the grammar
increases, the grammars become more expressive, but the ability to search them does not
improve. This leads to a widening gap between the expressive power of the grammar
and the ability to search it to find good and diverse hypotheses.
In this article we describe the following two refinements to Hiero which are in-
tended to address some of the limitations in its original formulation.
Lattice-based hierarchical translation We describe how the cube pruning procedure
can be replaced by standard operations with Weighted Finite State Transducers
(WFSTs) so that Hiero uses translation lattices rather than n-best lists in search.
We find that keeping partial translation hypotheses in lattice form greatly reduces
search errors. In some instances it is possible to perform translation without
any pruning at all so that search errors are completely eliminated. Consistent
with the observation by Chiang (2005), this leads to improvements in minimum
error rate training. Furthermore, the direct generation of translation lattices can
improve gains from subsequent language model and Minimum Bayes Risk (MBR)
rescoring.
Shallow-n grammars and additional nonterminal categories Nonterminals can be in-
corporated into hierarchical translation rules for the purpose of tuning the size
of the Hiero search space for individual language pairs. Shallow-n grammars are
described and shown to control the level of rule nesting, low-level rule catenation,
and the minimum and maximum spans of individual translation rules. In trans-
lation experiments we find that a shallow-1 grammar (one level of rule nesting)
is sufficiently expressive for Arabic-to-English translation, but that a shallow-3
grammar is required in Chinese-to-English translation to match the performance
of a full Hiero system that allows arbitrary rule nesting. These nonterminals are
introduced to control the Hiero search space and do not require estimation from
annotated?or parsed?parallel text, as can be required by translation systems
based on linguistically motivated grammars. We use this approach as the basis
of a general approach to SMT modeling. To control overgeneration, we revisit
the synchronous context-free grammar defined by hierarchical rules and take a
shallow-1 grammar as a starting point. We then increase the complexity of the
rules until the desired translation quality is found.
507
Computational Linguistics Volume 36, Number 3
With these refinements we find that hierarchical phrase-based translation can be effi-
ciently carried out with no (or minimal) search errors in large-data tasks and can achieve
state-of-the-art translation performance.
There are many benefits to formulating Hiero translation in terms of WFSTs. Fol-
lowing the manner in which Knight and Al-Onaizan (1998), Kumar, Deng, and Byrne
(2006), and Graehl, Knight, and May (2008) elucidate other machine translation models,
we can use WFST operations to make the operations of the Hiero decoder very clear. The
simplicity of the analysis makes it possible to focus on the underlying grammars and
avoid the complexities of heuristic search procedures. Once the decoder is formulated,
implementation is mostly straightforward using standard WFST techniques developed
for language processing (Mohri, Pereira, and Riley 2002). What difficulties arise are due
to using finite state techniques with grammars which are not themselves finite state.
We will show, however, that the basic operations which need to be performed, such as
extracting sufficient statistics for minimum error rate training, can be done relatively
easily and naturally.
1.1 Overview
In Section 2 we describe HiFST, which is a hierarchical phrase-based translation system
based on the OpenFST WFST libraries (Allauzen et al 2007). We describe how trans-
lation lattices can be generated over the Cocke-Younger-Kasami (CYK) grid used for
parsing under Hiero. We also review some modeling issues needed for practical trans-
lation, such as the efficient handling of source language deletions and the extraction of
statistics for minimum error rate training. This requires running HiFST in ?alignment
mode? (Section 2.3) to find all the rule derivations that generate a given set of translation
hypotheses.
In Section 3 we investigate parameters that control the size and nature of the
hierarchical phrase-based search space as defined by hierarchical translation rules. To
efficiently explore the largest possible space and avoid pruning in search, we introduce
ways to easily adapt the grammar to the reordering needs of each language pair. We
describe the use of additional nonterminal categories to limit the degree of rule nesting,
and can directly control the minimum or maximum span each translation rule can cover.
In Section 4 we report detailed translation results for Arabic-to-English and
Chinese-to-English, and review translation results for Spanish-to-English and Finnish-
to-English translation. In these experiments we contrast the performance of lattice-based
and cube pruning hierarchical decoding and we measure the impact on processing
time and translation performance due to changes in search parameters and grammar
configurations. We demonstrate that it is easy and feasible to compute the marginal
instead of the Viterbi probabilities when using WFSTs, and that this yields gains in
translation performance. And finally, we show that lattice-based translation performs
significantly better than k-best lists for the task of combining translation hypotheses
generated from alternative morphological segmentations of the data via lattice-based
MBR decoding.
2. Hierarchical Translation and Alignment with WFSTs
Hierarchical phrase-based rules define a synchronous context-free grammar (CFG) and
a particular search space of translation candidates. Table 1 shows the type of rules in-
cluded in a standard hierarchical phrase-based grammar, where T denotes the terminals
(words) and ? is a bijective function that relates the source and target nonterminals of
508
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Table 1
Rules contained in the standard hierarchical grammar.
standard hierarchical grammar
S??X,X? glue rule 1
S??S X,S X? glue rule 2
X???,?,?? , ?,? ? {X ? T}+ hiero rules
each rule (Chiang 2007). This function is defined if there are at least two nonterminals,
and for clarity of presentation may be omitted in general rule discussions. When ?,? ?
{T}+, that is, the rule contains no nonterminals, the rule is a simple lexical phrase pair.
The HiFST translation system is based on a variant of the CYK algorithm closely
related to CYK+ (Chappelier and Rajman 1998). Parsing follows the description of
Chiang (2005, 2007); it maintains back-pointers and employs hypothesis recombination
without pruning. The underlying model is a probabilisitic synchronous CFG consisting
of a set R = {Rr} of rules Rr : Nr ? ??r,?r? / pr, with ?glue? rules, S ? ?X,X? and
S ? ?S X,S X?. N denotes the set of nonterminal categories (examples are given in
Section 3), and pr denotes the rule probability, typically transformed to a cost cr; unless
otherwise noted we use the tropical semiring, so cr = ? log pr. T denotes the terminals
(words), and the grammar builds parses based on strings ?,? ? {N ? T}+. Each cell
in the CYK grid is specified by a nonterminal symbol and position in the CYK grid:
(N, x, y), which spans sx+y?1x on the source sentence.
In effect, the source language sentence is parsed using a CFG with rules N ? ?. The
generation of translations is a second step that follows parsing. For this second step, we
describe a method to construct word lattices with all possible translations that can be
produced by the hierarchical rules. Construction proceeds by traversing the CYK grid
along the back-pointers established in parsing. In each cell (N, x, y) in the CYK grid, we
build a target language word lattice L(N, x, y). This lattice contains every translation of
sx+y?1x from every derivation headed by N. These lattices also contain the translation
scores on their arc weights.
The ultimate objective is the word lattice L(S, 1, J) which corresponds to all the
analyses that cover the source sentence sJ1. Once this is built, we can apply a target lan-
guage model to L(S, 1, J) to obtain the final target language translation lattice (Allauzen,
Mohri, and Roark 2003).
2.1 Lattice Construction over the CYK Grid
In each cell (N, x, y), the set of rule indices used by the parser is denoted R(N, x, y), that
is, for r ? R(N, x, y), the rule N ? ??r,?r? was used in at least one derivation involving
that cell.
For each rule Rr, r ? R(N, x, y), we build a lattice L(N, x, y, r). This lattice is derived
from the target side of the rule ?r by concatenating lattices corresponding to the ele-
ments of ?r = ?r1...?
r
|?r|. If an ?
r
i is a terminal, creating its lattice is straightforward. If
?ri is a nonterminal, it refers to a cell (N
?, x?, y?) lower in the grid identified by the back-
pointer BP(N, x, y, r, i); in this case, the lattice used is L(N?, x?, y?). Taken together,
L(N, x, y, r) =
?
i=1..|?r|
L(N, x, y, r, i) (1)
509
Computational Linguistics Volume 36, Number 3
Figure 2
Production of target lattice L(S, 1, 3) using translation rules within CYK grid for sentence s1s2s3.
The grid is represented here in two dimensions (x, y). In practice only the first column accepts
both nonterminals (S,X). For this reason it is divided into two subcolumns.
L(N, x, y, r, i) =
{
A(?i) if ?i ? T
L(N?, x?, y?) otherwise (2)
where A(t), t ? T returns a single-arc acceptor which accepts only the symbol t. The
lattice L(N, x, y) is then built as the union of lattices corresponding to the rules in
R(N, x, y):
L(N, x, y) =
?
r?R(N,x,y)
L(N, x, y, r) (3)
Lattice union and concatenation are performed using the? and?WFST operations,
respectively, as described by Allauzen et al (2007). If a rule Rr has a cost cr, it is applied
to the exit state of the lattice L(N, x, y, r) prior to the operation of Equation (3).
2.1.1 An Example of Phrase-based Translation. Figure 2 illustrates this process for a three-
word source sentence s1s2s3 under monotonic phrase-based translation. The left-hand
side shows the state of the CYK grid after parsing using the rules R1 to R5. These include
three standard phrases, that is, rules with only terminals (R1, R2, R3), and the glue rules
(R4, R5). Arrows represent back-pointers to lower-level cells. We are interested in the
uppermost S cell (S, 1, 3), as it represents the search space of translation hypotheses
covering the whole source sentence. Two rules (R4, R5) are in this cell, so the lattice
L(S, 1, 3) will be obtained by the union of the two lattices found by the back-pointers of
these two rules. This process is explicitly derived in the right-hand side of Figure 2.
2.1.2 An Example of Hierarchical Translation. Figure 3 shows a hierarchical scenario for
the same sentence. Three rules, R6,R7,R8, are added to the example of Figure 2, thus
providing two additional derivations. This makes use of sublattices already produced
in the creation of L(S, 1, 3, 5) and L(X, 1, 3, 1) in Figure 2; these are shown within {}.
510
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Figure 3
Translation as in Figure 2 but with additional rules R6,R7,R8. Lattices previously derived appear
within {}.
2.2 A Procedure for Lattice Construction
Figure 4 presents an algorithm to build the lattice for every cell. The algorithm uses
memoization: If a lattice for a requested cell already exists, it is returned (line 2);
otherwise it is constructed via Equations (1)?(3). For every rule, each element of the
target side (lines 3,4) is checked as terminal or nonterminal (Equation (2)). If it is a
terminal element (line 5), a simple acceptor is built. If it is a nonterminal (line 6), the
lattice associated to its back-pointer is returned (lines 7 and 8). The complete lattice
L(N, x, y, r) for each rule is built by Equation (1) (line 9). The lattice L(N, x, y) for this
cell is then found by union of all the component rules (line 10, Equation (3)); this lattice
is then reduced by standard WFST operations (lines 11, 12, 13). It is important at this
point to remove any epsilon arcs which may have been introduced by the various WFST
union, concatenation, and replacement operations (Allauzen et al 2007).
We now address several important aspects of efficient implementation.
Figure 4
Recursive lattice construction from a CYK grid.
511
Computational Linguistics Volume 36, Number 3
Figure 5
Delayed translation WFST with derivations from Figures 2 and 3 before (left) and after
minimization (right).
2.2.1 Delayed Translation. Equation (2) leads to the recursive construction of lattices in
upper levels of the grid through the union and concatenation of lattices from lower
levels. If Equations (1) and (3) are actually carried out over fully expanded word lattices,
the memory required by the upper lattices will increase exponentially.
To avoid this, we use special arcs that serve as pointers to the low-level lattices. This
effectively builds a skeleton for the desired lattice and delays the creation of the final
word lattice until a single replacement operation is carried out in the top cell (S, 1, J).
To make this exact, we define a function g(N, x, y) which returns a unique tag for each
lattice in each cell, and use it to redefine Equation (2). With the back-pointer (N?, x?, y?) =
BP(N, x, y, r, i), these special arcs are introduced as
L(N, x, y, r, i) =
{
A(?i) if ?i ? T
A(g(N?, x?, y?)) otherwise (4)
The resulting lattices L(N, x, y) are a mix of target language words and lattice
pointers (Figure 5, left). Each still represents the entire search space of all translation
hypotheses covering the span, however. Importantly, operations on these lattices?such
as lossless size reduction via determinization and minimization (Mohri, Pereira, and
Riley 2002)?can still be performed. Owing to the existence of multiple hierarchical rules
which share the same low-level dependencies, these operations can greatly reduce the
size of the skeleton lattice; Figure 5 shows the effect on the translation example. This
process is carried out for the lattice at every cell, even at the lowest level where there
are only sequences of word terminals. As stated, size reductions can be significant. Not
all redundancy is removed, however, because duplicate paths may arise through the
concatenation and union of sublattices with different spans.
At the upper-most cell, the lattice L(S, 1, J) contains pointers to lower-level lattices.
A single FST replace operation (Allauzen et al 2007) recursively substitutes all pointers
by their lower-level lattices until no pointers are left, thus producing the complete
target word lattice for the whole source sentence. The use of the lattice pointer arc was
inspired by the ?lazy evaluation? techniques developed by Mohri, Pereira, and Riley
(2000), closely related to Recursive Transition Networks (Woods 1970; Mohri 1997). Its
implementation uses the infrastructure provided by the OpenFST libraries for delayed
composition.
2.2.2 Top-level Pruning and Search Pruning. The final translation lattice L(S, 1, J) can
be quite large after the pointer arcs are expanded. We therefore apply a word-based
512
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Figure 6
Transducers for filtering up to one (left) or two (right) consecutive deletions.
language model via WFST composition (Allauzen et al 2007) and perform likelihood-
based pruning based on the combined translation and language model scores. We call
this top-level pruning because it is performed over the topmost lattice.
Pruning can also be performed on the sublattices in each cell during search. One
simple strategy is to monitor the number of states in the determinized lattices L(N, x, y).
If this number is above a threshold, we expand any pointer arcs and apply a word-based
language model via composition. The resulting lattice is then reduced by likelihood-
based pruning, after which the LM scores are removed. These pruning strategies can be
very selective, for example allowing the pruning threshold to depend on the height of
the cell in the grid. In this way the risk of search errors can be controlled.
The same n-gram language model can be used for top-level pruning and search
pruning, although different WFST realizations are required. For top-level pruning, a
standard implementation as described by Allauzen et al (2007) is appropriate. For
search pruning, the WFST must allow for incomplete language model histories, because
many sublattice paths are incomplete translation hypotheses which do not begin with
a sentence-start marker. The language model acceptor is constructed so that initial
substrings of length less than the language model order are assigned no weight under
the language model.
2.2.3 Constrained Source Word Deletion. As a practical matter it can be useful to allow SMT
systems to delete some source words rather than to enforce their translation. Deletions
can be allowed in Hiero by including in the grammar a set of special deletion rules
of the type: X??s,NULL?. Unconstrained application of these rules can lead to overly
large and complex search spaces, however. We therefore limit the number of consecutive
source word deletions as we explore each cell of the CYK grid. This is done by standard
composition with an unweighted transducer that maps any word to itself, and up to k
NULL tokens to  arcs. In Figure 6 this simple transducer for k = 1 and k = 2 is drawn.
Composition of the lattice in each cell with this transducer filters out all translations
with more than k consecutive deleted words.
2.3 Hierarchical Phrase-Based Alignment with WFSTs
We now describe a method to apply our decoder in alignment mode. The objective in
alignment is to recover all the derivations which can produce a given translation. We do
this rather than keep track of the rules used during translation, because we find it faster
and more efficient first to generate translations and then, by running the system as an
aligner with a constrained target space, to extract all the relevant derivations with their
costs. As will be discussed in Section 2.3.1, this is useful for minimum error training,
where the contribution of each feature to the overall hypothesis cost is required for
system optimization.
513
Computational Linguistics Volume 36, Number 3
Figure 7
Transducer encoding simultaneously rule derivations R2R1R3R4 and R1R5R6, and the translation
t5t8. The input sentence is s1s2s3 and the grammar considered here contains the following rules:
R1: S??X,X?, R2: S??S X,S X? , R3: X??s1,t5?, R4: X??s2 s3,t8?, R5: X??s1 X s3,X t8? and R6:
X??s2,t5?.
Conceptually, we would like to create a transducer that represents the mapping
from all possible rule derivations to all possible translations, and then compose this
transducer with an acceptor for the translations of interest. Creating this transducer
which maps derivations to translations is not feasible for large translation grammars,
so we instead keep track of rules as they are used to generate a particular translation
output. We introduce two modifications into lattice construction over the CYK grid
described in Section 2.2:
1. In each cell transducers are constructed which map rule sequences to the
target language translation sequences they produce. In each transducer the
output strings are all possible translations of the source sentence span
covered by that cell; the input strings are all the rule derivations that
generate those translations. The rule derivations are expressed as
sequences of rule indices r given the set of rules R = {Rr}.
2. As these transducers are built they are composed with acceptors for
subsequences of the reference translations so that any translations not
present in the given set of reference translations are removed. In effect, this
replaces the general target language model used in translation with an
unweighted acceptor which accepts only specific sentences.
For alignment, Equations (1) and (2) are redefined as
L(N, x, y, r) = AT(r,)
?
i=1..|?r|
L(N, x, y, r, i) (5)
L(N, x, y, r, i) =
{
AT(,?i) if ?i ? T
L(N?, x?, y?) otherwise (6)
where AT(r, t), Rr ? R, t ? T returns a single-arc transducer which accepts the symbol
r in the input language (rule indices) and the symbol t in the output language (target
words). The weight assigned to each arc is the same in alignment as in translation. With
these definitions the goal lattice L(S, 1, J) is now a transducer with rule indices in the
input symbols and target words in the output symbols. A simple example is given
in Figure 7 where two rule derivations for the translation t5t8 are represented by the
transducer.
As we are only interested in those rule derivations that generate the given target
references, we can discard non-desired translations via standard FST composition of
514
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Figure 8
Construction of a substring acceptor. An acceptor for the strings t1t2t4 and t3t4 (left) and its
substring acceptor (right). In alignment the substring acceptor can be used to filter out undesired
partial translations via standard FST composition operations.
the lattice transducer with the given reference acceptor. In principle, this would be done
in the uppermost cell of the CYK, once the complete source sentence has been covered.
However, keeping track of all possible rule derivations and all possible translations until
the last cell may not be computationally feasible for many sentences. It is more desirable
to carry out this filtering composition in lower-level cells while constructing the lattice
over the CYK grid so as to avoid storing an increasing number of undesired translations
and derivations in the lattice. The lattice in each cell should contain translations formed
only from substrings of the references.
To achieve this we build an unweighted substring acceptor that accepts all sub-
strings of each target reference string. For instance, given the reference string t1t2 . . .
tJ, we build an acceptor for all substrings ti . . . tj, where 1 ? i ? j ? J. This is applied
to lattices in all cells (x, y) that do not span the whole sentence. In the uppermost
cell we compose with the reference acceptor which accepts only complete reference
strings. Given a lattice of target references, the unweighted substring acceptor is
built as:
1. change all non-initial states into final states
2. add one initial state and add  arcs from it to all other states
Figure 8 shows an example of a substring acceptor for the two references t1t2t4 and
t3t4. The substring acceptor also accepts an empty string, accounting for those rules
that delete source words, which in other words translate into NULL. In some instances
the final composition with the reference acceptor might return an empty lattice. If this
happens there is no rule sequence in the grammar that can generate the given source
and target sentences simultaneously.
We have described the use of transducers to encode mappings from rule deriva-
tions to translations. These transducers are somewhat impoverished relative to parse
trees and parse forests, which are more commonly used to encode this relationship. It
is easy to map from a parse tree to one of these transducers but the reverse essentially
requires re-parsing to recreate the tree structure. The structures of the parse trees asso-
ciated with a translation are not needed by many algorithms, however. In particular,
parameter optimization by MERT requires only the rules involved in translation. Our
approach keeps only what is needed by such algorithms. This approach also has prac-
tical advantages such as the ability to align directly to k-best lists or lattices of reference
translations.
515
Computational Linguistics Volume 36, Number 3
Figure 9
One arc from a rule acceptor that assigns a vector of K feature weights to each rule (top) and the
result of composition with the transducer of Figure 7 (after weight-pushing) (bottom). The
components of the final K-dimensional weight vector agree with the feature weights of the rule
sequence, for example ck = c2,k + c1,k + c3,k + c4,k for k = 1 . . .K.
2.3.1 Extracting Feature Values from Alignments. As described by Chiang (2007), scores are
associated with hierarchical translation rules through a factoring into features within a
log-linear model (Och and Ney 2002). We assume that we have a collection of K features
and that the cost cr of each rule Rr is cr =
?K
k=1 ?kc
r,k, where cr,k is the value of the kth
feature value for the rth rule and ?k is the weight assigned to the kth feature for all rules.
For a parse which makes use of the rules Rr1 . . .RrN , its cost
?N
n=1 c
rn can therefore
be written as
?K
k=1 ?k
?N
n=1 c
rn,k. The quantity
?N
n=1 c
rn,k is the contribution by the kth
feature to the overall translation score for that parse. These are the quantities which
need to be extracted from alignment lattices for use in procedures such as minimum
error rate training for estimation of the feature weights ?k.
The procedure described in Section 2.3 produces alignment lattices with scores
consistent with the total parse score. Further steps must be taken to factor this over-
all score to identify the contribution due to individual features or translation rules.
We introduce a rule acceptor which accepts sequences of rule indices, such as the
input sequences of the alignment transducer, and assigns weights in the form of
K-dimensional vectors. Each component of the weight vector corresponds to the feature
value for that rule. Arcs have the form 0
Rr/wr?? 0 where wr = [cr,1, . . . , cr,K]. An example
of composition with this rule acceptor is given in Figure 9 to illustrate how feature scores
are mapped to components of the weight vector. The same operations can be applied to
the (unweighted) alignment transducer on a much larger scale to extract the statistics
needed for minimum error rate training.
We typically apply this procedure in the tropical semiring (Viterbi likelihoods), so
that only the best rule derivation that generated each translation candidate is taken
into account when extracting feature contributions for MERT. However, given the
alignment transducer L, this could also be performed in the log semiring (marginal
likelihoods), taking into account the feature contributions from all rule derivations, for
each translation candidate. This would be adequate if the translation system also car-
ried out decoding in the log semiring, an experiment which is partially explored in
Section 4.4.
We note that the contribution of the language model to the overall translation score
cannot be calculated in this scheme, since the language model score cannot be factored
in terms of rules. To obtain the language model contribution, we simply carry out
WFST composition (Allauzen et al 2007) between an unweighted acceptor of the target
516
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Figure 10
Hierarchical translation grammar example and two parse trees with different levels of rule
nesting for the input sentence s1s2s3s4.
sentences and the n-gram language model used in translation. After determinization,
the cost of each path in the acceptor is then the desired LM feature contribution.
3. Shallow-n Translation Grammars: Translation Search Space Refinements
In this section we describe shallow-n grammars in order to reduce Hiero overgeneration
and adapt the grammar complexity to specific language pairs; the ultimate goal is to de-
fine the most constrained grammar that is capable of generating the desired movement
and translation, so that decoding can be performed without search errors.
Hiero can provide varying degrees of complexity in movement and translation.
Consider the example shown in Figure 10, which shows a hierarchical grammar defined
by six rules. For the input sentence s1s2s3s4, there are two possible parse trees as shown;
the rule derivations for each tree are R1R4R3R5 and R2R1R3R5R6. Along with each tree
is shown the translation generated and the phrase-level alignment. Comparing the two
trees and alignments, the leftmost tree makes use of more reordering when translating
from source to target through the nested application of the hierarchical rules R3 and R4.
For some language pairs this level of reordering may be required in translation, but for
other language pairs it may lead to overgeneration of unwanted hypotheses. Suppose
the grammar in this example is modified as follows:
1. A nonterminal X0 is introduced into hierarchical translation rules
R3:X??X0 s3,t5 X0?
R4:X??X0 s4,t3 X0?
2. Rules for lexical phrases are applied to X0
R5:X0??s1 s2,t1 t2?
R6:X0??s4,t7?
These modifications exclude parses in which hierarchical translation rules generate
other hierarchical rules, except at the 0th level which generate lexical phrases. Con-
sequently the left most tree of Figure 10 cannot be generated and t5t1t2t7 is the only
allowable translation of s1s2s3s4. We call this a ?shallow-1? grammar: The maximum
517
Computational Linguistics Volume 36, Number 3
degree of rule nesting allowed is 1 and only the glue rule can be applied above this
level.
The use of additional nonterminal categories is an elegant way to easily control
important aspects that can have a strong impact on the search space. A shallow-n
translation grammar can be formally defined as:
1. the usual nonterminal S
2. a set of nonterminals {X0, . . . ,XN}
3. two glue rules: S ? ?XN,XN? and S ? ?S XN,S XN?
4. hierarchical translation rules for levels n = 1, . . . ,N:
R: Xn???,?,?? , ?,? ? {{Xn?1} ? T}+
with the requirement that ? and ? contain at least one Xn?1
5. translation rules which generate lexical phrases:
R: X0???,?? , ?,? ? T+
3.1 Avoiding Some Spurious Ambiguity
The added requirement in condition (4) in the definition of shallow-n grammars is
included to avoid some instances in which multiple parses lead to the same translation.
It is not absolutely necessary but it can be added without any loss in representational
capability. To see the effect of this constraint, consider the following example with a
source sentence s1 s2 and a shallow-1 grammar defined by these four rules:
R1: S??X1,X1?
R2: X1??s1 s2,t2 t1?
R3: X1??s1 X0,X0 t1?
R4: X0??s2,t2?
There are two derivations R1R2 and R1R3R4 which yield identical translations. However
R2 would not be allowed under the constraint introduced here because it does not
rewrite an X1 to an X0.
3.2 Structured Long-Distance Movement
The basic formulation of shallow-n grammars allows only the upper-level nonterminal
category S to act within the glue rule. This can prevent some useful long-distance
movement, as might be needed to translate Arabic sentences in Verb-Subject-Object
order into English. It often happens that the initial Arabic verb requires long-distance
movement, but the subject which follows can be translated in monotonic order. For
instance, consider the following Romanized Arabic sentence:
TAlb AlwzrA? AlmjtmEyn Alywm fy dm$q <lY ...
(CALLED) (the ministers) (gathered) (today) (in Damascus) (FOR) ...
where the verb ?TAlb? must be translated into English so that it follows the translations
of the five subsequent Arabic words ?AlwzrA? AlmjtmEyn Alywm fy dm$q?, which
518
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
are themselves translated monotonically. A shallow-1 grammar cannot generate this
movement except in the relatively unlikely case that the five words following the verb
can be translated as a single phrase.
A more powerful approach is to define grammars which allow low-level rules to
form movable groups of phrases. Additional nonterminals {Mk} are introduced to allow
successive generation of k nonterminals XN?1 in monotonic order for both languages,
where K1 ? k ? K2. These act in the same manner as the glue rule does in the uppermost
level. Applying Mk nonterminals at the N?1 level allows one hierarchical rule to perform
a long-distance movement over the tree headed by Mk.
We further refine the definition of shallow-n grammars by specifying the allowable
values of k for the successive productions of nonterminals XN?1. There are many pos-
sible ways to formulate and constrain these grammars. If K2 = 1, then the grammar
is equivalent to the previous definition of shallow-n grammars, because monotonic
production is only allowed by the glue rule of level N. If K1 = 1 and K2 > 1, then the
search space defined by the grammar is greater than the standard shallow-n grammar
as it includes structured long-distance movement. Finally, if K1 > 1 then the search
space is different from standard shallow-n as the n level is only used for long-distance
movement.
Introduction of Mk nonterminals redefines shallow-n grammars as:
1. the usual nonterminal S
2. a set of nonterminals {X0, . . . ,XN}
3. a set of nonterminals {MK1 , . . . ,MK2} for K1 = 1, 2; K1 ? K2
4. two glue rules: S ? ?XN,XN? and S ? ?S XN,S XN?
5. hierarchical translation rules for level N:
R: XN???,?,?? , ?,? ? {{MK1 , . . . ,MK2} ? T}+
with the requirement that ? and ? contain at least one Mk
6. hierarchical translation rules for levels n = 1, . . . ,N ? 1:
R: Xn???,?,?? , ?,? ? {{Xn?1} ? T}+
with the requirement that ? and ? contain at least one Xn?1
7. translation rules which generate lexical phrases:
R: X0???,?? , ?,? ? T+
8. rules which generate k nonterminals XN?1:
if K1 == 2 :
R: Mk??XN?1 Mk?1,XN?1 Mk?1? , for k = 3, . . . ,K2
R: M2??XN?1 XN?1,XN?1 XN?1,I?
if K1 == 1 :
R: Mk??XN?1 Mk?1,XN?1 Mk?1? , for k = 2, . . . ,K2
R: M1??XN?1,XN?1?
where I denotes the identity function that enforces monotonocity in the nonterminals.
For example, with a shallow-1 grammar, M3 leads to the monotonic production of three
nonterminals X0, which leads to the production of three lexical phrase pairs; these can be
moved with a hierarchical rule of level 1. This is graphically represented by the leftmost
tree in Figure 11. With a shallow-2 grammar, M2 leads to the monotonic production of
519
Computational Linguistics Volume 36, Number 3
Figure 11
Movement allowed by two grammars: shallow-1, with K1 = 1, K2 = 3 (left), and shallow-2, with
K1 = 1, K2 = 3 (right). Both grammars allow movement of the bracketed term as a unit.
Shallow-1 requires that translation within the object moved be monotonic whereas shallow-2
allows up to two levels of reordering.
two nonterminals X1, a movement represented by the rightmost tree in Figure 11. This
movement cannot be achieved with a shallow-1 grammar.
3.3 Minimum and Maximum Rule Span
It is useful to define two parameters which further control the application of hierarchical
translation rules in generating the search space. Parameters hmax and hmin specify
the maximum and minimum height at which any hierarchical translation rule can be
applied in the CYK grid. In other words, a hierarchical rule can only be applied in cell
(x, y) if hmin? y ?hmax. Note that these parameters can also be set independently for
each nonterminal category.
3.4 Verb Movement Grammars for Arabic-to-English Translation
Following the discussion which motivated this section, we wish to model movement of
Arabic verbs when translating into English. We add to the shallow-n grammars a verb
restriction so that the hierarchical translation rules (5) apply only if the source language
string ? contains a verb. This encourages translations in which the Arabic verb is moved
at the uppermost level N.
3.5 Grammars Used for SMT Experiments
We now define the hierarchical grammars for the translation experiments which we
describe next.
Shallow-1,2,3 : Shallow-n grammars for n = 1, 2, 3. These grammars do not incorporate
any monotonicity constraints, that is K1 = K2 = 1.
Shallow-1, K1 = 1, K2 = 3 : hierarchical rules with one nonterminal can reorder a
monotonic production of up to three target language phrases of level 0.
520
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Shallow-1, K1 = 1, K2 = 3, vo : hierarchical rules with one nonterminal can reorder a
monotonic catenation of up to three target language phrases of level 0, but only if
one of the source terminals is tagged as a verb.
Shallow-2, K1 = 2,K2 = 3, vo : two levels of reordering with monotonic production
of up to three target language phrases of level 1, but only if one of the source
terminals is tagged as a verb.
4. Translation Experiments
In this section we report on hierarchical phrase-based translation experiments with
WFSTs. We focus mainly on the NIST Arabic-to-English and Chinese-to-English trans-
lation tasks; some results for other language pairs are summarized in Section 4.6.
Translation performance is evaluated using the BLEU score (Papineni et al 2001) as
implemented for the NIST 2009 evaluation.1 The experiments are organized as follows:
- Lattice-based and cube pruning hierarchical decoding (Section 4.2)
- Grammar configurations and search parameters and their effect on
translation performance and processing time (Section 4.3)
- Marginalization over translation derivations (Section 4.4)
- Combining translation lattices obtained from alternative morphological
decompositions of the input (Section 4.5)
4.1 Experimental Framework
For Arabic-to-English translation we use all allowed parallel corpora in the NIST MT08
(and MT09) Arabic Constrained Data track (?150M words per language). In addition to
reporting results on the MT08 set itself, we make use of a development set mt02-05-tune
formed from the odd numbered sentences of the NIST MT02 through MT05 evaluation
sets; the even numbered sentences form a validation set mt02-05-test. The mt02-05-tune
set has 2,075 sentences.
For Chinese-to-English translation we use all available parallel text for the GALE
2008 evaluation;2 this is approximately 250M words per language. We report translation
results on the NIST MT08 set, a development set tune-nw, and a validation set test-nw.
These tuning and test sets contain translations produced by the GALE program and
portions of the newswire sections of MT02 through MT05. The tune-nw set has 1,755
sentences, and test-nw set is similar.
The parallel texts for both language pairs are aligned using MTTK (Deng and Byrne
2008). We extract hierarchical rules from the aligned parallel texts using the constraints
developed by Chiang (2007). We further filter the extracted rules by count and pattern
as described by Iglesias et al(2009a). The following features are extracted from the
parallel data and used to assign scores to translation rules: source-to-target and target-
to-source phrase translation models, word and rule penalties, number of usages of the
glue rule, source-to-target and target-to-source lexical models, and three rule count
features inspired by Bender et al (2007).
1 See ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl
2 See http://projects.ldc.upenn.edu/gale/data/catalog.html
521
Computational Linguistics Volume 36, Number 3
We use two types of language model in translation. In first-pass translation we use
4-gram language models estimated over the English side of the parallel text (for each
language pair) and a 965 million word subset of monolingual data from the English
Gigaword Third Edition (LDC2007T07). These are the language models used if pruning
is needed during search. The main language model is a zero-cutoff stupid-backoff
(Brants et al 2007) 5-gram language model, estimated using 6.6B words of English text
from the English Gigaword corpus. These language models are converted to WFSTs
as needed (Allauzen, Mohri, and Roark 2003); failure transitions are used for correct
application of back-off weights. In tuning the systems, standard MERT (Och 2003)
iterative parameter estimation under IBM BLEU is performed on the development sets.
4.2 Contrast between HiFST and Cube Pruning
We contrast two hierarchical phrase-based decoders. The first decoder, HCP, is a k-best
decoder using cube pruning following the description by Chiang (2007); in our im-
plementation, these k-best lists contain only unique hypotheses (Iglesias et al 2009a),
which are obtained by extracting the 10,000 best candidates from each cell (including
the language model cost), using a priority queue to explore the cross-product of the
k-best lists from the cells pointed by nonterminals. We find that deeper k-best lists
(i.e., k = 100, 000) results in impractical decoding times and that fixed k-best list depths
yields better performance than use of a likelihood threshold parameter. The second
decoder, HiFST, is a lattice-based decoder implemented with WFSTs as described earlier.
Hypotheses are generated after determinization under the tropical semiring so that
scores assigned to hypotheses arise from a single minimum cost/maximum likelihood
derivation.
Translation proceeds as follows. After Hiero translation with optimized feature
weights and the first-pass language model, hypotheses are written to disk. For HCP we
save translations as 10,000-best lists, whereas HiFST generates word lattices. The first-
pass results are then rescored with the main 5-gram language model. In this operation
the first-pass language model scores are removed before the main language model
scores are applied. We then perform MBR rescoring. For the n-best lists we rescore
the top 1,000 hypotheses using the negative sentence-level BLEU score as the loss
function (Kumar and Byrne 2004); we have found that using a deeper k-best list is
impractically slow. For the HiFST lattices we use lattice-based MBR search procedures
described by Tromble et al (2008) in an implementation based on standard WFST
operations (Allauzen et al 2007).
4.2.1 Shallow-1 Arabic-to-English Translation. We translate Arabic-to-English with
shallow-1 hierarchical decoding: as described in Section 3, nonterminals are allowed
only to generate target language phrases. Table 2 shows results for mt02-05-tune, mt02-
05-test, and mt08. In this experiment we use MERT to find optimized parameters for
HCP and we use these parameter values in HiFST as well. This allows for a close
comparison of decoder behavior, independent of parameter optimization.
In these experiments, the first-pass translation quality of the two systems (Table 2
a vs. b) is nearly identical. The most notable difference in the first-pass behavior of the
decoders is their memory use. For example, for an input sentence of 105 words, HCP
uses 1.2Gb memory whereas HiFST takes only 900Mb under the same conditions. To
run HCP successfully requires cube pruning with the first-pass 4-gram language model.
By contrast, HiFST requires no pruning during lattice construction and the first pass
language model is not applied until the lattice is fully built at the upper most cell of the
522
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Table 2
Contrastive Arabic-to-English translation results (lower-cased IBM BLEU) after first-pass
decoding and subsequent rescoring steps. Decoding time reported for mt02-05-tune is in seconds
per word. Both systems are optimized using MERT over the k-best lists generated by HCP.
decoder time mt02-05-tune mt02-05-test mt08
a HCP 1.1 52.5 51.9 42.8
+5g - 53.4 52.9 43.5
+5g+MBR - 53.6 53.0 43.6
b HiFST 0.5 52.5 51.9 42.8
+5g - 53.6 53.2 43.9
+5g+LMBR - 54.3 53.7 44.8
CYK grid. For this grammar, HiFST is able to produce exact translations without any
search errors.
Search Errors Because both decoders are constrained to use exactly the same features,
we can compare their search errors on a sentence-by-sentence basis. A search error is
assigned to one of the decoders if the other has found a hypothesis with lower cost. For
mt02-05-tune, we find that in 18.5% of the sentences HiFST finds a hypothesis with lower
cost than HCP. In contrast, HCP never finds any hypothesis with lower cost for any
sentence. This is as expected: The HiFST decoder requires no pruning prior to applying
the first-pass language model, so search is exact.
Lattice/k-best Quality It is clear from the results that the lattices produced by HiFST
yield better rescoring results than the k-best lists produced by HCP. This is the case for
both 5-gram language model rescoring and MBR. In MT08 rescoring, HCP k-best lists
yield an improvement of 0.8 BLEU relative to the first-pass baseline, whereas rescoring
HiFST lattices yield an improvement of 2.0 BLEU. The advantage of maintaining a large
search space in lattice form during decoding is clear. The use of k-best lists in HCP limits
the gains from subsequent rescoring procedures.
Translation Speed HCP requires an average of 1.1 seconds per input word. HiFST
cuts this time by half, producing output at a rate of 0.5 seconds per word. It proves
much more efficient to process compact lattices containing many hypotheses rather than
independently processing each distinct hypothesis in k-best form.
4.2.2 Fully Hierarchical Chinese-to-English Translation. We translate Chinese-to-English
with full hierarchical decoding: nonterminals are allowed to generate other hierarchical
rules in recursion.
We apply the constraint hmax=10 for nonterminal category X, as described in Sec-
tion 2.2.2, so that only glue rules are allowed at upper levels of the CYK grid; this is
applied in both HCP and HiFST.
In HiFST any lattice in the CYK grid is pruned if it covers at least three source words
and contains more than 10,000 states. The log-likelihood pruning threshold relative to
the best path in the sublattices is 9.0.
Improved Optimization and Generalization Table 3 shows results for tune-nw, test-nw,
and mt08. The first two rows show results for HCP when using MERT parameters
optimized over k-best lists produced by HCP (row a) and by HiFST (row b); in the latter
case, we are tuning HCP parameters over the hypothesis list generated by HiFST. When
measured over test-nw this gives a 0.3 BLEU improvement. HCP benefits from tuning
523
Computational Linguistics Volume 36, Number 3
Table 3
Contrastive Chinese-to-English translation results (lower-cased IBM BLEU) after first-pass
decoding and subsequent rescoring steps. The MERT k-best column indicates which decoder
generated the k-best lists used in MERT optimization. The mt08 set contains 691 sentences of
newswire and 666 sentences of Web text.
decoder MERT k-best tune-nw test-nw mt08
a HCP HCP 32.8 33.1 ?
b HCP 32.9 33.4 28.2
+5g HiFST 33.4 33.8 28.7
+5g+MBR 33.6 34.0 28.9
c HiFST 33.1 33.4 28.1
+5g HiFST 33.8 34.3 29.0
+5g+LMBR 34.5 34.9 30.2
over the HiFST hypotheses and we conclude that using the k-best list obtained by the
HiFST decoder yields better parameters in optimization.
Search Errors Measured over the tune-nw development set, HiFST finds a hypothesis
with lower cost in 48.4% of the sentences. In contrast, HCP never finds any hypothesis
with a lower cost for any sentence, indicating that the described pruning strategy for
HiFST is much broader than that of HCP. Note that HCP search errors are more frequent
for this language pair. This is due to the larger search space required for full hierarchical
translation; the larger the search space, the more likely it is that search errors will be
introduced by the cube pruning algorithm.
Lattice/k-best Quality Large LMs and MBR both benefit from the richer space of
translation hypotheses contained in the lattices. Relative to the first-pass baseline in
MT08, rescoring HiFST lattices yields a gain of 2.1 BLEU, compared to a gain of 0.7
BLEU with HCP k-best lists.
4.2.3 Reliability of n-gram Posterior Distributions. MBR decoding under linear BLEU
(Tromble et al 2008) is driven mainly by the presence of high posterior n-grams in
the lattice; the low posterior n-grams have poor discriminatory power. In the following
experiment, we show that high posterior n-grams are more likely to be found in the
references, and that using the full evidence space of the lattice is much better than even
very large k-best lists for computing posterior probabilities. Let Ni = {w1, . . .,w|Ni|}
denote the set of n-grams of order i in the first-pass translation 1-best, and let Ri =
{w1, . . .,w|Ri|} denote the set of n-grams of order i in the union of the references.
For confidence threshold ?, let Ni,? = {w ? Ni : p(w|L) ? ?} denote the set of all
n-grams in Ni with posterior probability greater than or equal to ?, where p(w|L) is
the posterior probability of the n-gram w, that is, the sum of the posterior probabilities
of all translations containing at least one occurrence of w. The precision at order i for
threshold ? is the proportion of n-grams in Ni,? found in the references:
pi,? =
|Ri ?Ni,?|
|Ni,?|
. (7)
The average per-sentence 4-gram precision at a range of posterior probability thresholds
? is shown in Figure 12. The posterior probabilities are computed using either the full
524
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Figure 12
4-gram precisions for Arabic-to-English mt02-05-tune first-pass 1-best translations computed
using the full evidence space of the lattice and k-best lists of various sizes.
lattice L or a k-best list of the specified size. The 4-gram precision of the 1-best trans-
lations is approximately 0.35. At higher values of ?, the reference precision increases
considerably. Expanding the k-best list size from 1,000 to 10,000 hypotheses only slightly
improves the precision but much higher precisions are observed when the full evidence
space of the lattice is used. The improved precision results from more accurate estimates
of n-gram posterior probabilites and emphasizes once more the advantage of lattice-
based decoding and rescoring techniques.
4.3 Grammar Configurations and Search Parameters
We report translation performance and decoding speed as we vary hierarchical gram-
mar depth and the constraints on low-level rule concatenation (see Section 3). Unless
otherwise noted, hmin = 1 and hmax = 10 throughout (except for the ?S? nonterminal
category, where these constraints are not relevant).
4.3.1 Grammars for Arabic-to-English Translation. Table 4 reports Arabic-to-English trans-
lation results using the alternative grammar configurations described in Section 3.5.
Results are shown in first-pass decoding (HiFST rows), and in rescoring with a larger
5-gram language model for the most promising configurations (+5g rows). Decoding
time is reported for first-pass decoding only; rescoring time is negligible by comparison.
As shown in the upper part of Table 4, translation under a shallow-2 grammar does
not improve relative to a shallow-1 grammar, although decoding is much slower. This
indicates that the additional hypotheses generated when allowing a hierarchical depth
of two are not useful in Arabic-to-English translation. By contrast the shallow gram-
mars that allow long-distance movement for verbs only (shallow-1+K1,K2 = 1, 3, vo and
shallow-2+K1,K2 = 2, 3, vo), perform slightly better than shallow-1 grammar at a similar
decoding time. Performance differences increase when the larger 5-gram is applied
525
Computational Linguistics Volume 36, Number 3
Table 4
Contrastive Arabic-to-English translation results (lower-cased IBM BLEU) with various
grammar configurations. Decoding time reported in seconds per word for mt02-05-tune.
grammar time mt02-05-tune mt02-05-test mt08
HiFST shallow-1 0.8 52.7 52.0 42.9
+K1,K2 = 1, 3 1.3 52.6 51.9 42.8
+K1,K2 = 1, 3, vo 0.9 52.7 52.1 42.9
shallow-2 4.2 52.7 51.9 42.6
+K1,K2 = 2, 3, vo 1.8 52.8 52.2 43.0
+5g shallow-1 - 53.9 53.4 44.9
+K1,K2 = 1, 3, vo - 54.1 53.6 45.0
shallow-2
+K1,K2 = 2, 3, vo
- 54.2 53.8 45.0
(Table 4, bottom). This is expected given that these grammars add valid translation
candidates to the search space with similar costs; a language model is needed to select
the good hypotheses among all those introduced.
4.3.2 Grammars for Chinese-to-English Translation. Table 5 shows contrastive results in
Chinese-to-English translation for full hierarchical and shallow-n (n = 1, 2, 3) gram-
mars.3 Unlike Arabic-to-English translation, Chinese-to-English translation improves
as the hierarchical depth of the grammar is increased (i.e., for larger n). Decoding time
also increases significantly. The shallow-1 grammar constraints which worked well for
Arabic-to-English translation are clearly inadequate for this task; performance degrades
by approximately 1.0 BLEU relative to the full hierarchical grammar.
However, we find that translation under the shallow-3 grammar yields performance
nearly as good as that of the full hiero grammars; translation times are shorter and
yield degradations of only 0.1 to 0.3 BLEU. Translation can be made significantly faster
by constraining the shallow-3 search space with hmin = 9, 5, 2 for X2,X1, and X0, respec-
tively; translation speed is reduced from 10.8 sec/word to 3.8 sec/word at a degradation
of 0.2 to 0.3 BLEU relative to full Hiero.
Shallow-3 grammars describe a restricted search-space but appear to have expressive
power in Chinese-to-English translation which is very similar to that of a full Hiero
grammar. Each cell (x, y) is represented by a bigger set of nonterminals; this allows for
more effective pruning strategies during lattice construction. We note also that hmax
values greater than 10 yield little improvement. As shown in the five bottom rows of
Table 5, differences between grammar configurations tend to carry through after 5-gram
rescoring. In summary, a shallow-3 grammar and filtering with hmin = 9, 5, 2 lead to a
0.4 degradation in BLEU relative to full Hiero. As a final contrast, the mixed-case NIST
BLEU-4 for the HiFST system on mt08 is 28.6. This result is obtained under the same
evaluation conditions as the official NIST MT08 Constrained Training Track.4
3 We note that the scores in full hiero row do not match those of row c in Table 3 which were obtained with
a slightly simplified version of HiFST and optimized according to the 2008 NIST implementation of IBM
BLEU; here we use the 2009 implementation by NIST.
4 Full MT08 results are available at
www.nist.gov/speech/tests/mt/2008/doc/mt08 official results v0.html
526
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Table 5
Contrastive Chinese-to-English translation results (lower-cased IBM BLEU ) with various
grammar configurations and search parameters. Decoding time is reported in sec/word for
tune-nw.
grammar time tune-nw test-nw mt08 (nw)
HiFST shallow-1 0.7 33.6 33.4 32.6
shallow-2 5.9 33.8 34.2 32.7
+hmin=5 5.6 33.8 34.1 32.9
+hmin=7 4.0 33.8 34.3 33.0
shallow-3 8.8 34.0 34.3 33.0
+hmin=7 7.7 34.0 34.4 33.1
+hmin=9 5.9 33.9 34.3 33.1
+hmin=9,5,2 3.8 34.0 34.3 33.0
+hmin=9,5,2+hmax=11 6.1 33.8 34.4 33.0
+hmin=9,5,2+hmax=13 9.8 34.0 34.4 33.1
full hiero 10.8 34.0 34.4 33.3
+5g shallow-1 - 34.1 34.5 33.4
shallow-2 - 34.3 35.1 34.0
shallow-3 - 34.6 35.2 34.4
+hmin=9,5,2 - 34.5 34.8 34.2
full hiero - 34.5 35.2 34.6
4.4 Marginalization Over Translation Derivations
As has been discussed earlier, the translation model in hierarchical phrase-based ma-
chine translation allows for multiple derivations of a target language sentence. Each
derivation corresponds to a particular combination of hierarchical rules and it has been
argued that the correct approach to translation is to accumulate translation probability
by summing over the scores of all derivations (Blunsom, Cohn, and Osborne 2008).
Computing this sum for each of the many translation candidates explored during de-
coding is computationally difficult, however. For this reason the translation probability
is commonly computed using the Viterbi max-derivation approximation. This is the
approach taken in the previous sections in which translations scores were accumulated
under the tropical semiring.
The use of WFSTs allows the sum over alternative derivations of a target string
to be computed efficiently. HiFST generates a translation lattice realized as a weighted
transducer with output labels encoding words and input labels encoding the sequence
of rules corresponding to a particular derivation, and the cost of each path in the lattice
is the negative log probability of the derivation that generated the hypothesis.
Determinization applies the ? operator to all paths with the same word se-
quence (Mohri 1997). When applied in the log semiring, this operator computes the
sum of two paths with the same word sequence as x ? y = ?log(e?x + e?y) so that the
probabilities of alternative derivations can be summed.
Currently this operation is only performed in the top cell of the hierarchical decoder
so it is still an approximation to the true translation probability. Computing the true
translation probability would require the same operation to be repeated in every cell
during decoding, which is very time consuming. Note that the translation lattice was
generated with a language model and so the language model costs must be removed
527
Computational Linguistics Volume 36, Number 3
Table 6
Arabic-to-English results (lower-cased IBM BLEU) when determinizing the lattice at the
upper-most CYK cell with alternative semirings.
semiring mt02-05-tune mt02-05-test mt08
tropical HiFST 52.8 52.2 43.0
+5g 54.2 53.8 44.9
+5g+LMBR 55.0 54.6 45.5
log HiFST 53.1 52.6 43.2
+5g 54.6 54.2 45.2
+5g+LMBR 55.0 54.6 45.5
before determinization to ensure that only the derivation probabilities are included
in the sum. After determinization, the language model is reapplied and the 1-best
translation hypothesis can be extracted from the logarc determinized lattices.
Table 6 compares translation results obtained using the tropical semiring (Viterbi
likelihoods) and the log semiring (marginal likelihoods). First-pass translation shows
small gains in all sets: +0.3 and +0.4 BLEU for mt02-05-tune and mt02-05-test, and +0.2
for mt08. These gains show that the sum over alternative derivations can be easily
obtained in HiFST simply by changing semiring and that these alternative derivations
are beneficial to translation. The gains carry through to the large language model 5-gram
rescoring stage but after LMBR the final BLEU scores are unchanged. The hypotheses
selected by LMBR are in almost all cases exactly the same regardless of the choice of
semiring. This may be due to the fact that our current marginalization procedure is only
an approximation to the true marginal likelihoods, since the log semiring determiniza-
tion operation is applied only in the uppermost cell of the CYK grid and MERT training
is performed using regular Viterbi likelihoods.
We note that a close study of the interaction between LMBR and marginalization
over derivations is beyond the scope of this paper. Our purpose here is to show how
easily these operations can be done using WFSTs.
4.5 Combining Lattices Obtained from Alternative Morphological Decompositions
It has been shown that MBR decoding is a very effective way of combining translation
hypotheses obtained from alternative morphological decompositions of the same source
data. In particular, de Gispert et al (2009) show gains for Arabic-to-English and Finnish-
to-English when taking k-best lists obtained from two morphological decompositions
of the source language. Here we extend this approach to the case of translation lat-
tices and experiment with more than two alternative decompositions. We will show
that working with translation lattices gives significant improvements relative to k-best
lists.
In lattice-based MBR system combination, first-pass decoding results in a set of I
distinct translation lattices L(i), i = 1. . .I for each foreign sentence, with each lattice
produced by translating one of the alternative morphological decompositions. The
evidence space for MBR decoding is formed as the union of these lattices L =
?I
i=1 L(i).
The posterior probability of n-gram w in the union of lattices is computed as a simple
528
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Table 7
Arabic-to-English results (lower-cased IBM BLEU) when using alternative Arabic
decompositions, and their combination with k-best-based and lattice-based MBR.
configuration mt02-05-tune mt02-05-test mt08
a HiFST+5g 54.2 53.8 44.9
b HiFST+5g 53.8 53.6 45.0
c HiFST+5g 54.1 53.8 44.7
a+b +MBR 55.1 54.7 46.1
+LMBR 55.7 55.4 46.7
a+c +MBR 55.4 54.9 46.5
+LMBR 56.0 55.9 46.9
a+b+c +MBR 55.3 54.9 46.5
+LMBR 56.0 55.7 47.3
linear interpolation of the posterior probabilities according to the evidence space of each
individual lattice so that
p(w|L) =
I
?
i=1
?i pi(w|L(i) ), (8)
where the interpolation parameters 0 ? ?i ? 1 such that
?I
i=1 ?i = 1 specify the weight
associated with each system in the combination and are optimized with respect to
the tuning set. The system-specific posteriors required for the interpolation are com-
puted as
pi(w|L(i) ) =
?
E?L(i)w
Pi(E|F), (9)
where Pi(E|F) is the posterior probability of translation E given source sentence F and
the sum is taken over the subset L(i)w = {E ? L(i) : #w(E) > 0} of the lattice L(i) containing
paths with at least one occurrence of the n-gram w. These posterior probabilities are
used in MBR decoding under the linear approximation to the BLEU score described
in Tromble et al (2008). We find that for system combination, decoding often produces
output that is slightly shorter than required. A fixed per-word factor optimized on the
tuning set is applied when computing the gain and this results in output with improved
BLEU score and reduced brevity penalty.
Table 7 shows translation results in Arabic-to-English using three alternative mor-
phological decompositions of the Arabic text (upper rows a, b, and c). For each decom-
position an independent set of hierarchical rules is obtained from the respective parallel
corpus alignments. The decompositions were generated by the MADA toolkit (Habash
and Rambow 2005) with two alternative tokenization schemes, and by the Sakhr Arabic
Morphological Tagger, developed by Sakhr Software in Egypt.
The following rows show the results when combining with MBR the translation
hypotheses obtained from two or three decompositions. The table also shows a contrast
529
Computational Linguistics Volume 36, Number 3
Figure 13
Average per-sentence 4-gram reference precisions for Arabic-to-English mt02-05-tune
single-system MBR 1-best translations and the 1-best obtained through MBR system
combination.
between decoding the joint k-best lists (rows named MBR, with k = 1, 000) and decod-
ing the unioned translation lattices (rows named LMBR). In line with the findings of
de Gispert et al (2009), we find significant gains from combining k-best lists with respect
to using any one segmentation alone. Interestingly, here we find further gains when
applying lattice-based MBR instead of a k-best approach, obtaining consistent gains of
0.6?0.8 BLEU across all sets.
The results reported in Table 7 are very competitive. The mixed-case NIST BLEU-4
score for a+b+c LMBR system in MT08 is 44.9. This result is obtained under the same
evaluation conditions as the official NIST MT08 Constrained Training Track.5 For MT09,
the mixed-case BLEU-4 is 48.3, which ranks first in the Arabic-to-English NIST 2009
Constrained Data Track.6
4.5.1 System Combination and Reference Precision. We have demonstrated that MBR de-
coding of multiple lattices generated from alternative morphological segmentations
leads to significant improvements in BLEU score. We now show that one reason for
the improved performance is that lattice combination leads to better n-gram posterior
probability estimates. To combine two equally weighted lattices L(1) and L(2), the in-
terpolation weights are ?1 = ?2 = 12 ; Equation (8) simplifies as p(w|L) = 12 (p1(w|L(1) )+
p2(w|L(2))). Figure 13 plots average per-sentence reference precisions for the 4-grams in
the MBR 1-best of systems a and b and their combination (labeled a+b) at a range of
posterior probability thresholds 0 ? ? ? 1. Systems a and b have similar precisions at
all values of ?, confirming that the optimal interpolation weights for this combination
should be equal. The precision obtained using n-gram posterior probabilities computed
5 Full MT08 results are available at
www.nist.gov/speech/tests/mt/2008/doc/mt08 official results v0.html
6 Full MT09 results are available at www.itl.nist.gov/iad/mig/tests/mt/2009/ResultsRelease
530
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
from the combined lattices is higher than that of the individual systems. A higher
proportion of the n-grams assigned high posterior probability under the interpolated
distribution are found in the references and this is one of the reasons for the large gains
in BLEU in lattice-based MBR system combination.
4.6 European Language Translation
The HiFST described here has also been found to achieve competitive performance for
other language pairs, such as Spanish-to-English and Finnish-to-English.
For Spanish-to-English we carried out experiments on the shared task of the ACL
2008 Workshop on Statistical Machine Translation (Callison-Burch et al 2008) based on
the Europarl corpus. For the official test2008 evaluation set we obtain a BLEU score of
34.2 using a shallow-1 grammar. Similarly to the Arabic case, deeper grammars are not
found to improve scores for this task.
In Finnish-to-English, we conducted translation experiments based on the Europarl
corpus using 3,000 sentences from the Q4/2000 period for testing with a single ref-
erence. In this case, the shallow-1 grammar obtained a BLEU score of 28.0, whereas
the full hierarchical grammar only achieved 27.6. This is further evidence that full-
hierarchical grammars are not appropriate in all instances. In this case we suspect that
the use of Finnish words without morphological decomposition leads to data sparsity
problems and complicates the task of learning complex translation rules. The lack of
a large English language model suitable for this domain may also make it harder to
select the right hypothesis when the translation grammar produces many more English
alternatives.
5. Conclusions
We have described two linked investigations into hierarchical phrase-based translation.
We investigate the use of weighted finite state transducers rather than k-best lists to
represent the space of translation hypotheses. We describe a lattice-based Hiero de-
coder, with which we find reductions in search errors, better parameter optimization,
and improved translation performance. Relative to these reductions in search errors,
direct generation of target language translation lattices also leads to further translation
improvements through subsequent rescoring steps, such as MBR decoding and the
application of large n-gram language models. These steps can be carried out easily via
standard WFST operations.
As part of the machinery needed for our experiments we develop WFST procedures
for alignment and feature extraction so that statistics needed for system optimization
can be easily obtained and represented as transducers. In particular, we make use of
a lattice-based representation of sequences of rule applications, which proves useful
for minimum error rate training. In all instances we find that using lattices as compact
representations of translation hypotheses offers clear modeling advantages.
We also investigate refinements in translation search space through shallow-n gram-
mars, structured long-distance movement, and constrained word deletion. We find
that these techniques can be used to fit the complexity of Hiero translation systems to
individual language pairs. In translation from Arabic into English, shallow grammars
make it possible to explore the entire search space and to do so more quickly but with the
same translation quality as the full Hiero grammar. Even in complex translation tasks,
such as Chinese to English, we find significant speed improvements with minimal loss
531
Computational Linguistics Volume 36, Number 3
in performance using these methods. We take the view that it is better to perform exact
search of a constrained space than to risk search errors in translation.
We note finally that Chiang introduced Hiero as a model ?based on a synchronous
CFG, elsewhere known as a syntax-directed transduction grammar (Lewis and Stearns
1968).? We have taken this formulation as a starting point for the development of
novel realizations of Hiero. Our motivation has mainly been practical in that we seek
improved translation quality and efficiency through better models and algorithms.
Our approach suggests close links between Hiero and Recursive Transition Net-
works (Woods 1970; Mohri 1997). Although this connection is beyond the scope of this
paper, we do note that Hiero translation requires keeping track of two grammars, one
based on the Hiero translation rules and the other based on n-gram language model
probabilities. These two grammars have very different dependencies which suggests
that a full implementation of Hiero translation such as we have addressed does not
have a simple expression as an RTN.
Acknowledgments
This work was supported in part by the
GALE program of the Defense Advanced
Research Projects Agency, Contract No.
HR0011- 06-C-0022, and in part by the
Spanish government and the ERDF under
projects TEC2006-13694-C03-03 and
TEC2009-14094-C04-04.
References
Allauzen, Cyril, Mehryar Mohri, and Brian
Roark. 2003. Generalized algorithms for
constructing statistical language models.
In Proceedings of ACL, pages 557?564,
Sapporo.
Allauzen, Cyril, Michael Riley, Johan
Schalkwyk, Wojciech Skut, and Mehryar
Mohri. 2007. OpenFst: A general and
efficient weighted finite-state transducer
library. In Proceedings of CIAA,
pages 11?23, Prague.
Bender, Oliver, Evgeny Matusov, Stefan
Hahn, Sasa Hasan, Shahram Khadivi,
and Hermann Ney. 2007. The RWTH
Arabic-to-English spoken language
translation system. In Proceedings of
ASRU, pages 396?401, Kyoto.
Blunsom, Phil, Trevor Cohn, and Miles
Osborne. 2008. A discriminative latent
variable model for statistical machine
translation. In Proceedings of ACL-HLT,
pages 200?208, Columbus, OH.
Brants, Thorsten, Ashok C. Popat, Peng Xu,
Franz J. Och, and Jeffrey Dean. 2007.
Large language models in machine
translation. In Proceedings of EMNLP-ACL,
pages 858?867, Prague.
Callison-Burch, Chris, Cameron Fordyce,
Philipp Koehn, Christof Monz, and Josh
Schroeder. 2008. Further meta-evaluation
of machine translation. In Proceedings
of the ACL Workshop on Statistical
Machine Translation, pages 70?106,
Columbus, OH.
Chappelier, Jean-Ce?dric and Martin Rajman.
1998. A generalized CYK algorithm for
parsing stochastic CFG. In Proceedings of
TAPD, pages 133?137, Paris.
Chiang, David. 2005. A hierarchical
phrase-based model for statistical machine
translation. In Proceedings of ACL,
pages 263?270, Ann Arbor, MI.
Chiang, David. 2007. Hierarchical
phrase-based translation. Computational
Linguistics, 33(2):201?228.
de Gispert, Adria`, Sami Virpioja, Mikko
Kurimo, and William Byrne. 2009.
Minimum Bayes-Risk combination of
translation hypotheses from alternative
morphological decompositions. In
Proceedings of HLT/NAACL, Companion
Volume: Short Papers, pages 73?76,
Boulder, CO.
Deng, Yonggang and William Byrne. 2008.
HMM word and phrase alignment for
statistical machine translation. IEEE
Transactions on Audio, Speech, and Language
Processing, 16(3):494?507.
Graehl, Jonathan, Kevin Knight, and
Jonathan May. 2008. Training tree
transducers. Computational Linguistics,
34(3):391?427.
Habash, Nizar and Owen Rambow. 2005.
Arabic tokenization, part-of-speech
tagging and morphological
disambiguation in one fell swoop. In
Proceedings of the ACL, pages 573?580,
Ann Arbor, MI.
Iglesias, Gonzalo, Adria` de Gispert,
Eduardo R. Banga, and William Byrne.
2009a. Rule filtering by pattern for
532
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
efficient hierarchical translation. In
Proceedings of the EACL, pages 380?388,
Athens.
Iglesias, Gonzalo, Adria` de Gispert, Eduardo
R. Banga, and William Byrne. 2009b.
Hierarchical phrase-based translation with
weighted finite state transducers. In
Proceedings of HLT/NAACL, pages 433?441,
Boulder, CO.
Iglesias, Gonzalo, Adria` de Gispert, Eduardo
R. Banga, and William Byrne. 2009c. The
HiFST system for the Europarl
Spanish-to-English task. In Proceedings of
SEPLN, pages 207?214, Donosti.
Knight, Kevin and Yaser Al-Onaizan. 1998.
Translation with finite-state devices. In
Proceedings of the Third Conference of the
AMTA on Machine Translation and the
Information Soup, pages 421?437,
Langhorne, PA.
Koehn, Philipp, Franz Josef Och, and Daniel
Marcu. 2003. Statistical phrase-based
translation. In Proceedings of HLT-NAACL,
pages 127?133, Edmonton.
Kumar, Shankar and William Byrne. 2004.
Minimum Bayes-risk decoding for
statistical machine translation. In
Proceedings of HLT-NAACL, pages 169?176,
Boston, MA.
Kumar, Shankar, Yonggang Deng, and
William Byrne. 2006. A weighted finite
state transducer translation template
model for statistical machine translation.
Natural Language Engineering, 12(1):35?75.
Lewis, P. M., II, and R. E. Stearns. 1968.
Syntax-directed transduction. Journal of the
ACM, 15(3):465?488.
Mohri, Mehryar. 1997. Finite-state
transducers in language and speech
processing. Computational Linguistics,
23:269?311.
Mohri, Mehryar, Fernando Pereira, and
Michael Riley. 2000. The design principles
of a weighted finite-state transducer
library. Theoretical Computer Science,
231:17?32.
Mohri, Mehryar, Fernando Pereira, and
Michael Riley. 2002. Weighted finite-state
transducers in speech recognition.
Computer Speech and Language, 16:69?88.
Och, Franz J. 2003. Minimum error rate
training in statistical machine translation.
In Proceedings of ACL, pages 160?167,
Sapporo.
Och, Franz Josef and Hermann Ney. 2002.
Discriminative training and maximum
entropy models for statistical machine
translation. In Proceedings of the ACL,
pages 295?302, Philadelphia, PA.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2001. BLEU: A
method for automatic evaluation of
machine translation. In Proceedings of ACL,
pages 311?318, Toulouse.
Rosti, Antti-Veikko, Necip Fazil Ayan, Bing
Xiang, Spyros Matsoukas, Richard
Schwartz, and Bonnie Dorr. 2007.
Combining outputs from multiple
machine translation systems. In
Proceedings of HLT-NAACL, pages 228?235,
Rochester, NY.
Setiawan, Hendra, Min Yen Kan, Haizhou Li,
and Philip Resnik. 2009. Topological
ordering of function words in hierarchical
phrase-based translation. In Proceedings
of the ACL-IJCNLP, pages 324?332,
Singapore.
Sim, Khe Chai, William Byrne, Mark Gales,
Hichem Sahbi, and Phil Woodland. 2007.
Consensus network decoding for statistical
machine translation system combination.
In Proceedings of ICASSP, pages 105?108,
Honolulu, HI.
Tromble, Roy, Shankar Kumar, Franz J. Och,
and Wolfgang Macherey. 2008. Lattice
Minimum Bayes-Risk decoding for
statistical machine translation. In
Proceedings of EMNLP, pages 620?629,
Honolulu, HI.
Varile, Giovanni B. and Peter Lau. 1988.
Eurotra practical experience with a
multilingual machine translation system
under development. In Proceedings of the
Second Conference on Applied Natural
Language Processing, pages 160?167,
Austin, TX.
Woods, W. A. 1970. Transition network
grammars for natural language analysis.
Communications of the ACM,
13(10):591?606.
Wu, Dekai. 1997. Stochastic inversion
transduction grammars and bilingual
parsing of parallel corpora. Computational
Linguistics, 23(3):377?403.
533

