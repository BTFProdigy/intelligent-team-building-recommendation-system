Coling 2008: Companion volume ? Posters and Demonstrations, pages 87?90
Manchester, August 2008
Easily Identifiable Discourse Relations
Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani Nenkova, Alan Lee, Aravind Joshi
University of Pennsylvania
3330 Walnut Street
Philadelphia, PA 19104
Abstract
We present a corpus study of local dis-
course relations based on the Penn Dis-
course Tree Bank, a large manually anno-
tated corpus of explicitly or implicitly re-
alized relations. We show that while there
is a large degree of ambiguity in temporal
explicit discourse connectives, overall con-
nectives are mostly unambiguous and al-
low high-accuracy prediction of discourse
relation type. We achieve 93.09% accu-
racy in classifying the explicit relations
and 74.74% accuracy overall. In addition,
we show that some pairs of relations oc-
cur together in text more often than ex-
pected by chance. This finding suggests
that global sequence classification of the
relations in text can lead to better results,
especially for implicit relations.
1 Introduction
Discourse relations between textual units are con-
sidered key for the ability to properly interpret
or produce discourse. Various theories of dis-
course have been developed (Moore and Wiemer-
Hastings, 2003) and different relation taxonomies
have been proposed (Hobbs, 1979; McKeown,
1985; Mann and Thompson, 1988; Knott and
Sanders, 1998). Among the most cognitively
salient relations are causal (contingency), contrast
(comparison), and temporal.
Very often, the discourse relations are explicit,
signaled directly by the use of appropriate dis-
course connectives:
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
(E1) He is very tired because he played tennis all morning.
(E2) He is not very strong, but he can run amazingly fast.
(E3) We had some tea in the afternoon and later went to a
restaurant for a big dinner.
Discourse relations can also be implicit, inferred
by the context of the utterance and general world
knowledge.
(I1) I took my umbrella this morning. [because] The forecast
was rain in the afternoon.
(I2) She is never late for meetings. [but] He always arrives
10 minutes late.
(I3) She woke up early. [afterward] She had breakfast and
went for a walk in the park.
An additional complication for automatic clas-
sification of discourse relations is that even in the
presence of an explicit discourse connective, the
connective might be ambiguous between several
senses. For example, since can be used to signal
either a temporal or a contingency relation.
They have not spoken to each other since they argued last
fall. (Temporal)
I assumed you were not coming since you never replied to
the invitation. (Causal)
Several questions directly related to efforts in
automatic recognition of discourse relations arise:
In a general text, what is the proportion of ex-
plicit versus implicit relations? Since implicit rela-
tions are presumably harder to recognize automati-
cally, the larger their proportion, the more difficult
the overall prediction of discourse relation will be.
How ambiguous are discourse connectives?
The degree of ambiguity would give an upper
bound on the accuracy with which explicit rela-
tions can be identified. The more ambiguous dis-
course connectives are, the more difficult it would
be to automatically decide which discourse rela-
tion is expressed in a given sentence, even in the
presence of a connective.
87
In a text, are adjacent discourse relations inde-
pendent of each other or are certain sequences of
relations more likely? In the latter case, a ?dis-
course grammar? of text can be used and easy to
identify relations such as unambiguous explicit re-
lations can help determine the class of implicit re-
lations that immediately follow or precede them.
In this study, we address the above questions us-
ing the largest existing corpus manually annotated
with discourse relations?the Penn Discourse Tree
Bank (Prasad et al, 2008).
2 The Penn Discourse Tree Bank
The Penn Discourse Treebank (PDTB) is a new re-
source (Prasad et al, 2008) of annotated discourse
relations. The annotation covers the same 1 mil-
lion word Wall Street Journal (WSJ) corpus used
for the Penn Treebank (Marcus et al, 1994).
he PDTB is the first corpus to systematically
identify and distinguish explicit and implicit dis-
course relations. By definition, an explicit relation
is triggered by the presence of a discourse con-
nective which occurs overtly in the text. The dis-
course connective can essentially be viewed as a
discourse-level predicate which takes two clausal
arguments. For example, sentence E1 above could
be represented as BECAUSE(?He is very tired?,
?he played tennis all morning?). The corpus rec-
ognizes 100 such explicit connectives and contains
annotations for 19,458 explicit relations
1
.
The PDTB also contains provisions for the an-
notation of implicit discourse relations between
adjacent sentences which are inferred by the reader
but are not overtly marked by a discourse connec-
tive. In this case, the annotator was asked to pro-
vide a connective that best captured the inferred
relation. There are a total of 16,584 implicit rela-
tions annotated in the corpus.
2
In addition to discourse relations and their ar-
guments, the PDTB also provides the senses of
each relation(Miltsakaki et al, 2008). The tagset
of senses is organized hierarchically into three lev-
els - class, type, and subtype. The top class level
contains the four major semantic classes: Expan-
sion, Comparison, Contingency and Temporal.
1
The PDTB allows annotators to tag a relation with multi-
ple senses. In this work we count both of the annotated senses.
So even though there are only 18,459 explicit relations, there
are 19,458 explicit senses.
2
Again, because of multiple senses per relation, the 16,584
senses are part of 16,224 relations.
Class Explicit (%) Implicit (%) Total
Comparison 5590 (69.05%) 2505 (30.95%) 8095
Contingency 3741 (46.75%) 4261 (53.25%) 8002
Temporal 3696 (79.55%) 950 (20.45%) 4646
Expansion 6431 (42.04%) 8868 (57.96%) 15299
Table 1: Discourse relation distribution in seman-
tic and explicit/implicit classes in the PDTB
3 Distribution and ambiguity of
connectives
Table 1 shows the distribution of discourse rela-
tions between the four main relation classes and
their type of realization (implicit or explicit). In-
terestingly, temporal and comparison relations are
predominantly explicit. About 80% and 70%, re-
spectively, of their occurrences are marked by a
discourse connective. The contingency relations
are almost evenly distributed between explicit and
implicit. The expansion relations, the overall
largest class of discourse relations, is in most cases
implicit and not marked by a discourse connective.
Given the figures in Table 1, we would expect
that overall temporal and comparison relations will
be more easily identified since they are overtly
marked. Of course this would only be the case if
discourse markers are mostly unambiguous.
Here we show all connectives that appear more
than 50 times in the PDTB, their predominant
sense (comparison, contingency, temporal or ex-
pansion), as well as the percentage of occurrences
of the connective in its predominant sense. For
example the connective but has comparison as its
predominant sense and 97.19% of the 3,308 occur-
rences of this connective were comparisons.
Comparison but (3308; 97.19%), while (781; 66.07%),
however (485; 99.59%), although (328; 99.70%),
though (320; 100.00%), still (190; 98.42%), yet (101;
97.03%)
Expansion and (3000; 96.83%), also (1746; 99.94%), for
example (196; 100.00%), in addition (165; 100.00%),
instead (112; 97.32%), indeed (104; 95.19%), more-
over (101; 100.00%), for instance (98, 100.00%), or
(98; 96.94%), unless (95; 98.95%), in fact (82; 92.68%)
separately (74; 100.00%)
Contingency if (1223; 95.99%), because (858, 100.00%),
so (263; 100.00%), since (184; 52.17%), thus (112;
100.00%), as a result (78; 100.00%)
Temporal when (989; 80.18%), as (743; 70.26%), af-
ter (577; 99.65%), then (340; 93.24%), before (326;
100.00%), meanwhile (193; 48.70%), until (162;
87.04%), later (91; 98.90%), once (84; 95.24%)
The connectives that signal comparison and
contingency are mostly unambiguous. Obvious
exceptions are two of the connectives that are often
used to signal temporal relations: while and since.
88
The predominant senses of these connectives are
comparison (66.07%) and contingency (52.17%)
respectively. Disambiguating these problematic
connectives has already been addressed in previ-
ous work (Miltsakaki et al, 2005), but even the
predominantly temporal connectives are rather am-
biguous. For example less than 95% of the occur-
rances of meanwhile, as, when, until, and then are
temporal relaions.
While some connectives such as ?since? are am-
biguous, most are not. The discourse connec-
tives in the corpus appear in their predominant
sense 93.43% (for comparsion), 94.72% (for con-
tingency), 84.10% (for temporal), and 97.63% (for
expansion) of the time. Temporal connectives are
most ambiguous and connectives signaling expan-
sion are least ambiguous.
4 Automatic classification
The analyses in the previous section show two very
positive trends: many of the discourse relations are
explicitly marked by the use of a discourse connec-
tive, especially comparison and temporal relations,
and discourse connectives are overall mostly un-
ambiguous. These facts would suggest that even
based only on the connective, classification of dis-
course relations could be done well for all data (in-
cluding both implicit and explicit examples) and
particularly well for explicit examples alone. In-
deed, Table 2 shows the performance of a decision
tree classifier for discourse relations, on all data
and on the explicit subset in the second and third
column respectively.
We use the natural distribution of relation
classes found in theWall Street Journal texts, with-
out downsampling to get balanced distribution of
classes. There are four task settings, distinguishing
each type of relation from all others. For example,
comparison relations can be distinguished from all
other relations in the corpus with overall accuracy
of 91.28%, based only on the discourse connective
(first line in Table 2). The recall for recognizing
comparison relations is 0.66, directly reflecting the
fact that 31% of all comparison relations are im-
plicit (Table 1) and the connective feature did not
help at all in those cases. Over explicit data only,
the classification accuracy for comparison relation
versus any other relation is 97.23%, and precision
and recall is 0.95 and above.
As expected, the overall accuracy of identify-
ing contingency and expansion relations is lower,
Task All relations Explicit relations only
Comparison 91.28% (76.54%) 97.23% (69.72%)
Contingency 84.44% (76.81%) 93.99% (79.73%)
Temporal 94.79% (86.54%) 95.4% (79.98%)
Expansion 77.51% (55.67%) 97.61% (65.16%)
Table 2: Decision tree classification accuracy us-
ing only the presence of connectives as binary fea-
tures. The majority class is given in brackets.
Class Precision Recall
Temporal 0.841 [0.841] 0.729 [0.903]
Expansion 0.658 [0.973] 0.982 [0.957]
Contingency 0.948 [0.947] 0.369 [0.844]
Comparison 0.935 [0.935] 0.671 [0.971]
Table 3: Four-way classification. The first number
is for all data, thesecond for explicit relations only.
84.44% and 77.51% on all data respectively, re-
flecting the fact that these relations are often im-
plicit. But by themselves these accuracy numbers
are actually reasonable, setting a rather high base-
line for any more sophisticated method of classify-
ing discourse relations. On explicit data only, the
two-way classification accuracy for the four main
types of relations is 94% and higher.
In four-way classification, disambiguating be-
tween the four main semantic types of discourse
relations leads to 74.74% classification accuracy.
The accuracy for four-way classification of explicit
relations is 93.09%. The precision and recall for
each class is shown in Table 4. The worst per-
formance on the explicit portion of the data is the
precision for temporal relations and the recall for
contingency relations, both of which are 0.84.
5 N-gram discourse relation models
We have shown above that some relations, such as
comparison, can be easily identified because they
are often explicit and are expressed by an unam-
biguous connective. However, one must build a
more subtle automatic classifier to find the implicit
relations. We now look at the frequencies in which
various relations are adjacent in the PDTB. Results
from previous studies of discourse relations sug-
gest that the context of a relation can be helpful in
disambiguating the relation (Wellner et al, 2006).
Here we identify specific dependencies that exist
between sequences of relations.
We computed ?
2
statistics to test the indepen-
dence of each pair of relations. The question is:
do relations A and B occur adjacent to each other
more than they would simply due to chance? The
89
First Relation Second Relation ?
2
p-value
E. Comparison I. Contingency 20.1 .000007
E. Comparison E. Comparison 17.4 .000030
E. Comparison I. Expansion 9.91 .00161
I. Temporal E. Temporal 9.42 .00214
I. Contingency E. Contingency 9.29 .00230
I. Expansion E. Expansion 6.34 .0118
E. Expansion I. Expansion 5.50 .0191
I. Contingency E. Comparison 4.95 .0260
Table 4: ?
2
results for pairs of relations
pairs of implicit and explicit relations which have
significant associations with each other (pval <
0.05) are shown in Table 4. For example, ex-
plicit comparison and implicit contingency co-
occur much more often than would be expected if
they were independent. As explicit comparisons
are generally fairly easy to identify, knowing that
they tend to co-occur may be helpful when search-
ing for implicit contingency relations in a text.
6 Conclusion
We have tried to summarize the difficulty of find-
ing discourse relations using the Penn Discourse
Treebank. We noted that explicit and implicit rela-
tions are approximately evenly distributed overall,
making the task easier than many researchers have
feared. We have found that some relations, such as
temporal and comparison, are more likely to be ex-
plicit than implicit, making them relatively easier
to find, while contingency and expansion are more
often implicit. Among the discourse connectives,
the majority are not very ambiguous between the
different types of relations, with some notable ex-
ceptions such as since and meanwhile.
We have carried out a novel quantitative study
of the patterns of dependencies between discourse
relations. We found that while there does not ap-
pear to be a clear template for the sequence of
relations, there are individual relation pairs that
tend to co-occur. Specifically, we found that even
though contingency relations are likely to be im-
plicit and thus difficult to find, they are likely to
be found near an explicit comparison. We plan to
exploit these findings in future work, addressing
discourse relation labeling in text as a sequence la-
beling problem and using the explicit cue words
of surrounding relations as features for finding the
?hidden? implicit relations.
7 Acknowledgments
This work was partially supported by an Integra-
tive Graduate Education and Research Traineeship
grant from National Science Foundation (NSF-
IGERT 0504487) and by NSF Grant IIS -07-
05671. We would like to thank Nikhil Dinesh for
help with the PDTB, and Rashmi Prasad, Bonnie
Webber and Eleni Miltsakaki for insightful discus-
sions.
References
Hobbs, J. 1979. Coherence and coreference. Cognitive
Science, 3:67?90.
Knott, A. and T. Sanders. 1998. The classification of
coherence relations and their linguistic markers: An
exploration of two languages. Journal of Pragmat-
ics, 30(2):135?175.
Mann, W. and S. Thompson. 1988. Rhetorical struc-
ture theory: Toward a functional theory of text orga-
nization. Text, 8:243?281.
Marcus, M.P., B. Santorini, and M.A. Marcinkiewicz.
1994. Building a Large Annotated Corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
McKeown, Kathleen R. 1985. Text generation: us-
ing discourse strategies and focus constraints to gen-
erate natural language text. Cambridge University
Press, New York, NY, USA.
Miltsakaki, E., N. Dinesh, R. Prasad, A. Joshi, and
B. Webber. 2005. Experiments on sense annotations
and sense disambiguation of discourse connectives.
In Proceedings of the Fourth Workshop on Treebanks
and Linguistic Theories (TLT2005).
Miltsakaki, Eleni, Livio Robaldo, Alan Lee, and Ar-
avind Joshi. 2008. Sense annotation in the penn dis-
course treebank. Computational Linguistics and In-
telligent Text Processing, Lecture Notes in Computer
Science, 4919:275?286.
Moore, J. and P. Wiemer-Hastings. 2003. Discourse in
computational linguistics and artificial intelligence.
Prasad, R., N. Dinesh, A. Lee, E. Miltsakaki,
L. Robaldo, A. Joshi, and B. Webber. 2008. The
penn discourse treebank 2.0. In Proceedings of
the 6th International Conference on Language Re-
sources and Evaluation (LREC).
Wellner, B., J. Pustejovsky, C. Havasi, R. Sauri, and
A. Rumshisky. 2006. Classification of discourse co-
herence relations: An exploratory study using mul-
tiple knowledge sources. In Proceedings of the 7th
SIGDIAL Workshop on Discourse and Dialogue.
90
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 369?379, Prague, June 2007. c?2007 Association for Computational Linguistics
Detecting Compositionality of Verb-Object Combinations using Selectional
Preferences
Diana McCarthy
University of Sussex
Falmer, East Sussex
BN1 9QH, UK
dianam@sussex.ac.uk
Sriram Venkatapathy
International Institute
of Information Technology
Hyderabad, India
sriram@research.iiit.ac.in
Aravind K. Joshi
University of Pennsylvania,
Philadelphia
PA, USA.
joshi@linc.cis.upenn.edu
Abstract
In this paper we explore the use of se-
lectional preferences for detecting non-
compositional verb-object combinations. To
characterise the arguments in a given gram-
matical relationship we experiment with
three models of selectional preference. Two
use WordNet and one uses the entries from
a distributional thesaurus as classes for rep-
resentation. In previous work on selectional
preference acquisition, the classes used for
representation are selected according to the
coverage of argument tokens rather than be-
ing selected according to the coverage of
argument types. In our distributional the-
saurus models and one of the methods us-
ing WordNet we select classes for represent-
ing the preferences by virtue of the number
of argument types that they cover, and then
only tokens under these classes which are
representative of the argument head data are
used to estimate the probability distribution
for the selectional preference model. We
demonstrate a highly significant correlation
between measures which use these ?type-
based? selectional preferences and composi-
tionality judgements from a data set used in
previous research. The type-based models
perform better than the models which use to-
kens for selecting the classes. Furthermore,
the models which use the automatically ac-
quired thesaurus entries produced the best
results. The correlation for the thesaurus
models is stronger than any of the individ-
ual features used in previous research on the
same dataset.
1 Introduction
Characterising the semantic behaviour of phrases in
terms of compositionality has particularly attracted
attention in recent years (Lin, 1999; Schone and Ju-
rafsky, 2001; Bannard, 2002; Bannard et al, 2003;
Baldwin et al, 2003; McCarthy et al, 2003; Ban-
nard, 2005; Venkatapathy and Joshi, 2005). Typi-
cally the phrases are putative multiwords and non-
compositionality is viewed as an important feature
of many such ?words with spaces? (Sag et al, 2002).
For applications such as paraphrasing, information
extraction and translation, it is essential to take the
words of non-compositional phrases together as a
unit because the meaning of a phrase cannot be ob-
tained straightforwardly from the constituent words.
In this work we are investigate methods of deter-
mining semantic compositionality of verb-object 1
combinations on a continuum following previous
research in this direction (McCarthy et al, 2003;
Venkatapathy and Joshi, 2005).
Much previous research has used a combination
of statistics and distributional approaches whereby
distributional similarity is used to compare the con-
stituents of the multiword with the multiword itself.
In this paper, we will investigate the use of selec-
tional preferences of verbs. We will use the pref-
erences to find atypical verb-object combinations as
we anticipate that such combinations are more likely
to be non-compositional.
1We use object to refer to direct objects.
369
Selectional preferences of predicates have been
modelled using the man-made thesaurus Word-
Net (Fellbaum, 1998), see for example (Resnik,
1993; Li and Abe, 1998; Abney and Light, 1999;
Clark and Weir, 2002). There are also distribu-
tional approaches which use co-occurrence data to
cluster distributionally similar words together. The
cluster output can then be used as classes for se-
lectional preferences (Pereira et al, 1993), or one
can directly use frequency information from distri-
butionally similar words for smoothing (Grishman
and Sterling, 1994).
We used three different types of probabilistic
models, which vary in the classes selected for rep-
resentation over which the probability distribution of
the argument heads 2 is estimated. Two use WordNet
and the other uses the entries in a thesaurus of distri-
butionally similar words acquired automatically fol-
lowing (Lin, 1998). The first method is due to Li and
Abe (1998). The classes over which the probabil-
ity distribution is calculated are selected according
to the minimum description length principle (MDL)
which uses the argument head tokens for finding the
best classes for representation. This method has pre-
viously been tried for modelling compositionality of
verb-particle constructions (Bannard, 2002).
The other two methods (we refer to them as ?type-
based?) also calculate a probability distribution us-
ing argument head tokens but they select the classes
over which the distribution is calculated using the
number of argument head types (of a verb in a cor-
pus) in a given class, rather than the number of ar-
gument head tokens in contrast to previous WordNet
models (Resnik, 1993; Li and Abe, 1998; Clark and
Weir, 2002). For example, if the object slot of the
verb park contains the argument heads { car, car,
car, car, van, jeep } then the type-based models use
the word type ?car? only once when determining the
classes over which the probability distribution is to
be estimated. Classes are selected which maximise
the number of types that they cover, rather than the
number of tokens. This is done to avoid the selec-
tional preferences being heavily influenced by noise
from highly frequent arguments which may be poly-
semous and some or all of their meanings may not be
2Argument heads are the nouns occurring in the object slot
of the target verb.
semantically related to the ?prototypical? arguments
of the verb. For example car has a gondola sense in
WordNet.
The third method uses entries in a distributional
thesaurus rather than classes from WordNet. The en-
tries used as classes for representation are selected
by virtue of the number of argument types they en-
compass. As with the WordNet models, the tokens
are used to estimate a probability distribution over
these entries.
In the next section, we discuss related work on
identifying compositionality. In section 3, we de-
scribe the methods we are using for acquiring our
models of selectional preference. In section 4, we
test our models on a dataset used in previous re-
search. We compare the three types of models in-
dividually and also investigate the best performing
model when used in combination with other features
used in previous research. We conclude in section 5.
2 Related Work
Most previous work using distributional approaches
to compositionality either contrasts distributional
information of candidate phrases with constituent
words (Schone and Jurafsky, 2001; Bannard et al,
2003; Baldwin et al, 2003; McCarthy et al, 2003)
or uses distributionally similar words to detect non-
productive phrases (Lin, 1999).
Lin (1999) used his method (Lin, 1998) for au-
tomatic thesaurus construction. He identified can-
didate phrases involving several open-class words
output from his parser and filtered these by the log-
likelihood statistic. Lin proposed that if there is a
phrase obtained by substitution of either the head
or modifier in the phrase with a ?nearest neighbour?
from the thesaurus then the mutual information of
this and the original phrase must be significantly dif-
ferent for the original phrase to be considered non-
compositional. He evaluated the output manually.
As well as distributional similarity, researchers
have used a variety of statistics as indicators of
non-compositionality (Blaheta and Johnson, 2001;
Krenn and Evert, 2001). Fazly and Stevenson (2006)
use statistical measures of syntactic behaviour to
gauge whether a verb and noun combination is likely
to be a idiom. Although they are not specifically
detecting compositionality, there is a strong corre-
370
lation between syntactic rigidity and semantic id-
iosyncrasy.
Venkatapathy and Joshi (2005) combine differ-
ent statistical and distributional methods using sup-
port vector machines (SVMs) for identifying non-
compositional verb-object combinations. They ex-
plored seven features as measures of compositional-
ity:
1. frequency
2. pointwise mutual information (Church and
Hanks, 1990),
3. least mutual information difference with simi-
lar collocations, based on (Lin, 1999) and us-
ing Lin?s thesaurus (Lin, 1998) for obtaining
the similar collocations.
4. The distributed frequency of an object, which
takes an average of the frequency of occurrence
with an object over all verbs occurring with the
object above a threshold.
5. distributed frequency of an object, using the
verb, which considers the similarity between
the target verb and the verbs occurring with the
target object above the specified threshold.
6. a latent semantic approach (LSA) based
on (Schu?tze, 1998; Baldwin et al, 2003) and
considering the dissimilarity of the verb-object
pair with its constituent verb
7. the same LSA approach, but considering the
similarity of the verb-object pair with the ver-
bal form of the object (to capture support verb
constructions e.g. give a smile
Venkatapathy and Joshi (2005) produced a dataset
of verb-object pairs with human judgements of com-
positionality. We say more about this dataset and
Venkatapathy and Joshi?s results in section 4 since
we use the dataset for our experiments.
In this paper, we investigate the use of selec-
tional preferences to detect compositionality. Ban-
nard (2002) did some pioneering work to try and
establish a link between the compositionality of
verb particle constructions and the selectional pref-
erences of the multiword and its constituent verb.
His results were hampered by models based on (Li
and Abe, 1998) which involved rather uninforma-
tive models at the roots of WordNet. There are
several reasons for this. The classes for the model
are selected using MDL by compromising between a
simple model with few classes and one which ex-
plains the data well. The models are particularly
affected by the quantity of data available (Wagner,
2002). Also noise from frequent but idiosyncratic or
polysemous arguments weakens the signal. There
is scope for experimenting with other approaches
such as (Clark and Weir, 2002), however, we feel
a type-based approach is worthwhile to avoid the
noise introduced from frequent but polysemous ar-
guments and bias from highly frequent arguments
which might be part of a multiword rather than a pro-
totypical argument of the predicate in question, for
example eat hat. In contrast to Bannard, our experi-
ments are with verb-object combinations rather than
verb particle constructions. We compare Li and Abe
models with WordNet models which use the num-
ber of argument types to obtain the classes for rep-
resentation of the selectional preferences. In addi-
tion to experiments with these WordNet models, we
propose models using entries in distributional the-
sauruses for representing preferences.
3 Three Methods for Acquiring Selectional
Preferences
All models were acquired from verb-object data ex-
tracted using the RASP parser (Briscoe and Carroll,
2002) from the 90 million words of written English
from the BNC (Leech, 1992). We extracted verb and
common noun tuples where the noun is the argu-
ment head of the object relation. The parser was also
used to extract the grammatical relation data used
for acquisition of the thesaurus described below in
section 3.3.
3.1 TCMs
This approach is a reimplementation of Li and Abe
(1998). Each selectional preference model (referred
to as a tree cut model, or TCM) comprises a set of
disjunctive noun classes selected from all the pos-
sibilities in the WordNet hyponym hierarchy 3 us-
ing MDL (Rissanen, 1978). The TCM covers all the
3We use WordNet version 2.1 for the work in this paper.
371
noun senses in the WordNet hierarchy and is associ-
ated with a probability distribution over these noun
senses in the hierarchy reflecting the argument head
data occurring in the given grammatical relationship
with the specified verb. MDL finds the classes in the
TCM by considering the cost measured in bits of de-
scribing both the model and the argument head data
encoded in the model. A compromise is made by
having as simple a model as possible using classes
further up the hierarchy whilst also providing a good
model for the set of argument head tokens (TK).
The classes are selected by recursing from the top
of the WordNet hierarchy comparing the cost (or de-
scription length) of using the mother class to the cost
of using the hyponym daughter classes. In any path,
the mother is preferred unless using the daughters
would reduce the cost. If using the daughters for the
model is less costly than the mother then the recur-
sion continues to compare the cost of the hyponyms
beneath.
The cost (or description length) for a set of classes
is calculated as the model description length (mdl)
and the data description length (ddl) 4 :-
mdl + ddl
k
2 ? log |TK| + ?
?
tk?TK log p(tk) (1)
k, is the number of WordNet classes being cur-
rently considered for the TCM minus one. The MDL
method uses the size of TK on the assumption that
a larger dataset warrants a more detailed model. The
cost of describing the argument head data is calcu-
lated using the log of the probability estimate from
the classes currently being considered for the model.
The probability estimate for a class being considered
for the model is calculated using the cumulative fre-
quency of all the hyponym nouns under that class
that occur in TK , divided by the number of noun
senses that these nouns have, to account for their
polysemy. This cumulative frequency is also divided
by the total number of noun hyponyms under that
class in WordNet to obtain a smoothed estimate for
all nouns under the class. The probability of the
class is obtained by dividing this frequency estimate
by the total frequency of the argument heads. The
algorithm is described fully by Li and Abe (1998).
4See (Li and Abe, 1998) for a full explanation.
0.17
distancestreet
vancar
mile
street car distancelane corner
0.18 0.10 0.17 0.03
entity
physical_
entityentity
abstract_
way gondola
Example nouns
hyponym classes
locationvehicle
tcm
self?propelled
Figure 1: portion of the TCM for the objects of park.
A small portion of the TCM for the object slot of
park is shown in figure 1. WordNet classes are dis-
played in boxes with a label which best reflects the
meaning of the class. The probability estimates are
shown for the classes on the TCM. Examples of the
argument head data are displayed below the Word-
Net classes with dotted lines indicating membership
at a hyponym class beneath these classes. We can-
not show the full TCM due to lack of space, but we
show some of the higher probability classes which
cover some typical nouns that occur as objects of
park. Note that probability under the classes ab-
stract entity, way and location arise because of a
systematic parsing error where adverbials such as
distance in park illegally some distance from the
railway station are identified by the parser as ob-
jects. Systematic noise from the parser has an im-
pact on all the selectional preference models de-
scribed in this paper.
3.2 WNPROTOs
We propose a method of acquiring selectional pref-
erences which instead of covering all the noun
senses in WordNet, just gives a probability distribu-
tion over a portion of prototypical classes, we refer
to these models as WNPROTOs. A WNPROTO con-
sists of classes within the noun hierarchy which have
the highest proportion of word types occurring in
the argument head data, rather than using the num-
ber of tokens, or frequency, as is used for the TCMs.
This allows less frequent, but potentially informa-
tive arguments to have some bearing on the models
acquired to reduce the impact of highly frequent but
polysemous arguments. We then used the frequency
data to populate these selected classes.
372
The classes (C) in the WNPROTO are selected
from those which include at least a threshold of 2
argument head types 5 occurring in the training data.
Each argument head in the training data is disam-
biguated according to whichever of the WordNet
classes it occurs at or under which has the highest
?type ratio?. Let TY be the set of argument head
types in the object slot of the verb for which we are
acquiring the preference model. The type ratio for a
class (c) is the ratio of noun types (ty ? TY ) occur-
ring in the training data also listed at or beneath that
class in WordNet to the total number of noun types
listed at or beneath that particular class in WordNet
(wnty ? c). The argument types attested in thetraining data are divided by the number of Word-
Net classes that the noun (classes(ty)) belongs to,
to account for polysemy in the training data.
type ratio(c) =
?
ty?TY ?c
1
|classes(ty)|
|wnty ? c|
(2)
If more than one class has the same type ratio then
the argument is not used for calculating the probabil-
ity of the preference model. In this way, only argu-
ments that can be disambiguated are used for calcu-
lating the probability distribution. The advantage of
using the type ratio to determine the classes used to
represent the model and to disambiguate the argu-
ments is that it prevents high frequency verb noun
combinations from masking the information from
prototypical but low frequency arguments. We wish
to use classes which are as representative of the ar-
gument head types as possible to help detect when
an argument head is not related to these classes and
is therefore more likely to be non-compositional.
For example, the class motor vehicle is selected
for the WNPROTO model of the object slot of park
even though there are 5 meanings of car in WordNet
including elevator car and gondola. There are 174
occurrences of car which overwhelms the frequency
of the other objects (e.g. van 11, vehicle 8) but by
looking for classes with a high proportion of types
(rather than word tokens) car is disambiguated ap-
propriately and the class motor vehicle is selected
for representation.
5We have experimented with a threshold of 3 and obtained
similar results.
0.03
0.04
tanker
boat
pram.61
car
0.05
van
caravan
entity
physical_
entity
Example nouns
hyponym classes
vehicle
self?propelled
transport
vehicle
wheeled
model
motor
vehicle caravan
  classes in
Figure 2: Part of WNPROTO for the object slot of
park
The relative frequency of each class is obtained
from the set of disambiguated argument head tokens
and used to provide the probability distribution over
this set of classes. Note that in WNPROTO, classes
can be subsumed by others in the hyponym hierar-
chy. The probability assigned to a class is appli-
cable to any descendants in the hyponym hierarchy,
except those within any hyponym classes within the
WNPROTO. The algorithm for selecting C and cal-
culating the probability distribution is shown as Al-
gorithm 1. Note that we use brackets for comments.
In figure 2 we show a small portion of the WN-
PROTO for park. Again, WordNet classes are dis-
played in boxes with a label which best reflects the
meaning of the class. The probability estimates are
shown in the boxes for all the classes included in
the WNPROTO. The classes in the WNPROTO model
are shown with dashed lines. Examples of the ar-
gument head data are displayed below the WordNet
classes with dotted lines indicating membership at
a hyponym class beneath these classes. We cannot
show the full WNPROTO due to lack of space, but
we show some of the classes with higher probability
which cover some typical nouns that occur as objects
of park.
373
Algorithm 1 WNPROTO algorithm
C = (){classes in WNPROTO}
D = () {disambiguated ty ? TY }
fD = 0 {frequency of disambiguated items}
TY = argument head types {nouns occurring as objects of verb, with associated frequencies}
C1 ? WordNet
where |ty ? TY occurring in c ? C1| > 1
for all ty ? TY do
find c ? classes(ty) ? C1 where c = argmaxc typeratio(c)
if c & c /? C then
add c to C
add ty ? c to D {Disambiguated ty with c}
end if
end for
for all c ? C do
if |ty ? c ? D| > 1 then
fD = fD + frequency(ty){sum frequencies of types under classes to be used in model}
else
remove c from C {classes with less than two disambiguated nouns are removed}
end if
end for
for all c ? C do
p(c) = frequency-of-all-tys-disambiguated-to-class(c,D)fD {calculating class probabilities}
end for
Algorithm 2 DSPROTO algorithm
C = (){classes in DSPROTO}
D = () {disambiguated ty ? TY }
fD = 0 {frequency of disambiguated items}
TY = argument head types {nouns occurring as objects of verb, with associated frequencies}
C1 = cty ? TY where num-types-in-thesaurus(cty, TY ) > 1
order C1 by num-types-in-thesaurus(cty, TY ) {classes ordered by coverage of argument head types}
for all cty ? ordered C1 do
Dcty = () {disambiguated for this class}
for all ty ? TY where in-thesaurus-entry(cty, ty) do
if ty /? D then
add ty to Dcty {types disambiguated to this class only if not disambiguated by a class used already}
end if
end for
if |Dcty| > 1 then
add cty to C
for all ty ? Dcty do
add ty ? cty to D {Disambiguated ty with cty}
fD = fD + frequency(ty)
end for
end if
end for
for all cty ? C do
p(cty) = frequency-of-all-tys-disambiguated-to-class(cty,D)fD {calculating class probabilities}
end for
374
3.3 DSPROTOs
We use a thesaurus acquired using the method
proposed by Lin (1998). For input we used the
grammatical relation data from automatic parses of
the BNC. For each noun we considered the co-
occurring verbs in the object and subject relation,
the modifying nouns in noun-noun relations and
the modifying adjectives in adjective-noun relations.
Each thesaurus entry consists of the target noun and
the 50 most similar nouns, according to Lin?s mea-
sure of distributional similarity, to the target.
The argument head noun types (TY ) are used
to find the entries in the thesaurus as the ?classes?
(C) of the selectional preference for a given verb.
As with WNPROTOs, we only cover argument types
which form coherent groups with other argument
types since we wish i) to remove noise and ii) to
be able to identify argument types which are not re-
lated with the other types and therefore may be non-
compositional. As our starting point we only con-
sider an argument type as a class for C if its entry in
the thesaurus covers at least a threshold of 2 types. 6
To select C we use a best first search. This method
processes each argument type in TY in order of the
number of the other argument types from TY that it
has in its thesaurus entry of 50 similar nouns. An ar-
gument head is selected as a class for C (cty ? C) 7
if it covers at least 2 of the argument heads that are
not in the thesaurus entries of any of the other classes
already selected for C . Each argument head is dis-
ambiguated by whichever class in C under which it
is listed in the thesaurus and which has the largest
number of the TY in its thesaurus entry. When the
algorithm finishes processing the ordered argument
heads to select C , all argument head types are dis-
ambiguated by C apart from those which after dis-
ambiguation occur in isolation in a class without
other argument types. Finally a probability distri-
bution over C is estimated using the frequency (to-
kens) of argument types that occur in the thesaurus
entries for any cty ? C . If an argument type oc-
curs in the entry of more than one cty then it is as-
signed to whichever of these has the largest number
6As with the WNPROTOs, we experimented with a value of
3 for this threshold and obtained similar results.
7We use cty for the classes of the DSPROTO. These classes
are simply groups of nouns which occur under the entry of a
noun (ty) in the thesaurus.
class (p(c)) disambiguated objects (freq)
van (0.86) car (174) van (11) vehicle (8) . . .
mile (0.05) street (5) distance (4) mile (1) . . .
yard (0.03) corner (4) lane (3) door (1)
backside (0.02) backside (2) bum (1) butt (1) . . .
Figure 3: First four classes of DSPROTO model for
park
of disambiguated argument head types and its token
frequency is attributed to that class. We show the
algorithm as Algorithm 2.
The algorithms for WNPROTO algorithm 1 and
DSPROTO (algorithm 2) differ because of the na-
ture of the inventories of candidate classes (Word-
Net and the distributional thesaurus). There are a
great many candidate classes in WordNet. The WN-
PROTO algorithm selects the classes from all those
that the argument heads belong to directly and indi-
rectly by looping over all argument types to find the
class that disambiguates each by having the largest
type ratio calculated using the undisambiguated ar-
gument heads. The DSPROTO only selects classes
from the fixed set of argument types. The algorithm
loops over the argument types with at least two ar-
gument heads in the thesaurus entry and ordered by
the number of undisambiguated argument heads in
the thesaurus entry. This is a best first search to min-
imise the number of argument heads used in C but
maximise the coverage of argument types.
In figure 3, we show part of a DSPROTO model for
the object of park. 8 Note again that the class mile
arises because of a systematic parsing error where
adverbials such as distance in park illegally some
distance from the railway station are identified by
the parser as objects.
4 Experiments
Venkatapathy and Joshi (2005) produced a dataset of
verb-object pairs with human judgements of com-
positionality. They obtained values of rs between0.111 and 0.300 by individually applying the 7 fea-
tures described above in section 2. The best corre-
lation was given by feature 7 and the second best
was feature 3. They combined all 7 features using
SVMs and splitting their data into test and training
data and achieve a rs of 0.448, which demonstrates
8We cannot show the full model due to lack of space.
375
significantly better correlation with the human gold-
standard than any of the features in isolation
We evaluated our selectional preference models
using the verb-object pairs produced by Venkatapa-
thy and Joshi (2005). 9 This dataset has 765 verb-
object collocations which have been given a rat-
ing between 1 and 6, by two annotators (both flu-
ent speakers of English). Kendall?s Tau (Siegel and
Castellan, 1988) was used to measure agreement,
and a score of 0.61 was obtained which was highly
significant. The ranks of the two annotators gave a
Spearman?s rank-correlation coefficient (rs) of 0.71.
The Verb-Object pairs included some adjectives
(e.g. happy, difficult, popular), pronouns and com-
plements e.g. become director. We used the sub-
set of 638 verb-object pairs that involved common
nouns in the object relationship since our preference
models focused on the object relation for common
nouns. For each verb-object pair we used the pref-
erence models acquired from the RASP parses of the
BNC to obtain the probability of the class that this
object occurs under. Where the object noun is a
member of several classes (classes(noun) ? C)
in the model, the class with the largest probability
is used. Note though that for WNPROTOs we have
the added constraint that a hyponym class from C is
selected in preference to a hypernym in C . Compo-
sitionality of an object noun and verb is computed
as:-
comp(noun, verb) = maxc?classes(noun)?C p(c|verb) (3)
We use the probability of the class, rather than an
estimate of the probability of the object, because we
want to determine how likely any word belonging
to this class might occur with the given verb, rather
than the probability of the specific noun which may
be infrequent, yet typical, of the objects that occur
with this verb. For example, convertible may be
an infrequent object of park, but it is quite likely
given its membership of the class motor vehicle.
We do not want to assume anything about the fre-
quency of non-compositional verb-object combina-
tions, just that they are unlikely to be members of
classes which represent prototypical objects. We
9This verb-object dataset is available from
http://www.cis.upenn.edu/?sriramv/mywork.html.
method rs p < (one tailed)
selectional preferences
TCM 0.090 0.0119
WNPROTO 0.223 0.00003
DSPROTO 0.398 0.00003
features from V&J
frequency (f1) 0.141 0.00023
MI (f2) 0.274 0.00003
Lin99 (f3) 0.139 0.00023
LSA2 (f7) 0.209 0.00003
combination with SVM
f2,3,7 0.413 0.00003
f1,2,3,7 0.419 0.00003
DSPROTO f1,2,3,7 0.454 0.00003
Table 1: Correlation scores for 638 verb object pairs
will contrast these models with a baseline frequency
feature used by Venkatapathy and Joshi.
We use our selectional preference models to pro-
vide the probability that a candidate is represen-
tative of the typical objects of the verb. That is,
if the object might typically occur in such a rela-
tionship then this should lessen the chance that this
verb-object combination is non-compositional. We
used the probability of the classes from our 3 selec-
tional preference models to rank the pairs and then
used Spearman?s rank-correlation coefficient (rs) tocompare these ranks with the ranks from the gold-
standard.
Our results for the three types of preference mod-
els are shown in the first section of table 1. 10 All the
correlation values are significant, but we note that
using the type based selectional preference mod-
els achieves a far greater correlation than using the
TCMs. The DSPROTO models achieve the best re-
sults which is very encouraging given that they only
require raw data and an automatic parser to obtain
the grammatical relations.
We applied 4 of the features used by Venkatapa-
thy and Joshi (2005) 11 and described in section 2
to our subset of 638 items. These features were ob-
10We show absolute values of correlation following (Venkat-
apathy and Joshi, 2005).
11The other 3 features performed less well on this dataset so
we do not report the details here. This seems to be because they
worked particularly well with the adjective and pronoun data in
the full dataset.
376
tained using the same BNC dataset used by Venkat-
apathy and Joshi which was obtained using Bikel?s
parser (Bikel, 2004). We obtained correlation val-
ues for these features as shown in table 1 under
V&J. These features are feature 1 frequency, feature
2 pointwise mutual information, feature 3 based on
(Lin, 1999) and feature 7 LSA feature which consid-
ers the similarity of the verb-object pair with the ver-
bal form of the object. Pointwise mutual informa-
tion did surprisingly well on this 84% subset of the
data, however the DSPROTO preferences still out-
performed this feature. We combined the DSPROTO
and V&J features with an SVM ranking function and
used 10 fold cross validation as Venkatapathy and
Joshi did. We contrast the result with the V&J fea-
tures without the preference models. The results in
the bottom section of table 1 demonstrate that the
preference models can be combined with other fea-
tures to produce optimal results.
5 Conclusions and Directions for Future
Work
We have demonstrated that the selectional prefer-
ences of a verbal predicate can be used to indi-
cate if a specific combination with an object is non-
compositional. We have shown that selectional pref-
erence models which represent prototypical argu-
ments and focus on argument types (rather than to-
kens) do well at the task. Models produced from
distributional thesauruses are the most promising
which is encouraging as the technique could be ap-
plied to a language without a man-made thesaurus.
We find that the probability estimates from our
models show a highly significant correlation, and
are very promising for detecting non-compositional
verb-object pairs, in comparison to individual fea-
tures used previously.
Further comparison of WNPROTOs and
DSPROTOs to other WordNet models are war-
ranted to contrast the effect of our proposal for
disambiguation using word types with iterative
approaches, particularly those of Clark and Weir
(2002). A benefit of the DSPROTOs is that they
do not require a hand-crafted inventory. It would
also be worthwhile comparing the use of raw data
directly, both from the BNC and from google?s
Web 1T corpus (Brants and Franz, 2006) since
web counts have been shown to outperform the
Clark and Weir models on a pseudo-disambiguation
task (Keller and Lapata, 2003).
We believe that preferences should NOT be used
in isolation. Whilst a low preference for a noun
may be indicative of peculiar semantics, this may
not always be the case, for example chew the fat.
Certainly it would be worth combining the prefer-
ences with other measures, such as syntactic fixed-
ness (Fazly and Stevenson, 2006). We also believe it
is worth targeting features to specific types of con-
structions, for example light verb constructions un-
doubtedly warrant special treatment (Stevenson et
al., 2003)
The selectional preference models we have pro-
posed here might also be applied to other tasks. We
hope to use these models in tasks such as diathesis
alternation detection (McCarthy, 2000; Tsang and
Stevenson, 2004) and contrast with WordNet mod-
els previously used for this purpose.
6 Acknowledgements
We acknowledge support from the Royal Society
UK for a Dorothy Hodgkin Fellowship to the first
author. We thank the anonymous reviewers for their
constructive comments on this work.
References
Steven Abney and Marc Light. 1999. Hiding a semantic
class hierarchy in a Markov model. In Proceedings of
the ACL Workshop on Unsupervised Learning in Nat-
ural Language Processing, pages 1?8.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model ofmultiword expression decomposability. In Proceed-
ings of the ACL Workshop on multiword expressions:
analysis, acquisition and treatment, pages 89?96.
Colin Bannard, Timothy Baldwin, and Alex Lascarides.2003. A statistical approach to the semantics ofverb-particles. In Proceedings of the ACL Workshop
on multiword expressions: analysis, acquisition and
treatment, pages 65?72.
Colin. Bannard. 2002. Statistical techniquesfor automatically inferring the semantics of verb-particle constructions. Technical Report WP-2002-
06, University of Edinburgh, School of Informatics.http://lingo.stanford.edu/pubs/WP-2002-06.pdf.
377
Colin Bannard. 2005. Learning about the meaning of
verb-particle constructions from corpora. Computer
Speech and Language, 19(4):467?478.
Daniel M. Bikel. 2004. A distributional analysis of a lex-
icalized statistical parsing model. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2004), Barcelona, Spain,
July. Association for Computational Linguistics.
Don Blaheta and Mark Johnson. 2001. Unsuper-vised learning of multi-word verbs. In Proceedings
of the ACL Workshop on Collocations, pages 54?60,Toulouse, France.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
corpus version 1.1. Technical Report.
Edward Briscoe and John Carroll. 2002. Robust accurate
statistical annotation of general text. In Proceedings
of the Third International Conference on Language
Resources and Evaluation (LREC), pages 1499?1504,
Las Palmas, Canary Islands, Spain.
Kenneth Church and Patrick Hanks. 1990. Word asso-ciation norms, mutual information and lexicography.
Computational Linguistics, 19(2):263?312.
Stephen Clark and David Weir. 2002. Class-based prob-ability estimation using a semantic hierarchy. Compu-
tational Linguistics, 28(2):187?206.
Afsaneh Fazly and Suzanne Stevenson. 2006. Automat-ically constructing a lexicon of verb phrase idiomatic
combinations. In Proceedings of the 11th Conference
of the European Chapter of the Association for Com-
putational Linguistics (EACL-2006), pages 337?344,
Trento, Italy, April.
Christiane Fellbaum, editor. 1998. WordNet, An Elec-
tronic Lexical Database. The MIT Press, Cambridge,
MA.
Ralph Grishman and John Sterling. 1994. Generalizing
automatically generated selectional patterns. In Pro-
ceedings of the 15th International Conference of Com-
putational Linguistics. COLING-94, volume I, pages742?747.
Frank Keller and Mirella Lapata. 2003. Using the web toobtain frequencies for unseen bigrams. Computational
Linguistics, 29(3):459?484.
Brigitte Krenn and Stefan Evert. 2001. Can we do betterthan frequency? A case study on extracting PP-verb
collocations. In Proceedings of the ACL Workshop on
Collocations, pages 39?46, Toulouse, France.
Geoffrey Leech. 1992. 100 million words of English:
the British National Corpus. Language Research,28(1):1?13.
Hang Li and Naoki Abe. 1998. Generalizing case frames
using a thesaurus and the MDL principle. Computa-
tional Linguistics, 24(2):217?244.
Dekang Lin. 1998. Automatic retrieval and clusteringof similar words. In Proceedings of COLING-ACL 98,
Montreal, Canada.
Dekang Lin. 1999. Automatic identification of non-compositional phrases. In Proceedings of ACL-99,
pages 317?324, Univeristy of Maryland, College Park,Maryland.
Diana McCarthy, Bill Keller, and John Carroll. 2003.
Detecting a continuum of compositionality in phrasalverbs. In Proceedings of the ACL 03 Workshop: Multi-
word expressions: analysis, acquisition and treatment,pages 73?80.
Diana McCarthy. 2000. Using semantic preferences toidentify verbal participation in role switching alter-nations. In Proceedings of the First Conference of
the North American Chapter of the Association for
Computational Linguistics. (NAACL), pages 256?263,Seattle,WA.
Fernando Pereira, Nattali Tishby, and Lillian Lee. 1993.
Distributional clustering of English words. In Pro-
ceedings of the 31st Annual Meeting of the Association
for Computational Linguistics, pages 183?190.
Philip Resnik. 1993. Selection and Information:
A Class-Based Approach to Lexical Relationships.Ph.D. thesis, University of Pennsylvania.
Jorma Rissanen. 1978. Modelling by shortest data de-scription. Automatica, 14:465?471.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-take, and Dan Flickinger. 2002. Multiword expres-
sions: A pain in the neck for NLP. In Proceedings of
the Third International Conference on Intelligent Text
Processing and Computational Linguistics (CICLing
2002), pages 1?15, Mexico City, Mexico.
Patrick Schone and Daniel Jurafsky. 2001. Isknowledge-free induction of multiword unit dictionaryheadwords a solved problem? In Proceedings of the
2001 Conference on Empirical Methods in Natural
Language Processing, pages 100?108, Hong Kong.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?123.
Sidney Siegel and N. John Castellan. 1988. Non-
Parametric Statistics for the Behavioral Sciences.
McGraw-Hill, New York.
Suzanne Stevenson, Afsaneh Fazly, and Ryan North.2003. Statistical measures of the semi-productivity oflight verb constructions. In Proceedings of the ACL
2004 Workshop on Multiword Expressions: Integrat-
ing Processing, Barcelona, Spain.
378
Vivian Tsang and Suzanne Stevenson. 2004. Using se-
lectional profile distance to detect verb alternations. In
Proceedings of NAACL Workshop on Computational
Lexical Semantics (CLS-04), pages 30?37, Boston,
MA.
Sriram Venkatapathy and Aravind K. Joshi. 2005. Mea-suring the relative compositionality of verb-noun (v-n)collocations by integrating features. In Proceedings of
the joint conference on Human Language Technology
and Empirical methods in Natural Language Process-
ing, pages 899?906, Vancouver, B.C., Canada.
Andreas Wagner. 2002. Learning thematic role relations
for wordnets. In Proceedings of ESSLLI-2002 Work-
shop on Machine Learning Approaches in Computa-
tional Linguistics, Trento.
379
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 495?504,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
LTAG Dependency Parsing with Bidirectional Incremental Construction
Libin Shen
BBN Technologies
lshen@bbn.com
Aravind K. Joshi
University of Pennsylvania
joshi@cis.upenn.edu
Abstract
In this paper, we first introduce a new archi-
tecture for parsing, bidirectional incremental
parsing. We propose a novel algorithm for in-
cremental construction, which can be applied
to many structure learning problems in NLP.
We apply this algorithm to LTAG dependency
parsing, and achieve significant improvement
on accuracy over the previous best result on
the same data set.
1 Introduction
The phrase ?Bidirectional Incremental? may appear
self-contradictory at first sight, since incremental
parsing usually means left-to-right parsing in the
context of conventional parsing. In this paper, we
will extend the meaning of incremental parsing.
The idea of bidirectional parsing is related to
the bidirectional sequential classification method de-
scribed in (Shen et al, 2007). In that paper, a tagger
assigns labels to words of highest confidence first,
and then these labels in turn serve as the context of
later labelling operations. The bidirectional tagger
obtained the best results in literature on POS tagging
on the standard PTB dataset.
We extend this method from labelling to structure
learning, The search space of structure learning is
much larger, so that it is appropriate to exploit con-
fidence scores in search.
In this paper, we are interested in LTAG depen-
dency parsing because TAG parsing is a well known
problem of high computational complexity in reg-
ular parsing. In order to get a focus for the learn-
ing algorithm, we work on a variant of LTAG based
parsing in which we learn the word dependency re-
lations encoded in LTAG derivations instead of the
full-fledged trees.
1.1 Parsing
Two types of parsing strategies are popular in nat-
ural language parsing, which are chart parsing and
incremental parsing.
Suppose the input sentence is w1w2...wn. Let cell
[i, j] represent wiwi+1...wj , a substring of the sen-
tence. As far as CFG parsing is concerned, a chart
parser computes the possible structures over all pos-
sible cells [i, j], where 1 ? i ? j ? n. The order
of computing on these n(n + 1)/2 cells is based on
some partial order , such that [p1, p2]  [q1, q2] if
q1 ? p1 ? p2 ? q2. In order to employ dynamic
programming, one can only use a fragment of a hy-
pothesis to represent the whole hypothesis, which
is assumed to satisfy conditional independence as-
sumption. It is well known that richer context rep-
resentation gives rise to better parsing performance
(Johnson, 1998). However, the need for tractability
does not allow much internal information to be used
to represent a hypothesis. The designs of hypothe-
ses in (Collins, 1999; Charniak, 2000) show a del-
icate balance between expressiveness and tractabil-
ity, which play an important role in natural language
parsing.
Some recent work on incremental parsing
(Collins and Roark, 2004; Shen and Joshi, 2005)
showed another way to handle this problem. In
these incremental parsers, tree structures are used
to represent the left context. In this way, one can
access the whole tree to collect rich context in-
formation at the expense of being limited to beam
search, which only maintains k-best results at each
495
step. Compared to chart parsing, incremental pars-
ing searches for the analyses for only 2n ? 1 cells,
[1, 1], [2, 2], [1, 2], .., [i, i], [1, i], .., [1, n], incremen-
tally, while complex structures are used for the anal-
yses for each cell, which satisfy conditional inde-
pendence under a much weaker assumption.
In this paper, we call this particular approach
left-to-right incremental parsing, since one can also
search from right to left incrementally in a similar
way. A major problem of the left-to-right approach
is that one can only utilize the structural information
on the left side but not the right side.
1.2 Parsing as Bidirectional Construction
A natural way to handle this problem is to employ
bidirectional search, which means we can dynami-
cally search the space in two directions. So we ex-
pand the idea of incremental parsing by introducing
greedy search. Specifically, we look for the hypothe-
ses over the cell [1, n] by building analyses over
2n? 1 cells [ai,1, ai,2], i = 1, .., 2n? 1 step by step,
where [a2n?1,1, a2n?1,2] = [1, n]. Furthermore, for
any [ai,1, ai,2]
? ai,1 = ai,2, or
? ?j, k, such that [ai,1, ai,2] = [aj,1, ak,2], where
j < i, k < i and aj,2 + 1 = ak,1.
It is easy to show that the set {[ai,1, ai,2] | 1 ?
i ? 2n? 1} forms a tree relation, which means that
each cell except the last one will be used to build an-
other cell just once. In this framework, we can begin
with several starting points in a sentence and search
in any direction. So left-to-right parsing is only a
special case of incremental parsing defined in this
way. We still use complex structures to represent
the partial analyses, so as to employ both top-down
and bottom-up information as in (Collins and Roark,
2004; Shen and Joshi, 2005). Furthermore, we can
utilize the rich context on both sides of the partial
results.
Similar to bidirectional labelling in (Shen et al,
2007), there are two learning tasking in this model.
First, we need to learn which cell we should choose.
At each step, we can select only one path. Sec-
ondly, we need to learn which operation we should
take for a given cell. We maintain k-best candidates
for each cell instead of only one, which differenti-
ates this model from normal greedy search. So our
model is more robust. Furthermore, we need to find
an effective way to iterate between these two tasks.
Instead of giving an algorithm specially designed
for parsing, we generalize the problem for graphs. A
sentence can be viewed as a graph in which words
are viewed as vertices and neighboring words are
connected with an arc. In Sections 2 and 3, we
will propose decoding and training algorithms re-
spectively for graph-based incremental construction,
which can be applied to many structure learning
problems in NLP.
We will apply this algorithm to dependency pars-
ing of Lexicalized Tree Adjoining Grammar (Joshi
and Schabes, 1997). Specifically, we will train and
evaluate an LTAG dependency parser over the LTAG
treebank described in Shen et al (2008). We report
the experimental results on PTB section 23 of the
LTAG treebank. The accuracy on LTAG dependency
is 90.5%, which is 1.2 points over 89.3%, the previ-
ous best result (Shen and Joshi, 2005) on the same
data set.
It should be noted that PTB-based bracketed la-
belling is not an appropriate evaluation metric here,
since the experiments are on an LTAG treebank.
The derived trees in the LTAG treebank are different
from the CFG trees in PTB. Hence, we do not use
metrics such as labeled precision and labeled recall
for evaluation.
2 Graph-based Incremental Construction
2.1 Idea and Data Structures
Now we define the problem formally. We will use
dependency parsing as an example to illustrate the
idea.
We are given a connected graph G(V,E) whose
hidden structure is U , where V = {vi}, E ?
V ? V is a symmetric relation, and U = {uk} is
composed of a set of elements that vary with ap-
plications. As far as dependency parsing is con-
cerned, the input graph is simply a chain of ver-
tices, where E(vi?1, vi), and its hidden structure is
{uk = (vsk , vek , bk)}, where vertex vek depends on
vertex vsk with label bk.
A graph-based incremental construction algo-
rithm looks for the hidden structure in a bottom-up
496
style.
Let xi and xj be two sets of connected vertexes
in V , where xi ? xj = ? and they are directly con-
nected via an edge in E. Let yxi be a hypothesized
hidden structure of xi, and yxj a hypothesized hid-
den structure of xj .
Suppose we choose to combine yxi and yxj with
an operation r to build a hypothesized hidden struc-
ture for xk = xi ? xj . We say the process of con-
struction is incremental if the output of the opera-
tion, yxk = r(xi, xj, yxi, yxj) ? yxi ? yxj for all
the possible xi, xj , yxi, yxj and operation r. As far
as dependency parsing is concerned, incrementality
means that we cannot remove any links coming from
the substructures.
Once yxk is built, we can no longer use yxi or
yxj as a building block. It is easy to see that left
to right incremental construction is a special case of
our approach. So the question is how we decide the
order of construction as well as the type of operation
r. For example, in the very first step of dependency
parsing, we need to decide which two words are to
be combined as well as the dependency label to be
used.
This problem is solved statistically, based on the
features defined on the substructures involved in the
operation and their context. Suppose we are given
the weights of these features, we will show in the
next section how these parameters guide us to build
a set of hypothesized hidden structures with beam
search. In Section 3, we will present a Perceptron
like algorithm (Collins, 2002; Daume? III and Marcu,
2005) to obtain the parameters.
Now we introduce the data structure to be used in
our algorithms.
A fragment is a connected sub-graph of G(V,E).
Each fragment x is associated with a set of hypothe-
sized hidden structures, or fragment hypotheses for
short: Y x = {yx1 , ..., yxk}. Each yx is a possible frag-
ment hypothesis of x.
It is easy to see that an operation to combine two
fragments may depend on the fragments in the con-
text, i.e. fragments directly connected to one of the
operands. So we introduce the dependency relation
over fragments. Suppose there is a dependency re-
lation D ? F ? F , where F ? 2V is the set of all
fragments in graph G. D(xi, xj) means that any op-
eration on a fragment hypothesis of xi depends on
the features in the fragment hypothesis of xj , and
vice versa.
We are especially interested in the following two
dependency relations.
? level-0 dependency: D0(xi, xj) ?? i = j.
? level-1 dependency: D1(xi, xj) ?? xi and
xj are directly connected in G.
Level-0 dependency means that the features of
a hypothesis for a vertex xi do not depend on the
hypotheses for other vertices. Level-1 dependency
means that the features depend on the hypotheses of
nearby vertices only.
The learning algorithm for level-0 dependency is
similar to the guided learning algorithm for labelling
as described in (Shen et al, 2007). Level-1 depen-
dency requires more data structures to maintain the
hypotheses with dependency relations among them.
However, we do not get into the details of level-1
formalism in this papers for two reasons. One is the
limit of page space and depth of a conference pa-
per. On the other hand, our experiments show that
the parsing performance with level-1 dependency is
close to what level-0 dependency could provides.
Interested readers could refer to (Shen, 2006) for
detailed description of the learning algorithms for
level-1 dependency.
2.2 Algorithms
Algorithm 1 shows the procedure of building hy-
potheses incrementally on a given graph G(V,E).
Parameter k is used to set the beam width of search.
Weight vector w is used to compute score of an op-
eration.
We have two sets, H and Q, to maintain hypothe-
ses. Hypotheses in H are selected in beam search,
and hypotheses in Q are candidate hypotheses for
the next step of search in various directions.
We first initiate the hypotheses for each vertex,
and put them into set H . For example, in depen-
dency parsing, the initial value is a set of possible
POS tags for each single word. Then we use a queue
Q to collect all the possible hypotheses over the ini-
tial hypotheses H .
Whenever Q is not empty, we search for the hy-
pothesis with the highest score according to a given
weight vector w. Suppose we find (x, y). We select
497
Algorithm 1 Incremental Construction
Require: graph G(V,E);
Require: beam width k;
Require: weight vector w;
1: H ? initH();
2: Q? initQ(H);
3: repeat
4: (x?, y?)? arg max(x,y)?Q score(y);
5: H ? updateH(H,x?);
6: Q? updateQ(Q,H, x?);
7: until (Q = ?)
DT MD NNVB CDNN
NN NN
student will take four coursesthe
Figure 1: After initialization
top k-best hypotheses for segment x from Q and use
them to update H . Then we remove from Q all the
hypotheses for segments that have overlap with seg-
ment x. In the end, we build new candidate hypothe-
ses with the updated selected hypothesis set H , and
add them to Q.
2.3 An Example
We use an example of dependency parsing to illus-
trate the incremental construction algorithm first.
Suppose the input sentence is the student will take
four courses. We are also given the candidate POS
tags for each word. So the graph is just a linear struc-
ture in this case. We use level-0 dependency and set
beam width to two.
We use boxes to represent fragments. The depen-
dency links are from the parent to the child.
Figure 1 shows the result after initialization. Fig-
ure 2 shows the result after the first step, combining
the fragments of four and courses. Figure 3 shows
the result after the second step, combining the and
student, and figure 4 shows the result after the third
step, combining take and four courses. Due to lim-
ited space, we skip the rest operations.
2.4 Description
Now we will explain the functions in Algorithm 1
one by one.
DT NN VBMD CD NN
NN NN CD NN
student will take four coursesthe
Figure 2: Step 1
DT NN VBMD CD NN
DT NN NN NN CD NN
student will take four coursesthe
Figure 3: Step 2
? initH() initiates hypotheses for each vertex.
Here we set the initial fragment hypotheses,
Y xi = {yxi1 , ..., yxik }, where xi = {vi} con-
tains only one vertex.
? initQ(H) initiates the queue of candidate op-
erations over the current hypotheses H . Sup-
posed there exist segments xi and xj which are
directly connected in G. We apply all possi-
ble operations to all fragment hypotheses for xj
and xj , and add the result hypotheses in Q. For
example, we generate (x, y) with some opera-
tion r, where segment x is xi ? xj .
All the candidate operations are organized with
respect to the segments. For each segment, we
maintain top k candidates according to their
scores.
? updateH(H,x) is used to update hypotheses in
H . First, we remove from H all the hypotheses
whose corresponding segment is a sub-set of x.
Then, we add into H the top k hypotheses for
segment x.
? updateQ(Q,H, x) is also designed to complete
two tasks. First, we remove from Q all the
hypotheses whose corresponding segment has
overlap with segment x. Then, we add new
candidate hypotheses depending on x in a way
498
DT NN VBMD CD NN
DT NN NN CD NN
student will take four courses
MD
the
Figure 4: Step 3
Algorithm 2 Parameter Optimization
1: w? 0;
2: for (round r = 0; r < R; r++) do
3: load graph Gr(V,E), gold standard Hr;
4: initiate H and Q;
5: repeat
6: (x?, y?)? arg max(x,y)?Q score(y);
7: if (y? is compatible with Hr) then
8: update H and Q;
9: else
10: y? ? positive(Q,x?);
11: promote(w, y?);
12: demote(w, y?);
13: update Q with w;
14: end if
15: until (Q = ?)
16: end for
similar to the initQ(H) function. For each seg-
ment, we maintain the top k candidates for each
segment.
3 Parameter Optimization
In the previous section, we described an algorithm
for graph-based incremental construction for a given
weight vector w. In Algorithm 2, we present a Per-
ceptron like algorithm to obtain the weight vector
for the training data.
For each given training sample (Gr,Hr), where
Hr is the gold standard hidden structure of graph
Gr, we first initiate cut T , hypotheses HT and can-
didate queue Q by calling initH and initQ as in Al-
gorithm 1.
Then we use the gold standard Hr to guide the
search. We select candidate (x?, y?) which has the
highest operation score in Q. If y? is compatible with
Hr, we update H and Q by calling updateH and
updateQ as in Algorithm 1. If y? is incompatible
with Hr, we treat y? as a negative sample, and search
for a positive sample y? in Q with positive(Q,x?).
If there exists a hypothesis y?x? for fragment x?
which is compatible with Hr, then positive(Q,x?)
returns y?x? . Otherwise positive(Q,x?) returns the
candidate hypothesis which is compatible with Hr
and has the highest operation score in Q.
Then we update the weight vector w with y? and
y?. At the end, we update the candidate Q by using
the new weights w.
In order to improve the performance, we use Per-
ceptron with margin in the training (Krauth and
Me?zard, 1987). The margin is proportional to the
loss of the hypothesis. Furthermore, we use aver-
aged weights (Collins, 2002; Freund and Schapire,
1999) in Algorithm 1.
4 LTAG Dependency Parsing
We apply the new algorithm to LTAG dependency
parsing on an LTAG Treebank (Shen et al, 2008)
extracted from Penn Treebank (Marcus et al, 1994)
and Proposition Bank (Palmer et al, 2005). Penn
Treebank was previously used to train and evalu-
ate various dependency parsers (Yamada and Mat-
sumoto, 2003; McDonald et al, 2005). In these
works, Magerman?s rules are used to pick the head
at each level according to the syntactic labels in a
local context.
The dependency relation encoded in the LTAG
Treebank reveals deeper information for the follow-
ing two reasons. First, the LTAG architecture itself
reveals deeper dependency. Furthermore, the PTB
was reconciled with the Propbank in the LTAG Tree-
bank extraction (Shen et al, 2008).
We are especially interested in the two types of
structures in the LTAG Treebank, predicate adjunc-
tion and predicate coordination. They are used to
encode dependency relations which are unavailable
in other approaches. On the other hand, these struc-
tures turn out to be a big problem for the general rep-
resentation of dependency relations, including ad-
junction and coordination. We will show that the
algorithm proposed here provides a nice solution for
this problem.
499
has
says now
he
attach
attach
packagesunion
adjoin
attach
attach
Figure 5: Predicate Adjunction
4.1 Representation of the LTAG Treebank
In the LTAG Treebank (Shen et al, 2008), each word
is associated with a spinal template, which repre-
sents the projection from the lexical item to the root.
Templates are linked together to form a derivation
tree. The topology of the derivation tree shows a
type of dependency relation, which we call LTAG
dependency here.
There are three types of operations in the LTAG
Treebank, which are attachment, adjunction, and co-
ordination. Attachment is used to represent both
substitution and sister adjunction in the traditional
LTAG. So it is similar to the dependency relation in
other approaches.
The LTAG dependency can be a non-projective
relation thanks to the operation of adjunction. In
the LTAG Treebank, raising verbs and passive ECM
verbs are represented as auxiliary trees to be ad-
joined. In addition, adjunction is used to handle
many cases of discontinuous arguments in Prop-
bank. For example, in the following sentence,
ARG1 of says in Propbank is discontinuous, which
is First Union now has packages for seven customer
groups.
? First Union, he says, now has packages for
seven customer groups.
In the LTAG Treebank, the subtree for he says ad-
joins onto the node of has, which is the root of the
derivation tree, as shown in Figure 5.
Another special aspect of the LTAG Treebank is
the representation of predicate coordination. Figure
6 is the representation of the following sentence.
? I couldn?t resist rearing up on my soggy loafers
and saluting.
The coordination between rearing and saluting is
represented explicitly with a coord-structure, and
resist
rearing saluting
and
I
attach
attach attach
coordination
Figure 6: Predicate Coordination
continuedstock
pounded
amid
attach
adjoin
attach
Figure 7: Non-projective Adjunction
this coord-structure attaches to resist. It is shown
in (Shen et al, 2008) that coord-structures could en-
code the ambiguity of argument sharing, which can
be non-projective also.
4.2 Incremental Construction
We build LTAG derivation trees incrementally. A
hypothesis of a fragment is represented with a par-
tial derivation tree. When the fragment hypotheses
of two nearby fragments combine, the partial deriva-
tion trees are combined into one.
It is trivial to combine two partial derivation trees
with attachment. We simply attach the root of one
tree to some node on the other tree which is visible to
this root node. Adjunction is similar to attachment,
except that an adjoined subtree may be visible from
the other side of the derivation tree. For example, in
sentence
? The stock of UAL Corp. continued to be
pounded amid signs that British Airways ...
continued adjoins onto pounded, and amid attaches
to continued from the other side of the derivation
tree (pounded is between continued and amid), as
shown in Figure 7.
The predicate coordination is decomposed into a
set of operations to meet the need for incremen-
tal processing. Suppose a coordinated structure at-
taches to the parent node on the left side. We build
this structure incrementally by attaching the first
500
resist
rearing saluting
and
I
attach
attach
attach
conjoin
Figure 8: Conjunction
?
$
?
+
  	
@@R
.
.
.
.
.
.
.
.
.
..
.
.
.
.
.
.fl
QQs
@@R @@R
m1.1
m1.1.1
s1 s2
m1
m1.2
m
mr
m2 s
attach
Figure 9: Representation of nodes
conjunct to the parent and conjoining other con-
juncts to first one. In this way, we do not need to
force the coordination to be built before the attach-
ment. Either can be executed first. A sample is
shown in Figure 8.
4.3 Features
In this section, we will describe the features used in
LTAG dependency parsing. An operation is repre-
sented by a 4-tuple
? op = (type, dir, posleft, posright),
where type ? {attach, adjoin, conjoin} and dir
is used to represent the direction of the operation.
posleft and posright are the POS tags of the two
operands.
Features are defined on POS tags and lexical items
of the nodes in the context. In order to represent the
features, we use m for the main-node of the oper-
ation, s for the sub-node, mr for the parent of the
main-node, m1..mi for the children of m, and s1..sj
for the children of s, as shown in Figure 9. The in-
dex always starts from the side where the operation
takes place. We use the Gorn addresses to represent
the nodes in the subtrees rooted on m and s.
Furthermore, we use lk and rk to represent the
nodes in the left and right context of the flat sen-
tence. We use hl and hr to represent the head of the
hypothesis trees on the left and right context respec-
tively. Let x be a node. We use x.p to represent the
POS tag of node x, and x.w to represent the lexical
item of node x.
Table 1 show the features used in LTAG depen-
dency parsing. There are seven classes of features.
The first three classes of features are those defined
on only one operand, on both operands, and on the
siblings respectively. If gold standard POS tags are
used as input, we define features on the POS tags in
the context. If level-1 dependency is used, we define
features on the root node of the hypothesis partial
derivation trees in the neighborhood.
Half check and full check features are designed
for grammatical check. For example, in Figure 9,
node s attaches onto node m from left. Then nothing
can attach onto s from the right side. The children of
the right side of s are fixed, so we use the half check
features to check the completeness of the children
of the right half for s. Furthermore, we notice that
all the rightmost descendants of s and the leftmost
descendants of m at each level become unavailable
for any further operation. So their children are fixed
after this operation. All these nodes are in the form
of m1.1...1 or s1.1...1. We use full check features to
check the children from both sides for these nodes.
In the discussion above, we ignored adjunction
and conjunction. We need to slightly refine the con-
ditions of checking. Due to the limit of space, we
skip these cases.
5 Experiments
We use the same data set as in (Shen and Joshi,
2005). We use Sec. 2-21 of the LTAG Treebank for
training, Sec. 22 for feature selection, and Sec. 23
for test. Table 2 shows the comparison of different
models. Beam size is set to five in our experiments.
With level-0 dependency, our system achieves an ac-
curacy of 90.3% at the speed of 4.25 sentences a sec-
ond on a Xeon 3G Hz processor with JDK 1.5. With
level-1 dependency, the parser achieves 90.5% at
3.59 sentences a second. Level-1 dependency does
not provide much improvement due to the fact that
level-0 features provide most of the useful informa-
tion for this specific application.
It is interesting to compare our system with other
dependency parsers. The accuracy on LTAG depen-
501
category description templates
one operand Features defined on only one operand. For each
template tp, [type, dir, tp] is used as a feature.
(m.p), (m.w), (m.p,m.w), (s.p),
(s.w), (s.p, s.w)
two operands Features defined on both operands. For each tem-
plate tp, [op, tp] is used as a feature. In addition,
[op] is also used as a feature.
(m.w), (s.w), (m.w, s.w)
siblings Features defined on the children of the
main nodes. For each template tp,
[op, tp], [op,m.w, tp], [op,mr.p, tp] and
[op,mr.p,m.w, tp] are used as features.
(m1.p), (m1.p,m2.p), ..,
(m1.p,m2.p, ..,mi.p)
POS context In the case that gold standard POS tags are used
as input, features are defined on the POS tags of
the context. For each template tp, [op, tp] is used
as a feature.
(l2.p), (l1.p), (r1.p), (r2.p),
(l2.p, l1.p), (l1.p, r1.p),
(r1.p, r2.p)
tree context In the case that level-1 dependency is employed,
features are defined on the trees in the context.
For each template tp, [op, tp] is used as a feature.
(hl.p), (hr.p)
half check Suppose s1, ..., sk are all the children of s which
are between s and m in the flat sentence. For
each template tp, [tp] is used as a feature.
(s.p, s1.p, s2.p, .., sk.p),
(m.p, s.p, s1.p, s2.p, .., sk.p)
and (s.w, s.p, s1.p, s2.p, .., sk.p),
(s.w,m.p, s.p, s1.p, s2.p, .., sk.p)
if s.w is a verb
full check Let x1, x2, .., xk be the children of x, and xr
the parent of x. For any x = m1.1...1 or s1.1...1,
template tp, [tp(x)] is used as a feature.
(x.p, x1.p, x2.p, .., xk.p),
(xr.p, x.p, x1.p, x2.p, .., xk.p) and
(x.w, x.p, x1.p, x2.p, .., xk.p),
(x.w, xr.p, x.p, x1.p, x2.p, .., xk.p)
if x.w is a verb
Table 1: Features defined on the context of operation
model accuracy%
Shen and Joshi, 2005 89.3
level-0 dependency 90.3
level-1 dependency 90.5
Table 2: Experiments on Sec. 23 of the LTAG Treebank
dency is comparable to the numbers of the previ-
ous best systems on dependency extracted from PTB
with Magerman?s rules, for example, 90.3% in (Ya-
mada and Matsumoto, 2003) and 90.9% in (McDon-
ald et al, 2005). However, their experiments are on
the PTB, while ours is on the LTAG corpus.
It should be noted that it is more difficult to learn
LTAG dependencies. Theoretically, the LTAG de-
pendencies reveal deeper relations. Adjunction can
lead to non-projective dependencies, and the depen-
dencies defined on predicate adjunction are linguis-
tically more motivated, as shown in the examples in
Figure 5 and 7. The explicit representation of predi-
cate coordination also provides deeper relations. For
example, in Figure 6, the LTAG dependency con-
tains resist ? rearing and resist ? saluting,
while the Magerman?s dependency only contains
resist ? rearing. The explicit representation of
predicate coordination will help to solve for the de-
pendencies for shared arguments.
6 Discussion
In our approach, each fragment in the graph is asso-
ciated with a hidden structure, which means that we
cannot reduce it to a labelling task. Therefore, the
problem of interest to us is different from previous
502
work on graphical models, such as CRF (Lafferty et
al., 2001) and MMMN (Taskar et al, 2003).
McAllester et al (2004) introduced Case-Factor
Diagram (CFD) to transform a graph based con-
struction problem to a labeling problem. However,
adjunction, prediction coordination, and long dis-
tance dependencies in LTAG dependency parsing
make it difficult to implement. Our approach pro-
vides a novel alternative to CFD.
Our learning algorithm stems from Perceptron
training in (Collins, 2002). Variants of this method
have been successfully used in many NLP tasks, like
shallow processing (Daume? III and Marcu, 2005),
parsing (Collins and Roark, 2004; Shen and Joshi,
2005) and word alignment (Moore, 2005). Theoret-
ical justification for those algorithms can be applied
to our training algorithm in a similar way.
In our algorithm, dependency is defined on com-
plicated hidden structures instead of on a graph.
Thus long distance dependency in a graph becomes
local in hidden structures, which is desirable from
linguistic considerations.
The search strategy of our bidirectional depen-
dency parser is similar to that of the bidirectional
CFG parser in (Satta and Stock, 1994; Ageno and
Rodrguez, 2001; Kay, 1989). A unique contribu-
tion of this paper is that selection of path and deci-
sions about action are trained simultaneously with
discriminative learning. In this way, we can employ
context information more effectively.
7 Conclusion
In this paper, we introduced bidirectional incremen-
tal parsing, a new architecture of parsing. We pro-
posed a novel algorithm for graph-based incremen-
tal construction, and applied this algorithm to LTAG
dependency parsing, revealing deep relations, which
are unavailable in other approaches and difficult to
learn. We evaluated the parser on an LTAG Tree-
bank. Experimental results showed significant im-
provement over the previous best system. Incre-
mental construction can be applied to other structure
learning problems of high computational complex-
ity, for example, such as machine translation and se-
mantic parsing.
References
A. Ageno and H. Rodrguez. 2001. Probabilistic mod-
elling of island-driven parsing. In International Work-
shop on Parsing Technologies.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of the 1st Meeting of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics.
M. Collins and B. Roark. 2004. Incremental parsing with
the perceptron algorithm. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics (ACL).
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proceedings of the 2002
Conference of Empirical Methods in Natural Lan-
guage Processing.
H. Daume? III and D. Marcu. 2005. Learning as search
optimization: Approximate large margin methods for
structured prediction. In Proceedings of the 22nd In-
ternational Conference on Machine Learning.
Y. Freund and R. E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37(3):277?296.
M. Johnson. 1998. PCFG Models of Linguistic Tree
Representations. Computational Linguistics, 24(4).
A. K. Joshi and Y. Schabes. 1997. Tree-adjoining
grammars. In G. Rozenberg and A. Salomaa, editors,
Handbook of Formal Languages, volume 3, pages 69
? 124. Springer-Verlag.
M. Kay. 1989. Head-driven parsing. In Proceedings of
Workshop on Parsing Technologies.
W. Krauth and M. Me?zard. 1987. Learning algorithms
with optimal stability in neural networks. Journal of
Physics A, 20:745?752.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional random fields: Probabilistic models for segmen-
tation and labeling sequence data. In Proceedings of
the 18th International Conference on Machine Learn-
ing.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
D. McAllester, M. Collins, and F. Pereira. 2004. Case-
factor diagrams for structured probabilistic modeling.
In UAI 2004.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Pro-
ceedings of the 43th Annual Meeting of the Association
for Computational Linguistics (ACL).
503
R. Moore. 2005. A discriminative framework for bilin-
gual word alignment. In Proceedings of Human Lan-
guage Technology Conference and Conference on Em-
pirical Methods in Natural Language Processing.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1).
G. Satta and O. Stock. 1994. Bi-Directional Context-
Free Grammar Parsing for Natural Language Process-
ing. Artificial Intelligence, 69(1-2).
L. Shen and A. K. Joshi. 2005. Incremental LTAG Pars-
ing. In Proceedings of Human Language Technology
Conference and Conference on Empirical Methods in
Natural Language Processing.
L. Shen, G. Satta, and A. K. Joshi. 2007. Guided Learn-
ing for Bidirectional Sequence Classification. In Pro-
ceedings of the 45th Annual Meeting of the Association
for Computational Linguistics (ACL).
L. Shen, L. Champollion, and A. K. Joshi. 2008. LTAG-
spinal and the Treebank: a new resource for incremen-
tal, dependency and semantic parsing. Language Re-
sources and Evaluation, 42(1):1?19.
L. Shen. 2006. Statistical LTAG Parsing. Ph.D. thesis,
University of Pennsylvania.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
markov networks. In Proceedings of the 17th Annual
Conference Neural Information Processing Systems.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with Support Vector Machines. In
IWPT 2003.
504
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 811?818, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Incremental LTAG Parsing
Libin Shen and Aravind K. Joshi
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104, USA
 
libin,joshi  @linc.cis.upenn.edu
Abstract
We present a very efficient statistical in-
cremental parser for LTAG-spinal, a vari-
ant of LTAG. The parser supports the
full adjoining operation, dynamic predi-
cate coordination, and non-projective de-
pendencies, with a formalism of provably
stronger generative capacity as compared
to CFG. Using gold standard POS tags as
input, on section 23 of the PTB, the parser
achieves an f-score of 89.3% for syntactic
dependency defined on LTAG derivation
trees, which are deeper than the dependen-
cies extracted from PTB alone with head
rules (for example, in Magerman?s style).
1 Introduction
Lexicalized Tree Adjoining Grammar (LTAG) is a
formalism motived by both linguistic and computa-
tional perspectives (for a relatively recent review, see
(Joshi and Schabes, 1997)). Because of the intro-
duction of the adjoining operation, the TAG formal-
ism is provably stronger than Context Free Gram-
mar (CFG) both in the weak and the strong genera-
tive power. The TAG formalism provides linguisti-
cally attractive analysis of natural language (Frank,
2002). Recent psycholinguistic experiments (Sturt
and Lombardo, 2005) demonstrate that the adjoining
operation of LTAG is required for eager incremental
processing.
Vijay-Shanker and Joshi (1985) introduced the
first TAG parser in a CYK-like algorithm. Because
of the adjoining operation, the time complexity of
LTAG parsing is as large as 
	 , compared with
	 of CFG parsing, where  is the length of
the sentence to be parsed. Many LTAG parsers
were proposed, such as the head-driven Earley style
parser (Lavelli and Satta, 1991) and the head-corner
parser (van Noord, 1994). The high time complexity
prevents LTAG parsing from real-time applications.
In this paper, we work on LTAG-spinal (Shen and
Joshi, 2005), an interesting subset of LTAG, which
preserves almost all of the strong generative power
of LTAG, and it is both weakly and strongly more
powerful than CFG 1. We will present a statistical
incremental parsing for LTAG-spinal. As far as we
know, this parser is the first comprehensive attempt
of efficient statistical parsing with a formal grammar
with provably stronger generative power than CFG,
supporting the full adjoining operation, dynamic
predicate coordination, as well as non-projective de-
pendencies 2.
2 LTAG-spinal and the Treebank
We first briefly describe the LTAG-spinal formalism
and the LTAG-spinal treebank to be used in this pa-
per. More details are reported in (Shen and Joshi,
2005).
In LTAG-spinal, we have two different kinds of
elementary trees, initial trees and auxiliary trees,
which are the same as in LTAG. However, as the
name implies, an initial tree in LTAG-spinal only
contains the spine from the root to the anchor, and
an auxiliary tree only contains the spine and the foot
node directly connected to a node on the spine.
Three types of operations are used to connect the
elementary trees into a derivation tree, which are
attachment, adjunction and conjunction. We show
LTAG-spinal elementary trees and operations with
an example in Figure 1.
In Figure 1, each arc is associated with a character
which represents the type of operation. We use T for
attach, A for adjoin, and C for conjoin.
1Further formal results are described in (Shen and Joshi,
2005). There is also some relationship of LTAG-spinal to the
spinal form context-free tree grammar, as in (Fujiyoshi and Ka-
sai, 2000)
2In (Riezler et al, 2002), the MaxEnt model was used to
rerank the K-best parses generated by a rule-based LFG parser.
811
JJ
VP
S
JJ
VP
S
TO
CC
VP *VBZNNDT
XP
XP
WDT PRP
interestingnew andseemswhichparsera meto
T
T
T
T
T
A
T
C
VP XPXP XPXP
Figure 1: An example in LTAG-spinal. A=adjoin, T=attach, C=conjoin.
Attachment in LTAG-spinal is similar to sister
adjunction (Chiang, 2000) in Tree Insertion Gram-
mar (TIG) (Schabes and Waters, 1995). It represents
a combination of substitution and sister adjunction.
The attachment operation is designed to encode the
ambiguity of an argument and an adjunct.
Adjunction inserts part of the spine and the foot
node of an auxiliary tree into to the spine of another
tree. The adjunction operation can effectively do
wrapping, which distinguishes itself from sister ad-
junction. It is not difficult to see that adjunction only
happens on the spine of a tree. This property will be
exploited in the incremental parser.
Conjunction is similar to what was originally
proposed in (Sarkar and Joshi, 1996). However, in
LTAG-spinal, the conjunction operation is much eas-
ier to handle, since we only conjoin spinal elemen-
tary trees and we do not need to enumerate contrac-
tion sets for conjunction. In our formalization, con-
junction can be treated as a special adjunction, how-
ever, this is beyond the scope of this paper.
We use the LTAG-spinal treebank described in
(Shen and Joshi, 2005), which was extracted from
the Penn Treebank (PTB) (Marcus et al, 1994) with
Propbank (Palmer et al, 2005) annotations.
2.1 Relation to Traditional LTAG
LTAG-spinal preserves most of the strong genera-
tive power of LTAG. It can be shown that LTAG-
spinal with adjoining restrictions (Joshi and Sch-
abes, 1997) has stronger generative capacity as com-
pared to CFG. For example, there exists an LTAG-
spinal grammar that generates 
ffProceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 899?906, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Measuring the relative compositionality of verb-noun (V-N) collocations by
integrating features
Sriram Venkatapathy
 
Language Technologies Research Centre,
International Institute of Information
Technology - Hyderabad,
Hyderabad, India.
sriram@research.iiit.ac.in
Aravind K. Joshi
Department of Computer and
Information Science and Institute for
Research in Cognitive Science,
University of Pennsylvania,
Philadelphia, PA, USA.
joshi@linc.cis.upenn.edu
Abstract
Measuring the relative compositionality
of Multi-word Expressions (MWEs) is
crucial to Natural Language Processing.
Various collocation based measures have
been proposed to compute the relative
compositionality of MWEs. In this paper,
we define novel measures (both colloca-
tion based and context based measures) to
measure the relative compositionality of
MWEs of V-N type. We show that the
correlation of these features with the hu-
man ranking is much superior to the cor-
relation of the traditional features with the
human ranking. We then integrate the pro-
posed features and the traditional features
using a SVM based ranking function to
rank the collocations of V-N type based
on their relative compositionality. We
then show that the correlation between the
ranks computed by the SVM based rank-
ing function and human ranking is signif-
icantly better than the correlation between
ranking of individual features and human
ranking.
1 Introduction
The main goal of the work presented in this paper
is to examine the relative compositionality of col-
1Part of the work was done at Institute for Research in Cog-
nitive Science (IRCS), University of Pennsylvania, Philadel-
phia, PA 19104, USA, when he was visiting IRCS as a Visiting
Scholar, February to December, 2004.
locations of V-N type using a SVM based ranking
function. Measuring the relative compositionality of
V-N collocations is extremely helpful in applications
such as machine translation where the collocations
that are highly non-compositional can be handled in
a special way (Schuler and Joshi, 2004) (Hwang
and Sasaki, 2005).
Multi-word expressions (MWEs) are those whose
structure and meaning cannot be derived from their
component words, as they occur independently.
Examples include conjunctions like ?as well as?
(meaning ?including?), idioms like ?kick the bucket?
(meaning ?die?), phrasal verbs like ?find out? (mean-
ing ?search?) and compounds like ?village commu-
nity?. A typical natural language system assumes
each word to be a lexical unit, but this assumption
does not hold in case of MWEs (Becker, 1975)
(Fillmore, 2003). They have idiosyncratic interpre-
tations which cross word boundaries and hence are
a ?pain in the neck? (Sag et al, 2002). They account
for a large portion of the language used in day-to-
day interactions (Schuler and Joshi, 2004) and so,
handling them becomes an important task.
A large number of MWEs have a standard syn-
tactic structure but are non-compositional semanti-
cally. An example of such a subset is the class of
non-compositional verb-noun collocations (V-N col-
locations). The class of non-compositional V-N col-
locations is important because they are used very
frequently. These include verbal idioms (Nunberg
et al, 1994), support-verb constructions (Abeille,
1988), (Akimoto, 1989), among others. The ex-
pression ?take place? is a MWE whereas ?take a gift?
is not a MWE.
899
It is well known that one cannot really make a
binary distinction between compositional and non-
compositional MWEs. They do not fall cleanly into
mutually exclusive classes, but populate the con-
tinuum between the two extremes (Bannard et al,
2003). So, we rate the MWEs (V-N collocations in
this paper) on a scale from 1 to 6 where 6 denotes
a completely compositional expression, while 1 de-
notes a completely opaque expression.
Various statistical measures have been suggested
for ranking expressions based on their composition-
ality. Some of these are Frequency, Mutual Infor-
mation (Church and Hanks, 1989) , distributed fre-
quency of object (Tapanainen et al, 1998) and LSA
model (Baldwin et al, 2003) (Schutze, 1998). In
this paper, we define novel measures (both collo-
cation based and context based measures) to mea-
sure the relative compositionality of MWEs of V-N
type (see section 6 for details). Integrating these sta-
tistical measures should provide better evidence for
ranking the expressions. We use a SVM based rank-
ing function to integrate the features and rank the
V-N collocations according to their compositional-
ity. We then compare these ranks with the ranks
provided by the human judge. A similar compari-
son between the ranks according to Latent-Semantic
Analysis (LSA) based features and the ranks of hu-
man judges has been made by McCarthy, Keller and
Caroll (McCarthy et al, 2003) for verb-particle con-
structions. (See Section 3 for more details). Some
preliminary work on recognition of V-N collocations
was presented in (Venkatapathy and Joshi, 2004).
We show that the measures which we have defined
contribute greatly to measuring the relative compo-
sitionality of V-N collocations when compared to the
traditional features. We also show that the ranks as-
signed by the SVM based ranking function corre-
lated much better with the human judgement that the
ranks assigned by individual statistical measures.
This paper is organized in the following sections
(1) Basic Architecture, (2) Related work, (3) Data
used for the experiments, (4) Agreement between
the Judges, (5) Features, (6) SVM based ranking
function, (7) Experiments & Results, and (8) Con-
clusion.
2 Basic Architecture
Every V-N collocation is represented as a vector of
features which are composed largely of various sta-
tistical measures. The values of these features for
the V-N collocations are extracted from the British
National Corpus. For example, the V-N collocation
?raise an eyebrow? can be represented as
 
Frequency = 271, Mutual Information = 8.43, Dis-
tributed frequency of object = 1456.29, etc.  . A
SVM based ranking function uses these features to
rank the V-N collocations based on their relative
compositionality. These ranks are then compared
with the human ranking.
3 Related Work
(Breidt, 1995) has evaluated the usefulness of the
Point-wise Mutual Information measure (as sug-
gested by (Church and Hanks, 1989)) for the ex-
traction of V-N collocations from German text cor-
pora. Several other measures like Log-Likelihood
(Dunning, 1993), Pearson?s  (Church et al,
1991), Z-Score (Church et al, 1991) , Cubic As-
sociation Ratio (MI3), etc., have been also pro-
posed. These measures try to quantify the associ-
ation of two words but do not talk about quantify-
ing the non-compositionality of MWEs. Dekang Lin
proposes a way to automatically identify the non-
compositionality of MWEs (Lin, 1999). He sug-
gests that a possible way to separate compositional
phrases from non-compositional ones is to check the
existence and mutual-information values of phrases
obtained by replacing one of the words with a sim-
ilar word. According to Lin, a phrase is proba-
bly non-compositional if such substitutions are not
found in the collocations database or their mutual
information values are significantly different from
that of the phrase. Another way of determining the
non-compositionality of V-N collocations is by us-
ing ?distributed frequency of object? (DFO) in V-N
collocations (Tapanainen et al, 1998). The basic
idea in there is that ?if an object appears only with
one verb (or few verbs) in a large corpus we expect
that it has an idiomatic nature? (Tapanainen et al,
1998).
Schone and Jurafsky (Schone and Jurafsky, 2001)
applied Latent-Semantic Analysis (LSA) to the anal-
ysis of MWEs in the task of MWE discovery, by way
900
of rescoring MWEs extracted from the corpus. An
interesting way of quantifying the relative composi-
tionality of a MWE is proposed by Baldwin, Ban-
nard, Tanaka and Widdows (Baldwin et al, 2003).
They use LSA to determine the similarity between
an MWE and its constituent words, and claim that
higher similarity indicates great decomposability. In
terms of compositionality, an expression is likely
to be relatively more compositional if it is decom-
posable. They evaluate their model on English NN
compounds and verb-particles, and showed that the
model correlated moderately well with the Word-
net based decomposability theory (Baldwin et al,
2003).
McCarthy, Keller and Caroll (McCarthy et al,
2003) judge compositionality according to the de-
gree of overlap in the set of most similar words to
the verb-particle and head verb. They showed that
the correlation between their measures and the hu-
man ranking was better than the correlation between
the statistical features and the human ranking. We
have done similar experiments in this paper where
we compare the correlation value of the ranks pro-
vided by the SVM based ranking function with the
ranks of the individual features for the V-N collo-
cations. We show that the ranks given by the SVM
based ranking function which integrates all the fea-
tures provides a significantly better correlation than
the individual features.
4 Data used for the experiments
The data used for the experiments is British Na-
tional Corpus of 81 million words. The corpus is
parsed using Bikel?s parser (Bikel, 2004) and the
Verb-Object Collocations are extracted. There are
4,775,697 V-N collocations of which 1.2 million are
unique. All the V-N collocations above the fre-
quency of 100 (n=4405) are taken to conduct the ex-
periments so that the evaluation of the system is fea-
sible. These 4405 V-N collocations were searched in
Wordnet, American Heritage Dictionary and SAID
dictionary (LDC,2003). Around 400 were found in
at least one of the dictionaries. Another 400 were
extracted from the rest so that the evaluation set has
roughly equal number of compositional and non-
compositional expressions. These 800 expressions
were annotated with a rating from 1 to 6 by us-
ing guidelines independently developed by the au-
thors. 1 denotes the expressions which are totally
non-compositional while 6 denotes the expressions
which are totally compositional. The brief expla-
nation of the various ratings is as follows: (1) No
word in the expression has any relation to the ac-
tual meaning of the expression. Example : ?leave a
mark?. (2) Can be replaced by a single verb. Ex-
ample : ?take a look?. (3) Although meanings of
both words are involved, at least one of the words
is not used in the usual sense. Example : ?break
news?. (4) Relatively more compositional than (3).
Example : ?prove a point?. (5) Relatively less com-
positional than (6). Example : ?feel safe?. (6) Com-
pletely compositional. Example : ?drink coffee?.
5 Agreement between the Judges
The data was annotated by two fluent speakers of
English. For 765 collocations out of 800, both the
annotators gave a rating. For the rest, at least one
of the annotators marked the collocations as ?don?t
know?. Table 1 illustrates the details of the annota-
tions provided by the two judges.
Ratings 6 5 4 3 2 1
Annotator1 141 122 127 119 161 95
Annotator2 303 88 79 101 118 76
Table 1: Details of the annotations of the two anno-
tators
From the table 1 we see that annotator1 dis-
tributed the rating more uniformly among all the
collocations while annotator2 observed that a sig-
nificant proportion of the collocations were com-
pletely compositional. To measure the agreement
between the two annotators, we used the Kendall?s
TAU (   ) (Siegel and Castellan, 1988).   is the cor-
relation between the rankings1 of collocations given
by the two annotators.   ranges between 0 (little
agreement) and 1 (full agreement).   is defined as,


	



	




	
Relative Compositionality of Multi-word
Expressions: A Study of Verb-Noun (V-N)
Collocations
Sriram Venkatapathy1, and Aravind K. Joshi2
1 Language Technologies Research Center,
International Institute of Information Technology - Hyderabad, Hyderabad, India
sriram@research.iiit.ac.in
2 Department of Computer and Information Science
and Institute of Research in Cognitive Science,
University of Pennsylvania, Philadelphia, PA, USA
joshi@linc.cis.upenn.edu
Abstract. Recognition of Multi-word Expressions (MWEs) and their
relative compositionality are crucial to Natural Language Processing.
Various statistical techniques have been proposed to recognize MWEs.
In this paper, we integrate all the existing statistical features and in-
vestigate a range of classifiers for their suitability for recognizing the
non-compositional Verb-Noun (V-N) collocations. In the task of ranking
the V-N collocations based on their relative compositionality, we show
that the correlation between the ranks computed by the classifier and hu-
man ranking is significantly better than the correlation between ranking
of individual features and human ranking. We also show that the prop-
erties ?Distributed frequency of object? (as defined in [27]) and ?Nearest
Mutual Information? (as adapted from [18]) contribute greatly to the
recognition of the non-compositional MWEs of the V-N type and to the
ranking of the V-N collocations based on their relative compositionality.
1 Introduction
The main goals of the work presented in this paper are (1) To investigate a range
of classifiers for their suitability in recognizing the non-compositional V-N collo-
cations, and (2) To examine the relative compositionality of collocations of V-N
type. Measuring the relative compositionality of V-N collocations is extremely
helpful in applications such as machine translation where the collocations that
are highly non-compositional can be handled in a special way.
Multi-word expressions (MWEs) are those whose structure and meaning can-
not be derived from their component words, as they occur independently. Ex-
amples include conjunctions like ?as well as? (meaning ?including?), idioms like
 Part of the work was done at Institute for Research in Cognitive Science, University
of Pennsylvania, Philadelphia, PA 19104, USA, when he was visiting IRCS as a
visiting Scholar, February to December, 2004.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 553?564, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
554 S. Venkatapathy and A.K. Joshi
?kick the bucket? (meaning ?die?), phrasal verbs like ?find out? (meaning ?search?)
and compounds like ?village community?. A typical natural language system as-
sumes each word to be a lexical unit, but this assumption does not hold in case
of MWEs [6] [12]. They have idiosyncratic interpretations which cross word
boundaries and hence are a ?pain in the neck? [23]. They account for a large
portion of the language used in day-to-day interactions [25] and so, handling
them becomes an important task.
A large number of MWEs have a standard syntactic structure but are non-
compositional semantically. An example of such a subset is the class of non-
compositional verb-noun collocations (V-N collocations). The class of V-N col-
locations which are non-compositional is important because they are used very
frequently. These include verbal idioms [22], support-verb constructions [1] [2]
etc. The expression ?take place? is a MWE whereas ?take a gift? is not a MWE.
It is well known that one cannot really make a binary distinction between
compositional and non-compositional MWEs. They do not fall cleanly into mu-
tually exclusive classes, but populate the continuum between the two extremes
[4]. So, we rate the MWEs (V-N collocations in this paper) on a scale from 1
to 6 where 6 denotes a completely compositional expression, while 1 denotes a
completely opaque expression. But, to address the problem of identification, we
still need to do an approximate binary distinction. We call the expressions with
a rating of 4 to 6 compositional and the expressions with rating of 1 to 3 as
non-compositional. (See Section 4 for further details).
Various statistical measures have been suggested for identification of MWEs
and ranking expressions based on their compositionality. Some of these are Fre-
quency, Mutual Information [9], Log-Likelihood [10] and Pearson?s ?2 [8].
Integrating all the statistical measures should provide better evidence for rec-
ognizing MWEs and ranking the expressions. We use various Machine Learning
Techniques (classifiers) to integrate these statistical features and classify the V-
N collocations as MWEs or Non-MWEs. We also use a classifier to rank the V-N
collocations according to their compositionality. We then compare these ranks
with the ranks provided by the human judge. A similar comparison between
the ranks according to Latent-Semantic Analysis (LSA) based features and the
ranks of human judges has been done by McCarthy, Keller and Caroll [19] for
verb-particle constructions. (See Section 3 for more details). Some preliminary
work on recognition of V-N collocations was presented in [28].
In the task of classification, we show that the technique of weighted features
in distance-weighted nearest-neighbour algorithm performs slightly better than
other machine learning techniques. We also find that the ?distributed frequency
of object (as defined by [27])? and ?nearest mutual information (as adapted
from [18])? are important indicators of the non-compositionality of MWEs. In
the task of ranking, we show that the ranks assigned by the classifier correlated
much better with the human judgement than the ranks assigned by individual
statistical measures.
This paper is organised in the following sections (2) Basic Architecture,
(3) Related work, (4) Data used for the experiments, (5) Agreement between
Relative Compositionality of Multi-word Expressions 555
the Judges, (6) Features, (7) Experiments - Classification, (8) Experiments -
Ranking and (9) Conclusion.
2 Basic Architecture
Recognition of MWEs can be regarded as a classification task where every V-N
collocation can be classified either as a MWE or as a Non-MWE. Every V-N
collocation is represented as a vector of features which are composed largely of
various statistical measures. The values of these features for the V-N collocations
are extracted from the British National Corpus. For example, the V-N collocation
?raise an eyebrow? can be represented as
[ Frequency = 271, Mutual Information = 8.43, Log-Likelihood = 1456.29, etc.].
Now, to recognise the MWEs, the classifier has to do a binary classification
of this vector. So, ideally, the classifier should take the above information and
classify ?raise an eyebrow? as an MWE. The classifier can also be used to rank
these vectors according to their relative compositionality.
3 Related Work
Church and Hanks (1989) proposed a measure of association called Mutual In-
formation [9]. Mutual Information (MI) is the logarithm of the ratio between
the probability of the two words occurring together and the product of the prob-
ability of each word occurring individually. The higher the MI, the more likely
are the words to be associated with each other. The usefulness of the statistical
approach suggested by Church and Hanks [9] is evaluated for the extraction
of V-N collocations from German text Corpora [7]. Several other measures like
Log-Likelihood [10], Pearson?s ?2 [8], Z-Score [8] , Cubic Association Ratio
(MI3), Log-Log [17], etc., have been proposed. These measures try to quan-
tify the association of the two words but do not talk about quantifying the
non-compositionality of MWEs. Dekang Lin proposes a way to automatically
identify the non-compositionality of MWEs [18]. He suggests that a possible
way to separate compositional phrases from non-compositional ones is to check
the existence and mutual-information values of phrases obtained by replacing
one of the words with a similar word. According to Lin, a phrase is proba-
bly non-compositional if such substitutions are not found in the collocations
database or their mutual information values are significantly different from that
of the phrase. Another way of determining the non-compositionality of V-N col-
locations is by using ?distributed frequency of object?(DFO) in V-N collocations
[27]. The basic idea in there is that ?if an object appears only with one verb (or
few verbs) in a large corpus we expect that it has an idiomatic nature? [27].
Schone and Jurafsky [24] applied Latent-Semantic Analysis (LSA) to the
analysis of MWEs in the task of MWE discovery, by way of rescoring MWEs
extracted from the corpus. An interesting way of quantifying the relative com-
positionality of a MWE is proposed by Baldwin, Bannard, Tanaka and Widdows
[3]. They use latent semantic analysis (LSA) to determine the similarity between
556 S. Venkatapathy and A.K. Joshi
an MWE and its constituent words, and claim that higher similarity indicates
great decomposability. In terms of compositionality, an expression is likely to be
relatively more compositional if it is decomposable. They evaluate their model
on English NN compounds and verb-particles, and showed that the model cor-
related moderately well with the Wordnet based decomposibility theory [3].
Evert and Krenn [11] compare some of the existing statistical features for
the recognition of MWEs of adjective-noun and preposition-noun-verb types.
Galiano, Valdivia, Santiago and Lopez [14] use five statistical measures to clas-
sify generic MWEs using the LVQ (Learning Vector Quantization) algorithm. In
contrast, we do a more detailed and focussed study of V-N collocations and the
ability of various classifiers in recognizing MWEs. We also compare the roles of
various features in this task.
McCarthy, Keller and Caroll [19] judge compositionality according to the
degree of overlap in the set of most similar words to the verb-particle and head
verb. They showed that the correlation between their measures and the human
ranking was better than the correlation between the statistical features and
the human ranking. We have done similar experiments in this paper where we
compare the correlation value of the ranks provided by the classifier with the
ranks of the individual features for the V-N collocations. We show that the ranks
given by the classifier which integrates all the features provides a significantly
better correlation than the individual features.
4 Data Used for the Experiments
The data used for the experiments is British National Corpus of 81 million words.
The corpus is parsed using Bikel?s parser [5] and the Verb-Object Collocations
are extracted. There are 4,775,697 V-N of which 1.2 million were unique. All
the V-N collocations above the frequency of 100 (n=4405) are taken to conduct
the experiments so that the evaluation of the system is feasible. These 4405
V-N collocations were searched in Wordnet, American Heritage Dictionary and
SAID dictionary (LDC,2003). Around 400 were found in at least one of the dic-
tionaries. Another 400 were extracted from the rest so that the evaluation set
has roughly equal number of compositional and non-compositional expressions.
These 800 expressions were annotated with a rating from 1 to 6 by using guide-
lines independently developed by the authors. 1 denotes the expressions which
are totally non-compositional while 6 denotes the expressions which are totally
compositional. The brief explanation of the various rating are (1) No word in
the expression has any relation to the actual meaning of the expression. Exam-
ple: ?leave a mark?. (2) Can be replaced by a single verb. Example : ?take
a look?. (3) Although meanings of both words are involved, at least one of the
words is not used in the usual sense. Example : ?break news?. (4) Relatively
more compositional than (3). Example : ?prove a point?. (5) Relatively less
compositional than (6). Example : ?feel safe?. (6) Completely compositional.
Example : ?drink coffee?. For the experiments on classification (Section 7), we
call the expressions with ratings of 4 to 6 as compositional and the expressions
Relative Compositionality of Multi-word Expressions 557
with rating of 1 to 3 as non-compositional. For the experiments on ranking the
expressions based on their relative compositionality, we use all the 6 ratings to
represent the relative compositionality of these expressions.
5 Agreement Between the Judges
The data was annotated by two fluent speakers of English. For 765 collocations
out of 800, both the annotators gave a rating. For the rest, atleast one of the an-
notators marked the collocations as ?don?t know?. Table 1 illustrates the details
of the annotations provided by the two judges.
Table 1. Details of the annotations of the two annotators
Ratings 6 5 4 3 2 1 Compositional Non-Compositional
(4 to 6) (1 to 3)
Annotator1 141 122 127 119 161 95 390 375
Annotator2 303 88 79 101 118 76 470 195
From the table we see that annotator1 distributed the rating more uniformly
among all the collocations while annotator2 observed that a significant propor-
tion of the collocations were completely compositional. To measure the agree-
ment between the two annotators, we used the Kendall?s TAU (?). ? is the
correlation between the rankings1 of collocations given by the two annotators.
W ranges between 0 (little agreement) and 1 (full agreement). W is calculated
as below,
? =
?
i<j sgn(xi ? xj)sgn(yi ? yj)
?
(T0 ? T1)(T0 ? T2)
where T0 = n(n ? 1)/2, T1 =
?
ti(ti ? 1)/2, T2 =
?
ui(ui ? 1)/2 and where,
n is the number of collocations, ti is the number of tied x values of ith group of
tied x values and ui is the number of tied y values of ith group of tied y values.
We obtained a ? score of 0.61 which is highly significant. This shows that the
annotators were in a good agreement with each other in deciding the rating to
be given to the collocations. We also compare the ranking of the two annotators
using Spearman?s Rank-Correlation coefficient (rs) (more details in section 8).
We obtained a rs score of 0.71 indicating a good agreement between the an-
notators. A couple of examples where the annotators differed are (1) ?perform
a task? was rated 3 by annotator1 while it was rated 6 by annotator2 and (2)
?pay tribute? was rated 1 by annotator1 while it was rated 4 by annotator2.
The 765 samples annotated by both the annotators were then divided into a
training set and a testing set in several possible ways to cross-validate the results
of classification and ranking.
1 Computed from the ratings.
558 S. Venkatapathy and A.K. Joshi
6 Features
Each collocation is represented by a vector whose dimensions are the statistical
features obtained from the British National Corpus. This list of features are given
in Table 2.2 While conducting the experiments, all features are scaled from 0 to
1 to ensure that all features are represented uniformly.
Table 2. List of features and their top-3 example collocations
Feature Top-3 Feature Top-3
take place Mutual Information shrug shoulder
Frequency have effect [9] bridge gap
have time plead guilty
Cubic Association take place Log-Log shake head
Measure shake head [17] commit suicide
(Oakes, 1998) play role fall asleep
Log-Likelihood take place Pearson?s ?2 shake head
[10] shake head [8] commit suicide
play role fall asleep
T-Score take place Z-Score shake head
[9] have effect [26] commit suicide
shake head fall asleep
?-coefficient bridge gap Distributed come true
shrug shoulder freq. of object become difficult
press button (DFO) make sure
[27]
Nearest MI Collocations Whether object (Binary feature)
(NMI) with no can occur
[18] neigh. MI as a verb
Whether object (Binary feature)
is a nomin.
of some verb
7 Experiments - Classification
The evaluation data (765 vectors) is divided randomly into training and testing
vectors in 10 ways for cross-validation. The training data consists of 90% of 786
vectors and the testing data consists of the remaining.
We used various Machine Learning techinques to classify the V-N colloca-
tions into MWEs and non-MWEs. For every classifier, we calculated the average
accuracy of all the test sets of each of the annotators. We then compare the aver-
age accuracies of all the classifiers. We found that the classifier that we used, the
technique of weighted features in distance-weighted nearest-algorithm, performs
somewhat better than other machine learning techniques.
The following are brief descriptions of the classifiers that we used in this
paper.
2 The formulas of features are not given due to lack of space.
Relative Compositionality of Multi-word Expressions 559
7.1 Nearest-Neighbour Algorithm
This is an instance-based learning technique where the test vector is classified
based on its nearest vectors in the training data. The simple distance between
two vectors xi and xj is defined as d(xi,xj), where
d(xi, xj) =
?
?
?
?
n
?
r=1
(ar(xi) ? ar(xj))2.
Here, x is an instance of a vector and ar(x) is the value of the rth feature.
One can use K neighbours to judge the class of the test vector. The test
vector is assigned the class of maximum number of neighbours. This can be
furthur modified by calculating the inverse weighted distance between the test
vector and the neighbouring training vectors in each of the classes. The test
vector is then assigned the class which has the higher inverse-weighted distance.
One can also use all the training vectors and the weighted-distance principle to
classify the test vector.
The average classification accuracy of each of the above methods on the test
sets of each of the annotators is shown in Table 3.
Table 3. Average accuracies of MWE recognition using simple nearest-neighbour
algorithms and weighted distance nearest neighbour algorithms
Simple K-Nearest neighbour Weighted-distance Nearest neighbour
Type K=1 K=2 K=3 K=1 K=2 K=3 K=All
Annot.1 62.35 61.31 62.48 62.35 62.35 62.61 66.66
Annot.2 57.64 54.10 60.89 57.64 57.64 60.37 63.52
7.2 SVM-Based Classifiers
SVMs [15] have been very successful in attaining high accuracy for various
machine-learning tasks. Unlike the error-driven algorithms (Perceptron etc.),
SVM searches for the two distinct classes and maximizes the margin between
two classes. Data of higher dimension can also be classified using the appropriate
Kernel. We used Linear and Polynomial Kernel (degree=2) to test the evaluation
data. We also used the radial-basis network in SVMs to compare the results
because of their proximity to the nearest-neigbour algorithms.
Table 4. Average accuracies of MWE recognition using SVMs (Linear, Polynomial
and Radial Basis Function Kernel)
Linear Ker. Polynomial Ker. Radial Basis networks
Parameters ? = 0.5 ? = 1.0 ? = 1.5 ? = 2.0
Annot.1 65.89 65.75 67.06 66.66 66.93 67.06
Annot.2 62.61 65.09 64.17 63.51 62.99 62.99
560 S. Venkatapathy and A.K. Joshi
The average classification accuracy of each of the above methods on the test
sets of each of the annotators is shown in Table 4.
7.3 Weighted Features in Distance-Weighted Nearest-Neighbour
Algorithm
Among all the features used, only a few might be very relevant to recognizing
the non-compositionality of the MWE. As a result, the distance metric used
by the nearest-neighbour algorithm which depends on all the features might
be misleading. The distance between the neighbour will be dominated by large
number of irrelevant features.
A way of overcoming this problem is to weight each feature differently when
calculating the distance between the two instances. This also gives us an insight
into which features are mainly responsible for recognizing the non-compositional-
ity of MWEs. The jth feature can be multiplied by the weight zj , where the values
of z1...zn are chosen to minimize the true classification error of the learning
algorithm [20]. The distance using these weights is represented as
d(xi, xj) =
?
?
?
?
n
?
r=1
(zr ? (ar(xi) ? ar(xj)))2,
where zr is the weight of the rth feature.
The values of z1...zn can be determined by cross-validation of the training
data. We use leave-one-out cross-validation [21], in which the set of m training
vectors are repeatedly divided into a training set of m-1 and a test set of 1,
in all possible ways. So, each vector in the training data is classified using the
remaining vectors. The classification accuracy is defined as
Clacc = 100 ? (
m
?
1
classify(i)/m)
where classify(i)=1, if the ith training example is classified correctly using the
distance-weighted nearest neighbour algorithm, otherwise classify(i)=0.
Now, we try to maximize the classification accuracy in the following way,
? In every iteration, vary the weights of the features one by one.
? Choose the feature and its weight which brings the maximum increase in the
value of Clacc. One can also choose the feature and its weight such that it
brings the minimum increase in the value of Clacc.
? Update the weight of this particular feature and go for the next iteration.
? If there is no increase in classification accuracy, stop.
When the weights are updated such that there is maximum increase in classi-
fication accuracy in every step, the average accuracies are 66.92% and 64.30%
on the test sets of the two annotators respectively. But when the weights are
updated such there is a minimum increase in classification accuracy at every
Relative Compositionality of Multi-word Expressions 561
Table 5. The top three features according to the average weight when there is maxi-
mum increase in Clacc at every step
Annotator1 Weight Annotator2 Weight
DFO 1.09 MI 1.17
T-Score 1.0 T-Score 1.1
Z-Score 1.0 ?-coefficient 1.0
Table 6. The top three features according to the average weight calculated when there
is minimum increase in Clacc at every step
Annot.1 Weight Annot.2 Weight
DFO 1.07 MI 2.06
NMI 1.02 T-Score 1.0
Log-Like. 0.97 ?-coefficient 1.0
step, the average accuracies are 66.13% and 64.04% on the test sets of the
two annotators respectively, which are slightly better than that obtained by the
other Machine Learning Techniques.
In the above two methods (Updating weights such that there is maximum or
minimum increase in classification accuracy), we add the weights of the features
of each of the evaluation sets. According to the average weights, the top three
features (having high average weight) are shown in Tables 5 and 6.
In both the above cases, we find that the properties ?Mutual-Information?
and the compositionality oriented feature ?Distributed Frequency of an Object?
performed significantly better than the other features.
8 Experiments - Ranking
All the statistical measures show that the expressions ranked higher according
to their decreasing values are more likely to be non-compositional. We compare
these ranks with the average of the ranks given by the annotator (obtained from
his rating). To compare, we use Spearman Rank-Order Correlation Coefficient
(rs), defined as
rs =
(Ri ? R?)(Si ? S?)
?
?
(Ri ? R?)2
?
(Si ? S?)2
where Ri is the rank of ith x value, Si is the rank of ith y value, R? is the mean
of the Ri values and S? is the mean of Si values.
We use an SVM-based ranking system [16] for our training. Here, we use
10% of the 765 vectors for training and the remaining for testing. The SVM-
based ranking system builds a preference matrix of the training vectors to learn.
It then ranks the test vectors. The ranking system takes a lot of time to train
itself, and hence, we decided to use only a small proportion of the evaluation set
for training.
562 S. Venkatapathy and A.K. Joshi
Table 7. The correlation values of the ranking of individual features and the ranking
of classifier with the ranking of human judgements
MI -0.125 Z-Score -0.059
MI3 0.001 ?-coeff -0.102
Log-Log -0.086 DFO -0.113
Log-Likelihood 0.005 NMI -0.167
?2 -0.056 Class. 0.388
T-Score 0.045
We also compare our ranks (the average of the ranks suggested by the clas-
sifier) with the gold standard using the Spearman Rank-Order Correlation Co-
efficient. The results are shown in Table 7.
In Table 7, we observe that the correlation between the ranks computed by
the classifier and human ranking is better than the correlation between ranking
of individual statistical features and human ranking.
We observe that among all the statistical features the ranks based on the
properties ?Mutual Information?, ?Distributed Frequency of an Object? [27] and
?Nearest mutual information? [18] correlated better with the ranks provided
by the annotator. This is in accordance with the observation we made while
describing the classification experiments, where we observed that the proper-
ties ?Distributed Frequency of an Object? and ?Mutual Information? contributed
much to the classification of the expressions. When we compare the correlation
values of MI, Log-likelihood and ?2, we see that the Mutual-Information values
correlated better. This result is similar to the observation made by McCarthy,
Keller and Caroll [19] for phrasal verbs.
9 Conclusion
In this paper, we integrated the statistical features using various classifiers and
investigated their suitability for recognising non-compositional MWEs of the V-
N type. We also used a classifier to rank the V-N collocations according to their
relative compositionality. This type of MWEs constitutes a very large percent-
age of all MWEs and are crucial for NLP applications, especially for Machine
Translation. Our main results are as follows.
? The technique of weighted features in distance-weighted nearest neighbour
algorithm performs better than other Machine Learning Techniques in the
task of recognition of MWEs of V-N type.
? We show that the correlation between the ranks computed by the classi-
fier and human ranking is significantly better than the correlation between
ranking of individual features and human ranking.
? The properties ?Distributed frequency of object? and ?Nearest MI? contribute
greatly to the recognition of the non-compositional MWEs of the V-N type
and to the ranking of the V-N collocations based on their relative composi-
tionality.
Relative Compositionality of Multi-word Expressions 563
Our future work will consist of the following tasks
? Evaluate the effectiveness of the techniques developed in this paper for ap-
plications like Machine Translation.
? Improve our annotation guidelines and create more annotated data.
? Extend our approach to other types of MWEs.
Acknowledgements
We want to thank Libin Shen and Nikhil Dinesh for their help in clarifying
various aspects of Machine Learning Techniques. We would like to thank Roder-
ick Saxey and Pranesh Bhargava for annotating the data and Mark Mandel for
considerable editorial help.
References
1. Abeille, Anne . Light verb constuctions and extraction out of NP in a tree adjoining
grammar. Papers of the 24th Regional Meeting of the Chicago Linguistics Society.
(1988)
2. Akimoto, Monoji . Papers of the 24th Regional Meeting of the Chicago Linguistics
Society. Shinozaki Shorin . (1989)
3. Baldwin, Timothy and Bannard, Colin and Tanaka, Takaaki and Widdows, Do-
minic . An Empirical Model of Multiword Expression . Proceedings of the ACL-
2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment.
(2003)
4. Bannard, Colin and Baldwin, Timothy and Lascarides, Alex . A Statistical Ap-
proach to the Semantics of Verb-Particles . Proceedings of the ACL-2003 Workshop
on Multiword Expressions: Analysis, Acquisition and Treatment. (2003)
5. Bikel, Daniel M. . A Distributional Analysis of a Lexicalized Statistical Parsing
Model . Proceedings of EMNLP . (2004)
6. Becker, Joseph D. . The Phrasal Lexicon . Theoritical Issues of NLP, Workshop in
CL, Linguistics, Psychology and AI, Cambridge, MA. (1975)
7. Breidt, Elisabeth . Extraction of V-N-Collocations from Text Corpora: A Feasibil-
ity Study for German . CoRR-1996 . (1995)
8. Church, K. and Gale, W. and Hanks, P. and Hindle, D. . Parsing, word associations
and typical predicate-argument relations . Current Issues in Parsing Technology.
Kluwer Academic, Dordrecht, Netherlands, 1991 . (1991)
9. Church, K. and Patrick Hanks . Word Association Norms, Mutual Information,
and Lexicography . Proceedings of the 27th. Annual Meeting of the Association
for Computational Linguistics, 1990 . (1989)
10. Dunning, Ted . Accurate Methods for the Statistics of Surprise and Coincidence .
Computational Linguistics - 1993 . (1993)
11. Stefan Evert and Brigitte Krenn . Methods for the Qualitative Evaluation of Lexical
Association Measures . Proceedings of the ACL - 2001 . (2001)
12. Charles Fillmore . An extremist approach to multi-word expressions . A talk given
at IRCS, University of Pennsylvania, 2003. (2003)
13. Fontenelle and Bruls, Th. W. and Thomas, L. and Vanallemeersch, T. and Jansen,
J. . Survey of collocation extraction tools . Deliverable D-1a, MLAP-Project 93-19
DECIDE, University of Liege, Belgium. (1994)
564 S. Venkatapathy and A.K. Joshi
14. Diaz-Galiano, M.C. and Martin-Valdivia, M.T. and Martinez-Santiago, F. and
Urena-Lopez, L. A. . Multi-word Expressions Recognition with the LVQ Algorithm.
Proceedings of Methodologies and Evaluation of Multiword Unit in Real-world Ap-
plications, LREC, 2004 . (2004)
15. Joachims, T. . Making large-Scale SVM Learning Practical . Advances in Kernel
Methods - Support Vector Learning . (1999)
16. Joachims, T. . Optimizing Search Engines Using Clickthrough Data. Advances
in Kernel Methods - Support Vector Learning edings of the ACM Conference on
Knowledge Discovery and Data Mining (KDD), ACM, 2002. (2002)
17. Kilgariff, A. and Rosenzweig, J. . Framework and Results for English Senseval .
Computers and the Humanities, 2000 . (2000)
18. Dekang Lin . Automatic Identification of non-compositonal phrases. Proceedings
of ACL- 99, College Park, USA . (1999)
19. McCarthy, D. and Keller, B. and Carroll, J. . Detecting a Continuum of Composi-
tionality in Phrasal Verbs . Proceedings of the ACL-2003 Workshop on Multi-word
Expressions: Analysis, Acquisition and Treatment, 2003. (2003)
20. Mitchell, T. Instance-Based Learning . Machine Learning, McGraw-Hill Series in
Computer Science, 1997 . (1997)
21. Moore, A. W. and Lee, M.S. . Proceedings of the 11 International Conference on
Machine Learning, 1994. (1994)
22. Nunberg, G. and Sag, I. A. and Wasow, T. . Idioms . Language, 1994 . (1994)
23. Sag, I. A. and Baldwin, Timothy and Bond, Francis and Copestake, Ann and
Flickinger, Dan. . Multi-word expressions: a pain in the neck for nlp . Proceedings
of CICLing , 2002 . (2002)
24. Schone, Patrick and Jurafsky, Dan. Is Knowledge-Free Induction of Multiword Unit
Dictionary Headwords a Solved Problem? . Proceedings of EMNLP , 2001 . (2001)
25. Schuler, William and Joshi, Aravind K. Relevance of tree rewriting systems for
multi-word expressions. To be published. (2005)
26. Smadja, F. . Retrieving Collocations from Text : Xtract . Computational Linguis-
tics - 1993 . (1993)
27. Tapanainen, Pasi and Piitulaine, Jussi and Jarvinen, Timo Idiomatic object usage
and support verbs . 36th Annual Meeting of the Association for Computational
Linguistics . (1998)
28. Venkatapathy, Sriram and Joshi, Aravind K. Recognition of Multi-word Expres-
sions: A Study of Verb-Noun (V-N) Collocations. Proceedings of the International
Conference on Natural Language Processing, 2004. (2004)
Towards an Annotated Corpus of Discourse Relations in Hindi 
Rashmi Prasad*, Samar Husain?, Dipti Mishra Sharma? and Aravind Joshi* 
 
Abstract 
We describe our initial efforts towards 
developing a large-scale corpus of Hindi 
texts annotated with discourse relations. 
Adopting the lexically grounded approach 
of the Penn Discourse Treebank (PDTB), 
we present a preliminary analysis of 
discourse connectives in a small corpus. 
We describe how discourse connectives are 
represented in the sentence-level 
dependency annotation in Hindi, and 
discuss how the discourse annotation can 
enrich this level for research and 
applications. The ultimate goal of our work 
is to build a Hindi Discourse Relation Bank 
along the lines of the PDTB. Our work will 
also contribute to the cross-linguistic 
understanding of discourse connectives. 
1 Introduction 
An increasing interest in human language 
technologies such as textual summarization, 
question answering, natural language generation 
has recently led to the development of several 
discourse annotation projects aimed at creating 
large scale resources for natural language 
processing. One of these projects is the Penn 
Discourse Treebank (PDTB Group, 2006),1whose 
goal is to annotate the discourse relations holding 
between eventualities described in a text, for 
example causal and contrastive relations. The 
PDTB is unique in using a lexically grounded 
approach for annotation: discourse relations are 
anchored in lexical items (called ?explicit 
discourse connectives?) whenever they are 
                                                 
* University of Pennsylvania, Philadelphia, PA, USA, 
{rjprasad,joshi}@seas.upenn.edu 
? Language Technologies Research Centre, IIIT, Hyderabad, 
India, samar@research.iiit.ac.in, dipti@iiit.ac.in 
1 http://www.seas.upenn.edu/?pdtb 
 
explicitly realized in the text. For example, in (1), 
the causal relation between ?the federal 
government suspending US savings bonds sales? 
and ?Congress not lifting the ceiling on 
government debt? is expressed with the explicit 
connective ?because?.2 The two arguments of each 
connective are also annotated, and the annotations 
of both connectives and their arguments are 
recorded in terms of their text span offsets.3  
 
(1) The federal government suspended sales of U.S. 
savings bonds because Congress hasn?t lifted the 
ceiling on government debt. 
 
One of the questions that arises is how the 
PDTB style annotation can be carried over to 
languages other than English. It may prove to be a 
challenge cross-linguistically, as the guidelines and 
methodology appropriate for English may not 
apply as well or directly to other languages, 
especially when they differ greatly in syntax and 
morphology. To date, cross-linguistic 
investigations of connectives in this direction have 
been carried out for Chinese (Xue, 2005) and 
Turkish (Deniz and Webber, 2008). This paper 
explores discourse relation annotation in Hindi, a 
language with rich morphology and free word 
order. We describe our study of ?explicit 
connectives? in a small corpus of Hindi texts, 
discussing them from two perspectives. First, we 
consider the type and distribution of Hindi 
connectives, proposing to annotate a wider range 
                                                 
2 The PDTB also annotates implicit discourse relations, but 
only locally, between adjacent sentences. Annotation here 
consists of providing connectives (called ?implicit discourse 
connectives?) to express the inferred relation. Implicit 
connectives are beyond the scope of this paper, but will be 
taken up in future work. 
3 The PDTB also records the senses of the connectives, and 
each connective and its arguments are also marked for their 
attribution. Sense annotation and attribution annotation are not 
discussed in this paper. We will, of course, pursue these 
aspects in our future work concerning the building of a Hindi 
Discourse Relation Bank. 
 
The 6th Workshop on Asian Languae Resources, 2008
73
of connectives than the PDTB. Second, we 
consider how the connectives are represented in 
the Hindi sentence-level dependency annotation, in 
particular discussing how the discourse annotation 
can enrich the sentence-level structures. We also 
briefly discuss issues involved in aligning the 
discourse and sentence-level annotations.  
Section 2 provides a brief description of Hindi 
word order and morphology. In Section 3, we 
present our study of the explicit connectives 
identified in our texts, discussing them in light of 
the PDTB. Section 4 describes how connectives 
are represented in the sentence-level dependency 
annotation in Hindi. Finally, Section 5 concludes 
with a summary and future work. 
2 Brief Overview of Hindi Syntax and 
Morphology 
Hindi is a free word order language with SOV as 
the default order. This can be seen in (2), where 
(2a) shows the constituents in the default order, 
and the remaining examples show some of the 
word order variants of (2a). 
 
(2)  a. malaya       nao         samaIr         kao     iktaba    dI .  
           malay   ERG  sameer    DAT  book   gave 
           ?Malay gave the book to Sameer? (S-IO-DO-V)4 
       b. malaya nao iktaba samaIr kao dI. (S-DO-IO-V) 
       c. samaIr kao malaya nao iktaba dI. (IO-S-DO-V) 
       d. samaIr kao iktaba malaya nao dI. (IO-DO-S-V) 
       e. iktaba malaya nao samaIr kao dI. (DO-S-IO-V) 
        f. iktaba samaIr kao malaya nao dI.  (DO-IO-S-V) 
 
Hindi also has a rich case marking system, 
although case marking is not obligatory. For 
example, in (2), while the subject and indirect 
object are explicitly for the ergative (ERG) and 
dative (DAT) cases, the direct object is unmarked 
for the accusative. 
3 Discourse Connectives in Hindi 
Given the lexically grounded approach adopted for 
discourse annotation, the first question that arises 
is how to identify discourse connectives in Hindi. 
Unlike the case of the English connectives in the 
PDTB, there are no resources that alone or together 
provide an exhaustive list of connectives in the 
                                                 
4 S=Subject; IO=Indirect Object; DO=Direct Object; 
V=Verb; ERG=Ergative; DAT=Dative 
language. We did try to create a list from our own 
knowledge of the language and grammar, and also 
by translating the list of English connectives in the 
PDTB. However, when we started looking at real 
data, this list proved to be incomplete. For 
example, we discovered that the form of the 
complementizer ?ik? also functions as a temporal 
subordinator, as in (3). 
 
(3) [ vah  baalaTI      ko   gaMdo      panaI      sao      ApnaI    caaOklaoT  
      [he   bucket  of  dirty  water  from  his     chocolates 
      inakalanao          hI     vaalaa    qaa]    ik    {]sakI  mammaI       nao  
      taking-out  just doing was]  that  {his   mother ERG 
      ]sao    raok idyaa } 
      him stop did} 
?He was just going to take out the chocolates from 
the dirty water in the bucket when his mother stopped 
him.? 
 
The method of collecting connectives will 
therefore necessarily involve ?discovery during 
annotation?. However, we wanted to get some 
initial ideas about what kinds of connectives were 
likely to occur in real text, and to this end, we 
looked at 9 short stories with approximately 8000 
words. Our goal here is to develop an initial set of 
guidelines for annotation, which will be done on 
the same corpus on which the sentence-level 
dependency annotation is being carried out (see 
Section 4). Table 1 provides the full set of 
connectives we found in our texts, grouped by 
syntactic type. The first four columns give the 
syntactic grouping, the Hindi connective 
expressions, the English gloss, and the English 
equivalent expressions, respectively. The last 
column gives the number of occurrences we found 
of each expression. In the rest of this section, we 
describe the function and distribution of discourse 
connectives in Hindi based on our texts. In the 
discussion, we have noted our points of departure 
from the PDTB where applicable, both with 
respect to the types of relations being annotated as 
well as with respect to terminology. For argument 
naming, we use the PDTB convention: the clause 
with which the connective is syntactically 
associated is called Arg2 and the other clause is 
called Arg1. Two special conventions are followed 
for paired connectives, which we describe below. 
In all Hindi examples in this paper, Arg1 is 
enclosed in square brackets and Arg2 is in braces. 
The 6th Workshop on Asian Languae Resources, 2008
74
Connective Type Hindi Gloss English Num 
Sub. Conj. @yaaoMik 
(@yaaoM)ik..[salaIe 
(Agar|yadI)..tba|tao 
(jaba).. tba|tao 
jaba tk.. tba tk (ko ilae)  
jaOsao hI..(tao)  
[tnaa|eosaa..kI  
taik  
ik 
why-that 
(why)-that..this-for  
(if)..then 
(when)..then  
when till..then till (of for)  
as just..(then)  
so|such..that  
so-that  
that  
 
because  
because 
if..(then)  
when  
until  
as soon as  
so that  
so that  
when
  
2 
3 
15 
50 
2 
5 
12 
1 
5 
Sentential Relatives ijasasao 
jaao 
ijasako karNa 
which-with 
which 
which-of reason 
because of which 
because of which 
because of which 
5 
1 
1 
Subordinator pr 
(-kr|-ko|krko) 
samaya 
hue 
ko baad 
sao 
ko phlao 
ko ilae 
maoM 
ko karNa 
upon 
(do) 
time 
happening 
of later 
with 
of before 
of for 
in 
of reason
 
upon 
after|while 
while 
while 
after 
due to 
before 
in order to 
while 
because of
 
9 
111 
1 
28 
3 
1 
1 
4 
1 
3 
Coord. Conj. laoikna|pr|prntu 
AaOr|tqaa  
yaa  
yaaoM tao..pr  
naa kovala..balaik  
but  
and  
or  
such TOP..but  
not only..but
 
but  
and  
or  
but  
not only..but
 
51 
117 
2 
2 
1 
Adverbial tba  
baad maoM  
ifr  
[saIilae  
nahIM tao  
tBaI tao  
saao  
vahI|yahI nahIM  
then  
later in  
then  
this-for  
not then  
then-only TOP  
so  
that|this-only not  
then  
later  
then  
that is why  
otherwise  
that is why  
so  
not only that  
2 
5 
4 
7 
5 
1 
10 
1 
TOTAL    472 
     Table 1: A Partial List of Discourse Connectives in Hindi. Parentheses are used for optional  
     elements; ?|? is used for alternating elements; TOP = topic marker.
 
3.1 Types of Discourse Connectives 
3.1.1 Subordinating Conjunctions 
Finite adverbial subordinate clauses are 
introduced by independent lexical items called 
?subordinating conjunctions?, such as @yaaoMik 
(?because?), as in (4), and they typically occur as 
right or left attached to the main clause. 
 
(4) [maOM   [sa  saBaI    Qana        kao       rajya      ko   baadSaah  
      [I  this  all   wealth ACC kingdom of  king 
      kao      do      dota],    @yaaoMik       {vahI           samast 
     
 
 
  DAT give would], why-that {he-EMPH all 
      QartI     kI  sampda      ka  svaamaI   hO} 
      earth  of   wealth  of   lord  is} 
?I would give all this wealth to the king, because he 
alone is the lord of this whole world?s wealth.? 
 
As the first group in Table 1 shows, 
subordinating conjunctions in Hindi often come 
paired, with one element in the main clause and 
the other in the subordinate clause (Ex.5). One 
of these elements can also be implicit (Ex.6),  
The 6th Workshop on Asian Languae Resources, 2008
75
and in our texts, this was most often the 
subordinate clause element. 
 
(5)  @yaaoMik       {yah   tumharI  ja,maIna   pr   imalaa     hO},      [sailae 
       because {this  your  land  on  found  has}, this-for 
       [[sa       Qana      pr  tumhara  AiQakar  hO] 
       [this treasure on  your  right  is] 
?Because this was found on your land, you have the 
right to this treasure.? 
 
(6)  []saka  vaSa       calata]     tao    {vah   ]sao   Gar        sao 
       [her   power  walk] then  {she  it   home  from 
       baahr  inakala  dotI}  
       out  take  would} 
?Had it been in her power, she would have banished 
it from the house.? 
 
When both elements of the paired connective are 
explicit, their text spans must be selected 
discontinuously. The main clause argument is 
called Arg1 and the subordinate clause 
argument, Arg2. 
Subordinating conjunctions, whether single or 
paired, can occur in non-initial positions in their 
clause. However, this word order variability is 
not completely unconstrained. First, not all 
conjunctions display this freedom. For example, 
while ?jaba? (?when?) can be clause-medial (Ex. 
7), ?@yaaoMik? (?because?) cannot. Second, when the 
main clause precedes the subordinate clause, the 
main clause element, if explicit, cannot appear 
clause-initially at all. Consider the causal ?@yaaoMik.. 
[salaIe? (Ex.5), which represents the subordinate-
main clause order. In the reverse order, the 
explicit main clause ?[salaIe? (Ex.8) appears 
clause medially. Placing this element in clause-
initial position is not possible. 
 
(7) {lakD,haro         kI  p%naI      kao}    jaba    {yah 
      {woodcutter of  wife DAT} when {this 
       maalaUma            pD,a  ik     [sa  icaiDyaa  ko   karNa,  
       knowledge put  that this bird   of   reason 
       kama     CaoD,kr    Gar       Aa   gayaa     hO}   tao      [vah 
       work leaving home come went is} then  [she 
       ]sa    pr       barsa        pD,I]. 
       him on  anger-rain put] 
 ?When the woodcutter?s wife found out that he had 
left his work and come home to care for the bird, she 
raged at him.? 
 
(8)  [. . . pr  icaraga   kI  ba%tI    ]sakanaa  yaa   daohrI  
       [. . .but lamp of  light  light    or  another 
       ba%tI    lagaanaa]         Saayad [sailae         []icat nahIM  
       light  putting] perhaps this-for  [appropriate not 
       samaJato          qao]  ik     {tola  ka  Apvyaya   haogaa}. 
       Consider did]  that  {oil  of  waste   be-FUT}. 
?. . . but he did not consider it appropriate to light the 
lamp repeatedly or light another lamp, perhaps 
because it would be a waste of oil.? 
3.1.2 Sentential Relative Pronouns 
Since discourse relations are defined as holding 
between eventualities, we have also identified 
relations that are expressed syntactically as 
relative pronouns in sentential relative clauses, 
which modify the main clause verb denoting an 
eventuality, rather than some entity denoting 
noun phrase. For example, in (9), a 
result/purpose relation is conveyed between ?the 
man?s rushing home? and ?the bird being taken 
care of?, and we believe that this relation 
between the eventualities should be captured 
despite it?s syntactic realization as the relative 
pronoun ?ijasasao? (?because of which/so that?). (10) 
gives an example of a modified relative 
pronoun. 
 
(9) [saara  kama     caaoD,kr     vah  ]sa    baImaar   icaiD,yaa 
      [all  work  leaving  he  that  sick  bird 
      kao       ]zakr       dbaa     Gar     kI    Aaor       Baagaa], 
      ACC picking-up fast home of direction ran], 
      ijasasao             {]saka   sahI       [laaja   ikyaa  jaa    sako} 
      from-which {her    proper  care  do   go  able} 
?Leaving all his work, he picked up the bird and ran 
home very fast, so that the bird could be given proper 
care.? 
 
(10) [}M^TaoM       ko   hr     baar    kdma  rKnao       pr 
        [camels of  every time step keeping upon 
        icaiD,yaao M ko  isar     Aapsa          maoM   tqaa   }M^T      kI 
        birds of  head each-other in and camels of 
        gardna   sao    Tkra            rho     qao]   ijasako karNa 
        neck with hit-against be had] of-which reason 
       {]na     pixayaaoM   kI   drdBarI      caIKoM        inakla 
       {those birds  of   painful  screams come-out  
         rhI   qaIM}. 
         be had} 
?With each step of the camels, the birds heads were 
hitting against each other as well as with the camels? 
necks because of which the birds were screaming 
painfully.? 
3.1.3 Subordinators 
In contrast to the subordinating conjunctions, 
elements introducing non-finite subordinate 
clauses are called ?subordinators?. Unlike 
The 6th Workshop on Asian Languae Resources, 2008
76
English, where certain non-finite subordinate 
clauses, called ?free adjuncts?, appear without 
any overt marking so that their relationship with 
the main clause is unspecified, Hindi non-finite 
subordinate clauses almost always appear with 
overt marking. However, also unlike English, 
where the same elements may introduce both 
finite and non-finite clauses (cf. After leaving, 
she caught the bus vs. After she left, she caught 
the bus), different sets of elements are used in 
Hindi. In fact, as can be seen in the subordinator 
group in Table 1, the non-finite clause markers 
are either postpositions (Ex.11), particles 
following verbal participles (Ex.12), or suffixes 
marking serial verbs (Ex.13). 
 
(11) {mammaI          ko     manaa        krnao}      ko karNa     [ramaU 
        {mummy of  warning  doing} of reason [Ramu 
         qaaoD,I  qaaoD,I    caaOklaoT      baD,o     AnaMd        ko   saaqa 
         little little chocolate big  pleasure  of  with  
         Ka  rha      qaa]. 
         eat being be] 
?Because of his mother?s warning, Ramu was eating 
bits of chocolate with a lot of pleasure.? 
 
(12) . . . AaOr    {Kolato}      hue               [yah    BaUla     jaata hO 
        . . . and  {playing} happening [this forget go is 
         ik    yaid  ]saka  ima~        BaI    Apnao  iKlaaOnao    kao 
         that if    his   friends also their  toys     to 
         ]sao     haqa   nahIM   lagaanao         dota,  tao      ]sao 
         him hand not   touching did,  then  he  
           iktnaa        baura     lagata] 
         how-much bad   feel] 
?. . . and while playing, he forgets that if his friends 
too didn?t let him touch their toys, then how bad he 
would feel.? 
 
(13) {ApnaI  p%naI     sao     yah       sauna}kr      [lakD,hara  
        {self  wife from this   listen}-do  [woodcutter 
         bahut    duKI       huAa] 
         much sad  became] 
?Upon hearing this from his wife, the woodcutter 
became very sad.? 
 
While subordinators constitute a frequently-
used way to mark discourse relations, their 
annotation raises at least two difficult problems, 
both of which have implications for the 
reliability of annotation. The first is that these 
markers are used for marking both argument 
clauses and adjunct clauses, so that annotators 
would be required to make difficult decisions for 
distinguishing them: in the former case, the 
marker would not be regarded as a connective, 
while in the latter case, it would. Second, the 
clauses marked by these connectives often seem 
to be semantically weak. This is especially true 
of verbal participles, which are nonfinite verb 
appearing in a modifying relation with another 
finite verb. Whereas in some cases (Ex.12-13) 
the two verbs are perceived as each projecting 
?two distinct events? between which some 
discourse relation can be said to exist, in other 
cases (Ex.14), the two verbs seem to project two 
distinct actions but as part of a ?single complex 
event? (Verma, 1993). These judgments can be 
very subtle, however, and our final decision on 
whether to annotate such constructions will be 
made after some initial annotation and 
evaluation. 
 
(14) {doKto        hI         doKto      saba  baOla             Baagato } 
        {looking EMPH looking all buffalos running} 
         hue              [gaaoSaalaa   phu^Mca     gae] 
         happening [shed   reach  did] 
?Within seconds all the buffalos came running to the 
shed.? 
 
The naming convention for the arguments of 
subordinators is the same as for the 
subordinating conjunctions: the clause 
associated with the subordinator is called Arg2 
while its matrix clause is called Arg1. 
Unlike subordinating conjunctions, 
subordinators do not come paired and they can 
only appear clause-finally. Clause order, while 
not fixed, is restricted in that the nonfinite 
subordinate clause can appear either before the 
main clause or embedded in it, but never after 
the main clause.  
3.1.4 Coordinating Conjunctions 
Coordinating conjunctions in Hindi are found in 
both inter-sentential (Ex.15) and intra-sentential 
(Ex.16) contexts, they always appear as 
independent elements, and they almost always 
appear clause-initially. 5  For these connectives, 
                                                 
5 While the contrastive connectives  ?pr?, ?prntU? appear only 
clause-initially, it seems possible for the contrastive ?laoikna? 
to appear clause-medially, suggesting that these two types 
may correspond to the English ?but? and ?however?, 
respectively. However, we did not find any examples of 
clause-medial ?laoikna? in our texts, and this behavior will 
have to be verified with further annotation. 
The 6th Workshop on Asian Languae Resources, 2008
77
the first clause is called Arg1 and the second, 
Arg2. 
 
(15) [jaba      vah  laaOTta    tao       gaa-gaakr          ]saka  mana 
         [when he  return then sing-singing   his  mind 
         KuSa      kr    dotI].   laoikna  {]sakI p%naI   kao      vah 
         happy do gave].   But   {his wife DAT  the  
         icaiD,yaa   fUTI    AaM^K  nahIM    sauhatI   qaI}. 
         bird    torn  eye  not   bear  did} 
?Upon his return, she would make him happy by 
singing. But his wife could not tolerate the bird even 
a little bit.? 
 
(16) [ tBaI          drvaaja,a      Kulaa]  AaOr    {maalaikna  Aa 
        [then-only door opened]  and  {wife  come 
         ga[- }. 
        went} 
?Just then the door opened and the wife came in.? 
 
We also recognize paired coordinating 
conjunctions, such as ?naa kovala..balaik? (See Table 
1). The argument naming convention for these is 
the same as for the single conjunctions. 
3.1.5 Discourse Adverbials 
Discourse adverbials in Hindi modify their clau- 
ses as independent elements, and some of these 
are free to appear in non-initial positions in the 
clause. Example (17) gives an example of the 
consequence adverb, ?saao?. The Arg2 of discourse 
adverbials is the clause they modify, whereas 
Arg1 is the other argument. 
 
(17) [icaiD,yaa  jabaana      kT   jaanao     AaOr   maalaikna  ko  eosao 
        [bird    tongue  cut  going  and  wife  of  this  
        vyavahar       sao      Dr   ga[-     qaI]. saao    {vah     iksaI 
        behavior with fear go  had]. So  {she  some 
        trh        ]D,kr    calaI        ga[-}. 
        manner flying  walk    went}. 
?The bird was scared due to her tongue being cut and 
because of the wife?s behavior. So she somehow flew 
away.? 
 
As with the PDTB, one of our goals with the 
Hindi discourse annotation is to explore the 
structural distance of Arg1 from the discourse 
adverbial. If the Arg1 clause is found to be snon-
adjacent to the connective and the Arg2 clause, 
it may suggest that adverbials in Hindi behave 
anaphorically. In the texts we looked at, we did 
not find any instances of non-adjacent Arg1s. 
Addtional annotation will provide further 
evidence in this regard. 
4 Hindi Sentence-level Annotation 
andDiscourse Connectives 
The sentence-level annotation task in Hindi is 
an ongoing effort which aims to come up with a 
dependency annotated treebank for the NLP/CL 
community working on Indian languages. 
Presently a million word Hindi corpus is being 
manually annotated (Begum et al, 2008). The 
dependency annotation is being done on top of 
the corpus which has already been marked for 
POS tag and chunk information. The scheme has 
28 tags which capture various dependency 
relations. These relations are largely inspired by 
the Paninian grammatical framework. Given 
below are some relations, reflecting the 
argument structure of the verb. 
 
a) kta- (agent) (k1) 
b) kma- (theme) (k2) 
c) krNa (instrument) (k3) 
d) samp`dana sampradaan (recipient) (k4) 
e) Apadana (source) (k5) 
f) AiQakrNa (location) (k7) 
 
Figure 1 shows how Examples (2a-f) are 
represented in the framework. Note that agent 
and theme are rough translations for ?kta-? and 
?kma-? respectively. Unlike thematic roles, these 
relations are not purely semantic, and are 
motivated not only through verbal semantics but 
also through vibhaktis (postpositions) and TAM 
(Tense, aspect and modality) markers (Bharati et 
al., 1995). The relations are therefore syntactico-
semantic, and unlike thematic roles there is a 
greater binding between these relations and the 
syntactic cues. 
 
 
k1 k4 k2
 
 
 
Figure 1: Dependency Diagram for Example (2) 
Some discourse relations that we have identified 
are already clearly represented in the sentence-
level annotation. But for those that aren?t, the 
   dI 
   malaya    samaIr    iktaba 
The 6th Workshop on Asian Languae Resources, 2008
78
discourse level annotations will enrich the 
sentence-level. In the rest of this section, we 
discuss the representation of the different types 
of connectives at the sentence level, and discuss 
how the discourse annotation will add to the 
information present in the dependency 
structures. 
 
Subordinating Conjunctions Subordinating 
conjunctions are lexically represented in the 
dependency tree, taking the subordinating clause 
as their dependents while themselves attaching 
to the main verb (the root of the tree). Figure 2 
shows the dependency tree for Example (4) 
containing the subordinating conjunction ?@yaaoMik?. 
Note that the edge between the connective and 
the main verb gives us the causal relation 
between the two clauses, the relation label being 
?rh? (relation hetu ?cause?). Thus, the discourse 
level can be taken to be completely represented 
at the sentence-level. 
 
hE
k1 k2 k4 rh
ccofr6
k1sk1
r6
r6 
 
Figure 2: Dependency Tree for Subordinating 
Conjunction in Example (4) 
 
Paired Subordinating Conjunctions Unlike 
Example (4), however, the analysis for the 
paired connective in Example (5), given in 
Figure 3, is insufficient. Despite the lexical 
representation of the connective in the tree, the 
correct interpretation of the paired conjunction 
and the clauses which it relates is only possible 
at the discourse level. In particular, the 
dependencies don?t show that ?@yaaoMik? and ?[salaIe? 
are two parts of the same connective, expressing 
a single relation and taking the same two 
arguments. Thus, the discourse annotation will 
be able to provide the appropriate argument 
structure and semantics for these paired 
connectives.  
 
 
ccof
k2 k1 rh
ccof
k7p
r6
k1
 
 
Figure 3: Dependency Tree for Paired 
Subordinating Conjunction in Example (5) 
 
Subordinators As mentioned earlier, Hindi 
nonfinite subordinate clauses almost always 
appear with overt marking. But unlike the 
subordinating conjunctions, subordinators are 
not lexically represented in the dependency 
trees. Figure 4 gives the dependency 
representation for Example (11) containing a 
postposition subordinator ?ko karNa?, which relates 
the main and subordinate clauses causally. As 
the figure shows, while the causal relation label 
(?rh?) appears on the edge between the main and 
subordinate verbs, the subordinator itself is not 
lexically represented as the mediator of this 
relation. The lexically grounded annotation at 
the discourse level will thus provide the textual 
anchors of such relations, enriching the 
dependency representation. Furthermore, while 
many of the subordinators in Table 1 are fully 
specified in the dependency trees for the 
semantic relation they denote (e.g., ?pr? and ?maoM? 
marked as the ?k7t? (location in time) relation, 
and ?ko karNa? and ?sao? marked as the ?rh? 
(cause/reason) relation), others, like the particle 
?hue? are underspecified for their semantics, being 
marked only as ?vmod? (verbal modifier). The 
discourse-level annotation will thus be the 
source for the semantics of these subordinators. 
 
Coordinating Conjunctions Coordinating 
conjunctions at the sentence level anchor the 
root of the dependency tree. Figure 5 shows the 
 do dotao oo oo o   
   maOMOM O MO M   Qana   baadSaah    @yaaoMikoM oMoM   
rajya  
   vahI   svaamaI  
  sampda  
   QartI  
   [sailae 
    
AiQakar hOO OO 
    Qana  tuuuumhara  @yaaoMikoMoMo M   
  imalaa hOO OO 
    yah 
  
 ja,maIna, ,,  
 
  tumharIuuu  
The 6th Workshop on Asian Languae Resources, 2008
79
dependency representation of Example (16) 
containing a coordinating conjunction. 
 
 
rh k1 k2
vmod
k1
 
 
Figure 4: Dependency Tree for Subordinator in 
Example (11) 
 
 
ccof ccof
k7t k1 k1
 
 
Figure 5: Dependency Tree for Coordinating 
Conjunction in Example (16) 
 
While the sentence-level dependency analysis 
here is similar to the one we get at the discourse 
level, the semantics of these conjunctions are 
again underspecified, being all marked as ?ccof?, 
and can be obtained from the discourse level. 
 
Discourse Adverbials Like subordinating 
conjunctions, discourse adverbials are 
represented lexically in the dependency tree. 
They are attached to the verb of their clause as 
its child node and their denoted semantic 
relation is specified clearly. This can be seen 
with the temporal adverb ?tBaI? (?then-only?) and 
its semantic label ?k7t? in Figure 5. At the same 
time, since the Arg1 discourse argument of 
adverbials is most often in the prior context, the 
discourse annotation will enrich the semantics of 
these connectives by providing the Arg1 
argument. 
5 Summary and Future Work 
In this paper, we have described our study of 
discourse connectives in a small corpus of Hindi 
texts in an effort towards developing an 
annotated corpus of discourse relations in Hindi. 
Adopting the lexically grounded approach of the 
Penn Discourse Treebank, we have identified a 
wide range of connectives, analyzing their types 
and distributions, and discussing some of the 
issues involved in the annotation. We also 
described the representation of the connectives 
in the sentence-level dependency annotation 
being carried out independently for Hindi, and 
discussed how the discourse annotations can 
enrich the information provided at the sentence 
level. While we focused on explicit connectives 
in this paper, future work will investigate the 
annotation of implicit connectives, the semantic 
classification of connectives, and the attribution 
of connectives and their arguments. 
References 
Rafiya Begum, Samar Husain, Arun Dhwaj, Dipti 
Misra Sharma, Lakshmi Bai, and Rajeev Sangal. 
2008. Dependency annotation scheme for Indian 
languages. In Proceedings of IJCNLP-2008. 
Hyderabad, India. 
Akshar Bharati, Vineet Chaitanya, and Rajeev 
Sangal. 1995. Natural Language Processing: A 
Paninian Perspective. Prentice Hall of India. 
http://ltrc.iiit.ac.in/downloads/nlpbook/nlppanini.p
df. 
Manindra K. Verma (ed.). 1993. Complex Predicates 
in South Asian Languages. New Delhi: Manohar. 
The PDTB-Group. 2006. The Penn Discourse 
TreeBank 1.0 Annotation Manual. Technical 
Report IRCS-06-01, IRCS, University of 
Pennsylvania. 
Bonnie Webber, Aravind Joshi, Matthew Stone, and 
Alistair Knott. 2003. Anaphora and discourse 
structure. Computational Linguistics, 29(4):545?
587. 
Nianwen Xue. 2005. Annotating Discourse 
Connectives in the Chinese Treebank. In 
Proceedings of the ACL Workshop on Frontiers in 
Corpus Annotation II: Pie in the Sky. Ann Arbor, 
Michigan.  
Deniz Zeyrek and Bonnie Webber. 2008. A 
Discourse Resource for Turkish: Annotating 
Discourse Connectives in the METU Corpus. In 
Proceedings of IJCNLP-2008. Hyderabad, India. 
 
 
  Ka rha qaa 
manaa krnaoo oo rama   caaOO OOklaoTooo    AanaMdMMM  
 mammaI 
  AaOrOOO  
 Kulaauuu   Aa ga[-- -- 
maalaikna  drvaaja,a, ,,   tBaI  
The 6th Workshop on Asian Languae Resources, 2008
80
c? 2003 Association for Computational Linguistics
Anaphora and Discourse Structure
Bonnie Webber? Matthew Stone?
Edinburgh University Rutgers University
Aravind Joshi? Alistair Knott?
University of Pennsylvania University of Otago
We argue in this article that many common adverbial phrases generally taken to signal a discourse
relation between syntactically connected units within discourse structure instead work anaphor-
ically to contribute relational meaning, with only indirect dependence on discourse structure.
This allows a simpler discourse structure to provide scaffolding for compositional semantics and
reveals multiple ways in which the relational meaning conveyed by adverbial connectives can
interact with that associated with discourse structure. We conclude by sketching out a lexicalized
grammar for discourse that facilitates discourse interpretation as a product of compositional rules,
anaphor resolution, and inference.
1. Introduction
It is a truism that a text means more than the sum of its component sentences. One
source of additional meaning are relations taken to hold between adjacent sentences
?syntactically? connected within a larger discourse structure. It has been very difficult,
however, to say what discourse relations there are, either theoretically (Mann and
Thompson 1988; Kehler 2002; Asher and Lascarides 2003) or empirically (Knott 1996).
Knott?s empirical attempt to identify and characterize cue phrases as evidence
for discourse relations illustrates some of the difficulties. Knott used the following
theory-neutral test to identify cue phrases: For a potential cue phrase ? in naturally
occurring text, consider in isolation the clause in which it appears. If the clause ap-
pears incomplete without an adjacent left context, whereas it appears complete if ? is
removed, then ? is a cue phrase. Knott?s test produced a nonexhaustive list of about
two hundred different phrases from 226 pages of text. He then attempted to charac-
terize the discourse relation(s) conveyed by each phrase by identifying when (always,
sometimes, never) one phrase could substitute for another in a way that preserved
meaning. He showed how these substitution patterns could be a consequence of a set
of semantic features and their values. Roughly speaking, one cue phrase could always
substitute for another if it had the same set of features and values, sometimes do so if
it was less specific than the other in terms of its feature values, and never do so if their
values conflicted for one or more features.
? School of Informatics, University of Edinburgh, 2 Buccleuch Place, Edinburgh, EH8 9LW, UK. E-mail:
bonnie@inf.ed.ac.uk.
? Department of Computer Science, Rutgers Universtiy, 110 Frelinghuysen Road, Piscataway, NJ
08854-8019. E-mail: mdstone@cs.rutgers.edu.
? Department of Computer & Information Science, University of Pennsylvania, 200 South 33rd Street,
Philadelphia, PA 19104-6389. E-mail: joshi@linc.cis.upenn.edu.
? Department of Computer Science, University of Otago, P.O. Box 56, DUNEDIN 9015, New Zealand.
E-mail: alik@cs.otago.ac.nz.
546
Computational Linguistics Volume 29, Number 4
By assuming that cue phrases contribute meaning in a uniform way, Knott was
led to a set of surprisingly complex directed acyclic graphs relating cue phrases in
terms of features and their values, each graph loosely corresponding to some family of
discourse relations. But what if the relational meaning conveyed by cue phrases could
in fact interact with discourse meaning in multiple ways? Then Knott?s substitution
patterns among cue phrases may have reflected these complex interactions, as well as
the meanings of individual cue phrases themselves.
This article argues that cue phrases do depend on another mechanism for convey-
ing extrasentential meaning?specifically, anaphora. One early hint that adverbial cue
phrases (called here discourse connectives) might be anaphoric can be found in an
ACL workshop paper in which Janyce Wiebe (1993) used the following example to
question the adequacy of tree structures for discourse:
(1) a. The car was finally coming toward him.
b. He [Chee] finished his diagnostic tests,
c. feeling relief.
d. But then the car started to turn right.
The problem Wiebe noted was that the discourse connectives but and then appear to
link clause (1d) to two different things: then to clause (1b) in a sequence relation (i.e.,
the car?s starting to turn right being the next relevant event after Chee?s finishing his
tests) and but to a grouping of clauses (1a) and (1c) (i.e., reporting a contrast between,
on the one hand, Chee?s attitude toward the car coming toward him and his feeling
of relief and, on the other hand, his seeing the car turning right). (Wiebe doesn?t give
a name to the relation she posits between (1d) and the grouping of (1a) and (1c), but
it appears to be some form of contrast.)
If these relations are taken to be the basis for discourse structure, some possible
discourse structures for this example are given in Figure 1. Such structures might seem
advantageous in allowing the semantics of the example to be computed directly by
compositional rules and defeasible inference. However, both structures are directed
acyclic graphs (DAGs), with acyclicity the only constraint on what nodes can be con-
nected. Viewed syntactically, arbitrary DAGs are completely unconstrained systems.
They substantially complicate interpretive rules for discourse, in order for those rules
to account for the relative scope of unrelated operators and the contribution of syn-
tactic nodes with arbitrarily many parents.1
We are not committed to trees as the limiting case of discourse structure. For
example, we agree, by and large, with the analysis that Bateman (1999) gives of
(2) (vi) The first to do that were the German jewellers, (vii) in particular Klaus
Burie. (viii) And Morris followed very quickly after, (ix) using a lacquetry
technique to make the brooch, (x) and using acrylics, (xi) and exploring
the use of colour, (xii) and colour is another thing that was new at that
time.
1 A reviewer has suggested an alternative analysis of (1) in which clause (1a) is elaborated by clause
(1b), which is in turn elaborated by (1c), and clause (1d) stands in both a sequence relation and a
contrast relation to the segment as a whole. Although this might address Wiebe?s problem, the result is
still a DAG, and such a fix will not address the additional examples we present in section 2, in which a
purely structural account still requires DAGs with crossing arcs.
547
Webber et al Anaphora and Discourse Structure
b
seq
d
contrast
c
cb
elaboration
elaboration
a seq
a d
seq contrast
(i)
(ii)
Figure 1
Possible discourse structure for example (1). Each root and internal node is labeled by the type
of relation that Wiebe takes to hold between the daughters of that node. (i) uses an n-ary
branching sequence relation, whereas in (ii), sequence is binary branching.
(ix)(vi)
succession manner
(viii)
Figure 2
Simple multiparent structure.
in which clause (ix) stands in a manner relation with clause (viii), which in turn stands
in a succession (i.e., sequence) relation with clause (vi). This is illustrated in Figure 2,
which shows a DAG (rather than a tree), but without crossing dependencies.
So it is the cost of moving to arbitrary DAGs for discourse structure that we feel is
too great to be taken lightly. This is what has led us to look for another explanation for
these and other examples of apparent complex and crossing dependencies in discourse.
The position we argue for in this article, is that whereas adjacency and explicit
conjunction (coordinating conjunctions such as and, or, so, and but; subordinating con-
junctions such as although, whereas, and when) imply discourse relations between (the
interpretation of) adjacent or conjoined discourse units, discourse adverbials such as
then, otherwise, nevertheless, and instead are anaphors, signaling a relation between the
interpretation of their matrix clause and an entity in or derived from the discourse
context. This position has four advantages:
1. Understanding discourse adverbials as anaphors recognizes their
behavioral similarity to the pronouns and definite noun phrases (NPs)
that are the bread and butter of previous work on anaphora. This is
discussed in section 2.
2. By understanding and exploring the full range of phenomena for which
an anaphoric account is appropriate, we can better characterize anaphors
and devise more accurate algorithms for resolving them. This is explored
in section 3.
3. Any theory of discourse must still provide an account of how a sequence
of adjacent discourse units (clauses, sentences, and the larger units that
they can comprise) means more than just the sum of its component
548
Computational Linguistics Volume 29, Number 4
units. This is a goal that researchers have been pursuing for some time,
using both compositional rules and defeasible inference to determine
these additional aspects of meaning (Asher and Lascarides 1999; Gardent
1997; Hobbs et al 1993; Kehler 2002; Polanyi and van den Berg 1996;
Scha and Polanyi 1988; Schilder 1997a, 1997b; van den Berg 1996) If that
portion of discourse semantics that can be handled by mechanisms
already needed for resolving other forms of anaphora and deixis is
factored out, there is less need to stretch and possibly distort
compositional rules and defeasible inference to handle everything.2
Moreover, recognizing the possibility of two separate relations (one
derived anaphorically and one associated with adjacency and/or a
structural connective) admits additional richness to discourse semantics.
Both points are discussed further in section 4.
4. Understanding discourse adverbials as anaphors allows us to see more
clearly how a lexicalized approach to the computation of clausal syntax
and semantics extends naturally to the computation of discourse syntax
and semantics, providing a single syntactic and semantic matrix with
which to associate speaker intentions and other aspects of pragmatics
(section 5.)
The account we provide here is meant to be compatible with current approaches
to discourse semantics such as DRT (Kamp and Reyle 1993; van Eijck and Kamp
1997), dynamic semantics (Stokhof and Groenendijk 1999), and even SDRT (Asher
1993; Asher and Lascarides 2003), understood as a representational scheme rather
than an interpretive mechanism. It is also meant to be compatible with more detailed
analyses of the meaning and use of individual discourse adverbials, such as Jayes
and Rossari (1998a, 1998b) and Traugott, (1995, 1997). It provides what we believe to
be a more coherent account of how discourse meaning is computed, rather than an
alternative account of what that meaning is or what speaker intentions it is being used
to achieve.
2. Discourse Adverbials as Anaphors
2.1 Discourse Adverbials Do Not Behave like Structural Connectives
We take the building blocks of the most basic level of discourse structure to be explicit
structural connectives between adjacent discourse units (i.e., coordinating and subor-
dinating conjunctions and ?paired? conjunctions such as not only . . . but also, and on the
one hand . . . on the other (hand) and inferred relations between adjacent discourse units
(in the absense of an explicit structural connective). Here, adjacency is what triggers
the inference. Consider the following example:
(3) You shouldn?t trust John. He never returns what he borrows.
Adjacency leads the hearer to hypothesize that a discourse relation of something like
explanation holds between the two clauses. Placing the subordinate conjunction (struc-
tural connective) because between the two clauses provides more evidence for this rela-
2 There is an analogous situation at the sentence level, where the relationship between syntactic structure
and compositional semantics is simplified by factoring away intersentential anaphoric relations. Here
the factorization is so obvious that one does not even think about any other possibility.
549
Webber et al Anaphora and Discourse Structure
tion. Our goal in this section is to convince the reader that many discourse adverbials,
including then, also, otherwise, nevertheless, and instead, do not behave in this way.
Structural connectives and discourse adverbials do have one thing in common:
Like verbs, they can both be seen as heading a predicate-argument construction; unlike
verbs, their arguments are independent clauses. For example, both the subordinate
conjunction after and the adverbial then (in its temporal sense) can be seen as binary
predicates (e.g., sequence) whose arguments are clausally derived events, with the
earlier event in first position and the succeeding event in second.
But that is the only thing that discourse adverbials and structural connectives have
in common. As we have pointed out in earlier papers (Webber, Knott, and Joshi 2001;
Webber et al, 1999a, 1999b), structural connectives have two relevant properties: (1)
they admit stretching of predicate-argument dependencies; and (2) they do not admit
crossing of those dependencies. This is most obvious in the case of preposed subor-
dinate conjunctions (example (4)) or ?paired? coordinate conjunctions (example (5)).
With such connectives, the initial predicate signals that its two arguments will follow.
(4) Although John is generous, he is hard to find.
(5) On the one hand, Fred likes beans. On the other hand, he?s allergic to them.
Like verbs, structural connectives allow the distance between the predicate and its
arguments to be ?stretched? over embedded material, without loss of the dependency
between them. For the verb like and an object argument apples, such stretching without
loss of dependency is illustrated in example (6b).
(6) a. Apples John likes.
b. Apples Bill thinks he heard Fred say John likes.
That this also happens with structural connectives and their arguments is illustrated
in example (7) (in which the first clause of example (4) is elaborated by another pre-
posed subordinate-main clause construction embedded within it) and in example (8)
(in which the first conjunct of example (5) is elaborated by another paired-conjunction
construction embedded within it). Possible discourse structures for these examples are
given in Figure 3.
(7) a. Although John is very generous?
b. if you need some money,
c. you only have to ask him for it?
d. he?s very hard to find.
(8) a. On the one hand, Fred likes beans.
b. Not only does he eat them for dinner.
c. But he also eats them for breakfast and snacks.
d. On the other hand, he?s allergic to them.
But, as already noted, structural connectives do not admit crossing of predicate-
argument dependencies. If we admit crossing dependencies in examples (7) and (8),
we get
(9) a. Although John is very generous?
b. if you need some money?
550
Computational Linguistics Volume 29, Number 4
(i) (ii)
a
b
d
contrast[one/other]
elaboration
comparison[not only/but also]
a
b
d
elaboration
concession[although]
cc
condition[if]
Figure 3
Discourse structures associated with (i) example (7) and (ii) Example (8).
a cb
concession[although] condition[if]
elaboration
a b
elaboration
contrast[one/other] comparison[not only...]
(i) (ii)
dcd
Figure 4
(Impossible) discourse structures that would have to be associated with (i) Example (9) and
(ii) example (10).
c. he?s very hard to find?
d. you only have to ask him for it.
(10) a. On the one hand, Fred likes beans.
b. Not only does he eat them for dinner.
c. On the other hand, he?s allergic to them.
d. But he also eats them for breakfast and snacks.
Possible discourse structures for these (impossible) discourses are given in Figure 4.
Even if the reader finds no problem with these crossed versions, they clearly do not
mean the same thing as their uncrossed counterparts: In (10), but now appears to link
(10d) with (10c), conveying that despite being allergic to beans, Fred eats them for
breakfast and snacks. And although this might be inferred from (8), it is certainly not
conveyed directly. As a consequence, we stipulate that structural connectives do not
admit crossing of their predicate-argument dependencies.3
That is not all. Since we take the basic level of discourse structure to be a conse-
quence of (1) relations associated with explicit structural connectives and (2) relations
3 A reviewer has asked how much stretching is possible in discourse without losing its thread or having
to rephrase later material in light of the intervening material. One could ask a similar question about
the apparently unbounded dependencies of sentence-level syntax, which inattentive speakers are prone
to lose track of and ?fracture.? Neither question seems answerable on theoretical grounds alone, with
both demanding substantial amounts of empirical data from both written and spoken discourse. The
point we are trying to make is simply that there is a difference in discourse between any amount of
stretching and even the smallest amount of crossing.
551
Webber et al Anaphora and Discourse Structure
whose defeasible inference is triggered by adjacency, we stipulate that discourse struc-
ture itself does not admit crossing structural dependencies. (In this sense, discourse structure
may be truly simpler than sentence structure. To verify this, one might examine the
discourse structure of languages such as Dutch that allow crossing dependencies in
sentence-level syntax. Initial cursory examination does not give any evidence of cross-
ing dependencies in Dutch discourse.)
If we now consider the corresponding properties of discourse adverbials, we see
that they do admit crossing of predicate-argument dependencies, as shown in exam-
ples (11)?(13).
(11) a. John loves Barolo.
b. So he ordered three cases of the ?97.
c. But he had to cancel the order
d. because then he discovered he was broke.
(12) a. High heels are fine for going to the theater.
b. But wear comfortable shoes
c. if instead you plan to go to the zoo.
(13) a. Because Fred is ill
b. you will have to stay home
c. whereas otherwise the two of you could have gone to the zoo.
Consider first the discourse adverbial then in clause (11d). For it to get its first
argument from (11b) (i.e., the event that the discovery in (d) is ?after?), it must cross
the structural connection between clauses (c) and (d) associated with because). This
crossing dependency is illustrated in Figure 5(i). Now consider the discourse adverbial
instead) in clause (12c). For it to get its first argument from (12a) (i.e., going to the zoo is
an alternative to going to the theater), it must cross the structural connection between
clauses (12b) and (12c) associated with if. This crossing dependency is illustrated in
Figure 5(ii). Example (13) is its mirror image: For the discourse adverbial otherwise in
(13c) to get its first argument from (13a) (i.e., alternatives to the state/condition of
Fred being ill), it must cross the structural connection associated with because. This is
illustrated in Figure 5(iii).
Crossing dependencies are not unusual in discourse when one considers anaphora
(e.g., pronouns and definite NPs), as for example in
b
conseq[so]
contrast[but]
dc
seq[then]
b
explanation[because]
conditional[if]
a
contrast[but]
alt[instead]
a
c ba
contrast[whereas]
c
explanation?[because]
alt[otherwise]
(i) (ii) (iii)
Figure 5
Discourse structures for examples (11)?(13). Structural dependencies are indicated by solid
lines and dependencies associate with discourse adverbials are indicated by broken lines.
(explanation? is the inverse of explanation?i.e., with its arguments in reverse order. Such
relations are used to maintain the given linear order of clauses.)
552
Computational Linguistics Volume 29, Number 4
(14) Every mani tells every womanj hei meets that shej reminds himi of hisi
mother.
(15) Suei drives an Alfa Romeo. Shei drives too fast. Maryj races heri on week-
ends. Shej often beats heri. (Strube 1998)
This suggests that in examples (11)?(13), the relationship between the discourse ad-
verbial and its (initial) argument from the previous discourse might usefully be taken
to be anaphoric as well.4
2.2 Discourse Adverbials Do Behave like Anaphors
There is additional evidence to suggest that otherwise, then, and other discourse adver-
bials are anaphors. First, anaphors in the form of definite and demonstrative NPs can
take implicit material as their referents. For example, in
(16) Stack five blocks on top of one another. Now close your eyes and try
knocking {the tower, this tower} over with your nose.
both NPs refer to the structure which is the implicit result of the block stacking.
(Further discussion of such examples can be found in Isard [1975]; Dale [1992]; and
Webber and Baldwin [1992].) The same is true of discourse adverbials. In
(17) Do you want an apple? Otherwise you can have a pear.
the situation in which you can have a pear is one in which you don?t want an apple?
that is, one in which your answer to the question is ?no.? But this answer isn?t there
structurally: It is only inferred. Although it appears natural to resolve an anaphor to an
inferred entity, it would be much more difficult to establish such links through purely
structural connections: To do so would require complex transformations that introduce
invisible elements into discourse syntax with no deeper motivation. For example, in
(17), we would need a rule that takes a discourse unit consisting solely of a yes/no
question P? and replaces it with a complex segment consisting of P? and the clause
it is possible that P, with the two related by something like elaboration. Then and only
then could we account for the interpretation of the subsequent otherwise structurally,
by a syntactic link to the covert material (i.e., to the possibility that P holds, which
otherwise introduces an alterative to).
Second, discourse adverbials have a wider range of options with respect to their
initial argument than do structural connectives (i.e., coordinating and subordinating
conjunctions). The latter are constrained to linking a discourse unit on the right frontier
of the evolving discourse (i.e., the clause, sentence and larger discourse units to its
immediate left). Discourse adverbials are not so constrained. To see this, consider the
following example:
4 We are aware that crossing examples such as (11)?(13) are rare in naturally occurring discourse. We
believe that this is because they are only possible when, as here, strong constraints from the discourse
adverbial and from context prevent the adverbial from relating to the closest (leftmost) eventuality or
an eventuality coerced from that one. But rarity doesn?t necessarily mean ill-formedness or marginality,
as readers can see for themselves if they use Google to search the Web for strings such as because then, if
instead, and whereas otherwise, and consider (1) whether the hundreds, even thousands, of texts in which
these strings occur are ill-formed, and (2) what then, instead, and otherwise are relating in these texts.
One must look at rare events if one is studying complex linguistic phenomena in detail. Thus it is not
the case that only common things in language are real or worth further study.
553
Webber et al Anaphora and Discourse Structure
(18) If the light is red, stop. Otherwise you?ll get a ticket.
(If you do something other than stop, you?ll get a ticket.)
This can be paraphrased using the conjunction or:
If the light is red, stop, or you?ll get a ticket.
Here or links its right argument to a unit on the right frontier of the evolving discourse?
in this case, the clause stop on its immediate left. Now consider the related example
(19) If the light is red, stop. Otherwise go straight on.
(If the light is not red, go straight on.)
This cannot be paraphrased with or, as in
(20) If the light is red, stop, or go straight on.
even though both stop and If the light is red, stop are on the right frontier of the evolving
discourse structure. This is because otherwise is accessing something else, so that (20)
means something quite different from either (18) or (19). What otherwise is accessing,
which or cannot, is the interpretation of the condition alone.5 Thus discourse adver-
bials, like other anaphors, have access to material that is not available to structural
connectives.
Finally, discourse adverbials, like other anaphors, may require semantic represen-
tations in which their arguments are bound variables ranging over discourse entities.
That is, whereas it might be possible to represent Although P, Q using a binary modal
operator
(21) although(p, q)
where formulas p and q translate the sentences P and Q that although combines, we
cannot represent P . . .Nevertheless, Q this way. We need something more like
(22) p ? nevertheless(e, q)
The motivation for the variable e in this representation is that discourse adverbials,
like pronouns, can appear intrasententially in an analog of donkey sentences. Donkey
sentences such as example (23) are a special kind of bound-variable reading:
(23) Every farmer who owns a donkey feeds it rutabagas.
In donkey sentences, anaphors are interpreted as covarying with their antecedents:
The it that is being fed in (23) varies with the farmer who feeds it. These anaphors,
however, appear in a structural and interpretive environment in which a direct syn-
tactic relationship between anaphor and antecedent is normally impossible, so they
cannot be a reflex of true binding in the syntax-semantics interface. Rather, donkey
sentences show that discourse semantics has to provide variables to translate pronouns,
5 This was independently pointed out by several people when this work was presented at ESSLLI?01 in
Helsinki in August 2001. The authors would like to thank Natalia Modjeska, Lauri Karttunen, Mark
Steedman, Robin Cooper, and David Traum for bringing it to their attention.
554
Computational Linguistics Volume 29, Number 4
and that discourse mechanisms must interpret these variables as bound?even though
the pronouns appear ?free? by syntactic criteria.
Thus, it is significant that discourse adverbials can appear in their own version of
donkey sentences, as in
(24) a. Anyone who has developed innovative new software has then had to
hire a lawyer to protect his/her interests. (i.e., after developing innovative
new software)
b. Several people who have developed innovative new software have
nevertheless failed to profit from it. (i.e., despite having developed innovative
new software)
c. Every person selling ?The Big Issue? might otherwise be asking for
spare change. (i.e., if he/she weren?t selling ?The Big Issue?)
The examples in (24) involve binding in the interpretation of discourse adverbials.
In (24a), the temporal use of then locates each hiring event after the corresponding
software development. Likewise in (24b), the adversative use of nevertheless signals
each developer?s unexpected failure to turn the corresponding profit. And in (24c),
otherwise envisions each person?s begging if that person weren?t selling ?The Big Issue?.
Such bound interpretations require variables in the semantic representations and
alternative values for them in some model?hence the representation given in (22).
Indeed, it is clear that the binding here has to be the discourse kind, not the syntac-
tic kind, for the same reason as in (23), although we cannot imagine anyone arguing
otherwise, since discourse adverbials have always been treated as elements of dis-
course interpretation. So the variables must be the discourse variables usually used to
translate other kinds of discourse anaphors.6
These arguments have been directed at the behavioral similarity between discourse
adverbials and what we normally take to be discourse anaphors. But this isn?t the only
reason to recognize discourse adverbials as anaphors: In the next section, we suggest
a framework for anaphora that is broad enough to include discourse adverbials as
well as definite and demonstrative pronouns and NPs, along with other discourse
phenomena that have been argued to be anaphoric, such as VP ellipsis (Hardt 1992,
1999; Kehler 2002), tense (Partee 1984; Webber 1988) and modality (Kibble 1995; Frank
and Kamp 1997; Stone and Hardt 1999).
3. A Framework for Anaphora
Here we show how only a single extension to a general framework for discourse
anaphora is needed to cover discourse adverbials. The general framework is presented
in Section 3.1, and the extension in Section 3.2.
3.1 Discourse Referents and Anaphor Interpretation
The simplest discourse anaphors are coreferential: definite pronouns and definite NPs
that denote one (or more) discourse referents in focus within the current discourse
6 Although rhetorical structure theory (RST) (Mann and Thompson 1998) was developed as an account
of the relation between adjacent units within a text, Marcu?s guide to RST annotation (Marcu 1999) has
added an ?embedded? version of each RST relation in order to handle examples such as (24). Although
this importantly recognizes that material in an embedded clause can bear a semantic relation to its
matrix clause, it does not contribute to understanding the nature of the phenomenon.
555
Webber et al Anaphora and Discourse Structure
context. (Under coreference we include split reference, in which a plural anaphor such
as the companies denotes all the separately mentioned companies in focus within the
discourse context.) Much has been written about the factors affecting what discourse
referents are taken to be in focus. For a recent review by Andrew Kehler, see chap-
ter 18 of Jurafsky and Martin (2000). For the effect of different types of quantifiers on
discourse referents and focus, see Kibble (1995).
Somewhat more complex than coreference is indirect anaphora (Hellman and
Fraurud 1996) (also called partial anaphora [Luperfoy 1992], textual ellipsis [Hahn,
Markert, and Strube 1996], associative anaphora [Cosse 1996] bridging anaphora
[Clark 1975; Clark and Marshall 1981; Not, Tovena, and Zancanaro 1999], and in-
ferrables [Prince 1992]), in which the anaphor (usually a definite NP) denotes a dis-
course referent associated with one (or more) discourse referents in the current discourse
context; for example,
(25) Myra darted to a phone and picked up the receiver.
Here the receiver denotes the receiver associated with (by virtue of being part of) the
already-mentioned phone Myra darted to.
Coreference and indirect anaphora can be uniformly modeled by saying that the
discourse referent e? denoted by an anaphoric expression ? is either equal to or asso-
ciated with an existing discourse referent er, that is, e?=er or e? ?assoc(er). But coref-
erence and associative anaphora do not exhaust the space of constructs that derive
all or part of their sense from the discourse context and are thus anaphoric. Consider
?other NPs? (Bierner 2001a; Bierner and Webber 2000; Modjeska 2001, 2002), as in:
(26) Sue grabbed one phone, as Tom darted to the other phone.
Although ?other NPs? are clearly anaphoric, should the referent of the other phone
(e?)?the phone other than the one Sue grabbed (er)?simply be considered a case of
e? ? assoc(er)? Here are two reasons why they should not.
First, in all cases of associative anaphora discussed in the literature, possible as-
sociations have depended only on the antecedent er and not on the anaphor. For
example, only antecedents that have parts participate in whole-part associations (e.g.,
phone ? receiver). Only antecedents with functional schemata participate in schema-
based associations (e.g., lock ? key). In (26), the relationship between e?, the referent
of the other phone, and its antecedent, er, depends in part on the anaphor, and not just
on the antecedent?in particular, on the presence of the word other.
Second, we also have examples such as
(27) Sue lifted the receiver as Tom darted to the other phone.7
in which the referent of the other phone (e?) is the phone other than the phone associated
with the receiver that Sue lifted. Together, these two points argue for a third possibility,
in which an anaphoric element can convey a specific function f? that is idiosyncratic
to the anaphor, which may be applied to either er or an associate of er. The result of
that application is e?. For want of a better name, we will call these lexically specified
anaphors.
Other lexically specified anaphors include noun phrases headed by other (exam-
ple (28)), NPs with such but no postmodifying as phrase (example (29)), comparative
7 Modjeska (2001) discovered such examples in the British National Corpus.
556
Computational Linguistics Volume 29, Number 4
NPs with no postmodifying than phrase (example (30)), and the pronoun elsewhere
(example (31)) (Bierner 2001b)
(28) Some dogs are constantly on the move. Others lie around until you call
them.
(29) I saw a 2kg lobster in the fish store yesterday. The fishmonger said it takes
about five years to grow to such a size.
(30) Terriers are very nervous. Larger dogs tend to have calmer dispositions.
(31) I don?t like sitting in this room. Can we move elsewhere?
To summarize the situation with anaphors so far, we have coreference when e?=er,
indirect anaphora when e? ?assoc(er), and lexically specified anaphora when e?=f?(ei)
where ei = er or ei ?assoc(er).
3.2 Discourse Adverbials as Lexical Anaphors
There is nothing in this generalized approach to discourse anaphora that requires that
the source of er be an NP, or that the anaphor be a pronoun or NP. For example, the
antecedent er of a singular demonstrative pronoun (in English, this or that) is often
an eventuality that derives from a clause, a sentence, or a larger unit in the recent
discourse (Asher 1993; Byron 2002; Eckert and Strube 2000; Webber 1991). We will
show that this is the case with discourse adverbials as well.
The extension we make to the general framework presented above in order to
include discourse adverbials as discourse anaphors is to allow more general functions
f? to be associated with lexically specified anaphors. In particular, for the discourse
adverbials considered in this article, the function associated with an adverbial maps its
anaphoric argument?an eventuality derived from the current discourse context?to
a function that applies to the interpretation of the adverbial?s matrix clause (itself an
eventuality). The result is a binary relation that holds between the two eventualities
and is added to the discourse context. For example, in
(32) John loves Barolo. So he ordered three cases of the ?97. But he had to
cancel the order because he then discovered he was broke.
then, roughly speaking, contributes the fact that its matrix clause event (John?s find-
ing he was broke) is after the anaphorically derived event of his ordering the wine.8
Similarly, in
(33) John didn?t have enough money to buy a mango. Instead, he bought a
guava.
instead contributes the fact that its matrix clause event (buying a guava) is an alternative
to the anaphorically derived event of buying a mango. The relation between the two
sentences is something like result, as in So instead, he bought a guava.
8 Words and phrases that function as discourse adverbials usually have other roles as well: For example,
otherwise also serves as an adjectival modifier, as in I was otherwise occupied with grading exams. This
overloading of closed-class lexico-syntactic items is not unusual in English, and any ambiguities that
arise must be handled as part of the normal ambiguity resolution process.
557
Webber et al Anaphora and Discourse Structure
Note that our only concern here is with the compositional and anaphoric mech-
anisms by which adverbials contribute meaning. For detailed analysis of the lexical
semantics of adverbials (but no attention to mechanism), the reader is referred to Jayes
and Rossari (1998a, 1998b, Lagerwerf (1998), Traugott (1995, 1997), and others.
Formally, we represent the function that a discourse adverbial ? contributes as a
?-expression involving a binary relation R? that is idiosyncratic to ?, one of whose
arguments (represented here by the variable EV) is resolved anaphorically:
?x . R?(x, EV)
R? gets its other argument compositionally, when this ?-expression is applied to ??s
matrix clause S interpreted as an eventuality ?, that is,
[?x . R?(x, EV)]? ? R?(?, EV)
The result of both function application and resolving EV to some eventuality ei derived
from the discourse context either directly or by association is the proposition R?(?, ei),
one of whose arguments (ei) has been supplied by the discourse context and the other
(?) compositionally from syntax.
Note that this is a formal model, meant to have no implications for how pro-
cessing takes place. We have not tried at this stage to instantiate our view of how
discourse adverbials are resolved in the context of (simultaneous) sentence-level and
discourse-level processing. Our basic view is that resolution is initiated when the dis-
course adverbial (?) is encountered. As ??s matrix clause S is incrementally parsed and
interpreted, producing eventuality ?, the resolution process polls the discourse context
and either finds an appropriate eventuality ei (or creates one by a bridging inference,
as illustrated in the next section) such that R?(?, ei) makes sense with respect to the
discourse so far. As is the case with resolving a discourse deictic (Asher 1993; Byron
2002; Eckert and Strube 2000; Webber 1991) this resolution process would use syn-
tactic and semantic constraints that it accumulates as the incremental sentence-level
parser/interpreter processes S. As with discourse deixis, this is best seen as a con-
straint satisfaction problem that involves finding or deriving an eventuality from the
current discourse context that meets the constraints of the adverbial with respect to the
eventuality interpretation of the matrix clause. (Examples of this are given throughout
the rest of the article.)
3.3 A Logical Form for Eventualities
Before using this generalized view of anaphora to show what discourse adverbials
contribute to discourse and how they interact with discourse relations that arise from
adjacency or explicit discourse connectives, we briefly describe how we represent
clausal interpretations in logical form (LF).
Essentially, we follow Hobbs (1985) in using a rich ontology and a representation
scheme that makes explicit all the individuals and abstract objects (i.e., propositions,
facts/beliefs, and eventualities) (Asher 1993) involved in the LF interpretation of an
utterance. We do so because we want to make intuitions about individuals, eventual-
ities, lexical meaning, and anaphora as clear as possible. But certainly, other forms of
representation are possible.
In this LF representation scheme, each clause and each relation between clauses
is indexed by the label of its associated abstract object. So, for example, the LF inter-
pretation of the sentence
(34) John left because Mary left.
558
Computational Linguistics Volume 29, Number 4
would be written
e1:left(j) ? john(j) ? e2:left(m) ? mary(m) ? e3:because(e1,e2)
where the first argument of the asymmetric binary predicate because is the consequent
and the second is the eventuality leading to this consequent. Thus when because occurs
sentence-medially, as in the above example, the eventuality arguments are in the same
order as their corresponding clauses occur in the text. When because occurs sentence-
initially (as in Because Mary left, John did), the interpretation of the second clause (John
[left]) will appear as the first argument and the interpretation of the first clause (Mary
left) will appear as the second.9
The set of available discourse referents includes both individuals like j and m, and
also abstract objects like e1 and e2. We then represent resolved anaphors by reusing
these discourse referents. So, for example, the LF interpretation of the follow-on sen-
tence
(35) This upset Sue.
would be written
e4:upset(DPRO, s) ? sue(s)
where DPRO is the anaphoric variable contributed by the demonstrative pronoun this.
Since the subject of upset could be either the eventuality of John?s leaving or the fact
that he left because Mary left, DPRO could be resolved to either e1 or e3, that is,
a. e4:upset(e1, s) ? sue(s)
b. e4:upset(e3, s) ? sue(s)
depending on whether one took Sue to have been upset by (1) John?s leaving or (2)
that he left because Mary left.
3.4 The Contribution of Discourse Adverbials to Discourse Semantics
Here we step through some examples of discourse adverbials and demonstrate how
they make their semantic contribution to the discourse context. We start with exam-
ple (32), repeated here as (36):
(36) a. John loves Barolo.
b. So he ordered three cases of the ?97.
c. But he had to cancel the order
d. because he then discovered he was broke.
9 We are not claiming to give a detailed semantics of discourse connectives except insofar as they may
affect how discourse adverbials are resolved. Thus, for example, we are not bothering to distinguish
among different senses of because (epistemic vs. nonepistemic), while (temporal vs. concessive), since
(temporal vs. causal), etc. Of course, these distinctions are important to discourse interpretation, but
they are independent of and orthogonal to the points made in this article. Similarly, Asher (1993)
argues that a simple ontology of eventualities is too coarse-grained, and that discourse representations
need to distinguish different kinds of abstract objects, including actions, propositions, and facts as well
as eventualities. Different discourse connectives will require different kinds of abstract objects as
arguments. This distinction is also orthogonal to the points made in this article, because we can
understand these abstract referents to be associates of the corresponding Hobbsian eventualities and
leave the appropriate choice to the lexical semantics of discourse connectives. Byron (2002) advocates a
similar approach to resolving discourse anaphora.
559
Webber et al Anaphora and Discourse Structure
Using the above LF representation scheme and our notation from Section 3.2, namely,
? ? = the anaphoric expression (here, the discourse adverbial)
? R? = the relation name linked with ?
? S = the matrix clause/sentence containing ?
? ? = the interpretation of S as an abstract object
and ignoring, for now, the conjunction because (discussed in section 4), the relevant
elements of (36d) can be represented as:
? = then
R? = after
S = he [John] discovered he was broke
? = e4:find(j,e5), where e5:broke(j)
This means that the unresolved interpretation of (36d) is
[?x . R?(x,EV)]? ? [?x . after(x,EV)]e4 ? after(e4, EV)
The anaphoric argument EV is resolved to the eventuality e2, derived from (36b)?
e2:order(j, c1).
after(e4,EV) ? after(e4,e2)
That is, the eventuality of John?s finding he was broke is after that of John?s ordering
three cases of the ?97 Barolo. The resulting proposition after(e4,e2) would be given its
own index, e6, and added to the discourse context.
When then is understood temporally, as it is above, as opposed to logically, it
requires a culminated eventuality from the discourse context as its first argument
(which Vendler (1967) calls an achievement or an accomplishment). The ordering
event in (36b) is such a Vendlerian accomplishment. In example (37), though, there
is no culminated eventuality in the discourse context for then), to take as its first
argument.
(37) a. Go west on Lancaster Avenue.
b. Then turn right on County Line.
How does (37b) get its interpretation?
As with (36d), the relevant elements of (37b) can be represented as
? = then
R? = after
S = turn right on County Line
? = e3:turn-right(you, county line)
and the unresolved interpretation of (37b) is thus
[? x . after(x, EV)]e3 ? after(e3, EV)
560
Computational Linguistics Volume 29, Number 4
As for resolving EV, in a well-known article, Moens and Steedman (1988) discuss
several ways in which an eventuality of one type (e.g., a process) can be coerced into
an eventuality of another type (e.g., an accomplishment, which Moens and Steedman
call a culminated process). In this case, the matrix argument of then (the eventuality of
turning right on County Line) can be used to coerce the process eventuality in (37b) into
a culminated process of going west on Lancaster Avenue until County Line. We treat this
coercion as a type of associative or bridging inference, as in the examples discussed
in section 3.1. That is,
e2 = culmination(e1)?assoc(e1), where e1:go-west(you, lancaster ave)
Taking this e2 as the anaphoric argument EV of then yields the proposition
after(e3, e2)
That is, the eventuality of turning right onto County Line is after that of going west
on Lancaster Avenue to County Line. This proposition would be indexed and added
to the discourse context.
It is important to stress here that the level of representation we are concerned
with is essentially an LF for discourse. Any reasoning that might then have to be done
on the content of LFs might then require making explicit the different modal and
temporal contexts involved, their accessibility relations, the status of abstract objects
as facts, propositions or eventualities, and so on. But as our goal here is primarily
to capture the mechanism by means of which discourse adverbials are involved in
discourse structure and discourse semantics, we will continue to assume for as long
as possible that an LF representation will suffice.
Now it may appear as if there is no difference between treating adverbials as
anaphors and treating them as structural connectives, especially in cases like (37) in
which the antecedent comes from the immediately left-adjacent context, and in which
the only obvious semantic relation between the adjacent sentences appears to be the
one expressed by the discourse adverbial. (Of course, there may also be a separate
intentional relation between the two sentences [Moore and Pollack 1992], independent
of the relation conveyed by the discourse adverbial.)
One must distinguish, however, between whether a theory allows a distinction
to be made and whether that distinction needs to be made in a particular case. It
is clear that there are many examples in which the two approaches (i.e., a purely
structural treatment of all connectives, versus one that treats adverbials as linking into
the discourse context anaphorically) appear to make the same prediction. We have
already, however, demonstrated cases in which a purely structural account makes the
wrong prediction, and in the next section, we will demonstrate the additional power
of an account that allows for two relations between an adverbial?s matrix clause or
sentence and the previous discourse: one arising from the anaphoric connection and
the other inferred from adjacency or conveyed explicitly by a structural connective.
Before closing this section, we want to step through examples (19)?(20), repeated
here as examples (38)?(39).
(38) If the light is red, stop. Otherwise you?ll get a ticket.
(39) If the light is red, stop. Otherwise go straight on.
561
Webber et al Anaphora and Discourse Structure
Roughly speaking, otherwise conveys that the complement of its anaphorically derived
argument serves as the condition under which the interpretation of its structural ar-
gument holds. (This complement must be with respect to some contextually relevant
set.)10
If we represent a conditional relation between two eventualities with the asym-
metric relation if(e1,e2), where e1 is derived from the antecedent and e2 from the conse-
quent, and we approximate a single contextually relevant alternative e2 to an eventu-
ality e1 using a symmetric complement relation, complement(e1, e2), then we can represent
the interpretation of otherwise as
? x . if(VE, x), where complement(VE, EV)
where variable EV is resolved anaphorically to an eventuality in the current discourse
context that admits a complement. That is, otherwise requires a contextually relevant
complement to its antecedent and asserts that if that complement holds, the argument
to the ?-expression will as well. The resulting ?-expression applies to the interpretation
of the matrix clause of otherwise, resulting in the conditional?s being added to the
discourse context:
[?x . if(VE,x)] ? ? if(VE,?), where complement(VE,EV)
Here the relevant elements of (38b) and (39b) can be represented as
? = otherwise
R? = if
S38 = you get a ticket
?38 = e3, where e3:get ticket(you)
S39 = go straight on
?39 = e3? , where e3? :go straight(you)
The unresolved interpretations of (38b) and (39b) are thus:
[?x . if(VE38,x)] e3 ? if(VE38,e3), where complement(VE38,EV38)
[?x . if(VE39,x)] e3? ? if(VE39,e3? ), where complement(VE39,EV39)
As we showed in section 2.2, different ways of resolving the anaphoric argument lead
to different interpretations. In (38), the anaphoric argument is resolved to e2:stop(you),
10 Kruijff-Korbayova? and Webber (2001a) demonstrate that the information structure of sentences in the
previous discourse (theme-rheme partitioning, as well as focus within theme and within rheme
[Steedman 2000a]) can influence what eventualities er are available for resolving the anaphorically
derived argument of otherwise. This then correctly predicts different interpretations for ?otherwise? in
(i) and (ii):
(i) Q. How should I transport the dog?
A. You should carry the dog. Otherwise you might get hurt.
(ii) Q. What should I carry?
A. You should carry the dog. Otherwise you might get hurt.
In both (i) and (ii), the questions constrain the theme/rheme partition of the answer. Small capitals
represent focus within the rheme. In (i), the otherwise clause will be interpreted as warning the hearer
(H) that H might get hurt if he/she transports the dog in some way other than carrying it (e.g., H might
get tangled up in its lead). In (ii), the otherwise clause warns H that he/she might get hurt if what she
is carrying is not the dog (e.g., H might be walking past fanatical members of the Royal Kennel Club).
562
Computational Linguistics Volume 29, Number 4
whereas in (39), it is resolved to e1:red(light1). Thus the resulting interpretations of
(38b) and (39b) are, respectively,
if(e4,e3), where complement(e2,e4) and e2:stop(you)
(If you do something other than stop, you?ll get a ticket.)
if(e4? , e3? ), where complement(e1,e4? ) and e1:red(light)
(If the light is not red, go straight on.)
We have not been specific about how the anaphoric argument of otherwise (or
of any other discourse adverbial) is resolved, other than having it treated as a con-
straint satisfaction problem. This is the subject of current and future work, exploring
the empirical properties of resolution algorithms with data drawn from appropriately
annotated corpora and from psycholinguistic studies of human discourse interpreta-
tion. To this end, Creswell et al (2002) report on a preliminary annotation study of
discourse adverbials and the location and type of their antecedents. This initial ef-
fort involves nine discourse adverbials?three each from the classes of concessive,
result, and reinforcing (additive) conjuncts given in Quirk et al (1972). Meanwhile,
Venditti et al (2002) present a preliminary report on the use of a constraint satisfac-
tion model of interpretation, crucially combining anaphoric and structural reasoning
about discourse relations, to predict subjects? on-line interpretation of discourses in-
volving stressed pronouns. In addition, two proposals have recently been submitted to
construct a larger and more extensively annotated corpus, covering more adverbials,
based on what we have learned from this initial effort. This more extensive study
would be an adequate basis for developing resolution algorithms.11
3.5 Summary
In this section, we have presented a general framework for anaphora with the follow-
ing features:
? Anaphors can access one or more discourse referents or entities
associated with them through bridging inferences. These are sufficient
for interpreting anaphoric pronouns, definite NPs and demonstrative
NPs, allowing entities to be evoked by NPs or by clauses. In the case of
clauses, this may be on an as-needed basis, as in Eckert and Strube
(2000).
? A type of anaphor ? that we call lexically specified can also contribute
additional meaning through a function f? that is idiosyncratic to ?, that
can be applied to either an existing discourse referent or an entity
associated with it through a bridging inference. In the case of the
premodifier other, f? applied to its argument produces contextually
11 With respect to how many discourse adverbials there are, Quirk et al (1972) discuss 60 conjunctions
and discourse adverbials under the overall heading time relations and 123 under the overall heading
conjuncts. Some entries appear under several headings, so that the total number of conjunctions and
discourse adverbials they present is closer to 160. In another enumeration of discourse adverbials,
Forbes and Webber (2002) start with all annotations of sentence-level adverbials in the Penn Treebank,
then filter them systematically to determine which draw part of their meaning from the preceding
discourse and how they do so. What we understand from both of these studies is that there are fewer
than 200 adverbials to be considered, many of which are minor variations of one another (in contrast, by
contrast, by way of contrast, in comparison, by comparison, by way of comparison that are unlikely to differ in
their anaphoric properties, and some of which, such as contrariwise, hitherto, and to cap it all, will occur
only rarely in a corpus of modern English.
563
Webber et al Anaphora and Discourse Structure
relevant alternatives to that argument. In the case of the premodifier
such, it yields a set of entities that are similar to its argument in a
contextually relevant way.
? Discourse adverbials are lexically specified anaphors whose meaning
function f? is a ?-expression involving a binary relation R? that is
idiosyncratic to ?, one of whose arguments is resolved anaphorically and
the other is provided compositionally, when the ?-expression is applied
to ??s matrix clause interpreted as an eventuality ?.
In the next section, we move on to consider how the presence of both a semantic rela-
tion associated with a discourse adverbial and a semantic relation associated with the
adjacency of two clauses or a structural connective between them allows for interesting
interactions between the two.
4. Patterns of Anaphoric Relations and Structural/Inferred Relations
Prior to the current work, researchers have treated both explicit structural connec-
tives (coordinating and subordinating conjunctions, and ?paired? conjunctions) and
discourse adverbials simply as evidence for a particular structural relation holding
between adjacent units. For example, Kehler (2002) takes but as evidence of a contrast
relation between adjacent units, in general as evidence of a generalization relation,
in other words as evidence of an elaboration relation, therefore as evidence of a result
relation, because as evidence of an explanation relation, and even though as evidence
of a denial of preventer relation (Kehler 2002, Section 2.1). Here Kehler has probably
correctly identified the type of relation that holds between elements, but not which
elements it holds between.
In one respect, we follow previous researchers, in that we accept that when clauses,
sentences, or larger discourse units are placed adjacent to one another, listeners infer a
relation between the two, and that the structural connective (coordinate or subordinate
conjunction) gives evidence for the relation that is intended to hold between them.
Because we take discourse adverbials to contribute meaning through an anaphoric
connection with the previous discourse, however, this means that there may be two
relations on offer and opens up the possibility that the relation contributed by the
discourse adverbial can interact in more than one way with the relation conveyed
by a structural connective or inferred through adjacency. Below we show that this
prediction is correct.
We start from the idea that, in the absence of an explicit structural connective, de-
feasible inference correlates with structural attachment of adjacent discourse segments
in discourse structure, relating their interpretations. The most basic relation is that the
following segment in some way describes the same object or eventuality as the one it
abuts (elaboration). But evidence in the segments can lead (via defeasible inference) to
a more specific relation, such as one of the resemblance relations (e.g., parallel, contrast,
exemplification, generalisation), or cause-effect relations (result, explanation, violated expecta-
tion), or contiguity relations (narration) described in Hobbs (1990) and Kehler (2002). If
nothing more specific can be inferred, the relation will remain simply elaboration. What
explicit structural connectives can do is convey relations that are not easy to convey
by defeasible inference (e.g., if, conveying condition, and or, conveying disjunction) or
provide nondefeasible evidence for an inferrable relation (e.g., yet, so, and because).
Discourse adverbials can interact with structural connectives, with adjacency-
triggered defeasible inference, and with each other. To describe the ways in which we
564
Computational Linguistics Volume 29, Number 4
have so far observed discourse adverbials to interact with relations conveyed struc-
turally, we extend the notation used in the previous section:
? ? = discourse adverbial
? R? = the name of the relation associated with ?
? S = the matrix clause/sentence of ?
? ? = the logical form (LF) interpretation of S
adding the following:
? D = the discourse unit that is left-adjacent to S, to which a relationship
holds by either inference or a structural connective
? ? = the LF interpretation of D
? R = the name of the relation that holds with ?
Although ? is one argument of R, we show below that the other argument of R may
be one of at least two different abstract objects.
Case 1: ? separately serves as an argument to both R? and R. This is the case that
holds in example (36) (repeated here):
(36) a. John loves Barolo.
b. So he ordered three cases of the ?97.
c. But he had to cancel the order
d. because he then discovered he was broke.
We have already seen that the interpretation of the clause in (36d) following because
involves
R? = after
? = e4:discover(j,e5), where e5:broke(j)
[?x . after(x,EV)]e4 ? after(e4, EV)
where EV is resolved to e2:order(j, c1), and the proposition after(e4, e2) is added to the
discourse context?that is, John?s discovering he was broke is after his ordering the
wine.
Now consider the explanation relation R associated with because in (36d). It relates
e4, John?s finding he was broke, to the intepretation of (36c), e3:cancel(j,o1)?that is,
explanation(e4,e3). Clause 36d thus adds both explanation(e4,e3) and after(e4, e2) to the
discourse. Although these two propositions share an argument (e4), they are neverthe-
less distinct.12
12 Because eventuality e4, John?s finding he was broke, both explains the canceling and follows the ordering, it
follows that the canceling is after the ordering.
565
Webber et al Anaphora and Discourse Structure
Case 2: R?(?, ei) is an argument of R. In case 1, it is the interpretation of the ad-
verbial?s matrix clause ? that serves as one argument to the discourse relation R. In
contrast, in case 2, that argument is filled by the relation contributed by the discourse
adverbial (itself an abstract object available for subsequent reference). In both cases,
the other argument to R is ?.
One configuration in which case 2 holds is with the discourse adverbial otherwise.
Recall from section 3.4 that the interpretation of otherwise involves a conditional relation
between the complement of its anaphoric argument and the interpretation ? of its
matrix clause:
[?x . if(VE,x)] ? ? if(VE,?), where complement(VE,EV)
With variable EV resolved to an eventuality in the discourse context, it is the resulting
relation (viewed as an abstract object) that serves as one argument to R, with ? serving
as the other. We can see this most clearly by considering variants of examples (38) and
(39) that contain an explicit connective between the clauses. In (38), the conjunction
because is made explicit (example (40)), and in (39), the connective is simply and or but
(example (41)).
(40) If the light is red, stop, because otherwise you?ll get a ticket.
R? = if
?38 = e3:get ticket(you)
(41) If the light is red, stop, and/but otherwise go straight on.
R? = if
?39 = e3? :go straight(you)
In the case of (40), resolving otherwise contributes the relation
e6: if(e4,e3), where complement(e4,e2) and e2:stop(you)
(If you do something other than stop, you?ll get a ticket.)
At the level of LF, the abstract object e6 that is associated with the conditional relation
serves as one argument to the explanation relation contributed by because, with e2 being
the other. That is, because and otherwise together end up contributing explanation(e2,e6)
(i.e., your needing to stop is explained by the fact that if you do something other than
stop, you?ll get a ticket).
In the case of (41), resolving otherwise contributes the relation
e6? :if(e4? , e3? ), where complement(e4? ,e1) and e1:red(light)
(If the light is not red, go straight on.)
What is the discourse relation to which otherwise contributes this abstract object e6??
Whether the connective is and or but, both its conjuncts describe (elaborate) alternative
specializations of the same situation e0 introduced earlier in the discourse (e.g., e0 could
be associated with the first sentence of Go another mile and you?ll get to a bridge. If the
light is red, stop. Otherwise go straight on.) If the connective is and, what is added to
context might simply be elaboration(e6? ,e0). (Note that without otherwise, the relation
elaboration(e5,e0) would have been added to context, where e5 is the abstract object
associated with the interpretation of If the light is red, stop.) If the connective is but, then
one might also possibly add contrast(e6? ,e5)?that is, the situation that (if the light is
566
Computational Linguistics Volume 29, Number 4
red) you should stop is in contrast to the situation that if the light is not red, you
should go straight on.13
As is clear from the original pair of examples (38) and (39), interpretations can
arise through adjacency-triggered inference that are similar to those that arise with an
explicit connective. In either case, the above treatment demonstrates that there is no
need for a separate otherwise relation, as proposed in rhetorical structure theory (Mann
and Thompson 1988). We are not, however, entirely clear at this point when case 1
holds and when case 2 does. A more careful analysis is clearly required.
Case 3: R? is parasitic on R. Case 3 appears to apply with discourse adverbials such
as for example and for instance. The interpretation of such adverbials appears to be
parasitic on the relation associated with a structural connective or discourse adverbial
to their left, or on an inferred relation triggered by adjacency. The way to understand
this is to first consider intraclausal for example, where it follows the verb, as in
(42) Q. What does this box contain?
A. It contains, for example, some hematite.
The interpretation of for example here involves abstracting the meaning of its matrix
structure with respect to the material to its right, then making an assertion with respect
to this abstraction. That is, if the LF contributed by the matrix clause of (42A) is,
roughly,
i. contain(box1,hematite1)
then the LF resulting from the addition of for example can be written either with set
notation (as in (ii)), taking an entity to exemplify a set, or with ?-notation (as in (iii)),
taking an entity to exemplify a property:
ii. exemplify(hematite1, {X | contain(box1,X)})
iii. exemplify(hematite1, ?X . contain(box1,X))
Both express the fact that hematite is an example of what is contained in the box.14 Since
one can derive (i) logically from either (ii) or (iii), one might choose to retain only (ii) or
(iii) and derive (i) if and when it is needed. In the remainder of the article, we use the
? notation given in (iii). Note that from the perspective of compositional semantics, for
example resembles a quantifier, in that the scope of its interpretation is not isomorphic
to its syntactic position. Thus producing an interpretation for for example will require
techniques similar to those that have long been used in interpreting quantifiers (Woods,
1978; Barwise and Cooper 1981). We take this up again in section 5.
If we look at the comparable situation in discourse, such as (43)?(44), where for
example occurs to the right of a discourse connective, it can also be seen as abstracting
13 A much finer-grained treatment of the semantics of otherwise in terms of context-update potential is
given in Kruijff-Korbayova? and Webber (2001b). Here we are just concerned with its interaction with
structural connectives and adjacency-triggered relations.
14 The material to the right of for example can be any kind of constituent, including such strange ones as
John gave, for example, a flower to a nurse.
Here, a flower to a nurse would be an example of the set of object-recipient pairs within John?s givings.
Such nonstandard constituents are also found with coordination, which was one motivation for
combinatory categorial grammar (Steedman 1996). This just illustrates another case in which such
nonstandard constituents are needed.
567
Webber et al Anaphora and Discourse Structure
the interpretation of its discourse-level matrix structure, with respect to the material
to its right:
(43) John just broke his arm. So, for example, he can?t cycle to work now.
(44) You shouldn?t trust John because, for example, he never returns what he
borrows.
In (43), the connective so leads to
result(?,?)
being added to the discourse, where ? is the interpretation of John can?t cycle to work
now, and ? is the interpretation of John just broke his arm. For example then abstracts this
relation with respect to the material to its right (i.e., ?), thereby contributing
exemplify(?, ?X . result(X, ?))
That is, John can?t cycle to work is an example of what results from John?s breaking his
arm. Similarly, because in (44) leads to
explanation(?,?)
being added to the discourse, where ? is the interpretation of he never returns what he
borrows, ? is the interpretation of you shouldn?t trust John, and for example adds
exemplify(?, ?X . explanation(X,?))
that is, that ? is an example of the reasons for not trusting John.
For example interacts with discourse adverbials in the same way:
(45) Shall we go to the Lincoln Memorial? Then, for example, we can go to the
White House.
(46) As a money manager and a grass-roots environmentalist, I was very dis-
appointed to read in the premiere issue of Garbage that The Wall Street
Journal uses 220,000 metric tons of newsprint each year, but that only 1.4%
of it comes from recycled paper. By contrast, the Los Angeles Times, for ex-
ample, uses 83% recycled paper. [WSJ, from Penn Treebank /02/wsj-0269]
In example (45), the resolved discourse adverbial then leads to after(?,?) being added
to the discourse context, where ? is the interpretation of we can go to the White House, ?
is the interpretation of we shall go to the Lincoln Memorial, and for example adds
exemplify(?, ?X . after(X,?))
that is, that ? is an example of the events that [can] follow going to the Lincoln
Memorial. (As already noted, we are being fairly fast and loose regarding tense and
modality, in the interests of focusing on the types of interactions.)
568
Computational Linguistics Volume 29, Number 4
In example (46), the resolved discourse anaphor by contrast contributes contrast(?,?),
where ? is the interpretation of the Los Angeles Times?s using 83% recycled paper and ? is
the intepretation of only 1.4% of it [newsprint used by the WSJ] comes from recycled paper.
For example then contributes
exemplify(?, ?X . contrast(X,?))
that is, that ? is one example of contrasts with the WSJ?s minimal use of recycled
paper.
What occurs with discourse connectives and adverbials can also occur with rela-
tions added through adjacency-triggered defeasible inference, as in
(47) You shouldn?t trust John. For example, he never returns what he borrows.
explanation(?,?)
exemplify(?, ?X . explanation(?,X))
Here, as in (44), the relation provided by adjacency-triggered inference is R = explana-
tion, which is then used by for example.
But what about the many cases in which only exemplify seems present, as in
(48) In some respects they [hypertext books] are clearly superior to normal
books, for example they have database cross-referencing facilities ordinary
volumes lack. (British National Corpus, CBX 1087)
(49) He [James Bellows] and his successor, Mary Anne Dolan, restored respect
for the editorial product, and though in recent years the paper had been
limping along on limited resources, its accomplishments were notable. For
example, the Herald consistently beat its much-larger rival on disclosures
about Los Angeles Mayor Tom Bradley?s financial dealings.
There are at least two explanations: One is that for example simply provides direct
nondefeasible evidence for exemplify, which is the only relation that holds. The other
explanation follows the same pattern as the examples given above, but with no further
relation than elaboration(?,?). That is, we understand in (48) that having database cross-
referencing facilities elaborates the respects in which hypertext books are superior to
normal books, whereas in (49), we understand that the Herald?s [newspaper] consistently
beating its much-larger rival elaborates the claim that its accomplishments were notable. This
elaboration relation is then abstracted (in response to for example) to produce:
exemplify(?, ?X . elaboration(X, ?))
that is, that this is one example of many possible elaborations. Because this is more
specific than elaboration and seems to mean the same as exemplify(?,?), one might
simply take it to be the only relation that holds. Given that so many naturally occuring
instances of for example occur with elaboration, it is probably useful to persist with the
above shorthand. But it shouldn?t obscure the regular pattern that appears to hold.
Before going on to case 4, we should comment on an ambiguity associated with for
example. When for example occurs after an NP, a PP, or a clause that can be interpreted
as a general concept or a set, it can contribute a relation between the general concept
or set and an instance, rather than being parasitic on another relation. For example,
in:
569
Webber et al Anaphora and Discourse Structure
(50) In the case of the managed funds they will be denominated in a leading
currency, for example US dollar, . . . (BNC CBX 1590)
for example relates the general concept denoted by a leading currency to a specific in-
stance, U.S. dollars. (In British English, the BNC shows that most such examples occur
with such as?i.e., in the construction such as for example. This paraphrase does not work
with the predicate-abstracting for example that is of primary concern here, as in exam-
ple (42).)
But for example occurring after an NP, a PP, or a clause can, alternatively, contribute
a more subtle parasitic relationship to the previous clause, as in
(51) All the children are ill, so Andrew, for example, can?t help out in the shop.
This differs from both (43) and (50). That is, one cannot paraphrase (51) as (52) as in
(43) where for example follows so:
(52) All the children are ill, so for example Andrew can?t help out in the shop.
Example (52) simply specifies an example consequence of all the children being ill, as
does
(53) All the children are ill, so for example one of us has to be at home at all
times.
In contrast, (51) specifies an example consequence for Andrew, as one of the children.
Support for this comes from the fact that in (52), Andrew doesn?t have to be one of
the children: He could be their nanny or child minder, now stuck with dealing with a
lot of sick kids. But (51) is not felicitous if Andrew is not one of the children.
We suspect here the involvement of information structure (Steedman 2000a):
Whereas the interpretation conveyed by for example is parasitic on the adjacency rela-
tion (result in example (51)), its position after the NP Andrew in (51) may indicate a
contrastive theme with respect to the previous clause, according to which Andrew in
contrast to the other children suffers this particular consequence. But more work needs
to be done on this to gain a full understanding of what is going on.
Case 4: R? is a defeasible rule that incorporates R. Case 4 occurs with discourse
adverbials that carry the same presupposition as the discourse connectives although
and the concessive sense of while (Lagerwerf 1998). Case 4 shares one feature with
case 1, in that the discourse relation R conveyed by a structural connective or inferred
from adjacency holds between ? (the interpretation of the adverbial?s matrix clause)
and ? (the interpretation of the left-adjacent discourse unit). Where it differs is that
the result is then incorporated into the presupposition of the discourse adverbial. This
presupposition, according to Lagerwerf (1998), has the nature of a presupposed (or
conventionally implicated) defeasible rule that fails to hold in the current situation.
He gives as an example
(54) Although Greta Garbo was called the yardstick of beauty, she never mar-
ried.
This asserts both that Greta Garbo was called the yardstick of beauty and that she
never married. The first implies that Greta Garbo was beautiful. The example also
570
Computational Linguistics Volume 29, Number 4
presupposes that, in general, if a woman is beautiful, she will marry. If such a pre-
supposition can be accommodated, it will simply be added to the discourse context.
If not, the hearer will find the utterance confusing or possibly even insulting.
We argue here that the same thing happens with the discourse adverbials never-
theless and though. The difference is that, with discourse adverbials, the antecedent to
the rule derives anaphorically from the previous discourse, whereas the consequent
derives from the adverbial?s matrix clause. (With the conjunctions although and con-
cessive while, both arguments are provided structurally.)
We first illustrate case 4 with two examples in which nevertheless occurs in the
main clause of a sentence containing a preposed subordinate clause. The subordinate
conjunction helps clarify the relation between the clauses that forms the basis for the
presupposed defeasible rule. After these, we give a further example in which the
relation between the adjacent clauses comes through inference.
(55) While John is discussing politics, he is nevertheless thinking about his fish.
In (55), the conjunction while conveys a temporal relation R between the two clauses
it connects:
during(e2, e1), where e1:discuss(john,politics) and e2:think about(john,fish)
What nevertheless contributes to (55) is a defeasible rule based on this relation, which
we will write informally as
during(X,E) ? E:discuss(Y,politics)) > ?X:think about(Y,fish))
Normally, whatever one does during the time one is discussing politics, it is
not thinking about one?s fish.
This rule uses Asher and Morreau?s (1991) defeasible implication operator (>) and
abstracts over the individual (John), which seems appropriate for the general statement
conveyed by the present tense of the utterance.
Similarly, in
(56) Even after John has had three glasses of wine, he is nevertheless able to
solve difficult math problems.
the conjunction after contributes a relation between the two clauses it connects:
after(e2, e1), where e1:drink(john,wine) and e2:solve(john,hard problems)
What nevertheless contributes to this example is a defeasible rule that we will again
write informally as
after(X,E) ? E:drink(Y,wine)) > ?X:solve(Y,hard problems))
Normally, whatever one is able to do after one has had three glasses of wine, it
is not solving difficult algebra problems.
571
Webber et al Anaphora and Discourse Structure
Again, we have abstracted over the individual, as the presupposed defeasible rule
associated with the present-tense sentence appears to be more general than a statement
about a particular individual.15
On the other hand, in the following example illustrating a presupposed defeasi-
ble rule and a discourse relation associated with adjacency, it seems possible for the
presupposed defeasible rule to be about John himself:
(57) John is discussing politics. Nevertheless, he is thinking about his fish.
Here the discourse relation between the two clauses, each of which denotes a specific
event, is
during(e2, e1), where e1:discuss(john,politics) and e2:think about(john,fish)
(Note that our LF representation isn?t sufficiently rich to express the difference between
(55) and (57).) What nevertheless contributes here is the presupposed defeasible rule
during(X,e1) > ?X = e2
Normally what occurs during John?s discussing politics is not John?s thinking
about his fish.
Lagerwerf (1998) does not discuss how specific or general will be the presup-
posed defeasible rule that is accommodated or what factors affect the choice. Kruijff-
Korbayova? and Webber (2001a) also punt on the question, when considering the effect
of information structure on what presupposed defeasible rule is associated with al-
though. Again, this seems to be a topic for future work.
Summary
We have indicated four ways in which we have found the relation associated with a
discourse adverbial to interact with a relation R triggered by adjacency or conveyed
by structural connectives or, in some cases, by another relational anaphor:
1. ? separately serves as an argument to both R? and R.
2. R?(?, ei) is an argument of R.
3. R? is parasitic on R.
4. R? is a defeasible rule that incorporates R.
We do not know whether this list is exhaustive or whether a discourse adverbial
always behaves the same way vis-a`-vis other relations. Moreover, in the process of
setting down the four cases we discuss, we have identified several problems that we
have not addressed, on which further work is needed. Still, we hope that we have
convinced the reader of our main thesis: that by recognizing discourse adverbials
as doing something different from simply signaling the discourse relation between
adjacent discourse units and by considering their contribution as relations in their own
right, one can begin to characterize different ways in which anaphoric and structural
relations may themselves interact.
15 We speculate that the reason examples such as (55) and (56) sound more natural with the focus particle
even applied to the subordinate clause is that even conveys an even greater likelihood that the
defeasible rule holds, so nevertheless emphasizes its failure to do so.
572
Computational Linguistics Volume 29, Number 4
5. Lexicalized Grammar for Discourse Syntax and Semantics
The question we consider in this section is how the treatment we have presented of
discourse adverbials and structural connectives can be incorporated into a general
approach to discourse interpretation. There are three possible ways.
The first possibility is simply to incorporate our treatment of adverbials and con-
nectives into a sentence-level grammar, since such grammars already cover the syntax
of sentence-level conjunction (both coordinate and subordinate) and the syntax of
adverbials of all types. The problem with this approach is that sentence-level gram-
mars, whether phrasal or lexicalized, stop at explicit sentence-level conjunction and do
not provide any mechanism for forming the meaning of multiclausal units that cross
sentence-level punctuation. Moreover, as we have already shown in section 3, the
interpretation of discourse adverbials can interact with the implicit relation between
adjacent sentences, as well as with an explicitly signaled relation, so that a syntax and
compositional semantics that stops at the sentence will not provide all the structures
and associated semantics needed to build the structures and interpretations of interest.
The second possibility is to have a completely different approach to discourse-
level syntax and semantics than to sentence-level syntax and semantics, combining
(for example) a definite clause grammar with rhetorical structure theory. But as we
and others have already noted, this requires discourse semantics reaching further and
further into sentence-level syntax and semantics to handle relations between main and
embedded clauses, and between embedded clauses themselves, as in example (58).
(58) If they?re drunk and they?re meant to be on parade and you go to their
room and they?re lying in a pool of piss, then you lock them up for a day.
(The Independent, June 17, 1997)
Thus it becomes harder and harder to distinguish the scope of discourse-level syntax
and semantics from that at the sentence-level.
The third possibility is to recognize the overlapping scope and similar mechanisms
and simply extend a sentence-level grammar and its associated semantic mechanisms
to discourse. The additional responsibilities of the grammer would be to account for
the formation of larger units of discourse from smaller units; the projection of the
interpretation of smaller discourse units onto the interpretation of the larger discourse
units they participate in; and the effect of discourse unit interpretation on the evolv-
ing discourse model. There are two styles of grammar one could use for this: (1) a
phrase structure grammar (PSG) extended to discourse, as in Figure 6, or (2) a lexi-
calized grammar that extends to discourse, a sentence-level lexicalized grammar such
as tree-adjoining grammar (Joshi, 1987; XTAG-Group 2001) or combinatory categorial
grammar (CCG) (Steedman 1996, 2000b).
Whereas Polanyi and van den Berg (1996) extend a PSG to discourse, we argue
for extending a lexicalized grammar, even though TAG and CCG are weakly context-
sensitive (CS) and the power needed for a discourse grammar with no crossing de-
pendencies is only context-free (section 2.1). Our argument is based on our desire to
use a discourse grammar in natural language generation (NLG). It is well-known that
context-free PSGs (CF PSGs) set up a complex search space for NLG. A discourse
grammar specified in terms of phrase structure rules such as those shown in Figure 6
doesn?t provide sufficient guidance when reversed for use in generating discourse.
For example, one might end up having to guess randomly how many sentences and
connectives one had, in what order, before being able to fill in the sentences and con-
nectives with any content. More generally, trying to generate exactly a given semantics
573
Webber et al Anaphora and Discourse Structure
Seg := SPunct Seg | Seg SPunct | SPunct |
on the one hand Seg on the other hand Seg |
not only Seg but also Seg
SPunct := S Punctuation
Punctuation := . | ; | : | ? | !
S := S Coord S | S Subord S | Subord S S | Sadv S |
NP Sadv VP | S Sadv | . . .
Coord := and | or | but | so
Subord := although | after | because | before | ...
Sadv := DAdv | SimpleAdv
DAdv := instead | otherwise | for example | meanwhile | ...
SimpleAdv := yesterday | today | surprisingly | hopefully | ...
Figure 6
PS rules for a discourse grammar.
when semantics underspecifies syntactic dependency (as discourse semantics must, on
our account) is known to be intractable (Koller and Striegnitz 2002). An effective so-
lution is to generate semantics and syntax simultaneously, which is straightforward
with a lexicalized grammar (Stone et al 2001).
Given the importance of various types of inference in discourse understanding,
there is a second argument for using a lexicalized discourse grammar that derives from
the role of implicature in discourse. Gricean reasoning about implicatures requires a
hearer be able to infer the meaningful alternatives that a speaker had in composing a
sentence. With lexicalization, these alternatives can be given by a grammar, allowing
the hearer, for example, to ask sensible questions like ?Why did the speaker say ?in-
stead? here instead of nothing at all?? and draw implicatures from this. A CF PSG, on
the other hand, might suggest questions like ?Why did the speaker say two sentences
rather than one here?? which seem empirically not to lead to any real implicatures.
(On the contrast between choices, which seem to lead to implicatures, and mere alter-
native linguistic formulations, which do not seem to, see, for example, Dale and Reiter
[1995] and Levison [2000].)
In several previous papers (Webber, Knott, and Joshi, 2001; Webber et al, 1999a,
1999b), we described how our approach fits into the framework of tree-adjoining gram-
mar. This led to the initial version of a discourse parser (Forbes et al 2001) in which
the same parser that builds trees for individual clauses using clause-level LTAG trees
then combines them using discourse-level LTAG trees. Here we simply outline the
grammar, called DLTAG (section 5.1), then show how it supports the approach to
structural and anaphoric discourse connectives presented earlier (section 5.2).
(Of course, one still needs to account for how speakers realize their intentions
through text and how what is achieved through a single unit of text contributes to
what a speaker hopes to achieve through any larger unit in which it is embedded.
Preliminary accounts are given in Grosz and Sidner [1990] and Moser and Moore
[1996]. Given the complex relation between individual sentences and speaker inten-
tions, however, it is unlikely that the relation between multisentence discourse and
speaker intentions can be modeled in a straightforward way similar to the basically
monotonic compositional process that we have discussed in this article for discourse
semantics.)
574
Computational Linguistics Volume 29, Number 4
Dc
DcDc
subconj
(a)
Dc
Dc Dc
subconj
(b)
?:subconj_mid ?: subconj_pre
Figure 7
Initial trees for a subordinate conjunction: (a) postposed; (b) preposed. Dc stands for discourse
clause, ? indicates a substitution site, and subconj stands for the particular subordinate
conjunction that anchors the tree.
5.1 DLTAG and Discourse Syntax
A lexicalized TAG begins with the notion of a lexical anchor, which can have one
or more associated tree structures. For example, the verb likes anchors one tree corre-
sponding to John likes apples, another corresponding to the topicalized Apples John likes,
a third corresponding to the passive Apples are liked by John, and others as well. That
is, there is a tree for each minimal syntactic construction in which likes can appear, all
sharing the same predicate-argument structure. This syntactic/semantic encapsulation
is possible because of the extended domain of locality of LTAG.
A lexicalized TAG contains two kinds of elementary trees: initial trees that reflect
basic functor-argument dependencies and auxiliary trees that introduce recursion and
allow elementary trees to be modified and/or elaborated. Unlike the wide variety of
trees needed at the clause level, we have found that extending a lexicalized TAG to
discourse requires only a few elementary tree structures, possibly because clause-level
syntax exploits structural variation in ways that discourse doesn?t.
5.1.1 Initial Trees. DLTAG has initial trees associated with subordinate conjunctions,
with parallel constructions, and with some coordinate conjuctions. We describe each
in turn.
In the large LTAG developed by the XTAG project (XTAG-Group 2001) subordi-
nate clauses are seen as adjuncts to sentences or verb phrases (i.e., as auxiliary trees)
because they are outside the domain of locality of the verb. In DLTAG, however, it
is predicates on clausal arguments (such as coordinate and subordinate conjunctions)
that define the domain of locality. Thus, at this level, these predicates anchor initial
trees into which clauses substitute as arguments. Figure 7 shows the initial trees for (a)
postposed subordinate clauses and (b) preposed subordinate clauses.16 At both leaves
and root is a discourse clause (Dc): a clause or a structure composed of discourse
clauses.
One reason for taking something to be an initial tree is that its local dependencies
can be stretched long distance. At the sentence level, the dependency between apples
and likes in Apples John likes is localized in all the trees for likes. This dependency can
be stretched long distance, as in Apples, Bill thinks John may like. In discourse, as we
noted in section 2, local dependencies can be stretched long distance as well, as in
(59) a. Although John is generous, he?s hard to find.
16 Although in an earlier paper (Webber and Joshi 1998), we discuss reasons for taking the lexical anchors
of the initial trees in Figures 7 and 8 to be feature structures, following the analysis in Knott (1996) and
Knott and Mellish (1996), here we just take them to be specific lexical items.
575
Webber et al Anaphora and Discourse Structure
Dc
On the
one hand
On the
other
Dc Dc
?:contrast
Figure 8
An initial tree for parallel constructions. This particular tree is for a contrastive construction
anchored by on the one hand and on the other hand.
b. Although John is generous?for example, he gives money to anyone
who asks him for it?he?s hard to find.
(60) a. On the one hand, John is generous. On the other hand, he?s hard to
find.
b. On the one hand, John is generous. For example, suppose you needed
some money: You?d only have to ask him for it. On the other hand,
he?s hard to find.
Thus DLTAG also contains initial trees for parallel constructions as in (60). Such an
initial tree is shown in Figure 8. Like some initial trees in XTAG (XTAG-Group 2001),
such trees can have a pair of anchors. Since there are different ways in which dis-
course units can be parallel, we assume a different initial tree for contrast (on the one
hand. . . on the other (hand). . . ), disjunction (either. . . or. . . ), addition (not only. . . but also. . . ),
and concession (admittedly. . . but. . . ).
Finally, there are initial trees for structural connectives between adjacent sentences
or clauses that convey a particular relation between the connected units. One clear
example is so, conveying result. Its initial tree is shown in Figure 9. We will have a
better sense of what other connectives to treat as structural as a result of annotation
efforts of the sort described in Creswell et al (2002).17
5.1.2 Auxiliary Trees. DLTAG uses auxiliary trees in two ways: (1) for discourse units
that continue a description in some way, and (2) for discourse adverbials. Again we
describe each in turn.
17 For example, one might also have initial trees for marked uses of and and or that have a specific
meaning beyond simple conjunction or disjunction, as in
(61) a. Throw another spit ball and you?ll regret it.
b. Eat your spinach or you won?t get dessert.
These differ from the more frequent, simple coordinate uses of and and or in that the second conjunct
in these marked cases bears a discourse relation to the first conjunct (result in both (61a) and (61b)).
With simple coordinate uses of and and or, all conjuncts (disjuncts) bear the same relation to the same
immediately left-adjacent discourse unit. For example, in (62), each conjunct is a separate explanation
for not trusting John, wheras in (63), each disjunct conveys an alternative result of John?s good fortune:
(62) You shouldn?t trust John. He never returns what he borrows, and he bad-mouths his associates
behind their backs.
(63) John just won the lottery. So he will quit his job, or he will at least stop working overtime.
For simple coordinate uses of and and or, we have auxiliary trees (section 5.1.2).
576
Computational Linguistics Volume 29, Number 4
Dc
DcDc
?:so
so
Figure 9
Initial tree for coordinate conjunction so.
Dc
Dc Dc
?
.
Dc
Dc Dc
? and ?
S
S
then
(a) (b) (c)
?: punct1 ?: and ?: then
Figure 10
Auxiliary trees for basic elaboration. These particular trees are anchored by (a) the
punctuation mark ?period? and (b) and. The symbol ? indicates the foot node of the auxiliary
tree, which has the same label as its root. (c) Auxiliary tree for the discourse adverbial then.
?: punct1
?: punct1
3
?2
*
.
T1
T2
T1 T2
.
?1
0
Figure 11
TAG derivation of example (64).
First, auxiliary trees anchored by punctuation (e.g., period, comma, semicolon.) (Fig-
ure 10a) or by simple coordination (Figure 10b) are used to provide further description
of a situation or of one or more entities (objects, events, situations, states, etc.) within
the situation.18 The additional information is conveyed by the discourse clause that fills
its substitution site. Such auxiliary trees are used in the derivation of simple discourses
such as
(64) a. John went to the zoo.
b. He took his cell phone with him.
Figure 11 shows the DLTAG derivation of example (64), starting from LTAG deriva-
tions of the individual sentences.19 To the left of the horizontal arrow are the elemen-
tary trees to be combined: T1 stands for the LTAG tree for clause (64a), T2 for clause
18 The latter use of an auxiliary tree is related to dominant topic chaining in Scha and Polanyi (1988) and
entity chains in Knott et al (2001).
19 We comment on left-to-right incremental construction of DLTAG structures in parallel with
sentence-level LTAG structures at the end of Section 5.2.
577
Webber et al Anaphora and Discourse Structure
(64b), and ?:punct1 for the auxiliary tree assocated with the period after (64a). In the
derivation, the foot node of ?:punct1 is adjoined to the root of T1 and its substitution
site filled by T2, resulting in the tree to the right of the horizontal arrow. (A standard
way of indicating TAG derivations is shown under the horizontal arrow, where bro-
ken lines indicate adjunction and solid lines, substitution. Each line is labeled with the
address of the argument at which the operation occurs. ?1 is the derivation tree for
T1 and ?2, the derivation tree for T2.)
The other auxiliary trees used in the lexicalized discourse grammar are those for
discourse adverbials, which are simply auxiliary trees in a sentence-level LTAG (XTAG-
Group 2001), but with an interpretation that projects up to the discourse level. An
example is shown in Figure 10c. Adjoining such an adverbial to a clausal/sentential
structure contributes to how information conveyed by that structure relates to the
previous discourse.
There is some lexical ambiguity in this grammar, but no more than serious con-
sideration of adverbials and conjunctions demands. First, as already noted, discourse
adverbials have other uses that may not be anaphoric (65a?b) and may not be clausal
(65a?c):
(65) a. John ate an apple instead of a pear.
b. In contrast with Sue, Fred was tired.
c. Mary was otherwise occupied.
Second, many of the adverbials found in second position in parallel constructions
(e.g., on the other hand, at the same time, nevertheless) can also serve as simple adverbial
discourse connectives on their own. In the first case, they will be one of the two anchors
of an initial tree (Figure 8), and in the second, they will anchor a simple auxiliary tree
(Figure 10c). These lexical ambiguities correlate with structural ambiguity.
5.2 Example Derivations
It should be clear by now that our approach aims to explain discourse semantics in
terms of a product of the same three interpretive mechanisms that operate within
clause-level semantics:
? compositional rules on syntactic structure (here, discourse structure)
? anaphor resolution
? inference triggered by adjacency and structural connection
For the compositional part of semantics in DLTAG (in particular, computing interpre-
tations on derivation trees), we follow Joshi and Vijay-Shanker (2001). Roughly, they
compute interpretations on the derivation tree using a bottom-up procedure. At each
level, function application is used to assemble the interpretation of the tree from the
interpretation of its root node and its subtrees. Where multiple subtrees have function
types, the interpretation procedure is potentially nondeterministic: The resulting am-
biguities in interpretation may be admitted as genuine, or they may be eliminated by a
lexical specification. Multicomponent TAG tree sets are used to provide an appropriate
compositional treatment for quantifiers, which we borrow for interpreting for example
(examples (66c?d)).
In showing how DLTAG and an interpretative process on its derivations operate,
we must, of necessity, gloss over how inference triggered by adjacency or associated
with a structural connective provides the intended relation between adjacent discourse
578
Computational Linguistics Volume 29, Number 4
units: It may be a matter simply of statistical inference, as in Marcu and Echihabi
(2002), or of more complex inference, as in Hobbs et al (1993). As we noted, our view
is that there are three mechanisms at work in discourse semantics, just as there are in
clause-level semantics: Inference isn?t the only process involved. Thus the focus of our
presentation here is on how compositional rules and anaphor resolution (which itself
often appears to require inference) operate together with inference to yield discourse
semantics.
We start with previous examples (44) (here (66c)) and (47) (here (66d)) and two
somewhat simpler variants (66a?b):
(66) a. You shouldn?t trust John because he never returns what he borrows.
b. You shouldn?t trust John. He never returns what he borrows.
c. You shouldn?t trust John because, for example, he never returns what
he borrows.
d. You shouldn?t trust John. For example, he never returns what he bor-
rows.
This allows us to show how (66a?b) and (66c?d) receive similar interpretations, despite
having somewhat different derivations, and how the discourse adverbial for example
contributes both syntactically and semantically to those interpretations.
We let T1 stand for the LTAG parse tree for you shouldn?t trust John, ?1, for its
derivation tree, and interp(T1), for the eventuality associated with its interpretation.
Similarly, we let T2 stand for the LTAG parse tree for he never returns what he bor-
rows, ?2, for its derivation tree, and interp(T2), for the eventuality associated with its
interpretation.
Example (66a) involves an initial tree (?:because-mid) anchored by because (Fig-
ure 12). Its derived tree comes from T1 substituting at the left-hand substitution
site of ?:because-mid (index 1) and T2 at its right-hand substitution site (index 3).
Compositional interpretation of the resulting derivation tree yields explanation(interp
(T2),interp(T1)). (A more precise interpretation would distinguish between the direct
and epistemic causality senses of because, but the derivation would proceed in the
same way.)
In contrast with (66a), example (66b) employs an auxiliary tree (?:punct1) anchored
by a period (Figure 13). Its derived tree comes from T2 substituting at the right-hand
substitution site (index 3) of ?:punct1, and ?:punct1 adjoining at the root of T1 (index 0).
Compositional interpretation of the derivation tree yields merely that T2 continues the
description of the situation associated with T1, that is, elaboration(interp(T2),interp(T1)).
Further inference triggered by adjacency and structural connection leads to a con-
?:because_mid
?:because_mid
31
T2
T1
?1 ?2because
because
T1 T2
Figure 12
Derivation of example (66a). The derivation tree is shown below the arrow, and the derived
tree, to its right. (Node labels Dc have been omitted for simplicity.)
579
Webber et al Anaphora and Discourse Structure
?: punct1
?: punct1
*
T2 . .
T1
T1 T2
3
?2
?1
0
Figure 13
Derivation of example (66b).
D
D  *
cD{ c
c
}
for-ex2
?: for-ex1
0
3
?:?1
0
because_mid
?2
?:
1
T2
T1
?: for-ex1
because_mid?:T1
T2 because
?: for-ex2
because
for example
?
for example
Figure 14
Derivation of example (66c).
clusion of causality between them, that is, explanation(interp(T2),interp(T1)), but this
conclusion is defeasible because it can be denied without a contradiction: for example,
(67) You shouldn?t trust John. He never returns what he borrows. But that?s
not why you shouldn?t trust him.
Example (66c) differs from (66a) in containing for example in its second clause. As
noted earlier, for example resembles a quantifier with respect to its semantics, as its
interpretation takes wider scope than would be explained by its syntactic position. We
handle this in the same way that quantifiers are handled in Joshi and Vijay-Shanker
(2001) by associating with for example a two-element TAG tree set (Figure 14). Both
trees in the tree set participate in the derivation: The auxiliary tree ?:for ex1 adjoins
at the root of T2, whereas the auxiliary tree ?:for ex2 adjoins at the root of the higher
discourse unit. Since we saw from example (66a) that the interpretation of this higher
discourse unit is explanation(interp(T2),interp(T1)), the interpretation associated with
the adjoined ?:for ex2 node both embeds and abstracts this interpretation, yielding
exemplification(interp(T2), ?X . explanation(X,interp(T1))
That is, John?s never returning what he borrows is one instance of a set of explanations.
Similarly, example (66d) differs from (66b) in containing for example in its second
sentence (Figure 15). As in example (66b), an inferred relation is triggered between
the interpretations of T2 and T1, namely, explanation(interp(T2),interp(T1)). Then, as
a result of ?:for ex1 adjoining at T2 and ?:for ex2 adjoining at the root of the higher
580
Computational Linguistics Volume 29, Number 4
D
D
* .
.
D* }{ for-ex1?: for-ex2
0
punct1
?23
0
?:
?1
?:
0
T1
T2
?: for-ex2
?:
punct1
for example
T2
T1
for example ?
for-ex1?:
Figure 15
Derivation of example (66d).
discourse unit, for example again contributes the interpretation
exemplification(interp(T2), ?X . explanation(X,interp(T1))
Thus (66c) and (66d) differ only in the derivation of the interpretation that for example
then abstracts over.
The next example we will walk through is example (11) (repeated here as exam-
ple (68)):
(68) John loves Barolo. So he ordered three cases of the ?97. But he had to
cancel the order because then he discovered he was broke.
As shown in Figure 16, this example involves two initial trees (?:so, ?:because mid) for
the structural connectives so and because; an auxiliary tree for the structural connective
but (?:but), since but functions as a simple conjunction to continue the description of
the situation under discussion; an auxiliary tree (?:then) for the discourse adverbial
then; and initial trees for the four individual clauses T1?T4. As can be seen from the
derivation tree, T1 and T2 substitute into ?:so as its first and third arguments, and ?:but
root-adjoins to the result. The substitution argument of ?:but is filled by ?:because mid,
with T3 and T4 substituted in as its first and third arguments, and ?:then is root-
adjoined to T4. The interpretation contributed by then, after its anaphoric argument is
resolved to interp(T2), is
?4: after(interp(T4), interp(T2))
The interpretations derived compositionally from the structural connectives so, because,
and but are
?1: result(interp(T2), interp(T1))
?2: explanation(interp(T4), interp(S3))
?3: elaboration(?2,?1)
Further inference may then refine elaboration to contrast, based on how but is being
used.
Finally, we want to point out one more way in which texts that seem to be close
paraphrases get their interpretations in different ways. Consider the two texts in ex-
ample (69):
581
Webber et al Anaphora and Discourse Structure
*
so
?:then
*then
because
?:because_mid
?3
T1 ?1
?4T3
T2
T4
?: but
?: so
?: but
?:because_mid
?:
?2
3
1 3
0
?3 ?4
then
?1 ?2
31 0
because
thenT3
T4
T2
so
T1
but
?:
but
so
Figure 16
Derivation of example (68).
(69) a. You should eliminate part 2 before part 3 because part 2 is more sus-
ceptible to damage.
b. You should eliminate part 2 before part 3. This is because part 2 is
more susceptible to damage.
Example (69b) is a simpler version of an example in Moser and Moore (1995), in which
This is because is treated as an unanalyzed cue phrase, no different from because in (69a).
We show here that this isn?t necessary: One can analyze (69b) using compositional
semantics and anaphor resolution and achieve the same results.
First consider (69a). Given the interpretations of its two component clauses, its
overall interpretation follows in the same way as (66a), shown in Figure 12. Now
consider (69b) and the derivation shown in Figure 17. Here the initial tree ?:because mid
T1
T2
TB
?:because_mid
because
?: punct1
*
.
.
because
T2 TB
?: punct1
?:because_mid
31
?2 ??
?1
0
3
T1
Figure 17
Derivation of example (69b).
582
Computational Linguistics Volume 29, Number 4
has its two arguments filled by T2, the TAG analysis of this is and TB, the TAG analysis
of part 2 is more susceptible to damage. The overall derived tree for (69b) comes from
?:punct1 root-adjoining to T1 (the TAG analysis of You should eliminate part 2 before
part 3), with the subsitution site of ?:punct1 filled by the ?:because mid derivation.
The compositional interpretation of the derivation tree yields the interpretation of the
?:because mid tree (i1) as an elaboration of the interpretation of T1:
i1: explanation(interp(TB),interp(T2))
i2: elaboration(i1,interp(T1))
But this is not all. The pronoun this in T2 is resolved anaphorically to the nearest con-
sistent eventuality (Eckert and Strube 2000; Byron 2002) which in this case is interp(T1).
Taking this as the interpretation of T2 and substituting, we get
i1: explanation(interp(TB),interp(T1))
i2: elaboration(i1,interp(T1))
Notice that i1 is also the interpretation of (69a). To this, i2 adds the somewhat redun-
dant information that i1 serves to elaborate the advice in T1. Thus (69a) and (69b)
receive similar interpretations but by different means. This treatment has the added
advantage that one does not have to treat This is not because as a separate cue phrase.
Rather, negation simply produces
i1: ?explanation(interp(TB),interp(T1))
i2: elaboration(i1,interp(T1))
That is, T1 is elaborated by a denial of a (possible) explanation. Presumably, the text
would go on to provide the actual explanation.
Finally, we want to comment on the Holy Grail of discourse parsing: a realistic
model that is computed in parallel with incremental sentence-level parsing. Neither
the analyses given in this section nor the discourse parsing described in Forbes et
al. (2001) is done in a left-to-right incremental fashion, in parallel with incremental
left-to-right sentence-level parsing.
What would an integrated incremental method of sentence-discourse processing
require? At minimum, we believe it would involve:
? A left-to-right parser that would simultaneously compute increments to
sentence-level syntactic structure, sentence-level semantics,
discourse-level syntactic structure, and discourse-level semantics.
Increments to the latter two would occur only at clause boundaries and
with discourse adverbials and structural connectives.
? An incremental anaphor resolution mechanism, similar to that in Strube
(1998), but extended both to deictic pronouns, as in Eckert and Strube
(2000) and Byron (2002) and to the anaphoric argument of discourse
adverbials.
? Incremental computation of discourse structure in terms of elaboration
relations and further nondefeasible reasoning to more specific relations,
where possible.
A left-to-right parser that simultaneously produces sentence-level syntactic and
semantic analyses already exists for combinatory categorial grammar (Steedman 1996,
583
Webber et al Anaphora and Discourse Structure
2000b; Hockenmaier, Bierner, and Baldridge, forthcoming), and it would seem straight-
forward to extend such a parser to computing discourse-level syntax and semantics
as well. Similarly, it seems straightforward to produce an incremental version of any
of the current generation of anaphor resolution mechanisms, extended to deictic pro-
nouns, although current approaches attempt to resolve this and that only with the
interpretation of a single clause, not with that of any larger discourse unit. As these
approaches are also not very accurate as yet, incremental anaphor resolution awaits
improvements to anaphor resolution in general. Moreover, as we better understand
the specific anaphoric properties of discourse adverbials through empirical analysis
such as Creswell et al (2000), such anaphor resolution mechanisms can be extended
to include them as well.
As for building discourse structure incrementally in parallel with syntactic struc-
ture, there is no working prototype yet that will do what is needed. But we have no
doubt that as psycholinguistics and computation together develop a better understand-
ing of incremental semantic processing, researchers? desire for a working prototype
will eventually result in the development of one.
6. Conclusion
In this article, we have argued that discourse adverbials make an anaphoric, rather than
a structural, connection with the previous discourse (section 2), and we have provided
a general view of anaphora in which it makes sense to talk of discourse adverbials as
being anaphoric (section 3). We have then shown that this view of discourse adverbials
allows us to characterize a range of ways in which the relation contributed by a
discourse adverbial can interact with the relation conveyed by a structural connective
or inferred through adjacency (section 4), and then illustrated how discourse syntax
and semantics can be treated as an extension of sentence-level syntax and semantics,
using a lexicalized discourse grammar (section 5).
We are clearly not the first to have proposed a grammatical treatment of low-level
aspects of discourse semantics (Asher and Lascarides 1999; Gardent 1997; Polanyi and
van den Berg 1996; Scha and Polanyi 1988; Schilder 1997a, 1997b; van den Berg 1996),
but we are the first to have recognized that the key to avoiding problems of maintain-
ing a compositional semantics for discourse lies in recognizing discourse adverbials as
anaphors and not trying to shoehorn everything into a single class of discourse connec-
tives. Although we are not yet able to propose a solution to the problem of correctly
resolving discourse adverbials or a way of achieving the Holy Grail of computing
discourse syntax and semantics in parallel with incremental sentence processing, the
proposed approach does simplify issues of discourse structure and discourse semantics
in ways that have not before been possible.
Acknowledgments
The authors would like to thank Kate
Forbes, Katja Markert, Natalia Modjeska,
Rashmi Prasad, Eleni Miltsakaki, Cassandra
Creswell, Mark Steedman, members of the
University of Edinburgh Dialogue Systems
Group, and participants at ESSLLI?01 for
helpful criticism as the ideas in the article
were being developed. We would also like
to thank our three anonymous reviewers.
We believe that in addressing their
criticisms and suggestions, both the article?s
arguments and its presentation have
become clearer. This work has been funded
in part by EPSRC grant GR/M75129
(Webber), NSF grant CISE CDA 9818322
(Stone), and NSF grants NSF-STC SBR
8920230 and NSF-EIA02-24417 (Joshi).
References
Asher, Nicholas. 1993. Reference to Abstract
Objects in Discourse. Kluwer, Boston.
Asher, Nicholas and Alex Lascarides. 1999.
The semantics and pragmatics of
584
Computational Linguistics Volume 29, Number 4
presupposition. Journal of Semantics,
15(3):239?300.
Asher, Nicholas and Alex Lascarides. 2003.
Logics of Conversation. Cambridge
University Press, Cambridge, England.
Asher, Nicholas and Michael Morreau. 1991.
Commonsense entailment. In Proceedings
of the Ninth International Joint Conference on
Artificial Intelligence IJCAI?91, pages
387?392, Sydney, Australia.
Barwise, Jon and Robin Cooper. 1981.
Generalized quantifiers and natural
language. Linguistics and Philosophy,
4:159?219.
Bateman, John. 1999. The dynamics of
?surfacing?: An initial exploration. In
Proceedings of International Workshop on
Levels of Representation in Discourse
(LORID?99), pages 127?133, Edinburgh.
Bierner, Gann. 2001a. Alternative phrases
and natural language information
retrieval. In Proceedings of the 39th Annual
Conference of the Association for
Computational Linguistics, Toulouse,
France, July.
Bierner, Gann. 2001b. Alternative Phrases:
Theoretical Analysis and Practical Application.
Ph.D. thesis, University of Edinburgh.
Bierner, Gann and Bonnie Webber. 2000.
Inference through alternative set
semantics. Journal of Language and
Computation, 1(2):259?274.
Byron, Donna. 2002. Resolving pronominal
reference to abstract entities. In
Proceedings of the 40th Annual Meeting,
Association for Computational Linguistics,
pages 80?87, University of Pennsylvania.
Clark, Herbert. 1975. Bridging. In
Proceedings of Theoretical Issues in Natural
Language Processing (TINLAP-1), pages
169?174, Cambridge, MA.
Clark, Herbert and Catherine Marshall.
1981. Definite reference and mutual
knowledge. In Aravind Joshi, Bonnie
Webber, and Ivan Sag, editors, Elements of
Discourse Understanding. Cambridge
University Press, Cambridge, England,
pages 10?63.
Cosse, Michel. 1996. Indefinite associative
anaphora in French. In Proceedings of the
IndiAna Workshop on Indirect Anaphora,
University of Lancaster, Lancaster,
England.
Creswell, Cassandre, Kate Forbes, Eleni
Miltsakaki, Rashmi Prasad, Aravind Joshi,
and Bonnie Webber. 2002. The discourse
anaphoric properties of connectives. In
Proceedings of the Discourse Anaphora and
Anaphor Resolution Colloquium, Lisbon,
Portugal.
Dale, Robert. 1992. Generating Referring
Expressions. MIT Press, Cambridge, MA.
Dale, Robert and Ehud Reiter. 1995.
Computational interpretations of the
Gricean maxims in the generation of
referring expressions. Cognitive Science,
18:233?263.
Eckert, Miriam and Michael Strube. 2000.
Synchronising units and anaphora
resolution. Journal of Semantics, 17:51?89.
Forbes, Katherine, Eleni Miltsakaki, Rashmi
Prasad, Anoop Sarkar, Aravind Joshi, and
Bonnie Webber. 2001. D-LTAG system:
Discourse parsing with a lexicalized
tree-adjoining grammar. In ESSLLI?2001
Workshop on Information Structure, Discourse
Structure and Discourse Semantics, Helsinki,
Finland.
Forbes, Kate and Bonnie Webber. 2002. A
semantic account of adverbials as
discourse connectives. In Proceedings of
Third SIGDial Workshop, pages 27?36,
Philadelphia, PA.
Frank, Anette and Hans Kamp. 1997. On
context dependence in modal
constructions. In SALT-97, Stanford, CA.
Gardent, Claire. 1997. Discourse tree
adjoining grammars. Claus Report no. 89,
University of the Saarland, Saarbru?cken,
Germany.
Grosz, Barbara and Candace Sidner. 1990.
Plans for discourse. In Philip Cohen, Jerry
Morgan, and Martha Pollack, editors,
Intentions in Communication. MIT Press,
Cambridge, MA, pages 417?444.
Hahn, Udo, Katja Markert, and Michael
Strube. 1996. A conceptual reasoning
approach to textual ellipsis. In Proceedings
of the 12th European Conference on Artificial
Intelligence, pages 572?576, Budapest,
Hungary.
Hardt, Dan. 1992. VP ellipsis and contextual
interpretation. In Proceedings of
International Conference on Computational
Linguistics(COLING-92), pages 303?309,
Nantes.
Hardt, Dan. 1999. Dynamic interpretation of
verb phrase ellipsis. Linguistics and
Philosophy, 22:187?221.
Hellman, Christina and Kari Fraurud. 1996.
Proceedings of the IndiAna Workshop on
Indirect Anaphora. University of Lancaster,
Lancaster, England.
Hobbs, Jerry. 1985. Ontological promiscuity.
In Proceedings of the 23rd Annual Meeting of
the Association for Computational Linguistics,
pages 61?69, Palo Alto, CA. Morgan
Kaufmann.
Hobbs, Jerry. 1990. Literature and Cognition.
Volume 21 of CSLI Lecture Notes. Center
for the Study of Language and
Information, Stanford, CA.
585
Webber et al Anaphora and Discourse Structure
Hobbs, Jerry, Mark Stickel, Paul Martin, and
Douglas Edwards. 1993. Interpretation as
abduction. Artificial Intelligence,
63(1?2):69?142.
Hockenmaier, Julia, Gann Bierner, and Jason
Baldridge. Forthcoming. Providing
robustness for a CCG system. Journal of
Language and Computation.
Isard, Stephen. 1975. Changing the context.
In Edward Keenan, editor, Formal
Semantics of Natural Language. Cambridge
University Press, Cambridge, England,
pages 287?296.
Jayez, Jacques and Corinne Rossari. 1998a.
Pragmatic connectives as predicates. In
Patrick Saint-Dizier, editor, Predicative
Structures in Natural Language and Lexical
Knowledge Bases. Kluwer Academic,
Dordrecht, the Netherlands, pages
306?340.
Jayez, Jacques and Corinne Rossari. 1998b.
The semantics of pragmatic connectives
in TAG: The French donc example. In
Anne Abeille? and Owen Rambow, editors,
Proceedings of the TAG+4 Conference. CSLI
Publications, Stanford, CA.
Joshi, Aravind. 1987. An introduction to tree
adjoining grammar. In Alexis
Manaster-Ramer, editor, Mathematics of
Language. John Benjamins, Amsterdam,
pages 87?114.
Joshi, Aravind and K. Vijay-Shanker. 2001.
Compositional semantics with lexicalized
tree-adjoining grammar (LTAG): How
much underspecification is necessary? In
Harry Bunt, Reinhard Muskens, and Elias
Thijsse, editors, Computing Meaning,
Volume 2, Kluwer, Dordrecht, the
Netherlands, pages 147?163.
Jurafsky, Dan and James Martin. 2000.
Speech and Language Processing.
Prentice-Hall, Englewood Cliffs, NJ.
Kamp, Hans and Uwe Reyle. 1993. From
Discourse to Logic. Kluwer, Dordrecht, the
Netherlands.
Kehler, Andrew. 2002. Coherence, Reference
and the Theory of Grammar. CSLI
Publications, Stanford, CA.
Kibble, Rodger. 1995. Modal
insubordination. In Empirical Issues in
Formal Syntax and Semantics, Selected Papers
from the Colloque de Syntaxe et de Se?mantique
de Paris, pages 317?332.
Knott, Alistair. 1996. A Data-Driven
Methodology for Motivating a Set of Coherence
Relations. Ph.D. thesis, Department of
Artificial Intelligence, University of
Edinburgh.
Knott, Alistair and Chris Mellish. 1996. A
feature-based account of the relations
signalled by sentence and clause
connectives. Language and Speech,
39(2?3):143?183.
Knott, Alistair, Jon Oberlander, Mick
O?Donnell, and Chris Mellish. 2001.
Beyond elaboration: The interaction of
relations and focus in coherent text. In
T. Sanders, J. Schilperoord, and
W. Spooren, editors, Text Representation:
Linguistic and Psycholinguistic Aspects. John
Benjamins, Amsterdam, pages 181?196.
Koller, Alexander and Kristina Striegnitz.
2002. Generation as dependency parsing.
In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics,
pages 17?24, Philadelphia, PA.
Kruijff-Korbayova?, Ivana and Bonnie
Webber. 2001a. Concession, implicature
and alternative sets. In Fourth International
Workshop on Computational Semantics,
Tilburg, the Netherlands.
Kruijff-Korbayova?, Ivana and Bonnie
Webber. 2001b. Information structure and
the semantics of ?otherwise.? In
ESSLLI?2001 Workshop on Information
Structure, Discourse Structure and Discourse
Semantics, pages 61?78, Helsinki, Finland.
Lagerwerf, Luuk. 1998. Causal Connectives
Have Presuppositions. Holland Academic
Graphics, The Hague, the Netherlands.
Levinson, Stephen. 2000. Presumptive
Meanings: The Theory of Generalized
Conversational Implicature. MIT Press,
Cambridge, MA.
Luperfoy, Susann. 1992. The representation
of multimodal user interface dialogues
using discourse pegs. In Proceedings of the
30th Annual Meeting of the Association for
Computational Linguistics (ACL), pages
22?31, University of Delaware, Newark.
Mann, William and Sandra Thompson.
1988. Rhetorical structure theory: Toward
a functional theory of text organization.
Text, 8(3):243?281.
Marcu, Daniel. 1999. Instructions for
manually annotating the discourse
structure of texts. Available from
http://www.isi.edu/?marcu.
Marcu, Daniel and Abdessamad Echihabi.
2002. An unsupervised approach to
recognizing discourse relations. In
Proceedings of the 40th Annual Meeting,
Association for Computational Linguistics,
pages 368?375, University of
Pennsylvania, Philadelphia.
Modjeska, Natalia Nygren. 2001. Towards a
resolution of comparative anaphora: A
corpus study of ??other.? In PAPACOL,
Italy.
Modjeska, Natalia Nygren. 2002. Lexical
and grammatical role constraints in
resolving other-anaphora. In Proceedings of
586
Computational Linguistics Volume 29, Number 4
the Discourse Anaphora and Anaphor
Resolution Colloquium, Lisbon, Portugal.
Moens, Marc and Mark Steedman. 1988.
Temporal ontology and temporal
reference. Computational Linguistics,
14(1):15?28.
Moore, Johanna and Martha Pollack. 1992.
A problem for RST: The need for
multi-level discouse analysis.
Computational Linguistics, 18(4):537?544.
Moser, Megan and Johanna Moore. 1995.
Investigating cue selection and placement
in tutorial discourse. In Proceedings of the
33rd Annual Meeting, Association for
Computational Linguistics, pages 130?135,
MIT, Cambridge, MA.
Moser, Megan and Johanna Moore. 1996.
Toward a synthesis of two accounts of
discourse structure. Computational
Linguistics, 22(3):409?419.
Not, Elena, Lucia Tovena, and Massimo
Zancanaro. 1999. Positing and resolving
bridging anaphora in deverbal NPs. In
ACL?99 Workshop on the Relationship between
Discourse/Dialogue Structure and Reference,
College Park, MD.
Partee, Barbara. 1984. Nominal and
temporal anaphora. Linguistics and
Philosophy, 7(3):287?324.
Polanyi, Livia and Martin H. van den Berg.
1996. Discourse structure and discourse
interpretation. In P. Dekker and
M. Stokhof, editors, Proceedings of the Tenth
Amsterdam Colloquium, pages 113?131,
University of Amsterdam.
Prince, Ellen. 1992. The ZPG letter: Subjects,
definiteness and information-status. In
Susan Thompson and William Mann,
editors, Discourse Description: Diverse
Analyses of a Fundraising Text. John
Benjamins, Amsterdam, pages 295?325.
Quirk, Randolph, Sidney Greenbaum,
Geoffrey Leech, and Jan Svartik. 1972. A
Grammar of Contemporary English.
Longman, Harlow, England.
Scha, Remko and Livia Polanyi. 1988. An
augmented context free grammar for
discourse. In Proceedings of the 12th
International Conference on Computational
Linguistics (COLING?88), pages 573?577,
Budapest, Hungary, August.
Schilder, Frank. 1997a. Towards a theory of
discourse processing: Flashback sequences
described by D-trees. In Proceedings of the
Formal Grammar Conference (ESSLLI?97),
Aix-en-Provence, France, August.
Schilder, Frank. 1997b. Tree discourse
grammar, or how to get attached to a
discourse. In Proceedings of the Second
International Workshop on Computational
Semantics, Tilburg, the Netherlands,
January.
Steedman, Mark. 1996. Surface Structure and
Interpretation. Volume 30 of Linguistic
Inquiry Monograph, 5, MIT Press,
Cambridge, MA.
Steedman, Mark. 2000a. Information
structure and the syntax-phonology
interface. Linguistic Inquiry, 34:649?689.
Steedman, Mark. 2000b. The Syntactic
Process. MIT Press, Cambridge, MA.
Stokhof, Martin and Jeroen Groenendijk.
1999. Dynamic semantics. In Robert
Wilson and Frank Keil, editors, MIT
Encyclopedia of Cognitive Science. MIT Press.
Cambridge, MA, pages 247?249
Stone, Matthew, Christine Doran, Bonnie
Webber, Tonia Bleam, and Martha Palmer.
2001. Microplanning from communicative
intentions: Sentence planning using
descriptions (SPUD). Technical Report no.
RUCCS TR68, Department of Cognitive
Science, Rutgers University, New
Brunswick, NJ.
Stone, Matthew and Daniel Hardt. 1999.
Dynamic discourse referents for tense and
modals. In Harry Bunt, editor,
Computational Semantics. Kluwer,
Dordrecht, the Netherlands, pages
287?299.
Strube, Michael. 1998. Never look back: An
alternative to centering. In Proceedings,
COLING/ACL?98, pages 1251?1257,
Montreal, Quebec, Canada.
Traugott, Elizabeth. 1995. The role of the
development of discourse markers in a
theory of grammaticalization. Paper
presented at ICHL XII, Manchester,
England. Revised version of (1997)
available at http://www.stanford.
edu/traugott/ect-papersonline.html.
Traugott, Elizabeth. 1997. The discourse
connective after all: A historical
pragmatic account. Paper presented at
ICL, Paris. Available at http://www.
stanford.edu/traugott/ect-
papersonline.html.
van den Berg, Martin H. 1996. Discourse
grammar and dynamic logic. In P. Dekker
and M. Stokhof, editors, Proceedings of the
Tenth Amsterdam Colloquium, pages 93?111,
ILLC/Department of Philosophy,
University of Amsterdam.
van Eijck, Jan and Hans Kamp. 1997.
Representing discourse in context. In Jan
van Benthem and Alice ter Meulen,
editors, Handbook of Logic and Language.
Elsevier Science B.V., Amsterdam, pages
181?237.
Venditti, Jennifer J., Matthew Stone,
Preetham Nanda, and Paul Tepper. 2002.
Discourse constraints on the
587
Webber et al Anaphora and Discourse Structure
interpretation of nuclear-accented
pronouns. In Proceedings of Symposium on
Speech Prosody, Aix-en-Provence, France.
Available at http://www.lpl.
univ-aix.fr/sp2002/papers.htm.
Vendler, Zeno. 1967. Linguistics in Philosophy.
Cornell University Press, Ithaca, NY.
Webber, Bonnie. 1988. Tense as discourse
anaphor. Computational Linguistics,
14(2):61?73.
Webber, Bonnie. 1991. Structure and
ostension in the interpretation of
discourse deixis. Language and Cognitive
Processes, 6(2):107?135.
Webber, Bonnie and Breck Baldwin. 1992.
Accommodating context change. In
Proceedings of the 30th Annual Meeting of the
Association for Computational Linguistics
(ACL), pages 96?103, University of
Delaware, Newark.
Webber, Bonnie and Aravind Joshi. 1998.
Anchoring a lexicalized tree-adjoining
grammar for discourse. In COLING/ACL
Workshop on Discourse Relations and
Discourse Markers, pages 86?92, Montreal,
Quebec, Canada.
Webber, Bonnie, Alistair Knott, and Aravind
Joshi. 2001. Multiple discourse
connectives in a lexicalized grammar for
discourse. In Harry Bunt, Reinhard
Muskens, and Elias Thijsse, editors,
Computing Meaning, volume 2. Kluwer,
Dordrecht, the Netherlands, pages
229?249.
Webber, Bonnie, Alistair Knott, Matthew
Stone, and Aravind Joshi. 1999a.
Discourse relations: A structural and
presuppositional account using lexicalised
TAG. In Proceedings of the 36th Annual
Meeting of the Association for Computational
Linguistics, pages 41?48, College Park,
MD.
Webber, Bonnie, Alistair Knott, Matthew
Stone, and Aravind Joshi. 1999b. What
are little trees made of: A structural and
presuppositional account using lexicalised
TAG. In Proceedings of International
Workshop on Levels of Representation in
Discourse (LORID?99), pages 151?156,
Edinburgh.
Wiebe, Janyce. 1993. Issues in linguistic
segmentation. In Workshop on Intentionality
and Structure in Discourse Relations,
Association for Computational Linguistics,
pages 148?151, Ohio State University.
Woods, William. 1978. Semantics and
quantification in natural language
question answering. In Marshall C. Yovits,
editor, Advances in Computers, volume 17.
Academic Press, New York, pages 1?87.
XTAG-Group. 2001. A lexicalized tree
adjoining grammar for English. Technical
Report no. IRCS 01-03, University
of Pennsylvania, Philadelphia. Available at
ftp://ftp.cis.upenn.edu/pub/ircs/technical-
reports/01-03.
Event Detection and Summarization in Weblogs with Temporal Collocations 
Chun-Yuan Teng and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
{r93019, hhchen}@csie.ntu.edu.tw 
Abstract 
 
This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze 
the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is 
employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that 
may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. 
We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the 
temporal collocations capture the real world semantics and real world events over time. 
 
1. 
2. 
Introduction 
Compared with traditional media such as online news 
and enterprise websites, weblogs have several unique 
characteristics, e.g., containing abundant life experiences 
and public opinions toward different topics, highly 
sensitive to the events occurring in the real world, and 
associated with the personal information of bloggers. 
Some works have been proposed to leverage these 
characteristics, e.g., the study of the relationship between 
the content and bloggers? profiles (Adamic & Glance, 
2005; Burger & Henderson, 2006; Teng & Chen, 2006), 
and content and real events (Glance, Hurst & Tornkiyo, 
2004; Kim, 2005; Thelwall, 2006; Thompson, 2003). 
In this paper, we will use temporal collocation to 
model the term-to-term association over time.  In the past, 
some useful collocation models (Manning & Sch?tze, 
1999) have been proposed such as mean and variance, 
hypothesis test, mutual information, etc. Some works 
analyze the weblogs from the aspect of time like the 
dynamics of weblogs in time and location (Mei, et al, 
2006), the weblog posting behavior (Doran, Griffith & 
Henderson, 2006; Hurst, 2006), the topic extraction (Oka, 
Abe & Kato, 2006), etc. The impacts of events on social 
media are also discussed, e.g., the change of weblogs after 
London attack (Thelwall, 2006), the relationship between 
the warblog and weblogs (Kim, 2005; Thompson, 2003), 
etc. 
This paper is organized as follows. Section 2 defines 
temporal collocation to model the strength of term-to-term 
associations over time.  Section 3 introduces an event 
detection algorithm to detect the events in weblogs, and 
an event summarization algorithm to extract the 
description of an event in a specific time with temporal 
collocations. Section 4 shows and discusses the 
experimental results.  Section 5 concludes the remarks. 
Temporal Collocations 
We derive the temporal collocations from Shannon?s 
mutual information (Manning & Sch?tze, 1999) which is 
defined as follows (Definition 1). 
Definition 1 (Mutual Information) The mutual 
information of two terms x and y is defined as: 
)()(
),(log),(),(
yPxP
yxPyxPyxI =  
where P(x,y) is the co-occurrence probability of x and y, 
and P(x) and P(y) denote the occurrence probability of x 
and y, respectively. 
Following the definition of mutual information, we 
derive the temporal mutual information modeling the 
term-to-term association over time, and the definition is 
given as follows.  
 Definition 2 (Temporal Mutual Information) Given 
a timestamp t and a pair of terms x and y, the temporal 
mutual information of x and y in t is defined as: 
)|()|(
)|,(log)|,()|,(
tyPtxP
tyxPtyxPtyxI =
where P(x,y|t) is the probability of co-occurrence of terms 
x and y in timestamp t, P(x|t) and P(y|t) denote the 
probability of occurrences of x and y in timestamp t, 
respectively. 
To measure the change of mutual information in time 
dimension, we define the change of temporal mutual 
information as follows. 
Definition 3 (Change of Temporal Mutual 
Information) Given time interval [t1, t2], the change of 
temporal mutual information is defined as: 
12
12
21
)|,()|,(),,,(
tt
tyxItyxIttyxC ?
?=  
where C(x,y,t1,t2) is the change of temporal mutual 
information of terms x and y in time interval [t1, t2], I(x,y| 
t1) and I(x,y| t2) are the temporal mutual information in 
time t1 and t2, respectively. 
3. Event Detection 
Event detection aims to identify the collocations 
resulting in events and then retrieve the description of 
events. Figure 1 sketches an example of event detection. 
The weblog is parsed into a set of collocations. All 
collocations are processed and monitored to identify the 
plausible events.  Here, a regular event ?Mother?s day? 
and an irregular event ?Typhoon Chanchu? are detected.  
The event ?Typhoon Chanchu? is described by the words  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: An Example of Event Detection
?Typhoon?, ?Chanchu?, ?2k?, ?Eye?, ?Path? and 
?chinaphillippine?.  
The architecture of an event detection system includes 
a preprocessing phase for parsing the weblogs and 
retrieving the collocations; an event detection phase 
detecting the unusual peak of the change of temporal 
mutual information and identifying the set of collocations 
which may result in an event in a specific time duration; 
and an event summarization phase extracting the 
collocations related to the seed collocations found in a 
specific time duration. 
The most important part in the preprocessing phase is 
collocation extraction. We retrieve the collocations from 
the sentences in blog posts. The candidates are two terms 
within a window size. Due to the size of candidates, we 
have to identify the set of tracking terms for further 
analysis. In this paper, those candidates containing 
stopwords or with low change of temporal mutual 
information are removed. 
In the event detection phase, we detect events by 
using the peak of temporal mutual information in time 
dimension.  However, the regular pattern of temporal 
mutual information may cause problems to our detection. 
Therefore, we remove the regular pattern by seasonal 
index, and then detect the plausible events by measuring 
the unusual peak of temporal mutual information. 
If a topic is suddenly discussed, the relationship 
between the related terms will become higher. Two 
alternatives including change of temporal mutual 
information and relative change of temporal mutual 
information are employed to detect unusual events. Given 
timestamps t1 and t2 with temporal mutual information 
MI1 and MI2, the change of temporal mutual information 
is calculated by (MI2-MI1). The relative change of 
temporal mutual information is calculated by (MI2-
MI1)/MI1. 
For each plausible event, there is a seed collocation, 
e.g., ?Typhoon Chanchu?. In the event description 
retrieval phase, we try to select the collocations with the 
highest mutual information with the word w in a seed 
collocation. They will form a collocation network for the 
event.  Initially, the seed collocation is placed into the 
network.  When a new collocation is added, we compute 
the mutual information of the multiword collocations by 
the following formula, where n is the number of 
collocations in the network up to now. 
?= n iMInInformatioMutualMultiwo  
If the multiword mutual information is lower than a 
threshold, the algorithm stops and returns the words in the 
collocation network as a description of the event.  Figure 
2 sketches an example.  The collocations ?Chanchu?s 
path?, ?Typhoon eye?, and ?Chanchu affects? are added 
into the network in sequence based on their MI. 
We have two alternatives to add the collocations to 
the event description. The first method adds the 
collocations which have the highest mutual information 
as discussed above. In contrast, the second method adds 
the collocations which have the highest product of mutual 
information and change of temporal mutual information. 
 
 
 
 
 
 
Figure 2: An Example of Collocation network 
4. 
4.1. 
Experiments and Discussions 
Temporal Mutual Information versus 
Mutual Information 
In the experiments, we adopt the ICWSM weblog data 
set (Teng & Chen, 2007; ICWSM, 2007). This data set 
collected from May 1, 2006 through May 20, 2006 is 
about 20 GB. Without loss of generality, we use the 
English weblog of 2,734,518 articles for analysis. 
To evaluate the effectiveness of time information, we 
made the experiments based on mutual information 
(Definition 1) and temporal mutual information 
(Definition 2). The former called the incremental 
approach measures the mutual information at each time 
point based on all available temporal information at that 
time. The latter called the interval-based approach 
considers the temporal mutual information in different 
time stamps.  Figures 3 and 4 show the comparisons 
between interval-based approach and incremental 
approach, respectively, in the event of Da Vinci Code.   
We find that ?Tom Hanks? has higher change of 
temporal mutual information compared to ?Da Vinci 
Code?. Compared to the incremental approach in Figure 4, 
the interval-based approach can reflect the exact release 
date of ?Da Vinci Code.? 
 rd
=i 1 4.2. Evaluation of Event Detection 
We consider the events of May 2006 listed in 
wikipedia1 as gold standard. On the one hand, the events 
posted in wikipedia are not always complete, so that we 
adopt recall rate as our evaluation metric.  On the other 
hand, the events specified in wikipedia are not always 
discussed in weblogs.  Thus, we search the contents of 
blog post to verify if the events were touched on in our 
blog corpus. Before evaluation, we remove the events 
listed in wikipedia, but not referenced in the weblogs. 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Interval-based Approach in Da Vinci Code  
 
 
 
 
 
 
 
 
Figure 4: Incremental Approach in Da Vinci Code 
gure 5 sketches the idea of evaluation.  The left side 
of t s figure shows the collocations detected by our event 
dete tion system, and the right side shows the events 
liste  in wikipedia.  After matching these two lists, we 
can find that the first three listed events were correctly 
identified by our system.  Only the event ?Nepal Civil 
War? was listed, but not found. Thus, the recall rate is 
75% in this case. 
 
 
 
 
 
 
 
Figure 5: Evaluation of Event Detection Phase 
As discussed in Section 3, we adopt change of 
temporal mutual information, and relative change of 
temporal mutual information to detect the peak. In Figure 
6, we compare the two methods to detect the events in 
weblogs. The relative change of temporal mutual 
information achieves better performance than the change 
of temporal mutual information. 
                                                     
1 http://en.wikipedia.org/wiki/May_2006 
Table 1 and Table 2 list the top 20 collocations based 
on these two approaches, respectively. The results of the 
first approach show that some collocations are related to 
the feelings such as ?fell left? and time such as ?Saturday 
night?. In contrast, the results of the second approach 
show more interesting collocations related to the news 
events at that time, such as terrorists ?zacarias 
moussaoui? and ?paramod mahajan.? These two persons 
were killed in May 3. Besides, ?Geena Davis? got the 
golden award in May 3. That explains why the 
collocations detected by relative change of temporal 
mutual information are better than those detected by 
change of temporal mutual information. 
-20
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6: Performance of Event Detection Phase 
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
Collocations CMI Collocations CMI 
May 03 9276.08 Current music 1842.67
Illegal immigrants 5833.17 Hate studying 1722.32
Feel left 5411.57 Stephen Colbert 1709.59
Saturday night 4155.29 Thursday night 1678.78
Past weekend 2405.32 Can?t believe 1533.33
White house 2208.89 Feel asleep 1428.18
Red sox 2208.43 Ice cream 1373.23
Album tool 2120.30 Oh god 1369.52
Sunday morning 2006.78 Illegalimmigration 1368.12
16.56
f 
CMI
32.50
31.63
29.09
28.45
28.34
28.13Sunday night 1992.37 Pretty cool 13
Table 1: Top 20 collocations with highest change o
temporal mutual information 
Collocations CMI Collocations 
casinos online 618.36 Diet sodas 
zacarias moussaoui 154.68 Ving rhames 
Tsunami warning 107.93 Stock picks 
Conspirator zacarias 71.62 Happy hump 
Artist formerly 57.04 Wong kan 
Federal  
Jury 
41.78 Sixapartcom 
movabletype Wed 3 39.20 Aaron echolls 27.48
Pramod mahajan 35.41 Phnom penh 25.78
BBC  
Version 
35.21 Livejournal 
sixapartcom 
23.83  Fi
hi
c
dGeena davis 33.64 George yeo 20.34
Table 2: Top 20 collocations with highest relative change 
of mutual information 
4.3. Evaluation of Event Summarization 
As discussed in Section 3, we have two methods to 
include collocations to the event description. Method 1 
employs the highest mutual information, and Method 2 
utilizes the highest product of mutual information and 
change of temporal mutual information. Figure 7 shows 
the performance of Method 1 and Method 2. We can see 
that the performance of Method 2 is better than that of 
Method 1 in most cases. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7: Overall Performance of Event Summarization 
The results of event summarization by Method 2 are 
shown in Figure 8. Typhoon Chanchu appeared in the 
Pacific Ocean on May 10, 2006, passed through 
Philippine and China and resulted in disasters in these 
areas on May 13 and 18, 2006.  The appearance of the 
typhoon Chanchu cannot be found from the events listed 
in wikipedia on May 10.  However, we can identify the 
appearance of typhoon Chanchu from the description of 
the typhoon appearance such as ?typhoon named? and 
?Typhoon eye.  In addition, the typhoon Chanchu?s path 
can also be inferred from the retrieved collocations such 
as ?Philippine China? and ?near China?. The response of 
bloggers such as ?unexpected typhoon? and ?8 typhoons? 
is also extracted.   
 
 
 
 
 
 
 
 
 
 
Figure 8: Event Summarization for Typhoon Chanchu 
5. Concluding Remarks 
This paper introduces temporal mutual information to 
capture term-term association over time in weblogs. The 
extracted collocation with unusual peak which is in terms 
of relative change of temporal mutual information is 
selected to represent an event.  We collect those 
collocations with the highest product of mutual 
information and change of temporal mutual information 
to summarize the specific event.  The experiments on 
ICWSM weblog data set and evaluation with wikipedia 
event lists at the same period as weblogs demonstrate the 
feasibility of the proposed temporal collocation model 
and event detection algorithms. 
Currently, we do not consider user groups and 
locations. This methodology will be extended to model 
the collocations over time and location, and the 
relationship between the user-preferred usage of 
collocations and the profile of users. 
Acknowledgments 
Research of this paper was partially supported by 
National Science Council, Taiwan (NSC96-2628-E-002-
240-MY3) and Excellent Research Projects of National 
Taiwan University (96R0062-AE00-02). 
References 
Adamic, L.A., Glance, N. (2005). The Political 
Blogosphere and the 2004 U.S. Election: Divided 
They Blog. In: Proceedings of the 3rd International 
Workshop on Link Discovery, pp. 36--43. 
Burger, J.D., Henderson J.C. (2006). An Exploration of 
Observable Features Related to Blogger Age. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
15--20. 
Doran, C., Griffith, J., Henderson, J. (2006). Highlights 
from 12 Months of Blogs. In: Proceedings of AAAI 
2006 Spring Symposium on Computational 
Approaches to Analysing Weblogs, pp. 30--33. 
Glance, N., Hurst, M., Tornkiyo, T. (2004). Blogpulse: 
Automated Trend Discovery for Weblogs. In: 
Proceedings of WWW 2004 Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Hurst, M. (2006). 24 Hours in the Blogosphere. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
73--77. 
ICWSM (2007). http://www.icwsm.org/data.html 
Kim, J.H. (2005). Blog as an Oppositional Medium? A 
Semantic Network Analysis on the Iraq War Blogs. In: 
Internet Research 6.0: Internet Generations. 
 
Manning, C.D., Sch?tze, H. (1999). Foundations of 
Statistical Natural Language Processing, The MIT 
Press, London England. 
Mei, Q., Liu, C., Su, H., Zhai, C. (2006). A Probabilistic 
Approach to Spatiotemporal Theme Pattern Mining on 
Weblogs. In: Proceedings of the 15th International 
Conference on World Wide Web, Edinburgh, Scotland, 
pp. 533--542. 
Oka, M., Abe, H., Kato, K. (2006). Extracting Topics 
from Weblogs Through Frequency Segments. In: 
Proceedings of WWW 2006 Annual Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Teng, C.Y., Chen, H.H. (2006). Detection of Bloggers? 
Interest: Using Textual, Temporal, and Interactive 
Features. In: Proceeding of IEEE/WIC/ACM 
International Conference on Web Intelligence, pp. 
366--369. 
Teng, C.Y., Chen, H.H. (2007). Analyzing Temporal 
Collocations in Weblogs. In: Proceeding of 
International Conference on Weblogs and Social 
Media, 303--304. 
Thelwall, M. (2006). Blogs During the London Attacks: 
Top Information Sources and Topics. In: Proceedings 
of 3rd Annual Workshop on the Weblogging 
Ecosystem: Aggregation, Analysis and Dynamics. 
Thompson, G. (2003). Weblogs, Warblogs, the Public 
Sphere, and Bubbles. Transformations, 7(2). 
A SNoW based Supertagger with Application to NP Chunking
Libin Shen and Aravind K. Joshi
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104, USA
 
libin,joshi  @linc.cis.upenn.edu
Abstract
Supertagging is the tagging process of
assigning the correct elementary tree of
LTAG, or the correct supertag, to each
word of an input sentence1 . In this pa-
per we propose to use supertags to expose
syntactic dependencies which are unavail-
able with POS tags. We first propose a
novel method of applying Sparse Network
of Winnow (SNoW) to sequential models.
Then we use it to construct a supertagger
that uses long distance syntactical depen-
dencies, and the supertagger achieves an
accuracy of 
	 . We apply the su-
pertagger to NP chunking. The use of su-
pertags in NP chunking gives rise to al-
most 	 absolute increase (from 
to  ) in F-score under Transforma-
tion Based Learning(TBL) frame. The
surpertagger described here provides an
effective and efficient way to exploit syn-
tactic information.
1 Introduction
In Lexicalized Tree-Adjoining Grammar (LTAG)
(Joshi and Schabes, 1997; XTAG-Group, 2001),
each word in a sentence is associated with an el-
ementary tree, or a supertag (Joshi and Srinivas,
1994). Supertagging is the process of assigning the
correct supertag to each word of an input sentence.
The following two facts make supertagging attrac-
tive. Firstly supertags encode much more syntac-
tical information than POS tags, which makes su-
pertagging a useful pre-parsing tool, so-called, al-
most parsing (Srinivas and Joshi, 1999). On the
1By the correct supertag we mean the supertag that an LTAG
parser would assign to a word in a sentence.
other hand, as the term ?supertagging? suggests, the
time complexity of supertagging is similar to that of
POS tagging, which is linear in the length of the in-
put sentence.
In this paper, we will focus on the NP chunk-
ing task, and use it as an application of supertag-
ging. (Abney, 1991) proposed a two-phase pars-
ing model which includes chunking and attaching.
(Ramshaw and Marcus, 1995) approached chuck-
ing by using Transformation Based Learning(TBL).
Many machine learning techniques have been suc-
cessfully applied to chunking tasks, such as Regular-
ized Winnow (Zhang et al, 2001), SVMs (Kudo and
Matsumoto, 2001), CRFs (Sha and Pereira, 2003),
Maximum Entropy Model (Collins, 2002), Memory
Based Learning (Sang, 2002) and SNoW (Mun?oz et
al., 1999).
The previous best result on chunking in literature
was achieved by Regularized Winnow (Zhang et al,
2001), which took some of the parsing results given
by an English Slot Grammar-based parser as input to
the chunker. The use of parsing results contributed
 absolute increase in F-score. However, this
approach conflicts with the purpose of chunking.
Ideally, a chunker geneates n-best results, and an at-
tacher uses chunking results to construct a parse.
The dilemma is that syntactic constraints are use-
ful in the chunking phase, but they are unavail-
able until the attaching phase. The reason is that
POS tags are not a good labeling system to encode
enough linguistic knowledge for chunking. How-
ever another labeling system, supertagging, can pro-
vide a great deal of syntactic information.
In an LTAG, each word is associated with a set of
possible elementary trees. An LTAG parser assigns
the correct elementary tree to each word of a sen-
tence, and uses the elementary trees of all the words
to build a parse tree for the sentence. Elementary
trees, which we call supertags, contain more infor-
mation than POS tags, and they help to improve the
chunking accuracy.
Although supertags are able to encode long dis-
tance dependence, supertaggers trained with local
information in fact do not take full advantage of
complex information available in supertags.
In order to exploit syntactic dependencies in a
larger context, we propose a new model of supertag-
ging based on Sparse Network of Winnow (SNoW)
(Roth, 1998). We also propose a novel method of
applying SNoW to sequential models in a way anal-
ogous to the Projection-base Markov Model (PMM)
used in (Punyakanok and Roth, 2000). In contrast to
PMM, we construct a SNoW classifier for each POS
tag. For each word of an input sentence, its POS tag,
instead of the supertag of the previous word, is used
to select the corresponding SNoW classifier. This
method helps to avoid the sparse data problem and
forces SNoW to focus on difficult cases in the con-
text of supertagging task. Since PMM suffers from
the label bias problem (Lafferty et al, 2001), we
have used two methods to cope with this problem.
One method is to skip the local normalization step,
and the other is to combine the results of left-to-right
scan and right-to-left scan.
We test our supertagger on both the hand-coded
supertags used in (Chen et al, 1999) as well as
the supertags extracted from Penn Treebank(PTB)
(Marcus et al, 1994; Xia, 2001). On the dataset
used in (Chen et al, 1999), our supertagger achieves
an accuracy of 
	 .
We then apply our supertagger to NP chunking.
The purpose of this paper is to find a better way to
exploit syntactic information which is useful in NP
chunking, but not the machine learning part. So we
just use TBL, a well-known algorithm in the com-
munity of text chunking, as the machine learning
tool in our research. Using TBL also allows us to
easily evaluate the contribution of supertags with re-
spect to Ramshaw and Marcus?s original work, the
de facto baseline of NP chunking. The use of su-
pertags with TBL can be easily extended to other
machine learning algorithms.
We repeat Ramshaw and Marcus? Transformation
Based NP chunking (Ramshaw and Marcus, 1995)
algorithm by substituting supertags for POS tags in
the dataset. The use of supertags gives rise to almost
	 absolute increase (from  to  ) in F-
score under Transformation Based Learning(TBL)
frame. This confirms our claim that using supertag-
ging as a labeling system helps to increase the over-
all performance of NP Chunking. The supertag-
ger presented in this paper provides an opportunity
for advanced machine learning techniques to im-
prove their performance on chunking tasks by ex-
ploiting more syntactic information encoded in the
supertags.
2 Supertagging and NP Chunking
In (Srinivas, 1997) trigram models were used for su-
pertagging, in which Good-Turing discounting tech-
nique and Katz?s back-off model were employed.
The supertag for a word was determined by the lexi-
cal preference of the word, as well as by the contex-
tual preference of the previous two supertags. The
model was tested on WSJ section 20 of PTB, and
trained on section 0 through 24 except section 20.
The accuracy on the test data is 	 2.
In (Srinivas, 1997), supertagging was used for
NP chunking and it achieved an F-score of  .
(Chen, 2001) reported a similar result with a tri-
gram supertagger. In their approaches, they first su-
pertagged the test data and then uesd heuristic rules
to detect NP chunks. But it is hard to say whether
it is the use of supertags or the heuristic rules that
makes their system achieve the good results.
As a first attempt, we use fast TBL (Ngai and Flo-
rian, 2001), a TBL program, to repeat Ramshaw and
Marcus? experiment on the standard dataset. Then
we use Srinivas? supertagger (Srinivas, 1997) to su-
pertag both the training and test data. We run the
fast TBL for the second round by using supertags in-
stead of POS tags in the dataset. With POS tags we
achieve an F-score of 	 , but with supertags we
only achieve an F-score of 	 . This is not sur-
prising becuase Srinivas? supertag was only trained
with a trigram model. Although supertags are able
to encode long distance dependence, supertaggers
trained with local information in fact do not take full
advantage of their strong capability. So we must use
long distance dependencies to train supertaggers to
take full advantage of the information in supertags.
2This number is based on footnote 1 of (Chen et al, 1999).
A few supertags were grouped into equivalence classes for eval-
uation
The trigram model often fails in capturing the co-
occurrence dependence between a head word and
its dependents. Consider the phrase ?will join the
board as a nonexecutive director?. The occurrence
of join has influence on the lexical selection of as.
But join is outside the window of trigram. (Srini-
vas, 1997) proposed a head trigram model in which
the lexical selection of a word depended on the su-
pertags of the previous two head words , instead of
the supertags of the two words immediately leading
the word of interest. But the performance of this
model was worse than the traditional trigram model
because it discarded local information.
(Chen et al, 1999) combined the traditional tri-
gram model and head trigram model in their trigram
mixed model. In their model, context for the current
word was determined by the supertag of the previ-
ous word and context for the previous word accord-
ing to 6 manually defined rules. The mixed model
achieved an accuracy of 	 on the same dataset
as that of (Srinivas, 1997). In (Chen et al, 1999),
three other models were proposed, but the mixed
model achieved the highest accuracy. In addition,
they combined all their models with pairwise voting,
yielding an accuracy of Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 760?767,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Guided Learning for Bidirectional Sequence Classification
Libin Shen
BBN Technologies
Cambridge, MA 02138, USA
lshen@bbn.com
Giorgio Satta
Dept. of Inf. Eng?g.
University of Padua
I-35131 Padova, Italy
satta@dei.unipd.it
Aravind K. Joshi
Department of CIS
University of Pennsylvania
Philadelphia, PA 19104, USA
joshi@seas.upenn.edu
Abstract
In this paper, we propose guided learning,
a new learning framework for bidirectional
sequence classification. The tasks of learn-
ing the order of inference and training the
local classifier are dynamically incorporated
into a single Perceptron like learning algo-
rithm. We apply this novel learning algo-
rithm to POS tagging. It obtains an error rate
of 2.67% on the standard PTB test set, which
represents 3.3% relative error reduction over
the previous best result on the same data set,
while using fewer features.
1 Introduction
Many NLP tasks can be modeled as a sequence clas-
sification problem, such as POS tagging, chunking,
and incremental parsing. A traditional method to
solve this problem is to decompose the whole task
into a set of individual tasks for each token in the in-
put sequence, and solve these small tasks in a fixed
order, usually from left to right. In this way, the out-
put of the previous small tasks can be used as the
input of the later tasks. HMM and MaxEnt Markov
Model are examples of this method.
Lafferty et al (2001) showed that this approach
suffered from the so called label bias problem (Bot-
tou, 1991). They proposed Conditional Random
Fields (CRF) as a general solution for sequence clas-
sification. CRF models a sequence as an undirected
graph, which means that all the individual tasks are
solved simultaneously. Taskar et al (2003) improved
the CRF method by employing the large margin
method to separate the gold standard sequence la-
beling from incorrect labellings. However, the com-
plexity of quadratic programming for the large mar-
gin approach prevented it from being used in large
scale NLP tasks.
Collins (2002) proposed a Perceptron like learn-
ing algorithm to solve sequence classification in the
traditional left-to-right order. This solution does not
suffer from the label bias problem. Compared to the
undirected methods, the Perceptron like algorithm
is faster in training. In this paper, we will improve
upon Collins? algorithm by introducing a bidirec-
tional searching strategy, so as to effectively utilize
more context information at little extra cost.
When a bidirectional strategy is used, the main
problem is how to select the order of inference. Tsu-
ruoka and Tsujii (2005) proposed the easiest-first ap-
proach which greatly reduced the computation com-
plexity of inference while maintaining the accuracy
on labeling. However, the easiest-first approach only
serves as a heuristic rule. The order of inference is
not incorporated into the training of the MaxEnt clas-
sifier for individual labeling.
Here, we will propose a novel learning frame-
work, namely guided learning, to integrate classifi-
cation of individual tokens and inference order selec-
tion into a single learning task. We proposed a Per-
ceptron like learning algorithm (Collins and Roark,
2004; Daume? III and Marcu, 2005) for guided learn-
ing. We apply this algorithm to POS tagging, a clas-
sic sequence learning problem. Our system reports
an error rate of 2.67% on the standard PTB test set,
a relative 3.3% error reduction of the previous best
system (Toutanova et al, 2003) by using fewer fea-
tures. By using deterministic search, it obtains an
error rate of 2.73%, a 5.9% relative error reduction
760
over the previous best deterministic algorithm (Tsu-
ruoka and Tsujii, 2005).
The new POS tagger is similar to (Toutanova et
al., 2003; Tsuruoka and Tsujii, 2005) in the way
that we employ context features. We use a bidi-
rectional search strategy (Woods, 1976; Satta and
Stock, 1994), and our algorithm is based on Percep-
tron learning (Collins, 2002). A unique contribution
of our work is on the integration of individual clas-
sification and inference order selection, which are
learned simultaneously.
2 Guided Learning for Bidirectional
Labeling
We first present an example of POS tagging to show
the idea of bidirectional labeling. Then we present
the inference algorithm and the learning algorithm.
2.1 An Example of POS tagging
Suppose that we have an input sentence
Agatha found that book interesting
w1 w2 w3 w4 w5
(Step 0)
If we scan from left to right, we may find it
difficult to resolve the ambiguity of the label for
that, which could be either DT (determiner), or
IN (preposition or subordinating conjunction) in the
Penn Treebank. However, if we resolve the labels for
book and interesting, it would be relatively easy to
figure out the correct label for that.
Now, we show how bidirectional inference works
on this sample. Suppose we use beam search with
width of 2, and we use a window of (-2, 2) for con-
text features.
For the first step, we enumerate hypotheses for
each word. For example, found could have a label
VBN or VBD. Suppose that at this point the most
favorable action, out of the candidate hypotheses, is
the assignment of NN to book, according to the con-
text features defined on words. Then, we resolve the
label for book first. We maintain the top two hy-
potheses as shown below. Here, the second most fa-
vorable label for book is VB.
NN
VB
Agatha found that book interesting
w1 w2 w3 w4 w5
(Step 1)
At the second step, assume the most favorable ac-
tion is the assignment of label JJ to interesting in
the context of NN for book. Then we maintain the
top two hypotheses for span book interesting as
shown below. The second most favorable label for
interesting is still JJ, but in the context of VB for
book.
NN------JJ
VB------JJ
Agatha found that book interesting
w1 w2 w3 w4 w5
(Step 2)
Then, suppose we are most confident for assigning
labels VBD and VBN to found, in that order. We get
two separated tagged spans as shown below.
VBD NN------JJ
VBN VB------JJ
Agatha found that book interesting
w1 w2 w3 w4 w5
(Step 3)
In the next step, suppose we are most confident for
assigning label DT to that under the context of VBD
on the left and NN-JJ on the right side, as shown
below (second most favorable action, not discussed
here, is also displayed). After tagging w3, two sep-
arated spans merge into one, starting from found to
interesting.
VBD---DT---NN------JJ
VBD---IN---NN------JJ
Agatha found that book interesting
w1 w2 w3 w4 w5
(Step 4)
For the last step, we assign label NNP to Agatha,
which could be an out-of-vocabulary word, under the
context of VBD-DT on the right.
NNP---VBD---DT---NN------JJ
NNP---VBD---IN---NN------JJ
Agatha found that book interesting
w1 w2 w3 w4 w5
(Step 5)
This simple example has shown the advantage of
adopting a flexible search strategy. However, it is
still unclear how we maintain the hypotheses, how
we keep candidates and accepted labels and spans,
and how we employ dynamic programming. We will
answer these questions in the formal definition of the
inference algorithm in the next section.
761
2.2 Inference Algorithm
Terminology: Let the input sequence be
w1w2 ? ? ?wn. For each token wi, we are expected
to assign a label ti ? T, with T the label set.
A subsequence wi ? ? ?wj is called a span, and is
denoted [i, j]. Each span p considered by the al-
gorithm is associated with one or more hypotheses,
that is, sequences over T having the same length as
p. Part of the label sequence of each hypothesis is
used as a context for labeling tokens outside the span
p. For example, if a tri-gram model is adopted, we
use the two labels on the left boundary and the two
labels on the right boundary of the hypothesis for la-
beling outside tokens. The left two labels are called
the left interface, and the right two labels are called
the right interface. Left and right interfaces have
only one label in case of spans of length one.
A pair s = (Ileft , Iright) with a left and a right
interface is called a state. We partition the hypothe-
ses associated with span p into sets compatible with
the same state. In practice, for span p, we use a ma-
trix Mp indexed by states, so that Mp(s), s = (Ileft ,
Iright), is the set of all hypotheses associated with p
that are compatible with Ileft and Iright .
For a span p and a state s, we denote the associated
top hypothesis as
s.T = argmax
h?Mp(s)
V (h),
where V is the score of a hypothesis (defined in (1)
below). Similarly, we denote the top state for p as
p.S = argmax
s: Mp(s) 6=?
V (s.T ).
Therefore, for each span p, we have a top hypothe-
sis p.S.T , whose score is the highest among all the
hypotheses for span p.
Hypotheses are started and grown by means of
labeling actions. For each hypothesis h associated
with a span p we maintain its most recent labeling
action h.A, involving some token within p, as well
as the states h.SL and h.SR that have been used as
context by such an action, if any. Note that h.SL and
h.SR refer to spans that are subsequences of p. We
recursively compute the score of h as
V (h) = V (h.SL.T ) + V (h.SR.T ) + U(h.A), (1)
Algorithm 1 Inference Algorithm
Require: token sequence w1 ? ? ?wn;
Require: beam width B;
Require: weight vector w;
1: Initialize P , the set of accepted spans;
2: Initialize Q, the queue of candidate spans;
3: repeat
4: span p? ? argmaxp?Q U(p.S.T.A);
5: Update P with p?;
6: Update Q with p? and P ;
7: until (Q = ?)
where U is the score of an action. In other words,
the score of an hypothesis is the sum of the score
of the most recent action h.A and the scores of the
top hypotheses of the context states. The score of
an action h.A is computed through a linear function
whose weight vector is w, as
U(h.A) = w ? f(h.A), (2)
where f(h.A) is the feature vector of action h.A,
which depends on h.SL and h.SR.
Algorithm: Algorithm 1 is the inference algorithm.
We are given the input sequence and two parame-
ters, beam width B to determine the number of states
maintained for each span, and weight vector w used
to compute the score of an action.
We first initialize the set P of accepted spans with
the empty set. Then we initialize the queue Q of
candidate spans with span [i, i] for each token wi,
and for each t ? T assigned to wi we set
M[i,i]((t, t)) = {i? t},
where i ? t represents the hypothesis consisting of
a single action which assigns label t to wi. This pro-
vides the set of starting hypotheses.
As for the example Agatha found that book
interesting in the previous subsection, we have
? P = ?
? Q = {[1, 1], [2, 2], [3, 3], [4, 4], [5, 5]}
Suppose NN and VB are the two possible POS tags
for w4 book. We have
? M[4,4](NN, NN) = {h441 = 4? NN}
? M[4,4](VB, VB) = {h442 = 4? VB}
The most recent action of hypothesis h441 is to as-
sign NN to w4. According to Equation (2), the score
762
of this action U(h441.A) depends on the features de-
fined on the local context of action. For example,
f1001(h441.A) =
{
1 if t = NN ? w?1 = that
0 otherwise,
where w?1 represents the left word. It should be
noted that, for all the features depending on the
neighboring tags, the value is always 0, since those
tags are still unknown in the step of initialization.
Since this operation does not depend on solved tags,
we have V (h441) = U(h411.A), according to Equa-
tion (1).
The core of the algorithm repeatedly selects a can-
didate span from Q, and uses it to update P and Q,
until a span covering the whole sequence is added to
P and Q becomes empty. This is explained in detail
below.
At each step, we remove from Q the span p? such
that the action (not hypothesis) score of its top hy-
pothesis, p?.S.T , is the highest. This represents the
labeling action for the next move that we are most
confident about. Now we need to update P and Q
with the selected span p?. We add p? to P , and re-
move from P the spans included in p?, if any. Let
S be the set of removed spans. We remove from Q
each span which takes one of the spans in S as con-
text, and replace it with a new candidate span taking
p? (and another accepted span) as context. We always
maintain B different states for each span.
Back to the previous example, after Step 3 is com-
pleted, w2 found, w4 book and w5 interesting
have been tagged and we have
? P = {[2, 2], [4, 5]}
? Q = {[1, 2], [2, 5]}
There are two candidate spans in Q, each with its as-
sociated hypotheses and most recent actions. More
specifically, we can either solve w1 based on the con-
text hypotheses for [2, 2], resulting in span [1, 2], or
else solve w3 based on the context hypotheses in
[2, 2] and [4, 5], resulting in span [2, 5].
The top two states for span [2, 2] are
? M[2,2](VBD, VBD) = {h221 = 2? VBD}
? M[2,2](VBN, VBN) = {h222 = 2? VBN}
and the top two states for span [4, 5] are
? M[4,5](NN-JJ, NN-JJ)
= {h451 = (NN,NN)5? JJ}
? M[4,5](VB-JJ, VB-JJ)
= {h452 = (VB,VB)5? JJ}
Here (NN,NN)5 ? JJ represents the hypothesis
coming from the action of assigning JJ to w5 under
the left context state of (NN,NN). (VB,VB)5 ? JJ
has a similar meaning.1
We first compute the hypotheses resulting from all
possible POS tag assignments to w3, under all possi-
ble state combinations of the neighboring spans [2, 2]
and [4, 5]. Suppose the highest score action consists
in the assignment of DT under the left context state
(VBD, VBD) and the right context state (NN-JJ, NN-
JJ). We obtain hypothesis h251 = (VBD,VBD)3 ?
DT(NN-JJ, NN-JJ) with
V (h251) = V ((VBD,VBD).T ) +
V ((NN-JJ,NN-JJ).T ) + U(h251.A)
= V (h221) + V (h451) +w ? f(h251.A)
Here, features for action h251.A may depend on
the left tag VBD and right tags NN-JJ, which have
been solved before. More details of the feature func-
tions are given in Section 4.2. For example, we can
have features like
f2002(h251.A) =
{
1 if t = DT ? t+2 = JJ
0 otherwise,
We maintain the top two states with the highest
hypothesis scores, if the beam width is set to two.
We have
? M[2,5](VBD-DT, NN-JJ) = {h251 =
(VBD,VBD)3? DT(NN-JJ,NN-JJ)}
? M[2,5](VBD-IN, NN-JJ) = {h252 =
(VBD,VBD)3? IN(NN-JJ,NN-JJ)}
Similarly, we compute the top hypotheses and
states for span [1, 2]. Suppose now the hypothesis
with the highest action score is h251. Then we up-
date P by adding [2, 5] and removing [2, 2] and [4, 5],
which are covered by [2, 5]. We also update Q by re-
moving [2, 5] and [1, 2],2 and add new candidate span
[1, 5] resulting in
? P = {[2, 5]}
? Q = {[1, 5]}
1It should be noted that, in these cases, each state con-
tains only one hypothesis. However, if the span is longer than
4 words, there may exist multiple hypotheses for the same
state. For example, hypotheses DT-NN-VBD-DT-JJ and DT-
NN-VBN-DT-JJ have the same left interface DT-NN and right
interface DT-JJ.
2Span [1, 2] depends on [2, 2] and [2, 2] has been removed
from P . So it is no longer a valid candidate given the accepted
spans in P .
763
The algorithm is especially designed in such a way
that, at each step, some new span is added to P or
else some spans already present in P are extended
by some token(s). Furthermore, no pair of overlap-
ping spans is ever found in P , and the number of
pairs of overlapping spans that may be found in Q is
always bounded by a constant. This means that the
algorithm performs at most n iterations, and its run-
ning time is therefore O(B2n), that is, linear in the
length of the input sequence.
2.3 Learning Algorithm
In this section, we propose guided learning, a Per-
ceptron like algorithm, to learn the weight vector w,
as shown in Algorithm 2. We use p?.G to represent
the gold standard hypothesis on span p?.
For each input sequence Xr and the gold standard
sequence of labeling Yr, we first initialize P and Q
as in the inference algorithm. Then we select the
span for the next move as in Algorithm 1. If p?.S.T ,
the top hypothesis of the selected span p?, is com-
patible with the gold standard, we update P and Q
as in Algorithm 1. Otherwise, we update the weight
vector in the Perceptron style, by promoting the fea-
tures of the gold standard action, and demoting the
features of the action of the top hypothesis. Then
we re-generate the queue Q with P and the updated
weight vector w. Specifically, we first remove all the
elements in Q, and then generate hypotheses for all
the possible spans based on the context spans in P .
Hypothesis scores and action scores are calculated
with the updated weight vector w.
A special aspect of Algorithm 2 is that we main-
tain two scores: the score of the action represents the
confidence for the next move, and the score of the
hypothesis represents the overall quality of a partial
result. The selection for the next action directly de-
pends on the score of the action, but not on the score
of the hypothesis. On the other hand, the score of the
hypothesis is used to maintain top partial results for
each span.
We briefly describe the soundness of the Guided
Learning Algorithm in terms of two aspects. First,
in Algorithm 2 weight update is activated whenever
there exists an incorrect state s, the action score of
whose top hypothesis s.T is higher than that of any
state in each span. We demote this action and pro-
mote the gold standard action on the same span.
Algorithm 2 Guided Learning Algorithm
Require: training sequence pairs {(Xr, Yr)}1?r?R;
Require: beam width B and iterations I;
1: w? 0;
2: for (i? 1; i ? I; i++) do
3: for (r ? 1; r ? R; r++) do
4: Load sequence Xr and gold labeling Yr.
5: Initialize P , the set of accepted spans
6: Initialize Q, the queue of candidate spans;
7: repeat
8: p? ? argmaxp?Q U(p.S.T.A);
9: if (p?.S.T = p?.G) then
10: Update P with p?;
11: Update Q with p? and P ;
12: else
13: promote(w, f(p?.G.A));
14: demote(w, f(p?.S.T.A));
15: Re-generate Q with w and P ;
16: end if
17: until (Q = ?)
18: end for
19: end for
However, we do not automatically adopt the gold
standard action on this span. Instead, in the next
step, the top hypothesis of another span might be se-
lected based on the score of action, which means that
it becomes the most favorable action according to the
updated weights.
As a second aspect, if the action score of a gold
standard hypothesis is higher than that of any oth-
ers, this hypothesis and the corresponding span are
guaranteed to be selected at line 8 of Algorithm 2.
The reason for this is that the scores of the context
hypotheses of a gold standard hypothesis must be
no less than those of other hypotheses of the same
span. This could be shown recursively with respect
to Equation 1, because the context hypotheses of a
gold standard hypothesis are also compatible with
the gold standard.
Furthermore, if we take
(xi = f(p
?.G.A)? f(p?.S.T.A), yi = +1)
as a positive sample, and
(xj = f(p
?.S.T.A)? f(p?.G.A), yj = ?1)
as a negative sample, the weight updates at lines 13
764
and 14 are a stochastic approximation of gradient de-
scent that minimizes the squared errors of the mis-
classified samples (Widrow and Hoff, 1960). What
is special with our learning algorithm is the strategy
used to select samples for training.
In general, this novel learning framework lies be-
tween supervised learning and reinforcement learn-
ing. Guided learning is more difficult than super-
vised learning, because we do not know the order of
inference. The order is learned automatically, and
partial output is in turn used to train the local clas-
sifier. Therefore, the order of inference and the lo-
cal classification are dynamically incorporated in the
learning phase.
Guided learning is not as hard as reinforcement
learning. At each local step in learning, we always
know the undesirable labeling actions according to
the gold standard, although we do not know which
is the most desirable. In this approach, we can eas-
ily collect the automatically generated negative sam-
ples, and use them in learning. These negative sam-
ples are exactly those we will face during inference
with the current weight vector.
In our experiments, we have used Averaged Per-
ceptron (Collins, 2002; Freund and Schapire, 1999)
and Perceptron with margin (Krauth and Me?zard,
1987) to improve performance.
3 Related Works
Tsuruoka and Tsujii (2005) proposed a bidirectional
POS tagger, in which the order of inference is han-
dled with the easiest-first heuristic. Gime?nez and
Ma`rquez (2004) combined the results of a left-to-
right scan and a right-to-left scan. In our model, the
order of inference is dynamically incorporated into
the training of the local classifier.
Toutanova et al (2003) reported a POS tagger
based on cyclic dependency network. In their work,
the order of inference is fixed as from left to right. In
this approach, large beam width is required to main-
tain the ambiguous hypotheses. In our approach, we
can handle tokens that we are most confident about
first, so that our system does not need a large beam.
As shown in Section 4.2, even deterministic infer-
ence shows rather good results.
Our guided learning can be modeled as a search
algorithm with Perceptron like learning (Daume? III
and Marcu, 2005). However, as far as we know,
Data Set Sections Sentences Tokens
Training 0-18 38,219 912,344
Develop 19-21 5,527 131,768
Test 22-24 5,462 129,654
Table 1: Data set splits
the mechanism of bidirectional search with an on-
line learning algorithm has not been investigated be-
fore. In (Daume? III and Marcu, 2005), as well
as other similar works (Collins, 2002; Collins and
Roark, 2004; Shen and Joshi, 2005), only left-to-
right search was employed. Our guided learning al-
gorithm provides more flexibility in search with an
automatically learned order. In addition, our treat-
ment of the score of action and the score of hypoth-
esis is unique (see discussion in Section 2.3).
Furthermore, compared to the above works, our
guided learning algorithm is more aggressive on
learning. In (Collins and Roark, 2004; Shen and
Joshi, 2005), a search stops if there is no hypothe-
sis compatible with the gold standard in the queue
of candidates. In (Daume? III and Marcu, 2005), the
search is resumed after some gold standard compat-
ible hypotheses are inserted into a queue for future
expansion, and the weights are updated correspond-
ingly. However, there is no guarantee that the up-
dated weights assign a higher score to those inserted
gold standard compatible hypotheses. In our algo-
rithm, the gold standard compatible hypotheses are
used for weight update only. As a result, after each
sentence is processed, the weight vector can usually
successfully predict the gold standard parse. There-
fore our learning algorithm is aggressive on weight
update.
As far as this aspect is concerned, our algorithm
is similar to the MIRA algorithm in (Crammer and
Singer, 2003). In MIRA, one always knows the cor-
rect hypothesis. In our case, we do not know the
correct order of operations. So we use our form of
weight update to implement aggressive learning.
4 Experiments on POS Tagging
4.1 Settings
We apply our guided learning algorithm to POS tag-
ging. We carry out experiments on the standard
data set of the Penn Treebank (PTB) (Marcus et al,
1994). Following (Ratnaparkhi, 1996; Collins, 2002;
Toutanova et al, 2003; Tsuruoka and Tsujii, 2005),
765
Feature Sets Templates Error%
A Ratnaparkhi?s 3.05
B A + [t0, t1], [t0, t?1, t1], [t0, t1, t2] 2.92
C B + [t0, t?2], [t0, t2], [t0, t?2, w0], [t0, t?1, w0], [t0, t1, w0],
[t0, t2, w0], [t0, t?2, t?1, w0], [t0, t?1, t1, w0], [t0, t1, t2, w0]
2.84
D C + [t0, w?1, w0], [t0, w1, w0] 2.78
E D + [t0, X = prefix or suffix of w0], 4 < |X| ? 9 2.72
Table 2: Experiments on the development data with beam width of 3
we cut the PTB into the training, development and
test sets as shown in Table 1. We use tools provided
by CoNLL-2005 3 to extract POS tags from the mrg
files of PTB. So the data set is the same as previous
work. We use the development set to select features
and estimate the number of iterations in training. In
our experiments, we enumerate all the POS tags for
each word instead of using a dictionary as in (Ratna-
parkhi, 1996), since the size of the tag set is tractable
and our learning algorithm is efficient enough.
4.2 Results
Effect of Features: We first run the experiments to
evaluate the effect of features. We use templates to
define features. For this set of experiments, we set
the beam width B = 3 as a balance between speed
and accuracy. The guided learning algorithm usually
converges on the development data set in 4-8 itera-
tions over the training data.
Table 2 shows the error rate on the development
set with different features. We first use the same fea-
ture set used in (Ratnaparkhi, 1996), which includes
a set of prefix, suffix and lexical features, as well
as some bi-gram and tri-gram context features. Fol-
lowing (Collins, 2002), we do not distinguish rare
words. On set A, Ratnaparkhi?s feature set, our sys-
tem reports an error rate of 3.05% on the develop-
ment data set.
With set B, we include a few feature templates
which are symmetric to those in Ratnaparkhi?s set,
but are only available with bidirectional search. With
set C, we add more bi-gram and tri-gram features.
With set D, we include bi-lexical features. With set
E, we use prefixes and suffixes of length up to 9, as in
(Toutanova et al, 2003; Tsuruoka and Tsujii, 2005).
We obtain 2.72% of error rate. We will use this fea-
ture set on our final experiments on the test data.
Effect of Search and Learning Strategies: For the
second set of experiments, we evaluate the effect of
3http://www.lsi.upc.es/?srlconll/soft.html, package srlconll-
1.1.tgz.
Search Aggressive? Beam=1 Beam=3
L-to-R Yes 2.94 2.82
L-to-R No 3.24 2.75
Bi-Dir Yes 2.84 2.72
Bi-Dir No does not converge
Table 3: Experiments on the development data
search methods, learning strategies, and beam width.
We use feature set E for this set of experiments. Ta-
ble 3 shows the error rates on the development data
set with both left-to-right (L-to-R) and bidirectional
(Bi-Dir) search methods. We also tested both aggres-
sive learning and non-aggressive learning strategies
with beam width of 1 and 3.
First, with non-aggressive learning on bidirec-
tional search, the error rate does not converge to a
comparable number. This is due to the fact that the
search space is too large in bidirectional search, if
we do not use aggressive learning to constrain the
samples for learning.
With aggressive learning, the bidirectional ap-
proach always shows advantages over left-to-right
search. However, the gap is not large. This is
due to the fact that the accuracy of POS tagging
is very high. As a result, we can always keep the
gold-standard tags in the beam even with left-to-right
search in training.
This can also explain why the performance of left-
to-right search with non-aggressive learning is close
to bidirectional search if the beam is large enough.
However, with beam width = 1, non-aggressive
learning over left-to-right search performs much
worse, because in this case it is more likely that the
gold-standard tag is not in the beam.
This set of experiments show that guided learn-
ing is more preferable for tasks with higher ambi-
guities. In our recent work (Shen and Joshi, 2007),
we have applied a variant of this algorithm to depen-
dency parsing, and showed significant improvement
over left-to-right non-aggressive learning strategy.
Comparison: Table 4 shows the comparison with
the previous works on the PTB test sections.
766
System Beam Error%
(Ratnaparkhi, 1996) 5 3.37
(Tsuruoka and Tsujii, 2005) 1 2.90
(Collins, 2002) - 2.89
Guided Learning, feature B 3 2.85
(Tsuruoka and Tsujii, 2005) all 2.85
(Gime?nez and Ma`rquez, 2004) - 2.84
(Toutanova et al, 2003) - 2.76
Guided Learning, feature E 1 2.73
Guided Learning, feature E 3 2.67
Table 4: Comparison with the previous works
According to the experiments shown above, we
build our best system by using feature set E with
beam width B = 3. The number of iterations on
the training data is estimated with respect to the de-
velopment data. We obtain an error rate of 2.67%
on the test data. With deterministic search, or beam
with B = 1, we obtain an error rate of 2.73%.
Compared to previous best result on the same data
set, 2.76% by (Toutanova et al, 2003), our best re-
sult shows a relative error reduction of 3.3%. This
result is very promising, since we have not used any
specially designed features in our experiments. It is
reported in (Toutanova et al, 2003) that a crude com-
pany name detector was used to generate features,
and it gave rise to significant improvement in per-
formance. However, it is difficult for us to duplicate
exactly the same feature for the purpose of compari-
son, although it is convenient to use features like that
in our framework.
5 Conclusions
In this paper, we propose guided learning, a new
learning framework for bidirectional sequence clas-
sification. The tasks of learning the order of infer-
ence and training the local classifier are dynamically
incorporated into a single Perceptron like algorithm.
We apply this novel algorithm to POS tagging. It
obtains an error rate of 2.67% on the standard PTB
test set, which represents 3.3% relative error reduc-
tion over the previous best result (Toutanova et al,
2003) on the same data set, while using fewer fea-
tures. By using deterministic search, it obtains an
error rate of 2.73%, a 5.9% relative error reduction
over the previous best deterministic algorithm (Tsu-
ruoka and Tsujii, 2005). It should be noted that the
error rate is close to the inter-annotator discrepancy
on PTB, the standard test set for POS tagging, there-
fore it is very difficult to achieve improvement.
References
L. Bottou. 1991. Une approche the?orique de l?apprentissage
connexionniste: Applications a` la reconnaissance de la pa-
role. Ph.D. thesis, Universite? de Paris XI.
M. Collins and B. Roark. 2004. Incremental parsing with the
perceptron algorithm. In ACL-2004.
M. Collins. 2002. Discriminative training methods for hidden
markov models: Theory and experiments with perceptron al-
gorithms. In EMNLP-2002.
K. Crammer and Y. Singer. 2003. Ultraconservative online
algorithms for multiclass problems. Journal of Machine
Learning Research, 3:951?991.
H. Daume? III and D. Marcu. 2005. Learning as search opti-
mization: Approximate large margin methods for structured
prediction. In ICML-2005.
Y. Freund and R. E. Schapire. 1999. Large margin classifi-
cation using the perceptron algorithm. Machine Learning,
37(3):277?296.
J. Gime?nez and L. Ma`rquez. 2004. Svmtool: A general pos tag-
ger generator based on support vector machines. In LREC-
2004.
W. Krauth and M. Me?zard. 1987. Learning algorithms with
optimal stability in neural networks. Journal of Physics A,
20:745?752.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmentation and la-
beling sequence data. In ICML-2001.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1994.
Building a large annotated corpus of English: The Penn Tree-
bank. Computational Linguistics, 19(2):313?330.
A. Ratnaparkhi. 1996. A maximum entropy part-of-speech tag-
ger. In EMNLP-1996.
G. Satta and O. Stock. 1994. Bi-Directional Context-Free
Grammar Parsing for Natural Language Processing. Artifi-
cial Intelligence, 69(1-2).
L. Shen and A. K. Joshi. 2005. Incremental LTAG Parsing. In
EMNLP-2005.
L. Shen and A. K. Joshi. 2007. Bidirectional LTAG Depen-
dency Parsing. Technical Report 07-02, IRCS, UPenn.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
markov networks. In NIPS-2003.
K. Toutanova, D. Klein, C. Manning, and Y. Singer. 2003.
Feature-rich part-of-speech tagging with a cyclic dependency
network. In NAACL-2003.
Y. Tsuruoka and J. Tsujii. 2005. Bidirectional inference
with the easiest-first strategy for tagging sequence data. In
EMNLP-2005.
B. Widrow and M. E. Hoff. 1960. Adaptive switching circuits.
IRE WESCON Convention Record, part 4.
W. Woods. 1976. Parsers in speech understanding systems.
Technical Report 3438, Vol. 4, 1?21, BBN Inc.
767
? 
Comparing Lexicalized Treebank Grammars Extracted from 
Chinese, Korean, and English Corpora 
Fei  X ia ,  Chung-hye  Han,  Mar tha  Pa lmer ,  and  Arav ind  Josh i  
University of Pennsylvania 
Phi ladelphia PA 19104, USA 
{fxia, chunghye, mpalmer, j oshi}@linc, cis. upenn, edu 
Abst rac t  
In this paper, we present a method 
for comparing Lexicalized Tree Ad- 
joining Grammars extracted from 
annotated corpora for three lan- 
guages: English, Chinese and Ko- 
rean. This method makes it possi- 
ble to do a quantitative comparison 
between the syntactic structures of 
each language, thereby providing a 
way of testing the Universal Gram- 
mar Hypothesis, the foundation of 
modern linguistic theories. 
1 In t roduct ion  
The comparison of the grammars extracted 
from annotated corpora (i.e., Treebanks) is 
important on both theoretical and engineer- 
ing grounds. Theoretically, it allows us to do 
a quantitative testing of the Universal Gram- 
mar Hypothesis. One of the major concerns 
in modern linguistics is to establish an ex- 
planatory basis for the similarities and varia- 
tions among languages. The working assump- 
tion is that languages of the world share a set 
of universal linguistic principles and the ap- 
parent structural differences attested among 
languages can be explained as variation in 
the way the universal principles are instan- 
tiated. Comparison of the extracted syntac- 
tic trees allows us to quantitatively evaluate 
how similar the syntactic structures of differ- 
ent languages are. From an engineering per- 
spective the extracted grammars and the links 
between the syntactic structures in the gram- 
mars are valuable resources for NLP applica- 
tions, such as parsing, computational lexicon 
development, and machine translation (MT), 
to name a few. 
In this paper we first briefly discuss some 
linguistic characteristics of English, Chinese, 
and Korean, and introduce the Treebanks for 
the three languages. We then describe a 
tool that extracts Lexicalized Tree Adjoin- 
ing Grammars (LTAGs) from Treebanks and 
the results of its application to these three 
Treebanks. Next, we describe our methodol- 
ogy for automatic comparison of the extracted 
Treebank grammars, This consists primar- 
ily of matching syntactic structures (namely, 
templates and sub-templates) in each pair 
of Treebank grammars. The ability to per- 
form this type of comparison for different lan- 
guages has a definite positive impact on the 
possibility of sorting out the universal ver- 
sus language-dependent features of languages. 
Therefore, our grammar extraction tool is not 
only an engineering tool of great value in im- 
proving the efficiency and accuracy of gram- 
mar development, but it is also very useful for 
investigating theoretical linguistics. 
2 Three  Languages  and  Three  
' r reebanks  
In this section, we briefly discuss some lin- 
guistic characteristics of English, Chinese, 
and Korean, and introduce the Treebanks for 
these languages. 
2.1 Three  Languages 
These three languages belong to different lan- 
guage families: English is Germanic, Chinese 
is Sino-Tibetan, and Korean is Altaic (Com- 
rie, 1987). There are several major differences 
between these languages. First, both English 
52 
and Chinese have predominantly subject- 
verb-object (SVO) word order, whereas Ko- 
rean has underlying SOV order. Second, the 
word order in Korean is freer than in English 
and Chinese in the sense that argument NPs 
are freely permutable (subject o certain dis- 
course constraints). Third, Korean and Chi- 
nese freely allow subject and object deletion, 
but English does not. Fourth, Korean has 
richer inflectional morphology than English, 
whereas Chinese has little, if any, inflectional 
morphology. 
2.2 Three Treebanks  
The Treebanks that we used in this paper are 
the English Penn Treebank II (Marcus et al, 
1993), the Chinese Penn Treebank (Xia et 
al., 2000b), and the Korean Penn Treebank 
(Chung-hye Han, 2000). The main param- 
eters of these Treebanks are summarized in 
Table 1.1 The tags in each tagset can be 
classified into one of four types: (1) syntac- 
tic tags for phrase-level annotation, (2) Part- 
Of-Speech (POS) tags for head-level annota- 
tion, (3) function tags for grammatical func- 
tion annotation, and (4) empty category tags 
for dropped arguments, traces, and so on. 
We chose these Treebanks because they all 
use phrase structure annotation and their an- 
notation schemata re similar, which facili- 
tates the comparison between the extracted 
Treebank grammars. Figure 1 shows an an- 
notated sentence from the Penn English Tree- 
bank. 
3 LTAGs  and  Ext rac t ion  
A lgor i thm 
In this section, we give a brief introduction to 
the LTAG formalism and to a system named 
LexTract, which we build to extract LTAGs 
from Treeb~.nks. 
1The reason why the average sentence length for 
Korean is much shorter than those for English and 
Chinese is that a big portion of the corpus for Ko- 
rean Treebank includes dialogues that contain many 
one-word replies, whereas English and Chinese cor- 
pora consist of newspaper articles. 
((S (ppoLOC (IN at) 
(NP (NNP FNX)) 
(NP-SBJ-1 (bINS underwriters)) 
(ADVP (RB stin)) 
(VP (VBP draft) 
(NP (bINS policies)) 
(S-MNR 
(NP-SBJ (-NONE- *-1 )) 
(VP (VBG using) 
(NP 
(NP (iNN fountain) (NNS pens)) 
(CO and) 
(NP (VBG blotting) (NN papers)))))))) 
Figure 1: An example from Penn English 
Treebank 
3.1 LTAG fo rmal i sm 
LTAGs are based on the Tree Adjoining 
Grammar formalism developed by Joshi, 
Levy, and Takahashi (Joshi et al, 1975; Joshi 
and Schabes, 1997). The primitive elements 
of an LTAG are elementary trees (etrees). 
Each etree is associated with a lexical item 
(called the anchor of the tree) on its fron- 
tier. LTAGs possess many desirable proper- 
ties, such as the Extended Domain of Local- 
ity, which allows the encapsulation f all argu- 
ments of the anchor associated with an etree. 
There are two types of etrees: initial trees and 
auxiliary trees. An auxiliary tree represents 
a recursive structure and has a unique leaf 
node, called the foot node, which has the same 
syntactic category as the root node. Leaf 
nodes other than anchor nodes and foot nodes 
are substitution odes. Etrees are combined 
by two operations: substitution and adjunc- 
tion. The resulting structure of the combined 
etrees is called a derived tree. The combina- 
tion process is expressed as a derivation tree. 
Figure 2 shows the etrees, the derived tree, 
and the derivation tree for the sentence un- 
derwriters still draft policies. Foot and sub- 
stitution nodes are marked by ,, and $, re- 
spectively. The dashed and solid lines in the 
derivation tree are for adjunction and substi- 
tution operations, respectively. 
3.2 The Form of  Target Grammars 
Without further constraints, the etrees in 
the target grammar (i.e., the grammar to be 
extracted by LexTract) could be of various 
shapes. LexTract recognizes three types of 
53 
# of POS # ofsyntac- 
tags tic tags 
Language corpus size 
(words) 
English 1,174K 
Chinese 100K 
Korean 30K 
average sen- 
tence length 
23.85 words 
| Ib"~ ' - l l~ , ) _o~ 
34 
17 
of ftmc- # of empty cat- 
tion tags egory tags 
20 12 
26 7 
17 4 
Table 1: Size of the Treebanks and the tagsets used in each Treebank 
#h ~ vP -~,~ #3: S #4: 
I ADVP VP" ", ~, vP / I 
~s I ', . . . . .  ~ /Nns  
vBP ~/  I I ? ' v" , , .  
m~. , r ia .~ ~,ill draR pc u.mm,..~ 
(a) et re~ 
NF VP 
Jail~ draft NN$ 
I 
ptfliei~ 
draf t (#3)  
- - " - - ' "T ' - - - - " - - - -  
undcxwrRcrs(# 1 ) \ policies(#4) 
still(#2) 
(h) dcrivcd trc? (c) dczivatitm trcc 
Figure 2: Etrees, derived tree, and derivation 
tree for underwriters till draft policies 
x TM 
x ~ wq 
y~ X~= w~ X m )?,. cc  I X ~ 
xo z,~ /~  x, 
{ xo z~ x/~'X.z ,~
knti.~l itgm I 
(a) spinc-ctr~ (b) rm~-ctrce (c) c,,nj-etree 
Figure 3: Three types of elementary trees in 
the target grammar 
relation (namely, predicate-argument, modi- 
fication, and coordination relations) between 
the anchor of an etree and other nodes in the 
etree, and imposes the constraint that all the 
etrees to be extracted should fall into exactly 
one of the three patterns in Figure 3. 
The spine-etrees for predicate-argument 
relations. X ? is the head of X m and the 
anchor of the etree. The etree is formed 
by a spine X m ~ X m-1 ~ .. ~ X ? and 
the arguments of X ?. 
The mod-etrees for modification rela- 
tions. The root of the etree has two chil- 
dren, one is a foot node with the label 
Wq,  and the other node X m is a modifier 
of the foot node. X m is further expanded 
into a spine-etree whose head X ? is the 
anchor of the whole mod-etree. 
The conj-etrees for coordination rela- 
tions. In a conj-etree, the children of the 
root are two conjoined constituents and 
a node for a coordination conjunction. 
One conjoined constituent is marked as 
the foot node, and the other is expanded 
into a spine-etree whose head is the an- 
chor of the whole tree. 
Spine-etrees are initial trees, whereas mod- 
etrees and conj-etrees are auxiliary trees. 
3.3  Ext rac t ion  a lgor i thm 
The core of LexTract is an extraction algo- 
rithm that takes a Treebank sentence such as 
the one in Figure 1 and Treebank-specific in-
formation provided by the user of LexTract, 
and produces a set of etrees as in Figure 4 
and a derivation tree. We have described 
LexTract's architecture, its extraction algo- 
rithm, and its applications in (Xia, 1999; Xia 
et al, 2000a). Therefore, we shall not re- 
peat them in this paper other than point- 
ing out that LexTract is completely anguage- 
independent. 
3.4  Exper iments  
The results of running LexTract on English, 
Chinese, and Korean Treebanks are shown in 
Table 2. Templates are etrees with the lexical 
items removed. For instance, #3, #6, and #9 
in Figure 4 are three distinct etrees but they 
share the same template. 
Figure 5 shows the log frequency of tem- 
plates in the English Treebank and percent- 
age of template tokens covered by template 
54 
m 
m ~i \ ] lml~ i J  
template etree word etree types 
types types types per word type 
6926 131,397 49,206 
1140 21,125 10,772 
etree types 
per word token 
CFG rules 
(unlexicalized) 
2.67 34.68 1524 
1.96 9.13 515 
1.45 2.76 : 177 
Table 2: Grammars extracted from three Treebanks 
#1: re2: #3: #4: #5: #6: 
S NP NP VP S NP 
PP S* NNS ADVP VP* NP| VP NNS / ~  NNP 
, . . P ,  I I R', vBf'~-.~ I 
l FNX enderwri,~,~ { { polid~ 
s0ll draft at 
#7: #8: #9: #I0: #l l :  #12: 
VP NP NP NP NP 
VP" S NN NP" N VBG Np~ NIP" CC| NP 
NP VP 
{ ~ finmtain bltXting 
e V~O NP; pen.~ I 
pap~ 
~ing 
Figure 4: The extracted etrees from the fully 
bracketed ttree 
types. 2 In both cases, template types are 
sorted according to their frequencies and plot- 
ted on the X-axis. The figure shows that 
a small subset of template types, which oc- 
curs very frequently in the Treebank and can 
be seen as the core of the Treebank gram- 
mar, covers the majority of template tokens 
in the Treebank. For instance, the most 
frequent template type covers 9.37% of the 
template tokens and the top 100 (500, 1000 
and 1500, respectively) template types cover 
87.1% (96.6%, 98.4% and 99.0%, respectively) 
of the tokens, whereas about half (3440) of 
the template types occur once, accounting for 
only 0.32% of template tokens in total. 
4 Compar ing  Three  Treebank  
Grammars  
In this section, we describe our methodology 
for comparing Treebank gr3.mmars and the 
experimental results. 
4.1 Methodo logy  
To compare Treeb~nb grammars, we need to 
ensure that the Treebank grammars are based 
on the same tagset. To achieve that, we first 
create a new tagset that includes all the tags 
2If a template occurs n times in the corpus, it is 
counted as one template type but n template tokens. 
(a) Frequency (b) Coverage 
Figure 5: Etree template types and template 
tokens in the Penn English Treebank 
(X-axes: (a) and (b) template types 
Y-axes: (a) log frequency of templates; (b) 
percentage of template token covered by tem- 
plate types) 
from the three Treebanks. Then we merge 
some tags in this new tagset into a single tag. 
This step is necessary because certain distinc- 
tions among some tags in one language do not 
exist in another language. For example, the 
English Treebank has distinct tags for verbs 
in past tense, past participals, gerunds, and 
so on; however, no such distinction is mor- 
phologically marked in Chinese and, there- 
fore, the Chinese Treebank uses the same tag 
for verbs regardless of the tense and aspect. 
To make the conversion straightforward for 
verbs, we use a single tag for verbs in the new 
tagset. Next, we replace the tags in the origi- 
nal Treebanks with the tags in the new tagset, 
and then re-run LexTract to build Treebank 
gr~mraars from those Treebanks. 
Now that the Treebank grammars are based 
on the same tagset, we can compare them ac- 
cording to the templates and sub-templates 
that appear in more than one 'rreebank m 
that is, given a pair of Treebank grammars, 
we first calculate how many templates oc- 
cur in both grammars; 3 Next, we decompose 
SIdeally, to get more accurate comparison results, 
we would like to compare trees, rather than templates 
(which are non-lexicalized); however, comparing etrees 
requires bilingual parallel corpora, which we are cur- 
55 
templates: sub-templates: 
~ spine: S -> VP -> V 
NP| ~ subca~ (:NP, V@. NP) 
V@ NP! with root S 
(a) spine-etree template 
VP spine: PP-> P 
VP'~ PP ~ subeat: (P@, NP) 
with root PP 
P@ NP~ rood-pair: (VP*, PP) 
(b) mod-etree t mplate 
I . . . . . . .~  spine: NP->N 
NP* cc~ r~P ~ subeat ~@) with root NP 
lq@ conj-tuple: (NP*, CC, NP) 
(c) conj-etree t mplate 
Figure 6: The decompositions of etree tem- 
plates (In sub-templates, @ marks the anchor 
in subcategorization frame, * marks the mod- 
ifiee in a modifier-modifiee pair.) 
each template into a list of sub-templates (e.g., 
spines and subcategorization frames) and cal- 
culate how many of those sub-templates occur 
in both grammars. A template is decomposed 
as follows: A spine-etree template is decom- 
posed into a spine and a subcategorization 
frame; a mod-etree template is decomposed 
into a spine, a subcategorization frame, and a 
modifier-modifiee pair; a conj-etree template 
is decomposed into a spine, a subcategoriza- 
tion frame, and a coordination tuple. Figure 
6 shows examples of this decomposition for 
each type of template. 
4.2 Exper iments  
After tags in original Treebn.nks being re- 
placed with the tags in the new tagset, the 
numbers of templates in the new Treebank 
gra.mmars decrease by about 50%, as shown 
in the second colnmn of Table 3 (cf. the sec- 
ond column in Table 2). Table 3 also lists the 
numbers of sub-templates, such as spines and 
subcategorization frames, for each grammar. 
Table 4 lists the numbers of template types 
shared by each pair of Treeba.nk gr3.mmars 
and the percentage of the template tokens 
rently building. 
in each Treebank which are covered by these 
common template types. For example, there 
are 237 template types that appear in both 
English and Chinese Treebank grammars. 
These 237 template types account for 80.1% 
of template tokens in the English Treebank, 
and 81.5% of template tokens in the Chi- 
nese Treebank. The table shows that, al- 
though the number of matched templates are 
not very high, they are among the most fre- 
quent emplates and they account for the ma- 
jority of template tokens in the Treebanks. 
For instance, in the (Eng, Ch) pair, the 237 
template types that appear in both gram- 
mars is only 77.5% of all the English template 
types, but they cover 80.1% of template to- 
kens in the English Treebank. If we define the 
core grammar of a language as the set of the 
templates that occur very often in the Tree- 
bnnk, the data suggest hat the majority of 
the core grammars are easily inter-mappable 
structures for these three languages. 
If we compare sub-templates, rather than 
templates, in the Treebank grammars, the 
percentages of matched sub-template tokens 
(as in Table 5) are higher than the percent- 
ages of matched template tokens. This is be- 
cause two distinct templates may share com- 
mon sub-templates. 
4.3 Unmatched templates  
Our previous experiments ( ee Table 4) show 
that the percentages of unmatched template 
tokens in three Treebanks range from 16.0% 
to 43.8%, depending on the language pairs. 
Given a language pair, there are many pos- 
sible reasons why a template appears in one 
Treebank grammar, but not in the other. We 
divide those unmatched templates into two 
categories: spuriously unmatched templates 
and truly unmatched templates. 
Spuriously unmatched templates Spu- 
riously unmatched templates are templates 
that either should have found a matched tem- 
plate in the other gra.mmar or should not have 
been created by LexTract in the first place 
if the Treebanks were complete, uniformly 
annotated, and error-free. A spuriously un- 
matched template xists because of one of the 
56 
templates subtemplates 
spines subcat~ames mod-pairs 
Eng 3139 500 541 332 53 
Ch 547 108 180 152 18 
Kor 271 55 58 53 6 
(Eng,Ch) 
(Eng, Kor) 
(Ch, Kor) 
conj-tuples total 
1426 
458 
172 
Table 3: Treebank grammars with the new tagset 
type (#) 
token (%) 
type (#) 
token (%) 
type (:~) 
token (%) 
matched templates 
(237, 237) 
(80.1, 81.5) 
(83, 83) 
(57.7, 82.8) 
(59,59) 
(57.2, 84.0) 
templates with 
unique tags 
(536, 99) 
(2.8, 12.3) 
(2075, 6) 
(28.1, 0.1) 
(324,6) 
(29.4, 0.1) 
other unmatched 
templates 
(2366, 211) 
(17.1, 6.2) 
(981, 182) 
(14.2, 17.1) 
(164, 206) 
(13.4, 16.0) 
Table 4: Comparisons of templatea in three Treebank grammars 
following reasons: 
(Sl)  T reebank  size: The template is lin- 
guistically sound in both languages, and, 
therefore, should belong to the grarnmars 
for these languages. However, the tem- 
plate appears in only one Treebank gram- 
mar because the other Treebank is too 
small to include such a template. Figure 
7(S1) shows a template that is valid for 
both English and Chinese, but it appears 
only in the English Treebank, not in the 
Chinese Treebank. 
($2) Annotat ion  difference: Treebanks 
may choose different annotations for 
the same constructions; consequentially, 
the templates for those constructions 
look different. Figure 7($2) shows the 
templates used in English and Chinese 
for a VP such as "surged 7 (dollars)". 
In the template for English, the QP 
projects to an NP, but in the template 
for Chinese, it does not. 
($3) Treeb~nk annotat ion  error:  A tem- 
plate in a Treebank may result from an- 
notation errors in that Treebank. If no 
corresponding mistakes are made in the 
other Treebank, the template in the first 
Treebank will not match any template in 
the second 'I~reebank. For instance, in the 
English Treebank the word about in the 
sentence About 5 people showed up is of- 
ten mis-tagged as a preposition, resulting 
in the template in Figure 7($3). Not sur- 
prisingly, that template does not match 
any template in the Chinese Treebank. 
Tru ly  unmatched templates  A truly un- 
matched template is a template that does not 
match any template in the other Treebank 
even if we assume both Treebanks are per- 
fectly annotated. Here, we list three reasons 
why a truly unmatched template xist. 
(T1) Word order:  The word order deter- 
mines the positions of arguments w.r.t. 
their heads, and the positions of modi- 
fiers w.r.t, their modifiees. If two lan- 
guages have different word orders, their 
templates which include arguments of a 
head or a modifier are likely to look dif- 
ferent. For example, Figure 8(T1) show 
the templates for transitive verbs in Chi- 
nese and Korean grammars. The tem- 
plates do not match because of the dif- 
ferent positions of the object of the verb. 
(T2) Unique tags: For each pair of lan- 
guages, some Part-of-speech tags and 
syntactic tags may appear in only one 
language. Therefore, the templates with 
those tags will not match any templates 
in the other language. For instance, in 
Korean the counterparts of preposition 
phrases in English and Chinese are noun 
phrases (with postpositions attaching to 
them, not preposition phrases); there- 
fore, the templates with PP in Chinese, 
57 
(Eng,Ch) 
(Eng, Kor) 
(Ch, Kor) 
Table 
spines subcat frames rood-pairs conj-tuples total 
type (60,60) (92, 92) (83,83) (II,II) (246,246) 
token (94.7,87.2) (94.0, 86.3) (82.6, 80.0) (84.2, 99.1) (91.4, 85.2) 
type (39, 39) (40, 40) (46, 46) (1, 1) (126,126) 
token (70.3, 96.9) (62.1, 96.6) (56.8, 99.5) (9.3, 52.3) (63.4,97.3) 
type (28, 28) (25,25) (29,29) (I, I) (83, 83) 
token I (74.2, 99.2) (63.1, 98.1) (60.2, 93.4) (0.i, 0.4) (66.1, 96.9) 
5: Comparisons of sub-templates in three Treebank grammars 
VP 
VP* CC! VP 
V @ NIL Nl-~ 
English 
vp yP 
A 
VP* NP " VP* 
I QP Qp I 
I cD~ 
CD~ 
English Chinese 
QP 
P@ QP* 
English 
(S 1) Treebank size ($2) annotation difference ($3) annotation crmr 
Figure 7: Examples of spuriously unmatched templates 
such as the left one in Figure 8(T2), do 
not match any template in Korean. 
(T3) Un ique  syntact ic  re lat ions:  Some 
syntactic relations may be present in 
only one of the pair of languages being 
compared. For instance, the template 
in Figure 8(T3) is used for the sentence 
such as "You should go," said John, 
where the subject of the verb said ap- 
pears after the verb. No such template 
exists in Chinese. 
So far, we have listed six possible reasons 
for unmatched templates. Without manually 
examining all the unmatched templates, it is 
difficult to tell how many unmatched tem- 
plates are caused by a particular reason. Nev- 
ertheless, these reasons help us to interpret 
the results in Table 4. For instance, the ta- 
ble shows that Korean grammars cover only 
57.7% of template tokens in the English Tree- 
bank, and 57.2% in the Chinese Treebank, 
whereas the coverages for other language pairs 
are all above 80%. We suspect that this 
difference of coverage is mainly caused by 
(S1), (T1), and (T2). That is, first, Ko- 
rean Treebank is much smaller than the En- 
glish and the Chinese Treebanks, English and 
Chinese Treebanks may have many tree tem- 
plates that simply was not found in the Ko- 
rean Treebank; Second, English and Chinese 
are predominantly head-initial, whereas Ko- 
rean is head-final, therefore, many templates 
in English and Chinese can not find matched 
templates in Korean because of the word or- 
der difference; Third, Korean does not have 
preposition phrases, causing all the templates 
in English and Chinese with PPs become un- 
matched. To measure the effect of the word 
order factor to the matching rate, we re-did 
the experiment in Section 4.2, but this time 
we ignored the word order - -  that is, we treat 
templates as unordered trees. The results are 
given in Table 6. Comparing this table with 
Table 4, we can clearly see that, the percent- 
ages of matched templates increase substan- 
tially for (Eng, Kor) and (Ch, Kor) when the 
word order is ignored. Notice that the match- 
ing percentage for (Eng, Ch) does not change 
as much because the word orders in English 
and Chinese are much similar than the orders 
in English and Korean. 
5 Conclusion 
We have presented a method of quantitatively 
comparing LTAGs extracted from Treebanks. 
Our experimental results show a high pro- 
portion of easily inter-mappable structures, 
giving a positive implications for Universal 
Grammar hypothesis, We have also described 
a number of reasons why a particular tern- 
58 
$ $ 
A 
V~ NPt NPt V~ 
Chinese Korean 
(TI) word order 
vP  
A 
VP* NP VP* 
I 
P@ NPt N@ 
Chinese Korean 
(T2) unique rags 
s 
s(" 's  
NPt 
V~ $ 
! 
? 
English 
(T3) unique relation 
Figure 8: Truly unmatched templates 
(Eng,Ch) 
(Eng, Kor) 
(Ch, Kor) 
matched templates 
type (334, 259) 
token (82.8, 82.2) 
type (222, 167) 
token (66.4, 92.4) 
type (126,125) 
token (68.3, 97.3) 
tag mismatches 
i (536, 99) 
! (2.8, 12.3) 
I (2075, 6) 
! (28.1, 0.1) 
(324,6) 
(29.4, 0.1) 
other mismatches 
(2269, 189) 
(14.4, 5.5) 
(842, 98) 
(5.5, 7.5) 
(97, 140) 
(2.3, 2.6) 
Table 6: Comparisons of templates w/o orders 
plate does not match any template in other 
languages and tested the effect of word order 
on matching percentages. 
There are two natural extensions of this 
work. First, running an alignment algorithm 
on parallel bracketed corpora to produce 
word-to, word mappings. Given such word-to- 
word mappings and our template matching 
algorithm, we can automatically create lexi- 
calized etree-to-etree mappings, which can be 
used for semi-automatic transfer lexicon con- 
struction. Second, LexTract can build deriva- 
tion trees for each sentence in the corpora. By 
comparing derivation trees for parallel sen- 
tences in two languages, instances of struc- 
tural divergences (Dorr, 1993; Dorr, 1994; 
Palmer et al, 1998) can be automatically de- 
tected. 
Re ferences  
Chung-hye Hart. 2000. Bracketing Guide- 
lines for the Penn Korean Treebank (draft). 
www.cis.upenn.edu/xtag/korean.tag. 
Bernard Comrie. 1987. The World's Major Lan- 
guages. Oxford University Press, New York. 
B. J. Dorr. 1993. Machine ~D'anslation: a View 
from the Lexicon. MIT Press, Boston, Mass. 
B. J. Dorr. 1994. Machine translation diver- 
gences: a formal description and proposed so- 
lution. Computational Linguistics, 20(4):597- 
635. 
Aravind Joshi and Yves Schabes. 1997. Tree 
Adjoining Grammars. In A. Salomma and 
G. Rosenberg, editors, Handbook off For- 
mal Languages and Automata. Springer-Verlag, 
Herdelberg. 
Aravind K. Joshi, L. Levy, and M. Takahashi. 
1975. Tree Adjunct Grammars. Journal off 
Computer and System Sciences. 
M. Marcus, B. Santorini, and M. A. 
Marcinkiewicz. 1993.  Building a Large 
Annotated Corpus of English: the Penn 
Treebank. Computational Lingustics. 
Martha Palmer, Owen Rainbow, and Alexis Nasr. 
1998. Rapid Prototyping of Domain-Specific 
Machine Translation System. In Proc. of 
AMTA-1998, Langhorne, PA. 
Fei Xia, Martha Palmer, and Aravind Joshi. 
2000a. A Uniform Method of Grammar Ex- 
traction and its Applications. In Proc. off Joint 
SIGDAT Conference on Empirical Methods in 
Natural Language Processing and Very Large 
Corpora (EMNLP/VLC). 
Fei Xia, Martha Palmer, Nianwen Xue, 
Mary Ellen Okurowski, John Kovarik, Shizhe 
Huang, Tony Kroch, and Mitch Marcus. 
2000b. Developing Guidelines and Ensuring 
Consistency for Chinese Text Annotation. 
In Proc. off the 2nd International Confer- 
ence on Language Resources and Evaluation 
(LREC-2000), Athens, Greece. 
Fei Xia. 1999. Extracting Tree Adjoining Gram- 
mars from Bracketed Corpora. In Proc. off 5th 
Natural Language Processing Pacific Rim Sym- 
posium (NLPRS-99), Beijing, China. 
59 
A Uni form Method of Grammar Extract ion 
and Its Appl icat ions 
Fei  X ia  and Mar tha  Pa lmer  and Arav ind  Josh i  
Department  of Computer  and Information Science 
University of Pennsylvania 
Phi ladelphia PA 19104, USA 
{fxia, mpalmer, j oshi)@linc, cis. upenn, edu 
Abst rac t  
Grammars are core elements of many NLP ap- 
plications. In this paper, we present a system 
that automatically extracts lexicalized gram- 
mars from annotated corpora. The data pro- 
duced by this system have been used in sev- 
eral tasks, such as training NLP tools (such 
as Supertaggers) and estimating the coverage 
of harid-crafted grammars. We report experi- 
mental results on two of those tasks and com- 
pare our approaches with related work. 
1 In t roduct ion  
There are various grammar frameworks pro- 
posed for natural languages. We take Lexi- 
calized Tree-adjoining Grammars (LTAGs) as 
representative of a class of lexicalized gram- 
mars. LTAGs (Joshi et al, 1975) are ap- 
pealing for representing various phenomena 
in natural anguages due to its linguistic and 
computational properties. In the last decade, 
LTAG has been used in several aspects of 
natural language understanding (e.g., pars- 
ing (Schabes, 1990; Srinivas, 1997), semantics 
(Joshi and Vijay-Shanker, 1999; Kallmeyer 
and Joshi, 1999), and discourse (Webber and 
Joshi, 1998)) and a number of NLP applica- 
tions (e.g., machine translation (Palmer et al, 
1998), information retrieval (Chandrasekar 
and Srinivas, 1997), and generation (Stone 
and Doran, 1997; McCoy et al, 1992). This 
paper describes a system that extracts LTAGs 
from annotated corpora (i.e., Treebanks). 
There has been much work done on extract- 
ing Context-Free grammars (CFGs) (Shirai 
et al, 1995; Charniak, 1996; Krotov et al, 
1998). However, extracting LTAGs is more 
complicated than extracting CFGs because 
of the differences between LTAGs and CFGs. 
First, the primitive elements of an LTAG are 
lexicalized tree structures (called elementary 
trees), not context-free rules (which can be 
seen. as trees with depth one). Therefore, an 
LTAG extraction algorithm needs to examine 
a larger portion of a phrase structure to build 
an elementary tree. Second, the composition 
operations in LTAG are substitution (same 
as the one in a CFG) and adjunction. It is 
the operation of adjunction that distinguishes 
LTAG from all other formalisms. Third, un- 
like in CFGs, the parse trees (also known as 
derived trees in the LTAG) and the derivation 
trees (which describe how elementary trees 
are combined to form parse trees) are differ- 
ent in the LTAG formalism in the sense that 
a parse tree can be produced by several dis- 
tinct derivation trees. Therefore, to provide 
training data for statistical LTAG parsers, an 
LTAG extraction algorithm should also build 
derivation trees. 
For each phrase structure in a Treebank, 
our system creates a fully bracketed phrase 
structure, a set of elementary trees and 
a derivation tree. The data produced by 
our system have been used in several NLP 
tasks. We report experimental results on two 
of those applications and compare our ap- 
proaches with related work. 
2 LTAG fo rmal i sm 
The primitive elements of an LTAG are ele- 
mentary trees (etrees). Each etree is associ- 
ated with a lexical item (called the anchor of 
the tree) on its frontier. We choose LTAGs as 
our target grammars (i.e., the grammars to be 
extracted) because LTAGs possess many de- 
sirable properties, such as the Extended Do- 
main of Locality, which allows the encapsula- 
tion of all arguments of \[he anchor associated 
with an etree. There are two types of etrees:. 
initial trees and auxiliary trees. An auxiliary 
tree represents recursive structure and has a 
unique leaf node, called the foot node, which 
has the same syntactic ategory as the root 
node. Leaf nodes other than anchor nodes 
and foot nodes are substitutionnodes. Etrees 
are combined by two operations: substitution 
and adjunction, as in Figure 1 and 2. The 
53 
resulting structure of the combined etrees is 
called a derived tree. The combination pro- 
cess is expressed as a derivation tree. 
Figure 1: The substitution operation 
=>/_ _~ 
Figure 2: The adjunction operation 
Figure 3 shows the etrees, the derived tree, 
and the derivation tree for the sentence un- 
derwriters stil l draft policies. Foot and sub- 
stitution nodes are marked by . ,  and $, re- 
spectively. The dashed and solid lines in the 
derivation tree are for adjunction and substi- 
tution operations, respectively. 
3 System Overv iew 
We have built a system, called LexTract, for 
grammar extraction. The architecture ofLex- 
Tract is shown in Figure 4 (the parts that will 
be discussed in this paper are in bold). The 
core of LexTract is an extraction algorithm 
that takes a Treebank sentence such as the 
one in Figure 5 and produces the trees (el- 
ementary trees, derived trees and derivation 
trees) such as the ones in Figure 3. 
3.1 The Form of Target Grammars  
Without further constraints, the etrees in the 
target grammar could be of various shapes. 
#, .4" - - .  ~ ,  #3. ~: 
l :  " VP -- ' S NP A "- ~I --  NP 
I ADVP VP* ". NP I VP ' 
s~s I " . . . .  "~:~'-. i Ntis 
xa v!31> NP t /  I 
I t i ~ Iicll~i undenviStel'x st i l l  dr'aft Ix) ' 
(a) err??a 
draft(#3) 
I ADVP VP underwd lers (#1 ) ", policies(#4) 
NNS l ~ s t i l l (#2)  
slill ~ NN$ 
I 
(b) derived tree (c) derivation tr~ 
Figure 3: Etrees, derived tree and derivation 
tree for underwriters till draft policies 
i l.~xTract Syslcm i matde:d 
i ! L'rAo~ 
Treebonk ~peclflc tlerlvatim 
t ~ i implaulible 
Figure 4: Architecture of LexTract 
((S (PP-LOC (IN at) 
(NP (NNP FNX))) 
(NP-SBJ-I (NNS underwriters)) 
(ADVP (RiB still)) 
(VP (VBP draft) 
(NP (NNS policies)) 
(S-MNR 
(NP-SBJ (-NONE- *- 1)) 
(VP (VBG using) 
(NP 
(NP (NN fountain) (NNS pens)) 
(CC and) 
(NP (VBG blotting) (NN papers)))))))) 
Figure 5: A Treebank example 
Our system recognizes three types of rela- 
tion (namely, predicate-argument, modifica- 
tion, and coordination relations) between the 
anchor of an etree and other nodes in the etree, 
and imposes the constraint that all the etrees 
to be extracted should fall into exactly one of 
the three patterns in Figure 6. 
? The spine-etrees for predicate-argument 
relations. X ? is the head of X m and the 
anchor of the etree. The etree is formed 
by a spine X m --+ X m-1 -~ .. --+ X ? and 
the arguments of X ?. 
? The mod-etrees for modification rela- 
tions. The root of the etree has two chil- 
dren, one is a foot node with the label 
Wq, and the other node X m is a modifier 
X m 
A 
x '  
X ? zP~, 
lexical item 
X m 
Wq 
A 
Wq" X m ~. .  cclt x ~ 
xO zPl x o z~ I 
lcxical ilerrl I 
Icxicali lem 
(a) spinc..etree (b) mod-etz.~ (c) conj-etree 
Figure 6: Three types of elementary trees in 
the target grammar 
54 
of the foot node. X TM is further expanded 
into a spine-etree whose head X ? is the 
anchor of the whole mod-etree. 
? The conj-etrees for coordination rela- 
tions. In a conj-etree, the children of the 
root are two conjoined constituents and a 
node for a coordination conjunction. One 
conjoined constituent is marked as the 
foot node, and the other is expanded into 
a spine-etree whose head is the anchor of 
the whole tree. 
Spine-etrees are initial trees, whereas mod- 
etrees and conj-etrees are auxiliary trees. 
3.2 Treebank-spec i f i c  n format ion  
The phrase structures in the Treebank (ttrees 
for short) are partially bracketed in the sense 
that arguments and modifiers are not struc- 
turally distinguished. In order to construct 
the etrees, which make such distinction, Lex- 
Tract requires its user to provide additional 
information in the form of three tables: a 
Head Percolation Table, an Argument Table, 
and a Tagset Table. 
A Head Percolation Table has previ- 
ously been used in several statistical parsers 
(Magerman, 1995; Collins, 1997) to find heads 
of phrases. Our strategy for choosing heads is 
similar to the one in (Collins, 1997). An Ar- 
gument Table informs LexTract what types of 
arguments a head can take. The Tagset Table 
specifies what function tags always mark ar- 
guments (adjuncts, heads, respectively). Lex- 
Tract marks each sibling of a head as an argu- 
ment if the sibling can be an argument of the 
head according to the Argument Table and 
none of the function tags of the sister indi- 
cates that it is an adjunct. For example, in 
Figure 5, the head of the root S is the verb 
draft, and the verb has two siblings: the noun 
phrase policies is marked as an argument of 
the verb because from the Argument Table we 
know that verbs in general can take an NP ob- 
ject; the clause is marked as a modifier of the 
verb because, although verbs in general can 
take a sentential argument, the Tagset Table 
informs LexTract that the function tag -MNR 
(manner) always marks a modifier. 
3.3 Overv iew of  the  Ext ract ion  
A lgor i thm 
The extraction process has three steps: First, 
LexTract fully brackets each ttree; Second, 
LexTract decomposes the fully bracketed ttree 
((S (PP-LOC (IN at) 
(NP (NNP FNX))) 
(S (NP-SBJ-I (NNS underwriters)) 
(VP (ADVP (RB still)) 
(VP (VP (VBP draft) 
(NP (NNS policies))) 
(S-MNR 
(NP-SBJ (-NONE- *-1)) 
(VP (VBG using) 
(NP (NP (NN fountain) 
(NP (NNS pens))) 
(CC and) 
(NP (VBG blotting) 
(NP (NN papers)))))))) 
Figure 7: The fully bracketed ttree 
into a set of etrees; Third, LexTract builds the 
derivation tree for the ttree. 
3.3.1 Ful ly  b racket ing  ttrees 
As just mentioned, the ttrees in the Tree- 
bank do not explicitly distinguish arguments 
and modifiers, whereas etrees do. To account 
for this difference, we first fully bracket the 
ttrees by adding intermediate nodes so that 
at each level, one of the following relations 
holds between the head and its siblings: (1) 
head-argument relation, (2) modification re- 
lation, and (3) coordination relation. Lex- 
Tract achieves this by first choosing the head- 
child at each level and distinguishing argu- 
ments from adjuncts with the help of the three 
tables mentioned in Section 3.2, then adding 
intermediate nodes so that the modifiers and 
arguments of a head attach to different levels. 
Figure 7 shows the fully bracketed ttree. The 
nodes inserted by LexTract are in bold face. 
3.3.2 Bu i ld ing  etrees 
In this step, LexTract removes recursive struc- 
tures - which will become mod-etrees or conj- 
etrees - from the fully bracketed ttrees and 
builds spine-etrees for the non-recursive struc- 
tures. Starting from the root of a fully brack- 
eted ttree, LexTract first finds a unique path 
from the root to its head. It then checks each 
node e on the path. If a sibling of e in the ttree 
is marked as a modifier, LexTract marks e and 
e's parent, and builds a mod-etree (or a conj- 
etree if e has another sibling which is a con- 
junction) with e's parent as the root node, e as 
the foot node, and e's siblings as the modifier. 
Next, LexTract creates a spine-etree with the 
remaining unmarked nodes on the path and 
their siblings. Finally, LexTract repeats this 
process for the nodes that are not on the path. 
In Figure 8, which is the same as the one in 
Figure 7 except hat some nodes are numbered 
and split into the top and bottom pairs, 1 the 
1When a pair of etrees are combined ur ing parsing, 
55 
#5 
s2.b -"----...L ?" 
2 
" d I . 
#6 
Figure 8: The extracted et:rees can be seen as 
a decomposition of the fully bracketed ttree 
#1: #2: #3: #4: #5: #6: 
S NP NP VP S NP 
PP S" NNS ADVP VP = NPI VP NNS 
,s NP ~ { I RB VBP Na 
\[ FNX undcrwriten; ~ I pollciex 
still draft a\[ 
#7: #8: #9: #10: #I \[: #12: 
Vp NP NP NP "~ I cc NP 
v .  s NN N.  N~S I ~ VBG NP" NP" CCI NP 
Ne VP \[ NN 
l~,t,lntaln bloitln8 
? 
~per 
uffm8 
Figure 9: The extracted etrees from the fully 
bracketed ttree 
path from the root $1 to the head VBP is 
$1 ~ $2 ~ VP1 ~ VP2 --+ VP3 ~ VBP.  
Along the path the PP  ~ at FNX-  is a 
modifier of $2; therefore, Sl.b, S2.t, and the 
spine-etree rooted at PP  form a mod-etree 
#1. Similarly, the ADVP still is a modifier 
of VP2 and $3 is a modifier of VP3, and the 
corresponding structures form mod-etrees #4 
and #7. On the path from the root to VBP,  
Sl .t  and S2.b are merged (and so are VPi . t  
and VP3.b) to from the spine-etree #5. Re- 
peating this process for other nodes will gen- 
erate other trees such as trees #2, #3 and #6. 
The whole ttree yields twelve etrees as shown 
in Figure 9. 
3.3.3 Bu i ld ing  der ivat ion  t rees  
The fully bracketed ttree is in fact a derived 
tree of the sentence if the sentence is parsed 
with the etrees extracted by LexTract. In ad- 
dition to these etrees and the derived tree, we 
the root of one etree is merged with a node in the other 
etree. Splitting nodes into top and bottom pairs during 
the decomposition f the derived tree is the reverse 
process of merging nodes during parsing. For the sake 
of simplicity, we show the top and the bottom parts of 
a node only when the two parts will end up in different 
etrees. 
also need derivation trees to train statistical 
LTAG parsers. Recall that, in general, given 
a derived tree, the derivation tree that can 
generate the derived tree may not be unique. 
Nevertheless, given the fully bracketed ttree, 
the etrees, and the positions of the etrees in 
the ttree (see Figure 8), the derivation tree 
becomes unique if we choose either one of the 
following: 
? We adopt the traditional definition of 
derivation trees (which allows at most one 
adjunction at any node) and add an ad- 
ditional constraint which says that no ad- 
junction operation is allowed at the foot 
node of any auxiliary tree. 2 
? We adopt the definition of derivation 
trees in (Schabes and Shieber, 1992) 
(which allows multiple adjunction at any 
node) and require all mod-etrees adjoin 
to the etree that they modify. 
The user of LexTract can choose either op- 
tion and inform LexTract about his choice by 
setting a parameter. 3 Figure 10 shows the 
derivation tree based on the second option. 
draft (#5) 
a\[ (#1) underwriters(#3) ~i11(#4) policies(#6) using(#7) 
I I 
FNX(#2) pen(#9) 
fountain(#8) paper(#12) 
and(#10) bloldng(#l I) 
Figure 10: The derivation tree for the sentence 
3.4 Un iqueness  of  decompos i t ion  
To summarize, LexTract is a language- 
independent grammar extraction system, 
which takes Treebank-specific information 
(see Section 3.2) and a ttree T, and creates 
2Without his additional constraint, the derivation 
tree sometimes i  not unique. For example, in Figure 
8, both #4 and #7 modify the etree #5. If adjunc- 
tion were allowed at foot nodes, ~4 could adjoin to 
~7 at VP2.b, and #7 would adjoin to #5 at VPs.b. 
An alternative isfor #4 to adjoin to #5 at VPs.b and 
for ~7 to adjoin to ~4 at VP2.t. The no-adjunction- 
at-foot-node constraint would rule out the latter al- 
ternative and make the derivation tree unique. Note 
that this constraint has been adopted by several hand- 
crafted grammars such as the XTAG grammar for En- 
glish (XTAG-Group, 1998), because it eliminates this 
source of spurious ambiguity. 
SThis decision may affect parsing accuracy of an 
LTAG parser which uses the derivation trees for train- 
ing, but it will not affect he results reported in this 
paper. 
56 
(1) a fully bracketed ttree T*, (2) a set Eset  
of etrees, and (3) a derivation tree D for T*. 
Furthermore, Eset  is the only tree set that 
satisfies all the following conditions: 
(C1)  Decompos i t ion :  The tree set is a de- 
composition of T*, that is, T* would be 
generated if the trees in the set were com- 
bined via the substitution and adjunction 
operations. 
(C2)  LTAG formal i sm:  Each tree in the 
set is a valid etree, according to the LTAG 
formalism. For instance, each tree should 
be lexicalized and the arguments of the 
anchor should be encapsulated in the 
same etree. 
(C3) Target  g rammar :  Each tree in the 
set falls into one of the three types as 
specified in Section 3.1. 
(C4)  T reebank-spec i f i c  in fo rmat ion :  
The head/argument/adjunct distinction 
in the trees is made according to the 
Treebank-specific information provided 
by the user as specified in Section 3.2. 
S 
NP VP 
<1 I 
N V 
I t 
John left 
(T*) 
Ja,. I I 
John left 
(E l) (E2) 
\[ &~m t lea lc~hn \[ Icft 
(E) (E,) (Es) (E6) 
Figure 11: Tree sets for a fully bracketed ttree 
This uniqueness of the tree set may be quite 
surprising at first sight, considering that the 
number of possible decompositions of T* is 
~(2n), where n is the number of nodes in T*. 4 
Instead of giving a proof of the uniqueness, 
4Recall that the process of building etrees has two 
steps. First, LexTract treats each node as a pair of 
the top and bottom parts. The ttree is cut into pieces 
along the boundaries of the top and bottom parts of 
some nodes. The top and the bottom parts of each 
node belong to either two distinct pieces or one piece, 
as a result, there are 2 ~ distinct partitions. Second, 
some non-adjacent pieces in a partition can be glued 
together to form a bigger piece. Therefore, each par- 
tition will result in one or more decompositions of the 
ttree. In total, there are at least 2 n decompositions of 
the ttree. 
we use an example to illustrate how the con- 
ditions (C1)--(C4) rule out all the decompo- 
sitions except the one produced by LexTract. 
In Figure 11, the ttree T* has 5 nodes (i.e., 
S, NP, N, VP, and V). There are 32 distinct 
decompositions for T*, 6 of which are shown 
in the same figure. Out of these 32 decom- 
positions, only five (i.e., E2 - -  E6) are fully 
lexicalized - -  that is, each tree in these tree 
sets is anchored by a lexical item. The rest, 
including El, are not fully lexicalized, and are 
therefore ruled out by the condition (C2). For 
the remaining five etree sets, E2 - -  E4 are 
ruled out by the condition (C3), because ach 
of these tree sets has one tree that violates one 
constraint which says that in a spine-etree an 
argument of the anchor should be a substitu- 
tion node, rather than an internal node. For 
the remaining two, E5 is ruled out by (C4) 
because according to the Head Table provided 
by the user, the head of the S node should be 
V, not N. Therefore, E6, the tree set that is 
produced by LexTract, is the only etree set for 
T* that satisfies (C1)--(C4). 
3.5 The  Exper iments  
We have ran LexTract on the one-million- 
word English Penn Treebank (Marcus et 
al., 1993) and got two Treebank grammars. 
The first one, G1, uses the Treebank's 
tagset. The second Treebank grammar, 
G2, uses a reduced tagset, where some tags 
in the Treebank tagset are merged into a 
single tag. For example, the tags for verbs, 
MD/VB/VBP/VBZ/VBN/VBD/VBG,  are 
merged into a single tag V. The reduced 
tagset is basically the same as the tagset 
used in the XTAG grammar (XTAG-Group, 
1998). G2 is built so that we can compare 
it with the XTAG grammar, as will be 
discussed in the next section. We also ran the 
system on the 100-thousand-word Chinese 
Penn Treebank (Xia et al, 2000b) and on a 
30-thousand-word Korean Penn Treebank. 
The sizes of extracted grammars are shown in 
Table 1. (For more discussion on the Chinese 
and the Korean Treebanks and the compar- 
ison between these Treebank grammars, see 
(Xia et al, 2000a)). The second column of 
the table lists the numbers of unique tem- 
plates in each grammar, where templates are 
etrees with the lexical items removed, s The 
third column shows the numbers of unique 
5For instance, #3, #6 and #9 in Figure 9 are three 
different etrees but they share the same template. An 
etree can be seen as a (word, template) pair. 
57 
etrees. The average numbers of etrees for each 
word type in G1 and G2 are 2.67 and-2.38 
respectively. Because frequent words often 
anchor many etrees, the numbers increase by 
more than 10 times when we consider word 
token, as shown in the fifth and sixth columns 
of the table. G3 and G4 are much smaller 
than G1 and G2 because the Chinese and the 
Korean Treebanks are much smaller than the 
English Treebank. 
In addition to LTAGs, by reading context- 
free rules off the etrees of a Treebank LTAG, 
LexTract also produces CFGs. The numbers 
of unlexicalized context-free rules from G1-- 
G4 are shown in the last column of Table 1. 
Comparing with other CFG extraction algo- 
rithms such as the one in (Krotov et al, 1998), 
the CFGs produced by LexTract have sev- 
eral good properties. For example, they allow 
unary rules and epsilon rules, they are more 
compact and the size of the grammar remains 
monotonic as the Treebank grows. 
Figure 12 shows the log frequency of tem- 
plates and the percentage of template tokens 
covered by template types in G1. 6 In both 
cases, template types are sorted according to 
their frequencies and plotted on the X-axes. 
The figure indicates that a small portion of 
template types, which can be seen as the core 
of the grammar, cover majority of template 
tokens in the Treebank. For example, the first 
100 (500, 1000 and 1500, resp.) templates 
cover 87.1% (96.6~o, 98.4% and 99.0% resp.) 
of the tokens, whereas about half (3411) of 
the templates each occur only once, account- 
ing for only 0.29% of template tokens in total. 
4 App l i ca t ions  o f  LexTract  
In addition to extract LTAGs and CFGs, Lex- 
Tract has been used to perform the following 
tasks: 
? We use the Treebank grammars produced 
by LexTract to evaluate the coverage of 
hand-crafted grammars. 
? We use the (word, template) sequence 
produced by LexTract to re-train Srini- 
vas' Supertaggers (Srinivas, 1997). 
? The derivation trees created by LexTract 
are used to train a statistical LTAG 
parser (Sarkar, 2000). LexTract output 
has also been used to train an LR LTAG 
parser (Prolo, 2000). 
6Similar results hold for G2, G3 and G4. 
? We have used LexTract to retrieve the 
data from Treebanks to test theoret- 
ical linguistic hypotheses uch as the 
Tree-locality Hypothesis (Xia and Bleam, 
20O0). 
? LexTract has a filter that checks the 
plausibility of extracted etrees by decom- 
posing each etree into substructures and 
checking them. Implausible trees are of- 
ten caused by Treebank annotation er- 
rors. Because LexTract maintains the 
mappings between etree nodes and ttree 
nodes, it can detect certain types of an- 
notation errors. We have used LexTract 
for the final cleanup of the Penn Chinese 
Treebank. 
Due to space limitation, in this paper we 
will only discuss the first two tasks. 
4.1 Eva luat ing  the  coverage of 
hand-c ra f ted  grammars  
The XTAG grammar (XTAG-Group, 1998) 
is a hand-crafted large-scale grammar for En- 
glish, which has been developed at University 
of Pennsylvania n the last decade. It has been 
used in many NLP applications uch as gen- 
eration (Stone and Doran, 1997). Evaluating 
the coverage of such a grammar is important 
for both its developers and its users. 
Previous evaluations (Doran et al, 1994; 
Srinivas et al, 1998) of the XTAG grammar 
use raw data (i.e., a set of sentences with- 
out syntactic bracketing). The data are first 
parsed by an LTAG parser and the coverage 
of the grammar is measured as the percent- 
age of sentences in the data that get at least 
one parse, which is not necessarily the correct 
parse. For more discussion on this approach, 
see (Prasad and Sarkar, 2000). 
We propose a new evaluation method that 
takes advantage of Treebanks and LexTract. 
The idea is as follows: given a Treebank T and 
a hand-crafted grammar Gh, the coverage of 
Gh on T can be measured by the overlap of Gh 
and a Treebank grammar Gt that is produced 
by LexTract from T. In this case, we will esti- 
mate the coverage of the XTAG grammar on 
the English Penn Treebank (PTB) using the 
Treebank grammar G2. 
There are obvious differences between these 
two grammars. For example, feature struc- 
tures and multi-anchor etrees are present only 
in the XTAG grammar, whereas frequency in- 
formation is available only in G2. When we 
match templates in two grammars, we disre- 
58 
template etree 
types types 
Eng G1 6926 131,397 
Eng G2 2920 117,356 
Ch G3 1140 21,125 
Kor G4 634 9,787 
word 
types 
49,206 
49,206 
10,772 
6,747 
etree types etree types CFG rules 
per word type i per word token (unlexicalized) 
2.67~ 34.68 1524 
2.38 27.70 675 
1.96 9.13 515 
1.45 2.76 177 
Table 1: Grammars extracted from three Treebanks 
' r  
o.~ 
o.e 
0.7 
o.e 
o.5 
o.4 
o~ 
02  
o.~ 
o 
T~ m T ~  
(a) Frequency of templates (b) Coverage of templates 
Figure 12: Template types and template tokens in G1 
gard the type of information that is present 
only in one grammar. As a result, the map- 
ping between two grammars is not one-to-one. 
XTAG 
G~ 
~equency 
matched unmatched total 
templates templates 
497 507 1004 
215 2705 2920 
82.1% I 17.9% \[100% 
Table 2: Matched templates in two grammars 
Table 2 shows that 497 templates in the 
XTAG grammar and 215 templates in G2 
match, and the latter accounts for 82.1% of 
the template tokens in the PTB.  The remain- 
ing 17.9% template tokens in the PTB do not 
match any template in the XTAG grammar 
because of one of the following reasons: 
(T1) Incorrect templates in G2: These tem- 
plates result from Treebank annotation er- 
rors, and therefore, are not in XTAG. 
(T2) Coordination in XTAG: the templates 
for coordinations in XTAG are generated 
on the fly while parsing (Sarkar and Joshi, 
1996), and are not part of the 1004 templates. 
Therefore, the conj-etrees in G2, which ac- 
count for 3.4% of the template tokens in the 
Treebank, do not match any templates in 
XTAG. 
(T3) Alternative analyses: XTAG and PTB 
sometimes choose different analyses for the 
same phenomenon. For example, the two 
grammars treat reduced relative clauses dif- 
ferently. As a result, the templates used to 
handle those phenomena in these two gram- 
mars do not match according to our defini- 
tion. 
(T4) Construct ions not covered by XTAG: 
Some of such constructions are the unlike 
coordination phrase (UCP), parenthetical 
(PRN), and ellipsis. 
For (T1)--(T3),  the XTAG grammar can 
handle the corresponding constructions al- 
though the templates used in two grammars 
look very different. To find out what construc- 
tions are not covered by XTAG, we manually 
classify 289 of the most frequent unmatched 
templates in G2 according to the reason why 
they are absent from XTAG. These 289 tem- 
plates account for 93.9% of all the unmatched 
template tokens in the Treebank. The results 
are shown in Table 3, where the percentage is
with respect o all the tokens in the Treebank. 
From the table, it is clear that the most com- 
mon reason for mis-matches i (T3). Combin- 
ing the results in Table 2 and 3, we conclude 
that 97.2% of template tokens in the Treebank 
are covered by XTAG, while another 1.7% are 
not. For the remaining 1.1% template tokens, 
we do not know whether or not they are cov- 
ered by XTAG because we have not checked 
the remaining 2416 unmatched templates in 
G2. T 
To summarize, we have just showed that, 
7The number 97.2% is the sum of two numbers: 
the first one is the percentage of matched template to- 
kens (82.1% from Table 2). The secb-nd number is the 
percentage oftemplate tokens which fall under (T1)-- 
(T3), i.e., 16.8%-1.7%=15.1% from Table 3. 
59 
T1 T2 T3 T4 total 
type 51 52 ~93 93 289 
freq 1.1% 3.4% 10.6% 1.7% 16.8% 
Table 3: Classifications of 289 unmatched 
templates 
by comparing templates in the XTAG gram- 
mar with the 'IYeebank grammar produced by 
LexTract, we estimate that the XTAG gram- 
mar covers 97.2% of template tokens in the 
English Treebank. Comparing with previous 
evaluation approach, this :method has several 
advantages. First, the whole process is semi- 
automatic and requires little human effort. 
Second, the coverage can be calculated at ei- 
ther sentence l vel or etree level, which is more 
fine-grained. Third, the method provides a 
list of etrees that can be added to the gram- 
mar to improve its coverage. Fourth, there 
is no need to parse the whole corpus, which 
could have been very time-consuming. 
4.2 Training Super taggers  
A Supertagger (Joshi and Srinivas, 1994; 
Srinivas, 1997) assigns an etree template to 
each word in a sentence. The templates 
are also called Supertags because they in- 
clude more information than Part-of-Speech 
tags. Srinivas implemented the first Supertag- 
ger, and he also built a Lightweight Depen- 
dency Analyzer that assembles the Supertags 
of words to create an almost-parse for the sen- 
tence. Supertaggers have been found useful 
for several applications, such as information 
retrieval (Chandrasekar nd Srinivas, 1997). 
To use a Treebank to train a Supertagger, 
the phrase structures in the Treebank have to 
be converted into (word, Supertag) sequences 
first. Producing such sequences i  exactly one 
of LexTract's main functions, as shown previ- 
ously in Section 3.3.2 and Figure 9. 
Besides LexTract, there are two other at- 
tempts in converting the English Penn Tree- 
bank to train a Supertagger. Srinivas (1997) 
uses heuristics to map structural information 
in the Treebank into Supertags. His method 
is different from LexTract in that the set of 
Supertags in his method is chosen from the 
pre-existing XTAG grammar before the con- 
version starts, whereas LexTract extracts the 
Supertag set from Treebanks. His conversion 
program is also designed for this particular 
Supertag set, and it is not very-easy to port 
it to another Supertag set. A third difference 
is that the Supertags in his converted ata do 
not always fit together, due to the discrep- 
ancy between the XTAG grammar and the  
Treebank annotation and the fact that the 
XTAG grammar does not cover all the tem- 
plates in the Treebank (see Section 4.1). In 
other words, even if the Supertagger is 100% 
accurate, it is possible that the correct parse 
for a sentence can not be produced by com- 
bining those Supertags in the sentence. 
Another work in converting Treebanks into 
LTAGs is described in (Chen and Vijay- 
Shanker, 2000). The method is similar to ours 
in that both work use Head Percolation Tables 
to find the head and both distinguish adjuncts 
from modifiers using syntactic tags and func- 
tional tags. Nevertheless, there are several 
differences: only LexTract explicitly creates 
fully bracketed ttrees, which are identical to 
the derived trees for the sentences. As a re- 
sult, building etrees can be seen as a task of 
decomposing the fully bracketed ttrees. The 
mapping between the nodes in fully bracketed 
ttrees and etrees makes LexTract a useful tool 
for 'IYeebank annotation and error detection. 
The two approaches also differ in how they 
distinguish arguments from adjuncts and how 
they handle coordinations. 
Table 4 lists the tagging accuracy of the 
same trigram Supertagger (Srinivas, 1997) 
trained and tested on the same original PTB 
data. s The difference in tagging accuracy 
is caused by different conversion algorithms 
that convert the original PTB data into the 
(word, template) sequences, which are fed 
to the Supertagger. The results of Chen & 
Vijay-Shanker's method come from their pa- 
per (Chen and Vijay-Shanker, 2000). They 
built eight grammars. We just list two of them 
which seem to be most relevant: C4 uses a re- 
duced tagset while C3 uses the PTB tagset. 
As for Srinivas' results, we did not use the re- 
sults reported in (Srinivas, 1997) and (Chen et 
al., 1999) because they are based on different 
training and testing data. 9 Instead, we re-ran 
SAll use Section 2-21 of the PTB for training, and 
Section 22 or 23 for testing. We choose those sec- 
tions because several state-of-thwart parsers (Collins, 
1997; Ratnaparkhi, 1998; Charniak, 1997) are trained 
on Section 2-21 and tested on Section 23. We include 
the results for Section 22 because (Chen and Vijay- 
Shanker, 2000) is tested on that section. For Srinivas' 
and our grammars, the first line is the results tested on 
Section 23, and the second line is the one for Section 
22. Chen & Vijay-Shauker's results~e for Section 22 
only. 
9He used Section 0-24 minus Section 20 for training 
and the Section 20 for testing. 
60 
his Supertagger using his data on the sections 
that we have chosen. 1? We have calculated 
two baselines for each seg of data. The first 
one tags each word in testing data with the 
most common Supertag w.r.t the word in the 
training data. For an unknown word, just use 
its most common Supertag. For the second 
baseline, we use a trigram POS tagger to tag 
the words first, and then for each word we use 
the most common Supertag w.r.t, the (word, 
POS tag) pair. 
templates 
Srinivas' 483 
our G2 2920 
our G1 6926 
Chen's 2366 - -  
(sect 22) - -  8996 
C4 4911 
C3 8623 
basel base2 
72.59 74.24 
72.14 73.74 
71.45 74.14 
70.54 73.41 
69.70 71.82 
68.79 70.90 
acc 
85.78 
85.53 
84.41 
83.60 
82.21 
81.88 
77.8 - -  
- -  78.9 
78.90 
78.O0 
Table 4: Supertagging results based on three 
different conversion algorithms 
A few observations are in order. First, the 
baselines for Supertagging are lower than the 
one for POS tagging, which is 91%, indicat- 
ing Supertagging is harder than POS tagging. 
Second, the second baseline is slightly bet- 
ter than the first baseline, indicating using 
~?Noticeably, the results we report on Srinivas' data, 
85.78% on Section 23 and 85.53% on Section 22, axe 
lower than 92.2% reported in (Srinivas, 1997) and 
91.37% in (Chen et al, 1999). There axe several 
reasons for the difference. First, the size of training 
data in our report is smaller than the one for his pre- 
vious work; Second, we treat punctuation marks as 
normal words during evaluation because, like other 
words, punctuation marks can anchor etrees, whereas 
he treats the Supertags for punctuation marks as al- 
ways correct. Third, he used some equivalent classes 
during evaluations. If a word is mis-tagged as x, while 
the correct Supertag is y, he considers that not to be 
an error if x and y appear in the same equivalent class. 
We suspect that the reason that those Supertagging er- 
rors axe disregarded is that those errors might not af- 
fect parsing results when the Supertags are combined. 
For example, both adjectives and nouns can modify 
other nouns. The two templates (i.e. Supertags) rep- 
resenting these modification relations look the same 
except for the POS tags of the anchors. If a word 
which should be tagged with one Supertag is mis- 
tagged with the other Supertag, it is likely that the 
wrong Supertag can still fit with other Supertags in 
the sentence and produce the right parse. We did not 
use these quivalent classes in this experiment because 
we are not aware of a systematic way to find all the 
cases in which Supertagging errors do not affect the 
final parsing results. 
POS tags may improve the Supertagging ac- 
curacy, n Third, the Supertagging accuracy 
using G2 is 1.3-1.9% lower than the one using 
Srinivas' data. This is not surprising since the 
size of G2 is 6 times that of Srinivas' grammar. 
Notice that G1 is twice the size of G2 and 
the accuracy using G1 is 2% lower. Fourth, 
higher Supertagging accuracy does not neces- 
sarily means the quality of converted ata are 
better since the underlying rammars differ a 
lot with respect o the size and the coverage. 
A better measure will be the parsing accu- 
racy (i.e., the converted ata should be fed to 
a common LTAG parser and the evaluations 
should be based on parsing results). We are 
currently working on that. Nevertheless, the 
experiments show that the (word, template) 
sequences produced by LexTract are useful for 
training Supertaggers. Our results are slightly 
lower than the ones trained on Srinivas' data, 
but our conversion algorithm has several ap- 
pealing properties: LexTract does not use pre- 
existing Supertag set; LexTract is language- 
independent; he (word, Supertag) sequence 
produced by LexTract fit together. 
5 Conc lus ion  
We have presented a system for grammar ex- 
traction that produces an LTAG from a Tree- 
bank. The output produced by the system 
has been used in many NLP tasks, two of 
which are discussed in the paper. In the first 
task, by comparing the XTAG grammar with 
a Treebank grammar produced by LexTract, 
we estimate that the XTAG grammar covers 
97.2% of template tokens in the English Tree- 
bank. We plan to use the Treebank grammar 
to improve the coverage of the XTAG gram- 
mar. We have also found constructions that 
are covered in the XTAG grammar but do not 
appear in the Treebank. In the second task, 
LexTract converts the Treebank into a format 
that can be used to train Supertaggers, and 
the Supertagging accuracy is compatible to, if 
not better than, the ones based on other con- 
version algorithms. For future work, we plan 
to use derivation trees to train LTAG parsers 
directly and use LexTract to add semantic in- 
formation to the Penn Treebank. 
Re ferences  
R. Chandrasekar nd B. Srinivas. 1997. Glean- 
ing information from the Web: Using Syntax 
to Filter out Irrelevant Information. In Proc. of 
nThe baselines and results on Section 23 for (Chen 
and Vijay-Shanker, 2000) are not available to us. 
61 
AAAI 1997 Spring Symposium on NLP on the 
World Wide Web. 
Eugene Charniak. 1996. Treebank Grammars. In 
Proc. of AAAI-1996. 
Eugene Charniak. 1997. Statistical Parsing with 
a Context-Free Grammar and Word Statistics. 
In Proc. of AAAI-1997. 
John Chen and K. Vijay-Shanker. 2000. Auto- 
mated Extraction of TAGs from the Penn Tree- 
bank. In 6th International Workshop on Pars- 
ing Technologies (IWPT..2000), Italy. 
John Chen, Srinivas Bangalore, and K. Vijay- 
Shanker. 1999. New Models for Improving 
Supertag Disambiguation. In Proc. of EACL- 
1999. 
Mike Collins. 1997. Three Generative, Lexicalised 
Models for Statistical Parsing. In Proc. of the 
35th ACL. 
C. Doran, D. Egedi, B. A. Hockey, B. Srinivas, 
and M. Zaidel. 1994. XTAG System - A Wide 
Coverage Grammar for English. In Proc. of 
COLING-1994, Kyoto, Japan. 
Aravind Joshi and B. Srinivas. 1994. Disambigua- 
tion of Super Parts of Speech (or Supertags): 
Almost Parsing. In Proc. of COLING-1994. 
Aravind Joshi and K. Vijay-Shanker. 1999. Com- 
positional Semantics with LTAG: How Much 
Underspecification Is Necessary? In Proc. of 
3nd International Workshop on Computational 
Semantics. 
Aravind K. Joshi, L. Levy, and M. Takahashi. 
1975. Tree Adjunct Grammars. Journal of 
Computer and System Sciences. 
Laura Kallmeyer and Aravind Joshi. 1999. Un- 
derspecified Semantics with LTAG. 
Alexander Krotov, Mark Hepple, Robert 
Galzauskas, and Yorick Wilks. 1998. Compact- 
ing the Penn Treebank Grammar. In Proc. of 
A CL- COLING. 
David M. Magerman. 1995. Statistical Decision- 
Tree Models for Parsing. In Proc. of the 33rd 
ACL. 
M. Marcus, B. Santorini, and M. A. 
Marcinkiewicz. 1993.  Building a Large 
Annotated Corpus of English: the Penn 
Treebank. Computational Lingustics. 
K. F. McCoy, K. Vijay-Shanker, and G. Yang. 
1992. A Functional Approach to Generation 
with TAG. In Proc. of the 30th A CL. 
Martha Palmer, Owen Rainbow, and Alexis 
Nasr. 1998. Rapid Prototyping of Domain- 
Specific Machine Translation System. In Proc. 
of AMTA-1998, Langhorne, PA. 
Rashmi Prasad and Anoop Sarkar. 2000. Compar- 
ing Test-Suite Based Evaluation and Corpus- 
Based Evaluation of a Wide-Coverage Grammar 
for English. In Proc. of LREC satellite work- 
shop Using Evaluation within HLT Programs: 
Results and Trends, Athen, Greece. 
Carlos A. Prolo. 2000. An Efficient LR Parser 
Generator for TAGs. In 6th International 
Workshop on Parsing Technologies (IWPT 
2000), Italy. 
Adwait Ratnaparkhi. 1998. Maximum Entropy 
Models for Natural Language Ambiguity Resolu- 
tion. Ph.D. thesis, University of Pennsylvania. 
Anoop Sarkar and Aravind Joshi. 1996. Coordi- 
nation in Tree Adjoining Grammars: Formaliza- 
tion and Implementation. In Proc. of the 18th 
COLING, Copenhagen, Denmark. 
Anoop Sarkar. 2000. Practical Experiments in 
Parsing using Tree Adjoining Grammars. In 
Proc. of 5th International Workshop on TAG 
and Related Frameworks (TAG+5). 
The XTAG-Group. 1998. A Lexicalized Tree Ad- 
joining Grammar for English. Technical Report 
IRCS 98-18, University of Pennsylvania. 
Yves Schabes and Stuart Shieber. 1992. An Al- 
ternative Conception of Tree-Adjoining Deriva- 
tion. In Proc. of the 20th Meeting of the Asso- 
ciation for Computational Linguistics. 
Yves Schabes. 1990. Mathematical nd Computa- 
tional Aspects of Lexicalized Grammars. Ph.D. 
thesis, University of Pennsylvania. 
Kiyoaki Shirai, Takenobu Tokunaga, and Hozumi 
Tanaka. 1995.  Automatic Extraction of 
Japanese Grammar from a Bracketed Corpus. 
In Proc. of Natural Language Processing Pacific 
Rim Symposium (NLPRS-1995). 
B. Srinivas, Anoop Sarkar, Christine Doran, and 
Beth Ann Hockey. 1998. Grammar and Parser 
Evaluation in the XTAG Project. In Workshop 
on Evaluation of Parsing Systems, Granada, 
Spain. 
B. Srinivas. 1997. Complexity of Lexical De- 
scriptions and Its Relevance to Partial Parsing. 
Ph.D. thesis, University of Pennsylvania. 
Matthew Stone and Christine Doran. 1997. Sen- 
tence Planning as Description Using Tree Ad- 
joining Grammar. In Proc. of the 35th A CL. 
Bonnie Webber and Aravind Joshi. 1998. Anchor- 
ing a Lexicalized Tree Adjoining Grammar for 
Discourse. In Proc. of A CL-COLING Workshop 
on Discourse Relations and Discourse Markers. 
Fei Xia and Tonia Bleam. 2000. A Corpus-Based 
Evaluation of Syntactic Locality in TAGs. In 
Proc. of 5th International Workshop on TAG 
and Related Frameworks (TAG+5). 
Fei Xia, Chunghye Han, Martha Palmer, and 
Aravind Joshi. 2000a. Comparing Lexicalized 
Treebank Grammars Extracted from Chinese, 
Korean, and English Corpora. In Proc. of the 
2nd GT~inese Language Processing Workshop, 
Hong Kong, China. 
Fei Xia, Martha Palmer, Nianwen Xue, Mary Ellen 
Okurowski, John Kovarik, Shizhe Huang, Tony 
Kroch, and Mitch Marcus. 2000b. Developing 
Guidelines and Ensuring Consistency for Chi- 
nese Text Annotation. In Proc. of the 2nd In- 
ternational Conference on Language Resources 
and Evaluation (LREC-2000),-Athens, Greece. 
62 
An SVM Based Voting Algorithm
with Application to Parse Reranking
Libin Shen and Aravind K. Joshi
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104, USA
{libin,joshi}@linc.cis.upenn.edu
Abstract
This paper introduces a novel Support Vec-
tor Machines (SVMs) based voting algorithm
for reranking, which provides a way to solve
the sequential models indirectly. We have
presented a risk formulation under the PAC
framework for this voting algorithm. We have
applied this algorithm to the parse reranking
problem, and achieved labeled recall and pre-
cision of 89.4%/89.8% on WSJ section 23 of
Penn Treebank.
1 Introduction
Support Vector Machines (SVMs) have been successfully
used in many machine learning tasks. Unlike the error-
driven algorithms, SVMs search for the hyperplane that
separates a set of training samples that contain two dis-
tinct classes and maximizes the margin between these
two classes. The ability to maximize the margin is be-
lieved to be the reason for SVMs? superiority over other
classifiers. In addition, SVMs can achieve high perfor-
mance even with input data of high dimensional feature
space, especially because of the use of the ?kernel trick?.
However, the incorporation of SVMs into sequential
models remains a problem. An obvious reason is that
the output of an SVM is the distance to the separating
hyperplane, but not a probability. A possible solution
to this problem is to map SVMs? results into probabili-
ties through a Sigmoid function, and use Viterbi search
to combine those probabilities (Platt, 1999). However,
this approach conflicts with SVMs? purpose of achieving
the so-called global optimization1. First, this approach
may constrain SVMs to local features because of the left-
to-right scanning strategy. Furthermore, like other non-
generative Markov models, it suffers from the so-called
1By global we mean the use of quadratic optimization in
margin maximization.
label bias problem, which means that the transitions leav-
ing a given state compete only against each other, rather
than against all other transitions in the model (Lafferty et
al., 2001). Intuitively, it is the local normalization that
results in the label bias problem.
One way of using discriminative machine learning al-
gorithms in sequential models is to rerank the n-best out-
puts of a generative system. Reranking uses global fea-
tures as well as local features, and does not make lo-
cal normalization. If the output set is large enough, the
reranking approach may help to alleviate the impact of
the label bias problem, because the victim parses (i.e.
those parses which get penalized due to the label bias
problem) will have a chance to take part in the rerank-
ing.
In recent years, reranking techniques have been suc-
cessfully applied to the so-called history-based models
(Black et al, 1993), especially to parsing (Collins, 2000;
Collins and Duffy, 2002). In a history-based model, the
current decision depends on the decisions made previ-
ously. Therefore, we may regard parsing as a special form
of sequential model without losing generality.
Collins (2000) has proposed two reranking algorithms
to rerank the output of an existing parser (Collins, 1999,
Model 2). One is based on Markov Random Fields, and
the other is based on a boosting approach. In (Collins and
Duffy, 2002), the use of Voted Perceptron (VP) (Freund
and Schapire, 1999) for the parse reranking problem has
been described. In that paper, the tree kernel (Collins
and Duffy, 2001) has been used to efficiently count the
number of common subtrees as described in (Bod, 1998).
In this paper we will follow the reranking approach.
We describe a novel SVM-based voting algorithm for
reranking. It provides an alternative way of using a large
margin classifier for sequential models. Instead of using
the parse tree itself as a training sample, we use a pair of
parse trees as a sample, which is analogous to the pref-
erence relation used in the context of ordinal regression
(Herbrich et al, 2000). Furthermore, we justify the al-
gorithm through a modification of the proof of the large
margin rank boundaries for ordinal regression. We then
apply this algorithm to the parse reranking problem.
1.1 A Short Introduction of SVMs
In this section, we give a short introduction of Support
Vector Machines. We follow (Vapnik, 1998)?s definition
of SVMs. For each training sample (yi,xi), yi repre-
sents its class, and xi represents its input vector defined
on a d-dimensional space. Suppose the training samples
{(y1,x1), ..., (yn,xn)} (xi ? Rd, yi ? {?1, 1}) can be
separated by a hyperplane H: (x ? w) + b = 0, which
means
yi((xi ?w) + b) ? 1, (1)
where w is normal to the hyperplane. To train an SVM is
equivalent to searching for the optimal separating hyper-
plane that separates the training data without error and
maximizes the margin between two classes of samples. It
can be shown that maximizing the margin is equivalent to
minimizing ||w||2.
In order to handle linearly non-separable cases, we
introduce a positive slack variable ?i for each sample
(yi,xi). Then training can be reduced to the following
Quadratic Programming (QP) problem.
Maximize:
LD(?) ?
l?
i=1
?i ?
1
2
l?
i,j=1
yiyj?i?j(xi ? xj) (2)
subject to: 0 ? ?i ? C and
?
i ?iyi = 0,
where ?i(i = 1...l) are the Lagrange multipliers, l is the
total number of training samples, and C is a weighting
parameter for mis-classification.
Since linearly non-separable samples may become sep-
arable in a high-dimensional space, SVMs employ the
?kernel trick? to implicitly separate training samples in
a high-dimensional feature space. Let ? : Rd 7? Rh
be a function that maps a d-dimensional input vector
x to an h-dimensional feature vector ?(x). In order
to search for the optimal separating hyperplane in the
higher-dimensional feature space, we only need to sub-
stitute ?(xi) ? ?(xj) with xi ? xj in formula (2).
If there is a function K, such that K(xi,xj) = ?(xi)?
?(xj), we don?t need to compute ?(xi) explicitly. K is
called a kernel function. Thus during the training phase
we need to solve the following QP problem.
Maximize:
LD(?) ?
l?
i=1
?i ?
1
2
l?
i,j=1
yiyj?i?jK(xi,xj). (3)
subject to: 0 ? ?i ? C, and
?
i ?iyi = 0.
Let x be a test vector, the decision function is
f(x) = sgn(
Ns?
j=1
?jyjK(sj,x) + b) (4)
where sj is a training vector whose corresponding La-
grange multiplier ?j > 0. sj is called a support vector.
Ns is the total number of the support vectors. According
to (4), the decision function only depends on the support
vectors.
It is worth noting that not any function K can be used
as a kernel. We call function K : Rd ? Rd 7? R
a well-defined kernel if and only if there is a mapping
function ? : Rd 7? Rh such that, for any xi,xj ? Rd,
K(xi,xj) = ?(xi) ? ?(xj). One way of testing whether
a function is a well-defined kernel is to use the Mer-
cer?s theorem (Vapnik, 1998) by utilizing the positive
semidefinteness property. However, as far as a discrete
kernel is concerned, there is a more convenient way to
show that a function is a well-defined kernel. This is
achieved by showing that a function K is a kernel by find-
ing the corresponding mapping function ?. This method
was used in the proof of the string subsequence kernel
(Cristianini and Shawe-Tayor, 2000) and the tree kernel
(Collins and Duffy, 2001).
1.2 Large Margin Classifiers
SVMs are called large margin classifiers because they
search for the hyperplane that maximizes the margin. The
validity of the large margin method is guaranteed by the
theorems of Structural Risk Minimization (SRM) under
Probably Approximately Correct (PAC) framework2; test
error is related to training data error, number of training
samples and the capacity of the learning machine (Smola
et al, 2000).
Vapnik-Chervonenkis (VC) dimension (Vapnik, 1999),
as well as some other measures, is used to estimate the
complexity of the hypothesis space, or the capacity of the
learning machine. The drawback of VC dimension is that
it ignores the structure of the mapping from training sam-
ples to hypotheses, and concentrates solely on the range
of the possible outputs of the learning machine (Smola
et al, 2000). In this paper we will use another measure,
the so-called Fat Shattering Dimension (Shawe-Taylor et
al., 1998), which is shown to be more accurate than VC
dimension (Smola et al, 2000), to justify our voting al-
gorithm,
Let F be a family of hypothesis functions. The fat
shattering dimension of F is a function from margin ? to
the maximum number of samples such that any subset of
2SVM?s theoretical accuracy is much lower than their actual
performance. The ability to maximize the margin is believed to
be the reason for SVMs? superiority over other classifiers.
these samples can be classified with margin ? by a func-
tion in F . An upper bound of the expected error is given
in Theorem 1 below (Shawe-Taylor et al, 1998). We will
use this theorem to justify the new voting algorithm.
Theorem 1 Consider a real-valued function class F
having fat-shattering function bounded above by the
function afat : R ? N which is continuous from the
right. Fix ? ? R. If a learner correctly classifies m
independently generated examples z with h = T?(f) ?
T?(F ) such that erz(h) = 0 and ? = min |f(xi) ? ?|,
then with confidence 1 ? ? the expected error of h is
bounded from above by
2
m
(k log(
8em
k
) log(32m) + log(
8m
?
)) (5)
where k = afat(?/8).
2 A New SVM-based Voting Algorithm
Let xij be the jth candidate parse for the ith sentence in
training data. Let xi1 is the parse with the highest f -score
among all the parses for the ith sentence.
We may take xi1 as positive samples, and xij(j>1) as
negative samples. However, experiments have shown that
this is not the best way to utilize SVMs in reranking (Di-
jkstra, 2001). A trick to be used here is to take a pair of
parses as a sample: for any i and j > 1, (xi1, xij) is a
positive sample, and (xij , xi1) is a negative sample.
Similar idea was employed in the early works of parse
reranking. In the boosting algorithm of (Collins, 2000),
for each sample (parse) xij , its margin is defined as
F (xi1, ??) ? F (xij , ??), where F is a score function and
?? is the parameter vector. In (Collins and Duffy, 2002),
for each offending parse, the parameter vector updating
function is in the form of w = w + h(xi1) ? h(xij),
where w is the parameter vector and h returns the feature
vector of a parse. But neither of these two papers used a
pair of parses as a sample and defined functions on pairs
of parses. Furthermore, the advantage of using difference
between parses was not theoretically clarified, which we
will describe in the next section.
As far as SVMs are concerned, the use of parses or
pairs of parses both maximize the margin between xi1
and xij , but the one using a single parse as a sample
needs to satisfy some extra constraints on the selection
of decision function. However these constraints are not
necessary (see section 3.3). Therefore the use of pairs of
parses has both theoretic and practical advantages.
Now we need to define the kernel on pairs of parses.
Let (t1, t2), (v1, v2) are two pairs of parses. Let K is
any kernel function on the space of single parses. The
preference kernel PK is defined on K as follows.
PK((t1, t2), (v1, v2)) ? K(t1, v1)?K(t1, v2)
?K(t2, v1) +K(t2, v2) (6)
The preference kernel of this form was previously used
in the context of ordinal regression in (Herbrich et al,
2000). Then the decision function is
f((xj , xk)) =
Ns?
i=1
?iyiPK((si1, si2), (xj , xk)) + b
= b + (
Ns?
i=1
?iyi(K(si1, xj)?K(si2, xj)))
?(
Ns?
i=1
?iyi(K(si1, xk)?K(si2, xk))),
where xj and xk are two distinct parses of a sentence,
(si1, si2) is the ith support vector, and Ns is the total
number of support vectors.
As we have defined them, the training samples are
symmetric with respect to the origin in the space. There-
fore, for any hyperplane that does not pass through the
origin, we can always find a parallel hyperplane that
crosses the origin and makes the margin larger. Hence,
the outcome separating hyperplane has to pass through
the origin, which means that b = 0.
Therefore, for each test parse x, we only need to com-
pute its score as follows.
score(x) =
Ns?
i=1
?iyi(K(si1, x)?K(si2, x)), (7)
because
f((xj , xk)) = score(xj)? score(xk). (8)
2.1 Kernels
In (6), the preference kernel PK is defined on kernel K.
K can be any possible kernel. We will show that PK is
well-defined in the next section. In this paper, we con-
sider two kernels for K, the linear kernel and the tree
kernel.
In (Collins, 2000), each parse is associated with a set
of features. Linear combination of the features is used
in the decision function. As far as SVM is concerned,
we may encode the features of each parse with a vec-
tor. Dot product is used as the kernel K. Let u and v
are two parses. The computational complexity of linear
kernel O(|fu| ? |fv|), where |fu| and |fv| are the length
of the vectors associated with parse u and v respectively.
The goodness of the linear kernel is that it runs very fast
in the test phase, because coefficients of the support vec-
tors can be combined in advance. For a test parse x, the
computational complexity of test is only O(|fx|), which
is independent with the number of the support vectors.
In (Collins and Duffy, 2002), the tree kernel Tr is used
to count the total number of common sub-trees of two
parse trees. Let u and v be two trees. Because Tr can be
computed by dynamic programming, the computational
complexity of Tr(u, v) is O(|u| ? |v|), where |u| and |v|
are the tree sizes of u and v respectively. For a test parse
x, the computational complexity of the test is O(S ? |x|),
where S is the number of support vectors.
3 Justifying the Algorithm
3.1 Justifying the Kernel
Firstly, we show that the preference kernel PK defined
above is well-defined. Suppose kernel K is defined on
T ? T. So there exists ? : T 7? H, such that
K(x1, x2) = ?(x1) ? ?(x2) for any x1, x2 ? T.
It suffices to show that there exist space H? and
mapping function ?? : T?T ? H? such that
PK((t1, t2), (v1, v2)) = ??(t1, t2) ? ??(t1, t2), where
t1, t2, v1, v2 ? T.
According to the definition of PK, we have
PK((t1, t2), (v1, v2))
= K(t1, v1)? K(t1, v2)? K(t2, v1) + K(t2, v2)
= ?(t1) ? ?(v1)? ?(t1) ? ?(v2)
??(t2) ? ?(v1) + ?(t2) ? ?(v2)
= (?(t1)? ?(t2)) ? (?(v1)? ?(v2)), (9)
Let H? = H and ??(x1, x2) = ?(x1)??(x2). Hence
kernel PK is well-defined.
3.2 Margin Bound for SVM-based Voting
We will show that the expected error of voting is bounded
from above in the PAC framework. The approach used
here is analogous to the proof of ordinal regression (Her-
brich et al, 2000). The key idea is to show the equiva-
lence of the voting risk and the classification risk.
Let X be the set of all parse trees. For each x ? X ,
let x? be the best parse for the sentence related to x. Thus
the appropriate loss function for the voting problem is as
follows.
lvote(x, f) ?
{
1 if f(x?) < f(x)
0 otherwise (10)
where f is a parse scoring function.
Let E = {(x, x?)|x ? X} ? {(x?, x)|x ? X}. E
is the space of event of the classification problem, and
PrE((x, x?)) = PrE((x?, x)) = 12PrX(x). For any parse
scoring function f , let gf (x1, x2) ? sgn(f(x1)?f(x2)).
For classifier gf on space E , its loss function is
lclass(x1, x2, gf ) ?
?
?????
?????
1 if x1 = x?2 and
gf (x1, x2) = ?1
1 if x2 = x?1 and
gf (x1, x2) = +1
0 otherwise
Therefore the expected risk Rvote(f) for the voting
problem is equivalent to the expected risk Rclass(gf ) for
the classification problem.
Rvote(f)
= Ex?X(lvote(x, f))
= E(x,x?)?E(lvote(x, f)) +E(x?,x)?E(lvote(x, f))
= E(x1,x2)?E(lclass(x1, x2, gf ))
= Rclass(gf ) (11)
However, the definition of space E violates the inde-
pendently and identically distributed (iid) assumption.
Parses for the same sentence are not independent. If we
suppose that no two pairs of parses come from the same
sentence, then the idd assumption holds. In practice, the
number of sentences is very large, i.e. more than 30000.
So we may use more than one pair of parses of the same
sentence and still assume the idd property roughly, be-
cause for any two arbitrary pairs of parses, 29999 out of
30000, these two samples are independent.
Let ? ? mini=1..n,j=2..mi |f(xi1) ? f(xij)| =
mini=1..n,j=2..mi |g(xi1, xij)?0|. According to (11) and
Theorem 1 in section 1.2 we get the following theorem.
Theorem 2 If gf makes no error on the training data,
with confidence 1? ?
Rvote(f) = Rclass(gf )
?
2
m
(k log(
8em
k
) log(32m)
+ log(
8m
?
)), (12)
where k = afat(?/8),m =
?
i=1..n(mi ? 1).
3.3 Justifying Pairwise Samples
An obvious way to use SVM is to use each single parse,
instead of a pair of parse trees, as a training sample. Only
the best parse of each sentence is regarded as a positive
sample, and all the rest are regarded as negative samples.
Similar to the pairwise system, it also maximizes the mar-
gin between the best parse of a sentence and all incorrect
parses of this sentence. Suppose f is the function result-
ing from the SVM. It requires yijf(xij) > 0 for each
sample (xij , yij). However this constraint is not neces-
sary. We only need to guarantee that f(xi1) > f(xij).
This is the reason for using pairs of parses as training
samples instead of single parses.
We may rewrite the score function (7) as follows.
score(x) =
?
i,j
ci,jK(si,j , x), (13)
where i is the index for sentence, j is the index for parse,
and ?i
?
j ci,j = 0.
The format of score in (13) is the same as the deci-
sion function generated by an SVM trained on the single
parses as samples. However, there is a constraint that
the sum of the coefficients related to parses of the same
sentence is 0. So in this way we decrease the size of hy-
pothesis space based on the prior knowledge that only the
different segments of two distinct parses determine which
parse is better.
4 Related Work
The use of pairs of parse trees in our model is analogous
to the preference relation used in the ordinal regression
algorithm (Herbrich et al, 2000). In that paper, pairs of
objects have been used as training samples. For exam-
ple, let (r1, r2, ...rm) be a list of objects in the training
data, where ri ranks ith. Then pairs of objects (ri?1, ri)
are training samples. Preference kernel PK in our paper
is the same as the preference kernel in (Herbrich et al,
2000) in format.
However, the purpose of our model is different from
that of the ordinal regression algorithm. Ordinal regres-
sion searches for a regression function for ordinal values,
while our algorithm is designed to solve a voting prob-
lem. As a result, the two algorithms differ on the def-
inition of the margin. In ordinal regression, the margin
is min |f(ri) ? f(ri?1)|, where f is the regression func-
tion for ordinal values. In our algorithm, the margin is
min |score(xi1)? score(xij)|.
In (Kudo and Matsumoto, 2001), SVMs have been em-
ployed in the NP chunking task, a typical labeling prob-
lem. However, they have used a deterministic algorithm
for decoding.
In (Collins, 2000), two reranking algorithms were pro-
posed. In both of these two models, the loss functions are
computed directly on the feature space. All the features
are manually defined.
In (Collins and Duffy, 2002), the Voted Perceptron al-
gorithm was used to in parse reranking. It was shown
in (Freund and Schapire, 1999; Graepel et al, 2001) that
error bound of (voted) Perceptron is related to margins
existing in the training data, but these algorithm are not
supposed to maximize margins. Variants of the Percep-
tron algorithm, which are known as Approximate Maxi-
mal Margin classifier, such as PAM (Krauth and Mezard,
1987), ALMA (Gentile, 2001) and PAUM (Li et al,
2002), produce decision hyperplanes within ratio of the
maximal margin. However, almost all these algorithms
are reported to be inferior to SVMs in accuracy, while
more efficient in training.
Furthermore, these variants of the Perceptron algo-
rithm take advantage of the large margin existing in the
training data. However, in NLP applications, samples are
usually inseparable even if the kernel trick is used. SVMs
can still be trained to maximize the margin through the
method of soft margin.
5 Experiments and Analysis
We use SVM light (Joachims, 1998) as the SVM clas-
sifier. The soft margin parameter C is set to its default
value in SVM light.
We use the same data set as described in (Collins,
2000; Collins and Duffy, 2002). Section 2-21 of the Penn
WSJ Treebank (Marcus et al, 1994) are used as train-
ing data, and section 23 is used for final test. The train-
ing data contains around 40,000 sentences, each of which
has 27 distinct parses on average. Of the 40,000 training
sentences, the first 36,000 are used to train SVMs. The
remaining 4,000 sentences are used as development data.
The training complexity for SVM light is roughly
O(n2.1) (Joachims, 1998), where n is the number of the
training samples. One solution to the scaling difficulties
is to use the Kernel Fisher Discriminant as described in
(Salomon et al, 2002). In this paper, we divide train-
ing data into slices to speed up training. Each slice con-
tains two pairs of parses from each sentence. Specifically,
slice i contains positive samples ((p?k, pki),+1) and neg-
ative samples ((pki, p?k),?1), where p?k is the best parse
for sentence k, pki is the parse with the ith highest log-
likelihood in all the parses for sentence k and it is not the
best parse. There are about 60000 parses in each slice in
average. For each slice, we train an SVM. Then results
of SVMs are put together with a simple combination. It
takes about 2 days to train a slice on a P3 1.13GHz pro-
cessor.
As a result of this subdivision of the training data into
slices, we cannot take advantage of SVM?s global opti-
mization ability. This seems to nullify our effort to cre-
ate this new algorithm. However, our new algorithm is
still useful for the following reasons. Firstly, with the im-
provement in the computing resources, we will be able to
use larger slices so as to utilize more global optimization.
SVMs are superior to other linear classifiers in theory. On
the other hand, the current size of the slice is large enough
for other NLP applications like text chunking, although it
is not large enough for parse reranking. The last reason
is that we have achieved state-of-the-art results even with
the sliced data.
We have used both a linear kernel and a tree kernel.
For the linear kernel test, we have used the same dataset
as that in (Collins, 2000). In this experiment, we first train
22 SVMs on 22 distinct slices. In order to combine those
SVMs results, we have tried mapping SVMs? results to
probabilities via a Sigmoid as described in (Platt, 1999).
We use the development data to estimate parameter A
and B in the Sigmoid
Pi(y = 1|fi) =
1
1 + Ae?fiB
, (14)
Table 1: Results on section 23 of the WSJ Treebank.
LR/LP = labeled recall/precision. CBs = average number
of crossing brackets per sentence. 0 CBs, 2 CBs are the
percentage of sentences with 0 or ? 2 crossing brackets
respectively. CO99 = Model 2 of (Collins, 1999). CH00
= (Charniak, 2000). CO00 = (Collins, 2000).
?40 Words (2245 sentences)
Model LR LP CBs 0 CBs 2 CBs
CO99 88.5% 88.7% 0.92 66.7% 87.1%
CH00 90.1% 90.1% 0.74 70.1% 89.6%
CO00 90.1% 90.4% 0.73 70.7% 89.6%
SVM 89.9% 90.3% 0.75 71.7% 89.4%
?100 Words (2416 sentences)
Model LR LP CBs 0 CBs 2 CBs
CO99 88.1% 88.3% 1.06 64.0% 85.1%
CH00 89.6% 89.5% 0.88 67.6% 87.7%
CO00 89.6% 89.9% 0.87 68.3% 87.7%
SVM 89.4% 89.8% 0.89 69.2% 87.6%
where fi is the result of the ith SVM. The parse with
maximal value of
?
i Pi(y = 1|fi) is chosen as the top-
most parse. Experiments on the development data shows
that the result is better if Ae?fiB is much larger than 1.
Therefore
n?
i=1
Pi(y = 1|fi) =
n?
i=1
1
1 + Ae?fiB
?
n?
i
1
Ae?fiB
= A?ne(B
?n
i=1
fi) (15)
Therefore, we may use
?
i fi directly, and there is no
need to estimate A and B in (14). Then we combine
SVMs? result with the log-likelihood generated by the
parser (Collins, 1999). Parameter ? is used as the weight
of the log-likelihood. In addition, we find that our SVM
has greater labeled precision than labeled recall, which
means that the system prefer parses with less brackets.
So we take the number of brackets as another feature to
be considered, with weight ?. ? and ? are estimated on
the development data.
The result is shown in Table 1. The performance of
our system matches the results of (Charniak, 2000), but
is a little lower than the results of the Boosting system
in (Collins, 2000), except that the percentage of sen-
tences with no crossing brackets is 1% higher than that of
(Collins, 2000). Since we have to divide data into slices,
we cannot take full advantage of the margin maximiza-
tion.
Figure 1 shows the learning curves. ? is used to con-
Figure 1: Learning curves on the development dataset of
(Collins, 2000). X-axis stands for the number of slices to
be combined. Y-axis stands for the F -score.
0.89
0.892
0.894
0.896
0.898
0.9
0.902
0.904
0.906
0.908
0.91
0 5 10 15 20
number of slices
baseline
?=0
3
3
33
3333
33333333333333
3
?=200
+
+
++
++++++++++++++++++
+
?=500
2
2
2
2
2
2
2222
222222
222222
2
?=1000
??
??
??
??
??
???
??????
???
?
Table 2: Results on section 23 of the WSJ Treebank.
LR/LP = labeled recall/precision. CBs = average num-
ber of crossing brackets per sentence. CO99 = Model 2
of (Collins, 1999). CD02 = (Collins and Duffy, 2002)
?100 Words (2416 sentences)
Model LR LP CBs
CO99 88.1% 88.3% 1.06
CD02 88.6% 88.9% 0.99
SVM(tree) 88.7% 88.8% 0.99
trol the weight of log-likelihood given by the parser. The
proper value of ? depends on the size of training data.
The best result does not improve much after combining 7
slices of training data. We think this is due the limitation
of local optimization.
Our next experiment is on the tree kernel as it is used
in (Collins and Duffy, 2002). We have only trained 5
slices, since each slice takes about 2 weeks to train on
a P3 1.13GHz processor. In addition, the speed of test
for the tree kernel is much slower than that for the lin-
ear kernel. The experimental result is shown in Table 2
The results of our SVM system match the results of the
Voted Perceptron algorithm in (Collins and Duffy, 2002),
although only 5 slices, amounting to less than one fourth
of the whole training dataset, have been used.
6 Conclusions and Future Work
We have introduced a new approach for applying SVMs
to sequential models indirectly, and described a novel
SVM based voting algorithm inspired by the parse
reranking problem. We have presented a risk formula-
tion under the PAC framework for this voting algorithm,
and applied this algorithm to the parse reranking prob-
lem, and achieved LR/LP of 89.4%/89.8% on WSJ sec-
tion 23.
Experimental results show that the SVM with a linear
kernel is superior to the SVM with tree kernel in both
accuracy and speed. The SVM with tree kernel only
achieves a rather low f -score because it takes too many
unrelated features into account. The linear kernel is de-
fined on the features which are manually selected from a
large set of possible features.
As far as context-free grammars are concerned, it will
be hard to include more features into the current feature
set. If we simply use n-grams on context-free grammars,
it is very possible that we will introduce many useless fea-
tures, which may be harmful as they are in tree kernel sys-
tems. One way to include more useful features is to take
advantage of the derivation tree and the elementary trees
in Lexicalized Tree Adjoining Grammar (LTAG) (Joshi
and Schabes, 1997). The basic idea is that each elemen-
tary tree and every segment in a derivation tree is linguis-
tically meaningful.
We also plan to apply this algorithm to other sequen-
tial models, especially to the Supertagging problem. We
believe it will also be very useful to problems of POS
tagging and NP chunking. Compared to parse rerank-
ing, they have a much smaller training dataset and feature
size, which is more suitable for our SVM-based voting
problem.
Acknowledgments
We thank Michael Collins for help concerning the data
set, and Anoop Sarkar for his comments. We also thank
three anonymous reviewers for helpful comments.
References
E. Black, F. Jelinek, J. Lafferty, Magerman D. M.,
R. Mercer, and S. Roukos. 1993. Towards history-
based grammars: Using richer models for probabilistic
parsing. In Proceedings of the ACL 1993.
Rens Bod. 1998. Beyond Grammar: An Experience-
Based Theory of Language. CSLI Publica-
tions/Cambridge University Press.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of NAACL 2000.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings of Neural
Information Processing Systems (NIPS 14).
Michael Collins and Nigel Duffy. 2002. New ranking al-
gorithms for parsing and tagging: Kernels over discrete
structures, and the voted perceptron. In Proceedings of
ACL 2002.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proceedings of the 7th Inter-
national Conference on Machine Learning.
N. Cristianini and J. Shawe-Tayor. 2000. An introduction
to Support Vector Machines and other kernel-based
learning methods. Cambridge University Press.
Emma Dijkstra. 2001. Support vector machines for parse
selection. Master?s thesis, Univ. of Edinburgh.
Yoav Freund and Robert E. Schapire. 1999. Large mar-
gin classification using the perceptron algorithm. Ma-
chine Learning, 37(3):277?296.
Claudio Gentile. 2001. A new approximate maximal
margin classification algorithm. Journal of Machine
Learning Research, 2:213?242.
Thore Graepel, Ralf Herbrich, and Robert C. Williamson.
2001. From margin to sparsity. In Advances in Neural
Information Processing Systems 13.
Ralf Herbrich, Thore Graepel, and Klaus Obermayer.
2000. Large margin rank boundaries for ordinal re-
gression. In Advances in Large Margin Classifiers,
pages 115?132. MIT Press.
Thorsten Joachims. 1998. Making large-scale support
vector machine learning practical. In Advances in Ker-
nel Methods: Support Vector Machine. MIT Press.
A. Joshi and Y. Schabes. 1997. Tree-adjoining gram-
mars. In G. Rozenberg and A. Salomaa, editors, Hand-
book of Formal Languages, volume 3, pages 69 ? 124.
Springer.
W. Krauth and M. Mezard. 1987. Learning algorithms
with optimal stability in neural networks. Journal of
Physics A, 20:745?752.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In Proceedings of NAACL
2001.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional random fields: Probabilistic models for segmen-
tation and labeling sequence data. In Proceedings of
ICML.
Yaoyong Li, Hugo Zaragoza, Ralf Herbrich, John Shawe-
Taylor, and Jaz Kandola. 2002. The perceptron al-
gorithm with uneven margins. In Proceedings of the
International Conference of Machine Learning.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
John Platt. 1999. Probabilistic outputs for support vector
machines and comparisons to regularized likelihood
methods. In Advances in Large Margin Classifiers.
MIT Press.
Jesper Salomon, Simon King, and Miles Osborne. 2002.
Framewise phone classification using support vector
machines. In Proceedings of ICSLP 2002.
John Shawe-Taylor, Peter L. Bartlett, Robert C.
Williamson, and Martin Anthony. 1998. Structural
risk minimization over data-dependent hierarchies.
IEEE Trans. on Information Theory, 44(5):1926?1940.
A.J. Smola, P. Bartlett, B. Scho?lkopf, and C. Schuurmans.
2000. Introduction to large margin classifiers. In A.J.
Smola, P. Bartlett, B. Scho?lkopf, and C. Schuurmans,
editors, Advances in Large Margin Classifiers, pages
1?26. MIT Press.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
John Wiley and Sons, Inc.
Vladimir N. Vapnik. 1999. The Nature of Statistical
Learning Theory. Springer, 2 edition.
Using LTAG Based Features in Parse Reranking?
Libin Shen
Dept. of Computer & Info. Sci.
University of Pennsylvania
libin@cis.upenn.edu
Anoop Sarkar
School of Computing Science
Simon Fraser University
anoop@cs.sfu.ca
Aravind K. Joshi
Dept. of Computer & Info. Sci.
University of Pennsylvania
joshi@cis.upenn.edu
Abstract
We propose the use of Lexicalized Tree
Adjoining Grammar (LTAG) as a source
of features that are useful for reranking
the output of a statistical parser. In this
paper, we extend the notion of a tree ker-
nel over arbitrary sub-trees of the parse to
the derivation trees and derived trees pro-
vided by the LTAG formalism, and in ad-
dition, we extend the original definition
of the tree kernel, making it more lexi-
calized and more compact. We use LTAG
based features for the parse reranking task
and obtain labeled recall and precision of
89.7%/90.0% on WSJ section 23 of Penn
Treebank for sentences of length ? 100
words. Our results show that the use
of LTAG based tree kernel gives rise to
a 17% relative difference in f -score im-
provement over the use of a linear kernel
without LTAG based features.
1 Introduction
Recent work in statistical parsing has explored al-
ternatives to the use of (smoothed) maximum likeli-
hood estimation for parameters of the model. These
alternatives are distribution-free (Collins, 2001),
providing a discriminative method for resolving
parse ambiguity. Discriminative methods provide a
ranking between multiple choices for the most plau-
sible parse tree for a sentence, without assuming that
a particular distribution or stochastic process gener-
ated the alternative parses.
?We would like to thank Michael Collins for providing the
original n-best parsed data on which we ran our experiments
and the anonymous reviewers for their comments. The sec-
ond author is partially supported by NSERC, Canada (RGPIN:
264905).
Discriminative methods permit the use of feature
functions that can be used to condition on arbitrary
aspects of the input. This flexibility makes it possi-
ble to incorporate features of various of kinds. Fea-
tures can be defined on characters, words, part of
speech (POS) tags and context-free grammar (CFG)
rules, depending on the application to which the
model is applied.
Features defined on n-grams from the input are
the most commonly used for NLP applications.
Such n-grams can either be defined explicitly us-
ing some linguistic insight into the problem, or the
model can be used to search the entire space of n-
gram features using a kernel representation. One
example is the use of a polynomial kernel over se-
quences. However, to use all possible n-gram fea-
tures typically introduces too many noisy features,
which can result in lower accuracy. One way to
solve this problem is to use a kernel function that is
tailored for particular NLP applications, such as the
tree kernel (Collins and Duffy, 2001) for statistical
parsing.
In addition to n-gram features, more complex
high-level features are often exploited to obtain
higher accuracy, especially when discriminative
models are used for statistical parsing. For ex-
ample, all possible sub-trees can be used as fea-
tures (Collins and Duffy, 2002; Bod, 2003). How-
ever, most of the sub-trees are linguistically mean-
ingless, and are a source of noisy features thus limit-
ing efficiency and accuracy. An alternative to the use
of arbitrary sets of sub-trees is to use the set of ele-
mentary trees as defined in Lexicalized Tree Adjoin-
ing Grammar (LTAG) (Joshi and Schabes, 1997).
LTAG based features not only allow a more limited
and a linguistically more valid set of features over
sub-trees, they also provide the use of features that
use discontinuous sub-trees which are outside the
scope of previous tree kernel definitions using arbi-
trary sub-trees. In this paper, we use the LTAG based
features in the parse reranking problem (Collins,
2000; Collins and Duffy, 2002). We use the Sup-
port Vector Machine (SVM) (Vapnik, 1999) based
algorithm proposed in (Shen and Joshi, 2003) as the
reranker in this paper. We apply the tree kernel to
derivation trees of LTAG, and extract features from
derivation trees. Both the tree kernel and the linear
kernel on the richer feature set are used. Our exper-
iments show that the use of tree kernel on derivation
trees makes the notion of a tree kernel more power-
ful and more applicable.
2 Lexicalized Tree Adjoining Grammar
In this section, we give a brief introduction to the
Lexicalized Tree Adjoining Grammar (more details
can be found in (Joshi and Schabes, 1997)). In
LTAG, each word is associated with a set of elemen-
tary trees. Each elementary tree represents a possi-
ble tree structure for the word. There are two kinds
of elementary trees, initial trees and auxiliary trees.
Elementary trees can be combined through two op-
erations, substitution and adjunction. Substitution is
used to attach an initial tree, and adjunction is used
to attach an auxiliary tree. In addition to adjunction,
we also use sister adjunction as defined in the LTAG
statistical parser described in (Chiang, 2000).1 The
tree resulting from the combination of elementary
trees is is called a derived tree. The tree that records
the history of how a derived tree is built from the
elementary trees is called a derivation tree.2
We illustrate the LTAG formalism using an exam-
ple.
Example 1: Pierre Vinken will join the board as a
non-executive director.
The derived tree for Example 1 is shown in Fig. 1
(we omit the POS tags associated with each word to
save space), and Fig. 2 shows the elementary trees
for each word in the sentence. Fig. 3 is the deriva-
tion tree (the history of tree combinations). One of
1Adjunction is used in the case where both the root node and
the foot node appear in the Treebank tree. Sister adjunction is
used in generating modifier sub-trees as sisters to the head, e.g
in basal NPs.
2Each node ??n? in the derivation tree is an elementary tree
name ? along with the location n in the parent elementary tree
where ? is inserted. The location n is the Gorn tree address (see
Fig. 4).
S
NP
Pierre Vinken
VP
will VP
VP
join NP
the board
PP
as NP
a non-executive director
Figure 1: Derived tree (parse tree) for Example 1.
NP
Pierre
NP
Vinken
VP
will VP?
S
NP? VP
join NP?
NP
the
NP
board
VP
VP? PP
as NP?
NP
a
NP
non-executive
NP
director
?1: ?2: ?2: ?1:
?3: ?3: ?4: ?6: ?5: ?4:
Figure 2: Elementary trees for Example 1.
the properties of LTAG is that it factors recursion in
clause structure from the statement of linguistic con-
straints, thus making these constraints strictly local.
For example, in the derivation tree of Examples 1,
?1(join) and ?2(V inken) are directly connected
whether there is an auxiliary tree ?2(will) or not.
We will show how this property affects our redefined
tree kernel later in this paper. In our experiments in
this paper, we only use LTAG grammars where each
elementary tree is lexicalized by exactly one word
(terminal symbol) on the frontier.
3 Parse Reranking
In recent years, reranking techniques have been suc-
cessfully used in statistical parsers to rerank the out-
put of history-based models (Black et al, 1993). In
this paper, we will use the LTAG based features to
improve the performance of reranking. Our motiva-
tions for using LTAG based features for reranking
are the following:
? Unlike the generative model, it is trivial to in-
corporate features of various kinds in a rerank-
ing setting. Furthermore the nature of rerank-
ing makes it possible to use global features,
?1(join)??
?2(Vinken)?00?
?1(Pierre)?0?
?2(will)?01? ?3(board)?011?
?3(the)?0?
?4(as)?01?
?4(director)?011?
?5(non-executive)?0?
?6(a)?0?
Figure 3: Derivation tree: shows how the elementary
trees shown in Fig. 2 can be combined to provide an
analysis for the sentence in Example 1.
which allow us to combine features that are de-
fined on arbitrary sub-trees in the parse tree and
features defined on a derivation tree.
? Several hand-crafted and arbitrary features
have been exploited in the statistical parsing
task, especially when parsing the WSJ Penn
Treebank dataset where performance has been
finely tuned over the years. Showing a positive
contribution in this task will be a convincing
test for the use of LTAG based features.
? The parse reranking dataset is well established.
We use the dataset defined in (Collins, 2000).
In (Collins, 2000), two reranking algorithms were
proposed. One was based on Markov Random
Fields, and the other was based on the Boosting al-
gorithm. In both these models, the loss functions
were computed directly on the feature space. Fur-
thermore, a rich feature set was introduced that was
specifically selected by hand to target the limitations
of generative models in statistical parsing.
In (Collins and Duffy, 2002), the Voted Percep-
tron algorithm was used for parse reranking. The
S0
NP00? VP01
join010 NP011?
Figure 4: Example of how each node in an elemen-
tary tree has a unique node address using the Gorn
notation. 0 is the root with daughters 00, 01, and so
on recursively, e.g. first daughter 01 is 010.
VP
will VP
.
.
. PP
.
.
. NP
a
Figure 5: A sub-tree which is linguistically mean-
ingless.
tree kernel was used to compute the number of com-
mon sub-trees of two parse trees. The features used
by this tree kernel contains all the hand selected fea-
tures of (Collins, 2000). It is worth mentioning that
the f -scores reported in (Collins and Duffy, 2002)
are about 1% less than the results in (Collins, 2000).
In (Shen and Joshi, 2003), a SVM based rerank-
ing algorithm was proposed. In that paper, the no-
tion of preference kernels was introduced to solve
the reranking problem. Two distinct kernels, the tree
kernel and the linear kernel were used with prefer-
ence kernels.
4 Using LTAG Based Features
4.1 Motivation
While the tree kernel is an easy way to compute sim-
ilarity between two parse trees, it takes too many lin-
guistically meaningless sub-trees into consideration.
Let us consider the example sentence in Example
1. The parse tree, or derived tree, for this sentence
is shown in Fig. 1. Fig. 5 shows one of the lin-
guistically meaningless sub-trees. The number of
meaningless sub-trees is a misleading measure for
discriminating good parse trees from bad. Further-
more, the number of meaningless sub-trees is far
greater than the number of useful sub-trees. This
limits both efficiency and accuracy on the test data.
The use of unwanted sub-trees greatly increases the
hypothesis space of a learning machine, and thus de-
creases the expected accuracy on test data. In this
work, we consider the hypothesis that linguistically
meaningful sub-trees reveal correlations of interest
and therefore are useful in stochastic models.
We notice that each sub-tree of a derivation tree
is linguistically meaningful because it represents a
valid sub-derivation. We claim that derivation trees
provide a more accurate measure of similarity be-
tween two parses. This is one of the motivations
for applying tree kernels to derivation trees. Note
that the use of features on derivation trees is differ-
ent from the use of features on dependency graphs,
derivation trees include many complex patterns of
tree names and attachment sites and can represent
word to word dependencies that are not possible in
traditional dependency graphs.
For example, the derivation tree for Example 1
with and without optional modifiers such as ?4(as)
are minimally different. In contrast, in derived
(parse) trees, there is an extra VP node which
changes quite drastically the set of sub-trees with
and without the PP modifier. In addition, using only
sub-trees from the derived tree, we cannot repre-
sent a common sub-tree that contains only the words
Vinken and join since this would lead to a discontin-
uous sub-tree. However, LTAG based features can
represent such cases trivially.
The comparison between (Collins, 2000) and
(Collins and Duffy, 2002) in ?3 shows that it is hard
to add new features to improve performance. Our
hypothesis is that the LTAG based features provide
a novel set of abstract features that complement the
hand selected features from (Collins, 2000) and the
LTAG based features will help improve performance
in parse reranking.
4.2 Extracting Derivation Trees
Before we can use LTAG based features we need
to obtain an LTAG derivation tree for each parse
tree under consideration by the reranker. Our solu-
tion is to extract elementary trees and the derivation
tree simultaneously from the parse trees produced
by an n-best statistical parser. Our training and
test data consists of n-best output from the Collins
parser (see (Collins, 2000) for details on the dataset).
Since the Collins parser uses a lexicalized context-
free grammar as a basis for its statistical model, we
obtain parse trees that are of the type shown in Fig.
6. From this tree we extract elementary trees and
derivation trees by recursively traversing the spine
of the parse tree. The spine is the path from a non-
terminal lexicalized by a word to the terminal sym-
bol on the frontier equal to that word. Every sub-tree
rooted at a non-terminal lexicalized by a different
word is excised from the parse tree and recorded into
S(join)
NP-A(Vinken)
Pierre Vinken
VP(join)
will VP(join)
VP(join)
join NP-A(board)
the board
PP(as)
as NP-A(director)
a non-executive director
Figure 6: Sample output parse from the Collins
parser. Each non-terminal is lexicalized by the pars-
ing model. -A marks arguments recovered by the
parser.
the derivation tree as a substitution. Repeated non-
terminals on the spine (e.g. VP(join) . . . VP(join) in
Fig. 6) are excised along with the sub-trees hang-
ing off of it and recorded into the derivation tree as
an adjunction. The only other case is those sub-
trees rooted at non-terminals that are attached to
the spine. These sub-trees are excised and recorded
into the derivation tree as cases of sister adjunction.
Each sub-tree excised is recursively analyzed with
this method, split up into elementary trees and then
recorded into the derivation tree. The output of our
algorithm for the input parse tree in Fig. 6 is shown
in Fig. 2 and Fig. 3. Our algorithm is similar to
the derivation tree extraction explained in (Chiang,
2000), except we extract our LTAG from n-best sets
of parse trees, while in (Chiang, 2000) the LTAG is
extracted from the Penn Treebank.3 For other tech-
niques for LTAG grammar extraction see (Xia, 2001;
Chen and Vijay-Shanker, 2000).
4.3 Using Derivation Trees
In this paper, we have described two models to em-
ploy derivation trees. Model 1 uses tree kernels on
derivation trees. In order to make the tree kernel
more lexicalized, we extend the original definition
of the tree kernel, which we will describe below.
Model 2 abstracts features from derivation trees and
uses them with a linear kernel.
In Model 1, we combine the SVM results of the
tree kernel on derivation trees with the SVM results
given by a linear kernel based on features on the de-
rived trees.
3Also note that the path from the root node to the foot node
in auxiliary trees can be greater than one (for trees with S roots).
In Model 2, the vector space of the linear kernel
consists of both LTAG based features defined on the
derived trees and features defined on the derivation
tree. The following LTAG features have been used
in Model 2.
? Elementary tree. Each node in the derivation
tree is used as a feature.
? Bigram of parent and its child. Each pair
of parent elementary tree and child elementary
tree, as well as the type of operation (substi-
tution, adjunction or sister adjunction) and the
Gorn address on parent (see Fig. 4) is used as a
feature.
? Lexicalized elementary tree. Each elemen-
tary tree associated with its lexical item is used
as a feature.
? Lexicalized bigram. In Bigram of parent and
its child, each elementary tree is lexicalized
(we use closed class words, e.g. adj, adv, prep,
etc. but not noun or verb).
4.4 Lexicalized Tree Kernel
In (Collins and Duffy, 2001), the notion of a tree ker-
nel is introduced to compute the number of common
sub-trees of two parse trees. For two parse trees, p1
and p2, the tree kernel Tree(p1, p2) is defined as:
Tree(p1, p2) =
?
n1 in p1
n2 in p2
T (n1, n2) (1)
The recursive function T is defined as follows: If n1
and n2 have the same bracketing tag (e.g. S, NP, VP,
. . .) and the same number of children,
T (n1, n2) = ?
?
i
(1 + T (n1i, n2i)), (2)
where, nki is the ith child of the node nk, ? is a
weight coefficient used to control the importance of
large sub-trees and 0 < ? ? 1.
If n1 and n2 have the same bracketing tag but dif-
ferent number of children, T (n1, n2) = ?. If they
don?t have the same bracketing tag, T (n1, n2) = 0.
In (Collins and Duffy, 2002), lexical items are all
located at the leaf nodes of parse trees. Therefore
VP(join)
VP(join)
V(join) NP(board)
PP(as)
P(as) NP(director)
tree with root node n:
VP
VP
V NP
PP
P NP
ptn(n):
lex(n): (join, join, as)
Figure 7: A lexicalized sub-tree rooted at n and
its decomposition into a pattern, ptn(n) and corre-
sponding vector of lexical information, lex(n).
sub-trees that do not contain any leaf node are not
lexicalized. Furthermore, due to the introduction of
parameter ?, lexical information is almost ignored
for sub-trees whose root node is not close to the leaf
nodes, i.e. sub-trees rooted at S node.
In order to make the tree kernel more lexicalized,
we associate each node with a lexical item. For ex-
ample, Fig. 7 shows a lexicalized sub-tree and its
decomposition into features. As shown in Fig. 7 the
lexical information lex(t) extracted from the lexical-
ized tree consists of words from the root and its im-
mediate children. This is because we wish to ig-
nore irrelevant lexicalizations such as NP(board) in
Fig. 7.
A lexicalized sub-tree rooted on node n is split
into two parts. One is the pattern tree of n, ptn(n).
The other is the vector of lexical information of n,
lex(n), which contains the lexical items of the root
node and the children of the root.
For two tree nodes n1 and n2, the recursive func-
tion LT (n1, n2) used to compute the lexicalized tree
kernel is defined as follows.
LT (n1, n2) = (1 + Cnt(lex(n1), lex(n2)))
? T ?(ptn(n1), ptn(n2)), (3)
where T ? is the same as the original recursive func-
tion T defined in (2), except that T is defined on
parse tree nodes, while T ? is defined on patterns of
parse tree nodes. Cnt(x, y) counts the number of
common elements in vector x and y. For example,
Cnt((join, join, as), (join, join, in)) = 2, since
2 elements of the two vectors are the same.
It can be shown that the lexicalized tree kernel
counts the number of common sub-trees that meet
the following constraints.
? None or one node in the sub-tree is lexicalized
? The lexicalized node is the root node or a child
of the root, if applicable.
Therefore our new tree kernel is more lexicalized.
On the other hand, it immediately follows that the
lexicalized tree kernel is well-defined. It means that
we can embed the lexicalized tree kernel into a high
dimensional space. The proof is similar to the proof
for the tree kernel in (Collins and Duffy, 2001).
Another important advantage of the lexicalized
tree kernel is that it is more compressible. It is noted
in (Collins and Duffy, 2001) that training trees can
be combined by sharing sub-trees to speed up the
test. As far as the lexicalized tree kernel is con-
cerned, the pattern trees are more compressible be-
cause there is no lexical item at the leaf nodes of
pattern trees. Lexical information can be attached
to the nodes of the result pattern forest. In our ex-
periment, we select five parses from each sentence
in Collins? training data and represent these parses
with shared structure. The number of the nodes in
the pattern forest is only 1/7 of the total number of
the nodes the selected parse trees.
4.5 Tree Kernel for Derivation Trees
In order to apply the (lexicalized) tree kernel to
derivation trees, we need to make some modifica-
tions to the original recursive definition of the tree
kernel.
For derivation trees, the recursive function is trig-
gered if the two root nodes have the same non-
lexicalized elementary tree (sometimes called su-
pertag). Note that these two nodes will have the
same number of children which are initial trees (aux-
iliary trees are not counted). In comparison, the re-
cursive function in (2), T (n1, n2) is computed if and
only if n1 and n2 have the same bracketing tag and
they have the same number of children.
For each node, its children are attached with one
of the two distinct operations, substitution or adjunc-
tion. For substituted children, the computation of the
tree kernel is almost the same as that for CFG parse
tree. However, there is a problem with the adjoined
children. Let us first have a look at a sentence in
Penn Treebank.
Example 2: COMMERCIAL PAPER placed di-
rectly by General Motors Acceptance Corp.: 8.55%
30 to 44 days; 8.25% 45 to 59 days; 8.45% 60 to 89
days; 8% 90 to 119 days; 7.90% 120 to 149 days;
7.80% 150 to 179 days; 7.55% 180 to 270 days.
In this example, seven sub-trees of the same type
are sister adjoined to the same place of an initial tree.
So the number of common sub-trees increases dra-
matically if the tree kernel is applied on two similar
parses of this sentence. Experimental evidence indi-
cates that this is harmful to accuracy. Therefore, for
derivation trees, we are only interested in sub-trees
that contain at most 2 adjunction branches for each
node. The number of constrained common sub-trees
for the derivation tree kernel can be computed by
the recursive function DT over derivation tree nodes
n1, n2:
DT (n1, n2) = (1 + A1(n1, n2) + A2(n1, n2))
? T?(sub(n1), sub(n2)) (4)
where sub(nk) is the sub-tree of nk in which chil-
dren adjoined to the root of nk are pruned. T? is
similar to the original recursive function T defined
in (2), but it is defined on derivation tree nodes re-
cursively. A1 and A2 are used to count the number
of common sub-trees whose root nodes only contain
one or two adjunction children respectively.
A1(n1, n2) =
?
i,j
DT (a1i, a2j),
where, a1i is the ith adjunct of n1, and a2j is the jth
adjunct of n2. Similarly, we have:
A2(n1, n2) =
?
i<k,j<l
DT (a1i, a2j) ? DT (a1k, a2l)
The tree kernel for derivation trees is a well-defined
kernel function because we can easily define an em-
bedding space according to the definition of the new
tree kernel. By substituting DT for T ? in (3), we ob-
tain the lexicalized tree kernel for LTAG derivation
trees (using LT in (1)).
5 Experiments
As described above, we use the SVM based voting
algorithm (Shen and Joshi, 2003) in our reranking
experiments. We use preference kernels and pair-
wise parse trees in our reranking models.
We use the same data set as described in (Collins,
2000). Section 2-21 of the Penn WSJ Treebank are
used as training data, and section 23 is used for fi-
nal test. The training data contains around 40,000
sentences, each of which has 27 distinct parses on
average. Of the 40,000 training sentences, the first
36,000 are used to train SVMs. The remaining 4,000
sentences are used as development data.
Due to the computational complexity of SVM, we
have to divide training data into slices to speed up
training. Each slice contain two pairs of parses from
every sentence. Specifically, slice i contains pos-
itive samples ((p?k, pki),+1) and negative samples
((pki, p?k),?1), where p?k is the best parse for sen-
tence k, pki is the parse with the ith highest log-
likelihood in all the parses for sentence k and it is
not the best parse (Shen and Joshi, 2003). There are
about 60000 samples in each slice in average.
For the tree kernel SVMs of Model 1, we take
3 slices as a chunk, and train an SVM for each
chunk. Due to the limitation of computing resource,
we have only trained on 3 chunks. The results of
tree kernel SVMs are combined with simple com-
bination. Then the outcome is combined with the
result of the linear kernel SVMs trained on features
extracted from the derived trees which are reported
in (Shen and Joshi, 2003). For each parse, the num-
ber of the brackets in it and the log-likelihood given
by Collins? parser Model 2 are also used in the com-
putation of the score of a parse. For each parse p, its
score Sco(p) is defined as follows:
Sco(p) = MT (p) + ? ? ML(p) + ? ? l(p) + ? ? b(p),
where MT (p) is the output of the tree kernel SVMs,
ML(p) is the output of linear kernel SVMs, l(p) is
the log-likelihood of parse p, and b(p) is the num-
ber of brackets in parse p. We noticed that the SVM
systems prefers to give higher scores to the parses
with less brackets. As a result, the system has a high
precision but a low recall. Therefore, we take the
number of brackets, b(p), as a feature to make the
recall and precision balanced. The three weight pa-
rameters are tuned on the development data.
The results are shown in Table 1. With Model
1, we achieve LR/LP of 89.7%/90.0% on sentences
?40 Words (2245 sentences)
Model LR LP CBs 0 CBs 2 CBs
CO99 88.5% 88.7% 0.92 66.7% 87.1%
CO00 90.1% 90.4% 0.73 70.7% 89.6%
CD02 89.1% 89.4% 0.85 69.3% 88.2%
SJ 03 89.9% 90.3% 0.75 71.7% 89.4%
M1 90.2% 90.5% 0.72 72.3% 90.0%
M2 89.8% 90.3% 0.76 71.6% 89.6%
?100 Words (2416 sentences)
Model LR LP CBs 0 CBs 2 CBs
CO99 88.1% 88.3% 1.06 64.0% 85.1%
CO00 89.6% 89.9% 0.87 68.3% 87.7%
CD02 88.6% 88.9% 0.99 66.5% 86.3%
SJ 03 89.4% 89.8% 0.89 69.2% 87.6%
M1 89.7% 90.0% 0.86 70.0% 88.2%
M2 89.3% 89.8% 0.89 69.1% 87.7%
Table 1: Results on section 23 of the WSJ Tree-
bank. LR/LP = labeled recall/precision. CBs = av-
erage number of crossing brackets per sentence. 0
CBs, 2 CBs are the percentage of sentences with
0 or ? 2 crossing brackets respectively. CO99 =
(Collins, 1999) Model 2. CO00 = (Collins, 2000).
CD02 = (Collins and Duffy, 2002). SJ03 = linear
kernel of (Shen and Joshi, 2003). M1=Model 1.
M2=Model 2.
with ? 100 words. Our results show a 17% rel-
ative difference in f -score improvement over the
use of a linear kernel without LTAG based features
(Shen and Joshi, 2003). In addition, we also get
non-trivial improvement on the number of crossing
brackets. These results verify the benefit of using
LTAG based features and confirm the hypothesis that
LTAG based features provide a novel set of abstract
features that complement the hand selected features
from (Collins, 2000). Our results on Model 1 show
a 1% error reduction on the previous best reranking
result using the dataset reported in (Collins, 2000).
Also, Model 1 provides a 10% reduction in error
over (Collins and Duffy, 2002) where the features
from tree kernel were over arbitrary sub-trees.
For Model 2, we first train 22 SVMs on 22 dis-
tinct slices. Then we combine the results of individ-
ual SVMs with simple combination. However, the
overall performance does not improve. But we no-
tice that the use of LTAG based features gives rise to
0.874
0.875
0.876
0.877
0.878
0.879
0.88
0.881
0 5 10 15 20
ID of slices
without LTAG
with LTAG
Figure 8: Comparison of performance of individual
SVMs in Model 2: with and without LTAG based
features. X-axis stands for the ID of the slices on
which the SVMs are trained.Y-axis stands for the f -
score.
improvement on most of the single SVMs, as shown
in Fig. 8.
We think there are several reasons to account for
why our Model 2 doesn?t work as well for the full
task when compared with Model 1. Firstly, the train-
ing slice is not large enough. Local optimization on
each slice does not result in global optimization (as
seen in Fig. 8). Secondly, the LTAG based features
that we have used in the linear kernel in Model 2 are
not as useful as the tree kernel in Model 1.4 The last
reason is that we do not set the importance of LTAG
based features. One shortcoming of kernel methods
is that the coefficient of each feature must be set be-
fore the training (Herbrich, 2002). In our case, we
do not tune the coefficients for the LTAG based fea-
tures in Model 2.
6 Conclusions and Future Work
In this paper, we have proposed methods for using
LTAG based features in the parse reranking task.
The experimental results show that the use of LTAG
based features gives rise to improvement over al-
ready finely tuned results. We used LTAG based fea-
tures for the parse reranking task and obtain labeled
recall and precision of 89.7%/90.0% on WSJ sec-
tion 23 of Penn Treebank for sentences of length ?
100 words. Our results show that the use of LTAG
4In Model 1, we implicitly take every sub-tree of the deriva-
tion trees as a feature, but in Model 2, we only consider a small
set of sub-trees in a linear kernel.
based tree kernel gives rise to a 17% relative differ-
ence in f -score improvement over the use of a linear
kernel without LTAG based features. In future work,
we will use some light-weight machine learning al-
gorithms for which training is faster, such as vari-
ants of the Perceptron algorithm. This will allow us
to use larger training data chunks and take advan-
tage of global optimization in the search for relevant
features.
References
E. Black, F. Jelinek, J. Lafferty, Magerman D. M., R. Mercer,
and S. Roukos. 1993. Towards history-based grammars:
Using richer models for probabilistic parsing. In Proc. of the
ACL 1993.
R. Bod. 2003. An Efficient Implementation of a New DOP
Model. In Proc. of EACL 2003, Budapest.
J. Chen and K. Vijay-Shanker. 2000. Automated Extraction of
TAGs from the Penn Treebank. In Proc. of the 6th IWPT.
D. Chiang. 2000. Statistical Parsing with an Automatically-
Extracted Tree Adjoining Grammar. In Proc. of ACL-2000.
M. Collins and N. Duffy. 2001. Convolution kernels for natural
language. In Proc. of the 14th NIPS.
M. Collins and N. Duffy. 2002. New ranking algorithms for
parsing and tagging: Kernels over discrete structures, and
the voted perceptron. In Proc. of ACL 2002.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
M. Collins. 2000. Discriminative reranking for natural lan-
guage parsing. In Proc. of 7th ICML.
M. Collins. 2001. Parameter estimation for statistical parsing
models: Theory and practice of distribution-free methods.
In Proc. of IWPT 2001. Invited Talk at IWPT 2001.
R. Herbrich. 2002. Learning Kernel Classifiers: Theory and
Algorithms. MIT Press.
A. K. Joshi and Y. Schabes. 1997. Tree-adjoining grammars.
In G. Rozenberg and A. Salomaa, editors, Handbook of For-
mal Languages, volume 3, pages 69 ? 124. Springer.
L. Shen and A. K. Joshi. 2003. An SVM based voting algo-
rithm with application to parse reranking. In Proc. of CoNLL
2003.
V. N. Vapnik. 1999. The Nature of Statistical Learning Theory.
Springer, 2nd edition.
F. Xia. 2001. Investigating the Relationship between Gram-
mars and Treebanks for Natural Languages. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA.
Annotation and Data Mining of the Penn Discourse TreeBank
Rashmi Prasad
University of Pennsylvania
Philadelphia, PA 19104 USA
rjprasad@linc.cis.upenn.edu
Eleni Miltsakaki
University of Pennsylvania
Philadelphia, PA 19104 USA
elenimi@linc.cis.upenn.edu
Aravind Joshi
University of Pennsylvania
Philadelphia, PA 19104 USA
joshi@linc.cis.upenn.edu
Bonnie Webber
University of Edinburgh
Edinburgh, EH8 9LW Scotland
bonnie@inf.ed.ac.uk
Abstract
The Penn Discourse TreeBank (PDTB) is a new re-
source built on top of the Penn Wall Street Journal
corpus, in which discourse connectives are anno-
tated along with their arguments. Its use of stand-
off annotation allows integration with a stand-off
version of the Penn TreeBank (syntactic structure)
and PropBank (verbs and their arguments), which
adds value for both linguistic discovery and dis-
course modeling. Here we describe the PDTB and
some experiments in linguistic discovery based on
the PDTB alone, as well as on the linked PTB and
PDTB corpora.
1 Introduction
Large scale annotated corpora such as the Penn
TreeBank (Marcus et al, 1993) have played a cen-
tral role in speech and natural language research.
However, with the demand for more powerful NLP
applications comes a need for greater richness in
annotation ? hence, the development of PropBank
(Kingsbury and Palmer, 2002), which adds basic se-
mantics to the PTB in the form of verb predicate-
argument annotation and eventually similar annota-
tion of nominalizations. We have been developing
yet another annotation layer above these both. The
Penn Discourse TreeBank (PDTB) adds low-level
discourse structure and semantics through the anno-
tation of discourse connectives and their arguments,
using connective-specific semantic role labels. With
this added knowledge, the PDTB (together with the
PTB and PropBank) should support more in-depth
NLP research and more powerful applications.
Work on the PDTB is grounded in a lexical-
ized approach to discourse ? DLTAG (Webber and
Joshi, 1998; Webber et al, 1999a; Webber et al,
2000; Webber et al, 2003). Here, low-level dis-
course structure and semantics are taken to re-
sult (in part) from composing elementary predicate-
argument relations whose predicates come mainly
from discourse connectives1 and whose arguments
1Despite this, we have deliberately adopted a policy of hav-
come from units of discourse ? clausal, sentential
or multi-sentential units. The PDTB therefore dif-
fers from the RST-annotated corpus (Carlson et al,
2003) which starts with (abstract) rhetorical rela-
tions (Mann and Thompson, 1988) and annotates a
subset of the Penn WSJ corpus with those relations
that can be taken to hold between (primarily) pairs
of discourse spans identified in the corpus.
The current paper focuses on what can be dis-
covered through analyzing PDTB annotation, both
on its own and together with the Penn TreeBank.
Section 2 of the paper briefly reviews the theo-
retical background of the project, its current state,
the guidelines given to annotators, the annotation
tool they used (WordFreak), and the extent of inter-
annotator agreement. Section 3 shows how we have
used PDTB annotation, along with the PTB, to ex-
tract several features pertaining to discourse con-
nectives and their arguments, and discusses the rel-
evance of these features for NLP research and ap-
plications. Section 4 concludes with the summary.
2 Project overview
2.1 Theoretical background
The PDTB project builds on basic ideas presented
in Webber and Joshi (1998), Webber et al (1999b)
and Webber et al (2003) ? that connectives are
discourse-level predicates which project predicate-
argument structure on a par with verbs at the sen-
tence level. Webber and Joshi (1998) propose a
tree-adjoining grammar for discourse (DLTAG) in
which compositional aspects of discourse meaning
are formally defined, thus teasing apart composi-
tional from non-compositional layers of meaning.
In this framework, connectives are grouped into nat-
ural classes depending on the structure that they
project at the discourse level. Subordinate and coor-
dinating conjunctions, for example, require two ar-
ing the annotations independent of the DLTAG structural de-
scriptions for two reasons: (1) to make the annotated cor-
pus useful to researchers working in different frameworks and
(2) to simplify the annotators? task, thereby increasing inter-
annotator reliability.
guments that can be identified structurally from ad-
jacent units of discourse. What Webber et al (2003)
call anaphoric connectives (discourse adverbials,
such as otherwise, instead, furthermore, etc.) also
require two arguments ? one derived structurally,
and the other derived anaphorically from the pre-
ceding discourse. The crucial contribution of this
framework to the design of PDTB is what can be
seen as a bottom-up approach to discourse structure.
Specifically, instead of appealing to an abstract (and
arbitrary) set of discourse relations whose identifi-
cation may confound multiple sources of discourse
meaning, we start with the annotation of discourse
connectives and their arguments, thus exposing a
clearly defined level of discourse representation.
2.2 Project description
The PTDB project began in November 2002. The
first phase, including pilot annotations and prelim-
inary development of guidelines, was completed in
May 2003, and we expect to release the PDTB by
November 2005. Intermediate versions of the an-
notated corpus will be made available for feedback
from the community.
The PDTB corpus will include annotations of
four types of connectives: subordinating and co-
ordinating conjunctions, adverbial connectives and
implicit connectives. The final number of annota-
tions will amount to approximately 30K: 20K anno-
tations of the 250 types explicit connectives identi-
fied in the corpus and 10K annotations of implicit
connectives. The final version of the corpus will
also characterize the semantic role of each argu-
ment.
To date, we have annotated 10 explicit connec-
tives (therefore, as a result, instead, otherwise, nev-
ertheless, because, although, even though, when, so
that), amounting to a total of 2717 annotations, as
well as 386 tokens of implicit connectives. Anno-
tations have been performed by two to four annota-
tors.
2.3 Annotation guidelines
The annotation guidelines for PDTB have
been revised considerably since the pilot
phase of the project in May 2003. The cur-
rent version of the guidelines is available at
http://www.cis.upenn.edu/   pdtb. Below
we outline basic points from the guidelines.
What counts as a discourse connective? We
count as discourse connectives (1) all subordinat-
ing and coordinating conjunctions, (2) all discourse
adverbials, and (3) all inter-sentential implicit con-
nectives. Discourse adverbials include only those
adverbials which convey relationships between two
abstract objects such as events, states, propositions,
etc. (Asher, 1993). For instance, in Example 1, as
a result conveys a cause-effect relation between the
event of limiting the size of industries and that of
industries operating out of small, expensive, and in-
efficient units. In contrast, the semantic interpreta-
tion of the clausal adverbial strangely in Example 2
only requires a single event/state which it classifies
in the set of strange events/states.2
(1) [In the past, the socialist policies of the govern-
ment strictly limited the size of new steel mills,
petrochemical plants, car factories and other in-
dustrial concerns to conserve resources and re-
strict the profits businessmen could make]. As
a result, [industry operated out of small, expen-
sive, highly inefficient industrial units].
(2) Strangely, conventional wisdom inside the Belt-
way regards these transfer payments as ?uncon-
trollable? or ?nondiscretionary.?
Implicit connectives are taken to occur between
adjacent sentences not related by any explicit con-
nective. They are annotated with whatever explicit
connective the annotator feels could be inserted,
with the original meaning retained. Assessment of
inter-annotator agreement groups these annotations
into five coarse classes (Miltsakaki et al, 2004).
Currently, we are not annotating implicit connec-
tives intra-sententially (such as between a main
clause and a free adjunct) or across paragraphs.
What counts as a legal argument? The sim-
plest argument to a connective is what we take to
be the minimum unit of discourse. Because we
take discourse relations to hold between abstract
objects, we require that an argument contain at least
one clause-level predication (usually a verb ? tensed
or untensed), though it may span as much as a se-
quence of clauses or sentences. The two exceptions
are nominal phrases that express an event or a state,
and discourse deictics that denote an abstract ob-
ject.
What we describe to annotators as arguments to
discourse connectives are actually the textual span
from which the argument is derived (Webber et al,
1999a; Webber et al, 2003). This is especially clear
in the case of the first argument of instead in (3),
which does not actually include the negation, al-
though it is part of the selected text.3
2For a more detailed discussion of how discourse adver-
bials can be distinguished from clausal adverbials, see Forbes
(2003).
3For a corpus-based study of the arguments of instead, see
(Miltsakaki et al, 2003).
(3) [No price for the new shares has been set]. In-
stead, [the companies will leave it up to the mar-
ketplace to decide].
How far does an argument extend? One par-
ticularly significant addition to the guidelines came
as a result of differences among annotators as to
how large a span constituted the argument of a con-
nective. During pilot annotations, annotators used
three annotation tags: CONN for the connective
and ARG1 and ARG2 for the two arguments. To
this set, we have added two optional tags, SUP1
and SUP2 (supplementary), for cases when the an-
notator wants to mark textual spans s/he considers
to be useful, supplementary information for the in-
terpretation of an argument. Examples (4) and (5)
demonstrate its use. The arguments are shown in
square brackets, while spans providing supplemen-
tary information are shown in parentheses.4
(4) Although [started in 1965], [Wedtech didn?t re-
ally get rolling until 1975] (when Mr. Neuberger
discovered the Federal Government?s Section 8
minority business program).
(5) Because [mutual fund trades don?t take effect un-
til the market close] (? in this case, at 4 p.m. ?)
[these shareholders effectively stayed put].
2.4 Inter-Annotation Reliability
An extensive discussion of inter-annotator reliabil-
ity in the PDTB is presented in (Miltsakaki et al,
2004). The three things that are relevant to the dis-
cussion here of using the PDTB for linguistic dis-
covery are (1) the agreement criterion, (2) the level
of inter-annotator agreement, and (3) the types of
inter-annotator variation.
With respect to agreement, we did not use the
kappa statistic (Siegel and Castellan, 1988) because
it requires the data tokens to be classified into dis-
crete categories and PDTB annotation involves se-
lecting a span of text whose length is not prescribed
a priori.5 Instead of kappa, we assessed inter-
annotator agreement using an exact match crite-
rion: for any ARG1 or ARG2 token, agreement was
recorded as 1 when both annotators made identical
4SUP annotations have not been used in the current
experiments.
5Carlson et al (2003) avoid this by using two sets of cat-
egories: one set in which there is a separate category for each
span that could constitute an elementary discourse unit, and one
set in which there is only a separate category for each span that
at least one annotator has selected. Because the arguments of
connectives tend to be longer and hence more variable than the
elementary spans used in the RST-corpus, we do not see any
gain from introducing the first set of categories, and the second
set is equivalent to our exact match criterion.
textual selections for the annotation and 0 when the
annotators made non-identical selections.
Treating ARG1 and ARG2 annotations as inde-
pendent tokens for assessment, the total number of
inter-annotator judgments assessed for explicit con-
nectives was twice the number of connective tokens,
i.e, 5434. In this measure, we achieved a high-level
of agreement on the arguments to subordinate con-
junctions (92.4%), while lower agreement on ad-
verbials (71.8%).6 This difference between the two
types is not surprising, since locating the anaphoric
(ARG1) argument of adverbial connectives is be-
lieved to be a harder task than that of locating the
arguments of subordinating conjunctions. For ex-
ample, the anaphoric argument of the adverbial con-
nectives may be located in some non-adjacent span
of text, even several paragraphs away.
A detailed analysis of inter-annotator variation
shows that most of the disagreements (79%) in-
volved Partial Overlap ? that is, text that is com-
mon to what is selected separately by each annota-
tor. Partial overlap subsumes categories such as (a)
higher verb, where one of the annotators included
some extra clausal material that contained a higher
governing predicate, (b) dependent clause, where
one of the annotators included extra clausal mate-
rial which was syntactically dependent on the clause
selected by both, and (c) parenthetical, where one
of the annotators included text that occurred in the
middle of the other annotator?s selection. Example 6
illustrates a case of higher verb disagreement.
(6) a. [he knew the RDF was neither rapid nor de-
ployable nor a force] ? even though [it cost
$8 billion or $10 billion a year].
b. he knew [the RDF was neither rapid nor de-
ployable nor a force] ? even though [it cost
$8 billion or $10 billion a year].
The partial overlap disagreements are important
with respect to the experiments described in the next
section, because most of this variation turns out to
be irrelevant to the experiments. We will elaborate
on this further in the next section.
3 Data Mining
PDTB annotation indicates two things: the argu-
ments of each explicit discourse connective and the
lexical tokens that actually play a role as discourse
connectives. It should be clear that the former
6In Miltsakaki et al (2004), we have reported on the anno-
tation of implicit connectives as well. We achieved 72% agree-
ment on the use of explicit expressions in place of the implicit
connectives. More details on the implicit connective annotation
can be found in this work.
cannot be derived automatically from existing re-
sources, since determining the size and location of
the arguments is not simply a matter of sentential
syntax or verb predicate argument relations. But
the latter is also a non-trivial feature because every
lexical item that functions as a discourse connective
also has a range of other functions. While some of
these functions correlate with POS-tags other than
those used in annotating connectives, the PTB POS-
tags themselves cannot always be reliably distin-
guished, given inconsistencies in how the lexical
items are analyzed.
We believe that the PDTB annotation can con-
tribute to a range of linguistic discovery and lan-
guage modeling tasks, such as
 providing empirical evidence for the DLTAG
claim that discourse adverbials get one argu-
ment anaphorically, while structural connec-
tives such as conjunctions establish relations
between adjacent units of text (Creswell et al,
2002).
 acquiring common usage patterns of connec-
tives and identifying their dependencies, in or-
der to support ?natural? choices in Natural
Language Generation (di Eugenio et al, 1997;
Moser and Moore, 1995; Williams and Reiter,
2003).
 developing decision procedures for resolving
and interpreting discourse adverbials (Milt-
sakaki et al, 2003) which can be built on top of
discourse parsing systems (Forbes et al, 2003).
 developing ?word sense disambiguation? pro-
cedures for distinguishing among different
senses of a connective and hence interpret-
ing connectives correctly (e.g., distinguishing
between temporal and explanatory since, be-
tween hypothetical and counterfactual if, be-
tween epistemic and semantic because, etc.)
 providing empirical evidence for theories of
anaphoric phenomena such as verb phrase el-
lipsis that see them as sensitive to the type of
discourse relation in which they are expressed
(Hardt and Romero, 2002; Kehler, 2002).
The value of carrying out such studies using a sin-
gle corpus with multiple layers of annotation is that
relationships between phenomena are clearer. (The
downside is focusing on a single genre ? newspa-
per text ? and a particular ?house style? ? that of
the Wall Street Journal. However, developing the
PDTB may help facilitate the production of more
such corpora, through an initial pass of automatic
annotation, followed by manual correction, much
as was done in developing the PTB (Marcus et al,
1993).)
Here we present some preliminary experiments
we have carried out on the current version of the
PDTB. We automatically extracted features asso-
ciated with discourse connectives and their argu-
ments, both from the PDTB annotation alone as well
as from the integrated annotation of the PDTB and
PTB. The findings reveal novel patterns regarding
the location and size of the arguments of discourse
connectives and suggest additional experiments.
The multi-layered annotations for PDTB, PTB
(and soon to be available PropBank) are rendered in
XML within a ?stand-off? annotation architecture
in which multiple (independently conducted) anno-
tations refer to the same primary document. Word-
Freak directly renders the PDTB annotations in the
stand-off XML representation, but for the syntactic
layer, the PTB phrase structure constituent annota-
tions had to first be converted to the XML stand-off
representation.7
For preparing the connective tokens for data min-
ing, we started with the 2717 annotations for the
10 explicit connectives reported in Section 2.2 and
extracted those tokens on which we achieved full
?exact match? agreement as well as ?partial over-
lap? agreement on both the arguments (cf. Sec-
tion 2.4). We felt justified in combining both sets
because ?partial overlap? disagreements, which oc-
curred mostly within sentences, did not make any
overall difference to the features that were extracted.
The total number of tokens we obtained from this
was 2688. 51 tokens on this set had to be thrown out
since the official release of the Penn TreeBank did
not have the corresponding syntactic annotations for
these tokens.8 From the remaining 2637 tokens, we
extracted two sets of features, one for adverbials
(229 tokens) and the other for subordinating con-
junctions (2408 tokens).
For the adverbials, we wanted to determine
whether the results reported in earlier work
(Creswell et al, 2002) held up. Among other
things, this work examined whether (1) anaphoric
arguments could be reliably annotated, to facili-
tate the development of robust anaphora resolu-
tion algorithms, and (2) there were differences be-
7Thanks to Jeremy Lacivita for implementing the represen-
tation of PTB in stand-off XML form. The stand-off represen-
tation of PTB will be released together with the PDTB corpus.
8Researchers who are currently conducting or are planning
to conduct multi-layered annotations or experiments with the
Penn TreeBank should be aware that the official release con-
tains more source and PoS-tagged files than the parsed files.
Future annotations of the PDTB will only be performed on texts
that are parsed.
tween the type, size and location of the arguments
of anaphoric (adverbial) connectives and those of
structural connectives.
The high inter-annotator agreement reported in
this earlier study has now been confirmed by the
PDTB annotation (cf. Section 2.4). As for the other,
we automatically extracted some of the same fea-
tures that were hand-annotated in Creswell et al
(2002) to determine the distribution of these con-
nectives with respect to their position (POS) and
the size and location (LOC) of their anaphoric argu-
ments. These features are further described below:
POS: pertains to the position of the connective in
its host argument, i.e., the argument in which it oc-
curs.9 POS can take three defined values: INIT for
argument-initial position (Examples 7-9), MED for
argument-medial position (Examples 10-11), and
FINAL for argument-final position (Examples 12
and 13). Note that the host argument of the con-
nective is a sentence in Example 8 and 9, a VP con-
junct in Example 7, a free adjunct in Example 10,
the main clause of a sentence in Example 11, a sub-
ordinate clause in Example 12, and finally, the first
of the two coordinated sentences in Example 13.
LOC: pertains to the size and location of the
anaphoric argument of the connective. LOC can
take four defined values: SS for when the anaphoric
argument occurs in the same sentence as the con-
nective (Examples 7, 10 and 11), PS for when the
argument occurs in the immediately previous sen-
tence (Examples 12 and 13), PP for when the argu-
ment occurs in the immediately preceding sequence
of sentences (Example 8), and NC for when the ar-
gument occurs in some non-contiguous sentence(s)
(Example 9). A sentence is defined as minimally
a main clause and all of its attached subordinate
clauses, if any. Coordinated main clauses, by this
definition, are treated as separate sentences. Note
that according to the definition of the LOC feature,
the anaphoric argument may constitute the entire
sentence(s), as in Examples 8, 9 and 13, or it may be
part of the sentence(s), as in Examples 7 and 10-12.
An important aspect of the LOC feature is that
it involved the multi-layering of PDTB and PTB,
since the PDTB itself contains no information about
syntactic constituency or even sentence boundaries.
For deriving the LOC feature values, we needed in-
formation not only about the sentence boundaries
of texts, but also about coordinated clause bound-
aries, which requires accessing sentence-internal
constituents.
9We achieved 94.1% agreement on the host argument
(ARG2) annotations.
(7) INIT-SS: Rep. John LaFalce (D., N.Y.) said Mr.
Johnson refused [to testify jointly with Mr. Mul-
ford] and instead [asked to appear after the Trea-
sury official had completed his testimony].
(8) INIT-PP: [But Mr. Treybig questions whether
that will be enough to stop Tandem?s first main-
frame from taking on some of the functions that
large organizations previously sought from Big
Blue?s machines. ?The answer isn?t price re-
ductions, but new systems,? he said]. Never-
theless, [Tandem faces a variety of challenges,
the biggest being that customers generally view
the company?s computers as complementary to
IBM?s mainframes].
(9) INIT-NC: [For years, costume jewelry makers
fought a losing battle]. Jewelry displays in de-
partment stores were often cluttered and unin-
spired. And the merchandise was, well, fake.
As a result, [marketers of faux gems steadily lost
space in department stores to more fashionable
rivals ? cosmetics makers].
(10) MED-SS: Investors usually don?t want [to take
physical delivery of a contract], [preferring in-
stead to profit from its price swings and then end
any obligation to take delivery or make delivery
as it nears expiration].
(11) MED-SS: Although [bond prices weren?t as
volatile on Tuesday trading as stock prices],
[traders nevertheless said action also was much
slower yesterday in the Treasury market].
(12) FIN-PS: Buyers can look forward to double-
digit annual returns if [they are right]. But they
will have disappointing returns or even losses if
[interest rates rise] instead.
(13) FIN-PS: [Tons of delectably rotting potatoes,
barley and wheat will fill damp barns across the
land as thousands of farmers turn the state?s buy-
ers away]. [Many a piglet won?t be born] as a re-
sult, and many a ham will never hang in a butcher
shop.
The distribution of the POS feature values across
the different connectives, given in Table 1, shows
that the connectives in this set occurred predomi-
nantly in the initial position of their host argument.
The question of whether or not these different po-
sitions correlate with any aspect of the informa-
tion structure of the arguments (Forbes et al, 2003;
Kruijff-Korbayova? and Webber, 2001) is, however,
an open one and will need to be explored further
with the PDTB annotations.
INIT MED FIN TOTAL
201 (87.8%) 13 (5.7%) 15 (6.5%) 229
Table 1: Distribution of the Position (POS) of Dis-
course Adverbials
CONN SS PS PP NC Total
nevertheless 3 (9.7%) 17 (54.8%) 3 (9.7%) 8 (25.8%) 31
otherwise 2 (11.1%) 14 (77.8%) 1 (5.6%) 1 (5.6%) 18
as a result 3 (4.8%) 44 (69.8%) 5 (7.9%) 12 (19%) 63
therefore 11 (55%) 7 (35%) 1 (5%) 1 (5%) 20
instead 22 (22.7%) 62 (63.9%) 2 (2.1%) 11 (11.3%) 97
TOTAL 41 (17.9%) 144 (62.9%) 12 (5.2%) 33 (14.4%) 229
Table 2: Distribution for Location (LOC) of Anaphoric Argument of Adverbial Connectives
The distribution of the LOC values across the dif-
ferent connectives is shown in Table 2. We first look
at all the connectives taken together (i.e., the final
TOTAL row) and focus on differences in LOC and
what such differences suggest.
The first thing that is evident from the TOTAL
row in Table 2 is the significant proportion of ARG1
tokens that occur in a position non-adjacent to the
discourse adverbial (NC = 14.4%). This accords
with the results in (Creswell et al, 2002), in terms
of providing evidence that discourse adverbials (un-
like structural connectives) are not getting both their
arguments from structurally defined positions.
The second point that is evident from the TOTAL
row is the significant proportion of ARG1 tokens
in SS location. This includes instances of ARG1
in complement clauses (Example 7), subordinate
clauses (Example 11), relative clauses (both restric-
tive and non-restrictive, as in Example 14), pre-
ceding VP conjuncts (Example 15), and from main
clauses, where the adverbial is attached to a free ad-
junct, as in Example 16.
(14) [  The British pound], [pressured by last week?s
resignations of key Thatcher administration of-
ficials], nevertheless [  rose Monday to $1.5820
from Friday?s $1.5795].10
(15) Appealing to a young audience, [he scraps an
old reference to Ozzie and Harriet] and instead
[quotes the Grateful Dead].
(16) [The transmogrified brokers never let the C-word
cross their lips], instead [stressing such terms as
?safe,? ?insured? and ?guaranteed?].
While one might want to argue that the latter is
no different from adjacent full clauses and hence
should be treated the same as a location in the pre-
vious sentence (i.e., LOC=PS), the other SS cases
provide additional evidence for an anaphoric anal-
ysis of these discourse adverbials since there al-
ready exists a separate structural relation in each
case. Furthermore, in Example 7, the arguments of
the conjunction and, though not yet addressed by
our annotators, differ from the arguments of instead.
10The subscripts on the bracketed spans in this example indi-
cate discontinuous parts of the host argument of nevertheless.
Any attempt to treat instead as a structural connec-
tive will produce a syntactic analysis with crossing
branches ? a source of both theoretical and practical
(parsing) problems (Forbes et al, 2003).
Turning now to the individual analysis of adver-
bials, Table 2 shows that the 4 connectives other
than therefore pattern rather similarly with respect
to the location of the anaphoric argument (SS,
PS, PP, NC). All of them except therefore have
their antecedent predominantly in the previous sen-
tence (between 54.8% and 77.8%). The question
is whether the difference in how therefore patterns
? i.e., drawing its antecedent 55% of the time from
the same sentence ? is simply a consequence of hav-
ing such few data points (i.e., only 20) or a matter of
?house style? (with all the examples from the Wall
Street Journal) or a difference that is theoretically
motivated. If the answer lies in house style or the-
ory, then it is relevant to work in natural language
generation. Further annotation and analysis of ad-
verbials and their arguments in the PDTB will pro-
vide more information as to this puzzle.
At the start of this section, we indicated five dif-
ferent areas in which PDTB annotation could con-
tribute to linguistic discovery and language model-
ing. This data mining experiment illustrates the first
three, as well as providing information relevant to
further development of discourse parsing systems
and natural language generation systems. For fu-
ture work, we intend to explore further the extrac-
tion and study of other features related to discourse
adverbials. Two features that we are currently work-
ing to extract automatically pertain to (a) the co-
occurrence of discourse adverbials with other con-
nectives in the host argument, and (b) the syntac-
tic type and depth of the anaphoric arguments, such
as whether the argument was a finite or non-finite
complement clause, a relative clause, or a finite or
non-finite subordinate clause etc.
For the subordinating conjunctions (Table 3), we
extracted features pertaining to the relative position
of the two arguments of the conjunction. Subordi-
nating conjunctions often take their arguments in
the same sentence with the subordinate clause as
one argument and the main clause as its other ar-
gument. However, the subordinate clause can either
occur to the right of the main clause, i.e., postposed,
as in Example 17, or it can occur preposed, i.e., be-
fore the main clause, as in Example 18.
(17) ARG1-ARG2: But Sen. McCain says [Mr.
Keating broke off their friendship abruptly in
1987], because [the senator refused to press the
thrift executive?s case as vigorously as Mr. Keat-
ing wanted].
(18) ARG2-ARG1: Because [Swiss and EC insurers
are widely present on each other?s markets], [the
accord isn?t expected to substantially increase
near-term competition].
The distribution of the relative position of the
arguments of these connectives, given in Table 3,
shows significant differences across the connec-
tives.
CONN ARG1-ARG2 ARG2-ARG1 Total
when 545 (54%) 465 (46%) 1010
because 822 (90%) 93 (10%) 915
even though 77 (75%) 26 (25%) 103
although 129 (37%) 218 (63%) 347
so that 33 (100%) 0 (0%) 33
Total 1606 (67%) 802 (33%) 2408
Table 3: Distribution for Argument order for Subor-
dinating Conjunctions
There are a few interesting things to note here.
First, even if one considers only the four subordi-
nating conjunctions with  100 tokens, no two of
them pattern in the same way.
Second, with when, the almost equal distribution
of preposed and postposed tokens suggests either
free variation of the two patterns or different uses
of the two patterns, with each use favoring a differ-
ent pattern. The latter would accord with a theo-
retical distinction that has been made between post-
posed when expressing a purely temporal relation
between the two clauses, and preposed when ex-
pressing a contingent relation between them (Moens
and Steedman, 1988). Integrated evidence from the
PTB and PropBank may help distinguish the two
possibilities.
Third, there is a striking contrast between the pat-
terning of although and even though, especially if
one assumes that even though (like even when, even
after, even if, etc.) involves application of the topi-
calizer even to the subordinate clause, just as it can
apply to other constituents. Further annotation and
analysis of the PDTB will reveal whether all subor-
dinating conjunctions that co-occur with even pat-
tern like even though, or whether this is specific to
the concessive.
Finally, when Williams and Reiter (2003) exam-
ined 342 texts from the RST annotation of the Penn
TreeBank corpus (Carlson et al, 2003), they re-
ported that 77% of the instances of concessive re-
lations that they examined appeared in the order
ARG2-ARG1. (The eleven instances of although
that they examined and the three instances of even
though appeared in concessive relations, along with
instances of but, despite, however, etc.) If we were
to collapse together all instances of although and
even though annotated in the PDTB (totalling 450),
we would find that 46% (206) patterned as ARG1-
ARG2, and 54% of them (244) patterned as ARG2-
ARG1. This might lead us to draw a similar con-
clusion to Williams and Reiter (2003). But it would
also disguise the fact noted above that although and
even though pattern oppositely to one another. This
suggests (1) that making the feature extraction pro-
cedure specific to particular connectives, as in the
PDTB, will reveal distributional patterns that are
lost when more abstract relations are the focus of the
annotation, and (2) that a larger set of annotated to-
kens can show more reliable distributional patterns.
In sum, data mining of PDTB with respect to sub-
ordinating conjunctions has shown radically differ-
ent distribution patterns regarding the relative po-
sition of the arguments. Some of these have con-
firmed and strengthened previous theoretical claims
and some have suggested new and promising re-
search directions. Further work in this area will also
be extremely relevant for NLG sentence planning
components employing discourse relations (Walker
et al (2003), Stent et al (2004), among others),
where the sentence planner needs to make decisions
regarding cue placement. Finally, while our ap-
proach is ?syntactic?, with the distribution of the
connectives and their arguments being explored in
terms of whether they are subordinating conjunc-
tions, coordinating conjunctions, or adverbial con-
nectives, one can also explore the patterning of
connectives in terms of semantic categories, once
their semantic role annotation is complete (cf. Sec-
tion 2.2). The latter could be especially interesting
to cross-linguistic studies of discourse, as well as
to applications such as multilingual generation and
MT are envisaged.11
4 Summary
In this paper we have presented the Penn Dis-
course TreeBank (PDTB), a large-scale discourse-
level annotated corpus that is being developed to-
wards the creation of a multi-layered annotated cor-
pus, integrating the Penn TreeBank, PropBank and
11We thank an anonymous reviewer for pointing this out.
the PDTB. The PDTB encodes low-level discourse
structure information, marking discourse connec-
tives as indicators of discourse relations, and their
arguments. We have reported high inter-annotator
agreement for the PDTB annotation. Our data min-
ing experience and preliminary results show that the
multi-layered corpora is a rich source of information
that can be exploited towards the development of
powerful and efficient natural language understand-
ing and generation systems as well as towards large-
scale corpus-based research.
Acknowledgments
We are very grateful to Tom Morton and Jeremy
Lacivita for the development and modification of
the WordFreak annotation tool. Special thanks to
Jeremy for providing continuous technical support.
Thanks are also due to our annotators, Cassandre
Creswell, Driya Amandita, John Laury, Emily Paw-
ley, Alan Lee, Alex Derenzy and Steve Pettington.
References
Nicholas Asher. 1993. Reference to Abstract Objects in
Discourse. Kluwer Academic Publishers.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2003. Building a Discourse-Tagged
Corpus in the Framework of Rhetorical Structure
Theory. In Jan van Kuppevelt and Ronnie Smith, edi-
tors, Current Directions in Discourse and Dialogue.
Kluwer Academic Publishers.
Cassandre Creswell, Katherine Forbes, Eleni Miltsakaki,
Rashmi Prasad, Aravind Joshi, and Bonnie Webber.
2002. The Discourse Anaphoric Properties of Con-
nectives. In Proceedings of DAARC2002. Edic?o?es
Colibri.
Barbara di Eugenio, Johanna D. Moore, and Massimo
Paolucci. 1997. Learning Features that Predict Cue
Usage. In Proceedings of ACL/EACL 97.
Kate Forbes, Eleni Miltsakaki, Rashmi Prasad, Anoop
Sarkar, Aravind Joshi, and Bonnie Webber. 2003. D-
LTAG System: Discourse Parsing with a Lexicalized
Tree-Adjoining Grammar. Journal of Logic, Lan-
guage and Information, 12(3):261?279.
Kate Forbes. 2003. Discourse Semantics of S-Modifying
Adverbials. Ph.D. thesis, Department of Linguistics,
University of Pennsylvania.
Dan Hardt and Maribel Romero. 2002. Ellipsis and the
Structure of Discourse. In Proceedings of Sinn und
Bedeutung VI.
Andrew Kehler. 2002. Coherence, Reference and the
Theory of Grammar. CSLI Publications.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
bank to Propbank. In Proceedings of LREC-02.
Ivana Kruijff-Korbayova? and Bonnie Webber. 2001. In-
formation Structure and the Semantics of ?otherwise?.
In Proceedings of ESSLLI 2001: Workshop on Infor-
mation Structure, Discourse Structure and Discourse
Semantics.
William Mann and Sandra Thompson. 1988. Rhetorical
Structure Theory. Toward a Functional Theory of Text
Organization. Text, 8(3):243?281.
Mitch Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19:313?330.
Eleni Miltsakaki, Cassandre Creswell, Kate Forbes, Ar-
avind Joshi, and Bonnie Webber. 2003. Anaphoric
Arguments of Discourse Connectives: Semantic
Properties of Antecedents versus Non-Antecedents.
In Proceedings of the Computational Treatment of
Anaphora Workshop, EACL 2003.
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and
Bonnie Webber. 2004. Annotating Discourse Con-
nectives and their Arguments. In Proceedings of the
NAACL/HLT Workshop: Frontiers in Corpus Annota-
tion.
Marc Moens and Mark Steedman. 1988. Temporal On-
tology and Temporal Reference. Computational Lin-
guistics, 14(2):15?28.
Megan G. Moser and Johanna Moore. 1995. Investi-
gating Cue Selection and Placement in Tutorial Dis-
course. In Proceedings of ACL95.
Sidney Siegel and N. J. Castellan. 1988. Nonparama-
teric Statistics for the Behavioral Sciences. McGraw-
Hill, 2nd edition.
Amanda Stent, Rashmi Prasad, and Marilyn Walker.
2004. Trainable sentence planning for complex infor-
mation presentation in spoken dialog systems. In Pro-
ceedings of ACL-2004.
Marilyn Walker, Rashmi Prasad, and Amanda Stent.
2003. A Trainable Generator for Recommendations
in Multimodal Dialogue. In Eurospeech, 2003.
Bonnie Webber and Aravind Joshi. 1998. Anchoring a
Lexicalized Tree-Adjoining Grammar for Discourse.
In ACL/COLING Workshop on Discourse Relations
and Discourse Markers, Montreal.
Bonnie Webber, Alistair Knott, Matthew Stone, and Ar-
avind Joshi. 1999a. Discourse Relations: A Struc-
tural and Presuppositional Account Using Lexicalized
TAG. In Proceedings of ACL-99.
Bonnie Webber, Alistair Knott, Matthew Stone, and Ar-
avind Joshi. 1999b. What are Little Texts Made of? A
Structural and Presuppositional Account Using Lex-
icalized TAG. In Proceedings of the International
Workshop on Levels of Representation in Discourse
(LORID ?99).
Bonnie Webber, Alistair Knott, and Aravind Joshi.
2000. Multiple Discourse Connectives in a Lexi-
calized Grammar for Discourse. In Proceedings of
IWCS-00.
Bonnie Webber, Matthew Stone, Aravind Joshi, and Al-
istair Knott. 2003. Anaphora and Discourse Struc-
ture. Computational Linguistics, 29:545?587.
Sandra Williams and Ehud Reiter. 2003. A Corpus
Analysis of Discourse Relations for Natural Language
Generation. In Proceedings of Corpus Linguistics.
Annotating Discourse Connectives And Their Arguments
Eleni Miltsakaki
University of Pennsylvania
Philadelphia, PA 19104 USA
elenimi@linc.cis.upenn.edu
Rashmi Prasad
University of Pennsylvania
Philadelphia, PA 19104 USA
rjprasad@linc.cis.upenn.edu
Aravind Joshi
University of Pennsylvania
Philadelphia, PA 19104 USA
joshi@linc.cis.upenn.edu
Bonnie Webber
University of Edinburgh
Edinburgh, EH8 9LW Scotland
bonnie@inf.ed.ac.uk
Abstract
This paper describes a new, large scale
discourse-level annotation project ? the Penn
Discourse TreeBank (PDTB). We present an
approach to annotating a level of discourse
structure that is based on identifying discourse
connectives and their arguments. The PDTB
is being built directly on top of the Penn Tree-
Bank and Propbank, thus supporting the extrac-
tion of useful syntactic and semantic features
and providing a richer substrate for the devel-
opment and evaluation of practical algorithms.
We provide a detailed preliminary analysis of
inter-annotator agreement ? both the level of
agreement and the types of inter-annotator vari-
ation.
1 Introduction
Large scale annotated corpora have played a critical role
in speech and natural language research. The Penn Tree-
Bank (PTB) is an example of such a resource with world-
wide impact on natural language processing (Marcus et
al., 1993). However, the PTB deals with text only at
the sentence level: with the demand for more power-
ful NLP applications comes a need for greater richness
in annotation. At the sentence level, Penn Propbank
is adding predicate-argument annotation to sentences in
PTB (Kingsbury and Palmer, 2002). At the discourse-
level are efforts to produce corpora annotated with rhetor-
ical relations (Carlson et al, 2003). This paper describes
a more basic discourse-level annotation project ? the
Penn Discourse TreeBank (PDTB) ? that aims to produce
a large-scale corpus in which discourse connectives are
annotated, along with their arguments.
There have been several approaches to describing dis-
course in terms of discourse relations (Mann and Thomp-
son, 1988; Asher and Lascarides, 1998; Polanyi and
van den Berg, 1996). In these approaches, the additional
meaning the discourse contributes beyond the sentence
derives from discourse relations. Specification of the dis-
course relations for a discourse thus constitutes a descrip-
tion of a certain level of discourse structure.
Rather than starting from (abstract) discourse rela-
tions, we describe an approach to annotating a large-
scale corpus in terms of a more basic characterisation
of discourse structure in terms of discourse connectives
and their arguments. The motivation for such an ap-
proach stems from work by Webber and Joshi (1998),
Webber et al (1999a), Webber et al (2000) which inte-
grates sentence level structures with discourse level struc-
ture (using tree-adjoining grammars for both cases, LTAG
and DLTAG, respectively).1 This allows structural com-
position and its associated semantic composition at the
sentence level to be smoothly carried over to the dis-
course level, a goal also shared by Gardent (1997),
Schilder (1997) and Polanyi and van den Berg (1996),
among others.2
Discourse connectives and their arguments can be suc-
cessfully annotated with high reliability (cf. Section
4). This is not surprising, given that the task resem-
bles that of annotating verbs and their arguments at
the sentence level (Kingsbury and Palmer, 2002). In
fact, we use a fine-grained, lexically grounded annota-
tion in which argument labels are specific to the dis-
1In the PDTB annotations, we have deliberately adopted
a policy to make the annotations independent of the DLTAG
framework for two reasons: (1) to make the annotated corpus
widely useful to researchers working in different frameworks
and (2) to make the annotators? task easier, thereby increasing
interannotator reliability.
2However, the approaches in Gardent (1997),
Schilder (1997), and Polanyi and van den Berg (1996) are
different in two ways: a) the process by which discourse
derives compositional aspects of meaning is considered entirely
separate from how clauses do so, and b) only two mechanisms
are used for deriving discourse semantics ? compositional
semantics and inference.
course connectives involved, in much the same way as
in Kingsbury and Palmer (2002). In contrast, a recent
attempt (Carlson et al, 2003) at using RST-type rela-
tions for annotating a much smaller corpus has already
revealed difficulties involved in reliably annotating more
abstract discourse relations. Moreover, this type of anno-
tation does not contain any record of the basis on which
a relation was assigned.
The paper is organized as follows. Section 2 provides
a brief overview of the fundamental ideas that provide
the basis for the design of the PDTB annotation. Section
3 gives a detailed description of the annotation project,
including information about the size of the corpus, com-
pleted annotations as well as annotation instructions as
formulated in the guidelines. Section 4 presents data
analysis based on current annotations as well as results
from inter-annotator agreement. Section 5 wraps up with
a summary of the work.
2 Theoretical background
The annotation project presented in this paper builds
on basic ideas presented in Webber and Joshi (1998),
Webber et al (1999b) and Webber et al (2003) ? that
connectives are discourse-level predicates which project
predicate-argument structure on a par with verbs at the
sentence level. Webber and Joshi (1998) propose a tree-
adjoining grammar for discourse (DLTAG) in which
compositional aspects of discourse meaning are for-
mally defined, thus teasing apart compositional from non-
compositional layers of meaning. In this framework, con-
nectives are grouped into natural classes depending on the
structure that they project at the discourse level. Subordi-
nate and coordinating conjunctions, for example, require
two arguments that can be identified structurally from ad-
jacent units of discourse. What Webber et al (2003) call
anaphoric discourse connectives (some, but not all, dis-
course adverbials, such as ?otherwise?, ?instead?, ?fur-
thermore?, etc.) also require two arguments, but only one
of them derives structurally. For the complete interpreta-
tion of these connectives, their other argument needs to
be recovered. The crucial contribution of this framework
to the design of the current project is what can be seen
as a bottom-up approach to discourse structure. Specifi-
cally, instead of appealing to an abstract (and arbitrary)
set of discourse relations whose identification involves
confounding multiple sources of discourse meaning, we
start with the annotation of discourse connectives and
their arguments, thus exposing a clearly defined level of
discourse representation.
3 Project description
The PTDB project began in November 2002. The first
phase, including pilot annotations and preliminary devel-
opment of guidelines, was completed in May 2003. The
PDTB is expected to be released by November 2005. In-
termediate versions of the annotated corpus will be made
available for receiving feedback.
The PDTB corpus will include annotations of four
types of connectives: subordinating conjunctions, coor-
dinating conjunctions, adverbial connectives and implicit
connectives. We specify each of these types in more de-
tail in Section 3.1. The final number of annotations in
the corpus will amount to approximately 30,000; 10,000
implicit connectives and 20,000 annotations of the 250
explicit connectives identified in the corpus. The final
version of the corpus will also contain characterizations
of the semantic roles associated with the arguments of
each type of connective.
In this paper we present the results of annotating 10
explicit connectives, amounting to a total of 2717 anno-
tations, as well as 386 tokens of implicit connectives. The
set of 10 connectives comprises the adverbial connectives
?therefore?, ?as a result?, ?instead?, ?otherwise?, ?never-
theless?, and the subordinate conjunctions ?because?, ?al-
though?, ?even though?, ?when?, and ?so that?. In all
cases, annotations have been performed by four annota-
tors. While this slows down the annotation process con-
siderably, the nature, significance and magnitude of the
project as well as the well-known complexity of discourse
annotation tasks impels us to strive for maximum relia-
bility, achieved by having the task performed by multiple
annotators.3
Individual annotation proceeds one connective at a
time. The annotation tool WordFreak4 is used to iden-
tify all instances of the given connective in the corpus,
and these are then annotated independently and manu-
ally by four annotators. This way, the annotators quickly
gain experience with that connective and develop a better
understanding of its predicate-argument characteristics.
Similarly, for the annotation of implicit connectives, all
instances (as specified in the guidelines, see Section 3.2)
are identified one file at a time. For this task, the anno-
tators are required to read the entire file so that they can
make well-informed and reliable decisions about the im-
plicit connectives and their arguments. In addition, after
the arguments of each implicit connective have been iden-
tified, the annotators provide, if possible, an explicit con-
nective (or other suitable expression) that best expresses
the inferred relation. As with explicit connectives, anno-
tations of implicit connectives are done by four annota-
3When inter-annotator consistency has stabilized, we intend
to reduce the number of annotators to three, or maybe two at the
minimum.
4WordFreak was developed by Tom Morton at the University
of Pennsylvania. It has been substantially modified by Jeremy
Lacivita to fit the needs of the PDTB project. A snapshot of the
tool can be seen at http://www.cis.upenn.edu/?pdtb.
tors.
Compared with Propbank?s annotation of verb
predicate-argument structures, annotation of arguments
of discourse predicates is different in interesting ways.
Propbank annotators have to determine the number of ar-
guments required by each verb. In contrast, discourse
connectives exhibit a clear predicate-argument structure
requiring only two arguments. The main challenge we
have discovered for annotating discourse connectives is
determining the extent of their arguments. Even subor-
dinate conjunctions whose arguments never cross a sen-
tence boundary may sometimes be the source of disagree-
ment between annotators.
In what follows, we present a brief overview of the
classes of connectives that we annotate, followed by
highlights of the annotation manual and relevant corpus
examples.
3.1 Discourse connectives
We classify discourse connectives into four classes: sub-
ordinate and coordinating conjunctions, adverbials and
implicit connectives. Examples of each type are given be-
low, with their arguments shown in square brackets and
the connectives, in italics.
3.1.1 Subordinate conjunctions
Subordinate conjunctions introduce clauses that are
syntactically dependent on a main clause. The most com-
mon types of relations that they express are temporal
(e.g., ?when?, ?as soon as?), causal e.g., ?because?), con-
cessive (e.g., ?although?, ?even though?), purpose (e.g.,
?so that?, ?in order that?) and conditional (e.g., ?if?, ?un-
less). Clauses introduced with a subordinate conjunction
may be preposed (or, more rarely, interposed) with re-
spect to the main clause, as shown in (1).
(1) Because [the drought reduced U.S. stockpiles], [they
have more than enough storage space for their new
crop], and that permits them to wait for prices to rise.
3.1.2 Coordinating conjunctions
Coordinating conjunctions are ones such as ?and?,
?but?, and ?or?. Example (2) shows the annotation of an
instance of the conjunction ?and?.
(2) [William Gates and Paul Allen in 1975 developed
an early language-housekeeper system for PCs], and
[Gates became an industry billionaire six years after
IBM adapted one of these versions in 1981].
Instances of coordinating conjunctions which coordi-
nate nominal or other non-clausal constituents are ex-
cluded from annotation. We also exclude cases of VP-
coordination because in such cases the arguments of the
connective can be retrieved automatically from the syn-
tactic layer.
3.1.3 Adverbial connectives
Adverbial connectives are sentence-modifying adverbs
which express a discourse relation (Forbes, 2003). The
class of adverbial connectives includes ?however?, ?there-
fore?, ?then?, ?otherwise?, etc. In this class, we have also
included prepositional phrases with a similar sentence
modifying function such as ?as a result?, ?in addition?,
?in fact?, etc. Example (3) shows the annotation of an
instance of the adverbial connective ?as a result?.
(3) ...[many analysts expected energy prices to rise at the
consumer level too]. As a result, [many economists
were expecting the consumer price index to increase
significantly more than it did].
The arguments of adverbial connectives may or may
not be adjacent to the sentence containing the connective.
In a few cases, an argument may be found one or two
paragraphs away from the connective.
3.1.4 Implicit connectives
Implicit connectives are identified between adjacent
sentences with no explicit connectives.5 The annotation
of implicit connectives is intended to capture the connec-
tion between two sentences appearing in adjacent posi-
tions. For example, in (4), the two adjacent sentences
are connected in a way similar to having the explicit con-
nective ?but? contrasting them. Indeed, for implicit con-
nectives, annotators are asked to provide, when possible,
an explicit connective that best describes the inferred re-
lation. The explicit connective provided in (4) was ?in
contrast?.
(4) ...[The $6 billion that some 40 companies are looking to
raise in the year ending March 31 compares with only
$2.7 billion raised on the capital market in the previous
fiscal year]. IMPLICIT-(In contrast) [In fiscal 1984 be-
fore Mr. Gandhi came to power, only $810 million was
raised].
3.2 Annotation guidelines
The annotation guidelines for PDTB have been revised
considerably since the pilot phase of the project in May
2003. The current version of the guidelines is available at
http://www.cis.upenn.edu/?pdtb. Below we out-
line the basic points.
3.2.1 What counts as a discourse connective?
We count as discourse connectives (1) all subordinat-
ing and coordinating conjunctions, (2) certain adverbials,
and (3) implicit connectives. The adverbials include only
those which convey a relation between events or states.
For example, in (5) ?as a result? conveys a cause-effect re-
lation between the event of limiting the size of new steel
5There are, of course, other implicit connectives that we are
not taking into account.
mills and that of the industry operating out of small, ex-
pensive and highly inefficient units. In contrast, the se-
mantic interpretation of ?strangely? in (6) only requires a
single event/state which it classifies in the set of strange
events/states.6
(5) [In the past, the socialist policies of the government
strictly limited the size of new steel mills, petrochem-
ical plants, car factories and other industrial concerns to
conserve resources and restrict the profits businessmen
could make]. As a result, industry operated out of small,
expensive, highly inefficient industrial units.
(6) Strangely, conventional wisdom inside the Beltway re-
gards these transfer payments as ?uncontrollable? or
?nondiscretionary.?
The guidelines also highlight instances of lexical items
with multiple functions, only one of which is as a dis-
course connective. For example, ?when? can either serve
as a subordinate conjunction or introduce a relative clause
modifying a nominal phrase, as in (7), where the when-
clause modifies the nominal ?1985?.7Here we again ben-
efit from building discourse annotation on top of Penn
TreeBank because the syntactic annotation of when-
clauses distinguishes the two functions: When-relatives
are marked as NP-modifiers adjoining to an NP, whereas
adverbial when-clauses adjoin to a sentential node.
(7) Attorneys have argued since 1985, when the law took
effect.
Similarly, some since-clauses function as NP modifiers
as shown in (8). In such cases, ?since? is not annotated as
a connective. As in the case of when-clauses, instances of
NP modifying since-clauses can be identified in the Penn
TreeBank by virtue of their syntactic annotation.
(8) In the decade since the communist nation emerged from
isolation, its burgeoning trade with the West has lifted
Hong Kong?s status as a regional business partner.
Finally, implicit connectives count as connectives.
They are identified between adjacent sentences which do
not contain any other explicit connectives. Currently, we
are not annotating implicit connectives intra-sententially,
such as between the matrix clause and free adjunct in Ex-
ample (9). We plan to incorporate annotations of implicit
intra-sentential connectives at a later stage of the project.
(9) Second, they channel monthly mortgage payments into
semiannual payments, reducing the administrative bur-
den on investors.
6For a more detailed discussion of the basis for distin-
guishing discourse adverbials from clausal adverbials, see
Forbes (2003).
7In cases of when-relatives, a when-clause can be annotated
as SUP (see Section 3.2.3).
3.2.2 What counts as a legal argument?
Because we take discourse relations to hold between
abstract objects, we require that an argument contains at
least one predicate along with its arguments. Of course,
a sequence of clauses or sentences may also form a legal
argument, containing multiple predicates.
Because our annotations are done directly on top of
the Penn TreeBank, annotators may select as an argument
certain textual spans that appear to exclude one or more
arguments of the predicate. These are cases in which
these arguments are directly retrievable from the syntac-
tic annotation. Thus, we are able to select only the pred-
icates that are required for the interpretation of the dis-
course connective and simultaneously access their argu-
ments for the complete interpretation of the clause while
keeping the annotations of single arguments simple and
maximally contiguous. In (10), for example, the relative
clause is marked as one of the two arguments of the con-
nective ?even though?. The subject of the verb in the rela-
tive clause is directly retrievable from the Penn TreeBank
annotation. Similarly, in (11) the subject of the infinitival
clause is also available from the syntactic representation.
(10) Workers described ?clouds of dust? [that hung over
parts of the factory] even though [exhaust fans venti-
lated the air].
(11) The average maturity for funds open only to institutions,
considered by some [to be a stronger indicator] because
[those managers watch the market closely], reached a
high point for the year ? 33 days.
There are two exceptions to the requirement that an
argument include a verb ? these are nominal phrases that
express an event or a state, and discourse deictics that
denote an event or state. In (12), for example, the nominal
phrase ?fainting spells? can be marked as a legal argument
of the connective ?when? because the phrase expresses an
event of fainting.
(12) Its symptoms include a cold sweat at the sound of de-
bate, clammy hands in the face of congressional crit-
icism, and [fainting spells] when [someone writes the
word ?controversy.?]
Discourse deictic expressions are forms such as ?this?
and ?that? that can be used to denote the interpretation
of clausal textual spans from the preceding discourse.
In (13), for example, ?that? denotes the interpretation of
the sentence immediately preceding it. Our annotators
are guided to make argument selections that assume that
anaphoric and deictic expressions have been resolved.
Thus, in (13), they are able to select ?That?s? as one ar-
gument of the connective ?because?.
(13) Airline stocks typically sell at a discount of about one-
third to the stock market?s price-earnings ratio ? which
is currently about 13 times earnings. [That?s] because
[airline earnings, like those of auto makers, have been
subject to the cyclical ups-and-downs of the economy].
The annotators are also informed that in some cases,
an argument of a connective must be derived from the
selected textual span (Webber et al, 1999a; Webber et al,
2003). This is the case for the first argument of ?instead?
in (14), which does not include the negation, although it
is contained in the selected text.8
(14) [No price for the new shares has been set]. Instead, [the
companies will leave it up to the marketplace to decide].
In sum, legal arguments can be groups of sentences,
single sentences (a main clause and its subordinate
clauses), single clauses (tensed or non-tensed), NPs that
specify events or situations, and discourse deictic expres-
sions.
3.2.3 How far does an argument extend?
One particularly significant addition to the guidelines
came as a result of differences among annotators as to
how large a span constituted the argument of a connec-
tive. During pilot annotations, annotators used three an-
notation tags: CONN for the connective and ARG1 and
ARG2 for the two arguments. To this set, we have added
the optional tags SUP1, SUP2 (supplementary) for cases
when the annotator wants to mark textual spans s/he con-
siders to be useful, supplementary information for the
interpretation an argument. Example (15) demonstrates
the use of SUP1. Arguments are shown in square brack-
ets, while spans providing supplementary information are
shown in parentheses.
(15) Although [started in 1965], [Wedtech didn?t really get
rolling until 1975] (when Mr. Neuberger discovered the
Federal Government?s Section 8 minority business pro-
gram).
4 Data analysis
To test the reliability of the annotation, we first con-
sidered the kappa statistic (Siegel and Castellan, 1988)
which is used extensively in empirical studies of dis-
course (Carletta, 1996). The kappa coefficient provides
an inter-annotator agreement figure for any number of an-
notators by measuring pairwise agreement between them
and by correcting for chance expected agreement. How-
ever, the statistic requires the data tokens to be classified
into discrete categories, and as a result, we could not ap-
ply it to our data since the PDTB annotation tokens can-
not be classified as such. Rather, annotation in the PDTB
constitutes either selection of a span of text for the ar-
guments of connectives which can be of indeterminate
length or providing explicit expressions for implicit con-
nectives from an open-ended class of expressions.
8For a preliminary corpus-based analysis of the arguments
of ?instead?, see Miltsakaki et al (2003).
Instead, we have assessed inter-annotator agreement in
terms of agreement/disagreement on span or named ex-
pression identity for each token as a percentage of the
pairs of spans or expressions that actually matched ver-
sus those that should have. For the argument annotations,
we use a most conservative measure - the exact match
criterion. In addition, we also used different diagnostics
for the argument annotations for the explicit connectives,
reporting percentage agreement on different classes of to-
kens, such as those in which the first argument (ARG1)
annotations and second argument (ARG2) annotations
were counted independently, as well as those in which the
ARG1 and ARG2 annotations (for each connective) were
counted together as a single token. For all the argument
annotations, the computation of agreement excluded the
supplementary annotations (cf. Section 3.2.3).
We present here agreement results on ARG1 and
ARG2 annotations by two annotators for the annotation
of ten explicit connectives, amounting to a total of 2717
annotations, and 368 annotations of implicit connectives,
including agreement results on the explicit expression the
annotators used in in place of the implicit connectives as
well as the ARG1 and ARG2 annotations of the implicit
connectives.9 The ten explicit connectives include 5 sub-
ordinating conjunctions (when, because, even though, al-
though, and so that) and 5 adverbials (nevertheless, oth-
erwise, instead, therefore, and as a result).
4.1 Inter-annotator Agreement
4.1.1 Explicit connectives
For the explicit connective annotations, we used two
diagnostics for measuring inter-annotator agreement. In
the first diagnostic , we took the class of tokens as the to-
tal number of argument annotations, treating ARG1 and
ARG2 annotations as independent tokens. The total num-
ber of tokens in this class is therefore twice the number
of connective tokens, i.e, 5434. We recorded agreement
using the exact match criterion. That is, for any ARG1
or ARG2 token, agreement was recorded as 1 when both
annotators made identical textual selections for the an-
notation and 0 when the annotators made non-identical
selections.
We achieved 90.2% agreement (4900/5434 tokens)
on the annotations for this class. Agreement on only
ARG1 tokens was 86.3%, and agreement on only ARG2
tokens was 94.1%. Further distribution of the agree-
ments by connective is given in Table 1. Connectives
are grouped in the table by type (subordinating conjunc-
tion (SUBCONJ) and adverbial (ADV)). The second col-
9Right now SUP1 and SUP2 annotations are for our use only
and are not included in the current evaluations. Additional an-
notations by another 2 annotators are currently underway. The
2 annotators of the explicit connectives are different from the 2
annotators of the implicit connectives.
umn gives the number of agreeing tokens for each con-
nective and the third column gives the total number of
(ARG1+ARG2) tokens available for that connective. The
last column gives the percent agreement for the connec-
tive in that row, i.e., as a percentage of tokens for which
agreement was 1 (column 2) versus the total number of
tokens for that connective (column 3).
CONNECTIVES AGR No. Conn. Total %AGR
when 1877 2032 92.4%
because 1703 1824 93.4%
even though 194 206 94.1%
although 635 704 90.1%
so that 66 74 89.2%
TOTAL SUBCONJ 4469 4834 92.4%
nevertheless 56 94 59.6%
otherwise 44 46 95.7%
instead 172 236 72.9%
as a result 110 168 65.5%
therefore 49 56 87.5%
TOTAL ADV. 431 600 71.8%
OVERALL TOTAL 4900 5434 90.2%
Table 1: Distribution of Agreement by Connective, with
ARG1 and ARG2 Annotations Counted Independently
The table shows that we achieved high agreement
on argument annotations of subordinating conjunctions
(92.4%). Average agreement on the adverbials was lower
(71.8%). This difference between the two types is not sur-
prising, since locating the anaphoric (ARG1) argument of
adverbial connectives is believed to be a harder task than
that of locating the arguments of subordinating conjunc-
tions. For example, the anaphoric argument of the ad-
verbial connectives may be located in some non-adjacent
span of text, even several paragraphs away. Arguments of
subordinating conjunctions, on the other hand, can most
often be found in spans of text adjacent to the connective.
The table also shows that there was uniform agreement
across the different subordinating conjunctions (roughly
90%), whereas the adverbials showed more variation.
In particular, agreement on otherwise and therefore was
high (95.7% and 87.5% respectively), while lower for
the other three adverbials, instead (72.9%), as a result
(65.5%), and nevertheless (59.6%). This suggests either
greater variability in how these adverbials are interpreted
or greater complexity in their interpretation, which results
in more variability when people are forced to associate an
interpretation with a particular text span.
We also computed agreement using a second more
conservative diagnostic in which we took the class of to-
kens as the total number of connective tokens (2717) so
that the ARG1 and ARG2 annotations for each connec-
tive were treated together as part of the same token. Here
again, we recorded agreement using the exact match mea-
sure. That is, for any connective token, agreement was
recorded as 1 when both annotators made identical tex-
tual selections for the annotation of both arguments and
0 when the annotators made non-identical selections for
any one or both arguments.
We achieved 82.8% agreement (2249/2717 tokens) on
the annotations for this class. Table 2 gives the distribu-
tion of the agreements by connective. The table shows
relatively lower agreements when compared with the first
diagnostic, for both subordinating conjunctions (86%) as
well as adverbials (57%). However, this difference is un-
derstandable since the token class as defined for this di-
agnostic yields a stricter measure of agreement.
CONNECTIVES AGR No. Conn. Total %AGR
when 868 1016 86.4%
because 804 912 88.2%
even though 91 103 88.3%
although 288 352 81.8%
so that 27 34 79.4%
TOTAL SUBCONJ 2078 2417 86.0%
nevertheless 18 47 38.3%
otherwise 21 23 91.3%
instead 72 118 61.0%
as a result 38 84 45.2%
therefore 22 28 78.6%
TOTAL ADV. 171 300 57.0%
OVERALL TOTAL 2249 2717 82.8%
Table 2: Distribution of Agreement by Connective, with
ARG1 and ARG2 Annotations Counted Together
We classified disagreements into 4 major types. The
result of classifying the 534 disagreements from Diag-
nostic 1 (Table 1) is given in Table 3. The third column
gives the percent of the total disagreements for each type.
DISAGREEMENT TYPE No. %
Missing Annotations 72 13.5%
No Overlap 30 5.6%
Partial Overlap
Parentheticals 53 9.9%
higher verb 181 33.9%
dependent clause 182 34.1%
Other 6 1.1%
Unresolved 10 1.9%
TOTAL 534 100%
Table 3: Disagreement Classification
The majority of disagreements (79%) were due to
Partial Overlap, which subsumes the categories Higher
Verb, Dependent Clause, Parenthetical and Other. Par-
tial Overlap means that there was partial overlap in the
annotations selected by the two annotators. Higher verb
includes tokens where one of the annotators included the
governing predicate for the clause marked by both anno-
tators. The higher clause occurred on the left or right pe-
riphery of the lower clause. Dependent Clause includes
tokens where one of the annotators included extra clausal
material that is syntactically dependent on the clause that
was selected by both, and that occurs on the left or right
periphery of the common text. Parenthetical means
that one of the annotators included a medial parentheti-
cal, while the other did not. The intervening text could be
the main as well as the dependent clause. An example is
provided below:
(16) Bankers said [warrants for Hong Kong stocks are at-
tractive] because [they give foreign investors], wary of
volatility in the colony?s stock market, [an opportunity
to buy shares without taking too great a risk].
(17) Bankers said [warrants for Hong Kong stocks are at-
tractive] because [they give foreign investors, wary of
volatility in the colony?s stock market, an opportunity
to buy shares without taking too great a risk].
Other included tokens with partial overlap between an-
notations, but in addition included a combination of more
than type, such as higher verb+dependent clause.
Note that disagreements that contain a partial over-
lap could be counted as agreeing tokens if we relaxed
the more conservative exact match measure to a partial
match measure. Our subjective view was that in several
cases, the ?extra? textual material, especially those fit-
ting the dependent clause and parenthetical category did
not make any significant semantic contribution in terms
of their inclusion or exclusion in the argument. With the
partial match measure, excluding these cases reduces the
disagreements to half the given number, giving us 94.5%
agreement overall.
The No Overlap tokens were cases of true disagree-
ment in that there was no overlap in the annotations se-
lected by the annotators. These tokens constituted 5.6%
of the disagreements. Examples (18) and (19) shows the
two annotations for a token in which there was no over-
lap in the ARG1 annotation. Missing Annotations also
constituted a substantial proportion of the disagreements
(13.5%) and was used for tokens where the annotation
was missing for one annotator. Note that these don?t re-
ally count as disagreement, since all connectives are pre-
theoretically assumed to require two arguments. Unre-
solved includes tokens which have introduced new issues
for the annotation guidelines and cannot be resolved at
this time. These include issues such as how to treat com-
paratives, certain types of adjunct clauses, certain types
of nominalizations etc.
(18) [The word ?death? cannot be escaped entirely by the
industry], but salesmen dodge it wherever possible or
cloak it in euphemisms, [preferring to talk about ?sav-
ings? and ?investment?] instead.
(19) The word ?death? cannot be escaped entirely by the
industry, but salesmen dodge it wherever possible or
[cloak it in euphemisms], preferring [to talk about ?sav-
ings? and ?investment?] instead.
4.1.2 Implicit connectives
For the 386 tokens of implicit connectives, we ana-
lyzed inter-annotator agreement between two annotators
for (a) the explicit connectives they provided in place of
an implicit connective, and (b) the argument annotations
of the implicit connectives.
As a preliminary step in analyzing agreement on the
type of explicit connective provided by the annotators in
place of an implicit connective, we considered 5 groups
of connectives conveying : a) additional information
(e.g., ?furthermore?, ?in addition?) b) cause-effect rela-
tions (e.g., ?because?, ?as a result?), c) temporal relations
(e.g., ?then?, ?simultaneously?), d) contrastive relations
(e.g., ?however?, ?although?), and e) restatement or sum-
marization (e.g., ?in other words?, ?in sum?). 10 Agree-
ment was then computed on these basic groups of con-
nectives.11 From the total of 386 tokens of implicit con-
nectives, 9 were excluded from the analysis due to tech-
nical error (missing annotation). For the remaining 307
tokens, we achieved 72% agreement on the type of ex-
plicit connective that best conveyed the interpretation of
the implicit connective.
For the argument annotations of the implicit connec-
tives, we present agreement results from using the first
diagnostic used for the explicit connectives. That is, we
counted ARG1 and ARG2 annotations as independent to-
kens and computed percent agreement using the exact
match criterion. On the 772 ARG1 and ARG2 tokens,
we achieved 85.1% (657/772) agreement between 2 an-
notators. The analysis of the 115 disagreements is given
in Table 4. Note that here again, the number of disagree-
ments reduces to half using the partial match measure for
the parenthetical and dependent clause classes, giving us
92.6% agreement overall.
DISAGREEMENT TYPE No. %
Missing Annotations 6 5.2%
No Overlap 2 1.7%
Partial Overlap
parenthetical 13 11.3%
higher verb 24 20.9%
dependent clause 44 38.3%
sentence 19 16.5%
other 3 2.6%
Unresolved 4 3.5%
TOTAL 115 100%
Table 4: Disagreement Classification for Implicit Con-
nective ARG Annotations
10These groups are based on types of coherence relations de-
rived from corpus-based distributions of connectives presented
in (Knott, 1996). Initially, we also considered a group of con-
nectives expressing hypothetical relations but no such connec-
tives were identified in the annotations.
11Some polysemous connectives such as ?while? and ?in fact?
appeared in more than one group.
5 Summary
In this paper we presented a new and innovative
discourse-level annotation project, the Penn Discourse
TreeBank (PDTB), in which discourse connectives and
their arguments are annotated, thereby defining a clear
level of discourse structure that can be reliably annotated
for a large corpus. Our inter-annotator results confirm our
expectations of high agreement and annotation reliability.
At a later stage of the project, we plan to provide seman-
tic characterizations of the arguments of connectives and
resolve any cases of polysemy that might arise.
Acknowledgments
We are very grateful to Tom Morton and Jeremy Lacivita
for the development and special modification of the
WordFreak annotation tool. Special thanks to Jeremy for
providing continuous technical support. Thanks are also
due to our annotators, Cassie Creswell, Driya Amandita,
John Laury, Emily Pawley, Alan Lee, Alex Derenzy and
Steve Pettington. Also, many thanks to Katherine Forbes
Riley and Jean Carletta for their comments and sugges-
tions. Finally, we would like to thank the reviewers for
their very useful comments. This work was partially sup-
ported by NSF Grant EIA 02-24417.
References
Nicholas Asher and Alex Lascarides. 1998. The seman-
tics and pragmatics of presupposition. Journal of Se-
mantics, 15(3):239?300.
Jean Carletta. 1996. Assessing agreement on classifica-
tion tasks. Computational Linguistics, 22:249?254.
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski,
2003. Current Directions in Discourse and Dialogue,
chapter Building a Discourse-Tagged Corpus in the
Framework of Rhetorical Structure Theory. Kluwer
Academic Publishers.
Kate Forbes. 2003. Discourse Semantics of S-Modifying
Adverbials. Ph.D. thesis, Department of Linguistics,
University of Pennsylvania.
Claire Gardent. 1997. Discourse tree adjoining gram-
mars. Claus 89, University of the Saarlandes, Saar-
brucken.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
bank to Propbank. In Third International Confer-
ence on Language Resources and Evaluation, LREC-
02, Las Palmas, Canary Islands, Spain.
Alistair Knott. 1996. A Data-Driven Methodology for
Motivating a Set of Coherence Relations. Ph.D. thesis,
University of Edinburgh.
William Mann and Sandra Thompson. 1988. Rhetori-
cal structure theory. toward a functional theory of text
organization. Text, 8(3):243?281.
Mitch Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of english: the Penn Treebank. Computational
Linguistics, 19:313?330.
Eleni Miltsakaki, Cassandre Creswell, Kate Forbes, Ar-
avind Joshi, and Bonnie Webber. 2003. Anaphoric
arguments of discourse connectives: Semantic prop-
erties of antecedents versus non-antecedents. In Pro-
ceedings of the Computational Treatment of Anaphora
Workshop, EACL 2003, Budapest.
Livia Polanyi and Martin van den Berg. 1996. Discourse
structure and discourse interpretation. In Proceedings
of the Tenth Amsterdam Colloquium, University of Am-
sterdam, pages 113?131.
Frank Schilder. 1997. Discourse tree grammar or how
to get attached to a discourse? In Proceedings of the
the second International Workshop on Computational
Semantics (IWCS-II), Tilburg, The Netherlands, pages
261?273.
Sidney Siegel and N. J. Castellan. 1988. Nonparama-
teric Statistics for the Behavioral Sciences. McGraw-
Hill, 2nd edition.
Bonnie Webber and Aravind Joshi. 1998. Anchoring a
lexicalized tree adjoining grammar for discourse. In
ACL/COLING Workshop on Discourse Relations and
Discourse Markers, Montreal, pages 8?92. Montreal,
Canada.
Bonnie Webber, Alistair Knott, Matthew Stone, and Ar-
avind Joshi. 1999a. Discourse relations: A struc-
tural and presuppositional account using lexicalized
TAG. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics, Mary-
land, pages 41?48. College Park MD.
Bonnie Webber, Alistair Knott, Matthew Stone, and Ar-
avind Joshi. 1999b. What are little texts made of? A
structural and presuppositional account using lexical-
ized TAG. In Proceedings of the International Work-
shop on Levels of Representation in Discourse (LORID
?99), Edinburgh, pages 145?149.
Bonnie Webber, Alistair Knott, and Aravind Joshi. 2000.
Multiple discourse connectives in a lexicalized gram-
mar for discourse. In Proceedings of the Third Interna-
tional Workshop on Computational Semantics, Tilburg,
The Netherlands.
Bonnie Webber, Matthew Stone, Aravind Joshi, and Al-
istair Knott. 2003. Anaphora and discourse structure.
Computational Linguistics, 29:545?587.
Proposition Bank II:  Delving Deeper 
 
 
Olga Babko-Malaya, Martha Palmer, Nianwen Xue, Aravind Joshi1, Seth Kulick 
University of Pennsylvania 
{malayao/mpalmer/xueniwen/joshi/skulick}@linc.cis.upenn.edu 
                                                          
1 Associated with Penn Discourse Treebank (PDTB). Other members of the project are Eleni Miltsakaki, Rashmi Prasad, 
(Univ. of PA) and Bonnie Webber (Univ. of Edinburgh) 
 
 
Abstract 
The PropBank project is creating a corpus of 
text annotated with information about basic 
semantic propositions. PropBank I (Kingsbury 
& Palmer, 2002) added a layer of predicate-
argument information, or semantic roles, to 
the syntactic structures of the English Penn 
Treebank.   This paper presents an overview 
of the second phase of PropBank Annotation, 
PropBank II, which is being applied to Eng-
lish and Chinese, and includes (Neodavid-
sonian) eventuality variables, nominal 
references, sense tagging, and connections to 
the Penn Discourse Treebank (PDTB), a pro-
ject for annotating discourse connectives and 
their arguments. 
1 Introduction 
An important question is the degree to which current 
statistical NLP systems can be made more domain-
independent without prohibitive costs, either in terms of 
engineering or annotation.  The Proposition Bank is 
designed as a broad-coverage resource to facilitate the 
development of more general systems.  It focuses on the 
argument structure of verbs, and provides a complete 
corpus annotated with semantic roles, including partici-
pants traditionally viewed as arguments and ad-
juncts.  Correctly identifying the semantic roles of the 
sentence constituents is a crucial part of interpreting 
text, and in addition to forming a component of the in-
formation extraction problem, can serve as an interme-
diate step in machine translation or automatic 
summarization. 
 
The Proposition Bank project takes a practical approach 
to semantic representation, adding a layer of predicate-
argument information, or semantic roles, to the syntactic 
structures of the Penn Treebank.  The resulting resource 
can be thought of as shallow, in that it does not repre-
sent co-reference, quantification, and many other 
higher-order phenomena, but also broad, in that it cov-
ers every verb in the corpus and allows representative 
statistics to be calculated. The semantic annotation pro-
vided by PropBank is only a first approximation at cap-
turing the full richness of semantic representation. 
Additional annotation of nominalizations and other 
noun predicates has already begun at NYU. This paper 
presents an overview of the second phase of PropBank 
Annotation, PropBank II, which is being applied to Eng-
lish and Chinese and includes (Neodavidsonian) eventu-
ality variables, nominal references, sense tagging, and 
discourse connectives.   
2 PropBank I 
PropBank (Kingsbury & Palmer, 2002) is an annotation 
of the Wall Street Journal portion of the Penn Treebank 
II (Marcus, 1994) with `predicate-argument' structures, 
using sense tags for highly polysemous words and se-
mantic role labels for each argument. An important goal 
is to provide consistent semantic role labels across dif-
ferent syntactic realizations of the same verb, as in the 
window in [ARG0 John] broke [ARG1 the window] and 
[ARG1 The window] broke. PropBank can provide fre-
quency counts for (statistical) analysis or generation 
components in a machine translation system, but pro-
vides only a shallow semantic analysis in that the anno-
tation is close to the syntactic structure and each verb is 
its own predicate. 
 
In PropBank, semantic roles are defined on a verb-by-
verb basis.  An individual verb's semantic arguments are 
simply numbered, beginning with 0.  Polysemous verbs 
have several Framesets, corresponding to a relatively 
coarse notion of word senses, with a separate set of 
numbered roles, a roleset, defined for each Frameset.  
For instance, leave has both a DEPART Frameset ([ARG0 
John] left [ARG1 the room]) and a GIVE Frameset, ([ARG0 
I] left [ARG1 my pearls] [ARG2 to my daughter-in-law] 
[ARGM-LOC in my will].)   While most Framesets have 
three or four numbered roles, as many as six can appear, 
in particular for certain verbs of motion. Verbs can take 
any of a set of general, adjunct-like arguments 
(ARGMs), such as LOC (location), TMP (time), DIS 
(discourse connectives), PRP (purpose) or DIR (direc-
tion).  Negations (NEG) and modals (MOD) are also 
marked. 
 
The same annotation philosophy has been extended to 
the Penn Chinese Proposition Bank (Xue and Palmer, 
2003). The Chinese PropBank annotation is performed 
on a smaller (250k words) and yet growing corpus an-
notated with syntactic structures (Xue et al2004). The 
same syntactic alternations that form the basis for the 
English PropBank annotation also exist in robust quanti-
ties in Chinese, even though it may not be the case that 
the same exact verbs (meaning verbs that are close 
translations of one another) have the exact same range 
of syntactic realization for Chinese and English.  For 
example, in (1), "xin-nian/New Year  zhao-dai-
hui/reception" plays the same role in (a) and (b), which 
is the event or activity held,  even though it occurs in 
different syntactic positions. Assigning the same argu-
ment label, Arg1, to both instances, captures this regu-
larity. It is worth noting that the predicate ?ju-
xing/hold" does not have passive morphology in (1a), 
despite of what its English translation suggests. Like the 
English PropBank, the adjunct-like elements receive 
more general labels like TMP or LOC, as also illustrated 
in (1). The tag set for Chinese and English PropBanks 
are to a large extent similar and more details can be 
found in (Xue and Palmer, 2003). 
  
(1) a. [ARG1 xin-nian/New Year zhao-dai-
hui/reception] [ARGM-TMP jin-tian/today] [ARGM-
LOC zai/at diao-yu-tai/Diaoyutai guo-bin-guan/state 
guest house ju-xing/hold]  
"The New Year reception was held in Diaoyutai State 
Guest House today." 
 
          b. [ARG0 tang-jia-xuan/Tang Jiaxuan] [ARGM-
TMP jin-tian/today] [ARGM-LOC zai/at diao-yu-
tai/Diaoyutai guo-bin-guan/state guest house] ju-
xing/hold [arg1 xin-nian/New Year zhao-dai-
hui/reception] 
"Tang Jiaxuan was holding the New Year Reception in 
Diaoyutai State Guest House today." 
 
For polysemous verbs that take different sets of seman-
tic roles, we also distinguish different Framesets. (2) 
and (3) illustrate the different Framesets of "tong-
guo/pass", which correspond loosely with major senses 
of the verb.  The Frameset in (2) roughly means "pass 
by voting" while the Frameset illustrated by (3) means 
"pass through". The different Framesets are generally 
reflected in the different alternation patterns, which can 
serve as a cue for statistical systems performing Frame-
set disambiguation. (2) is similar to the causa-
tive/inchoative alternation (Levin, 1993). In contrast, (3) 
shows object drop. 
 
(2) a. [ARG0 guo-hui/Congress] zui-jin/recently tong-
guo/pass le/ASP [ARG1 zhou-ji/interstate yin-hang-
fa/banking law] 
    "The U.S. Congress recently passed the inter-state 
banking law." 
      b. [ARG1 zhou-ji/interstate yin-hang-fa/banking 
law] zui-jin/recently tong-guo/pass le/ASP 
       "The inter-state banking law passed recently." 
 
(3) a. [ARG0 huo-che/train] zheng-zai/now tong-
guo/pass [ARG1 sui-dao/tunnel] 
       "The train is passing through the tunne." 
        b. [ARG0 huo-che/train]  zheng-zai/now  tong-
guo/pass. 
       "The train is passing." 
 
There are also some notable differences between Chi-
nese PropBank and English PropBank. In general, the 
verbs in the Chinese PropBank are less polysemous, 
with the vast majority of the verbs having just one 
Frameset. On the other hand, the Chinese PropBank has 
more verbs (including static verbs which are generally 
translated into adjectives in English) normalized by the 
corpus size.  
3 Adding Event Variables to PropBank 
Event variables provide a rich analytical tool for analyz-
ing verb meaning. Positing that there is an event vari-
able allows for a straightforward representation of the 
logical form of adverbial modifiers, the capturing of 
pronominal reference to events, and the representation 
of nouns that refer to events. For example, event vari-
ables make it possible to have direct reference to an 
event with a noun phrase, as in (4a) destruction, and to 
refer back to an event with a pronoun (as illustrated in 
(4b) That): 
 
(4) a. The destruction of Pompeii happened in the 1st 
century. 
       b. Brutus stabbed Caesar. That was a pivotal event 
in history. 
 
PropBank I annotations can be translated straightfor-
wardly into logical representations with event variables, 
as illustrated in (5), with relations being defined as 
predicates of events, and Args and ArgMs representing 
relations between event variables and corresponding 
phrases.   
 
(5) a. Mr. Bush met him privately, in the White House,          
on Thursday. 
 
     b. PropBank annotation 
  Rel:  met                      
       Arg0: Mr. Bush  
       ArgM-MNR: privately 
       ArgM-LOC: in the White House 
       ArgM-TMP: on Thursday 
 
     c. Logical representation with an event variable  
 ?e meeting(e) & Arg0(e, Mr. Bush) & Arg1(e, he) 
& MNR(e, privately) & LOC(e, ?in the White 
House?) & TIME(e, ?on Thursday?) 
 
As the representation in (5c) shows, we adopt Neo-
davidsonian analysis of events, which follows Parsons 
(1990) in treating arguments on a par with modifiers in 
the event structure. An alternative analysis is the origi-
nal Davidsonian analysis of events (Davidson 1967), 
where the arguments of the verb are analyzed as its 
logical arguments. 
  
Our choice of a Neodavidsonian representation is moti-
vated by its predictions with respect to obligatoriness of 
arguments. Under the Davidsonian approach, arguments 
are logical arguments of the verb and thus must be im-
plied by the meaning of the sentence, either explicitly or 
implicitly (i.e. existentially quantified). On the other 
hand, it has been a crucial assumption in PropBank that 
not all roles must necessarily be present in each sen-
tence. For example, the Frameset for the verb serve, 
shown in (6a) has three roles: Arg0, Arg1, and Arg2. 
Actual usages of the verb, on the other hand, do not 
require the presence of all three roles. For example, the 
sentence in (6b), as its PropBank annotation in (6c) 
shows, does not include Arg1. 
 
(6)  a.  serve.01 "act, work":            
Arg0:worker 
Arg1:job, project 
Arg2:employer 
 
     b.  Each new trading roadblock is likely to be beaten     
by institutions seeking better ways *trace* to serve 
their high-volume clients.  
 
c. Arg0:  *trace* -> institutions 
     REL:    serve 
     Arg2:   their high-volume clients 
 
As the representations in (7) illustrate, only the Neo-
davidsonian representation gives the correct interpreta-
tion of this sentence. 
 
(7) Davidsonian representation: 
 ?e ?z serve(e, institutions, z, their high-volume 
clients) 
 
Neodavidsonian representation: 
     ?e serve(e)&Arg0(e, institutions)&Arg2(e, their 
high-volume clients) 
 
Assuming a Neodavidsonian representation, we can 
analyze all Args and certain types of modifiers as predi-
cates of events.  The types of ArgMs that can be ana-
lyzed as predicates of event variables are shown below: 
 
? MNR:   to manage businesses profitably 
? TMP:    to run the company for 23 years 
? LOC:    to use the notes on the test 
? DIR:     to jump up 
? CAU:    because of ? 
? PRP:      in order to ? 
 
Whereas for the most part, translating these adverbials 
into modifiers of event variables does not require man-
ual annotation, certain constructions need human revi-
sion. For example, in the sentence in (8a) the temporal 
ArgM ?for the past five years? does not modify the event 
variable e introduced by the verb manage, as our auto-
matic translation would predict. The revised analysis of 
this sentence, given in (8b), follows Krifka 1989, who 
proposed that negated sentences refer to maximal events 
? events that have everything that happened during their 
running time as a part. Annotation of this sentence 
would thus require us to introduce an additional event 
variable, the maximal event e?, which has a duration 
?for the past five years? and has no event of unions 
managing wage increases as part. 
 
(8)  a. For the past five years, unions have not managed 
to win wage increases. 
       b. ?e? TMP(e?, ?for the past five years?) &  
??e(e<e? & managing(e) & Arg0(e, unions) & 
Arg1(e, ?win wage increases?)) 
 
Further annotation involves linking empty categories in 
PropBank to event variables in cases of control, as illus-
trated in (9), where event variables can be viewed as the 
appropriate antecedents for PRO, marked as ?*? below: 
    
(9) The car collided with a lorry, * killing both drivers. 
 
And, finally, we will consider tagging variables accord-
ing to the aspectual class of the eventuality they denote, 
such as states or events. Events, such as John built a 
house, involve some kind of change and usually imply 
that some condition, which obtains when the event be-
gins, is terminated by the event. States, on the other 
hand, do not involve any change and hold for varying 
amounts of time. It does not make sense to ask how long 
a state took (as opposed to events), and whether the 
state is culminated or finished.  
 
This distinction between states and events plays an im-
portant role for the temporal analysis of discourse, as 
the following examples (from Kamp and Reyle 1993) 
illustrate: 
 
(10) a. A man entered the White Hart. Bill served him a 
beer. 
       b. I arrived at the Olivers? cottage on Friday night. 
It was not a propitious beginning to my visit. She 
was ill and he in a foul mood. 
 
If a non-initial sentence denotes an event, then it is typi-
cally understood as following the event described by the 
preceding sentence. For example, in (10a), the event of 
Bill serving a beer is understood as taking place after 
the event of ?a man entering the White Hart? was com-
pleted.  On the other hand, states are interpreted as tem-
porally overlapping with the time of the preceding 
sentence, as illustrated in (10b). The sentences she was 
ill and he was in a foul mood seem to describe a state of 
affairs obtaining at the time of the speaker?s arrival.  
 
As this example illustrates, there are different types of 
temporal relations between eventualities (as we will call 
both events and states) and adverbials that modify them, 
such as temporal overlap and temporal containment. 
Furthermore, these relations crucially depend on the 
aspectual properties of the sentence. Translation of PB 
annotations to logical representations with eventuality 
variables and tagging these variables according to their 
aspectual type would thus make it possible to provide an 
analysis of temporal relations. This analysis should also 
be compatible with a higher level of annotation of tem-
poral structure (e.g. Ferro et al 2001).   
4 Annotation of Nominal Coreference 
Our approach to coreference annotation is based on the 
recognition of the different types of relationships that 
might be called "coreference".  The most straightfor-
ward case is that of two semantically definite NPs that 
refer to identical entities, as in (11).  Anaphoric rela-
tions (very broadly defined) are those in which one NP 
(or possessive adjective) has no referential value of its 
own but depends on an antecedent for its interpretation. 
In some cases this can be relatively simple, as in (12), in 
which the pronoun He takes John Smith as its antece-
dent.  However, in some cases, as in (13), the antecedent 
may not even be a referring expression, or can, as in 
(14), refer to an entity that may or may not exist, with 
the non-existent a car being the antecedent of it.  The 
anaphor does not have to be an NP, as in (15), in which 
the possessive their, which takes many companies as its 
antecedent, is an adjective. 
 
(11) John Smith of Company X arrived yesterday.  Mr. 
Smith said that..." 
(12) John Smith of Company X arrived yesterday.  He 
said that..." 
(13) No team spoke about its system. 
(14) I want to buy a car.  I need it to go to work. 
(15) Many companies raised their payouts by more than 
10%. 
 
Another level of complexity is raised by NPs that are 
not anaphors, in that they have their own reference (per-
haps abstract or nonexistent), but are not in an identity 
relationship with an antecedent, but rather describe a 
property of that antecedent.  Typical cases of this are 
predicate nominals, as in (16), or appositives, as in (17), 
and other cases as in (18). 
 
(16) Larry is a university lecturer. 
(17) Larry, the chair of his department, became presi-
dent. 
(18) The stock price fell from $4.02 to $3.85 
 
As has been discussed (e.g., van Deemter & Kibble, 
2001), such cases have fundamentally different proper-
ties than either the identity relationships of (11) or the 
anaphoric relationships of (12)-(15).   
 
Annotation of nominal co-reference is being done in 
two passes. The first pass involves annotation of true 
co-reference between semantically definite NPs`. The 
issue here is to consider what the semantically definite 
nouns are.  Initially, they are defined as proper nouns 
(named entities), either as NPs (America) or prenominal 
adjectives (American politicians).   
 
(19) The last time the S&P 500 yield dropped below 3% 
was in the summer of 1987... There have been only 
seven other times when the yield on the S&P 500 
dropped....   
 
It is reasonable to expand this to definite descriptions, 
so that in (19), the S&P 500 yield and the yield on the 
S&P 500 are marked as coreferring.  However, some 
definite NPs can refer to clauses, not NPs, such as The 
pattern in (20), and we will not do such cases of clausal 
antecedents on the first pass. 
 
(20) The index fell 40% in 1975 and jumped 80% in 
1976.  The pattern is an unusual one. 
 
Anaphoric relations are being done on a "need-to-
annotate" basis.   For each anaphoric NP or possessive 
adjective, the annotator needs to determine its antece-
dent.  As discussed, this is a different type of relation 
than identity, and this distinction will be noted in the 
annotation. The issue here is what we consider an ana-
phoric element to be.  We consider all cases of pro-
nouns, possessives, reflexives, and NPs with that/those 
to be potential cases of anaphors (again, broadly de-
fined). However, as with definite NPs, we only mark 
those that have an NP antecedent, and not clausal ante-
cedents.  For example, in (21), it refers to the current 
3.3% reading, and so would be marked as being in an 
antecedent-anaphor relation. In (22), it refers to having 
the dividend increases, which is not an NP, and so 
would not be marked as being in an anaphor relation in 
the first pass. Similar considerations apply to potential 
anaphors like those NP, that NP, etc. 
 
 (21) ...the current 3.3% reading isn't as troublesome as 
it might have been. 
(22) Having the dividend increases is a supportive ele-
ment in the market outlook, but I don't think it's a 
main consideration". 
 
Note that placing the burden on the anaphors to deter-
mine what gets marked as being in an anaphor-
antecedent leaves it open as to what the antecedent 
might be, other than the requirement just mentioned of it 
being an NP.  Not only might it be non-referring NPs as 
in  (13) or (14), it could even be a generic, as in (23), in 
which books is the antecedent for they. 
 
(23) I like books.  They make me smile. 
 
The second pass will tackle the more difficult issues: 
 
1. Descriptive NPs, as in (16)-(18).  While the informa-
tion provided by these cases would be extremely valu-
able for information extraction and other systems, there 
are some uncertain issues here, mostly focusing on how 
such descriptors describe the antecedent at different 
moments in time and/or space.  The crucial question is 
therefore what to take the descriptor to be.   
 
(24) Henry Higgins might become the president of 
Dreamy Detergents. 
 
For example, in (18), it can't be just $4.02 and $3.85, 
since this does not include information about *when* 
the stock price had such values. The same issue arises 
for (17).  As van Deemter & Kibble point out, such 
cases can interact with issues of modality in uncertain 
ways, as illustrated in (24).  Just saying that in (24) the 
president of Dreamy Detergents is in the same type of 
relationship with Henry Higgins as a university lecturer 
is with Larry in (16) would be very misleading. 
 
2. Clausal antecedents - Here we will handle cases of it 
and other anaphor elements and definite NPs referring 
to non-NPs as antecedents, as in (21).  This will most 
likely be done by referring to the eventuality variable 
associated with the antecedent. 
5 Linking to the Penn Discourse Treebank 
(PDTB)  
The Penn Discourse Treebank (PDTB) is currently be-
ing built by the PDTB team at the University of Penn-
sylvania, providing the next appropriate level of  
annotation: the annotation of the predicate argument 
structure of connectives (Miltsakaki et al2004a/b). The 
PDTB project is based on the idea that discourse con-
nectives can be thought of as predicates with their asso-
ciated argument structure. This perspective of discourse 
is based on a series of papers extending lexicalized tree-
adjoining grammar (LTAG) to discourse (DLTAG), 
beginning with Webber and Joshi (1998).2  This level of 
annotation is quite complex for a variety of reasons, 
such as the lack of available literature describing dis-
course connectives and frequent occurrences of empty 
(lexically null) connectives between two sentences that 
cannot be ignored. Also, unlike the predicates at the 
sentence level, some of the discourse connectives, espe-
cially discourse adverbials, take their arguments ana-
phorically and not structurally, requiring an intimate 
association with event variable representation.  
 
The long-range goal of the PDTB project is to develop a 
large scale and reliably annotated corpus that will en-
code coherence relations associated with discourse con-
nectives, including their argument structure and 
anaphoric links, thus exposing a clearly defined level of 
discourse structure and supporting the extraction of a 
range of inferences associated with discourse connec-
tives. This annotation will reference the Penn Treebank 
(PTB) annotations as well as PropBank. 
 
In PDTB, a variety of connectives are considered, such 
as subordinate and coordinate conjunctions, adverbial 
connectives and implicit connectives amounting to a 
total of approximately 20,000 annotations; 10,000 im-
                                                          
2 The PDTB annotations are deliberately kept independ-
ent of DLTAG framework for two reasons: (1) to make the 
annotated corpus widely useful to researchers working in 
different frameworks and (2) to make the annotation task 
easier, thereby increasing interannotator reliability. 
plicit connectives and 10,000 annotations of the 250 
explicit connectives identified in the corpus (for details 
see (Miltsakaki et al2004a and Miltsakaki et al2004b).  
Current annotations in PDTB are performed by four 
annotators. Individual annotation proceeds one connec-
tive at a time. This way, the annotators quickly gain 
experience with that connective and develop a better 
understanding of its predicate-argument characteristics. 
For the annotation of implicit connectives, the annota-
tors are required to provide an explicit connective that 
best expressed the inferred relation. 
 
The PDTB is expected to be released by November 
2005. The final version of the corpus  will also contain 
characterizations of the semantic roles associated with 
the arguments of each type of connective as well as 
links to PropBank. 
6.   Annotation of Word Senses 
The critical question with respect to sense tagging in-
volves the choice of senses.  In other words, which 
sense inventory, and which level of granularity with 
respect to that sense inventory?  The PropBank Frames 
Files for the verbs include coarse-grained sense distinc-
tions based primarily on usages of a verb that have dif-
ferent numbers of predicate-arguments. These are 
termed Framesets ? referring to the set of roles for each 
one and the corresponding set of syntactic frames.  We 
are currently sense-tagging the annotated predicates for 
lemmas with multiple Framesets, which can be done 
quickly and accurately with an inter-annotator agree-
ment of over 90%.  The distinctions made by the 
Framesets are very coarse, and each one would map to 
several standard dictionary entries for the lemma in 
question.  More fine-grained sense distinctions could be 
useful for Automatic Content Extraction, yet it remains 
to be determined exactly which distinctions are neces-
sary and what methodology should be followed to pro-
vide additional word sense annotation. 
 
Palmer et al(2004b) present an hierarchical approach to 
verb senses, where different levels of sense distinctions, 
from PropBank Framesets to WordNet senses, form a 
continuum of granularity. At the intermediate level of 
sense hierarchy we are considering manual groupings of 
the SENSEVAL-2 verb senses (Palmer, et.al., 2004a), 
developed in a separate project. Given a large disagree-
ment rate between annotators (average inter-annotator 
agreement rate for Senseval-2 verbs was only 71%), 
verbs were grouped by two or more people into sets of 
closely related senses, with grouping differences being 
reconciled, and the sense groups were used for coarse-
grained scoring of the systems. These groupings of 
WordNet senses were shown to reconcile a substantial 
portion of the manual and automatic tagging disagree-
ments, showing that many of these disagreements are 
fairly subtle.  Using the groups as a more coarse-grained 
set of sense distinctions improved ITA and system 
scores by almost 10%, to 82% and 69%, respectively 
(Palmer, et. al. 2004a). 
 
We have been investigating whether or not the groups 
can provide an intermediate level of hierarchy in be-
tween the PropBank Framesets and the WN 1.7 senses.  
Based on our existing WN 1.7 tags and Frameset tags of 
the Senseval2 verbs in the Penn Treebank, 95% of the 
verb instances map directly from sense groups to 
Framesets, with each Frameset typically corresponding 
to two or more sense groups. Using the PropBank 
coarse-grained senses as a starting place, and WordNet 
sense tagging for over 1000 verbs produced automati-
cally through mapping VerbNet to PropBank (Kipper, 
et. al., 2004), we have the makings of a large scale tag-
ging experiment on the Penn Treebank.  This will en-
able investigations into the applicability of clearly 
defined criteria for sense distinctions at varying levels 
of granularity, and produce a large, 1M word corpus of 
sense-tagged text for training WSD systems 
 
The hierarchical approach to verb senses, as utilized by 
most standard dictionaries as well as Hector (Atkins, 
?93), and as applied to SENSEVAL-2, presents obvious 
advantages for the problem of Word Sense Disambigua-
tion. The human annotation task is simplified, since 
there are fewer choices at each level and clearer distinc-
tions between them.  The automated systems can com-
bine training data from closely related senses to 
overcome the sparse data problem, and both humans 
and systems can back off to a more coarse-grained 
choice when fine-grained choices prove too difficult.  
 
Conclusion 
This paper has presented an overview of the second 
phase of PropBank Annotation, PropBank II, which is 
being applied to English and Chinese. It  includes (Neo-
davidsonian) eventuality variables, nominal references, 
an hierarchical approach to sense tagging, and connec-
tions to the Penn Discourse Treebank (PDTB), a project 
for annotating discourse connectives and their argu-
ments. 
References 
 
Atkins, S. (1993) Tools for computer-aided corpus lexi-
cography: The Hector Project.  Actu Linguistica Hunguricu, 
41:5-72. 
 
Carlson, L., Marcu, D. and Okurowski, M. E. (2002). 
Building a Discourse-Tagged Corpus in the Framework of 
Rhetorical Structure Theory. In Current Directions in Dis-
course and Dialogue, Jan van Kuppevelt and Ronnie Smith 
eds., Kluwer Academic Publishers. To appear. 
 
Davidson, D. 1967.  The Logical Form of Action Sen-
tences. In The Logic of Decision and Action, ed. Nicholas 
Rescher.  81--95. Pittsburgh: University of Pittsburgh 
Press.  Republished in Donald Davidson, Essays on Actions 
and Events,Oxford University Press, Oxford, 1980. 
 
Edmonds, P. and Cotton, S. 2001. SENSEVAL-2: Over-
view. In Proceedings of SENSEVAL-2: Second International 
Workshop on Evaluating Word Sense Disambiguation Sys-
tems, ACL-SIGLEX, Toulouse, France. 
 
Ferro L, I. Mani, B. Sundheim and G.Wilson 2001 TIDES 
Temporal Annotation Guidelines, MITRE Technical Report, 
MTR 01W0000041.  
 
Kamp, H. and U.Reyle. 1993. From Discourse to Logic, 
Kluwer, Dordrecht. 
 
Kingsbury, P. and Palmer, M, (2002), From TreeBank to 
PropBank, Third International Conference on Language  Re-
sources and Evaluation, LREC-02, Las Palmas, Canary Is-
lands, Spain, May 28- June 3. 
 
Kilgarriff, A. and Palmer, M.. 2000. Introduction to the 
special issue on Senseval, Computers and the Humanities, 
34(1-2):1-13. 
 
Kipper K., B. Snyder, and M. Palmer. (to appear, 2004) 
"Extending a verb-lexicon using a semantically annotated 
corpus". Proceedings of the 4th International Conference on 
Language Resources and Evaluation (LREC-04). Lisbon, 
Portugal, 2004. 
 
Krifka, M. 1989. Nominalreferenz und Zeitkonstitution. 
M?nchen, Wilhelm Fink Verlag 
 
Levin, B. 1993. English Verb Classes and Alternations: a 
Preliminary Investigation. Chicago: The University of Chi-
cago Press. 
 
Mann, W. and S. Thompson. 1986. ?Relational Proposi-
tions in Discourse?, Discourse Processes 9, 57-90. 
Marcu, D. 2000. The Theory and Practice of Discourse 
Parsing and Summarization. The MIT Press. 
Miltsakaki, E., R. Prasad, A. Joshi and B. Webber. 2004a. 
The Penn Discourse Treebank. In Proceedings of the 4th In-
ternational Conference on Language Resources and Evalua-
tion (LREC 2004), Lisbon. 
 
Miltsakaki, E., R. Prasad, A. Joshi and B. Webber. 2004b. 
Annotation of Discourse Connectives and Their Arguments, in 
Proceedings of the HLT-EACL Workshop on Frontiers in 
Corpus Annotation, Boston, Massachussetts. 
      Palmer, M., Dang, H. T, and Fellbaum, C., 2004a.  Making 
fine-grained and coarse-grained sense distinctions, both manu-
ally and automatically, under revision for Natural Language 
Engineering. 
Palmer, M., Babko-Malaya, O., Dang, H. T., 2004b. Dif-
ferent Sense Granularities for Different Applications, to ap-
pear in the Scalable Natural Language Understanding 
Workshop, held in conjunction with HLT/NAACL-04, May, 
2004. 
 
Parsons, T. 1990.  Events in the Semantics of Eng-
lish.  Cambridge, MA: MIT Press. 
  
van Deemter, K. and R. Kibble. 2000. ?On Coreferring: 
Coreference in MUC and Related Annotation Schemes?, 
Computational Linguistics 26:629-637. 
 
Webber B. and A. Joshi. 1998. Anchoring a lexicalized 
tree-adjoining grammar for discourse. In ACL/COLING 
Workshop on Discourse Relations and Discourse Markers, 
Montreal, Canada, pp. 41-48. 
 
 
Xue, N. and Palmer, M. 2003. Annotating the Propositions 
in the Penn Chinese Treebank. In the Proceedings of the Sec-
ond SIGHAN Workshop on Chinese Language Processing.  
Sapporo, Japan. 
  
Xue, Nianwen, Xia, Fei, Chiou, Fu-dong and Palmer, 
Martha. 2004. The Penn Chinese Treebank: phrase structure 
annotation of a large corpus. Natural Language Engineering, 
10(4):1-30, June 2004.  
 
 
Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 29?36,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Attribution and the (Non-)Alignment of Syntactic and Discourse Arguments
of Connectives
Nikhil Dinesh and Alan Lee and Eleni Miltsakaki and Rashmi Prasad and Aravind Joshi
University of Pennsylvania
Philadelphia, PA 19104 USA
fnikhild,aleewk,elenimi,rjprasad,joshig@linc.cis.upenn.edu
Bonnie Webber
University of Edinburgh
Edinburgh, EH8 9LW Scotland
bonnie@inf.ed.ac.uk
Abstract
The annotations of the Penn Discourse
Treebank (PDTB) include (1) discourse
connectives and their arguments, and (2)
attribution of each argument of each con-
nective and of the relation it denotes. Be-
cause the PDTB covers the same text as
the Penn TreeBank WSJ corpus, syntac-
tic and discourse annotation can be com-
pared. This has revealed significant dif-
ferences between syntactic structure and
discourse structure, in terms of the argu-
ments of connectives, due in large part to
attribution. We describe these differences,
an algorithm for detecting them, and fi-
nally some experimental results. These re-
sults have implications for automating dis-
course annotation based on syntactic an-
notation.
1 Introduction
The overall goal of the Penn Discourse Treebank
(PDTB) is to annotate the million word WSJ cor-
pus in the Penn TreeBank (Marcus et al, 1993) with
a layer of discourse annotations. A preliminary re-
port on this project was presented at the 2004 work-
shop on Frontiers in Corpus Annotation (Miltsakaki
et al, 2004a), where we described our annotation
of discourse connectives (both explicit and implicit)
along with their (clausal) arguments.
Further work done since then includes the an-
notation of attribution: that is, who has expressed
each argument to a discourse connective (the writer
or some other speaker or author) and who has ex-
pressed the discourse relation itself. These ascrip-
tions need not be the same. Of particular interest is
the fact that attribution may or may not play a role
in the relation established by a connective. This may
lead to a lack of congruence between arguments at
the syntactic and the discourse levels. The issue of
congruence is of interest both from the perspective
of annotation (where it means that, even within a
single sentence, one cannot merely transfer the an-
notation of syntactic arguments of a subordinate or
coordinate conjunction to its discourse arguments),
and from the perspective of inferences that these an-
notations will support in future applications of the
PDTB.
The paper is organized as follows. We give a brief
overview of the annotation of connectives and their
arguments in the PDTB in Section 2. In Section 3,
we describe the annotation of the attribution of the
arguments of a connective and the relation it con-
veys. In Sections 4 and 5, we describe mismatches
that arise between the discourse arguments of a con-
nective and the syntactic annotation as provided by
the Penn TreeBank (PTB), in the cases where all the
arguments of the connective are in the same sen-
tence. In Section 6, we will discuss some implica-
tions of these issues for the theory and practice of
discourse annotation and their relevance even at the
level of sentence-bound annotation.
2 Overview of the PDTB
The PDTB builds on the DLTAG approach to dis-
course structure (Webber and Joshi, 1998; Webber
et al, 1999; Webber et al, 2003) in which con-
nectives are discourse-level predicates which project
predicate-argument structure on a par with verbs at
29
the sentence level. Initial work on the PDTB has
been described in Miltsakaki et al (2004a), Milt-
sakaki et al (2004b), Prasad et al (2004).
The key contribution of the PDTB design frame-
work is its bottom-up approach to discourse struc-
ture: Instead of appealing to an abstract (and arbi-
trary) set of discourse relations whose identification
may confound multiple sources of discourse mean-
ing, we start with the annotation of discourse con-
nectives and their arguments, thus exposing a clearly
defined level of discourse representation.
The PDTB annotates as explicit discourse connec-
tives all subordinating conjunctions, coordinating
conjunctions and discourse adverbials. These pred-
icates establish relations between two abstract ob-
jects such as events, states and propositions (Asher,
1993).1
We use Conn to denote the connective, and Arg1
and Arg2 to denote the textual spans from which the
abstract object arguments are computed.2 In (1), the
subordinating conjunction since establishes a tem-
poral relation between the event of the earthquake
hitting and a state where no music is played by a
certain woman. In all the examples in this paper, as
in (1), Arg1 is italicized, Arg2 is in boldface, and
Conn is underlined.
(1) She hasn?t played any music since the earthquake
hit.
What counts as a legal argument? Since we take
discourse relations to hold between abstract objects,
we require that an argument contains at least one
clause-level predication (usually a verb ? tensed or
untensed), though it may span as much as a sequence
of clauses or sentences. The two exceptions are
nominal phrases that express an event or a state, and
discourse deictics that denote an abstract object.
1For example, discourse adverbials like as a result are dis-
tinguished from clausal adverbials like strangely which require
only a single abstract object (Forbes, 2003).
2Each connective has exactly two arguments. The argument
that appears in the clause syntactically associated with the con-
nective, we call Arg2. The other argument is called Arg1. Both
Arg1 and Arg2 can be in the same sentence, as is the case for
subordinating conjunctions (e.g., because). The linear order of
the arguments will be Arg2 Arg1 if the subordinate clause ap-
pears sentence initially; Arg1 Arg2 if the subordinate clause ap-
pears sentence finally; and undefined if it appears sentence me-
dially. For an adverbial connective like however, Arg1 is in the
prior discourse. Hence, the linear order of its arguments will be
Arg1 Arg2.
Because our annotation is on the same corpus as
the PTB, annotators may select as arguments textual
spans that omit content that can be recovered from
syntax. In (2), for example, the relative clause is
selected as Arg1 of even though, and its subject can
be recovered from its syntactic analysis in the PTB.
In (3), the subject of the infinitival clause in Arg1 is
similarly available.
(2) Workers described ?clouds of blue dust? that hung
over parts of the factory even though exhaust fans
ventilated the air.
(3) The average maturity for funds open only to institu-
tions, considered by some to be a stronger indicator
because those managers watch the market closely,
reached a high point for the year ? 33 days.
The PDTB also annotates implicit connectives be-
tween adjacent sentences where no explicit connec-
tive occurs. For example, in (4), the two sentences
are contrasted in a way similar to having an explicit
connective like but occurring between them. Anno-
tators are asked to provide, when possible, an ex-
plicit connective that best describes the relation, and
in this case in contrast was chosen.
(4) The $6 billion that some 40 companies are looking to
raise in the year ending March 21 compares with only
$2.7 billion raise on the capital market in the previous
year. IMPLICIT - in contrast In fiscal 1984, before
Mr. Gandhi came into power, only $810 million
was raised.
When complete, the PDTB will contain approxi-
mately 35K annotations: 15K annotations of the 100
explicit connectives identified in the corpus and 20K
annotations of implicit connectives.3
3 Annotation of attribution
Wiebe and her colleagues have pointed out the
importance of ascribing beliefs and assertions ex-
pressed in text to the agent(s) holding or making
them (Riloff and Wiebe, 2003; Wiebe et al, 2004;
Wiebe et al, 2005). They have also gone a consid-
erable way towards specifying how such subjective
material should be annotated (Wiebe, 2002). Since
we take discourse connectives to convey semantic
predicate-argument relations between abstract ob-
jects, one can distinguish a variety of cases depend-
ing on the attribution of the discourse relation or its
3The annotation guidelines for the PDTB are available at
http://www.cis.upenn.edu/pdtb.
30
arguments; that is, whether the relation or arguments
are ascribed to the author of the text or someone
other than the author.
Case 1: The relation and both arguments are at-
tributed to the same source. In (5), the concessive
relation between Arg1 and Arg2, anchored on the
connective even though is attributed to the speaker
Dick Mayer, because he is quoted as having said
it. Even where a connective and its arguments are
not included in a single quotation, the attribution can
still be marked explicitly as shown in (6), where only
Arg2 is quoted directly but both Arg1 and Arg2 can
be attibuted to Mr. Prideaux. Attribution to some
speaker can also be marked in reported speech as
shown in the annotation of so that in (7).
(5) ?Now, Philip Morris Kraft General Foods? parent
company is committed to the coffee business and to
increased advertising for Maxwell House,? says Dick
Mayer, president of the General Foods USA division.
?Even though brand loyalty is rather strong for cof-
fee, we need advertising to maintain and strengthen
it.?
(6) B.A.T isn?t predicting a postponement because the
units ?are quality businesses and we are en-
couraged by the breadth of inquiries,? said Mr.
Prideaux.
(7) Like other large Valley companies, Intel also noted
that it has factories in several parts of the nation,
so that a breakdown at one location shouldn?t leave
customers in a total pinch.
Wherever there is a clear indication that a relation
is attributed to someone other than the author of the
text, we annotate the relation with the feature value
SA for ?speaker attribution? which is the case for
(5), (6), and (7). The arguments in these examples
are given the feature value IN to indicate that they
?inherit? the attribution of the relation. If the rela-
tion and its arguments are attributed to the writer,
they are given the feature values WA and IN respec-
tively.
Relations are attributed to the writer of the text by
default. Such cases include many instances of re-
lations whose attribution is ambiguous between the
writer or some other speaker. In (8), for example,
we cannot tell if the relation anchored on although
is attributed to the spokeswoman or the author of the
text. As a default, we always take it to be attributed
to the writer.
Case 2: One or both arguments have a different at-
tribution value from the relation. While the default
value for the attribution of an argument is the attribu-
tion of its relation, it can differ as in (8). Here, as in-
dicated above, the relation is attributed to the writer
(annotated WA) by default, but Arg2 is attributed to
Delmed (annotated SA, for some speaker other than
the writer, and other than the one establishing the
relation).
(8) The current distribution arrangement ends in March
1990 , although Delmed said it will continue to pro-
vide some supplies of the peritoneal dialysis prod-
ucts to National Medical, the spokeswoman said.
Annotating the corpus with attribution is neces-
sary because in many cases the text containing the
source of attribution is located in a different sen-
tence. Such is the case for (5) where the relation
conveyed by even though, and its arguments are at-
tributed to Dick Mayer.
We are also adding attribution values to the anno-
tation of the implicit connectives. Implicit connec-
tives express relations that are inferred by the reader.
In such cases, the author intends for the reader to
infer a discourse relation. As with explicit connec-
tives, we have found it useful to distinguish implicit
relations intended by the writer of the article from
those intended by some other author or speaker. To
give an example, the implicit relation in (9) is at-
tributed to the writer. However, in (10) both Arg1
and Arg2 have been expressed by the speaker whose
speech is being quoted. In this case, the implicit re-
lation is attributed to the speaker.
(9) Investors in stock funds didn?t panic the week-
end after mid-October?s 190-point market plunge.
IMPLICIT-instead Most of those who left stock
funds simply switched into money market funds.
(10) ?People say they swim, and that may mean they?ve
been to the beach this year,? Fitness and Sports. ?It?s
hard to know if people are responding truthfully.
IMPLICIT-because People are too embarrassed to
say they haven?t done anything.?
The annotation of attribution is currently under-
way. The final version of the PDTB will include an-
notations of attribution for all the annotated connec-
tives and their arguments.
Note that in the Rhetorical Structure Theory
(RST) annotation scheme (Carlson et al, 2003), at-
tribution is treated as a discourse relation. We, on
the other hand, do not treat attribution as a discourse
31
relation. In PDTB, discourse relations (associated
with an explicit or implicit connective) hold between
two abstracts objects, such as events, states, etc. At-
tribution relates a proposition to an entity, not to an-
other proposition, event, etc. This is an important
difference between the two frameworks. One conse-
quence of this difference is briefly discussed in Foot-
note 4 in the next section.
4 Arguments of Subordinating
Conjunctions in the PTB
A natural question that arises with the annotation
of arguments of subordinating conjunctions (SUB-
CONJS) in the PDTB is to what extent they can be
detected directly from the syntactic annotation in the
PTB. In the simplest case, Arg2 of a SUBCONJ is its
complement in the syntactic representation. This is
indeed the case for (11), where since is analyzed as
a preposition in the PTB taking an S complement
which is Arg2 in the PDTB, as shown in Figure 1.
(11) Since the budget measures cash flow, a new $1 di-
rect loan is treated as a $1 expenditure.
Furthermore, in (11), since together with its com-
plement (Arg2) is analyzed as an SBAR which mod-
ifies the clause a new $1 direct loan is treated as a
$1 expenditure, and this clause is Arg1 in the PDTB.
Can the arguments always be detected in this
way? In this section, we present statistics showing
that this is not the case and an analysis that shows
that this lack of congruence between the PDTB and
the PTB is not just a matter of annotator disagree-
ment.
Consider example (12), where the PTB requires
annotators to include the verb of attribution said
and its subject Delmed in the complement of al-
though. But although as a discourse connective de-
nies the expectation that the supply of dialysis prod-
ucts will be discontinued when the distribution ar-
rangement ends. It does not convey the expectation
that Delmed will not say such things. On the other
hand, in (13), the contrast established by while is be-
tween the opinions of two entities i.e., advocates and
their opponents.4
4This distinction is hard to capture in an RST-based pars-
ing framework (Marcu, 2000). According to the RST-based an-
notation scheme (Carlson et al, 2003) ?although Delmed said?
and ?while opponents argued? are elementary discourse units
(12) The current distribution arrangement ends in March
1990, although Delmed said it will continue to pro-
vide some supplies of the peritoneal dialysis prod-
ucts to National Medical, the spokeswoman said.
(13) Advocates said the 90-cent-an-hour rise, to $4.25 an
hour by April 1991, is too small for the working poor,
while opponents argued that the increase will still
hurt small business and cost many thousands of
jobs.
In Section 5, we will identify additional cases. What
we will then argue is that it will be insufficient to
train an algorithm for identifying discourse argu-
ments simply on the basis of syntactically analysed
text.
We now present preliminary measurements of
these and other mismatches between the two corpora
for SUBCONJS. To do this we describe a procedural
algorithm which builds on the idea presented at the
start of this section. The statistics are preliminary in
that only the annotations of a single annotator were
considered, and we have not attempted to exclude
cases in which annotators disagree.
We consider only those SUBCONJS for which both
arguments are located in the same sentence as the
connective (which is the case for approximately 99%
of the annotated instances). The syntactic configura-
tion of such relations pattern in a way shown in Fig-
ure 1. Note that it is not necessary for any of Conn,
Arg1, or Arg2 to have a single node in the parse tree
that dominates it exactly. In Figure 1 we do obtain a
single node for Conn, and Arg2 but for Arg1, it is
the set of nodes fNP; V Pg that dominate it exactly.
Connectives like so that, and even if are not domi-
nated by a single node, and cases where the annota-
tor has decided that a (parenthetical) clausal element
is not minimally necessary to the interpretation of
Arg2 will necessitate choosing multiple nodes that
dominate Arg2 exactly.
Given the node(s) in the parse tree that dominate
Conn (fINg in Figure 1), the algorithm we present
tries to find node(s) in the parse tree that dominate
Arg1 and Arg2 exactly using the operation of tree
subtraction (Sections 4.1, and 4.2). We then discuss
its execution on (11) in Section 4.3.
annotated in the same way: as satellites of the relation Attribu-
tion. RST does not recognize that satellite segments, such as
the ones given above, sometimes participate in a higher RST
relation along with their nuclei and sometimes not.
32
S12
SBAR NP
A new $1 direct
loan
VP
is treated as a
$1 expenditure
IN S
2
the budget mea-
sures cash flowsince
Given N
Conn
= fINg, our goal is to find N
Arg1
=
fNP; V Pg, and N
Arg2
= fS
2
g. Steps:
 h
Conn
= IN
 x
Conn+Arg2
= SBAR  parent(h
Conn
)
 x
Conn+Arg1+Arg2
= S
12
 lowest Ancestor
parent(x
Conn+Arg2
)
with la-
bel S or SBAR. Note that x 2 Ancestor
x
 N
Arg2
= x
Conn+Arg2
 N
Conn
= SBAR  fINg
= fS
2
g
 N
Arg1
= x
Conn+Arg1+Arg2
  fx
Conn+Arg2
g
= S
12
  fSBARg
= fNP; V Pg
Figure 1: The syntactic configuration for (11), and the execution of the tree subtraction algorithm on this configuration.
4.1 Tree subtraction
We will now define the operation of tree subtraction
the graphical intuition for which is given in Figure
2. Let T be the set of nodes in the tree.
Definition 4.1. The ancestors of any node t 2 T ,
denoted by Ancestor
t
 T is a set of nodes such
that t 2 Ancestor
t
and parent(u; t) ) ([u 2
Ancestor
t
] ^ [Ancestor
u
Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 31?38,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Annotating Attribution in the Penn Discourse TreeBank
Rashmi Prasad and Nikhil Dinesh and Alan Lee and Aravind Joshi
University of Pennsylvania
Philadelphia, PA 19104 USA
 
rjprasad,nikhild,aleewk,joshi  @linc.cis.upenn.edu
Bonnie Webber
University of Edinburgh
Edinburgh, EH8 9LW Scotland
bonnie@inf.ed.ac.uk
Abstract
An emerging task in text understanding
and generation is to categorize information
as fact or opinion and to further attribute
it to the appropriate source. Corpus an-
notation schemes aim to encode such dis-
tinctions for NLP applications concerned
with such tasks, such as information ex-
traction, question answering, summariza-
tion, and generation. We describe an anno-
tation scheme for marking the attribution
of abstract objects such as propositions,
facts and eventualities associated with dis-
course relations and their arguments an-
notated in the Penn Discourse TreeBank.
The scheme aims to capture the source and
degrees of factuality of the abstract ob-
jects. Key aspects of the scheme are anno-
tation of the text spans signalling the attri-
bution, and annotation of features record-
ing the source, type, scopal polarity, and
determinacy of attribution.
1 Introduction
News articles typically contain a mixture of infor-
mation presented from several different perspec-
tives, and often in complex ways. Writers may
present information as known to them, or from
some other individual?s perspective, while further
distinguishing between, for example, whether that
perspective involves an assertion or a belief. Re-
cent work has shown the importance of recogniz-
ing such perspectivization of information for sev-
eral NLP applications, such as information extrac-
tion, summarization, question answering (Wiebe
et al, 2004; Stoyanov et al, 2005; Riloff et al,
2005) and generation (Prasad et al, 2005). Part of
the goal of such applications is to distinguish be-
tween factual and non-factual information, and to
identify the source of the information. Annotation
schemes (Wiebe et al, 2005; Wilson and Wiebe,
2005; PDTB-Group, 2006) encode such distinc-
tions to facilitate accurate recognition and repre-
sentation of such perspectivization of information.
This paper describes an extended annotation
scheme for marking the attribution of discourse re-
lations and their arguments annotated in the Penn
Discourse TreeBank (PDTB) (Miltsakaki et al,
2004; Prasad et al, 2004; Webber et al, 2005), the
primary goal being to capture the source and de-
grees of factuality of abstract objects. The scheme
captures four salient properties of attribution: (a)
source, distinguishing between different types of
agents to whom AOs are attributed, (b) type, re-
flecting the degree of factuality of the AO, (c) sco-
pal polarity of attribution, indicating polarity re-
versals of attributed AOs due to surface negated
attributions, and (d) determinacy of attribution, in-
dicating the presence of contexts canceling the en-
tailment of attribution. The scheme also describes
annotation of the text spans signaling the attri-
bution. The proposed scheme is an extension of
the core scheme used for annotating attribution
in the first release of the PDTB (Dinesh et al,
2005; PDTB-Group, 2006). Section 2 gives an
overview of the PDTB, Section 3 presents the ex-
tended annotation scheme for attribution, and Sec-
tion 4 presents the summary.
2 The Penn Discourse TreeBank (PDTB)
The PDTB contains annotations of discourse rela-
tions and their arguments on the Wall Street Jour-
nal corpus (Marcus et al, 1993). Following the
approach towards discourse structure in (Webber
et al, 2003), the PDTB takes a lexicalized ap-
31
proach towards the annotation of discourse rela-
tions, treating discourse connectives as the an-
chors of the relations, and thus as discourse-level
predicates taking two abstract objects (AOs) as
their arguments. For example, in (1), the subordi-
nating conjunction since is a discourse connective
that anchors a TEMPORAL relation between the
event of the earthquake hitting and a state where
no music is played by a certain woman. (The 4-
digit number in parentheses at the end of examples
gives the WSJ file number of the example.)
(1) She hasn?t played any music since the earthquake
hit. (0766)
There are primarily two types of connectives
in the PDTB: ?Explicit? and ?Implicit?. Explicit
connectives are identified form four grammati-
cal classes: subordinating conjunctions (e.g., be-
cause, when, only because, particularly since),
subordinators (e.g., in order that), coordinating
conjunctions (e.g., and, or), and discourse adver-
bials (e.g., however, otherwise). In the examples
in this paper, Explicit connectives are underlined.
For sentences not related by an Explicit connec-
tive, annotators attempt to infer a discourse rela-
tion between them by inserting connectives (called
?Implicit? connectives) that best convey the in-
ferred relations. For example, in (2), the inferred
CAUSAL relation between the two sentences was
annotated with because as the Implicit connective.
Implicit connectives together with their sense clas-
sification are shown here in small caps.
(2) Also unlike Mr. Ruder, Mr. Breeden appears to
be in a position to get somewhere with his agenda.
Implicit=BECAUSE (CAUSE) As a former White
House aide who worked closely with Congress, he
is savvy in the ways of Washington. (0955)
Cases where a suitable Implicit connective
could not be annotated between adjacent sentences
are annotated as either (a) ?EntRel?, where the
second sentence only serves to provide some fur-
ther description of an entity in the first sentence
(Example 3); (b) ?NoRel?, where no discourse re-
lation or entity-based relation can be inferred; and
(c) ?AltLex?, where the insertion of an Implicit
connective leads to redundancy, due to the rela-
tion being alternatively lexicalized by some ?non-
connective? expression (Example 4).
(3) C.B. Rogers Jr. was named chief executive officer of
this business information concern. Implicit=EntRel
Mr. Rogers, 60 years old, succeeds J.V. White, 64,
who will remain chairman and chairman of the ex-
ecutive committee (0929).
(4) One in 1981 raised to $2,000 a year from $1,500
the amount a person could put, tax-deductible,
into the tax-deferred accounts and widened cov-
erage to people under employer retirement plans.
Implicit=AltLex (consequence) [This caused] an ex-
plosion of IRA promotions by brokers, banks, mu-
tual funds and others. (0933)
Arguments of connectives are simply labelled
Arg2, for the argument appearing in the clause
syntactically bound to the connective, and Arg1,
for the other argument. In the examples here, Arg1
appears in italics, while Arg2 appears in bold.
The basic unit for the realization of an AO ar-
gument of a connective is the clause, tensed or un-
tensed, but it can also be associated with multiple
clauses, within or across sentences. Nominaliza-
tions and discourse deictics (this, that), which can
also be interpreted as AOs, can serve as the argu-
ment of a connective too.
The current version of the PDTB also contains
attribution annotations on discourse relations and
their arguments. These annotations, however, used
the earlier core scheme which is subsumed in the
extended scheme described in this paper.
The first release of the Penn Discourse
TreeBank, PDTB-1.0 (reported in PDTB-
Group (2006)), is freely available from
http://www.seas.upenn.edu/?pdtb.
PDTB-1.0 contains 100 distinct types of Explicit
connectives, with a total of 18505 tokens, anno-
tated across the entire WSJ corpus (25 sections).
Implicit relations have been annotated in three
sections (Sections 08, 09, and 10) for the first
release, totalling 2003 tokens (1496 Implicit
connectives, 19 AltLex relations, 435 EntRel
tokens, and 53 NoRel tokens). The corpus also
includes a broadly defined sense classification for
the implicit relations, and attribution annotation
with the earlier core scheme. Subsequent releases
of the PDTB will include Implicit relations
annotated across the entire corpus, attribution
annotation using the extended scheme proposed
here, and fine-grained sense classification for both
Explicit and Implicit connectives.
3 Annotation of Attribution
Recent work (Wiebe et al, 2005; Prasad et al,
2005; Riloff et al, 2005; Stoyanov et al, 2005),
has shown the importance of recognizing and rep-
resenting the source and factuality of information
in certain NLP applications. Information extrac-
tion systems, for example, would perform better
32
by prioritizing the presentation of factual infor-
mation, and multi-perspective question answering
systems would benefit from presenting informa-
tion from different perspectives.
Most of the annotation approaches tackling
these issues, however, are aimed at performing
classifications at either the document level (Pang
et al, 2002; Turney, 2002), or the sentence or word
level (Wiebe et al, 2004; Yu and Hatzivassiloglou,
2003). In addition, these approaches focus primar-
ily on sentiment classification, and use the same
for getting at the classification of facts vs. opin-
ions. In contrast to these approaches, the focus
here is on marking attribution on more analytic se-
mantic units, namely the Abstract Objects (AOs)
associated with predicate-argument discourse re-
lations annotated in the PDTB, with the aim of
providing a compositional classification of the fac-
tuality of AOs. The scheme isolates four key prop-
erties of attribution, to be annotated as features:
(1) source, which distinguishes between different
types of agents (Section 3.1); (2) type, which en-
codes the nature of relationship between agents
and AOs, reflecting the degree of factuality of the
AO (Section 3.2); (3) scopal polarity, which is
marked when surface negated attribution reverses
the polarity of the attributed AO (Section 3.3), and
(4) determinacy, which indicates the presence of
contexts due to which the entailment of attribu-
tion gets cancelled (Section 3.4). In addition, to
further facilitate the task of identifying attribution,
the scheme also aims to annotate the text span
complex signaling attribution (Section 3.5)
Results from annotations using the earlier attri-
bution scheme (PDTB-Group, 2006) show that a
significant proportion (34%) of the annotated dis-
course relations have some non-Writer agent as
the source for either the relation or one or both ar-
guments. This illustrates the simplest case of the
ambiguity inherent for the factuality of AOs, and
shows the potential use of the PDTB annotations
towards the automatic classification of factuality.
The annotations also show that there are a variety
of configurations in which the components of the
relations are attributed to different sources, sug-
gesting that recognition of attributions may be a
complex task for which an annotated corpus may
be useful. For example, in some cases, a rela-
tion together with its arguments is attributed to the
writer or some other agent, whereas in other cases,
while the relation is attributed to the writer, one
or both of its arguments is attributed to different
agent(s). For Explicit connectives. there were 6
unique configurations, for configurations contain-
ing more than 50 tokens, and 5 unique configura-
tions for Implicit connectives.
3.1 Source
The source feature distinguishes between (a) the
writer of the text (?Wr?), (b) some specific agent
introduced in the text (?Ot? for other), and (c)
some generic source, i.e., some arbitrary (?Arb?)
individual(s) indicated via a non-specific reference
in the text. The latter two capture further differ-
ences in the degree of factuality of AOs with non-
writer sources. For example, an ?Arb? source for
some information conveys a higher degree of fac-
tuality than an ?Ot? source, since it can be taken
to be a ?generally accepted? view.
Since arguments can get their attribution
through the relation between them, they can be an-
notated with a fourth value ?Inh?, to indicate that
their source value is inherited from the relation.
Given this scheme for source, there are broadly
two possibilities. In the first case, a relation
and both its arguments are attributed to the same
source, either the writer, as in (5), or some other
agent (here, Bill Biedermann), as in (6). (At-
tribution feature values assigned to examples are
shown below each example; REL stands for the
discourse relation denoted by the connective; At-
tribution text spans are shown boxed.)
(5) Since the British auto maker became a takeover
target last month, its ADRs have jumped about
78%. (0048)
REL Arg1 Arg2
[Source] Wr Inh Inh
(6) ?The public is buying the market when in re-
ality there is plenty of grain to be shipped,?
said Bill Biedermann  (0192)
REL Arg1 Arg2
[Source] Ot Inh Inh
As Example (5) shows, text spans for im-
plicit Writer attributions (corresponding to im-
plicit communicative acts such as I write, or I say),
are not marked and are taken to imply Writer attri-
bution by default (see also Section 3.5).
In the second case, one or both arguments have
a different source from the relation. In (7), for
example, the relation and Arg2 are attributed to
the writer, whereas Arg1 is attributed to another
agent (here, Mr. Green). On the other hand, in (8)
and (9), the relation and Arg1 are attributed to the
writer, whereas Arg2 is attributed to another agent.
33
(7) When Mr. Green won a $240,000 verdict in a land
condemnation case against the state in June 1983,
he says Judge O?Kicki unexpectedly awarded him
an additional $100,000. (0267)
REL Arg1 Arg2
[Source] Wr Ot Inh
(8) Factory orders and construction outlays were largely
flat in December while purchasing agents said
manufacturing shrank further in October. (0178)
REL Arg1 Arg2
[Source] Wr Inh Ot
(9) There, on one of his first shopping trips, Mr.
Paul picked up several paintings at stunning prices.
 Afterward, Mr. Paul is said by Mr. Guterman
to have phoned Mr. Guterman, the New York de-
veloper selling the collection, and gloated. (2113)
REL Arg1 Arg2
[Source] Wr Inh Ot
Example (10) shows an example of a generic
source indicated by an agentless passivized attri-
bution on Arg2 of the relation. Note that pas-
sivized attributions can also be associated with
a specific source when the agent is explicit, as
shown in (9). ?Arb? sources are also identified
by the occurrences of adverbs like reportedly, al-
legedly, etc.
(10) Although index arbitrage is said to add liquidity to
markets, John Bachmann,  says too much liq-
uidity isn?t a good thing. (0742)
REL Arg1 Arg2
[Source] Wr Ot Arb
We conclude this section by noting that ?Ot?
is used to refer to any specific individual as the
source. That is, no further annotation is provided
to indicate who the ?Ot? agent in the text is. Fur-
thermore, as shown in Examples (11-12), multiple
?Ot? sources within the same relation do not indi-
cate whether or not they refer to the same or differ-
ent agents. However, we assume that the text span
annotations for attribution, together with an inde-
pendent mechanism for named entity recognition
and anaphora resolution can be employed to iden-
tify and disambiguate the appropriate references.
(11) Suppression of the book, Judge Oakes observed ,
would operate as a prior restraint and thus involve
the First Amendment. Moreover, and
here Judge Oakes went to the heart of the question ,
?Responsible biographers and historians con-
stantly use primary sources, letters, diaries, and
memoranda. (0944)
REL Arg1 Arg2
[Source] Wr Ot Ot
(12) The judge was considered imperious, abrasive and
ambitious, those who practiced before him say .
Yet, despite the judge?s imperial bearing, no one
ever had reason to suspect possible wrongdoing,
says John Bognato, president of Cambria  .(0267)
REL Arg1 Arg2
[Source] Wr Ot Ot
3.2 Type
The type feature signifies the nature of the rela-
tion between the agent and the AO, leading to dif-
ferent inferences about the degree of factuality of
the AO. In order to capture the factuality of the
AOs, we start by making a three-way distinction
of AOs into propositions, facts and eventualities
(Asher, 1993). This initial distinction allows for
a more semantic, compositional approach to the
annotation and recognition of factuality. We de-
fine the attribution relations for each AO type as
follows: (a) Propositions involve attribution to an
agent of his/her (varying degrees of) commitment
towards the truth of a proposition; (b) Facts in-
volve attribution to an agent of an evaluation to-
wards or knowledge of a proposition whose truth
is taken for granted (i.e., a presupposed proposi-
tion); and (c) Eventualities involve attribution to
an agent of an intention/attitude towards an even-
tuality. In the case of propositions, a further dis-
tinction is made to capture the difference in the de-
gree of the agent?s commitment towards the truth
of the proposition, by distinguishing between ?as-
sertions? and ?beliefs?. Thus, the scheme for the
annotation of type ultimately uses a four-way dis-
tinction for AOs, namely between assertions, be-
liefs, facts, and eventualities. Initial determination
of the degree of factuality involves determination
of the type of the AO.
AO types can be identified by well-defined se-
mantic classes of verbs/phrases anchoring the at-
tribution. We consider each of these in turn.
Assertions are identified by ?assertive predi-
cates? or ?verbs of communication? (Levin, 1993)
such as say, mention, claim, argue, explain etc.
They take the value ?Comm? (for verbs of Com-
munication). In Example (13), the Ot attribution
on Arg1 takes the value ?Comm? for type. Im-
plicit writer attributions, as in the relation of (13),
also take (the default) ?Comm?. Note that when an
argument?s attribution source is not inherited (as
in Arg1 in this example) it also takes its own inde-
pendent value for type. This example thus conveys
that there are two different attributions expressed
within the discourse relation, one for the relation
and the other for one of its arguments, and that
both involve assertion of propositions.
34
(13) When Mr. Green won a $240,000 verdict in a land
condemnation case against the state in June 1983,
he says Judge O?Kicki unexpectedly awarded him
an additional $100,000. (0267)
REL Arg1 Arg2
[Source] Wr Ot Inh
[Type] Comm Comm Null
In the absence of an independent occurrence of
attribution on an argument, as in Arg2 of Exam-
ple (13), the ?Null? value is used for the type on
the argument, meaning that it needs to be derived
by independent (here, undefined) considerations
under the scope of the relation. Note that unlike
the ?Inh? value of the source feature, ?Null? does
not indicate inheritance. In a subordinate clause,
for example, while the relation denoted by the sub-
ordinating conjunction may be asserted, the clause
content itself may be presupposed, as seems to be
the case for the relation and Arg2 of (13). How-
ever, we found these differences difficult to deter-
mine at times, and consequently leave this unde-
fined in the current scheme.
Beliefs are identified by ?propositional attitude
verbs? (Hintikka, 1971) such as believe, think, ex-
pect, suppose, imagine, etc. They take the value
?PAtt? (for Propostional Attitude). An example of
a belief attribution is given in (14).
(14) Mr. Marcus believes spot steel prices will continue
to fall through early 1990 and then reverse them-
selves. (0336)
REL Arg1 Arg2
[Source] Ot Inh Inh
[Type] PAtt Null Null
Facts are identified by the class of ?factive and
semi-factive verbs? (Kiparsky and Kiparsky, 1971;
Karttunen, 1971) such as regret, forget, remember,
know, see, hear etc. They take the value ?Ftv?
(for Factive) for type (Example 15). In the current
scheme, this class does not distinguish between
the true factives and semi-factives, the former in-
volving an attitute/evaluation towards a fact, and
the latter involving knowledge of a fact.
(15) The other side , he argues knows Giuliani has al-
ways been pro-choice, even though he has personal
reservations. (0041)
REL Arg1 Arg2
[Source] Ot Inh Inh
[Type] Ftv Null Null
Lastly, eventualities are identified by a class of
verbs which denote three kinds of relations be-
tween agents and eventualities (Sag and Pollard,
1991). The first kind is anchored by verbs of influ-
ence like persuade, permit, order, and involve one
agent influencing another agent to perform (or not
perform) an action. The second kind is anchored
by verbs of commitment like promise, agree, try,
intend, refuse, decline, and involve an agent com-
mitting to perform (or not perform) an action. Fi-
nally, the third kind is anchored by verbs of ori-
entation like want, expect, wish, yearn, and in-
volve desire, expectation, or some similar mental
orientation towards some state(s) of affairs. These
sub-distinctions are not encoded in the annotation,
but we have used the definitions as a guide for
identifying these predicates. All these three types
are collectively referred to and annotated as verbs
of control. Type for these classes takes the value
?Ctrl? (for Control). Note that the syntactic term
control is used because these verbs denote uni-
form structural control properties, but the primary
basis for their definition is nevertheless semantic.
An example of the control attribution relation an-
chored by a verb of influence is given in (16).
(16) Eward and Whittington had planned to leave the bank
earlier, but Mr. Craven had persuaded them to re-
main until the bank was in a healthy position.
(1949)
REL Arg1 Arg2
[Source] Ot Inh Inh
[Type] Ctrl Null Null
Note that while our use of the term source ap-
plies literally to agents responsible for the truth of
a proposition, we continue to use the same term
for the agents for facts and eventualities. Thus,
for facts, the source represents the bearers of atti-
tudes/knowledge, and for considered eventualities,
the source represents intentions/attitudes.
3.3 Scopal Polarity
The scopal polarity feature is annotated on re-
lations and their arguments to primarily identify
cases when verbs of attribution are negated on the
surface - syntactically (e.g., didn?t say, don?t think)
or lexically (e.g., denied), but when the negation in
fact reverses the polarity of the attributed relation
or argument content (Horn, 1978). Example (17)
illustrates such a case. The ?but? clause entails an
interpretation such as ?I think it?s not a main con-
sideration?, for which the negation must take nar-
row scope over the embedded clause rather than
the higher clause. In particular, the interpretation
of the CONTRAST relation denoted by but requires
that Arg2 should be interpreted under the scope
of negation.
35
(17) ?Having the dividend increases is a supportive ele-
ment in the market outlook, but I don?t think it?s a
main consideration,? he says. (0090)
REL Arg1 Arg2
[Source] Ot Inh Inh
[Type] Comm Null PAtt
[Polarity] Null Null Neg
To capture such entailments with surface nega-
tions on attribution verbs, an argument of a con-
nective is marked ?Neg? for scopal polarity when
the interpretation of the connective requires the
surface negation to take semantic scope over the
lower argument. Thus, in Example (17), scopal
polarity is marked as ?Neg? for Arg2.
When the neg-lowered interpretations are not
present, scopal polarity is marked as the default
?Null? (such as for the relation and Arg1 of Ex-
ample 17).
It is also possible for the surface negation of at-
tribution to be interpreted as taking scope over the
relation, rather than an argument. We have not ob-
served this in the corpus yet, so we describe this
case with the constructed example in (18). What
the example shows is that in addition to entailing
(18b) - in which case it would be annotated par-
allel to Example (17) above - (18a) can also en-
tail (18c), such that the negation is intrepreted as
taking semantic scope over the ?relation? (Lasnik,
1975), rather than one of the arguments. As the
scopal polarity annotations for (18c) show, low-
ering of the surface negation to the relation is
marked as ?Neg? for the scopal polarity of the re-
lation.
(18) a. John doesn?t think Mary will get cured because
she took the medication.
b.   John thinks that because Mary took the
medication, she will not get cured.
REL Arg1 Arg2
[Source] Ot Inh Inh
[Type] PAtt Null Null
[Polarity] Null Neg Null
c.   John thinks that Mary will get cured
not because she took the medication (but be-
cause she has started practising yoga.)
REL Arg1 Arg2
[Source] Ot Inh Inh
[Type] PAtt Null Null
[Polarity] Neg Null Null
We note that scopal polarity does not capture
the appearance of (opaque) internal negation that
may appear on arguments or relations themselves.
For example, a modified connective such as not
because does not take ?Neg? as the value for sco-
pal polarity, but rather ?Null?. This is consistent
with our goal of marking scopal polarity only for
lowered negation, i.e., when surface negation from
the attribution is lowered to either the relation or
argument for interpretation.
3.4 Determinacy
The determinacy feature captures the fact that the
entailment of the attribution relation can be made
indeterminate in context, for example when it ap-
pears syntactically embedded in negated or condi-
tional contexts.. The annotation attempts to cap-
ture such indeterminacy with the value ?Indet?.
Determinate contexts are simply marked as the de-
fault ?Null?. For example, the annotation in (19)
conveys the idea that the belief or opinion about
the effect of higher salaries on teachers? perfor-
mance is not really attributed to anyone, but is
rather only being conjectured as a possibility.
(19) It is silly libel on our teachers to think they would
educate our children better if only they got a few
thousand dollars a year more. (1286)
REL Arg1 Arg2
[Source] Ot Inh Inh
[Type] PAtt Null Null
[Polarity] Null Null Null
[Determinacy] Indet Null Null
3.5 Attribution Spans
In addition to annotating the properties of attribu-
tion in terms of the features discussed above, we
also propose to annotate the text span associated
with the attribution. The text span is annotated as
a single (possibly discontinuous) complex reflect-
ing three of the annotated features, namely source,
type and scopal polarity. The attribution span also
includes all non-clausal modifiers of the elements
contained in the span, for example, adverbs and
appositive NPs. Connectives, however, are ex-
cluded from the span, even though they function
as modifiers. Example (20) shows a discontinu-
ous annotation of the attribution, where the paren-
thetical he argues is excluded from the attribution
phrase the other side knows, corresponding to the
factive attribution.
(20) The other side , he argues knows Giuliani has al-
ways been pro-choice, even though he has personal
reservations. (0041)
REL Arg1 Arg2
[Source] Ot Inh Inh
[Type] Ftv Null Null
[Polarity] Null Null Null
[Determinacy] Null Null Null
Inclusion of the fourth feature, determinacy,
is not ?required? to be included in the current
scheme because the entailment cancelling contexts
36
can	 be very complex. For example, in Exam-
ple (19), the conditional interpretation leading to
the indeterminacy of the relation and its arguments
is due to the syntactic construction type of the en-
tire sentence. It is not clear how to annotate the
indeterminacy induced by such contexts. In the
example, therefore, the attribution span only in-
cludes the anchor for the type of the attribution.
Spans for implicit writer attributions are left un-
marked since there is no corresponding text that
can be selected. The absence of a span annota-
tion is simply taken to reflect writer attribution,
together with the ?Wr? value on the source fea-
ture.
Recognizing attributions is not trivial since they
are often left unexpressed in the sentence in which
the AO is realized, and have to be inferred from the
prior discourse. For example, in (21), the relation
together with its arguments in the third sentence
are attributed to Larry Shapiro, but this attribution
is implicit and must be inferred from the first sen-
tence.
(21) ?There are certain cult wines that can command these
higher prices,? says Larry Shapiro of Marty?s, 
?What?s different is that it is happening with young
wines just coming out. We?re seeing it partly because
older vintages are growing more scarce.? (0071)
REL Arg1 Arg2
[Source] Ot Inh Inh
The spans for such implicit ?Ot? attributions
mark the text that provides the inference of the
implicit attribution, which is just the closest occur-
rence of the explicit attribution phrase in the prior
text.
The final aspect of the span annotation is that
we also annotate non-clausal phrases as the an-
chors attribution, such as prepositional phrases
like according to X, and adverbs like reportedly,
allegedly, supposedly. One such example is shown
in (22).
(22) No foreign companies bid on the Hiroshima project,
according to the bureau . But the Japanese prac-
tice of deep discounting often is cited by Ameri-
cans as a classic barrier to entry in Japan?s mar-
ket. (0501)
REL Arg1 Arg2
[Source] Wr Ot Inh
[Type] Comm Comm Null
[Polarity] Null Null Null
[Determinacy] Null Null Null
Note that adverbials are free to pick their own type
of attribution. For example, supposedly as an at-
tribution adverb picks ?PAtt? as the value for type.
3.6 Attribution of Implicit Relations
Implicit connectives and their arguments in the
PDTB are also marked for attribution. Implicit
connectives express relations that are inferred by
the reader. In such cases, the writer intends for
the reader to infer a discourse relation. As with
Explicit connectives, implicit relations intended
by the writer of the article are distinguished from
those intended by some other agent introduced by
the writer. For example, while the implicit rela-
tion in Example (23) is attributed to the writer, in
Example (24), both Arg1 and Arg2 have been
expressed by someone else whose speech is be-
ing quoted: in this case, the implicit relation is at-
tributed to the other agent.
(23) The gruff financier recently started socializing in
upper-class circles. Implicit = FOR EXAMPLE
(ADD.INFO) Although he says he wasn?t keen on go-
ing, last year he attended a New York gala where
his daughter made her debut. (0800)
REL Arg1 Arg2
[Source] Wr Inh Inh
[Type] Comm Null Null
[Polarity] Null Null Null
[Determinacy] Null Null Null
(24) ?We asked police to investigate why they are
allowed to distribute the flag in this way.
Implicit=BECAUSE (CAUSE) It should be con-
sidered against the law,?
said Danny Leish, a spokesman for the association .
REL Arg1 Arg2
[Source] Ot Inh Inh
[Type] Comm Null Null
[Polarity] Null Null Null
[Determinacy] Null Null Null
For implicit relations, attribution is also anno-
tated for AltLex relations but not for EntRel and
NoRel, since the former but not the latter refer to
the presense of discourse relations.
4 Summary
In this paper, we have proposed and described an
annotation scheme for marking the attribution of
both explicit and implicit discourse connectives
and their arguments in the Penn Discourse Tree-
Bank. We discussed the role of the annotations for
the recognition of factuality in natural language
applications, and defined the notion of attribution.
The scheme was presented in detail with exam-
ples, outlining the ?feature-based annotation? in
terms of the source, type, scopal polarity, and
determinacy associated with attribution, and the
?span annotation? to highlight the text reflecting
the attribution features.
37
Ackno


wledgements
The Penn Discourse TreeBank project is partially
supported by NSF Grant: Research Resources,
EIA 02-24417 to the University of Pennsylva-
nia (PI: A. Joshi). We are grateful to Lukasz
Abramowicz and the anonymous reviewers for
useful comments.
References
Nicholas. Asher. 1993. Reference to Abstract Objects
in Discourse. Kluwer, Dordrecht.
Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Rashmi
Prasad, Aravind Joshi, and Bonnie Webber. 2005.
Attribution and the (non)-alignment of syntactic and
discourse arguments of connectives. In Proceedings
of the ACL Workshop on Frontiers in Corpus Anno-
tation II: Pie in the Sky, Ann Arbor, Michigan.
Jaakko Hintikka. 1971. Semantics for propositional at-
titudes. In L. Linsky, editor, Reference and Modal-
ity, pages 145?167. Oxford.
Laurence Horn. 1978. Remarks on neg-raising. In
Peter Cole, editor, Syntax and Semantics 9: Prag-
matics. Academic Press, New York.
Lauri Karttunen. 1971. Some observations on factiv-
ity. Papers in Linguistics, 4:55?69.
Carol Kiparsky and Paul Kiparsky. 1971. Fact. In
D. D. Steinberg and L. A. Jakobovits, editors, Se-
mantics: An Interdisciplinary Reader in Philosophy,
Linguistics and Psychology, pages 345?369. Cam-
bridge University Press, Cambridge.
Howard Lasnik. 1975. On the semantics of nega-
tion. In Contemporary Research in Philosophi-
cal Logic and Linguistic Semantics, pages 279?313.
Dordrecht: D. Reidel.
Beth Levin. 1993. English Verb Classes And Alter-
nations: A Preliminary Investigation. University of
Chicago Press.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and
Bonnie Webber. 2004. Annotating discourse con-
nectives and their arguments. In Proceedings of the
HLT/NAACL Workshop on Frontiers in Corpus An-
notation, pages 9?16, Boston, MA.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2002), pages 79?86.
Rashmi Prasad, Eleni Miltsakaki, Aravind Joshi, and
Bonnie Webber. 2004. Annotation and data mining
of the Penn Discourse Treebank. In Proceedings of
the ACL Workshop on Discourse Annotation, pages
88?95, Barcelona, Spain.
Rashmi Prasad, Aravind Joshi, Nikhil Dinesh, Alan
Lee, Eleni Miltsakaki, and Bonnie Webber. 2005.
The Penn Discourse TreeBank as a resource for nat-
ural language generation. In Proceedings of the
Corpus Linguistics Workshop on Using Corpora for
NLG.
Ellen Riloff, Janyce Wiebe, and Willian Phillips. 2005.
Exploiting subjectivity classification to improve in-
formation extraction. In Proceedings of the 20th Na-
tional Conference on Artificial Intelligence (AAAI-
2005).
Ivan A. Sag and Carl Pollard. 1991. An integrated
theory of complement control. Language, 67(1):63?
113.
The PDTB-Group. 2006. The Penn Discourse Tree-
Bank 1.0 Annotation Manual. Technical Report
IRCS-06-01, Institute for Research in Cognitive Sci-
ence, University of Pennsylvania.
Veseli Stoyanov, Claire Cardie, and Janyce Wiebe.
2005. Multi-perspective question answering using
the OpQA corpus. In Proceedings of HLT-EMNLP.
Peter D. Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of ACL 2002,
pages 417?424.
Bonnie Webber, Aravind Joshi, M. Stone, and Alis-
tair Knott. 2003. Anaphora and discourse structure.
Computational Linguistics, 29(4):545?587.
Bonnie Webber, Aravind Joshi, Eleni Miltsakaki,
Rashmi Prasad, Nikhil Dinesh, Alan Lee, and
K. Forbes. 2005. A short introduction to the PDTB.
In Copenhagen Working Papers in Language and
Speech Processing.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce,
Matthew Bell, and Melanie Martin. 2004. Learn-
ing subjective language. Computational Linguistics,
30(3):277?308.
Janyce Wiebe, Theresa Wilson, , and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 1(2).
Theresa Wilson and Janyce Wiebe. 2005. Annotating
attributions and private states. In Proceedings of the
ACL Workshop on Frontiers in Corpus Annotation
II: Pie in the Sky, Ann Arbor, Michigan.
Hon Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: separating facts
from opinions and identifying the polarity of opinion
sentences. In Proceedings of EMNLP-2003, pages
129?136, Saporo, Japan.
38
Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 20?27,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Using Information about Multi-word Expressions
for the Word-Alignment Task
Sriram Venkatapathy1
Language Technologies Research Center,
Indian Institute of
Information Technology,
Hyderabad, India.
sriramv@linc.cis.upenn.edu
Aravind K. Joshi
Department of Computer and
Information Science and Institute for
Research in Cognitive Science,
University of Pennsylvania, PA, USA.
joshi@linc.cis.upenn.edu
Abstract
It is well known that multi-word expres-
sions are problematic in natural language
processing. In previous literature, it has
been suggested that information about
their degree of compositionality can be
helpful in various applications but it has
not been proven empirically. In this pa-
per, we propose a framework in which
information about the multi-word expres-
sions can be used in the word-alignment
task. We have shown that even simple
features like point-wise mutual informa-
tion are useful for word-alignment task in
English-Hindi parallel corpora. The align-
ment error rate which we achieve (AER =
0.5040) is significantly better (about 10%
decrease in AER) than the alignment error
rates of the state-of-art models (Och and
Ney, 2003) (Best AER = 0.5518) on the
English-Hindi dataset.
1 Introduction
In this paper, we show that measures representing
compositionality of multi-word expressions can
be useful for tasks such as Machine Translation,
word-alignment to be specific here. We use an on-
line learning framework called MIRA (McDon-
ald et al, 2005; Crammer and Singer, 2003) for
training a discriminative model for the word align-
ment task (Taskar et al, 2005; Moore, 2005). The
discriminative model makes use of features which
represent the compositionality of multi-word ex-
pressions.
1At present visiting Institute for Research in Cognitive
Science, University of Pennsylvania, PA, USA.
Multi-word expressions (MWEs) are those
whose structure and meaning cannot be derived
from their component words, as they occur inde-
pendently. Examples include conjunctions such
as ?as well as? (meaning ?including?), idioms like
?kick the bucket? (meaning ?die?) phrasal verbs
such as ?find out? (meaning ?search?) and com-
pounds like ?village community?. They can be de-
fined roughly as idiosyncratic interpretations that
cross word boundaries (Sag et al, 2002).
A large number of MWEs have standard
syntactic structure but are semantically non-
compositional. Here, we consider the class of verb
based expressions (verb is the head of the phrase),
which occur very frequently. This class of verb
based multi-word expressions include verbal id-
ioms, support-verb constructions, among others.
The example ?take place? is a MWE but ?take a
gift? is not.
In the past, various measures have been sug-
gested for measuring the compositionality of
multi-word expressions. Some of these are mu-
tual information (Church and Hanks, 1989), dis-
tributed frequency (Tapanainen et al, 1998) and
Latent Semantic Analysis (LSA) model (Baldwin
et al, 2003). Even though, these measures have
been shown to represent compositionality quite
well, compositionality itself has not been shown to
be useful in any application yet. In this paper, we
explore this possibility of using the information
about compositionality of MWEs (verb based) for
the word alignment task. In this preliminary work,
we use simple measures (such as point-wise mu-
tual information) to measure compositionality.
The paper is organized as follows. In section 2,
we discuss the word-alignment task with respect
to the class of multi-word expressions of interest
in this paper. In section 3, we show empirically,
20
the behavior of verb based expressions in a paral-
lel corpus (English-Hindi in our case). We then
discuss our alignment algorithm in section 4. In
section 5, we describe the features which we have
used in our training model. Section 6 discusses the
training algorithm and in section 7, the results of
our discriminative model for the word alignment
task. Related work and conclusion follow in sec-
tion 8 and 9 respectively.
2 Task: Word alignment of verbs and
their dependents
The task is to align the verbs and their dependents
(arguments and adjuncts) in the source language
sentence (English) with words in the target lan-
guage sentence (Hindi). The dependents of the
verbs in the source sentence are represented by
their head words. Figure 1. shows an example
of the type of multi-word expressions which we
consider for alignment.
subj
obj
 prep_in
event place
took
The cycling
Philadelphia
     (The cycling event took place in Philadelphia) 
Figure 1: Example of MWEs we consider
In the above example, the goal will the to align
the words ?took?, ?event?, ?place? and ?Philadel-
phia? with corresponding word(s) in the target lan-
guage sentence (which is not parsed) using a dis-
criminative approach. The advantage in using the
discriminative approach for alignment is that it lets
you use various compositionality based features
which are crucial towards aligning these expres-
sions. Figure 2. shows the appropriate alignment
of the expression in Figure 1. with the words in the
target language. The pair (take place), in English,
a verb and one of its dependents is aligned with a
single verbal unit in Hindi.
It is essential to obtain the syntactic roles for de-
pendents in the source language sentence as they
are required for computing the compositionality
value between the dependents and their verbs. The
Philadelphia   mein   saikling     kii    pratiyogitaa   hui
Philadelphia
The cycling
took
placeevent
 prep_in
objsubj
Figure 2: Alignment of Verb based expression
syntactic roles on the source side are obtained by
applying simple rules to the output of a depen-
dency parser. The dependency parser which we
used in our experiments is a stochastic TAG based
dependency parser (Shen, 2006). A sentence
could have one or more verbs. We would like
to align all the expressions represented by those
verbs with words in the target language.
3 Behavior of MWEs in parallel corpora
In this section, we will briefly discuss the com-
plexity of the alignment problem based on the
verb based MWE?s. From the word aligned sen-
tence pairs, we compute the fraction of times a
source sentence verb and its dependent are aligned
together with the same word in the target lan-
guage sentence. We count the number of times a
source sentence verb and its dependent are aligned
together with the same word in the target lan-
guage sentence, and divide it by the total num-
ber of dependents. The total size of our word
aligned corpus is 400 sentence pairs which in-
cludes both training and test sentences. The total
number of dependents present in these sentences
are 2209. Total number of verb dependent pairs
which aligned with same word in target language
are 193. Hence, the percentage of such occur-
rences is 9%, which is a significant number.
4 Alignment algorithm
In this section, we describe the algorithm for align-
ing verbs and their dependents in the source lan-
guage sentence with the words in the target lan-
guage. Let V be the number of verbs and A be the
number of dependents. Let the number of words in
21
the target language be N. If we explore all the ways
in which the V + A words in the source sentence
are aligned with words in the target language be-
fore choosing the best alignment, the total number
of possibilites are NV+A. This is computationally
very expensive. Hence, we use a Beam-search al-
gorithm to obtain the K-best alignments.
Our algorithm has three main steps.
1. Populate the Beam : Use the local features
(which largely capture the co-occurence in-
formation between the source word and the
target word) to determine the K-best align-
ments of verbs and their dependents with
words in the target language.
2. Re-order the Beam: Re-order the above
alignments using more complex features
(which include the global features and the
compositionality based feature(s)).
3. Post-processing : Extend the alignment(s) of
the verb(s) (on the source side) to include
words which can be part of the verbal unit
on the target side.
For a source sentence, let the verbs and depen-
dents be denoted by s
ij
. Here i is the index of
the verb (1 <= i <= V ). The variable j is
the index of the dependents (0 <= j <= A)
except when j = 0 which is used to represent
the verb itself. Let the source sentences be de-
noted as S = fs
ij
g and the target sentences by
T = ft
n
g. The alignment from a source sen-
tence S to target sentence T is defined as the map-
ping a = fa
ijn
j a
ijn
 (s
ij
! t
n
);8i; jg. A
beam is used to store a set of K-best alignments
between a source sentence and the target sentence.
It is represented using the symbol B where B
k
(0 <= k <= K) is used to refer to a particular
alignment configuration.
4.1 Populate the Beam
The task in this step is to obtain the K-best can-
didate alignments using local features. The local
features mainly contain the coccurence informa-
tion between a source and a target word and are in-
dependent of other alignment links or words in the
sentences. Let the local feature vector be denoted
as f
L
(s
ij
; t
k
). The score of a particular alignment
link is computed by taking the dot product of the
weight vector W with the local feature vector (of
words connected by the alignment link). Hence,
the local score will be
sore
L
(s
ij
; t
k
) = W:f
L
(s
ij
; t
k
)
The total score of an alignment configuration is
computed by adding the scores of individual links
in the alignment configuration. Hence, the align-
ment score will be
sore
La
(a; S; T ) =
X
sore
L
(s
ij
; t
k
)
8s
ij
2 S & s
ij
! t
k
2 a
We propose an algorithm of order O((V +
A)Nlog(N) + K) to compute the K-best align-
ment configurations. First, the local scores of each
verb and its dependents are computed for each
word in the target sentence and stored in a lo-
cal beam denoted by b
ij
. The local beams cor-
responding to all the verbs and dependents are
then sorted. This operation has the complexity
(V + A) N log(N).
The goal now is to pick the K-best configura-
tions of alignment links. A single slot in the local
beam corresponds to one alignment link. We de-
fine a boundary which partitions each local beam
into two sets of slots. The slots above the bound-
ary represent the slots which have been explored
by the algorithm while slots below the boundary
have still to be explored. The figure 3. shows the
boundary which cuts across the local beams.
Bb (i,j)
Beam
Alignment
Boundary
Local Beams
Figure 3: Boundary
We keep on modifying the boundary untill all
the K slots in the Alignment Beam are filled with
the K-best configurations. At the beginning of the
algorithm, the boundary is a straight line passing
through the top of all the local beams. The top slot
of the alignment beam at the beginning represents
22
the combination of alignment links with the best
local scores.
The next slot b
ij
[p? (from the set of unexplored
slots) to be included in the boundary is the slot
which has the least difference in score from the
score of the slot at the top of its local beam. That
is, we pick the slot b
ij
[p? such that sore(b
ij
[p?) 
sore(b
ij
[1?) is the least among all the unexplored
slots (or alignment links). Trivially, b
ij
[p  1? was
already a part of the boundary.
When the slot b
ij
[p? is included in the boundary,
various configurations, which now contain b
ij
[p?,
are added to the alignment beam. The new con-
figurations are the same as the ones which previ-
ously contained b
ij
[p   1? but with the replace-
ment of b
ij
[p  1? by b
ij
[p?. The above procedure
ensures that the the alignment configurations are
K-best and are sorted according to the scores ob-
tained using local features.
4.2 Re-order the beam
We now use global features to re-order the beam.
The global features look at the properties of the en-
tire alignment configuration instead of alignment
links locally.
The global score is defined as the dot product of
the weight vector and the global feature vector.
sore
G
(a) = W:f
G
(a)
The overall score is calculated by adding the local
score and the global score.
sore(a) = sore
La
(a) + sore
G
(a)
The beam is now sorted based on the overall
scores of each alignment. The alignment config-
uration at the top of the beam is the best possible
alignment between source sentence and the target
sentence.
4.3 Post-processing
The first two steps in our alignment algorithm
compute alignments such that one verb or depen-
dent in the source language side is aligned with
only one word in the target side. But, in the case
of compound verbs in Hindi, the verb in English is
aligned to all the words which represent the com-
pound verb in Hindi. For example, in Figure 3, the
verb ?lost? is aligned to both ?khoo? and ?dii?.
Our alignment algorithm would have aligned
?lost? only to ?khoo?. Hence, we look at the win-
dow of words after the word which is aligned to
mainee    Shyam   ki    kitaaba  khoo   dii
Shyam?s
bookI
lost
Figure 4: Case of compound verb in Hindi
the source verb and check if any of them is a verb
which has not been aligned with any word in the
source sentence. If this condition is satisfied, we
align the source verb to these words too.
5 Parameters
As the number of training examples (294 sen-
tences) is small, we choose to use very representa-
tive features. Some of the features which we used
in this experiment are as follows,
5.1 Local features (F
L
)
The local features which we consider are mainly
co-occurence features. These features estimate the
likelihood of a source word aligning to a target
word based on the co-occurence information ob-
tained from a large sentence aligned corpora1.
1. DiceWords: Dice Coefficient of the source
word and the target word
DCoe (s
ij
; t
k
) =
2  Count(s
ij
; t
k
)
Count(s
ij
) + Count(t
k
)
where Count(s
ij
; t
k
) is the number of times
the word t
k
was present in the translation of
sentences containing the word s
ij
in the par-
allel corpus.
2. DiceRoots: Dice Coefficient of the lemma-
tized forms of the source and target words.
It is important to consider this feature be-
cause the English-Hindi parallel corpus is not
large and co-occurence information can be
learnt effectively only after we lemmatize the
words.
3. Dict: Whether there exists a dictionary entry
from the source word s
ij
to the target word
150K sentence pairs originally collected as part of TIDES
MT project and later refined at IIIT-Hyderabad, India.
23
tk
. For English-Hindi, we used a dictionary
available at IIIT - Hyderabad, India.
4. Null: Whether the source word s
ij
is aligned
to nothing in the target language.
5.2 Global features
The following are the four global features which
we have considered,
 AvgDist: The average distance between the
words in the target language sentence which
are aligned to the verbs in the source lan-
guage sentence . AvgDist is then normalized
by dividing itself by the number of words in
the target language sentence. If the average
distance is small, it means that the verbs in
the source language sentence are aligned with
words in the target language sentence which
are located at relatively close distances, rela-
tive to the length of the target language sen-
tence.
This feature expresses the distribution of
predicates in the target language.
 Overlap: This feature stores the count of
pairs of verbs in the source language sentence
which align with the same word in the target
language sentence. Overlap is normalized by
dividing itself by the total pairs of verbs.
This feature is used to discourage overlaps
among the words which are alignments of
verbs in the source language sentence.
 MergePos: This feature can be considered as
a compositionality based feature. The part
of speech tag of a dependent is essential to
determine the likelihood of the dependent to
align with the same word in the target lan-
guage sentence as the word to which its verb
is aligned.
This binary feature is active when the align-
ment links of a dependent and its verb
merge. For example, in Figure 5., the feature
?merge RP? will be active (that is, merge RP
= 1).
 MergeMI: This is a compositionality based
feature which associates point-wise mutual
information (apart from the POS informa-
tion) with the cases where the dependents
which have the same alignment in the target
He/N away/RP
ran/V
    vaha     bhaaga     gayaa
Figure 5: Example of MergePos feature
language as their verbs. This features which
notes the the compositionality value (repre-
sented by point-wise mutual information in
our experiments) is active if the alignment
links of dependent and its verb merge.
The mutual information (MI) is classified
into three groups depending on its absolute
value. If the absolute value of mutual infor-
mation rounded to nearest integer is in the
range 0-2, it is considered LOW. If the value
is in the range 3-5, it is considered MEDIUM
and if it is above 5, it is considered HIGH.
The feature ?merge RP HIGH? is active in
the example shown in figure 6.
He/N away/RP
ran/V
    vaha     bhaaga     gayaa
MI = HIGH
Figure 6: Example of MergeMI feature
6 Online large margin training
For parameter optimization, we have used an on-
line large margin algorithm called MIRA (Mc-
Donald et al, 2005) (Crammer and Singer, 2003).
We describe the training algorithm that we used
very briefly. Our training set is a set of English-
Hindi word aligned parallel corpus. We get the
verb based expressions in English by running a de-
pendency parser (Shen, 2006). Let the number of
sentence pairs in the training data be m. We have
24
fS
q
; T
q
; a^
q
g for training where q <= m is the in-
dex number of the sentence pair fS
q
; T
q
g in the
training set and a^
q
is the gold alignment for the
pair fS
q
; T
q
g. Let W be the weight vector which
has to be learnt, W
i
be the weight vector after the
end of ith update. To avoid over-fitting, W is ob-
tained by averaging over all the weight vectors W
i
.
A generic large margin algorithm is defined fol-
lows for the training instances fS
q
; T
q
; a^
q
g,
1. Initialize W
0
, W , i
2. for p:1 to NIterations
3. for q:1 to m
4. Get K-Best predictions 
q
= fa
1
; a
2
:::a
k
g
for the training example (S
q
; T
q
; a^
q
) using
the current model W i and applying step
1 and 2 of section 4. Compute W i+1 by
updating W i based on (S
q
; T
q
; a^
q
; 
q
).
5. i = i + 1
6. W = W + W i+1
7. W = W
NIterationsm
The goal of MIRA is to minimize the change in
W
i such that the score of the gold alignment a^ ex-
ceeds the score of each of the predictions in  by a
margin which is equal to the number of mistakes in
the predictions when compared to gold alignment.
While computing the number of mistakes, the mis-
takes due to the mis-alignment of head verb could
be given greater weight, thus prompting the opti-
mization algorithm to give greater importance to
verb related mistakes and thereby improving over-
all performance.
Step 4 in the algorithm mentioned above can
be substituted by the following optimization
problem,
minimize k(W i+1  W i)k
s.t. 8k, sore(a^
q
; S
q
; T
q
)   sore(a
q;k
; S
q
; T
q
)
>= Mistakes(a
k
; a^
q
; S
q
; T
q
)
The above optimization problem is converted to
the Dual form using one Lagrangian multiplier for
each constraint. In the Dual form, the Lagrangian
multipliers are solved using Hildreth?s algorithm.
Here, prediction of  is similar to the prediction
of K   best classes in a multi-class classification
problem. Ideally, we need to consider all the possi-
ble classes and assign margin constraints based on
every class. But, here the number of such classes
is exponential and thus we restrict ourselves to the
K   best classes.
7 Results on word-alignment task
7.1 Dataset
We have divided the 400 word aligned sentence
pairs into a training set consisting of 294 sen-
tence pairs and a test set consisting of 106 sentence
pairs. The source sentences are all dependency
parsed (Shen, 2006) and only the verb and its de-
pendents are considered for both training and test-
ing our algorithm. Our training algorithm requires
that the each of the source words is aligned to only
one or zero target words. For this, we use simple
heuristics to convert the training data to the appro-
priate format. For the words aligned to a source
verb, the first verb is chosen as the gold alignment.
For the words aligned to any dependent which is
not a verb, the last content word is chosen as the
alignment link. For test data, we do not make any
modifications and the final output from our align-
ment algorithm is compared with the original test
data.
7.2 Experiments with Giza
We evaluated our discriminative approach by com-
paring it with the state-of-art Giza++ alignments
(Och and Ney, 2003). The metric that we have
used to do the comparison is the Alignment Error
Rate (AER). The results shown below also contain
Precision, Recall and F-measure.
Giza was trained using an English-Hindi
aligned corpus of 50000 sentence pairs. In Table
1., we report the results of the GIZA++ alignments
run from both the directions (English to Hindi and
Hindi to English). We also show the results of the
intersected model. See Table 1. for the results of
the GIZA++ alignments.
Prec. Recall F-meas. AER
Eng! Hin 0.45 0.38 0.41 0.5874
Hin! Eng 0.46 0.27 0.34 0.6584
Intersected 0.82 0.19 0.31 0.6892
Table 1: Results of GIZA++ - Original dataset
We then lemmatize the words in both the source
and target sides of the parallel corpora and then
run Giza++ again. As the English-Hindi dataset
25
of 50000 sentence pairs is relatively small, we ex-
pect lemmatizing to improve the results. Table 2.
shows the results. As we hoped, the results after
lemmatizing the word forms are better than those
without.
Prec. Recall F-meas. AER
Eng! Hin 0.52 0.40 0.45 0.5518
Hin! Eng 0.53 0.30 0.38 0.6185
Intersected 0.82 0.23 0.36 0.6446
Table 2: Results of GIZA++ - lemmatized set
7.3 Experiments with our model
We trained our model using the training set of 294
word aligned sentence pairs. For training the pa-
rameters, we used a beam size of 3 and number of
iterations equal to 3. Table 3. shows the results
when we used only the basic local features (Dice-
Words, DiceRoots, Dict and Null) to train and test
our model.
Prec. Recall F-meas. AER
Local Feats. 0.47 0.38 0.42 0.5798
Table 3: Results using the basic features
When we add the the global features (AvgDist,
Overlap), we obtain the AER shown in Table 4.
Prec. Recall F-meas. AER
+ AvgD., Ove. 0.49 0.39 0.43 0.5689
Table 4: Results using the features - AvgDist,
Overlap
Now, we add the transition probabilities ob-
tained from the experiments with Giza++ as fea-
tures in our model. Table 5. contains the results.
The compositionality related features are now
added to our discriminative model to see if there is
any improvement in performance. Table 6. shows
the results by adding one feature at a time.
We observe that there is an improvement in the
AER by using the compositionality based features,
thus showing that compositionality based features
aid in the word-alignment task in a significant way
(AER = 0.5045).
8 Related work
Various measures have been proposed in the past
to measure the compositionality of multi-word ex-
Prec. Recall F-meas. AER
+ Giza++ prob. 0.54 0.44 0.49 0.5155
Table 5: Results using the Giza++ probabilities
Prec. Recall F-meas. AER
+ MergePos 0.54 0.45 0.49 0.5101
+ MergeMI 0.55 0.45 0.50 0.5045
Table 6: Results using the compositionality based
features
pressions of various types. Some of them are Fre-
quency, Point-wise mutual information (Church
and Hanks, 1989), Distributed frequency of object
(Tapanainen et al, 1998), Distributed frequency
of object using verb information (Venkatapathy
and Joshi, 2005), Similarity of object in verb-
object pair using the LSA model (Baldwin et al,
2003), (Venkatapathy and Joshi, 2005) and Lex-
ical and Syntactic fixedness (Fazly and Steven-
son, 2006). These features have largely been eval-
uated by the correlation of the compositionality
value predicted by these measures with the gold
standard value suggested by human judges. It has
been shown that the correlation of these measures
is higher than simple baseline measures suggest-
ing that these measures represent compositionality
quite well. But, the compositionality as such has
not been used in any specific application yet.
In this paper, we have suggested a framework
for using the compositionality of multi-word ex-
pressions for the word alignment task. State-of-art
systems for doing word alignment use generative
models like GIZA++ (Och and Ney, 2003; Brown
et al, 1993). Discriminative models have been
tried recently for word-alignment (Taskar et al,
2005; Moore, 2005) as these models give the abil-
ity to harness variety of complex features which
cannot be provided in the generative models. In
our work, we have used the compositionality of
multi-word expressions to predict how they align
with the words in the target language sentence.
For parameter optimization for the word-
alignment task, Taskar, Simon and Klein (Taskar
et al, 2005) used a large margin approach by fac-
toring the structure level constraints to constraints
at the level of an alignment link. We cannot do
such a factorization because the scores of align-
ment links in our case are not computed in a com-
pletely isolated manner. We use an online large
margin approach called MIRA (McDonald et al,
26
2005; Crammer and Singer, 2003) which fits well
with our framework. MIRA has previously been
used by McDonald, Pereira, Ribarov and Hajic
(McDonald et al, 2005) for learning the param-
eter values in the task of dependency parsing.
It should be noted that previous word-alignment
experiments such as Taskar, Simon and Klein
(Taskar et al, 2005) have been done with very
large datasets and there is little word-order vari-
ation in the languages involved. Our dataset is
small at present and there is substantial word order
variation between the source and target languages.
9 Conclusion and future work
In this paper, we have proposed a discriminative
approach for using the compositionality informa-
tion about verb-based multi-word expressions for
the word-alignment task. For training our model,
use used an online large margin algorithm (Mc-
Donald et al, 2005). For predicting the alignment
given a model, we proposed a K-Best beam search
algorithm to make our prediction algorithm com-
putationally feasible.
We have investigated the usefulness of simple
features such as point-wise mutual information for
the word-alignment task in English-Hindi bilin-
gual corpus. We have show that by adding the
compositionality based features to our model, we
obtain an decrease in AER from 0.5155 to 0.5045.
Our overall results are better than those obtained
using the GIZA++ models (Och and Ney, 2003).
In future, we will experiment with more ad-
vanced compositionality based features. But, this
would require a larger dataset for training and we
are working towards buidling such a large dataset.
Also, we would like to conduct similar exper-
iments on other language pairs (e.g. English-
French) and compare the results with the state-of-
art results reported for those languages.
References
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model
of multiword expression decomposability. In Di-
ana McCarthy Francis Bond, Anna Korhonen and
Aline Villavicencio, editors, Proceedings of the ACL
2003 Workshop on Multiword Expressions: Analy-
sis, Acquisition and Treatment, pages 89?96.
P. Brown, S. A. Pietra, V. J. Della, Pietra, and R. L.
Mercer. 1993. The mathmatics of stastistical ma-
chine translation. In Computational Linguistics.
Kenneth Church and Patrick Hanks. 1989. Word as-
sociation norms, mutual information, and lexicog-
raphy. In Proceedings of the 27th. Annual Meet-
ing of the Association for Computational Linguis-
tics, 1990.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. In
Journal of Machine Learning Research.
Afsaneh Fazly and Suzanne Stevenson. 2006. Auto-
matically constructing a lexicon of verb phrase id-
iomatic combinations. In Proceedings of European
Chapter of Association of Computational Linguis-
tics. Trento, Italy, April.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 523?530, Vancouver,
British Columbia, Canada, October. Association of
Computational Linguistics.
Robert C. Moore. 2005. A discriminative frame-
work for bilingual word alignment. In Proceedings
of Human Language Technology Conference and
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 81?88, Vancouver, British
Columbia, Canada, October. Association of Compu-
tational Linguistics.
F. Och and H. Ney. 2003. A systematic comparisoin
of various statistical alignment models. In Compu-
tational Linguistics.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multi-word
expressions: a pain in the neck for nlp. In Proceed-
ings of CICLing , 2002.
Libin Shen. 2006. Statistical LTAG Parsing. Ph.D.
thesis.
Pasi Tapanainen, Jussi Piitulaine, and Timo Jarvinen.
1998. Idiomatic object usage and support verbs. In
36th Annual Meeting of the Association for Compu-
tational Linguistics.
Ben Taskar, Locoste-Julien Simon, and Klein Dan.
2005. A discriminative machine learning approach
to word alignment. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Process-
ing, pages 73?80, Vancouver, British Columbia,
Canada, October. Association of Computational
Linguistics.
Sriram Venkatapathy and Aravind Joshi. 2005. Mea-
suring the relative compositionality of verb-noun (v-
n) collocations by integrating features. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 899?906. Association
of Computational Linguistics, Vancouver, British
Columbia, Canada, October.
27
Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 17?24,
Sydney, July 2006. c?2006 Association for Computational Linguistics
The Metagrammar Goes Multilingual:
A Cross-Linguistic Look at the V2-Phenomenon
Alexandra Kinyon
Department of CIS
University of Pennsylvania
kinyon@linc.cis.upenn.edu
Tatjana Scheffler
Department of Linguistics
University of Pennsylvania
tatjana@ling.upenn.edu
Aravind K. Joshi
Department of CIS
University of Pennsylvania
joshi@linc.cis.upenn.edu
Owen Rambow
CCLS
Columbia University
rambow@cs.columbia.edu
SinWon Yoon
UFRL
Universite? Paris 7
swyoon@linguist.jussieu.fr
Abstract
We present an initial investigation into
the use of a metagrammar for explic-
itly sharing abstract grammatical specifi-
cations among languages. We define a
single class hierarchy for a metagrammar
which allows us to automatically gener-
ate grammars for different languages from
a single compact metagrammar hierarchy.
We use as our linguistic example the verb-
second phenomenon, which shows con-
siderable variation while retaining a ba-
sic property, namely the fact that the verb
can appear in one of two positions in the
clause.
1 An Overview of Metagrammars
A metagrammar (MG) factors common properties
of TAG elementary trees to avoid redundancy, ease
grammar development, and expand coverage with
minimal effort: typically, from a compact man-
ually encoded MG of a few dozen classes, one
or more TAGs with several hundreds of elemen-
tary trees are automatically generated. This is
appealing from a grammar engineering point of
view, and also from a linguistic point of view:
cross-linguistic generalizations are expressed di-
rectly in the MG. In this paper, we extend some
earlier work on multilingual MGs (Candito, 1998;
Kinyon and Rambow, 2003) by proposing cross-
linguistic and framework-neutral syntactic invari-
ants, which we apply to TAG. We focus on the
verb-second phenomenon as a prototypical exam-
ple of cross-language variation.
The notion of Metagrammar Metagrammars
were first introduced by Candito (1996) to manu-
ally encode syntactic knowledge in a compact and
abstract class hierarchy which supports multiple
inheritance, and from which a TAG is automati-
cally generated offline. Candito?s class hierarchy
imposes a general organization of syntax into three
dimensions:
  Dimension 1: to encode initial subcategoriza-
tion frames i.e. TAG tree families
  Dimension 2: to encode valency alternations
/ redistribution of syntactic functions
  Dimension 3: to encode the surface realiza-
tion of arguments.
Each class in the MG hierarchy is associated
with a partial tree description The tool computes
a set of well-formed classes by combining exactly
one terminal class from dimension 1, one termi-
nal class from dimension 2, and  terminal classes
from dimensions 3 ( being the number of argu-
ments subcategorized by the lexical head anchor-
ing the elementary tree(s) generated). The con-
junction of the tree descriptions associated with
each well-formed class in the set yields a minimal
satisfying description, which results in the gener-
ation of one or more elementary trees. Candito?s
tool was used to develop a large TAG for French
as well as a medium-size TAG for Italian Candito
(1999), so multilinguality was addressed from the
start, but each language had its dedicated hierar-
chy, with no sharing of classes despite the obvious
similarities between Italian and French. A related
approach was proposed by (Xia, 2001); the work
of Evans, Gazdar, and Weir (2000) also has some
common elements with MG.
Framework- and language-neutral syntactic
invariants Using a MG, and following Can-
dito, we can postulate cross-linguistic and cross-
framework syntactic invariants such as:
17
  The notion of subcategorization
  The existence of a finite number of syntactic
functions (subject, object etc.)
  The existence of a finite number of syntactic
categories (NP, PP, etc.)
  The existence of valency alternations (Can-
dito?s dimension 2)
  The existence, orthogonal to valency alterna-
tions, of syntactic phenomena which do not
alter valency, such as wh-movement (Can-
dito?s dimension 3).
These invariants ? unlike other framework-
specific syntactic assumptions such as the exis-
tence of ?movement? or ?wh-traces? ? are ac-
cepted by most if not all existing frameworks, even
though the machinery of a given framework may
not necessarily account explicitly for each invari-
ant. For instance, TAG does not have an explicit
notion of syntactic function: although by conven-
tion node indices tend to reflect a function, it is not
enforced by the framework?s machinery.1
Hypertags Based on such framework- and
language-neutral syntactic properties, Kinyon
(2000) defined the notion of Hypertag (HT), a
combination of Supertags (ST) Srinivas (1997)
and of the MG. A ST is a TAG elementary tree,
which provides richer information than standard
POS tagging, but in a framework-specific man-
ner (TAG), and also in a grammar-specific manner
since a ST tagset can?t be ported from one TAG
to another TAG. A HT is an abstraction of STs,
where the main syntactic properties of any given
ST is encoded in a general readable Feature Struc-
ture (FS), by recording which MG classes a ST in-
herited from when it was generated. Figure 1 illus-
trates the  ST, HT pair for Par qui sera accom-
pagne?e Marie ?By whom will Mary be accompa-
nied?. We see that a HT feature structure directly
reflects the MG organization, by having 3 features
?Dimension 1?, ?Dimension 2? and ?Dimension
3?, where each feature takes its value from the MG
terminal classes used to generate a given ST.
The XMG Tool Candito?s tool brought a sig-
nificant linguistic insight, therefore we essentially
retain the above-mentioned syntactic invariants.
However, more recent MG implementations have
been developed since, each adding its significant
contribution to the underlying metagrammatical
hypothesis.
In this paper, we use the eXtensible MetaGram-
mar (XMG) tool which was developed by Crabbe?
1But several attempts have been made to explicitly add
functions to TAG, e.g. by Kameyama (1986) to retain the
benefits of both TAG and LFG, or by Prolo (2006) to account
for the coordination of constituents of different categories,
yet sharing the same function.
S
PP
P
par
NWh
(qui)
S
Aux
sera
V
accompagne?e
N 
(Marie)


DIMENSION1 STRICTTRANSITIVE
DIMENSION2 PERSONALFULLPASSIVE
DIMENSION3 SUBJECT INVERTEDSUBJECTCOMPLEMENT WHQUESTIONEDBYCOMPLEMENT	



Figure 1: A  SuperTag, HyperTag pair for ac-
compagne?e (?accompanied?) obtained with Can-
dito?s MetaGrammar compiler
(2005). In XMG, an MG consists of a set of
classes similar to those in object-oriented pro-
gramming, which are structured into a multiple
inheritance hierarchy. Each class specifies a par-
tial tree description (expressed by dominance and
precedence constraints). The nodes of these tree
fragment descriptions may be annotated with fea-
tures. Classes may instantiate each other, and they
may be parametrized (e.g., to hand down features
like the grammatical function of a substitution
node). The compiler unifies the instantiations of
tree descriptions that are called. This unification
is additionally guided by node colors, constraints
that specify that a node must not be unified with
any other node (red), must be unified (white), or
may be unified, but only with a white node (black).
XMG allows us to implement a hierarchy similar
to that of Candito, but it also allows us to modify
and extend it, as no structural assumptions about
the class hierarchy are hard-coded.
2 The V2 Phenomenon
The Verb-Second (V2) phenomenon is a well-
known set of data that demonstrates small-scale
cross-linguistic variation. The examples in (1)
show German, a language with a V2-constraint:
(1a) is completely grammatical, while (1b) is not.
This is considered to be due to the fact that the
finite verb is required to be located in ?second po-
sition? (V2) in German. Other languages with a
V2 constraint include Dutch, Yiddish, Frisian, Ice-
landic, Mainland Scandinavian, and Kashmiri.
(1) a. Auf
on
dem
the
Weg
path
sieht
sees
der
the
Junge
boy
eine
a
Ente.
duck
?On the path, the boy sees a duck.?
18
b. * Auf
on
dem
the
Weg
path
der
the
Junge
boy
sieht
sees
eine
a
Ente.
duck
Int.: ?On the path, the boy sees a duck.?
Interestingly, these languages differ with re-
spect to how exactly the constraint is realized.
Rambow and Santorini (1995) present data from
the mentioned languages and provide a set of pa-
rameters that account for the exhibited variation.
In the following, for the sake of brevity, we will
confine the discussion to two languages: German,
and Yiddish. The German data is as follows (we
do not repeat (1a) from above):
(2) a. Der
the
Junge
boy
sieht
sees
eine
a
Ente
duck
auf
on
dem
the
Weg.
path
?On the path, the boy sees a duck.?
b. . . . ,
. . . ,
dass
that
der
the
Junge
boy
auf
on
dem
the
Weg
path
eine
a
Ente
duck
sieht.
sees
?. . . , that the boy sees a duck on the path.?
c. Eine
a
Ente
duck
sieht
sees
der
the
Junge.
boy
?The boy sees a duck.?
The Yiddish data:
(3) a. Dos
the
yingl
boy
zet
sees
oyfn
on-the
veg
path
a
a
katshke.
duck
?On the path, the boy sees a duck.?
b. Oyfn
on-the
veg
path
zet
sees
dos
the
yingl
boy
a
a
katshke.
duck.
?On the path, the boy sees a duck.?
c. . . . ,
. . . ,
az
that
dos
the
yingl
boy
zet
sees
a
a
katshke
duck
?. . . , that the boy sees a duck.?
While main clauses exhibit V2 in German, embed-
ded clauses with complementizers are verb-final
(2b). In contrast, Yiddish embedded clauses must
also be V2 (3c).
3 Handling V2 in the Metagrammar
It is striking that the basic V2 phenomenon is the
same in all of these languages: the verb can ap-
pear in either its underlying position, or in sec-
ond position (or, in some cases, third). We claim
that what governs the appearance of the verb
in these different positions (and thus the cross-
linguistic differences) is that the heads?the verbal
head and functional heads such as auxiliaries and
complementizers?interact in specific ways. For
example, in German a complementizer is not com-
patible with a verbal V2 head, while in Yiddish it
is. We express the interaction among heads by as-
signing the heads different values for a set of fea-
tures. Which heads can carry which feature values
is a language-specific parameter. Our implementa-
tion is based on the previous pen-and-pencil anal-
ysis of Rambow and Santorini (1995), which we
have modified and extended.
The work we present in this paper thus has
a threefold interest: (1) we show how to han-
dle an important syntactic phenomenon cross-
linguistically in a MG framework; (2) we partially
validate, correct, and extend a previously proposed
linguistically-motivated analysis; and (3) we pro-
vide an initial fragment of a MG implementa-
tion from which we generate TAGs for languages
which are relatively less-studied and for which no
TAG currently exists (Yiddish).
4 Elements of Our Implementation
In this paper, we only address verbal elementary
trees. We define a verbal realization to be a com-
bination of three classes (or ?dimensions? in Can-
dito?s terminology): a subcategorization frame,
a redistribution of arguments/valency alternation
(in our case, voice, which we do not further dis-
cuss), and a topology, which encodes the posi-
tion and characteristics of the verbal head. Thus,
we reinterpret Candito?s ?Dimension 3? to con-
centrate on the position of the verbal heads, with
the different argument realizations (topicalized,
base position) depending on the available heads,
rather than defined as first-class citizens. The sub-
cat and argument redistributions result in a set of
structures for arguments which are left- or right-
branching (depending on language and grammat-
ical function). Figure 2 shows some argument
structures for German. The topology reflects the
basic clause structure, that is, the distribution of ar-
guments and adjuncts, and the position of the verb
(initial, V2, final, etc.). Our notion of sentence
topology is thus similar to the notion formalized
by Gerdes (2002). Specifically, we see positions
of arguments and adjuncts as defined by the posi-
tions of their verbal heads. However, while Gerdes
(2002) assumes as basic underlying notions the
fields created by the heads (the traditional Vorfeld
for the topicalized element and the Mittelfeld be-
tween the verb in second position and the verb in
clause-final position), we only use properties of
the heads. The fields are epiphenomenal for us.As
mentioned above, we use the following set of fea-
tures to define our MG topology:
  I (finite tense and subject-verb agreement):
creates a specifier position for agreement
which must be filled in a derivation, but al-
lows recursion (i.e., adjunction at IP).
  Top (topic): a feature which creates a spec-
ifier position for the topic (semantically rep-
resented in a lambda abstraction) which must
be filled in a derivation, and which does not
allow recursion.
  M (mood): a feature with semantic content
(to be defined), but no specifier.
  C (complementizer): a lexical feature intro-
duced only by complementizers.
We can now define our topology in more detail.
It consists of two main parts:
19
German:
What Features Introduced Directionality
1 Verb (clause-final) +I head-final
2 Verb (V2, subject-inital) +M, +Top, +I head-initial
3 Verb (V2, non-subject-initial) +M, +Top head-initial
4 Complementizer +C, +M head-initial
Yiddish:
What Features Introduced Directionality
1 Verb +I head-initial
2 Verb (V2, subject-inital) +M, +Top, +I head-initial
3 Verb (V2, non-subject-initial) +M, +Top head-initial
4 Complementizer +C head-initial
Figure 4: Head inventories for German and Yiddish.
1:

 
 
 
 
 
 
 
 

CAT V
I +
TOP 
M 
C 
black












 
 
 
 
 
 
 
 

CAT V
I 
TOP 
M 
C 
white











v
2:

 
 
 
 
 
 
 
 

CAT V
I +
TOP +
M +
C 
black











v

 
 
 
 
 
 
 
 

CAT V
I 
TOP 
M 
C 
white











3:

 
 
 
 
 
 
 
 

CAT V
I +
TOP +
M +
C 
black











v

 
 
 
 
 
 
 
 

CAT V
I +
TOP 
M 
C 
white











4:

 
 
 
 
 
 
 
 

CAT V
I +
TOP 
M +
C +
black











comp

 
 
 
 
 
 
 
 

CAT V
I +
TOP 
M 
C 
white











Figure 5: Head structures for German corresponding to the table in Figure 4 (above)
1:

 
 
 
 
 
 
 
 

CAT V
I +
TOP 
M 
C 
black











v

 
 
 
 
 
 
 
 

CAT V
I 
TOP 
M 
C 
white











2:

 
 
 
 
 
 
 
 

CAT V
I +
TOP +
M +
C 
black











v

 
 
 
 
 
 
 
 

CAT V
I 
TOP 
M 
C 
white











3:

 
 
 
 
 
 
 
 

CAT V
I +
TOP +
M +
C 
black











v

 
 
 
 
 
 
 
 

CAT V
I +
TOP 
M 
C 
white











4:

 
 
 
 
 
 
 
 

CAT V
I +
TOP +
M +
C +
black











comp

 
 
 
 
 
 
 
 

CAT V
I +
TOP +
M +
C 
white











Figure 6: Head structures for Yiddish corresponding to the table in Figure 4 (below)
20
 
 
 

CAT V
I +
TOP +
black






NP 

 
 
 

CAT V
I +
TOP +
white







 
 
 

CAT V
I +
TOP 
black






NP 

 
 
 

CAT V
I +
TOP 
white







 
 
 

CAT V
I 
TOP +
black






NP 

 
 
 

CAT V
I 
TOP +
white







 
 
 

CAT V
I 
TOP 
black






NP 

 
 
 

CAT V
I 
TOP 
white






Figure 2: The argument structures

CAT V
white 

 
 
 
 
 
 
 
 

CAT V
I 
TOP 
M 
C 
black











	
Figure 3: The projection structure; feature values
can be filled in at the top feature structure to con-
trol the derivation.
  The projection includes the origin of the
verb in the phrase structure (with an empty
head since we assume it is no longer there)
and its maximal projection. It is shown in
Figure 3. The maximal projection expresses
the expected feature content. For example,
if we want to model non-finite clauses, the
maximal projection will have [I], while root
V2 clauses will have [+Top], and embedded
finite clauses with complementizers will have
[+I,+C].
  Structures for heads, which can be head-
initial or head-final. They introduce catego-
rial features. Languages differ in what sort of
heads they have. Which heads are available
for a given language is captured in a head in-
ventory, i.e., a list of possible heads for that
language (which use the head structure just
mentioned). Two such lists are shown in Fig-
ure 4, for German and Yiddish. The corre-
sponding head structures are shown in Fig-
ures 5 and 6.
A topology is a combination of the projection
and any combination of heads allowed by the
language-specific head inventory. This is hard
to express in XMG, so instead we list the spe-
cific combinations allowed. One might ask how
we derive trees for language without the V2 phe-
nomenon. Languages without V2 will usually
have a smaller set of possible heads. We are work-
ing on a metagrammar for Korean in parallel with
our work on the V2 languages. Korean is very
much like German without the V2 phenomenon:
the verbal head can only be in clause-final position
(i.e., head 1 from Figure 5. However, passiviza-
tion and scrambling can be treated the same way
in Korean and German, since these phenomena are
independent of V2.
5 Sample Derivation
Given a feature ordering (C  M  Top  I) and
language-specific head inventories as in Figure 4,
we compile out MGs for German (Figure 5) and
Yiddish (Figure 6).2 The projection and the ar-
gument realizations do not differ between the two
languages: thus, these parts of the MG can be
reused. The features, which were introduced for
descriptive reasons, now guide the TAG compila-
tion: only certain heads can be combined. Further-
more, subjects and non-subjects are distinguished,
as well as topicalized and non-topicalized NPs
(producing 4 kinds of arguments so far). The com-
piler picks out any number of compatible elements
from the Metagrammar and performs the unifica-
tions of nodes that are permitted (or required) by
2All terminal nodes are ?red?; spine nodes have been an-
notated with their color.
21
the node descriptions and the colors. By way of
example, the derivations of elementary trees which
can be used in a TAG analysis of German (2c) and
Yiddish (3c) are shown in Figures 7 and 8, respec-
tively.
6 Conclusion and Future work
This paper showed how cross-linguistic general-
izations (in this case, V2) can be incorporated into
a multilingual MG. This allows not only the reuse
of MG parts for new (often, not well-studied) lan-
guages, but it also enables us to study small-scale
parametric variation between languages in a con-
trolled and formal way. We are currently modify-
ing and extending our implementation in several
ways.
The Notion of Projection In our current ap-
proach, the verb is never at the basis of the pro-
jection, it has always been removed into a new
location. This may seem unmotivated in certain
cases, such as German verb-final sentences. We
are looking into using the XMG unification to ac-
tually place the verb at the bottom of the projection
in these cases.
Generating Top and Bottom Features The
generated TAG grammar currently does not have
top and bottom feature sets, as one would expect
in a feature-based TAG. These are important for
us so we can force adjunction in adjunct-initial V2
sentences (where the element in clause-initial po-
sition is not an argument of the verb). We intend
to follow the approach laid out in Crabbe? (2005) in
order to generate top and bottom feature structures
on the nodes of the TAG grammar.
Generating test-suites to document our
grammars Since XMG offers more complex
object-oriented functionalities, including in-
stances, and therefore recursion, it is now
straightforward to directly generate parallel mul-
tilingual sentences directly from XMG, without
any intermediate grammar generation step. The
only obstacle remains the explicit encoding of
Hypertags into XMG.
Acknowledgments
We thank Yannick Parmentier, Joseph Leroux,
Bertrand Gaiffe, Benoit Crabbe?, the LORIA XMG
team, and Julia Hockenmaier for their invaluable
help; Eric de la Clergerie, Carlos Prolo and the
Xtag group for their helpful feedback, comments
and suggestions on different aspects of this work;
and Marie-He?le`ne Candito for her insights. This
work was supported by NSF Grant 0414409 to the
University of Pennsylvania.
References
Candito, M. H. 1998. Building parallel LTAG for French and
Italian. In Proc. ACL-98. Montreal.
Candito, M.H. 1996. A principle-based hierarchical repre-
sentation of LTAGs. In Proc. COLING-96. Copenhagen.
Candito, M.H. 1999. Repre?sentation modulaire et
parame?trable de grammaires e?lectroniques lexicalise?es.
Doctoral Dissertation, Univ. Paris 7.
Cle?ment, L., and A. Kinyon. 2003. Generating parallel mul-
tilingual LFG-TAG grammars using a MetaGrammar. In
Proc. ACL-03. Sapporo.
Clergerie, E. De La. 2005. From metagrammars to factorized
TAG/TIG parsers. In IWPT-05. Trento.
Crabbe?, B. 2005. Repre?sentation informatique de grammaires
fortement lexicalise?es. Doctoral Dissertation, Univ. Nancy
2.
Evans, R., G. Gazdar, and D. Weir. 2000. Lexical rules
are just lexical rules. In Tree Adjoining Grammars, ed.
A. Abeille? and O. Rambow. CSLI.
Gerdes, K. 2002. DTAG. attempt to generate a useful TAG for
German using a metagrammar. In Proc. TAG+6. Venice.
Kameyama, M. 1986. Characterising LFG in terms of TAG.
In Unpublished report. Univ. of Pennsylvania.
Kinyon, A. 2000. Hypertags. In Proc. COLING-00. Sar-
rebrucken.
Kinyon, A., and O. Rambow. 2003. Generating cross-
language and cross-framework annotated test-suites using
a MetaGrammar. In Proc. LINC-EACL-03. Budapest.
Prolo, C. 2006. Handling unlike coordinated phrases in TAG
by mixing Syntactic Category and Grammatical Function.
In Proc. TAG+8. Sidney.
Rambow, Owen, and Beatrice Santorini. 1995. Incremental
phrase structure generation and a universal theory of V2.
In Proceedings of NELS 25, ed. J.N. Beckman, 373?387.
Amherst, MA: GSLA.
Srinivas, B. 1997. Complexity of lexical descriptions and its
relevance for partial parsing. Doctoral Dissertation, Univ.
of Pennsylvania.
Xia, F. 2001. Automatic grammar generation from two per-
spectives. Doctoral Dissertation, Univ. of Pennsylvania.
XTAG Research Group. 2001. A lexicalized tree adjoin-
ing grammar for English. Technical Report IRCS-01-03,
IRCS, University of Pennsylvania.
22
 
 
 

CAT V
I 
TOP +
black






NP 

 
 
 

CAT V
I 
TOP +
white







 
 
 
 
 
 
 
 

CAT V
I +
TOP +
M +
C 
black











v

 
 
 
 
 
 
 
 

CAT V
I +
TOP 
M 
C 
white












 
 
 

CAT V
I +
TOP 
black






NP 

 
 
 

CAT V
I +
TOP 
white







 
 
 
 
 
 
 
 

CAT V
I +
TOP 
M 
C 
black












 
 
 
 
 
 
 
 

CAT V
I 
TOP 
M 
C 
white












CAT V
white 

 
 
 
 
 
 
 
 

CAT V
I 
TOP 
M 
C 
black











Object-Topicalized + Head 3 + Subject-Non-Topicalized + Head 1 + Projection
(White and Black nodes next to each other are unified.)
Figure 7: Derivation of the German elementary tree NP

 V NP  (2d).
23
 
 
 
 
 
 
 
 

CAT V
I +
TOP 
M +
C +
black











comp

 
 
 
 
 
 
 
 

CAT V
I +
TOP 
M +
C 
white












 
 
 

CAT V
I +
TOP +
black






NP 

 
 
 

CAT V
I +
TOP +
white







 
 
 
 
 
 
 
 

CAT V
I +
TOP +
M +
C 
black











v

 
 
 
 
 
 
 
 

CAT V
I 
TOP 
M 
C 
white












 
 
 

CAT V
I 
TOP 
black






NP 

 
 
 

CAT V
I 
TOP 
white







CAT V
white 

 
 
 
 
 
 
 
 

CAT V
I 
TOP 
M 
C 
black











Head 4 (Comp) + Subject-Topicalized + Head 2 + Object-Non-Topicalized + Projection
Figure 8: Derivation of the Yiddish elementary tree Comp NP  V NP

 (3c).
24
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 293?300,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Protein folding and chart parsing
Julia Hockenmaier Aravind K. Joshi
Institute for Research in Cognitive Science
University of Pennsylvania
Philadelphia, PA 19104, USA
 	

Workshop on Computationally Hard Problemsand Joint Inference in Speech and Language Processing, pages 1?8,
New York City, New York, June 2006. c?2006 Association for Computational Linguistics
A Syntax-Directed Translator with Extended Domain of Locality
Liang Huang
Dept. of Comp. & Info. Sci.
Univ. of Pennsylvania
Philadelphia, PA 19104
lhuang3@cis.upenn.edu
Kevin Knight
Info. Sci. Inst.
Univ. of Southern California
Marina del Rey, CA 90292
knight@isi.edu
Aravind Joshi
Dept. of Comp. & Info. Sci.
Univ. of Pennsylvania
Philadelphia, PA 19104
joshi@linc.cis.upenn.edu
Abstract
A syntax-directed translator first parses
the source-language input into a parse-
tree, and then recursively converts the tree
into a string in the target-language. We
model this conversion by an extended tree-
to-string transducer that have multi-level
trees on the source-side, which gives our
system more expressive power and flexi-
bility. We also define a direct probabil-
ity model and use a linear-time dynamic
programming algorithm to search for the
best derivation. The model is then ex-
tended to the general log-linear frame-
work in order to rescore with other fea-
tures like n-gram language models. We
devise a simple-yet-effective algorithm to
generate non-duplicate k-best translations
for n-gram rescoring. Initial experimen-
tal results on English-to-Chinese transla-
tion are presented.
1 Introduction
The concept of syntax-directed (SD) translation
was originally proposed in compiling (Irons, 1961;
Lewis and Stearns, 1968), where the source program
is parsed into a tree representation that guides the
generation of the object code. Following Aho and
Ullman (1972), a translation, as a set of string pairs,
can be specified by a syntax-directed translation
schema (SDTS), which is essentially a synchronous
context-free grammar (SCFG) that generates two
languages simultaneously. An SDTS also induces a
translator, a device that performs the transformation
induces implements
SD translator
(source parser + recursive converter)
specifies translation
(string relation)
SD translation schema
(synchronous grammar)
Figure 1: The relationship among SD concepts,
adapted from (Aho and Ullman, 1972).
?
?
?
?
?
?
S
NP(1)? VP
VB(2)? NP
(3)
?
,
S
VB(2)? NP
(1)
? NP
(3)
?
?
?
?
?
?
?
Figure 2: An example of complex reordering repre-
sented as an STSG rule, which is beyond any SCFG.
from input string to output string. In this context, an
SD translator consists of two components, a source-
language parser and a recursive converter which is
usually modeled as a top-down tree-to-string trans-
ducer (Ge?cseg and Steinby, 1984). The relationship
among these concepts is illustrated in Fig. 1.
This paper adapts the idea of syntax-directed
translator to statistical machine translation (MT).
We apply stochastic operations at each node of the
source-language parse-tree and search for the best
derivation (a sequence of translation steps) that con-
verts the whole tree into some target-language string
with the highest probability. However, the structural
divergence across languages often results in non-
isomorphic parse-trees that is beyond the power of
SCFGs. For example, the S(VO) structure in English
is translated into a VSO word-order in Arabic, an in-
stance of complex reordering not captured by any
1
SCFG (Fig. 2).
To alleviate the non-isomorphism problem, (syn-
chronous) grammars with richer expressive power
have been proposed whose rules apply to larger frag-
ments of the tree. For example, Shieber and Sch-
abes (1990) introduce synchronous tree-adjoining
grammar (STAG) and Eisner (2003) uses a syn-
chronous tree-substitution grammar (STSG), which
is a restricted version of STAG with no adjunctions.
STSGs and STAGs generate more tree relations than
SCFGs, e.g. the non-isomorphic tree pair in Fig. 2.
This extra expressive power lies in the extended do-
main of locality (EDL) (Joshi and Schabes, 1997),
i.e., elementary structures beyond the scope of one-
level context-free productions. Besides being lin-
guistically motivated, the need for EDL is also sup-
ported by empirical findings in MT that one-level
rules are often inadequate (Fox, 2002; Galley et al,
2004). Similarly, in the tree-transducer terminology,
Graehl and Knight (2004) define extended tree trans-
ducers that have multi-level trees on the source-side.
Since an SD translator separates the source-
language analysis from the recursive transformation,
the domains of locality in these two modules are or-
thogonal to each other: in this work, we use a CFG-
based Treebank parser but focuses on the extended
domain in the recursive converter. Following Gal-
ley et al (2004), we use a special class of extended
tree-to-string transducer (xRs for short) with multi-
level left-hand-side (LHS) trees.1 Since the right-
hand-side (RHS) string can be viewed as a flat one-
level tree with the same nonterminal root from LHS
(Fig. 2), this framework is closely related to STSGs:
they both have extended domain of locality on the
source-side, while our framework remains as a CFG
on the target-side. For instance, an equivalent xRs
rule for the complex reordering in Fig. 2 would be
S(x1:NP, VP(x2:VB, x3:NP))? x2 x1 x3
While Section 3 will define the model formally,
we first proceed with an example translation from
English to Chinese (note in particular that the in-
verted phrases between source and target):
1Throughout this paper, we will use LHS and source-side
interchangeably (so are RHS and target-side). In accordance
with our experiments, we also use English and Chinese as the
source and target languages, opposite to the Foreign-to-English
convention of Brown et al (1993).
(a) the gunman was [killed]1 by [the police]2 .
parser ?
(b)
S
NP-C
DT
the
NN
gunman
VP
VBD
was
VP-C
VBN
killed
PP
IN
by
NP-C
DT
the
NN
police
PUNC
.
r1, r2 ?
(c) qiangshou
VP
VBD
was
VP-C
VBN
killed
PP
IN
by
NP-C
DT
the
NN
police
?
r3 ?
(d) qiangshou bei
NP-C
DT
the
NN
police
VBN
killed
?
r5 ? r4 ?
(e) qiangshou bei [jingfang]2 [jibi]1 ?
Figure 3: A synatx-directed translation process for
Example (1).
(1) the gunman was killed by the police .
qiangshou
[gunman]
bei
[passive]
jingfang
[police]
jibi
[killed]
?
.
Figure 3 shows how the translator works. The En-
glish sentence (a) is first parsed into the tree in (b),
which is then recursively converted into the Chinese
string in (e) through five steps. First, at the root
node, we apply the rule r1 which preserves the top-
level word-order and translates the English period
into its Chinese counterpart:
(r1) S (x1:NP-C x2:VP PUNC (.) ) ? x1 x2 ?
2
Then, the rule r2 grabs the whole sub-tree for ?the
gunman? and translates it as a phrase:
(r2) NP-C ( DT (the) NN (gunman) )? qiangshou
Now we get a ?partial Chinese, partial English? sen-
tence ?qiangshou VP ?? as shown in Fig. 3 (c). Our
recursion goes on to translate the VP sub-tree. Here
we use the rule r3 for the passive construction:
(r3)
VP
VBD
was
VP-C
x1:VBN PP
IN
by
x2:NP-C
? bei x2 x1
which captures the fact that the agent (NP-C, ?the
police?) and the verb (VBN, ?killed?) are always
inverted between English and Chinese in a passive
voice. Finally, we apply rules r4 and r5 which per-
form phrasal translations for the two remaining sub-
trees in (d), respectively, and get the completed Chi-
nese string in (e).
2 Previous Work
It is helpful to compare this approach with recent ef-
forts in statistical MT. Phrase-based models (Koehn
et al, 2003; Och and Ney, 2004) are good at learn-
ing local translations that are pairs of (consecutive)
sub-strings, but often insufficient in modeling the re-
orderings of phrases themselves, especially between
language pairs with very different word-order. This
is because the generative capacity of these models
lies within the realm of finite-state machinery (Ku-
mar and Byrne, 2003), which is unable to process
nested structures and long-distance dependencies in
natural languages.
Syntax-based models aim to alleviate this prob-
lem by exploiting the power of synchronous rewrit-
ing systems. Both Yamada and Knight (2001) and
Chiang (2005) use SCFGs as the underlying model,
so their translation schemata are syntax-directed as
in Fig. 1, but their translators are not: both systems
do parsing and transformation in a joint search, es-
sentially over a packed forest of parse-trees. To this
end, their translators are not directed by a syntac-
tic tree. Although their method potentially consid-
ers more than one single parse-tree as in our case,
the packed representation of the forest restricts the
scope of each transfer step to a one-level context-
free rule, while our approach decouples the source-
language analyzer and the recursive converter, so
that the latter can have an extended domain of local-
ity. In addition, our translator also enjoys a speed-
up by this decoupling, with each of the two stages
having a smaller search space. In fact, the recursive
transfer step can be done by a a linear-time algo-
rithm (see Section 5), and the parsing step is also
fast with the modern Treebank parsers, for instance
(Collins, 1999; Charniak, 2000). In contrast, their
decodings are reported to be computationally expen-
sive and Chiang (2005) uses aggressive pruning to
make it tractable. There also exists a compromise
between these two approaches, which uses a k-best
list of parse trees (for a relatively small k) to approx-
imate the full forest (see future work).
Besides, our model, as being linguistically mo-
tivated, is also more expressive than the formally
syntax-based models of Chiang (2005) and Wu
(1997). Consider, again, the passive example in rule
r3. In Chiang?s SCFG, there is only one nonterminal
X, so a corresponding rule would be
? was X(1) by X(2), bei X(2) X(1) ?
which can also pattern-match the English sentence:
I was [asleep]1 by [sunset]2 .
and translate it into Chinese as a passive voice. This
produces very odd Chinese translation, because here
?was A by B? in the English sentence is not a pas-
sive construction. By contrast, our model applies
rule r3 only if A is a past participle (VBN) and B
is a noun phrase (NP-C). This example also shows
that, one-level SCFG rule, even if informed by the
Treebank as in (Yamada and Knight, 2001), is not
enough to capture a common construction like this
which is five levels deep (from VP to ?by?).
There are also some variations of syntax-directed
translators where dependency structures are used
in place of constituent trees (Lin, 2004; Ding and
Palmer, 2005; Quirk et al, 2005). Although they
share with this work the basic motivations and simi-
lar speed-up, it is difficult to specify re-ordering in-
formation within dependency elementary structures,
so they either resort to heuristics (Lin) or a sepa-
rate ordering model for linearization (the other two
3
works).2 Our approach, in contrast, explicitly mod-
els the re-ordering of sub-trees within individual
transfer rules.
3 Extended Tree-to-String Tranducers
In this section, we define the formal machinery of
our recursive transformation model as a special case
of xRs transducers (Graehl and Knight, 2004) that
has only one state, and each rule is linear (L) and
non-deleting (N) with regarding to variables in the
source and target sides (henth the name 1-xRLNs).
Definition 1. A 1-xRLNs transducer is a tuple
(N,?,?,R) where N is the set of nonterminals, ?
is the input alphabet, ? is the output alphabet, and
R is a set of rules. A rule in R is a tuple (t, s, ?)
where:
1. t is the LHS tree, whose internal nodes are la-
beled by nonterminal symbols, and whose fron-
tier nodes are labeled terminals from ? or vari-
ables from a set X = {x1, x2, . . .};
2. s ? (X ??)? is the RHS string;
3. ? is a mapping from X to nonterminals N .
We require each variable xi ? X occurs exactly once
in t and exactly once in s (linear and non-deleting).
We denote ?(t) to be the root symbol of tree t.
When writing these rules, we avoid notational over-
head by introducing a short-hand form from Galley
et al (2004) that integrates the mapping into the tree,
which is used throughout Section 1. Following TSG
terminology (see Figure 2), we call these ?variable
nodes? such as x2:NP-C substitution nodes, since
when applying a rule to a tree, these nodes will be
matched with a sub-tree with the same root symbol.
We also define |X | to be the rank of the rule, i.e.,
the number of variables in it. For example, rules r1
and r3 in Section 1 are both of rank 2. If a rule has
no variable, i.e., it is of rank zero, then it is called a
purely lexical rule, which performs a phrasal trans-
lation as in phrase-based models. Rule r2, for in-
stance, can be thought of as a phrase pair ?the gun-
man, qiangshou?.
Informally speaking, a derivation in a transducer
is a sequence of steps converting a source-language
2Although hybrid approaches, such as dependency gram-
mars augmented with phrase-structure information (Alshawi et
al., 2000), can do re-ordering easily.
r1
r2 r3
r4 r5
r1
r2 r6
r4 r7
r5
(a) (b)
Figure 4: (a) the derivation in Figure 3; (b) another
derviation producing the same output by replacing
r3 with r6 and r7, which provides another way of
translating the passive construction:
(r6) VP ( VBD (was) VP-C (x1:VBN x2:PP ) )? x2 x1
(r7) PP ( IN (by) x1:NP-C )? bei x1
tree into a target-language string, with each step ap-
plying one tranduction rule. However, it can also
be formalized as a tree, following the notion of
derivation-tree in TAG (Joshi and Schabes, 1997):
Definition 2. A derivation d, its source and target
projections, noted E(d) and C(d) respectively, are
recursively defined as follows:
1. If r = (t, s, ?) is a purely lexical rule (? = ?),
then d = r is a derivation, where E(d) = t and
C(d) = s;
2. If r = (t, s, ?) is a rule, and di is a (sub-)
derivation with the root symbol of its source
projection matches the corresponding substitu-
tion node in r, i.e., ?(E(di)) = ?(xi), then
d = r(d1, . . . , dm) is also a derivation, where
E(d) = [xi 7? E(di)]t and C(d) = [xi 7?
C(di)]s.
Note that we use a short-hand notation [xi 7? yi]t
to denote the result of substituting each xi with yi
in t, where xi ranges over all variables in t.
For example, Figure 4 shows two derivations for
the sentence pair in Example (1). In both cases, the
source projection is the English tree in Figure 3 (b),
and the target projection is the Chinese translation.
Galley et al (2004) presents a linear-time algo-
rithm for automatic extraction of these xRs rules
from a parallel corpora with word-alignment and
parse-trees on the source-side, which will be used
in our experiments in Section 6.
4
4 Probability Models
4.1 Direct Model
Departing from the conventional noisy-channel ap-
proach of Brown et al (1993), our basic model is a
direct one:
c? = argmax
c
Pr(c | e) (2)
where e is the English input string and c? is the
best Chinese translation according to the translation
model Pr(c | e). We now marginalize over all En-
glish parse trees T (e) that yield the sentence e:
Pr(c | e) =
?
??T (e)
Pr(?, c | e)
=
?
??T (e)
Pr(? | e) Pr(c | ?) (3)
Rather than taking the sum, we pick the best tree ??
and factors the search into two separate steps: pars-
ing (4) (a well-studied problem) and tree-to-string
translation (5) (Section 5):
?? = argmax
??T (e)
Pr(? | e) (4)
c? = argmax
c
Pr(c | ??) (5)
In this sense, our approach can be considered as
a Viterbi approximation of the computationally ex-
pensive joint search using (3) directly. Similarly, we
now marginalize over all derivations
D(??) = {d | E(d) = ??}
that translates English tree ? into some Chinese
string and apply the Viterbi approximation again to
search for the best derivation d?:
c? = C(d?) = C(argmax
d?D(??)
Pr(d)) (6)
Assuming different rules in a derivation are ap-
plied independently, we approximate Pr(d) as
Pr(d) =
?
r?d
Pr(r) (7)
where the probability Pr(r) of the rule r is estimated
by conditioning on the root symbol ?(t(r)):
Pr(r) = Pr(t(r), s(r) | ?(t(r)))
= c(r)?
r?:?(t(r?))=?(t(r)) c(r?)
(8)
where c(r) is the count (or frequency) of rule r in
the training data.
4.2 Log-Linear Model
Following Och and Ney (2002), we extend the direct
model into a general log-linear framework in order
to incorporate other features:
c? = argmax
c
Pr(c | e)? ? Pr(c)? ? e??|c| (9)
where Pr(c) is the language model and e??|c| is the
length penalty term based on |c|, the length of the
translation. Parameters ?, ?, and ? are the weights
of relevant features. Note that positive ? prefers
longer translations. We use a standard trigram model
for Pr(c).
5 Search Algorithms
We first present a linear-time algorithm for searching
the best derivation under the direct model, and then
extend it to the log-linear case by a new variant of
k-best parsing.
5.1 Direct Model: Memoized Recursion
Since our probability model is not based on the noisy
channel, we do not call our search module a ?de-
coder? as in most statistical MT work. Instead, read-
ers who speak English but not Chinese can view it as
an ?encoder? (or encryptor), which corresponds ex-
actly to our direct model.
Given a fixed parse-tree ??, we are to search
for the best derivation with the highest probability.
This can be done by a simple top-down traversal
(or depth-first search) from the root of ??: at each
node ? in ??, try each possible rule r whose English-
side pattern t(r) matches the subtree ??? rooted at ?,
and recursively visit each descendant node ?i in ???
that corresponds to a variable in t(r). We then col-
lect the resulting target-language strings and plug
them into the Chinese-side s(r) of rule r, getting
a translation for the subtree ??? . We finally take the
best of all translations.
With the extended LHS of our transducer, there
may be many different rules applicable at one tree
node. For example, consider the VP subtree in
Fig. 3 (c), where both r3 and r6 can apply. As a re-
sult, the number of derivations is exponential in the
size of the tree, since there are exponentially many
5
decompositions of the tree for a given set of rules.
This problem can be solved by memoization (Cor-
men et al, 2001): we cache each subtree that has
been visited before, so that every tree node is visited
at most once. This results in a dynamic program-
ming algorithm that is guaranteed to run in O(npq)
time where n is the size of the parse tree, p is the
maximum number of rules applicable to one tree
node, and q is the maximum size of an applicable
rule. For a given rule-set, this algorithm runs in time
linear to the length of the input sentence, since p
and q are considered grammar constants, and n is
proportional to the input length. The full pseudo-
code is worked out in Algorithm 1. A restricted
version of this algorithm first appears in compiling
for optimal code generation from expression-trees
(Aho and Johnson, 1976). In computational linguis-
tics, the bottom-up version of this algorithm resem-
bles the tree parsing algorithm for TSG by Eisner
(2003). Similar algorithms have also been proposed
for dependency-based translation (Lin, 2004; Ding
and Palmer, 2005).
5.2 Log-linear Model: k-best Search
Under the log-linear model, one still prefers to
search for the globally best derivation d?:
d? = argmax
d?D(??)
Pr(d)? Pr(C(d))?e??|C(d)| (10)
However, integrating the n-gram model with the
translation model in the search is computationally
very expensive. As a standard alternative, rather
than aiming at the exact best derivation, we search
for top-k derivations under the direct model using
Algorithm 1, and then rerank the k-best list with the
language model and length penalty.
Like other instances of dynamic programming,
Algorithm 1 can be viewed as a hypergraph search
problem. To this end, we use an efficient algo-
rithm by Huang and Chiang (2005, Algorithm 3)
that solves the general k-best derivations problem
in monotonic hypergraphs. It consists of a normal
forward phase for the 1-best derivation and a recur-
sive backward phase for the 2nd, 3rd, . . . , kth deriva-
tions.
Unfortunately, different derivations may have the
same yield (a problem called spurious ambiguity),
due to multi-level LHS of our rules. In practice, this
results in a very small ratio of unique strings among
top-k derivations. To alleviate this problem, deter-
minization techniques have been proposed by Mohri
and Riley (2002) for finite-state automata and ex-
tended to tree automata by May and Knight (2006).
These methods eliminate spurious ambiguity by ef-
fectively transforming the grammar into an equiva-
lent deterministic form. However, this transforma-
tion often leads to a blow-up in forest size, which is
exponential to the original size in the worst-case.
So instead of determinization, here we present a
simple-yet-effective extension to the Algorithm 3 of
Huang and Chiang (2005) that guarantees to output
unique translated strings:
? keep a hash-table of unique strings at each vertex
in the hypergraph
? when asking for the next-best derivation of a ver-
tex, keep asking until we get a new string, and
then add it into the hash-table
This method should work in general for any
equivalence relation (say, same derived tree) that can
be defined on derivations.
6 Experiments
Our experiments are on English-to-Chinese trans-
lation, the opposite direction to most of the recent
work in SMT. We are not doing the reverse direction
at this time partly due to the lack of a sufficiently
good parser for Chinese.
6.1 Data Preparation
Our training set is a Chinese-English parallel corpus
with 1.95M aligned sentences (28.3M words on the
English side). We first word-align them by GIZA++,
then parse the English side by a variant of Collins
(1999) parser, and finally apply the rule-extraction
algorithm of Galley et al (2004). The resulting rule
set has 24.7M xRs rules. We also use the SRI Lan-
guage Modeling Toolkit (Stolcke, 2002) to train a
Chinese trigram model with Knesser-Ney smooth-
ing on the Chinese side of the parallel corpus.
Our evaluation data consists of 140 short sen-
tences (< 25 Chinese words) of the Xinhua portion
of the NIST 2003 Chinese-to-English evaluation set.
Since we are translating in the other direction, we
use the first English reference as the source input
and the Chinese as the single reference.
6
Algorithm 1 Top-down Memoized Recursion
1: function TRANSLATE(?)
2: if cache[?] defined then . this sub-tree visited before?
3: return cache[?]
4: best? 0
5: for r ? R do . try each rule r
6: matched, sublist? PATTERNMATCH(t(r), ?) . tree pattern matching
7: if matched then . if matched, sublist contains a list of matched subtrees
8: prob? Pr(r) . the probability of rule r
9: for ?i ? sublist do
10: pi, si ? TRANSLATE(?i) . recursively solve each sub-problem
11: prob? prob ? pi
12: if prob > best then
13: best? prob
14: str ? [xi 7? si]s(r) . plug in the results
15: cache[?]? best, str . caching the best solution for future use
16: return cache[?] . returns the best string with its prob.
6.2 Initial Results
We implemented our system as follows: for each in-
put sentence, we first run Algorithm 1, which returns
the 1-best translation and also builds the derivation
forest of all translations for this sentence. Then we
extract the top 5000 non-duplicate translated strings
from this forest and rescore them with the trigram
model and the length penalty.
We compared our system with a state-of-the-art
phrase-based system Pharaoh (Koehn, 2004) on the
evaluation data. Since the target language is Chi-
nese, we report character-based BLEU score instead
of word-based to ensure our results are indepen-
dent of Chinese tokenizations (although our lan-
guage models are word-based). The BLEU scores
are based on single reference and up to 4-gram pre-
cisions (r1n4). Feature weights of both systems are
tuned on the same data set.3 For Pharaoh, we use the
standard minimum error-rate training (Och, 2003);
and for our system, since there are only two in-
dependent features (as we always fix ? = 1), we
use a simple grid-based line-optimization along the
language-model weight axis. For a given language-
model weight ?, we use binary search to find the best
length penalty ? that leads to a length-ratio closest
3In this sense, we are only reporting performances on the
development set at this point. We will report results tuned and
tested on separate data sets in the final version of this paper.
Table 1: BLEU (r1n4) score results
system BLEU
Pharaoh 25.5
direct model (1-best) 20.3
log-linear model (rescored 5000-best) 23.8
to 1 against the reference. The results are summa-
rized in Table 1. The rescored translations are better
than the 1-best results from the direct model, but still
slightly worse than Pharaoh.
7 Conclusion and On-going Work
This paper presents an adaptation of the clas-
sic syntax-directed translation with linguistically-
motivated formalisms for statistical MT. Currently
we are doing larger-scale experiments. We are also
investigating more principled algorithms for inte-
grating n-gram language models during the search,
rather than k-best rescoring. Besides, we will extend
this work to translating the top k parse trees, instead
of committing to the 1-best tree, as parsing errors
certainly affect translation quality.
7
References
A. V. Aho and S. C. Johnson. 1976. Optimal code gen-
eration for expression trees. J. ACM, 23(3):488?501.
Alfred V. Aho and Jeffrey D. Ullman. 1972. The The-
ory of Parsing, Translation, and Compiling, volume I:
Parsing. Prentice Hall, Englewood Cliffs, New Jersey.
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas.
2000. Learning dependency translation models as col-
lections of finite state head transducers. Computa-
tional Linguistics, 26(1):45?60.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19:263?311.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. of NAACL, pages 132?139.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of the 43rd
ACL.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Thomas H. Cormen, Charles E. Leiserson, Ronald L.
Rivest, and Clifford Stein. 2001. Introduction to Al-
gorithms. MIT Press, second edition.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probablisitic synchronous dependency in-
sertion grammars. In Proceedings of the 43rd ACL.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of ACL
(companion volume), pages 205?208.
Heidi J. Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In In Proc. of EMNLP.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In HLT-
NAACL.
F. Ge?cseg and M. Steinby. 1984. Tree Automata.
Akade?miai Kiado?, Budapest.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In HLT-NAACL, pages 105?112.
Liang Huang and David Chiang. 2005. Better k-best
Parsing. In Proceedings of the Nineth International
Workshop on Parsing Technologies (IWPT-2005), 9-10
October 2005, Vancouver, Canada.
E. T. Irons. 1961. A syntax-directed compiler for AL-
GOL 60. Comm. ACM, 4(1):51?55.
Aravind Joshi and Yves Schabes. 1997. Tree-adjoining
grammars. In G. Rozenberg and A. Salomaa, editors,
Handbook of Formal Languages, volume 3, pages 69
? 124. Springer, Berlin.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT-NAACL, pages 127?133.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proc. of AMTA, pages 115?124.
Shankar Kumar and William Byrne. 2003. A weighted
finite state transducer implementation of the alignment
template model for statistical machine translation. In
Proc. of HLT-NAACL, pages 142?149.
P. M. Lewis and R. E. Stearns. 1968. Syntax-directed
transduction. Journal of the ACM, 15(3):465?488.
Dekang Lin. 2004. A path-based transfer model for ma-
chine translation. In Proceedings of the 20th COLING.
Jonathan May and Kevin Knight. 2006. A better n-best
list: Practical determinization of weighted finite tree
automata. Submitted to HLT-NAACL 2006.
Mehryar Mohri and Michael Riley. 2002. An efficient
algorithm for the n-best-strings problem. In Proceed-
ings of the International Conference on Spoken Lan-
guage Processing 2002 (ICSLP ?02), Denver, Col-
orado, September.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proc. of ACL.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Computa-
tional Linguistics, 30:417?449.
Franz Och. 2003. Minimum error rate training for statis-
tical machine translation. In Proc. of ACL.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In Proceedings of the 43rd ACL.
Stuart Shieber and Yves Schabes. 1990. Synchronous
tree-adjoining grammars. In Proc. of COLING, pages
253?258.
Andrea Stolcke. 2002. Srilm: an extensible language
modeling toolkit. In Proc. of ICSLP.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proc. of ACL.
8
Extracting Formal Specifications from Natural
Language Regulatory Documents
Nikhil Dinesh, Aravind Joshi, and Insup Lee
Department of Computer Science, Univeristy of Pennsylvania,
Philadelphia, PA - 19104 USA
nikhild,joshi,lee@cis. upenn.edu
Bonnie Webber
University of Edinburgh, Edinburgh, EH8 9LW Scotland
bonnie@inf.ed. ac. uk
Abstract
Formal verification techniques provide a way to determine whether regulatory doc-
uments are consistent and whether implementations conform to them. To apply
these techniques a formal description of the regulation needs to be extracted. We
present a framework, under which NLP techniques can be brought to bear, to aid
a requirements engineer in extracting the formal description.
1 Introduction
Regulatory documents, which include the vast bodies of legislation, operating
procedures and organizational policy, are meant to be accessible to the people
affected by them. Hence, they have to be in natural language (NL). On
the other hand, regulations are expected to be consistent, and the governed
entities/events are expected to conform to the regulation.
For example, the Food and Drug Administration?s Code of Federal Reg-
ulations (FDA CFR) governs the bloodbanks in America. 1 The bloodbanks
perform safety-critical functions like the testing of blood for communicable
disease agents (like HIV). It is highly desirable to determine whether (a) the
CFR is consistent, and (b) a bloodbank?s implementation of such a function
conforms to the CFR.
? This research was supported in part by NSF CCF-0429948 and ARO 911NF-05-1-0158
1 http://www.gpoaccess.gov/cfr/index.html
The problem of creating descriptions of regulation which can be checked
for consistency has been explored by several authors [1,8], but the challenge of
checking an implementation for conformance has not been addressed, and this
is the main goal of our work. The conformance guarantees can be obtained
if formal descriptions of regulation and implementations are available, and if
verification techniques [4] can be applied to these descriptions. But extracting
a formal description of regulation is expensive, as regulatory bases like the
CFR are large (about a million words) and complex.
Formal descriptions of regulation are usually extracted by an individual
who has a background in logic, e.g., [1,8]. We will call this individual the
requirements engineer. In this paper, we describe a framework to assist a
requirements engineer in extracting a formal description of regulation for use
in conformance checking.
An overview of the framework, the theoretical background and the various
constraints that apply is given in Section 2. This lets us determine the nature
of the description that needs to be extracted from the regulation. We then
turn to the question of how these descriptions might be composed. In Section
3, we attempt to map the denotations of sentences assigned by Kratzer [12]
to a form that can be used for the task at hand. Some difficulties arise in
this mapping, mainly because notions of obligation (that which is required)
and permission (that which is allowed) are not captured in the denotations.
We argue that an account of these notions is essential to the task at hand.
Section 4 describes a semantic representation, and composition procedure to
assist the requirements engineer in extracting the required description. By
treating obligations and permissions as different dimensions of the description
computed, the difficulties encountered in Section 3 are addressed.
The approach is motivated by our case study of the FDA CFR, and we
use (1) and (2) as examples through the course of this paper. 2 (1) conveys
an obligation to perform a test for HIV and Hepatitis B, and (2) conveys a
permission not to test source plasma (a blood component) for Hepatitis B.
(1) Except as specified in (2), you must test each donation of human blood or blood
component, for evidence of infection due to the Human immunodeficiency virus,
and the Hepatitis B virus.
(2) You are not required to test donations of Source Plasma for evidence of infection
due to the Hepatitis B virus.
2 A Framework
To determine whether an implementation (bloodbank) conforms to the regu-
lation (CFR), we extract specifications in the Computation Tree Logic (CTL)
from the CFR. Then, given a description of a bloodbank?s procedure (as a
finite transition system, or model) there is an efficient search procedure to
2 (1) and (2) are modified versions of sentences that appear in the FDA CFR 610.40. The
actual sentences are very long, and the modifications are made in the interests of space.
determine if the model conforms to the CTL specification [3]. This is known
as temporal model checking [2,13]. The problem of conformance checking is
thus split into three steps:
(1) Extract CTL specifications from the regulation - This is done by a
requirements engineer, and our goal is to assist her. We use CTL as the
specification language, because it allows for efficient model checking [3].
(2) Obtain a model of an implementation - We assume the availability of
models. There are tools that aid in extracting models from software [5], and
in creating models if they cannot be extracted directly [11].
(3) Apply model checking to determine if the model conforms to the CTL
specification.
Formally, a model can be defined as follows:
Definition 2.1 A model M is the five-tuple (S, I, ?, pi,?), where:
(a) S is a set of states, I ? S is a non-empty set of initial states,
(b) ? ? S ? S is a total transition relation (that is, ?s ? S : [?t ? S : (s, t) ? ?]),
(c) pi is a set of propositions (with power set 2pi), and
(d) ? : S ? 2pi is a function from states to sets of propositions. ?(s) for s ? S can be
thought of as the propositions true at s.
Figure 1(a) and 1(b) show models of two different bloodbanks. The left-
most state is the initial state. Each state is labeled with ?(s). The propo-
sitions have the following interpretation: d? is true (d? ? ?(s)) iff a donation
of blood or blood component is being processed, sp? is true iff a donation of
source plasma is being processed, thiv? is true iff a test for HIV has been per-
formed, and thepb? is true iff a test for Hepatitis B has been performed. The
use of the propositions deo (denoting deontic accessibility) and app1 (denoting
the application of a permission) is explained in later sections.
deo, d?,
sp?,
thiv?,
thepb?
deo, d?,
thiv?,
thepb?
(a) A model of a bloodbank
which tests all donations
deo, d?,
sp?,
thiv?,
app1
deo, d?,
thiv? ,
thepb?
(b) A model of a bloodbank
which does not test dona-
tions of source plasma for
Hepatitis B
Fig. 1. Two models of bloodbanks
Definition 2.2 Given a finite set of propositions pi, CTL formulas are defined induc-
tively as follows:
(a) p ? pi is a formula,
(b) Boolean combinations and negations of formulas are formulas,
(c) if ?, and ? are formulas, then AG(?) (on all paths, globally ?), AX(?) (on all
paths, at the next state ?), and ?AU?) (on all paths, ? until ?) are formulas.
The only temporal operator in CTL that we use is AG (for reasons that
we describe below), and hence rather than define the interpretation formally,
we will give some examples. Let M1 be the model in Figure 1(a), and M2 be
the model in Figure 1(b). The CTL specification AG(deo ? (d? ? thiv?)) holds
of both models, since on all paths (from the initial state, the leftmost one in
Figures 1(a), and 1(b)), globally, in all deontically accessible states deo, if a
donation of blood or blood component is being processed d?, it is tested for HIV
thiv?. Hence, we write M1  AG(deo ? (d? ? thiv?)), and M2  AG(deo ? (d? ?
thiv?)). Also, M1  AG(deo ? (sp? ? thepb?)). But, M2 6 AG(deo ? (sp? ? thepb?))
(since there is a state s with sp? ? ?(s), and thepb? 6? ?(s)).
2.1 Approaches to extracting specifications
The central problem we face is that CTL and other temporal logics that lend
themselves to model checking are not expressive enough for a compositional
semantic procedure to be defined for natural language. One reason is that
CTL, like propositional logic, cannot express relations between entities.
There are several routes one might take to address this problem, i.e., design
more expressive logics that allow for tractable model checking, focus on a
subset of NL from which an automatic translation is guaranteed, or make
the procedure machine-assisted. While the design of more expressive logics
makes the composition of specifications easier, using them for model checking
needs the creation of more expressive models (which requires more effort).
As a result, there is a trade-off between amount of effort spent in obtaining
models, and that in obtaining the specifications. Our decision to work with
less expressive models is motivated by the extensive tool support available
for creating and extracting such models [5,11]. Further, subsets of NL for
which automatic translation is guaranteed, such as the one derived by Holt
and Klein [10], assume (among other things) that references are resolved and
hence cannot be directly applied to regulatory documents. We are thus left
with the choice of making the procedure machine-assisted.
There have been two kinds of machine-assisted approaches to extracting
temporal logic specifications: (a) composing the semantics in a general seman-
tic framework which is then mapped to temporal logic [7], and (b) attempting
to compose the semantics in the temporal logic directly [6]. In the latter ap-
proach, a human specifies denotations for a portion of the sentence, and the
rest of the composition happens automatically. We attempt to compose the
semantics in a temporal logic directly like [6], as it lends itself to defining
semantic representations with which a requirements engineer can interact in
well-defined ways.
2.2 Constraints on the CTL specifications
We apply two constraints to the CTL specifications:
(i) The specifications extracted should hold of all and only the valid mod-
els. There may be several implementations that aim to conform to a single
base of regulation. Given (1) and (2), the models in Figures 1(a) and 1(b) are
both valid. This is an important difference from the NL sentences considered
in previous approaches, which were elicited from appropriate users by pre-
senting them with a single model. For example, Holt and Klein [10] obtained
specifications by asking users to describe a particular timing diagram.
(ii) To account for the variation between models, all temporal information
about the governed entities/events is modelled through propositions. The only
use of the temporal operators in CTL is to obtain a quantification over paths
and states. A mapping will need to be performed so that the propositions
used in the specifications can be evaluated at a states in different models, and
the critical assumption is that this mapping will be very easy to specify.
3 From Sets of Worlds to Sets of Models
Several approaches in formal semantics take sentences to denote sets of worlds.
For normative statements, we assume (following Kratzer [12]) that worlds are
connected by an accessibility relation. Consider (1) in Section 1 which among
other things requires a test for Hepatitis B if no exceptions apply. A denotation
of this requirement is given in (3), and is the set of worlds w0, such that for
every deontically accessible world w, for every entity x such that x is a donation
in that world d?(x,w), if no exception holds of that donation ?e?(x,w), a test
for Hepatitis B is carried out for that donation thepb?(x,w). We will assume
that negation has the highest precedence. Therefore ?a ? b ? (?a) ? b, and
brackets are used to resolve other ambiguities.
(3) ?w0.?w : (w ? deo(w0)? (?x : (d?(x,w) ? (?e?(x,w)? thepb?(x,w)))))
A difference between worlds in Kratzer?s denotations and states in a model
is that: in a state there is no notion of entities and relations between them.
All that is available at a state s is the set of propositions which are true at
that state ?(s). To map (3) to a form that is useful for checking conformance,
we need two assumptions.
First, we assume that regulation denotes the set of models that conform to
it. Intuitively speaking, w0 in (3) can be thought of as a model in its entirety,
and w ? deo(w0) correspond to special states in the model. A universal quan-
tification over accessible worlds can be replaced with the CTL AG operator.
We then obtain the denotation in (4), read as : on every path in M, if a state is
deontically accessible, for each donation x at that state, if no exception holds,
a test is carried out. In a model, only special states (like when the bloodbank
has finished processing all the donations it has received) need to conform to
the regulation, and deo can be thought of as marking those states.
(4) ?M. M  AG(deo? (?x : (d?(x)? (?e?(x)? thepb?(x)))))
(4) is still not in CTL because of the universal quantification over enti-
ties x at a state. The universal quantifier can be eliminated by assuming a
serial processing model. This has the effect that at the deontically accessible
states, exactly one donation is under consideration (e.g. the models in Fig-
ures 1(a) and 1(b)). In the sections of the CFR that we examined, a universal
quantification over entities is absolutely essential when these entities corre-
spond to inputs of an implemenation. This assumption lets us tie the inputs
to states, and use the quantification over states to achieve the quantification
over entities. Thus (4) can be reduced to (5).
(5) ?M. M  AG(deo? (d? ? (?e? ? thepb?)))
A problem that is encountered in taking this approach is that there is no
distinction between obligations, and permissions (both of which stem from the
Hohfeldian legal conceptions of right, duty, privilege, and no right [9]). While
this did not cause a problem for the obligation in (1), if one were to follow the
same procedure for the permission in (2), we would get the denotation in (6).
(6) ?M. M  ?(AG(deo ? (sp? ? thepb?)))
A model satisfies (6) only if there is some path in which there is a state that
is deontically accessible, and if a donation of source plasma is being processed
it is not tested. This is too strong a requirement, because an organization may
choose not to do what it is permitted to do. The model in Figure 1(a) is a
valid model, which would be declared invalid if (6) were required of it.
Another problem is that it is not clear how one would use (6) in interpreting
the exemption e? in (5). A reasonable candidate is e? ? deo ? (sp? ? ?thepb?).
But this is not the exemption because it is true in every deontically accessible
state in which a donation of source plasma is not being processed. Consider a
state s at which sp? = false (sp? 6? ?(s)). At s, e? ? (deo ? (false ? ?thepb?)) ?
(deo? true) ? true. The specification in (5), at s is: AG(deo? (?e? ?thepb?)) ?
AG(deo ? (?true ? ?thepb?)) ? AG(deo ? true) ? AG(true) ? true . Therefore, a
model that doesn?t test any donation for Hepatitis B would conform to (5).
We now turn to the task of addressing these problems by revising how the
specifications are composed.
4 Extracting the specifications
To aid the requirements engineer in extracting the specifications, the idea
is to present her with intermediate semantic representations of the sentence
with which she interacts. The intermediate representations that we use fall
into the category of abstract syntax trees (ASTs). ASTs are generally used as
intermediate representations in compiling code in a high-level programming
language to machine dependant code. The internal nodes in ASTs are oper-
ators (predicates/meta-predicates), the subtrees they dominate are operands
(arguments), and leaf nodes correspond to variables or constants (the require-
ments engineer specifies the denotation of the leaves). An AST encodes the
resolution of scope ambiguities, i.e., if p1 dominates p2 in the AST, then p1
outscopes p2.
Section 4.1 describes some phenomena in natural language that can be
used in the construction of the ASTs, and how these ASTs can be interpreted.
In Section 4.2, we describe how the ASTs and their interpretation for (1) and
(2) (in Figures 3 and 4) address the problems described in Section 3. 3
4.1 Abstract Syntax Trees (ASTs) and their interpretation
To capture the distinction between obligations and permissions, the denotation
of each node N in an AST is given by the 3-tuple: [[N ]] =
0
@
?N
ON
PN
1
A , where ON
is a set of propositional logic formulas which correspond to the obligations
that have been satisified, and PN is a set of propositional logic formulas that
correspond to the permissions that have been taken, and ?N is a propositional
logic formula which can be thought of as indicating whether N is true at a
state. The set of obligations O obtained from the policy base is the union
of the obligations obtained at the root of the AST for each sentence. The
denotation of the policy base is then given by: ?M. M  AG
0
@deo ?
^
??O
?
1
A . We
now identify various linguistic constructions that can be used to obtain ASTs.
Copy
p i z T l1, l2....ln
p
T :
[
z ? l1, i ? 1
]
... T :
[
z ? ln, i ? n
]
Fig. 2. Semantics of the Copy meta-predicate
Copy
and i z each
x is a donation of hu-
man blood or blood com-
ponent ?(1). i .1
except as
specified in (2) -
?(1). i .2
must
you, test x , for evi-
dence of infection due
to z - ?(1). i .3
the Human immuno-
deficiency vius
the Hepatitis B virus
and
?
?
?(1).1.1 ? (??(1).1.2 ? ?(1).1.3)
{?(1).1.1 ? (??(1).1.2 ? ?(1).1.3)}
{}
?
?
?
?
?(1).1.1
{}
{}
?
?
?
?
??(1).1.2 ? ?(1).1.3
{??(1).1.2 ? ?(1).1.3}
{}
?
?
?
?
?(1).1.2
{}
{}
?
?
?
?
?(1).1.3
{?(1).1.3}
{}
?
?
?
?
?(1).1.3
{}
{}
?
?
...
Fig. 3. AST and its interpretation for (1)
Distributive readings and the Copy meta-predicate: (1) is ambigu-
ous between a collective reading (where there is a single test for both the
3 We assume that obligation and permission denoting categories, e.g. must, do not occur
in contexts like antecedent clauses of subordinating conjunctions (like if), and restrictors of
determiners. Handling these cases requires an extension to CTL which is beyond the scope
of this paper.
every
x is a donation of Source
Plasma - ?2.1
not
are required
you to test x for evidence of
infection due to the Hepatitis B
virus z of this section - ?2.2
?
?
?(2).1 ? app1 ? ??(2).2
{}
{?(2).1 ? app1 ? ??(2).2}
?
?
?
?
?(2).1
{}
{}
?
?
?
?
app1 ? ??(2).2
{}
{app1 ? ??(2).2}
?
?
?
?
?(2).2
{?(2).2}
{}
?
?
?
?
?(2).2
{}
{}
?
?
Fig. 4. AST and its interpretation for (2)
diseases), and a distributive reading (where there are separate tests for each
disease). However, (2) gives an exemption to a test for one of the diseases, and
this suggests that a distributive reading may be more appropriate in the spec-
ifications extracted, and that the distributivity has scope over the exception.
Hence Copy dominates except in Figure 3.
The interpretation of the Copy meta-predicate is given in Figure 2. It is
called a meta-predicate because it is a function from an AST to another AST,
by simple variable substitution. For the AST for (1) shown in Figure 3, this
results in an AST rooted with and with subtrees corresponding to each of the
tests. The interpretation of and in this context is given by:
and
0
@
?1A
O1A
P1A
1
A ...
0
@
?nA
OnA
PnA
1
A =
0
@
?ni=1?iA
?ni=1OiA
?ni=1PiA
1
A
The RHS of the equation corresponds to the denotation of the node labeled
and in the AST (shaded in gray in Figure 3).
Universally Quantified NPs correponding to inputs: As mentioned
in Section 3, the universal quantification over inputs (donations) is achieved
by associating states with unique inputs. The interpretation of the determiner
each is designed with idea that the obligations will be evaluated at each state.
each
0
@
?A
{}
{}
1
A
0
@
?B
OB
PB
1
A =
0
@
?A ? ?B
{?A ? ?BO.j |?BO.j ? OB}
{?A ? ?BP.j |?BP.j ? PB}
1
A
The interpretation of the determiner no is similar to that of each/every,
except that a negation needs to be applied to the nuclear scope. We discuss
the interpretation of negation in what follows.
Conditional and Exceptive constructions: There are several predi-
cates that denote conditions and exceptions. For example, the subordinat-
ing conjunctions if , unless, and except as, coordinating conjunctions like
except that or but. The interpretation of if is the same as that for every. The
interpretation of predicates like except as, and unless are similar, the only
difference being that ??A is used instead of ?A in the RHS.
Modals and Negation: The semantics of modals and negation are given
below:
must
0
@
?A
{}
{}
1
A =
0
@
?A
{?A}
{}
1
A may
0
@
?A
{}
{}
1
A =
0
@
appi ? ?A
{}
{appi ? ?A}
1
A
not
0
@
?A
OA
PA
1
A =
0
@
??A
{??AP.j |?AP.j ? PA}
{appj ? ??AO.j |?AO.j ? OA}
1
A , where ??A =
?
appj ? ??A ?A ? ?AO.j ? OA
??A otherwise
must(A) results in the interpretation that ?A is an obligation. may(A)
results in the interpretation that appi ? ?A is a permission, where appi is
a variable introduced which the implementation must set to true when the
permission is applied (we discuss its use in Section 4.2). And intuitively, the
interpretation of negation captures the idea that may(?A) ? not(must(A)).
4.2 Discussion
There are two obligations obtained at the root of the AST for (1): ?(1).1.1 ?
(??(1).1.2 ? ?(1).1.3) ? d? ? (?e?1 ? thiv?) and ?(1).2.1 ? (??(1).2.2 ? ?(1).2.3) ?
d? ? (?e?2 ? thepb?) , where d? is true iff the donation is one of blood or blood
component, e?1 and e?2 are the exceptions to the required test for each disease,
and thiv? and thepb? are true iff tests for HIV and Hepatitis B respectively
have been performed. The computation of the second obligation is not shown
in Figure 3, and is obtained from the second child of and (in the AST shaded
in gray). Note that the individual propositions like d? need to be specified by
the requirements engineer at the leaf nodes of the AST.
Figure 4 shows the AST and its interpretation for (2). The permission
obtained at the root node is : ?(2).1?app1???(2).2 ? sp??app1??thepb? where sp?
is true iff a donation of source plasma is being processed, and thepb? is true iff
a test for the Hepatitis B virus has been carried out.
The use of the app1 proposition is as follows. It is possible for the regula-
tion to cancel the permission given in (2), but there may be several cases in
which permission not to test a donation of source plasma for Hepatitis B is
given. Suppose the case under consideration is one where the permission in
(2) is cancelled, but the organization doesn?t test a donation of source plasma
for Hepatitis B because a different permission can be applied. Since the per-
mission being applied sets thepb? to false, and sp? is true, the only way for the
implementation to indicate that the permission in (2) is not being applied is
by setting app1 to false. Setting e?1 ? false, and e?2 ? sp? ? app1 ? ?thepb?:
?O.1 ? d? ? (?false? thiv?), and ?O.2 ? d? ? (?(sp? ? app1 ? ?thepb?)? thepb?)
Considering just these obligations, the denotation of the regulatory doc-
ument would be: ?M. M  AG(deo ? (?O.1 ? ?O.2)) . Therefore, a bloodbank
could decide not to test a donation of source plasma for Hepatitis B, but they
would always have to test a donation for HIV.
5 Conclusions and Future Work
We have described a framework to assist a requirements engineer in extracting
CTL specifications from regulatory documents. An account of obligations and
permissions turns out to be essential in composing the specifications. The
composition procedure (defined in Section 4) was applied to a large part of
the FDA CFR 610.40. While it does seem to scale well, providing tool support
to extract and interact with the ASTs is vital. To this end, we plan to conduct
a small scale annotation of ASTs which will let us determine the accuracy with
which these representations can be computed. On the user interface side, we
are working on ways of presenting the ASTs to the requirements engineer.
References
[1] Breuker, J. and N. den Haan, Separating world and regulation knowledge: where is the
logic?, in: M. Sergot, editor, Proceedings of the third international conference on AI
and Law (1991), pp. 41?51.
[2] Clarke, E. M. and E. A. Emerson, Synthesis of synchronization skeletons for branching
time temporal logic, in: Logic of Programs: Workshop, 1981.
[3] Clarke, E. M., E. A. Emerson and A. P. Sistla, Automatic verification of finite-
state concurrent systems using temporal logic specifications, ACM Transactions on
Programming Languages and Systems 8 (1986), pp. 244?263.
[4] Clarke, E. M. and J. M. Wing, Formal methods: State of the art and future directions,
ACM Computing Surveys 28 (1996), pp. 626?643.
[5] Corbett, J. C., M. B. Dwyer, J. Hatcliff, S. Laubach, C. S. Pasareanu, Robby and
H. Zheng, Bandera: Extracting finite-state models from java source code, in: Proceedings
of the International Conference on Software Engineering (ICSE), 2000.
[6] Fantechi, A., S. Gnesi, G. Ristori, M. Carenini, M. Marino and M. Moreschini, Assisting
requirements formalization by means of natural language translation, Formal Methods
in System Design 4 (1994), pp. 243?263.
[7] Fuchs, N. and R. Schwitter, Attempto controlled english (ace), in: First International
Workshop on Controlled Language Applications, 1996.
[8] Glasse, E., T. V. Engers and A. Jacobs, Power: An integrated method for legislation and
regulations from their design to their use in e-government services and law enforcement,
in: M.-F. Moens, editor, Digitale Wetgeving, Digital Legislation, Die Keure Brugge, 2003
pp. 175?204, iSBN 90 5958 039 7.
[9] Hohfeld, W. N., Fundamental legal conceptions as applied in judicial reasoning, Yale
Law Journal 23 (1913), pp. 16?59.
[10] Holt, A. and E. Klein, A semantically-derived subset of English for hardware
verification, in: 37th Annual Meeting of the ACL, 1999.
[11] Holzmann, G., The Spin model checker, IEEE Trans. on Software Engineering 23
(1997), pp. 279?295.
[12] Kratzer, A., The notational category of modality, in: H.-J. Eikmeyer and H. Rieser,
editors, Words, Worlds, and Contexts. New approaches to Word Semantics, deGruyter,
Berlin, 1981 .
[13] Queille, J. P. and J. Sifakis, Specification and verification of concurrent systems in
CAESAR, in: Proceeding of the Fifth ISP, 1981.
Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 49?56,
Rochester, New York, April 2007. c?2007 Association for Computational Linguistics
Discriminative word alignment by learning the alignment structure
and syntactic divergence between a language pair
Sriram Venkatapathy1
Language Technologies Research
Centre, IIIT -Hyderabad
Hyderabad - 500019, India.
sriram@research.iiit.ac.in
Aravind K. Joshi
Department of Computer and
Information Science and Institute for
Research in Cognitive Science,
University of Pennsylvania, PA, USA.
joshi@linc.cis.upenn.edu
Abstract
Discriminative approaches for word align-
ment have gained popularity in recent
years because of the flexibility that they
offer for using a large variety of features
and combining information from various
sources. But, the models proposed in the
past have not been able to make much use
of features that capture the likelihood of an
alignment structure (the set of alignment
links) and the syntactic divergence be-
tween sentences in the parallel text. This is
primarily because of the limitation of their
search techniques. In this paper, we pro-
pose a generic discriminative re-ranking
approach for word alignment which allows
us to make use of structural features effec-
tively. These features are particularly use-
ful for language pairs with high structural
divergence (like English-Hindi, English-
Japanese). We have shown that by us-
ing the structural features, we have ob-
tained a decrease of 2.3% in the absolute
value of alignment error rate (AER). When
we add the cooccurence probabilities ob-
tained from IBM model-4 to our features,
we achieved the best AER (50.50) for the
English-Hindi parallel corpus.
1 Introduction
In this paper, we propose a discriminative re-
ranking approach for word alignment which al-
lows us to make use of structural features effec-
tively. The alignment algorithm first generates
11Part of the work was done at Institute for Research
in Cognitive Science (IRCS), University of Pennsylvania,
Philadelphia, PA 19104, USA, when he was visiting IRCS
as a Visiting Scholar, February to December, 2006.
a list of k-best alignments using local features.
Then it re-ranks this list of k-best alignments us-
ing global features which consider the entire align-
ment structure (set of alignment links) and the syn-
tactic divergence that exists between the sentence
pair. Use of structural information associated with
the alignment can be particularly helpful for lan-
guage pairs for which a large amount of unsuper-
vised data is not available to measure accurately
the word cooccurence values but which do have a
small set of supervised data to learn the structure
and divergence across the language pair. We have
tested our model on the English-Hindi language
pair. Here is an example of an alignment between
English-Hindi which shows the complexity of the
alignment task for this language pair.
Figure 1: An example of an alignment between an
English and a Hindi sentence
To learn the weights associated with the param-
eters used in our model, we have used a learning
framework called MIRA (The Margin Infused Re-
laxed Algorithm) (McDonald et al, 2005; Cram-
mer and Singer, 2003). This is an online learning
algorithm which looks at one sentence pair at a
time and compares the k-best predictions of the
alignment algorithm with the gold alignment to
update the parameter weights appropriately.
In the past, popular approaches for doing word
alignment have largely been generative (Och and
Ney, 2003; Vogel et al, 1996). In the past cou-
ple of years, the discriminative models for doing
word alignment have gained popularity because of
49
the flexibility they offer in using a large variety of
features and in combining information from vari-
ous sources.
(Taskar et al, 2005) cast the problem of align-
ment as a maximum weight bipartite matching
problem, where nodes correspond to the words
in the two sentences. The link between a pair
of words, (ep,hq) is associated with a score
(score(ep,hq)) reflecting the desirability of the ex-
istence of the link. The matching problem is
solved by formulating it as a linear programming
problem. The parameter estimation is done within
the framework of large margin estimation by re-
ducing the problem to a quadratic program (QP).
The main limitation of this work is that the fea-
tures considered are local to the alignment links
joining pairs of words. The score of an align-
ment is the sum of scores of individual alignment
links measured independently i.e., it is assumed
that there is no dependence between the align-
ment links. (Lacoste-Julien et al, 2006) extend
the above approach to include features for fertil-
ity and first-order correlation between alignment
links of consecutive words in the source sentence.
They solve this by formulating the problem as a
quadratic assignment problem (QAP). But, even
this algorithm cannot include more general fea-
tures over the entire alignment. In contrast to the
above two approaches, our approach does not im-
pose any constraints on the feature space except
for fertility (?1) of words in the source language.
In our approach, we model the one-to-one and
many-to-one links between the source sentence
and target sentence. The many-to-many alignment
links are inferred in the post-processing stage us-
ing simple generic rules. Another positive aspect
of our approach is the application of MIRA. It, be-
ing an online approach, converges fast and still re-
tains the generalizing capability of the large mar-
gin approach.
(Moore, 2005) has proposed an approach which
does not impose any restrictions on the form of
model features. But, the search technique has cer-
tain heuristic procedures dependent on the types
of features used. For example, there is little vari-
ation in the alignment search between the LLR
(Log-likelihood ratio) based model and the CLP
(Conditional-Link Probability) based model. LLR
and CLP are the word association statistics used
in Moore?s work (Moore, 2005). In contrast to
the above approach, our search technique is more
general. It achieves this by breaking the search
into two steps, first by using local features to get
the k-best alignments and then by using struc-
tural features to re-rank the list. Also, by using
all the k-best alignments for updating the parame-
ters through MIRA, it is possible to model the en-
tire inference algorithm but in Moore?s work, only
the best alignment is used to update the weights
of parameters. (Fraser and Marcu, 2006) have
proposed an algorithm for doing word alignment
which applies a discriminative step at every iter-
ation of the traditional Expectation-Maximization
algorithm used in IBM models. This model still
relies on the generative story and achieves only a
limited freedom in choosing the features. (Blun-
som and Cohn, 2006) do word alignment by com-
bining features using conditional random fields.
Even though their approach allows one to include
overlapping features while training a discrimina-
tive model, it still does not allow us to use fea-
tures that capture information of the entire align-
ment structure.
In Section 2, we describe the alignment search
in detail. Section 3 describes the features that
we have considered in our paper. Section 4 talks
about the Parameter optimization. In Section 5,
we present the results of our experiments. Section
6 contains the conclusion and our proposed future
work.
2 Alignment Search
The goal of the word alignment algorithm is to link
words in the source language with words in the tar-
get language to get the alignments structure. The
best alignment structure between a source sen-
tence and a target sentence can be predicted by
considering three kinds of information, (1) Prop-
erties of alignment links taken independently, (2)
Properties of the entire alignment structure taken
as a unit, and (3) The syntactic divergence between
the source sentence and the target sentence, given
the alignment structure. Using the set of alignment
links, the syntactic structure of the source sentence
is first projected onto the target language to ob-
serve the divergence.
Let ep and hq denote the source and target
words respectively. Let n be the number of words
in source sentence and m be the number of words
in target sentence. Let S be the source sentence
and T be the target sentence.
50
2.1 Populate the Beam
The task in this step is to obtain the k-best candi-
date alignment structures using the local features.
The local features mainly contain the cooccurence
information between a source and a target word
and are independent of other alignment links in
the sentence pair. Let the local feature vector be
denoted as fL(ep, hq). The score of a particular
alignment link is computed by taking a dot prod-
uct of the weight vector W with the local feature
vector of the alignment link. More formally, the
local score of an alignment link is
scoreL(ep, hq) = W.fL(ep, hq)
The total score of an alignment structure is com-
puted by adding the scores of individual alignment
links present in the alignment. Hence, the score of
an alignment structure a? is,
scoreLa(a?, S, T ) =
?
(ep,hq)?a?
scoreL(ep, hq)
We have proposed a dynamic programming al-
gorithm of worst case complexity O(nm2 + nk2)
to compute the k-best alignments. First, the local
score of each source word with every target word
is computed and stored in local beams associated
with the source words. The local beams corre-
sponding to all the source words are sorted and the
top-k alignment links in each beam are retained.
This operation has the worst-case complexity of
O(nm2).
Now, the goal is to get the k-best alignments in
the global beam. The global beam initially con-
tains no alignments. The k best alignment links of
the first source word e0 are added to the global
beam. To add the alignment links of the next
source word to the global beam, the k2 (if k < m)
combinations of the alignments in the global beam
and alignments links in the local beam are taken
and the best k are retained in the global beam.
If k > m, then the total combinations taken are
mk. This is repeated till the entries in all the lo-
cal beams are considered, the overall worst case
complexity being O(nk2) (or O(nmk) if k > m).
2.2 Reorder the beam
We now have the k-best alignments using the local
features from the last step. We then use global fea-
tures to reorder the beam. The global features look
at the properties of the entire alignment structure
instead of the alignment links locally.
Let the global feature vector be represented as
fG(a?). The global score is defined as the dot prod-
uct of the weight vector and the global feature vec-
tor.
scoreG(a?) = W.fG(a?)
The overall score is calculated by adding the local
score and the global score.
score(a?) = scoreLa(a?) + scoreG(a?)
The beam is now sorted based on the overall scores
of each alignment. The alignment at the top of
the beam is the best possible alignment between
source sentence and the target sentence.
2.3 Post-processing
The previous two steps produce alignment struc-
tures which contain one-to-one and many-to-one
links. In this step, the goal is to extend the best
alignment structure obtained in the previous step
to include the other alignments links of one-to-
many and many-to-many types.
The majority of the links between the source
sentence and the target sentence are one-to-one.
Some of the cases where this is not true are the in-
stances of idioms, alignment of verb groups where
auxiliaries do not correspond to each other, the
alignment of case-markers etc. Except for the
cases of idioms in target language, most of the
many-to-many links between a source and target
sentences can be inferred from the instances of
one-to-one and many-to-one links using three lan-
guage language specific rules (Hindi in our case)
to handle the above cases. Figure 1, Figure 2 and
Figure 3 depict the three such cases where many-
to-many alignments can be inferred. The align-
ments present at the left are those which can be
predicted by our alignment model. The alignments
on the right side are those which can be inferred in
the post-processing stage.
.....  are  playing ......
....... khel rahe hain 
.....  are  playing ......
....... khel rahe hain 
    
   (play  cont  be)
Figure 2: Inferring the many-to-many alignments
of verb and auxiliaries
After applying the language specific rules, the
dependency structure of the source sentence is tra-
versed to ensure the consistency of the alignment
51
John  ne  ....
John ..........
John  ne  ....
John ..........
Figure 3: Inferring the one-to-many alignment to
case-markers in Hindi
... kicked the bucket 
..........  mara gaya
... kicked the bucket 
..........  mara gaya
 (die   go?light verb)
Figure 4: Inferring many-to-many alignment for
source idioms
structure. If there is a dependency link between
two source words eo and ep, where eo is the head
and ep is the modifier and if eo and ep are linked
to one or more common target word(s), it is log-
ical to imagine that the alignment should be ex-
tended such that both eo and ep are linked to the
same set of target words. For example, in Figure 4,
new alignment link is first formed between ?kick?
and ?gayA? using the language specific rule, and
as ?kick? and ?bucket? are both linked to ?mara?,
?bucket? is also now linked to ?gayA?. Similarity,
?the? is linked to both ?mara? and ?gayA?. Hence,
the rules are applied by traversing through the de-
pendency tree associated with the source sentence
words in depth-first order. The dependency parser
used by us was developed by (Shen, 2006). The
following summarizes this step,
? Let w be the next word considered in the dependency
tree, let pw be the parent of w.
? If w and pw are linked to one or more common
word(s) in target language, align w to all target
words which are aligned to pw.
? Else, Use the target-specific rules (if they match)
to extend the alignments of w.
? Recursively consider all the children of w
3 Parameters
As the number of training examples is small, we
chose to use features (both local and structural)
which are generic. Some of the features which we
used in this experiment are as follows:
3.1 Local features (FL)
The local features which we consider are mainly
co-occurrence features. These features estimate
the likelihood of a source word aligning to a tar-
get word based on the co-occurrence information
obtained from a large sentence aligned corpora1.
3.1.1 DiceWords
Dice Coefficient of the source word and the tar-
get word (Taskar et al, 2005).
DCoeff(ep, hq) = 2 ? Count(ep, hq)Count(ep) + Count(hq)
where Count(ep, hq) is the number of times the
word hq was present in the translation of sentences
containing the word ep in the parallel corpus.
3.1.2 DiceRoots
Dice Coefficient of the lemmatized forms of the
source and target words. It is important to consider
this feature for language pairs which do not have a
large unsupervised sentence aligned corpora. Co-
occurrence information can be learnt better after
we lemmatize the words.
3.1.3 Dict
This feature tests whether there exists a dictio-
nary entry from the source word ep to the target
word hq. For English-Hindi, we used a medium-
coverage dictionary (25000 words) available from
IIIT - Hyderabad, India 2.
3.1.4 Null POS
These parameters measures the likelihood of a
source word with a particular part of speech tag3 to
be aligned to no word (Null) on the target language
side. This feature was extremely useful because
it models the cooccurence information of words
with nulls which is not captured by the features
DiceWords and DiceRoots. Here are some of the
features of this type with extreme estimated pa-
rameter weights.
3.2 Lemmatized word pairs
The word pairs themselves are a good indicator
of whether an alignment link exists between the
word pair or not. Also, taking word-pairs as fea-
ture helps in the alignment of some of the most
common words in both the languages. A variation
of this feature was used by (Moore, 2005) in his
paper.
150K sentence pairs originally collected as part of TIDES
MT project and later refined at IIIT-Hyderabad, India.
2http://ltrc.iiit.ac.in/onlineServices/Dictionaries/Dict Frame.html
3We have limited the number of POS tags by considering
only the first alphabets of Penn Tags as our POS tag cate-
gories
52
Param. weight Param. weight
Null ? 0.2737 null C -0.7030
Null U 0.1969 null D -0.6914
Null L 0.1814 null V -0.6360
Null . 0.0383 null N -0.5600
Null : 0.0055 null I -0.4839
Table 1: Top Five Features each with Maximum
and Minimum weights
Other parameters like the relative distance be-
tween the source word ep and the target word hq,
RelDist(ep, hq) = abs(j/|e| ? k/|h|), which are
mentioned as important features in the previous
literature, did not perform well for the English-
Hindi language pair. This is because of the pre-
dominant word-order variation between the sen-
tences of English and Hindi (Refer Figure 1).
3.3 Structural Features (FG)
The global features are used to model the prop-
erties of the entire alignment structure taken as a
unit, between the source and the target sentence.
In doing so, we have attempted to exploit the syn-
tactic information available on both the source and
the target sides of the corpus. The syntactic infor-
mation on the target side is obtained by projecting
the syntactic information of the source using the
alignment links. Some of the features which we
have used in our work are in the following subsec-
tion.
3.3.1 Overlap
This feature considers the instances in a sen-
tence pair where a source word links to a target
word which is a participant in more than one align-
ment links (has a fertility greater than one). This
feature is used to encourage the source words to
be linked to different words in the target language.
For example, we would prefer the alignment in
Figure 6 when compared to the alignment in Fig-
ure 5 even before looking at the actual words. This
parameter captures such prior information about
the alignment structure.
Figure 5: Alignment where many source words are
linked to one target word
Figure 6: Alignment where the source words are
aligned to many different target words
Formally, it is defined as
Overlap(a?) =
?
hq?T,Fert(hq)>1 Fert
2(hq)
?
h?T Fert(h)
where T is the Hindi sentence. ? Fert2(hq) is
measured in the numerator so that a more uniform
distribution of target word fertilities be favored in
comparison to others. The weight of overlap as
estimated by our model is -6.1306 which indicates
the alignments having a low overlap value are pre-
ferred.
3.3.2 NullPercent
This feature measures the percentage of words
in target language sentence which are not aligned
to any word in the source language sentence. It is
defined as
NullPercent =
|hq|hq?T,Fertility(hq)==0
|h|h?T
3.3.3 Direction DepPair
The following feature attempts to capture the
first order interdependence between the alignment
links of pairs of source sentence words which are
connected by dependency relations. One way in
which such an interdependence can be measured
is by noting the order of the target sentence words
linked to the child and parent of a source sentence
dependency relation. Figures 7, 8 and 9 depict
the various possibilities. The words in the source
sentence are represented using their part-of-speech
tags. These part-of-speech tags are also projected
onto the target words. In the figures p is the parent
and c is the part-of-speech of the child.
p c
c p
Figure 7: Target word linked to a child precedes
the target word linked to a parent
53
p c
p c
Figure 8: Target word linked to a parent precedes
the target word linked to a child
p c
p c
Figure 9: Parent and the child are both linked to
same target word
The situation in Figure 9 is an indicator that the
parent and child dependency pair might be part or
whole of a multi-word expression on the source
side. This feature thus captures the divergence be-
tween the source sentence dependency structure
and the target language dependency structure (in-
duced by taking the alignment as a constraint).
Hence, in the test data, the alignments which do
not express this divergence between the depen-
dency trees are penalized. For example, the align-
ment in Figure 10 will be heavily penalized by
the model during re-ranking step primarily for two
reasons, 1) The word aligned to the preposition
?of? does not precede the word aligned to the noun
?king? and 2) The word aligned to the preposition
?to? does not succeed the word aligned to the noun
?king?.
......... to the king of Rajastan .......
......  Rajastan  ke   Raja  ko   ..........
( Rajastan   of    King   to  )
Figure 10: A simple example of an alignment
that would be penalized by the feature Direc-
tion DepPair
3.3.4 Direction Bigram
This feature is a variation of the previous fea-
ture. In the previous feature, the dependency pair
on the source side was projected to the target side
to observe the divergence of the dependency pair.
In this feature, we take a bigram instead of a de-
pendency pair and observe its order in the target
side. This feature is equivalent to the first-order
features used in the related work.
There are three possibilities here, (1) The words
of the bigram maintain their order when projected
onto the target words, (2) The words of the bigram
are reversed when projected, (3) Both the words
are linked to the same word of the target sentence.
4 Online large margin training
For parameter optimization, we have used an on-
line large margin algorithm called MIRA (Mc-
Donald et al, 2005) (Crammer and Singer, 2003).
We will briefly describe the training algorithm that
we have used. Our training set is a set of English-
Hindi word aligned parallel corpus. Let the num-
ber of sentence pairs in the training data be t. We
have {Sr, Tr, a?r} for training where r ? t is the
index number of the sentence pair {Sr, Tr} in the
training set and a?r is the gold alignment for the
pair {Sr, Tr}. Let W be the weight vector which
has to be learnt, Wi be the weight vector after the
end of ith update. To avoid over-fitting, W is ob-
tained by averaging over all the weight vectors Wi.
A generic large margin algorithm is defined
follows for the training instances {Sr, Tr, a?r},
Initialize W0, W , i
for p = 1 to Iterations do
for r = 1 to t do
Get K-Best predictions ?r = {a1, a2...ak}
for the training example (Sr, Tr, a?r)
using the current model W i and applying
step 1 and 2 of section 4. Compute W i+1
by updating W i based on
(Sr, Tr, a?r, ?r).
i = i + 1
W = W + W i+1
W = WIterations?m
end for
end for
The goal of MIRA is to minimize the change in
W i such that the score of the gold alignment a? ex-
ceeds the score of each of the predictions in ? by a
margin which is equal to the number of mistakes in
the predictions when compared to the gold align-
ment. One could choose a different loss function
which assigns greater penalty for certain kinds of
mistakes when compared to others.
Step 4 (Get K-Best predictions) in the algo-
54
rithm mentioned above can be substituted by the
following optimization problem,
minimize ?(W i+1 ? W i)?
s.t. ?k, score(a?r, Sr, Tr)? score(aq,k, Sr, Tr)
>= Mistakes(ak, a?r, Sr, Tr)
For optimization of the parameters, ideally, we
need to consider all the possible predictions and
assign margin constraints based on every predic-
tion. But, here the number of such classes is ex-
ponential and therefore we restrict ourselves to the
k ? best predictions.
We estimate the parameters in two steps. In the
first step, we estimate only the weights of the lo-
cal parameters. After that, we keep the weights
of local parameters constant and then estimate the
weights of global parameters. It is important to
decouple the parameter estimation to two steps.
We also experimented estimating the parameters
in one stage but as expected, it had an adverse
impact on the parameter weights of local features
which resulted in generation of poor k-best list af-
ter the first step while testing.
5 Experiments and Results
5.1 Data
We have used English-Hindi unsupervised data of
50000 sentence pairs4. This data was used to ob-
tain the cooccurence statistics such as DiceWords
and DiceRoots which we used in our model. This
data was also used to obtain the predictions of
GIZA++ (Implements the IBM models and the
HMM model). We take the alignments of GIZA++
as baseline and evaluate our model for the English-
Hindi language pair.
The supervised training data which is used to
estimate the parameters consists of 4252 sentence
pairs. The development data consists of 100 sen-
tence pairs and the test data consists of 100 sen-
tence pairs. This supervised data was obtained
from IRCS, University of Pennsylvania. For train-
ing our model, we need to convert the many-to-
many alignments in the corpus to one-to-one or
may-to-one alignments. This is done by applying
inverse operations of those performed during the
post-processing step (section 2.3).
4Originally collected as part of TIDES MT project and
later refined at IIIT-Hyderabad, India.
5.2 Experiments
We first obtain the predictions of GIZA++ to ob-
tain the baseline accuracies. GIZA++ was run in
four different modes 1) English to Hindi, 2) Hindi
to English, 3) English to Hindi where the words in
both the languages are lemmatized and 4) Hindi to
English where the words are lemmatized. We then
take the intersections of the predictions run from
both the directions (English to Hindi and Hindi to
English). Table 2 contains the results of experi-
ments with GIZA++. As the recall of the align-
ment links of the intersection is very low for this
dataset, further refinements of the alignments as
suggested by (Och and Ney, 2003) were not per-
formed.
Mode Prec. Rec. F-meas. AER
Normal: Eng-Hin 47.57 40.87 43.96 56.04
Normal: Hin-Eng 47.97 38.50 42.72 57.28
Normal: Inter. 88.71 27.52 42.01 57.99
Lemma.: Eng-Hin 53.60 44.58 48.67 51.33
Lemma.: Hin-Eng 53.83 42.68 47.61 52.39
Lemma.: Inter. 86.14 32.80 47.51 52.49
Table 2: GIZA++ Results
In Table 3, we observe that the best result
(51.33) is obtained when GIZA++ is run after lem-
matizing the words on the both sides of the unsu-
pervised corpus. The best results obtained without
lemmatizing is 56.04 when GIZA++ is run from
English to Hindi.
The table 4 summarizes the results when we
used only the local features in our model.
Features Prec. Rec. F-meas. AER
DiceRoots 41.49 38.71 40.05 59.95
+ DiceWords
+ Null POS 42.82 38.29 40.43 59.57
+ Dict. 43.94 39.30 41.49 58.51
+ Word pairs 46.27 41.07 43.52 56.48
Table 3: Results using local features
We now add the global features. While esti-
mating the parameter weights associated with the
global features, we keep the weights of local fea-
tures constant. We choose the appropriate beam
size as 50 after testing with several values on the
development set. We observed that the beam sizes
(between 10 and 100) did not affect the alignment
error rates very much.
55
Features Prec. Rec. F-meas. AER
Local feats. 46.27 41.07 43.52 56.48
Local feats. 48.17 42.76 45.30 54.70
+ Overlap
Local feats. 47.93 42.55 45.08 54.92
+ Direc. Deppair
Local feats. 48.31 42.89 45.44 54.56
+ Direc. Bigram
Local feats. 48.81 43.31 45.90 54.10
+ All Global feats.
Table 4: Results after adding global features
We see that by adding global features, we ob-
tained an absolute increase of about 2.3 AER sug-
gesting the usefulness of structural features which
we considered. Also, the new AER is much better
than that obtained by GIZA++ run without lem-
matizing the words.
We now add the IBM Model-4 parameters (co-
occurrence probabilities between source and tar-
get words) obtained using GIZA++ and our fea-
tures, and observe the results (Table 6). We can
see that structural features resulted in a significant
decrease in AER. Also, the AER that we obtained
is slightly better than the best AER obtained by the
GIZA++ models.
Features Prec. Rec. F-meas. AER
IBM Model-4 Pars. 48.85 43.98 46.29 52.71
+ LocalFeats
IBM Model-4 Pars. 48.95 50.06 49.50 50.50
+ All feats.
Table 5: Results after combining IBM model-4 pa-
rameters with our features
6 Conclusion and Future Work
In this paper, we have proposed a discriminative
re-ranking approach for word alignment which al-
lows us to make use of structural features effec-
tively. We have shown that by using the structural
features, we have obtained a decrease of 2.3% in
the absolute value of alignment error rate (AER).
When we combine the prediction of IBM model-4
with our features, we have achieved an AER which
is slightly better than the best AER of GIZA++
for the English-Hindi parallel corpus (a language
pair with significant structural divergences). We
expect to get large improvements when we add
more number of relevant local and structural fea-
tures. We also plan to design an appropriate de-
pendency based decoder for machine translation
to make good use of the parameters estimated by
our model.
References
Phil Blunsom and Trevor Cohn. 2006. Discriminative
word alignment with conditional random fields. In
Proceedings of the 21st COLING and 44th Annual
Meeting of the ACL, Sydney, Australia, July. ACL.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. In
Journal of Machine Learning Research.
Alexander Fraser and Daniel Marcu. 2006. Semi-
supervised training for statistical word alignment. In
Proceedings of the 21st COLING and 44th Annual
Meeting of the ACL, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Simon Lacoste-Julien, Ben Taskar, Dan Klein, and
Michael I. Jordan. 2006. Word alignment via
quadratic assignment. In Proceedings of the Human
Language Technology Conference of the NAACL,
Main Conference, pages 112?119, New York City,
USA, June. Association for Computational Linguis-
tics.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-project dependency pars-
ing using spanning tree algorithms. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 523?530, Vancouver,
British Columbia, Canada, October. Association of
Computational Linguistics.
Robert C. Moore. 2005. A discriminative frame-
work for bilingual word alignment. In Proceedings
of Human Language Technology Conference and
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 81?88, Vancouver, British
Columbia, Canada, October. Association of Compu-
tational Linguistics.
F. Och and H. Ney. 2003. A systematic comparisoin
of various statistical alignment models. In Compu-
tational Linguistics.
Libin Shen. 2006. Statistical LTAG Parsing. Ph.D.
thesis.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005. A discriminative machine approach to word
alignment. In Proceedings of HLT-EMNLP, pages
73?80, Vancouver, British Columbia, Canada, Octo-
ber. Association of Computational Linguistics.
Stefan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical
translation. In Proceedings of the 16th International
Conference on Computational Linguistics.
56
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 1?8,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Multi-Component Tree Adjoining Grammars,  
Dependency Graph Models, and Linguistic Analyses 
Joan Chen-Main* and Aravind K. Joshi*+ 
*Institute for Research in Cognitive Science, and 
+Dept of Computer and Information Science 
University of Pennsylvania 
Philadelphia, PA 19104-6228 
{chenmain,joshi}@seas.upenn.edu 
 
 
Abstract 
Recent work identifies two properties that 
appear particularly relevant to the charac-
terization of graph-based dependency mod-
els of syntactic structure1: the absence of 
interleaving substructures (well-nestedness) 
and a bound on a type of discontinuity 
(gap-degree ? 1) successfully describe 
more than 99% of the structures in two de-
pendency treebanks (Kuhlmann and Nivre 
2006).2 Bodirsky et al (2005) establish that 
every dependency structure with these two 
properties can be recast as a lexicalized 
Tree Adjoining Grammar (LTAG) deriva-
tion and vice versa. However, multi-
component extensions of TAG (MC-TAG), 
argued to be necessary on linguistic 
grounds, induce dependency structures that 
do not conform to these two properties 
(Kuhlmann and M?hl 2006). In this paper, 
we observe that several types of MC-TAG 
as used for linguistic analysis are more re-
strictive than the formal system is in prin-
ciple. In particular, tree-local MC-TAG, 
tree-local MC-TAG with flexible composi-
                                                 
1 Whereas weak equivalence of grammar classes is only con-
cerned with string sets and fails to shed light on equivalence at 
the structural level, our work involves the equivalence of deri-
vations and graph based models of dependencies.  Thus, our 
work is relevant to certain aspects of grammar engineering 
that weak equivalence does not speak to. 
2 These properties hold for many of the so-called non-
projective dependency structures and the corresponding non-
context free structures associated with TAG, further allowing 
CKY type dynamic programming approaches to parsing to 
these dependency graphs. 
tion (Kallmeyer and Joshi 2003), and spe-
cial cases of set-local TAG as used to de-
scribe certain linguistic phenomena satisfy 
the well-nested and gap degree ? 1 criteria. 
We also observe that gap degree can dis-
tinguish between prohibited and allowed 
wh-extractions in English, and report some 
preliminary work comparing the predic-
tions of the graph approach and the MC-
TAG approach to scrambling. 
1 Introduction 
Bodirsky et al (2005) introduce a class of graphi-
cal dependency models, called graph drawings 
(which differ from standard dependency struc-
tures), that are equivalent to lexicalized Tree Ad-
joining Grammar (LTAG) derivations (Joshi and 
Schabes 1997).  Whereas TAG is a generative 
framework in which each well-formed expression 
corresponds with a legitimate derivation in that 
system, the graph drawing approach provides a set 
of structures and a set of constraints on well-
formedness.  Bodirsky et al offer the class of 
graph drawings that satisfy these constraints as a 
model-based perspective on TAG.  Section 2 
summarizes this relationship between TAG deriva-
tions and these graph drawings. 
In related work, Kuhlmann and Nivre (2006) 
evaluate a number of constraints that have been 
proposed to restrict the class of dependency struc-
tures characterizing natural language with respect 
to two dependency treebanks: the Prague Depend-
ency Treebank (PDT) (Haji? et al, 2001) and the 
Danish Dependency Treebank (DDT) (Kromann, 
2003).  The results indicate that two properties 
provide good coverage of the structures in both 
1
treebanks.3  The first is a binary well-nestedness 
constraint.4  The other is a bound on gap degree, a 
graded measure of discontinuity.  These results are 
given in Table 1.  What is noteworthy is that the 
graph drawings which correspond to LTAG deriva-
tions share these two properties: LTAG induced 
graph drawings are both well-nested and have gap 
degree ? 1, and for every graph drawing that is 
both well-nested and gap degree ? 1, there exists a 
corresponding LTAG derivation (M?hl 2006). In 
section 3, these two properties are defined. 
 
property Danish Dep. 
Treebank 
Prague Dep. 
Tree-bank 
all structures n = 4393 n = 73088 
well-nested 99.89% 99.89% 
gap degree 0 84.95% 76.85% 
gap degree 1 14.89% 22.72% 
gap degree ? 1 99.84% 99.57% 
Table 1. Relevant results from Kuhlmann and 
Nivre (2006). 
 
In section 4, we show that gap degree can be 
used to distinguish between strong island viola-
tions and weak island violations in English.  This 
supports the notion that gap-degree is a linguisti-
cally relevant measure. 
Although TAG is a linguistically expressive 
formalism, a closer look at the linguistic data has 
motivated extensions of TAG.5  One of the most 
widely used extensions for handling cases that are 
difficult for classic TAG is Multi-Component TAG 
(Weir 1988). Like basic TAG, MC-TAG is a for-
malism for rewriting nodes of a tree as other trees.  
The set of underived trees are called elementary 
trees. The rewriting is accomplished via two opera-
tions: substitution, which rewrites a leaf node la-
beled X with a tree rooted in a node labeled X, and 
adjoining, which rewrites a node X with a tree that 
labels both its root and a distinguished leaf node, 
the foot, with X.  The observation that linguistic 
dependencies typically occur within some sort of 
local domain is expressed in the TAG hypothesis 
that all such dependencies occur within the basic 
                                                 
3 A third property based on edge degree also characterizes the 
structures, but has no clear relationship to TAG-derivations.  
Thus, reference to it is omitted in the remaining text. See 
Kuhlmann and Nivre (2006) for the definition of edge degree. 
4 Well-nestedness differs from projectivity.  (See section 3.) 
5 For a readable introduction, see Chapter 1 of Frank (2002). 
building blocks of the grammar.  Recursive struc-
ture is ?factored out,? which allows apparent non-
local dependencies to be recast as local ones. 
Whereas basic TAG takes the basic unit to be a 
single elementary tree, MC-TAG extends the do-
main of locality to encompass a set of elementary 
trees.  That is, these sets are the objects over which 
the combinatory operations apply. The MC-
extension allows for linguistically satisfying ac-
counts for a number of attested phenomena, such 
as: English extraposition (Kroch and Joshi 1986), 
subj-aux inversion in combination with raising 
verbs (Frank 1992), anaphoric binding (Ryant and 
Scheffler 2006), quantifier scope ambiguity (Joshi 
et al 2003), clitic climbing in Romance (Bleam 
1994), and Japanese causatives (Heycock 1986). 
The primary concern of this paper is the recon-
ciliation of the observation noted above, that MC-
TAG appears to be on the right track for a good 
generative characterization of natural language, 
with a second observation: The graph drawings 
that correspond to MC-TAG derivations, are not 
guaranteed to retain the properties of basic-TAG 
induced graph drawings.  Kuhlmann and M?hl 
(2006) report that if an entire MC set is anchored 
by a single lexical element (the natural extension 
of ?lexicalization? of TAGs to MC-TAGs), then 
the class of dependency structures is expanded 
with respect to both conditions that characterized 
the TAG-induced graph drawings: MC-TAG in-
duced graph drawings include structures that are 
not well-nested, have gap degree > 1, or both.  As 
Kuhlmann and M?hl point out, the gap degree in-
creases with the number of components, which we 
will elaborate in section 6.  This is true even if we 
require that all components of a set combine with a 
single elementary tree (i.e. tree-local MC-TAG, 
which is known to allow more derivation structures 
(i.e. derivation trees) than TAG, although they gen-
erate the same set of derived trees). If we suppose 
that the characterization of dependency structures 
as reported by Kuhlmann and Nivre (2006) for 
Czech and Danish extends cross-linguistically, i.e. 
the dependency structures for natural language 
falls within the class of well-nested and gap degree 
? 1 dependency structures, then MC-TAG appears 
to correspond to the wrong class of model-
theoretic dependency structures.  It is desirable to 
account for the apparent mismatch. 
One possibility is that the linguistic analyses that 
depend on a multi-component approach are ex-
2
tremely infrequent, and that this is reflected in the 
small proportion (< 1%) of data in the PDT and 
DDT that are not both well-nested and gap degree 
? 1.  A second possibility is that the structures in 
the PDT and DDT are actually not good represen-
tatives of the structures needed to characterize 
natural languages in general.  However, a look at 
the cases in which MC-TAG is employed reveals 
that these particular analyses yield derivations that 
correspond to graph drawings that do satisfy well-
nestedness and have gap degree ? 1.  In practice, 
MC-TAG seems to be used more restrictively than 
what the formal system allows in principle. This 
keeps the corresponding graph drawings within the 
class of structures identified by Bodirsky et al 
(2005) as a model of TAG derivations, and by 
Kuhlmann and Nivre (2006) as empirically rele-
vant.  Lastly, we compare the scrambling patterns 
that are possible in an MC-TAG extension with 
those that conform to the well-nestedness and gap 
degree ? 1 properties of the graph approach. 
2 TAG-induced Graph Dependencies 
The next two sections are intended to provide an 
intuition for the terms defined more formally in 
Bodirsky et al (2005) and Kuhlmann and Nivre 
(2006). In the former, the authors define their de-
pendency structures of interest, called graph draw-
ings, as a three-tuple: a set of nodes, a dominance 
relation, and a (total) precedence relation.  These 
dependency structures are based on information 
from both a TAG-derivation and that derivation?s 
final phrase structure.  The anchor of each elemen-
tary tree of a strictly lexicalized TAG (LTAG) is 
used as a node label in the induced dependency 
structure.  E.g. suppose tree A is anchored by lexi-
cal item a in the LTAG grammar.  Then a will be a 
node label in any dependency structure induced by 
an LTAG derivation involving tree A. 
To see how the dominance relation and prece-
dence relation mirror the derivation and the final 
derived phrase structure, let us further suppose that 
LTAG tree B is anchored by lexical item b.  Node 
a dominates node b in the dependency structure iff 
Tree A dominates tree B in the derivation struc-
ture. (I.e. tree B must substitute or adjoin into tree 
A during the TAG-derivation.6)  Node a precedes 
                                                 
6 Whereas in standard dependency graphs, adjunction of t2 to 
t1 generally corresponds to a dependency directed from t2 to 
node b in the dependency structure iff a linearly 
precedes b in the derived phrase structure tree. 
An example based on the cross-serial dependen-
cies seen in Dutch subordinate clauses is given in 
Figure 1.  In the graph drawing in (4), the four 
nodes names, {Jan, de kinderen, zag, zwemmen}, 
are the same set as the anchors of the elementary 
trees in (1), which is the same as the set of termi-
nals in (3), the derived phrase structure.  The or-
dering of these nodes is exactly the ordering of the 
terminals in (3). The directed edges between the 
nodes mirrors the immediate dominance relation 
represented in (2), the derivation structure showing 
how the trees in (1) combine.  E.g. Just as the 
zwemmen node has the zag and de kinderen nodes 
as its two children in (2), so does the zwemmen 
node dominate zag and de kinderen in (4). 
M?hl (2006) provides the formal details show-
ing that such LTAG-induced dependency struc-
tures have the properties of being 1) well-nested 
and 2) gap degree ? 1, and, conversely, that any 
structures with these properties have a correspond-
ing LTAG derivation.7  These properties are de-
fined in the next section. 
  (1) (2) 
 
 
 
    
  (2) (4) 
 
 
 
 
Figure 1. Derivation for Jan de kinderen zag 
zwemmen and corresponding graph drawing 
3 Properties of Dependency Graphs 
3.1 Gap-Degree 
It will be useful to first define the term projection. 
Definition: The projection of a node x is the set of 
nodes dominated by x (including x). (E.g. in (4), 
the projection of zag = {Jan, zag}.)  
                                                                             
t1, in a TAG-induced dependency graph, adjoining t2 to t1 
corresponds to the reverse dependency. 
7 This result refers to single graph drawings and particular 
LTAG derivation.  See Kuhlmann and M?hl (2007) on the 
relationship between sets of graph drawings and LTAGs. 
 
Y
NP X*
zag 
(saw) 
X 
NP
zwemmen
(swim) 
S 
Jan
de kinderen 
(the children) 
 
zag 
Jan 
de kinderen
zwemmen
Jan  de kinderen  zag  zwemmen 
 
Jan 
NP  
de kinderen
(the children) 
NP
 
Y
NP X*
zag
(saw)
X  
X
NP
zwemmen 
(swim) 
S 
3
Recall that the nodes of a graph drawing are in a 
precedence relation, and that this precedence rela-
tion is total. 
Definition: A gap is a discontinuity with respect to 
precedence in the projection of a node in the draw-
ing.  (E.g. in (4), de kinderen is the gap preventing 
Jan and zag from forming a contiguous interval.) 
Definition: The gap degree of a node is the num-
ber of gaps in its projection. (E.g. the gap degree of 
node zag = 1.)  
Definition: The gap degree of a drawing is the 
maximum among the gap degrees of its nodes.  
(E.g. in (4), only the projection of zag is inter-
rupted by a gap.  Thus, the gap degree of the graph 
drawing in (4) = 1.) 
In TAG drawings, a gap arises from an interrup-
tion of the dependencies in an auxiliary tree.  If B 
is adjoined into A, the gap is the material in A that 
is below the foot node of B.  E.g. in figure 1, De 
kinderen is substituted into the zwemmen tree be-
low the node into which the zag tree adjoins into 
the zwemmen tree.  Thus, de kinderen interrupts the 
pronounced material on the left of the zag tree?s 
foot node, Jan, from the pronounced material on 
the right of the foot node, zag. 
3.2 Well-Nestedness 
Definition: If the roots of two subtrees in the 
drawing are not in a dominance relation, then the 
trees are disjoint. (E.g. in (5), the subtrees rooted in 
b and c are disjoint, while the subtrees rooted in a 
and b are not.) 
Definition: If nodes x1, x2 belong to tree X, nodes 
y1, y2 belong to tree Y, precedence orders these 
nodes: x1 > y1 > x2 > y2, and X and Y are disjoint, 
then trees X and Y interleave. (E.g. in (5), b and d 
belong to the subtree rooted in b, while c and e be-
long to the subtree rooted in c.  These two subtrees 
are disjoint. Since the nodes are ordered b > c > d 
> e, the two trees interleave.) 
Definition: If there is no interleaving between dis-
joint subtrees, then a graph drawing is well-nested.  
(e.g. (4) is well-nested, but (5) is not) 
 
(5) 
 
 
Non-well nested graph drawing 
 
4 Island Effects and Gap-Degree 
When standard TAG analyses of island effects are 
adopted (see Frank 2002), we observe that differ-
ences in gap degree align with the division be-
tween wh-extractions that are attested in natural 
language (grammatical wh-movement and weak 
island effects) and those claimed to be prohibited 
(strong island effects). Specifically, four strong 
island violations, extraction from an adverbial 
modifier, relative clause, complex NP, or subject, 
correspond to structures of gap degree 1, while 
cyclic wh-movement and a weak island violation 
(extraction from a wh-island) are gap degree 0 in 
English. Interestingly, while it is clear that weak 
islands vary in their island status from language to 
language, strong islands have been claimed to 
block extraction cross-linguistically.  We tenta-
tively postulate that gap degree is useful for char-
acterizing strong islands cross-linguistically. 
An example is given in (6), a standard TAG 
derivation for adverbial modification: the after-tree 
adjoins into the buy-tree (the matrix clause), the 
got-tree substitutes into the after-tree, and the two 
arguments who and a-raise substitute into the got-
tree. In (7), the corresponding dependency struc-
ture, the projection of got includes who, which is 
separated from got by the string comprising the 
matrix clause and adverbial. Clearly, we do not 
want to claim that any gap degree of 1 is a sure 
source of ungrammaticality.  However, it is possi-
ble that a gap degree of 1 in conjunction with a wh-
element yields ungrammaticality.  For the particu-
lar set of islands we examined, we postulate that 
the projection of the node immediately dominating 
the wh-element is prohibited from containing gaps. 
 
  (6) 
 
 
 
  (7)* 
 
 
 
 
Figure 2. LTAG derivation and graph drawing 
for *Who did Jane buy a house after got a raise? 
 
a b c d e
Who     did      Jane     buy    a-house    after     got    a-raise 
Jane
got 
a-house 
buy 
did after 
Who a-raise 
4
    (8a) (8w) ? ? (11a) (11d) (11e) 
 
 
 (8b) (8c) (8d) (8e) (8x) (8y) (8z) (11b) ? ? (11c) ?  ?   
 
  (9)  (10) 
    (12) 
 
 
 
 
 
 
5 MC-TAG-induced Dependency Graphs 
5.1 Gap-Degree Beyond 1 
As reviewed in section 3, the source of every gap 
in a TAG drawing comes from an interruption of 
the dependencies in an auxiliary tree.  Since the 
auxiliary tree only has one foot, it only has a slot 
for a single gap.  A MC-set, however, could be 
comprised of two auxiliary trees.  This means there 
are slots for two gaps, one associated with each 
foot.  Furthermore, a gap may arise as a result of 
any pronounced material between the two compo-
nents.  Thus, when we already have at least one 
foot, adding an additional foot increases the maxi-
mum gap degree by 2.  The maximum gap degree 
= 1 + 2(n ? 1) = 2n ? 1, where n is the max # of 
foot nodes in any elementary tree set. 
As an example, consider the composition of the 
trees in (8), Figure 3 (Kuhlmann, p.c.) The tree set 
in (8w) is comprised of two auxiliary trees.  One 
tree, (8w?), adjoins into (8a), and a gap is created 
by the material in (8a) that falls below the foot 
node of (8w?), namely b.  When (8w?) is adjoined 
into (8?) at node V, a second gap is created below 
(8w?) by d.  A third gap is created by the material 
between the two components.  (9) shows the de-
rived phrase structure, and (10), the corresponding 
graph drawing.  The projection of node w, {w, x, y, 
z} has three discontinuities, nodes b, c, and d. 
5.2 Non-Well-Nestedness 
Kuhlmann and M?hl (2006) show that even a tree-
local MC-TAG that allows only substitution can 
induce a non-well-nested graph drawing.  Figure 4 
replicates their example.  This derivation involves 
two MC-sets, (11b) and (11c).  The tree anchored 
by d, (11d), substitutes into the second component 
of the set anchored by b, (11b).  Similarly, the tree 
anchored by e, (11e), substitutes into the second 
component of the set anchored by c, (11c).  Both 
MC-sets compose into the tree anchored by a, 
yielding the derived phrase structure in (12). The 
corresponding graph drawing is exactly our earlier 
example of non-well-nestedness in (5). 
6 MC-TAG in Practice 
We now turn to cases in which linguists have used 
MC-TAGs to account for cases argued to have no 
satisfying solution in basic TAG.   Unlike the ex-
amples in 5.1 and 5.2, these particular MC-deriva-
tions correspond to dependency structures that are 
well-nested and have gap degree ? 1. Table 2 
summarizes these cases.  The last column indicates 
the type of MC-extension assumed by the analysis: 
tree-local MC-TAGs, tree-local MC-TAGs with 
flexible composition, the mirror operation to ad-
joining; if tree ? adjoins into tree ?, the combina-
tion can be alternatively viewed as tree ? ?flexibly? 
composing with tree ? (Joshi et al 2003, Kall-
meyer and Joshi 2003)8, and set-local MC-TAGs.  
Set-local MC-TAGs are generally more powerful 
than TAGs, but since these particular cases induce 
well-nested graph drawings of gap degree ? 1, we 
can conclude that set-local MC-TAG as used in 
                                                 
8 I.e. When composing A and B, we can take A as the function 
and B as the argument or vice versa. For CFGs, such flexibil-
ity has no added benefit. For categorical type grammars, this 
kind of flexibility is accomplished via type raising, which 
allows for some new types of constituents but does not give 
rise to any new word orders. For tree local MC-TAGs, such 
flexibility does allow more word orders (permutations) to be 
generated than are possible without flexible composition. 
 
W 
B 
V 
A 
C a 
D 
E 
E 
e 
C 
c 
D 
d 
B 
b 
X 
x 
Y 
y 
Z 
z 
X Y
V 
V w Z
W 
W 
 A 
a 
X Y 
V 
V w Z 
W 
W 
E 
e 
C 
c 
D 
d 
B 
b 
x y z 
a     w    b    z      c     x    d     y     e 
 
B W 
A
C a Y 
 W
X 
B 
b 
 Y 
Z 
C 
c 
X 
d 
Z 
e 
 A 
a Y 
Z 
W 
X 
C 
c 
B 
b 
d e 
Figure 4. Non-well-nested MC-
TAG induced graph drawing Figure 3. MC-TAG induced graph drawing of gap degree 3
5
Table 2. Canonical tree sets used in MC-TAG analyses of several phenomena 
 
these cases is weakly equivalent to TAG.  
From Table 2, we can draw two generalizations.  
First, in an MC-TAG analysis, a two-component 
set is typically used.  One of the trees is often a 
very small piece of structure that corresponds to 
the ?base position,? surface position, or scope posi-
tion of a single element.  Second, the auxiliary tree 
components typically have elements with phono-
logical content only on one side of the foot. 
At this point, we make explicit an assumption 
that we believe aligns with Bodirsky et al (2005).  
Since silent elements, such as traces, do not anchor 
an elementary tree, they do not correspond to a 
node in the dependency structure. 
 
6.1 Why the Gap-Degree Remains ? 1 
Recall that in example (8), each of the two compo-
nents in the example MC-TAG has a foot with 
phonological material on both sides, giving rise to 
two gaps, and a third gap is created via the material 
between the two components.  In contrast, in the 
MC-TAG sets shown in Table 2, the auxiliary trees 
have pronounced material only on one side of the 
foot node.  This eliminates the gap that would have 
arisen due to the interruption of material on the left 
side of the foot from the right side of the foot as a 
result of the pronounced material beneath the foot.  
The only way to obtain pronounced material on 
both sides of the foot node is to adjoin a compo-
nent into one of these auxiliary trees.  Interestingly, 
the set-local analyses (in which all components of 
a set must combine with components of a single set 
vs. tree-local MC-TAG) for clitic climbing and 
Japanese causatives do posit recursive components 
adjoining into other recursive components, but 
only while maintaining all pronounced material on 
one side of the foot. In the absence of a deriva-
tional step resulting in pronounced material on 
analysis 
source 
phenomenon first  
component 
second  
component 
MC-type 
Kroch 
and Joshi  
1986 
English extraposition 
A man arrived who knew Mary. 
Auxiliary Auxiliary Tree-
local 
 
Frank 
1992 
subj-aux inversion with raising verb constructions 
Does Gabriel seem to like gnocchi? 
Non-auxiliary Auxiliary Tree-
local 
 
Ryant 
and 
Scheffler 
2006 
anaphoric binding 
Johni likes himselfi. 
Auxiliary Non-auxiliary Tree-
local + 
flexible 
compo-
sition 
Joshi, 
Kall-
meyer, & 
Romero 
2003 
quantifier scope ambiguity 
An FBI agent is spying on every professor. 
(?y [prof(y) ??x [agent(x) ? spy (x, y)] ]) OR 
(?x [agent(x) ??y [prof(y)? spy (x, y)] ]) 
Auxiliary Non-auxiliary Tree-
local + 
flexible 
compo-
sition 
Bleam 
1994 
clitic climbing in Romance 
Mari telo    quiere permitir ver. 
Mari you-it wants to permit to see 
?Mari wants to permit you to see it.? 
Auxiliary Non-auxiliary Set-
local 
Heycock 
1986 
Japanese causatives 
Watasi-wa Mitiko-ni Taroo-o ik ?ase (?sase) ?ta. 
I           TOP        DAT        ACC go  ?CS   ?CS   ?PST 
?I made Mitiko make Taroo go.? 
Auxiliary Auxiliary Set-
local 
NP*
ei 
S'
NP
 
ei 
I 
seem 
V I'* 
VP 
I' 
 
doesi 
C
 NP*  
himself 
NP 
 
S* 
NP[+wh]   knew NP  
S'i 
S 
 
NP
e 
S* Vi
VP
S  
V * 
-ase 
Vi 
V 
I 
tei
I *
I  
V 
permitir 
VP 
VP 
ei 
 S* 
 
every 
DET N 
NP 
6
both sides of a foot, the only remaining possible 
gap is that which arises from pronounced material 
that appears between the two components. 
Note that the observation about the position of 
pronounced material applies only to auxiliary trees 
in sets with multiple components.  That is, auxil-
iary trees that comprise a singleton set may still 
have pronounced material on both sides of the foot. 
6.2 Why the Structures Remain Well-Nested 
Since Kuhlmann and M?hl (2006) show that even 
a MC-TAG that allows only non-auxiliary trees in 
MC-sets will expand the drawings to include non-
well-nested drawings, there is no way to pare back 
the MC-TAG via restrictions on the types of trees 
allowed in MC-sets so as to avoid interleaving. 
Recall that to satisfy the definition of interleav-
ing, it is necessary that the two MC-sets are not in 
any dominance relation in the derivation structure.  
In Kuhlmann and M?hl?s example, this is satisfied 
because the two MC-sets are sisters in the deriva-
tion; they combine into the same tree.  In the lin-
guistic analyses considered here, no more than one 
MC-set combines into the same tree.  For tree-local 
MC-TAG, it appears to be sufficient to bar more 
than one MC-set from combining into a single tree.   
7 MC-TAG and Scrambling 
In subordinate clauses in Standard German, the 
canonical order of verbs and their subject argu-
ments is a nested dependency order. However, 
other orderings are also possible.  For example, in 
the case of a clause-final cluster of three verbs, the 
canonical order is as given in (13), NP1NP2NP3 
V3V2V1, but all the other permutations of the NP 
arguments are also possible orderings. All six per-
mutations of the NPs can be derived via tree-local 
MC-TAG.  From the graph-model perspective 
adopted here, this is unsurprising: All the se-
quences are well-nested and have gap degree ? 1. 
 
(13)   NP1   NP2   NP3   V3                 V2       V1 
 . . . Hans Peter Marie schwimmen lassen sah 
. . . Hans Peter Marie swim            make  saw 
 ? . . . Hans saw Peter make Marie swim.? 
 
However, with an additional level of embed-
ding, i.e. four NPs and four verbs, the situation is 
different, both linguistically and formally.  Our 
focus is on making the formal predictions of a lin-
guistically informed system precise. We start with 
a tree-local MC-TAG that is restricted to linguisti-
cally motivated tree-sets and to semantically co-
herent derivations.  The former linguistic restric-
tion is illustrated in (14), the possible tree-sets an-
chored by a verb that takes a VP argument.  The 
latter linguistic restriction is that there is no seman-
tic feature clash at any stages of the derivation: the 
VP argument of Vi must be associated with Vi+1. 
 
(14) ? 
 
 ? 
 
Single and two-component sets for Vi 
 
As MC-TAG is enriched in various ways (by al-
lowing flexible composition, multiple adjoining at 
the same node, and/or components from the same 
MC-set to target the same node), all 24 orderings 
where the nouns permute while the verbs remain 
fixed can be derived. (We are aware that German 
also allows verbs to scramble.) Taking the depend-
ency structures of these sequences to consist of an 
edge from each verb Vi to its subject NP and to the 
head of its argument VP, Vi+1, we can compare the 
predictions of the graph drawing approach and the 
MC-TAG approach. It turns out that the permuta-
tions of gap degree ? 1 and those of gap-degree 2 
do not align in an obvious way with particular en-
richments. For example, NP4NP2NP3NP1V4V3V2V1 
(gap degree 2) is derivable via basic tree-local MC-
TAG, but NP3NP1NP4NP2V4V3V2V1 and 
NP3NP2NP4NP1V4V3V2V1 (also gap degree 2) ap-
pear to require both flexible composition and al-
lowing components from the same MC-set to tar-
get the same node. 
8 Conclusion and Future Work 
This paper reviews the connection established in 
previous work between TAG derivations and 
model-theoretic graph drawings, i.e. well-nested 
dependency structures of gap degree ? 1, and re-
ports several observations that build on this work.  
First, additional evidence of the linguistic rele-
vance of the gap degree measure is given. The gap 
degree measure can distinguish wh-movement that 
is assumed to be generally disallowed from wh-
movement that is permitted in natural language. 
Second, we observe that the graph drawings in-
 
NPi
NP VP(i+1)*
VP
VP
VP 
(e)i Vi
 
NPi 
NP VP(i+1)* 
VP* 
VP 
VP 
VP 
(e)i Vi
7
duced by MC-TAGs used in linguistic analyses 
continue to fall within the class of well-nested, gap 
degree ? 1 dependency structures. While 
Kuhlmann and M?hl (2006) show that MC-TAGs 
in which each set has a single lexical anchor in-
duce graph drawings that are outside this class, this 
extra complexity in the dependency structures does 
not appear to be utilized. Even for the crucial cases 
used to argue for MC-extensions, MC-TAG is used 
in a manner requiring less complexity than the 
formal system allows.  Examining these particular 
grammars lays the groundwork for identifying a 
natural class of MC-TAG grammars whose deriva-
tions correspond to well-nested graph drawings of 
gap degree ? 1. Specifically, the observations sug-
gest the class to be MC-TAGs in which 1) compo-
nent sets have up to two members, 2) auxiliary 
trees that are members of non-singleton MC-sets 
have pronounced material on only one side of the 
foot, whether the auxiliary member is derived or 
not, and 3) up to one MC-set may combine into 
each tree. Though these constraints appears stipu-
lative from a formal perspective, a preliminary 
look suggests that natural language will not require 
their violation. That is, we may find linguistic jus-
tification for these constraints.  Lastly, in ongoing 
work, we explore how allowing flexible composi-
tion and multiple adjoining enables MC-TAGs to 
derive a range of scrambling patterns. 
References 
Tonia Bleam. 2000. Clitic climbing and the power of 
Tree Adjoining Grammar. In A. Abeill? and O. Ram-
bow (eds.), Tree Adjoining Grammars: formalisms, 
linguistic analysis and processing. Stanford: CSLI 
Publications, 193-220. (written in 1994). 
Manuel Bodirsky, Marco Kuhlmann, and Mathias M?hl. 
2005. Well-nested drawings as models of syntactic 
structure.  In 10th Conference of Formal Grammar 
and 9th Meeting on Mathematics of Language (FG-
MoL), Edinburgh, UK. 
Robert Frank. 1992. Syntactic Locality and Tree Adjoin-
ing Grammar: grammatical, acquisition, and proc-
essing perspectives. PhD dissertation, University of 
Pennsylvania, Philadelphia, USA. 
Robert Frank. 2002. Phrase Structure Composition and 
Syntactic Dependencies. MIT Press. 
Jan Haji?, Barbora Vidova Hladka, Jarmila Panevov?, 
Eva Haji?ov?, Petr Sgall, and Petr Pajas. 2001. Pra-
gue Dependency Treebank 1.0. LDC, 2001T10. 
Caroline Heycock. 1986. The structure of the Japanese 
causative. Technical Report MS-CIS-87-55, Univer-
sity of Pennsylvania. 
Aravind K. Joshi, Laura Kallmeyer, and Maribel Ro-
mero. 2003. Flexible composition in LTAG: quanti-
fier scope and inverse linking. In H. Bunt and R. 
Muskens (eds.), Computing Meaning 3. Dordrecht: 
Kluwer. 
Aravind K. Joshi and Y. Schabes. 1997. Tree-Adjoining 
Grammars. In G. Rozenberg and A. Salomaa (eds.): 
Handbook of Formal Languages. Berlin: Springer, 
69?123. 
Laura Kallmeyer and Aravind K. Joshi. 2003. Factoring 
predicate argument and scope semantics: underspeci-
fied semantics with LTAG. Research on Language 
and Computation 1(1-2), 3-58. 
Anthony Kroch and Aravind K. Joshi. 1990. Extraposi-
tion in a Tree Adjoining Grammar. In G. Huck and 
A. Ojeda, eds., Syntax and Semantics: Discontinuous 
Constituents, 107-149. 
Matthias Trautner Kromann. 2003. The Danish Depend-
ency Treebank and the DTAG treebank tool. In 2nd 
Workshop on Treebanks and Linguistic Theories 
(TLT), 217-220. 
Marco Kuhlmann and Mathias M?hl. 2006. Extended 
cross-serial dependencies in Tree Adjoining Gram-
mars. In Proceedings of the 8th International Work-
shop on Tree Adjoining Grammar and Related For-
malisms, Sydney, Australia, 121-126. 
Marco Kuhlmann and Mathias M?hl. 2007. Mildly con-
text-sensitive dependency languages. In 45th Annual 
Meeting of the Association for Computational Lin-
guistics (ACL), Prague, Czech Republic. 
Marco Kuhlmann and Joakim Nivre. 2006. Mildly non-
projective dependency structures. In 21st Interna-
tional Conference on Computational Linguistics and 
44th Annual Meeting of the Association for Computa-
tional Linguistics (COLING-ACL), Companion Vol-
ume, Sydney, Australia. 
Mathias M?hl. 2006. Drawings as Models of Syntactic 
Structure: Theory and Algorithms, Masters thesis, 
Saarland University, Saarbr?cken, Germany. 
Neville Ryant and Tatjana Scheffler. 2006. Binding of 
anaphors in LTAG. In Proceedings of the 8th Interna-
tional Workshop on Tree Adjoining Grammar and 
Related Formalisms, Sydney, Australia, 65-72. 
David Weir. 1988. Characterizing mildly context-
sensitive grammar formalisms. PhD dissertation, 
University of Pennsylvania, Philadelphia, USA.
8
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 92?93,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
 
A Pilot Annotation to Investigate Discourse Connectivity in Biomedical Text  
 
 
 
Hong Yu, Nadya Frid, Susan McRoy Rashmi Prasad, Alan Lee, Aravind Joshi 
University of Wisconsin-Milwaukee University of Pennsylvania 
P.O.Box 413 3401 Walnut Street 
Milwaukee, WI 53201 Philadelphia, PA 19104, USA 
Hongyu,frid,mcroy@uwm.edu Rjprasad,aleewk,joshi@seas.upenn.edu
 
 
 
 
 
 
Abstract 
The goal of the Penn Discourse Treebank 
(PDTB) project is to develop a large-scale cor-
pus, annotated with coherence relations marked 
by discourse connectives. Currently, the primary 
application of the PDTB annotation has been to 
news articles. In this study, we tested whether 
the PDTB guidelines can be adapted to a differ-
ent genre. We annotated discourse connectives 
and their arguments in one 4,937-token full-text 
biomedical article. Two linguist annotators 
showed an agreement of 85% after simple con-
ventions were added. For the remaining 15% 
cases, we found that biomedical domain-specific 
knowledge is needed to capture the linguistic 
cues that can be used to resolve inter-annotator 
disagreement. We found that the two annotators 
were able to reach an agreement after discussion. 
Thus our experiments suggest that the PDTB an-
notation can be adapted to new domains by mini-
mally adjusting the guidelines and by adding 
some further domain-specific linguistic cues. 
1 Introduction 
Large scale annotated corpora, e.g., the Penn 
TreeBank (PTB) project (Marcus et al 1993), 
have played an important role in text-mining. 
The Penn Discourse Treebank (PDTB) 
(http://www.seas.upenn.edu/~pdtb) (Prasad et al 
2008a) annotates the argument structure, seman-
tics, and attribution of discourse connectives and 
their arguments. The current release of PDTB-
2.0 contains the annotations of 1,808 Wall Street 
Journal articles (~1 million words) from the 
Penn TreeBank (Marcus et al 1993) II distribu-
tion and a total of 40,600 discourse connective  
tokens (Prasad et al 2008b). This work exam-
ines whether the PDTB annotation guidelines 
can be adapted to a different genre, the biomedi-
cal literature.  
2 Notation 
A discourse connective can be defined as a 
word or multiword expression that signals a 
discourse relation. Discourse connectives 
can be subordinating conjunctions (e.g., be-
cause, when, although), coordinating con-
junctions (e.g., but, or, nor) and adverbials 
(e.g., however, as a result, for example). A 
discourse connective takes in two argu-
ments, Arg1 and Arg2. Arg2 is the argument 
that appears in the clause that is syntacti-
cally bound to the connective and Arg1 is 
the other argument. In the sentence ?John 
failed the exam because he was lazy? the dis-
course connective is underlined, Arg1 ap-
pears in italics and Arg2 appears in bold. 
3 A Pilot Annotation 
Following the PDTB annotation manual (Prasad 
et al 2008b), we conducted a pilot annotation of 
discourse connectivity in biomedical text. As an 
initial step, we only annotated the three most 
92
important components of a discourse relation; 
namely, a discourse connective and its two ar-
guments; we did not annotate attribution. Two 
linguist annotators independently annotated one 
full-text biomedical article (Verpy et al 1999) 
that we randomly selected. The article is 4,937 
tokens long. When the annotation work was 
completed, we measured the inter-annotator 
agreement, following the PDTB exact match 
criterion (Miltsakaki et al 2004). According to 
this criterion, a discourse relation is in dis-
agreement if there is disagreement on any text-
span (i.e., the discourse connective or any of its 
two arguments). In addition, we also measured 
the agreement in the components (i.e., discourse 
connectives and the arguments). We discussed 
the annotation results and made suggestions to 
adapt the PDTB guidelines to biomedical text.  
4 Results and Discussion 
The first annotator identified 74 discourse con-
nectives, and the second annotator identified 75, 
68 of which were the same as those identified by 
the first annotator. The combined total number 
of discourse connectives was 81. The overall 
agreement in discourse connective identification 
was 68/81=84%.  
 
Of the 68 discourse connectives that were anno-
tated by both annotators, 31 were an exact 
match, 31 had an exact match for Arg1, and 54 
had an exact match for Arg2. The overall 
agreement for the 68 discourse relations is 
45.6% for exact match, 45.6% for Arg1, and 
79.4% for Arg2. The PDTB also reported a 
higher level of agreement in annotating Arg2 
than in annotating Arg1 (Miltsakaki et al 2004). 
We manually analyzed the cases with disagree-
ment. We found the disagreements are nearly all 
related to the annotation of citation references, 
supplementary clauses, and other conventions. 
When a few conventions for these cases were 
added, the inter-annotator agreement went up to 
85%. We also found that different interpretation 
of a relation and its arguments by annotators 
plays an important role for the remaining 15% 
inconsistency, and domain-specific knowledge 
is necessary to resolve such cases.   
 
5 New Conventions 
After the completion of the pilot annotation and 
the discussion, we decided to add the following 
conventions to the PDTB annotation guidelines 
to address the characteristics of biomedical text: 
 
i. Citation references are to be annotated as 
a part of an argument because the inclu-
sion will benefit many text-mining tasks 
including identifying the semantic rela-
tions among citations. 
ii. Clausal supplements (e.g., relative or 
parenthetical constructions) that modify  
arguments but are not minimally 
necessary for the interpretation of the 
relation,  are annotated as part of the 
arguments. 
iii. We will annotate a wider variety of 
nominalizations as arguments than 
allowed by the PDTB guidelines. 
 
We anticipate that these changes will both de-
crease the amount of effort required for annota-
tion and increase the reliability of the 
annotation. 
6 References 
Marcus M, Santorini B, Marcinkiewicz M (1993) 
Building a Large Annotated Corpus of Eng-
lish: The Penn Treebank. Computational 
Linguistics 19 
Miltsakaki E, Prasad R, Joshi A, Webber B (2004) 
Annotating discourse connectives and their 
arguments. Paper presented at Proceedings 
of the NAACL/HLT Workshop: Frontiers in 
Corpus Annotation 
Prasad R, Dinesh N, Lee A, Miltsakaki E, Robaldo L, 
Joshi A, Webber B (2008a) The Penn Dis-
course Treebank 2.0. Paper presented at The 
6th International Conference on Language 
Resources and Evaluation (LREC). Marra-
kech, Morroco 
Prasad R, Miltsakaki E, Dinesh N, Lee A, Joshi A, 
Robaldo L, Webber B (2008b) The Penn 
Discourse TreeBank 2.0 Annotation Manual. 
Technical Report: IRCS-08-01 
Verpy E, Leibovici M, Petit C (1999) Characteriza-
tion of otoconin-95, the major protein of 
murine otoconia, provides insights into the 
formation of these inner ear biominerals. 
Proc Natl Acad Sci U S A 96:529-534 
 
 
93
  
The Hindi Discourse Relation Bank 
Umangi Oza*, Rashmi Prasad?, Sudheer Kolachina*, Dipti Misra Sharma* and 
Aravind Joshi? 
*Language Technologies Research Centre 
IIIT Hyderabad, Gachibowli, Hyderabad, Andhra Pradesh, India 500032 
oza.umangi,sudheer.kpg08@gmail.com,dipti@iiit.ac.in 
 
?Institute for Research in Cognitive Science/Computer and Information Science 
3401 Walnut Street, Suite 400A 
Philadelphia, PA USA 19104 
rjprasad,joshi@seas.upenn.edu 
 
  
Abstract 
We describe the Hindi Discourse Relation 
Bank project, aimed at developing a large 
corpus annotated with discourse relations. 
We adopt the lexically grounded approach of 
the Penn Discourse Treebank, and describe 
our classification of Hindi discourse connec-
tives, our modifications to the sense classifi-
cation of discourse relations, and some cross-
linguistic comparisons based on some initial 
annotations carried out so far. 
1 Introduction 
To enable NLP research and applications beyond 
the sentence-level, corpora annotated with dis-
course level information have been developed. 
The recently developed Penn Discourse Tree-
bank (PDTB) (Prasad et al, 2008), for example, 
provides annotations of discourse relations (e.g., 
causal, contrastive, temporal, and elaboration 
relations) in the Penn Treebank Corpus. Recent 
interest in cross-linguistic studies of discourse 
relations has led to the initiation of similar dis-
course annotation projects in other languages as 
well, such as Chinese (Xue, 2005), Czech (Mla-
dov? et al, 2008), and Turkish (Deniz and Web-
ber, 2008). In this paper, we describe our ongo-
ing work on the creation of a Hindi Discourse 
Relation Bank (HDRB), broadly following the 
approach of the PDTB.1 The size of the HDRB 
corpus is 200K words and it is drawn from a 
400K word corpus on which Hindi syntactic de-
pendency annotation is being independently con-
ducted (Begum et al, 2008). Source corpus texts 
are taken from the Hindi newspaper Amar Ujala, 
and comprise news articles from several do-
mains, such as politics, sports, films, etc. We 
                                                 
1 An earlier study of Hindi discourse connectives towards 
the creation of HDRB is presented in Prasad et al (2008). 
present our characterization of discourse connec-
tives and their arguments in Hindi (Section 2), 
our proposals for modifying the sense classifica-
tion scheme (Section 3), and present some cross-
linguistics comparisons based on annotations 
done so far (Section 4). Section 5 concludes with 
a summary and future work.  
2 Discourse Relations and Arguments 
Following the PDTB approach, we take dis-
course relations to be realized in one of three 
ways: (a) as explicit connectives, which are 
?closed class? expressions drawn from well-
defined grammatical classes; (b) as alternative 
lexicalizations (AltLex), which are non-
connective expressions that cannot be defined as 
explicit connectives; and (c) as implicit connec-
tives, which are implicit discourse relations ?in-
ferred? between adjacent sentences not related by 
an explicit connective. When no discourse rela-
tion can be inferred between adjacent sentences, 
either an entity-based coherence relation (called 
EntRel) or the absence of a relation (called No-
Rel) is marked between the sentences. The two 
abstract object relata of a discourse relation are 
called the relation?s arguments (named Arg1 and 
Arg2), and argument annotation follows the ?mi-
nimality principle? in that only as much is se-
lected as the argument text span as is minimally 
necessary to interpret the relation. Finally, each 
discourse relation is assigned a sense label based 
on a hierarchical sense classification. 
2.1 Explicit Connectives 
In addition to the three major grammatical 
classes of Explicit connectives in the PDTB ? 
subordinating conjunctions, coordinating con-
junctions, and adverbials ? we recognize three 
other classes, described below. 
 
  
Sentential Relatives: These are relative pro-
nouns that conjoin a relative clause with its ma-
trix clause. As the name suggests, only relatives 
that modify verb phrases are treated as discourse 
connectives, and not those that modify noun 
phrases. Some examples are ????? (so that), 
????? ???? (because of which). 
 
1) [???? ??? ?????? ?? ?? ??????? ?? ????? ??? 
?? ?? ?? ????] ????? {???? ??? ???? ???? 
?? ???} 
?[Dropping all his work, he picked up the bird 
and ran towards the dispensary], so that {it 
could be given proper treatment}.? 
Subordinators: These include postpositions (Ex. 
2), verbal participles, and suffixes that introduce 
non-finite clauses with an abstract object inter-
pretation.2 
2) [?? ?? ????? ???]?? {??-??-?? ???????? ???? 
????? ???}? 
?Upon [hearing Baa?s words], {Gandhiji felt very 
ashamed}.? 
Particles: Particles such as ??, ?? act as dis-
course connectives. ?? is an emphatic inclusive 
particle used to suggest the inclusion of verbs, 
entities, adverbs, and adjectives. Instances of 
such particles which indicate the inclusion of 
verbs are taken as discourse connectives (Ex. 3) 
while others are not. 
3) ??? ?? ? ????? ????? ?? ??? ??? ? ???? ? ?? 
?????? ?? ??? ??? ??? ??? ???]?{?????? ??? 
??? ??? ???????? ???} ?? {?? ??? ???}? 
?[People see this as a consequence of the improv-
ing relation between the two countries]. {The 
Kashmiris are} also {learning an political lesson 
from this}.? 
2.2 Arguments of Discourse Relations 
In the PDTB, the assignment of the Arg1 and 
Arg2 labels to a discourse relation?s arguments is 
syntactically driven, in that the Arg2 label is as-
                                                 
2 Subordinators that denote the manner of an action are not 
discourse connectives, but since such disambiguation is a 
difficult task, we have decided to annotate subordinators in 
a later phase of the project.  
 
signed to the argument with which the connec-
tive was syntactically associated, while the Arg1 
label is assigned to the ?other? argument. In 
HDRB, however, the Arg1/Arg2 label assign-
ment is semantically driven, in that it is based on 
the ?sense? of the relation to which the argu-
ments belong.  Thus, each sense definition for a 
relation specifies the sense-specific semantic role 
of each of its arguments, and stipulates one of the 
two roles to be Arg1, and the other, Arg2.   For 
example, the ?cause? sense definition, which in-
volves a causal relation between two eventuali-
ties, specifies that one of its arguments is the 
cause, while the other is the effect, and further 
stipulates that the cause will be assigned the label 
Arg2, while the effect will be assigned the label 
Arg1.  Apart from giving meaning to the argu-
ment labels, our semantics-based convention has 
the added advantage simplifying the sense classi-
fication scheme. This is discussed further in Sec-
tion 3.  
2.3 Implicit Discourse Relations 
The HDRB annotation of implicit discourse rela-
tions largely follows the PDTB scheme. The only 
difference is that while implicit relations in 
PDTB are annotated only between paragraph-
internal adjacent sentences, we also annotate 
such relations across paragraph boundaries.  
3 Senses of Discourse Relations  
Broadly, we follow the PDTB sense classifica-
tion in that we take it to be a hierarchical classi-
fication, with the four top level sense classes of 
?Temporal?, ?Contingency?, ?Comparison?, and 
?Expansion?. Further refinements to the top class 
level are provided at the second type level and 
the third subtype level. Here, we describe our 
points of departure from the PDTB classification. 
The changes are partly motivated by general 
considerations for capturing additional senses, 
and partly by language-specific considerations. 
Figure 1 reflects the modifications we have made 
to the sense scheme. These are described below. 
 
Eliminating argument-specific labels: In the 
PDTB sense hierarchy, the tags at the type level 
are meant to express further refinements of the 
relations? semantics, while the tags at the subtype 
level are meant to reflect different orderings of 
the arguments (see Section 2.2). In HDRB, we 
eliminate these argument-ordering labels from 
the subtype level, since these labels don?t direct-
ly pertain to the meaning of discourse relations. 
  
All levels in the sense hierarchy thus have the 
purpose of specifying the semantics of the rela-
tion to different degrees of granularity. The rela-
tive ordering of the arguments is instead speci-
fied in the definition of the type-level senses, and 
is inherited by the more refined senses at the sub-
type level.   
 
 
 
     
 
Figure 1: HDRB (Modified) Sense Classification 
 
 
Uniform treatment of pragmatic relations: As 
in PDTB, discourse relations in HDRB are 
pragmatic when their relations have to be in-
ferred from the propositional content of the ar-
guments. However, we replace the PDTB prag-
matic senses with a uniform three-way classifica-
tion. Each pragmatic sense at the type level is 
further distinguished into three subtypes: ?epis-
temic? (Sweetser 1990), ?speech-act? (Sweetser 
1990), and ?propositional?. The propositional 
subtype involves the inference of a complete 
proposition. The relation is then taken to hold 
between this inferred proposition and the propo-
sitional content of one of the arguments. 
 
The ?Goal? sense: Under the ?Contingency? 
class, we have added a new type ?Goal?, which 
applies to relations where the situation described 
in one of the arguments is the goal of the situa- 
tion described in the other argument (which 
enables the achievement of the goal).   
4 Initial Annotation Experiments 
Based on the guidelines as described in this pa-
per, we annotated both explicit and implicit rela-
tions in 35 texts (averaging approx. 250 
words/text) from the HDRB corpus. A total of 
602 relation tokens were annotated. Here we 
present some useful distributions we were able to 
derive from our initial annotation, and discuss 
them in light of cross-linguistic comparisons of 
discourse relations.  
 
Types and Tokens of Discourse Relations: Ta-
ble 1 shows the overall distribution of the differ-
ent relation types, i.e., Explicit, AltLex, Implicit, 
EntRel, and NoRel. The second column reports 
the number of unique expressions used to realize 
the relation ? Explicit, Implicit and AltLex ? 
while the third column reports the total number 
of tokens and relative frequencies.  
 
Relations Types Tokens (%) 
Explicit 49 189 (31.4%) 
Implicit 35 185 (30.7%) 
AltLex 25 37 (6.14%) 
EntRel NA 140 (23.25%) 
NoRel NA 51 (8.5%) 
TOTAL 109 602 
Table 1: Distribution of Discourse Relations 
 
These distributions show some interesting simi-
larities and differences with the PDTB distribu-
tions (cf. Prasad et al, 2008). First, given that 
Hindi has a much richer morphological paradigm 
than English; one would have expected that it 
would have fewer explicit connectives. That is, 
one might expect Hindi to realize discourse rela-
tions morphologically more often than not, just 
as it realizes other syntactic relations.  However, 
even in the small data set of 602 tokens that we 
have annotated so far, we have found 49 unique 
explicit connectives, which is roughly half the 
number reported for the 1 million words anno-
tated in English texts in PDTB. It is expected that 
we will find more unique types as we annotate 
additional data. The relation type distribution 
  
thus seems to suggest that the availability of 
richer morphology in a language doesn?t affect 
connective usage. Second, the percentage of Alt-
Lex relations is higher in HDRB ? 6.14% com-
pared to 1.5% in PDTB, suggesting that Hindi 
makes greater usage of non-connective cohesive 
links with the prior discourse. Further studies are 
needed to characterize the forms and functions of 
AltLex expressions in both English and Hindi. 
 
Senses of Discourse Relations: We also ex-
amined the distributions for each sense class in 
HDRB and computed the relative frequency of 
the relations realized explicitly and implicitly. 
Cross-linguistically, one would expect languages 
to be similar in whether or not a relation with a 
particular sense is realized explicitly or implicit-
ly, since this choice lies in the domain of seman-
tics and inference, rather than syntax. Thus, we 
were interested in comparing the sense distribu-
tions in HDRB and PDTB. Table 2 shows these 
distributions for the top class level senses. (Here 
we counted the AltLex relations together with 
explicit connectives.) 
 
Sense Class Explicit (%) Implicit (%) 
Contingency 57 (58.2%) 41 (41.8%) 
Comparison 68 (76.5%) 21 (23.5%) 
Temporal 43 (65.2%) 23 (34.8%) 
Expansion 64(40%) 94(60%) 
Table 2: Distribution of Class Level Senses 
 
The table shows that sense distributions in 
HDRB are indeed similar to those reported in the 
PDTB (cf. Prasad et al, 2008). That is, the 
chances of ?Expansion? and ?Contingency? rela-
tions being explicit are lower compared to 
?Comparison? and ?Temporal? relations.    
5 Summary and Future Work 
This paper has reported on the Hindi Discourse 
Relation Bank (HDRB) project, in which dis-
course relations, their arguments, and their 
senses are being annotated. A major goal of our 
work was to investigate how well the Penn Dis-
course Treebank (PDTB) and its guidelines could 
be adapted for discourse annotation of Hindi 
texts. To a large extent, we have successfully 
adapted the PDTB scheme. Proposed changes 
have to do with identification of some new syn-
tactic categories for explicit connectives, and 
some general and language-driven modifications 
to the sense classification. From our initial anno-
tations, we found that (a) there doesn?t seem to 
be an inverse correlation between the usage fre-
quency of explicit connectives and the morpho-
logical richness of a language, although there 
does seem to be an increased use of cohesive 
devices in such a language; and (b) sense distri-
butions confirm the lack of expectation of cross-
linguistic ?semantic? differences. Our future goal 
is to complete the discourse annotation of a 200K 
word corpus, which will account for half of the 
400K word corpus being also annotated for syn-
tactic dependencies. We also plan to extend the 
annotation scheme to include attributions.   
 
Acknowledgements 
This work was partially supported by NSF grants 
EIA-02-24417, EIA-05-63063, and IIS-07-
05671.  
References 
Rafiya Begum, Samar Husain, Arun Dhwaj, Dipti 
Misra Sharma, Lakshmi Bai, and Rajeev Sangal. 
2008. Dependency annotation scheme for Indian 
languages. Proc. of IJCNLP-2008.  
Lucie Mladov?, ??rka Zik?nov? and Eva Haji?ov?. 
2008. From Sentence to Discourse: Building an 
Annotation Scheme for Discourse Based on Prague 
Dependency Treebank. Proc. of LREC-2008. 
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie 
Webber. 2008. The Penn Discourse TreeBank 2.0. 
Proc. of LREC-2008.  
Rashmi Prasad, Samar Husain, Dipti Mishra Sharma, 
and Aravind Joshi. 2008. Towards an Annotated 
Corpus of Discourse Relations in Hindi. Proc. of 
IJCNLP-2008. 
 Eve Sweetser.1990. From etymology to pragmat-
ics: Metaphorical and cultural aspects of se-
mantic structure .   Cambridge University Press. 
Nianwen Xue. 2005. Annotating Discourse Connec-
tives in the Chinese Treebank. Proc. of the ACL 
Workshop on Frontiers in Corpus Annotation 
II: Pie in the Sky.  
Deniz Zeyrek and Bonnie Webber. 2008. A Discourse 
Resource for Turkish: Annotating Discourse Con-
nectives in the METU Corpus. Proc. of IJCNLP-
2008.  
Coling 2010: Poster Volume, pages 1023?1031,
Beijing, August 2010
Realization of Discourse Relations by Other Means: Alternative
Lexicalizations
Rashmi Prasad and Aravind Joshi
University of Pennsylvania
rjprasad,joshi@seas.upenn.edu
Bonnie Webber
University of Edinburgh
bonnie@inf.ed.ac.uk
Abstract
Studies of discourse relations have not, in
the past, attempted to characterize what
serves as evidence for them, beyond lists
of frozen expressions, or markers, drawn
from a few well-defined syntactic classes.
In this paper, we describe how the lexical-
ized discourse relation annotations of the
Penn Discourse Treebank (PDTB) led to
the discovery of a wide range of additional
expressions, annotated as AltLex (alterna-
tive lexicalizations) in the PDTB 2.0. Fur-
ther analysis of AltLex annotation sug-
gests that the set of markers is open-
ended, and drawn from a wider variety
of syntactic types than currently assumed.
As a first attempt towards automatically
identifying discourse relation markers, we
propose the use of syntactic paraphrase
methods.
1 Introduction
Discourse relations that hold between the content
of clauses and of sentences ? including relations
of cause, contrast, elaboration, and temporal or-
dering ? are important for natural language pro-
cessing tasks that require sensitivity to more than
just a single sentence, such as summarization, in-
formation extraction, and generation. In written
text, discourse relations have usually been con-
sidered to be signaled either explicitly, as lexical-
ized with some word or phrase, or implicitly due
to adjacency. Thus, while the causal relation be-
tween the situations described in the two clauses
in Ex. (1) is signalled explicitly by the connective
As a result, the same relation is conveyed implic-
itly in Ex. (2).
(1) John was tired. As a result he left early.
(2) John was tired. He left early.
This paper focusses on the problem of how to
characterize and identify explicit signals of dis-
course relations, exemplified in Ex. (1). To re-
fer to all such signals, we use the term ?discourse
relation markers? (DRMs). Past research (e.g.,
(Halliday and Hasan, 1976; Martin, 1992; Knott,
1996), among others) has assumed that DRMs
are frozen or fixed expressions from a few well-
defined syntactic classes, such as conjunctions,
adverbs, and prepositional phrases. Thus the lit-
erature presents lists of DRMs, which researchers
try to make as complete as possible for their cho-
sen language. In annotating lexicalized discourse
relations of the Penn Discourse Treebank (Prasad
et al, 2008), this same assumption drove the ini-
tial phase of annotation. A list of ?explicit con-
nectives? was collected from various sources and
provided to annotators, who then searched for
these expressions in the text and annotated them,
along with their arguments and senses. The same
assumption underlies methods for automatically
identifying DRMs (Pitler and Nenkova, 2009).
Since expressions functioning as DRMs can also
have non-DRM functions, the task is framed as
one of classifying given individual tokens as DRM
or not DRM.
In this paper, we argue that placing such syn-
tactic and lexical restrictions on DRMs limits
a proper understanding of discourse relations,
which can be realized in other ways as well. For
example, one should recognize that the instantia-
tion (or exemplification) relation between the two
sentences in Ex. (3) is explicitly signalled in the
second sentence by the phrase Probably the most
egregious example is, which is sufficient to ex-
press the instantiation relation.
(3) Typically, these laws seek to prevent executive
branch officials from inquiring into whether cer-
tain federal programs make any economic sense or
proposing more market-oriented alternatives to reg-
ulations. Probably the most egregious example is a
1023
proviso in the appropriations bill for the executive
office that prevents the president?s Office of Man-
agement and Budget from subjecting agricultural
marketing orders to any cost-benefit scrutiny.
Cases such as Ex. (3) show that identifying
DRMs cannot simply be a matter of preparing a
list of fixed expressions and searching for them in
the text. We describe in Section 2 how we identi-
fied other ways of expressing discourse relations
in the PDTB. In the current version of the cor-
pus (PDTB 2.0.), they are labelled as AltLex (al-
ternative lexicalizations), and are ?discovered? as
a result of our lexically driven annotation of dis-
course relations, including explicit as well as im-
plicit relations. Further analysis of AltLex anno-
tations (Section 3) leads to the thesis that DRMs
are a lexically open-ended class of elements which
may or may not belong to well-defined syntactic
classes. The open-ended nature of DRMs is a
challenge for their automated identification, and
in Section 4, we point to some lessons we have
already learned from this annotation. Finally, we
suggest that methods used for automatically gen-
erating candidate paraphrases may help to expand
the set of recognized DRMs for English and for
other languages as well (Section 5).
2 AltLex in the PDTB
The Penn Discourse Treebank (Prasad et al,
2008) constitutes the largest available resource of
lexically grounded annotations of discourse rela-
tions, including both explicit and implicit rela-
tions.1 Discourse relations are assumed to have
two and only two arguments, called Arg1 and
Arg2. By convention, Arg2 is the argument syn-
tactically associated with the relation, while Arg1
is the other argument. Each discourse relation is
also annotated with one of the several senses in the
PDTB hierarchical sense classification, as well as
the attribution of the relation and its arguments.
In this section, we describe how the annotation
methodology of the PDTB led to the identification
of the AltLex relations.
Since one of the major goals of the annota-
tion was to lexically ground each relation, a first
step in the annotation was to identify the explicit
1http://www.seas.upenn.edu/?pdtb
markers of discourse relations. Following stan-
dard practice, a list of such markers ? called ?ex-
plicit connectives? in the PDTB ? was collected
from various sources (Halliday and Hasan, 1976;
Martin, 1992; Knott, 1996; Forbes-Riley et al,
2006).2 These were provided to annotators, who
then searched for these expressions in the corpus
and marked their arguments, senses, and attribu-
tion.3 In the pilot phase of the annotation, we
also went through several iterations of updating
the list, as and when annotators reported seeing
connectives that were not in the current list. Im-
portantly, however, connectives were constrained
to come from a few well-defined syntactic classes:
? Subordinating conjunctions: e.g., because,
although, when, while, since, if, as.
? Coordinating conjunctions: e.g., and, but,
so, either..or, neither..nor.
? Prepositional phrases: e.g., as a result, on
the one hand..on the other hand, insofar as,
in comparison.
? adverbs: e.g., then, however, instead, yet,
likewise, subsequently
Ex. (4) illustrates the annotation of an explicit
connective. (In all PDTB examples in the paper,
Arg2 is indicated in boldface, Arg1 is in italics,
the DRM is underlined, and the sense is provided
in parentheses at the end of the example.)
(4) U.S. Trust, a 136-year-old institution that is one of
the earliest high-net worth banks in the U.S., has
faced intensifying competition from other firms that
have established, and heavily promoted, private-
banking businesses of their own. As a result,
U.S. Trust?s earnings have been hurt. (Contin-
gency:Cause:Result)
After all explicit connectives in the list were
annotated, the next step was to identify implicit
discourse relations. We assumed that such rela-
tions are triggered by adjacency, and (because of
resource limitations) considered only those that
held between sentences within the same para-
graph. Annotators were thus instructed to supply
a connective ? called ?implicit connective? ? for
2All explicit connectives annotated in the PDTB are listed
in the PDTB manual (PDTB-Group, 2008).
3These guidelines are recorded in the PDTB manual.
1024
each pair of adjacent sentences, as long as the re-
lation was not already expressed with one of the
explicit connectives provided to them. This proce-
dure led to the annotation of implicit connectives
such as because in Ex. (5), where a causal relation
is inferred but no explicit connective is present in
the text to express the relation.
(5) To compare temperatures over the past 10,000
years, researchers analyzed the changes in concen-
trations of two forms of oxygen. (Implicit=because)
These measurements can indicate temperature
changes, . . . (Contingency:Cause:reason)
Annotators soon noticed that in many cases,
they were not able to supply an implicit connec-
tive. Reasons supplied included (a) ?there is a re-
lation between these sentences but I cannot think
of a connective to insert between them?, (b) ?there
is a relation between the sentences for which I
can think of a connective, but it doesn?t sound
good?, and (c) ?there is no relation between the
sentences?. For all such cases, annotators were
instructed to supply ?NONE? as the implicit con-
nective. Later, we sub-divided these ?NONE? im-
plicits into ?EntRel?, for the (a) type above (an
entity-based coherence relation, since the second
sentence seemed to continue the description of
some entity mentioned in the first); ?NoRel? (no
relation) for the (c) type; and ?AltLex?, for the (b)
type, which we turn to next.
Closer investigation of the (b) cases revealed
that the awkwardness perceived by annotators
when inserting an implicit connective was due to
redundancy in the expression of the relation: Al-
though no explicit connective was present to re-
late the two sentences, some other expression ap-
peared to be doing the job. This is indeed what
we found. Subsequently, instances of AltLex were
annotated if:
1. A discourse relation can be inferred between
adjacent sentences.
2. There is no explicit connective present to re-
late them.
3. The annotator is not able to insert an im-
plicit connective to express the inferred rela-
tion (having used ?NONE? instead), because
inserting it leads to an awkward redundancy
in expressing the relation.
Under these conditions, annotators were in-
structed to look for and mark as Altlex, whatever
alternative expression appeared to denote the re-
lation. Thus, for example, Ex. (6) was annotated
as AltLex because although a causal relation is in-
ferred between the sentences, inserting a connec-
tive like because makes expression of the relation
redundant. Here the phrase One reason is is taken
to denote the relation and is marked as AltLex.
(6) Now, GM appears to be stepping up the pace of its
factory consolidation to get in shape for the 1990s.
One reason is mounting competition from new
Japanese car plants in the U.S. that are pour-
ing out more than one million vehicles a year
at costs lower than GM can match. (Contin-
gency:Cause:reason)
The result of this procedure led to the annota-
tion of 624 tokens of AltLex in the PDTB. We
turn to our analysis of these expressions in the
next section.
3 What is found in AltLex?
Several questions arise when considering the Alt-
Lex annotations. What kind of expressions are
they? What can we learn from their syntax?
Do they project discourse relations of a different
sort than connectives? How can they be identi-
fied, both during manual annotation and automat-
ically? To address these questions, we examined
the AltLex annotation for annotated senses, and
for common lexico-syntactic patterns extracted
using alignment with the Penn Treebank (Marcus
et al, 1993).4
3.1 Lexico-syntactic Characterization
We found that we could partition AltLex annota-
tion into three groups by (a) whether or not they
belonged to one of the syntactic classes admit-
ted as explicit connectives in the PDTB, and (b)
whether the expression was frozen (ie, blocking
free substitution, modification or deletion of any
of its parts) or open-ended. The three groups are
shown in Table 1 and discussed below.
4The source texts of the PDTB come from the Penn
Treebank (PTB) portion of the Wall Street Journal corpus.
The PDTB corpus provides PTB tree alignments of all its
text span annotations, including connectives, AltLex?s, argu-
ments of relations, and attribution spans.
1025
AltLex Group No (%) Examples
Syntactically
admitted, lexi-
cally frozen
92 (14.7%) quite the contrary (ADVP), for one thing (PP), as well (ADVP),
too (ADVP), soon (ADVP-TMP), eventually (ADVP-TMP),
thereafter (RB), even (ADVP), especially (ADVP), actually
(ADVP), still (ADVP), only (ADVP), in response (PP)
Syntactically
free, lexically
frozen
54 (8.7%) What?s more (SBAR-ADV), Never mind that (ADVP-
TMP;VB;DT), To begin with (VP), So (ADVP-PRD-TPC),
Another (DT), further (JJ), As in (IN;IN), So what if
(ADVP;IN), Best of all (NP)
Syntactically
and lexically
free
478 (76.6%) That compares with (NP-SBJ;VBD;IN), After these payments
(PP-TMP), That would follow (NP-SBJ;MD;VB), The plunge
followed (NP-SBJ;VBD), Until then (PP-TMP), The increase
was due mainly to (NP-SBJ;VBD;JJ;RB;TO), That is why (NP-
SBJ;VBZ;WHADVP), Once triggered (SBAR-TMP)
TOTAL 624 ?
Table 1: Breakdown of AltLex by Syntactic and Lexical Flexibility. Examples in the third column are
accompanied (in parentheses) with their PTB POS tags and constituent phrase labels obtained from the
PDTB-PTB alignment.
Syntactically admitted and lexically frozen:
The first row shows that 14.7% of the strings an-
notated as AltLex belong to syntactic classes ad-
mitted as connectives and are similarly frozen.
(Syntactic class was obtained from the PDTB-
PTB alignment.) So, despite the effort in prepar-
ing a list of connectives (cf. Section 1), additional
ones were still found in the corpus through AltLex
annotation. This suggests that any pre-defined list
of connectives should only be used to guide anno-
tators in a strategy for ?discovering? connectives.
Syntactically free and lexically frozen: AltLex
expressions that were frozen but belonged to syn-
tactic classes other than those admitted for the
PDTB explicit connectives accounted for 8.7%
(54/624) of the total (Table 1, row 2). For exam-
ple, the AltLex What?s more (Ex. 7) is parsed as
a clause (SBAR) functioning as an adverb (ADV).
It is also frozen, in not undergoing any change (eg,
What?s less, What?s bigger, etc.5
(7) Marketers themselves are partly to blame: They?ve
increased spending for coupons and other short-
term promotions at the expense of image-building
advertising. What?s more, a flood of new prod-
ucts has given consumers a dizzying choice of
5Apparently similar headless relative clauses such as
What?s more exciting differ from What?s more in not func-
tioning as adverbials, just as NPs.
brands, many of which are virtually carbon
copies of one other. (Expansion:Conjunction)
Many of these AltLex annotations do not con-
stitute a single constituent in the PTB, as with
Never mind that. These cases suggest that ei-
ther the restrictions on connectives as frozen ex-
pressions should be relaxed to admit all syntactic
classes, or the syntactic analyses of these multi-
word expressions is irrelevant to their function.
Both syntactically and lexically free: This
third group (Table 1, row 3) constitutes the major-
ity of AltLex annotations ? 76.6% (478/624). Ad-
ditional examples are shown in Table 2. Common
syntactic patterns here include subjects followed
by verbs (Table 2a-c), verb phrases with comple-
ments (d), adverbial clauses (e), and main clauses
with a subordinating conjunction (f).
All these AltLex annotations are freely modifi-
able, with their fixed and modifiable parts shown
in the regular expressions defined for them in Ta-
ble 2. Each has a fixed ?core? phrase shown as
lexical tokens in the regular expression, e.g, con-
sequence of, attributed to, plus obligatory and op-
tional elements shown as syntactic labels. Op-
tional elements are shown in parentheses. <NX>
indicates any noun phrase, <PPX>, any prepo-
sitional phrase, <VX>, any verb phrase, and
1026
AltLex String AltLex Pattern
(a) A consequence of their departure could be ... <DTX> consequence (<PPX>) <VX>
(b) A major reason is ... <DTX> (<JJX>) reason (<PPX>) <VX>
(c) Mayhap this metaphorical connection made ... (<ADVX>) <NX> made
(d) ... attributed the increase to ... attributed <NX> to
(e) Adding to that speculation ... Adding to <NX>
(f) That may be because ... <NX> <VX> because
Table 2: Complex AltLex strings and their patterns
<JJX>, any adjectival phrase
These patterns show, for example, that other
variants of the identified AltLex A major reason
is include The reason is, A possible reason for the
increase is, A reason for why we should consider
DRMs as an open class is, etc. This is robust sup-
port for our claim that DRMs should be regarded
as an open class: The task of identifying them can-
not simply be a matter of checking an a priori list.
Note that the optional modification seen here
is clearly also possible with many explicit con-
nectives such as if (eg, even if just if, only if ),
as shown in Appendix C of the PDTB manual
(PDTB-Group, 2008). This further supports the
thesis that DRMs should be treated as an open
class that includes explicit connectives.
3.2 Semantic Characterization
AltLex strings were annotated as denoting the dis-
course relation that held between otherwise un-
marked adjacent utterances (Section 2). We found
them to convey this relation in much the same
way as anaphoric discourse adverbials. Accord-
ing to (Forbes-Riley et al, 2006), discourse ad-
verbials convey both the discourse relation and an
anaphoric reference to its Arg1. The latter may be
either explicit (e.g., through the use of a demon-
strative like ?this? or ?that?), or implicit. Thus,
both as a result of that and as a result are dis-
course adverbials in the same way: the latter refers
explicitly to Arg1 via the pronoun ?that?, while
former does so via an implicit internal argument.
(A result must be a result of something.)
The examples in Table 2 make this same two?
part semantic contribution, albeit with more com-
plex expressions referring to Arg1 and more com-
plex modification of the expression denoting the
relation. For example, in the AltLex shown in
(Table 2c), Mayhap this metaphorical connection
made (annotated in Ex. (8)), the relation is de-
noted by the causal verb made, while Arg1 is
referenced through the definite description this
metaphorical connection. In addition, the adverb
Mayhap further modifies the relational verb.
(8) Ms. Bartlett?s previous work, which
earned her an international reputation
in the non-horticultural art world, of-
ten took gardens as its nominal subject.
Mayhap this metaphorical connection made
the BPC Fine Arts Committee think she had a
literal green thumb. (Contingency:Cause:Result)
These complex AltLex?s also raise the question
of why we find them at all in language. One part of
the answer is that these complex AltLex?s are used
to convey more than just the meaning of the rela-
tion. In most cases, we found that substituting the
AltLex with an adverbial connective led to some
aspect of the meaning being lost, as in Ex. (9-
10). Substituting For example for the AltLex with
an (necessary) accompanying paraphrase of Arg2
loses the information that the example provided as
Arg2 is possibly the most egregious one. The con-
nective for example does not allow similar modi-
fication. This means that one must use a different
strategy such as an AltLex expression.
(9) Typically, these laws seek to prevent exec-
utive branch officials from inquiring into
whether certain federal programs make
any economic sense or proposing more
market-oriented alternatives to regulations.
Probably the most egregious example is a pro-
viso in the appropriations bill for the executive
office that prevents the president?s Office of
Management and Budget from subjecting agri-
cultural marketing orders to any cost-benefit
scrutiny. (Expansion:Instantiation)
(10) For example, a proviso in the appropriations bill
for the executive office prevents the president?s Of-
1027
fice of Management and Budget from subjecting
agricultural marketing orders to any cost-benefit
scrutiny.
Another part of the answer to Why AltLex? is
that it can serve to convey a relation for which the
lexicon lacks an adverbial connective. For exam-
ple, while English has several adverbial connec-
tives that express a ?Cause:Consequence? relation
(eg, as a result, consequently, etc.), it lacks an
adverbial connective expressing ?Cause:Reason?
(or explanation) albeit having at least two sub-
ordinating conjunctions that do so (because and
since). Thus, we find an AltLex whenever this re-
lation needs to be expressed between sentences, as
shown in Ex. (11).
(11) But a strong level of investor withdrawals is
much more unlikely this time around, fund man-
agers said. A major reason is that investors al-
ready have sharply scaled back their purchases
of stock funds since Black Monday. (Contin-
gency:Cause:reason)
Note, however, that even for such relations such
as Cause:Reason, it is still not the case that a list of
canned expressions will be sufficient to generate
the Altlex or to identify them, since this relation
can itself be further modified. In Ex. (12), for ex-
ample, the writer intends to convey that there are
multiple reasons for the walkout, although only
one of them is eventually specified in detail.
(12) In Chile, workers at two copper mines, Los
Bronces and El Soldado, which belong to the
Exxon-owned Minera Disputada, yesterday voted
to begin a full strike tomorrow, an analyst
said. Reasons for the walkout, the analyst said,
included a number of procedural issues, such as
a right to strike. (Contingency:Cause:reason)
4 Lessons learned from AltLex
Like all lexical phenomena, DRMs appear to
have a power-law distribution, with some very
few high-frequency instances like (and, but), a
block of mid-frequency instances (eg, after, be-
cause, however), and many many low-frequency
instances in the ?long tail? (eg, much as, on the
contrary, in short, etc.). Given the importance
of DRMs for recognizing and classifying dis-
course relations and their arguments, what have
we learned from the annotation of AltLex?
First, the number of expressions found through
AltLex annotation, that belong to syntactic classes
admitted as connectives and also similarly frozen
(Table 1, row 1) shows that even in the PDTB,
there are additional instances of what we have
taken to be explicit connectives. By recognizing
them and unambiguously labelling their senses,
we will start to reduce the number of ?hard cases?
of implicit connectives whose sense has to be rec-
ognized (Marcu and Echihabi, 2002; Sporleder
and Lascarides, 2008; Pitler et al, 2009; Lin et al,
2009). Secondly, the number of tokens of expres-
sions from other syntactic classes that have been
annotated as AltLex (Table 1, rows 2 and 3) may
actually be higher than was caught via our Alt-
Lex annotation, thus making them even more im-
portant for discourse processing. To assess this,
we selected five of them and looked for all their
tokens in the WSJ raw files underlying both the
PTB and the PDTB. After eliminating those to-
kens that had already been annotated, we judged
whether the remaining ones were functioning as
connectives. Table 3 shows the expressions we
used in the first column, with the second and third
columns reporting the number of tokens annotated
in PDTB, and the number of additional tokens in
the WSJ corpus functioning as connectives. (The
asterisk next to the expressions is a wild card to al-
low for variations along the lines discussed for Ta-
ble 2.) These results show that these DRMs occur
two to three times more frequently than already
annotated.
Increased frequencies of AltLex occurrence are
also observed in discourse annotation projects un-
dertaken subsequent to the PDTB, since they were
able to be more sensitive to the presence of Alt-
Lex. The Hindi Discourse Relation Bank (HDRB)
(Oza et al, 2009), for example, reports that 6.5%
of all discourse relations in the HDRB have been
annotated as AltLex, compared to 1.5% in the
PDTB. This also provides cross-linguistic evi-
dence of the importance of recognizing the full
range of DRMs in a language.
5 Identifying DRMs outside the PDTB
As the set of DRMs appears to be both open-ended
and distributed like much else in language, with
a very long tail, it is likely that many are miss-
ing from the one-million word WSJ corpus anno-
tated in the PDTB 2.0. Indeed, in annotating En-
1028
AltLex Annotated Unannotated
The reason* 8 15
That?s because 11 16
The result* 12 18
That/This would* 5 16
That means 11 17
TOTAL 47 82
Table 3: Annotated and Unannotated instances of AltLex
glish biomedical articles with discourse relations,
Yu et al(2008) report finding many DRMs that
don?t appear in the WSJ (e.g., as a consequence).
If one is to fully exploit DRMs in classifying
discourse relations, one must be able to identify
them all, or at least many more of them than we
have to date. One method that seems promising
is Callison-Burch?s paraphrase generation through
back-translation on pairs of word-aligned corpora
(Callison-Birch, 2007). This method exploits the
frequency with which a word or phrase is back
translated (from texts in language A to texts in
language B, and then back from texts in language
B to texts in language A) across a range of pivot
languages, into other words or phrases.
While there are many factors that introduce
low-frequency noise into the process, including
lexical ambiguity and errors in word alignment,
Callison-Burch?s method benefits from being able
to use the many existing word-aligned translation
pairs developed for creating translation models for
SMT. Recently, Callison-Burch showed that para-
phrase errors could be reduced by syntactically
constraining the phrases identified through back-
translation to ones with the same syntactic cat-
egory as assigned to the source (Callison-Birch,
2008), using a large set of syntactic categories
similar to those used in CCG (Steedman, 2000).
For DRMs, the idea is to identify through back-
translation, instances of DRMs that were neither
included in our original set of explicit connec-
tive nor subsequently found through AltLex an-
notation. To allow us to carry out a quick pi-
lot study, Callison-Burch provided us with back-
translations of 147 DRMs (primarily explicit con-
nectives annotated in the PDTB 2.0, but also in-
cluding a few from other syntactic classes found
through AltLex annotation). Preliminary analysis
of the results reveals many DRMs that don?t ap-
pear anywhere in the WSJ Corpus (eg, as a con-
sequence, as an example, by the same token), as
well as additional DRMs that appear in the cor-
pus but were not annotated as AltLex (e.g., above
all, after all, despite that). Many of these latter
instances appear in the initial sentence of a para-
graph, but the annotation of implicit connectives
? which is what led to AltLex annotation in the
first place (Section 2) ? was not carried out on
these sentences.
There are two further things to note before clos-
ing this discussion. First, there is an additional
source of noise in using back-translation para-
phrase to expand the set of identified DRMs. This
arises from the fact that discourse relations can
be conveyed either explicitly or implicitly, and
a translated text may not have made the same
choices vis-a-vis explicitation as its source, caus-
ing additional word alignment errors (some of
which are interesting, but most of which are not).
Secondly, this same method should prove useful
for languages other English, although there will be
an additional problem to overcome for languages
(such as Turkish) in which DRMs are conveyed
through morphology as well as through distinct
words and phrases.
6 Related work
We are not the first to recognize that discourse re-
lations can realized by more than just one or two
syntactic classes. Halliday and Hasan (1976) doc-
ument prepositional phrases like After that being
used to express conjunctive relations. More im-
portantly, they note that any definite description
can be substituted for the demonstrative pronoun.
1029
Similarly, Taboada (2006), in looking at how of-
ten RST-based rhetorical relations are realized by
discourse markers, starts by considering only ad-
verbials, prepositional phrases, and conjunctions,
but then notes the occurrence of a single instance
of a nominal fragment The result in her corpus.
Challenging the RST assumption that the basic
unit of a discourse is a clause, with discourse rela-
tions holding between adjacent clausal units, Kib-
ble (1999) provides evidence that informational
discourse relations (as opposed to intentional dis-
course relations) can hold intra-clausally as well,
with the relation ?verbalized? and its arguments
realized as nominalizations, as in Early treatment
with Brand X can prevent a cold sore developing.
Since his focus is intra-clausal, he does not ob-
serve that verbalized discourse relations can hold
across sentences as well, where a verb and one
of its arguments function similarly to a discourse
adverbial, and in the end, he does not provide a
proposal for how to systematically identify these
alternative realizations. Le Huong et al (2003),
in developing an algorithm for recognizing dis-
course relations, consider non-verbal realizations
(called NP cues) in addition to verbal realizations
(called VP cues). However, they provide only one
example of such a cue (?the result?). Like Kib-
ble (1999), Danlos (2006) and Power (2007) also
focus only on identifying verbalizations of dis-
course relations, although they do consider cases
where such relations hold across sentences.
What has not been investigated in prior work
is the basis for the alternation between connec-
tives and AltLex?s, although there are several ac-
counts of why a language may provide more than
one connective that conveys the same relation.
For example, the alternation in Dutch between
dus (?so?), daardoor (?as a result?), and daarom
(?that?s why?) is explained by Pander Maat and
Sanders (2000) as having its basis in ?subjectiv-
ity?.
7 Conclusion and Future Work
Categorizing and identifying the range of ways in
which discourse relations are realized is impor-
tant for both discourse understanding and gener-
ation. In this paper, we showed that existing prac-
tices of cataloguing these ways as lists of closed
class expressions is problematic. We drew on our
experience in creating the lexically grounded an-
notations of the Penn Discourse Treebank, and
showed that markers of discourse relations should
instead be treated as open-class items, with uncon-
strained syntactic possibilities. Manual annota-
tion and automatic identification practices should
develop methods in line with this finding if they
aim to exhaustively identify all discourse relation
markers.
Acknowledgments
We want to thank Chris Callison-Burch, who
graciously provided us with EuroParl back-
translation paraphrases for the list of connectives
we sent him. This work was partially supported
by NSF grant IIS-07-05671.
References
Callison-Birch, Chris. 2007. Paraphrasing and Trans-
lation. Ph.D. thesis, School of Informatics, Univer-
sity of Edinburgh.
Callison-Birch, Chris. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of Conference on Empirical Methods in
Natural Language Processing (EMNLP).
Danlos, Laurence. 2006. Discourse verbs. In Pro-
ceedings of the 2nd Workshop on Constraints in Dis-
course, pages 59?65, Maynooth, Ireland.
Forbes-Riley, Katherine, Bonnie Webber, and Aravind
Joshi. 2006. Computing discourse semantics: The
predicate-argument semantics of discourse connec-
tives in D-LTAG. Journal of Semantics, 23:55?106.
Halliday, M. A. K. and Ruqaiya Hasan. 1976. Cohe-
sion in English. London: Longman.
Huong, LeThanh, Geetha Abeysinghe, and Christian
Huyck. 2003. Using cohesive devices to recog-
nize rhetorical relations in text. In Proceedings of
4th Computational Linguistics UK Research Collo-
quium (CLUK 4), University of Edinburgh, UK.
Kibble, Rodger. 1999. Nominalisation and rhetorical
structure. In Proceedings of ESSLLI Formal Gram-
mar conference, Utrecht.
Knott, Alistair. 1996. A Data-Driven Methodology
for Motivating a Set of Coherence Relations. Ph.D.
thesis, University of Edinburgh, Edinburgh.
1030
Lin, Ziheng, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the penn
discourse treebank. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing, Singapore.
Maat, Henk Pander and Ted Sanders. 2000. Do-
mains of use or subjectivity? the distribution of
three dutch causal connectives explained. TOPICS
IN ENGLISH LINGUISTICS, pages 57?82.
Marcu, Daniel and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse re-
lations. In Proceedings of the Association for Com-
putational Linguistics.
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Martin, James R. 1992. English text: System and
structure. Benjamins, Amsterdam.
Oza, Umangi, Rashmi Prasad, Sudheer Kolachina,
Dipti Mishra Sharma, and Aravind Joshi. 2009.
The hindi discourse relation bank. In Proceedings
of the ACL 2009 Linguistic Annotation Workshop III
(LAW-III), Singapore.
Pitler, Emily and Ani Nenkova. 2009. Using syntax to
disambiguate explicit discourse connectives in text.
In Proceedings of the Joint Conference of the 47th
Meeting of the Association for Computational Lin-
guistics and the 4th International Joint Conference
on Natural Language Processing, Singapore.
Pitler, Emily, Annie Louis, and Ani Nenkova. 2009.
Automatic sense prediction for implicit discourse
relations in text. In Proceedings of the Joint Con-
ference of the 47th Meeting of the Association for
Computational Linguistics and the 4th International
Joint Conference on Natural Language Processing.
Power, Richard. 2007. Abstract verbs. In ENLG ?07:
Proceedings of the Eleventh European Workshop on
Natural Language Generation, pages 93?96, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Prasad, Rashmi, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank 2.0.
In Proceedings of 6th International Conference on
Language Resources and Evaluation (LREC 2008).
PDTB-Group. 2008. The Penn Discourse TreeBank
2.0 Annotation Manual. Technical Report IRCS-08-
01, Institute for Research in Cognitive Science, Uni-
versity of Pennsylvania.
Sporleder, Caroline and Alex Lascarides. 2008. Using
automatically labelled examples to classify rhetori-
cal relations: an assessment. Natural Language En-
gineering, 14(3):369?416.
Steedman, Mark. 2000. The Syntactic Process. MIT
Press, Cambridge MA.
Taboada, Maite. 2006. Discourse markers as signals
(or not) of rhetorical relations. Journal of Pragmat-
ics, 38(4):567?592.
Yu, Hong, Nadya Frid, Susan McRoy, P Simpson,
Rashmi Prasad, Alan Lee, and Aravind Joshi. 2008.
Exploring discourse connectivity in biomedical text
for text mining. In Proceedings of the 16th Annual
International Conference on Intelligent Systems for
Molecular Biology BioLINK SIG Meeting, Toronto,
Canada.
1031
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1202?1212,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Computing Logical Form on Regulatory Texts?
Nikhil Dinesh
Artificial Intelligence Center
SRI International
Menlo Park, CA - 94025
dinesh@ai.sri.com
Aravind Joshi and Insup Lee
Department of Computer Science
University of Pennsylvania
Philadelphia, PA - 19104
{joshi,lee}@seas.upenn.edu
Abstract
The computation of logical form has been pro-
posed as an intermediate step in the translation
of sentences to logic. Logical form encodes
the resolution of scope ambiguities. In this
paper, we describe experiments on a modest-
sized corpus of regulation annotated with a
novel variant of logical form, called abstract
syntax trees (ASTs). The main step in com-
puting ASTs is to order scope-taking opera-
tors. A learning model for ranking is adapted
for this ordering. We design features by study-
ing the problem of comparing the scope of one
operator to another. The scope comparisons
are used to compute ASTs, with an F-score of
90.6% on the set of ordering decisons.
1 Introduction
May (1985) argued for a level of logical form as a
prelude to translating sentences to logic. Just as a
parse tree determines the constituent structure of a
sentence, a logical form of a sentence represents one
way of resolving scope ambiguities. The level of
logical form is an appealing layer of modularity; it
allows us to take a step beyond parsing in studying
scope phenomenon, and yet, avoid the open problem
of fully translating sentences to logic.
Data-driven analyses of scope have been of in-
terest in psycholinguistics (Kurtzman and MacDon-
ald, 1993) and more recently in NLP (Srinivasan
and Yates, 2009). The focus has typically been
?This research was supported in part by ONR MURI
N00014-07-1-0907, NSF CNS-1035715, NSF IIS 07-05671,
and SRI International.
on predicting the preferred scopal ordering of sen-
tences with two quantifying determiners, for exam-
ple, in the sentence ?every kid climbed a tree?. In
the related problem of translating database queries
to logic, Zettlemoyer and Collins (2009) and Wong
and Mooney (2007) consider the scope of adjectives
in addition to determiners, for example the scope of
?cheapest? in the noun phrase ?the cheapest flights
from Boston to New York?. To our knowledge, em-
pirical studies of scope have been restricted to phe-
nomenon between and within noun phrases.
In this paper, we describe experiments on a novel
annotation of scope phenomenon in regulatory texts
? Section 610 of the Food and Drug Administra-
tion?s Code of Federal Regulations1 (FDA CFR).
Determiners, modals, negation, and verb phrase
modifiers are the main scope-taking operators. We
have annotated 195 sentences with a variant of log-
ical form, called abstract syntax trees (ASTs). Our
focus is on the problem of computing the AST, given
a (variant of a) parse tree of a sentence.
The long term goal of this work is to assist in the
translation of regulation to logic, for the application
of conformance checking. The problem is to for-
mally determine whether an organization conforms
to regulation, by checking the organization?s records
using the logical translation of regulation. Confor-
mance checking has been of interest in a variety of
regulatory contexts, and examples include privacy
policy (Barth et al, 2006; Jones and Sergot, 1992;
Anderson, 1996) and business contracts (Governa-
tori et al, 2006; Grosof et al, 1999).
We now discuss some problems that arise in defin-
1http://www.gpoaccess.gov/cfr/index.html
1202
ing logical form and the assumptions that we make
to circumvent these problems.
1.1 Problems and Assumptions
A key assumption of logical form is that the trans-
lation from language to logic is syntax-based. As
a result, the logic needs to be expressive enough to
accomodate a syntactic translation. There is no con-
sensus logic for constructs, such as, plurals, purpose
clauses, and certain modals. This leads to the fol-
lowing problem in defining logical form.
How do we define the logical form of a sentence,
without defining the logic? We adopt a specific for-
malism that accomodates a subset of the constructs
found in regulation. We generalize from the formal-
ized constructs to other constructs. Some of these
generalizations may need revision in the future.
We assume that sentences in regulation are trans-
lated to statements in logic of the form:
(id) ?(x1, ..., xn) 7? ?(x1, ..., xn)
where, ?id? is an identifier, ? is the precondition,
? is the postcondition, and x1, ..., xn are free vari-
ables. The distinction between pre and postcondi-
tions has been adopted by most logics for regula-
tion, to accomodate exceptions to laws (Sergot et al,
1986; Makinson and van der Torre, 2000; Governa-
tori et al, 2006). The pre and postconditions are
expressed in a modal logic that we designed in prior
work (Dinesh et al, 2011). In describing the logi-
cal form, we will sketch how the logical form can
be mapped to logic. But, we do not assume that the
reader has a detailed understanding of the logic.
Given the assumptions about the logic, our goal
is to transform a regulatory sentence into a structure
that lets us determine: (I) the constituents of a sen-
tence that contribute to the pre/postcondition, and
(II) the scope of operators in the pre/postcondition.
The structures that we use are called abstract syn-
tax trees (ASTs), which can be understood as a re-
stricted kind of logical form for regulatory texts.
1.2 Contributions and Outline
In this paper, we focus on the problem of computing
the AST given a (kind of) parse tree for a sentence.
The main step is is to order or rank scope-taking
operators. A learning model for ranking is adapted
for this ordering. We design features by studying the
problem of comparing the scope of one operator to
another. The pairwise scope comparisons are then
used to compute ASTs, with an F-score of 90.6% on
the set of ordering decisons.
The rest of this paper is organized as follows. We
define ASTs using an example in Section 2, and
setup the learning problem in Section 3. We then de-
scribe the corpus using statistics about operators in
Section 4. In Section 5, we describe experiments on
comparing the scope of an operator to another. We
use the pairwise scope comparisons, in Section 6 to
comput the AST. We discuss related work in Sec-
tion 7 and conclude in Section 8.
2 Abstract Syntax Trees
We describe abstract syntax trees (ASTs) using an
example from CFR Section 610.11:
(1) A general safety test for the detection of extra-
neous toxic contaminants shall be performed on
biological products intended for administration
to humans.
We discuss the translation in logic and the AST
for the fragment of (1) that appears in black. In or-
der to keep figures to a manageable size, we restrict
attention to fragments of sentences, by graying out
portions. The term AST is borrowed from compil-
ers (Aho et al, 1986), where it is used as an interme-
diate step in the semantic interpretation of programs.
Translation in Logic: The sentence (1) is formally
expressed as:
(1) bio prod(x) 7? Om(x)(?y : test(y) ? ?(x, y))
where, ?(x, y) = gensaf(y)?ag(y,m(x))?ob(y, x)
The predicates and function symbols are read as
follows. bio prod(x) - ?x is a biological product?.
m(x) denotes the manufacturer of x. The modal op-
erator O stands for ?obligation?. test(y) - ?y is a
test (event)?. gensaf(y) - ?y is a general safety pro-
cedure?. ag(y,m(x)) - ?the agent of y ism(x)?, and
ob(y, x) - ?the object of the event y is x?. The for-
malized version of the law is read as follows: ?If x
is a biological product, then the manufacturer m(x)
is required/obligated to perform a general safety test
y which has x as its object?. We refer the reader
to (Dinesh et al, 2011) for details on the logic.
1203
The distinction between pre and postconditions
is a non-trivial assumption. As with all logic-
programming formalisms, only free variables are
?shared? between pre and postconditons. This im-
plies that all existential quantification, modals, and
negation appear within the pre or postcondition. In
the example above, the existential quantifier (?y)
and the modal (O) appear within the postcondition.
Abstract Syntax Tree: The AST for (1) is shown in
Figure 1. The main nodes of interest are the inter-
nal nodes labeled ? . An internal node with n + 1
children corresponds to an n-ary operator. The first
child of the internal node is the operator. Opera-
tors are labeled with a part-of-speech tag, for exam-
ple, ?D? for determiner, ?M? for modal, and ?O? for
other. The remaining n children are its arguments.
We use the term nuclear scope to refer to the last
(nth) argument of the operator, and the term restric-
tor to refer to any other argument. We borrow these
terms from the literature on quantifier scope for de-
terminers (Heim and Kratzer, 1998, Chapter 7).
For example, the phrase ?general safety test? is in
the restrictor of the operator A, and the variable y
is in its nuclear scope. The modal shall is a unary
operator, and doesn?t have a restrictor. Non-unary
operators bind the variable displayed on the internal
node. The variable y is bound by the operator A.
Implicit operators are inserted when there is no
overt word or phrase. In Figure 1, the implicit oper-
ators are underlined. The generic noun phrase ?bi-
ological products? is associated with the implicit de-
terminer all. Similarly, we use the implicit operator
Post to mark the position of the postcondition.
?x
D
all
R
bio. prod.
?
O
Post
?
M
shall
?
M
be
?y
D
A
R
gen. saf. test
.
y performed on x
Figure 1: Example of an abstract syntax tree (AST).
We conclude this section with some notation for
describing ASTs. Given an AST for a sentences, we
say that an operator oi scopes over oj, denoted oi 
oj, if oj appears in the nuclear scope of oi. For ex-
ample, in Figure 1, we have all  Post, all  shall,
all  A, Post  A, and shall  A. In addition, we
say that the restrictor of oi scopes over oj, denoted
R(oi)  oj, if oj appears in the restrictor of oi. Such
configurations occur with PP-modification of NPs,
and we discuss examples in later sections.
3 Computing ASTs ? Overview
In this section, we give an overview of our approach
to computing ASTs. We will assume as given a Pro-
cessed Parse Tree (PPT) of a sentence, with the op-
erators and their restrictors identified. An example
is discussed in Section 3.1. Given such a PPT, the
AST is computed in two steps: (1) finding the preter-
minal at which an operator takes scope, and (2) or-
dering the operators associated with a preterminal.
We describe the second step in Section 3.2, and then
briefly outline the first step in Section 3.3. The steps
are described in reverse order, because in most cases,
the operators associated with a preterminal are deter-
mined directly by syntactic attachment.
3.1 Processed Parse Trees
We compute ASTs from processed parse trees
(PPTs) of sentences. Figure 2 gives the PPT cor-
responding to the AST in Figure 1.
.
?
P
Post
?y
D
A
R
gen. saf. test
?
M
shall
?
M
be
.
performed on
?x
D
IMP
R
bio. prod.
Figure 2: Processed parse tree (PPT) for (1).
A PPT provides the set of operators in a sen-
tence, associated with their restrictors. For exam-
ple, the determiner ?a? has the restrictor general
safety test. The phrase biological products has no
explicit determiner associated with it, and the cor-
responding operator in the PPT is labeled ?IMP?
for implicit. In addition, the postcondition marker
?Post? is also identified. Except for the postcon-
1204
dition marker, annotator-specified implicit operators
are not given in the PPT.
There are two main types of nodes in the PPT ?
operators and preterminals. The nodes labeled with
the symbol ?, e.g., ? and ?x , correspond to op-
erators. The root of the PPT and the restrictors of
the operators, are the preterminals. Based on this
example, it may seem that a sentence just has a list
of operators. While this is true of example (1), em-
bedded operators arise, for example, in the context
of PP-modification of NPs and relative clauses. We
will discuss an example in Section 3.3.
In this work, the PPTs are obtained by removing
all scope decisions from the AST. To a first approxi-
mation, we start by removing all operators from the
AST, and then, replace the corresponding variables
by the operators. Implicit unary operators (such as
the postcondition marker) are placed at the start of
the preterminal.
It is worthwhile to consider whether it is rea-
sonable to assume PPTs as given. We believe that
this assumption is (slightly) stronger than assuming
perfect parse trees. Although the PPT leaves cer-
tain chunks of the sentence unprocessed, in most
cases, the unprocessed chunks correspond to base
NPs. The main additional piece of information is the
existence of a postcondition marker for each main
clause of a sentence. We believe that computation
of PPTs is better seen as a problem of syntax rather
than scope, and we set it aside to future work. Our
focus here is on converting a PPT to an AST.
3.2 Ordering Operators
The problem of learning to order a set of items is
not new. Cohen et al (1998) give a learning theo-
retic perspective, and Liu (2009) surveys informa-
tion retrieval applications. The approach that we use
can be seen as a probabilistic version of the boosting
approach developed by Cohen et al (1998). We ex-
plain the step of ordering operators, by revisiting the
example of the general safety test, from Section 2.
Given the PPT in Figure 2, we compute the AST
in Figure 1 by ordering or ranking the operators. For
example, we need to determine that the implicit de-
terminer associated with biological products is uni-
versal, and hence, we have IMP  Post. However,
the determiner ?A? associated with general safety
test is existential, and hence, we have Post  A.
We now develop some notation to describe the
scopal ordering of operators. A PPT ? is viewed
as a set of preterminal nodes, and we will write ?
(a) p ? ? to denote that p occurs in ? , and (b)
|? | to denote the number of preterminals in ? . A
preterminal p is viewed as an ordered set of oper-
ators p = (o1, ..., o|p|). For example, in Figure 2,
the root preterminal p has |p| = 5, and the operators
o1 = Post, o2 = A, o3 = shall, and so on.
An AST ? contains a ranking of operators asso-
ciated with each preterminal, denoted r?(p). The
ranks of operators are denoted by subscripts. Let
p = (o1, ..., o5) be the root preterminal of the PPT
in Figure 2. The ranking associated with the AST in
Figure 1 is given by r?(p) = (o12, o25, o33, o44, o51). For
example, o25 = A denotes that the determiner ?A? ap-
pears second in the surface order (Figure 2) and fifth
or lowest in the scope order (Figure 1). Similarly,
o51 = IMP denotes that the implicit determiner ap-
pears fifth or last in the surface order (Figure 2) and
first or highest in the scope order (Figure 1). Note
that the subscript suffices to identify the position of
an operator in the AST.
Model: We now describe the learning model for or-
dering operators. Given a PPT ? , let A(?) be the set
of all possible ASTs. Our goal is to find the AST
which has the highest probability given the PPT:
?? = arg max
??A(?)
P (?|?)
The conditional probability of an AST is defined as:
P (?|?) =
?
p??
P (r?(p)|?)
P (r?(p)|?) =
|p|?1?
i=1
|p|?
j=i+1
P (oi  oj|?)
In other words, P (?|?) is modeled as the product
of the probabilities of the ranking of each pretermi-
nal, which is in turn expressed as the product of the
probabilities of the pairwise ordering decisions. The
model falls under the class of pairwise ranking ap-
proaches (Liu, 2009). We will consider the problem
of estimating the probabilities in Section 5, and the
problem of searching for the best AST in Section 6.
1205
.?
P
Post
?x3
D
IMP
R
samp. of
?x1
D
any
R
.
lot of
?x2
D
a
R
lic. prod.
?
M
may
?
M
be
.
...
Figure 3: PPT for (2)
?x1
D
any
R
?x2
D
a
R
lic. prod.
.
lot of x2
?
P
Post
?
M
may
?
M
be
?x3
D
some
R
samp. of x1
.
...
Figure 4: AST for (2)
3.3 Finding the Scope Preterminal
In the example that we discussed in the previous sec-
tion, there were no embedded operators, i.e., an op-
erator or its variable located in the restrictor of an-
other. An embedded operator can either ? (a) take
scope within the restrictor of the embedding oper-
ator, or (b) outscope the embedding operator. To
account for the second case, we need to determine
whether it is appropriate to lift an embedded opera-
tor to a higher preterminal than the one to which it
is associated syntactically.
We discuss an example of inverse linking (Larson,
1985) to illustrate the problem. Consider the follow-
ing sentence:
(2) Samples of any lot of a licensed product, except
for radioactive biological products, together with
the protocols showing results of applicable tests,
may at any time be required to be sent to the Di-
rector, Center for Biologics Evaluation and Re-
search.
The PPT and AST for (2) are shown in Figures 3
and 4 respectively. Consider the noun phrase ?IMP
samples of any lot of a licensed product? in the
.
?
P
Post
?x1
D
any
R
.
lot of
?x2
D
a
R
lic. prod.
?x3
D
IMP
R
samp. of x1
?
M
may
?
M
be
.
...
Figure 5: Second PPT for (2), obtained from the PPT in
Figure 3, by raising any to the root preterminal.
PPT. The implicit determiner IMP in the PPT is in-
terpreted as the existential determiner some in the
AST. The three operators are related as follows in
the AST: any  some and R(any)  a, i.e., any
outscopes the implicit determiner, and a appears in
the restrictor of any. Observe that the variables x1
and x2 , which are associated with any and a, ap-
pear in the restrictors of some and any respectively.
As a result, in the PPT, in Figure 3, any and a appear
in the restrictor of IMP and any. The PPT provides
a standard parse of PP-modification of NPs.
The important feature of this example is that
the determiner ?any? is syntactically embedded in
the restrictor of IMP in the PPT (Figure 4), but it
outscopes the implicit determiner in the AST (Fig-
ure 3). As a result, the PPT in Figure 3 cannot be
converted to the AST in Figure 4 simply by ranking
sibling operators (as we did in the previous section).
To handle such cases, we convert the PPT in Fig-
ure 3 to a second PPT (shown in Figure 5). The only
allowed operation during this conversion is to raise
an embedded operator to a higher preterminal. The
PPT in Figure 5 is obtained by raising any to the
root preterminal, making it a sibling of the implicit
determiner IMP in the PPT in Figure 5. This second
PPT can be converted to the AST by reordering sib-
ling operators. The learning model used for this step
is similar to the one used to order operators, and in
the interests of space, we omit the details.
4 Brief Overview of the Corpus
We have annotated 195 sentences from the FDA
CFR Section 610 with ASTs. The operators are di-
vided into the following types ? determiners (e.g.,
1206
every, a, at least), modal auxiliaries (e.g., must,
be), VP modifiers (e.g., if, for, after), negation and
coordinating conjunctions (e.g., and, but, or). The
majority of the corpus was annotated by a single an-
notator. However, to estimate inter-annotator agree-
ment, a set of 32 sentences was annotated by a
second annotator. In this section, we restrict our-
selves to presenting statistics that highlight part of
the guidelines and motivate the features that we use
to order operators. An example-based justification
of guidelines, and a discussion of inter-annotator
agreement can be found in (Dinesh, 2010).
De Re vs De Dicto: We narrow our focus to one part
of the annotation, the de re vs de dicto distinction.
Informally, operators with de re scope occur in the
precondition of the logical translation of a sentence,
while those with de dicto scope occur in the post-
condition. This distinction is of key importance in
the application of conformance checking, as it helps
determine the facts that need to be provided by an
organization (de re), and the actions that an organi-
zation is required to take (de dicto).
For simplicity, we further restrict attention to op-
erators that are siblings of the postcondition in the
AST, and ignore the operators embedded in preposi-
tional phrases and clauses, for example. A (main
clause) operator o is said to have de re scope iff
it outscopes the postcondition marker (o  Post).
Otherwise, the operator is said to have de dicto scope
(Post  o). In the example of the general safety test
from Section 2, the implicit determiner associated
with ?biological products? has de re scope, while all
other operators in the sentence have de dicto scope.
Operator Number of De Re Scope
Type Instances Percentage
Determiner 277 59.9%
Modal Aux 268 0%
VP Modifier 132 68.2%
CC 36 22.2%
Neg 33 0%
Other 74 17.6%
Table 1: De Re scope distribution. An operator has de re
scope iff it outscopes the postcondition marker.
Table 1 shows the percentage of each type of op-
erator that has de re scope. Modal auxiliaries and
negation are umambigous to this distinction, and al-
ways have de dicto scope. Note that a type of opera-
tor with 50% occuring de re is ambiguous, while 0%
or 100% are unambiguous. Thus, from Table 1, we
can conclude that determiners, and VP modifiers are
the most ambiguous types. And, more features are
needed to disambiguate them.
Determiner Number of De Re Scope
Type Instances Percentage
Universal 74 100%
Existential 12 0%
Ambiguous 50 28%
Deictic 127 53.5%
Other 14 35.7%
Table 2: De Re scope distribution for determiners.
Determiners: We divide the determiners into the
following subtypes: universal/generic (e.g., every,
all), existential (some), ambiguous (e.g., a, an), de-
ictic (e.g., the, those), and other (e.g., at least, at
most). The guidelines for annotation were as fol-
lows ? (a) universal determiners have de re scope,
(b) existential determiners have de dicto scope, and
(c) for other determiners, the annotator needs to de-
cide whether a particular use is interpreted existen-
tially or universally. Table 2 shows de re scope dis-
tribution for each of these subtypes. As expected,
universal and existential determiners are unambigu-
ous, while ambiguous and deictic determiners show
more variety. For example, the deictic determiner
the can refer to a specific entity (?the FDA?) or have
a universal interpretation (?the products?).
Thus, to disambiguate between de re and de dicto
interpretations for determiners, we need two types of
features ? (1) Features to predict whether ambiguous
and deictic determiners are universal or not, and (2)
Features to determine the type of implicit determin-
ers. In Table 2, we assume that the type of implicit
determiners are given. This assumption is unreal-
istic. Rather, we need to predict the type of such
determiners, during the computation of the AST.
VP Modifier Number of De Re Scope
Type Instances Percentage
Temporal and Conditional 73 100%
Purpose 8 0%
References to Laws 33 0.9%
Other 29 65.5%
Table 3: De Re scope distribution for VP modifiers.
1207
VP Modifiers: We divide the VP modifiers into the
following subtypes: temporal and conditional (e.g.,
after, if), purpose (for), references to laws (which
are a special type of modifier in the legal domain,
e.g., ?as specified in paragraph (c)?), and other (e.g.,
regardless, notwithstanding). Table 3 shows the
percentage of each subtype of modifier that has de
re scope. Following the guidelines for annotation,
the temporal and conditional modifiers are always de
re, the purpose modifiers and modifiers conveying
references to laws are always de dicto.
5 Comparing the Scope of Operators
We now consider a subproblem in computing the
AST ? comparing the scope of pairs of operators.
In Section 6, we will use the classifiers that perform
comparisons, to compute the AST. All experiments
in this section use the MAXENT implentation from
the MALLET toolkit (McCallum, 2002). We begin
by revisiting de re-de dicto distinction from Sec-
tion 4. Then, we generalize to other comparisons.
De Re vs De Dicto: The (binary) classification
problem is as follows. Our observations are triples
x = (o, o?, ?) are such that there is a preterminal
p ? ? , {o, o?} ? p, and o? = Post. In other words,
we are considering operators (o) that are siblings of
the postcondition marker (o?). An observation has
the label 1 if o  o? (de re scope), and a label of 0
otherwise (de dicto scope).
Features: We use the following (classes of) fea-
tures for an observation x = (o, o?, ?):
? TYPE - The type and subtype of the operator.
We use the subtypes from Section 4 only for
explicit operators.
? PRE-VERB - Tracks whether o and o? appear
before or after the main verb of the sentence.
? PRE-VERB + PERF - Conjunction of the previ-
ous feature with whether the main verb is per-
form. The verb perform is frequent in the CFR,
and its subject is typically given de dicto scope,
as it is the main predicate of the sentence.
? POS - The part-of-speech of the head word. For
example, for the noun phrase biological prod-
ucts, the head word is products, and the POS is
NNS (plural common noun). And, this POS tag
may indicate a generic/universal interpretation.
Count MAJORITY TYPE ALL
All 823 66.2% 84.1% 89.2%
No MD 522 53.2% 74.9% 83.7%
DT 277 59.9% 62.9% 81.2%
Imp. DT 100 69% 76%
Table 4: De Re vs De Dicto classification. Average accu-
racies over 10-fold cross-validation. The rows describe
the subset of observations considered, and the columns
describe the subset of features used.
Experiments: We evaluate the features by perform-
ing 10-fold cross-validation. The results are summa-
rized in Table 4. The rows describe the subset of ob-
servations used. ?All? includes all observations, ?No
MD? excludes the modal auxiliaries, ?DT? includes
only the determiners, and ?Imp. DT? includes only
implicit determiners. The columns describe the fea-
tures used. MAJORITY is the majority baseline, i.e.,
the accuracy obtained by predicting the most fre-
quent class or the majority class. The majority class
is de dicto when all operators are considered (the
first row), and de re in all other rows. The TYPE
column gives the accuracy when only the type and
subtypes are used as features. This column does not
apply to implicit determiners, as the subtype infor-
mation is unavailable. And, finally, the ALL column
gives the accuracy when all features are used.
From Table 4, we can conclude that the TYPE fea-
ture is useful in making the de re-de dicto distinc-
tion, and further gains are obtained by using ALL
features. The most dramatic improvement is for de-
terminers, and indeed, our features were designed
for this case. However, the performance gains are
not very high for implicit determiners, and further
investigation is needed.
Next, we apply the features to more general oper-
ator comparisons. The first row of Table 5 considers
observations x = (o, o?, ?), where o and o? are sib-
lings, and predicts whether o  o?. The second row
considers observations where o? is embedded syn-
tactically within o, and predicts whether R(o)  o?.
In other words, the problem is to determine whether
a syntactically embedded operator remains scopally
embedded, or whether it has inverse scope (see Sec-
tion 3.3).
1208
Count MAJORITY TYPE ALL
Siblings 2793 76.1% 83.3% 87.5%
Embedded 5081 95% 95.3% 96.4%
Table 5: Ordering siblings and embedded operators.
Average accuracies over 10-fold cross-validation. The
columns describe the subset of features used.
6 From Operator Comparisons to ASTs
We now consider the problem of computing the
AST given the classifiers for comparing operators.
Section 6.1 describes the algorithms used. In Sec-
tion 6.2, we develop metrics to evaluate the com-
putation of ASTs. We conclude, in Section 6.3, by
evaluating different algorithms using the metrics.
6.1 Algorithms
We begin by discussing the intractability of the prob-
lem of ranking or ordering operators. Then, we
sketch the search heuristics used.
Intractability: The decision version of the rank-
ing problem is NP-complete. A similar result is es-
tablished by Cohen et al (1998) in the context of a
boosting approach to ranking.
Theorem 1. The following problem is NP-complete:
Input: A PPT ? , a preterminal p ? ? , probabilities
P (oi  oj|?), and c ? [0, 1]
Output: Yes, if there is an ordering r such that
P (r(p)|?) ? c
The proof is by reduction from ACYCLIC SUB-
GRAPH (Karp, 1972) ? finding a subgraph which is
acyclic and has at least k edges.
Heuristics: To order operators, we use a beam
search procedure. Each search state consists preter-
minal, in which the first i ranks have been assigned
to operators. We then search over next states by as-
signing the rank i+1 to one of the remaining opera-
tors. We used a beam size of 104 in our experiments.
In most cases, the number of operators per preter-
minal is less than 7. As a result, the total number
of possible orderings is typically less than 7!, and a
beam size of 104 is sufficient to compute an exact or-
dering. In other words, due to the size restrictions, in
most cases, beam search is equivalent to exact (ex-
haustive) search.
To handle embedded operators, we use a simple
greedy heuristic. We enumerate the operators in the
initial PPT, corresponding to an in-order traversal.
For each operator, we attach it to the most likely an-
cestor, given the attachment decisions for the previ-
ous operators. This heuristic is optimal for the case
where the depth of embedding is at most 1, which is
the common case.
6.2 Metrics
In this section, we describe metrics used to evaluate
the computation of ASTs. Let ? be the initial PPT,
? the correct AST, and ?? the computed AST. We
define accuracy at various levels.
The simplest metric is to define accuracy at the
level of ASTs, i.e., by computing the fraction of cases
for which ? = ??. However, this metric is harsh,
in the sense that it does not give algorithms partial
credit for getting a portion of the AST correct.
The next possible metric is to define accuracy at
the level of preterminals. Let p be a preterminal.
Note that ? , ? and ?? share the same set of preter-
minals, but may associate different operators with
them. We say that p is correct in ??, if it is asso-
ciated with the same set of operators as in ?, and
for all {o, o?} ? p, we have o  o? w.r.t. ?? iff
o  o? w.r.t. ?. In other words, the preterminals
are identical, both in terms of the set of operators
and the ordering between pairs of operators. While
preterminal-level accuracy gives partial credit, it is
still a little harsh, in the sense that an algorithm
which makes one ordering mistake at a preterminal
is penalized the same as an algorithm which makes
multiple mistakes.
Finally, we consider metrics to define accuracy at
the level of pairs of operators. Let p be a preter-
minal. The set Pairs(p, ?) consists of pairs of op-
erators (o, o?) such that o and o? are both associ-
ated with p in ?, and o = o? or o  o?. The set
Pairs(p, ??) is defined similarly using ?? instead of
?. Given the sets Pairs(p, ?) and Pairs(p, ??), pre-
cision, recall, and f-score are defined in the usual
way. We leave the details to the reader.
6.3 Results
We evaluate the following algorithms:
1. No Embedding ? The AST is computed purely
by reordering operators within a preterminal in
the PPT.
1209
(a) SURFACE ? No reordering is performed,
i.e., the order of operators in the AST re-
spects the surface order
(b) TYPE ? Using only type and subtype in-
formation for the operators
(c) ALL ? Using all the features described in
Section 5
2. ALL+ ? The initial PPT is transformed into a
second PPT before reordering (as described in
Section 3.3). All features are used.
Prec. Rec. F p ?
SURF. 86.9% 82.7% 84.6% 81% 4.2%
TYPE 90.4% 86% 88.1% 83.6% 24.7%
ALL 92% 87.6% 89.8% 85.1% 33.5%
ALL+ 91.9% 89.4% 90.6% 85.9% 36.2%
Table 6: Performance of the algorithms in computing the
ASTs. Averaged over 10-fold cross-validation. 195 ASTs
in total, an average of 8.6 preterminals per AST, and 1.8
operators per preterminal.
Table 6 summarizes the performance of the al-
gorithms, under the various metrics. The accura-
cies are averaged over 10-fold cross-validation. A
total of 195 ASTs are used. The average number
of preterminals per AST is 8.6, with an average of
1.8 operators per preterminal. The best number un-
der each metric is shown in bold-face. By adding
features, we improve the precision from 86.9% to
90.4% to 92% in moving from SURFACE to TYPE
to ALL. By handling embedded operators, we im-
prove the recall from 87.6% to 89.4% in moving
from ALL to ALL+. As we saw in Section 5, in 95%
of the cases, the embedded operators respects syn-
tactic scope, and as a result, we obtain only modest
gains from handling embedded operators.
The reader may feel that the F-score of 90.6% is
quite high given the size of our training data. This
score is inflated by inclusion of reflexive pairs, of
the form (o, o). Such pairs are included for the
following (technical) reasons. The algorithm that
handles embedded operators (ALL+) usually raises
them from a single operator node (as in Figure 3) to
a multi-operator node (as in Figure 5). If it makes
an incorrect decision to raise an operator it takes a
precision hit, at the multi-operator node (because
it has some false positives). By contrast, an algo-
rithm loses precision for failing to correctly raise,
only when we encounter the single operator node.
For these reasons, it is better to consider the rela-
tive improvement in F-score over the baseline. The
relative improvement of ALL+ over SURFACE in
terms of F-score is 36.6%. We believe that the
preterminal-level accuracy is more indicative in an
absolute sense. Furthermore, when we restrict atten-
tion to those preterminals with two or more opera-
tors in the PPT, the accuracy of ALL+ is 69.4%.
7 Related Work
ASTs can be seen as a middle ground between two
lines of research in translating sentences to logic.
At one end of the spectrum, we have methods
that achieve good accuracy on restricted texts. The
two main corpora that have been considered are the
GEOQUERY corpus (Thompson et al, 1997) and
the ATIS-3 corpus (Dahl et al, 1994). The GEO-
QUERY corpus consists of queries to a geographical
database. The queries were collected from students
participating in a study and the average sentence
length is 8 words. The ATIS corpus is collected
from subjects? interaction with a database of flight
information, using spoken natural language. The ut-
terances have be transcribed, and the average sen-
tence length is 10 words (Berant et al, 2007). Algo-
rithms, which achieve good accuracy, have been de-
veloped to compute the logical translation for these
queries (Zettlemoyer and Collins, 2005; Wong and
Mooney, 2007; Zettlemoyer and Collins, 2009). The
annotated sentences in the FDA CFR Section 610.40
are longer (about 30 words on average), and contain
modalities which are not present in these corpora.
At the other end of the spectrum, Bos et al (Bos et
al., 2004) have developed a broad-coverage parser to
translate sentences to a logic based on discourse rep-
resentation theory. Here, there is no direct method to
evaluate the correctness of the translation. However,
indirect evaluations are possible, for example, by
studying improvement in textual entailment tasks.
To summarize, there are techniques that either
produce an accurate translation for sentences in a
limited domain, or produce some translation for sen-
tences in a broader range of texts. ASTs offer a mid-
dle ground in two ways. First, we focus on regula-
1210
tory texts which are less restricted than the database
queries in the GEOQUERY and ATIS corpora, but do
not exhibit anaphoric phenomenon found in genres,
such as, newspaper text. In (Dinesh et al, 2007), we
discuss lexical statistics that show significant differ-
ences in the distribution of anaphoric items in the
CFR and Wall Street Journal (WSJ) corpora. For ex-
ample, the frequency of pronouns and anaphoric dis-
course connectives is significantly lower in the CFR
than in the WSJ. Instead, the CFR has an idiosyn-
cratic mechanism for referring to sentences, using
phrases such as ?except as specified in paragraph
(c) and (d)?. A question of interest is whether the
GEOQUERY and ATIS corpora show similar pecu-
liarities in terms of anaphora. The second difference
between our approach and others is that we do not
attempt to translate all the way to logic. The level of
logical form lets us obtain a direct evaluation, while
leaving open the design of parts of the logic.
8 Conclusions
We described experiments on a modest-sized cor-
pus of regulatory sentences, annotated with a novel
variant of logical form, called abstract syntax trees
(ASTs). An example from the corpus was presented
in Section 2 and some statistics, describing the cor-
pus, were discussed in Section 4. In Sections 3, 5,
and 6, we developed and tested algorithms to con-
vert a processed parse tree (PPT) to an AST. The
main step in this conversion was to rank or order the
operators at a preterminal. We presented a proba-
bilistic model for ranking, investigated the design of
features, and developed search heuristics. The best
algorithm, which uses all features and handles em-
bedded operators, achieves an F-score of 90.6%.
An important direction for further inquiry is in the
design of better features. Various types of features
have been proposed for the scopal ordering of deter-
miners. Examples include syntactic features (Ioup,
1975; Reinhart, 1983), such as position and voice,
semantic features (Grimshaw, 1990; Jackendoff,
1972), such as thematic roles. More recently, Srini-
vasan and Yates (2009) showed how pragmatic in-
formation, for example ?there are more people than
cities?, can be leveraged for scope disambiguation.
We experimented with lexico-syntactic features in
this work, and leave an investigation of semantic and
pragmatic features to future work.
Acknowledgements
We thank Claire Cardie, Steve Kimbrough, Annie
Louis, Fernando Pereira, Emily Pitler, Oleg Sokol-
sky, and the anonymous reviewers for helpful com-
ments on earlier versions of this paper.
References
A. Aho, R. Sethi, and J. Ullman. 1986. Compilers: Prin-
ciples, Techniques, and Tools. Addison-Wessley.
R. J. Anderson. 1996. A security policy model for clin-
cial information systems. In Proceedings of the IEEE
Symposium on Security and Privacy.
A. Barth, A. Dutta, J. C. Mitchell, and H. Nissenbaum.
2006. Privacy and contextual integrity: Framework
and applications. In Proceedings IEEE Symposium on
Security and Privacy.
J. Berant, Y. Gross, M. Mussel, B. Sandbank, E. Ruppin,
and S. Edelman. 2007. Boosting unsupervised gram-
mar induction by splitting complex sentences on func-
tion words. In Proceedings of the Boston University
Conference on Language Development.
J. Bos, S. Clark, M. Steedman, J. R. Curran, and J. Hock-
enmaier. 2004. Wide-coverage semantic representa-
tions from a CCG parser. In Proceedings of COLING.
W. W. Cohen, R. E. Schapire, and Y. Singer. 1998.
Learning to order things. Journal of Artificial Intel-
ligence Research, 10:243?270.
D. Dahl, M. Bates, M. Brown, W. Fisher, K. Hunicke-
Smith, D. Pallett, C. Pao, A. Rudnicky, and
E. Shriberg. 1994. Expanding the scope of the ATIS
task: the ATIS-3 corpus. In Proceedings of the ARPA
HLT Workshop.
N. Dinesh, A. Joshi, I. Lee, and O. Sokolsky. 2007.
Logic-based regulatory conformance checking. In
Proceedings of the 14th Monterey Workshop.
N. Dinesh, A. Joshi, I. Lee, and O. Sokolsky. 2011. Per-
mission to speak: A logic for access control and con-
formance. Journal of Logic and Algebraic Program-
ming, 80(1):50?74.
N. Dinesh. 2010. Regulatory Conformance Checking:
Logic and Logical Form. Ph.D. thesis, University of
Pennsylvania.
G. Governatori, Z. Milosevic, and S. Sadiq. 2006. Com-
pliance checking between business processes and busi-
ness contracts. In 10th International Enterprise Dis-
tributed Object Computing Conference (EDOC).
J. Grimshaw. 1990. Argument Structure. MIT Press.
1211
B. Grosof, Y. Labrou, and H. Y. Chan. 1999. A declar-
ative approache to business rules in contracts: Cour-
teous logic programs in xml. In ACM Conference on
Electronic Commerce.
Irene Heim and Angelika Kratzer. 1998. Semantics in
Generative Grammar. Blackwell.
G. Ioup. 1975. Some universals for quantifier scope.
Syntax and Semantics, 4:37?58.
R. Jackendoff. 1972. Semantic Interpretation in Genera-
tive Grammar. MIT Press.
A. J. I. Jones and M. J. Sergot. 1992. Formal spec-
ification of security requirements using the theory
of normative positions. In European Symposium on
Reasearch in Computer Security (ESORICS).
R. M. Karp. 1972. Reducibility among combinatorial
problems. In R. E. Miller and J. W. Thatcher, editors,
Complexity of Computer Computations, pages 85?103.
Plenum Press.
H. S. Kurtzman and M. C. MacDonald. 1993. Resolution
of quantifier scope ambiguities. Cognition, 48:243?
279.
R. K. Larson. 1985. Quantifying to np. Manuscript,
MIT.
T. Liu. 2009. Learning to rank for information retrieval.
Foundations and Trends in Information Retrieval, 3(3).
D. Makinson and L. van der Torre. 2000. Input/output
logics. Journal of Philosophical Logic, 29:383?408.
R. May. 1985. Logical Form: Its structure and deriva-
tion. MIT Press.
A. McCallum. 2002. MALLET: A machine learning for
language toolkit. http://mallet.cs.umass.edu.
T. Reinhart. 1983. Anaphora and Semantic Interpreta-
tion. Croom Helm.
M.J. Sergot, F.Sadri, R.A. Kowalski, F.Kriwaczek,
P.Hammond, and H.T. Cory. 1986. The british na-
tionality act as a logic program. Communications of
the ACM, 29(5):370?86.
P. Srinivasan and A. Yates. 2009. Quantifier scope
disambiguation using extracted pragmatic knowledge:
Preliminary results. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP).
C. A. Thompson, R. J. Mooney, and L. R. Tang. 1997.
Learning to parse natural language database queries
into logical form. In Proceedings of the Workshop on
Automata Induction, Grammatical Inference and Lan-
guage Acquisition.
Y. W. Wong and R. J. Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics (ACL).
L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Pro-
ceedings of UAI.
L. S. Zettlemoyer and M. Collins. 2009. Learning
context-dependent mappings from sentences to logi-
cal form. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics (ACL).
1212
Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), page 89,
Beijing, August 2010
Multi-Word Expressions as Discourse Relation Markers (DRMs)
Aravind K. Joshi
University of Pennsylvania
joshi@seas.upenn.edu
1 Invited Talk Abstract
Usually, by Multi-Word Expressions (MWEs) we mean expressions whose structure and meaning can-
not be derived from their component words as they occur independently. In this talk I will discuss a
different kind of multi-word expressions that behave as discourse relation markers (DRMs), yet do not
seem to belong to well-defined syntactic classes. The apparent open-endedness of these expressions is
a challenge for their automatic identification.1
2 Speaker Biography
Aravind Joshi is the Henry Salvatori Professor of Computer and Cognitive Science at the University
of Pennsylvania. He has worked on formal grammars, complexity of syntactic processing, and aspects
of discourse coherence. He has been the President of ACL, a member of ICCL, and a member of the
National Academy of Engineering, USA.
1This work is carried out in the context of the Penn Discourse Treebank (PDTB), jointly with Rashmi Prasad and Bonnie
Webber.
89
Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 66?74,
COLING 2010, Beijing, August 2010.
A Discriminative Approach for Dependency Based
Statistical Machine Translation
Sriram Venkatapathy
LTRC, IIIT-Hyderabad
sriram@research.iiit.ac.in sangal@mail.iiit.ac.in
Rajeev Sangal
LTRC, IIIT-Hyderabad
Aravind Joshi
University of Pennsylvania
joshi@seas.upenn.edu karthik.gali@gmail.com
Karthik Gali1
Talentica
Abstract
In this paper, we propose a dependency
based statistical system that uses discrim-
inative techniques to train its parameters.
We conducted experiments on an English-
Hindi parallel corpora. The use of syntax
(dependency tree) allows us to address the
large word-reorderings between English
and Hindi. And, discriminative training
allows us to use rich feature sets, includ-
ing linguistic features that are useful in the
machine translation task. We present re-
sults of the experimental implementation
of the system in this paper.
1 Introduction
Syntax based approaches for Machine Translation
(MT) have gained popularity in recent times be-
cause of their ability to handle long distance re-
orderings (Wu, 1997; Yamada and Knight, 2002;
Quirk et al, 2005; Chiang, 2005), especially for
divergent language pairs such as English-Hindi
(or English-Urdu). Languages such as Hindi are
also known for their rich morphology and long
distance agreement of features of syntactically re-
lated units. The morphological richness can be
handled by employing techniques that factor the
lexical items into morphological factors. This
strategy is also useful in the context of English-
Hindi MT (Bharati et al, 1997; Bharati et al,
1This work was done at LTRC, IIIT-Hyderabad, when he
was a masters student, till July 2008
2002; Ananthakrishnan et al, 2008; Ramanathan
et al, 2009) where there is very limited paral-
lel corpora available, and breaking words into
smaller units helps in reducing sparsity. In or-
der to handle phenomenon such as long-distance
word agreement to achieve accurate generation of
target language words, the inter-dependence be-
tween the factors of syntactically related words
need to be modelled effectively.
Some of the limitations with the syntax based
approaches such as (Yamada and Knight, 2002;
Quirk et al, 2005; Chiang, 2005) are, (1) They
do not offer flexibility for adding linguistically
motivated features, and (2) It is not possible to
use morphological factors in the syntax based ap-
proaches. In a recent work (Shen et al, 2009), lin-
guistic and contextual information was effectively
used in the framework of a hierarchical machine
translation system. In their work, four linguistic
and contextual features are used for accurate se-
lection of translation rules. In our approach in
contrast, linguistically motivated features can be
defined that directly effect the prediction of var-
ious elements in the target during the translation
process. This features use syntactic labels and col-
location statistics in order to allow effective train-
ing of the model.
Some of the other approaches related to our
model are the Direct Translation Model 2 (DTM2)
(Ittycheriah and Roukos, 2007), End-to-End Dis-
criminative Approach to MT (Liang et al, 2006)
and Factored Translation Models (Koehn and
Hoang, 2007). In DTM2, a discriminative trans-
66
lation model is defined in the setting of a phrase
based translation system. In their approach, the
features are optimized globally. In contrast to
their approach, we define a discriminative model
for translation in the setting of a syntax based ma-
chine translation system. This allows us to use
both the power of a syntax based approach, as
well as, the power of a large feature space during
translation. In our approach, the weights are op-
timized in order to achieve an accurate prediction
of the individual target nodes, and their relative
positions.
We propose an approach for syntax based sta-
tistical machine translation which models the fol-
lowing aspects of language divergence effectively.
? Word-order variation including long-
distance reordering which is prevalent
between language pairs such as English-
Hindi and English-Japanese.
? Generation of word-forms in the target lan-
guage by predicting the word and its factors.
During prediction, the inter-dependence of
factors of the target word form with the fac-
tors of syntactically related words is consid-
ered.
To accomplish this goal, we visualize the prob-
lem of MT as transformation from a morpho-
logically analyzed source syntactic structure to a
target syntactic structure1 (See Figure 1). The
transformation is factorized into a series of mini-
transformations, which we address as features of
the transformation. The features denote the vari-
ous linguistic modifications in the source structure
to obtain the target syntactic structure. Some of
the examples of features are lexical translation of
a particular source node, the ordering at a particu-
lar source node etc. These features can be entirely
local to a particular node in the syntactic structure
or can span across syntactically related entities.
More about the features (or mini-transformations)
is explained in section 3. The transformation of
a source syntactic structure is scored by taking a
weighted sum of its features 2. Let ? represent
1Note that target structure contains only the target fac-
tors. An accurate and deterministic morphological generator
combines these factors to produce the target word form.
2The features can be either binary-values or real-valued
the transformation of source syntactic structure s,
the score of transformation is computed as repre-
sented in Equation 1.
score(? |s) =
?
i
wi ? fi(?, s) (1)
In Equation 1, f ?is are the various features of
transformation and w?is are the weights of the fea-
tures. The strength of our approach lies in the flex-
ibility it offers in incorporating linguistic features
that are useful in the task of machine translation.
These features are also known as prediction fea-
tures as they map from source language informa-
tion to information in the target language that is
being predicted.
During decoding a source sentence, the goal
is to choose a transformation that has the high-
est score. The source syntactic structure is tra-
versed in a bottom-up fashion and the target syn-
tactic structure is simultaneously built. We used
a bottom-up traversal while decoding because it
builds a contiguous sequence of nodes for the sub-
trees during traversal enabling the application of a
wide variety of language models.
In the training phase, the task is to learn the
weights of features. We use an online large-
margin training algorithm, MIRA (Crammer et
al., 2005), for learning the weights. The weights
are locally updated at every source node during
the bottom-up traversal of the source structure.
For training the translation model, automatically
obtained word-aligned parallel corpus is used. We
used GIZA++ (Och and Ney, 2003) along with the
growing heuristics to word-align the training cor-
pus.
The basic factors of the word used in our exper-
iments are root, part-of-speech, gender, number
and person. In Hindi, common nouns and verbs
have gender information whereas, English doesn?t
contain that information. Apart from the basic
factors, we also consider the role information pro-
vided by labelled dependency parsers. For com-
puting the dependency tree on the source side, We
used stanford parser (Klein and Manning, 2003)
in the experiments presented in this chapter3.
3Stanford parser gives both the phrase-structure tree as
well as dependency relations for a sentence.
67
root=mila,   tense=PAST
gnp=m3sg
root=se
gnp=x3sgroot=raamgnp=m1sg
root=shyaam
gnp=m1sg
root=pay,   tense=PAST
gnp=x3sg,  role=X 
paid/VBD
root=Ram,  gnp=x1sg
Ram/NNP
role=subj
visit/NN
root=visit,  gnp=x3sg
role=obj role=vmodroot=to,     gnp=x3sg
to/TO
root=Shyam,  gnp=x1sg
role=pmod
Shyam/NNP
root=a,    gnp=x3sg
role=nmod
a/DT
Figure 1: Transformation from source structure to target language
The function words such as prepositions and
auxiliary verbs largely express the grammatical
roles/functions of the content words in the sen-
tence. In fact, in many agglutinative languages,
these words are commonly attached to the con-
tent word to form one word form. In this pa-
per, we also conduct experiments where we begin
by grouping the function words with their corre-
sponding function words. These groups of words
are called local-word groups. In these cases, the
function words are considered as factors of the
content words. Section 2 explains more about the
local word groups in English and Hindi.
2 Local Word Groups
Local word groups (LWGs) (Bharati et al, 1998;
Vaidya et al, 2009) consist of a content word and
its associated function words. Local word group-
ing reduces a sentence to a sequence of content
words with the case-markers and tense-markers
acting as their factors. For example, consider
an English sentence ?People of these island have
adopted Hindi as a means of communication?.
?have adopted? is a LWG with root ?adopt? and
tense markers being ?have ed?. Another example
for the LWG will be ?of communication? where
?communication? is the root, and ?of? is the case-
marker. It is to be noted that Local word grouping
is different from chunking, where more than one
content word can be part of a chunk. We obtain lo-
cal word groups in English by processing the out-
put of the stanford parser. In Hindi, the function
words always appear immediately after the con-
tent word4, and it requires simple pattern
matching to obtain the LWGs. The rules ap-
plied are, (1) VM (RB|VAUX)+, and (2) N.* IN.
3 Features
There are three types of transformation features
explored by us, (1) Local Features, (2) Syntactic
Features and, (3) Contextual Features. In this sec-
tion, we describe each of these categories of fea-
tures representing different aspects of transforma-
tion with examples.
3.1 Local Features
The local features capture aspects of local trans-
formation of an atomic treelet in the source
structure to an atomic treelet in the target lan-
guage. Atomic treelet is a semantically non-
decomposible group of one or more nodes in the
syntactic structure. It usually contains only one
node, except for the case of multi-word expres-
sions (MWEs). Figure 2 presents the examples of
local transformation.
Some of the local features used by us in our ex-
periments are (1) dice coefficient, (2) dice coeffi-
cient of roots, (3) dice coefficient of null transla-
tions, (4) treelet translation probability, (5) gnp-
gnp pair, (5) preposition-postposition pair, (6)
tense-tense pair, (7) part-of-speech fertility etc.
Dice coefficients and treelet translation probabil-
ities are measures that express the statistical co-
occurrence of the atomic treelets.
4case-markers are called postpositions
68
root=Ram,  gnp=x1sg
Ram/NNP
role=subj
root=pay,   tense=PASTgnp=x3sg,  role=X 
paid/VBD
visit/NN
root=visit,  gnp=x3sg
role=obj
root=mila,   tense=PASTgnp=m3sgroot=raamgnp=m1sg
Figure 2: Local transformations
3.2 Syntactic Features
The syntactic features are used to model the differ-
ence in the word orders of the two languages. At
every node of the source syntactic structure, these
features define the changes in the relative order
of children during the process of transformation.
They heavily use source information such as part-
of-speech tags and syntactic roles of the source
nodes. One of the features used is reorderPostags.
This feature captures the change in relative po-
sitions of children with respect to their parents
during the tree transformation. An example fea-
ture for the transformation given in Figure 1 is
shown in Figure 3.
  IN   NNP
   
   VB    
  NNP
     VB
 TO
Figure 3: Syntactic feature - reorder postags
The feature reorderPostags is in the form of a
complete transfer rule. To handle cases, where the
left-hand side of ?reorderPostags? does not match
the syntactic structure of the source tree, the sim-
pler feature functions are used to qualify various
reorderings. Instead of using POS tags, feature
functions can be defined that use syntactic roles.
Apart from the above feature functions, we can
also have features that compute the score of a par-
ticular order of children using syntactic language
models (Gali and Venkatapathy, 2009; Guo et al,
2008). Different features can be defined that use
different levels of information pertaining to the
atomic treelet and its children.
3.3 Contextual Features
Contextual features model the inter-dependence
of factors of nodes connected by dependency arcs.
These features are used to enable access to global
information for prediction of target nodes (words
and its factors).
One of the features diceCoeffParent, relates the
parent of a source node to the corresponding target
node (see figure 4.
x1
x2
x3 x4
y
dice
Figure 4: Use of Contextual (parent) information
of x2 for generation of y
The use of this feature is expected to address of
the limitations of using ?atomic treelets? as the ba-
sic units in contrast to phrase based systems which
consider arbitrary sequences of words as units to
encode the local contextual information. In my
case, We relate the target treelet with the contex-
tual information of the source treelet using feature
functions rather than using larger units. Similar
features are used to connect the context of a source
node to the target node.
Various feature functions are defined to han-
dle interaction between the factors of syntacti-
cally related treelets. The gender-number-person
agreement is a factor that is dependent of gender-
number-person factors of the syntactically related
treelets in Hindi. The rules being learnt here
are simple. However, more complex interac-
tions can also be handled though features such as
prep Tense where, the case-marker in the target is
linked to the tense of parent verb.
4 Decoding
The goal is to compute the most probable target
sentence given a source sentence. First, the source
sentence is analyzed using a morphological ana-
lyzer5, local word grouper (see section 2) and a
dependency parser. Given the source structure,
the task of the decoding algorithm is to choose the
transformation that has the maximum score.
5http://www.cis.upenn.edu/?xtag/
69
The dependency tree of the source language
sentence is traversed in a bottom-up fashion for
building the target language structure. At every
source node during the traversal, the local trans-
formation is first computed. Then, the relative or-
der of its children is then computed using the syn-
tactic features. This results in a target structure
associated with the subtree rooted at the particular
node. The target structure associated with the root
node of the source structure is the result of the best
transformation of the entire source structure.
Hence, the task of computing the best transfor-
mation of the entire source structure is factorized
into the tasks of computing the best transforma-
tions of the source treelets. The equation for com-
puting the score of a transformation, Equation 1,
can be modified as Equation 2 given below.
score(? |s) =
?
r
|r| ?
?
i
wi ? fi(?r, r) (2)
where, ?j is the local transformation of the
source treelet r. The best transformation ?? of
source sentence s is,
?? = argmax? score(? |s) (3)
5 Training Algorithm
The goal of the training algorithm is to learn the
feature weights from the word aligned corpus. For
word-alignment, we used the IBM Model 5 imple-
mented in GIZA++ along with the growing heuris-
tics (Koehn et al, 2003). The gold atomic treelets
in the source and their transformation is obtained
by mapping the source node to the target using the
word-alignment information. This information is
stored in the form of transformation tables that is
used for the prediction of target atomic treelets,
prepositions and other factors. The transformation
tables are pruned in order to limit the search and
eliminate redundant information. For each source
element, only the top few entries are retained in
the table. This limit ranges from 3 to 20.
We used an online-large margin algorithm,
MIRA (McDonald and Pereira, 2006; Crammer
et al, 2005), for updating the weights. During
parameter optimization, it is sometimes impossi-
ble to achieve the gold transformation for a node
because the pruned transformation tables may not
lead to the target gold prediction for the source
node. In such cases where the gold transforma-
tion is unreachable, the weights are not updated
at all for the source node as it might cause erro-
neous weight updates. We conducted our exper-
iments by considering both the cases, (1) Identi-
fying source nodes with unreachable transforma-
tions, and (2) Updating weights for all the source
nodes (till a maximum iteration limit). The num-
ber of iterations on the entire corpus can also be
fixed. Typically, two iterations have been found to
be sufficient to train the model.
The dependency tree is traversed in a bottom-up
fashion and the weights are updated at each source
node.
6 Experiments and Results
The important aspects of the translation model
proposed in this paper have been implemented.
Some of the components that handle word in-
sertions and non-projective transformations have
not yet been implemented in the decoder, and
should be considered beyond the scope of this
paper. The focus of this work has been to
build a working syntax based statistical machine
translation system, which can act as a plat-
form for further experiments on similar lines.
The system would be available for download
at http://shakti.iiit.ac.in/?sriram/vaanee.html. To
evaluate this experimental system, a restricted
set of experiments are conducted. The experi-
ments are conducted on the English-Hindi lan-
guage pair using a corpus in tourism domain con-
taining 11300 sentence pairs6.
6.1 Training
6.1.1 Configuration
For training, we used DIT-TOURISM-ALIGN-
TRAIN dataset which is the word-aligned dataset
of 11300 sentence pairs. The word-alignment is
done using GIZA++ (Och and Ney, 2003) toolkit
and then growing heuristics are applied. For
our experiments, we use two growing heuristics,
GROW-DIAG-FINAL-AND and GROW-DIAG-
FINAL as they cover most number of words in
both the sides of the parallel corpora.
6DIT-TOURISM corpus
70
Number of Training Sentences 500
Iterations on Corpus 1-2
Parameter optimization algorithm MIRA
Beam Size 1-20
Maximum update attempts at source node 1-4
Unreachable updates False
Size of transformation tables 3
Table 1: Training Configuration
The training of the model can be performed un-
der different configurations. The configurations
that we used for the training experiments are given
in Table 6.1.1.
6.2 Results
For the complete training, the number of sen-
tences that should be used for the best perfor-
mance of the decoder should be the complete set.
In the paper, we have conducted experiments by
considering 500 training sentences to observe the
best training configuration.
At a source node, the weight vector is itera-
tively updated till the system predicts the gold
transformation. We conducted experiments by fix-
ing the maximum number of update attempts. A
source node, where the gold transformation is not
achieved even after the maximum updates limit,
the update at this source node is termed a update
failure. The source nodes, where the gold trans-
formation is achieved even without making any
updates is known as the correct prediction.
At some of the source nodes, it is not possible
to arrive at the gold target transformation because
of limited size of the training corpus. At such
nodes, we have avoided doing any weight update.
As the desired transformation is unachievable, any
attempt to update the weight vector would cause
noisy weight updates.
We observe various parameters to check the ef-
fectiveness of the training configuration. One of
the parameters (which we refer to as ?updateHits?)
computes the number of successful updates (S)
performed at the source nodes in contrast to num-
ber of failed updates (F ). Successful updates re-
sult in the prediction of the transformation that is
same as the reference transformation. A failed up-
date doesn?t result in the achievement of the cor-
rect prediction even after the maximum iteration
limit (see section 6.1.1) is reached. At some of the
source nodes, the reference transformations are
unreachable (U ). The goal is to choose the con-
figuration that has least number of average failed
updates (F ) because it implies that the model has
been learnt effectively.
UpdateHit
K m P S F U
1. 1 4 1680 2692 84 4081
2. 5 4 1595 2786 75 4081
3. 10 4 1608 2799 49 4081
4. 20 4 1610 2799 47 4081
Table 2: Training Statistics - Effect of Beam Size
From Table 2, we can see that the bigger beam
size leads to a better training of the model. The
beam size was varied between 1 and 20, and the
number of update failures (F ) was observed to be
least at K=20.
UpdateHit
K m P S F U
1. 20 1 1574 2724 158 4081
2. 20 2 1598 2767 91 4081
3. 20 4 1610 2799 47 4081
Table 3: Training Statistics - Effect of maximum
update attempts
In Table 3, we can see that an higher limit on
the maximum number of update attempts results
in less number of update attempts as expected. A
much higher value of m is not preferable because
the training updates makes noisy updates in case
of difficult nodes i.e., the nodes where target trans-
formation is reachable in theory, but is unreach-
able given the set of features.
UpdateHit
K i P S F U
1. 1 1 1680 2692 84 4081
2. 1 2 1679 2694 83 4081
Table 4: Training Statistics - Effect of number of
iterations
Now, we examine the effect of number of it-
71
erations on the quality of the model. In table 4,
we can observe that the number of iterations on
the data has no effect on the quality of the model.
This implies, that the model is adequately learnt
after one pass through the data. This is possible
because of the multiple number of update attempts
allowed at every node. Hence, the weights are up-
dated at a node till the model prediction is consis-
tent with the gold transformation.
Based on the above observations, we consider
the configuration 4 in Table 2 for the decoding ex-
periments.
Now, we present some of the top features
weights leant by the best configuration. The
weights convey that important properties of trans-
formation are being learnt well. Table 5 presents
the weights of the features ?diceRoot?, ?dice-
RootChildren? and ?diceRootParent?.
Feature Weight
dice 75.67
diceChildren 540.31
diceParent 595.94
treelet translation probability (ttp) 1 0.77
treelet translation probability (ttp) 2 389.62
Table 5: Weights of dice coefficient based features
We see that the dice coefficient based local and
contextual features have a positive impact on the
selection of correct transformations. A feature
that uses a syntactic language model to compute
the perplexity per word has a negative weight of
-1.115.
Table 6 presents the top-5 entries of contex-
tual features that describe the translation of source
argument ?nsubj? using contextual information
(?tense? of its parent).
Feature Weight
roleTenseVib:nsubj+NULL NULL 44.194196513246
roleTenseVib:nsubj+has VBN ne 14.4541356715382
roleTenseVib:nsubj+VBD ne 10.9241093097953
roleTenseVib:nsubj+VBP meM 6.14149937079584
roleTenseVib:nsubj+VBP NULL 4.76795730621754
Table 6: Top weights of a contextual feature :
preposition+Tense-postposition
Table 7 presents the top-10 ordering relative po-
sition feature where the head word is a verb. In
this feature, the relative position (left or right) of
the head and the child is captured. For example, a
feature ?relPos:amod-NN?, if active, conveys that
an argument with the role ?amod? is at the left of
a head word with POS tag ?NN?.
Feature Weight
relPos:amod-NN 6.70
relPos:NN-appos 1.62
relPos:lrb-NN 1.62
Table 7: Top weights of relPos feature
6.3 Decoding
We computed the translation accuracies using two
metrics, (1) BLEU score (Papineni et al, 2002),
and (2) Lexical Accuracy (or F-Score) on a test
set of 30 sentences. We compared the accuracy
of the experimental system (Vaanee) presented in
this paper, with Moses (state-of-the-art translation
system) and Shakti (rule-based translation system
7) under similar conditions (with using a develop-
ment set to tune the models). The rule-based sys-
tem considered is a general domain system tuned
to the tourism domain. The best BLEU score for
Moses on the test set is 0.118, and the best lexi-
cal accuracy is 0.512. The best BLEU score for
Shakti is 0.054, and the best lexical accuracy is
0.369.
In comparison, the best BLEU score of Vaanee
is 0.067, while the best lexical accuracy is 0.445.
As observed, the decoding results of the experi-
mental system mentioned here are not yet compa-
rable to the state-of-art. The main reasons for the
low translation accuracies are,
1. Poor Quality of the dataset
The dataset currently available for English-
Hindi language pair is noisy. This is an
extremely large limiting factor for a model
which uses rich linguistic information within
the statistical framework.
2. Low Parser accuracy
7http://shakti.iiit.ac.in/
72
The parser accuracy on the English-Hindi
dataset is low, the reasons being, (1) Noise,
(2) Length of sentences, and (3) Wide scope
of the tourism domain.
3. Word insertions not implemented yet
4. Non-projectivity not yet handled
5. BLEU is not an appropriate metric
BLEU is not an appropriate metric (Anan-
thakrishnan et al, ) for measuring the trans-
lation accuracy into Indian languages.
6. Model is context free as far as targets words
are concerned. Selection depends on chil-
dren but not parents and siblings
This point concerns the decoding algorithm.
The current algorithm is greedy while chos-
ing the best translation at every source node.
It first explores the K-best local transforma-
tions at a source node. It then makes a greedy
selection of the predicted subtree based on
it?s overall score after considering the predic-
tions at the child nodes, and the relative posi-
tion of the local transformation with respect
the predictions at the child nodes.
The problem in this approach is that, an er-
ror once made at a lower level of the tree
is propogated to the top, causing more mis-
takes. A computationally reasonable solution
to this problem is to maintain a K-best list
of predicted subtrees corresponding to every
source node. This allows rectification of a
mistake made at any stage.
The system, however, performs better than the
rule based system. As observed earlier, the right
type of information is being learnt by the model,
and the approach looks promising. The limitations
expressed here shall be addressed in the future.
7 Conclusion
In this work, we presented a syntax based de-
pendency model to effectively handle problems in
translation from English to Indian languages such
as, (1) Large word order variation, and (2) Ac-
curate generation of word-forms in the target lan-
guage by predicted the word and its factors. The
model that we have proposed, has the flexibility of
adding rich linguistic features.
An experimental version of the system has been
implemented, which is available for download at
http://shakti.iiit.ac.in/?sriram/vaanee.html. This
can facilitate as a platform for future research in
syntax based statistical machine translation from
English to Indian languages. We also plan to per-
form experiments using this system between Eu-
ropean languages in future.
The performance of the implemented transla-
tion system, is not yet comparable to the state-
of-art results primarily for two reasons, (1) Poor
quality of available data, because of which our
model which uses rich linguistic information
doesn?t perform as expected, and (2) Components
for word insertion and non-projectivity handling
are yet to be implemented in this version of the
system.
References
Ananthakrishnan, R, B Pushpak, M Sasikumar, and
Ritesh Shah. Some issues in automatic evaluation
of english-hindi mt: more blues for bleu. ICON-
2007.
Ananthakrishnan, R., Jayprasad Hegde, Pushpak Bhat-
tacharyya, and M. Sasikumar. 2008. Simple syntac-
tic and morphological processing can help english-
hindi statistical machine translation. In Proceedings
of IJCNLP-2008. IJCNLP.
Bharati, Akshar, Vineet Chaitanya, Amba P Kulkarni,
and Rajeev Sangal. 1997. Anusaaraka: Machine
translation in stages. A Quarterly in Artificial Intel-
ligence, NCST, Bombay (renamed as CDAC, Mum-
bai).
Bharati, Akshar, Medhavi Bhatia, Vineet Chaitanya,
and Rajeev Sangal. 1998. Paninian grammar
framework applied to english. South Asian Lan-
guage Review, (3).
Bharati, Akshar, Rajeev Sangal, Dipti M Sharma, and
Amba P Kulkarni. 2002. Machine translation activ-
ities in india: A survey. In Proceedings of workshop
on survey on Research and Development of Machine
Translation in Asian Countries.
Chiang, David. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?05), pages
263?270, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
73
Crammer, K., R. McDonald, and F. Pereira. 2005.
Scalable large-margin online learning for structured
classification. Technical report, University of Penn-
sylvania.
Gali, Karthik and Sriram Venkatapathy. 2009. Sen-
tence realisation from bag of words with depen-
dency constraints. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, Companion Volume:
Student Research Workshop and Doctoral Consor-
tium, pages 19?24, Boulder, Colorado, June. Asso-
ciation for Computational Linguistics.
Guo, Yuqing, Josef van Genabith, and Haifeng Wang.
2008. Dependency-based n-gram models for gen-
eral purpose sentence realisation. In Proceedings
of the 22nd International Conference on Compu-
tational Linguistics (Coling 2008), pages 297?304,
Manchester, UK, August. Coling 2008 Organizing
Committee.
Ittycheriah, Abraham and Salim Roukos. 2007. Di-
rect translation model 2. In Human Language Tech-
nologies 2007: The Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics; Proceedings of the Main Conference,
pages 57?64, Rochester, New York, April. Associa-
tion for Computational Linguistics.
Klein, Dan and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics, pages 423?430, Sapporo,
Japan, July. Association for Computational Linguis-
tics.
Koehn, Philipp and Hieu Hoang. 2007. Factored
translation models. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 868?876.
Koehn, P., F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of the Hu-
man Language Technology Conference 2003 (HLT-
NAACL 2003), Edmonton, Canada, May.
Liang, P., A. Bouchard-Cote, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to
machine translation. In International Conference
on Computational Linguistics and Association for
Computational Linguistics (COLING/ACL).
McDonald, R. and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
EACL.
Och, F.J. and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
Papineni, Kishore, Salim Roukos, Todd Ward, and W.J.
Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In Proceedings of
40th Annual Meeting of the Association of Compu-
tational Linguistics, pages 313?318, Philadelphia,
PA, July.
Quirk, Chris, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In Proceedings of the 43rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL?05), pages 271?279, Ann
Arbor, Michigan, June. Association for Computa-
tional Linguistics.
Ramanathan, Ananthakrishnan, Hansraj Choudhary,
Avishek Ghosh, and Pushpak Bhattacharyya. 2009.
Case markers and morphology: Addressing the crux
of the fluency problem in english-hindi smt. In Pro-
ceedings of ACL-IJCNLP 2009. ACL-IJCNLP.
Shen, Libin, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of lin-
guistic and contextual information for statistical ma-
chine translation. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 72?80, Singapore, August. Asso-
ciation for Computational Linguistics.
Vaidya, Ashwini, Samar Husain, Prashanth Reddy, and
Dipti M Sharma. 2009. A karaka based annotation
scheme for english. In Proceedings of CICLing ,
2009.
Wu, Dekai. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3):377?404.
Yamada, Kenji and Kevin Knight. 2002. A decoder
for syntax-based statistical mt. In Proceedings of
40th Annual Meeting of the Association for Compu-
tational Linguistics, pages 303?310, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
74
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 59?62,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Using entity features to classify implicit discourse relations
Annie Louis, Aravind Joshi, Rashmi Prasad, Ani Nenkova
University of Pennsylvania
Philadelphia, PA 19104, USA
{lannie,joshi,rjprasad,nenkova}@seas.upenn.edu
Abstract
We report results on predicting the sense
of implicit discourse relations between ad-
jacent sentences in text. Our investigation
concentrates on the association between
discourse relations and properties of the
referring expressions that appear in the re-
lated sentences. The properties of inter-
est include coreference information, gram-
matical role, information status and syn-
tactic form of referring expressions. Pre-
dicting the sense of implicit discourse re-
lations based on these features is consid-
erably better than a random baseline and
several of the most discriminative features
conform with linguistic intuitions. How-
ever, these features do not perform as well
as lexical features traditionally used for
sense prediction.
1 Introduction
Coherent text is described in terms of discourse re-
lations such as ?cause? and ?contrast? between its
constituent clauses. It is also characterized by en-
tity coherence, where the connectedness of the text
is created by virtue of the mentioned entities and
the properties of referring expressions. We aim to
investigate the association between discourse rela-
tions and the way in which references to entities
are realized. In our work, we employ features re-
lated to entity realization to automatically identify
discourse relations in text.
We focus on implicit relations that hold be-
tween adjacent sentences in the absence of dis-
course connectives such as ?because? or ?but?.
Previous studies on this task have zeroed in on
lexical indicators of relation sense: dependencies
between words (Marcu and Echihabi, 2001; Blair-
Goldensohn et al, 2007) and the semantic orien-
tation of words (Pitler et al, 2009), or on general
syntactic regularities (Lin et al, 2009).
The role of entities has also been hypothesized
as important for this task and entity-related fea-
tures have been used alongside others (Corston-
Oliver, 1998; Sporleder and Lascarides, 2008).
Corpus studies and reading time experiments per-
formed by Wolf and Gibson (2006) have in fact
demonstrated that the type of discourse relation
linking two clauses influences the resolution of
pronouns in them. However, the predictive power
of entity-related features has not been studied in-
dependently of other factors. Further motivation
for studying this type of features comes from new
corpus evidence (Prasad et al, 2008), that about a
quarter of all adjacent sentences are linked purely
by entity coherence, solely because they talk about
the same entity. Entity-related features would be
expected to better separate out such relations.
We present the first comprehensive study of the
connection between entity features and discourse
relations. We show that there are notable differ-
ences in properties of referring expressions across
the different relations. Sense prediction can be
done with results better than random baseline us-
ing only entity realization information. Their per-
formance, however, is lower than a knowledge-
poor approach using only the words in the sen-
tences as features. The addition of entity features
to these basic word features is also not beneficial.
2 Data
We use 590 Wall Street Journal (WSJ) articles
with overlapping annotations for discourse, coref-
erence and syntax from three corpora.
The Penn Discourse Treebank (PDTB) (Prasad
et al, 2008) is the largest available resource of
discourse relation annotations. In the PDTB, im-
plicit relations are annotated between adjacent
sentences in the same paragraph. They are as-
signed senses from a hierarchy containing four top
level categories?Comparison, Contingency, Tem-
poral and Expansion.
59
An example ?Contingency? relation is shown
below. Here, the second sentence provides the
cause for the belief expressed in the first.
Ex 1. These rate indications aren?t directly comparable.
Lending practices vary widely by location.
Adjacent sentences can also become related
solely by talking about a common entity without
any of the above discourse relation links between
their propositions. Such pairs are annotated as En-
tity Relations (EntRels) in the PDTB, for example:
Ex 2. Rolls-Royce Motor Cars Inc. said it expects its U.S
sales to remain steady at about 1,200 cars in 1990. The luxury
auto maker last year sold 1,214 cars in the U.S.
We use the coreference annotations from the
Ontonotes corpus (version 2.9) (Hovy et al, 2006)
to compute our gold-standard entity features. The
WSJ portion of this corpus contains 590 articles.
Here, nominalizations and temporal expressions
are also annotated for coreference but we use the
links between noun phrases only. We expect these
features computed on the gold-standard annota-
tions to represent an upper bound on the perfor-
mance of entity features.
Finally, the Penn Treebank corpus (Marcus et
al., 1994) is used to obtain gold-standard parse and
grammatical role information.
Only adjacent sentences within the same para-
graph are used in our experiments.
3 Entity-related features
We associate each referring expression in a sen-
tence with a set of attributes as described below.
In Section 3.2, we detail how we combine these
attributes to compute features for a sentence pair.
3.1 Referring expression attributes
Grammatical role. In exploratory analysis of
Comparison relations, we often observed parallel
syntactic realizations for entities in the subject po-
sition of the two sentences:
Ex 3. {Longer maturities}E1 are thought to indicate de-
clining interest rates. {Shorter maturities}E2 are considered
a sign of rising rates because portfolio managers can capture
higher rates sooner.
So, for each noun phrase, we record whether
it is the subject of a main clause (msubj), subject
of other clauses in the sentence (esubj) or a noun
phrase not in subject position (other).
Given vs. New. When an entity is first intro-
duced in the text, it is considered a new entity.
Subsequent mentions of the same entity are given
(Prince, 1992). New-given distinction could help
to identify some of the Expansion and Entity re-
lations. When a sentence elaborates on another, it
might contain a greater number of new entities.
We use the Ontonotes coreference annotations
to mark the information status for entities. For
an entity, if an antecedent is found in the previ-
ous sentences, it is marked as given, otherwise it
is a new entity.
Syntactic realization. In Entity relations, the sec-
ond sentence provides more information about a
specific entity in the first and a definite description
for this second mention seems likely. Also, given
the importance of named entities in news, entities
with proper names might be the ones frequently
described using Entity relations.
We use the part of speech (POS) tag associated
with the head of the noun phrase to assign one of
the following categories: pronoun, nominal, name
or expletive. When the head does not belong to
the above classes, we simply record its POS tag.
We also mark whether the noun phrase is a definite
description using the presence of the article ?the?.
Modification. We expected modification proper-
ties to be most useful for predicting Comparison
relations. Also, named or new entities in Entity
relations are very likely to have post modification.
We record whether there are premodifiers or
postmodifiers in a given referring expression. In
the absence of pre- and postmodifiers, we indicate
bare head realization.
Topicalization. Preposed prepositional or ad-
verbial phrases before the subject of a sentence
indicate the topic under which the sentence is
framed. We observed that this property is frequent
in Comparison and Temporal relations. An exam-
ple Comparison is shown below.
Ex 4. {Under British rules}T1, Blue Arrow was able to
write off at once $1.15 billion in goodwill arising from the
purchase. {As a US-based company}T2, Blue Arrow would
have to amortize the good will over as many as 40 years, cre-
ating a continuing drag on reported earnings.
When the left sibling of a referring expression is
a topicalized phrase, we mark the topic attribute.
Number. Using the POS tag of the head word, we
note whether the entity is singular or plural.
3.2 Features for classification
Next, for each sentence pair, we associate two sets
of features using the attributes described above.
60
Let S1 and S2 denote the two adjacent sentences
in a relation, where S1 occurs first in the text.
Sentence level. These features characterize S1
and S2 individually. For each sentence, we add a
feature for each of the attributes described above.
The value of the feature is the number of times that
attribute is observed in the sentence; i.e., the fea-
ture S1given would have a value of 3 if there are 3
given entities in the first sentence.
Sentence pair. These features capture the interac-
tions between the entities present in S1 and S2.
Firstly, for each pair of entities (a, b), such that
a appears in S1 and b appears in S2, we assign
one of the following classes: (i) SAME: a and b
are coreferent, (ii) RELATED: their head words are
identical, (iii) DIFFERENT: neither coreferent nor
related. The RELATED category was introduced to
capture the parallelism often present in Compari-
son relations. Even though the entities themselves
are not coreferent, they share the same head word
(i.e. longer maturities and shorter maturities).
For features, we use the combination of the
class ((i), (ii) or (iii)) with the cross product of
the attributes for a and b. For example if a has
attributes {msubj, noun, ...} and b has attributes
{esubj, defdesc, ...} and a and b are corefer-
ent, we would increment the count for features?
{sameS1msubjS2esubj, sameS1msubjS2defdesc,
sameS1nounS2esubj, sameS1nounS2defdesc ...}.
Our total set of features observed for instances
in the training data is about 2000.
We experimented with two variants of fea-
tures: one using coreference annotations from
the Ontonotes corpus (gold-standard) and an-
other based on approximate coreference informa-
tion where entities with identical head words are
marked as coreferent.
4 Experimental setup
We define five classification tasks which disam-
biguate if a specific PDTB relation holds between
adjacent sentences. In each task, we classify the
relation of interest (positive) versus a category
with a naturally occurring distribution of all of the
other relations (negative).
Sentence pairs from sections 0 to 22 of WSJ are
used as training data and we test on sections 23
and 24. Given the skewed distribution of positive
and negative examples for each task, we randomly
downsample the negative instances in the training
set to be equal to the positive examples. The sizes
of training sets for the tasks are
Expansion vs other (4716)
Contingency vs other (2466)
Comparison vs other (1138)
Temporal vs other (474)
EntRel vs other (2378)
Half of these examples are positive and the
other negative in each case.
The test set contains 1002 sentence pairs:
Comp. (133), Cont. (230), Temp. (34), Expn.
(369), EntRel (229), NoRel1 (7). We do not down-
sample our test set. Instead, we evaluate our pre-
dictions on the natural distribution present in the
data to get a realistic estimate of performance.
We train a linear SVM classifier (LIBLIN-
EAR2) for each task.3 The optimum regulariza-
tion parameter was chosen using cross validation
on the training data.
5 Results
5.1 Feature analysis
We ranked the features (based on gold-standard
coreference information) in the training sets by
their information gain. We then checked which
attributes are common among the top five features
for different classification tasks.
As we had expected, the topicalization attribute
and RELATED entities frequently appear among
the top features for Comparison.
Features with the name attribute were highly
predictive of Entity relations as hypothesized.
However, while we had expected Entity relations
to have a high rate of coreference, we found coref-
erent mentions to be very indicative of Temporal
relations: all the top features involve the SAME at-
tribute. A post-analysis showed that close to 70%
of Temporal relations involve coreferent entities
compared to around 50% for the other classes.
The number of pronouns in the second sentence
was most characteristic of the Contingency rela-
tion. In the training set for Contingency task,
about 45% of sentences pairs belonging to Contin-
gency relation have a pronoun in the second sen-
tence. This is considerably larger than 32%, which
is the percentage of sentence pairs in the negative
examples with a pronoun in second sentence.
1PDTB relation for sentence pair when both entity and
discourse relations are absent, very rare about 1% of our data.
2http://www.csie.ntu.edu.tw/?cjlin/liblinear/
3SVMs with linear kernel gave the best performance. We
also experimented with SVMs with radial basis kernel, Naive
Bayes and MaxEnt classifiers.
61
5.2 Performance on sense prediction
The classification results (fscores) are shown in
Table 1. The random baseline (Base.) represents
the results if we predicted positive and negative re-
lations according to their proportion in the test set.
Entity features based on both gold-standard
(EntGS) and approximate coreference (EntApp)
outperform the random baseline for all the tasks.
The drop in performance without gold-standard
coreference information is strongly noticable only
for Expansion relations.
The best improvement from the baseline is seen
for predicting Contingency and Entity relations,
with around 15% absolute improvement in fscore
with both EntGS and EntApp features. The im-
provements for Comparisons and Expansions are
around 11% in the approximate case. Temporal
relations benefit least from these features. These
relations are rare, comprising 3% of the test set
and harder to isolate from other relations. Overall,
our results indicate that discourse relations and en-
tity realization have a strong association.
5.3 Comparison with lexical features
In the context of using entity features for sense
prediction, one would also like to test how these
linguistically rich features compare with simpler
knowledge-lean approaches used in prior work.
Specifically, we compare with word pairs, a
simple yet powerful set of features introduced by
Marcu and Echihabi (2001). These features are the
cross product of words in the first sentence with
those in the second.
We trained classifiers on the word pairs from the
sentences in the PDTB training sets. In Table 1,
we report the performance of word pairs (WP) as
well as their combination with gold-standard en-
tity features (WP+EntGS). Word pairs turn out as
stronger predictors for all discourse relations com-
pared to our entity features (except for Expansion
prediction with EntGS features). Further, no ben-
efits over word pair results are obtained by com-
bining entity realization information.
6 Conclusion
In this work, we used a task-based approach to
show that the two components of coherence?
discourse relations and entities?are related and
interact with each other. Coreference, givenness,
syntactic form and grammatical role of entities can
predict the implicit discourse relation between ad-
Task Base. EntGS EntApp WP WP+EntGS
Comp vs Oth. 13.27 24.18 24.14 27.30 26.19
Cont vs Oth. 22.95 37.57 38.16 38.17 38.99
Temp vs Oth. 3.39 7.58 5.61 11.09 10.04
Expn vs Oth. 36.82 52.42 47.82 48.54 49.06
Ent vs Oth. 22.85 38.03 36.73 38.48 38.14
Table 1: Fscore results
jacent sentences with results better than random
baseline. However, with respect to developing au-
tomatic discourse parsers, these entity features are
less likely to be useful. They do not outperform
or complement simpler lexical features. It would
be interesting to explore whether other aspects of
entity reference might be useful for this task, such
as bridging anaphora. But currently, annotations
and tools for these phenomena are not available.
References
S. Blair-Goldensohn, K. McKeown, and O. Rambow.
2007. Building and refining rhetorical-semantic re-
lation models. In HLT-NAACL.
S.H. Corston-Oliver. 1998. Beyond string matching
and cue phrases: Improving efficiency and coverage
in discourse analysis. In The AAAI Spring Sympo-
sium on Intelligent Text Summarization.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. Ontonotes: the 90% solution.
In NAACL-HLT.
Z. Lin, M. Kan, and H.T. Ng. 2009. Recognizing im-
plicit discourse relations in the Penn Discourse Tree-
bank. In EMNLP.
D. Marcu and A. Echihabi. 2001. An unsupervised ap-
proach to recognizing discourse relations. In ACL.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1994.
Building a large annotated corpus of english: The
penn treebank. Computational Linguistics.
E. Pitler, A. Louis, and A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in
text. In ACL-IJCNLP.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki,
L. Robaldo, A. Joshi, and B. Webber. 2008. The
penn discourse treebank 2.0. In LREC.
E. Prince. 1992. The zpg letter: subject, definiteness,
and information status. In Discourse description:
diverse analyses of a fund raising text, pages 295?
325. John Benjamins.
C. Sporleder and A. Lascarides. 2008. Using automat-
ically labelled examples to classify rhetorical rela-
tions: An assessment. Natural Language Engineer-
ing, 14:369?416.
F. Wolf and E. Gibson. 2006. Coherence in natural
language: data structures and applications. MIT
Press.
62
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 147?156,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Discourse indicators for content selection in summarization
Annie Louis, Aravind Joshi, Ani Nenkova
University of Pennsylvania
Philadelphia, PA 19104, USA
{lannie,joshi,nenkova}@seas.upenn.edu
Abstract
We present analyses aimed at eliciting
which specific aspects of discourse pro-
vide the strongest indication for text im-
portance. In the context of content selec-
tion for single document summarization of
news, we examine the benefits of both the
graph structure of text provided by dis-
course relations and the semantic sense
of these relations. We find that structure
information is the most robust indicator
of importance. Semantic sense only pro-
vides constraints on content selection but
is not indicative of important content by it-
self. However, sense features complement
structure information and lead to improved
performance. Further, both types of dis-
course information prove complementary
to non-discourse features. While our re-
sults establish the usefulness of discourse
features, we also find that lexical overlap
provides a simple and cheap alternative
to discourse for computing text structure
with comparable performance for the task
of content selection.
1 Introduction
Discourse relations such as cause, contrast or
elaboration are considered critical for text inter-
pretation, as they signal in what way parts of a text
relate to each other to form a coherent whole. For
this reason, the discourse structure of a text can be
seen as an intermediate representation, over which
an automatic summarizer can perform computa-
tions in order to identify important spans of text
to include in a summary (Ono et al, 1994; Marcu,
1998; Wolf and Gibson, 2004). In our work, we
study the content selection performance of differ-
ent types of discourse-based features.
Discourse relations interconnect units of a text
and discourse formalisms have proposed different
resulting structures for the full text, i.e. tree (Mann
and Thompson, 1988) and graph (Wolf and Gib-
son, 2005). This structure is one source of in-
formation from discourse which can be used to
compute the importance of text units. The seman-
tics of the discourse relations between sentences
could be another indicator of content importance.
For example, text units connected by ?cause? and
?contrast? relationships might be more important
content for summaries compared to those convey-
ing ?elaboration?. While previous work have fo-
cused on developing content selection methods
based upon individual frameworks (Marcu, 1998;
Wolf and Gibson, 2004; Uzda et al, 2008), little is
known about which aspects of discourse are actu-
ally correlated with content selection power.
In our work, we separate out structural and se-
mantic features and examine their usefulness. We
also investigate whether simpler intermediate rep-
resentations can be used in lieu of discourse. More
parsimonious, easy to compute representations of
text have been proposed for summarization. For
example, a text can be reduced to a set of highly
descriptive topical words, the presence of which
is used to signal importance for content selection
(Lin and Hovy, 2002; Conroy et al, 2006). Sim-
ilarly, a graph representation of the text can be
computed, in which vertices represent sentences,
and the nodes are connected when the sentences
are similar in terms of word overlap; properties of
the graph would then determine the importance of
the nodes (Erkan and Radev, 2004; Mihalcea and
Tarau, 2005) and guide content selection.
We compare the utility of discourse features for
single-document text summarization from three
frameworks: Rhetorical Structure Theory (Mann
and Thompson, 1988), Graph Bank (Wolf and
Gibson, 2005), and Penn Discourse Treebank
(PDTB) (Prasad et al, 2008). We present a de-
tailed analysis of the predictive power of different
types of discourse features for content selection
147
and compare discourse-based selection to simpler
non-discourse methods.
2 Data
We use a collection of Wall Street Journal (WSJ)
articles manually annotated for discourse infor-
mation according to three discourse frameworks.
The Rhetorical Structure Theory (RST) and Graph
Bank (GB) corpora are relatively small compared
to the Penn Discourse Treebank (PDTB) annota-
tions that cover the 1 million word WSJ part of the
Penn Treebank corpus (Marcus et al, 1994). Our
evaluation requires gold standard summaries writ-
ten by humans, so we perform our experiments on
a subset of the overlapping documents for which
we also have human summaries available.
2.1 RST corpus
RST (Mann and Thompson, 1988) proposes that
coherent text can be represented as a tree formed
by the combination of text units via discourse re-
lations. The RST corpus developed by Carlson et
al. (2001) contains discourse tree annotations for
385 WSJ articles from the Penn Treebank corpus.
The smallest annotation units in the RST corpus
are sub-sentential clauses, also called elementary
discourse units (EDUs). Adjacent EDUs combine
through rhetorical relations into larger spans such
as sentences. The larger units recursively partici-
pate in relations with others, yielding one hierar-
chical tree structure covering the entire text.
The discourse units participating in a RST re-
lation are assigned either nucleus or satellite sta-
tus; a nucleus is considered to be more central,
or important, in the text than a satellite. Rela-
tions composed of one nucleus and one satellite
are called mononuclear relations. On the other
hand, in multinuclear relations, two or more text
units participate, and all are considered equally
important. The RST corpus is annotated with 53
mononuclear and 25 multinuclear relations. Rela-
tions that convey similar meaning are grouped, re-
sulting in 16 classes of relations: Cause, Comparison,
Condition, Contrast, Attribution, Background, Elaboration,
Enablement, Evaluation, Explanation, Joint, Manner-Means,
Topic-Comment, Summary, Temporal and Topic-Change.
2.2 Graph Bank corpus
Sometimes, texts cannot be described in a tree
structure as hypothesized by the RST. For exam-
ple, crossing dependencies and nodes with multi-
ple parents appear frequently in texts and do not
allow a tree structure to be built (Lee et al, 2008).
To address this problem, general graph representa-
tion was proposed by Wolf and Gibson (2005) as
a more realistic model of discourse structure.
Graph annotations of discourse are available for
135 documents (105 from AP Newswire and 30
from the WSJ) as part of the Graph Bank cor-
pus (Wolf and Gibson, 2005). Clauses are the ba-
sic discourse segments in this annotation. These
units are represented as the nodes in a graph, and
are linked with one another through 11 differ-
ent rhetorical relations: Cause-effect, Condition, Vio-
lated expectation, Elaboration, Example, Generalization, At-
tribution, Temporal sequence, Similarity, Contrast and Same.
The edge between two nodes representing a rela-
tion is directed in the case of asymmetric relations
such as Cause and Condition and undirected for
symmetric relations like Similarity and Contrast.
2.3 Penn Discourse Treebank
The Penn Discourse Treebank (PDTB) (Prasad et
al., 2008) is theory-neutral and does not make
any assumptions about the form of the overall dis-
course structure of text. Instead, this approach fo-
cuses on local and lexically-triggered discourse re-
lations. Annotators identify explicit signals such
as discourse connectives: ?but?, ?because?, ?while?
and mark the text spans which they relate. The
relations between these spans are called explicit
relations. In addition, adjacent sentences in a dis-
course are also semantically related even in the ab-
sence of explicit markers. In the PDTB, these are
called implicit relations and are annotated between
adjacent sentences in the same paragraph.
For both implicit and explicit relations, senses
are assigned from a hierarchy containing four
top-level categories: Comparison (contrast, prag-
matic contrast, concession, pragmatic concession), Contin-
gency (cause, pragmatic cause, condition, pragmatic con-
dition) , Expansion (conjunction, instantiation, restate-
ment, alternative, exception, list) and Temporal (asyn-
chronous, synchronous). The top level senses are di-
vided into types and subtypes that represent more
fine grained senses?the second level senses are
listed in parentheses above.
PDTB also provides annotations for the text
spans of the two arguments (referred to Arg1 and
Arg2) involved in a relation. In explicit relations,
the argument syntactically bound to the discourse
connective is called Arg2. The other argument is
148
referred to as Arg1. For implicit relations, the ar-
gument occurring first in the text is named Arg1,
the one appearing later is called Arg2.
2.4 Human summaries
Human summaries are available for some of the
WSJ articles. These summaries are extractive: hu-
man judges identified and extracted important text
units from the source articles and used them as
such to compose the summary.
The RST corpus contains summaries for 150
documents. Two annotators selected the most im-
portant EDUs from these documents and created
summaries that contain about square root of the
number of EDUs in the source document. For
convenience, we adopt sentences as the common
unit for comparison across all frameworks. So,
we mapped the summary EDUs to the sentences
which contain them. Two variable length sum-
maries for each document were obtained in this
way. In some documents, it was not possible to
align EDUs automatically with gold standard sen-
tence boundaries given by the Penn Treebank and
these were not used in our work. We perform
our experiments on the remaining 124 document-
summary pairs. These documents consisted of
4,765 sentences in total, of which 1,152 were la-
beled as important sentences because they con-
tained EDUs selected by at least one annotator.
The Graph Bank corpus also contains human
summaries. However, only 15 are for documents
for which RST and PDTB annotations are also
available. These summaries were created by fif-
teen human annotators who ranked the sentences
in each document on a scale from 1 (low impor-
tance) to 7 (very important for a summary). For
each document, we ordered the sentences accord-
ing to the average rank from the annotators, and
created a summary of 100 words using the top
ranked sentences. The number of summary (im-
portant) sentences is 67, out of a total of 308 sen-
tences from the 15 documents.
3 Features for content selection
In this section, we describe two sets of discourse
features?structural and semantic. The structure
features are derived from RST trees and do not
involve specific relations. Rather they compute
the importance of a segment as a function of its
position in the global structure of the entire text.
On the other hand, semantic features indicate the
sense of a relation between two sentences and
do not involve structure information. We com-
pute these from the PDTB annotations. To un-
derstand the benefits of discourse information, we
also study the performance of some non-discourse
features standardly used in summarization.
3.1 Structural features: RST-based
Prior work in text summarization has developed
content selection methods using properties of the
RST tree: the nucleus-satellite distinction, notions
of salience and the level of an EDU in the tree.
In early work, Ono et al (1994) suggested
a penalty score for every EDU based on their
nucleus-satellite status. Since satellites of rela-
tions are considered less important than the corre-
sponding nuclei, spans that appear as satellites can
be assigned a lower score than the nucleus spans.
This intuition is implemented by Ono et al (1994)
as a penalty value for each EDU, defined as the
number of satellite nodes found on the path from
the root of the tree to that EDU. Figure 1 shows
the RST tree (Carlson et al, 2002) for the follow-
ing sentence which contains four EDUs.
1. [Mr. Watkins said] 2. [volume on Interprovincial?s sys-
tem is down about 2% since January] 3. [and is expected to
fall further,] 4. [making expansion unnecessary until perhaps
the mid-1990s.]
The spans of individual EDUs are represented
at the leaves of the tree. At the root of the tree, the
span covers the entire text. The path from EDU 1
to the root contains one satellite node. It is there-
fore assigned a penalty of 1. Paths to the root from
all other EDUs involve only nucleus nodes and
subsequently these EDUs do not incur any penalty.
Figure 1: RST tree for the example sentence in
Section 3.1.
Marcu (1998) proposed another method to uti-
lize the nucleus-satellite distinction, rewarding nu-
cleus status instead of penalizing satellite. He put
forward the idea of a promotion set, consisting of
149
salient/important units of a text span. The nu-
cleus is the more salient unit in the full span of
a mononuclear relation. In a multinuclear relation,
all the nuclei are salient units of the larger span.
For example, in Figure 1, EDUs 2 and 3 partici-
pate in a multinuclear (List) relation. As a result,
both EDUs 2 and 3 appear in the promotion set of
their combined span. The salient units (promotion
set) of each text span are shown above the horizon-
tal line which represents the span. At the leaves,
salient units are the EDUs themselves.
For the purpose of identifying important con-
tent, units in the promotion sets of nodes close to
the root are hypothesized to be more important
than those at lower levels. The highest promo-
tion of an EDU occurs at the node closest to the
root which contains that EDU in its promotion set.
The depth of the tree from the highest promotion
is assigned as the score for that EDU. Hence, the
closer to the root an EDU is promoted, the better
its score. Since EDUs 2, 3 and 4 are promoted all
the way up to the root of the tree, the score as-
signed to them is equal to 4, the total depth of the
tree. EDU 1 receives a depth score of 3.
However, notice that EDUs 2 and 3 are pro-
moted to the root from a greater depth than EDU
4 but all three receive the same depth score. But
an EDU promoted successively over multiple lev-
els should be more important than one which is
promoted fewer times. In order to make this dis-
tinction, a promotion score was also introduced by
Marcu (1998) which is a measure of the number
of levels over which an EDU is promoted. Now,
EDUs 2 and 3 receive a promotion score of three
while the score of EDU 4 is only two.
For our experiments, we use the nucleus-
satellite penalty, depth and promotion based scores
as features. Because all these scores depend on the
length of the document, another set of the same
features normalized by number of words in the
document are also included. The penalty/score for
a sentence is computed as the maximum of the
penalties/scores of its constituent EDUs.
3.2 Semantic features: PDTB-based
These features represent sentences purely in terms
of the relations which they participate in. For each
sentence, we use the PDTB annotations to encode
the sense of the relation expressed by the sentence
and the type of realization (explicit or implicit).
For example, the sentence below expresses a
Contingency relation.
In addition, its machines are easier to operate, so cus-
tomers require less assistance from software.
For such sentences that contain both the argu-
ments of a relation ie., expresses the relation by
itself, we set the feature ?expresses relation?. For
the above sentence, the binary feature ?expresses
Contingency relation? would be true.
Alternatively, sentences participating in multi-
sentential relations will have one of the following
features on: ?contains Arg1 of relation? or ?con-
tains Arg2 of relation?. Therefore, for the follow-
ing sentences in an Expansion relation, we record
the feature ?contains Arg1 of Expansion relation?
for sentence (1) and for sentence (2), ?contains
Arg2 of Expansion relation?.
(1) Wednesday?s dominant issue was Yasuda &Marine In-
surance, which continued to surge on rumors of speculative
buying. (2) It ended the day up 80 yen to 1880 yen.
We combine the implicit/explicit type distinc-
tion of the relations with the other features de-
scribed so far, doubling the number of features.
We also added features that use the second level
sense of a relation. So, the relevant features for
sentence (1) above would be ?contains Arg1 of
Implicit Expansion relation? as well as ?contains
Arg1 of Implicit Restatement relation? (Restate-
ment is a type of Expansion relation (Section 2.3)).
In addition, we include features measuring the
number of relations shared by a sentence (implicit,
explicit and total) and the distance between argu-
ments of explicit relations (the distance of Arg1
when the sentence contains Arg2).
3.3 Non-discourse features
We use standard non-discourse features used in
summarization: length of the sentence, whether
the sentence is paragraph initial or the first sen-
tence of a document, and its offsets from docu-
ment beginning as well as paragraph beginning
and end (Edmundson, 1969). We also include the
average, sum and product probabilities of the con-
tent words appearing in sentences (Nenkova et al,
2006) and the number of topic signature words in
the sentence (Lin and Hovy, 2000).
4 Predictive power of features
We used the human summaries from the RST cor-
pus to study which features strongly correlate with
the important sentences selected by humans. For
binary features such as ?does the sentence con-
150
tain a Contingency relation?, a chi-square test was
computed to measure the association between a
feature and sentence class (in summary or not in
summary). For real-valued features, comparison
between important and unimportant/non-summary
sentences was done using a two-sided t-test. The
significant features from our different classes are
reported in the Appendix?Tables 5, 6 and 7. A
brief summary of the results is provided below.
Significant features that have higher values for
sentences selected in a summary are:
Structural: depth score and promotion score?both normal-
ized and unnormalized.
Semantic-PDTB-level11: contains Arg1 of Explicit Expan-
sion, contains Arg1 of Implicit Contingency, contains Arg1
of Implicit Expansion, distance of other argument
Non-discourse: length, is the first sentence in the article, is
the first sentence in the paragraph, offset from paragraph end,
number of topic signature terms present, average probability
of content words, sum of probabilities of content words
Significant features that have higher values for
sentences not selected in a summary are:
Structural: Ono penalty?normalized and unnormalized.
Semantic-PDTB-level1: expresses Explicit Expansion, ex-
presses Explicit Contingency, contains Arg2 of Implicit Tem-
poral relation, contains Arg2 of Implicit Contingency, con-
tains Arg2 of Implicit Expansion, contains Arg2 of Implicit
Comparison, number of shared implicit relations, total shared
relations
Non-discourse: offset from paragraph beginning, offset
from article beginning, sentence probability based on content
words.
All the structural features prove to be strong in-
dicators for content selection. RST depth and pro-
motion scores are higher for important sentences.
Unimportant sentences have high penalties.
On the other hand, note that most of the sig-
nificant sense features are descriptive of the ma-
jority class of sentences?those not important or
not selected to appear in the summary (refer Ta-
ble 7). For example, the second arguments of
all the first level implicit PDTB relations are not
preferred in human summaries. Most of the sec-
ond level sense features also serve as indicators
for what content should not be included in a sum-
mary. Such features can be used to derive con-
straints on what content is not important, but there
are only few indicators associated with important
sentences. Overall, out of the 25 first and second
1Features based on the PDTB level 1 senses. The signif-
icant features based on the level 2 senses are reported in the
appendix.
level sense features which turned out to be signifi-
cantly related to a sentence class, only 8 are those
indicative of important content.
Another compelling observation is that highly
cognitively salient discourse relations such as
Contrast and Cause are not indicative of important
sentences. Of the features that indicate the occur-
rence of a particular relation in a sentence, only
two are significant, but they are predictive of non-
important sentences. These are ?expresses Ex-
plicit Expansion? (also subtypes Conjunction and
List) and ?expresses Explicit Contingency?.
An additional noteworthy fact is the differences
between implicit and explicit relations that hold
across sentences. For implicit relations, the tests
show a strong indication that the second arguments
of Implicit Contingency or Expansion would not
be included in a summary, their first arguments
however are often important and likely to appear
in a summary. At the same time, for explicit rela-
tions, there is no regularity for any of the relations
of which of the two arguments is more important.
All the non-discourse features turned out highly
significant (Table 6). Longer sentences, those in
the beginning of an article or its paragraphs and
sentences containing frequent content words are
preferred in human summaries.
5 Classification performance
We now test the strengths and complementary be-
havior of these features in a classification task to
predict important sentences from input texts.
5.1 Comparison of feature classes
Table 1 gives the overall accuracy, as well as pre-
cision and recall for the important/summary sen-
tences. Features classes were combined using lo-
gistic regression. The reported results are from 10-
fold cross-validation runs on sentences from the
124 WSJ articles for which human summaries are
available in the RST corpus. For the classifier us-
ing sense information from the PDTB, all the fea-
tures described in Section 3.2 were used.
The best class of features turn out to be the
structure-based ones. They outperform both non-
discourse (ND) and sense features by a large mar-
gin. F-measure for the RST-based classifier is
33.50%. The semantic type of relations, on the
other hand, gives no indication of content impor-
tance obtaining an F-score of only 9%. Non-
discourse features provide an F-score of 19%,
151
which is much better than the semantic class but
still less than structural discourse features.
The structure and semantic features are com-
plementary to each other. The performance of
the classifier is substantially improved when both
types of features are used (line 6 in Table 1). The
F-score for the combined classifier is 40%, which
amounts to 7% absolute improvement over the
structure-only classifier.
Discourse information is also complementary
to non-discourse. Adding discourse structure
or sense features to non-discourse (ND) features
leads to better classification decisions (lines 4, 5
in Table 1). Particularly notable is the improve-
ment when sense and non-discourse features are
combined?over 10% better F-score than the classi-
fier using only non-discourse features. The overall
best classifier is the combination of discourse?
structure as well as sense?and non-discourse fea-
tures. Here, recall for important sentences is 34%
and the precision of predictions is 62%.
We also evaluated the features using ROUGE
(Lin and Hovy, 2003; Lin, 2004). ROUGE com-
putes ngram overlaps between human reference
summaries and a given system summary. This
measure allows us to compare the human sum-
maries and classifier predictions at word level
rather than using full sentence matches.
To perform ROUGE evaluation, summaries for
our different classes of features were obtained as
follows. Important sentences for each document
were predicted using a logistic regression classi-
fier trained on all other documents. When the
number of sentences predicted to be important
was not sufficient to meet the required summary
length, sentences predicted with lowest confidence
to be non-important were selected. All summaries
were truncated to 100 words. Stemming was used,
and stop words were excluded from the calcula-
tion. Both human extracts were used as references.
The results from this evaluation are shown in
Table 2. They closely mirror the results obtained
using precision and recall. The sense features per-
form worse than the structural and non-discourse
features. The best set of features is the one com-
bining structure, sense and non-discourse features,
with ROUGE-1 score (unigram overlap) of 0.479.
Overall, combining types of features considerably
improves results in all cases. However, unlike
in the precision and recall evaluation, structural
and non-discourse features perform very similarly.
Features used Acc P R F
structural 78.11 63.38 22.77 33.50
semantic 75.53 44.31 5.04 9.05
non-discourse (ND) 77.25 67.48 11.02 18.95
ND + semantic 77.38 59.38 20.62 30.61
ND + structural 78.51 63.49 26.05 36.94
semantic + structural 77.94 58.39 30.47 40.04
structural + semantic + ND 78.93 61.85 34.42 44.23
Table 1: Accuracy (Acc) and Precision (P), Recall
(R) and F-score (F) of important sentences.
Features ROUGE Features ROUGE
structural + semantic + ND 0.479 ND 0.432
structural + ND 0.468 LEAD 0.411
structural + semantic 0.453 semantic 0.369
semantic + ND 0.444 TS 0.338
structural 0.433
Table 2: ROUGE-1 recall scores
Their ROUGE-1 recall scores are 0.433 and 0.432
respectively. The top ranked sentences by both
sets of features appear to contain similar content.
We also evaluated sentences chosen by two
baseline summarizers. The first, LEAD, includes
sentences from the beginning of the article up to
the word limit. This simple method is a very com-
petitive baseline for single document summariza-
tion. The second baseline ranks sentences based
on the proportion of topic signature (TS) words
contained in the sentences (Conroy et al, 2006).
This approach leads to very good results in identi-
fying important content for multi-document sum-
maries where there is more redundancy, but it is
the worst when measured by ROUGE-1 on this
single document task. Structure and non-discourse
features outperform both these baselines.
5.2 Tree vs. graph discourse structure
Wolf and Gibson (2004) showed that the Graph
Bank annotations of texts can be used for sum-
marization with results superior to that based on
RST trees. In order to derive the importance of
sentences from the graph representation, they use
the PageRank algorithm (Page et al, 1998). These
scores, similar to RST features, are based only on
the link structure; the semantic type of the relation
linking the sentences is not used. In Table 3, we
report the performance of structural features from
RST and Graph Bank on the 15 documents with
overlapping annotations from the two frameworks.
As discussed by Wolf and Gibson (2004), we
find that the Graph Bank discourse representation
(GB) leads to better sentence choices than using
RST trees. The F-score is 48% for the GB clas-
152
Features Acc P R F ROUGE
RST-struct. 81.61 63.00 31.56 42.05 0.569
GB-struct. 82.58 62.50 39.16 48.15 0.508
Table 3: Tree vs graph-based discourse features
sifier and 42% for the RST classifier. The better
performance of GB method comes from higher re-
call scores compared to RST. Their precision val-
ues are comparable. But, in terms of ngram-based
ROUGE scores, the results from RST (0.569)
turn out slightly better than GB (0.508). Over-
all, discourse features based on structure turn out
as strong indicators of sentence importance and
we find both tree and graph representations to be
equally useful for this purpose.
6 Lexical approximation to discourse
structure
In prior work on summarization, graph models of
text have been proposed that do not rely on dis-
course. Rather, lexical similarity between sen-
tences is used to induce graph structure (Erkan
and Radev, 2004; Mihalcea and Tarau, 2005).
PageRank-based computation of sentence impor-
tance have been used on these models with good
results. Now, we would like to see if the discourse
graphs from the Graph Bank (GB) corpus would
be more helpful for determining content impor-
tance than the general text graph based on lexi-
cal similarity (LEX). We perform this comparison
on the 15 documents that we used in the previous
section for evaluating tree versus graph structures.
We used cosine similarity to link sentences in the
lexical graph. Links with similarity less than 0.1
were removed to filter out weak relationships.
The classification results are shown in Table 4.
The similarity graph representation is even more
helpful than RST or GB: the F-score is 53% com-
pared to 42% for RST and 48% for GB. The most
significant improvement from the lexical graph is
in terms of precision 75% which is more than 10%
higher compared to RST and GB features. Using
ROUGE as the evaluation metric, the lexical sim-
ilarity graph, LEX (0.557), gives comparable per-
formance with both GB (0.508) and RST (0.569)
representations (refer Table 3). Therefore, for use
in content selection, lexical overlap information
appears to be a good proxy for building text struc-
ture in place of discourse relations.
Features Acc P R F ROUGE
LEX-struct. 83.23 75.17 41.14 53.18 0.557
Table 4: Performance of lexrank summarizer
7 Discussion
We have analyzed the contribution of different
types of discourse features?structural and seman-
tic. Our results provide strong evidence that dis-
course structure is the most useful aspect. Both
tree and graph representations of discourse can be
used to compute the importance of text units with
very good results. On the other hand, sense in-
formation from discourse does not provide strong
indicators of good content but some constraints
as to which content should not be included in
a summary. These sense features complement
structure information leading to improved perfor-
mance. Further, both these types of discourse fea-
tures are complementary to standardly used non-
discourse features for content selection.
However, building automatic parsers for dis-
course information has proven to be a hard task
overall (Marcu, 2000; Soricut and Marcu, 2003;
Wellner et al, 2006; Sporleder and Lascarides,
2008; Pitler et al, 2009) and the state of cur-
rent parsers might limit the benefits obtainable
from discourse. Moreover, discourse-based struc-
ture is only as useful for content selection as sim-
pler text structure built using lexical similarity.
Even with gold standard annotations, the perfor-
mance of structural features based on the RST
and Graph Bank representations is not better than
that obtained from automatically computed lexical
graphs. So, even if robust discourse parsers exist
to use these features on other test sets, it is not
likely that discourse features would provide better
performance than lexical similarity. Therefore, for
content selection in summarization, current sys-
tems can make use of simple lexical structures to
obtain similar performance as discourse features.
But it should be remembered that summary
quality does not depend on content selection per-
formance alone. Systems should also produce lin-
guistically well formed summaries and currently
systems perform poorly on this aspect. To address
this problem, discourse information is vital. The
most comprehensive study of text quality of au-
tomatically produced summaries was performed
by Otterbacher et al (2002). A collection of 15
automatically produced summaries was manually
edited in order to correct any problems. The study
153
found that discourse and temporal ordering prob-
lems account for 34% and 22% respectively of all
the required revisions. Therefore, we suspect that
for building summarization systems, most benefits
from discourse can be obtained with regard to text
quality compared to the task of content selection.
We plan to focus on this aspect of discourse use
for our future work.
References
L. Carlson, D. Marcu, and M. E. Okurowski. 2001.
Building a discourse-tagged corpus in the frame-
work of rhetorical structure theory. In Proceedings
of SIGdial, pages 1?10.
L. Carlson, D. Marcu, andM. E. Okurowski. 2002. Rst
discourse treebank. Corpus number LDC 2002T07,
Linguistic Data Consortium, Philadelphia.
J. Conroy, J. Schlesinger, and D. O?Leary. 2006.
Topic-focused multi-document summarization using
an approximate oracle score. In Proceedings of
ACL.
H.P. Edmundson. 1969. New methods in automatic
extracting. Journal of the ACM, 16(2):264?285.
G. Erkan and D. Radev. 2004. Lexrank: Graph-based
centrality as salience in text summarization. Journal
of Artificial Intelligence Research (JAIR).
A. Lee, R. Prasad, A. Joshi, and B. Webber. 2008. De-
partures from Tree Structures in Discourse: Shared
Arguments in the Penn Discourse Treebank. In Pro-
ceedings of the Constraints in Discourse Workshop.
C. Lin and E. Hovy. 2000. The automated acquisition
of topic signatures for text summarization. In Pro-
ceedings of COLING, pages 495?501.
C. Lin and E. Hovy. 2002. Manual and automatic
evaluation of summaries. In Proceedings of the ACL
Workshop on Automatic Summarization.
C. Lin and E. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
Proceedings of HLT-NAACL.
C. Lin. 2004. ROUGE: a package for automatic eval-
uation of summaries. In Proceedings of ACL Text
Summarization Workshop.
W.C. Mann and S.A. Thompson. 1988. Rhetorical
structure theory: Towards a functional theory of text
organization. Text, 8.
D. Marcu. 1998. To build text summaries of high qual-
ity, nuclearity is not sufficient. In Working Notes
of the the AAAI-98 Spring Symposium on Intelligent
Text Summarization, pages 1?8.
D. Marcu. 2000. The rhetorical parsing of unrestricted
texts: A surface-based approach. Computational
Linguistics, 26(3):395?448.
M. Marcus, B. Santorini, and M. Marcinkiewicz.
1994. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguis-
tics, 19(2):313?330.
R. Mihalcea and P. Tarau. 2005. An algorithm for
language independent single and multiple document
summarization. In Proceedings of IJCNLP.
A. Nenkova, L. Vanderwende, and K. McKeown.
2006. A compositional context sensitive multi-
document summarizer: exploring the factors that in-
fluence summarization. In Proceedings of SIGIR.
K. Ono, K. Sumita, and S. Miike. 1994. Abstract gen-
eration based on rhetorical structure extraction. In
Proceedings of COLING, pages 344?348.
J.C. Otterbacher, D.R. Radev, and A. Luo. 2002. Revi-
sions that improve cohesion in multi-document sum-
maries: a preliminary study. In Proceedings of ACL
Text Summarization Workshop, pages 27?36.
L. Page, S. Brin, R. Motwani, and T. Winograd. 1998.
The pagerank citation ranking: Bringing order to
the web. Technical report, Stanford Digital Library
Technologies Project.
E. Pitler, A. Louis, and A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in
text. In Proceedings of ACL-IJCNLP, pages 683?
691.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki,
L. Robaldo, A. Joshi, and B. Webber. 2008. The
penn discourse treebank 2.0. In Proceedings of
LREC.
R. Soricut and D. Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical informa-
tion. In Proceedings of HLT-NAACL.
C. Sporleder and A. Lascarides. 2008. Using automat-
ically labelled examples to classify rhetorical rela-
tions: An assessment. Natural Language Engineer-
ing, 14:369?416.
V.R. Uzda, T.A.S. Pardo, and M.G. Nunes. 2008.
Evaluation of automatic text summarization meth-
ods based on rhetorical structure theory. Intelligent
Systems Design and Applications, 2:389?394.
B. Wellner, J. Pustejovsky, C. Havasi, A. Rumshisky,
and R. Saur??. 2006. Classification of discourse co-
herence relations: An exploratory study using mul-
tiple knowledge sources. In Proceedings of SIGdial,
pages 117?125.
F. Wolf and E. Gibson. 2004. Paragraph-, word-, and
coherence-based approaches to sentence ranking: A
comparison of algorithm and human performance.
In Proceedings of ACL, pages 383?390.
F. Wolf and E. Gibson. 2005. Representing discourse
coherence: A corpus-based study. Computational
Linguistics, 31(2):249?288.
154
Appendix: Feature analysis
This appendix provides the results from statistical tests for identifying predictive features from the dif-
ferent classes (RST-based structural features?Table 5, Non-discourse features?Table 6 and PDTB-based
sense features?Table 7).
For real-valued features, we performed a two sided t-test between the corresponding feature values
for important versus non-important sentences. For features which turned out significant in each set, the
value of the test statistic and significance levels are reported in the tables.
For binary features, we report results from a chi-square test to measure how indicative a feature is
for the class of important or non-important sentences. For results from the chi-square test, a (+/-) sign
is enclosed within parentheses for each significant feature to indicate whether the observed number of
times the feature was true in important sentences is greater (+) than the expected value (indication that
this feature is frequently associated with important sentences). When the observed frequency is less than
the expected value, a (-) sign is appended.
RST Features t-stat p-value
Ono penalty -21.31 2.2e-16
Depth score 16.75 2.2e-16
Promotion score 16.00 2.2e-16
Normalized penalty -11.24 2.2e-16
Normalized depth score 17.24 2.2e-16
Normalized promotion score 14.36 2.2e-16
Table 5: Significant RST-based features
Non-discourse features t-stat p-value
Sentence length 3.14 0.0017
Average probability of content words 9.32 2.2e-16
Sum probability of content words 11.83 2.2e-16
Product probability of content words -5.09 3.8e-07
Number of topic signature terms 9.47 2.2e-16
Offset from article beginning -12.54 2.2e-16
Offset from paragraph beginning -28.81 2.2e-16
Offset from paragraph end 7.26 5.8e-13
?
2 p-value
First sentence? 224.63 (+) 2.2e-16
Paragraph initial? 655.82 (+) 2.2e-16
Table 6: Significant non-discourse features
155
PDTB features t-stat p-value
No. of implicit relations involved -9.13 2.2e-16
Total relations involved -6.95 4.9e-12
Distance of Arg1 3.99 6.6e-05
Based on level 1 senses
?
2 p-value
Expresses explicit Expansion 12.96 (-) 0.0003
Expresses explicit Contingency 7.35 (-) 0.0067
Arg1 explicit Expansion 12.87 (+) 0.0003
Arg1 implicit Contingency 13.84 (+) 0.0002
Arg1 implicit Expansion 29.10 (+) 6.8e-08
Arg2 implicit Temporal 4.58 (-) 0.0323
Arg2 implicit Contingency 60.28 (-) 8.2e-15
Arg2 implicit Expansion 134.60 (-) 2.2e-16
Arg2 implicit Comparison 27.59 (-) 1.5e-07
Based on level 2 senses
?
2 p-value
Expresses explicit Conjunction 8.60 (-) 0.0034
Expresses explicit List 4.41 (-) 0.0358
Arg1 explicit Conjunction 10.35 (+) 0.0013
Arg1 implicit Conjunction 5.26 (+) 0.0218
Arg1 implicit Instantiation 18.94 (+) 1.4e-05
Arg1 implicit Restatement 15.35 (+) 8.9-05
Arg1 implicit Cause 12.78 (+) 0.0004
Arg1 implicit List 5.89 (-) 0.0153
Arg2 explicit Asynchronous 4.23 (-) 0.0398
Arg2 explicit Instantiation 10.92 (-) 0.0009
Arg2 implicit Conjunction 51.57 (-) 6.9e-13
Arg2 implicit Instantiation 12.08 (-) 0.0005
Arg2 implicit Restatement 28.24 (-) 1.1e-07
Arg2 implicit Cause 58.62 (-) 1.9e-14
Arg2 implicit Contrast 30.08 (-) 4.2e-08
Arg2 implicit List 12.31 (-) 1.9e-14
Table 7: Significant PDTB-based features
156
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 25?30,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
Tree-Rewriting Models of Multi-Word Expressions
William Schuler
Department of Linguistics
The Ohio State University
schuler@ling.osu.edu
Aravind K. Joshi
Dept. Computer and Information Science
University of Pennsylvania
joshi@linc.cis.upenn.edu
Abstract
Multi-word expressions (MWEs) account for
a large portion of the language used in day-
to-day interactions. A formal system that is
flexible enough to model these large and often
syntactically-rich non-compositional chunks as
single units in naturally occurring text could
considerably simplify large-scale semantic an-
notation projects, in which it would be un-
desirable to have to develop internal compo-
sitional analyses of common technical expres-
sions that have specific idiosyncratic meanings.
This paper will first define a notion of functor-
argument decomposition on phrase structure
trees analogous to graph coloring, in which the
tree is cast as a graph, and the elementary
structures of a grammar formalism are colors.
The paper then presents a formal argument
that tree-rewriting systems, a class of grammar
formalism that includes Tree Adjoining Gram-
mars, are able to produce a proper superset
of the functor-argument decompositions that
string-rewriting systems can produce.
1 Introduction
Multi-word expressions (MWEs), whose structure
and meaning cannot be derived from their compo-
nent words as they occur independently, account for
a large portion of the language used in day-to-day
interactions. Indeed, the relatively low frequency of
comparable single-word paraphrases for elementary
spatial relations like ?in front of? (compare to ?be-
fore?) or ?next to? (compare to ?beside?) suggest a
fundamentality of expressions, as opposed to words,
as a basic unit of meaning in language (Becker, 1975;
Fillmore, 2003). Other examples of MWEs are id-
ioms such as ?kick the bucket? or ?spill the beans?,
which have figurative meanings as expressions that
sometimes even allow modification (?spill some of the
beans?) and variation in sentence forms (?which beans
were spilled??), but are not available when the com-
ponent words of the MWE occur independently. A
formal system that is flexible enough to model these
large and often syntactically-rich non-compositional
chunks as single units in naturally occurring text
could considerably simplify large-scale semantic an-
notation projects, in which it would be undesirable
to have to develop internal compositional analyses
of common technical expressions that have specific
idiosyncratic meanings.
Models have been proposed for MWEs based on
string-rewriting systems such as HPSG (Sag et al,
2002), which model compositionality as string ad-
jacency of a functor and an argument substring.
This string-rewriting model of compositionality es-
sentially treats each projection of a head word as
a functor, each capable of combining with an argu-
ment to yield a higher-level projection or functor.
The set of projections from a lexical head can there-
fore be thought of as a single elementary structure:
an n-ary functor, subsuming the arguments of the
individual functors at each projection. This kind of
approach is intuitive for fully-compositional analy-
ses (e.g. in which a transitive verb like ?hold? is a
functor and a NP complement like ?the basket? is an
argument), but is less natural when applied to sub-
strings of MWEs (e.g. treating pick as a functor and
up as an argument in the verb-particle MWE pick
. . . up), since some of these arguments do not have
any semantic significance (in the pick . . . up exam-
ple , there is no coherent meaning for Up such that
Jpick X upK = Pick(JXK,Up)).
This paper will argue that tree-rewriting systems,
a class of grammar formalisms that includes Tree
Adjoining Grammars (Joshi, 1985; Joshi and Sch-
abes, 1997), are a more natural candidate for mod-
eling MWEs since they can model entire fragments
of phrase structure trees as elementary (locally non-
compositional) semantic building blocks, in addition
to the set of head-projections used in string-rewriting
25
NA
proverbial
N?
S
NP? VP
V
kick
NP
D
the
N
bucket
Figure 1: Composition of elementary trees for idiom
MWE ?kick the bucket? and adjective ?proverbial,? with
the same semantics as an adverb ?proverbially? adjoining
at the VP.
systems. This allows more flexibility in defining the
functor-argument decomposition of a given phrase
structure tree.
This will be demonstrated by reducing the functor-
argument decompositions (compositional accounts of
semantics assigned to portions of phrase structure
trees) of string-rewriting systems to a special case
of functor-argument decompositions of tree-rewriting
systems. Discussion in this paper will focus on
string-rewriting systems augmented with unification
(such as HPSG) because in this framework the issue
of multi-word expressions has been discussed (Sag
et al, 2002). The arguments in this paper also ap-
ply to other string rewriting systems such as catego-
rial grammars (Ajdukiewicz, 1935; Bar-Hillel, 1953;
Steedman, 2000), but in these formalisms the issues
concerning MWEs have not been extensively devel-
oped. Essentially, this paper formalizes the intuition
(Abeille?, 1993) that the extended domain of locality
of tree-rewriting systems allows them to provide a
compositional account of the semantics assigned to
multi-word or idiomatic portions of phrase structure
trees using elementary units that, after composition,
may end up partially discontinuous in these trees.
For example, a portion of a phrase structure tree for
?kick the bucket? with a single interpretation equiv-
alent to ?die? can be modified through adjunction
of the adjective ?proverbial? at the noun constituent
?bucket? without postulating separate semantics for
?kick? (see Figure 1).
2 Definitions
String rewriting systems are sets of rules for re-
placing symbols with other symbols in strings. A
rewriting of some start symbol into a set of lexical
symbols is called a derivation. Rewrite rules in a
string rewriting system can be defined to have des-
ignated functor and argument symbols. Any deriva-
tion ? can therefore yield a functor-argument decom-
position D(?), essentially defining a set of semantic
functor-argument dependencies among structured el-
ementary categories.
For simplicity, a functor-argument decomposition
will be defined as a mapping from the constituent
nodes in a phrase structure tree to the nodes in
the elementary structures used to derive that tree.
This can be thought of as a coloring of phrase struc-
ture nodes, in which colors correspond to elementary
structures in the rewriting system. The elementary
structures used in such a decomposition may then
be considered n-ary functors, which may take sev-
eral arguments, each of a different color.
In string-rewriting systems such as HPSG, these
n-ary functors consist of a head word and its pro-
jections, and the arguments of the functor are the
non-projecting child of each such projection. Fig-
ure 2 shows feature-based and categorial analyses
for the MWE ?. . . to the . . . power? (as in ?raise Y
to the X power?) which is taken here to have unam-
biguous meaning (in a technical context) as Y X or
Pow(Y,X), and is analyzed here to wrap around an
ordinal number argument X and then adjoin onto a
verb phrase ?raise Y ? as a modifier.1 Because their
elementary structures are projected up from individ-
ual head words, these systems prohibit an analysis
of this MWE as a single wrapping functor. Instead,
MWEs like this must be decomposed into individual
functor words (e.g. power) and argument words (e.g.
the, and to).
Tree-rewriting systems, on the other hand, allow
elementary structures to contain nodes which are nei-
ther projections nor argument sites. This permits
an analysis of ?to the . . . power? as a single functor
wrapped around its argument (see Figure 3), with-
out having to specify functor-argument relations be-
tween power, to, and the.
More generally, string-rewriting systems use ele-
mentary structures (n-ary functors) that originate
at the lexical item and exhibit a bottom-up branch-
ing structure, branching to an argument site and a
higher level projection at each step. In contrast, tree-
rewriting systems use elementary structures that
originate at a phrasal or clausal node and exhibit
1We are using the MWE ?. . . to the . . . power? as a sim-
ple example with an unambiguous meaning in the domain
of mathematics to illustrate our main points in the context
of both adjunction and substitution operations. Alternative
analyses are possible (e.g. with ?the? or additional modifiers
adjoining in, to allow variations like ?to every even power un-
der six?), but in any case the words ?to? and ?power? on either
side of the X argument are taken to be idiosyncratic to this
expression of Y X . Since it is analyzed as a modifier, this ex-
ample can be used to demonstrate coindexation of structure
in a tree-rewriting system.
26
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
label : power
left :
[
label : ORD
]
proj :
?
?
?
?
?
?
?
?
?
?
?
?
?
?
label : N1
left :
[
label : the
]
proj :
?
?
?
?
?
?
?
?
?
?
label : NP
left :
[
label : to
]
proj :
?
?
?
?
?
?
label : PP
left :
[ label : VP
1
]
proj :
[ label : VP
1
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
=
power
?
OR l
N1
p
the l
NP
p
to l
PP
p
VP l1
VP
p
1
Figure 2: Elementary structures for a verb-phrase-modifying preposition in a functor-argument analysis derived from
a feature structure grammar. Here, ? indicates the origin node and boxed numbers indicate coindexations.
a top-down branching structure that mirrors that of
a phrase structure tree. As one might expect, there
are tree-rewriting systems (namely those whose el-
ementary structures contain multiple lexical items)
that can produce functor-argument decompositions
(?colorings?) of a phrase structure tree which can-
not be produced by a string-rewriting system. More
surprisingly however, this paper will show that the
converse is not true: in other words, for any string-
rewriting system there always exists a tree-rewriting
system that can produce the same functor-argument
decomposition of a phrase structure tree. Thus, the
set of functor-argument decompositions that can be
produced by tree-rewriting systems is a proper super-
set of those that can be produced by string-rewriting
systems.
This is surprising because, taken as a class,
there is no inherent difference in recognition com-
plexity between string-rewriting systems and tree-
rewriting systems (as may be the case between spe-
cific members of these classes, say between CGs
and TAGs), since both are worst-case exponential
if unconstrained coindexation of structure is allowed
(as in unification grammars). This is also surpris-
ing because, since they branch upward, the ele-
mentary structures of string-rewriting systems can
specify complex functors as arguments, which the
downward-branching elementary structures of tree-
rewriting systems cannot. However, this paper will
show that this ability to specify complex functors
as arguments does not confer any additional flexibil-
ity in calculating functor-argument decompositions
of phrase structure trees, and can be factored out
with no loss in expressivity.
VP
VP? PP
to? NP
the? N1
OR? power?
=
VP
?
1
VP
1
1 PP
2
to
1
? NP
2
the
1
? N1
2
OR
1
power
2
?
Figure 3: Elementary structure for a verb-phrase-
modifying prepositional phrase ?to the . . . power? in a
tree-rewriting system, derived from a tree-adjoining
grammar. Here, ? indicates the origin node, ? indicates a
non-argument node (or lexical ?anchor?), and boxed num-
bers indicate coindexations.
3 Reduction of string-rewriting
systems to tree-rewriting systems
The first step will be to define an n-ary functor in a
string-rewriting system as a kind of elementary struc-
ture ? (a tree in fact), whose nodes ?? branch ?up-
ward? into sub-structure nodes (connected by depart-
ing arcs labeled l, r, or p,) specifying a left or right
argument category (???l or ???r) and a projected
category (???p), rather than branching ?downward?
into left and right child constituents as in an ordi-
nary phrase structure tree.2 In order to extend this
reduction to feature-based systems, these elemen-
tary structures will also be augmented with coindex-
ation sets I of elementary structure nodes that must
be identical (in terms of labels and departing arcs)
in any functor-argument decomposition of a phrase
structure tree.
2Here, a node ?? is defined by the path of concatenated
arcs ? that lead to it from the origin or root ?? .
27
SNP
Cube
VP
VP
raises NP
the sum
PP
to NP
the NP
ORD
third
power
? :
raise
?
NPr
VP
p
NP l
S
p
? :
power
?
OR l
N1
p
the l
NP
p
to l
PP
p
VP l1
VP
p
1
?p?p?DFA(??)
?p?I(?p?p?p?l)
Figure 4: Decomposition (?coloring?) of a phrase structure tree ? for the sentence ?Cube raises the sum to the third
power?, using elementary structures ? and ? shown at right. Dotted lines from phrase structure tree nodes ?? to
elementary structure nodes ?? indicate that ?? generates ?? in the functor-argument decomposition: ?? ?DFA(??).
Dashed lines from elementary structure nodes ?? to other elementary structure nodes ?? indicate that ?? is among
the nodes identified with ?? as arguments of ? in the decomposition. Boxed identifiers indicated coindices between
nodes ?? and ??? in ? such that ?I?? . ?? , ??? ?I.
Figure 4 shows a functor-argument decomposition
(or ?coloring?) of a phrase structure tree using these
upward-branching elements.
The upward-branching elementary structures used
in any such decomposition can then be converted
into a normal form in which all argument nodes are
atomic (have no departing arcs), using the following
transformations of elementary structures to equiva-
lent structures that fit together generate the same
functor-argument decomposition. This is done by si-
multaneously excising ?matched? material from both
the argument branch of an elementary structure and
the top of the elementary structure that is its argu-
ment in the given decomposition.
The use of coindexation sets complicates this
transformation somewhat. Initial configurations of
coindexation sets in upward-branching elementary
structures can be exhaustively partitioned into three
classes, defined with respect to the ?trunk? of the el-
ementary structure, which is the set of nodes con-
nected to the origin by paths containing only p arcs.
These classes are:
1. coindexations with more than one coindexed
node on the trunk,
2. coindexations with fewer than one coindexed
node on the trunk, and
3. coindexations with exactly one coindexed node
on the trunk.
Elementary structures in the first class, with more
than one coindexed node on the trunk, are equivalent
to graphs with directed cycles, and are ordinarily
excluded from feature-based analyses, so they will
be ignored here.
Elementary structures in the second class, with
fewer than one coindexed node on the trunk,
can be converted to equivalent structures with
no coindices (which trivially satisfies the above
argument-atomicity requirement), using the simulta-
neous excision of ?matched? structure in functor and
argument structures described above, by simply ex-
tending this to cover the portion of the argument
elementary structure that extends all the way to the
top of the trunk.
Elementary structures in the third class, with
exactly one coindexed node on the trunk, can
be converted to equivalent structures that sat-
isfy argument-atomicity using a three-step process.
First, the upward-branching sub-structures above
these coindexed nodes (if any) are unified, so the arcs
departing from each coindexed node will be recur-
sively identical (this must be possible in any feature-
based grammar, or the coindexation would be ill-
formed, and should therefore be excluded). The coin-
dexation is then recursively slid up along the p arc
departing from each such node, until the coindexa-
28
tion set contains nothing but atomic categories (with
no departing arcs). Finally, the argument nodes are
made to be atomic using the simultaneous excision of
?matched? structure in functor and argument struc-
tures described above, leaving an (atomic) coindex-
ation at each (atomic) argument position in each af-
fected branch.
Elementary structures with multiple class 3 coin-
dexation sets I and I ? (which cannot be deleted
as described above for class 2 sets) can be trans-
formed into structures with a single coindexation
set I by copying the portion of the trunk between
the (unique) on-trunk members of each initial set I
and I ? onto every other node in the set I ? that con-
tains the lower trunk node (this copy should include
the coindex belonging to I). The coindexation set
I ? containing the lower on-trunk node is then simply
deleted.
The normal-form upward-branching structures re-
sulting from this transformation can now be con-
verted into downward-branching elementary trees in
a tree-rewriting system (with coindexed nodes corre-
sponding to ?root? and ?foot? nodes as defined for tree-
adjoining grammars) by simply replacing each pair
of argument and conclusion arcs with a pair of left-
child and right-child arcs departing the conclusion
node. Since the normal form for upward-branching
elementary structures allows only atomic arguments,
this re-drawing of arcs must result in well-formed
downward-branching elementary trees in every case.3
In particular, this conversion results in a subset of
tree-rewriting systems in which each (binary) branch
of every elementary tree must have exactly one argu-
ment position and one non-argument position among
its two children. This is a special case of a more
general class of tree-rewriting systems, which may
have two argument positions or no argument po-
sitions among the children at each binary branch.
Such trees are not equivalent to trees with a single ar-
gument position per branch, because they will result
in different functor-argument decompositions (?col-
orings?) of a target phrase structure tree. Moreover,
it is precisely these non-string-rewriting-equivalent
elementary trees that are needed to model the lo-
cal non-compositionality of larger multi-word expres-
sions like ?threw X to the lions? (see Figure 5), be-
cause only downward branches with multiple non-
3Recognition and parsing of feature-based grammars, and
of tree-rewriting systems whose elementary trees contain mul-
tiple foot nodes, are both exponential in the worst case. How-
ever, both types of grammars are amenable to regular-from re-
strictions which prohibit recursive adjunction at internal (non-
root, non-foot) tree nodes, and thereby constrain recognition
and parsing complexity to cubic time for most kinds of natural
language grammars (Rogers, 1994).
S
NP? VP
VP
threw? NP?
PP
to? NP
the? lions?
Figure 5: Elementary structure for MWE idiom ?threw
. . . to the lions,? allowing modification to both VP, PP
and NP sub-constituents (e.g. ?threw your friends today
right to the proverbial lions).
argument children can produce the multi-level sub-
trees containing the word ?threw? and the word ?lions?
in the same elementary unit.
4 Conclusion
This paper has shown that tree-rewriting systems
are able to produce a superset of the functor-
argument decompositions that can be produced by
string-rewriting systems such as categorial gram-
mars and feature-structure grammars such as HPSG.
This superset additionally allows elementary units
to contain multiple (lexical) leaves, which a string-
rewriting system cannot. This makes tree-rewriting
systems ideally suited to the analysis of natural lan-
guage texts that contain many multi-word expres-
sions with idiosyncratic (non-compositional) mean-
ings. Although neither the tree-rewriting nor the
string-rewriting analyses defined above can be gen-
erated in guaranteed polynomial time (since they
may require the construction of unbounded stacks
of unrecognized structure during bottom-up recogni-
tion), they can both be made polynomial (indeed, cu-
bic) by the introduction of ?regular form? constraints
(Rogers, 1994), which limit this stack in the same
way in both cases.
In contrast with representations like that of
(Villavicencio et al, 2004), in which concepts are dis-
tributed over several lexical entries, a tree-rewriting
representation such as the one described in this pa-
per allows only a single lexical entry to be listed for
each concept. For example:
... throw ... to the lions:
(s(np0!)(vp(v)(np1!)(pp(p)(np(d)(n)))))
... to the ... power:
(vp(vp0*)(pp(p)(np(d)(n(a1!)(n)))))
(using the notation ?!? and ?*? for substitution sites
and foot nodes, respectively). It is anticipated that
this will simplify the organization of lexical resources
for multi-word expressions.
29
References
Abeille?, Anne. 1993. The flexibility of french idioms:
a representation with lexicalized tree adjoining gram-
mar. In A. Schenk and E. van der Linden, editors,
Idioms. Erlbaum.
Ajdukiewicz, Kazimierz. 1935. Die syntaktische kon-
nexitat. In S. McCall, editor, Polish Logic 1920-1939.
Oxford University Press, pages 207?231. Translated
from Studia Philosophica 1: 1?27.
Bar-Hillel, Yehoshua. 1953. A quasi-arithmetical nota-
tion for syntactic description. Language, 29:47?58.
Becker, Joseph D. 1975. The phrasal lexicon. In Pro-
ceedings of the Workshop on Theoretical Issues in Nat-
ural Language Processing, Workshop in Computational
Linguisitcs, Psychology, and AI, Cambridge, MA.
Fillmore, Charles J. 2003. Multiword expressions,
November. Invited talk at the Institute for Research in
Cognitive Science (IRCS), University of Pennsylvania.
http://www.cis.upenn.edu/?ircs/colloq/2003/fall/fillmore.html.
Joshi, Aravind and Yves Schabes. 1997. Tree-adjoning
grammars. In G. Rozenberg and A. Salomaa, edi-
tors, Handbook of Formal Languages. Springer-Verlag,
Berlin, pages 69?123.
Joshi, Aravind K. 1985. How much context sensitivity
is necessary for characterizing structural descriptions:
Tree adjoining grammars. In L. Karttunen D. Dowty
and A. Zwicky, editors, Natural language parsing: Psy-
chological, computational and theoretical perspectives.
Cambridge University Press, Cambridge, U.K., pages
206?250.
Rogers, James. 1994. Capturing CFLs with tree ad-
joining grammars. In Proceedings of the 32nd Annual
Meeting of the Association for Computational Linguis-
tics (ACL?94).
Sag, Ivan, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword expres-
sions: a pain in the neck for nlp. In Proceedings
of the Third International Conference on Intelligent
Text Processing and Computational Linguistics (CI-
CLING?02), pages 1?15, Mexico City, Mexico.
Steedman, Mark. 2000. The syntactic process. MIT
Press/Bradford Books, Cambridge, MA.
Villavicencio, Aline, Ann Copestake, Benjamin Waldron,
and Fabre Lambeau. 2004. Lexical encoding of
mwes. In Takaaki Tanaka, Aline Villavicencio, Fran-
cis Bond, and Anna Korhonen, editors, Second ACL
Workshop on Multiword Expressions: Integrating Pro-
cessing, pages 80?87, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.
30
Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 42?54,
Jeju, Republic of Korea, 10 July 2012. c?2012 Association for Computational Linguistics
Discourse Structure and Computation: Past, Present and Future
Bonnie Webber
School of Informatics
University of Edinburgh
Edinburgh UK EH8 9AB
bonnie.webber@ed.ac.uk
Aravind Joshi
Dept of Computer & Information Science
University of Pennsylvania
Philadelphia PA 19104-6228
joshi@seas.upenn.edu
Abstract
The discourse properties of text have long
been recognized as critical to language tech-
nology, and over the past 40 years, our un-
derstanding of and ability to exploit the dis-
course properties of text has grown in many
ways. This essay briefly recounts these de-
velopments, the technology they employ, the
applications they support, and the new chal-
lenges that each subsequent development has
raised. We conclude with the challenges faced
by our current understanding of discourse, and
the applications that meeting these challenges
will promote.
1 Why bother with discourse?
Research in Natural Language Processing (NLP) has
long benefitted from the fact that text can often be
treated as simply a bag of words or a bag of sen-
tences. But not always: Position often matters ?
e.g., It is well-known that the first one or two sen-
tences in a news report usually comprise its best ex-
tractive summary. Order often matters ? e.g., very
different events are conveyed depending on how
clauses and sentences are ordered.
(1) a. I said the magic words, and a genie ap-
peared.
b. A genie appeared, and I said the magic
words.
Adjacency often matters ? e.g., attributed mate-
rial may span a sequence of adjacent sentences,
and contrasts are visible through sentence juxtaposi-
tion. Context always matters ? e.g., All languages
achieve economy through minimal expressions that
can only convey intended meaning when understood
in context.
Position, order, adjacency and context are intrin-
sic features of discourse, and research on discourse
processing attempts to solve the challenges posed by
context-bound expressions and the discourse struc-
tures that give rise, when linearized, to position, or-
der and adjacency.
But challenges are not why Language Technol-
ogy (LT) researchers should care about discourse:
Rather, discourse can enable LT to overcome known
obstacles to better performance. Consider auto-
mated summarization and machine translation: Hu-
mans regularly judge output quality in terms that in-
clude referential clarity and coherence. Systems can
only improve here by paying attention to discourse
? i.e., to linguistic features above the level of n-
grams and single sentences. (In fact, we predict that
as soon as cheap ? i.e., non-manual ? methods are
found for reliably assessing these features ? for ex-
ample, using proxies like those suggested in (Pitler
et al, 2010) ? they will supplant, or at least com-
plement today?s common metrics, Bleu and Rouge
that say little about what matters to human text un-
derstanding (Callison-Burch et al, 2006).)
Consider also work on automated text simplifica-
tion: One way that human editors simplify text is
by re-expressing a long complex sentence as a dis-
course sequence of simple sentences. Researchers
should be able to automate this through understand-
ing the various ways that information is conveyed
in discourse. Other examples of LT applications
already benefitting from recognizing and applying
discourse-level information include automated as-
sessment of student essays (Burstein and Chodorow,
2010); summarization (Thione et al, 2004), infor-
42
mation extraction (Patwardhan and Riloff, 2007;
Eales et al, 2008; Maslennikov and Chua, 2007),
and more recently, statistical machine translation
(Foster et al, 2010). These are described in more
detail in (Webber et al, 2012).
Our aim here then, on this occasion of ACL?s 50th
Annual Meeting, is to briefly describe the evolution
of computational approaches to discourse structure,
reflect on where the field currently stands, and what
new challenges it faces in trying to deliver on its
promised benefit to Language Technology.
2 Background
2.1 Early Methods
The challenges mentioned above are not new.
Question-Answering systems like LUNAR (Woods,
1968; Woods, 1978) couldn?t answer successive
questions without resolving context-bound expres-
sions such as pronouns:
(2) What is the concentration of silicon in brec-
cias?
?breccia1, parts per million?
?breccia2, parts per million?
? . . . ?
What is it in volcanics? (Woods, 1978)
Early systems for human interaction with animated
agents, including SHRDLU (Winograd, 1973) and
HOMER (Vere and Bickmore, 1990), faced the
same challenge. And early message understand-
ing systems couldn?t extract relevant information
(like when a sighted submarine submerged ? ?went
sinker?) without recognizing relations implicit in the
structure of a message, as in
(3) VISUAL SIGHTING OF PERISCOPE FOL-
LOWED BY ATTACK WITH ASROC AND TOR-
PEDO. WENT SINKER. LOOSEFOOT 722/723
CONTINUE SEARCH. (Palmer et al, 1993)
The same was true of early systems for processing
narrative text (under the rubric story understanding).
They took on the problem of recognizing events that
had probably happened but hadn?t been mentioned
in the text, given the sequence of events that had
been (Lehnert, 1977; Rumelhart, 1975; Schank and
Abelson, 1977; Mandler, 1984).
Since these early systems never saw more than
a handful of examples, they could successfully em-
ploy straight-forward, but ad hoc methods to handle
the discourse problems the examples posed. For ex-
ample, LUNAR used a single 10-position ring buffer
to store discourse entities associated with both the
user?s and the system?s referring expressions, resolv-
ing pronouns by looking back through the buffer
for an appropriate entity and over-writing previous
buffer entries when the buffer was full.
The next wave of work in computational dis-
course processing sought greater generality through
stronger theoretical grounding, appealing to then-
current theories of discourse such as Centering The-
ory (Grosz et al, 1986; Grosz et al, 1995), used as
a basis for anaphor resolution (Brennan et al, 1987;
Walker et al, 1997; Tetreault, 2001) and text genera-
tion (Kibble and Power, 2000), Rhetorical Structure
Theory (Mann and Thompson, 1988), used as a ba-
sis for text generation (Moore, 1995) and document
summarization (Marcu, 2000b), and Grosz and Sid-
ner?s theory of discourse based on intentions (Grosz
and Sidner, 1986a) and shared plans (Grosz and
Sidner, 1990), used in developing animated agents
(Johnson and Rickel, 2000). Issues related to fully
characterizing centering are explored in great detail
in (Kehler, 1997) and (Poesio et al, 2004).
The approaches considered during this period
never saw more than a few handfuls of examples.
But, as has been clear from developments in PoS-
tagging, Named Entity Recognition and parsing,
Language Technology demands approaches that can
deal with whatever data are given them. So subse-
quent work in computational discourse processing
has similarly pursued robustness through the use of
data-driven approaches that are usually able to cap-
ture the most common forms of any phenomenon
(ie, the 80% at the high end of the Zipfian distri-
bution), while giving up on the long tail. This is
described in Section 3.
2.2 Early Assumptions
While early work focussed on the correct assump-
tion that much was implicit in text and had to be
inferred from the explicit sequence of sentences that
constituted a text, work during the next period fo-
cussed on the underlying structure of discourse and
its consequences. More specificaly, it assumed that
the sequence of sentences constituting the text were
covered by a single tree structure, similar to the sin-
gle tree structure of phrases and clauses covering the
43
s1 s2
s3
condition
condition
s1
s2 s3motivation
motivation
(a) (b)
Figure 1: Proposed discourse structures for Ex. 4: (a) In
terms of informational relations; (b) in terms of inten-
tional relations
words in a sentence. At issue though was the nature
of the structure.
One issue concerned the nature of the relation be-
tween parent and child nodes in a discourse tree,
and/or the relation between siblings. While Rhetor-
ical Structure Theory (Mann and Thompson, 1988)
posited a single discourse relation holding between
any two discourse units (i.e., units projecting to ad-
jacent text spans), Moore and Pollack (1992) gave an
example of a simple discourse (Ex. 4) in which dif-
ferent choices about the discourse relation holding
between pairs of units, implied different and non-
isomorphic structures.
(4) Come home by 5:00. s1 Then we can go to the
hardware store before it closes. s2 That way we
can finish the bookshelves tonight. s3
Example 4 could be analysed purely in terms of
information-based discourse relations, in which s1
specified the CONDITION under which s2 held,
which in turn specified the CONDITION under which
s3 held. This would make s1 subordinate to s2,
which in turn would be subordinate to s3, as in Fig-
ure 1a. Alternatively, Example 4 could be analysed
purely in terms of intention-based (pragmatic) rela-
tions, in which s2 would be MOTIVATION for s1,
while s3 would be MOTIVATION for s2. This would
make s3 subordinate to s2, which in turn would be
subordinate to s1, as in Figure 1b. In short, the
choice of relation was not merely a matter of labels,
but had structural implications as well.
Another issue during this period concerned the
nature of discourse structure: Was it really a tree?
Sibun (1992), looking at people?s descriptions of the
layout of their house or apartment, argued that they
resembled different ways of linearizing a graph of
the rooms and their connectivity through doors and
halls. None of these linearizations were trees. Sim-
ilarly, Knott et al (2001), looking at transcriptions
of museum tours, argued that each resembled a lin-
ear sequence of trees, with one or more topic-based
connections between their root nodes ? again, not
a single covering tree structure. Wiebe (1993), look-
ing at simple examples such as
(5) The car was finally coming toward him. s1
He finished his diagnostic tests, s2
feeling relief. s3
But then the car started to turn right. s4
pointed multiple lexical items explicitly relating a
clause to multiple other clauses. Here, but would
relate s4 to s3 via a CONTRAST relation, while then
would relate s4 to s2 via a temporal SUCCESSION
relation.
The most well-known of work from this period
is that of Mann and Thompson (1988), Grosz and
Sidner (1986b), Moore and Moser (1996), Polanyi
and van den Berg (1996), and Asher and Lascarides
(2003).1
The way out of these problems was also a way
to achieve the robustness required of any Language
Technology, and that lay in the growing consensus
towards the view that discourse does not have a sin-
gle monolithic hierarchical structure. Rather, dif-
ferent aspects of a discourse give rise to different
structures, possibly with different formal properties
(Stede, 2008; Stede, 2012; Webber et al, 2012).
These different structures we describe in the next
section, while the fact that this can?t be the end of
the story, we take up in Section 4.
3 The Situation Today
Recent years have seen progress to differing degrees
on at least four different types of discourse struc-
tures: topic structure, functional structure, event
structure, and a structure of coherence relations.
First we say a bit about the structures, and then about
the resources employed in recognizing and labelling
them.
3.1 Types of discourse structures
Topic structure and automated topic segmentation
aims to break a discourse into a linear sequence of
1For a historical account and assessent of work in automated
anaphora resolution in this period and afterwards, we direct the
reader to Strube (2007), Ng (2010) and Stede (2012).
44
topics such the geography of a country, followed by
its history, its demographics, its economy, its legal
structures, etc. Segmentation is usually done on a
sentence-by-sentence basis, with segments not as-
sumed to overlap. Methods for topic segmenation
emply semantic, lexical and referential similarity
or, more recently, language models (Bestgen, 2006;
Chen et al, 2009; Choi et al, 2001; Eisenstein and
Barzilay, 2008; Galley et al, 2003; Hearst, 1997;
Malioutov and Barzilay, 2006; Purver et al, 2006;
Purver, 2011).
Functional structure and automated functional
segmentation aims to identify sections within a dis-
course that serve different functions. These func-
tions are genre-specific. In the case of scien-
tific journals, high-level sections generally include
the Background (work that motivates the objec-
tives of the work and/or the hypothesis or claim
being tested), followed by its Methods, and Re-
sults, ending with a Discussion of the results or out-
comes, along with conclusions to be drawn. Finer-
grained segments might include the advantage of
a new method (method-new-advantage) or of an
old method (method-old-advantage) or the disad-
vantage of one or the other (Liakata et al, 2010).
Again, segmentation is usually done on a sentence-
by-sentence basis, with sentences not assumed to
fill more than one function. Methods for functional
segmentation have employed specific cue words and
phrases, as well as more general language models
(Burstein et al, 2003; Chung, 2009; Guo et al,
2010; Kim et al, 2010; Lin et al, 2006; McKnight
and Srinivasan, 2003; Ruch et al, 2007; Mizuta
et al, 2006; Palau and Moens, 2009; Teufel and
Moens, 2002; Teufel et al, 2009; Agarwal and Yu,
2009). The BIO approach to sequential classica-
tion (Beginning/Inside/Outside) used in Named En-
tity Recognition has also proved useful (Hirohata
et al, 2008), recognizing that the way the start of
a functional segement is signalled may differ from
how it is continued.
Note that topic segmentation and functional seg-
mentation are still not always distinguished. For
example, in (Jurafsky and Martin, 2009), the term
discourse segmentation is used to refer to any seg-
mentation of a discourse into a ?high-level? linear
structure. Nevertheless, segmentation by function
exploits different features (and in some cases, dif-
ferent methods) than segmentation by topic, so they
are worth keeping distinct.
Attention to event structure and the identification
of events within a text is a more recent phenomena,
after a hiatus of over twenty years. Here we just
point to work by (Bex and Verheij, 2010; Cham-
bers and Jurafsky, 2008; Do et al, 2011; Finlayson,
2009).
The automated identification of discourse rela-
tions aims to identify discourse relations such as
CONDITION and MOTIVATION, as in Example 4,
and CONTRAST and SUCCESSION, as in Exam-
ple 5. These have also been called coherence re-
lations or rhetorical relations. Methods used de-
pend on whether or not a text is taken to be divisible
into a covering sequence of a non-overlapping dis-
course units related to adjacent units by discourse
relations as in Rhetorical Structure Theory (Mann
and Thompson, 1988) or to both adjacent and non-
adjacent units as in the Discourse GraphBank (Wolf
and Gibson, 2005). If such a cover is assumed,
methods involve parsing a text into units using lex-
ical and punctuational cues, followed by labelling
the relation holding between them (Marcu, 2000a;
Marcu, 2000b; Wolf and Gibson, 2005). If text is
not assumed to be divisible into discourse units, then
methods involve finding evidence for discourse re-
lations (including both explicit words and phrases,
and clausal and sentential adjacency) and their ar-
guments, and then labelling the sense of the iden-
tified relation (Elwell and Baldridge, 2008; Ghosh
et al, 2011; Lin et al, 2010; Lin, 2012; Prasad et
al., 2010a; Wellner, 2008; Wellner and Pustejovsky,
2007).
3.2 Resources for discourse structure
All automated systems for segmenting and labelling
text are grounded in data ? whether the data has
informed the manual creation of rules or has been
a source of features for an approach based on ma-
chine learning. In the case of topic structure and
high-level functional structure, there is now a sub-
stantial amount of data that is freely available. For
other types of discourse structure, manual annota-
tion has been required and, depending on the type of
structure, different amounts are currently available.
More specifically, work on topic structure and
segmentation has been able to take advantage of the
45
large, free, still-growing wikipedia, where articles
on similar topics tend to show similar explicit seg-
mentation into sub-topics. This is certainly the case
with the English wikipedia. If similar wikipedia
evolving in other languages lack explicit segmenta-
tion, it may be that cross-lingual techniques may be
able to project explicit segmentation from English-
language articles.
With respect to high-level functional structure,
some work on automated segmentation has been
able to exploit explicit author-provided indicators
of structure, such as the author-structured abstracts
now required by bio-medical journals indexed by
MedLine. Researchers have used these explicitly
structured abstracts to segment abstracts that lack
explicit structure (Chung, 2009; Guo et al, 2010;
Hirohata et al, 2008; Lin et al, 2006).
For all other kinds of discourse structures, ded-
icated manual annotation has been required, both
for segmentation and labelling, and many of these
resources have been made available for other re-
searchers. For fine-grained functional structure,
there is the ART corpus (Liakata et al, 2010)2.
For discourse relations annotated in the RST
framework, there is the RST Discourse TreeBank of
English text (Carlson et al, 2003), available through
the Linguistic Data Consortium (LDC), as well as
similarly annotated corpora in Spanish (da Cunha et
al., 2011), Portugese (Pardo et al, 2008) and Ger-
man (Stede, 2004).
For discourse relations annotated in the lexically-
grounded approach first described in (Webber and
Joshi, 1998), there is the Penn Discourse TreeBank
(Prasad et al, 2008) in English, as well as corpora
in Modern Standard Arabic (Al-Saif and Markert,
2010; Al-Saif and Markert, 2011), Chinese (Xue,
2005; Zhou and Xue, 2012), Czech (Mladova? et al,
2008), Danish (Buch-Kromann et al, 2009; Buch-
Kromann and Korzen, 2010), Dutch (van der Vliet
et al, 2011), Hindi (Oza et al, 2009), and Turk-
ish (Zeyrek and Webber, 2008; Zeyrek et al, 2009;
Zeyrek et al, 2010). Also available are discourse-
annotated journal articles in biomedicine (Prasad et
al., 2011) and discourse-annotated dialogue (Tonelli
et al, 2010).
2http://www.aber.ac.uk/en/cs/research/cb/projects/art/art-
corpus/
4 New challenges
Although the largely empirically-grounded, multi-
structure view of discourse addresses some of the
problems that previous computational approaches
encountered, it also reveals new ones, while leaving
some earlier problems still unaddressed.
4.1 Evidence for discourse structures
The first issue has to do with what should be taken as
evidence for a particular discourse structure. While
one could simply consider all features that can be
computed reliably and just identify the most accu-
rate predictors, this is both expensive and, in the end,
unsatisfying.
With topic structure, content words do seem to
provide compelling evidence for segmentation, ei-
ther using language models or semantic relatedness.
On the other hand, this might be improved through
further evidence in the form of entity chains, as ex-
plored earlier in (Kan et al, 1998), but using to-
day?s more accurate approaches to automated coref-
erence recognition (Strube, 2007; Charniak and El-
sner, 2009; Ng, 2010).
Whatever the genre, evidence for function struc-
ture seems to come from the frequency and distri-
bution of closed-class words, particular phrases (or
phrase patterns), and in the case of speech, into-
nation. So, for example, Niekrasz (2012) shows
that what he calls participant-relational features
that indicate the participants relationships to the
text provide convincing evidence for segmenting
oral narrative by the type of narrative activity tak-
ing place. These features include the distribution
and frequency of first and second person pronouns,
tense, and intonation. But much work remains to
be done in this area, in establishing what provides
reliable evidence within a genre and what evidence
might be stable across genres.
Evidence for discourse relations is what we have
given significant thought to, as the Penn Discourse
TreeBank (Prasad et al, 2008) and related corpora
mentioned in Section 3.2 aim to ground each in-
stance of a discourse relation in the evidence that
supports it. The issue of evidence is especially
important because none of these corpora has yet
been completely annotated with discourse relations.
Completing the annotation and developing robust
46
automated segmentation techniques requires iden-
tifying what elements of the language provide evi-
dence for coherence relations, and under what con-
ditions.
The two main types of evidence for discourse re-
lations in English are the presence of a discourse
connective and sentence adjacency. Discourse con-
nectives annotated in the PDTB 2.0 come from
a list of subordinating and coordinating conjunc-
tions, and discourse adverbials ? a subset of those
identified by Forbes-Riley et al(2006). Subse-
quently, Prasad et al (2010b) used Callison-Burch?s
technique for identifying syntax-constrained para-
phrases (Callison-Burch, 2008) to identify addi-
tional discourse connectives, some of which don?t
appear in the PDTB corpus and some of which
appear in the corpus but were not identified and
annotated as discourse connectives. English isn?t
alone in lacking a complete list of discourse con-
nectives: While German has the massive Hand-
buch de deutschen Konnektoren (Pasch et al, 2003),
even this resource has been found to be incomplete
through clever application of automated tagging and
word-alignment of parallel corpora (Versley, 2010).
Evidence for discourse relations in the PDTB also
comes from lexical or phrasal elements that are out-
side the initial set of conjunctions and discourse ad-
verbials. This evidence has been called alternative
lexicalization or AltLex (Prasad et al, 2010b), and
includes (in English) clause-initial what?s more (Ex-
ample 6) and that means (Example 7).
(6) A search party soon found the unscathed air-
craft in a forest clearing much too small to have
allowed a conventional landing. What?s more,
the seven mail personnel aboard were missing.
[wsj 0550]
(7) The two companies each produce market
pulp, containerboard and white paper. That
means goods could be manufactured closer
to customers, saving shipping costs, he said.
[wsj 0317]
The discovery of these other forms of evidence3
raises the question of when it is that a word or phrase
signals a discourse relation. For example, only 15 of
the 33 tokens of that means in the PDTB were anno-
tated as evidence of a discourse relation. While the
3which English is not alone in having, cf. (Rysova, 2012)
three paragraph-initial instances were left unanno-
tated due to resource limitations (ie, no paragraph
initial sentences were annotated unless they con-
tained an explicit discourse connective), the major-
ity were ignored because they followed an explicit
connective.
As Wiebe?s example (5) showed, there can be
multiple explicit discourse connectives in a clause,
each of which is evidence for a separate discourse re-
lation (albeit possibly between the same arguments).
All of these are annotated in the PDTB ? eg, both but
and then in
(8) Congress would have 20 days to reject the
package with a 50% majority, but then a Presi-
dent could veto that rejection. [wsj 1698]
The question is whether an AltLex in the context of
an explicit connective also provides evidence of a
distinct discourse relation ? for example, with the
conjunction with But in
(9) At a yearling sale, a buyer can go solo and get
a horse for a few thousand dollars. But that
means paying the horse?s maintenance; on av-
erage, it costs $25,000 a year to raise a horse.
[wsj 1174]
As noted above, the PDTB 2.0 also admits sen-
tence adjacency as evidence for one, or even two,
implicit discourse relations, as in
(10) And some investors fault Mr. Spiegel?s life
style; [Implicit = because, for instance] he
earns millions of dollars a year and flies
around in Columbia?s jet planes. [wsj 0179]
Here, the implicit token of because is associ-
ated with a discourse relation labelled CONTIN-
GENCY.CAUSE.REASON, while the implicit token
of for instance is associated with one labelled EX-
PANSION.RESTATEMENT.SPECIFICATION.
The question is whether sentence adjacency could
also serve as evidence for a distinct discourse rela-
tion, even when there is also an explicit discourse
adverbial, as in the following three instances of in-
stead. Here, Ex. 11 can be paraphrased as And in-
stead, Ex. 12 as But instead, and Ex.13 as So in-
stead.
(11) But many banks are turning away from strict
price competition. Instead, they are trying to
47
build customer loyalty by bundling their ser-
vices into packages and targeting them to small
segments of the population. [wsj 0085]
(12) The tension was evident on Wednesday
evening during Mr. Nixon?s final banquet
toast, normally an opportunity for reciting plat-
itudes about eternal friendship. Instead, Mr.
Nixon reminded his host, Chinese President
Yang Shangkun, that Americans haven?t for-
given China?s leaders for the military assault
of June 3-4 that killed hundreds, and perhaps
thousands, of demonstrators. [wsj 0093]
(13) Since stars are considerably more massive than
planets, such wobbles are small and hard to
see directly. Instead, Dr Marcy and others like
him look for changes that the wobbles cause in
the wavelength of the light from the star. [The
Economist, 10 November 2007]
These examples suggest that the presence of an
explicit connective should not, in all cases, be con-
sidered evidence for the absense of an implicit con-
nective. Once the set of explicit connectives have
been identified that can co-occur with each other
(including for example and for instance, as well as
instead), automated parsers for coherence relations
can be made to consider the presence of an implicit
connective whenever one of these is seen.
4.2 Variability in discourse annotation
Another issue relates to variability in annotating dis-
course structure: Inter-annotator agreement can be
very low in annotating pragmatic and discourse-
related phenomena. While we will illustrate the
point here in terms of annotating coherence rela-
tions, for other examples, the general point is illus-
trated in papers from the DGfS Workshop on Beyond
Semantics4 and in an upcoming special issue of the
journal Discourse and Dialogue devoted to the same
topic.
The Penn Wall Street Journal corpus contains
twenty-four (24) reports of errata in previously-
appearing articles. Twenty-three (23) consist of a
single pair of sentences, with no explicit discourse
connective signalling the relation between them.5
4http://www.linguistics.ruhr-uni-bochum.de/beyondsem/
5The other report contains three sentences, again with no
explicit connectives.
One sentence reports the error, and the other, the cor-
rect statement ? e.g.
(14) VIACOM Inc.?s loss narrowed to $21.7 million
in the third quarter from $56.9 million a year
ago. Thursday?s edition misstated the narrow-
ing. [wsj 1747]
In twenty of the errata (class C1), the correct
statement is given in the first sentence and the er-
ror, in the second; In the other three (class C2), it is
the other way around. One might think that the two
sentences in the twenty C1 reports would be anno-
tated as having the same discourse relation holding
between them, and the same with the two sentences
in the three C2 reports. But that is not the case: The
twenty C1 reports presented to annotators at differ-
ent times ended up being labelled with six different
discourse relations. There was even variability in
labelling the three members of the C2 class: They
were labelled with one discourse relation, and one
with a completely different one.
What should one conclude from this variability?
One possibility is that there is one right answer,
and annotators just vary in their ability to iden-
tify it. This would mean it would be beneficial to
have a large troop of annotators (so that the major-
ity view could prevail). Another possibility is that
there is more than one right answer, which would
imply multi-label classification so that multiple la-
bels could hold to different degrees. A third possi-
bility reflects the view from Beyond Semantics that
it is often very hard to transfer results from theoreti-
cal linguistics based on toy examples to naturally-
occurring texts. In this case, variability is a con-
sequence of the still exploratory nature of much
discourse annotation. In the case of errata, while
clearly some relation holds between the pair of sen-
tences, it may actually not be any of those used in
annotating the PDTB. That is, as Grosz and Sidner
(1986b) argued several years ago, the sentences may
only be related by their communicative intentions ?
one sentence intended to draw the reader?s attention
to the specific error that was made (so that the reader
knows what was mis-stated), the other intended to
correct it. One might then take the sense annotation
of discourse relations as still exploratory in the wide
range of corpora being annotated with this informa-
tion (cf. Section 3.2).
48
4.3 Systematic relations between discourse
structures
Fortunately for approaches to automated discourse
structure recognition, the lack of isomorphism be-
tween different discourse structures does not neces-
sarily mean that they are completely independent.
This belief that different aspects of discourse would
be related, is what led Grosz and Sidner (1986b) to
propose a theory that linked what they called the in-
tentional structure of discourse, with its linguistic
structure and with the reader or listener?s cognitive
attentional structure.
With respect to the different types of discourse
structure considered here, (Lin, 2012) has consid-
ered the possibility of systematic relations between
Teufel?s Argumentative Zone labelling of scientific
texts in a corpus developed for her PhD thesis
(Teufel, 1999) and PDTB-style discourse relations,
both within and across sentences. This is certainly
worth additional study, for the value it can bring to
automated methods of discourse structure recogni-
tion.
4.4 Intentional structure
When computational discourse processing turned
to machine learning methods based on reliably-
identifiable features, it abandoned (at least temporar-
ily) the centrality of pragmatics and speaker in-
tentions to discourse. That is, there were few or
no features that directly indicated or could serve
as reliable proxies for what role speaker intended
his/her utterance to play in the larger discourse. But
both Niekrasz? work on meeting segmentation (Sec-
tion 4.1) and the discussion in Section 4.2 of errata
and variability in their labelling draws new attention
to this old question, and not just to Moore and Pol-
lack?s observation (Section 3) that intentional and
informational characterizations may confer differ-
ent, non-isomorphic structures over a text. It may
also be the case that neither structure may provide a
complete cover: A new visit is warranted.
4.5 Discourse and inference
Not only were intentions abandoned in the move
to data-intensive methods, so was inference and is-
sues of how readers and listeners recover informa-
tion that isn?t explicit. What?s missing can be an
unmentioned event, with classic examples coming
from the restaurant script (Lehnert, 1977), where
someone enters a restaurant, sits down at a ta-
ble and gives their order to a waiter, where un-
mentioned inter alia is an event in which the per-
son becomes informed of what items the restaurant
has to offer, say through being given a menu. Or
it can be an unmentioned fact, such as that pro-
gram trading involves the computer-executed trad-
ing of a basket of fifteen or more stocks. The
latter explains the annotation of an implicit EX-
PANSION.RESTATEMENT.GENERALIZATION rela-
tion between the two sentences in
(15) ?You?re not going to stop the idea of trading a
basket of stocks,? says Vanderbilt?s Prof. Stoll.
?Program trading is here to stay, and computers
are here to stay, and we just need to understand
it.? [wsj 0118]
The problem here with inference is when labelling
an implicit coherence relation requires inferred in-
formation about its arguments, those arguments may
have quite different features than when all the infor-
mation needed to label the relation is explicit.
5 Conclusion
There are still large challenges ahead for compu-
tational discourse modelling. But we are hopeful
that greater openness to how information is con-
veyed through discourse, as well as richer modelling
techniques developed for other problems, will allow
needed progress to be made. If we can improve sys-
tem performance in recognizing the roles that utter-
ances are meant to play in discourse in one genre,
perhaps it will help us generalize and transport this
intention recognition between genres. We also hope
for progress in finding more ways to take advantage
of unannotated data in discourse research; in un-
derstanding more about inter-dependencies between
features of different types of discourse structure; in
continuing to carry out related computational dis-
course research and development in multiple lan-
guages and genres, so as to widen the access to the
knowledge gained; and in exploiting discourse in
Language Technology applications, including infor-
mation extraction and SMT.
49
References
Shashank Agarwal and Hong Yu. 2009. Automati-
cally classifying sentences in full-text biomedical arti-
cles into introduction, methods, results and discussion.
Bioinformatics, 25(23):3174?3180.
Amal Al-Saif and Katja Markert. 2010. The Leeds Ara-
bic Discourse Treebank: Annotating discourse con-
nectives for Arabic. In Proceedings, 7th International
Conference on Language Resources and Evaluation
(LREC 2010).
Amal Al-Saif and Katja Markert. 2011. Modelling dis-
course relations for Arabic. In Proceedings, Empirical
Methods in Natural Language Processing, pages 736?
747.
Nicholas Asher and Alex Lascarides. 2003. Logics
of Conversation. Cambridge University Press, Cam-
bridge UK.
Yves Bestgen. 2006. Improving text segmentation us-
ing Latent Semantic Analysis: A reanalysis of Choi,
Wiemer-Hastings, and Moore (2001). Computational
Linguistics, 32(1):5?12.
Floris Bex and Bart Verheij. 2010. Story schemes for
argumentation about the facts of a crime. In Proceed-
ings, AAAI Fall Symposium on Computational Narra-
tives, Menlo Park CA. AAAI Press.
Susan E. Brennan, Marilyn Walker Friedman, and Carl J.
Pollard. 1987. A centering approach to pronouns. In
Proceedings of the 25th Annual Meeting, Association
for Computational Linguistics, pages 155?162, Stan-
ford University, Stanford CA.
Matthias Buch-Kromann and I?rn Korzen. 2010. The
unified annotation of syntax and discourse in the
Copenhagen Dependency Treebanks. In Proceedings
of the Fourth Linguistic Annotation Workshop, pages
127?131, July.
Matthias Buch-Kromann, I?rn Korzen, and Henrik H?eg
Mu?ller. 2009. Uncovering the ?lost? structure of
translations with parallel treebanks. In Fabio Alves,
Susanne Go?pferich, and Inger Mees, editors, Copen-
hagen Studies of Language: Methodology, Technol-
ogy and Innovation in Translation Process Research,
Copenhagen Studies of Language, vol. 38, pages 199?
224. Copenhagen Business School.
Jill Burstein and Martin Chodorow. 2010. Progress and
new directions in technology for automated essay eval-
uation. In R Kaplan, editor, The Oxford Handbook of
Applied Linguistics, pages 487?497. Oxford Univer-
sity Press, 2 edition.
Jill Burstein, Daniel Marcu, and Kevin Knight. 2003.
Finding the WRITE stuff: Automatic identification of
discourse structure in student essays. IEEE Intelligent
Systems: Special Issue on Advances in Natural Lan-
guage Processing, 18:32?39.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of bleu in ma-
chine translation research. In 11th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, pages 249?256, Trento, Italy.
Chris Callison-Burch. 2008. Syntactic constraints
on paraphrases extracted from parallel corpora. In
EMNLP ?08: Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski.
2003. Building a discourse-tagged corpus in the
framework of rhetorical structure theory. In J. van
Kuppevelt & R. Smith, editor, Current Directions in
Discourse and Dialogue. Kluwer, New York.
Nathanael Chambers and Dan Jurafsky. 2008. Unsuper-
vised learning of narrative event chains. In Proceed-
ings, Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 789?797.
Eugene Charniak and Micha Elsner. 2009. Em works
for pronoun anaphora resolution. In Proc. European
Chapter of the Association for Computational Linguis-
tics.
Harr Chen, S. R. K. Branavan, Regina Barzilay, and
David Karger. 2009. Global models of document
structure using latent permutations. In Proceedings,
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL), pages 371?379.
Freddy Y. Y. Choi, Peter Wiemer-Hastings, and Johanna
Moore. 2001. Latent Semantic Analysis for text seg-
mentation. In EMNLP ?01: Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 109?117.
Grace Chung. 2009. Sentence retrieval for abstracts of
randomized controlled trials. BMC Medical Informat-
ics and Decision Making, 10(9), February.
Iria da Cunha, Juan-Manuel Torres-Moreno, and Gerardo
Sierra. 2011. On the development of the rst spanish
treebank. In Proc. 5th Linguistic Annotation Work-
shop, pages 1?10, Portland OR.
Quang Xuan Do, Yee Seng Chan, and Dan Roth. 2011.
Minimally supervised event causality identification.
In Proceedings, Conference on Empirical Methods in
Natural Language Processing, pages 294?303.
James Eales, Robert Stevens, and David Robertson.
2008. Full-text mining: Linking practice, protocols
and articles in biological research. In Proceedings of
the BioLink SIG, ISMB 2008.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In EMNLP ?08:
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 334?343.
50
Robert Elwell and Jason Baldridge. 2008. Dis-
course connective argument identication with connec-
tive specic rankers. In Proceedings of the IEEE Con-
ference on Semantic Computing (ICSC-08).
Mark Finlayson. 2009. Deriving narrative morphologies
via analogical story merging. In Proceedings, 2nd In-
ternational Conference on Analogy, pages 127?136.
Katherine Forbes-Riley, Bonnie Webber, and Aravind
Joshi. 2006. Computing discourse semantics: The
predicate-argument semantics of discourse connec-
tives in D-LTAG. Journal of Semantics, 23:55?106.
George Foster, Pierre Isabelle, and Roland Kuhn. 2010.
Translating structured documents. In Proceedings of
AMTA.
Michel Galley, Kathleen McKeown, Eric Fosler-Lussier,
and Hongyan Jing. 2003. Discourse segmentation of
multi-party conversation. In Proceedings of the 41st
Annual Conference of the Association for Computa-
tional Linguistics.
Sucheta Ghosh, Sara Tonelli, Giuseppe Riccardi, and
Richard Johansson. 2011. End-to-end discourse
parser evaluation. In Proceedings, IEEE Conference
on Semantic Computing (ICSC-11).
Barbara Grosz and Candace Sidner. 1986a. Attention,
intention and the structure of discourse. Computa-
tional Linguistics, 12(3):175?204.
Barbara Grosz and Candace Sidner. 1986b. Attention,
intention and the structure of discourse. Computa-
tional Linguistics, 12(3):175?204.
Barbara Grosz and Candace Sidner. 1990. Plans for dis-
course. In Philip Cohen, Jerry Morgan, and Martha
Pollack, editors, Intentions in Communication, pages
417?444. MIT Press.
Barbara Grosz, Aravind Joshi, and Scott Weinstein.
1986. Towards a computational theory of dis-
course interpretation. Widely circulated unpublished
manuscript.
Barbara Grosz, Aravind Joshi, and Scott Weinstein.
1995. Centering: A framework for modelling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
Yufan Guo, Anna Korhonen, Maria Liakata, Ilona Silins,
Lin Sun, and Ulla Stenius. 2010. Identifying the infor-
mation structure of scientific abstracts. In Proceedings
of the 2010 BioNLP Workshop, July.
Marti Hearst. 1997. TextTiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33?64.
Kenji Hirohata, Naoki Okazaki, Sophia Ananiadou, and
Mitsuru Ishizuka. 2008. Identifying sections in scien-
tic abstracts using conditional random fields. In Pro-
ceedings of the 3rd International Joint Conference on
Natural Language Processing, pages 381?388.
W. Lewis Johnson and Jeff Rickel. 2000. Animated ped-
agogical agents: Face-to-face interaction in interactive
learning environments. Int?l J. Artificial Intelligence
in Education, 11:47?78.
Dan Jurafsky and James Martin. 2009. Speech and Lan-
guage Processing. Prentice-Hall, Englewood Cliffs
NJ, 2 edition.
Min-Yen Kan, Judith Klavans, and Kathleen McKeown.
1998. Linear segmentation and segment significance.
In Proceedings of the Sixth Workshop on Very Large
Corpora.
Andrew Kehler. 1997. Current theories of centering for
pronoun intepretation: A critical evaluation. Compu-
tational Linguistics, 23(3):467?475.
Rodger Kibble and Richard Power. 2000. An integrated
framework for text planning and pronominalisation. In
Proc. of the First International Conference on Natural
Language Generation, pages 77?84, Mitzpe Ramon,
Israel, June.
Su Nam Kim, David Martinez, and Lawrence Cavedon.
2010. Automatic classification of sentences for evi-
dence based medicine. In Proc. ACM 4th Int?l Work-
shop on Data and Text Mining in Biomedical informat-
ics, pages 13?22.
Alistair Knott, Jon Oberlander, Mick O?Donnell, and
Chris Mellish. 2001. Beyond elaboration: The
interaction of relations and focus in coherent text.
In T Sanders, J Schilperoord, and W Spooren, ed-
itors, Text Representation:Linguistic and psycholin-
guistic aspects, pages 181?196. John Benjamins Pub-
lishing.
Wendy Lehnert. 1977. A conceptual theory of question
answering. In Proc 5th International Joint Conference
on Artificial Intelligence, pages 158?164.
Maria Liakata, Simone Teufel, Advaith Siddharthan, and
Colin Batchelor. 2010. Corpora for the conceptuali-
sation and zoning of scientific papers. In Proceedings
of the 7th International Conference on Language Re-
sources and Evaluation (LREC 2010).
Jimmy Lin, Damianos Karakos, Dina Demner-Fushman,
and Sanjeev Khudanpur. 2006. Generative content
models for structural analysis of medical abstracts. In
Proceedings of the HLT-NAACL Workshop on BioNLP,
pages 65?72.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan.
2010. A PDTB-styled end-to-end discourse
parser. Technical report, Department of Com-
puting, National University of Singapore, November.
http://arxiv.org/abs/1011.0835.
Ziheng Lin. 2012. Discourse Parsing: Inferring Dis-
course Structure, Modelling Coherence, and its Appli-
cations. Ph.D. thesis, National University of Singa-
pore.
51
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting of
the Association for Computational Linguistics.
Jean Mandler. 1984. Stories, scripts, and scenes: As-
pects of schema theory. Lawrence Erlbaum Asso-
ciates, Hillsdale NJ.
William Mann and Sandra Thompson. 1988. Rhetorical
Structure Theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
Daniel Marcu. 2000a. The rhetorical parsing of unre-
stricted texts: A surface-based approach. Computa-
tional Linguistics, 26(3):395?448.
Daniel Marcu. 2000b. The theory and practice of dis-
course parsing and summarization. MIT Press.
Mstislav Maslennikov and Tat-Seng Chua. 2007. A
multi-resolution framework for information extraction
from free text. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 592?599. Association for Computational
Linguistics.
Larry McKnight and Padmini Srinivasan. 2003. Cate-
gorization of sentence types in medical abstracts. In
Proceedings of the AMIA Annual Symposium, pages
440?444.
Yoko Mizuta, Anna Korhonen, Tony Mullen, and Nigel
Collier. 2006. Zone analysis in biology articles as a
basis for information extraction. International Journal
of Medical Informatics, 75:468487.
Lucie Mladova?, S?a?rka Zika?nova?, and Eva Hajic?ova?.
2008. From sentence to discourse: Building an an-
notation scheme for discourse based on the Prague De-
pendency Treebank. In Proceedings of the 6th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC 2008).
Johanna Moore and Martha Pollack. 1992. A problem
for RST: The need for multi-level discouse analysis.
Computational Linguistics, 18(4):537?544.
Johanna Moore. 1995. Participating in Explanatory Di-
alogues. MIT Press, Cambridge MA.
Megan Moser and Johanna Moore. 1996. Toward a syn-
thesis of two accounts of discourse structure. Compu-
tational Linguistics, 22(3):409?419.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first 15 years. In Proc. 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1396?1411, Uppsala, Sweden.
John Niekrasz. 2012. Toward Summarization of Com-
municative Activities in Spoken Conversation. Ph.D.
thesis, University of Edinburgh.
Umangi Oza, Rashmi Prasad, Sudheer Kolachina,
Dipti Misra Sharma, and Aravind Joshi. 2009. The
hindi discourse relation bank. In Proc. 3rd ACL Lan-
guage Annotation Workshop (LAW III), Singapore, Au-
gust.
Raquel Mochales Palau and Marie-Francine Moens.
2009. Argumentation mining: the detection, classifi-
cation and structure of arguments in text. In Proc. 12th
International Conference on Artificial Intelligence and
Law, ICAIL ?09, pages 98?107. ACM.
Martha Palmer, Carl Weir, Rebecca Passonneau, and Tim
Finin. 1993. The kernal text understanding system.
Artificial Intelligence, 63:17?68.
Thiago Alexandre Salgueiro Pardo, Maria das Gracas
Volpe Nunes, and Lucia Helena Machado Rino. 2008.
Dizer: An automatic discourse analyzer for brazilian
portuguese. Lecture Notes in Artificial Intelligence,
3171:224?234.
Renate Pasch, Ursula Brausse, Eva Breindl, and Ulrich
Wassner. 2003. Handbuch der deutschen Konnek-
toren. Walter de Gruyter, Berlin.
Siddharth Patwardhan and Ellen Riloff. 2007. Effective
information extraction with semantic affinity patterns
and relevant regions. In Proceedings of the 2007 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-07).
Emily Pitler, Annie Louis, and Ani Nenkova. 2010.
Automatic evaluation of linguistic quality in multi-
document summarization. In Proc., 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 544?554, Uppsala, Sweden.
Massimo Poesio, Rosemary Stevenson, Barbara Di Eu-
genio, and Janet Hitzeman. 2004. Centering: A para-
metric theory and its instantiations. Computational
Linguistics, 30(3):300?363.
Livia Polanyi and Martin H. van den Berg. 1996.
Discourse structure and discourse interpretation. In
P. Dekker and M. Stokhof, editors, Proceedings of the
Tenth Amsterdam Colloquium, pages 113?131, Uni-
versity of Amsterdam.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, and et al 2008. The Penn Discourse Treebank
2.0. In Proceedings of the 6th International Confer-
ence on Language Resources and Evaluation.
Rashmi Prasad, Aravind Joshi, and Bonnie Webber.
2010a. Exploiting scope for shallow discourse pars-
ing. In Proceedings of the 7th International Confer-
ence on Language Resources and Evaluation (LREC
2010).
Rashmi Prasad, Aravind Joshi, and Bonnie Webber.
2010b. Realization of discourse relations by other
means: Alternative lexicalizations. In Proceed-
ings, International Conf. on Computational Linguis-
tics (COLING).
Rashmi Prasad, Susan McRoy, Nadya Frid, Aravind
Joshi, and Hong Yu. 2011. The Biomedical Discourse
52
Relation Bank. BMC Bioinformatics, 12(188):18
pages. http://www.biomedcentral.com/1471-
2015/12/188.
Matthew Purver, Tom Griffiths, K.P. Ko?rding, and Joshua
Tenenbaum. 2006. Unsupervised topic modelling for
multi-party spoken discourse. In Proceedings, Inter-
national Conf. on Computational Linguistics (COL-
ING) and the Annual Meeting of the Association for
Computational Linguistics, pages 17?24.
Matthew Purver. 2011. Topic segmentation. In Gokhan
Tur and Renato de Mori, editors, Spoken Language
Understanding: Systems for Extracting Semantic In-
formation from Speech. Wiley, Hoboken NJ.
Patrick Ruch, Celia Boyer, Christine Chichester, Imad
Tbahriti, Antoine Geissbu?hler, Paul Fabry, and et al
2007. Using argumentation to extract key sentences
from biomedical abstracts. International Journal of
Medical Informatics, 76(2?3):195?200.
David Rumelhart. 1975. Notes on a schema for stories.
In Dan Bobrow and Alan Collins, editors, Representa-
tion and Understanding: Studies in Cognitive Science.
Academic Press, New York.
Magdalena Rysova. 2012. Alternative lexicalizations of
discourse connectives in czech. In Proc. 8th Int?l Conf.
Language Resources and Evaluation (LREC 2012).
Roger Schank and Robert Abelson. 1977. Scripts, Plans,
Goals and Understanding: an Inquiry into Human
Knowledge Structures. Lawrence Erlbaum, Hillsdale
NJ.
Penni Sibun. 1992. Generating text without trees. Com-
putational Intelligence, 8(1):102?122.
Manfred Stede. 2004. The Potsdam Commentary Cor-
pus. In ACL Workshop on Discourse Annotation,
Barcelona, Spain, July.
Manfred Stede. 2008. RST revisted: Disentangling nu-
clearity. In Cathrine Fabricius-Hansen and Wiebke
Ramm, editors, ?Subordination? versus ?Coordination?
in Sentence and Text, pages 33?59. John Benjamins,
Amsterdam.
Manfred Stede. 2012. Discourse Processing. Morgan &
Claypool Publishers.
Michael Strube. 2007. Corpus-based and ma-
chine learning approaches to anaphora resolution.
In Monika Schwarz-Friesel, Manfred Consten, and
Mareile Knees, editors, Anaphors in Text: Cognitive,
formal and applied approaches to anaphoric refer-
ence, pages 207?222. John Benjamins Publishing.
Joel Tetreault. 2001. A corpus-based evaluation of cen-
tering and pronoun resolution. Computational Lin-
guistics, 27(4):507?520.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles - experiments with relevance and
rhetorical status. Computational Linguistics, 28:409?
445.
Simone Teufel, Advaith Siddharthan, and Colin Batche-
lor. 2009. Towards discipline-independent argumen-
tative zoning: evidence from chemistry and compu-
tational linguistics. In Proceedings, Conference on
Empirical Methods in Natural Language Processing,
pages 1493?1502.
Simone Teufel. 1999. Argumentative Zoning: Informa-
tion Extraction from Scientific Text. Ph.D. thesis, Uni-
versity of Edinburgh.
Gian Lorenzo Thione, Martin van den Berg, Livia
Polanyi, and Chris Culy. 2004. Hybrid text summa-
rization: combining external relevance measures with
structural analysis. In Proceedings of the ACL 2004
Workshop Text Summarization Branches Out.
Sara Tonelli, Guiseppe Riccardi, Rashmi Prasad, and
Aravind Joshi. 2010. Annotation of discourse re-
lations for conversational spoken dialogs. In Proc.
7th Int?l Conf. Language Resources and Evaluation
(LREC 2010).
Nynke van der Vliet, Ildiko? Berzla?novich, Gosse Bouma,
Markus Egg, and Gisela Redeker. 2011. Building a
discourse-annotated Dutch text corpus. In Bochumer
Linguistische Arbeitsberichte, pages 157?171.
Steven Vere and Timothy Bickmore. 1990. A basic
agent. Computational Intelligence, 6(1):41?60.
Yannick Versley. 2010. Discovery of ambiguous and un-
ambiguous discourse connectives via annotation pro-
jection. In Workshop on the Annotation and Exploita-
tion of Parallel Corpora (AEPC). NODALIDA.
Marilyn Walker, Aravind Joshi, and Ellen Prince. 1997.
Centering in Discourse. Oxford University Press, Ox-
ford, England.
Bonnie Webber and Aravind Joshi. 1998. Anchor-
ing a lexicalized tree-adjoining grammar for discourse.
In Coling/ACL Workshop on Discourse Relations and
Discourse Markers, pages 86?92, Montreal, Canada.
Bonnie Webber, Markus Egg, and Valia Kordoni. 2012.
Discourse structure and language technology. Natural
Language Engineering.
Ben Wellner and James Pustejovsky. 2007. Automati-
cally identifying the arguments of discourse connec-
tives. In Proceedings of the 2007 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-07).
Ben Wellner. 2008. Sequence Models and Ranking
Methods for Discourse Parsing. Ph.D. thesis, Bran-
deis University.
Janyce Wiebe. 1993. Issues in linguistic segmentation.
In Workshop on Intentionality and Structure in Dis-
course Relations, Association for Computational Lin-
guistics, pages 148?151, Ohio StateUniversity.
Terry Winograd. 1973. A procedural model of language
understanding. In Roger Schank and Ken Colby, ed-
itors, Computer Models of Thought and Language,
53
pages 152?186. W.H. Freeman. Reprinted in Grosz
et al (eds), Readings in Natural Language Processing.
Los Altos CA: Morgan Kaufmann Publishers, 1986,
pp.249-266.
Florian Wolf and Edward Gibson. 2005. Representing
discourse coherence: A corpus-based study. Compu-
tational Linguistics, 31:249?287.
William Woods. 1968. Procedural semantics for a
question-answering machine. In Proceedings of the
AFIPS National Computer Conference, pages 457?
471, Montvale NJ. AFIPS Press.
William Woods. 1978. Semantics and quantification in
natural language question answering. In Advances in
Computers, volume 17, pages 1?87. Academic Press,
New York.
Nianwen Xue. 2005. Annotating discourse connectives
in the chinese treebank. In ACL Workshop on Frontiers
in Corpus Annotation II, Ann Arbor MI.
Deniz Zeyrek and Bonnie Webber. 2008. A discourse re-
source for Turkish: Annotating discourse connectives
in the METU corpus. In Proceedings of the 6th Work-
shop on Asian Language Resources (ALR6).
Deniz Zeyrek, U?mut Deniz Turan, Cem Bozsahin, Ruket
C?ak?c?, and et al 2009. Annotating Subordinators in
the Turkish Discourse Bank. In Proceedings of the 3rd
Linguistic Annotation Workshop (LAW III).
Deniz Zeyrek, Is??n Demirs?ahin, Ay?s??g?? Sevdik-
C?all?, Hale O?gel Balaban, I?hsan Yalc??nkaya, and
U?mut Deniz Turan. 2010. The annotation scheme
of the Turkish Discourse Bank and an evaluation of
inconsistent annotations. In Proceedings of the 4th
Linguistic Annotation Workshop (LAW III).
Yuping Zhou and Nianwen Xue. 2012. Pdtb-style dis-
course annotation of chinese text. In Proc. 50th An-
nual Meeting of the ACL, Jeju Island, Korea.
54
