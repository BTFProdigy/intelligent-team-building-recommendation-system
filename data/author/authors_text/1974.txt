Using Co-occurrence Statistics as an Information Source 
for Partial Parsing of Chinese 
Elliott Franeo DRABEK 
The State Key Laboratory 
for Intelligent Technology and Systems 
Department ofComputer Science 
Tsinghua University, Beijing 100084 
elliott_drabek@ACM.org 
Qiang ZHOU 
The State Key Laboratory 
for Intelligent Technology and Systems 
Department ofComputer Science 
Tsinghua University, Beijing 100084 
zhouq@slOOOe.cs.tsinghua.edu.cn 
Abstract 
Our partial parser for Chinese uses a learned 
classifier to guide a bottom-up parsing 
process. We describe improvements in
performance obtained by expanding the 
information available to the classifier, from 
POS sequences only, to include measures of 
word association derived from 
co-occurrence statistics. We compare 
performance using different measures of 
association, and find that Yule's coefficient 
of colligation Y gives somewhat better 
results over other measures. 
Introduction 
In learning-based approaches to syntactic 
parsing, the earliest models developed generally 
ignored the individual identities of words, 
making decisions based only on their 
part-of-speech classes. On the othor hand, 
many later models see each word as a 
monolithic entity, with parameters estimated 
separately for each word type. In between have 
been models which auempt o generalize by 
considering similarity between words, where 
knowledge about similarity is deduced fi'om 
hand-written sources (e.g. thesauri), or induced 
from text. For example, The SPATTER parser 
(Magerman, 1995) makes use of the output of a 
clustering algorithm based on co-occurrence 
information. Because this co-occurrence 
information can be derived from inexpensive 
data with a minimum of pre-processing, it can be 
very inclusive and informative about even 
relatively rare words, thus increasing the 
generalization capability of the parser trained on 
a much smaller fully annotated corpus. 
The cunent work is in this spirit, making 
complementary use of a relatively small 
treebank for syntactic information and a 
relatively large collection of flat text for 
co-occurrence information. However, we do 
not use any kind of clustering, instead using the 
co-occurrence data directly. Our parser is a 
bottom-up arser whose actions are guided by a 
machine-learning-based decision-making 
module (we use the SNoW learner developed at 
the University of Illinois, Urbana..Champaign 
(Roth, 1998) for its strength with potentially 
very large feature sets and for its ease of use). 
The learner is able to directly use statistics 
derived from the co-occu~euce data to guide its 
decisions. 
We collect a variety of statistical measures of 
association based on bigram co-occurrence data 
(specifically, mutual information, t-score, X 2, 
likelihood ratio and Yule's coefficient of 
colligation Y), and make the statistics available 
to the decision-making module. We use 
labelled constituent precision and recall to 
compare performance of different versions of 
our parser on unseen test data. We observe a
marked improvement in some of the versions 
using the co-occurrence data, with strongest 
performance observed in the versions using 
Yule's coefficient of  colligation Y and mutual 
information, and more modest improvements in 
those using the other measures. 
1 Background 
1.I Our Task m Partial Parsing 
The current work has developed inthe context 
of developing a partial or "chunk" parser for 
Chinese, whose task is to identify certain kinds 
of local syntactic structure. The syntactic 
22 
analysis we use largely follows the outline of 
Steven Abney's work (Abney, 1994). We 
adopt he concept of a "e-head" and an "s-head" 
for each phrase, where the e-head corresponds 
roughly to the generally used concept of head 
(e.g., the main verb in a verb phrase, or the 
preposition in a prepositional phrase), and the 
s-bead is the "main content word" of a phrase 
(e.g., the main verb in a verb phrase, but the 
object of the preposition in a prepositional 
phrase). The core of our chunk definition is also 
in line with Abney's: A chunk is essentially the 
contiguous range of words s-headed by a given 
major content word. Within this basic 
framework, we make some aecorunaodations to 
the Chinese language and to practicality. For 
example, by our understanding of Abney's 
definition, a numeral-classifier phrase followed 
immediately by the noun it modifies should 
constitute two separate chunks. However such 
units seem likely to be useful in further 
processing, and easy to accurately identify, so 
we chose to include them in our definition of 
chunk. 
For simplicity and consistency, we adopt a 
very restricted phrase-structured syntactic 
formalism, somewhat similar to a 
phrase-structured formulation of a dependency 
grammar. In our formalism, all constituents are 
bina_ry branching, and the purpose of the 
non-terminal labels is restricted to indicating the 
direction of dependency between the two 
children. Figure 1 shows an example sentence 
with some indicative structures. 
Dependencies within individual chunks are 
shown with heavy arrows. A fight-pointing 
dependency, such as the three dependencies 
within the noun phrase " ) l~ .~t :~y~r~" ,  
corresponds to a constituent labelled 
"right-headed". A left-pointing dependency, 
such as that between the verb "~,.~\[~" and its 
aspect particle "T ' ,  corresponds toa constituent 
labelled "left-headed". These are cases where 
the s-head and the e-head of the phrase are 
identical. When they are not identical, we 
have a "two-headed" ependency, like those in 
the phrase "~_L \ [~ ' .  Here, the relation 
between "~"  and ".J~" (and between "~ 
_.L" and "~")  is that the left constituent 
provides the s-head of the phrase, while the right 
constituent provides the e-head. 
These four non-terminal categories can 
descn'be high- or low- level syntactic structures. 
However, for chunking we wish to leave the 
higher-level structures of a sentence unspecified, 
leaving only a list of local structares. We treat 
this in a consistent way by adding a fifth 
non-terminM category "unspecified", and 
replacing all higher str~tures with a backbone 
of strictly left-branching "unspecified" nodes, 
anchored to a special "wall" token to the left of 
the sentence. This backbone structure is shown 
by the light lines in the figure. 
1.2  Our  Data  Sources  ~ One Large and One 
Small 
During development, we made use of two 
corpora. The first is a relatively small-scale 
treebank of approximately 3500 sentences, 
39,000 words, and 55,000 characters (Zhou, 
1996). We transformed this corpus by annotating 
each phrase with c-heads and s-heads, using a 
large collection of hand-written rules, and then 
extracted chunks from this transformed version. 
The second corpus, which we use only as a 
source of co-occurrence statistics, ismuch larger, 
with approximately 67,000 sentences, 1.5 
million words, and 2.2 million characters, with 
sentences eparated and words separated and 
marked with parts-of-speech, but with no further 
syntactic annotation (Zhou and Sun, 1999). In 
the current work we make no use of the 
part-of-speech annotation, taking co-occurrence 
? counts of word-types alone. 
1.3 Our Framework - -  C lass i f ier -Guided 
Shi f t -Reduce Pars ing 
The parsing framework we use has been 
chosen for maximum simplicity, aided by the 
simplicity of the syntactic framework. In 
parsing, we model a left-to-right shift-reduce 
automaton which builds a parse-tree 
constituent-by-constituent in a deterministic 
left--to-right process. The parsing process is 
thus reduced to making a series of decisions of 
exactly what to build. 
For training, we extract the series of actions 
the shift-reduce parser would have had to make 
to produce the trees from the surface structure of 
the sentences. This gives a long series of 
state-action pairs: "when the parser was in 
state X, it took action Y'. The state description 
X is set of binary predicates describing the local 
surface structure of the sentence and the contents 
23 
cucurbit vegetable raising methods already occur lperf.\] foundation on \[rel.\] change 
Methods of raising cucurbit vegetables have changed fundamentally. 
Figure 1. An example sentence annotated according to our system. 
of the stack. We describe these predicates in 
detail below. This series of state-action pairs is 
presented to the SNoW learner, which tries to 
learn to predict the parser actions from the 
parser states, attempting to find a linear 
diserimin:mt over these binary predicates which 
best accounts for the corresponding actions in 
the training data. 
These parse actions can be either "shift a word 
from the right on to the stack", or "reduce the 
? , top elements of the stack" into a single 
constituent. Because our syntactic framework 
is strictly binary branching, each reduce action 
operates on exactly the top two items on the 
stack, so the automaton eed only choose a 
category for the new constituent. This decision 
turns out to be nearly trivial, and we were able to 
achieve 100% accuracy on our test set using 
only part-of-speech information, so in the 
remainder of this paper we discuss only issues 
relating to the more difficult decision of whether 
to shift or reduce. 
Within the shift-reduce decisions, over half 
are pre<letermined by the basic requirements of
the framework. For example, if there are no 
words left to shift, we can only reduce. If there 
is only one item on the stack, we can only shift. 
These decisions are handled by simple 
deterministic rules within the parser and are not 
shown to the classifier either in training or in 
parsing. 
In the first version of the parser, prior to the 
introduction of co-occurrence statistics, the 
information available to the classifier is limited 
to parts-of-speech of words in the surface 
structure of the sentence, nonterminal categories 
of constituents already built on the stack, and 
parts-of-speech of the s- and e-heads of 
constituents already built on the stack. These 
are collected into schemas representing sets of 
poss~le binary predicates. Table 1 shows a 
representative subset of this original set of 18 
predicate schemas (space does not allow us to 
present all of them). The total of all the 
instantiations of all these templates presents a
potentially huge feature set, so we rely on an 
important property of the SNoW architecture, 
that it can handle an indefinitely large set of 
24 
Pred icate  Schema 
POS (Surface-word \[k\] ) = t 
Range o f  
Parameters  
-I K k_< 2 
POS(Sur face-word \ [k \ ] )  = tl /~ POS(Sur face-word \ [k  + I\]) = t2 -2 ~ k ~ 1 
Category(Stack \ [k \ ]  ) = c 0 ~ k ~ 1 
Category(Stack \ [k \ ]  ) = ci /k Category  (Stack \[k + I\] ) = c2 0 ~ k -< 1 
POS(S-head(Stack \ [k \ ] ) )  = t 0 ~ k ~ 2 
POS(S-head(Stack \ [k \ ] ) )  =t l /k  POS(S-head(Stack \ [k+ I\])) =t2  0 ~ k -< 1 
POS(S-head(Stack \ [kx \ ]  )) = tx /k POS (Sur face-word \ [k2 \ ] )  = t2 0 --< kx ~ 1 
- I~  k2-<0 
Category(Stack\[kx\] )  = c /k POS(C-head(Stack\[kx\]) )  = t~ /~ 
POS (Surface-word \[k2\] ) = t2. 
0-< kz~ 1 
- i~  k2-<0 
Table 1. A Subset of the Feature Schemas in the Original Version of the Parser. The variables t, tt, 
and t2 range over the set of part-of-speech categories, while the variables c, et, and c2 range over the 
set of non-terminal categories. Surface words are indexed relative to the parsing position, such that 
Surface-word\[O\] is the next word to be shifted. 
features, actually using only those features 
which are active. The set of these actually 
active features is reasonable for our set of 
schemas. 
2 Enriching the Feature Set with 
Co-occurrence Statistics 
statist ic(wl,  w2) ~ Xl 
s ta t i s t i c  (wl, w2) ~- X2 
rather than mutually-exclusive predicates of 
the form: 
X0 < stat ist ic(wl,  w2) ~-Xl  
X0 < stat ist ic(wl,  w2) ~ X2 
2.1 Measures of Association 
Table 2 shows the definitions of the five 
measures we have chosen to compare in the 
current work, taken from (Manning and Shfitze, 
1999), (Kageura, 1999). 
These measures are based on empirical counts 
of word occurrences and co-occurrences. 
Because these events are very prone to 
zero-counts, both for unseen bigrams and for 
unseen words, we applied Simple Good-Turing 
smoothing (Gale and Sampson, 1995) to both 
bigram and word counts. 
2.2 Making Measures of Association Available to 
the Parser 
To make the measures of association available 
to the parser, we started by discretizing each 
measure, that, is substituting for each continuous 
measurement a set of binary predicates coarsely 
describing its approximate value. We used a 
very simple form of discretization, counting 
occurrences of each value, and then dividing the 
values into bins of approximately equal counts. 
Informal exploration showed consistently better 
performance when bin membership was made 
cumulative; that is, using 
non-mutually-exclusive pr dicates of the form: 
Using these cumulative predicates, parsing 
accuracy consistently improved with increases in 
the number of bins, though the rate of 
improvement slowed at the same time. The 
cost of increasing the number of bins came 
primarily in the algorithm's training time. We 
chose thirty-two to be a good number of bins. 
The predicates resulting ~om discretization 
are predicates over values of a statistic. To 
apply these predicates in parsing, we created 
features relating to particular slots within 
parse-state descriptions. Specifically, we made 
three new feature schemas available to the 
Winnow learner, as shown in Table 3. Each of 
these feature schemas is an extension to one 
available to the original parser. In each case, 
the original schema was of the form: 
POS(wl) = t~ A POS(w2) = t2 
And the extended schema was of the form: 
POS(wl) = tl A POS(w2) = t2 A 
stat ist ic(wl,  w2) ~ X 
In this way, the learner is able to condition 
separately depending on the parts-of-speech of 
the two words in question. This is based on the 
intuitions that different eases for part-of-speech 
combinations would behave veery differently, 
and that the training data was sufficient that 
25 
Measure  Definition 
Mx lo~- c(w~'w2) 
c(w. . )c( . ,  w2) ) 
T-score  4C(Wl, w2)f C (Wd~-2~ w2) ~ Cl, W,,W2) 1} 
x' c(.,.)(c(w. - c(w. ))2 
c( , . )c( , ,)c( . , )c( . ) 
Likeli- 
hood 
Ratio 
L I ,  c(*.-) ) ct.,-) ) j 
Note :  LogL(p; n; k) = k log(p) + (n - k) log(1 - p)  
Yule's Y .~- I  
4 +1 
Note: C(O. o)c(w,, 
C( WI, W2 )C( WI, W2) 
Table 2. Definitions of the Five Measures of Association. c(x~,wz) represents the count of the 
event that x and y occur adjacent and in this order in the training corpus. - 'w  represents ummation 
over all words other than w, and ? represents ummation over all words. 
performance would not be hurt by the resulting 
sub-division; however we have no specific 
empirical support for this. 
2.3 Experimental Results 
We trained a series of SNoW networks using 
features sets extended with each of thef ive 
measures, and tested five versions of our parser, 
One using each of the resulting networks. This 
was done on a held-out test set comprising 
approximately ten percent of our treebank. The 
resulting measurements for labeled constituent 
precision and recall are shown in Table 4, 
arranged according to the geometric mean of the 
two measurements. 
It is clear from the table that co-occurrence 
information can be made useful, and that the 
measure used to represent this information has a 
large influence on its usefulness. There is also 
a large disparity between the in~rovement in
precision, 1.7%, and the improvement in recall, 
4.1%. We con jec ture  that this is because the 
parser odg/nally tended to err in the direction of 
splitting words into separate chunks, the 
commoner case, while with the co-occurrence 
infommtion, it is able to pick out some cases 
where a strong association suggests that words 
be joined in the same chunk. 
3 Related Work 
Statistical measures of association appfied to 
bigram co-occurrence counts have been used 
most extensively in terminology and collocation 
extraction. (Manning and Shfitze, 1999) 
contains a good introduction to this topic. 
(Kageura, 1999) is an especially good empirical 
comparison of the performance of several 
measures of association on a set of tasks in both 
terminology extraction and in morpheme 
splitting of Chinese character sequences. This 
latter tasks which can be seen as a very restricted 
form of parsing, has been treated in a body of 
interesting work, including (Sun, Shen and Tsou, 
1998), (Lee, 1999) . This work has generally 
used vee/y simple heuristic ontrol policies, such 
as repeatedly splitting at the point of lowest 
mutual information. The use of similar 
26 
Pred icate  Schema 
P0S (Sur face -word \ [k \ ] )  = tz A POS (Sur face -word \ [k  + 1\] ) 
Stat ist ic(Surface-word\[k\ ]  , Sur face -word  \[k + I\] ) ~< X 
=t2  A 
Range of 
Parameters  
-2_< k~ 1 
POS(S-head(Staek \ [k \ ] ) )  = tx /k POS(S -head(Stack \ [k  + i\])) = t2 /~ 0 ~- k ~ 1 
S ta t i s t i c (S -head(Stack \ [k \ ] ) ,S -head(Stack \ [k  + i\])) ~- X 
POS (S -head(Stack  \[kz\] ) ) = tz /k POS (Sur face-word\ [k2\ ] )  
Statistic (S -head (Stack \[kl\] ) , Sur face-word  \[k2\] ) ~- X 
Table 3. Augmented Feature Schemas. 
approaches for general parsing received some 
early exploration (Brill, Magerman, Marcus and 
Santofini, 1990), (Magerman and Marcus, 1990), 
but this approach seems to have lost popularity. 
This may be because using co-occurrence 
statistics as a sole source of guidance may 
become insufficient as the object of parsing 
moves from the veery local structure of word 
splitting to the longer-distance dependencies of
general parsing. The current work attempts to 
remedy this by using a general eafing device 
to balance co-occurrence statistics with other 
information to be integrated into a larger control 
policy. 
Conclusions and Future Work 
Our experiments show that simple statistical 
information gathered ~om the unprocessed 
surface structure of large-scale text has value in 
guiding parsing decisions. However, we feel 
that there is still a great deal of further advantage 
to be gained from this approach. Our next step 
will be to include co-oecu~ence information 
from a much larger corpus, containing on the 
order of 108 characters. 
We would also like to experiment with other 
definitions of co-occurrence. (Yuret, 1998) 
describes some very interesting work, in a 
different framework from ours, in which a parser 
using only co-occurrence mutual information 
was able to achieve a high precision but low 
recall when co-occurrence was defined as 
adjacent co-occurrence, and low precision but 
high recall when co-occurrence was defined as 
occurrence within the same sentence. We 
would like to experiment with ways of balancing 
these two measures. 
We also suspect hat significant gal.~ are 
possible through a more sophisticated inclusion 
of the statistics in the decision making process. 
The current diseretization scheme is very simple, 
but there is ample empirical evidence that 
= t2 /k 0 ~ kx -< 1 
-i ~ k2 ~ 0 
discrefization which takes into account arget 
categories can significantly improve 
classification accuracy (Dougherty, Kohavi, and 
Sahami, 1995). 
The several articles we have cited which use 
exclusively co-occurrence information to predict 
constituent boundaries are very interesting for 
the simplicity of their control structures, but in 
one important way they are more complex than 
the current work: they make decisions by 
explicitly comparing the measures of association 
between different pairs of words. We predict 
that augmenting the feature set to allow our 
parser to be sensitive to this kind of information 
would be a very valuable xtension. 
A related issue is the choice of learning 
methodology. The Winnow learner has served us 
well with its ability to handle very large feature 
sets, but it is weak in its ability to take 
advantage of the interaction between features. 
We would like to experiment with learning 
methods which do not suffer from this weakness, 
and with methods for automatic feature 
extraction which could supplement Winnow. 
We experimented with a nondeterministic 
control policy for the parser, using cost-front 
search to fred the most probable series of 
parsing decisions, but we found this not to be 
very useful. Over a series of comparative 
experiments, the non.deterministic ontrol 
policy consistently raised precision by a small 
margin, lowered recall by a small margin, 
increased run times by an order of magnitude or 
more, and for about 10% of the test.set 
sentences exhausted system resources before 
finding any parse at all. We posit that these 
problems may in part be due to the fact that 
while the Winnow learner is otherwise quite 
well adapted for our purposes, its output is not 
intended to be interpreted probabilistically. In 
the future we intend to run parallel experiments 
with more probabilisticaUy oriented learners; we 
27 
Measure of Association Precision Recall 
Yule's Y 0.882 0.875 
Geometric Mean 
0.879 
Mutual Information 0.885 0.857 0.871 
Likelihood Ratio 0.879 0.845 0.862 
X z 0.870 0.848 0.859 
T-score 0.870 0.836 0.853 
None (Ori~na! Feature Set) 0.868 0.834 0.851 
Table 4. Accuracy Measurements of Parsing with Different Measures of Association 
are espeeiaUy interested in experimenting with a 
Maximum Entropy model. 
In the larger context, we plan to experiment 
with more sophisticated, model-based 
unsupervised learning methods, including 
clustering and beyond, and ways of providing 
their gathered knowledge to the parser, to make 
the fullest possible use of the vast wealth of 
un-annotated text available. 
Acknowledgements 
The research was supported by National 
Natural Science Foundation of China (NSFC) 
(Grant No. 69903007) and National 973 
Foundation (Grant No. G 1998030507-2). 
References 
Steven Abney (1994) Parsing by Chunks. 
http://www.sfs.phil.uni-tuebingen.de/~abney/ 
Erie Brill, David Magerrnan, Mitch Marcus and B. 
Santorini (1990) Deducing Linguistic Structure 
from the Statistics of Large Corpora. 
Proceedings of the DARPA Speech and Natural 
Language Workshop, pp. 275-281. 
Kenneth W. Church and P. Hanks (1990) Word 
Associaffon Norms, Mutual Information, and 
Lexicography. Computational Linguistics, 
Computational Linguistics, 16/1, pp. 22-29. 
Ted Dunning (1993) Accurate Methods for the 
Statistics of Surprise and Coincidence. 
Computational Linguistics, Computational 
Linguistics, 19/1, pp. 61-74. 
Kyo Kageura (1999) Bigram Statistics R~isited: A
Comparative Examination of Some Statistical 
Measures in Morphological Analysis of Japanese 
Kanfi Sequences. Journal of Quantitative 
Linguistics, 6/22, pp. 149-166. 
James Dougherty, Ron Kohavi and Mehran Sahami 
(1995) Supervised and Uusupen, ised 
Discretization of Continuous Features. In 
"Machine Learning: Proceedings of the Twelfth 
International Conference", Morgan Kaufmann 
Publishers. 
William A. Gale and Geoffrey Sampson (1995) 
GoOd-Turing Frequency Estimation without Tears. 
Journal of Quantitative Linguistics, 2, pp. 217-237. 
Christopher D. Manning and Hinrich Shfitze (1999) 
Foundaffons of Statistical Natural Language 
Processing. The MIT Press, Cambridge, 
Massachusetts. 
David M. Magerman (1995) Natural Language 
Parsing as Statistical Pattern Recognition. Ph.D. 
Dissertation, Stanford University. 
David Magerman and Mitch Marcus (1990) 
Parsing a Natural Language Using Mutual 
Information Statistics. In "Proceedings, Eighth 
National Conference on Artificial Intelligence 
(AAAI 9O)". 
Adwait Ratnaparkhi (1997) A Linear 
Observed- Time Statistical Parser Based on 
Maximum Entropy Models. In "Proceedings of the 
Second Conference on Empirical Methods in 
Natural Language Processing". 
Dan Roth (1998) Learning to Resolve Natural 
Language Ambiguities, a Unified Approach. In 
"AAAI'9$ ". 
K-Y. Su, M-W. Wu, and J-S. Chang (1994) A 
Corpus-Based Approach to Automatic Compound 
Extraction. In "Proceedings of the 32rid Annual 
Meeting of the ACL", pp. 27-30. 
Maosong Sun, Dayang Shen, and Benjamin tC Tsou 
(1998) Chinese Word Segmentation without 
Using Lexicon and Hand-Crafted Training Data. 
In "Proceedings of the 36th Annual Meeting of the 
ACL", pp. 1265-1271. 
Aboy Wong, Dekai Wu (1999) Are Phrase 
Structured Grammars Useful in Statistical 
Parsing?. In "Proceedings of the Fifth Natural 
Language Processing Pacific Rim Symposium", pp. 
120-125. 
Deniz Yuret (1998) Discovery of Lexical Re~tions 
Using Lexical Attraction. Ph.D. Thesis, 
Massachusetts In titute of Technology. 
Qiang Zhou (1996) Phrase Bracketing and 
Annotating on Chinese Language Corpus. (in 
Chinese), Ph.D. Thesis, Beijing University. 
28 
Evaluation challenges in large-scale document summarization
Dragomir R. Radev
U. of Michigan
radev@umich.edu
Wai Lam
Chinese U. of Hong Kong
wlam@se.cuhk.edu.hk
Arda C?elebi
USC/ISI
ardax@isi.edu
Simone Teufel
U. of Cambridge
simone.teufel@cl.cam.ac.uk
John Blitzer
U. of Pennsylvania
blitzer@seas.upenn.edu
Danyu Liu
U. of Alabama
liudy@cis.uab.edu
Horacio Saggion
U. of Sheffield
h.saggion@dcs.shef.ac.uk
Hong Qi
U. of Michigan
hqi@umich.edu
Elliott Drabek
Johns Hopkins U.
edrabek@cs.jhu.edu
Abstract
We present a large-scale meta evaluation
of eight evaluation measures for both
single-document and multi-document
summarizers. To this end we built a
corpus consisting of (a) 100 Million auto-
matic summaries using six summarizers
and baselines at ten summary lengths in
both English and Chinese, (b) more than
10,000 manual abstracts and extracts, and
(c) 200 Million automatic document and
summary retrievals using 20 queries. We
present both qualitative and quantitative
results showing the strengths and draw-
backs of all evaluation methods and how
they rank the different summarizers.
1 Introduction
Automatic document summarization is a field that
has seen increasing attention from the NLP commu-
nity in recent years. In part, this is because sum-
marization incorporates many important aspects of
both natural language understanding and natural lan-
guage generation. In part it is because effective auto-
matic summarization would be useful in a variety of
areas. Unfortunately, evaluating automatic summa-
rization in a standard and inexpensive way is a diffi-
cult task (Mani et al, 2001). Traditional large-scale
evaluations are either too simplistic (using measures
like precision, recall, and percent agreement which
(1) don?t take chance agreement into account and (2)
don?t account for the fact that human judges don?t
agree which sentences should be in a summary) or
too expensive (an approach using manual judge-
ments can scale up to a few hundred summaries but
not to tens or hundreds of thousands).
In this paper, we present a comparison of six
summarizers as well as a meta-evaluation including
eight measures: Precision/Recall, Percent Agree-
ment, Kappa, Relative Utility, Relevance Correla-
tion, and three types of Content-Based measures
(cosine, longest common subsequence, and word
overlap). We found that while all measures tend
to rank summarizers in different orders, measures
like Kappa, Relative Utility, Relevance Correlation
and Content-Based each offer significant advantages
over the more simplistic methods.
2 Data, Annotation, and Experimental
Design
We performed our experiments on the Hong Kong
News corpus provided by the Hong Kong SAR of
the People?s Republic of China (LDC catalog num-
ber LDC2000T46). It contains 18,146 pairs of par-
allel documents in English and Chinese. The texts
are not typical news articles. The Hong Kong News-
paper mainly publishes announcements of the local
administration and descriptions of municipal events,
such as an anniversary of the fire department, or sea-
sonal festivals. We tokenized the corpus to iden-
tify headlines and sentence boundaries. For the En-
glish text, we used a lemmatizer for nouns and verbs.
We also segmented the Chinese documents using the
tool provided at http://www.mandarintools.com.
Several steps of the meta evaluation that we per-
formed involved human annotator support. First, we
Cluster 2 Meetings with foreign leaders
Cluster 46 Improving Employment Opportunities
Cluster 54 Illegal immigrants
Cluster 60 Customs staff doing good job.
Cluster 61 Permits for charitable fund raising
Cluster 62 Y2K readiness
Cluster 112 Autumn and sports carnivals
Cluster 125 Narcotics Rehabilitation
Cluster 199 Intellectual Property Rights
Cluster 241 Fire safety, building management concerns
Cluster 323 Battle against disc piracy
Cluster 398 Flu results in Health Controls
Cluster 447 Housing (Amendment) Bill Brings Assorted Improvements
Cluster 551 Natural disaster victims aided
Cluster 827 Health education for youngsters
Cluster 885 Customs combats contraband/dutiable cigarette operations
Cluster 883 Public health concerns cause food-business closings
Cluster 1014 Traffic Safety Enforcement
Cluster 1018 Flower shows
Cluster 1197 Museums: exhibits/hours
Figure 1: Twenty queries created by the LDC for
this experiment.
asked LDC to build a set of queries (Figure 1). Each
of these queries produced a cluster of relevant doc-
uments. Twenty of these clusters were used in the
experiments in this paper.
Additionally, we needed manual summaries or ex-
tracts for reference. The LDC annotators produced
summaries for each document in all clusters. In or-
der to produce human extracts, our judges also la-
beled sentences with ?relevance judgements?, which
indicate the relevance of sentence to the topic of the
document. The relevance judgements for sentences
range from 0 (irrelevant) to 10 (essential). As in
(Radev et al, 2000), in order to create an extract of
a certain length, we simply extract the top scoring
sentences that add up to that length.
For each target summary length, we produce an
extract using a summarizer or baseline. Then we
compare the output of the summarizer or baseline
with the extract produced from the human relevance
judgements. Both the summarizers and the evalua-
tion measures are described in greater detail in the
next two sections.
2.1 Summarizers and baselines
This section briefly describes the summarizers we
used in the evaluation. All summarizers take as input
a target length (n%) and a document (or cluster) split
into sentences. Their output is an n% extract of the
document (or cluster).
? MEAD (Radev et al, 2000): MEAD is
a centroid-based extractive summarizer that
scores sentences based on sentence-level and
inter-sentence features which indicate the qual-
ity of the sentence as a summary sentence. It
then chooses the top-ranked sentences for in-
clusion in the output summary. MEAD runs on
both English documents and on BIG5-encoded
Chinese. We tested the summarizer in both lan-
guages.
? WEBS (Websumm (Mani and Bloedorn,
2000)): can be used to produce generic and
query-based summaries. Websumm uses a
graph-connectivity model and operates under
the assumption that nodes which are connected
to many other nodes are likely to carry salient
information.
? SUMM (Summarist (Hovy and Lin, 1999)):
an extractive summarizer based on topic signa-
tures.
? ALGN (alignment-based): We ran a sentence
alignment algorithm (Gale and Church, 1993)
for each pair of English and Chinese stories.
We used it to automatically generate Chinese
?manual? extracts from the English manual ex-
tracts we received from LDC.
? LEAD (lead-based): n% sentences are chosen
from the beginning of the text.
? RAND (random): n% sentences are chosen at
random.
The six summarizers were run at ten different tar-
get lengths to produce more than 100 million sum-
maries (Figure 2). For the purpose of this paper, we
only focus on a small portion of the possible experi-
ments that our corpus can facilitate.
3 Summary Evaluation Techniques
We used three general types of evaluation measures:
co-selection, content-based similarity, and relevance
correlation. Co-selection measures include preci-
sion and recall of co-selected sentences, relative util-
ity (Radev et al, 2000), and Kappa (Siegel and
Castellan, 1988; Carletta, 1996). Co-selection meth-
ods have some restrictions: they only work for ex-
tractive summarizers. Two manual summaries of the
same input do not in general share many identical
sentences. We address this weakness of co-selection
Lengths #dj
05W 05S 10W 10S 20W 20S 30W 30S 40W 40S FD
E-FD - - - - - - - - - - x 40
E-LD X X X X x x X X X X - 440
E-RA X X X X x x X X X X - 440
E-MO x x X x x x X x X x - 540
E-M2 - - - - - X - - - - - 20
E-M3 - - - - - X - - - - - 8
E-S2 - - - - - X - - - - - 8
E-WS - X - X x x - X - X - 160
E-WQ - - - - - X - - - - - 10
E-LC - - - - - - x - - - - 40
E-CY - X - X - x - X - X - 120
E-AL X X X X X X X X X X - 200
E-AR X X X X X X X X X X - 200
E-AM X X X X X X X X X X - 200
C-FD - - - - - - - - - - x 40
C-LD X X X X x x X X X X - 240
C-RA X X X X x x X X X X - 240
C-MO X x X x x x X x X x - 320
C-M2 - - - - - X - - - - - 20
C-CY - X - X - x - X - X - 120
C-AL X X X X X X X X X X - 180
C-AR X X X X X X X X X X - 200
C-AM - X X X X X X X X - 120
X-FD - - - - - - - - - - x 40
X-LD X X X X x x X X X X - 240
X-RA X X X X x x X X X X - 240
X-MO X x X x x x X x X x - 320
X-M2 - - - - - X - - - - - 20
X-CY - X - X - x - X - X - 120
X-AL X X X X X X X X X X - 140
X-AR X X X X X X X X X X - 160
X-AM - X X X X X X X - X - 120
Figure 2: All runs performed (X = 20 clusters, x = 10 clusters). Language: E = English, C = Chinese,
X = cross-lingual; Summarizer: LD=LEAD, RA=RAND, WS=WEBS, WQ=WEBS-query based, etc.; S =
sentence-based, W = word-based; #dj = number of ?docjudges? (ranked lists of documents and summaries).
Target lengths above 50% are not shown in this table for lack of space. Each run is available using two
different retrieval schemes. We report results using the cross-lingual retrievals in a separate paper.
measures with several content-based similarity mea-
sures. The similarity measures we use are word
overlap, longest common subsequence, and cosine.
One advantage of similarity measures is that they
can compare manual and automatic extracts with
manual abstracts. To our knowledge, no system-
atic experiments about agreement on the task of
summary writing have been performed before. We
use similarity measures to measure interjudge agree-
ment among three judges per topic. We also ap-
ply the measures between human extracts and sum-
maries, which answers the question if human ex-
tracts are more similar to automatic extracts or to
human summaries.
The third group of evaluation measures includes
relevance correlation. It shows the relative perfor-
mance of a summary: how much the performance
of document retrieval decreases when indexing sum-
maries rather than full texts.
Task-based evaluations (e.g., SUMMAC (Mani
et al, 2001), DUC (Harman and Marcu, 2001), or
(Tombros et al, 1998) measure human performance
using the summaries for a certain task (after the
summaries are created). Although they can be a
very effective way of measuring summary quality,
task-based evaluations are prohibitively expensive at
large scales. In this project, we didn?t perform any
task-based evaluations as they would not be appro-
priate at the scale of millions of summaries.
3.1 Evaluation by sentence co-selection
For each document and target length we produce
three extracts from the three different judges, which
we label throughout as J1, J2, and J3.
We used the rates 5%, 10%, 20%, 30%, 40% for
most experiments. For some experiments, we also
consider summaries of 50%, 60%, 70%, 80% and
90% of the original length of the documents. Figure
3 shows some abbreviations for co-selection that we
will use throughout this section.
3.1.1 Precision and Recall
Precision and recall are defined as:
PJ2 (J1) =
A
A+ C
,RJ2 (J1) =
A
A+ B
J2
Sentence in
Extract
Sentence not
in Extract
Sentence in
Extract
A B A+ B
J1 Sentence not
in Extract
C D C +D
A+ C B +D N = A +
B+C+D
Figure 3: Contingency table comparing sentences
extracted by the system and the judges.
In our case, each set of documents which is com-
pared has the same number of sentences and also
the same number of sentences are extracted; thus
P = R.
The average precision Pavg(SY STEM) and re-
call Ravg(SY STEM) are calculated by summing
over individual judges and normalizing. The aver-
age interjudge precision and recall is computed by
averaging over all judge pairs.
However, precision and recall do not take chance
agreement into account. The amount of agreement
one would expect two judges to reach by chance de-
pends on the number and relative proportions of the
categories used by the coders. The next section on
Kappa shows that chance agreement is very high in
extractive summarization.
3.1.2 Kappa
Kappa (Siegel and Castellan, 1988) is an evalua-
tion measure which is increasingly used in NLP an-
notation work (Krippendorff, 1980; Carletta, 1996).
Kappa has the following advantages over P and R:
? It factors out random agreement. Random
agreement is defined as the level of agreement
which would be reached by random annotation
using the same distribution of categories as the
real annotators.
? It allows for comparisons between arbitrary
numbers of annotators and items.
? It treats less frequent categories as more im-
portant (in our case: selected sentences), simi-
larly to precision and recall but it also consid-
ers (with a smaller weight) more frequent cate-
gories as well.
The Kappa coefficient controls agreement P (A)
by taking into account agreement by chance P (E) :
K =
P (A)? P (E)
1? P (E)
No matter how many items or annotators, or how
the categories are distributed, K = 0 when there is
no agreement other than what would be expected by
chance, and K = 1 when agreement is perfect. If
two annotators agree less than expected by chance,
Kappa can also be negative.
We report Kappa between three annotators in the
case of human agreement, and between three hu-
mans and a system (i.e. four judges) in the next sec-
tion.
3.1.3 Relative Utility
Relative Utility (RU) (Radev et al, 2000) is tested
on a large corpus for the first time in this project.
RU takes into account chance agreement as a lower
bound and interjudge agreement as an upper bound
of performance. RU allows judges and summarizers
to pick different sentences with similar content in
their summaries without penalizing them for doing
so. Each judge is asked to indicate the importance
of each sentence in a cluster on a scale from 0 to
10. Judges also specify which sentences subsume or
paraphrase each other. In relative utility, the score
of an automatic summary increases with the impor-
tance of the sentences that it includes but goes down
with the inclusion of redundant sentences.
3.2 Content-based Similarity measures
Content-based similarity measures compute the sim-
ilarity between two summaries at a more fine-
grained level than just sentences. For each automatic
extract S and similarity measure M we compute the
following number:
sim(M,S, {J1, J2, J3}) =
M(S, J1) +M(S, J2) +M(S, J3)
3
We used several content-based similarity mea-
sures that take into account different properties of
the text:
Cosine similarity is computed using the follow-
ing formula (Salton, 1988):
cos(X,Y ) =
?
xi ? yi
??
(xi)2 ?
??
(yi)2
where X and Y are text representations based on
the vector space model.
Longest Common Subsequence is computed as
follows:
lcs(X,Y ) = (length(X) + length(Y )? d(X,Y ))/2
where X and Y are representations based on
sequences and where lcs(X,Y ) is the length of
the longest common subsequence between X and
Y , length(X) is the length of the string X , and
d(X,Y ) is the minimum number of deletion and in-
sertions needed to transform X into Y (Crochemore
and Rytter, 1994).
3.3 Relevance Correlation
Relevance correlation (RC) is a new measure for as-
sessing the relative decrease in retrieval performance
when indexing summaries instead of full documents.
The idea behind it is similar to (Sparck-Jones and
Sakai, 2001). In that experiment, Sparck-Jones and
Sakai determine that short summaries are good sub-
stitutes for full documents at the high precision end.
With RC we attempt to rank all documents given a
query.
Suppose that given a queryQ and a corpus of doc-
uments Di, a search engine ranks all documents in
Di according to their relevance to the query Q. If
instead of the corpus Di, the respective summaries
of all documents are substituted for the full docu-
ments and the resulting corpus of summaries Si is
ranked by the same retrieval engine for relevance to
the query, a different ranking will be obtained. If
the summaries are good surrogates for the full docu-
ments, then it can be expected that rankings will be
similar.
There exist several methods for measuring the
similarity of rankings. One such method is Kendall?s
tau and another is Spearman?s rank correlation. Both
methods are quite appropriate for the task that we
want to perform; however, since search engines pro-
duce relevance scores in addition to rankings, we
can use a stronger similarity test, linear correlation
between retrieval scores. When two identical rank-
ings are compared, their correlation is 1. Two com-
pletely independent rankings result in a score of 0
while two rankings that are reverse versions of one
another have a score of -1. Although rank correla-
tion seems to be another valid measure, given the
large number of irrelevant documents per query re-
sulting in a large number of tied ranks, we opted for
linear correlation. Interestingly enough, linear cor-
relation and rank correlation agreed with each other.
Relevance correlation r is defined as the linear
correlation of the relevance scores (x and y) as-
signed by two different IR algorithms on the same
set of documents or by the same IR algorithm on
different data sets:
r =
?
i
(xi ? x)(yi ? y)
??
i
(xi ? x)2
??
i
(yi ? y)2
Here x and y are the means of the relevance scores
for the document sequence.
We preprocess the documents and use Smart to
index and retrieve them. After the retrieval process,
each summary is associated with a score indicating
the relevance of the summary to the query. The
relevance score is actually calculated as the inner
product of the summary vector and the query vec-
tor. Based on the relevance score, we can produce a
full ranking of all the summaries in the corpus.
In contrast to (Brandow et al, 1995) who run 12
Boolean queries on a corpus of 21,000 documents
and compare three types of documents (full docu-
ments, lead extracts, and ANES extracts), we mea-
sure retrieval performance under more than 300 con-
ditions (by language, summary length, retrieval pol-
icy for 8 summarizers or baselines).
4 Results
This section reports results for the summarizers and
baselines described above. We relied directly on the
relevance judgements to create ?manual extracts? to
use as gold standards for evaluating the English sys-
tems. To evaluate Chinese, we made use of a ta-
ble of automatically produced alignments. While
the accuracy of the alignments is quite high, we
have not thoroughly measured the errors produced
when mapping target English summaries into Chi-
nese. This will be done in future work.
4.1 Co-selection results
Co-selection agreement (Section 3.1) is reported in
Figures 4, and 5). The tables assume human perfor-
mance is the upper bound, the next rows compare
the different summarizers.
Figure 4 shows results for precision and recall.
We observe the effect of a dependence of the nu-
merical results on the length of the summary, which
is a well-known fact from information retrieval eval-
uations.
Websumm has an advantage over MEAD for
longer summaries but not for 20% or less. Lead
summaries perform better than all the automatic
summarizers, and better than the human judges.
This result usually occurs when the judges choose
different, but early sentences. Human judgements
overtake the lead baseline for summaries of length
50% or more.
5% 10% 20% 30% 40%
Humans .187 .246 .379 .467 .579
MEAD .160 .231 .351 .420 .519
WEBS .310 .305 .358 .439 .543
LEAD .354 .387 .447 .483 .583
RAND .094 .113 .224 .357 .432
Figure 4: Results in precision=recall (averaged over
20 clusters).
Figure 5 shows results using Kappa. Random
agreement is 0 by definition between a random pro-
cess and a non-random process.
While the results are overall rather low, the num-
bers still show the following trends:
? MEAD outperforms Websumm for all but the
5% target length.
? Lead summaries perform best below 20%,
whereas human agreement is higher after that.
? There is a rather large difference between the
two summarizers and the humans (except for
the 5% case for Websumm). This numerical
difference is relatively higher than for any other
co-selection measure treated here.
? Random is overall the worst performer.
? Agreement improves with summary length.
Figures 6 and 7 summarize the results obtained
through Relative Utility. As the figures indicate,
random performance is quite high although all non-
random methods outperform it significantly. Fur-
ther, and in contrast with other co-selection evalua-
tion criteria, in both the single- and multi-document
5% 10% 20% 30% 40%
Humans .127 .157 .194 .225 .274
MEAD .109 .136 .168 .192 .230
WEBS .138 .128 .146 .159 .192
LEAD .180 .198 .213 .220 .261
RAND .064 .081 .097 .116 .137
Figure 5: Results in kappa, averaged over 20 clus-
ters.
case MEAD outperforms LEAD for shorter sum-
maries (5-30%). The lower bound (R) represents the
average performance of all extracts at the given sum-
mary length while the upper bound (J) is the inter-
judge agreement among the three judges.
5% 10% 20% 30% 40%
R 0.66 0.68 0.71 0.74 0.76
RAND 0.67 0.67 0.71 0.75 0.77
WEBS 0.72 0.73 0.76 0.79 0.82
LEAD 0.72 0.73 0.77 0.80 0.83
MEAD 0.78 0.79 0.79 0.81 0.83
J 0.80 0.81 0.83 0.85 0.87
Figure 6: RU per summarizer and summary length
(Single-document).
5% 10% 20% 30% 40%
R 0.64 0.66 0.69 0.72 0.74
RAND 0.63 0.65 0.71 0.72 0.74
LEAD 0.71 0.71 0.76 0.79 0.82
MEAD 0.73 0.75 0.78 0.79 0.81
J 0.76 0.78 0.81 0.83 0.85
Figure 7: RU per summarizer and summary length
(Multi-document).
4.2 Content-based results
The results obtained for a subset of target lengths
using content-based evaluation can be seen in Fig-
ures 8 and 9. In all our experiments with tf ? idf -
weighted cosine, the lead-based summarizer ob-
tained results close to the judges in most of the target
lengths while MEAD is ranked in second position.
In all our experiments using longest common sub-
sequence, no system obtained better results in the
majority of the cases.
10% 20% 30% 40%
LEAD 0.55 0.65 0.70 0.79
MEAD 0.46 0.61 0.70 0.78
RAND 0.31 0.47 0.60 0.69
WEBS 0.52 0.60 0.68 0.77
Figure 8: Cosine (tf?idf ). Average over 10 clusters.
10% 20% 30% 40%
LEAD 0.47 0.55 0.60 0.70
MEAD 0.37 0.52 0.61 0.70
RAND 0.25 0.38 0.50 0.58
WEBS 0.39 0.45 0.53 0.64
Figure 9: Longest Common Subsequence. Average
over 10 clusters.
The numbers obtained in the evaluation of Chi-
nese summaries for cosine and longest common sub-
sequence can be seen in Figures 10 and 11. Both
measures identify MEAD as the summarizer that
produced results closer to the ideal summaries (these
results also were observed across measures and text
representations).
10% 20% 30% 40%
SUMM 0.44 0.65 0.71 0.78
LEAD 0.54 0.63 0.68 0.77
MEAD 0.49 0.65 0.74 0.82
RAND 0.31 0.50 0.65 0.71
Figure 10: Chinese Summaries. Cosine (tf ? idf ).
Average over 10 clusters. Vector space of Words as
Text Representation.
10% 20% 30% 40%
SUMM 0.32 0.53 0.57 0.65
LEAD 0.42 0.49 0.54 0.64
MEAD 0.35 0.50 0.60 0.70
RAND 0.21 0.35 0.49 0.54
Figure 11: Chinese Summaries. Longest Common
Subsequence. Average over 10 clusters. Chinese
Words as Text Representation.
We have based this evaluation on target sum-
maries produced by LDC assessors, although other
alternatives exist. Content-based similarity mea-
sures do not require the target summary to be a sub-
set of sentences from the source document, thus,
content evaluation based on similarity measures
can be done using summaries published with the
source documents which are in many cases available
(Teufel and Moens, 1997; Saggion, 2000).
4.3 Relevance Correlation results
We present several results using Relevance Correla-
tion. Figures 12 and 13 show how RC changes de-
pending on the summarizer and the language used.
RC is as high as 1.0 when full documents (FD) are
compared to themselves. One can notice that even
random extracts get a relatively high RC score. It is
also worth observing that Chinese summaries score
lower than their corresponding English summaries.
Figure 14 shows the effects of summary length and
summarizers on RC. As one might expect, longer
summaries carry more of the content of the full doc-
ument than shorter ones. At the same time, the rel-
ative performance of the different summarizers re-
mains the same across compression rates.
C112 C125 C241 C323 C551 AVG10
FD 1.00 1.00 1.00 1.00 1.00 1.000
MEAD 0.91 0.92 0.93 0.92 0.90 0.903
WEBS 0.88 0.82 0.89 0.91 0.88 0.843
LEAD 0.80 0.80 0.84 0.85 0.81 0.802
RAND 0.80 0.78 0.87 0.85 0.79 0.800
SUMM 0.77 0.79 0.85 0.88 0.81 0.775
Figure 12: RC per summarizer (English 20%).
C112 C125 C241 C323 C551 AVG10
FD 1.00 1.00 1.00 1.00 1.00 1.000
MEAD 0.78 0.87 0.93 0.66 0.91 0.850
SUMM 0.76 0.75 0.85 0.84 0.75 0.755
RAND 0.71 0.75 0.85 0.60 0.74 0.744
ALGN 0.74 0.72 0.83 0.95 0.72 0.738
LEAD 0.72 0.71 0.83 0.58 0.75 0.733
Figure 13: RC per summarizer (Chinese, 20%).
5% 10% 20% 30% 40%
FD 1.000 1.000 1.000 1.000 1.000
MEAD 0.724 0.834 0.916 0.946 0.962
WEBS 0.730 0.804 0.876 0.912 0.936
LEAD 0.660 0.730 0.820 0.880 0.906
SUMM 0.622 0.710 0.820 0.848 0.862
RAND 0.554 0.708 0.818 0.884 0.922
Figure 14: RC per summary length and summarizer.
5 Conclusion
This paper describes several contributions to text
summarization:
First, we observed that different measures rank
summaries differently, although most of them
showed that ?intelligent? summarizers outperform
lead-based summaries which is encouraging given
that previous results had cast doubt on the ability of
summarizers to do better than simple baselines.
Second, we found that measures like Kappa, Rel-
ative Utility, Relevance Correlation and Content-
Based, each offer significant advantages over more
simplistic methods like Precision, Recall, and Per-
cent Agreement with respect to scalability, applica-
bility to multidocument summaries, and ability to
include human and chance agreement. Figure 15
Property Prec, recall Kappa Normalized RU Word overlap, cosine, LCS Relevance Correlation
Intrinsic (I)/extrinsic (E) I I I I E
Agreement between human extracts X X X X X
Agreement human extracts and automatic extracts X X X X X
Agreement human abstracts and human extracts X
Non-binary decisions X X
Takes random agreement into account by design X X
Full documents vs. extracts X X
Systems with different sentence segmentation X X
Multidocument extracts X X X X
Full corpus coverage X X
Figure 15: Properties of evaluation measures used in this project.
presents a short comparison of all these evaluation
measures.
Third, we performed extensive experiments using
a new evaluation measure, Relevance Correlation,
which measures how well a summary can be used
to replace a document for retrieval purposes.
Finally, we have packaged the code used for this
project into a summarization evaluation toolkit and
produced what we believe is the largest and most
complete annotated corpus for further research in
text summarization. The corpus and related software
is slated for release by the LDC in mid 2003.
References
Ron Brandow, Karl Mitze, and Lisa F. Rau. 1995. Auto-
matic Condensation of Electronic Publications by Sen-
tence Selection. Information Processing and Manage-
ment, 31(5):675?685.
Jean Carletta. 1996. Assessing Agreement on Classifica-
tion Tasks: The Kappa Statistic. CL, 22(2):249?254.
Maxime Crochemore and Wojciech Rytter. 1994. Text
Algorithms. Oxford University Press.
William A. Gale and Kenneth W. Church. 1993. A
program for aligning sentences in bilingual corpora.
Computational Linguistics, 19(1):75?102.
Donna Harman and Daniel Marcu, editors. 2001. Pro-
ceedings of the 1st Document Understanding Confer-
ence. New Orleans, LA, September.
Eduard Hovy and Chin Yew Lin. 1999. Automated Text
Summarization in SUMMARIST. In Inderjeet Mani
and Mark T. Maybury, editors, Advances in Automatic
Text Summarization, pages 81?94. The MIT Press.
Klaus Krippendorff. 1980. Content Analysis: An Intro-
duction to its Methodology. Sage Publications, Bev-
erly Hills, CA.
Inderjeet Mani and Eric Bloedorn. 2000. Summariz-
ing Similarities and Differences Among Related Doc-
uments. Information Retrieval, 1(1).
Inderjeet Mani, The?re`se Firmin, David House, Gary
Klein, Beth Sundheim, and Lynette Hirschman. 2001.
The TIPSTER SUMMAC Text Summarization Evalu-
ation. In Natural Language Engineering.
Dragomir R. Radev, Hongyan Jing, and Malgorzata
Budzikowska. 2000. Centroid-Based Summarization
of Multiple Documents: Sentence Extraction, Utility-
Based Evaluation, and User Studies. In Proceedings
of the Workshop on Automatic Summarization at the
6th Applied Natural Language Processing Conference
and the 1st Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Seattle, WA, April.
Horacio Saggion. 2000. Ge?ne?ration automatique
de re?sume?s par analyse se?lective. Ph.D. the-
sis, De?partement d?informatique et de recherche
ope?rationnelle. Faculte? des arts et des sciences. Uni-
versite? de Montre?al, August.
Gerard Salton. 1988. Automatic Text Processing.
Addison-Wesley Publishing Company.
Sidney Siegel and N. John Jr. Castellan. 1988. Non-
parametric Statistics for the Behavioral Sciences.
McGraw-Hill, Berkeley, CA, 2nd edition.
Karen Sparck-Jones and Tetsuya Sakai. 2001. Generic
Summaries for Indexing in IR. In Proceedings of the
24th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 190?198, New Orleans, LA, September.
Simone Teufel and Marc Moens. 1997. Sentence Ex-
traction as a Classification Task. In Proceedings of the
Workshop on Intelligent Scalable Text Summarization
at the 35th Meeting of the Association for Computa-
tional Linguistics, and the 8th Conference of the Eu-
ropean Chapter of the Assocation for Computational
Linguistics, Madrid, Spain.
Anastasios Tombros, Mark Sanderson, and Phil Gray.
1998. Advantages of Query Biased Summaries in In-
formation Retrieval. In Eduard Hovy and Dragomir R.
Radev, editors, Proceedings of the AAAI Symposium
on Intelligent Text Summarization, pages 34?43, Stan-
ford, California, USA, March 23?25,. The AAAI
Press.
Improving Bitext Word Alignments
via Syntax-based Reordering of English
Elliott Franco Dra?bek and David Yarowsky
Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218, USA
{edrabek,yarowsky}@cs.jhu.edu
Abstract
We present an improved method for automated word
alignment of parallel texts which takes advantage
of knowledge of syntactic divergences, while avoid-
ing the need for syntactic analysis of the less re-
source rich language, and retaining the robustness of
syntactically agnostic approaches such as the IBM
word alignment models. We achieve this by using
simple, easily-elicited knowledge to produce syntax-
based heuristics which transform the target lan-
guage (e.g. English) into a form more closely resem-
bling the source language, and then by using stan-
dard alignment methods to align the transformed
bitext. We present experimental results under vari-
able resource conditions. The method improves
word alignment performance for language pairs such
as English-Korean and English-Hindi, which exhibit
longer-distance syntactic divergences.
1 Introduction
Word-level alignment is a key infrastructural tech-
nology for multilingual processing. It is crucial for
the development of translation models and transla-
tion lexica (Tufis?, 2002; Melamed, 1998), as well as
for translingual projection (Yarowsky et al, 2001;
Lopez et al, 2002). It has increasingly attracted at-
tention as a task worthy of study in its own right
(Mihalcea and Pedersen, 2003; Och and Ney, 2000).
Syntax-light alignment models such as the five
IBM models (Brown et al, 1993) and their rela-
tives have proved to be very successful and robust
at producing word-level alignments, especially for
closely related languages with similar word order
and mostly local reorderings, which can be cap-
tured via simple models of relative word distortion.
However, these models have been less successful at
modeling syntactic distortions with longer distance
movement. In contrast, more syntactically informed
approaches have been constrained by the often weak
syntactic correspondences typical of real-world par-
allel texts, and by the difficulty of finding or induc-
ing syntactic parsers for any but a few of the world?s
most studied languages.
Our approach uses simple, easily-elicited knowl-
edge of divergences to produce heuristic syntax-
based transformations from English to a form
(English?) more closely resembling the source lan-
English Transform
Traces
RetraceSource| \|
English
English?
Run GIZA++
Source|/ |
English?
Source
Language-specific
Heuristics
Figure 1: System Architecture
guage, and then using standard alignment methods
to align the transformed version to the target lan-
guage. This approach retains the robustness of syn-
tactically agnostic models, while taking advantage
of syntactic knowledge. Because the approach relies
only on syntactic analysis of English, it can avoid
the difficulty of developing a full parser for a new
low-resource language.
Our method is rapid and low cost. It requires
only coarse-grained knowledge of basic word order,
knowledge which can be rapidly found in even the
briefest grammatical sketches. Because basic word
order changes very slowly with time, word order of
related languages tends to be very similar. For ex-
ample, even if we only know that a language is of
the Northern-Indian/Sanskrit family, we can easily
guess with high confidence that it is systematically
head-final. Because our method can be restricted
to only bi-text pre-processing and post-processing,
it can be used as a wrapper around any existing
word-alignment tool, without modification, to pro-
vide improved performance by minimizing alignment
distortion.
2 Prior Work
The 2003 HLT-NAACL Workshop on Building and
Using Parallel Texts (Mihalcea and Pedersen, 2003)
reflected the increasing importance of the word-
alignment task, and established standard perfor-
mance measures and some benchmark tasks.
There is prior work studying systematic cross-
English:
Hindi:
use of plutonium is to manufacture nuclear weapons
plutoniyama kaa
?s
istemaala
use
paramaanu
nuclear
hathiyaara banaane
manufacture
ke lie hotaa hai
is
the
NP
NP
PP
S
VP
VP
VP
NP
plutonium weapons to
Figure 2: Original Hindi-English sentence pair with gold-standard word-alignments.
English?:
Hindi: plutoniyama
plutonium
kaa
?s
istemaala
use
paramaanu
nuclear
hathiyaara banaane ke lie hotaa hai
is
plutonium of the use nuclear weapons manufacture to is
S
VP
PP
NP VP
VP
NP NP
weapons manufacture to
Figure 3: Transformed Hindi-English? sentence pair with gold-standard word-alignments. Rotated nodes are
marked with an arc.
linguistic structural divergences, such as the DUSTer
system (Dorr et al, 2002). While the focus on ma-
jor classes of structural variation such as manner-of-
motion verb-phrase transformations have facilitated
both transfer and generation in machine translation,
these divergences have not been integrated into a
system that produces automatic word alignments
and have tended to focus on more local phrasal varia-
tion rather than more comprehensive sentential syn-
tactic reordering.
Complementary prior work (e.g. Wu, 1995) has
also addressed syntactic transduction for bilingual
parsing, translation, and word-alignment. Much of
this work depends on high-quality parsing of both
target and source sentences, which may be unavail-
able for many ?lower density? languages of interest.
Tree-to-string models, such as (Yamada and Knight,
2001) remove this dependency, and such models are
well suited for situations with large, cleanly trans-
lated training corpora. By contrast, our method re-
tains the robustness of the underlying aligner to-
wards loose translations, and can if necessary use
knowledge of syntactic divergences even in the ab-
sence of any training corpora whatsoever, using only
a translation lexicon.
3 System
Figure 1 shows the system architecture. We start
by running the Collins parser (Collins, 1999) on the
English side of both training and testing data, and
apply our source-language-specific heuristics to the
Language VP AP NP
English VO AO AN, NR
Hindi OV OA AN, RN
Korean OV OA AN, RN
Chinese VO AOA AN, RN
Romanian VO AO NA, NR
Table 1: Basic word order for three major phrase
types ? VP: verb phrases with Verb and Object,
AP: appositional (prepositional or postpositional)
phrases with Apposition and Object, and NP: noun
phrases withNoun andAdjective orRelative clause.
Chinese has both prepositions and postpositions.
resulting trees. This yields English? text, along with
traces recording correspondences between English?
words and the English originals. We use GIZA++
(Och and Ney, 2000) to align the English? with the
source language text, yielding alignments in terms
of the English?. Finally, we use the traces to map
these alignments to the original English words.
Figure 2 shows an illustrative Hindi-English sen-
tence pair, with true word alignments, and parse-
tree over the English sentence. Although it is only
a short sentence, the large number of crossing align-
ments clearly show the high-degree of reordering,
and especially long-distance motion, caused by the
syntactic divergences between Hindi and English.
Figure 3 shows the same sentence pair after En-
glish has been transformed into English? by our sys-
tem. Tree nodes whose children have been reordered
 20
 25
 30
 35
 40
 45
 50
 55
 60
 65
 70
 3  3.2  3.4  3.6  3.8  4  4.2  4.4  4.6  4.8
F
-
m
e
a
s
u
r
e
log(number of training sentences)
E? MethodDirect
Figure 4: Hindi alignment performance
 0
 5
 10
 15
 20
 25
 3  3.2  3.4  3.6  3.8  4  4.2  4.4
F
-
m
e
a
s
u
r
e
log(number of training sentences)
E? MethodDirect
Figure 5: Korean alignment performance
are marked by a subtended arc. Crossings have been
eliminated, and the alignment is now monotonic.
Table 1 shows the basic word order of three major
phrase types for each of the languages we treated. In
each case, our heuristics transform the English trees
to achieve these same word orders. For the Chinese
case, we apply several more language-specific trans-
formations. Because Chinese has both prepositions
and postpositions, we retain the original preposition
and add an additional bracketing postposition. We
also move verb modifiers other than noun phrases to
the left of the head verb.
4 Experiments
For each language we treated, we assembled
sentence-aligned, tokenized training and test cor-
pora, with hand-annotated gold-standard word
alignments for the latter1. We did not apply any
sort of morphological analysis beyond basic word to-
kenization. We measured system performance with
wa eval align.pl, provided by Rada Mihalcea and
Ted Pedersen.
Each training set provides the aligner with infor-
mation about lexical affinities and reordering pat-
terns. For Hindi, Korean and Chinese, we also tested
our system under the more difficult situation of hav-
ing only a bilingual word list but no bitext available.
This is a plausible low-resource language scenario
 25
 30
 35
 40
 45
 50
 55
 3  3.5  4  4.5  5
F
-
m
e
a
s
u
r
e
log(number of training sentences)
E? MethodDirect
Figure 6: Chinese alignment performance
 35
 40
 45
 50
 55
 60
 65
 70
 75
 3  3.2  3.4  3.6  3.8  4  4.2  4.4  4.6
F
-
m
e
a
s
u
r
e
log(number of training sentences)
E? MethodDirect
Figure 7: Romanian alignment performance
# Train Direct English?
Sents P R F P R F
Hindi
Dict only 16.4 13.8 15.0 18.5 15.6 17.0
1000 26.8 23.0 24.8 28.4 24.4 26.2
3162 35.7 31.6 33.5 38.4 33.5 35.8
10000 46.6 42.7 44.6 50.4 45.2 47.6
31622 60.1 56.0 58.0 63.6 58.5 61.0
63095 64.7 61.7 63.2 66.3 62.2 64.2
Korean
Dict only 26.6 12.3 16.9 27.5 12.9 17.6
1000 9.4 7.3 8.2 11.3 8.7 9.8
3162 13.2 10.2 11.5 16.0 12.4 14.0
10000 15.2 12.0 13.4 17.0 13.3 14.9
30199 21.5 16.9 18.9 21.9 17.2 19.3
Chinese
Dict only 44.4 30.4 36.1 44.5 30.5 36.2
1000 33.0 22.2 26.5 30.8 22.6 26.1
3162 44.6 28.9 35.1 41.7 30.0 34.9
10000 51.1 34.0 40.8 50.7 35.8 42.0
31622 60.4 39.0 47.4 55.7 39.7 46.4
100000 66.0 43.7 52.6 63.7 45.4 53.0
Romanian
1000 49.6 27.7 35.6 50.1 28.0 35.9
3162 57.9 33.4 42.4 57.6 33.0 42.0
10000 72.6 45.5 55.9 71.3 45.0 55.2
48441 84.7 57.8 68.7 83.5 57.1 67.8
Table 2: Performance in Precision, Recall, and F-
measure (per cent) of all systems.
Source # Test Mean Correlation
Language Sents Length Direct E?
Hindi 46 16.3 54.1 60.1
Korean 100 20.2 10.2 31.6
Chinese 88 26.5 60.2 63.7
Romanian 248 22.7 81.1 80.6
Table 3: Test set characteristics, including number
of sentence pairs, mean length of English sentences,
and correlation r2 between English and source-
language normalized word positions in gold-standard
data, for direct and English? situations.
and a test of the ability of the system to take sole
responsibility for knowledge of reordering.
Table 3 describes the test sets and shows the cor-
relation in gold standard aligned word pairs between
the position of the English word in the English sen-
tence and the position of the source-language word
in the source-language sentence (normalizing the po-
sitions to fall between 0 and 1). The baseline (di-
rect) correlations give quantitative evidence of dif-
fering degrees of syntactic divergence with English,
and the English? correlations demonstrate that our
heuristics do have the effect of better fitting source
language word order.
5 Results
Figures 4, 5, 6 and 7 show learning curves for sys-
tems trained on parallel sentences with and with-
out the English? transforms. Table 2 provides fur-
ther detail, and also shows the performance of sys-
tems trained without any bitext, but only with ac-
cess to a bilingual translation lexicon. Our sys-
tem achieves consistent, substantial performance im-
provement under all situations for English-Hindi
and English-Korean language pairs, which exhibit
longer distance SOV?SVO syntactic divergence.
For English-Romanian and English-Chinese, neither
significant improvement nor degradation is seen, but
these are language pairs with quite similar sentential
word order to English, and hence have less opportu-
nity to benefit from our syntactic transformations.
6 Conclusions
We have developed a system to improve the per-
formance of bitext word alignment between English
and a source language by first reordering parsed
English into an order more closely resembling that
1Hindi training: news text from the LDC for the 2003
DARPA TIDES Surprise Language exercise; Hindi testing:
news text from Rebecca Hwa, then at the University of Mary-
land; Hindi dictionary: The Hindi-English Dictionary, v. 2.0
from IIIT (Hyderabad) LTRC; Korean training: Unbound
Bible; Korean testing: half from Penn Korean Treebank and
half from Universal declaration of Human Rights, aligned by
Woosung Kim at the Johns Hopkins University; Korean dic-
tionary: EngDic v. 4; Chinese training: news text from FBIS;
Chinese testing: Penn Chinese Treebank news text aligned by
Rebecca Hwa, then at the University of Maryland; Chinese
dictionary: from the LDC; Romanian training and testing:
(Mihalcea and Pedersen, 2003).
of the source language, based only on knowledge
of the coarse basic word order of the source lan-
guage, such as can be obtained from any cross-
linguistic survey of languages, and requiring no pars-
ing of the source language. We applied the sys-
tem to the task of aligning English with Hindi, Ko-
rean, Chinese and Romanian. Performance improve-
ment is greatest for Hindi and Korean, which exhibit
longer-distance constituent reordering with respect
to English. These properties suggest the proposed
English? word alignment method can be an effective
approach for word alignment to languages with both
greater cross-linguistic word-order divergence and an
absence of available parsers.
References
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra,
and R. L. Mercer. 1993. The mathematics of sta-
tistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
B. J. Dorr, L. Pearl, R. Hwa, and N. Habash. 2002.
DUSTer: A method for unraveling cross-language
divergences for statistical word-level alignment.
In Proceedings of AMTA-02, pages 31?43.
A. Lopez, M. Nosal, R. Hwa, and P. Resnik. 2002.
Word-level alignment for multilingual resource ac-
quisition. In Proceedings of the LREC-02 Work-
shop on Linguistic Knowledge Acquisition and
Representation.
I. D. Melamed. 1998. Empirical methods for MT
lexicon development. Lecture Notes in Computer
Science, 1529:18?9999.
R. Mihalcea and T. Pedersen. 2003. An evalua-
tion exercise for word alignment. In Rada Mi-
halcea and Ted Pedersen, editors, Proceedings of
the HLT-NAACL 2003 Workshop on Building and
Using Parallel Texts, pages 1?10.
F. J. Och and H. Ney. 2000. A comparison of align-
ment models for statistical machine translation.
In Proceedings of COLING-00, pages 1086?1090.
D. I. Tufis?. 2002. A cheap and fast way to build
useful translation lexicons. In Proceedings of
COLING-02, pages 1030?1036.
D. Wu. 1995. Stochastic inversion transduction
grammars, with application to segmentation,
bracketing, and alignment of parallel corpora. In
Proceedings of IJCAI-95, pages 1328?1335.
K. Yamada and K. Knight. 2001. A syntax-based
statistical translation model. In Proceedings of
ACL-01, pages 523?530.
D. Yarowsky, G. Ngai, and R. Wicentowski. 2001.
Inducing multilingual text analysis tools via ro-
bust projection across aligned corpora. In Pro-
ceedings of HLT-01, pages 161?168.
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 49?56,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Induction of Fine-grained Part-of-speech Taggers via
Classifier Combination and Crosslingual Projection
Elliott Franco Dra?bek
Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218
edrabek@cs.jhu.edu
David Yarowsky
Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218
yarowsky@cs.jhu.edu
Abstract
This paper presents an original approach
to part-of-speech tagging of fine-grained
features (such as case, aspect, and adjec-
tive person/number) in languages such as
English where these properties are gener-
ally not morphologically marked.
The goals of such rich lexical tagging
in English are to provide additional fea-
tures for word alignment models in bilin-
gual corpora (for statistical machine trans-
lation), and to provide an information
source for part-of-speech tagger induction
in new languages via tag projection across
bilingual corpora.
First, we present a classifier-combination
approach to tagging English bitext with
very fine-grained part-of-speech tags nec-
essary for annotating morphologically
richer languages such as Czech and
French, combining the extracted fea-
tures of three major English parsers,
and achieve fine-grained-tag-level syntac-
tic analysis accuracy higher than any indi-
vidual parser.
Second, we present experimental results
for the cross-language projection of part-
of-speech taggers in Czech and French via
word-aligned bitext, achieving success-
ful fine-grained part-of-speech tagging of
these languages without any Czech or
French training data of any kind.
1 Introduction
Most prior research in part-of-speech (POS) tag-
ging has focused on supervised learning over a
tagset such as the Penn Treebank tagset for En-
glish, which is restricted to features that are mor-
phologically distinguished in the focus language.
Thus the only verb person/number distinction made
in the Brown Corpus/Penn Treebank tagset is VBZ
(3rd-person-singular-present), with no correspond-
ing person/number distinction in other tenses. Sim-
ilarly, adjectives in English POS tagsets typically
have no distinctions for person, number or case be-
cause such properties have no morphological surface
distinction, although they do for many other lan-
guages.
This essential limitation of the Brown/Penn POS
subtag inventory to morphologically realized dis-
tinctions in English dramatically simplifies the prob-
lem by reducing the tag entropy per surface form
(the adjective tall has only one POS tag (JJ) rather
than numerous singular, plural, nominative, ac-
cusative, etc. variants), increasing both the stand-
alone effectiveness of lexical prior models and word-
suffix models for part-of-speech tagging.
However, for many multilingual applications, in-
cluding feature-based word alignment in bilingual
corpora and machine translation into morphologi-
cally richer languages, it is helpful to extract finer-
grained lexical analyses on the English side that
more closely parallel the morphologically realized
tagset of the second (source or target) language.
In particular, prior work on translingual part-of-
speech tagger projection via parallel bilingual cor-
pora (e.g. Yarowsky et al, 2001) has been limited
to inducing part-of-speech taggers in second lan-
guages (such as French or Czech) that only assign
tags at the granularity of their source language (i.e.
49
the Penn Treebank-granularity distinctions from En-
glish). The much richer English tagsets achieved
here can allow these tagger projection techniques to
transfer richer tag distinctions (such as case and verb
person/number) that are important to the full analy-
sis of these languages, using only bilingual corpora
with the morphologically impoverished English.
For quickly retargetable machine translation, the
primary focus of effort is overcoming the extreme
scarcity of resources for the low density source lan-
guage. Sparsity of conditioning events for a transla-
tion model can be greatly reduced by the availabil-
ity of automatic source-language analysis. In this
research we attempt to induce models for the au-
tomatic analysis of morphological features such as
case, tense, number, and polarity in both the source
and target languages with this end in mind.
2 Prior Work
2.1 Fine-grained part-of-speech tagging
Most prior work in fine-grained part-of-speech tag-
ging has been limited to languages such as Czech
(e.g. Hajic? and Hladka?, 1998) or French (e.g. Fos-
ter etc.) where finer-grained tagset distinctions are
morphologically marked and hence natural for the
language. In support of supervised tagger learn-
ing of these languages, fine-trained tagset inven-
tories have been developed by the teams above
at Charles University (Czech) and Universite? de
Montre?al (French). The tagset developed by Hajic?
forms the basis of the distinctions used in this paper.
The other major approach to fine-grained tagging
involves using tree-based tags that capture grammat-
ical structure. Bangalore and Joshi (1999) have uti-
lized ?supertags? based on tree-structures of various
complexity in the tree-adjoining grammar model.
Using such tags, Brants (2000) has achieved the au-
tomated tagging of a syntactic-structure-based set of
grammatical function tags including phrase-chunk
and syntactic-role modifiers trained in supervised
mode from a treebank of German.
2.2 Classifier combination for part-of-speech
tagging
There has been broad work in classifier combination
at the tag-level for supervised POS tagging mod-
els. For example, Ma`rquez and Rodr??guez (1998)
have performed voting over an ensemble of decision
tree and HMM-based taggers for supervised En-
glish tagging. Murata et al (2001) have combined
neural networks, support vector machines, decision
lists and transformation-based-learning approaches
for Thai part-of-speech tagging. In each of these
cases, annotated corpora containing the full tagset
granularity are required for supervision.
Henderson and Brill (1999) have approached
parsing through classifier combination, using bag-
ging and boosting for the performance-weighted
voting over the parse-trees from three anonymous
statistical phrase-structure-based parsers. However,
as their switching and voting models assumed equiv-
alent phrase-structure conventions for merger com-
patibility, it is not clear how a dependency parsing
model or other divergent syntactic models could be
integrated into this framework. In contrast, the ap-
proach presented below can readily combine syntac-
tic analyses from highly diverse parse structure mod-
els by first projecting out all syntactic analyses onto
a common fine-grained lexical tag inventory.
2.3 Projection-based Bootstrapping
Yarowsky et al (2001) performed early work in the
cross-lingual projection of part-of-speech tag anno-
tations from English to French and Czech, by way of
word-aligned parallel bilingual corpora. They also
used noise-robust supervised training techniques to
train stand-alone French and Czech POS taggers
based on these projected tags. Their projected
tagsets, however, were limited to those distinctions
captured in the English Penn treebank inventory,
and hence failed to make many of the finer grained
distinctions traditionally assumed for French and
Czech POS tagging, such as verb person, number,
and polarity and noun/adjective case.
Probst (2003) pursued a similar methodology for
the purposes of tag projection, using a somewhat
expanded tagset inventory (e.g. including adjec-
tive number but not case), and focusing on target-
language monolingual modeling using morpheme
analysis. Cucerzan and Yarowsky (2003) addressed
the problem of grammatical gender projection via
the use of small seed sets based on natural gender.
Another distinct body of work addresses the prob-
lem of parser bootstrapping based on syntactic de-
pendency projection (e.g. Hwa et al 2002), often
using approaches based in synchronous parsing (e.g.
Smith and Smith, 2004).
50
Word Core Prsn Num. Case Tns/ Pol. Voi.
POS Asp.
The DT 3 PL. NOM.
books NN 3 PL. NOM.
were VB 3 PL. PAST + ACT.
provoking VB 3 PL. PAST- + ACT.
PROG.
laughter NN 3 S. ACC.
with IN
their DT 3 PL. ?WITH?
curious JJ 3 PL. ?WITH?
titles NN 3 PL. ?WITH?
Figure 1: Example of fine-grained English POS tags
Word Core Prsn Num. Case Tns/ Pol. Voice
POS Asp.
Les DT 3 PL. NOM.
livres NN 3 PL. NOM.
provoquaient VB 3 PL. PAST- + ACT.
PROGR.
des DT 3 PL. ACC.
rires NN 3 PL. ACC.
avec IN
ses DT 3 PL. ?WITH?
titres NN 3 PL. ?WITH?
curieux JJ 3 PL. ?WITH?
Figure 2: Example of fined-grained POS tags pro-
jected onto a French translation
3 Tagsets
We use Penn treebank-style part-of-speech tags as
a substrate for further enrichment (for all of the ex-
periments described here, text was first tagged us-
ing the fnTBL part-of-speech tagger (Ngai and Flo-
rian, 2001)). Each Penn tag is mapped to a core
part-of-speech tag, which determines the set of fine-
grained tags further applicable to each word. The
fine-grained tags applicable to nouns, verbs, and ad-
jective are shown in Table 1. This paper concentrates
on these most important core parts-of-speech.
The example English sentence in Figure 1 illus-
trates several key points about our tagset. Some of
the information we are interested in is already ex-
pressed by the Penn-style tags ? the NN titles is plu-
ral; the VBD were is in the past tense. For these, our
goal is simply to make these facts explicit.
On the other hand, curious could also be meaning-
fully said to be semantically plural, and most impor-
tantly for us, the corresponding word in a translation
of this sentence into many other languages would
be morphologically plural. Similarly, the head verb
provoking is also semantically in the past tense, and
is likely to be translated to a past-tense form in many
languages, even though in this example the actual
tense marking is on were. We expect the ?past-
ness? of the action to be much more stable cross-
linguistically, than the particular division of labor
between the head word and the auxiliary. By prop-
VB JJ NN Range
Person       1 / 2 / 3
Number       SINGULAR
PLURAL
Case     NOMINATIVE
ACCUSATIVE
GENITIVE
PREPOSITION-?IN?
PREPOSITION-?OF?
. . .
Degree   POSITIVE
COMPARATIVE
SUPERLATIVE
Tense   PAST
PRESENT
FUTURE
Perfectivity   + / ?
Progressivity   + / ?
Polarity   + / ?
Voice   ACTIVE / PASSIVE
Table 1: The fine-grained POS inventory used for
English
agating these features from where they are explicit
to where they are not, we hope to make information
more directly available for projection. Another im-
portant class of information we would like to make
available concerns syntactic relations, which many
languages mark with morphological case. This is an
issue that involves deep, complex, and ambiguous
mappings, which we are not yet prepared to treat in
their fullness. For now, we observe that curious and
titles are both dominated by with.
Because of intent to mark whatever information
is recoverable, some of our tags require some in-
terpretation. For example, English has little or no
morphological realization of syntactic case, but the
essential information of case, relationship of a noun
with its governor, is recoverable from contextual
information, so we defined it in these terms. To
avoid loss of information, we chose to remain ag-
nostic about deeper analyses, such as the identifi-
cation of theta roles or predicate-argument relation-
ships, and restricted ourselves to a direct represen-
tation of surface relationships. We identified sub-
jects, direct and indirect objects, non-heads of noun
compounds, possessives, and temporal adjuncts, and
created a distinct tag for the objects of each distinct
preposition.
Our ideal would be to have as expansive and de-
tailed a tagset as possible, a ?quasi-universal? tagset
which could cover whatever set of distinctions might
be relevant for any language onto which we might
51
Feature Antecedent   CONSEQUENT
Noun Number NN   SINGULAR
NNS   PLURAL
Verb Tense VBD   PAST
(will

shall) RB* VB   FUTURE
Figure 3: Examples of locally recoverable features
project our analysis. A completely universal tagset
would require that the morphological distinctions
made by the world?s languages come from a limited
pool of possibilities, based on non-arbitrary seman-
tic distinctions, and further would require that the
relevant semantic information be recoverable from
English text. The tagset we are using now is shaped
in part by exceptions to these conditions. For ex-
ample, we have put off implementing tagging of
gender given the notoriously arbitrary and inconsis-
tent assignment of grammatical gender across lan-
guages (although Cucerzan and Yarowsky (2003)
were able to show success on projection-based anal-
ysis of grammatical gender as well).
In the end, we have settled on a set of distinc-
tions very similar to those realized by the morpho-
logically richer of the European languages, with the
noticeable absence of gender. Table 1 describes the
features we chose on this basis (definiteness and
mood features were developed for English but not
projected to French or Czech, and are not treated in
this paper).
4 Methods ? English Tagging
The features we tagged vary widely in their degree
of morphological versus syntactic marking, and the
difficulty of their monolingual English detection.
For some, tagging is simply a matter of explicitly
separating information contained in the Penn part-
of-speech tags, while others can be tagged to a high
degree of accuracy with simple heuristics based on
local word and part-of-speech tag patterns. These
include number for nouns and adjectives, person
(trivially) for nouns, degree for adjectives, polarity,
voice, and aspect (perfectivity and progressivity) for
verbs, as well as tense for some verbs. Figure 3
shows example rules for some of these easier cases.
The more difficult features are those whose de-
tection requires some degree of syntactic analysis.
These include case, which summarizes the relation
of each noun with its governor, and the agreement-
based features: we define person, number, and case
for attributive adjectives by agreement with their
head nouns, number and person for verbs and predi-
cate adjectives by agreement with their subjects, and
tense for some verbs by agreement with their in-
flected auxiliaries.
We investigated four individual approaches for
the syntax-features ? a regular-expression-based
quasi-parser, a system based on Dekang Lin?s Mini-
Par (Lin, 1993), a system based on the Collins parser
(Collins, 1999), and one based on the CMU Link
Grammar Parser (Sleator and Temperley, 1993),
as well as a family of voting-based combination
schemes.
4.1 Regular-expression Quasi-parser
The regular-expression ?quasi-parser? takes a direct
approach, using several dozen heuristics based on
regular-expression-like patterns over words, Penn
part-of-speech tags, and the output of the fnTBL
noun chunker. Use of the noun chunker fa-
cilitates identification of noun/dependent relation-
ships within chunks, and extends the range of pat-
terns identifying noun/governor relationships across
chunks.
The output of the quasi-parser consists of two
parts: a case tag for each noun in a sentence, and
a set of agreement links across which other features
are then spread. We call this a direct approach be-
cause the links are defined operationally, directly in-
dicating the spreading action, rather than represent-
ing any deeper syntactic analysis.
In the diagram of the example sentence below, an
arrow from one word to another indicates that the
former takes features from the latter. The example
also shows the context patterns by which the nouns
in the sentence receive case.
+<<<<<+ +>>>>>>>>>>>>>+
| | | |
+>>>>+ +<<<<<<<+ | +>>>>>>+
| | | | | | |
<The books> were provoking laughter with <their curious titles>
Word Context Pattern   CASE TAG
laughter VB (genitive-NP)*     ACCUSATIVE
titles with (genitive-NP)*     PREP-WITH
books default   NOMINATIVE
4.2 MiniPar and the CMU Link Grammar
Parser
For MiniPar, the Collins parser, and the CMU
link grammar parser, we developed for each a set
of minimal-complexity heuristics to transform the
parser output into the specific conceptions of depen-
dency and case we had developed for the first pass.
52
MiniPar produces a labeled dependency graph,
which yields a straightforward extraction of the in-
formation needed for this task. Case tagging is a
simple matter of mapping the set of dependency la-
bels to our case inventory. Our agreement links
are almost a subset of MiniPar?s dependencies (with
some special treatment of subject/auxiliary/main-
verb triads, as shown in the example sentence).
The figure below presents MiniPar?s raw output
for the example sentence, along with some exam-
ple dependency-label/case-tag rules. The agreement
links extracted from the dependency graph are iden-
tical (in this case) to those produced by the regular-
expression quasi-parser.
mod pcomp-n
+<<<<<<+<<<<<<<<<<<<<<<<<<<+
| | |
s | | gen |
+>>>>>>>>>>>>>+ | | +>>>>>>>>>>>>>+
| | | | | |
det| be | obj | | | mod |
+>>>+ +>>>>>>>+<<<<<<<<+ | | +>>>>>>+
| | | | | | | | |
| | | | | | | | |
The books were provoking laughter with their curious titles
Word Dependency Label   CASE TAG
books s   NOMINATIVE
laughter obj   ACCUSATIVE
titles pcomp-n:with   PREP-WITH
The output of the CMU link grammar parser has
properties similar to MiniPar, and thus tag extraction
was handled in a similar fashion.
4.3 Collins Parser
The Collins Parser produces a Penn-Treebank-style
constituency tree, with head labels. Although we
could have used the head-labels to operate on the
dependency graph as with MiniPar, we chose to con-
centrate on addressing the weakest point of our pre-
vious systems, the identification of case. Our algo-
rithm traces the path from each noun to the root of
the tree, stopping at the first node which we judged
to reliably indicate case.
We did not directly extract any further informa-
tion from the Collins parser output. Instead, the
remainder of the system is identical to the regular-
expression quasi-parser. However, because the sys-
tem uses nominative case to identify verb sub-
jects, we did expect to see some improvements in
agreement-based features as well.
S
NPB
The books
VP
were VP
provoking NPB
laughter
PP
with NPB
their curious titles
Word Path to Root   CASE TAG
books NPB:S   NOMINATIVE
laughter NPB:VP:VP:VP:S   ACCUSATIVE
titles NPB:PP(with):VP:VP:S   PREP-WITH
4.4 Parser Combination
The fine-grained taggers based on the four partic-
ipating parsers exhibited significant differences in
their strengths and weaknesses, suggesting poten-
tial benefit from combining them. Lacking tag-level
numerical scores and development data for weight-
training, we restricted ourselves to simple voting
mechanisms. We chose to do all of the combinations
at the end of the process, voting separately on tags
for specific features of specific words. Without tag-
level probabilities from the one-best parser outputs,
we were still able to use the combination protocols
to achieve a coarse-grained confidence measure.
We compared a series of seven combination pro-
tocols of increasing leniency to investigate preci-
sion/recall tradeoffs. The strictest, ?4:0?, produces
an output only when there are four votes for the fa-
vored tag, and no votes for any other. Analogously,
protocols ?3:0?, ?2:0? and ?1:0? also allow no dissent,
but allow progressively more abstentions. Continu-
ing the sequence, protocol ?2:1? proposes a tag as
long as there is a clear majority, ?2:2? as long as sup-
porters are not outnumbered by dissenters, and ?1:3?
whenever possible. To break ties in the latter two
protocols, we favored first the CMU Link Parser,
then Collins, then MiniPar, then Regexp. (Lacking
sufficient labeled data for fine-tuning, we ordered
them arbitrarily.)
5 Evaluation of English POS Tagging
Before we began the development of our taggers, we
created standard tagging guidelines, and hand anno-
tated a 3013-word segment of the English side of the
Canadian Hansards, to be used for evaluation.
53
Core Feature MiniPar Regexp Collins CMU Link 1:3
POS
num 86.8 87.7 87.7 87.9 88.4
case 65.1 74.5 76.4 79.2 80.6
JJ deg 100 100 100 100 100
?French? 86.8 87.7 87.7 87.9 88.4
?Czech? 57.9 64.3 67.1 68.1 70.5
num 99.7 99.7 99.7 99.7 99.7
NN case 65.9 74.8 77.8 77.3 80.0
?French? 99.7 99.7 99.7 99.7 99.7
?Czech? 65.0 74.8 77.8 77.2 79.9
num 77.2 64.8 65.5 66.8 78.1
tns 77.2 66.8 67.1 67.1 76.3
prsn 88.0 75.0 74.3 73.4 86.5
VB pol 96.3 96.6 96.6 96.6 96.6
voice 88.0 88.0 88.0 88.0 88.0
?French? 61.8 61.3 61.0 61.3 67.5
?Czech? 61.3 61.1 60.8 61.1 67.1
overall ?French? 82.6 82.5 82.4 83.2 85.2
?Czech? 62.5 67.8 69.4 70.5 73.3
Table 2: English tagging forced-choice accuracy
Core Feature Mini Regexp Collins CMU 2:0 1:0 1:2
POS Par Link
num 79.1 81.3 81.3 82.2 81.2 83.8 83.9
JJ case 72.1 79.2 83.0 78.9 78.1 79.1 84.2
deg 100 100 100 100 100 100 100
?Czech? 67.6 72.2 76.0 74.3 70.4 73.4 77.9
num 99.7 99.7 99.7 99.7 99.7 99.7 99.7
NN case 68.5 75.5 78.6 77.9 72.6 72.5 78.1
?Czech? 68.1 75.2 78.3 77.7 72.2 72.1 77.8
tns 78.0 68.5 68.7 68.0 68.7 78.3 78.3
num 72.7 61.3 61.2 61.3 61.1 76.1 77.1
prsn 77.2 66.5 65.4 63.9 64.0 78.3 79.0
VB pol 96.3 96.6 96.6 96.5 96.5 96.5 96.6
voice 88.0 88.0 88.0 88.0 88.0 88.0 88.0
?French? 61.7 50.7 50.2 50.1 50.6 64.8 65.6
?Czech? 61.1 50.5 49.9 49.8 50.4 64.5 65.2
all ?French? 81.9 78.7 78.5 78.5 83.6 78.9 83.9
?Czech? 65.4 66.0 67.8 69.3 68.9 63.5 72.9
Table 3: English tagging F-measure
3:0
 55
 60
 65
 70
 75
 80
 85
 90
 95
 70  75  80  85  90  95  100
R
ec
al
l
Consensus
MiniPar
2:0
Precision
Noun Case
CMULink
Collins
RegExp
2:21:3
2:1
1:0
4:0
 50
Figure 4: Precision versus Recall ? Noun case
Verb number
 40
 45
 50
 55
 60
 65
 70
 75
 80
 86  88  90  92  94
R
ec
al
l
Precision
Consensus
2:2
1:3 2:1 1:0
MiniPar
Collins
CMULink RegExp
3:0
2:0
4:0
 35
Figure 5: Precision versus Recall ? Verb number
Table 2 shows system accuracy on test data in
a forced-choice evaluation, where abstentions were
replaced by the most common tag for the each situa-
tion (the combination system is that one biased most
heavily towards recall.)
In addition to the individual features, we also list
?pseudo-French? and ?pseudo-Czech?. These rep-
resent exact-match accuracies for composite fea-
tures comprising those features typically realized in
French or Czech POS taggers. For example, pseudo-
Czech verb accuracy of 67.1% indicates that for
67.1% of verb instances, the Czech-realized features
of number, tense, perfectivity, progressivity, polar-
ity, and voice were all correct. These give an indica-
tion of the quality of the starting point for crosslin-
gual bootstrapping to the respective languages.
Besides the forced-choice scenario, we were also
interested in the effect of allowing abstentions for
low-confidence cases. Table 3 shows the F-measure
of precision and recall for the individual systems, as
well as a range of combination systems. Figures 4
and 5 show (for two example features) the clear pre-
cision/recall tradeoff. Performance of the consensus
systems is higher than the individual parser-based
taggers at all levels of tag precision or recall.
Unfortunately, because MiniPar does its own inte-
grated tokenization and part-of-speech tagging, we
found that a significant portion of the errors seemed
to stem from discrepancies where MiniPar disagreed
on the segmentation or the core part-of-speech of the
words in question.
6 Cross-lingual POS Tag Projection and
Bootstrapping
Our cross-lingual POS tag projection process is sim-
ilar to Yarowsky et al (2001). It begins by perform-
ing a statistical sentence and word alignment of the
bilingual corpora (described below), and then trans-
fers both the coarse- and fine-grained tags achieved
from classifier combination on the English side via
the higher confidence word alignments (based on the
intersection of the 1-best word alignments induced
from French to English and English to French. The
projected tags then serve as noisy monolingual train-
ing data in the source language.
There are several notable differences and exten-
sions: The first major difference is that the projected
fine-grained tag set is much more detailed, including
such additional properties as noun case, adjective
54
case and number, and verb person, number, voice,
and polarity. Because these span the subtag features
normally assumed for Czech and French part-of-
speech taggers, the projection work presented here
for the first time shows the translingual projection
and induction of full-granularity Czech and French
taggers, rather than the much less complete and
coarser-grained prior projection work.
The other major differences are in the method
of target-language monolingual tagger generaliza-
tion from the projected tags. We pursue a combi-
nation of trie-based lexical prior models and local-
agreement-based context models. The lexical prior
trie model, as illustrated in Figure 6 for noun num-
ber, shows how the hierarchically-smoothed lexical
prior conditioned on variable length suffixes can as-
sign noun number probabilities to both previously
seen words (with full-word-length suffixes stored)
and to new words in test data, based on backoff to
partially matching suffixes.
The context models are based on exploiting agree-
ment phenomena of the fine-grained tag features in
local context.  
	 	Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 79?82,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Models for Inuktitut-English Word Alignment
Charles Schafer and Elliott Franco Dr?abek
Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218, USA
{cschafer,edrabek}@cs.jhu.edu
Abstract
This paper presents a set of techniques for bitext word align-
ment, optimized for a language pair with the characteristics of
Inuktitut-English. The resulting systems exploit cross-lingual
affinities at the sublexical level of syllables and substrings, as
well as regular patterns of transliteration and the tendency to-
wards monotonicity of alignment. Our most successful systems
were based on classifier combination, and we found different
combination methods performed best under the target evalua-
tion metrics of F-measure and alignment error rate.
1 Introduction
Conventional word-alignment methods have been suc-
cessful at treating many language pairs, but may be lim-
ited in their ability to generalize beyond the Western Eu-
ropean language pairs for which they were originally
developed, to pairs which exhibit more complex diver-
gences in word order, morphology and lexical granular-
ity. Our approach to Inuktitut-English alignment was to
carefully consider the data in identifying difficulties par-
ticular to Inuktitut-English as well as possible simplify-
ing assumptions. We used these observations to construct
a novel weighted finite-state transducer alignment model
as well as a specialized transliteration model. We com-
bined these customized systems with 3 systems based
on IBM Model 4 alignments under several methods of
classifier combination. These combination strategies al-
lowed us to produce multiple submissions targeted at the
distinct evaluation measures via a precision/recall trade-
off.
2 Special Characteristics of the
Inuktitut-English Alignment Problem
Guided by the discussion of Inuktitut in Mallon (1999),
we examined the Nunavut Hansards training and hand-
labeled trial data sets in order to identify special chal-
lenges and exploitable characteristics of the Inuktitut-
English word alignment problem. We were able to iden-
tify three: (1) Importance of sublexical Inuktitut units;
(2) 1-to-N Inuktitut-to-English alignment cardinality; (3)
Monotonicity of alignments.
2.1 Types and TokensInuktitut has an extremely productive agglutinative mor-
phology, and an orthographic word may combine very
many individual morphemes. As a result, in Inuktitut-
English bitext we observe Inuktitut sentences with many
fewer word tokens than the corresponding English sen-
tences; the ratio of English to Inuktitut tokens in the
training corpus is 1.85.1 This suggests the importance of
looking below the Inuktitut word level when computing
lexical translation probabilities (or alignment affinities).
To reinforce the point, consider that the ratio of training
corpus types to tokens is 0.007 for English, and 0.194 for
Inuktitut. In developing a customized word alignment
solution for Inuktitut-English, a major goal was to han-
dle the huge number of Inuktitut word types seen only
once in the training corpus (337798 compared to 8792
for English), without demanding the development of a
morphological analyzer.
2.2 AlignmentConsidering English words in English sentence order,
4.7% of their alignments to Inuktitut were found to be
retrograde; that is, involving a decrease in Inuktitut word
position with respect to the previous English word?s
aligned Inuktitut position. Since this method of counting
retrograde alignments would assign a low count to mass
movements of large contiguous chunks, we also mea-
sured the number of inverted alignments over all pairs
of English word positions. That is, the sum
?e?a=|e|?1a=1 ?
b=|e|
b=a+1?i1?I(e,a)?i2?I(e,b)(1 if i1 > i2)was computed over all Inuktitut alignment sets I(e, x),
for e the English sentence and x the English word po-
sition. Dividing this sum by the obvious denominator
(replacing (1 if i1 > i2) with (1) in the sum) yielded avalue of 1.6% inverted alignments.
Table 1 shows a histogram of alignment cardinalities
for both English and Inuktitut. Ninety-four percent of
English word tokens, and ninety-nine percent of those
having a non-null alignment, align to exactly one Inuk-
titut word. In development of a specialized word aligner
for this language pair (Section 3), we made use of the
observed reliability of these two properties, monotonic-
ity and 1-to-N cardinality.
3 Alignment by Weighted Finite-State
Transducer Composition
We designed a specialized alignment system to handle
the above-mentioned special characteristics of Inuktitut-
1Though this ratio increases to 2.21 when considering only longer
sentences (20 or more English words), ignoring common short, formu-
laic sentence pairs such as ( Hudson Bay ) ( sanikiluaq ) .
79
% Words Having Specified Alignment Cardinality
NULL 1 2 3 4 5 6 7
English 5 94 <1 <1 0 0 0 0
Inuktitut 3 43 20 14 10 5 3 2
Table 1: Alignment cardinalities for English-Inuktitut word
alignment, computed over the trial data.
English alignment. Our weighted finite-state transducer
(WFST) alignment model, illustrated in Figure 1, struc-
turally enforces monotonicity and 1-to-N cardinality, and
exploits sublexical information by incorporating associ-
ation scores between English words and Inuktitut word
substrings, based on co-occurrence in aligned sentences.
For each English word, an association score was com-
puted not only with each Inuktitut word, but also with
each Inuktitut character string of length ranging from
2 to 10 characters. This is similar to the technique de-
scribed in Martin et al (2003) as part of their construc-
tion of a bilingual glossary from English-Inuktitut bi-
text. However, our goal is different and we keep all the
English-Inuktitut associations, rather than selecting only
the ?best? ones using a greedy method, as do they. Addi-
tionally, before extracting all substrings from each Inuk-
titut word, we added a special character to the word?s
beginning and end (e.g., makkuttut ? makkuttut ), in
order to exploit any preferences for word-initial or -final
placement.
The heuristic association score chosen was
p(worde |wordi) ? p(wordi |worde), computed over allthe aligned sentence pairs. We have in the past observed
this to be a useful indicator of word association, and it
has the nice property of being in the range (0,1].
The WFST aligner is a composition of 4 transduc-
ers.2 The structure of the entire WFST composition en-
forces monotonicity, Inuktitut-to-English 1-N cardinal-
ity, and Inuktitut word fertilities ranging between 1 and
7. This model was implemented using the ATT finite-
state toolkit (Mohri et al, 1997). In Figure 1, [1] is
a linear transducer mapping each English position in a
particular English test sentence to the word at that posi-
tion. It is constructed so as to force each English word
to participate in exactly 1 alignment. [2] is a single-state
transducer mapping English word to Inuktitut substrings
(or full words) with weights derived from the association
scores.3 [3] is a transducer mapping Inuktitut substrings
(and full words) to their position in the Inuktitut test sen-
tence. Its construction allows a single Inuktitut position
to correspond to multiple English positions, while en-
forcing monotonicity. [4] is a transducer regulating the
allowed ?fertility? values of Inuktitut words; each Inuk-
titut word is permitted a fertility of between 1 and 7. The
fertility values are assigned the probabilities correspond-
ing to observed relative frequencies in the trial data, and
2Bracketed numbers in the following discussion refer to the compo-
nent transducers as illustrated in Figure 1.
3Transducers [2] and [4] are shared across all sentence decodings.
1
1 1
1 1 1
<epsilon> (0.82)
<epsilon> 
(1.56)
<epsilon> 
(1.90)
<epsilon> 
<epsilon> 
<epsilon> 
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
illug/1
_pijjut/1
atuqa/2
_inna/2
_amm/3
_am/3
kkutt/4
_makku/4
ajunga/5
akainnar/5
<epsilon><epsilon> <epsilon> <epsilon> <epsilon>
_pijjutigillugu_/1 _innatuqait_/2 _amma_/3 _makkuttut_/4 _uqausiqakainnarumajunga_/5
1/in 2/regards 3/to 4/elders
. . .
6/youth5/and
in regards to elders and youth i want to make general comments  
pijjutigillugu innatuqait amma makkuttut uqausiqakainnarumajunga
and/_am (.54)
youth/_makku (1.10)
youth/_makkuttut_ (3.89)
regards/_pijjutigillugu_ (3.49)
regards/_pijjuti (2.98)
.
.
.
.
.
.
[1]
elders/_inna (0.90)
elders/_innat (1.09)
general/_uqausi (4.54)
and/_amma (.49)
and/_amm (.49)
. . .
[2]
[3]
[4]
(1.90)
2
2 2
2 2 2
<epsilon> (0.82)
<epsilon> 
(1.56)
<epsilon> 
<epsilon> 
<epsilon> 
<epsilon> 
. . .
Figure 1: WFST alignment system in composition order, in-
stantiated for an example sentence from the development (trial)
data. To save space, only a representative portion of each ma-
chine is drawn. Transition weights are costs in the tropical
(min,+) semiring, derived from negative logs of probabilities
and association scores. Nonzero costs are indicated in paren-
theses.
are not conditioned on the identity of the Inuktitut word.
4 English-Inuktitut Transliteration
Although in this corpus English and Inuktitut are both
written in Roman characters, English names are signifi-
cantly transformed when rendered in Inuktitut text. Con-
sider the following English/Inuktitut pairs from the train-
ing corpus: Chartrand/saaturaan, Chretien/kurittian
and the set of training corpus-attested Inuktitut render-
ings of Williams, Campbell, and McLean shown in Ta-
ble 2(A) (which does not include variations containing
the common -mut lexeme, meaning ?to [a person]? (Mal-
lon, 1999)).
Clearly, not only does the English-to-Inuktitut trans-
formation radically change the name string, it does so
in a nondeterministic way which appears to be influ-
enced not only by the phonological preferences of Inuk-
titut but also by differing pronunciations of the name in
question and possibly by differing conventions of trans-
lators (note, for example, maklain versus mikliin for
McLean).
We trained a probabilistic finite-state transducer
(FST) to identify English-Inuktitut transliterated pairs
in aligned sentences. Training string pairs were ac-
quired from the training bitext in the following manner.
Whenever single instances of corresponding honorifics
were found in a sentence pair ? these included the cor-
respondences (Ms , mis); (Mrs , missa/missis); (Mr ,
80
(A) (B)
Williams McLean k sh
ailiams makalain k -4.2 s -7.2
uialims makkalain q -6.2
uilialums maklaain w
uiliam maklain b ui -5.8
uiliammas maklainn p -4.3 v -6.1
uiliams maklait v -5.0
uilians makli o
uliams maklii z a -4.2
viliams makliik j -5.2 aa -4.6
makliin s -5.8 uu -4.9
Campbell maklin u -5.1
kaampu malain ch
kaampul matliin s -5.6 u
kaamvul miklain k -6.8 uu -5.5
kamvul mikliin u -5.6
miklin a -6.2
Table 2: (A) Training-corpus-attested renderings of Williams,
Campbell, and McLean. (B) Top learned Inuktitut substi-
tutions and their log probabilities for several English (shown
underlined) orthographic characters (and character sequences).
Where top substitutions for English characters are shown, none
equal or better were omitted.
mista/mistu) ? the immediately following capitalized En-
glish words (up to 2) were extracted and the same num-
ber of Inuktitut words were extracted to be used as train-
ing pairs. Thus, given the appearance in aligned sen-
tences of ?Mr. Quirke? and ?mista kuak?, the training
pair (Quirke,kuak) would be extracted. Common dis-
tractions such as ?Mr Speaker? were filtered out. In or-
der to focus on the native English name problem (Inuk-
titut name rendering into English is much less noisy) the
English extractions were required to have appeared in a
large, news-corpus-derived English wordlist. This pro-
cedure resulted in a conservative, high-quality list of 434
unique name pairs. The probabilistic FST model we se-
lected was that of a memoryless (single-state) transducer
representing a joint distribution over character substitu-
tions, English insertions, and Inuktitut insertions. This
model is identical to that presented in Ristad and Yianilos
(1997). Prior to training, common English digraphs (e.g.,
?th? and ?sh?) were mapped to unique single characters,
as were doubled consonants. Inuktitut ?ng? and common
two-vowel sequences were also mapped to unique single
characters to elicit higher-quality results from the memo-
ryless transduction model employed. Some results of the
transducer training are displayed in Table 2(B). Proba-
bilistic FST weight training was accomplished using the
Dyna modeling language and DynaMITE parameter op-
timization toolkit (Eisner et al 2004). The translitera-
tion modeling described here differs from such previous
transliteration work as Stalls and Knight (1998) in that
there is no explicit modeling of pronunciation, only a di-
rect transduction between written forms.
In applying transliteration on trial/test data, the
following criteria were used to select English words for
transliteration: (1) Word is capitalized (2) Word is not in
the exclusion list.4 For the top-ranked transliteration of
the English word present in the Inuktitut sentence, all
occurrences of that word in that sentence are marked as
aligned to the English word.
We have yet to evaluate English-Inuktitut translitera-
tion in isolation on a large test set. However, accuracy
on the workshop trial data was 4/4 hypotheses correct,
and on test data 2/6 correct. Of the 4 incorrect test
hypotheses, 2 were mistakes in identifying the correct
transliteration, and 2 mistakes resulted from attempting
to transliterate an English word such as ?Councillors?
which should not be transliterated. Even with a rela-
tively low accuracy, the transliteration model, which is
used only as an individual voter in combination systems,
is unlikely to vote for the incorrect choice of another sys-
tem. Its purpose under system combination is to push a
good alignment link hypothesis up to the required vote
threshold.5
5 IBM Model 4 Alignments
As a baseline and contributor to our combination sys-
tems, we ran GIZA++ (Och and Ney, 2000), to produce
alignments based on IBM Model 4. The IBM align-
ment models are asymmetric, requiring that one lan-
guage be idenitifed as the ?e? language, whose words
are allowed many links each, and the other as the ?f? lan-
guage, whose words are allowed at most one link each.
Although the observed alignment cardinalities naturally
suggest identifying Inuktitut as the ?e? language and En-
glish as the ?f? language, we ran both directions for com-
pleteness.
As a crude first attempt to capture sublexical corre-
spondences in the absence of a method for morpheme
segmentation, we developed a rough syllable segmenter
(spending approximately 2 person-hours), ran GIZA++
to produce alignments treating the syllables as words,
and chose, for each English word, the Inuktitut word or
words the largest number of whose syllables were linked
to it.
In the nomenclature of our results tables, giza++ syl-
labized refers to the latter system, giza++ E(1)-I(N) rep-
resents GIZA++ run with English as the ?e? language,
and giza++ E(N)-I(1) sets English as the ?f? language.
6 System Performance and Combination
Methods
We observed the 4 main systems (3 GIZA++ variants and
WFST) to have significantly different performance pro-
files in terms of precision and recall. Consistently, WFST
4Exclusion list was compiled as follows: (a) capitalized words in
2000 randomly selected English training sentences were examined,
Words such as Clerk, Federation, and Fisheries, which are frequently
capitalized but should not be transliterated, were put into the exclusion
list; in addition, any word with frequency > 50 in the training corpus
was excluded, on the rationale that common-enough words would have
well-estimated translation probabilities already. 50 may seem like a
high threshold until one considers the high variability of the transliter-
ation process as demonstrated in Table 2(A).
5Refer to Section 6 for detailed descriptions of voting.
81
SYSTEM P R F AER |H|/|T |
Individual system performance Trial Data
giza++ E(1)-I(N) 63.4 26.6 37.5 32.9 0.42
giza++ E(N)-I(1) 68.2 59.4 63.5 28.6 0.87
giza++ syllabized 83.6 44.5 58.1 18.3 0.53
WFST 70.3 72.7 71.5 27.8 1.03
Combination system performance Trial Data
F/AER Emphasis 85.4 63.5 72.9 12.3 0.74
AER Emphasis (1) 92.6 44.2 59.9 8.8 0.48
AER Emphasis (2) 95.1 38.0 54.3 9.5 0.40
F Emphasis 74.8 77.6 76.2 21.9 1.04
Recall Emphasis 66.9 82.1 73.8 28.9 1.23
Individual system performance Test Data
giza++ E(1)-I(N) 49.7 18.6 27.0 45.2 0.37
giza++ E(N)-I(1) 64.6 56.2 60.1 32.7 0.87
giza++ syllabized 84.9 44.0 57.9 15.6 0.52
WFST 65.4 68.3 66.8 33.7 1.04
(submitted) Combination system performance Test Data
F/AER Emphasis 84.4 58.6 69.2 14.3 0.69
AER Emphasis (1) 90.7 39.4 54.9 11.5 0.43
AER Emphasis (2) 96.7 32.3 48.4 9.5 0.33
F Emphasis 70.7 73.8 72.2 26.7 1.04
Recall Emphasis 62.6 81.7 70.1 34.2 1.31
Table 3: System performance evaluated on trial and test data.
The precision, recall and F-measure cited are the unlabeled
version (?probable,? in the nomenclature of this shared task).
The gold standard truth for trial data contained 710 alignments.
The test gold standard included 1972 alignments. The column
|H|/|T | lists ratio of hypothesis set size to truth set size for each
system.
won out on F-measure while giza++ syllabized attained
better alignment error rate (AER). Refer to Table 3 for
details of performance on trial and test data.
We investigated a number of system combination
methods, three of which were finally selected for use
in submitted systems. There were two basic methods of
combination: per-link voting and per-English-word vot-
ing.6 In per-link voting, an alignment link is included if
it is proposed by at least a certain number of the partic-
ipating individual systems. In per-English-word voting,
the best outgoing link is chosen for each English word
(the link which is supported by the greatest number of in-
dividual systems). Any ties are broken using the WFST
system choice. A high-recall variant of per-English-word
voting was included in which ties at vote-count 1 (in-
dicating a low-confidence decision) are not broken, but
rather all systems? choices are submitted as hypotheses.
The transliteration model described in Section 4 was
included as a voter in each combination system, though it
made few hypotheses (6 on the test data). Composition of
the submitted systems was as follows: F/AER Empha-
6Combination methods we elected not to submit included voting
with trained weights and various stacked classifiers. The reasoning was
that with such a small development data set ? 25 sentences ? it was
unsafe to put faith in any but the simplest of classifier combination
schemes.
sis - per-link voting with decision criterion >= 2 votes,
over all 5 described systems (WFST, 3 GIZA++ vari-
ants, transliteration). AER Emphasis (I) per-link voting,
>= 2 votes, over all systems except giza++ E(N)-I(1).
AER Emphasis (II) per-link voting, >= 3 votes, over
all systems. F Emphasis per-English-word voting, over
all systems, using WFST as tiebreaker. Recall Empha-
sis per-English-word voting, over all systems, high-recall
variant.
We elected to submit these systems because each
tailors to a distinct evaluation criterion (as suggested
by the naming convention). Experiments on trial data
convinced us that minimizing AER and maximizing F-
measure in a single system would be difficult. Mini-
mizing AER required such high-precision results that the
tradeoff in recall greatly lowered F-measure. It is inter-
esting to note that system combination does provide a
convenient means for adjusting alignment precision and
recall to suit the requirements of the problem or evalua-
tion standard at hand.
7 Conclusions
We have presented several individual and combined sys-
tems for word alignment of Inuktitut-English bitext. The
most successful individual systems were those targeted
to the specific characteristics of the language pair. The
combined systems generally outperformed the individual
systems, and different combination methods were able to
optimize for performance under different evaluation met-
rics. In particular, per-English-word voting performed
well on F-measure, while per-link voting performed well
on AER.
Acknowledgements: Many thanks to Eric Goldlust, David
Smith, and Noah Smith for help in using the Dyna language.
References
J. Eisner, E. Goldlust, and N. A. Smith. 2004. Dyna: A declarative
language for implementing dynamic programs. In Proceedings of the
42nd Annual Meeting of the Association for Computational Linguistics
(ACL 2004), Companion Volume, pages 218-221.
M. Mallon. 1999. Inuktitut linguistics for technocrats. Technical re-
port, Ittukuluuk Language Programs, Iqaluit, Nunavut, Canada.
J. Martin, H. Johnson, B. Farley, and A. Maclachlan. 2003. Align-
ing and using an English-Inuktitut parallel corpus. In Proceedings of
Workshop on Building and Using Parallel Texts: Data Driven Machine
Translation and Beyond, HLT-NAACL 2003.
M. Mohri, F. Pereira, and M. Riley. 1997.
ATT General-purpose finite-state machine software tools.
http://www.research.att.com/sw/tools/fsm/.
F. J. Och and H. Ney. 2000. Improved statistical alignment models. In
Proceedings of the 38th Annual Meeting of the Association for Compu-
tational Linguistics, pages 440?447.
E. S. Ristad and P. N. Yianilos. 1997. Learning string edit distance. In
Machine Learning: Proceedings of the Fourteenth International Con-
ference, pages 287?295.
B. Stalls and K. Knight. 1998. Translating names and technical terms
in arabic text. In Proceedings of the COLING/ACL Workshop on Com-
putational Approaches to Semitic Languages.
82
