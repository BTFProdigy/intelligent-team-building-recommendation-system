Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 145?152
Manchester, August 2008
Mind the Gap: Dangers of Divorcing Evaluations of Summary Content
from Linguistic Quality
John M. Conroy
IDA/Center for Computing Sciences
Bowie, Maryland, USA
conroy@super.org
Hoa Trang Dang
Information Access Division
National Institute of Standards and Technology
Gaithersburg, Maryland, USA
hoa.dang@nist.gov
Abstract
In this paper, we analyze the state of cur-
rent human and automatic evaluation of
topic-focused summarization in the Docu-
ment Understanding Conference main task
for 2005-2007. The analyses show that
while ROUGE has very strong correlation
with responsiveness for both human and
automatic summaries, there is a signifi-
cant gap in responsiveness between hu-
mans and systems which is not accounted
for by the ROUGE metrics. In addition
to teasing out gaps in the current auto-
matic evaluation, we propose a method
to maximize the strength of current auto-
matic evaluations by using the method of
canonical correlation. We apply this new
evaluation method, which we call ROSE
(ROUGE Optimal Summarization Evalua-
tion), to find the optimal linear combina-
tion of ROUGE scores to maximize corre-
lation with human responsiveness.
1 Introduction
ROUGE (Lin, 2004) and its linguistically-
motivated descendent, Basic Elements (BE) (Hovy
et al, 2005), evaluate a summary by computing its
overlap with a set of model (human) summaries;
ROUGE considers lexical n-grams as the unit
for comparing the overlap between summaries,
while Basic Elements uses larger units of com-
parison based on the output of syntactic parsers.
The ROUGE/BE toolkit has become the standard
automatic method for evaluating the content of
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
machine-generated summaries, but the correlation
of these automatic scores with human evaluation
metrics has not always been consistent.
In this paper, we analyze the state of current
human and automatic evaluation of topic-focused
summarization. Using the results of the Document
Understanding Conference main task for 2005-
2007 we explore the correlation between variants
of ROUGE and the human metrics of responsive-
ness and linguistic quality. The analyses expose
a number of challenges and several surprising re-
sults. In particular, while ROUGE has very strong
correlation with responsiveness for both human
and system summaries, there is a significant gap
in responsiveness between humans and systems
which is not accounted for by the ROUGE metrics.
One cause of the gap is that many automatic sum-
marizers truncate the last sentence of their sum-
mary, which shows significant reduction in the re-
sponsiveness score but does not result in a statis-
tically significant drop in ROUGE scores. In ad-
dition to teasing out gaps in the current automatic
evaluation, we propose a method to maximize the
strength of current automatic evaluations by us-
ing the method of canonical correlation. We apply
this new evaluation method, which we call ROSE
(ROUGE Optimal Summarization Evaluation), to
find the optimal linear combination of ROUGE
metrics to maximize correlation with human re-
sponsiveness.
2 DUC 2005-2007 Task and Evaluation
The main task for DUC 2005-2007 was a com-
plex question-focused summarization task that re-
quired summarizers to piece together information
from multiple documents to answer a question or
set of questions as posed in a DUC topic state-
ment. The topic statement was a request for infor-
145
mation that could not be met by just stating a name,
date, quantity, etc. The summarization task was the
same for both human and automatic summarizers:
Given a topic statement and a set of 25-50 rele-
vant newswire documents, the summarization task
was to create from the documents a brief, well-
organized, fluent summary that answered the need
for information expressed in the topic statement.
The summary could be no longer than 250 words.
Summaries over the size limit were truncated, and
no bonus was given for creating a shorter sum-
mary.
NIST Assessors developed the DUC topics used
as test data. There were 50 DUC topics each year
in 2005-2006, and 45 topics in DUC 2007. Each
year, 10 NIST assessors produced a total of 4 hu-
man summaries for each of the topics. The asses-
sor who developed a particular topic always wrote
one of the 4 summaries for that topic.
NIST manually assessed each summary for both
content and readability. Readability was assessed
using a set of linguistic quality questions; sum-
mary content was assessed using the pseudo-
extrinsic measure of content responsiveness.
All summaries for a given topic were judged by
a single assessor who was usually the same as the
topic developer. In all cases, the assessor was one
of the summarizers for the topic. Assessors first
judged each summary for a topic for readability,
assigning a separate score for each of 5 linguis-
tic qualities; each summary for the topic was then
judged for content responsiveness. Each of these
manual evaluations was based on a five-point scale
(1=very poor, 5=very good), resulting in 6 scores
for each summary.
2.1 Evaluation of Readability
The readability of the summaries was assessed us-
ing five linguistic quality questions which mea-
sured qualities of the summary that did not involve
comparison with a reference summary or DUC
topic. The linguistic qualities measured were Q1:
Grammaticality, Q2: Non-redundancy, Q3: Refer-
ential clarity, Q4: Focus, and Q5: Structure and
coherence.
2.2 Evaluation of Content
NIST performed manual pseudo-extrinsic evalu-
ation of peer summaries in the form of assess-
ment of responsiveness. Responsiveness differs
from other measures of summary content such as
SEE coverage (Lin and Hovy, 2002) and Pyramid
scores (Nenkova and Passonneau, 2004) in that it
does not compare a peer summary against a set of
known human summaries. Rather, the assessor is
given a list of randomly ordered, unlabeled sum-
maries (both human and system-generated) for a
topic, and must assign a responsiveness score to
each summary (after having read all the summaries
first).
In DUC 2005-2007, NIST assessors assigned
a content responsiveness score to each summary;
content responsiveness indicated the amount of in-
formation in the summary that helped to satisfy the
information need expressed in the topic statement.
For content responsiveness, the linguistic quality
of the summary was to play a role in the assess-
ment only insofar as it interfered with the expres-
sion of information and reduced the amount of in-
formation that was conveyed.
In DUC 2006, assessors assigned an additional
overall responsiveness score, which was based on
both information content and readability. Asses-
sors judged overall responsiveness only after judg-
ing all their topics for readability and content re-
sponsiveness; however, they were not given di-
rect access to these previously assigned scores, but
were told to give their ?gut? reaction to the overall
responsiveness of each summary.
The content responsiveness score provides a
coarse manual measure of information coverage;
overall responsiveness reflects a combination of
readability and content. Content responsiveness
was largely responsible for determining how as-
sessors perceived the overall quality of a sum-
mary, but readability also played an important role.
While poor readability could downgrade the over-
all responsiveness of a summary that had very
good content responsiveness, very good readabil-
ity could sometimes bolster the overall responsive-
ness score of a less information-laden summary
(Dang, 2006). Attempts at greater readability in
2006 paid off among the peers with the best over-
all responsiveness scores. However, the automatic
peers generally had poor readability, and the aver-
age overall responsiveness for each peer was gen-
erally much lower than its average content respon-
siveness.
In addition to the human assessment of respon-
siveness, NIST computed three ?official? auto-
matic scores using ROUGE and Basic Elements:
ROUGE-2, ROUGE-SU4, and ROUGE-BE recall.
For the BE evaluation, summaries were parsed
146
withMinipar (Lin, 2005), and BE-F were extracted
and matched using the Head-Modifier criterion.
Jackknifing was used for each [peer, topic] pair
so that human and automatic peers could be com-
pared.
3 An Analysis of the Metrics
Figure 1 shows the average scores for each sum-
marizer for DUC 2005, 2006, and 2007. For
each year we report the Pearson correlation coeffi-
cient for ROUGE-2, ROUGE-SU4, and ROUGE-
BE (denoted ?
R2
, ?
SU4
and ?
BE
), against content
responsiveness. This correlation is computed in-
cluding just the systems as the human summariz-
ers are clearly distributed differently.
1
To high-
light the trend in the correlation we fit the systems
data using robust linear regression. This line could
be used to extrapolate the system performance if
ROUGE scores were to increase.
As seen in Figure 1, while both the manual
and the automatic ROUGE scores of the human
summarizers remained relatively constant over the
years, the systems made significant progress in
their automatic scores, with the top systems per-
forming within statistical confidence of the human
summarizers in the ROUGE metrics as reported by
Conroy et al (2007). While the content respon-
siveness scores of the systems also increased as a
group over the years, all systems performed signif-
icantly worse than humans in content responsive-
ness as measured by Tukey?s honestly significant
difference criterion (Conroy et al, 2007). Thus,
there is not only a gap in performance between
humans and systems on this task as measured
manually by content responsiveness, but there is
also a ?metric gap? in using any single variant of
ROUGE to predict content responsiveness. This
metric gap becomes more pronounced as system
performance improves to the point where ROUGE
is unable to distinguish between systems and hu-
mans.
We turn next to an analysis of sources of the per-
formance gap and ?metric gap?. Responsiveness
is a subjective measure, and because NIST uses
the same humans both to generate abstracts and to
evaluate the abstracts, there is the possibility that
humans may give high scores to their own abstract
1
One system each year in 2005-2007 had formatting prob-
lems in their summaries which resulted in abnormally low
ROUGE-BE scores. While these systems are included in the
scatter plot, they are not included in the correlation coefficient
computation.
Figure 1: Scatter plot of average manual con-
tent responsiveness vs. automatic ROUGE scores
(ROUGE-BE, ROUGE-2, and ROUGE-SU4) for
humans (filled points) and systems (unfilled
points), for DUC 2005-2007.
147
just because it not surprisingly ?says what they
would say.? To test this hypothesis we performed
one-side Student T?test, testing if the group of
?Self-Assessed? abstracts had significantly higher
responsiveness for DUC 2005-2007. Indeed, as
Table 1 shows in each year and for both content
and overall responsiveness, humans gave signifi-
cantly higher scores to their own abstracts than the
other human abstracts. This bias adds to the gap
in content responsiveness between the human and
automatic summarizers. Fortunately, this effect is
dampened by the fact that NIST used 10 asses-
sors and on average a human got to assess their
own abstract only 25% of the time. It is notewor-
thy to add that at the Multi-lingual Summarization
Evaluation of 2006, the human assessors were not
the abstractors. This and other factors, notably an
easier task, lead to there being no gap in perfor-
mance between the human and the top scoring sys-
tem (Schlesinger et al, 2008).
Table 1: Mean responsiveness assessment by hu-
mans for their own (Self) vs. Other abstracts.
Data Self Other Signif
DUC 2005 Content 4.88 4.61 0.00277
DUC 2006 Content 4.96 4.68 0.00052
DUC 2006 Overall 4.94 4.67 0.00326
DUC 2007 Content 4.87 4.65 0.01931
We next examine the correlation between re-
sponsiveness and each of the five (manual) met-
rics for linguistic quality. We divide the correlation
into three groups: Human (the group of 10 human
summarizers), Systems (the automatic systems en-
tered into DUC), and Combined (the union of these
two groups). Table 2 gives the Pearson correlation
coefficient and the p?value of statistical signifi-
cance between content responsiveness and each of
the five linguistic quality questions for DUC 2005-
2007. For DUC 2005 there is no significant corre-
lation between the average score of a human or au-
tomatic summarizer on linguistic questions and the
content responsiveness score. The fact that there is
a significant correlation in the ?Combined? case is
primarily due to the fact that the human summa-
rizers scored higher as a group than the systems in
the content metric as well as the linguistic metrics.
In DUC 2006 and 2007, the linguistic question
which rewards summaries for not having redun-
dancy (Q2) has a significant negative correlation
with content responsiveness in the group of sys-
tems. This negative correlation is due largely to the
fact that a number of low scoring systems (includ-
ing the baseline) have no significant redundancy.
Rarely does any system have sentences which are
near duplicates. However, many systems, even
those with relatively high responsiveness scores,
still suffer from clause level redundancy, much of
it in the form of noun phrases for which a human
summarizer would employ pronouns.
Table 3 gives additional correlations between
overall responsiveness and the linguistic questions
for DUC 2006. We contrast the correlations for
DUC 2006 in Table 2 vs. those in Table 3. Not sur-
prisingly, overall responsiveness, which intention-
ally penalizes summaries for linguistic problems,
does correlate more strongly with the linguistic
questions than content responsiveness. Also, we
note that the DUC 2007 correlations for content
responsiveness appear more like those for DUC
2006 overall responsiveness than the correspond-
ing correlations for DUC 2006 content responsive-
ness. NIST did not have sufficient time in 2007
to perform an overall responsiveness evaluation.
We hypothesize that the assessors, many of whom
worked on DUC 2006, may have inadvertently
taken linguistic quality into account more in 2007
than in 2006 for the content responsiveness, since
only one measure was done in 2007.
Finally, it was hypothesized at the DUC 2006
workshop
2
that the human assessors penalize sys-
tems in content responsiveness which end with a
sentence fragment, more than could be accounted
for by the missing content of the sentence frag-
ment. We tested the hypothesis by comparing
the average grammaticality (Q1), content respon-
siveness, and ROUGE scores of the 15 systems
in DUC 2007 that ended their summaries with a
complete sentence, against the 17 systems whose
summaries ended with a sentence fragment. Ta-
ble 4 gives a summary of the results. As measured
by a Student T?test, systems that ended their
summaries with a complete sentence had signif-
icantly higher content responsiveness scores than
those that did not; however, there was no signifi-
cant difference in ROUGE scores. The table lists
ROUGE-2 as an example; these results are consis-
tent with both ROUGE-BE and ROUGE-SU4.
Because linguistic quality clearly influences
content responsiveness, automatic methods of
evaluating summary content that try to maximize
2
Lucy Vanderwende, personal communication
148
Table 2: Correlation and p-values between Content Responsiveness and Linguistic Quality Questions,
DUC 2005-2007
Year Group Q1 Q2 Q3 Q4 Q5
Grammar Non-redund. Refer. Clarity Focus Structure/Coherence
2005 Humans -0.10( 0.78) 0.03( 0.94) 0.06( 0.87) 0.23( 0.53) 0.31( 0.39)
2005 Systems -0.05( 0.79) 0.15( 0.42) 0.19( 0.29) 0.30( 0.10) 0.08( 0.66)
2005 Combined 0.72( 0.00) 0.75( 0.00) 0.87( 0.00) 0.90( 0.00) 0.91( 0.00)
2006 Humans 0.26( 0.47) 0.15( 0.69) 0.04( 0.91) 0.64( 0.05) 0.40( 0.26)
2006 Systems 0.33( 0.05) -0.38( 0.03) 0.27( 0.11) 0.41( 0.01) 0.16( 0.35)
2006 Combined 0.74( 0.00) 0.68( 0.00) 0.86( 0.00) 0.87( 0.00) 0.89( 0.00)
2007 Humans 0.80( 0.01) 0.73( 0.02) 0.24( 0.51) 0.57( 0.09) 0.47( 0.17)
2007 Systems 0.60( 0.00) -0.43( 0.01) 0.59( 0.00) 0.71( 0.00) 0.49( 0.00)
2007 Combined 0.77( 0.00) 0.72( 0.00) 0.85( 0.00) 0.92( 0.00) 0.90( 0.00)
Table 3: Correlation and p-values between Overall Responsiveness and Linguistic Quality Questions,
DUC 2006
Group Q1 Q2 Q3 Q4 Q5
Grammar Non-redund. Refer. Clarity Focus Structure/Coherence
Humans 0.60( 0.06) 0.27( 0.45) 0.39( 0.26) 0.74( 0.01) 0.82( 0.00)
Systems 0.49( 0.00) -0.23( 0.19) 0.55( 0.00) 0.64( 0.00) 0.49( 0.00)
Combined 0.77( 0.00) 0.72( 0.00) 0.89( 0.00) 0.89( 0.00) 0.93( 0.00)
Table 4: Average scores of DUC 2007 systems
ending with a complete sentence vs. those ending
with a fragment.
Metric Sentence Fragment Signif
Grammaticality 3.88 3.24 0.011
Content Resp. 2.79 2.46 0.021
ROUGE-2 0.098 0.092 0.408
correlation with content responsiveness should at-
tempt to include some measures of linguistic qual-
ity. We hypothesize that different variants of
ROUGE may capture different qualities of a sum-
mary; for example, ROUGE-1 may be a good in-
dicator of the relevance of summary content, but
ROUGE variants that take into account larger con-
texts may capture linguistic qualities of the sum-
mary. Hence, a combination of scores (includ-
ing measures of linguistic quality) would be a bet-
ter predictor of ?content? responsiveness.
3
In the
3
An additional weakness in the automatic metrics, which
we do not attempt to address in our current work, is their in-
ability to adequately handle the generalizations that are often
made in model summaries (Dang, 2006), which are abstrac-
tive as opposed to the extractive summaries of most systems.
next section, we present a new evaluation metric
that finds a linear combination of ROUGE met-
rics which, in general, has stronger correlation
with content responsiveness than any of the cur-
rent ROUGE metrics.
4 ROSE: Un Melange de ROUGEs
We developed an automatic content evaluation
model which combines multiple ROUGE scores
using canonical correlation (Hotelling, 1935).
Canonical correlation finds the linear combination
of ROUGE scores that has maximum correlation
with human responsiveness on a given data set.
As this family of models is a ?blend? of ROUGE
scores we call this metric ROSE, for ROUGE Op-
timal Summarization Evaluation. We first apply
canonical correlation for each year of DUC using
a Monte Carlo method. We then report on pre-
liminary experiments that use ROSE models from
one year to predict content responsiveness in sub-
sequent years.
4.1 Blending ROUGE Scoring with a
Canonical Correlation Model
Suppose we are given a set of ROUGE scores and
the corresponding content responsiveness scores.
149
We let a
ij
, for i = 1, ...,m and j = 1, .., n, be the
ROUGE score of type j for the summarizer i, and
b
i
the human content evaluation metric. Canonical
correlation finds an n?long vector x such that
x = argmax ?(
n
?
j=1
a
ij
x
j
, b
i
), (1)
where ?(x, y) is the Pearson correlation between
x and y. A similar approach has been used by Liu
and Gildea (2007) in the application of machine
translation metrics, where they use a gradient opti-
mization method to solve the maximization prob-
lem.
Canonical correlation actually solves a more
general correlation optimization problem, where
the goal is to find two linear combinations of vari-
ables to maximize the correlation between two
sub-spaces. In the application of document sum-
marization, we may wish to consider a matrix B of
human evaluation metrics where b
ij
is the j?th hu-
man evaluation for the i?th summarizer. We could
include, for example, content and overall respon-
siveness or linguistic questions. Here we solve for
(x, y) in the equation below:
(x, y) = argmax ?(
n
?
j=1
a
ij
x
j
,
k
?
j=1
b
ij
y
j
). (2)
This maximization procedure can be solved via a
generalized eigenvalue problem, which we com-
puted in Matlab using a routine distributed by
Borga (2000). For the case studied here, as given
in Equation (1), the generalized eigenvalue reduces
to a linear least squares problem.
To find strong canonical correlations we decided
to explore a large space of metrics. To this end, we
included in our optimization 7 ROUGE automatic
metrics: ROUGE-1,2,3,4,L,SU4, and BE to pre-
dict content responsiveness and (for DUC 2006)
overall responsiveness. As our analyses of the pre-
vious section indicated for DUC 2006 and 2007
there was a significant correlation between the lin-
guistic questions and content responsiveness. We
add questions 1 and 4 to our canonical correla-
tion model to see to what extent these questions
could improve the correlation with content respon-
siveness. While the linguistic questions evaluation
scores are manually generated we combine them
with the automatic methods of ROUGE in an at-
tempt see to what extent these non-content scores
can better model both content and overall respon-
siveness. Thus, in all we consider 9 variables to
predict responsiveness. In order to perform an
evaluation that would avoid over-fitting the data
we used a Monte Carlo method of resampling to
evaluate which of the 2
9
? 1 = 511 combinations
of variables (canonical variates) to include in the
model.
4
In each experiment of the Monte Carlo method
we randomly held back 1/4 of the data (human and
system summarizers) for testing and used 3/4 of
the data to build a canonical variate model. We
found 4000 random samples sufficient to achieve
accuracy within at least 2 digits. For each of
the canonical variate models, 4000 trials are per-
formed and then the computed model is applied
to the held-back portion of the data and its Pear-
son correlation and p-value is reported. These
4000 correlations (and p-values) are then used to
estimate the median correlation for a canonical
variate. The median is computed from the sub-
set of 4000 experiments with statistically signif-
icant correlations on the testing data (95% con-
fidence, a p-value less then 0.05). The canoni-
cal variate with the highest estimated median cor-
relation is then compared with the best perform-
ing ROUGE method. We compare the best of
504=511-7 canonical variates with the best of the
7 ROUGE variants by using the Mann-Whitney U-
test, which tests for equal medians.
The procedure is then repeated using only the
systems to find the ROSE model that gives the best
prediction for just machine summarizers.
Table 5 gives the results of the Monte Carlo ex-
periments. In each case the best canonical variate
and the estimated median correlation are reported
over the set of ROUGE scores and the ROUGE
scores in union with the linguistic questions. As
these results are based on 4000 trials they are more
reliable than the simple correlation analysis done
using the three official DUC automatic metrics,
ROUGE-2, SU4, and BE. We note, in particular,
that occasionally ROUGE-1 and ROUGE-L were
found to be the best predictor even when linguistic
questions were allowed in the model. Not surpris-
ingly, the human evaluation of overall responsive-
ness was harder to predict and the optimal variants
included both linguistic questions 1 and 4.
The ROSE models give the best combinations
4
We also removed one system each year that had a poor
ROUGE-BE score due to formatting problems.
150
Table 5: Monte Carlo Results for Canonical Correlation Model. A * by a variant indicates that it differs
significantly from the best single ROUGE correlation with a p-value of 10
?7
or less as measured by a
Mann Whitney U-test.
Year Metric Summarizer Best ROUGE Corr. ROSE
ROUGE
Corr. ROSE
(ROUGE,Q)
Corr.
2005 Content All BE 0.976 R1,R2,R4,SU4,BE* 0.981 R1,R2,R3,RL,SU4,BE,Q4* 0.986
2005 Content Systems R2 0.939 R1,R2,RL 0.940 R2,RL,SU4,Q4 0.941
2006 Content All RL 0.928 R1,R2,R3,R4* 0.942 RL,Q1* 0.960
2006 Content Systems R1 0.900 R1 0.900 R1 0.900
2007 Content All BE 0.937 R1,R4,RL,BE 0.940 BE,Q4* 0.966
2007 Content Systems R3 0.906 RL,BE* 0.915 R1,RL,BE,Q1* 0.929
2006 Overall All BE 0.893 R1,R2,R3,R4* 0.913 R3,R4,Q1,Q4* 0.946
2006 Overall Systems RL 0.854 RL 0.854 R1,R3,SU4,Q1,Q4* 0.894
of ROUGE scores to give maximum correlation
with the human judgement of content or overall
responsiveness. The ROSE models based on just
ROUGE for the automatic summarizers are an ap-
propriate method to use to compare systems that
did not compete in DUC with those that did.
4.2 Applying ROSE across the Years
To further evaluate the generality of the ROSE
model we apply DUC 2005 canonical correlation
models to DUC 2006 and DUC 2007, and simi-
larly apply the DUC 2006 model to the DUC 2007
data. In these experiments we measure the stabil-
ity of a ROSE model from one year to the next.
(Note, we have also computed a model based on
the combined data of DUC 2005 and DUC 2006
for use with DUC 2007 and these results are com-
parable to those presented.) Here, for simplicity,
we restrict the ROSE model to use only the ?offi-
cial? ROUGE metrics to build a model based on a
given year and then evaluate that model on a subse-
quent year. Table 6 gives results for ROSE models
constructed from only ROUGE-2, ROUGE-SU4,
ROUGE-BE, and content responsiveness to create
the ROSE model for each year; results are also
given for ROSE models (ROSE
+Q1,4
) which also
includes the linguistic questions on grammaticality
(Q1) and focus (Q4).
The ROSE models built from only ROUGE
scores had mixed results, sometimes performing
worse than a single ROUGE score (e.g., the ROSE
model trained on DUC 2005 and evaluated on
DUC 2006), but in other cases performing as well
as or better than single ROUGE scores. These
preliminary results with ROSE illustrate the diffi-
culty in finding a single canonical variate that can
be used from year to year to build ROSE mod-
els based on previous years? data. We hypothesize
that the task is made more difficult due to humans
changing their criteria for judging content respon-
siveness over the years.
On the other hand, ROSE
+Q1,4
models that in-
cluded the linguistic questions Q1 and Q4 always
yielded the best correlation with content respon-
siveness both for the systems and for the group of
combined systems and human summarizers.
5 Conclusions
We analyzed the results of the topic-focused sum-
marization task using the data from DUC 2005-
2007. Our main concern was to expose causes of
the gap that currently exists between automatic and
human evaluation of summary content. As the au-
tomatic ROUGE scores of system summaries ap-
proaches that of human summaries, the disparity
between automatic and manual measures of sum-
mary content becomes a more important concern.
We find that there is a slight bias in the human eval-
uation: humans give their own summaries signifi-
cantly higher scores. Furthermore, the responsive-
ness metric appears to be time varying, i.e., the hu-
mans changed their standards for judging respon-
siveness over the years, making it difficult to use
automatic scores from one year to predict respon-
siveness in another year.
Assessors naturally tend toward taking linguis-
tic quality into account when assessing summaries.
The instructions for assessing content responsive-
ness implicitly acknowledges this; what is surpris-
ing is the extent to which linguistic quality does
influence content responsiveness. In particular, we
demonstrated that content responsiveness in DUC
2006 and 2007 correlated with the linguistic qual-
ity questions of grammar (Q1) and focus (Q4),
and that systems were significantly penalized in
content responsiveness when their summary ended
151
Table 6: Correlation and p-values between content responsiveness and various metrics for each ?Test?
year of DUC. ROSE models were constructed using DUC data from ?Train? year and evaluated on data
from ?Test? year.
Train/Test Summarizer R2 SU4 BE Q1 Q4 ROSE ROSE
+Q1,4
2005/2006 Humans 0.64(0.05) 0.69(0.03) 0.57(0.09) 0.26(0.47) 0.64(0.05) 0.59 (0.07) 0.61(0.06)
2005/2006 Systems 0.83(0.00) 0.85(0.00) 0.85(0.00) 0.33(0.06) 0.41(0.02) 0.83 (0.00) 0.85(0.00)
2005/2006 All 0.90(0.00) 0.88(0.00) 0.90(0.00) 0.74(0.00) 0.87(0.00) 0.90 (0.00) 0.93(0.00)
2005/2007 Humans 0.41(0.24) 0.26(0.47) 0.55(0.10) 0.80(0.01) 0.57(0.09) 0.53 (0.12) 0.57(0.09)
2005/2007 Systems 0.88(0.00) 0.84(0.00) 0.89(0.00) 0.56(0.00) 0.68(0.00) 0.90 (0.00) 0.92(0.00)
2005/2007 All 0.91(0.00) 0.88(0.00) 0.92(0.00) 0.77(0.00) 0.92(0.00) 0.92 (0.00) 0.94(0.00)
2006/2007 Humans 0.41(0.24) 0.26(0.47) 0.55(0.10) 0.80(0.01) 0.57(0.09) 0.52(0.12) 0.67(0.03)
2006/2007 Systems 0.88(0.00) 0.84(0.00) 0.89(0.00) 0.56(0.00) 0.68(0.00) 0.89 (0.00) 0.90(0.00)
2006/2007 All 0.91(0.00) 0.88(0.00) 0.92(0.00) 0.77(0.00) 0.92(0.00) 0.92( 0.00) 0.96(0.00)
with a sentence fragment even though the auto-
matic content measures did not show a statisti-
cally significant difference. The influence of lin-
guistic quality on ?content? responsiveness con-
tributes to the evaluation gap that we see between
ROUGE/BE and this coarse human measure of
summary content.
Automatic methods of evaluating summary con-
tent that try to maximize correlation with content
responsiveness should therefore attempt to include
some measures of linguistic quality. We found that
a blending of ROUGE scores using canonical cor-
relation gave higher correlations with content and
overall responsiveness. When the linguistic ques-
tions Q1 and Q4 were added to the ROSE model,
correlations of up to 0.96 were observed. This re-
sult leads to a natural question: What automatic
methods could be used to approximate the linguis-
tic questions? The work of Barzilay and Lapata
(2005) on local coherence might be a possible can-
didate for estimating focus (Q4), while an auto-
matic parser could be run on the summaries and
the induced score could be used as a surrogate for
grammaticality (Q1).
References
Barzilay, Regina and Mirella Lapata. 2005. Modeling
local coherence: An entity-based approach. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?05), pages
141?148, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
Borga, Magnus. 2000. Matlab function cca().
Conroy, John M., Judith D. Schlesinger, and Dianne P.
O?Leary. 2007. CLASSY 2007 at DUC 2007. In
Proceedings of the Seventh Document Understand-
ing Conference (DUC), Rochester, New York.
Dang, Hoa Trang. 2006. Overview of DUC 2006. In
Proceedings of the Sixth Document Understanding
Conference (DUC), New York City, New York.
Hotelling, H. 1935. The most predictable criterion.
Journal of Educational Psychology, 26:139?142.
Hovy, Eduard, Chin-Yew Lin, and Liang Zhou. 2005.
Evaluating duc 2005 using Basic Elements. In Pro-
ceedings of the Fifth Document Understanding Con-
ference (DUC), Vancouver, Canada.
Lin, Chin-Yew and Eduard Hovy. 2002. Manual and
automatic evaluation of summaries. In Proceedings
of the ACL-02 Workshop on Automatic Summariza-
tion, Philadelphia, PA.
Lin, Chin-Yew. 2004. ROUGE: A package for
automatic evaluation of summaries. In Proceed-
ings of the ACL-04 Workshop: Text Summarization
Branches Out, pages 74?81, Barcelona, Spain.
Lin, Dekang. 2005. A dependency-base method for
evaluating broad-coverage parsers. In Proceedings
of the Nineteenth International Joint Conference on
Artificial Intelligence (IJCAI), Edinburgh, Scotland.
Liu, Ding and Daniel Gildea. 2007. Source-language
features and maximum correlation training for ma-
chine translation evaluation. In Proceedings of the
2007 Meeting of the North American chapter of the
Association for Computational Linguistics (NAACL-
07).
Nenkova, Ani and Rebecca Passonneau. 2004. Evalu-
ating content selection in summarization: The pyra-
mid method. In Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 145?152, Boston, MA.
Schlesinger, Judith D., Dianne P. O?Leary, and John M.
Conroy. 2008. Arabic/English multi-document
summarization with CLASSY?the past and the fu-
ture. In Conference on Intelligent Text Processing
and Computational Linguistics 2008. Lecture Notes
in Computer Science, Springer-Verlag. to appear.
152
QCS: A Tool for Querying, Clustering, and Summarizing Documents
Daniel M. Dunlavy
University of Maryland
ddunlavy@cs.umd.edu
John Conroy
IDA/CCS
conroy@super.org
Dianne P. O?Leary
University of Maryland
oleary@cs.umd.edu
Abstract
The QCS information retrieval (IR) system is
presented as a tool for querying, clustering, and
summarizing document sets. QCS has been de-
veloped as a modular development framework,
and thus facilitates the inclusion of new tech-
nologies targeting these three IR tasks. Details
of the system architecture, the QCS interface,
and preliminary results are presented.
1 Introduction
QCS is a software tool and development framework for
efficient, organized, and streamlined IR from generic
document sets. The system is designed to match a query
to relevant documents, cluster the resulting subset of doc-
uments by topic, and produce a single summary for each
topic. Using QCS for IR, the amount of redundant infor-
mation presented to a user is reduced and the results are
categorized by content.
A survey of previous work using a combination of clus-
tering and summarization to improve IR can be found in
Radev et al (2001b). Of existing IR systems employ-
ing this combination, QCS most resembles the NewsIn-
Essence system (Radev et al, 2001a) in that both sys-
tems can produce multi-document summaries from doc-
ument sets clustered by topic. However, NewsInEssence
is designed for IR from HTML-linked document sets and
QCS has been designed for IR from generic document
sets. Furthermore, one of the most important aspects of
QCS is its modularity, with the ability to plug in alterna-
tive implementations of query-based retrieval, document
clustering, and summarization algorithms.
2 Querying, Clustering, Summarizing
QCS employs a vector space model (Salton et al, 1975)
to represent a set of documents. Choices for the term
weighting currently include the following:
? Local: term frequency, log, binary
? Global: none, normal, idf, idf2, entropy
? Normalization: none, normalized
Detailed descriptions of each of these weighting factors
as well as strategies for using each of these are presented
by Dumais (1991) and Kolda and O?Leary (1998).
The current computational methods used for retrieving
a set of documents that best match a query, clustering a
set of documents by topic, and creating a summary of
multiple documents are as follows:
? Querying: Latent Semantic Indexing (LSI)
? Clustering: spherical k-means
? Summarization: a hidden Markov model (HMM)
and pivoted QR
Detailed descriptions of these methods presented in Deer-
wester et al (1990), Dhillon and Modha (2001), and
Schlesinger et al (2002), respectively.
The interface to QCS (see Figure 1) consists of a col-
lection of JavaTM 1 servlets which format input to and
output from QCS via dynamic HTML documents. This
approach allows all of the computation and formatting to
take place on a JavaTM server, with the only requirement
on the users? systems being that of an HTML-enabled
browser application (e.g., Netscape R? 7.0 ).
3 Results
QCS was tested using data from the 2002 Document Un-
derstanding Conference (http://duc.nist.gov/),
a conference focusing on summarization and the evalu-
ation of summarization systems. The data consisted of
567 news articles categorized into four types, with one
type consisting of articles covering a single natural disas-
ter event reported within a seven day window.
Results of one test producing 100-word extract sum-
maries can be seen in Figure 1, where the query consisted
of the words, ?hurricane? and ?earthquake?. The top
three scoring clusters contained a total 55 articles (32, 11,
and 12, respectively), producing the summaries shown
in the figure. The topics of these three summaries were
a hurricane near Jamaica, catastrophe insurance, and an
earthquake in California, respectively. Despite the lim-
itations of automatic summarization, this example illus-
1 JavaTM is a trademark of Sun Microsystems, Inc.
                                                               Edmonton, May-June 2003
                                                            Demonstrations , pp. 11-12
                                                         Proceedings of HLT-NAACL 2003
Figure 1: The interface to the QCS system includes 1) an input section for the query and choice of document set,
2) a navigation section with links to clustered documents (Q: top documents retrieved for the query and their scores,
C: documents from which summary sentences were drawn and the sentence indices, S: links to multiple or single
document summaries), and 3) an output viewing section, which here contains the default output of multiple document
summaries for the topic clusters.
trates the utility of summarizing by cluster rather than
producing a single summary of the retrieved documents.
Further results are planned for the demonstration, in-
cluding results of using QCS against the data from the
2003 Document Understanding Conference.
Acknowledgements
The authors would like to thank C. David Levermore and
William D. Dorland of the University of Maryland for
their helpful remarks concerning the QCS system.
References
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. JASIS,
41(6):391?407.
Inderjit S. Dhillon and Dharmendra S. Modha. 2001.
Concept decompositions for large sparse text data us-
ing clustering. Machine Learning, 42(1):143?175.
Susan T. Dumais. 1991. Improving the retrieval of infor-
mation from external sources. Behav. Res. Meth. Instr.,
23(6):229?326.
Tamara G. Kolda and Dianne P. O?Leary. 1998. A
semidiscrete matrix decomposition for latent seman-
tic indexing in information retrieval. ACM Trans. Inf.
Sys., 16(4):322?346.
Dragomir R. Radev, Sasha Blair-Goldensohn, Zhu
Zhang, and Revathi Sundara Raghavan. 2001a.
Newsinessence: A system for domain-independent,
real-time news clustering and multi-document summa-
rization. In Human Language Technology Conference,
San Diego, CA.
Dragomir R. Radev, Weiguo Fan, and Zhu Zhang.
2001b. Webinessence: A personalized web-based
multi-document summarization and recommendation
system. In NAACL Workshop on Automatic Summa-
rization, Pittsburgh, PA.
Gerard Salton, A. Wong, and C.S. Yang. 1975. A vec-
tor space model for information retrieval. Communi-
cations of the ACM, 18(11):613?620.
Judith D. Schlesinger, Mary Ellen Okurowski, John M.
Conroy, Dianne P. O?Leary, Anthony Taylor, Jean
Hobbs, and Wilson Harold T. Wilson. 2002. Under-
standing machine performance in the context of hu-
man performance for multi-document summarization.
In Proc. of the Workshop on Automatic Summarization.
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 152?159,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Topic-Focused Multi-document Summarization
Using an Approximate Oracle Score
John M. Conroy, Judith D. Schlesinger
IDA Center for Computing Sciences
Bowie, Maryland, USA
conroy@super.org, judith@super.org
Dianne P. O?Leary
University of Maryland
College Park, Maryland, USA
oleary@cs.umd.edu
Abstract
We consider the problem of producing a
multi-document summary given a collec-
tion of documents. Since most success-
ful methods of multi-document summa-
rization are still largely extractive, in this
paper, we explore just how well an ex-
tractive method can perform. We intro-
duce an ?oracle? score, based on the prob-
ability distribution of unigrams in human
summaries. We then demonstrate that with
the oracle score, we can generate extracts
which score, on average, better than the
human summaries, when evaluated with
ROUGE. In addition, we introduce an ap-
proximation to the oracle score which pro-
duces a system with the best known per-
formance for the 2005 Document Under-
standing Conference (DUC) evaluation.
1 Introduction
We consider the problem of producing a multi-
document summary given a collection of doc-
uments. Most automatic methods of multi-
document summarization are largely extractive.
This mimics the behavior of humans for sin-
gle document summarization; (Kupiec, Pendersen,
and Chen 1995) reported that 79% of the sentences
in a human-generated abstract were a ?direct
match? to a sentence in a document. In contrast,
for multi-document summarization, (Copeck and
Szpakowicz 2004) report that no more than 55% of
the vocabulary contained in human-generated ab-
stracts can be found in the given documents. Fur-
thermore, multiple human summaries on the same
collection of documents often have little agree-
ment. For example, (Hovy and Lin 2002) report
that unigram overlap is around 40%. (Teufel and
van Halteren 2004) used a ?factoid? agreement
analysis of human summaries for a single doc-
ument and concluded that a resulting consensus
summary is stable only if 30?40 summaries are
collected.
In light of the strong evidence that nearly half
of the terms in human-generated multi-document
abstracts are not from the original documents, and
that agreement of vocabulary among human ab-
stracts is only about 40%, we pose two coupled
questions about the quality of summaries that can
be attained by document extraction:
1. Given the sets of unigrams used by four hu-
man summarizers, can we produce an extract
summary that is statistically indistinguish-
able from the human abstracts when mea-
sured by current automatic evaluation meth-
ods such as ROUGE?
2. If such unigram information can produce
good summaries, can we replace this infor-
mation by a statistical model and still produce
good summaries?
We will show that the answer to the first question
is, indeed, yes and, in fact, the unigram set infor-
mation gives rise to extract summaries that usually
score better than the 4 human abstractors! Sec-
ondly, we give a method to statistically approxi-
mate the set of unigrams and find it produces ex-
tracts of the DUC 05 data which outperform all
known evaluated machine entries. We conclude
with experiments on the extent that redundancy
removal improves extracts, as well as a method
of moving beyond simple extracting by employ-
ing shallow parsing techniques to shorten the sen-
tences prior to selection.
152
2 The Data
The 2005 Document Understanding Conference
(DUC 2005) data used in our experiments is par-
titioned into 50 topic sets, each containing 25?50
documents. A topic for each set was intended
to mimic a real-world complex questioning-
answering task for which the answer could not
be given in a short ?nugget.? For each topic,
four human summarizers were asked to provide
a 250-word summary of the topic. Topics were
labeled as either ?general? or ?specific?. We
present an example of one of each category.
Set d408c
Granularity: Specific
Title: Human Toll of Tropical Storms
Narrative: What has been the human toll in death or injury
of tropical storms in recent years? Where and when have
each of the storms caused human casualties? What are the
approximate total number of casualties attributed to each of
the storms?
Set d436j
Granularity: General
Title: Reasons for Train Wrecks
Narrative: What causes train wrecks and what can be done
to prevent them? Train wrecks are those events that result
in actual damage to the trains themselves not just accidents
where people are killed or injured.
For each topic, the goal is to produce a 250-
word summary. The basic unit we extract from
a document is a sentence.
To prepare the data for processing, we
segment each document into sentences using
a POS (part-of-speech) tagger, NLProcessor
(http://www.infogistics.com/posdemo.htm). The
newswire documents in the DUC 05 data have
markers indicating the regions of the document,
including titles, bylines, and text portions. All of
the extracted sentences in this study are taken from
the text portions of the documents only.
We define a ?term? to be any ?non-stop word.?
Our stop list contains the 400 most frequently oc-
curring English words.
3 The Oracle Score
Recently, a crisp analysis of the frequency of
content words used by humans relative to the
high frequency content words that occur in the
relevant documents has yielded a simple and
powerful summarization method called SumBa-
sic (Nenkova and Vanderwende, 2005). SumBa-
sic produced extract summaries which performed
nearly as well as the best machine systems for
generic 100 word summaries, as evaluated in DUC
2003 and 2004, as well as the Multi-lingual Sum-
marization Evaluation (MSE 2005).
Instead of using term frequencies of the corpus
to infer highly likely terms in human summaries,
we propose to directly model the set of terms (vo-
cabulary) that is likely to occur in a sample of hu-
man summaries. We seek to estimate the proba-
bility that a term will be used by a human sum-
marizer to first get an estimate of the best possible
extract and later to produce a statistical model for
an extractive summary system. While the primary
focus of this work is ?task oriented? summaries,
we will also address a comparison with SumBa-
sic and other systems on generic multi-document
summaries for the DUC 2004 dataset in Section 8.
Our extractive summarization system is given a
topic, ? , specified by a text description. It then
evaluates each sentence in each document in the
set to determine its appropriateness to be included
in the summary for the topic ?.
We seek a statistic which can score an individ-
ual sentence to determine if it should be included
as a candidate. We desire that this statistic take
into account the great variability that occurs in
the space of human summaries on a given topic
?. One possibility is to simply judge a sentence
based upon the expected fraction of the ?human
summary?-terms that it contains. We posit an or-
acle, which answers the question ?Does human
summary i contain the term t??
By invoking this oracle over the set of terms
and a sample of human summaries, we can
readily compute the expected fraction of human
summary-terms the sentence contains. To model
the variation in human summaries, we use the or-
acle to build a probabilistic model of the space
of human abstracts. Our ?oracle score? will then
compute the expected number of summary terms a
sentence contains, where the expectation is taken
from the space of all human summaries on the
topic ?.
We model human variation in summary gener-
ation with a unigram bag-of-words model on the
terms. In particular, consider P (t|?) to be the
probability that a human will select term t in a
summary given a topic ?. The oracle score for a
sentence x, ?(x), can then be defined in terms of
153
P :
?(x) =
1
|x|
?
t?T
x(t)P (t|?)
where |x| is the number of distinct terms sentence
x contains, T is the universal set of all terms used
in the topic ? and x(t) = 1 if the sentence x con-
tains the term t and 0 otherwise. (We affectionally
refer to this score as the ?Average Jo? score, as it is
derived the average uni-gram distribution of terms
in human summaries.)
While we will consider several approximations
to P (t|?) (and, correspondingly, ?), we first ex-
plore the maximum-likelihood estimate of P (t|?)
given by a sample of human summaries. Suppose
we are given h sample summaries generated in-
dependently. Let cit(?) = 1 if the i-th summary
contains the term t and 0 otherwise. Then the
maximum-likelihood estimate of P (t?) is given
by
P? (t|?) =
1
h
h?
i=1
cit(?).
We define ?? by replacing P with P? in the defi-
nition of ?. Thus, ?? is the maximum-likelihood
estimate for ?, given a set of h human summaries.
Given the score ??, we can compute an extract
summary of a desired length by choosing the top
scoring sentences from the collection of docu-
ments until the desired length (250 words) is ob-
tained. We limit our selection to sentences which
have 8 or more distinct terms to avoid selecting in-
complete sentences which may have been tagged
by the sentence splitter.
Before turning to how well our idealized score,
??, performs on extract summaries, we first define
the scoring mechanism used to evaluate these sum-
maries.
4 ROUGE
The state-of-the-art automatic summarization
evaluation method is ROUGE (Recall Oriented
Understudy for Gisting Evaluation, (Hovy and Lin
2002)), an n-gram based comparison that was mo-
tivated by the machine translation evaluation met-
ric, Bleu (Papineni et. al. 2001). This system uses
a variety of n-gram matching approaches, some of
which allow gaps within the matches as well as
more sophistcated analyses. Surprisingly, simple
unigram and bigram matching works extremely
well. For example, at DUC 05, ROUGE-2 (bi-
gram match) had a Spearman correlation of 0.95
and a Pearson correlation of 0.97 when compared
with human evaluation of the summaries for re-
sponsiveness (Dang 2005). ROUGE-n for match-
ing n?grams of a summary X against h model
human summaries is given by:
Rn(X) =
?h
j=1
?
i?Nn min(Xn(i),Mn(i, j))
?h
j=1
?
i?Nn Mn(i, j),
where Xn(i) is the count of the number of
times the n-gram i occurred in the summary and
Mn(i, j) is the number of times the n-gram i
occurred in the j-th model (human) summary.
(Note that for brevity of notation, we assume that
lemmatization (stemming) is done apriori on the
terms.)
When computing ROUGE scores, a jackknife
procedure is done to make comparison of machine
systems and humans more amenable. In particu-
lar, if there are k human summaries available for
a topic, then the ROUGE score is computed for a
human summary by comparing it to the remaining
k ? 1 summaries, while the ROUGE score for a
machine summary is computed against all k sub-
sets of size k ? 1 of the human summaries and
taking the average of these k scores.
5 The Oracle or Average Jo Summary
We now present results on the performance of
the oracle method as compared with human sum-
maries. We give the ROUGE-2 (R2) scores as
well as the 95% confidence error bars. In Fig-
ure 1, the human summarizers are represented by
the letters A?H, and systems 15, 17, 8, and 4
are the top performing machine summaries from
DUC 05. The letter ?O? represents the ROUGE-2
scores for extract summaries produced by the ora-
cle score, ??. Perhaps surprisingly, the oracle pro-
duced extracts which performed better than the hu-
man summaries! Since each human only summa-
rized 10 document clusters, the human error bars
are larger. However, even with the large error bars,
we observe that the mean ROUGE-2 scores for the
oracle extracts exceeds the 95% confidence error
bars for several humans.
While the oracle was, of course, given the un-
igram term probabilities, its performance is no-
table on two counts. First, the evaluation met-
ric scored on 2-grams, while the oracle was only
given unigram information. In a sense, optimizing
for ROUGE-1 is a ?sufficient statistic? scoring at
154
the human level for ROUGE-2. Second, the hu-
mans wrote abstracts while the oracle simply did
extracting. Consequently, the documents contain
sufficient text to produce human-quality extract
summaries as measured by ROUGE. The human
performance ROUGE scores indicate that this ap-
proach is capable of producing automatic extrac-
tive summaries that produce vocabulary compara-
ble to that chosen by humans. Human evaluation
(which we have not yet performed) is required to
determine to what extent this high ROUGE-2 per-
formance is indicative of high quality summaries
for human use.
The encouraging results of the oracle score nat-
urally lead to approximations, which, perhaps,
will give rise to strong machine system perfor-
mance. Our goal is to approximate P (t|?), the
probability that a term will be used in a human
abstract. In the next section, we present two ap-
proaches which will be used in tandem to make
this approximation.
Figure 1: The Oracle (Average Jo score) Score ??
6 Approximating P (t|?)
We seek to approximate P (t|?) in an analo-
gous fashion to the maximum-likelihood estimate
P? (t|?). To this end, we devise methods to isolate
a subset of terms which would likely be included
in the human summary. These terms are gleaned
from two sources, the topic description and the
collection of documents which were judged rele-
vant to the topic. The former will give rise to query
terms and the latter to signature terms.
6.1 Query Term Identification
A set of query terms is automatically ex-
tracted from the given topic description. We
identified individual words and phrases from
both the <topic> (Title) tagged paragraph as
well as whichever of the <narr> (Narrative)
Set d408c: approximate, casualties,
death, human, injury, number, recent,
storms, toll, total, tropical, years
Set d436j: accidents, actual, causes,
damage, events, injured, killed, prevent,
result, train, train wrecks, trains, wrecks
Table 1: Query Terms for ?Tropical Storms? and
?Train Wrecks? Topics
tagged paragraphs occurred in the topic descrip-
tion. We made no use of the <granularity>
paragraph marking. We tagged the topic de-
scription using the POS-tagger, NLProcessor
(http://www.infogistics.com/posdemo.htm), and
any words that were tagged with any NN (noun),
VB (verb), JJ (adjective), or RB (adverb) tag were
included in a list of words to use as query terms.
Table 1 shows a list of query terms for our two
illustrative topics.
The number of query terms extracted in this way
ranged from a low of 3 terms for document set
d360f to 20 terms for document set d324e.
6.2 Signature Terms
The second collection of terms we use to estimate
P (t|?) are signature terms. Signature terms are
the terms that are more likely to occur in the doc-
ument set than in the background corpus. They
are generally indicative of the content contained
in the collection of documents. To identify these
terms, we use the log-likelihood statistic suggested
by Dunning (Dunning 1993) and first used in sum-
marization by Lin and Hovy (Hovy and Lin 2000).
The statistic is equivalent to a mutual information
statistic and is based on a 2-by-2 contingency ta-
ble of counts for each term. Table 2 shows a list of
signature terms for our two illustrative topics.
6.3 An estimate of P (t|?)
To estimate P (t|?), we view both the query terms
and the signature terms as ?samples? from ideal-
ized human summaries. They represent the terms
that we would most likely see in a human sum-
mary. As such, we expect that these sample terms
may approximate the underlying set of human
summary terms. Given a collection of query terms
and signature terms, we can readily estimate our
target objective, P (t|?) by the following:
Pqs(t|?) =
1
2
qt(?) +
1
2
st(?)
155
Set d408c: ahmed, allison, andrew,
bahamas, bangladesh, bn, caribbean,
carolina, caused, cent, coast, coastal,
croix, cyclone, damage, destroyed, dev-
astated, disaster, dollars, drowned, flood,
flooded, flooding, floods, florida, gulf,
ham, hit, homeless, homes, hugo, hurri-
cane, insurance, insurers, island, islands,
lloyd, losses, louisiana, manila, miles,
nicaragua, north, port, pounds, rain,
rains, rebuild, rebuilding, relief, rem-
nants, residents, roared, salt, st, storm,
storms, supplies, tourists, trees, tropi-
cal, typhoon, virgin, volunteers, weather,
west, winds, yesterday.
Set d436j: accident, accidents, am-
munition, beach, bernardino, board,
boulevard, brake, brakes, braking, cab,
car, cargo, cars, caused, collided, col-
lision, conductor, coroner, crash, crew,
crossing, curve, derail, derailed, driver,
emergency, engineer, engineers, equip-
ment, fe, fire, freight, grade, hit, holland,
injured, injuries, investigators, killed,
line, locomotives, maintenance, mechan-
ical, miles, morning, nearby, ntsb, oc-
curred, officials, pacific, passenger, pas-
sengers, path, rail, railroad, railroads,
railway, routes, runaway, safety, san,
santa, shells, sheriff, signals, southern,
speed, station, train, trains, transporta-
tion, truck, weight, wreck
Table 2: Signature Terms for ?Tropical Storms?
and ?Train Wrecks? Topics
Figure 2: Scatter Plot of ?? versus ?qs
where st(?)=1 if t is a signature term for topic ?
and 0 otherwise and qt(?) = 1 if t is a query term
for topic ? and 0 otherwise.
More sophisticated weightings of the query and
signature have been considered; however, for this
paper we limit our attention to the above ele-
mentary scheme. (Note, in particular, a psuedo-
relevance feedback method was employed by
(Conroy et. al. 2005), which gives improved per-
formance.)
Similarly, we estimate the oracle score of a sen-
tence?s expected number of human abstract terms
as
?qs(x) =
1
|x|
?
t?T
x(t)Pqs(t|?)
where |x| is the number of distinct terms that sen-
tence x contains, T is the universal set of all terms
and x(t) = 1 if the sentence x contains the term t
and 0 otherwise.
For both the oracle score and the approximation,
we form the summary by taking the top scoring
sentences among those sentences with at least 8
distinct terms, until the desired length (250 words
for the DUC05 data) is achieved or exceeded. (The
threshold of 8 was based upon previous analysis
of the sentence splitter, which indicated that sen-
tences shorter than 8 terms tended not be be well
formed sentences or had minimal, if any, content.)
If the length is too long, the last sentence chosen
is truncated to reach the target length.
Figure 2 gives a scatter plot of the oracle score
? and its approximation ?qs for all sentences with
at least 8 unique terms. The overall Pearson corre-
lation coefficient is approximately 0.70. The cor-
relation varies substantially over the topics. Fig-
ure 3 gives a histogram of the Pearson correlation
coefficients for the 50 topic sets.
156
Figure 3: Histogram of Document Set Pearson Co-
efficients of ?? versus ?qs
7 Enhancements
In the this section we explore two approaches to
improve the quality of the summary, linguistic pre-
processing (sentence trimming) and a redundancy
removal method.
7.1 Linguistic Preprocessing
We developed patterns using ?shallow parsing?
techniques, keying off of lexical cues in the sen-
tences after processing them with the POS-tagger.
We initially used some full sentence eliminations
along with the phrase eliminations itemized be-
low; analysis of DUC 03 results, however, demon-
strated that the full sentence eliminations were not
useful.
The following phrase eliminations were made,
when appropriate:
? gerund clauses;
? restricted relative-clause appositives;
? intra-sentential attribution;
? lead adverbs.
See (Dunlavy et. al) for the specific rules used
for these eliminations. Comparison of two runs
in DUC 04 convinced us of the benefit of applying
these phrase eliminations on the full documents,
prior to summarization, rather than on the selected
sentences after scoring and sentence selection had
been performed. See (Conroy et. al. 2004) for
details on this comparison.
After the trimmed text has been generated, we
then compute the signature terms of the document
sets and recompute the approximate oracle scores.
Note that since the sentences have usually had
some extraneous information removed, we expect
some improvement in the quality of the signature
terms and the resulting scores. Indeed, the median
ROUGE-2 score increases from 0.078 to 0.080.
7.2 Redundancy Removal
The greedy sentence selection process we de-
scribed in Section 6 gives no penalty for sentences
which are redundant to information already con-
tained in the partially formed summary. A method
for reducing redundancy can be employed. One
popular method for reducing redundancy is max-
imum marginal relevance (MMR) (2). Based on
previous studies, we have found that a pivoted
QR, a method from numerical linear algebra, has
some advantages over MMR and performs some-
what better.
Pivoted QR works on a term-sentence matrix
formed from a set of candidate sentences for in-
clusion in the summary. We start with enough
sentences so the total number of terms is approx-
imately twice the desired summary length. Let B
be the term-sentence matrix with Bij = 1 if sen-
tence j contains term i.
The columns of B are then normalized so their
2-norm (Euclidean norm) is the corresponding ap-
proximate oracle score, i.e. ?qs(bj), where bj is
the j-th column ofB.We call this normalized term
sentence matrix A.
Given a normalized term-sentence matrix A,
QR factorization attempts to select columns of A
in the order of their importance in spanning the
subspace spanned by all of the columns. The stan-
dard implementation of pivoted QR decomposi-
tion is a ?Gram-Schmidt? process. The first r sen-
tences (columns) selected by the pivoted QR are
used to form the summary. The number r is cho-
sen so that the summary length is close to the tar-
get length. A more complete description can be
found in (Conroy and O?Leary 2001).
Note, that the selection process of using the piv-
oted QR on the weighted term sentence matrix
will first choose the sentence with the highest ?pq
score as was the case with the greedy selection
process. Its subsequent choices are affected by
previous choices as the weights of the columns are
decreased for any sentence which can be approxi-
mated by a linear combination of the current set of
selected sentences. This is more general than sim-
ply demanding that the sentence have small over-
lap with the set of previous chosen sentences as
157
Figure 4: ROUGE-2 Performance of Oracle Score
Approximations ?? vs. Humans and Peers
would be done using MMR.
8 Results
Figure 4 gives the ROUGE-2 scores with error
bars for the approximations of the oracle score as
well as the ROUGE-2 scores for the human sum-
marizers and the top performing systems at DUC
2005. In the graph, qs is the approximate oracle,
qs(p) is the approximation using linguistic prepro-
cessing, and qs(pr) is the approximation with both
linguistic preprocessing and redundancy removal.
Note that while there is some improvement using
the linguistic preprocessing, the improvement us-
ing our redundancy removal technique is quite mi-
nor. Regardless, our system using signature terms
and query terms as estimates for the oracle score
performs comparably to the top scoring system at
DUC 05.
Table 3 gives the ROUGE-2 scores for the re-
cent DUC 06 evaluation which was essentially
the same task as for DUC 2005. The manner in
which the linguistic preprocessing is performed
has changed from DUC 2005, although the types
of removals have remained the same. In addition,
pseudo-relevance feedback was employed for re-
dundancy removal as mentioned earlier. See (Con-
roy et. al. 2005) for details.
While the main focus of this study is task-
oriented multidocument summarization, it is in-
structive to see how well such an approach would
perform for a generic summarization task as with
the 2004 DUC Task 2 dataset. Note, the ? score
for generic summaries uses only the signature
term portion of the score, as no topic descrip-
tion is given. We present ROUGE-1 (rather than
Submission Mean 95% CI Lower 95% CI Upper
O (?) 0.13710 0.13124 0.14299
C 0.13260 0.11596 0.15197
D 0.12380 0.10751 0.14003
B 0.11788 0.10501 0.13351
G 0.11324 0.10195 0.12366
F 0.10893 0.09310 0.12780
H 0.10777 0.09833 0.11746
J 0.10717 0.09293 0.12460
I 0.10634 0.09632 0.11628
E 0.10365 0.08935 0.11926
A 0.10361 0.09260 0.11617
24 0.09558 0.09144 0.09977
?(pr)qs 0.09160 0.08729 0.09570
15 0.09097 0.08671 0.09478
12 0.08987 0.08583 0.09385
8 0.08954 0.08540 0.09338
23 0.08792 0.08371 0.09204
?(p)qs 0.08738 0.08335 0.09145
?qs 0.08713 0.08317 0.09110
28 0.08700 0.08332 0.09096
Table 3: Average ROUGE 2 Scores for DUC06:
Humans A-I
ROUGE-2) scores with stop words removed for
comparison with the published results given in
(Nenkova and Vanderwende, 2005).
Table 4 gives these scores for the top perform-
ing systems at DUC04 as well as SumBasic and
?(pr)qs , the approximate oracle based on signature
terms alone with linguistic preprocess trimming
and pivot QR for redundancy removal. As dis-
played, ?(pr)qs scored second highest and within the
95% confidence intervals of the top system, peer
65, as well as SumBasic, and peer 34.
Submission Mean 95% CI Lower 95% CI Upper
F 0.36787 0.34442 0.39467
B 0.36126 0.33387 0.38754
O (?) 0.35810 0.34263 0.37330
H 0.33871 0.31540 0.36423
A 0.33289 0.30591 0.35759
D 0.33212 0.30805 0.35628
E 0.33277 0.30959 0.35687
C 0.30237 0.27863 0.32496
G 0.30909 0.28847 0.32987
?(pr)qs 0.308 0.294 0.322
peer 65 0.308 0.293 0.323
SumBasic 0.302 0.285 0.319
peer 34 0.290 0.273 0.307
peer 124 0.286 0.268 0.303
peer 102 0.285 0.267 0.302
Table 4: Average ROUGE 1 Scores with stop
words removed for DUC04, Task 2
158
9 Conclusions
We introduced an oracle score based upon the
simple model of the probability that a human
will choose to include a term in a summary.
The oracle score demonstrated that for task-based
summarization, extract summaries score as well
as human-generated abstracts using ROUGE. We
then demonstrated that an approximation of the or-
acle score based upon query terms and signature
terms gives rise to an automatic method of summa-
rization, which outperforms the systems entered
in DUC05. The approximation also performed
very well in DUC 06. Further enhancements based
upon linguistic trimming and redundancy removal
via a pivoted QR algorithm give significantly bet-
ter results.
References
Jamie Carbonnell and Jade Goldstein ?The of MMR,
diversity-based reranking for reordering documents
and producing summaries.? In Proc. ACM SIGIR,
pages 335?336.
JohnM. Conroy and Dianne P. O?Leary. ?Text Summa-
rization via HiddenMarkovModels and Pivoted QR
Matrix Decomposition?. Technical report, Univer-
sity of Maryland, College Park, Maryland, March,
2001.
John M. Conroy and Judith D. Schlesinger and
Jade Goldstein and Dianne P. O?Leary, Left-
Brain Right-Brain Multi-Document Summariza-
tion, Document Understanding Conference 2004
http://duc.nist.gov/ 2004
John M. Conroy and Judith D. Schlesinger and Jade
Goldstein, CLASSY Tasked Based Summarization:
Back to Basics, Document Understanding Confer-
ence 2005 http://duc.nist.gov/ 2005
John M. Conroy and Judith D. Schlesinger Dianne
P. O?Leary, and Jade Goldstein, Back to Basciss:
CLASSY 2006, Document Understanding Confer-
ence 2006 http://duc.nist.gov/ 2006
Terry Copeck and Stan Szpakowicz 2004 Vocabulary
Agreement Among Model Summaries and Source
Documents In ACL Text Summarization Workshop,
ACL 2004.
Hoa Trang Dang Overview of DUC 2005 Document
Understanding Conference 2005 http://duc.nist.gov
Daniel M. Dunlavy and John M. Conroy and Judith
D. Schlesinger and Sarah A. Goodman and Mary
Ellen Okurowski and Dianne P. O?Leary and Hans
van Halteren, ?Performance of a Three-Stage Sys-
tem for Multi-Document Summarization?, DUC 03
Conference Proceedings, http://duc.nist.gov/, 2003
Ted Dunning, ?Accurate Methods for Statistics of Sur-
prise and Coincidence?, Computational Linguistics,
19:61-74, 1993.
Julian Kupiec,, Jan Pedersen, and Francine Chen. ?A
Trainable Document Summarizer?. Proceedings of
the 18th Annual International SIGIR Conference
on Research and Development in Information Re-
trieval, pages 68?73, 1995.
Chin-Yew Lin and Eduard Hovy. The automated ac-
quisition of topic signatures for text summarization.
In Proceedings of the 18th conference on Computa-
tional linguistics, pages 495?501, Morristown, NJ,
USA, 2000. Association for Computational Linguis-
tics.
Chin-Yew Lin and Eduard Hovy. Manual and Auto-
matic Evaluation of Summaries In Document Un-
derstanding Conference 2002 http:/duc.nist.gov
Multi-Lingual Summarization Evaluation
http://www.isi.edu/ cyl/MTSE2005/MLSummEval.html
NLProcessor http://www.infogistics.com/posdemo.htm
Ani Nenkova and Lucy Vanderwende. 2005. The
Impact of Frequency on Summarization,MSR-TR-
2005-101. Microsoft Research Technical Report.
Kishore Papineni and Salim Roukos and Todd Ward
and Wei-Jing Zhu, Bleu: a method for automatic
evaluation of machine translation, Technical Re-
port RC22176 (W0109-022), IBM Research Divi-
sion, Thomas J. Watson Research Center (2001)
Simone Teufel and Hans van Halteren. 2004. 4:
Evaluating Information Content by Factoid Analy-
sis: Human Annotation and Stability, EMNLP-04,
Barcelona
159
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 467?473,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Ranking Human and Machine Summarization Systems
Peter Rankel
University of Maryland
College Park, Maryland
rankel@math.umd.edu
John M. Conroy
IDA/Center for Computing Sciences
Bowie, Maryland
conroyjohnm@gmail.com
Eric V. Slud
University of Maryland
College Park, Maryland
evs@math.umd.edu
Dianne P. O?Leary
University of Maryland
College Park, Maryland
oleary@cs.umd.edu
Abstract
The Text Analysis Conference (TAC) ranks
summarization systems by their average score
over a collection of document sets. We in-
vestigate the statistical appropriateness of this
score and propose an alternative that better
distinguishes between human and machine
evaluation systems.
1 Introduction
For the past several years, the National Institute of
Standards and Technology (NIST) has hosted the
Text Analysis Conference (TAC) (previously called
the Document Understanding Conference (DUC))
(Nat, 2010). A major theme of this conference is
multi-document summarization: machine summa-
rization of sets of related documents, sometimes
query-focused and sometimes generic. The sum-
marizers are judged by how well the summaries
match human-generated summaries in either auto-
matic metrics such as ROUGE (Lin and Hovy, 2003)
or manual metrics such as responsiveness or pyra-
mid evaluation (Nenkova et al, 2007). Typically the
systems are ranked by their average score over all
document sets.
Ranking by average score is quite appropriate un-
der certain statistical hypotheses, for example, when
each sample is drawn from a distribution which
differs from the distribution of other samples only
through a location shift (Randles and Wolfe, 1979).
However, a non-parametric (rank-based) analysis of
variance on the summarizers? scores on each docu-
ment set revealed an impossibly small p-value (less
Figure 1: Confidence Intervals from a non-parametric
Tukey?s honestly significant difference test for 46 TAC
2010 update document sets. The blue confidence interval
(for document set d1032) does not overlap any of the 30
red intervals. Hence, the test concludes that 30 document
sets have mean significantly different from the mean of
d1032.
467
Figure 2: Overall Responsiveness scores.
Figure 3: Linguistic scores.
Figure 4: Pyramid scores.
Figure 5: ROUGE-2 scores for the TAC 2010 update
summary task, organized by document set (y-axis) and
summarizer (x-axis). The 51 summarizers fall into two
distinct groups: machine systems (first 43 columns) and
humans (last 8 columns). Note that each human only
summarized half of the document sets, thus creating 23
missing values in each of the last 8 columns. Black is
used to indicate missing values in the last 8 columns and
low scores in the first 43 columns.
than 10?12 using Matlab?s kruskalwallis 1),
providing evidence that a summary?s score is not
independent of the document set. This effect can
be seen in Figure 1, showing the confidence bands,
as computed by a Tukey honestly significant differ-
ence test for each document set?s difficulty as mea-
sured by the mean rank responsiveness score for
TAC 2010. The test clearly shows that the summa-
rizer performances on different document sets have
different averages.
We further illustrate this in Figures 2 ? 5, which
show the scores of various summarizers on vari-
ous document sets using standard human and au-
tomatic evaluation methods (Dang and Owczarzak,
2008) of overall responsiveness, linguistic quality,
pyramid scores, and ROUGE-2 using color to indi-
cate the value of the score. Some rows are clearly
darker, indicating overall lower scores for the sum-
1The Kruskal-Wallis test performs a one-way analysis of
variance of document-set differences after first converting the
summary scores for each sample to their ranks within the pooled
sample. Computed from the converted scores, the Kruskal-
Wallis test statistic is essentially the ratio of the between-group
sum of squares to the combined within-group sum of squares.
468
maries of these documents, and the variances of the
scores differ row-by-row. These plots show qualita-
tively what the non-parametric analysis of variance
demonstrates statistically. While the data presented
was for the TAC 2010 update document sets, similar
results hold for all the TAC 2008, 2009, and 2010
data. Hence, it may be advantageous to measure
summarizer quality by accounting for heterogeneity
of documents within each test set. A non-parametric
paired test like the Wilcoxon signed-rank is one way
to do this. Another way would be paired t-tests.
In the paper (Conroy and Dang, 2008) the authors
noted that while there is a significant gap in perfor-
mance between machine systems and human sum-
marizers when measured by average manual met-
rics, this gap is not present when measured by the
averages of the best automatic metric (ROUGE). In
particular, in the DUC 2005-2007 data some systems
have ROUGE performance within the 95% confi-
dence intervals of several human summarizers, but
their pyramid, linguistic, and responsiveness scores
do not achieve this level of performance. Thus,
the inexpensive automatic metrics, as currently em-
ployed, do not predict well how machine summaries
compare to human summaries.
In this work we explore the use of document-
paired testing for summarizer comparison. Our main
approach is to consider each pair of two summa-
rizers? sets of scores (over all documents) as a bal-
anced two-sample dataset, and to assess that pair?s
mean difference in scores through a two-sample T
or Wilcoxon test, paired or unpaired. Our goal has
been to confirm that human summarizer scores are
uniformly different and better on average than ma-
chine summarizer scores, and to rate the quality of
the statistical method (T or W, paired or unpaired)
by the consistency with which the human versus
machine scores show superior human performance.
Our hope is that paired testing, using either the stan-
dard paired two-sample t-test or the distribution-
free Wilcoxon signed-rank test, can provide greater
power in the statistical analysis of automatic metrics
such as ROUGE.
2 Size and Power of Tests
Statistical tests are generally compared by choosing
rejection thresholds to achieve a certain small prob-
ability of Type I error (usually as ? = .05). Given
multiple tests with the same Type I error, one prefers
the test with the smallest probability of Type II error.
Since power is defined to be one minus the Type II
error probability, we prefer the test with the most
power. Recall that a test-statistic S depending on
available data-samples gives rise to a rejection re-
gion by defining rejection of the null hypothesis H0
as the event {S ? c} for a cutoff or rejection thresh-
old c chosen so that
P (S ? c) ? ?
for all probability laws compatible with the null hy-
pothesis where the (nominal) significance level ?
is chosen in advance by the statistician, usually as
? = .05. However, in many settings, the null hy-
pothesis comprises many possible probability laws,
as here where the null hypothesis is that the under-
lying probability laws for the score-samples of two
separate summarizers are equal, without specifying
exactly what that probability distribution is. In this
case, the significance level is an upper bound for the
attained size of the test, defined as supP?H0 P (S ?
c), the largest rejection probability P (S ? c)
achieved by any probability law compatible with the
null hypothesis. The power of the test then depends
on the specific probability law Q from the consid-
ered alternatives in HA. For each such Q, and given
a threshold c, the power for the test at Q is the re-
jection probability Q(S ? c). These definitions re-
flect the fact that the null and alternative hypothe-
ses are composite, that is, each consists of multiple
probability laws for the data. One of the advan-
tages of considering a distribution-free two-sample
test statistic such as the Wilcoxon is that the proba-
bility distribution for the statistic S is then the same
for all (continuous, or non-discrete) probability laws
P ? H0, so that one cutoff c serves for all of H0
with all rejection probabilities equal to ?. 2
Two test statistics, say S and S?, are generally
compared in terms of their powers at fixed alterna-
tives Q in the alternative hypothesis HA, when their
respective thresholds c, c? have been defined so that
the sizes of the respective tests, supP?H0 P (S ?
2The Wilcoxon test is not distribution-free for discrete data.
However, the discrete TAC data can be thought of as rounded
continuous data, rather than as truly discrete data.
469
c) and supP?H0 P (S? ? c?), are approximatelyequal. In this paper, the test statistics under consid-
eration are ? in one-sided testing ? the (unpaired)
two-sample t test with pooled sample variance (T ),
the paired two-sample t test (T p), and the (paired)
signed-rank Wilcoxon test (W ); and for two-sided
testing, S is defined by the absolute value of one
of these statistics. The thresholds c for the tests
can be defined either by theoretical distributions, by
large-sample approximations, or by data-resampling
(bootstrap) techniques, and (only) in the last case
are these thresholds data-dependent, or random. We
explain these notions with respect to the two-sample
data-structure in which the scores from the first sum-
marizer are denoted X1, . . . , Xn, where n is the
number of documents with non-missing scores for
both summarizers, and the scores from the second
summarizer are Y1, . . . , Yn. Let Zk = Xk ? Yk
denote the document-wise differences between the
summarizers? scores, and Z? = n?1?nk=1 Zk betheir average. Then the paired statistics are defined
as
T p =
?
n(n? 1) Z?/(
n?
k=1
(Zk ? Z?)2)1/2
and
W =
n?
k=1
sgn(Zk)R+k
where R+k is the rank of |Zk| among
|Z1|, . . . , |Zn|. Note that under both null and alter-
native hypotheses, the variates Zk are assumed in-
dependent identically distributed (iid), while under
H0, the random variables Zk are symmetric about 0.
The t-statistic T p is ?parametric? in the sense that
exact theoretical calculations of probabilities P (a <
T p < b) depend on the assumption of normality of
the differences Zk, and when that holds, the two-
sided cutoff c = c(T p) is defined as the 1 ? ?/2
quantile of the tn?1 distribution with n ? 1 degrees
of freedom. However, when n is moderately or
very large, the cutoff is well approximated by the
standard-normal 1 ? ?/2 quantile z?/2, and T p be-
comes approximately nonparametrically valid with
this cutoff, by the Central Limit Theorem. The
Wilcoxon signed-rank statistic W has theoretical
cutoff c = c(W ) which depends only on n, when-
ever the data Zk are continuously distributed; but for
large n, the cutoff is given simply as ?n3/12 ? z?/2.
When there are ties (as might be common in discrete
data), the calculation of cutoffs and p-values for
Wilcoxon becomes slightly more complicated and
is no longer fully nonparametric except in a large-
sample approximate sense.
The situation for the two-sample unpaired t-
statistic T currently used in TAC evaluation is not
so neat. Even when the two samplesX = {Xk}nk=1and Y = {Yk}nk=1 are independent, exact theoret-ical distribution of cutoffs is known only under the
parametric assumption that the scores are normally
distributed (and in the case of the pooled-sample-
variance statistic, that Var(Xk) = Var(Yk).) How-
ever, an essential element of the summarization data
is the heterogeneity of documents. This means that
while {Xk}nk=1 can be viewed as iid scores whendocuments are selected randomly ? and not neces-
sarily equiprobably ? from the ensemble of all pos-
sible documents, the Yk and Xk samples are de-
pendent. Still, the pairs {(Xk, Yk)}nk=1, and there-fore the differences {Zk}nk=1, are iid which is whatmakes paired testing valid. However, there is no the-
oretical distribution for T from which to calculate
valid quantiles c for cutoffs, and therefore the use of
the unpaired t-statistic cannot be recommended for
TAC evaluation.
What can be done in a particular dataset, like the
TAC summarization score datsets we consider, to
ascertain the approximate validity of theoretically
derived large-sample cutoffs for test statistics? In
the age of plentiful and fast computers, quite a lot,
through the powerful computational machinery of
the bootstrap (Efron and Tibshirani, 1993).
The idea of bootstrap hypothesis testing (Efron
and Tibshirani, 1993), (Bickel and Ren, 2001) is to
randomly sample with replacement (the rows with
non-missing data in) the dataset {(Xk, Yk)}nk=1 insuch a way as to generate representative data that
plausibly would have been seen if two-sample score
data had been generated from two equally effec-
tive summarizers with score distributional charac-
teristics like the pooled scores from the two ob-
served summarizers. We have done this in two dis-
tinct ways, each creating 2000 datasets with n paired
scores:
MC Monte Carlo Method. For each of many it-
470
erations (in our case 2000), define a new
dataset {(X ?k, Y ?k)}nk=1 by independently swap-ping Xk and Yk with probability 1/2. Hence,
(X ?k, Y ?k) = (Xk, Yk) with probability 1/2 and
(Yk, Xk) with probability 1/2.
HB Hybrid MC/Bootstrap. For each of 2000
iterations, create a re-sampled dataset
{(X ??k , Y ??k )}nk=1 in the following way. First,sample n pairs (Xk, Yk) with replacement
from the original dataset. Then, as above,
randomly swap the components of each pair,
each with 1/2 probability.
Both of these two methods can be seen to gener-
ate two-sample data satisfying H0, with each score-
sample?s distribution obtained as a mixture of the
distributions actually generating the X and Y sam-
ples. The empirical qth quantiles for a statistic
S = S(X,Y) such as |W | or |T p| are estimated
from the resampled data as F??1S (q), where F?S(t) issimply the fraction of times (out of 2000) that the
statistic S applied to the constructed dataset had a
value less than or equal to t. The upshot is that the
1 ? ? empirical quantile for S based on either of
these simulation methods serves as a data-dependent
cutoff c attaining approximate size ? for all H0-
generated data. The MC and HB methods will be
employed in Section 4 to check the theoretical p-
values.
3 Relative Efficiency ofW versus T p
Statistical theory does have something to say about
the comparative powers of paired W versus T p
statistics. These statistics have been studied (Ran-
dles and Wolfe, 1979), in terms of their asymp-
totic relative efficiency for location-shift alternatives
based on symmetric densities (f(z??) is a location-
shift of f(z)). For many pairs of parametric and
rank-based statistics S, S?, including W and T p, the
following assertion has been proved for testing H0
at significance level ?.
First assume the Zk are distributed according to
some density f(z ? ?), where f(z) is a symmet-
ric function (f(?z) = f(z)). Next assume ? = 0
under H0. When n gets large the powers at any al-
ternatives with very small ? = ?/?n, ? 6= 0, can
be made asymptotically equal by using samples of
size n with statistic S and of size ? ? n with statistic
S?. Here ? = ARE(S, S?) is a constant not depend-
ing on n or ? but definitely depending on f , called
asymptotic relative efficiency of S with respect to S?.
(The smaller ? < 1 is, the more statistic S? is pre-
ferred among the two.)
Using this definition, it is known (Randles and
Wolfe 1979, Sec. 5.4 leading up to Table 5.4.7 on
p. 167) that the Wilcoxon signed-rank statistic W
provides greater robustness and often much greater
efficiency than the paired T, with ARE which is 0.95
with f a standard normal density, and which is never
less than 0.864 for any symmmetric density f . How-
ever, in our context, continuous scores such as pyra-
mid exhibit document-specific score differences be-
tween summarizers which often have approximately
normal-looking histograms, and although the alter-
natives perhaps cannot be viewed as pure location
shifts, it is unsurprising in view of the ARE theory
cited above that the W and T paired tests have very
similar performance. Nevertheless, as we found by
statistical analysis of the TAC data, both are far su-
perior to the unpaired T-statistic, with either theoret-
ical or empirical bootstrapped p-values.
4 Testing Setup and Results
To evaluate our ideas, we used the TAC data from
2008-2010 and focused on three manual metrics
(overall responsiveness, pyramid score, and lin-
guistic quality score) and two automatic metrics
(ROUGE-2 and ROUGE-SU4). We make the as-
sumption, backed by both the scores given and com-
ments made by NIST summary assessors 3, that au-
tomatic summarization systems do not perform at
the human level of performance. As such, if a statis-
tic based on an automatic metric, such as ROUGE-
2, were to show fewer systems performing at human
level of performance than the statistic of averaging
scores, such a statistic would be preferable because
3Assessors have commented privately at the Text Analysis
Conference 2008, that while the origin of the summary is hid-
den from them, ?we know which ones are machine generated.?
Thus, automatic summarization fails the Turing test of machine
intelligence (Turing, 1950). This belief is also supported by
(Conroy and Dang, 2008) and (Dang and Owczarzak, 2008). Fi-
nally, our own results show no matter how you compare human
and machine scores all machines systems score significantly
worse than humans.
471
2008: 2145 = (662
) pairs 2009: 1830 = (612
) pairs 2010: 1275 = (512
) pairs
Metric Unpair-T Pair-T Wilc. Unpair-T Pair-T Wilc. Unpair-T Pair-T Wilc.
Linguistic 1234 1416 1410 1000 1182 1173 841 939 934
Overall 1202 1353 1342 982 1149 1146 845 894 889
Pyramid 1263 1417 1418 1075 1238 1216 875 933 926
ROUGE-2 1243 1453 1459 1016 1182 1193 812 938 939
ROUGE-SU4 1333 1493 1507 1059 1241 1254 894 983 976
Table 1: Number of significant differences found when testing for the difference of all pairs of summarization systems
(including humans).
2008: 464 = 58? 8 pairs 2009: 424 = 53? 8 pairs 2010: 344 = 43? 8 pairs
Metric Unpair-T Pair-T Wilc. Unpair-T Pair-T Wilc. Unpair-T Pair-T Wilc.
Linguistic 464 464 464 424 424 424 344 344 344
Overall 464 464 464 424 424 424 344 344 344
Pyramid 464 464 464 424 424 424 344 344 344
ROUGE-2 375 409 402 323 350 341 275 309 305
ROUGE-SU4 391 418 414 354 378 373 324 331 328
Table 2: Number of significant differences resulting from 8 ? (N ? 8) tests for human-machine system means or
signed-rank comparisons.
of its greater power in the machine vs. human sum-
marization domain.
For each of these metrics, we first created a score
matrix whose (i, j)-entry represents the score for
summarizer j on document set i (these matrices gen-
erated the colorplots in Figures 2 ? 5). We then per-
formed a Wilcoxon signed-rank test on certain pairs
of columns of this matrix (any pair consisting of one
machine system and one human summarizer). As a
baseline, we did the same testing with a paired and
an unpaired t-test. Each of these tests resulted in a
p-value, and we counted how many were less than
.05 and called these the significant differences.
The results of these tests (shown in Table 2),
were somewhat surprising. Although we expected
the nonparametric signed-rank test to perform better
than an unpaired t-test, we were surprised to see that
a paired t-test performed even better. All three tests
always reject the null hypotheses when human met-
rics are used. This is what we?d like to happen with
automatic metrics as well. As seen from the table,
the paired t-test and Wilcoxon signed-rank test offer
a good improvement over the unpaired t-test.
The results in Table 1 are less clear, but still posi-
tive. In this case, we are comparing pairs of machine
summarization systems. In contrast to the human vs.
machine case, we do not know the truth here. How-
ever, since the number of significant differences in-
creases with paired testing here as well, we believe
this also reflects the greater discriminatory power of
paired testing.
We now apply the Monte Carlo and Hybrid Monte
Carlo to check the theoretical p-values reported in
Tables 1 and 2. The empirical quantiles found
by these methods generally confirm the theoreti-
cal p-value test results reported there, especially
in Table 2. In the overall tallies of all compar-
isons (Table 1), it seems that the bootstrap results
(comparing only W and the un-paired T ) make
W look still stronger for linguistic and overall re-
sponsiveness versus the T ; but for the pyramid
and ROUGE scores, the bootstrap p-values bring T
slightly closer to W although it still remains clearly
inferior, achieving roughly 10% fewer rejections.
5 Conclusions and Future Work
In this paper we observed that summarization sys-
tems? performance varied significantly across doc-
ument sets on the Text Analysis Conference (TAC)
data. This variance in performance suggested that
paired testing may be more appropriate than the
t-test currently employed at TAC to compare the
472
performance of summarization systems. We pro-
posed a non-parametric test, the Wilcoxon signed-
rank test, as a robust more powerful alternative to
the t-test. We estimated the statistical power of the
t-test and the Wilcoxon signed-rank test by calcu-
lating the number of machine systems whose per-
formance was significantly different than that of hu-
man summarizers. As human assessors score ma-
chine systems as not achieving human performance
in either content or responsiveness, automatic met-
rics such as ROUGE should ideally indicate this dis-
tinction. We found that the paired Wilcoxon test
significantly increases the number of machine sys-
tems that score significantly different than humans
when the pairwise test is performed on ROUGE-2
and ROUGE-SU4 scores. Thus, we demonstrated
that the Wilcoxon paired test shows more statistical
power than the t-test for comparing summarization
systems.
Consequently, the use of paired testing should not
only be used in formal evaluations such as TAC, but
also should be employed by summarization devel-
opers to more accurately assess whether changes to
an automatic system give rise to improved perfor-
mance.
Further study needs to analyze more summariza-
tion metrics such as those proposed at the recent
NIST evaluation of automatic metrics, Automati-
cally Evaluating Summaries of Peers (AESOP) (Nat,
2010). As metrics become more sophisticated and
aim to more accurately predict human judgements
such as overall responsiveness and linguistic qual-
ity, paired testing seems likely to be a more power-
ful statistical procedure than the unpaired t-test for
head-to-head summarizer comparisons.
Throughout our research in this paper, we treated
each separate kind of scores on a document set as
data for one summarizer to be compared with the
same kind of scores for other summarizers. How-
ever, it might be more fruitful to treat all the scores
as multivariate data and compare the summarizers
that way. Multivariate statistical techniques such as
Principal Component Analysis may play a construc-
tive role in suggesting highly discriminating new
composite scores, perhaps leading to statistics with
even more power to measure a summary?s quality.
ROUGE was inspired by the success of the
BLEU (BiLingual Evaluation Understudy), an n-
gram based evaluation for machine translation (Pa-
pineni et al, 2002). It is likely that paired testing
may also be appropriate for BLEU as well and will
give additional discriminating power between ma-
chine translations and human translations.
References
Peter J. Bickel and Jian-Jian Ren. 2001. The Bootstrap
in Hypothesis Testing. In State of the Art in Statistics
and Probability Theory, Festschrift for Willem R. van
Zwet, volume 36 of Lecture Notes? Monograph Series,
pages 91?112. Institute of Mathematical Statistics.
John M. Conroy and Hoa Trang Dang. 2008. Mind the
Gap: Dangers of Divorcing Evaluations of Summary
Content from Linguistic Quality. In Proceedings of
the 22nd International Conference on Computational
Linguistics - Volume 1, COLING ?08, pages 145?152,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Hoa T. Dang and Karolina Owczarzak. 2008. Overview
of the tac 2008 update summarization task. In Pro-
ceedings of the 1st Text Analysis Conference (TAC),
Gaithersburg, Maryland, USA.
B. Efron and R. J. Tibshirani. 1993. An Introduction to
the Bootstrap. Chapman & Hall, New York.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic Eval-
uation of Summaries Using N-gram Co-Occurrences
Statistics. In Proceedings of the Conference of the
North American Chapter of the Association for Com-
putational Linguistics, Edmonton, Alberta.
National Institute of Standards and Technology. 2010.
Text Analysis Conference, http://www.nist.gov/tac.
Ani Nenkova, Rebecca Passonneau, and Kathleen McK-
eown. 2007. The Pyramid Method: Incorporating
Human Content Selection Variation in Summarization
Evaluation. ACM Transactions on Speech and Lan-
guage Processing, 4(2).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
R.H. Randles and D.A. Wolfe. 1979. Introduction to
the Theory of Nonparametric Statistics. Wiley series
in probability and mathematical statistics. Probability
and mathematical statistics. Wiley.
Alan Turing. 1950. Computing Machinery and Intelli-
gence. Mind, 59(236):433?460.
473
Squibs
Nouveau-ROUGE: A Novelty Metric for
Update Summarization
John M. Conroy?
IDA/Center for Computing Sciences
Judith D. Schlesinger?
IDA/Center for Computing Sciences
Dianne P. O?Leary??
University of Maryland
An update summary should provide a fluent summarization of new information on a time-
evolving topic, assuming that the reader has already reviewed older documents or summaries.
In 2007 and 2008, an annual summarization evaluation included an update summarization
task. Several participating systems produced update summaries indistinguishable from human-
generated summaries when measured using ROUGE. However, no machine system performed
near human-level performance in manual evaluations such as pyramid and overall responsive-
ness scoring.
We present a metric called Nouveau-ROUGE that improves correlation with manual
evaluation metrics and can be used to predict both the pyramid score and overall responsiveness
for update summaries. Nouveau-ROUGE can serve as a less expensive surrogate for manual
evaluations when comparing existing systems and when developing new ones.
1. Introduction
Update summaries focus on what is new relative to a previous body of information.
They pose new challenges both to algorithm developers and to evaluation of sum-
maries. In 2007, DUC (Document Understanding Conference) introduced an update
summarization task, repeated in 2008 for TAC (Text Analysis Conference).1 This task
consisted of producing a multi-document summary for a set of articles on a single topic,
followed by one (2008) or two (2007) multi-document summaries for sets of articles on
? Institute for Defense Analyses, Center for Computing Sciences, 17100 Science Drive, Bowie, MD 20715
USA. E-mail: {judith,conroy}@super.org.
?? Computer Science Department, Institute for Advanced Computer Studies, University of Maryland,
College Park, MD 20742 USA. E-mail: oleary@cs.umd.edu.
1 DUC (http://duc.nist.gov), the summarization evaluation event, was replaced in 2008 by TAC
(http://www.nist.gov/tac). Both were sponsored by NIST, the U.S. National Institute of Standards
and Technology.
Submission received: 14 January 2010; revised submission received: 15 May 2010; accepted for publication:
27 September 2010.
? 2011 Association for Computational Linguistics
Computational Linguistics Volume 37, Number 1
the same topic published at later dates. The goal was to generate a good first summary,
along with update(s) that contained new content and minimized redundancy.
The modifier manual is used to identify evaluations, and the corresponding scores,
produced by humans. The modifier automatic is used to identify evaluations, and
the corresponding scores, produced by machines. Similarly, human-generated and
machine-generated will be used to distinguish between summaries created by humans
and those generated by machine systems, respectively.2
Because we are working with update summarization, there is a minimum of two
summaries for a set of documents. The first summary for the document set is called the
original (Task A) summary and a later summary is called an update (Task B) summary.
Several machine summarizing systems produced update summaries that were
statistically indistinguishable from human-generated summaries, as measured by the
ROUGE metrics, the standard metrics for automatic evaluation of summaries. However,
none of these machine systems performed near human levels in overall responsiveness
or pyramid evaluation, the currently used manual evaluation metrics.
We define the metric gap (or gap) as the distance between a prediction of a manual
score, based on automatic scores, and the observed manual score.
The purpose of our work is to investigate and mitigate this metric gap by in-
troducing an automatic evaluation that is a better predictor of manual evaluation.
Reducing the metric gap is important for two reasons. First, the gap severely limits the
usefulness of automatic evaluation and forces the use of much more expensive manual
evaluation for comparing existing systems. More importantly, however, this gap is a
severe handicap to research on new update summarization methods because it makes
it difficult to evaluate new ideas and compare them with existing methods.
In Section 2, we analyze the results of the TAC 2008 summarization task, demon-
strating the large gap between ROUGE automatic metrics and manual evaluation of
update summaries. In Section 3, we modify ROUGE to produce scores that correlate
significantly better with manual evaluation. We evaluate our new metric on TAC 2008
data in Section 4, demonstrating its superiority as a predictor of manual evaluations.
2. State-of-the-Art Evaluation of Update Summaries
TAC 2008 presented 48 20-document sets, with 10 documents in each of two subsets, A
and B. Subset B documents were more recent. Original summaries were generated for
the A subsets and update summaries were then produced for the B subsets.
In TAC 2008, ROUGE was used for automatic evaluation. ROUGE (Lin and Hovy
2000) compares any summary to any other (typically human-generated) summary using
a recall-oriented approach. ROUGE-1 and -2 are based on unigrams and bigrams,
respectively; ROUGE-SU4 uses bigrams with a maximum skip distance of 4 between
bigrams; ROUGE-BE (Hovy, Lin, and Zhou 2005) is an n-gram approach based on basic
elements, computed via parsing or automatic entity recognition. ROUGE-2, ROUGE-
SU4, and ROUGE-BE were the official automatic metrics for TAC 2008, used to com-
pare machine-generated summaries to human-generated summaries, and to compare
human-generated summaries to each other using a jackknife approach. In addition to
the three official metrics, we include ROUGE-1 in our study as it is often competitive
with the official metrics.
2 NIST uses ?model? for human-generated summaries and ?peer? for machine-generated summaries.
2
Conroy, Schlesinger, and O?Leary Nouveau-ROUGE
Three manual evaluation metrics were used in TAC 2008: pyramid, overall respon-
siveness, and linguistic quality (not considered in our work). The pyramid method
(Nenkova and Passonneau 2004) is a content-based metric for which human annotators
mark content units in the human-generated summaries. The content units are collected
across a set of human-generated summaries for a topic, and a weight is computed
based on how many human-generated summaries include this content unit. TAC 2008
also used a manual overall responsiveness score. After evaluating data from 2005?
2007 (Dang 2007; Conroy and Dang 2008), NIST decided that this score, which eval-
uates summary usefulness including linguistic quality, is a reliable and stable manual
evaluation.
We analyzed the three official TAC 2008 automatic evaluation scores to see how
well they predict the manual evaluation metrics of overall responsiveness and pyramid
score. Figure 1 shows scatter plots of responsiveness and pyramid scores vs. the three
official ROUGE measures for the TAC 2008 update task. Each solid data point represents
the average score for a human summarizer over 24 document sets, and a dashed line
marks the minimum; each open data point represents the average score for a machine
system. We also show robust linear least squares fits to the data as well as the Pearson
correlation coefficients between ROUGE-BE, -2, and -SU4 and the manual-evaluation
scores. Surprisingly, these correlations are higher for the update task than for the
original summarization task (data not shown); nevertheless, the gap between the lines?
predictions and the scores for the human-generated summaries is larger.
Figure 1
TAC 2008: The update task (Task B) responsiveness and pyramid scores vs. ROUGE scores;
human-generated summaries (solid points) and machine-generated summaries (open points).
3
Computational Linguistics Volume 37, Number 1
We report correlation coefficients only for the machine-generated summaries. Scores
for the human-generated summaries are distributed differently, and correlation for the
set of human-generated summaries is often not significant due to the small number of
human summarizers.
The correlation coefficients in Figure 1 show that the automatic metrics do well
in predicting responsiveness and pyramid scoring for machine-generated summaries. In
contrast, the scores for human-generated summaries far exceed the predictions, with
a large gap between predicted and actual scores. As may be expected, ROUGE is more
highly correlated with the pyramid evaluation, which is a pure content evaluation score,
whereas the responsiveness score also reflects linguistic quality.
3. Improving Automatic Evaluation?Nouveau-ROUGE
We more formally define the metric gap to be the absolute value of the difference between
a manual evaluation score and our prediction of it based only on automatic evaluation
scores. A number of TAC 2008 machine systems performed within statistical confidence
of human performance in the automatic evaluation metrics, but no system performed
near human performance in the manual evaluations. This has also been observed in
previous summarization evaluations (Conroy and Dang 2008). Progress has been made
in closing this metric gap but it persists, especially for update summaries.
A good update summary must contain essential information but focus on new
information. When a machine-generated update summary is good, it is similar to the
human-generated update summaries. This is assessed quite well by a ROUGE score. But the
machine-generated update summary should also be different from the human-generated
original summaries, and we need an automatic metric to assess this difference, or lack of
redundancy. We suggest using a ROUGE score to measure similarity, and thus redun-
dancy, between a given original summary and an update summary: A high ROUGE
score indicates high redundancy.
To illustrate this, we used the CLASSY algorithm (Conroy, Schlesinger, and O?Leary
2006; Schlesinger, O?Leary, and Conroy 2008) to produce original summaries and update
summaries for the TAC 2008 data. We also produced update summaries using a variant,
projected-CLASSY, that reduces overlap by using a linear algebra projection of the
term-sentence matrix (Conroy, Schlesinger, and O?Leary 2007) of candidate sentences
against the matrix for the original (Task A) summary in order to favor new infor-
mation. Table 1 gives average ROUGE-2 scores and 95% confidence intervals, com-
puted via bootstrapping (Efron and Tibshirani 1993), over the 48 document sets.
Two scores are given: R(BB)2 compares each CLASSY update (Task B) summary to the
human-generated summaries, and R(AB)2 compares each to the original (Task A) model
summaries. Whereas the two variants score comparably using R(BB)2 , there is a significant
difference in the R(AB)2 metric, as desired.
Table 1
TAC 2008: Average ROUGE-2 scores and 95% confidence intervals for update summaries
produced by two variants of CLASSY.
Variation R(BB)2 R
(AB)
2
projected-CLASSY 0.087 (0.080, 0.094) 0.075 (0.070, 0.079)
CLASSY 0.089 (0.082, 0.096) 0.083 (0.078, 0.088)
4
Conroy, Schlesinger, and O?Leary Nouveau-ROUGE
Table 2
TAC 2008: Nouveau-ROUGE ?-parameters.
Predicting Responsiveness Predicting Pyramid Scores
?i,0 ?i,1 ?i,2 ?i,0 ?i,1 ?i,2
R1 ?0.0271 ?7.3550 13.4227 ?0.2143 ?1.9011 3.1118
R2 0.9126 ?5.4536 21.1556 ?0.0143 ?1.3499 4.3778
RSU4 1.1381 ?2.6931 35.8555 0.0346 ?1.1680 7.2589
RBE 1.0602 ?5.0811 24.8365 0.0145 ?1.3156 5.0446
Given this evidence, we propose predicting manual scores for update summaries by
using two ROUGE scores, R(AB)i and R
(BB)
i (i = 1, 2, SU4, ...), in a three-parameter model
called Nouveau-ROUGE:
Ni = ?i,0 + ?i,1R
(AB)
i + ?i,2R
(BB)
i
We determine the ? parameters (Table 2) using robust linear regression on the TAC 2008
evaluation data so that the Nouveau-ROUGE score Ni best predicts the manual scores
of responsiveness and pyramid performance.
Nouveau-ROUGE could be used by researchers to predict how a new system would
compare with the TAC 2008 systems in overall responsiveness and pyramid scoring, a
comparison that up to now has been impossible.
4. Evaluating Nouveau-ROUGE
We evaluate Nouveau-ROUGE using cross validation studies to demonstrate that if
manual scores are available for a subset of summaries (in this case, those from 29
machine systems, half of those that participated in TAC 2008), then Nouveau-ROUGE
can predict manual scores for the remaining (held-back) summaries.
4.1 Improved Correlation with Manual Evaluation
Correlation scores between automatic and manual scores have traditionally been used
as a measure of the effectiveness of automatic evaluation as a surrogate for manual
evaluation. Pearson correlation coefficients, shown in Table 3, were computed for the
scores for the held-back subset of summaries. Correlation is indeed higher for the
Nouveau-ROUGE scores than for any of the ROUGE scores.
Table 3
Correlation scores for TAC 2008 human evaluations.
Average Responsiveness Score Average Pyramid Score
Metric i = 1 i = 2 i = SU4 i = BE i = 1 i = 2 i = SU4 i = BE
R(AB)i 0.676 0.576 0.619 0.490 0.698 0.592 0.634 0.483
R(BB)i 0.870 0.921 0.902 0.933 0.910 0.952 0.933 0.964
Ni 0.888 0.929 0.912 0.935 0.946 0.961 0.951 0.969
5
Computational Linguistics Volume 37, Number 1
Figure 2
TAC 2008: ROUGE and Nouveau-ROUGE responsiveness and pyramid predictions for
subtask B.
6
Conroy, Schlesinger, and O?Leary Nouveau-ROUGE
Table 4
TAC 2008: Median Pearson correlation coefficients for automatic vs. manual evaluations.
Average Responsiveness Score Jackknife Pyramid Score
Metric R(AB)i R
(BB)
i Ni p-value R
(AB)
i R
(BB)
i Ni p-value
R1 0.378 0.804 0.920 5.4e-284 0.406 0.837 0.943 3.5e-307
R2 0.149 0.889 0.925 1.6e-104 0.177 0.909 0.941 1.8e-99
RSU4 0.267 0.846 0.913 1.3e-176 0.291 0.875 0.933 8.7e-214
RBE 0.222 0.913 0.919 6.2e-09 0.243 0.924 0.933 1.7e-17
Figure 2 shows that ROUGE-BE and ROUGE-1 predictions of both responsiveness
and pyramid scores are inferior to the Nouveau-ROUGE-BE predictions. Plots for N2
and NSU4 are omitted due to space restrictions, but performance improvement relative
to ROUGE is greater than that for NBE and less than that for N1.
4.2 Validation Using Bootstrapping Experiments
To show that our results are not due to a lucky partitioning of the data, we used
bootstrapping (Efron and Tibshirani 1993), a resampling method, which allows us to
compute our statistical confidence in the results. This model assumes that observed data
(scores for the 58 systems) characterize all data. Given this model, the proper sampling
method is to choose subsets with replacement. We chose 58 systems (with replacement)
and used half to determine the Nouveau-ROUGE parameters and half to test the model.
We repeated this process 1,000 times. Table 4 gives the correlation coefficients (for the
tested-half of the data) for all four ROUGE metrics with each of the manual evalua-
tions when comparing the machine-generated summaries with the human-generated
summaries. Data in the columns labeled ?p-value? result from a Mann-Whitney U-
test for equal medians of R(BB)i and Ni of the distributions of correlations returned by
the bootstrapping procedure. Because all p-values are small, we can conclude that the
differences between the R(BB)i and Ni correlation scores are statistically significant for all
variants of ROUGE.
4.3 Narrowing the Gap for Update Summaries
Table 5 gives the median gap on the TAC 2008 data for predicting responsiveness and
pyramid scores. Recall that the gap is the absolute value of the difference between the
Table 5
Narrowing the TAC 2008 metric gap.
Responsiveness Metric Gap Pyramid Metric Gap
Metric R Gap N Gap p-value R Gap N Gap p-value
R1 2.025 1.277 7.8e-03 0.285 0.187 7.8e-03
R2 1.655 1.518 7.8e-03 0.241 0.197 7.8e-03
RSU4 1.887 1.344 7.8e-03 0.273 0.206 7.8e-03
RBE 1.591 1.547 7.8e-03 0.229 0.206 7.8e-03
7
Computational Linguistics Volume 37, Number 1
manual score and the prediction of it. The median gap is always smaller for Nouveau-
ROUGE than for ROUGE; in fact, the gap is smaller on every trial.
We used the Wilcox sign test to test the significance of this observation. The null
hypothesis is that the differences in the gaps between ROUGE and Nouveau-ROUGE
has median 0. The p-values from the Wilcox test indicate that the null hypothesis is true
with probability 1128 ? 7.812 ? 10?3, so it can be safely rejected.
5. Conclusion
Our new metric, Nouveau-ROUGE, includes a measure of novelty for an update sum-
mary. We demonstrated that it has higher correlation to manual evaluation of overall
responsiveness and to pyramid scores than does ROUGE. The most obvious deficiency
in ROUGE is the lack of a linguistic quality measurement, which we take to encompass all
language-related issues: lexical, syntactic, and semantic. Therefore, we conjecture that
most remaining prediction error in Nouveau-ROUGE is a result of omitting linguistic
quality and caution that better prediction would be achieved only for systems of
comparable linguistic quality.
We believe that responsiveness is an imperfect surrogate for task-based summary
evaluation such as that done in SUMMAC (Mani et al 1999). We would welcome
a return to task-based evaluation, as well as research increasing the reliability and
consistency of manual evaluation metrics. Investigation could also quantify the impact
of low responsiveness and pyramid scores on the ability to perform a specific task.
References
Conroy, John M. and Hoa Trang Dang. 2008.
Mind the gap: Dangers of divorcing
evaluations of summary content from
linguistic quality. In Proceedings of the 22nd
International Conference on Computational
Linguistics (Coling 2008), pages 145?152,
Manchester.
Conroy, John M., Judith D. Schlesinger, and
Dianne P. O?Leary. 2006. Topic-focused
multi-document summarization using an
approximate oracle score. In Proceedings of
the COLING/ACL 2006 Main Conference
Poster Sessions, pages 152?159, Sydney.
Conroy, John M., Judith D. Schlesinger, and
Dianne P. O?Leary. 2007. CLASSY 2007 at
DUC 2007. In Proceedings of the Seventh
Document Understanding Conference (DUC),
Rochester, NY. Available at http://duc.
nist.gov/pubs.html#2007.
Dang, Hoa Trang. 2007. Overview of DUC
2007. In Proceedings of the Seventh Document
Understanding Conference (DUC), Rochester,
NY. Available at http://duc.nist.gov/
pubs.html#2005.
Efron, B. and R. J. Tibshirani. 1993. An
Introduction to the Bootstrap. Chapman
& Hall, New York.
Hovy, Eduard, Chin-Yew Lin, and Liang
Zhou. 2005. Evaluating DUC 2005 using
basic elements. In Proceedings of the Fifth
Document Understanding Conference (DUC),
Vancouver. Available at www-nlpir/
nist.gov/projects/duc/pubs.html.
Lin, Chin-Yew and Eduard Hovy. 2000. The
automated acquisition of topic signatures
for text summarization. In Proceedings
of the 18th Conference on Computational
Linguistics, pages 495?501, Morristown, NJ.
Mani, Inderjeet, Therese Firmin, David
House, Gary Klein, Beth Sundheim, and
Lynette Hirschman. 1999. The TIPSTER
SUMMAC text summarization evaluation.
In Proceedings of EACL?99: Ninth Conference
of the European Chapter of the Association for
Computational Linguistics, pages 77?85,
Bergen.
Nenkova, Ani and Rebecca Passonneau.
2004. Evaluating content selection in
summarization: The pyramid method.
In Proceedings of the Human Language
Technology Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 145?152,
Boston, MA.
Schlesinger, Judith D., Dianne P. O?Leary,
and John M. Conroy. 2008. Arabic/
English multi-document summarization
with CLASSY?The past and the future.
In Alexander F. Gelbukh, editor, CICLing,
volume 4919 of Lecture Notes in Computer
Science. Springer, Haifa, pages 568?581.
8

Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 359?362,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Assessing the Effect of Inconsistent Assessors on Summarization Evaluation
Karolina Owczarzak
National Institute of Standards and Technology
Gaithersburg, MD 20899
karolina.owczarzak@gmail.com
Peter A. Rankel
University of Maryland
College Park, Maryland
rankel@math.umd.edu
Hoa Trang Dang
National Institute of Standards and Technology
Gaithersburg, MD 20899
hoa.dang@nist.gov
John M. Conroy
IDA/Center for Computing Sciences
Bowie, Maryland
conroy@super.org
Abstract
We investigate the consistency of human as-
sessors involved in summarization evaluation
to understand its effect on system ranking and
automatic evaluation techniques. Using Text
Analysis Conference data, we measure anno-
tator consistency based on human scoring of
summaries for Responsiveness, Readability,
and Pyramid scoring. We identify inconsis-
tencies in the data and measure to what ex-
tent these inconsistencies affect the ranking
of automatic summarization systems. Finally,
we examine the stability of automatic metrics
(ROUGE and CLASSY) with respect to the
inconsistent assessments.
1 Introduction
Automatic summarization of documents is a re-
search area that unfortunately depends on human
feedback. Although attempts have been made at au-
tomating the evaluation of summaries, none is so
good as to remove the need for human assessors.
Human judgment of summaries, however, is not per-
fect either. We investigate two ways of measuring
evaluation consistency in order to see what effect it
has on summarization evaluation and training of au-
tomatic evaluation metrics.
2 Assessor consistency
In the Text Analysis Conference (TAC) Summariza-
tion track, participants are allowed to submit more
than one run (usually two), and this option is of-
ten used to test different settings or versions of the
same summarization system. In cases when the sys-
tem versions are not too divergent, they sometimes
produce identical summaries for a given topic. Sum-
maries are randomized within each topic before they
are evaluated, so the identical copies are usually in-
terspersed with 40-50 other summaries for the same
topic and are not evaluated in a row. Given that each
topic is evaluated by a single assessor, it then be-
comes possible to check assessor consistency, i.e.,
whether the assessor judged the two identical sum-
maries in the same way.
For each summary, assessors conduct content
evaluation according to the Pyramid framework
(Nenkova and Passonneau, 2004) and assign it Re-
sponsiveness and Readability scores1, so assessor
consistency can be checked in these three areas sep-
arately. We found between 230 (in 2009) and 430
(in 2011) pairs of identical summaries for the 2008-
2011 data (given on average 45 topics, 50 runs, and
two summarization conditions: main and update),
giving in effect anywhere from around 30 to 60 in-
stances per assessor per year. Using Krippendorff?s
alpha (Freelon, 2004), we calculated assessor con-
sistency within each year, as well as total consis-
tency over all years? data (for those assessors who
worked multiple years). Table 1 shows rankings of
assessors in 2011, based on their Readability, Re-
sponsiveness, and Pyramid judgments for identical
summary pairs (around 60 pairs per assessor).
Interestingly, consistency values for Readability
are lower overall than those for Responsiveness and
Pyramid, even for the most consistent assessors.
Given that Readability and Responsiveness are eval-
uated in the same way, i.e. by assigning a numeri-
cal score according to detailed guidelines, this sug-
1http://www.nist.gov/tac/2011/Summarization/Guided-
Summ.2011.guidelines.html
359
ID Read ID Resp ID Pyr
G 0.867 G 0.931 G 0.975
D 0.866 D 0.875 D 0.970
A 0.801 H 0.808 H 0.935
H 0.783 A 0.750 A 0.931
F 0.647 F 0.720 E 0.909
C 0.641 E 0.711 C 0.886
E 0.519 C 0.490 F 0.872
Table 1: Annotator consistency in assigning Readability
and Responsiveness scores and in Pyramid evaluation, as
represented by Krippendorff?s alpha for interval values,
on 2011 data.
gests that Readability as a quality of text is inher-
ently more vague and difficult to pinpoint.
On the other hand, Pyramid consistency values
are generally the highest, which can be explained
by how the Pyramid evaluation is designed. Even
if the assessor is inconsistent in selecting Sum-
mary Content Units (SCUs) across different sum-
maries, as long as the total summary weight is sim-
ilar, the summary?s final score will be similar, too.2
Therefore, it would be better to look at whether as-
sessors tend to find the same SCUs (information
?nuggets?) in different summaries on the same topic,
and whether they annotate them consistently. This
can be done using the ?autoannotate? function of
the Pyramid process, where all SCU contributors
(selected text strings) from already annotated sum-
maries are matched against the text of a candidate
(un-annotated) summary. The autoannotate func-
tion works fairly well for matching between extrac-
tive summaries, which tend to repeat verbatim whole
sentences from source documents.
For each summary in 2008-2011 data, we autoan-
notated it using all remaining manually-annotated
summaries from the same topic, and then we com-
pared the resulting ?autoPyramid? score with the
score from the original manual annotation for that
summary. Ideally, the autoPyramid score should
be lower or equal to the manual Pyramid score: it
would mean that in this summary, the assessor se-
lected as relevant all the same strings as s/he found
in the other summaries on the same topic, plus possi-
bly some more information that did not appear any-
2The final score is based on total weight of all SCUs found
in the summary, so the same weight can be obtained by select-
ing a larger number of lower-weight SCUs or a smaller number
of higher-weight SCUs (or the same number of similar-weight
SCUs which nevertheless denote different content).
Figure 1: Annotator consistency in selecting SCUs in
Pyramid evaluation, as represented by the difference be-
tween manual Pyramid and automatic Pyramid scores
(mP-aP), on 2011 data.
where else. If the autoPyramid score is higher than
the manual Pyramid score, it means that either (1)
the assessor missed relevant strings in this summary,
but found them in other summaries; or (2) the strings
selected as relevant elsewhere in the topic were acci-
dental, and as such not repeated in this summary. Ei-
ther way, if we then average out score differences for
all summaries for a given topic, it will give us a good
picture of the annotation consistency in this partic-
ular topic. Higher average autoPyramid scores sug-
gest that the assessor was missing content, or other-
wise making frequent random mistakes in assigning
content. Figure 1 shows the macro-average differ-
ence between manual Pyramid scores and autoPyra-
mid scores for each assessor in 2011.3 For the most
part, it mirrors the consistency ranking from Table
1, confirming that some assessors are less consistent
than others; however, certain differences appear: for
instance, Assessor A is one of the most consistent in
assigning Readability scores, but is not very good at
selecting SCUs consistently. This can be explained
by the fact that the Pyramid evaluation and assigning
Readability scores are different processes and might
require different skills and types of focus.
3 Impact on evaluation
Since human assessment is used to rank participat-
ing summarizers in the TAC Summarization track,
3Due to space constraints, we report figures for only 2011,
but the results for other years are similar.
360
Pearson?s r Spearman?s rho
-1 worst -2 worst -1 worst -2 worst
Readability 0.995 0.993 0.988 0.986
Responsiveness 0.996 0.989 0.986 0.946
Pyramid 0.996 0.992 0.978 0.960
mP-aP 0.996 0.987 0.975 0.943
Table 2: Correlation between the original summarizer
ranking and the ranking after excluding topics by one or
two worst assessors in each category.
we should examine the potential impact of incon-
sistent assessors on the overall evaluation. Because
the final summarizer score is the average over many
topics, and the topics are fairly evenly distributed
among assessors for annotation, excluding noisy
topics/assessors has very little impact on summa-
rizer ranking. As an example, consider the 2011 as-
sessor consistency data in Table 1 and Figure 1. If
we exclude topics by the worst performing assessor
from each of these categories, recalculate the sum-
marizer rankings, and then check the correlation be-
tween the original and newly created rankings, we
obtain results in Table 2.
Although the impact on evaluating automatic
summarizers is small, it could be argued that exclud-
ing topics with inconsistent human scoring will have
an impact on the performance of automatic evalua-
tion metrics, which might be unfairly penalized by
their inability to emulate random human mistakes.
Table 3 shows ROUGE-2 (Lin, 2004), one of the
state-of-the-art automatic metrics used in TAC, and
its correlations with human metrics, before and af-
ter exclusion of noisy topics from 2011 data. The
results are fairly inconclusive: it seems that in most
cases, removing topics does more harm than good,
suggesting that the signal-to-noise ratio is still tipped
in favor of signal. The only exception is Readability,
where ROUGE records a slight increase in correla-
tion; this is unsurprising, given that consistency val-
ues for Readability are the lowest of all categories,
and perhaps here removing noise has more impact.
In the case of Pyramid, there is a small gain when
we exclude the single worst assessor, but excluding
two assessors results in a decreased correlation, per-
haps because we remove too much valid information
at the same time.
A different picture emerges when we examine
how well ROUGE-2 can predict human scores on
the summary level. We pooled together all sum-
Readability Responsiveness Pyramid mP-aP
before 0.705 0.930 0.954 0.954
-1 worst 0.718 0.921 0.961 0.942
-2 worst 0.718 0.904 0.952 0.923
Table 3: Correlation between the summarizer rankings
according to ROUGE-2 and human metrics, before and
after excluding topics by one or two worst assessors in
that category.
Readability Responsiveness Pyramid mP-aP
before 0.579 0.694 0.771 0.771
-1 worst 0.626 0.695 0.828 0.752
-2 worst 0.628 0.721 0.817 0.741
Table 4: Correlation between ROUGE-2 and human met-
rics on a summary level before and after excluding topics
by one or two worst assessors in that category.
maries annotated by each particular assessor and cal-
culated the correlation between ROUGE-2 and this
assessor?s manual scores for individual summaries.
Then we calculated the mean correlation over all
assessors. Unsurprisingly, inconsistent assessors
tend to correlate poorly with automatic (and there-
fore always consistent) metrics, so excluding one
or two worst assessors from each category increases
ROUGE?s average per-assessor summary-level cor-
relation, as can be seen in Table 4. The only ex-
ception here is when we exclude assessors based on
their autoPyramid performance: again, because in-
consistent SCU selection doesn?t necessarily trans-
late into inconsistent final Pyramid scores, exclud-
ing those assessors doesn?t do much for ROUGE-2.
4 Impact on training
Another area where excluding noisy topics might be
useful is in training new automatic evaluation met-
rics. To examine this issue we turned to CLASSY
(Rankel et al, 2011), an automatic evaluation met-
ric submitted to TAC each year from 2009-2011.
CLASSY consists of four different versions, each
aimed at predicting a particular human evaluation
score. Each version of CLASSY is based on one
of three regression methods: robust regression, non-
negative least squares, or canonical correlation. The
regressions are calculated based on a collection of
linguistic and content features, derived from the
summary to be scored.
CLASSY requires two years of marked data to
score summaries in a new year. In order to predict
361
the human metrics in 2011, for example, CLASSY
uses the human ratings from 2009 and 2010. It first
considers each subset of the features in turn, and us-
ing each of the regression methods, fits a model to
the 2009 data. The subset/method combination that
best predicts the 2010 scores is then used to pre-
dict scores for 2011. However, the model is first re-
trained on the 2010 data to calculate the coefficients
to be used in predicting 2011.
First, we trained all four CLASSY versions on
all available 2009-2010 topics, and then trained
again excluding topics by the most inconsistent as-
sessor(s). A different subset of topics was ex-
cluded depending on whether this particular version
of CLASSY was aiming to predict Responsiveness,
Readability, or the Pyramid score. Then we tested
CLASSY?s performance on 2011 data, ranking ei-
ther automatic summarizers (NoModels case) or hu-
man and automatic summarizers together (AllPeers
case), separately for main and update summaries,
and calculated its correlation with the metrics it was
aiming to predict. Table 5 shows the result of this
comparison. For Pyramid, (a) indicates that ex-
cluded topics were selected based on Krippendorff?s
alpha, and (b) indicates that topics were excluded
based on their mean difference between manual and
automatic Pyramid scores.
The results are encouraging; it seems that remov-
ing noisy topics from training data does improve the
correlations with manual metrics in most cases. The
greatest increase takes place in CLASSY?s correla-
tions with Responsiveness for main summaries in
AllPeers case, and for correlations with Readabil-
ity. While none of the changes are large enough
to achieve statistical significance, the pattern of im-
provement is fairly consistent.
5 Conclusions
We investigated the consistency of human assessors
in the area of summarization evaluation. We con-
sidered two ways of measuring assessor consistency,
depending on the metric, and studied the impact of
consistent scoring on ranking summarization sys-
tems and on the performance of automatic evalu-
ation systems. We found that summarization sys-
tem ranking, based on scores for multiple topics,
was surprisingly stable and didn?t change signifi-
NoModels AllPeers
main update main update
Pyramid
CLASSY1 Pyr 0.956 0.898 0.945 0.936
CLASSY1 Pyr new (a) 0.950 0.895 0.932 0.955
CLASSY1 Pyr new (b) 0.960 0.900 0.940 0.955
Responsiveness
CLASSY2 Resp 0.951 0.903 0.948 0.963
CLASSY2 Resp new 0.954 0.907 0.973 0.950
CLASSY4 Resp 0.951 0.927 0.830 0.949
CLASSY4 Resp new 0.943 0.928 0.887 0.946
Readability
CLASSY3 Read 0.768 0.705 0.844 0.907
CLASSY3 Read new 0.793 0.721 0.858 0.906
Table 5: Correlations between CLASSY and human met-
rics on 2011 data (main and update summaries), before
and after excluding most inconsistent topic from 2009-
2010 training data for CLASSY.
cantly when several topics were removed from con-
sideration. However, on a summary level, remov-
ing topics scored by the most inconsistent assessors
helped ROUGE-2 increase its correlation with hu-
man metrics. In the area of training automatic met-
rics, we found some encouraging results; removing
noise from the training data allowed most CLASSY
versions to improve their correlations with the man-
ual metrics that they were aiming to model.
References
Deen G. Freelon. 2010. ReCal: Intercoder Reliability
Calculation as a Web Service. International Journal
of Internet Science, Vol 5(1).
Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. Text Summarization
Branches Out: Proceedings of the ACL-04 Workshop,
78?81. Barcelona, Spain.
Ani Nenkova and Rebecca J. Passonneau. 2004. Evaluat-
ing content selection in summarization: The Pyramid
method. Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the Association for Computational Linguistics, 145?
152. Boston, MA.
Rebecca J. Passonneau, Ani Nenkova, Kathleen McKe-
own, and Sergey Sigelman. 2005. Applying the Pyra-
mid method in DUC 2005. Proceedings of the 5th
Document Understanding Conference (DUC). Van-
couver, Canada.
Peter A. Rankel, John M. Conroy, and Judith D.
Schlesinger. 2012. Better Metrics to Automatically
Predict the Quality of a Text Summary. Proceedings
of the SIAM Data Mining Text Mining Workshop 2012.
362
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 131?136,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Decade of Automatic Content Evaluation of News Summaries:
Reassessing the State of the Art
Peter A. Rankel
University of Maryland
rankel@math.umd.edu
John M. Conroy
IDA / Center for Computing Sciences
conroy@super.org
Hoa Trang Dang
National Institute of Standards and Technology
hoa.dang@nist.gov
Ani Nenkova
University of Pennsylvania
nenkova@seas.upenn.edu
Abstract
How good are automatic content metrics
for news summary evaluation? Here we
provide a detailed answer to this question,
with a particular focus on assessing the
ability of automatic evaluations to identify
statistically significant differences present
in manual evaluation of content. Using
four years of data from the Text Analysis
Conference, we analyze the performance
of eight ROUGE variants in terms of ac-
curacy, precision and recall in finding sig-
nificantly different systems. Our exper-
iments show that some of the neglected
variants of ROUGE, based on higher or-
der n-grams and syntactic dependencies,
are most accurate across the years; the
commonly used ROUGE-1 scores find
too many significant differences between
systems which manual evaluation would
deem comparable. We also test combina-
tions of ROUGE variants and find that they
considerably improve the accuracy of au-
tomatic prediction.
1 Introduction
ROUGE (Lin, 2004) is a suite of automatic eval-
uations for summarization and was introduced a
decade ago as a reasonable substitute for costly
and slow human evaluation. The scores it pro-
duces are based on n-gram or syntactic overlap be-
tween an automatic summary and a set of human
reference summaries. However, the field does not
have a good grasp of which of the many evalua-
tion scores is most accurate in replicating human
judgements. This state of uncertainty has led to
problems in comparing published work, as differ-
ent researchers choose to publish different variants
of scores.
In this paper we reassess the strengths of
ROUGE variants using the data from four years
of Text Analysis Conference (TAC) evaluations,
2008 to 2011. To assess the performance of the au-
tomatic evaluations, we focus on determining sta-
tistical significance1 between systems, where the
gold-standard comes from comparing the systems
using manual pyramid and responsiveness evalu-
ations. In this setting, computing correlation co-
efficients between manual and automatic scores is
not applicable as it does not take into account the
statistical significance of the differences nor does
it allow the use of more powerful statistical tests
which use pairwise comparisons of performance
on individual document sets. Instead, we report
on the accuracy of decisions on pairs of systems,
as well as the precision and recall of identifying
pairs of systems which exhibit statistically signifi-
cant differences in content selection performance.
2 Background
During 2008?2011, automatic summarization sys-
tems at TAC were required to create 100-word
summaries. Each year there were two multi-
document summarization sub-tasks, the initial
summary and the update summary, usually re-
ferred to as task A and task B, respectively. The
test inputs in each consisted of about 10 docu-
ments and the type of summary varied between
query-focused and guided. There are between 44
and 48 test inputs on which systems are compared
for each task.
In 2008 and 2009, task A was to produce a
1For the purpose of this study, we define a difference as
significant when the test statistic attains a value correspond-
ing to a p-value less than 0.05.131
query-focused summary in response to a user in-
formation need stated both as a brief statement
and a paragraph-long description of the informa-
tion the user seeks to find. In 2010 and 2011 task
A was ?guided summarization?, where the test in-
puts came from a small set of predefined domains.
These domains included accidents and natural dis-
asters, attacks, health and safety, endangered re-
sources, investigations and trials. Systems were
provided with a list of important aspects of infor-
mation for each domain and were asked to cover as
many of these aspects as possible. The writers of
the reference summaries for evaluation were given
similar instructions. In all four years, task B was
to produce an update summary for each of the in-
puts given in task A (query-focused or guided). In
each case, a new, subsequent set of documents re-
lated to the topic of the respective test set for task
A was provided to the system. The task was to
generate an update summary aimed at a user who
has already read all documents in the inputs for
task A.
The two manual evaluation approaches used in
TAC 2008?2011 are modified pyramid (Nenkova
et al, 2007) and overall responsiveness. The pyra-
mid method requires several reference summaries
for each input. These are manually analyzed to
discover content units based on meaning rather
than specific wording. Each content unit is as-
signed a weight equal to the number of reference
summaries that included that content unit. The
modified pyramid score is defined as the sum of
weights of the content units in the summary nor-
malized by the weight of an ideally informative
summary which expresses n content units, where
n is equal to the average of content units in the ref-
erence summaries. Responsiveness, on the other
hand, is based on direct human judgements, with-
out the need for reference summaries. Assessors
are presented with a statement of the user?s infor-
mation need and the summary they need to evalu-
ate. Then they rate how well they think the sum-
mary responds to the information need contained
in the topic statement. Responsiveness was rated
on a ten-point scale in 2009, and on a five-point
scale in all other years.
For each sub-task during 2008?2011, we ana-
lyze the performance of only the top 30 systems,
which roughly corresponds to the systems that per-
formed better than or around the median according
to each manual metric. Table 1 gives the number
of significant differences among the top 30 partici-
pating systems. We keep only the best performing
systems for the analysis because we are interested
in studying how well automatic evaluation metrics
can correctly compare very good systems.
Year Pyr A Pyr B Resp A Resp B
2008 82 109 68 105
2009 146 190 106 92
2010 165 139 150 128
2011 39 83 5 11
Table 1: Number of pairs of significantly different
systems among the top 30 across the years. There
is a total of 435 pairs in each year.
3 Which ROUGE is best?
In this section, we study the performance of
several ROUGE variants, including ROUGE-n,
for n = 1, 2, 3, 4, ROUGE-L, ROUGE-W-1.2,
ROUGE-SU4, and ROUGE-BE-HM (Hovy et al,
2006). ROUGE-n measures the n-gram recall of
the evaluated summary compared to the available
reference summaries. ROUGE-L is the ratio of
the number of words in the longest common sub-
sequence between the reference and the evaluated
summary and the number of words in the refer-
ence. ROUGE-W-1.2 is a weighted version of
ROUGE-L. ROUGE-SU4 is a combination of skip
bigrams and unigrams, where the skip bigrams are
formed for all words that appear in the text with
no more than four intervening words in between.
ROUGE-BE-HM computes recall of dependency
syntactic relations between the summary and the
reference.
To evaluate how well an automatic evalua-
tion metric reproduces human judgments, we use
prediction accuracy similar to Owczarzak et al
(2012). For each pair of systems in each subtask,
we compare the results of two Wilcoxon signed-
rank tests, one using the manual evaluation scores
for each system and one using the automatic evalu-
ation scores for each system (Rankel et al, 2011).2
The accuracy then is simply the percent agreement
between the results of these two tests.
2We use the Wilcoxon test as it was demonstrated by
Rankel et al (2011) to give more statistical power than un-
paired tests. As reported by Yeh (2000), other tests such as
randomized testing, may also be appropriate. There is con-
siderable variation in system performance for different inputs
(Nenkova and Louis, 2008) and paired tests remove the effect
of the input.132
Responsiveness Pyramid
Metric Acc P R BA Acc P R BA
R1 0.58 (0.61) 0.24 0.64 0.57 0.62 (0.66) 0.37 0.67 0.61
R2 0.64 (0.63) 0.28 0.60 0.59 0.68 (0.69) 0.43 0.63 0.64
R3 0.70 (0.63) 0.31 0.48 0.60 0.73 (0.68) 0.49 0.53 0.66
R4 0.73 (0.64) 0.33 0.40 0.60 0.74 (0.65) 0.50 0.45 0.65
RL 0.50 (0.59) 0.20 0.56 0.54 0.54 (0.63) 0.29 0.60 0.55
R-SU4 0.61(0.62) 0.26 0.61 0.58 0.65 (0.68) 0.40 0.65 0.63
R-W-1.2 0.52(0.62) 0.21 0.54 0.55 0.57(0.64) 0.32 0.62 0.57
R-BE-HM 0.70 (0.63) 0.30 0.49 0.59 0.74(0.68) 0.49 0.56 0.66
Table 2: Accuracy, Precision, Recall, and Balanced Accuracy of each ROUGE variant, averaged across
all eight tasks in 2008-2011, with and (without) significance.
As can be seen in Table 1, the manual evalua-
tion metrics often did not show many significant
differences between systems.3 Thus, it is clear
that the percent agreement will be high for an ap-
proach for automatic evaluation that always pre-
dicts zero significant differences. As traditionally
done when dealing which such skewed distribu-
tions of classes, we also examine the precision
and recall with respect to finding significant dif-
ferences of several ROUGE variants, to better as-
sess the quality of their prediction. To identify a
measure that is strong at both predicting signifi-
cant and non-significant differences we compute
balanced accuracy, the mean of the accuracy of
predicting significant differences and the accuracy
of predicting no significant difference.4
Each of these four measures for judging the per-
formance of ROUGE variants has direct intuitive
interpretation, unlike other opaque measures such
as correlation coefficients and F-measure which
have formal definitions which do not readily yield
to intuitive understanding.
3This is a somewhat surprising finding which may warrant
further investigation. One possible explanation is that differ-
ent systems generate similar summaries. Recent work has
shown that this is unlikely to be the case because the collec-
tion of summaries from several systems indicates better what
content is important than the single best summary (Louis and
Nenkova, 2013). The short summary length for which the
summarizers are compared may also contribute to the fact
that there are few significant difference. In early NIST eval-
uations manual evaluations could not distinguish automatic
and human summaries based on summaries of length 50 and
100 words and there were more significant differences be-
tween systems for 200-word summaries than for 100-word
summaries (Nenkova, 2005).
4More generally, one could define a utility function which
gives costs associated with errors and benefits to correct pre-
diction. Balanced accuracy weighs all errors as equally bad
and all correct prediction as equally good (von Neumann and
Morgenstern, 1953).
Few prior studies have taken statistical signifi-
cance into account during the assessment of auto-
matic metrics for evaluation. For this reason we
first briefly discuss ROUGE accuracy without tak-
ing significance into account. In this special case,
agreement simply means that the automatic and
manual evaluations agree on which of two systems
is better, based on each system?s average score for
all test inputs for a given task. It is very rare that
the average scores of two systems are equal, so
there is always a better system in each pair, and
random prediction would have 50% accuracy.
Many papers do not report the significance of
differences in ROUGE scores (for the ROUGE
variant of their choice), but simply claim that their
system X with higher average ROUGE score than
system Y is better than system Y . Table 2 lists
the average accuracy with significance taken into
account and then in parentheses, accuracy without
taking significance into account. The data demon-
strate that the best accuracy of the eight ROUGE
metrics is a meager 64% for responsiveness when
significance is not taken into account. So the con-
clusion about the relative merit of systems would
be different from that based on manual evaluation
in one out of three comparisons. However, the
best accuracy rises to 73% when significance is
taken into account; an incorrect conclusion will be
drawn in one out of four comparisons. The reduc-
tion in error is considerable.
Furthermore, ROUGE-3 and ROUGE-4, which
are rarely reported, are among the most accurate.
Note also, these results differ considerably from
those reported by Owczarzak et al (2012), where
ROUGE-2 was shown to have accuracy of 81% for
responsiveness and 89% for pyramid. The wide
differences are due to the fact we are only consid-133
ering systems which scored in the top 30. This il-
lustrates that our automatic metrics are not as good
at discriminating systems near the top. These find-
ings give strong support for the idea of requiring
authors to report the significance of the difference
between their summarization system and the cho-
sen baseline; the conclusions about relative merits
of the system would be more similar to those one
would draw from manual evaluation.
In addition to accuracy, Table 2 gives precision,
recall and balanced accuracy for each of the eight
ROUGE measures when significance is taken into
account. ROUGE-1 is arguably the most widely
used score in the literature and Table 2 reveals an
interesting property: ROUGE-1 has high recall but
low precision. This means that it reports many sig-
nificant differences, most of which do not exist ac-
cording to the manual evaluations.
Balanced accuracy helps us identify which
ROUGE variants are most accurate in finding
statistical significance and correctly predicting
that two systems are not significantly different.
For the pyramid evaluation, the variants with
best balanced accuracy (66%) are ROUGE-3 and
ROUGE-BE, with ROUGE-4 just a percent lower
at 65%. For responsiveness the configuration is
similar, with ROUGE-3 and ROUGE-4 tied for
best (60%), and ROUGE-BE just a percent lower.
The good performance of higher-order n-grams
is quite surprising because these are practically
never used for reporting results in the literature.
Based on our results however, they are much more
likely to accurately reproduce conclusions that
would have been drawn from manual evaluation
of top-performing systems.
4 Multiple hypothesis tests to combine
ROUGE variants
We now consider a method to combine multiple
evaluation scores in order to obtain a stronger en-
semble metric. The idea of combining ROUGE
variants has been explored in the prior litera-
ture. Conroy and Dang (2008), for example, pro-
posed taking linear combinations of ROUGE met-
rics. This approach was extended by Rankel et al
(2012) by including measures of linguistic quality.
Recently, Amigo? et al (2012) applied the ?hetero-
geneity principle? and combined ROUGE scores
to improve the precision relative to a human evalu-
ation metric. Their results demonstrate that a con-
sensus among ROUGE scores can predict more ac-
curately if an improvement in a human evaluation
metric will be achieved.
Along the lines of these investigations, we ex-
amine the performance of a simple combination
of variants: Call the difference between two sys-
tems significant only when all the variants in the
combination indicate significance. As in the sec-
tion above, a paired Wilcoxon signed-rank test is
used to determine the level of significance.
ROUGE Combination Acc Prec Rec BA
R1 R2 R4 RBE 0.76 0.77 0.36 0.76
R1 R4 RBE 0.76 0.76 0.36 0.76
R2 R4 RBE 0.76 0.74 0.40 0.75
R4 RBE 0.76 0.73 0.41 0.75
R1 R2 R4 0.76 0.71 0.40 0.74
R1 R4 0.75 0.70 0.40 0.73
R2 R4 0.75 0.68 0.44 0.73
R1 R2 RBE 0.75 0.66 0.48 0.72
R2 RBE 0.75 0.64 0.52 0.72
R4 0.74 0.62 0.47 0.70
R1 RBE 0.74 0.62 0.49 0.70
R1 R2 0.73 0.57 0.62 0.70
RBE 0.73 0.57 0.58 0.68
R2 0.71 0.53 0.69 0.68
R1 0.62 0.43 0.69 0.63
Table 3: Accuracy, Precision, Recall, and Bal-
anced Accuracy of each ROUGE combination on
TAC 2008-2010 pyramid.
We considered all possible combinations of four
ROUGE metrics that exhibited good properties
in the analyses presented so far: ROUGE-1 (be-
cause of its high recall), ROUGE-2 (because of
high accuracy when significance is not taken into
account) and ROUGE-4 and ROUGE-BE, which
showed good balanced accuracy.
The performance of these combinations for re-
producing the decisions in TAC 2008-2010 based
on the pyramid5 evaluation are given in Table 3.
The best balanced accuracy (76%) is for the com-
bination of all four variants. As more variants are
combined, precision increases but recalls drops.
5 Comparison with automatic
evaluations from AESOP 2011
In 2009-2011, TAC ran the task of Automatically
Evaluating Summaries of Peers (AESOP), to com-
5The ordering of the metric combinations relative to re-
sponsiveness was almost identical to the ordering relative to
the pyramid evaluation, and precision and recall exhibited the
same trend as more metrics were added to the combination.134
Pyramid A Pyramid B Responsiveness A Responsiveness B
Evaluation Metric Acc P R BA Acc P R BA Acc P R BA Acc P R BA
CLASSY1 0.60 0.02 0.60 0.50 0.84 0.03 0.18 0.50 0.61 0.14 0.64 0.54 0.70 0.21 0.22 0.52
DemokritosGR1 0.59 0.01 0.20 0.50 0.79 0.07 0.55 0.53 0.66 0.18 0.79 0.58 0.64 0.17 0.24 0.49
uOttawa3 0.44 0.01 0.60 0.50 0.48 0.02 0.36 0.50 0.52 0.13 0.77 0.55 0.43 0.13 0.36 0.46
DemokritosGR2 0.78 0.01 0.20 0.50 0.76 0.06 0.55 0.52 0.76 0.23 0.69 0.60 0.67 0.22 0.29 0.52
C-S-IIITH4 0.69 0.01 0.20 0.50 0.77 0.07 0.64 0.53 0.82 0.29 0.74 0.63 0.60 0.15 0.24 0.47
C-S-IIITH1 0.60 0.01 0.40 0.50 0.70 0.06 0.82 0.53 0.69 0.20 0.79 0.59 0.60 0.22 0.42 0.52
BEwT-E 0.73 0.01 0.20 0.50 0.80 0.01 0.09 0.49 0.79 0.25 0.72 0.61 0.72 0.31 0.39 0.58
R1-R2-R4-RBE 0.89 0.40 0.44 0.67 0.76 0.27 0.17 0.55 0.88 0.00 0.00 0.49 0.91 0.03 0.09 0.50
R1-R4-RBE 0.89 0.40 0.44 0.67 0.77 0.35 0.24 0.59 0.88 0.00 0.00 0.49 0.90 0.03 0.09 0.50
All ROUGEs 0.89 0.40 0.44 0.67 0.75 0.26 0.16 0.54 0.88 0.00 0.00 0.49 0.91 0.04 0.09 0.51
Table 4: Best performing AESOP systems from TAC 2011; Scores within the 95% confidence interval
of the best are in bold face.
pare automatic evaluation methods for automatic
summarization. Here we show how the submit-
ted AESOP metrics compare to the best ROUGE
variants that we have established so far. We report
the results on 2011 only, because even when the
same team participated in more than one year, the
metrics submitted were different and the 2011 re-
sults represent the best effort of these teams. How-
ever, as we saw in Table 1, in 2011 there were very
few significant differences between the top sum-
marization systems. In this sense the tasks that
year represent a challenging dataset for testing au-
tomatic evaluations.
The results for the best AESOP systems (ac-
cording to one or more measures), and the cor-
responding results for the ROUGE combinations
are shown in Table 4. These AESOP systems are:
CLASSY1 (Conroy et al, 2011; Rankel et al,
2012), DemokritosGR1 and 2 (Giannakopoulos et
al., 2008; Giannakopoulos et al, 2010), uOttawa3
(Kennedy et al, 2011), C-S-IITH1 and 4 (Kumar
et al, 2011; Kumar et al, 2012), and BEwT-E
(Tratz and Hovy, 2008).6 The combination metrics
achieve the highest accuracy by generally predict-
ing correctly when there are no significant differ-
ences between the systems. In addition, for 2008-
2010, where far more differences between systems
occur, the results of Table 3 show the combina-
tion metrics outperformed use of a single metric
and are competitive with the best metrics of AE-
SOP 2011. Thus, the combination metrics have
the ability to discriminate under both conditions
giving good prediction of human evaluation.
6To perform the comparison in the table the scores for
each system and document set were needed. Some systems
have changed after TAC 2011, but the data needed for these
comparisons were not available. BEwT-E did not participate
in AESOP 2011 and these data were provided by Stephen
Tratz. Special thanks to Stephen for providing these data.
6 Conclusion
We have tested the best-known automatic evalu-
ation metrics (ROUGE) on several years of TAC
data and compared their performance with re-
cently developed AESOP metrics. We discovered
that some of the rarely used variants of ROUGE
perform surprisingly well, and that by combin-
ing different ROUGEs together, one can create
an evaluation metric that is extremely competi-
tive with metrics submitted to the latest AESOP
task. Our results were reported in terms of sev-
eral different measures, and in each case, com-
pared how well the automatic metric predicted sig-
nificant differences found in manual evaluation.
We believe strongly that developers should include
statistical significance when reporting differences
in ROUGE scores of theirs and other systems,
as this improves the accuracy and credibility of
their results. Significant improvement in multi-
ple ROUGE scores is a significantly stronger in-
dicator that the developers have made a notewor-
thy improvement in text summarization. Systems
that report significant improvement using a com-
bination of ROUGE-BE (or its improved version
BEwT-E) in conjunction with ROUGE-1, 2, and
4, are more likely to give rise to summaries that
humans would judge as significantly better.
Acknowledgments
The authors would like to thank Ed Hovy who
raised the question ?How well do automatic met-
rics perform when comparing top systems?? Ed?s
comments helped motivate this work. In addition,
we would like to thank our anonymous referees for
their insightful comments, which contributed sig-
nificantly to this paper.
135
References
Enrique Amigo?, Julio Gonzalo, and Felisa Verdejo.
2012. The heterogeneity principle in evaluation
measures for automatic summarization. In Pro-
ceedings of Workshop on Evaluation Metrics and
System Comparison for Automatic Summarization,
pages 36?43, Montre?al, Canada, June. Association
for Computational Linguistics.
John M. Conroy and Hoa Trang Dang. 2008. Mind
the gap: Dangers of divorcing evaluations of sum-
mary content from linguistic quality. In Proceedings
of the 22nd International Conference on Compu-
tational Linguistics (Coling 2008), pages 145?152,
Manchester, UK, August. Coling 2008 Organizing
Committee.
John M. Conroy, Judith D. Schlesinger, and Dianne P.
O?Leary. 2011. Nouveau-ROUGE: A Novelty Met-
ric for Update Summarization. Computational Lin-
guistics, 37(1):1?8.
George Giannakopoulos, Vangelis Karkaletsis,
George A. Vouros, and Panagiotis Stamatopoulos.
2008. Summarization system evaluation revisited:
N-gram graphs. TSLP, 5(3).
George Giannakopoulos, George A. Vouros, and Van-
gelis Karkaletsis. 2010. Mudos-ng: Multi-
document summaries using n-gram graphs (tech re-
port). CoRR, abs/1012.2042.
Eduard Hovy, Chin-Yew Lin, Liang Zhou, and Ju-
nichi Fukumoto. 2006. Automated summarization
evaluation with basic elements. In Proceedings of
the Fifth International Conference on Language Re-
sources and Evaluation (LREC?06), pages 899?902.
Alistair Kennedy, Anna Kazantseva Saif Mohammad,
Terry Copeck, Diana Inkpen, and Stan Szpakowicz.
2011. Getting emotional about news. In Fourth Text
Analysis Conference (TAC 2011).
Niraj Kumar, Kannan Srinathan, and Vasudeva Varma.
2011. Using unsupervised system with least linguis-
tic features for tac-aesop task. In Fourth Text Analy-
sis Conference (TAC 2011).
N. Kumar, K. Srinathan, and V. Varma. 2012. Us-
ing graph based mapping of co-occurring words and
closeness centrality score for summarization evalua-
tion. Computational Linguistics and Intelligent Text
Processing, pages 353?365.
Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Stan Szpakowicz
Marie-Francine Moens, editor, Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74?81, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.
Annie Louis and Ani Nenkova. 2013. Automatically
assessing machine summary content without a gold
standard. Computational Linguistics, 39:267?300.
Ani Nenkova and Annie Louis. 2008. Can you sum-
marize this? identifying correlates of input difficulty
for multi-document summarization. In ACL, pages
825?833.
Ani Nenkova, Rebecca J. Passonneau, and Kathleen
McKeown. 2007. The pyramid method: Incorpo-
rating human content selection variation in summa-
rization evaluation. TSLP, 4(2).
Ani Nenkova. 2005. Discourse factors in multi-
document summarization. In AAAI, pages 1654?
1655.
Karolina Owczarzak, John M. Conroy, Hoa Trang
Dang, and Ani Nenkova. 2012. An assessment
of the accuracy of automatic evaluation in summa-
rization. In Proceedings of Workshop on Evaluation
Metrics and System Comparison for Automatic Sum-
marization, pages 1?9, Montre?al, Canada, June. As-
sociation for Computational Linguistics.
Peter Rankel, John Conroy, Eric Slud, and Dianne
O?Leary. 2011. Ranking human and machine sum-
marization systems. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 467?473, Edinburgh, Scot-
land, UK., July. Association for Computational Lin-
guistics.
Peter A. Rankel, John M. Conroy, and Judith D.
Schlesinger. 2012. Better metrics to automatically
predict the quality of a text summary. Algorithms,
5(4):398?420.
Stephen Tratz and Eduard Hovy. 2008. Summarisa-
tion evaluation using transformed basic elements. In
Proceedings TAC 2008. NIST.
John von Neumann and Oskar Morgenstern. 1953.
Theory of games and economic behavior. Princeton
Univ. Press, Princeton, NJ, 3. ed. edition.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Pro-
ceedings of the 18th conference on Computational
linguistics - Volume 2, COLING ?00, pages 947?
953, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
136
Proceedings of the Workshop on Evaluation Metrics and System Comparison for Automatic Summarization, pages 1?9,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
An Assessment of the Accuracy of Automatic Evaluation in Summarization
Karolina Owczarzak
Information Access Division
National Institute of Standards and Technology
karolina.owczarzak@gmail.com
John M. Conroy
IDA Center for Computing Sciences
conroy@super.org
Hoa Trang Dang
Information Access Division
National Institute of Standards and Technology
hoa.dang@nist.gov
Ani Nenkova
University of Pennsylvania
nenkova@seas.upenn.edu
Abstract
Automatic evaluation has greatly facilitated
system development in summarization. At the
same time, the use of automatic evaluation
has been viewed with mistrust by many, as its
accuracy and correct application are not well
understood. In this paper we provide an as-
sessment of the automatic evaluations used for
multi-document summarization of news. We
outline our recommendations about how any
evaluation, manual or automatic, should be
used to find statistically significant differences
between summarization systems. We identify
the reference automatic evaluation metrics?
ROUGE 1 and 2?that appear to best emu-
late human pyramid and responsiveness scores
on four years of NIST evaluations. We then
demonstrate the accuracy of these metrics in
reproducing human judgements about the rel-
ative content quality of pairs of systems and
present an empirical assessment of the rela-
tionship between statistically significant dif-
ferences between systems according to man-
ual evaluations, and the difference according
to automatic evaluations. Finally, we present a
case study of how new metrics should be com-
pared to the reference evaluation, as we search
for even more accurate automatic measures.
1 Introduction
Automatic evaluation of content selection in sum-
marization, particularly the ROUGE evaluation
toolkit (Lin and Hovy, 2003), has been enthusias-
tically adopted by researchers since its introduction
in 2003. It is now standardly used to report results in
publications; however we have a poor understanding
of the accuracy of automatic evaluation. How often
do we publish papers where we report an improve-
ment according to automatic evaluation, but never-
theless, a standard manual evaluation would have led
us to different conclusions? In our work we directly
address this question, and hope that our encouraging
findings contribute to a better understanding of the
strengths and shortcomings of automatic evaluation.
The aim of this paper is to give a better assessment
of the automatic evaluation metrics for content se-
lection standardly used in summarization research.
We perform our analyses on data from the 2008-
2011 Text Analysis Conference (TAC)1 organized
by the National Institute of Standards and Technol-
ogy (NIST). We choose these datasets because in
early evaluation initiatives, the protocol for manual
evaluation changed from year to year in search of
stable manual evaluation approaches (Over et al,
2007). Since 2008, however, the same evaluation
protocol has been applied by NIST assessors and we
consider it to be the model that automatic metrics
need to emulate.
We start our discussion by briefly presenting the
manual procedure for comparing systems (Section
2) and how these scores should be best used to iden-
tify significant differences between systems over a
given test set (Section 3). Then, we embark on our
discussion of the accuracy of automatic evaluation
and its ability to reproduce manual scoring.
To begin our analysis, we assess the accuracy of
common variants of ROUGE on the TAC 2008-2011
datasets (Section 4.1). There are two aspects of eval-
uation that we pay special attention to:
Significant difference Ideally, all system compar-
isons should be performed using a test for sta-
1http://www.nist.gov/tac/
1
tistical significance. As both manual metrics
and automatic metrics are noisy, a statistical
hypothesis test is needed to estimate the prob-
ability that the differences observed are what
would be expected if the systems are compa-
rable in their performance. When this proba-
bility is small (by convention 0.05 or less) we
reject the null hypothesis that the systems? per-
formance is comparable.
It is important to know if scoring a system via
an automatic metric will lead to conclusions
about the relative merits of two systems differ-
ent from what one would have concluded on the
basis of manual evaluation. We report very en-
couraging results, showing that automatic met-
rics rarely contradict manual metrics, and some
metrics never lead to contradictions. For com-
pleteness, given that most papers do not report
significance, we also compare the agreement
between manual and automatic metrics without
taking significance into account.
Type of comparison Established manual evalua-
tions have two highly desirable properties: (1)
they can tell apart good automatic systems from
bad automatic systems and (2) they can differ-
entiate automatic summaries from those pro-
duced by humans with high accuracy. Both
properties are essential. Obviously, choosing
the better system in development cycles is key
in eventually improving overall performance.
Being able to distinguish automatic from man-
ual summaries is a general sanity test 2 that any
evaluation adopted for wide use is expected to
pass?it is useless to report system improve-
ments when it appears that automatic methods
are as good as human performance3. As we will
see, there is no single ROUGE variant that has
both of these desirable properties.
Finally, in Section 5, we discuss ways to compare
other automatic evaluation protocols with the refer-
2For now, automatic systems do not have the performance
of humans, thus, the ability to distinguish between human and
automatically generated summaries is an exemplar of the wider
problem of distinguishing high quality summaries from others.
3Such anomalous findings, when using automatic evalua-
tion, have been reported for some summarization genres such
as summarization of meetings (Galley, 2006).
ence ROUGE metrics we have established. We de-
fine standard tests for significance that would iden-
tify evaluations that are significantly more accurate
than the current reference measures, thus warrant-
ing wider adoption for future system development
and reporting of results. As a case study we apply
these to the TAC AESOP (Automatically Evaluating
Summaries of Peers) task which called for the devel-
opment of novel evaluation techniques that are more
accurate than ROUGE evaluations.
2 Manual evaluation
Before automatic evaluation methods are developed,
it is necessary to establish a desirable manual eval-
uation which the automatic methods will need to re-
produce. The type of summarization task must also
be precisely specified?single- or multi-document
summarization, summarization of news, meetings,
academic articles, etc. Saying that an automatic
evaluation correlates highly with human judgement
in general, is disturbingly incomplete, as the same
automatic metric can predict some manual evalu-
ation scores for some summarization tasks well,
while giving poor correlation with other manual
scores for certain tasks (Lin, 2004; Liu and Liu,
2010).
In our work, we compare automatic metrics with
the manual methods used at TAC: Pyramid and Re-
sponsiveness. These manual metrics primarily aim
to assess if the content of the summary is appro-
priately chosen to include only important informa-
tion. They do not deal directly with the linguistic
quality of the summary?how grammatical are the
sentences or how well the information in the sum-
mary is organized. Subsequently, in the experiments
that we present in later sections, we do not address
the assessment of automatic evaluations of linguistic
quality (Pitler et al, 2010), but instead analyze the
performance of ROUGE and other related metrics
that aim to score summary content.
The Pyramid evaluation (Nenkova et al, 2007) re-
lies on multiple human-written gold-standard sum-
maries for the input. Annotators manually identify
shared content across the gold-standards regardless
of the specific phrasing used in each. The pyra-
mid score is based on the ?popularity? of informa-
tion in the gold-standards. Information that is shared
2
across several human gold-standards is given higher
weight when a summary is evaluated relative to the
gold-standard. Each evaluated summary is assigned
a score which indicates what fraction of the most
important information for a given summary size is
expressed in the summary, where importance is de-
termined by the overlap in content across the human
gold-standards.
The Responsiveness metric is defined for query-
focused summarization, where the user?s informa-
tion need is clearly stated in a short paragraph. In
this situation, the human assessors are presented
with the user query and a summary, and are asked
to assign a score that reflects to what extent the sum-
mary satisfies the user?s information need. There are
no human gold-standards, and the linguistic quality
of the summary is to some extent incorporated in the
score, because information that is presented in a con-
fusing manner may not be seen as relevant, while it
could be interpreted by the assessor more easily in
the presence of a human gold-standard. Given that
all standard automatic evaluation procedures com-
pare a summary with a set of human gold-standards,
it is reasonable to expect that they will be more accu-
rate in reproducing results from Pyramid evaluation
than results from Responsiveness judgements.
3 Comparing systems
Evaluation metrics are used to determine the rela-
tive quality of a summarization system in compari-
son to one or more systems, which is either another
automatic summarizer, or a human reference sum-
marizer. Any evaluation procedure assigns a score
to each summary. To identify which of the two sys-
tems is better, we could simply average the scores
of summaries produced by each system in the test
set, and compare these averages. This approach is
straightforward; however, it gives no indication of
the statistical significance of the difference between
the systems. In system development, engineers may
be willing to adopt new changes only if they lead
to significantly better performance that cannot be at-
tributed to chance.
Therefore, in order to define more precisely what
it means for a summarization system to be ?bet-
ter? than another for a given evaluation, we employ
statistical hypothesis testing comparisons of sum-
marization systems on the same set of documents.
Given an evaluation of two summarization systems
A and B we have the following:
Definition 1. We say a summarizer A ?signifi-
cantly outperforms? summarizer B for a given
evaluation score if the null hypothesis of the fol-
lowing paired test is rejected with 95% confidence.
Given two vectors of evaluation scores x and y,
sampled from the corresponding random vari-
ables X and Y, measuring the quality of sum-
marizer A and B, respectively, on the same col-
lection of document sets, with the median of x
greater than the median of y,
H0 : The median of X ? Y is 0.
Ha : The median of X ? Y is not 0.
We apply this test using human evaluation met-
rics, such as pyramid and responsiveness, as well as
automatic metrics. Thus, when comparing two sum-
marization systems we can, for example, say system
A significantly outperforms system B in responsive-
ness if the null hypothesis can be rejected. If the null
hypothesis cannot be rejected, we say system A does
not significantly perform differently than system B.
A complicating factor when the differences be-
tween systems are tested for significance, is that
some inputs are simply much harder to summarize
than others, and there is much variation in scores
that is not due to properties of the summarizers
that produced the summaries but rather properties of
the input text that are summarized (Nenkova, 2005;
Nenkova and Louis, 2008).
Given this variation in the data, the most appropri-
ate approach to assess significance in the difference
between system is to use paired rank tests such as
a paired Wilcoxon rank-sum test, which is equiva-
lent to the Mann-Whitney U test. In these tests, the
scores of the two systems are compared only for the
same input and ranks are used instead of the actual
difference in scores assigned by the evaluation pro-
cedures. Prior studies have shown that paired tests
for significance are indeed able to discover consid-
erably more significant differences between systems
than non-paired tests, in which the noise of input dif-
ficulty obscures the actual difference in system per-
3
formance (Rankel et al, 2011). For this paper, we
perform all testing using the Wilcoxon sign rank test.
4 How do we identify a good metric?
If we treat manual evaluation metrics as our gold
standard, then we require that a good automatic met-
ric mirrors the distinctions made by such a man-
ual metric. An automatic metric for summarization
evaluation should reliably predict how well a sum-
marization system would perform relative to other
summarizers if a human evaluation were performed
on the summaries. An automatic metric would hope
to answer the question:
Would summarizer A significantly outper-
form summarizer B when evaluated by a
human?
We address this question by evaluating how well
an automatic metric agrees with a human metric in
its judgements in the following cases:
? all comparisons between different summariza-
tion systems
? all comparisons between systems and human
summarizers.
Depending on the application, we may record the
counts of agreements and disagreements or we may
normalize these counts to estimate the probability
that an automatic evaluation metric will agree with a
human evaluation metric.
4.1 Which is the best ROUGE variant
In this section, we set out to identify which of the
most widely-used versions of ROUGE have highest
accuracy in reproducing human judgements about
the relative merits of pairs of systems. We exam-
ine ROUGE-1, ROUGE-2 and ROUGE-SU4. For
all experiments we use stemming and for each ver-
sion we test scores produced both with and without
removing stopwords. This corresponds to six differ-
ent versions of ROUGE that we examine in detail.
ROUGE outputs several scores including preci-
sion, recall, and an F-measure. However, the most
informative score appears to be recall as reported
when ROUGE was first introduced (Lin and Hovy,
2003). Given that in the data we work with, sum-
maries are produced for a specified length in word
s (and all summaries are truncated to the predefined
length), recall on the task does not allow for artifi-
cially high scores which would result by producing
a summary of excessive length.
The goal of our analysis is to identify which of the
ROUGE variants is most accurate in correctly pre-
dicting which of two participating systems is the bet-
ter one according to the manual pyramid and respon-
siveness scores. We use the data for topic-focused
summarization from the TAC summarization track
in 2008-20114.
Table 1 gives the overview of the 2008-2011 TAC
Summarization data, including the number of top-
ics and participants. For each topic there were four
reference (model) summaries, written by one of the
eight assessors; as a result, there were eight human
?summarizers,? but each produced summaries only
for half of the topics.
year topics automatic human references/
summarizers summarizers topic
2008 48 58 8 4
2009 44 55 8 4
2010 46 43 8 4
2011 44 50 8 4
Table 1: Data in TAC 2008-2011 Summarization track.
We compare each pair of participating systems
based on the manual evaluation score. For each pair,
we are interested in identifying the system that is
better. We consider both the case when an appropri-
ate test for statistical significance has been applied to
pick out the better system as well as the case where
simply the average scores of systems over the test set
are compared. The latter use of evaluations is most
common in research papers on summarization; how-
ever, in summarization system development, testing
for significance is important because a difference in
summarizer scores that is statistically significant is
much more likely to reflect a true difference in qual-
ity between the two systems.
Therefore, we look at agreement between
ROUGE and manual metrics in two ways:
? agreement about significant differences be-
tween summarizers, according to a paired
4In all these years systems also competed on producing up-
date summaries. We do not report results on this task for the
sake of simplifying the discussion.
4
Auto only Human-Automatic
Pyr Resp Pyr Resp
diff no diff contr diff no diff contr diff no diff contr diff no diff contr
r1m 91 59 0.85 87 51 1.34 91 75 0.06 91 100 0.45
r1ms 90 59 0.83 84 50 3.01 91 75 0.06 90 100 0.45
r2m 91 68 0.19 88 60 0.47 75 75 0.62 75 100 1.02
r2ms 88 72 0 84 62 0.65 73 75 1.56 72 100 1.95
r4m 91 64 0.62 87 56 0.91 82 75 0.43 82 100 0.83
r4ms 90 64 0.04 85 55 1.15 83 75 0.81 83 100 1.20
Table 2: Average percentage agreement between ROUGE and manual metrics about significant differences on TAC
2008-2011 data. r1 = ROUGE-1, r2 = ROUGE-2, r4 = ROUGE-SU4, m = stemmed, s = stopwords removed; diff =
agreement on significant differences, no diff = agreement on lack of significant differences, contr = contradictions.
Auto only Human-Automatic
Pyr Resp Pyr Resp
metric sig all sig all sig all sig all
r1m 77 87 70 82 90 99 90 99
r1ms 77 88 69 80 90 98 90 98
r2m 81 89 75 83 75 94 75 94
r2ms 81 89 74 81 72 93 72 93
r4m 80 88 73 82 82 96 82 96
r4ms 79 89 71 81 83 96 83 96
Table 3: Average agreement between ROUGE and manual metrics on TAC 2008-2011 data. r1 = ROUGE-1, r2 =
ROUGE-2, r4 = ROUGE-SU4, m = stemmed, s = stopwords removed; sig = agreement on significant differences, all
= agreement on all differences.
Wilcoxon test. No adjustments for multiple
comparisons are made.
? agreement about any differences between sum-
marizers (whether significant on not).
Agreements occur when the two evaluation met-
rics make the same distinction between System A
and System B: A is significantly better than B, A is
significantly worse than B, or A and B are not sig-
nificantly different from each other. Contradictions
occur when both metrics find a significant difference
between A and B, but in opposite directions; this is
a much more serious case than a mere lack of agree-
ment (i.e., when one metric says A and B are not
significantly different, and the other metric finds a
significant difference).
Table 2 shows the average percentage agreement
between ROUGE and Pyramid/Responsiveness
when it comes to identifying significant differences
or lack thereof. Column diff shows the recall
of significant differences between pairs of systems
(i.e., how many significant differences determined
by Pyramid/Responsiveness are found by ROUGE);
column no diff gives the recall of the cases where
there are no significant differences between two sys-
tems according to Pyramid/Responsiveness.
There are a few instances of contradictions, as
well, but their numbers are fairly small. ?Auto only?
refers to comparisons between automatic summariz-
ers only; ?Human-Automatic? refers to cases when
a human summarizer is compared to an automatic
summarizer. There are fewer human summarizers,
so there are fewer ?Human-Automatic? comparisons
than ?Auto only? ones.
There are a few exceptional cases where the hu-
man summarizer is not significantly better than the
automatic summarizers, even according to the man-
ual evaluation, which accounts for the uniform val-
ues in the ?no difference? column (this is proba-
bly because the comparison is performed for much
fewer test inputs).
Table 3 combines the number of agreements in
the ?difference? and ?no difference? columns from
Table 2 into the sig column, which shows accu-
racy: in checking system pairs for significant differ-
ences, in how many cases does ROUGE make the
same decision as the manual metric (there is/isn?t
a significant difference between A and B). Ta-
ble 3 also gives the number of agreements about
any differences between systems, not only those
that reached statistical significance; in other words,
agreements on system pairwise rankings. In both
5
tables we see that removing stopwords often de-
creases performance of ROUGE, although not al-
ways. Also, there is no clear winner in the ROUGE
comparison: while ROUGE-2 with stemming is the
best at distinguishing among automatic summariz-
ers, ROUGE-1 is the most accurate when it comes
to human?automatic comparisons. To reflect this,
we adopt both ROUGE-1 and ROUGE-2 (with stem-
ming, without removing stopwords) as our reference
automatic metrics for further comparisons.
Reporting pairwise accuracy of automatic evalua-
tion measures has several advantages over reporting
correlations between manual and automatic metrics.
In correlation analysis, we cannot obtain any sense
of how accurate the measure is in identifying statis-
tically significant differences. In addition, pairwise
accuracy is more interpretable than correlations and
gives some provisional indication about how likely
it is that we are drawing a wrong conclusion when
relying on automatic metric to report results.
Table 3 tells us that when statistical significance
is not taken into account, in 89% of cases ROUGE-
2 scores will lead to the same conclusion about the
relative merits of systems as the expensive Pyramid
evaluation. In 83% of cases the conclusions will
agree with the Responsiveness evaluation. The accu-
racy of identifying significant differences is worse,
dropping by about 10% for both Pyramid and Re-
sponsiveness.
Finally, we would like to get empirical estimates
of the relationship between the size of the difference
in ROUGE-2 scores between two systems and the
agreement between manual and ROUGE-2 evalua-
tion. The goal is to check if it is the case that if
one system scores higher than another by x ROUGE
points, then it would be safe to assume that a manual
evaluation would have led to the same conclusion.
Figure 1 shows a histogram of differences in
ROUGE-2 scores. The pairs for which this differ-
ence was significant are given in red and for those
where the difference is not significant are given in
blue. The histogram clearly shows that in general,
the size of improvement cannot be used to replace a
test for significance. Even for small differences in
ROUGE score (up to 0.007) there are about 15 pairs
out of 200 for which the difference is in fact signif-
icant according to Pyramid or Responsiveness. As
the difference in ROUGE-2 scores between the two
systems increases, there are more significant differ-
ences. For differences greater than 0.05, all differ-
ences are significant.
Figure 2 shows the histograms of differences in
ROUGE-2 scores, split into cases where the pairwise
ranking of systems according to ROUGE agrees
with manual evaluation (blue) and disagrees (red).
For score differences smaller than 0.013, about half
of the times ROUGE-2 would be wrong in identify-
ing which system in the pair is the better one accord-
ing to manual evaluations. For larger differences the
number of disagreements drops sharply. For this
dataset, a difference in ROUGE-2 scores of more
than 0.04 always corresponds to an improvement in
the same direction according to the manual metrics.
5 Looking for better metrics
In the preceding sections, we established that
ROUGE-2 is the best ROUGE variant for compar-
ing two automatic systems, and ROUGE-1 is best in
distinguishing between humans and machines. Ob-
viously, it is of great interest to develop even bet-
ter automatic evaluations. In this section, we out-
line a simple procedure for deciding if a new au-
tomatic evaluation is significantly better than a ref-
erence measure. For this purpose, we consider the
automatic metrics from the TAC 2011 AESOP task,
which called for the development of better automatic
metrics for summarization evaluation NIST ( 2011).
For each automatic evaluation metric, we estimate
the probability that it agrees with Pyramid or Re-
sponsiveness. Figure 3 gives the estimated proba-
bility of agreement with Pyramid and Overall Re-
sponsiveness for all AESOP 2011 metrics with an
agreement of 0.6 or more. The metrics are plot-
ted with error bars giving the 95% confidence in-
tervals for the probability of agreement with the
manual evaluations. The red-dashed line is the
performance of the reference automatic evaluation,
which is ROUGE-2 for machine only and ROUGE-
1 for comparing machines and human summariz-
ers. Metrics whose 95% confidence interval is be-
low this line are significantly worse (as measured
by the z-test approximation of a binomial test) than
the baseline. Conversely, those whose 95% con-
fidence interval is above the red line are signifi-
cantly better than the baseline. Thus, just ROUGE-
6
Figure 1: Histogram of the differences in ROUGE-2 score versus significant differences as determined by Pyramid
(left) or Responsiveness (right).
Figure 2: Histogram of the differences in ROUGE-2 score versus differences as determined by Pyramid (left) or
Responsiveness (right).
BE (the MINIPAR variant of ROUGE-BE), one of
NIST?s baselines for AESOP, significantly outper-
formed ROUGE-2 for predicting pyramid compar-
isons; and 4 metrics: ROUGE-BE, DemokritosGR2,
catholicasc1, and CLASSY1, all significantly out-
perform ROUGE-2 for predictiong responsiveness
comparisons. Descriptions of these metrics as well
as the other proposed metrics can be found in the
TAC 2011 proceedings (NIST, 2011).
Similarly, Figure 4 gives the estimated probabil-
ity when the comparison is made between human
and machine summarizers. Here, 10 metrics are sig-
nificantly better than ROUGE-1 in predicting com-
parisons between automatic summarization systems
and human summarizers in both pyramid and re-
sponsiveness. The ROUGE-SU4 and ROUGE-BE
baselines are not shown here but their performance
was approximately 57% and 46% respectively.
If we limit the comparisons to only those where
a significant difference was measured by Pyramid
and also Overall Responsiveness, we get the plots
given in Figure 5 for comparing automatic summa-
rization systems. (The corresponding plot for com-
parisons between machines and humans is omitted
as all differences are significant.) The results show
that there are 6 metrics that are significantly better
than ROUGE-2 for correctly predicting when a sig-
nificant difference in pyramid scores occurs, and 3
metrics that are significantly better than ROUGE-2
for correctly predicting when a significant difference
in responsiveness occurs.
6 Discussion
In this paper we provided a thorough assessment
of automatic evaluation in summarization of news.
We specifically aimed to identify the best variant
of ROUGE on several years of TAC data and dis-
covered that ROUGE-2 recall with stemming and
stopwords not removed, provides the best agreement
with manual evaluations. The results shed positive
light on the automatic evaluation, as we find that
ROUGE-2 agrees with manual evaluation in almost
90% of the case when statistical significance is not
computed, and about 80% when it is. However,
these numbers are computed in a situation where
many very different systems are compared?some
7
Figure 3: Pyramid and Responsiveness Agreement of AESOP 2011 Metrics for automatic summarizers.
Figure 4: Pyramid and Responsiveness Significant Difference Agreement of AESOP 2011 Metrics for all summarizers.
8
Figure 5: Pyramid and Responsiveness Significant Difference Agreement of AESOP 2011 Metrics for automatic
summarizers.
very good, others bad. We examine the size of dif-
ference in ROUGE score and identify that for differ-
ences less than 0.013 a large fraction of the conclu-
sions drawn by automatic evaluation will contradict
the conclusion drawn by a manual evaluation. Fu-
ture studies should be more mindful of these find-
ings when reporting results.
Finally, we compare several alternative automatic
evaluation measures with the reference ROUGE
variants. We discover that many new proposals are
better than ROUGE in distinguishing human sum-
maries from machine summaries, but most are the
same or worse in evaluating systems. The Basic El-
ements evaluation (ROUGE-BE) appears to be the
strongest contender for an automatic evaluation to
augment or replace the current reference.
References
Paul Over and Hoa Dang and Donna Harman. 2007.
DUC in context. Inf. Process. Manage. 43(6), 1506?
1520.
Chin-Yew Lin and Eduard H. Hovy. 2003. Auto-
matic Evaluation of Summaries Using N-gram Co-
occurrence Statistics. Proceeding of HLT-NAACL.
Michel Galley. 2006. A Skip-Chain Conditional Ran-
dom Field for Ranking Meeting Utterances by Impor-
tance. Proceeding of EMNLP, 364?372.
Feifan Liu and Yang Liu. 2010. Exploring correlation
between ROUGE and human evaluation on meeting
summaries. Trans. Audio, Speech and Lang. Proc.,
187?196.
C.Y. Lin. 2004. Looking for a Few Good Metrics: Au-
tomatic Summarization Evaluation - How Many Sam-
ples are Enough? Proceedings of the NTCIR Work-
shop 4.
Ani Nenkova and Rebecca J. Passonneau and Kathleen
McKeown. 2007. The Pyramid Method: Incorporat-
ing human content selection variation in summariza-
tion evaluation. TSLP 4(2).
Emily Pitler and Annie Louis and Ani Nenkova. 2010.
Automatic Evaluation of Linguistic Quality in Multi-
Document Summarization. Proceedings of ACL, 544?
554.
Ani Nenkova. 2005. Automatic Text Summarization of
Newswire: Lessons Learned from the Document Un-
derstanding Conference. AAAI, 1436?1441.
Ani Nenkova and Annie Louis. 2008. Can You Summa-
rize This? Identifying Correlates of Input Difficulty
for Multi-Document Summarization. ACL, 825?833.
Peter Rankel and John M. Conroy and Eric Slud and Di-
anne P. O?Leary. 2011. Ranking Human and Machine
Summarization Systems. Proceedings of EMNLP,
467?473.
National Institute of Standards and Technology.
2011. Text Analysis Workshop Proceedings
http://www.nist.gov/tac/publications/index.html.
9
Proceedings of the MultiLing 2013 Workshop on Multilingual Multi-document Summarization, pages 29?38,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
ACL 2013 MultiLing Pilot Overview
Jeff Kubina
U.S. Department of Defense
9800 Savage Rd., Ft. Meade, MD 20755
jmkubin@tycho.ncsc.mil
John M. Conroy, Judith D. Schlesinger
IDA/Center for Computing Sciences
17100 Science Dr., Bowie, MD
conroy@super.org, drj1945@gmail.com
Abstract
The 2013 Association for Computational
Linguistics MultiLing Pilot posed a task
to measure the performance of multi-
lingual, single-document, summarization
systems using a dataset derived from many
Wikipedias. The objective of the pilot
was to assess automatic summarization of
multilingual text documents outside the
news domain and the potential of using
Wikipedia articles for such research. This
report describes the pilot task, the dataset,
the methods used to evaluate the submitted
summaries, and the overall performance of
each participant?s system.
1 Introduction
Document summarization is an active subject of
research and development. The ACM Digital Li-
brary has about 806 reports on the subject pub-
lished since 1993, with over half of them appear-
ing in the last five years. While the impetus for
much of this research is the annual Text Anal-
ysis Conference (TAC) workshop on document
summarization, there is a growing demand in the
consumer market for news summarization appli-
cations being met by tablet and smart-phone ap-
plications such as Clipped1, Summoner2, TLDR3,
and Yahoo News. Yahoo and Google even ac-
quired two companies developing such applica-
tions, Summly (Stelter, 2013) and Wavii (Tsotsis,
2013) respectively, earlier this year. While sum-
marization technology for news sources is com-
ing to fruition, the performance of such technol-
ogy on non-English documents outside the news
domain has not been throughly assessed and may
need further research. Since the datasets used by
1
http://goo.gl/dFKD9
2
http://goo.gl/0QFaZ
3
http://goo.gl/qEgCs
the TAC summarization workshops have predom-
inately been English news articles, with some ex-
ceptions (Giannakopoulos et al, 2011), the objec-
tive of the 2013 ACL MultiLing Pilot was to assess
the performance of automatic multilingual single-
document summarization systems on non-English
text outside the news domain and to determine the
potential of using Wikipedia articles for such re-
search.
This report starts with a description of the task
and dataset, the methods used to evaluate the sub-
mitted summaries, the performance of each partic-
ipating system, and concludes with an assessment
of the pilot and potential future work.
2 Task and Dataset Description
The objective of each participant system of the pi-
lot was simple: compute a summary for each doc-
ument in at least two of the datasets languages.
No restrictions were placed on the languages that
could be chosen nor was any target summary size
specified.
The dataset was derived from a corpus cre-
ated in 2010 to measure the performance of the
CLASSY (Conroy et al, 2009) summarization al-
gorithm on non-English documents outside the
news domain. At the time such a corpus did not
exist so one was created from the Wikipedias.
To date there are Wikipedias in 285 languages
comprising over 75 million pages. Some of the
Wikipedias maintain a list of Feature Articles,
which are articles reviewed and voted upon by ed-
itors as the best that fulfill Wikipedia?s require-
ments in accuracy, neutrality, completeness, and
style. One such requirement is that the article have
a lead section that should
. . . be able to stand alone as a concise
overview. It should . . . summarize the
most important points . . . [and] material
in the lead should roughly reflect its im-
29
portance to the topic . . . 4
So the lead section of a featured article is
an excellent summary of it, hence, the fea-
tured articles were used to create the corpus.
In 2010 there were 41 Wikipedias with more
than nine featured articles. The Perl module
Text::Corpus::Summaries::Wikipedia
5 was de-
veloped to automatically create the corpus from
the featured articles of those Wikipedias. The cor-
pus is publicly available (Kubina, 2010) and the
Perl module can be used to create an updated cor-
pus.
The dataset for the pilot was created from a
subset of the 2010 corpus. This was done to en-
sure that each language had 30 articles and that
the size of each article?s body text was sufficiently
large. First, for each article the summary and body
were compressed to approximate their informa-
tion content size. For example, given a Chinese
and English article with the same character length
the Chinese article will usually contain more in-
formation than an English article and their com-
pressed sizes will approximation their true infor-
mation content. Next, if the compressed body
size of an article was less than five times its com-
pressed summary size, then the article was dis-
carded. The factor of five was simply chosen to
ensure the body of each article was sufficiently
large relative to the summary size. For each lan-
guage the median of the ratio of compressed body
size to compressed summary size was computed
and only the 30 articles closest to the median were
included in the dataset. This filtering reduced the
corpus from 12, 819 articles in 41 languages to the
dataset containing 1, 200 articles in 40 languages.
For each language in the dataset Table 1 contains
the mean size of the articles, their bodies, and their
summaries, in characters.
3 Evaluation Methods and Results
Four teams submitted the results of six summa-
rization systems. The teams are denoted by AIS,
LAN, MD, and MUS; the MD team submitted
three systems. Throughout this report the systems
are denoted by AIS, LAN, MD1, MD2, MD3, and
MUS. Table 2 contains the list of languages sub-
mitted for each system and the mean size, in char-
acters, of the summaries submitted.
4
http://en.wikipedia.org/wiki/Wikipedia:LEAD
5
http://goo.gl/ySgOS
For the evaluation a baseline summary was ex-
tracted from the each article in the dataset that is
the prefix substring of the article?s body text with
the same length as the text in the lead section of
the article. For the remainder of this report the
lead section of an article is called the human sum-
mary. An oracle summary was also computed for
each article by heuristically extracting sentences
from its body text to maximize its ROUGE-2 score
against the human summary until its size exceeded
the human summary, upon which it was truncated.
Submitted summaries were automatically eval-
uated against the human summary of each arti-
cle using ROUGE-1, ROUGE-2 (Lin, 2004) and
MeMoG (Giannakopoulos et al, 2008). For
ROUGE, the languages Chinese, Japanese, Ko-
rean, and Thai were tokenized into individual
characters. For MeMoG the character n-gram size
used for each language is listed in Table 3, which
is the n-gram size that maximized the standard de-
viation divided by the mean of the n-gram fre-
quency distribution of the language in the dataset.
So the selected n-gram size maximizes the vari-
ability of the distribution values relative to their
mean. A shorter n-gram size would inflate the
MeMoG scores because of their inherent frequent
co-occurrence and conversely a longer size would
penalize MeMoG scores due to their infrequent
co-occurrence.
Each scoring method was performed twice, first
by truncating, if necessary, each system summary
to the size of the human summary, which is called
HSS-scoring. The second set of scores were com-
puted by truncating all the summaries of an ar-
ticle, including the human summary, to the size
of the shortest summary amongst the system and
human summaries for the article, which is called
SSS-scoring. For HSS-scoring the system sum-
maries shorter that the human summary are penal-
ized since ROUGE is recall oriented. Alternately,
SSS-scoring gives preference to shorter system
summaries that have their best content (extracted
sentences) first.
The performance for HSS-scoring of the sys-
tems on the seven languages that at least two teams
submitted summaries for are given in Figures 1, 2,
and 3. Table 4 gives an overview of how often sig-
nificant differences in each of the three automatic
metrics was observed. In particular, the last row
gives the fraction of times that an non-parametric
analysis of variance (ANOVA) indicated that the
30
Table 1: Dataset Languages and Sizes
ISO LANGUAGE ARTICLE BODY SUMMARY
af Afrikaans 24752 (10214) 23448 (10230) 1303 (196)
ar Arabic 27845 (9490) 26354 (9530) 1491 (220)
bg Bulgarian 23965 (9248) 22981 (9250) 984 (134)
ca Catalan 30611 (15248) 29322 (15274) 1289 (140)
cs Czech 26300 (10453) 24777 (10414) 1522 (190)
de German 32023 (12522) 31160 (12530) 862 (53)
el Greek 26072 (11113) 24937 (11096) 1134 (224)
en English 26572 (9010) 24860 (9013) 1712 (114)
eo Esperanto 22295 (10031) 21304 (10022) 990 (106)
es Spanish 40467 (19563) 38726 (19533) 1740 (113)
eu Basque 17886 (9845) 17231 (9821) 655 (91)
fa Persian 15132 (7630) 14099 (7217) 1032 (517)
fi Finnish 27379 (11783) 26353 (11805) 1025 (105)
fr French 41578 (21952) 40186 (21959) 1392 (73)
he Hebrew 18492 (8283) 17697 (8283) 794 (82)
hr Croatian 21132 (11094) 20276 (11113) 855 (96)
hu Hungarian 26256 (12161) 25175 (12139) 1081 (90)
id Indonesian 18550 (9131) 17649 (9124) 901 (148)
it Italian 39189 (19235) 38042 (19220) 1146 (80)
ja Japanese 14352 (11890) 14131 (11895) 221 (38)
ka Georgian 15282 (9570) 14558 (9551) 723 (124)
ko Korean 17140 (7899) 16416 (7889) 724 (175)
ml Malayalam 27329 (10645) 26158 (10639) 1170 (331)
ms Malay 19346 (16577) 18436 (16348) 909 (411)
nl Dutch 29575 (16346) 28580 (16363) 994 (89)
nn Norwegian-Nynorsk 16107 (8056) 15384 (7917) 722 (297)
no Norwegian-Bokmal 30225 (17652) 29218 (17594) 1006 (125)
pl Polish 23028 (12853) 22067 (12861) 960 (66)
pt Portuguese 30967 (17998) 29310 (18004) 1657 (110)
ro Romanian 21921 (12812) 20782 (12773) 1139 (108)
ru Russian 34069 (13792) 33134 (13771) 934 (70)
sh Serbo-Croatian 21776 (21469) 21060 (21341) 716 (308)
sk Slovak 21694 (10067) 20983 (10071) 711 (169)
sl Slovenian 17900 (7222) 17077 (7194) 823 (135)
sr Serbian 30239 (9812) 28927 (9764) 1312 (176)
sv Swedish 23476 (10169) 22314 (10156) 1162 (99)
th Thai 27041 (8312) 25425 (8291) 1616 (226)
tr Turkish 32956 (16423) 31346 (16338) 1610 (257)
vi Vietnamese 35376 (16099) 33857 (16050) 1518 (161)
zh Chinese 10110 (4341) 9608 (4357) 501 (42)
Table 1: The table lists the languages in the dataset with the first column containing the ISO code for
each the language, the second column the name of the language, and the remaining columns containing
the mean size, in characters, and standard deviation, in parentheses, of the entire article, their bodies, and
their summaries. For example, for English the mean size of the human summaries is 1,712 characters.
31
Table 2: Mean Summary Size For Submitted Languages of Systems
ISO LANGUAGE AIS LAN MD1 MD2 MD3 MUS SUM
af Afrikaans 966 953 967 1303
ar Arabic 1461 876 858 874 2232 1491
bg Bulgarian 1302 969 946 967 984
ca Catalan 911 921 925 1289
cs Czech 1061 1020 1062 1522
de German 1492 1072 1037 1087 862
el Greek 1367 989 979 991 1134
en English 1262 1551 944 957 958 1197 1712
eo Esperanto 947 933 956 990
es Spanish 922 916 927 1740
eu Basque 1154 1151 1167 655
fa Persian 793 792 800 1032
fi Finnish 1328 1284 1323 1025
fr French 936 930 952 1392
he Hebrew 871 867 876 1098 794
hr Croatian 979 954 976 855
hu Hungarian 1092 1064 1089 1081
id Indonesian 1091 1085 1091 901
it Italian 981 952 975 1146
ja Japanese 546 564 563 221
ka Georgian 1180 1195 1218 723
ko Korean 663 638 656 724
ml Malayalam 670 648 676 1170
ms Malay 1089 1089 1098 909
nl Dutch 994 974 1000 994
nn Norwegian-Nynorsk 928 908 929 722
no Norwegian-Bokmal 967 937 977 1006
pl Polish 1086 1056 1083 960
pt Portuguese 942 936 939 1657
ro Romanian 1311 938 940 948 1139
ru Russian 1095 1046 1078 934
sh Serbo-Croatian 969 955 983 716
sk Slovak 1026 997 1031 711
sl Slovenian 967 949 981 823
sr Serbian 990 954 979 1312
sv Swedish 997 990 1006 1162
th Thai 553 566 563 1616
tr Turkish 1166 1132 1152 1610
vi Vietnamese 696 684 691 1518
zh Chinese 523 559 552 501
Table 2: The mean summary size, in characters, for each language submitted by each system including
the mean of the human summaries in the last column named SUM.
32
Table 3: N-gram Size Per Language for MeMoG
ISO LANGUAGE SIZE ISO LANGUAGE SIZE
af Afrikaans 5 ka Georgian 3
ar Arabic 3 ko Korean 1
bg Bulgarian 4 ml Malayalam 3
ca Catalan 4 ms Malay 4
cs Czech 4 nl Dutch 4
de German 4 nn Norwegian-Nynorsk 4
el Greek 4 no Norwegian-Bokmal 4
en English 5 pl Polish 4
eo Esperanto 4 pt Portuguese 4
es Spanish 4 ro Romanian 4
eu Basque 4 ru Russian 4
fa Persian 4 sh Serbo-Croatian 3
fi Finnish 4 sk Slovak 4
fr French 4 sl Slovenian 4
he Hebrew 3 sr Serbian 4
hr Croatian 4 sv Swedish 5
hu Hungarian 4 th Thai 3
id Indonesian 5 tr Turkish 5
it Italian 5 vi Vietnamese 5
ja Japanese 1 zh Chinese 1
Table 3: The table lists the n-gram size used for each language when evaluating summaries using
MeMoG, which is the n-gram size that maximized the standard deviation divided by the mean of the
n-gram frequency distribution of the language in the dataset.
33
Figure 1: ROUGE-1 scores for HSS.
Figure 2: ROUGE-2 scores for HSS.
34
Figure 3: MeMoG scores for HSS.
Table 4: Fraction of time a system beat the base-
line for HSS.
System ROUGE-1 ROUGE-2 MeMoG
AIC 2/5 0/5 0/5
LAN 0/2 0/2 0/2
MD1 15/40 4/40 2/39
MD2 16/40 4/40 0/39
MD3 15/40 4/40 0/39
MUS 2/3 1/3 0/3
ANOVA 28/40 13/40 5/39
Table 4: The table gives the fraction of languages
each system significantly outperform the base-
line. The last line gives the number of times an
ANOVA rejected the null hypothesis, indicating
significance.
medians of the system scores were not the same,
using a rejection threshold of 0.05. Also, the frac-
tion of time that each system significantly outper-
formed the lead baseline is also recorded. A paired
Wilcoxon test was invoked whenever the ANOVA
indicated a significant difference was present, with
a threshold of 0.05.
Lastly, each systems performance for SSS-
scoring is provided in Figures 4, 5, and 6. Sur-
prisingly, the results change little. Lastly Table 5
contains the number of times that each system beat
the baseline summary with a 95% confidence mea-
sured as a result of the non-parametric ANOVA
and the Wilcoxon paired sign rank test. The results
show that the number of significant differences go
down for ROUGE scores and up for MeMoG.
4 Summary
Overall, the authors believe the pilot was success-
ful in that it exposed researchers to the poten-
tial for using Wikipedia articles for summarization
research and demonstrated that generating sum-
maries for the genre of Wikipedia articles is a more
challenging task than newswire documents. No-
tably, no system outperformed the baseline for En-
glish! In hindsight this is not too surprising since
news articles have a prose style6 significantly dif-
ferent from Wikipedia articles7. Wikipedia arti-
cles are written as expositions having a topical
flow that can vary significantly between sections
but news articles are written in a style8 that ad-
dresses the most important information first?the
6
http://en.wikipedia.org/wiki/News_style
7
http://en.wikipedia.org/wiki/MOS:
8
http://en.wikipedia.org/wiki/Inverted_pyramid
35
Figure 4: ROUGE-1 scores for SSS.
Figure 5: ROUGE-2 scores for SSS.
36
Figure 6: MeMoG scores for SSS.
Table 5: Fraction of the time a system beat the lead
baseline for SSS.
System ROUGE-1 ROUGE-2 MeMoG
AIC 0/5 0/5 0/5
LAN 0/2 0/2 0/2
MD1 8/40 2/40 6/39
MD2 10/40 2/40 2/39
MD3 7/40 2/40 4/39
MUS 0/3 0/3 0/3
ANOVA 11/40 5/40 7/39
Table 5: The table gives the fraction of lan-
guages that each system significantly outperform
the baseline on. The last line contains the number
of times an ANOVA rejected the null hypothesis,
indicating significance.
who, what, when, where and why?with the sub-
sequent text providing more details. Hence news
articles have a more even topical flow. The authors
hope these results stimulate research and devel-
opment of summarization algorithms outside the
news domain.
As for the metrics, ROUGE-1 observed the
most significant differences among the systems
and MeMoG observed the least as measured by
a non-parametric ANOVA. However, a human
evaluation of the summaries generated would be
needed to detemine which of the automatic metrics
is best at predicting significant differences among
systems for such data.
References
John M. Conroy, Judith D. Schlesinger, and Dianne P.
O?leary. 2009. Classy 2009: summarization and
metrics. In Proceedings of the text analysis confer-
ence (TAC).
George Giannakopoulos, Vangelis Karkaletsis, George
Vouros, and Panagiotis Stamatopoulos. 2008.
Summarization system evaluation revisited: N-
gram graphs. ACM Trans. Speech Lang. Process.,
5(3):5:1?5:39, October.
George Giannakopoulos, Mahmoud El-Haj, Beno?t
Favre, Marina Litvak, Josef Steinberger, and Va-
37
sudeva Varma. 2011. Tac 2011 multiling pilot
overview.
Jeff Kubina. 2010. Wikipedia featured article cor-
pus. http://goo.gl/AmMGN. [Online; accessed
30-May-2013].
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74?81.
Brian Stelter. 2013. He has millions and a new job at
yahoo. soon, he?ll be 18. New York Times.
Alexia Tsotsis. 2013. Google buys wavii for north of
$30 million. TechCrunch.
38
Proceedings of the MultiLing 2013 Workshop on Multilingual Multi-document Summarization, pages 55?63,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Multilingual Summarization: Dimensionality Reduction and a Step
Towards Optimal Term Coverage
John M. Conroy
IDA / Center for Computing Sciences
conroy@super.org
Sashka T. Davis
IDA / Center for Computing Sciences
stdavi3@super.org
Jeff Kubina
Department of Defense
jmkubin@tycho.ncsc.mil
Yi-Kai Liu
National Institute of Standards and Technology
yi-kai.liu@nist.gov
Dianne P. O?Leary
University of Maryland and NIST
oleary@cs.umd.edu
Judith D. Schlesinger
IDA/Center for Computing Sciences
drj1945@gmail.com
Abstract
In this paper we present three term weight-
ing approaches for multi-lingual docu-
ment summarization and give results on
the DUC 2002 data as well as on the
2013 Multilingual Wikipedia feature arti-
cles data set. We introduce a new interval-
bounded nonnegative matrix factorization.
We use this new method, latent semantic
analysis (LSA), and latent Dirichlet alo-
cation (LDA) to give three term-weighting
methods for multi-document multi-lingual
summarization. Results on DUC and TAC
data, as well as on the MultiLing 2013
data, demonstrate that these methods are
very promising, since they achieve ora-
cle coverage scores in the range of hu-
mans for 6 of the 10 test languages. Fi-
nally, we present three term weighting ap-
proaches for the MultiLing13 single docu-
ment summarization task on the Wikipedia
featured articles. Our submissions signifi-
cantly outperformed the baseline in 19 out
of 41 languages.
1 Our Approach to Single and
Multi-Document Summarization
The past 20 years of research have yielded a
bounty of successful methods for single docu-
ment summarization (SDS) and multi-document
summarization (MDS). Techniques from statistics,
machine learning, numerical optimization, graph
theory, and combinatorics are generally language-
independent and have been applied both to single
and multi-document extractive summarization of
multi-lingual data.
In this paper we extend the work of our re-
search group, most recently discussed in Davis et
al. (2012) for multi-document summarization, and
apply it to both single and multi-document multi-
lingual document summarization. Our extractive
multi-document summarization performs the fol-
lowing steps:
1. Sentence boundary detection;
2. Tokenization and term identification;
3. Term-sentence matrix generation;
4. Term weight determination;
5. Sentence selection;
6. Sentence ordering.
Sentence boundary detection and tokenization are
language dependent, while steps (3)-(6) are lan-
guage independent. We briefly discuss each of
these steps.
We use a rule based sentence splitter FASST-
E (very Fast, very Accurate Sentence Splitter for
Text ? English) (Conroy et al, 2009) and its multi-
lingual extensions (Conroy et al, 2011) for deter-
mining the boundary of individual sentences.
Proper tokenization improves the quality of
the summary and may include stemming and
also morphological analysis to disambiguate com-
pound words in languages such as Arabic. To-
kenization may also include stop word removal.
The result of this step is that each sentence is rep-
resented as a sequence of terms, where a term can
be a single word, a sequence of words, or char-
acter n-grams. The specifics of tokenization are
discussed in Section 2.
Matrix generation (the vector space model) was
pioneered by Salton (1991). Later Dumais (1994)
introduced dimensionality reduction in document
retrieval systems, and this approach has also been
55
used by many researchers for document summa-
rization. (In addition to our own work, see, for
example Steinberger and Jezek (2009).) We con-
struct a single term-sentence matrix A = (ai,j),
where i = 1, . . . ,m ranges over all terms, and
j = 1, . . . , n ranges over all sentences, for ei-
ther a single document, when we perform SDS, or
for a collection of documents for MDS. The row
labels of the term-sentence matrix are the terms
T = (t1, . . . , tm) determined after tokenization.
The column labels are the sentences S1, . . . , Sn of
the document(s). The entries of the matrix A are
defined by
ai,j = `i,jgi,
Here, `i,j is the local weight, which is 1 when term
i appears in sentence j and 0 otherwise.
The global weight gi should be proportional to
the contribution of the term in describing the major
themes of the document. While the global weight
could be used as a term weight in a sentence se-
lection scheme, it may be beneficial to perform di-
mensionality reduction on the matrix A and com-
pute term weights based on the lower dimensional
matrix. In this work we seek to find strong term
weights for both single- and and multi-document
summarization. These cases are handled sepa-
rately, as we found that multi-document summa-
rization benefits a lot from dimensionality reduc-
tion while single document summarization does
not.
Our previous multi-document summarization
algorithm, OCCAMS (Davis et al, 2012), used
the linear algebraic technique of Latent Seman-
tic Analysis (LSA) to determine term weights and
used techniques from combinatorial optimization
for sentence selection. In our CLASSY algorithms
(e.g., (Conroy et al, 2011)), we used both a lan-
guage model and machine learning as two alterna-
tive approaches to assign term weights. CLASSY
then used linear algebraic techniques or an inte-
ger linear program for sentence selection. Sec-
tion 3 describes the term weights we use when
we summarize single documents. In Section 4
we present three different dimensionality reduc-
tion techniques for the term-sentence matrix A.
Once term weight learning has assigned weights
for each term of the document(s) and dimension-
ality reduction has been applied (if desired), the
next step, sentence selection, chooses a set of sen-
tences of maximal length L for the extract sum-
mary. These sentences should cover the major
themes of the document(s), minimize redundancy,
and satisfy the bound on the length of the sum-
mary. We discuss our OCCAMS V sentence se-
lection algorithm in Section 5.
Sentence ordering is performed using an ap-
proximate traveling salesperson algorithm (Con-
roy et al, 2009).
Three term weighting variants were used to gen-
erate summaries for each of the 10 languages in
the MultiLing 2013 multi-document summariza-
tion task. The target summary length was set to be
250 words for all languages except Chinese, where
700 characters were generated.
We now present the details of our improvements
to our algorithms and results of our experiments.
2 From Text to Term-Sentence Matrix
After sentence boundaries are determined, we
used one of three simple tokenization methods
and then one of two term-creation methods, as
summarized in Table 1. Languages were divided
into three categories: English, non-English lan-
guages with space delimited words, and ideo-
graphic languages (Chinese for MDS and Chi-
nese, Japanese, and Thai for the SDS pilot task).
For non-ideographic languages, tokens are formed
based on a regular expression. For English, to-
kens are defined as contiguous sequences of upper
or lower case letters and numbers. For other non-
ideographic languages, tokens were defined sim-
ilarly, and the regular expression describes what
characters are used to break the tokens. These
characters include white space and most punctu-
ation except the apostrophe. For English, Porter
stemming was used for both SDS and MDS, with a
stop list of approximately 600 words for SDS. For
English and other word-based languages, lower-
cased bi-tokens were used in MDS and lower-
cased tokens for SDS. For all languages, and both
SDS and MDS, Dunning?s mutual information
statistic (Dunning, 1993) is used to select terms,
using the other documents as background. The p-
value (rejection threshold), initially set at 5.0e-4,
is repeatedly doubled until the number of terms
is at least twice the length of the target summary
(250 for MDS, 150 words or 500 characters for
SDS). Note that these terms are high confidence
signature terms (Lin and Hovy, 2000) i.e., the p-
value is small. We describe our terms as high mu-
tual information (HMI), since Dunning?s statistic
is equivalent to mutual information as defined by
56
Language Tokens Terms for MDS Terms for SDS
English [?A-Za-z0-9] HMI bi-tokens HMI non-stop-word
tokens
Non-English [\s.?,";:?![](){}<>&*=+@#$] HMI bi-tokens HMI tokens
Ideographic 4-byte grams HMI tokens HMI tokens
Table 1: Term and token definition as a function of language and task.
Cover and Thomas (1991).
3 Determining Term Weights for Single
Document Summarization
For SDS we consider three term weighting meth-
ods.
The first is global entropy as proposed by Du-
mais et al for information retrieval (Dumais,
1994) (Rehder et al, 1997) and by Steinberger and
Jezek for document summarization (Steinberger
and Jezek, 2009). Global entropy weighting is
given by
w(GE)i = 1?
?
j pi,j ? log pi,j
log n
,
where n is the number of sentences, pi,j = tij/fi,
tij is the number of times term i appears in sen-
tence j, and fi is the total number of times term i
appears in all sentences.1
The second term weighting is simply the loga-
rithm of frequency of the term in all the sentences:
w(LF)i = 1 + log(fi).
Log frequency is motivated by the fact that the
sum of the term scores for a given sentence is
(up to an affine transformation) the log probabil-
ity of generating that sentence by sampling terms
independently at random, where the probability
of each term is estimated by maximum likelihood
from the observed frequencies fi.
The third method is a personalized variant of
TextRank, which was first proposed by Mihal-
cea (2005) and motivated by PageRank (Page et
al., 1999). The personalized version smooths the
Markov chain used in TextRank (PageRank) with
term (page) preferences. Previously, a sentence
based version of personalization has been used
for summarization; see, for example, Zhao et al
(2009). Our current work may be the first use of
1We make the usual convenient definition that pi log pi =
0 when pi = 0.
a term based personalized TextRank (TermRank),
which we call PTR. The personalization vector we
choose is simply the normalized frequency, and
the Markov chain is defined by the transition ma-
trix
M =
1
2
LLTD +
1
2
peT
where
pi = fi/
?
i
(fi),
L is the incidence term-sentence matrix. The el-
ements of L are previously defined local weights,
`ij . The vector e is all 1?s and D is a diagonal
matrix chosen to make the column sums equal to
one. The estimated weight vector used by OC-
CAMS V, w(PTR), is computed using 5 iterations
of the power method to approximate the station-
ary vector of this matrix. Note, there is no need to
form the matrix M since the applications of M to
a vector may be achieved by vector operations and
matrix-vector multiplies by L and LT .
We test the performance of these three term
weighting methods on two data sets: DUC 2002
English single-document data and the Wikipedia
Pilot at MultiLing 2013.
3.1 Results for DUC 2002 Data
The DUC 2002 English single-document data con-
tains 567 newswire documents for which there are
one or two human-generated summaries.
In addition to computing ROUGE-2 scores, we
also compute an oracle coverage score (Conroy et
al., 2006). At TAC 2011 (Conroy et al, 2011)
(Rankel et al, 2012) bigram coverage was shown
to be a useful feature for predicting the perfor-
mance of automated summarization systems rela-
tive to a human summarizer. Oracle unigram cov-
erage score is defined by
C1(X) =
?
i?T
f1(i),
where T is the set of terms and f1(i) is the frac-
tion of humans who included the ith term in the
57
Term Weight ROUGE-2 C1 Group
PTR 0.194 19.1 1
LF 0.192 18.7 2
GE 0.190 18.6 2
Table 2: ROUGE-2 and Coverage bi-grams Scores
summary. More generally, we define Cn in similar
way for n-gram oracle coverage scores. Coverage
scores differ from ROUGE scores since the score
is not affected by the number of times that a given
human or machine-generated summary uses the
term, but only whether or not the term is included
in the machine summary and the estimated frac-
tion of humans that would use this term. We note
that this score can be modified to compute scores
for human summarizers using the analogous jack-
knife procedure employed by ROUGE.
Table 2 gives a summary of the results. We
ran a Wilcoxon test to check for statistical dis-
tinguishability in the performance of the different
term-weighting methods. Methods were placed in
the same group if they produced results in cover-
age (C1) that were indistinguishable. More pre-
cisely, we used the null hypothesis that the differ-
ence between the vector of scores for two methods
has median 0. If the p-value of two consecutive
entries in the table was less than 0.05, the group
label was increased and is shown in the last col-
umn.
Log frequency (LF) and global entropy (GE)
are correlated. For the DUC 2002 data they per-
form comparably. Personalized term rank (PTR)
weighting is statistically stronger than the other
two approaches, as measured by the oracle term
coverage score. For these data the definition of
term for the purposes of the computation of the
oracle coverage score is non-stop word stemmed
(unigram) tokens.
3.2 Results for the Wikipedia Pilot at
MultiLing 2013
This task involves single-document summariza-
tion for 1200 Wikipedia feature articles: 30 doc-
uments in each of 40 languages. For each doc-
ument, the organizers generated a baseline lead
summary consisting of the first portion of the fea-
ture article following the ?hidden summary.? Sum-
mary lengths were approximately 150 words for
all non-ideograph languages and 500 characters
for the ideograph languages. Sentences were or-
dered in the order selected by OCCAMS V. Thus,
sentences covering the largest number of relevant
terms, as measured by the term-weighting scheme,
will appear first.
Results of this pilot study will be presented in
detail in the overview workshop paper, but we note
here that, as measured by ROUGE-1, in 19 of the
40 languages, at least one of our three submit-
ted methods significantly outperformed the lead-
summary baseline.
4 Dimensionality Reduction
The goal of dimensionality reduction is to iden-
tify the major factors of the term-sentence ma-
trix A and to throw away those factors which are
?irrelevant? for summarization. Here we survey
three algorithms: the well-known LSA, the more
recent latent Dirichlet alocation (LDA), and the
new interval-bounded nonnegative matrix factor-
ization.
4.1 Latent Semantic Analysis
Davis et al (2012) successfully used an approx-
imation to A, computed using the singular value
decomposition (SVD) A = USV T . They used
the first 200 columns U200 of the singular vector
matrix U and the corresponding part of the singu-
lar value matrix S. They eliminated negative en-
tries in U200 by taking absolute values. The term
weights were computed as the L1 norm (sum of
the entries) in the rows of W = |U200|S200.
Our method is similar, except that we use 250
columns and form them in a slightly different way.
Observe that in the SVD, if ui is a column of U
and vTi is a row of V , then they can be replaced
by ?ui and ?vTi . This is true since if D is any
diagonal matrix with entries +1 and ?1, then
A = USV T = (UD)S(DV T ).
Therefore, we propose choosingD so that the sum
of the positive entries in each column of U is max-
imized. Then we form U? by setting each negative
entry of UD to zero and form W = U?250S250.
4.2 Latent Dirichlet Allocation
We use the term-sentence matrix to train a simple
generative topic model based on LDA (Blei et al,
2003). This model is described by the following
parameters: the number of terms m; the number
of topics k; a vector w representing a probability
distribution over topics; and an m? k matrix A in
58
which each column represents a probability distri-
bution over terms.
In this model, sentences are generated inde-
pendently. We use the ?pure-topic? LDA model
and assume, for simplicity, that the length of the
sentence is fixed a priori. First, a topic i ?
{1, . . . , k} is chosen from the probability distribu-
tion w. Then, terms are generated by sampling in-
dependently from the distribution specified by the
ith column of the matrix A.
We train this model using a recently-developed
spectral algorithm based on third-order tensor de-
compositions (Anandkumar et al, 2012a; Anand-
kumar et al, 2012b). This algorithm is guaranteed
to recover the parameters of the LDA model, pro-
vided that the columns of the matrixA are linearly
independent. For our experiments, we used a Mat-
lab implementation from Hsu (2012).
4.3 Interval Bounded Nonnegative Matrix
Factorization (IBNMF)
We also use a new method for dimensionality
reduction, a nonnegative matrix factorization al-
gorithm that handles uncertainty in a new way
(O?Leary and et al, In preparation).
Since the term-sentence matrix A is not known
with certainty, let?s suppose that we are given up-
per and lower bound matrices U and L so that
L ? A ? U . We compute a sparse nonnega-
tive low-rank approximation toA of the formXY ,
where X is nonnegative (i.e., X ? 0) and has
r columns and Y is nonnegative and has r rows.
This gives us an approximate nonnegative factor-
ization of A of rank at most r.
We choose to measure closeness of two ma-
trices using the Frobenius norm-squared, where
?Z?2F denotes the sum of the squares of the entries
of Z. Since A is sparse, we also want X and Y to
be sparse. We use the common trick of forcing this
by minimizing the sum of the entries of the matri-
ces, denoted by sum(X) + sum(Y ). This leads
us to determine X and Y by choosing a weighting
constant ? and solving
min
X,Y,Z
? ?XY ? Z?2F + sum(X) + sum(Y )
subject to the constraints
L ? Z ? U,
X ? 0,
Y ? 0.
We simplify this problem by noting that for any
W = XY , the entries of the optimal Z are
zij =
?
?
?
`ij , wij ? `ij ,
wij , `ij ? wij ? uij ,
uij , uij ? wij .
We solve our minimization problem by an alter-
nating algorithm, iterating by fixing X and deter-
mining the optimal Y and then fixing Y and de-
termining the optimal X . Either non-negativity is
imposed during the solution to the subproblems,
making each step more expensive, or negative en-
tries of the updated matrices are set to zero, ruin-
ing theoretical convergence properties but yielding
a more practical algorithm. Each iteration reduces
the distance to the term matrix, but setting nega-
tive values to zero increases it again.
For our summarization system we chose r = 50
and ? = 1000. We scaled the rows of the matrix
using global entropy weights and used L = 0.9A
and U = 1.1A.
4.4 Term Weighting and Dimension Choice
for Multi-Document Summarization
A natural term weighting can be obtained by com-
puting the row sums of the dimension-reduced
approximation to the term-sentence matrix. For
LSA, the resulting term weights are the sum of
the entries in the rows of W = U?250S250. For
the LDA method the initial matrix is the matrix of
counts. The model has three components similar
to that of the SVD in LSA, and the term weights
are computed analogously. For IBNMF, the term
weights are the sum of the entries in the rows of
the optimal XY .
Each of the three dimensionality reduction
methods require us to specify the dimension of the
?topic space.? We explored this question using the
DUC 2005-2007 and the TAC 2011 data. Tables 3,
4, 5, and 6 give the average ROUGE-2, ROUGE-
4, and bi-gram coverage scores, with confidence
intervals, for the dimension that gave the best cov-
erage. The optimal ranks were 250 for LSA, 5 for
LDA, and 50 for IBNMF. We emphasize the these
results are very strong despite the fact that no use
of the topic descriptions or the guided summary
aspects for the TAC 2010 and 2011 are used. Thus,
we treat these data as if the task were to generate
a generic summary, as is the case in the MultiLing
2013 task. 2
2We note that some of the coverage (C2), and ROUGE-
59
System R2 R4 C2
A 0.117 ( 0.106,0.129) 0.016 ( 0.011, 0.021) 26.333 (23.849,28.962)
C 0.118 ( 0.105,0.131) 0.016 ( 0.012, 0.022) 25.882 (23.086,28.710)
E 0.105 ( 0.092,0.120) 0.016 ( 0.010, 0.022) 23.625 (18.938,28.573)
F 0.100 ( 0.089,0.111) 0.014 ( 0.010, 0.019) 23.500 (19.319,27.806)
B 0.100 ( 0.086,0.115) 0.013 ( 0.008, 0.019) 23.118 (20.129,26.285)
D 0.100 ( 0.089,0.113) 0.012 ( 0.007, 0.017) 22.957 (20.387,25.742)
I 0.099 ( 0.085,0.116) 0.010 ( 0.007, 0.014) 21.806 (17.722,26.250)
H 0.088 ( 0.077,0.101) 0.011 ( 0.007, 0.016) 20.972 (17.389,24.750)
J 0.100 ( 0.090,0.111) 0.010 ( 0.007, 0.013) 20.472 (17.167,24.389)
G 0.097 ( 0.085,0.108) 0.012 ( 0.008, 0.017) 20.111 (16.694,24.000)
LSA250 0.085 ( 0.076,0.093) 0.008 ( 0.006, 0.009) 17.950 (17.072,18.838)
IBNMF50 0.079 ( 0.068,0.089) 0.007 ( 0.005, 0.009) 17.730 (16.843,18.614)
LDA5 0.077 ( 0.074,0.080) 0.008 ( 0.007, 0.009) 17.165 (16.320,18.024)
Table 3: DUC 2005
System R2 R4 C2
C 0.133 ( 0.116,0.152) 0.025 ( 0.018, 0.033) 30.517 (26.750,34.908)
D 0.124 ( 0.108,0.140) 0.017 ( 0.011, 0.023) 27.283 (23.567,31.050)
B 0.118 ( 0.105,0.134) 0.015 ( 0.012, 0.020) 25.933 (23.333,29.033)
G 0.113 ( 0.102,0.124) 0.016 ( 0.011, 0.022) 25.717 (23.342,28.017)
H 0.108 ( 0.098,0.117) 0.013 ( 0.010, 0.016) 24.767 (22.433,27.067)
F 0.109 ( 0.093,0.128) 0.016 ( 0.010, 0.023) 24.183 (20.650,28.292)
I 0.106 ( 0.096,0.116) 0.012 ( 0.008, 0.015) 24.133 (22.133,26.283)
J 0.107 ( 0.093,0.125) 0.015 ( 0.010, 0.022) 23.933 (20.908,27.233)
A 0.104 ( 0.093,0.116) 0.015 ( 0.010, 0.022) 23.283 (20.483,26.283)
E 0.104 ( 0.089,0.119) 0.014 ( 0.010, 0.020) 22.950 (19.833,26.450)
LDA5 0.103 ( 0.099,0.107) 0.012 ( 0.011, 0.013) 22.620 (21.772,23.450)
IBNMF50 0.095 ( 0.091,0.099) 0.010 ( 0.009, 0.011) 22.400 (21.615,23.177)
LSA250 0.099 ( 0.096,0.103) 0.012 ( 0.011, 0.013) 22.335 (21.497,23.200)
Table 4: DUC 2006
System R2 R4 C2
D 0.175 ( 0.157,0.196) 0.038 ( 0.029, 0.050) 39.481 (34.907,44.546)
C 0.151 ( 0.134,0.169) 0.035 ( 0.024, 0.049) 34.148 (29.870,38.926)
E 0.139 ( 0.125,0.154) 0.025 ( 0.020, 0.030) 30.907 (27.426,34.574)
J 0.139 ( 0.120,0.160) 0.028 ( 0.019, 0.038) 30.759 (25.593,36.389)
B 0.140 ( 0.116,0.163) 0.027 ( 0.019, 0.036) 30.537 (25.815,35.537)
I 0.136 ( 0.113,0.159) 0.022 ( 0.014, 0.030) 30.537 (25.806,35.241)
G 0.134 ( 0.118,0.150) 0.027 ( 0.018, 0.035) 30.259 (26.509,33.926)
F 0.134 ( 0.120,0.149) 0.024 ( 0.017, 0.033) 29.944 (26.481,33.870)
A 0.133 ( 0.117,0.149) 0.024 ( 0.016, 0.033) 29.315 (25.685,33.093)
H 0.130 ( 0.117,0.143) 0.020 ( 0.015, 0.027) 28.815 (25.537,32.185)
IBNMF50 0.140 ( 0.122,0.158) 0.023 ( 0.017, 0.031) 28.350 (27.092,29.567)
LSA250 0.125 ( 0.120,0.130) 0.022 ( 0.020, 0.024) 28.144 (26.893,29.344)
LDA5 0.124 ( 0.118,0.129) 0.021 ( 0.019, 0.023) 27.722 (26.556,28.893)
Table 5: DUC 2007
60
System R2 R4 C2
IBNMF50 0.132 ( 0.124,0.140) 0.033 ( 0.029, 0.038) 12.585 (11.806,13.402)
D 0.128 ( 0.110,0.146) 0.024 ( 0.017, 0.032) 12.212 (10.394,14.045)
LSA250 0.128 ( 0.120,0.136) 0.030 ( 0.025, 0.034) 12.210 (11.441,12.975)
A 0.119 ( 0.099,0.138) 0.024 ( 0.016, 0.033) 11.591 ( 9.758,13.455)
LDA5 0.120 ( 0.112,0.128) 0.028 ( 0.024, 0.033) 11.409 (10.678,12.159)
E 0.118 ( 0.099,0.138) 0.025 ( 0.016, 0.035) 11.288 ( 9.409,13.258)
H 0.115 ( 0.097,0.132) 0.020 ( 0.014, 0.027) 11.212 ( 9.439,12.955)
B 0.111 ( 0.099,0.125) 0.018 ( 0.013, 0.023) 10.591 ( 9.379,11.864)
F 0.109 ( 0.090,0.128) 0.017 ( 0.010, 0.025) 10.530 ( 8.515,12.500)
C 0.110 ( 0.095,0.126) 0.015 ( 0.010, 0.021) 10.379 ( 8.939,11.924)
G 0.110 ( 0.092,0.127) 0.016 ( 0.010, 0.023) 10.258 ( 8.682,11.894)
Table 6: TAC 2011
5 Sentence Selection
Our sentence selection algorithm, OCCAMS V,
is an extension of the one used in (Davis et al,
2012), which uses the (1 ? e?1/2)-approximation
scheme for the Budgeted Maximal Coverage
(BMC) problem and the Dynamic Programming
based FPTAS for the knapsack problem.
Algorithm OCCAMS V (T, D,W, c, L)
1. K1 = Greedy BMC(T,D,W, c, L)
2. K2 = Smax ? Greedy BMC(T ?,D?,W, c?, L?)),
where Smax = argmax{Si?D}
{?
tj?Si w(tj)
}
and T ?,D?,W, c?, L? represent quantities updated
by deleting sentence Smax from the collection.
3. K3 = KS(Greedy BMC(T,D,W, c, 5L), L);
4. K4 = KS(K ?4, L), where
K ?4 = Smax ? Greedy BMC(T
?,D?,W, C?, 5L?));
5. K = argmaxk=1,2,3,4
{?
T (Ki)w(ti)
}
where T (Ki) is the set of terms covered by Ki.
This algorithm selects minimally overlapping
sentences, thus reducing redundancy, while maxi-
mizing term coverage. The algorithm guarantees
a (1? e?1/2) approximation ratio for BMC.
We use the m terms T = {t1, . . . , tm} and
their corresponding weightsW = {w1, . . . , wm}.
We also use the n sentences D = {S1, . . . , Sn},
where each Si is the set of terms in the ith sen-
tence, so that Si ? T . We define c to be a vec-
tor whose components are the lengths of each sen-
tence. Our algorithm, OCCAMS V, determines
four candidate sets of summary sentences and then
2 scores reported in (Davis et al, 2012), where a rank 200
approximation and a large background corpus were used,
are higher than the ones reported here, where a small self-
background and a rank 250 approximation is used.
chooses the one with maximal coverage weight.
The first three candidate sets were used in the OC-
CAMS algorithm (Davis et al, 2012). The set
K1 is determined using the Greedy BMC heuris-
tic of Khuller et al (1999) to maximize the sum
of weights corresponding to terms in the sum-
mary sentences. The set K2 is determined the
same way, but the sentence that has the best sum
of weights is forced to be included. The third
candidate K3 is determined by applying a fully
polynomial-time approximation scheme (FPTAS)
dynamic programming algorithm, denoted by KS,
to the knapsack problem using sentences chosen
by the Greedy BMC heuristic, asking for a length
of 5L. The fourth candidate K4 is similar, but the
sentence with the best sum of weights is forced to
be included in the input to KS.
OCCAMS V guarantees an approximation ratio
of (1? e?1/2) for the result because the quality of
the solution chosen is no worse than the approxi-
mation ratio achieved by the OCCAMS algorithm.
6 Coverage Results for MultiLing 2013
We defined a term oracle coverage score in Section
3.1, an automatic summarization evaluation score
that computes the expected number of n-grams
that a summary will have in common with a hu-
man summary selected at random, assuming that
humans select terms independently. As reported
in (Davis et al, 2012), the 2-gram oracle cover-
age correlates as well with human evaluations of
English summaries as ROUGE-2 does for English
newswire summaries.3 It is natural then to ask to
what extent oracle coverage scores can predict a
summary?s quality for other languages.
3Here a term is defined as a stemmed 2-gram token.
61
For each of the 10 MultiLing 2013 languages
we can tokenize and generate bigrams (or charac-
ter n-grams for Chinese) for the human-generated
summaries and the machine-generated summaries.
Table 7 gives the average oracle term (bi-gram)
coverage score (C2) for the lowest-scoring human
and for each of the dimensionality reduction algo-
rithms described in Section 4.
In all but four of the languages (Romanian,
Hindi, Spanish, and Chinese), at least one of our
methods scored higher than the lowest scoring hu-
man. As with the DUC and TAC testing, the LDA
method of term-weighting was the weakest of the
three. In fact, in eight of the languages one or both
of OCCAMS V(LSA) and OCCAMS V(IBNMF)
(indicated in boldface in the table) scored signif-
icantly higher than OCCAMS V(LDA) (p-value
< 0.05 using a paired Wilcoxon test).
The human coverage scores for three of the lan-
guages (Romanian, Hindi, and Chinese) are sur-
prisingly high. Examining these data more closely
indicates that a large number of the summaries are
nearly identical. As an example, in one of the Ro-
manian document sets, there were 266 bi-grams
in the union of the three summaries, and the sum-
mary length was 250. Document sets similar to
this are the major cause of the anomalously high
scores for humans in these languages.
Language Human LSA IBNMF LDA
english 37 38 37 34
arabic 22 29 28 23
czech 22 34 35 33
french 28 38 38 34
greek 19 25 25 24
hebrew 16 19 22 19
hindi 64 20 20 18
spanish 47 40 44 36
romanian 118 31 28 29
chinese 68 23 24 18
Table 7: MultiLing 2013 Coverage Results
Human evaluation of the multi-lingual multi-
document summaries is currently under way.
These evaluations will be extremely informative
and will help measure to what extent ROUGE,
coverage, and character n-gram based methods
such as MeMoG (Giannakopoulos et al, 2010),
are effective in predicting performance.
7 Conclusions and Future Work
In this paper we presented three term weight-
ing approaches for single document multi-lingual
summarization. These approaches were tested on
the DUC 2002 data and on a submission to the
MultiLing 2013 single document pilot task for all
40 languages. Automatic evaluation of these sum-
maries with ROUGE-1 indicates that the strongest
of the approaches significantly outperformed the
lead baseline. The Wikipedia feature articles pose
a challenge due to their variable summary size and
genre. Further analysis of the results as well as hu-
man evaluation of the submitted summaries would
deepen our understanding.
A new nonnegative matrix factorization
method, interval bounded nonnegative matrix
factorization (IBNMF), was used. This method
allows specifying interval bounds, which give
an intuitive way to express uncertainty in the
term-sentence matrix.
For MDS we presented a variation of a LSA
term-weighting for OCCAMS V as well as novel
use of both of the IBNMF and an LDA model.
Based on automatic evaluation using cover-
age, it appears that the LSA method and the
IBNMF term-weighting give rise to competitive
summaries with term coverage scores approach-
ing that of humans for 6 of the 10 languages. The
automatic evaluation of these summaries, which
should soon be finished, will be illuminating.
Note: Contributions to this article by NIST, an agency of the
US government, are not subject to US copyright. Any men-
tion of commercial products is for information only, and does
not imply recommendation or endorsement by NIST.
References
Anima Anandkumar, Dean P. Foster, Daniel Hsu, Sham
Kakade, and Yi-Kai Liu. 2012a. A spectral al-
gorithm for latent dirichlet alocation. In Peter L.
Bartlett, Fernando C. N. Pereira, Christopher J. C.
Burges, Le?on Bottou, and Kilian Q. Weinberger, ed-
itors, NIPS, pages 926?934.
Anima Anandkumar, Rong Ge, Daniel Hsu, Sham M.
Kakade, and Matus Telgarsky. 2012b. Tensor de-
compositions for learning latent variable models.
CoRR, abs/1210.7559.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022.
62
John M. Conroy, Judith D. Schlesinger, and Dianne P.
O?Leary. 2006. Topic-focused multi-document
summarization using an approximate oracle score.
In Proceedings of the ACL?06/COLING?06 Confer-
ences, pages 152?159, Sydney, Australia, July.
John M Conroy, Judith D Schlesinger, and Dianne P
O?leary. 2009. Classy 2009: summarization and
metrics. Proceedings of the text analysis conference
(TAC).
John M Conroy, Judith D Schlesinger, Jeff Kubina, Pe-
ter A Rankel, and Dianne P O?Leary. 2011. Classy
2011 at tac: Guided and multi-lingual summaries
and evaluation metrics. Proceedings of the Text
Analysis Conference.
Thomas M. Cover and Joy A. Thomas. 1991. El-
ements of information theory. Wiley-Interscience,
New York, NY, USA.
Sashka T. Davis, John M. Conroy, and Judith D.
Schlesinger. 2012. Occams - an optimal combina-
torial covering algorithm for multi-document sum-
marization. In Jilles Vreeken, Charles Ling, Mo-
hammed Javeed Zaki, Arno Siebes, Jeffrey Xu Yu,
Bart Goethals, Geoffrey I. Webb, and Xindong Wu,
editors, ICDM Workshops, pages 454?463. IEEE
Computer Society.
Susan T. Dumais. 1994. Latent semantic indexing
(lsi): Trec-3 report. In TREC, pages 105?115.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):61?74.
George Giannakopoulos, George A. Vouros, and Van-
gelis Karkaletsis. 2010. Mudos-ng: Multi-
document summaries using n-gram graphs (tech re-
port). CoRR, abs/1012.2042.
Daniel Hsu. 2012. Estimating a simple topic model.
http://cseweb.ucsd.edu/?djhsu/
code/learn_topics.m.
Samir Khuller, Anna Moss, and Joseph Naor. 1999.
The Budgeted Maximum Coverage Problem. Inf.
Process. Lett., 70(1):39?45.
Chin-Yew Lin and Eduard Hovy. 2000. The automated
acquisition of topic signatures for text summariza-
tion. In Proceedings of the 18th conference on Com-
putational linguistics, pages 495?501, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Rada Mihalcea. 2005. Language independent extrac-
tive summarization. In Proceedings of ACL 2005,
Ann Arbor, MI, USA.
Dianne P. O?Leary and et al In preparation. An inter-
val bounded nonnegative matrix factorization. Tech-
nical report.
Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1999. The pagerank citation rank-
ing: Bringing order to the web. Technical Report
1999-66, Stanford InfoLab, November. Previous
number = SIDL-WP-1999-0120.
Peter A. Rankel, John M. Conroy, and Judith D.
Schlesinger. 2012. Better metrics to automatically
predict the quality of a text summary. Algorithms,
5(4):398?420.
Bob Rehder, Michael L. Littman, Susan T. Dumais, and
Thomas K. Landauer. 1997. Automatic 3-language
cross-language information retrieval with latent se-
mantic indexing. In TREC, pages 233?239.
Gerard Salton. 1991. The smart information retrieval
system after 30 years - panel. In Abraham Book-
stein, Yves Chiaramella, Gerard Salton, and Vijay V.
Raghavan, editors, SIGIR, pages 356?358. ACM.
Josef Steinberger and Karel Jezek. 2009. Update
summarization based on novel topic distribution. In
ACM Symposium on Document Engineering, pages
205?213.
Lin Zhao, Lide Wu, and Xuanjing Huang. 2009. Using
query expansion in graph-based approach for query-
focused multi-document summarization. Informa-
tion Processing & Management, 45(1):35 ? 41.
63
