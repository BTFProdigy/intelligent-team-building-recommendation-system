Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1113?1120,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Learning to Say It Well:
Reranking Realizations by Predicted Synthesis Quality
Crystal Nakatsu and Michael White
Department of Linguistics
The Ohio State University
Columbus, OH 43210 USA
fcnakatsu,mwhiteg@ling.ohio-state.edu
Abstract
This paper presents a method for adapting
a language generator to the strengths and
weaknesses of a synthetic voice, thereby
improving the naturalness of synthetic
speech in a spoken language dialogue sys-
tem. The method trains a discriminative
reranker to select paraphrases that are pre-
dicted to sound natural when synthesized.
The ranker is trained on realizer and syn-
thesizer features in supervised fashion, us-
ing human judgements of synthetic voice
quality on a sample of the paraphrases rep-
resentative of the generator?s capability.
Results from a cross-validation study indi-
cate that discriminative paraphrase rerank-
ing can achieve substantial improvements
in naturalness on average, ameliorating the
problem of highly variable synthesis qual-
ity typically encountered with today?s unit
selection synthesizers.
1 Introduction
Unit selection synthesis1?a technique which con-
catenates segments of natural speech selected from
a database?has been found to be capable of pro-
ducing high quality synthetic speech, especially
for utterances that are similar to the speech in the
database in terms of style, delivery, and coverage
(Black and Lenzo, 2001). In particular, in the lim-
ited domain of a spoken language dialogue sys-
tem, it is possible to achieve highly natural synthe-
sis with a purpose-built voice (Black and Lenzo,
2000). However, it can be difficult to develop
1See e.g. (Hunt and Black, 1996; Black and Taylor, 1997;
Beutnagel et al, 1999).
a synthetic voice for a dialogue system that pro-
duces natural speech completely reliably, and thus
in practice output quality can be quite variable.
Two important factors in this regard are the label-
ing process for the speech database and the direc-
tion of the dialogue system?s further development,
after the voice has been built: when labels are as-
signed fully automatically to the recorded speech,
label boundaries may be inaccurate, leading to un-
natural sounding joins in speech output; and when
further system development leads to the genera-
tion of utterances that are less like those in the
recording script, such utterances must be synthe-
sized using smaller units with more joins between
them, which can lead to a considerable dropoff in
quality.
As suggested by Bulyko and Ostendorf (2002),
one avenue for improving synthesis quality in a di-
alogue system is to have the system choose what
to say in part by taking into account what is likely
to sound natural when synthesized. The idea is
to take advantage of the generator?s periphrastic
ability:2 given a set of generated paraphrases that
suitably express the desired content in the dialogue
context, the system can select the specific para-
phrase to use as its response according to the pre-
dicted quality of the speech synthesized for that
paraphrase. In this way, if there are significant
differences in the predicted synthesis quality for
the various paraphrases?and if these predictions
are generally borne out?then, by selecting para-
phrases with high predicted synthesis quality, the
dialogue system (as a whole) can more reliably
produce natural sounding speech.
In this paper, we present an application of dis-
2See e.g. (Iordanskaja et al, 1991; Langkilde and Knight,
1998; Barzilay and McKeown, 2001; Pang et al, 2003) for
discussion of paraphrase in generation.
1113
criminative reranking to the task of adapting a lan-
guage generator to the strengths and weaknesses
of a particular synthetic voice. Our method in-
volves training a reranker to select paraphrases
that are predicted to sound natural when synthe-
sized, from the N-best realizations produced by
the generator. The ranker is trained in super-
vised fashion, using human judgements of syn-
thetic voice quality on a representative sample of
the paraphrases. In principle, the method can be
employed with any speech synthesizer. Addition-
ally, when features derived from the synthesizer?s
unit selection search can be made available, fur-
ther quality improvements become possible.
The paper is organized as follows. In Section 2,
we review previous work on integrating choice in
language generation and speech synthesis, and on
learning discriminative rerankers for generation.
In Section 3, we present our method. In Section 4,
we describe a cross-validation study whose results
indicate that discriminative paraphrase reranking
can achieve substantial improvements in natural-
ness on average. Finally, in Section 5, we con-
clude with a summary and a discussion of future
work.
2 Previous Work
Most previous work on integrating language gen-
eration and synthesis, e.g. (Davis and Hirschberg,
1988; Prevost and Steedman, 1994; Hitzeman et
al., 1998; Pan et al, 2002), has focused on how
to use the information present in the language
generation component in order to specify contex-
tually appropriate intonation for the speech syn-
thesizer to target. For example, syntactic struc-
ture, information structure and dialogue context
have all been argued to play a role in improving
prosody prediction, compared to unrestricted text-
to-speech synthesis. While this topic remains an
important area of research, our focus is instead
on a different opportunity that arises in a dialogue
system, namely, the possibility of choosing the ex-
act wording and prosody of a response according
to how natural it is likely to sound when synthe-
sized.
To our knowledge, Bulyko and Ostendorf
(2002) were the first to propose allowing the
choice of wording and prosody to be jointly deter-
mined by the language generator and speech syn-
thesizer. In their approach, a template-based gen-
erator passes a prosodically annotated word net-
work to the speech synthesizer, rather than a single
text string (or prosodically annotated text string).
To perform the unit selection search on this ex-
panded input efficiently, they employ weighted
finite-state transducers, where each step of net-
work expansion is then followed by minimiza-
tion. The weights are determined by concatena-
tion (join) costs, relative frequencies (negative log
probabilities) of the word sequences, and prosodic
prediction costs, for cases where the prosody is
not determined by the templates. In a perception
experiment, they demonstrated that by expand-
ing the space of candidate responses, their system
achieved higher quality speech output.
Following (Bulyko and Ostendorf, 2002), Stone
et al (2004) developed a method for jointly de-
termining wording, speech and gesture. In their
approach, a template-based generator produces
a word lattice with intonational phrase breaks.
A unit selection algorithm then searches for a
low-cost way of realizing a path through this
lattice that combines captured motion samples
with recorded speech samples to create coherent
phrases, blending segments of speech and mo-
tion together phrase-by-phrase into extended ut-
terances. Video demonstrations indicate that natu-
ral and highly expressive results can be achieved,
though no human evaluations are reported.
In an alternative approach, Pan and Weng
(2002) proposed integrating instance-based real-
ization and synthesis. In their framework, sen-
tence structure, wording, prosody and speech
waveforms from a domain-specific corpus are si-
multaneously reused. To do so, they add prosodic
and acoustic costs to the insertion, deletion and
replacement costs used for instance-based surface
realization. Their contribution focuses on how to
design an appropriate speech corpus to facilitate
an integrated approach to instance-based realiza-
tion and synthesis, and does not report evaluation
results.
A drawback of these approaches to integrating
choice in language generation and synthesis is that
they cannot be used with most existing speech syn-
thesizers, which do not accept (annotated) word
lattices as input. In contrast, the approach we in-
troduce here can be employed with any speech
synthesizer in principle. All that is required is
that the language generator be capable of produc-
ing N-best outputs; that is, the generator must be
able to construct a set of suitable paraphrases ex-
1114
pressing the desired content, from which the top
N realizations can be selected for reranking ac-
cording to their predicted synthesis quality. Once
the realizations have been reranked, the top scor-
ing realization can be sent to the synthesizer as
usual. Alternatively, when features derived from
the synthesizer?s unit selection search can be made
available?and if the time demands of the dia-
logue system permit?several of the top scoring
reranked realizations can be sent to the synthe-
sizer, and the resulting utterances can be rescored
with the extended feature set.
Our reranking approach has been inspired by
previous work on reranking in parsing and gen-
eration, especially (Collins, 2000) and (Walker et
al., 2002). As in Walker et al?s (2002) method for
training a sentence plan ranker, we use our gen-
erator to produce a representative sample of para-
phrases and then solicit human judgements of their
naturalness to use as data for training the ranker.
This method is attractive when there is no suit-
able corpus of naturally occurring dialogues avail-
able for training purposes, as is often the case for
systems that engage in human-computer dialogues
that differ substantially from human-human ones.
The primary difference between Walker et al?s
work and ours is that theirs examines the impact
on text quality of sentence planning decisions such
as aggregation, whereas ours focuses on the im-
pact of the lexical and syntactic choice at the sur-
face realization level on speech synthesis quality,
according to the strengths and weaknesses of a
particular synthetic voice.
3 Reranking Realizations by Predicted
Synthesis Quality
3.1 Generating Alternatives
Our experiments with integrating language gener-
ation and synthesis have been carried out in the
context of the COMIC3 multimodal dialogue sys-
tem (den Os and Boves, 2003). The COMIC sys-
tem adds a dialogue interface to a CAD-like ap-
plication used in sales situations to help clients re-
design their bathrooms. The input to the system
includes speech, handwriting, and pen gestures;
the output combines synthesized speech, an ani-
mated talking head, deictic gestures at on-screen
objects, and direct control of the underlying appli-
cation.
3COnversational Multimodal Interaction with Computers,
http://www.hcrc.ed.ac.uk/comic/.
Drawing on the materials used in (Foster and
White, 2005) to evaluate adaptive generation in
COMIC, we selected a sample of 104 sentences
from 38 different output turns across three dia-
logues. For each sentence in the set, a variant was
included that expressed the same content adapted
to a different user model or adapted to a differ-
ent dialogue history. For example, a description
of a certain design?s colour scheme for one user
might be phrased as As you can see, the tiles have
a blue and green colour scheme, whereas a vari-
ant expression of the same content for a different
user could be Although the tiles have a blue colour
scheme, the design does also feature green, if the
user disprefers blue.
In COMIC, the sentence planner uses XSLT to
generate disjunctive logical forms (LFs), which
specify a range of possible paraphrases in a nested
free-choice form (Foster and White, 2004). Such
disjunctive LFs can be efficiently realized us-
ing the OpenCCG realizer (White, 2004; White,
2006b; White, 2006a). Note that for the experi-
ments reported here, we manually augmented the
disjunctive LFs for the 104 sentences in our sam-
ple to make greater use of the periphrastic capa-
bilities of the COMIC grammar; it remains for fu-
ture work to augment the COMIC sentence plan-
ner produce these more richly disjunctive LFs au-
tomatically.
OpenCCG includes an extensible API for inte-
grating language modeling and realization. To se-
lect preferred word orders, from among all those
allowed by the grammar for the input LF, we used
a backoff trigram model trained on approximately
750 example target sentences, where certain words
were replaced with their semantic classes (e.g.
MANUFACTURER, COLOUR) for better general-
ization. For each of the 104 sentences in our sam-
ple, we performed 25-best realization from the dis-
junctive LF, and then randomly selected up to 12
different realizations to include in our experiments
based on a simulated coin flip for each realization,
starting with the top-scoring one. We used this
procedure to sample from a larger portion of the
N-best realizations, while keeping the sample size
manageable.
Figure 1 shows an example of 12 paraphrases
for a sentence chosen for inclusion in our sample.
Note that the realizations include words with pitch
accent annotations as well as boundary tones as
separate, punctuation-like words. Generally the
1115
 this
H
design
H
uses tiles from Villeroy and Boch
H
?s Funny Day
H
collection LL% .
 this
H
design
H
is based on the Funny Day
H
collec-
tion by Villeroy and Boch
H
LL% .
 this
H
design
H
is based on Funny Day
H
LL% , by
Villeroy and Boch
H
LL% .
 this
H
design
H
draws from the Funny Day
H
collec-
tion by Villeroy and Boch
H
LL% .
 this
H
one draws from Funny Day
H
LL% , by
Villeroy and Boch
H
LL% .
 here
L+H
LH% we have a design that is based on
the Funny Day
H
collection by Villeroy and Boch
H
LL% .
 this
H
design
H
draws from Villeroy and Boch
H
?s
Funny Day
H
series LL% .
 here is a design that draws from Funny Day
H
LL% ,
by Villeroy and Boch
H
LL% .
 this
H
one draws from Villeroy and Boch
H
?s
Funny Day
H
collection LL% .
 this
H
draws from the Funny Day
H
collection by
Villeroy and Boch
H
LL% .
 this
H
one draws from the Funny Day
H
collection by
Villeroy and Boch
H
LL% .
 here is a design that draws from Villeroy and Boch
H
?s Funny Day
H
collection LL% .
Figure 1: Example of sampled periphrastic alter-
natives for a sentence.
quality of the sampled paraphrases is very high,
only occasionally including dispreferred word or-
ders such as We here have a design in the family
style, where here is in medial position rather than
fronted.4
3.2 Synthesizing Utterances
For synthesis, OpenCCG?s output realizations are
converted to APML,5 a markup language which
allows pitch accents and boundary tones to be
specified, and then passed to the Festival speech
synthesis system (Taylor et al, 1998; Clark et al,
2004). Festival uses the prosodic markup in the
text analysis phase of synthesis in place of the
structures that it would otherwise have to predict
from the text. The synthesiser then uses the con-
text provided by the markup to enforce the selec-
4In other examples medial position is preferred, e.g. This
design here is in the family style.
5Affective Presentation Markup Language; see
http://www.cstr.ed.ac.uk/projects/
festival/apml.html.
tion of suitable units from the database.
A custom synthetic voice for the COMIC sys-
tem was developed, as follows. First, a domain-
specific recording script was prepared by select-
ing about 150 sentences from the larger set of tar-
get sentences used to train the system?s n-gram
model. The sentences were greedily selected with
the goals of ensuring that (i) all words (including
proper names) in the target sentences appeared at
least once in the record script, and (ii) all bigrams
at the level of semantic classes (e.g. MANUFAC-
TURER, COLOUR) were covered as well. For the
cross-validation study reported in the next section,
we also built a trigram model on the words in the
domain-specific recording script, without replac-
ing any words with semantic classes, so that we
could examine whether the more frequent occur-
rence of the specific words and phrases in this part
of the script is predictive of synthesis quality.
The domain-specific script was augmented with
a set of 600 newspaper sentences selected for di-
phone coverage. The newspaper sentences make
it possible for the voice to synthesize words out-
side of the domain-specific script, though not
necessarily with the same quality. Once these
scripts were in place, an amateur voice talent was
recorded reading the sentences in the scripts dur-
ing two recording sessions. Finally, after the
speech files were semi-automatically segmented
into individual sentences, the speech database was
constructed, using fully automatic labeling.
We have found that the utterances synthesized
with the COMIC voice vary considerably in their
naturalness, due to two main factors. First, the
system underwent further development after the
voice was built, leading to the addition of a va-
riety of new phrases to the system?s repertoire, as
well as many extra proper names (and their pro-
nunciations); since these names and phrases usu-
ally require going outside of the domain-specific
part of the speech database, they often (though not
always) exhibit a considerable dropoff in synthe-
sis quality.6 And second, the boundaries of the au-
tomatically assigned unit labels were not always
accurate, leading to problems with unnatural joins
and reduced intelligibility. To improve the reliabil-
ity of the COMIC voice, we could have recorded
more speech, or manually corrected label bound-
6Note that in the current version of the system, proper
names are always required parts of the output, and thus the
discriminative reranker cannot learn to simply choose para-
phrases that leave out problematic names.
1116
aries; the goal of this paper is to examine whether
the naturalness of a dialogue system?s output can
be improved in a less labor-intensive way.
3.3 Rating Synthesis Quality
To obtain data for training our realization reranker,
we solicited judgements of the naturalness of the
synthesized speech produced by Festival for the
utterances in our sample COMIC corpus. Two
judges (the first two authors) provided judgements
on a 1?7 point scale, with higher scores represent-
ing more natural synthesis. Ratings were gathered
using WebExp2,7 with the periphrastic alternatives
for each sentence presented as a group in a ran-
domized order. Note that for practical reasons,
the utterances were presented out of the dialogue
context, though both judges were familiar with the
kinds of dialogues that the COMIC system is ca-
pable of.
Though the numbers on the seven point scale
were not assigned labels, they were roughly taken
to be ?horrible,? ?poor,? ?fair,? ?ok,? ?good,? ?very
good? and ?perfect.? The average assigned rating
across all utterances was 4.05 (?ok?), with a stan-
dard deviation of 1.56. The correlation between
the two judges? ratings was 0.45, with one judge?s
ratings consistently higher than the other?s.
Some common problems noted by the judges
included slurred words, especially the sometimes
sounding like ther or even their; clipped words,
such as has shortened at times to the point of
sounding like is, or though clipped to unintelligi-
bility; unnatural phrasing or emphasis, e.g. occa-
sional pauses before a possessive ?s, or words such
as style sounding emphasized when they should
be deaccented; unnatural rate changes; ?choppy?
speech from poor joins; and some unintelligible
proper names.
3.4 Ranking
While Collins (2000) and Walker et al (2002)
develop their rankers using the RankBoost algo-
rithm (Freund et al, 1998), we have instead cho-
sen to use Joachims? (2002) method of formu-
lating ranking tasks as Support Vector Machine
(SVM) constraint optimization problems.8 This
choice has been motivated primarily by conve-
nience, as Joachims? SVMlight package is easy to
7http://www.hcrc.ed.ac.uk/web exp/
8See (Barzilay and Lapata, 2005) for another application
of SVM ranking in generation, namely to the task of ranking
alternative text orderings for local coherence.
use; we leave it for future work to compare the
performance of RankBoost and SVMlight on our
ranking task.
The ranker takes as input a set of paraphrases
that express the desired content of each sentence,
optionally together with synthesized utterances
for each paraphrase. The output is a ranking of
the paraphrases according to the predicted natu-
ralness of their corresponding synthesized utter-
ances. Ranking is more appropriate than classifi-
cation for our purposes, as naturalnesss is a graded
assessment rather than a categorical one.
To encode the ranking task as an SVM con-
straint optimization problem, each paraphrase j
of a sentence i is represented by a feature vector
(s
ij
) = hf
1
(s
ij
); : : : ; f
m
(s
ij
)i, where m is the
number of features. In the training data, the fea-
ture vectors are paired with the average value of
their corresponding human judgements of natural-
ness. From this data, ordered pairs of paraphrases
(s
ij
; s
ik
) are derived, where s
ij
has a higher nat-
uralness rating than s
ik
. The constraint optimiza-
tion problem is then to derive a parameter vector
~w that yields a ranking score function ~w  (s
ij
)
which minimizes the number of pairwise rank-
ing violations. Ideally, for every ordered pair
(s
ij
; s
ik
), we would have ~w (s
ij
) > ~w (s
ik
);
in practice, it is often impossible or intractable to
find such a parameter vector, and thus slack vari-
ables are introduced that allow for training errors.
A parameter to the algorithm controls the trade-off
between ranking margin and training error.
In testing, the ranker?s accuracy can be deter-
mined by comparing the ranking scores for ev-
ery ordered pair (s
ij
; s
ik
) in the test data, and
determining whether the actual preferences are
borne out by the predicted preference, i.e. whether
~w  (s
ij
) > ~w  (s
ik
) as desired. Note that
the ranking scores, unlike the original ratings, do
not have any meaning in the absolute sense; their
import is only to order alternative paraphrases by
their predicted naturalness.
In our ranking experiments, we have used
SVMlight with all parameters set to their default
values.
3.5 Features
Table 1 shows the feature sets we have investigated
for reranking, distinguished by the availability of
the features and the need for discriminative train-
ing. The first row shows the feature sets that are
1117
Table 1: Feature sets for reranking.
Discriminative
Availability no yes
Realizer NGRAMS WORDS
Synthesizer COSTS ALL
available to the realizer. There are two n-gram
models that can be used to directly rank alterna-
tive realizations: NGRAM-1, the language model
used in COMIC, and NGRAM-2, the language
model derived from the domain-specific recording
script; for feature values, the negative logarithms
are used. There are also two WORDS feature
sets (shown in the second column): WORDS-BI,
which includes NGRAMS plus a feature for every
possible unigram and bigram, where the value of
the feature is the count of the unigram or bigram
in a given realization; and WORDS-TRI, which
includes all the features in WORDS-BI, plus a
feature for every possible trigram. The second
row shows the feature sets that require informa-
tion from the synthesizer. The COSTS feature set
includes NGRAMS plus the total join and target
costs from the unit selection search. Note that a
weighted sum of these costs could be used to di-
rectly rerank realizations, in much the same way
as relative frequencies and concatenation costs are
used in (Bulyko and Ostendorf, 2002); in our
experiments, we let SVMlight determine how to
weight these costs. Finally, there are two ALL fea-
ture sets: ALL-BI includes NGRAMS, WORDS-
BI and COSTS, plus features for every possi-
ble phone and diphone, and features for every
specific unit in the database; ALL-TRI includes
NGRAMS, WORDS-TRI, COSTS, and a feature
for every phone, diphone and triphone, as well as
specific units in the database. As with WORDS,
the value of a feature is the count of that feature in
a given synthesized utterance.
4 Cross-Validation Study
To train and test our ranker on our feature sets,
we partitioned the corpus into 10 folds and per-
formed 10-fold cross-validation. For each fold,
90% of the examples were used for training the
ranker and the remaining unseen 10% were used
for testing. The folds were created by randomly
choosing from among the sentence groups, result-
ing in all of the paraphrases for a given sentence
occurring in the same fold, and each occurring ex-
Table 2: Comparison of results for differing fea-
ture sets, topline and baseline.
Features Mean Score SD Accuracy (%)
BEST 5.38 1.11 100.0
WORDS-TRI 4.95 1.24 77.3
ALL-BI 4.95 1.24 77.9
ALL-TRI 4.90 1.25 78.0
WORDS-BI 4.86 1.28 76.8
COSTS 4.69 1.27 68.2
NGRAM-2 4.34 1.38 56.2
NGRAM-1 4.30 1.29 53.3
RANDOM 4.11 1.22 50.0
actly once in the testing set as a whole.
We evaluated the performance of our ranker
by determining the average score of the best
ranked paraphrase for each sentence, under each
of the following feature combinations: NGRAM-
1, NGRAM-2, COSTS, WORDS-BI, WORDS-
TRI, ALL-BI, and ALL-TRI. Note that since we
used the human ratings to calculate the score of
the highest ranked utterance, the score of the high-
est ranked utterance cannot be higher than that
of the highest human-rated utterance. Therefore,
we effectively set the human ratings as the topline
(BEST). For the baseline, we randomly chose an
utterance from among the alternatives, and used
its associated score. In 15 tests generating the ran-
dom scores, our average scores ranged from 3.88?
4.18. We report the median score of 4.11 as the
average for the baseline, along with the mean of
the topline and each of the feature subsets, in Ta-
ble 2.
We also report the ordering accuracy of each
feature set used by the ranker in Table 2. As men-
tioned in Section 3.4, the ordering accuracy of the
ranker using a given feature set is determined by
c=N , where c is the number of correctly ordered
pairs (of each paraphrase, not just the top ranked
one) produced by the ranker, and N is the total
number of human-ranked ordered pairs.
As Table 2 indicates, the mean of BEST is 5.38,
whereas our ranker using WORDS-TRI features
achieves a mean score of 4.95. This is a difference
of 0.42 on a seven point scale, or only a 6% dif-
ference. The ordering accuracy of WORDS-TRI
is 77.3%.
We also measured the improvement of our
ranker with each feature set over the random base-
line as a percentage of the maximum possible
gain (which would be to reproduce the human
topline). The results appear in Figure 2. As the
1118
010203040506070
NGR
AM-1
NGR
AM-2
COS
TS
WOR
DS-B
IA
LL-T
RI
ALL-
BI
WOR
DS-T
RI
Figure 2: Improvement as a percentage of the
maximum possible gain over the random baseline.
figure indicates, the maximum possible gain our
ranker achieves over the baseline is 66% (using the
WORDS-TRI or ALL-BI feature set) . By com-
parison, NGRAM-1 and NGRAM-2 achieve less
than 20% of the possible gain.
To verify our main hypothesis that our ranker
would significantly outperform the baselines,
we computed paired one-tailed t-tests between
WORDS-TRI and RANDOM (t = 2:4, p <
8:9x10
 13), and WORDS-TRI and NGRAM-1
(t = 1:4, p < 4:5x10 8). Both differences were
highly significant. We also performed seven post-
hoc comparisons using two-tailed t-tests, as we
did not have an a priori expectation as to which
feature set would work better. Using the Bonfer-
roni adjustment for multiple comparisons, the p-
value required to achieve an overall level of signif-
icance of 0.05 is 0.007. In the first post-hoc test,
we found a significant difference between BEST
and WORDS-TRI (t = 8:0,p < 1:86x10 12),
indicating that there is room for improvement of
our ranker. However, in considering the top scor-
ing feature sets, we did not find a significant dif-
ference between WORDS-TRI and WORDS-BI
(t = 2:3, p < 0:022), from which we infer that the
difference among all of WORDS-TRI, ALL-BI,
ALL-TRI and WORDS-BI is not significant also.
This suggests that the synthesizer features have
no substantial impact on our ranker, as we would
expect ALL-TRI to be significantly higher than
WORDS-TRI if so. However, since COSTS does
significantly improve upon NGRAM2 (t = 3:5,
p < 0:001), there is some value to the use of syn-
thesizer features in the absence of WORDS. We
also looked at the comparison for the WORDS
models and COSTS. While WORDS-BI did not
perform significantly better than COSTS ( t =
2:3, p < 0:025), the added trigrams in WORDS-
TRI did improve ranker performance significantly
over COSTS (t = 3:7, p < 3:29x10 4). Since
COSTS ranks realizations in the much the same
way as (Bulyko and Ostendorf, 2002), the fact that
WORDS-TRI outperforms COSTS indicates that
our discriminative reranking method can signifi-
cantly improve upon their non-discriminative ap-
proach.
5 Conclusions
In this paper, we have presented a method for
adapting a language generator to the strengths
and weaknesses of a particular synthetic voice by
training a discriminative reranker to select para-
phrases that are predicted to sound natural when
synthesized. In contrast to previous work on
this topic, our method can be employed with any
speech synthesizer in principle, so long as fea-
tures derived from the synthesizer?s unit selec-
tion search can be made available. In a case
study with the COMIC dialogue system, we have
demonstrated substantial improvements in the nat-
uralness of the resulting synthetic speech, achiev-
ing two-thirds of the maximum possible gain, and
raising the average rating from ?ok? to ?good.? We
have also shown that in this study, our discrimina-
tive method significantly outperforms an approach
that performs selection based solely on corpus fre-
quencies together with target and join costs.
In future work, we intend to verify the results
of our cross-validation study in a perception ex-
periment with na??ve subjects. We also plan to in-
vestigate whether additional features derived from
the synthesizer can better detect unnatural pauses
or changes in speech rate, as well as F0 contours
that fail to exhibit the targeting accenting pattern.
Finally, we plan to examine whether gains in qual-
ity can be achieved with an off-the-shelf, general
purpose voice that are similar to those we have ob-
served using COMIC?s limited domain voice.
Acknowledgements
We thank Mary Ellen Foster, Eric Fosler-Lussier
and the anonymous reviewers for helpful com-
ments and discussion.
References
Regina Barzilay and Mirella Lapata. 2005. Modeling
local coherence: An entity-based approach. In Pro-
1119
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, Ann Arbor.
Regina Barzilay and Kathleen McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Proc.
ACL/EACL.
M. Beutnagel, A. Conkie, J. Schroeter, Y. Stylianou,
and A. Syrdal. 1999. The AT&T Next-Gen TTS
system. In Joint Meeting of ASA, EAA, and DAGA.
Alan Black and Kevin Lenzo. 2000. Limited domain
synthesis. In Proceedings of ICSLP2000, Beijing,
China.
Alan Black and Kevin Lenzo. 2001. Optimal data
selection for unit selection synthesis. In 4th ISCA
Speech Synthesis Workshop, Pitlochry, Scotland.
Alan Black and Paul Taylor. 1997. Automatically clus-
tering similar units for unit selection in speech syn-
thesis. In Eurospeech ?97.
Ivan Bulyko and Mari Ostendorf. 2002. Efficient in-
tegrated response generation from multiple targets
using weighted finite state transducers. Computer
Speech and Language, 16:533?550.
Robert A.J. Clark, Korin Richmond, and Simon King.
2004. Festival 2 ? build your own general pur-
pose unit selection speech synthesiser. In 5th ISCA
Speech Synthesis Workshop, pages 173?178, Pitts-
burgh, PA.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proc. ICML.
James Raymond Davis and Julia Hirschberg. 1988.
Assigning intonational features in synthesized spo-
ken directions. In Proc. ACL.
Els den Os and Lou Boves. 2003. Towards ambient
intelligence: Multimodal computers that understand
our intentions. In Proc. eChallenges-03.
Mary Ellen Foster and Michael White. 2004. Tech-
niques for Text Planning with XSLT. In Proc. 4th
NLPXML Workshop.
Mary Ellen Foster and Michael White. 2005. As-
sessing the impact of adaptive generation in the
COMIC multimodal dialogue system. In Proc.
IJCAI-05 Workshop on Knowledge and Representa-
tion in Practical Dialogue Systems.
Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. 1998.
An efficient boosting algorithm for combining pref-
erences. In Machine Learning: Proc. of the Fif-
teenth International Conference.
Janet Hitzeman, Alan W. Black, Chris Mellish, Jon
Oberlander, and Paul Taylor. 1998. On the use of
automatically generated discourse-level information
in a concept-to-speech synthesis system. In Proc.
ICSLP-98.
A. Hunt and A. Black. 1996. Unit selection in a
concatenative speech synthesis system using a large
speech database. In Proc. ICASSP-96, Atlanta,
Georgia.
Lidija Iordanskaja, Richard Kittredge, and Alain
Polgu?ere. 1991. Lexical selection and paraphrase
in a meaning-text generation model. In Ce?cile L.
Paris, William R. Swartout, and William C. Mann,
editors, Natural Language Generation in Artificial
Intelligence and Computational Linguistics, pages
293?312. Kluwer.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proc. KDD.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
Proc. COLING-ACL.
Shimei Pan and Wubin Weng. 2002. Designing a
speech corpus for instance-based spoken language
generation. In Proc. of the International Natural
Language Generation Conference (INLG-02).
Shimei Pan, Kathleen McKeown, and Julia Hirschberg.
2002. Exploring features from natural language
generation for prosody modeling. Computer Speech
and Language, 16:457?490.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations:
Extracting paraphrases and generating new sen-
tences. In Proc. HLT/NAACL.
Scott Prevost and Mark Steedman. 1994. Specify-
ing intonation from context for speech synthesis.
Speech Communication, 15:139?153.
Matthew Stone, Doug DeCarlo, Insuk Oh, Christian
Rodriguez, Adrian Stere, Alyssa Lees, and Chris
Bregler. 2004. Speaking with hands: Creating ani-
mated conversational characters from recordings of
human performance. ACM Transactions on Graph-
ics (SIGGRAPH), 23(3).
P. Taylor, A. Black, and R. Caley. 1998. The architec-
ture of the the Festival speech synthesis system. In
Third International Workshop on Speech Synthesis,
Sydney, Australia.
Marilyn A. Walker, Owen C. Rambow, and Monica Ro-
gati. 2002. Training a sentence planner for spo-
ken dialogue using boosting. Computer Speech and
Language, 16:409?433.
Michael White. 2004. Reining in CCG Chart Realiza-
tion. In Proc. INLG-04.
Michael White. 2006a. CCG chart realization from
disjunctive logical forms. In Proc. INLG-06. To ap-
pear.
Michael White. 2006b. Efficient Realization of Coor-
dinate Structures in Combinatory Categorial Gram-
mar. Research on Language & Computation, on-
line first, March.
1120
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 76?79,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Learning Contrastive Connectives in Sentence Realization Ranking
Crystal Nakatsu
Department of Linguistics
The Ohio State Univeristy
Columbus, OH, USA
cnakatsu@ling.osu.edu
Abstract
We look at the average frequency of con-
trastive connectives in the SPaRKy Restaurant
Corpus with respect to realization ratings by
human judges. We implement a discriminative
n-gram ranker to model these ratings and ana-
lyze the resulting n-gram weights to determine
if our ranker learns this distribution. Surpris-
ingly, our ranker learns to avoid contrastive
connectives. We look at possible explanations
for this distribution, and recommend improve-
ments to both the generator and ranker of the
sentence plans/realizations.
1 Introduction
Contrastive discourse connectives are words or
phrases such as however and on the other hand.
They indicate a contrastive discourse relation be-
tween two units of discourse. While corpus-based
studies on discourse connectives usually look at nat-
urally occurring human-authored examples, in this
study, we investigate the set of connectives used
in the automatically generated SPaRKy Restaurant
Corpus1. Specifically, we consider the relationship
between connective usage and judges ratings, and
investigate whether our n-gram ranker learns the
preferred connective usage. Based on these findings
and previous work on contrastive connectives, we
present suggestions for modifying both the genera-
tor and the ranker in order to improve the generation
of realizations containing contrastive connectives.
1We thank Marilyn Walker and her research team for mak-
ing all of the MATCH system data available for our study, espe-
cially including the SPaRKy Restaurant Corpus.
2 Corpus Study
2.1 SPaRKy Restaurant Corpus
The SPaRKy Restaurant Corpus was generated by
the MATCH Spoken Language Generator (Walker et
al., 2007) which consists of a dialog manager, SPUR
text planner (Walker et al, 2004), SPaRKy sentence
planner (Walker et al, 2007), and RealPro surface
realizer (Lavoie and Rambow, 1997).
The corpus contains realizations for 3 dialogue
strategies:
? RECOMMEND (REC): recommend an entity from
a set of entities
? COMPARE-2 (C2): compare 2 entities
? COMPARE-3 (C3): compare 3 or more entities
Each strategy contains 30 content plans from
which either 16 or 20 sentence plans were generated
by the SPaRKy sentence plan generator. 4 sentence
plans were discarded due to duplication upon real-
ization, totaling 1756 realizations in the corpus.2
A content plan consists of several assertions and
the relations which hold between them. Con-
tent plans from the RECOMMEND strategy ex-
clusively employ the Rhetorical Structure Theory
(RST) (Mann and Thompson, 1987) relation JUS-
TIFY while those from COMPARE-2 use CONTRAST
and ELABORATION. COMPARE-3 content plans
consists mostly of CONTRAST and ELABORATION
relations, though some use only JUSTIFY. In addi-
2The total number of realizations reported here is inconsis-
tent with the information reported in (Walker et al, 2007). In
corresponding with the authors of that paper, it is unclear why
this is the case; however, the difference in reported amounts is
quite small, and so should not affect the outcome of this study.
76
Strategy Alt # Rating Rank Realization
3 3 7 Sonia Rose has very good decor but Bienvenue has decent decor.
7 1 16 Sonia Rose has very good decor. On the other hand, Bienvenue has decent decor.
8 4.5 13 Bienvenue has decent decor. Sonia Rose, on the other hand, has very good decor.
C2 10 4.5 5 Bienvenue has decent decor but Sonia Rose has very good decor.
11 1 12 Sonia Rose has very good decor. However, Bienvenue has decent decor.
13 5 14 Bienvenue has decent decor. However, Sonia Rose has very good decor.
14 5 3 Sonia Rose has very good decor while Bienvenue has decent decor.
15 4 4 Bienvenue has decent decor while Sonia Rose has very good decor.
17 1 15 Bienvenue?s price is 35 dollars. Sonia Rose?s price, however, is 51 dollars. Bienvenue has decent decor.
However, Sonia Rose has very good decor.
Figure 1: Some alternative [Alt] realizations of SPaRKy sentence plans from a COMPARE-2 [C2] plan, with averaged
human ratings [Rating] (5 = highest rating) and ranks assigned by the n-gram ranker [Rank] (1 = top ranked).
tion, the SPaRKy sentence plan generator adds the
INFER relation to assertions whose relations were
not specified by the content planner.
During the sentence planning phase, SPaRKy or-
ders the clauses and combines them using randomly
selected clause-combining operations. During this
process, a clause-combining operation may insert 1
of 7 connectives according to the RST relation that
holds between two discourse units (i.e. inserting
since or because for a JUSTIFY relation; and, how-
ever, on the other hand, while, or but for a CON-
TRAST relation; or and for an INFER relation).
After each sentence plan is generated, it is real-
ized by the RealPro surface realizer and the result-
ing realization is rated by two judges on a scale of
1-5, where 5 is highly preferred. These ratings are
then averaged, producing a range of 9 possible rat-
ings from {1, 1.5, ..., 5}.
2.2 Ratings/Connectives Correlation
From the ratings of the examples in Figure 1, we
can see that some of the SPaRKy sentence plan re-
alizations seem more natural than others. Upon fur-
ther analysis, we noticed that utterances containing
many contrastive connectives seemed less preferred
than those with fewer or no contrastive connectives.
To quantify this observation, we calculated the av-
erage number of connectives (aveci) used per real-
ization with rating i, using aveci = Totalci/Nri ,
where Totalci is the total number of connectives in
realizations with rating i, and Nri is the number of
realizations with rating i.
We use Pearson?s r to calculate each correlation
(in each case, df = 7). For both COMPARE strategies
(represented in Figure 2(a) and 2(b)), we find a sig-
nificant negative correlation for the average number
of connectives used in realizations with a given rat-
ing (C2: r = ?0.97, p < 0.01; and C3: r = ?0.93,
p < 0.01). These correlations indicate that judges?
ratings decreased as the average frequency of the
connectives increased.
Further analysis of the individual correlations
used in the comparative strategies show that there is
a significant negative correlation for however (C2:
r = ?0.91, p < 0.01; and C3: r = ?0.86,
p < 0.01) and on the other hand (C2: r = ?0.89,
p < 0.01; and C3: r = ?0.84, p < 0.01) in both
COMPARE strategies. In addition, in COMPARE-3,
the frequencies of while and but are also signifi-
cantly and strongly negatively correlated with the
judges? ratings (r = ?0.86, p < 0.01 and r =
?0.90, p < 0.01, respectively), though there is no
such correlation between the use of these connec-
tives and their ratings in COMPARE-2.
Added together, all the contrastive connectives
show strong, significant negative correlations be-
tween their average frequencies and judges? ratings
for both comparative strategies (C2: r = ?0.93,
p < 0.01; C3:r = ?0.88, p < 0.01).
Interestingly, unlike in the COMPARE strategies,
there is a positive correlation (r = 0.73, p > 0.05)
between the judges? ratings and the average fre-
quency of all connectives used in the RECOMMEND
strategy (see Figure 2(c)). Since this strategy only
uses and, since, and because and does not utilize any
contrastive connectives, this gives further evidence
that only contrastive connectives are dispreferred.
2.3 N-gram Ranker and Features
To acertain whether these contrastive connectives
are being learned by the ranker, we re-implemented
the n-gram ranker using SVM-light (Joachims,
77
0.00.51.01.52.02.5 1
1.5
22.
53
3.5
44.
55
Rating
Average Number of Connectives per 
utterance
(a) Compare 2
0.01.02.03.04.05.06.07.0 1
1.5
22.
53
3.5
44.
55
Rating
Average Number of Connectives per 
utterance
(b) Compare 3
00.20.40.60.811.21.4 1
1.5
22.
53
3.5
44.
55
Rating
Average Number of Connectives per 
utterance
(c) Recommend
00.20.40.60.811.21.4 1
1.52
2.53
3.54
4.55
Rating
Average Number of Connectives per 
utterance
and howeve
r
hand while but since because contras
tive
total
Figure 2: Correlation Graphs: The thick solid line indicate the correlation of all the connectives summed together,
while the thick dashed line indicates the correlation of the 4 contrastive connectives summed together.
Strategy however o.t.o.h while but all contrastives
C2 25.0% 25.0% 0.9% 2.7% 53.6%
C3 9.9% 10.9% 0.0% 3.1% 24.0%
Table 1: The proportion of the 20% most negatively
weighted features for all contrastive connectives.
2002). As in Walker et. al (2007), we first pre-
pared the SPaRKy Restaurant Corpus by replacing
named entity tokens (e.g numbers, restaurant names,
etc.) with their corresponding type (e.g. NUM for
61), and added BEGIN and END tokens to mark the
boundaries of each realization. We then trained our
ranker to learn which unigrams, bigrams, and tri-
grams are associated with the ratings given to the
realizations in the training set.
Although we implemented our ranker in order to
carry out an error analysis on the individual fea-
tures (i.e. n-grams) used by the ranker, we also
found that our n-gram ranker performed compara-
bly (REC: 3.5; C2: 4.1; C3: 3.8)3 to the full-featured
SPaRKy ranker (REC: 3.6; C2: 4.0; C3: 3.6) out of
a possible best (human-performance) score of (REC:
4.2; C2: 4.5; C3: 4.2).
Using a perl script4, we extracted feature weights
learned by the ranker from the models built dur-
ing the training phase. After averaging the feature
weights across 10 training partitions, we examined
the top 20% (C2:112/563 features; C3: 192/960
features) most negatively weighted features in each
strategy to see whether our ranker was learning to
avoid contrastive connectives. Table 1 shows that
features containing contrastive connectives make up
3These scores were calculated using using the TopRank
evaluation metric (Walker et al, 2007).
4written by Thorsten Joachims
53.6% of the 20%most negatively weighted features
in COMPARE-2 and 24.0% of the 20% of the most
negatively weighted features used in COMPARE-3.
Interestingly, COMPARE-2 features that contained
either however or on the other hand (o.t.o.h) make
up the bulk of the contrastive connectives found in
the negatively weighted features, mirroring the re-
sults of the correlations for COMPARE-2. This indi-
cates that the discriminative n-gram ranker learns to
avoid using contrastive connectives.
3 Contrastive Connectives Usage
3.1 Usage Restrictions
Previous work on contrastive connectives have
found that these connectives often have different re-
strictions on their location in the discourse struc-
ture, with respect to maintaining discourse coher-
ance (Quirk et al, 1972; Grote et al, 1995).
Quirk et. al. (1972) classifies however and on
the other hand as subordinating conjuncts, a class
of connectives that do not allow their clauses to be
reordered without changing the perlocutionary force
of the sentence (e.g. contrast C2: Alts # 11 & 13 in
Figure 1). In addition, on the other hand prompts
readers to regard the 2nd clause as more important
(Grote et al, 1995). Given that both however and
on the other hand contain the same restrictions on
clause ordering, it seems reasonable that they would
pattern the same with respect to assigning clausal
prominence. This predicts that if the human judges
rated the SPaRKy realizations based on the expecta-
tion of a particular perlocutionary act (e.g., that the
comparison highlights the restaurant with the best
decor), they would prefer realizations where how-
ever or on the other hand were attached to the more
78
desirable of the contrasted qualities. When we ex-
amine the SPaRKy realizations and ratings, this in-
deed seems to be the case ? when the better property
is ordered last, the realization was rated very highly
(e.g. Alt 8 & 13 in Figure 1), but when the lesser
property was ordered last, the realization was rated
poorly (e.g. Alt 7 & 11 in Figure 1).
In contrast, while and but are not subordinating
conjuncts and so are not subject to the clause or-
dering restriction. Thus, realizations with their con-
trasted clauses in either order should be rated simi-
larly, and indeed, this is what we find in the corpus
(e.g. Alts 3&10, and 14&15 in Figure 1).
3.2 Other Factors
In addition to clause order, another factor that may
contribute to the awkwardness of however and on
the other hand in some usages is that both of these
connectives seem to be rather ?grand? for these sim-
ple contrasts. Intuitively, these connectives seem
to indicate a larger contrast than while and but, so
when they are used to indicate small contrasts (e.g.
contrasting only one quality), or contrasts close to-
gether on the scale (e.g. good vs. decent) instead
of diametric opposites, they sound awkward. In ad-
dition, however and on the other hand may also be
seeking ?heavy? arguments that contain more syl-
lables, words, or complex syntax. Lastly, human-
authored comparisons, such as in this example from
CNET.com:
...[it] has two convenient USB ports at the bottom of the
front panel. Its beige predecessor, on the other hand,
supplied these only on the back of the box.
seem to indicate that when our expectations of ar-
gument order are violated, the 2nd clause is often
qualified by words such as ?just? or ?only?, as if to
acknowledge the flaunted preference.
4 Discussion and Future Work
Due to the poverty of highly rated instances of con-
trastive connective usage (particularly for however
and on the other hand), our ranker learns to avoid
these connectives in most situations. However, the
ratings suggest that people do not dislike these con-
trastives unilaterally, but rather prefer them in spe-
cific usage patterns only. One way to combat this
problem is to modify the sentence planner to take
into account these semantic preferences for argu-
ment ordering when selecting a contrastive connec-
tive. This should produce a wider variety of can-
didates that observe this ordering preference, and
thus provide the ranker with more highly rated can-
diates that use contrastive connectives. This is not
to say that only candidates observing this preference
should be generated, but merely that a wider variety
of candiates should be generated so that the ranker
has more opportunities to learn the restrictions sur-
rounding the use of contrastive connectives.
As for the ranker, we can also identify features
that are sensitive to these linguistic properties. Cur-
rently, n-gram features don?t capture the semantic
nuances such as argument order or the scalar dis-
tance between property values, so identifying fea-
tures that capture this type of information should
improve the ranker. Together, these improvements
to both the quality of the generated candidate space
and the ranking model should improve the accuracy
of the top-rated/selected candidate.
References
B. Grote, N. Lenke, and M. Stede. 1995. Ma(r)king con-
cessions in english and german. In Proceedings of the
Fifth European Workshop on Natural Language Gen-
eration., May.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proc. KDD.
B. Lavoie and O. Rambow. 1997. A fast and portable
realizer for text generation systems. In Proceedings
of the 5th. Conference on Applied Natural Language
Processing, pages 265?268, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
William C. Mann and Sandra A. Thompson. 1987.
Rhetorical structure theory: A theory of text organiza-
tion. Technical Report ISI/RS-87-190, University of
Southern California Information Sciences Instuture.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1972.
A Comprehensive Grammar of the English Language.
Longman.
M. A. Walker, S. J. Whittaker, A. Stent, P. Maloor, J. D.
Moore, M. Johnston, and G Vasireddy. 2004. Genera-
tion and evaluation of user tailored responses in multi-
modal dialogue. Cognitive Science, 28(5):811?840.
M. Walker, A. Stent, F. Mairesse, and Rashmi Prasad.
2007. Individual and domain adaptation in sentence
planning for dialogue. Journal of Artificial Intelli-
gence Research (JAIR), 30:413?456.
79
Automatic Semantic Grouping in a Spoken Language User Interface 
Toolkit 
 
Hassan Alam, Hua Cheng, Rachmat Hartono, Aman Kumar, Paul Llido, Crystal Nakatsu, Huy 
Nguyen, Fuad Rahman, Yuliya Tarnikova, Timotius Tjahjadi and Che Wilcox 
 
BCL Technologies Inc. 
Santa Clara, CA 95050 U.S.A. 
fuad@bcltechnologies.com 
 
 
Abstract 
With the rapid growth of real 
application domains for NLP systems, 
there is a genuine demand for a general 
toolkit from which programmers with no 
linguistic knowledge can build specific 
NLP systems. Such a toolkit should 
provide an interface to accept sample 
sentences and convert them into 
semantic representations so as to allow 
programmers to map them to domain 
actions. In order to reduce the workload 
of managing a large number of semantic 
forms individually, the toolkit will 
perform what we call semantic grouping 
to organize the forms into meaningful 
groups. In this paper, we present three 
semantic grouping methods: similarity-
based, verb-based and category-based 
grouping, and their implementation in 
the SLUI toolkit. We also discuss the 
pros and cons of each method and how 
they can be utilized according to the 
different domain needs. 
1 Introduction and Motivation 
With the improvement of natural language 
processing (NLP) and speech recognition 
techniques, spoken language will become the 
input of choice for software user interfaces, as 
it is the most natural way of communication. In 
the mean time, the domains for NLP systems, 
especially those handling speech input, have 
grown rapidly in recent years. However, most 
computer programmers do not have enough 
linguistic knowledge to develop an NLP 
system to handle speech input. There is a 
genuine demand for a general toolkit from 
which programmers with no linguistic 
knowledge can rapidly build speech based 
NLP systems to handle their domain specific 
problems more accurately (Alam, 2000). The 
toolkit will allow programmers to generate 
Spoken Language User Interface (SLUI) front 
ends for new and existing applications using, 
for example, a program-through-example 
method. In this methodology, the programmer 
will specify a set of sample input sentences or 
a domain corpus for each task. The toolkit will 
then organize the sentences by meaning and 
even generate a large set of syntactic variations 
for a given sentence. It will also generate the 
code that takes a user?s spoken request and 
executes a command on an application. This 
methodology is similar to using a GUI toolkit 
to develop a graphical user interface so that 
programmers can develop GUI without 
learning graphics programming. Currently this 
is an active research area, and the present work 
is funded by the Advanced Technology 
Program (ATP) of the National Institute of 
Standards and Technology (NIST). 
In the program-through-example approach, 
the toolkit should provide an interface for the 
programmers to input domain specific corpora 
and then process the sentences into semantic 
representations so as to capture the semantic 
meanings of the sentences. In a real world 
application, this process results in a large 
number of semantic forms. Since the 
programmers have to manually build the links 
between these forms and their specific domain 
actions, they are likely to be overwhelmed by 
the workload imposed by the large number of 
individual semantic forms. In order to 
significantly reduce this workload, we can 
organize these forms in such a way so that the 
programmers can manipulate them as groups 
rather than as individual items. This will speed 
up the generation process of the domain 
specific SLUI system. We call this process the 
semantic grouping process. 
One straightforward way to group is to 
organize different syntactic forms expressing 
the same meaning together. For example,  
 
(1.1) I want to buy this book online. 
(1.2) Can I order this book online? 
(1.3) How can I purchase this book online? 
(1.4) What do I need to do to buy this book 
online?  
 
The semantic forms of the above sentences 
may not be the same, but the action the 
programmer has in mind in an e-business 
domain is more or less the same: to actually 
buy the book online. In addition to the above 
sentences, there are many variations that an 
end-user might use. The embedded NLP 
system should be able to recognize the 
similarity among the variations so that the 
SLUI system can execute the same command 
upon receiving the different queries. This 
requires a group to contain only sentences with 
the same meaning. However in real 
applications, this might be difficult to achieve 
because user requests often have slight 
differences in meaning. 
This difficulty motivates a different style 
for semantic grouping: organizing the semantic 
forms into groups so that those in the same 
group can be mapped roughly to the same 
action. The action can be either a command, 
e.g., buy something, or concerning an object, 
e.g., different ways of gathering information 
about an object. For example, sentence (1.5) 
would be grouped together with the above 
example sentences because it poses the same 
request: buy books; and sentences (1.6) to (1.8) 
would be in one group because they are all 
about price information. 
 
(1.5) I want to buy the latest book about e-
business. 
 
(1.6) Please send me a price quote. 
(1.7) What is the reseller price? 
(1.8) Do you have any package pricing for 
purchasing multiple products at once? 
This type of grouping is the focus of this 
paper. We propose three grouping methods: 
similarity-based grouping, verb-based 
grouping and category-based grouping. The 
process of grouping semantic forms is domain 
dependent and it is difficult to come up with a 
generally applicable standard to judge whether 
a grouping is appropriate or not. Different 
grouping techniques can give programmers 
different views of their data in order to satisfy 
different goals.  
This paper is organized into 6 sections. In 
Section 2, we briefly describe the system for 
which the grouping algorithms are proposed 
and implemented. Section 3 presents the three 
grouping methods in detail. In Section 4, we 
describe how the algorithms are implemented 
in our system. We test the methods using a set 
a sentences from our corpus and discuss the 
pros and cons of each method in Section 5. 
Finally, in Section 6, we draw conclusions and 
propose some future work. 
2 SLUITK 
As mentioned in the previous section, the 
Spoken Language User Interface Toolkit 
(SLUITK) allows programmers with no 
linguistic knowledge to rapidly develop a 
spoken language user interface for their 
applications. The toolkit should incorporate 
the major components of an NLP front 
end, such as a spell checker, a parser and a 
semantic representation generator. Using 
the toolkit, a programmer will be able to create 
a system that incorporates complex NLP 
techniques such as syntactic parsing and 
semantic understanding.  
2.1 The Work Flow 
Using an Automatic Speech Recognition 
(ASR) system, the SLUITK connects user 
input to the application, allowing spoken 
language control of the application. The 
SLUITK generates semantic representations of 
each input sentence. We refer to each of these 
semantic representations as a frame, which is 
basically a predicate-argument representation 
of a sentence.  
The SLUITK is implemented using the 
following steps: 
1. SLUITK begins to create a SLUI by 
generating semantic representations of 
sample input sentences provided by the 
programmer. 
2. These representations are expanded using 
synonym sets and other linguistic devices, 
and stored in a Semantic Frame Table 
(SFT). The SFT becomes a 
comprehensive database of all the 
possible commands a user could request a 
system to do. It has the same function as 
the database of parallel translations in an 
Example-based machine translation 
system (Sumita and Iida, 1991). 
3. The toolkit then creates methods for 
attaching the SLUI to the back end 
applications. 
4. When the SLUI enabled system is 
released, a user may enter an NL 
sentence, which is translated into a 
semantic frame by the system. The SFT is 
then searched for an equivalent frame. If a 
match is found, the action or command 
linked to this frame is executed. 
 
In a real application, a large number of 
frames might be generated from a domain 
corpus. The semantic grouper takes the set of 
frames as the input and outputs the same 
frames organized in a logical manner.  
2.2 The Corpus 
We use a corpus of email messages from our 
customers for developing and testing the 
system. These email messages contain 
questions, comments and general inquiries 
regarding our document-conversion products.  
We modified the raw email programmatically 
to delete the attachments, HTML and other 
tags, headers and sender information. In 
addition, we manually deleted salutations, 
greetings and any information that was not 
directly related to customer support. The 
corpus contains around 34,640 lines and 
170,000 words. We constantly update it with 
new email from our customers.  
We randomly selected 150 sentential 
inquiries to motivate and test the semantic 
grouping methods discussed in this paper. 
3 Semantic Grouping 
We have mentioned in Section 1 that grouping 
semantic frames is domain dependent.  
Grouping depends on the nature of the 
application and also the needs of the domain 
programmer. Since this is a real world 
problem, we have to consider the efficiency of 
grouping. It is not acceptable to let the 
programmer wait for hours to group one set of 
semantic forms. The grouping should be fairly 
fast, even on thousands of frames. 
These different considerations motivate 
several grouping methods: similarity-based 
grouping, verb-based grouping and category-
based grouping. In this section, we describe 
each of these methods in detail. 
3.1 Similarity-based Grouping  
Similarity-based grouping gathers sentences 
with similar meanings together, e.g., sentences 
(1.1) to (1.4). There is a wide application for 
this method. For example, in open domain 
question-answering systems, questions need to 
be reformulated so that they will match 
previously posted questions and therefore use 
the cached answers to speed up the process 
(Harabagiu et al, 2000).   
The question reformulation algorithm of 
Harabagiu et al tries to capture the similarity 
of the meanings expressed by two sentences. 
For a given set of questions, the algorithm 
formulates a similarity matrix from which 
reformulation classes can be built. Each class 
represents a class of equivalent questions. 
The algorithm for measuring the similarity 
between two questions tries to find lexical 
relationships between every two questions that 
do not contain stop words. The algorithm 
makes use of the WordNet concept hierarchy 
(Fellbaum, 1998) to find synonym and 
hypernym relations between words. 
This algorithm does not infer information 
about the meanings of the questions, but rather 
uses some kind of similarity measurement in 
order to simulate the commonality in meaning. 
This is a simplified approach. Using different 
threshold, they can achieve different degrees of 
similarity, from almost identical to very 
different. 
This method can be used for similarity-
based grouping to capture the similarity in 
meanings expressed by different sentences. 
3.2 Verb-based Grouping 
Among the sentences normally used in the e-
business domain, imperative sentences often 
appear in sub-domains dominated by 
command-and-control requests. In such an 
application, the verb expresses the command 
that the user wants to execute and therefore 
plays the most important role in the sentence. 
Based on this observation, a grouping can be 
based on the verb or verb class only. For 
example, sentences with buy or purchase etc. 
as the main verbs are classified into one group 
whereas those with download as the main verb 
are classified into a different group, even when 
the arguments of the verbs are the same. 
This is similar to sorting frames by the 
verb, taking into account simple verb synonym 
information.  
3.3 Category-based Grouping 
Since SLUITK is a generic toolkit whereas the 
motivation for grouping is application 
dependent, we need to know how the 
programmer wants the groups to be organized. 
We randomly selected 100 sentences from our 
corpus and asked two software engineers to 
group them in a logical order. They came up 
with very different groups, but their thoughts 
behind the groups are more or less the same. 
This motivates the category-based grouping. 
This grouping method puts less emphasis 
on each individual sentence, but tries to 
capture the general characteristics of a given 
corpus.  For example, we want to group by the 
commands (e.g., buy) or objects (e.g., a 
software) the corpus is concerned with. If a 
keyword of a category appears in a given 
sentence, we infer that sentence belongs to the 
category. For example, sentences (1.6) to (1.8) 
will be grouped together because they all 
contain the keyword price. 
These sentences will not be grouped 
together by the similarity-based method 
because their similarity is not high enough, nor 
by the verb-based method because the verbs 
are all different. 
4 Grouping in SLUITK 
Because we cannot foresee the domain needs 
of the programmer, we implemented all three 
methods in SLUITK so that the programmer 
can view their data in several different ways.  
The programmer is able to choose which type 
of grouping scheme to implement. 
In the question reformulation algorithm of 
(Harabagiu, et al 2000), all words are treated 
identically in the question similarity 
measurement. However, our intuition from 
observing the corpus is that the verb and the 
object are more important than other 
components of the sentence and therefore 
should be given more weight when measuring 
similarity. In Section 4.1, we describe our 
experiment with the grouping parameters to 
test our intuition. 
4.1 Experimenting with Parameters 
We think that there are two main 
parameters affecting the grouping result: the 
weight of the syntactic components and the 
threshold for the similarity measurement in the 
similarity-based method. Using 100 sentences 
from our corpus, we tried four different types 
of weighting scheme and three thresholds with 
the category-based methods. Human judgment 
on the generated groups confirmed our 
intuition that the object plays the most 
important role in grouping and the verb is the 
second most important. The differences in 
threshold did not seem to have a significant 
effect on the similarity-based grouping.  This 
is probably due to the strict similarity 
measurement. 
This experiment gives us a relatively 
optimal weighting scheme and threshold for 
the similarity-based grouping. 
One relevant issue concerns the 
simplification of the semantic frames. For a 
sentence with multiple verbs, we can simplify 
the frame based on the verbs used in the 
sentence. The idea is that some verbs such as 
action verbs are more interesting in the e-
business domain than others, e.g., be and have. 
If we can identify such differences in the verb 
usage, we can simplify the semantic frames by 
only keeping the interesting verb frames. For 
example, in the following sentences, the verb 
buy is more interesting than be and want, and 
the generated semantic frames should contain 
only the frame for buy. 
 
(4.1) Is it possible to buy this software online? 
(4.2) I want to buy this software online.  
 
Figure 1: A screen shot of SLUITK 
We make use of a list of stop-words from 
(Frakes, 1992) in order to distinguish between 
interesting and uninteresting verbs. We look 
for frames headed by stop-words and follow 
some heuristics to remove the sub-frame of the 
stop-word. For example, if there is at least one 
verb that is not a stop-word, we remove all 
other stop-words from the frame. In the 
sentence [Is it possible to] buy the software in 
Germany?, be is a stop-word, so only the 
frame for buy is kept. This process removes the 
redundant part of a frame so that the grouping 
algorithm only considers the most important 
part of a frame. 
4.2 Implementation in SLUITK 
Figure 1 shows a screen shot of the interface of 
the SLUITK, which shows several grouped 
semantic frames. In this section, we give more 
detail about the implementation of the three 
grouping methods used in SLUITK.  
 
Similarity-based grouping 
 
Similar to (Harabagiu, et al 2001), our 
similarity-based grouping algorithm calculates 
the similarity between every two frames in the 
input collection. If the similarity is above a 
certain threshold, the two frames are 
considered similar and therefore should be 
grouped together. If two frames in two 
different groups are similar, then the two 
groups should be combined to a single group. 
The central issue here is how to measure the 
similarity between two frames. 
Since we have found that some syntactic 
components are more important to grouping 
than others, we use a weighted scheme to 
measure similarity. For each frame, all words 
(except for stop-words) are extracted and used 
for similarity calculation. We give different 
weights to different sentence components. 
Since in an e-business domain, the verb and 
the object of a sentence are usually more 
important than other components because they 
express the actions that the programmers want 
to execute, or the objects for which they want 
to get more information, the similarity of these 
components are emphasized through the 
weighting scheme. The similarity score of two 
frames is the summation of the weights of the 
matched words.  
There is a match between two words when 
we find a lexical relationship between them. 
We extend the method of (Harabagiu, et al 
2000) and define a lexical relationship between 
two words W1 and W2 as in the following: 
Table 1 : Comparison of grouping methods 
 
 
1. If W1 and W2 have a common 
morphological root. Various stemming 
packages can be used for this purpose, for 
example, Porter Stemmer (Porter, 1997). 
2. If W1 and W2 are synonyms, i.e., W2 is 
in the WordNet synset of W1. 
3. If the more abstract word is a WordNet 
hypernym of the other. 
4. If one word is the WordNet holonym of 
the other (signaling part of, member of 
and substance of relations); 
5. If W1 is the WordNet antonym of W2.  
 
Domain specific heuristics can also be used 
to connect words. For example, in the e-
business domain, you and I can be treated as 
antonyms in the following sentences: 
 
(4.3) Can I buy this software? 
(4.4) Do you sell this software? 
 
When none of the above is true, there is no 
lexical relation between two given words. 
Because the similarity-based grouping 
needs to consult WordNet frequently for 
lexical relations, it becomes very slow for even 
a few hundred frames. We have to change the 
algorithm to speed up the process, as it is too 
slow for real world applications.  
Instead of comparing every two frames, we 
put all the words from an existing group 
together. When a new frame is introduced, we 
compare the words in this new frame with the 
word collection of each group. The similarity 
scores are added up as before, but it needs to 
be normalized over the number of words in the 
collection. When the similarity is above a 
certain threshold, the new frame is classified as 
a member of the group. This significantly 
reduces the comparison needed for classifying 
a frame, and therefore reduces the number of 
times WordNet needs to be consulted. 
We compared this improved algorithm with 
the original one on 30 handcrafted examples; 
the generated groups are very similar. 
 
Verb-based grouping 
 
The verb-based grouping implementation is 
fairly straightforward and has been described 
in Section 3.2. 
 
Category-base grouping 
 
For the category-based method, we first count 
all the non stop-words in a given corpus and 
retrieve a set of most frequent words and their 
corresponding word classes from the corpus. 
This process also makes use of the WordNet 
synonym, hypernym, holonym and antonym 
information. These word classes form the 
categories of each group. We then check the 
verbs and objects of each sentence to see if 
they match these words. That is, if a category 
word or a lexically related word appears as the 
verb or the object of a sentence, the sentence is 
classified as a member of that group. For 
example, we can pick the most frequent 20 
words and divide the corpus into 21 groups, 
where the extra group contains all sentences 
that cannot be classified. The programmer can 
decide the number of groups they want. This 
gives the programmer more control over the 
grouping result. 
5 Discussion 
We tested the three methods on 100 sentences 
from our corpus. We had 5 people evaluate the 
generated groups. They all thought that 
grouping was a very useful feature of the 
toolkit. Based on their comments, we 
summarize the pros and cons of each method 
in Table 1. 
The similarity-based grouping produces a 
large number of groups, most of which contain 
only one sentence. This is because there are 
usually several unrelated words in each 
sentence, which decreases the similarity 
scores. In addition, using WordNet we 
sometimes miss the connections between 
lexical items. The verb-based grouping 
 Similarity-based Verb-based Category-based 
Group Size small small large 
Number of Groups large large variable 
Speed slow on large corpus fast slow on large corpus 
Application general command-and-control only general 
produces slightly larger groups, but also 
produces many single sentence groups. 
Another problem is that when sentences 
contain only stop-word verbs, e.g., be, the 
group will look rather arbitrary. For example, a 
group of sentences with be as the main verb 
can express completely different semantic 
meanings. The small group size is a 
disadvantage of both methods. The number of 
groups of the category-based grouping can 
change according to the user specification. In 
general it produces less groups than the other 
methods and the group size is much larger, but 
the size becomes smaller for less frequent 
category words. 
Both the similarity-based and category-
based grouping methods are slow because they 
frequently need to use WordNet to identify 
lexical relationships. The verb-based method is 
much faster, which is the primary advantage of 
this method. 
The verb-based method should be used in a 
command-and-control domain because it 
requires at least one non stop-word verb in the 
sentence. However, it will have a hard time in 
a domain that needs to handle questions. From 
the point of view of assigning a domain 
specific action to a group, this grouping is the 
best because each verb can be mapped to an 
action.  Therefore, the programmer can link an 
action to each group rather than to each 
individual frame. When the group size is 
relatively large, this can greatly reduce the 
workload of the programmer.  
The category-based method produces a 
better view of the data because the sentences in 
each group seem to be consistent with the 
keywords of the category. The disadvantage is 
that it is difficult to link a group to a single 
action, and the programmer might have to re-
organize the groups during action assignment.  
The similarity-based method did not 
perform well on the testing corpus, but it might 
work better on a corpus containing several 
different expressions of the same semantic 
information. 
In summary, each method has its 
advantages and disadvantages. The decision of 
which one to choose depends mainly on the 
needs of the domain programmer and the 
composition of the input corpus. 
 
6 Conclusions and Future Work 
In this paper we propose semantic grouping as 
a way to solve the problem of manipulating 
semantic frames in developing a general 
Spoken Language User Interface Toolkit 
(SLUITK). We introduced three methods for 
grouping semantic frames generated by the 
NLP components of the toolkit. We tested the 
methods and discussed the advantages and 
disadvantages of each method. Since the 
judgment of the grouping result is application 
dependent, the methods co-exist in our 
SLUITK to suit the requirement of different 
applications. 
Future work includes improving the 
efficiency and accuracy of the methods and 
testing them on a larger corpus. 
 
References 
Alam H. (2000) Spoken Language Generic 
User Interface (SLGUI). Technical Report 
AFRL-IF-RS-TR-2000-58, Air Force Research 
Laboratory, Rome.  
Fellbaum C. (1998) WordNet, An 
Electronic Lexical Database, The MIT Press, 
Cambridge, Massachusetts. 
Frakes W. and Baeza-Yates R. (1992) 
Information Retrieval, Data Structures and 
Algorithms, Prentice-Hall. 
HaraBagiu S. and Moldovan D. and Pasca 
M. and Mihalcea R. and Surdeanu M. and 
Bunescu R. and Girju R. and Rus V. and 
Morarescu P. (2000) FALCON: Boosting 
Knowledge for Answer Engines, TREC 9. 
Porter M. (1997) An algorithm for suffix 
stripping, in Readings in Information 
Retrieval, Karen Sparck Jones and Peter Willet 
(ed), San Francisco: Morgan Kaufmann. 
Sumita E. and Iida H. (1991) Experiments 
and Prospects of Example-Based Machine 
Translation. In Proceedings of the Annual 
Meeting of the Association for Computational 
Linguistics, pp. 185-192. 
 
Extending A Broad-Coverage Parser for a General NLP Toolkit 
 
Hassan Alam, Hua Cheng, Rachmat Hartono, Aman Kumar, Paul Llido, Crystal Nakatsu, Fuad 
Rahman, Yuliya Tarnikova, Timotius Tjahjadi and Che Wilcox  
 
BCL Technologies Inc. 
Santa Clara, CA 95050 U.S.A. 
fuad@bcltechnologies.com 
 
Abstract 
With the rapid growth of real world 
applications for NLP systems, there 
is a genuine demand for a general 
toolkit from which programmers 
with no linguistic knowledge can 
build specific NLP systems. Such a 
toolkit should have a parser that is 
general enough to be used across 
domains, and yet accurate enough for 
each specific application. In this 
paper, we describe a parser that 
extends a broad-coverage parser, 
Minipar (Lin, 2001), with an 
adaptable shallow parser so as to 
achieve both generality and accuracy 
in handling domain specific NL 
problems. We test this parser on our 
corpus and the results show that the 
accuracy is significantly higher than 
a system that uses Minipar alone. 
1 Introduction 
With the improvement of natural language 
processing (NLP) techniques, domains for 
NLP systems, especially those handling speech 
input, are rapidly growing. However, most 
computer programmers do not have enough 
linguistic knowledge to develop NLP systems. 
There is a genuine demand for a general toolkit 
from which programmers with no linguistic 
knowledge can rapidly build NLP systems that 
handle domain specific problems more 
accurately (Alam, 2000). The toolkit will allow 
programmers to generate natural language 
front ends for new and existing applications 
using, for example, a program-through-
example method. In this methodology, the 
programmer will specify a set of sample input 
sentences or a domain corpus for each task. 
The toolkit will then organize the sentences by 
similarity and generate a large set of syntactic 
variations of a given sentence. It will also 
generate the code that takes a user?s natural 
language request and executes a command on 
an application. Currently this is an active 
research area, and the Advanced Technology 
Program (ATP) of the National Institute of 
Standards and Technology (NIST) is funding 
part of the work.  
In order to handle natural language input, 
an NLP toolkit must have a parser that maps a 
sentence string to a syntactic structure. The 
parser must be both general and accurate. It 
has to be general because programmers from 
different domains will use the toolkit to 
generate their specific parsers. It has to be 
accurate because the toolkit targets commercial 
domains, which usually require high accuracy.  
The accuracy of the parser directly affects the 
accuracy of the generated NL interface. In the 
program-through-example approach, the 
toolkit should convert the example sentences 
into semantic representations so as to capture 
their meanings. In a real world application, this 
process will involve a large quantity of data. If 
the programmers have to check each syntactic 
or semantic form by hand in order to decide if 
the corresponding sentence is parsed correctly, 
they are likely to be overwhelmed by the 
workload imposed by the large number of 
sentences, not to mention that they do not have 
the necessary linguistic knowledge to do this. 
Therefore the toolkit should have a broad-
coverage parser that has the accuracy of a 
parser designed specifically for a domain. 
One solution is to use an existing parser 
with relatively high accuracy.  Using existing 
parsers such as (Charniak, 2000; Collins, 
1999) would eliminate the need to build a 
parser from scratch. However, there are two 
problems with such an approach. First, many 
parsers claim high precision in terms of the 
number of correctly parsed syntactic relations 
rather than sentences, whereas in commercial 
applications, the users are often concerned 
with the number of complete sentences that are 
parsed correctly. The precision might drop 
considerably using this standard. In addition, 
although many parsers are domain 
independent, they actually perform much 
better in the domains they are trained on or 
implemented in. Therefore, relying solely on a 
general parser would not satisfy the accuracy 
needs for a particular domain.  
Second, since each domain has its own 
problems, which cannot be foreseen in the 
design of the toolkit, customization of the 
parser might be needed. Unfortunately, using 
an existing parser does not normally allow this 
option. One solution is to build another parser 
on top of the general parser that can be 
customized to address domain specific parsing 
problems such as ungrammatical sentences. 
This domain specific parser can be built 
relatively fast because it only needs to handle a 
small set of natural language phenomena. In 
this way, the toolkit will have a parser that 
covers wider applications and in the mean time 
can be customized to handle domain specific 
phenomena with high accuracy. In this paper 
we adopt this methodology. 
The paper is organized into 6 sections. In 
Section 2, we briefly describe the NLP toolkit 
for which the parser is proposed and 
implemented. Section 3 introduces Minipar, 
the broad-coverage parser we choose for our 
toolkit, and the problems this parser has when 
parsing a corpus we collected in an IT domain. 
In Section 4, we present the design of the 
shallow parser and its disadvantages. We 
describe how we combine the strength of the 
two parsers and the testing result in Section 5. 
Finally, in Section 6, we draw conclusions and 
propose some future work. 
2 NLP Toolkit 
In the previous section, we mentioned a 
Natural Language Processing Toolkit 
(NLPTK) that allows programmers with no 
linguistic knowledge to rapidly develop natural 
language user interfaces for their applications. 
The toolkit should incorporate the major 
components of an NLP system, such as a spell 
checker, a parser and a semantic representation 
generator. Using the toolkit, a software 
engineer will be able to create a system that 
incorporates complex NLP techniques such as 
syntactic parsing and semantic understanding.  
In order to provide NL control to an 
application, the NLPTK needs to generate 
semantic representations for input sentences. 
We refer to each of these semantic forms as a 
frame, which is basically a predicate-argument 
representation of a sentence.  
The NLPTK is implemented using the 
following steps: 
 
1. NLPTK begins to create an NLP front end 
by generating semantic representations of 
sample input sentences provided by the 
programmer. 
2. These representations are expanded using 
synonym sets and stored in a Semantic 
Frame Table (SFT), which becomes a 
comprehensive database of all the 
possible commands a user could request 
the system to do.  
3. The toolkit then creates methods for 
attaching the NLP front end to the back 
end applications. 
4. When the NLP front end is released, a user 
may enter an NL sentence, which is 
translated into a semantic frame by the 
system. The SFT is then searched for an 
equivalent frame. If a match is found, the 
action or command linked to this frame is 
executed. 
In order to generate semantic 
representations in Step 1, the parser has to 
parse the input sentences into syntactic trees. 
During the process of building an NLP system, 
the programmer needs to customize the parser 
of the toolkit for their specific domain. For 
example, the toolkit provides an interface to 
highlight the domain specific words that are 
not in the lexicon.  The toolkit then asks the 
programmer for information that helps the 
system insert the correct lexical item into the 
lexicon. The NLPTK development team must 
handle complicated customizations for the 
programmer. For example, we might need to 
change the rules behind the domain specific 
parser to handle certain natural language input. 
In Step 4, when the programmer finishes 
building an NLP application, the system will 
implement a domain specific parser. The 
toolkit has been completely implemented and 
tested.  
We use a corpus of email messages from 
our customers for developing the system. 
These emails contain questions, comments and 
general inquiries regarding our document-
conversion products. We modified the raw 
email programmatically to delete the 
attachments, HTML tags, headers and sender 
information. In addition, we manually deleted 
salutations, greetings and any information not 
directly related to customer support. The 
corpus contains around 34,640 lines and 
170,000 words. We constantly update it with 
new emails from our customers.  
From this corpus, we created a test corpus 
of 1000 inquiries to test existing broad-
coverage parsers and the parser of the toolkit. 
3 Minipar in NLPTK 
We choose to use Minipar (Lin, 2001), a 
widely known parser in commercial domains, 
as the general parser of NLPTK. It is worth 
pointing out that our methodology does not 
depend on any individual parser, and we can 
use any other available parser.  
3.1 Introduction to Minipar 
Minipar is a principle-based, broad-coverage 
parser for English (Lin, 2001). It represents its 
grammar as a network of nodes and links, 
where the nodes represent grammatical 
categories and the links represent types of 
dependency relationships. The grammar is 
manually constructed, based on the Minimalist 
Program (Chomsky, 1995). 
Minipar constructs all possible parses of an 
input sentence. It makes use of the frequency 
counts of the grammatical dependency 
relationships extracted by a collocation 
extractor (Lin, 1998b) from a 1GB corpus 
parsed with Minipar to resolve syntactic 
ambiguities and rank candidate parse trees. 
The dependency tree with the highest ranking 
is returned as the parse of the sentence.  
The Minipar lexicon contains about 
130,000 entries, derived from WordNet 
(Fellbaum, 1998) with additional proper 
names. The lexicon entry of a word lists all 
possible parts of speech of the word and its 
subcategorization frames (if any).  
Minipar achieves about 88% precision and 
80% recall with respect to dependency 
relationships (Lin, 1998a), evaluated on the 
SUSANNE corpus (Sampson, 1995), a subset 
of the Brown Corpus of American English. 
3.2 Disadvantages of Minipar 
In order to see how well Minipar performs in 
our domain, we tested it on 584 sentences from 
our corpus. Instead of checking the parse trees, 
we checked the frames corresponding to the 
sentences, since the accuracy of the frames is 
what we are most concerned with. If any part 
of a frame was wrong, we treated it as an error 
of the module that contributed to the error. We 
counted all the errors caused by Minipar and 
its accuracy in terms of correctly parsed 
sentences is 77.6%. Note that the accuracy is 
actually lower because later processes fix some 
errors in order to generate correct frames. 
The majority of Minipar errors fall in the 
following categories: 
 
1. Tagging errors: some nouns are mis-
tagged as verbs. For example, in Can I get 
a copy of the batch product guide?, guide 
is tagged as a verb. 
2. Attachment errors: some prepositional 
phrases (PP) that should be attached to 
their immediate preceding nouns are 
attached to the verbs. For example, in Can 
Drake convert the PDF documents in 
Japanese?, in Japanese is attached to 
convert. 
3. Missing lexical entries: some domain 
specific words such as download and their 
usages are not in the Minipar lexicon. 
This introduces parsing errors because 
such words are tagged as nouns by 
default.  
4. Inability to handle ungrammatical 
sentences: in a real world application, it is 
unrealistic to expect the user to enter only 
grammatical sentences. Although Minipar 
still produces a syntactic tree for an 
ungrammatical sentence, the tree is ill 
formed and cannot be used to extract the 
semantic information being expressed. 
 
In addition, Minipar, like other broad-
coverage parsers, cannot be adapted to specific 
applications. Its accuracy does not satisfy the 
needs of our toolkit. We have to build another 
parser on top of Minipar to enable domain 
specific customizations to increase the parsing 
accuracy. 
4 The Shallow Parser 
Our NLPTK maps input sentences to action 
requests. In order to perform an accurate 
mapping the toolkit needs to get information 
such as the sentence type, the main predicate, 
the arguments of the predicate, and the 
modifications of the predicate and arguments 
from a sentence. In other words, it mostly 
needs local dependency relationships. 
Therefore we decided to build a shallow parser 
instead of a full parser. A parser that captures 
the most frequent verb argument structures in a 
domain can be built relatively fast. It takes less 
space, which can be an important issue for 
certain applications. For example, when 
building an NLP system for a handheld 
platform, a light parser is needed because the 
memory cannot accommodate a full parser. 
4.1 Introduction 
We built a KWIC (keyword in context) verb 
shallow parser. It captures only verb predicates 
with their arguments, verb argument modifiers 
and verb adjuncts in a sentence. The resulting 
trees contain local and subjacent dependencies 
between these elements. 
The shallow parser depends on three levels 
of information processing: the verb list, 
subcategorization (in short, subcat) and 
syntactic rules. The verb subcat system is 
derived from Levin?s taxonomy of verbs and 
their classes (Levin, 1993). We have 24 verb 
files containing 3200 verbs, which include all 
the Levin verbs and the most frequent verbs in 
our corpus. A verb is indexed to one or more 
subcat files and each file represents a particular 
alternation semantico-syntactic sense. We have 
272 syntactic subcat files derived from the 
Levin verb semantic classes. The syntactic 
rules are marked for argument types and 
constituency, using the Penn Treebank tagset 
(Marcus, 1993). They contain both generalized 
rules, e.g.,  .../NN, and specified rules, e.g., 
purchase/VBP. An example subcat rule for the 
verb purchase looks like this: .../DT .../JJ 
.../NN, .../DT .../NN from/RP .../NN for/RP 
.../NN. The first element says that purchase 
takes an NP argument, and the second says that 
it takes an NP argument and two PP adjuncts. 
We also encoded specific PP head class 
information based on the WordNet concepts in 
the rules for some attachment disambiguation. 
The shallow parser works like this: it first 
tags an incoming sentence with Brill tagger 
(Brill, 1995) and matches verbs in the tagged 
sentence with the verb list. If a match is found, 
the parser will open the subcat files indexed to 
that verb and gather all the syntactic rules in 
these specific subcat files. It then matches the 
verb arguments with these syntactic rules and 
outputs the results into a tree. The parser can 
control over-generation for any verb because 
the syntactic structures are limited to that 
particular verb's syntactic structure set from 
the Levin classes. 
4.2 Disadvantages of Shallow Parser 
The disadvantages of the shallow parser are 
mainly due to its simplified design, including: 
1. It cannot handle sentences whose main 
verb is be or phrasal sentences without a 
verb because the shallow parser mainly 
targets command-and-control verb 
argument structures.  
2. It cannot handle structures that appear 
before the verb. Subjects will not appear 
in the parse tree even though it might 
contain important information. 
3. It cannot detect sentence type, for 
example, whether a sentence is a question 
or a request. 
4. It cannot handle negative or passive 
sentences. 
We tested the shallow parser on 500 
sentences from our corpus and compared the 
results with the output of Minipar. We 
separated the sentences into five sets of 100 
sentences. After running the parser on each set, 
we fixed the problems that we could identify.  
This was our process of training the parser. 
Table 1 shows the data obtained from one such 
cycle. Since the shallow parser cannot handle 
sentences with the main verb be, these 
sentences are excluded from the statistics. So 
the test set actually contains 85 sentences. 
In Table 1, the first column and the first 
row show the statistics for the shallow parser 
and Minipar respectively. The upper half of the 
table is for the unseen data, where 55.3% of 
the sentences are parsed correctly and 11.8% 
incorrectly (judged by humans) by both 
parsers. 18.9% of the sentences are parsed 
correctly by Minipar, but incorrectly by the 
shallow parser, and 14.1% vise versa. The 
lower half of the table shows the result after 
fixing some shallow parser problems, for 
example, adding a new syntactic rule. The 
accuracy of the parser is significantly 
improved, from 69.4% to 81.2%. This shows 
the importance of adaptation to specific 
domain needs, and that in our domain, the 
shallow parser outperforms Minipar. 
 
SP/MP Correct 
(74.1%) 
Wrong 
(25.9%) 
Correct (69.4%) 47 (55.3%) 12 (14.1%) 
Wrong (30.6%) 16 (18.9%) 10 (11.8%) 
SP/MP Correct 
(74.1%) 
Wrong 
(25.9%) 
Correct (81.2%) 53 (62.4%) 16 (18.8%) 
Wrong (18.8%) 10 (11.8%) 6 (7.1%) 
Table 1: Comparison of the shallow 
parser with Minipar on 85 sentences 
The parsers do not perform equally well on 
all sets of sentences. For some sets, the 
accuracies of Minipar and the shallow parser 
drop to 60.9% and 67.8% respectively. 
5 Extending Minipar with the 
Shallow Parser 
Each parser has pros and cons. The advantage 
of Minipar is that it is a broad-coverage parser 
with relatively high accuracy, and the 
advantage of the shallow parser is that it is 
adaptable. For this reason, we intend to use 
Minipar as our primary parser and the shallow 
parser a backup. Table 1 shows only a small 
percentage of sentences parsed incorrectly by 
both parsers (about 7%). If we always choose 
the correct tree between the two outputs, we 
will have a parser with much higher accuracy. 
Therefore, combining the advantages of the 
two parsers will achieve better performance in 
both coverage and accuracy. Now the question 
is how to decide if a tree is correct or not.  
5.1 Detecting Parsing Errors 
In an ideal situation, each parser should 
provide a confidence level for a tree that is 
comparable to each other. We would choose 
the tree with higher confidence. However, this 
is not possible in our case because weightings 
of the Minipar trees are not publicly available, 
and the shallow parser is a rule-based system 
without confidence information.  
Instead, we use a few simple heuristics to 
decide if a tree is right or wrong, based on an 
analysis of the trees generated for our test 
sentences. For example, given a sentence, the 
Minipar tree is incorrect if it has more than one 
subtree connected by a top-level node whose 
syntactic category is U (unknown).  A shallow 
parser tree is wrong if there are unparsed 
words at the end of the sentence after the main 
verb (except for interjections). We have three 
heuristics identifying a wrong Minipar tree and 
two identifying a wrong shallow parser tree. If 
a tree passes these heuristics, we must label the 
tree as a good parse.  This may not be true, but 
we will compensate for this simplification 
later. The module implementing these 
heuristics is called the error detector. 
We tested the three heuristics for Minipar 
trees on a combination of 84 requestive, 
interrogative and declarative sentences. The 
results are given in the upper part of Table 2. 
The table shows that 45 correct Minipar trees 
(judged by humans) are identified as correct by 
the error detector and 18 wrong trees are 
identified as wrong, so the accuracy is 75%. 
Tagging errors and some attachment errors 
cannot be detected. 
 
MP/ED Correct 
(76.2%) 
Wrong 
(23.8%) 
Correct (56%) 45 (53.6%) 2 (2.4%) 
Wrong (44%) 19 (22.6%) 18 (21.4%) 
SP/ED Correct 
(73%) 
Wrong 
(26%) 
Correct (59%) 58 (58%) 1 (1%) 
Wrong (40%) 15 (15%) 25 (25%) 
Table 2: The performance of the parse 
tree error detector 
We tested the two heuristics for shallow 
parser trees on 100 sentences from our corpus 
and the result is given in the lower part of 
Table 2. The accuracy is about 83%. We did 
not use the same set of sentences to test the 
two sets of heuristics because the coverage of 
the two parsers is different. 
5.2 Choosing the Better Parse Trees 
We run the two parsers in parallel to generate 
two parse trees for an input sentence, but we 
cannot depend only on the error detector to 
decide which tree to choose because it is not 
accurate enough. Table 2 shows that the error 
detector mistakenly judges some wrong trees 
as correct, but not the other way round. In 
other words, when the detector says a tree is 
wrong, we have high confidence that it is 
indeed wrong, but when it says a tree is 
correct, there is some chance that the tree is 
actually wrong. This motivates us to 
distinguish three cases: 
1. When only one of the two parse trees is 
detected as wrong, we choose the correct 
tree, because no matter what the correct 
tree actually is, the other tree is definitely 
wrong so we cannot choose it. 
2. When both trees are detected as wrong, we 
choose the Minipar tree because it handles 
more syntactic structures. 
3. When both trees are detected as correct, 
we need more analysis because either 
might be wrong. 
We have mentioned in the previous sections 
the problems with both parsers. By comparing 
their pros and cons, we come up with 
heuristics for determining which tree is better 
for the third case above.  
The decision flow for selecting the better 
parse is given in Figure 1. Since the shallow 
parser cannot handle negative and passive 
sentences as well as sentences with the main 
verb be, we choose the Minipar trees for such 
sentences. The shallow parser outperforms 
Minipar on tagging and some PP attachment 
because it checks the WordNet concepts. So, 
when we detect differences concerning part-of-
speech tags and PP attachment in the parse 
trees, we choose the shallow parser tree as the 
output. In addition, we prefer the parse with 
bigger NP chunks.  
We tested these heuristics on 200 sentences 
and the result is shown in Table 3. The first 
row specifies whether a Minipar tree or a 
shallow parser tree is chosen as the final 
output. The first column gives whether the 
final tree is correct or incorrect according to 
human judgment. 88% of the time, Minipar 
trees are chosen and they are 82.5% accurate. 
The overall contribution of Minipar to the 
accuracy is 73.5%. The improvement from just 
using Minipar is about 7%, from about 75.5% 
to 82.5%. This is a significant improvement. 
The main computational expense of 
running two parsers in parallel is time. Since 
our shallow parser has not been optimized, the 
extended parser is about 2.5 times slower than 
Minipar alone. We hope that with some 
optimization, the speed of the system will 
increase considerably. Even in the current time 
frame, it takes less than 0.6 second to parse a 
15 word sentence. 
 
Final tree MP tree 
(88%) 
SP tree 
(11%) 
Correct (82.5%) 73.5% 9% 
Wrong (16.5%) 14.5% 2% 
Table 3: Results for the extended parser 
6 Conclusions and Future Work 
In this paper we described a parser that extends 
a broad-coverage parser, Minipar, with a 
domain adaptable shallow parser in order to 
achieve generality and higher accuracy at the 
same time. This parser is an important 
component of a general NLP Toolkit, which 
helps programmers quickly develop an NLP 
front end that handles natural language input 
from their end users. We tested the parser on 
200 sentences from our corpus and the result 
shows significant improvement over using 
Minipar alone. 
Future work includes improving the 
efficiency and accuracy of the shallow parser. 
Also, we will test the parser on a different 
domain to see how much work is required to 
switch to a new domain. 
 
References 
Alam H. (2000) Spoken Language Generic 
User Interface (SLGUI). Technical Report 
AFRL-IF-RS-TR-2000-58, Air Force Research 
Laboratory, Rome.  
Brill E. (1992) A Simple Rule-based Part of 
Speech Tagger. In Proceedings of the 3rd 
Conference on Applied Natural Language 
Processing. 
Charniak E. (2000) A Maximum-Entropy-
Inspired Parser. In Proceedings of the 1st 
Meeting of NAACL. Washington. 
Chomsky N. (1995) Minimalist Program. 
MIT Press. 
Collins M. (1999) Head-Driven Statistical 
Models for Natural Language Parsing. PhD 
Dissertation, University of Pennsylvania. 
Is the sentence
passive?
Is the SP tree empty?
Is the SP  tree correct?
Adding the sentence type
and subject of the M P tree to
the SP tree
No
No
Yes
Accept M P tree
M inipar (M P) tree Shallow Parser (SP) tree
Final parse tree
Yes
No
Is the sentence
negative?
No
Yes
Yes
Yes
No
No
Yes
No
Is the M P tree correct?
No
Is the size of the M P
tree bigger than or
equal to that of the
SP  tree?
Is the length of an
NP chunk in  the M P
tree longer?
Yes
Does the M P tree
have less verb  tags?
Yes
No
No
Yes
Yes
No
Yes Does SP finds a verb
when M P assigns a
sentence type as NP?
Are the verb  tags in
the two trees
inconsistent?
Are there unequal
number of verb tags
in the trees?
Figure 1: Decision flow for parse tree selection 
 
Levin B. (1993) English Verb Classes and 
Alternations: A Preliminary Investigation. 
University of Chicago Press, Chicago. 
Lin D. (1998a) Dependency-based 
Evaluation of Minipar. In Workshop on the 
Evaluation of Parsing Systems, Spain. 
Lin D. (1998b) Extracting Collocations 
from Text Corpora. In Workshop on 
Computational Terminology, Montreal, 
Canada, pp. 57-63. 
Lin D. (2001) Latat: Language and Text 
Analysis Tools. In Proceedings of Human 
Language Technology Conference, CA, USA. 
Marcus M., Santorini B. and Marcinkiewicz 
M. (1993) Building a Large Annotated Corpus 
of English: The Penn Treebank, Computational 
Linguistics, vol. 19, no. 2, pp. 313-330.  
Sampson G. (1995) English for the 
Computer. Oxford University Press. 
Proceedings of the 14th European Workshop on Natural Language Generation, pages 30?39,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Enhancing the Expression of Contrast
in the SPaRKy Restaurant Corpus
David M. Howcroft and Crystal Nakatsu and Michael White
Department of Linguistics
The Ohio State University
Columbus, OH 43210, USA
{howcroft,cnakatsu,mwhite}@ling.osu.edu
Abstract
We show that Nakatsu & White?s (2010)
proposed enhancements to the SPaRKy
Restaurant Corpus (SRC; Walker et al,
2007) for better expressing contrast do in-
deed make it possible to generate better
texts, including ones that make effective
and varied use of contrastive connectives
and discourse adverbials. After first pre-
senting a validation experiment for natu-
ralness ratings of SRC texts gathered using
Amazon?s Mechanical Turk, we present
an initial experiment suggesting that such
ratings can be used to train a realization
ranker that enables higher-rated texts to be
selected when the ranker is trained on a
sample of generated restaurant recommen-
dations with the contrast enhancements
than without them. We conclude with a
discussion of possible ways of improving
the ranker in future work.
1 Introduction
To lessen the need for handcrafting in developing
generation systems, Walker et al (2007) extended
the overgenerate-and-rank methodology (Langk-
ilde and Knight, 1998; Mellish et al, 1998; Walker
et al, 2002; Nakatsu and White, 2006) to complex
information presentation tasks involving variation
in rhetorical structure. They illustrated their ap-
proach by developing SPaRKy (Sentence Planning
with Rhetorical Knowledge), a sentence planner
for generating restaurant recommendations and
comparisons in the context of the MATCH (Mul-
timodal Access To City Help) system (Walker et
al., 2004), and showed that SPaRKY can produce
texts comparable to those of MATCH?s template-
based generator.
Despite the evident importance of expressing
contrast clearly in making comparisons among
restaurants, Nakatsu (2008) surprisingly found
that most of the examples involving contrastive
connectives in the SPaRKy Restaurant Corpus
(SRC) received low ratings by the human judges.
Even though the low ratings were not necessarily
directly attributable to the use of a contrastive con-
nective in many cases, Nakatsu conjectured that
the large proportion of low-rated examples con-
taining contrastive connectives would make it dif-
ficult to train a ranker to learn to use contrastive
connectives effectively without augmenting the
corpus with better examples of contrast. Sub-
sequently, Nakatsu and White (2010) proposed
a set of enhancements to the SRC intended to
better express contrast?including ones employ-
ing multiple connectives in the same clause that
are problematic for RST (Mann and Thompson,
1988)?and showed how they could be generated
with Discourse Combinatory Categorial Grammar
(DCCG), an extension of CCG (Steedman, 2000)
designed to enable multi-sentence grammar-based
generation. However, Nakatsu and White did not
evaluate empirically whether these contrast en-
hancements were successful.
In this paper, we show that Nakatsu &
White?s (2010) proposed SRC contrast enhance-
ments do indeed make it possible to generate bet-
ter texts: in particular, we present an initial ex-
periment that shows that the oracle best restau-
rant recommendations including the contrast en-
hancements have significantly higher human rat-
ings for naturalness than comparable texts without
these enhancements, and which suggests that even
a basic n-gram ranker trained on the enhanced
recommendations can select texts with higher rat-
ings. The paper is structured as follows. In
Section 2, we review Nakatsu & White?s pro-
posed enhancements to the SRC for better express-
ing contrast?including the use of structural con-
nectives together with discourse adverbials?and
how they can be generated with DCCG. In Sec-
30
tion 3, we first present a validation experiment
showing that naturalness ratings gathered on Ama-
zon?s Mechanical Turk (AMT) are comparable to
those for the same texts in the original SRC; then,
we present our method of generating and selecting
a sample of new restaurant recommendation texts
with and without the contrast enhancements for
rating on AMT. In Section 4, we describe how we
trained discriminative n-gram rankers using cross
validation on the gathered ratings. In Section 5,
we present the oracle and cross validation results
in terms of mean scores of the top-ranked text. In
Section 6, we analyze how the individual contrast
enhancements affected the naturalness ratings and
discuss issues that may be still hampering natu-
ralness. Finally, in Section 7, we conclude with
a summary and a discussion of possible ways of
creating improved rankers in future work.
2 Enhancing Contrast with Discourse
Combinatory Categorial Grammar
Figure 1 (Nakatsu, 2008) shows examples from
the SRC where some of the SPaRKy realizations
are clearly more natural than others. In Nakatsu?s
experiments, she found that the use of contrastive
connectives was negatively correlated with human
ratings, and that an n-gram ranker learned to dis-
prefer texts containing these connectives. In an-
alyzing these unexpected results, Nakatsu noted
two factors that appeared to hamper the natural-
ness of the contrastive connective usage. First,
consistent with Grote et al?s (1995) observation
that however and on the other hand (unlike but and
while) signal that the clause they attach to is the
more important one, we might expect realizations
to be preferred when these connectives appear
with the more desirable of the contrasted qualities.
Such preferences do indeed appear to be present
in the SRC: for example, in Figure 1, alts 8 &
13?where the better property is ordered second?
are rated highly, while alts 7 & 11?where the
better property is ordered first?are rated poorly.
Nakatsu further observed that in human-authored
comparisons, when the second clause expresses
the lesser property, it is often qualified by only
or just; consistent with this observation, alts 7 &
11 do seem to improve with the inclusion of these
modifiers.
The second factor noted by Nakatsu that may
contribute to the awkwardness of however and on
the other hand is that both of these connectives
seem to be rather ?grand? for the rather simple
contrasts in Figure 1, and may sound more natu-
ral when used with heavier arguments.
Based on these observations, Nakatsu and
White (2010) proposed a set of enhancements to
the SRC, all of which are exemplified in Figure 2.
1
The enhancements include (i) optional summary
statements that give an overall assessment of each
restaurant based on the average of their property
values, thereby allowing contrasts to be expressed
over larger text spans; (ii) adverbial modifiers
only, just and merely to express a lesser value of
a given property than one mentioned earlier;
2
(iii)
the modifers also and too to signal the repetition
of the same value for a given property (Strieg-
nitz, 2004); and (iv) contrastive connectives for
different properties of the same restaurant, exem-
plified here by the contrast between decent decor
and mediocre food quality for Bienvenue.
In the text plan in Figure 2, <1>?<4> cor-
respond to the propositions in the original SRC
text plan and (1?)?(2?) are the new summary-level
propositions. Following Webber et al (2003),
Nakatsu and White (2010) take only, merely, just,
also, and too to be discourse adverbials, whose
discourse relations are allowed to cut across the
primary tree structure established by the other re-
lations in the figure. Note that in addition to go-
ing beyond RST?s limitation to tree-structured dis-
courses, the example also contains clauses em-
ploying multiple discourse connectives, where one
is a structural connective (such as however or
while) and the other is a discourse adverbial.
To realize such texts, Nakatsu & White intro-
duce Discourse Combinatory Categorial Grammar
(DCCG), an extension of CCG (Steedman, 2000)
to the discourse level. DCCG follows Discourse
Lexicalized Tree Adjoining Grammar (Webber,
2004) in providing a lexicalized treatment of struc-
tural connectives and discourse adverbials, but dif-
fers in doing so in a single CCG, rather than sep-
arate sentence-level and discourse-level grammars
whose interaction is not straightforward. As such,
DCCG requires no changes to the OpenCCG real-
izer (White, 2006b; White, 2006a; White and Ra-
1
In the text, words intended to help indicate similarities
and contrasts are italicized. Note that we have added overall
and on the whole to the summary statements to better indicate
their summarizing role.
2
The second value must be a less extreme one on the same
side of the scale; in principle, it could be merely poor rather
than horrible, but such low attribute values did not occur in
the corpus.
31
Strategy Alt # Rating Rank Realization
3 3 7 Sonia Rose has very good decor but Bienvenue has decent decor.
7 1 16 Sonia Rose has very good decor. On the other hand, Bienvenue has decent decor.
8 4.5 13 Bienvenue has decent decor. Sonia Rose, on the other hand, has very good decor.
C2 10 4.5 5 Bienvenue has decent decor but Sonia Rose has very good decor.
11 1 12 Sonia Rose has very good decor. However, Bienvenue has decent decor.
13 5 14 Bienvenue has decent decor. However, Sonia Rose has very good decor.
14 5 3 Sonia Rose has very good decor while Bienvenue has decent decor.
15 4 4 Bienvenue has decent decor while Sonia Rose has very good decor.
17 1 15 Bienvenue?s price is 35 dollars. Sonia Rose?s price, however, is 51 dollars. Bienvenue has decent decor.
However, Sonia Rose has very good decor.
Figure 1: Some alternative [Alt] realizations of SPaRKy sentence plans from a COMPARE-2 [C2] plan, with averaged
human ratings [Rating] (5 = highest rating) and ranks assigned by the n-gram ranker [Rank] (1 = top ranked).
tion, the SPaRKy sentence plan generator adds the
INFER relation to assertions whose relations were
not specified by the content planner.
During the sentence planning phase, SPaRKy or-
ders the clauses and combines them using randomly
selected clause-combining operations. During this
process, a clause-combining operation may insert 1
of 7 connectives according to the RST relation that
holds between two discourse units (i.e. inserting
since or because for a JUSTIFY relation; and, how-
ever, on the other hand, while, or but for a CON-
TRAST relation; or and for an INFER relation).
After each sentence plan is generated, it is real-
ized by the RealPro surface realizer and the result-
ing realization is rated by two judges on a scale of
1-5, where 5 is highly preferred. These ratings are
then averaged, producing a range of 9 possible rat-
ings from {1, 1.5, ..., 5}.
2.2 Ratings/Connectives Correlation
From the ratings of the examples in Figure 1, we
can see that some of the SPaRKy sentence plan re-
alizations seem more natural than others. Upon fur-
ther analysis, we noticed that utterances containing
many contrastive connectives seemed less preferred
than those with fewer or no contrastive connectives.
To quantify this observation, we calculated the av-
erage number of connectives (ave
c
i
) used per real-
ization with rating i, using ave
c
i
= Total
c
i
/N
r
i
,
where Total
c
i
is the total number of connectives in
realizations with rating i, and N
r
i
is the number of
realizations with rating i.
We use Pearson?s r to calculate each correlation
(in each case, df = 7). For both COMPARE strategies
(represented in Figure 2(a) and 2(b)), we find a sig-
nificant negative correlation for the average number
of connectives used in realizations with a given rat-
ing (C2: r =  0.97, p < 0.01; and C3: r =  0.93,
p < 0.01). These correlations indicate that judges?
ratings decreased as the average frequency of the
connectives increased.
Further analysis of the individual correlations
used in the comparative strategies show that there is
a significant negative correlation for however (C2:
r =  0.91, p < 0.01; and C3: r =  0.86,
p < 0.01) and on the other hand (C2: r =  0.89,
p < 0.01; and C3: r =  0.84, p < 0.01) in both
COMPARE strategies. In addition, in COMPARE-3,
the frequencies of while and but are also signifi-
cantly and strongly negatively correlated with the
judges? ratings (r =  0.86, p < 0.01 and r =
 0.90, p < 0.01, respectively), though there is no
such correlation between the use of these connec-
tives and their ratings in COMPARE-2.
Added together, all the contrastive connectives
show strong, significant negative correlations be-
tween their average frequencies and judges? ratings
for both comparative strategies (C2: r =  0.93,
p < 0.01; C3:r =  0.88, p < 0.01).
Interestingly, unlike in the COMPARE strategies,
there is a positive correlation (r = 0.73, p > 0.05)
between the judges? ratings and the average fre-
quency of all connectives used in the RECOMMEND
strategy (see Figure 2(c)). Since this strategy only
uses and, since, and because and does not utilize any
contrastive connectives, this gives further evidence
that only contrastive connectives are dispreferred.
2.3 N-gram Ranker and Features
To acertain whether these contrastive connectives
are being learned by the ranker, we re-implemented
the n-gram ranker using SVM-light (Joachims,
77
Figure 1: Some alternative (Alt) realizatio s of SPaRKy se tence plans from a COMPARE2 (C2) plan,
with averaged human ratings (Rating; 5 = highest rating) and ranks (Rank; 1 = top ranked) assigned by
an n-gram ranker (Nakatsu, 2008)
Generating with Discourse Combinatory Categorial Grammar / 53
contrast
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
W
W
W
W
W
W
W
W
W
W
W
W
W
W
W
W
W
evidence
l
l
l
l
l
l
l
l
l
L
L
L
L
L
L
L
evidence
l
l
l
l
l
l
l
l
l
L
L
L
L
L
L
L
nucleus:(1?)
assert-summary
(mod:good)
infer
o
o
o
o
o
o
o
O
O
O
O
O
O
O
O
nucleus:(2?)
assert-summary
(mod:mediocre)
{infer|contrast}
o
o
o
o
o
o
o
o
O
O
O
O
O
O
O
O
nucleus:<1>
assert-com-decor
(mod:decent)
nucleus:<3>
assert-com-food-
quality (mod:very good)
YY
nucleus:<2>
assert-com-decor
(mod:decent)
nucleus:<4>
assert-com-food-
quality (mod:mediocre)
bb bb
merely
additive
merely
FIGURE 31 SPaRKy tp-tree altered with new relations and summary
statements, corresponding to Example 50.
in bold font. Lastly, the connectives also and only, which represent the
additional relatio s, additive and merely, respectively, are indicated
in small caps.
(50) (1?): Soni Ros is a good restaurant.
<1>: It has decent decor and
<3>: very good food quality.
(2?): However, Bienvenue is just a mediocre restaurant.
<2>: While it also has decent decor,
<4>: it only has mediocre food quality.
7 Related Work
In terms of its discourse theoretical basis, DCCG is most closely re-
lated to D-LTAG. In general, as Webber (2006) observes, discourse
grammars vary in their theoretical style, from wholly based on de-
pendency relations (e.g. Halliday and Hasan 1976) to adherence to a
completely constituent-based model (e.g. Rhetorical Structure Theory
[RST], Mann and Thompson 1988; Linguistic Discourse Model, Polanyi
1988, Polanyi and van den Berg 1996). Dependency-based discourse
theories are advantageous because they allow discourse relations to ex-
ist between non-adjacent discourse units, lifting restrictions on which
clauses can serve as discourse arguments of a given relation. Compu-
(1?): Sonia Rose is a good restaurant overall.
<1>: It has decent decor and
<3>: very good food quality.
(2?): However, Bienvenue is just a mediocre
restaurant on the whole.
<2>: While it also has decent decor,
<4>: it only has mediocre food quality.
Figure 2: Modified SPaRKy text plan for text with new relations and summary statements intended to
enhance contrast (Nakatsu and White, 2010)
32
Generating with Discourse Combinatory Categorial Grammar / 25
ot1h, A. however, B. otoh, C. however, D.
ts
ot1h
ts
however
ts
otoh
ts
however
TC TC
ts
CUE
\
?
ts
CUE
ts
CUE
\
?
ts
CUE
< <
ts
ot1h
ts
otoh
TC
ts
nil
/
?
ts
otoh
>
ts
nil
TC
turn
nil
FIGURE 16 A DCCG derivation of nested contrast relations
Returning now to the intrasentential conjunctions that express con-
trast, their categories remain the same as in the preceding section, ex-
cept for the addition of the requirement that they combine with clauses
having nil values for the cue feature:
(39) a. {while, but } ` s
e,nil
\
?
s
e
1
,nil
\
?
punc
,
/
?
s
e
2
,nil
:
@
e
(contrast-rel ^ hArg1ie
1
^ hArg2ie
2
)
b. while ` s
e,nil
/
?
s
e
2
,nil
/
?
punc
,
/
?
s
e
1
,nil
:
@
e
(contrast-rel ^ hArg1ie
1
^ hArg2ie
2
)
Since these categories do not need to look outside the sentence to find
both of their discourse arguments, they do not change the cue values
of their result categories.
To conclude this section, we address the question of whether it is a
necessary move to employ unary type-changing rules in order to handle
intersentential discourse connectives in CCG. As noted in the preceding
section, the lexicalized categories for connectives o?ered therein suggest
that there is no problem in principle with devising a purely lexicalized
approach to discourse connectives; accordingly, the cue threading ap-
proach presented in this section appears to yield grammars with cov-
erage equivalent to purely lexicalized alternatives. Nevertheless, as we
have seen, the purely lexicalized approach leads to a proliferation of lex-
ical category ambiguity, and while lexical rules might be employed to
systematically assign the necessary lexical categories, the cue threading
approach is clearly more economical. Similar considerations led Hock-
enmaier and Steedman (2002, 2007) to make extensive use of type-
changing rules in their broad coverage grammar of English, indicating
that such rules have an important role to play in practical grammars.
Hockenmaier and Steedman further argued that the formal power of
the system is una?ected as long as (i) only a finite number of unary
rules are employed and (ii) the rules are designed so that they cannot
recursively apply to their own output, as is the case here.
Figure 3: DCCG derivation of nested contrast re-
lations (Nakatsu and White, 2010)
26 / LiLT volume 4, issue 1 September 2010
it also as poor decor
np s
CUE
\np/
?
(s
CUE
\np) s
nil
\np
nom
>
s
nil
\np
nom
<
s
nil
FIGURE 17 A DCCG derivation of a clause including the discourse
adverbial also.
5.3 Discourse Adverbials and Anaphora Resolution
Unlike structural connectives, which find their discourse arguments via
cue threading, discourse adverbials find one argument syntactically,
and the other through anaphora resolution. To illustrate how DCCG
accomplishes this, consider (1) from Section 2, repeated below:
(1) b
1
: Bienven e is a mediocre restaurant.
h
1
: It has poor decor and mediocre food quality.
b
3
: However, Sonia Rose is a good restaurant.
h
2
: While it also has poor decor,
h
3
: it has excellent food quality.
As illustrated by the derivation of the clause for h
2
in Figure 17, the pre-
verbal modifier category for also in (40c) below takes a VP category
s
e,CUE
\np as its argument and returns a VP category as its result,
adding an additive relation to the semantics.
(40) a. also ` s
e,CUE
/
?
s
e,CUE
/
?
punc
,
:
@
e
(hModi(a ^ additive-rel ^ hArg1ie
1
))
b. also ` s
e,CUE
\
?
s
e,CUE
\
?
punc
,
:
@
e
(hModi(a ^ additive-rel ^ hArg1ie
1
))
c. also ` s
e,CUE
\np/
?
(s
e,CUE
\np) :
@
e
(hModi(a ^ additive-rel ^ hArg1ie
1
))
Since discourse adverbials such as also do not necessarily find their dis-
course arguments in structurally adjacent text segments, they do not
use cue threading. Instead, the cue value on discourse adverbials is left
underspecfied, as seen in all the lexical entries for also in (40). These
underspecified values then unify with the cue value of the input cat-
egory, threading any undischarged structural connectives through. In
this way, a discourse adverbial and a structural connective can appear
on the same clause (e.g. However, Bienvenue also has good decor).
In our example, the underspecified cue value of the argument cate-
gory in (40c) is unified with the nil cue value from the input category
Figure 4: DCCG derivation of a clause with
the discourse adverbial also (Nakatsu and White,
2010)
jkumar, 2009) in order to generate texts that vary
in size fro single sen ences to entire paragraphs.
In DCCG, the technique of cue threa i g is
used to all w structural connectives?including
paired ones such as on the one hand . . . on the
other hand?to project beyond the sentence level,
while allowing no more than one to be active at
a time. In this way, structural connectives can
be nested, as sketched in Figu e 3, but cannot
cross. In the figure, the value of the cue featur for
each text segment (ts) is shown (where ot1h and
otoh abbreviate on the one hand and on the other
h nd); th se cue values can be propagated through
a derivation, allowing the discourse relations to
project, but must be discharged (to nil) in a com-
plete d rivation, thereby nsuring that the intended
discourse relations are actually realized. By con-
trast, discourse adverbials introduce their relations
anaphorically and are transparent to cue thread-
ing, as sketched in Figure 4, making u of typical
adverb categories syntactically. See Nakatsu and
White (2010) for further details.
3 Crowd Sourcing Ratings
To collect human judgements from a diverse group
of speakers of US English, we used Amazon?s
Mechanical Turk service (AMT) to run two ex-
periments. In the first experiment, subjects rated
the naturalness of 174 passages used in Walker et
al.?s (2007) study. As detailed in Section 5, this
validation experiment confirmed that the judge-
ments collected on AMT correlate with those of
the raters in Walker et al?s (2007) study. Our sec-
ond experiment collected ratings on 300 passages
realized with modifications for better contrast ex-
pression (WITHMODS) and 300 passages without
these modifications (NOMODS), both realized us-
ing OpenCCG. While this does not admit a direct
comparison to the realizations produced by Walker
et al (2007), this controls for differences between
the generators other than the variable of interest:
the contrastive enhancements. In addition to these
materials, five passages from the SRC were seen
by all subjects to control for anomalous subject be-
havior.
3.1 Survey Format
Each survey used demographic questions to de-
termine the native speaker status of the subject.
Instructions for completing comprehension ques-
tions and rating realizations followed the demo-
graphic questions.
3
Each subject saw fifteen stim-
uli, each consisting of a sample user query and the
target passage as in Figure 5. After reading the
stimulus, the subject answered a yes-or-no com-
prehension question (see ?3.2). Finally the subject
rated the naturalness of the passage on a seven-
point Likert scale ranging from very unnatural to
very natural. At the survey?s conclusion, the sub-
ject could offer free-form feedback, explain their
r sponses, or ask questions of the researchers. The
average completion time across all experiments
was about ten minutes.
Passage selection is detailed in ?3.3 and ?3.4.
3.2 Quality Control
We used three strategies to filter out low-quality
responses from AMT subjects.
Comprehension Questions A template-based
yes-or-no question (exemplified in Figure 5) fol-
lowed each passage. Subjects who answered less
than 75% of these questions correctly were re-
jected and not paid, in accordance with the pro-
tocol approved by our human subjects review
board. Responses from three subjects were ex-
cluded from analysis on this basis.
Uniform Ratings When a subject gave the
same rating for all passages in a given survey (and
in disagreement with other subjects), we took this
to mean that the subject was paying attention only
3
These materials, along with the generated passages
and their ratings are available at http://www.ling.
ohio-state.edu/?mwhite/data/enlg13/.
33
Figure 5: Sample survey stimulus and comprehension question
Method # subjects excluded
Comprehension Questions 3
Uniform Answers 1
SAME5 0
Native Speaker Status 2
Table 1: Number of subjects excluded based on
quality control measures or native language.
to the comprehension questions that ensured pay-
ment. Only one subject was excluded on this ba-
sis, though they were still paid for answering the
comprehension questions correctly.
SAME5 Passages Five passages were chosen
from the original SRC realizations for which the
original ratings (from Walker et al 2007) were
identical for both judges. The passages were se-
lected such that the first and third authors of this
paper agreed with the general valence and rela-
tive rankings of the passages. That is, we took
two unambiguously bad realizations, two unam-
biguously good realizations, and one realization
near the middle of the spectrum to represent a gold
standard for rating to compare subjects against. If
any subject?s ratings on these five passages were
clear outliers, we could remove that subject?s data
for anomalous behavior, but this measure proved
unnecessary for the subjects in the present study.
3.3 Validating AMT
Data Selection In this experiment, we sampled
174 of the 1757 realizations from the SRC rated
by subjects A and B in Walker et al?s (2007) ex-
periment.
The SRC realizations were divided randomly
into two groups. Within one group, realizations
were labelled by subject A?s rating for that real-
ization. Subject B?s rating was used for the other
group. Taking the poles of the rating scale and its
midpoint, the realizations were further partitioned
into six sets: realizations rated 1, 3, and 5 by sub-
ject A and realizations rated 1, 3, and 5 by subject
B. This division of the data ensured that the re-
alizations used would cover the full spectrum of
ratings while being representative of the SRC rat-
ings with respect to, e.g., inter-annotator ratings
correlations.
From each of these six sets, we chose 10 COM-
PARE2, 10 COMPARE3, and 10 RECOMMEND re-
alizations,
4
each of these groups representing a
different realization task in the SRC. The COM-
PARE2 and COMPARE3 tasks involved the com-
parison of two restaurants or three or more restau-
rants, respectively. In the RECOMMEND context,
the sytem had to generate a recommendation for a
single restaurant.
Subject Demographics Thirty-six subjects re-
sponded to this survey initially, but one was re-
jected based on a failure to answer the compre-
hension questions and data from another had to be
excluded for non-native speaker status. Two addi-
tional subjects were recruited to replace their data.
This resulted in a subject pool with a mean age
(std. dev.) of 34.67 (9.35) years. Twenty-four sub-
jects identified as female and twelve identified as
male. Each subject received $2.50 for the survey,
estimated to take approximately 20 minutes.
3.4 Rating OpenCCG Realizations
Data Selection We selected 15 content plans
(CPs) from the SRC where the use of the con-
trastive modifiers was licensed: five COMPARE2,
five COMPARE3, and five RECOMMEND CPs.
Each of the 112 textplans (TPs) that produced
4
Except that subject A used the rating ?5? less than subject
B. To compensate, we used as many 5-point ratings as were
available from subject A and then filled in the remainder of
the 10 slots with realizations rated ?4?. We mirrored these
selections in the data from subject B for consistency.
34
the SRC realizations for these CPs was then pre-
processed for realization in OpenCCG both with
contrast enhancements (WITHMODS) and without
them (NOMODS).
Both structural choices and ordering choices
are encoded in these TPs.
5
Structural choices in-
clude decisions about how to group the restau-
rant properties to be expressed, such as deciding
whether to describe one restaurant in its entirety
and then the other (i.e. a serial structure) or al-
ternating between one restaurant and the other, di-
rectly contrasting particular attributes (i.e. a back-
and-forth structure). Ordering choices fixed the
order of presentation of restaurant attributes in se-
rial plans and the order of presentation of attribute
contrasts in back-and-forth plans. As discussed in
?6, there turn out to be interesting interactions be-
tween these aggregation choices and the contrast
enhancements, interactions which we did not ex-
plore directly in this experiment.
Processing each TP produced a different LF for
each possible combination of aggregation choices
and contrastive modifications, resulting in approx-
imately 41k logical forms (LFs) for the TPs WITH-
MODS and 88k LFs for the TPs with NOMODS.
6
Each realization received two language model
(LM) scores, one based on the semantic classes
used during realization (LM
SC
) and one based on
the Gigaword corpus (LM
GW
). LM
SC
used a tri-
gram model over modified texts based on the SRC
where specific entities (e.g. restaurant names like
Caffe Buon Gusto) were replaced with their se-
mantic class (e.g. RESTAURANT). The LM scores
were normalized by CP, such that the scores for a
given CP summed to 1 in each LM. These were
then linearly combined with weights slightly pre-
ferring the LM
SC
score to produce a combined
LM score for each realization.
Sampling then proceeded without replacement,
weighted by the combined LM score for each real-
ization. For the NOMODS sample, 20 realizations
were chosen this way, but, in the WITHMODS sam-
ple, a series of regular expression filters were used
to ensure adequate representation of the modifica-
tions in the surveys. These filters selected (without
5
This differs from Walker et al (2007), wherein reorder-
ings were allowed in mapping from tp-trees to sp-trees and
d-trees.
6
In future work we will explore a probabilistic rather than
exhaustive mapping algorithm to produce only LFs that are
more likely to result in more fluent realizations?not unlike
the weighted aggregation done by Walker et al?s (2007) sen-
tence plan generator.
replacement) 10 realizations such that every con-
trastive modification licensed by a particular CP
was represented, leaving 10 realizations to be se-
lected by weighted sampling without replacement.
This process resulted in 300 passages in each of
the two conditions (WITHMODS, NOMODS): 20
realizations for each of the 15 CPs. Each survey
included 5 realizations WITHMODS paired by CP
with 5 realizations with NOMODS as well as the
SAME5 realizations. As noted earlier, pairing real-
izations in this way helps to control for differences
in the variety of aggregation choices and surface
realizations used in the SRC as opposed to our
SRC-inspired grammar for OpenCCG.
Subject Demographics Sixty-eight subjects
responded to these 180 surveys initially. Subjects
were allowed to complete up to six distinct sur-
veys. One subject?s data was excluded for non-
native status and another?s was excluded on the
basis of uniform ratings (as detailed in ?3.2). To
compensate for the eight surveys completed by
these subjects and ten surveys mistakenly admin-
istered in draft format, we recollected data for 18
of the 180 surveys. This resulted in a final pool of
80 subjects with an average (std. dev.) age 37.15
(13.5) years. Forty identified as female, thirty-
nine identified as male, and one identified as non-
gendered.
Because subjects in the validation study com-
pleted the survey in about 10 minutes on average
with a standard deviation of about 5 minutes, we
scaled the pay to $2.00 per survey in this experi-
ment. Since subjects could participate in this ex-
periment multiple times, they could receive up to
$12.00 for their contribution.
4 Training a Text Ranker
To perform the ranking, we trained a basic n-
gram ranker using SVM
light
in preference ranking
mode.
7
We used the average ratings obtained in ?3
as target value.
The feature set was composed of 2 types of fea-
tures. The first feature type are the two language
model scores from ?3.4, LM
SC
and LM
GW
. The
second feature type consisted of n-gram counts.
We indexed the unigrams and bigrams in each cor-
pus and used each as a feature whose value was the
number of times it appeared in a given realization.
We trained the ranker on, and extracted n-gram
7
SVM
light
is an implementation of support vector ma-
chines by (Joachims, 2002).
35
12
3
4
5
6
7
1 2 3 4 5Average rating from Walker et al (2007)
Ave
rag
e ra
ting
 fro
m A
MT
 su
bjec
ts
Figure 6: Average ratings from our experiment
and Walker et al (2007), accompanied by a line
of best fit. Jitter (0.1) applied to each point mini-
mizes overlap.
features from, 3 different corpora drawn from the
data selection in ?3.4. The first corpus contains
299 selections WITHMODS (1 selection was dis-
carded for only being rated once), the second cor-
pus contains 300 selections with NOMODS, and
the third corpus contains BOTH of the first two cor-
pora combined.
To train and test the ranker, we performed 15-
fold cross-validation on each corpus. Within each
training fold, we had 14 training examples, corre-
sponding to 14 CPs. Each training example con-
sisted of all of a given CP?s realizations and their
ratings. After training, the realizations for the re-
maining CP were ranked.
In order to evaluate the ranker, we used the
TopRank metric (Walker et al, 2007). For each
of the ranked CP realization sets, we extracted the
target values (i.e. the average rating given by sub-
jects) of the highest ranked realization. We then
averaged the target scores of all of the top-ranked
realizations across the 15 training folds to produce
the Top Rank metric. The oracle best score is
the score of the highest rated realization, as de-
termined by the average score assigned to that re-
alization by the subjects.
5 Results
Validation Figure 6 shows the correlation be-
tween the average ratings of our subjects on AMT
and the average ratings assigned by subjects A and
B in Walker et al (2007). This correlation was
0.31 (p < 0.01, Kendall?s tau), while the corre-
lation between subjects A and B was only 0.28
BOTH WITHMODS NOMODS
human 6.61 (0.28) 6.46 (0.43) 6.49 (0.26)
bigram 6.00 (0.58) 5.62 (0.83) 5.51 (1.02)
Table 2: TopRank scores and standard deviations
for the oracle (human) & bigram (bigram) ranks.
(p < 0.01, Kendall?s tau). On this basis we con-
clude that using AMT workers as subjects to rate
sentences for their naturalness is at least as rea-
sonable as having two expert annotators labelling
realizations for their overall quality.
SAME5 Comparison There was no signifi-
cant difference (p = 0.16, using Welch?s t-test)
between the scores given to the SAME5 stimuli
in the two experiments,
8
indicating that subjects
used the rating scale similarly in both experiments.
The mean ratings for the rest of the validation re-
alizations was 5.31 (1.43) and the mean for the
OpenCCG-based realizations in the ranking exper-
iment was 4.96 (1.51), which is significantly lower
according to Welch?s t-test (p < 0.01). This high-
lights the underlying differences between the two
generation systems, validating our choice to use
OpenCCG for both the WITHMODS and NOMODS
realizations to better examine the impact of the
contrast enhancements.
Ranking Table 2 reports the oracle results,
along with our ranker?s results, using the TopRank
metric. Most indicative of the benefit of the con-
trastive enhancements is the performance of the
oracle score for the BOTH (6.61) condition com-
pared to the NOMODS condition (6.49), which is
significantly higher according to a paired t-test
(p = 0.01).
We also found that the bigram ranker with the
averaged raw ratings was better at predicting the
top rank of the combined (BOTH) corpus (6.00 vs.
oracle-best of 6.61) than either of the other two,
and better on the WITHMODS condition (5.62)
than on the NOMODS condition (5.51). However,
a two-tailed t-test revealed that the difference was
not quite signficant between BOTH and NOMODS
at the conventional level (p = 0.06), though the
p-value did meet the 0.1 threshold sometimes em-
ployed in small-scale experiments. The perfor-
mance of the different rankers, as compared to the
oracle scores, can be seen in Figure 7.
These preliminary results with a simple ranker
8
Validation experiment mean (std. dev.) 4.89 (1.79) ver-
sus 5.10 (1.75) in the ranking experiment.
36
34
5
6
7
human bigramMethod
Top
Ran
k
Corpus
both
withMods
noMods
Figure 7: TopRank scores for each of the rankers
with standard error bars.
are promising, motivating future work on improv-
ing the ranker in addition to enlarging the dataset.
6 Discussion
To assess the impact of the enhancement options,
we performed a linear regression between the
contrast-related patterns we used for data selec-
tion and the normalized ratings, with scikit-learn?s
implementation of the Bayesian Ridge method of
regularizing weights.
9
In looking at examples,
we found that the number of discourse adverbials
appeared to be a factor, so we then added these
counts as features. The coefficients and corpus
counts appear in Table 3. The results show that
the discourse adverbials were effective some of
the time, especially when used sparingly and in
conjunction with while. The ?heavier? contrastive
connectives however and on the one/other hand
were dispreferred, perhaps in part because they
ended up appearing too often with small, single-
restaurant contrasts, as there were relatively few
examples of summary statements, most of which
were somewhat disfluent due to a medial choice
for overall / on the whole.
Table 4 shows examples that illustrate both suc-
cesses and remaining issues. At the top, two pairs
of examples are given where the normalized av-
erage ratings are higher with the inclusion of just
and only, and where the rating drops off greatly
when however is used with a lesser value and no
adverbial of this kind, as expected. At the bottom,
the first example shows one instance where the use
of multiple adverbials is dispreferred. A possible
9http://scikit-learn.org/stable/
modules/linear_model.html
pattern coeff count
| disc advb | = 1 0.23 102
while 0.19 38
also has 0.13 47
has . . . too 0.12 39
has only 0.09 43
while . . .disc advb 0.09 16
contrastive . . . overall 0.07 8
has just 0.04 46
however . . .disc advb 0.03 4
but -0.03 20
, however , -0.05 10
only has -0.06 30
has merely -0.11 46
on the whole -0.14 33
just has -0.16 29
merely has -0.16 8
| disc advb | = 2 -0.18 32
. however , -0.21 64
on the other hand -0.21 40
| disc advb | >= 3 -0.27 50
overall -0.29 34
on the one hand -0.36 22
Table 3: Coefficients of linear regression between
contrast-related patterns and normalized ratings,
along with pattern counts, where disc adv is one
of just, only, merely, also, too and contrastive is
one of while, however, on the one/other hand
factor here may be that in addition to there being
several similar adverbials in a row, they all involve
long-distance antecedents, which may be difficult
to process. Finally, the last example shows a real-
ization that receives a relatively high rating despite
the use of two adverbials; note, however, that since
this passage uses a back-and-forth text plan, the
antecedents of the adverbials are all very local.
10
Turning to the survey feedback, many subjects
provided insightful comments regarding the task.
The most frequent comment pointed out that our
comprehension questions sometimes precipitated
a false implicature: when asked if a restaurant had
decent decor, subjects commented that they felt
that answering ?no? meant implying that it had
terrible decor. Similar problems occurred when a
restaurant had, e.g., very good decor and the sub-
jects were asked if it had good decor. Despite oc-
casional deviations from our intended exact-match
interpretation of these questions, no subjects were
excluded for scoring too low as a result of this.
10
As one reviewer points out, there?s also an interaction be-
tween how attributes are aggregated and the ability to express
contrast. For example, contrasting the attributes for which a
restaurant scores highly with those for which it scores poorly
requires the aggregation of attributes with like valence, as in
?This restaurant has superb decor and very good service but
only mediocre food quality.? Our future work on aggregation
will explore this interaction as well.
37
Strategy Mods? Rating Realization
C2 Y 1.13 Da Andrea?s price is 28 dollars. Gene?s?s price is 33 dollars. Da Andrea has very good
food quality while Gene?s has just good food quality.
C2 N 0.73 Da Andrea?s price is 28 dollars. Gene?s?s price is 33 dollars. Da Andrea has very good
food quality while Gene?s has good food quality.
C2 Y 1.04 Da Andrea?s price is 28 dollars. Gene?s?s price is 33 dollars. Da Andrea has very good
food quality. However, Gene?s has only good food quality.
C2 N -0.63 Da Andrea?s price is 28 dollars. Gene?s?s price is 33 dollars. Da Andrea has very good
food quality. However, Gene?s has good food quality.
C3 Y -1.85 Daniel and Jo Jo offer exceptional value among the selected restaurants. Daniel, on the
whole, is a superb restaurant. Daniel?s price is 82 dollars. Daniel has superb decor. It has
superb service and superb food quality. Jo Jo, overall, is an excellent restaurant. Jo Jo?s
price is 59 dollars. Jo Jo just has very good decor. It just has excellent service. It has
merely excellent food quality.
C2 Y 1.12 Japonica?s price is 37 dollars while Dojo?s price is 14 dollars. Japonica has excellent food
quality while Dojo has merely decent food quality. Japonica has decent decor. Dojo has
only mediocre decor.
Table 4: Examples illustrating successful and problematic contrast enhancements
In order to elicit rankings at a variety of points
on the naturalness scale, our selection included a
number of realizations with lower quality over-
all, which subjects picked up on. For example,
one subject commented that, ?Repeatedly using
the name of each restaurant over and over in sim-
ple sentences make[s] almost all of these excerpts
sound horrifyingly awkward,? while another ob-
served, ?The constant [use] of more sentences, in-
stead of using conjunction words . . . makes it seem
as if the system is rambling and lost in though[t]
process.?
Several subjects also pointed out that it would
be more natural to discuss the cost of an average
meal at a restaurant than to state that a restau-
rant?s price is some particular number of dollars.
Though these domain-specific lexical preferences
are tangential to the focus of this paper, they sug-
gest that exploring options to expand the range
of realizations for more naturally expressing these
properties might be a fruitful direction for future
work.
In addition to expressing an explicit prefer-
ence for serial rather than back-and-forth text-
plans, subjects also commented that higher level
contrastive adverbials like however work better
when they are used sparingly at a high level, rein-
forcing the findings in our regressions. We also re-
ceived suggestions for future work improving the
expression of contrast: some subjects suggested
that using better and worse to make explicit com-
parisons between restaurants would improve the
naturalness, and one subject suggested explicitly
stating which restaurant is (say) the cheapest as in
White et al (2010).
7 Conclusions and Future Work
In this paper, we have shown using ratings gath-
ered on AMT that Nakatsu & White?s (2010) pro-
posed enhancements to the SPaRKy Restaurant
Corpus (Walker et al, 2007) for better express-
ing contrast do indeed make it possible to generate
better texts, and an initial experiment suggested
that even a basic n-gram ranker can do so automat-
ically. A regression analysis further revealed that
while using a few discourse adverbials sparingly
was effective, using too many discourse adverbials
had a negative impact, with antecedent distance
potentially an important factor. In future work, we
plan to improve upon this basic n-gram ranker to
take these observations into account and validate
these initial findings on a larger dataset. In the pro-
cess we will explore the interaction between con-
trast expression and aggregation and seek to bet-
ter model the felicity conditions for ?weighty? top
level adverbials such as however.
Acknowledgments
This work was supported in part by NSF grant
IIS-1143635. Special thanks to the anonymous
reviewers, the Clippers computational linguistics
discussion group at Ohio State, and to Mark Dras,
Francois Lareau, and Yasaman Motazedi at Mac-
quarie University.
References
Brigitte Grote, Nils Lenke, and Manfred Stede. 1995.
Ma(r)king concessions in English and German. In
Proc. of the Fifth European Workshop on Natural
Language Generation.
38
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proc. KDD.
Irene Langkilde and Kevin Knight. 1998. The practi-
cal value of n-grams in generation. In Proc. INLG-
98.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: Towards a functional
theory of text organization. TEXT, 8(3):243?281.
Chris Mellish, Alistair Knott, Jon Oberlander, and
Mick O?Donnell. 1998. Experiments using stochas-
tic search for text planning. In Proc. INLG-98.
Crystal Nakatsu and Michael White. 2006. Learning
to say it well: Reranking realizations by predicted
synthesis quality. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 1113?1120, Syd-
ney, Australia, July. Association for Computational
Linguistics.
Crystal Nakatsu and Michael White. 2010. Generat-
ing with discourse combinatory categorial grammar.
Linguistic Issues in Language Technology, 4(1):1?
62.
Crystal Nakatsu. 2008. Learning contrastive connec-
tives in sentence realization ranking. In Proceedings
of the 9th SIGdial Workshop on Discourse and Dia-
logue, pages 76?79, Columbus, Ohio, June. Associ-
ation for Computational Linguistics.
Mark Steedman. 2000. The Syntactic Process. MIT
Press.
Kristina Striegnitz. 2004. Generating Anaphoric Ex-
pressions ? Contextual Inference in Sentence Plan-
ning. Ph.D. thesis, University of Saalandes & Uni-
versit de Nancy.
Marilyn A. Walker, Owen C. Rambow, and Monica Ro-
gati. 2002. Training a sentence planner for spo-
ken dialogue using boosting. Computer Speech and
Language, 16:409?433.
M. A. Walker, S. J. Whittaker, A. Stent, P. Mal-
oor, J. D. Moore, M. Johnston, and G Vasireddy.
2004. Generation and evaluation of user tailored re-
sponses in multimodal dialogue. Cognitive Science,
28(5):811?840.
M. Walker, A. Stent, F. Mairesse, and Rashmi Prasad.
2007. Individual and domain adaptation in sentence
planning for dialogue. Journal of Artificial Intelli-
gence Research (JAIR), 30:413?456.
Bonnie Webber, Matthew Stone, Aravind Joshi, and
Alistair Knott. 2003. Anaphora and discourse struc-
ture. Computational Linguistics, 29(4).
Bonnie Webber. 2004. D-LTAG: Extending lex-
icalized TAG to discourse. Cognitive Science,
28(5):751?779.
Michael White and Rajakrishnan Rajkumar. 2009.
Perceptron reranking for CCG realization. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, pages 410?
419, Singapore, August. Association for Computa-
tional Linguistics.
Michael White, Robert A. J. Clark, and Johanna D.
Moore. 2010. Generating tailored, comparative de-
scriptions with contextually appropriate intonation.
Computational Linguistics, 36(2):159?201.
Michael White. 2006a. CCG chart realization from
disjunctive logical forms. In Proc. INLG-06.
Michael White. 2006b. Efficient Realization of Coor-
dinate Structures in Combinatory Categorial Gram-
mar. Research on Language and Computation,
4(1):39?75, June.
39
