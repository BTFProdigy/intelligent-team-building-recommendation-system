Proceedings of NAACL HLT 2007, Companion Volume, pages 41?44,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Exploring Affect-Context Dependencies for Adaptive System Development
Kate Forbes-Riley
Learning R&D Ctr.
Univ. Pittsburgh
Pittsburgh, PA 15260
forbesk@pitt.edu
Mihai Rotaru
Computer Science Dpt.
Univ. Pittsburgh
Pittsburgh, PA 15260
mrotaru@cs.pitt.edu
Diane J. Litman
Learning R&D Ctr.
Computer Science Dpt.
Univ. Pittsburgh
Pittsburgh, PA 15260
litman@cs.pitt.edu
Joel Tetreault
Learning R&D Ctr.
Univ. Pittsburgh
Pittsburgh, PA 15260
tetreaul@pitt.edu
Abstract
We use ?2 to investigate the context de-
pendency of student affect in our com-
puter tutoring dialogues, targeting uncer-
tainty in student answers in 3 automati-
cally monitorable contexts. Our results
show significant dependencies between
uncertain answers and specific contexts.
Identification and analysis of these depen-
dencies is our first step in developing an
adaptive version of our dialogue system.
1 Introduction
Detecting and adapting to user affect is being ex-
plored by many researchers to improve dialogue sys-
tem quality. Detection has received much atten-
tion (e.g., (Litman and Forbes-Riley, 2004; Lee and
Narayanan, 2005)), but less work has been done on
adaptation, due to the difficulty of developing re-
sponses and applying them at the right time. Most
work on adaptation takes a context-independent ap-
proach: use the same type of response after all in-
stances of an affective state. For example, Liu and
Picard (2005)?s health assessment system responds
with empathy to all instances of user stress.
Research suggests, however, that it may be more
effective to take a context-dependent approach: de-
velop multiple responses for each affective state,
whose use depends on the state?s context. E.g., in the
tutoring domain, Pon-Barry et al (2006) show that
human tutors use multiple responses to uncertain
student answers, depending on the answer?s correct-
ness and prior context. In the information-seeking
domain, it is commonly believed that while an apol-
ogy is a good default response to user frustration (as
in (Klein et al, 2002)), one context requires a differ-
ent response: after several frustrated user turns, the
call should be forwarded to a human operator.
A context-dependent approach to affect adapta-
tion must address 2 issues: in what contexts to adapt,
and what responses to use there. This paper ad-
dresses the first issue and targets student uncertainty
in our computer tutoring dialogues. Although our di-
alogues have a Question-Answer format, our system
contains 275 tutor questions. Treating each question
as a context is too labor-intensive for adaptation de-
velopment and creates a data sparsity issue. Instead
we treat automatically monitorable question proper-
ties as contexts. Here we examine 3 contexts: the di-
alogue act interpretation, and the discourse structure
depth and transition, of the prior tutor question. We
use ?2 to investigate the context dependency of un-
certain student answers (correct and incorrect). Our
results show that some contexts are significantly as-
sociated with uncertain answers. Our next step will
be to use these significant dependencies to develop
system responses to uncertain answers in these con-
texts. These responses will be based both on our
hypotheses about why uncertainty is associated with
these contexts, and on analyses of human tutor re-
sponses to uncertain answers in these contexts.
2 Student Uncertainty and Prior Context
ITSPOKE is a speech-enabled version of a text-
based tutoring system (VanLehn et al, 2002). The
student first types an essay answering one of 5 qual-
itative physics problems. ITSPOKE parses the es-
say, extracts dialogue topics concerning misconcep-
tions, then engages the student in dialogue. In this
study we used 2 ITSPOKE corpora containing 4590
student turns over 185 dialogues from 37 students.
Figure 1 shows an annotated dialogue excerpt.
41
ITSPOKE4: Good. Does the packet have an acceleration? If yes please specify its direction. [SAQ, Advance, D1]
STUDENT4: horizontally forward yes [non-uncertain, incorrect]
ITSPOKE5: Well... To find the packet?s acceleration we will use Newton?s 2nd law. What is the definition of this law? [LAQ,
Push, D2]
STUDENT5: an object in motion tends to stay in motion until it?s act by an outside force [uncertain, incorrect]
ITSPOKE6: Well... Newton?s 2nd law says that the net force acting on an object equals the mass times the acceleration of theobject. We can define Newton?s 2nd law as F=m*a. Hence what can we determine about the acceleration of the packet given
that we know the net force acting on it is not zero and points straight down? [DAQ, Advance, D2]
STUDENT6: I don?t know [uncertain, incorrect]
ITSPOKE7: Well... < further discussion of Newton?s 2nd law...> So once again on the original question. Does the packet have anacceleration? If yes please specify its direction.[SAQ, PopUp, D1]
Figure 1: ITSPOKE Annotated Dialogue Excerpt
2.1 Uncertainty and Correctness Annotations
ITSPOKE, like most computer tutors, responds only
to student correctness. ITSPOKE labels each an-
swer as correct or incorrect1 . If correct, ITSPOKE
moves on to the next question. If incorrect, then for
questions on simple topics, ITSPOKE gives the cor-
rect answer and moves on, while for questions on
complex topics (ITSPOKE4, Figure 1), ITSPOKEinitiates a sub-dialogue with remediation questions
(ITSPOKE5 - ITSPOKE6), before moving on.
Recent computer tutoring research has shown in-
terest in responding to student affect2 over cor-
rectness. Uncertainty is of particular interest: re-
searchers hypothesize that uncertainty and incorrect-
ness each create an opportunity to learn (VanLehn
et al, 2003). They cannot be equated, however.
First, an uncertain answer may be correct or incor-
rect (Pon-Barry et al, 2006). Second, uncertainty in-
dicates that the student perceives a possible miscon-
ception in their knowledge. Thus, system responses
to uncertain answers can address both the correct-
ness and the perceived misconception.
In our ITSPOKE corpora, each student answer
has been manually annotated as uncertain or non-
uncertain3 : uncertain is used to label answers ex-
pressing uncertainty or confusion about the material;
non-uncertain is used to label all other answers.
1We have also manually labeled correctness in our data;
agreement between ITSPOKE and human is 0.79 Kappa (90%).
2We use ?affect? to cover emotions and attitudes that affect
how students communicate. Although some argue ?emotion?
and ?attitude? should be distinguished, some speech researchers
find the narrow sense of ?emotion? too restrictive because it ex-
cludes states where emotion is present but not full-blown, in-
cluding arousal and attitude (Cowie and Cornelius, 2003).
3A second annotator relabeled our dataset, yielding inter-
annotator agreement of 0.73 Kappa (92%).
2.2 Context Annotations
Here we examine 3 automatically monitorable tutor
question properties as our contexts for uncertainty:
Tutor Question Acts: In prior work one annotator
labeled 4 Tutor Question Acts in one ITSPOKE cor-
pus (Litman and Forbes-Riley, 2006)4: Short (SAQ),
Long (LAQ), and Deep Answer Question (DAQ) dis-
tinguish the question in terms of content and the type
of answer it requires. Repeat (RPT) labels variants
of ?Can you repeat that?? after rejections. From
these annotations we built a hash table associating
each ITSPOKE question with a Question Act label;
with this table we automatically labeled ITSPOKE
questions in our second ITSPOKE corpus.
Discourse Structure Depth/Transition: In prior
work we showed that the discourse structure Depth
and Transition for each ITSPOKE turn can be au-
tomatically annotated (Rotaru and Litman, 2006).
E.g., as shown in Figure 1, ITSPOKE4,7 have depth1 and ITSPOKE5,6 have depth 2. We combine lev-els 3 and above (3+) due to data sparsity. 6 Transi-
tion labels represent the turn?s position relative to the
prior ITSPOKE turn: NewTopLevel labels the first
question after an essay. Advance labels questions at
the same depth as the prior question (ITSPOKE4,6).
Push labels the first question in a sub-dialogue
(after an incorrect answer) (ITSPOKE5). After asub-dialogue, ITSPOKE asks the original question
again, labeled PopUp (ITSPOKE7), or moves on tothe next question, labeled PopUpAdv. SameGoal la-
bels both ITSPOKE RPTS (after rejections) and re-
peated questions after timeouts.
4Our Acts are based on related work (Graesser et al, 1995).
Two annotators labeled the Acts in 8 dialogues in a parallel hu-
man tutoring corpus, with agreement of 0.75 Kappa (90%).
42
3 Uncertainty Context Dependencies
We use the ?2 test to investigate the context depen-
dency of uncertain (unc) or non-uncertain (nonunc)
student answers that are correct (C) or incorrect (I).
First, we compute an overall ?2 value between each
context variable and the student answer variable. For
example, the Question Act variable (QACT) has 4
values: SAQ, LAQ, DAQ, RPT. The answer vari-
able (SANSWER) also has 4 values: uncC, uncI,
nonuncC, nonuncI. Table 1 (last column) shows the
?2 value between these variables is 203.38, which
greatly exceeds the critical value of 16.92 (p? 0.05,
df=9), indicating a highly significant dependency.
Significance increases as the ?2 value increases.
Dependency Obs. Exp. ?2
QACT ? SANSWER 203.38
LAQ ? uncC + 72 22 133.98
LAQ ? uncI + 43 27 11.17
LAQ ? nonuncC - 96 151 50.13
LAQ ? nonuncI = 48 60 3.10
DAQ ? uncC = 22 22 0.01
DAQ ? uncI + 37 27 4.57
DAQ ? nonuncC = 135 149 3.53
DAQ ? nonuncI = 63 59 0.35
SAQ ? uncC - 285 328 41.95
SAQ ? uncI - 377 408 17.10
SAQ ? nonuncC + 2368 2271 66.77
SAQ ? nonuncI - 875 898 5.31
RPT ? uncC - 7 14 4.15
RPT ? uncI = 22 18 1.25
RPT ? nonuncC - 70 98 20.18
RPT ? nonuncI + 70 39 33.59
Table 1: Tutor Question Act Dependencies (p?.05:
critical ?2=16.92 (df=9); critical ?2=3.84 (df=1))
However, this does not tell us which variable val-
ues are significantly dependent. To do this, we create
a binary variable from each value of the context and
answer variables. E.g., the binary variable for LAQ
has 2 values: ?LAQ? and ?Anything Else?, and the
binary variable for uncC has 2 values: ?uncC? and
?Anything Else?. We then compute the ?2 value be-
tween the binary variables. Table 1 shows this value
is 133.98, which greatly exceeds the critical value of
3.84 (p? 0.05, df=1). The table also shows the ob-
served (72) and expected (22) counts. Comparison
determines the sign of the dependency: uncC occurs
significantly more than expected (+) after LAQ. The
?=? sign indicates a non-significant dependency.
Table 1 shows uncertain answers (uncC and uncI)
occur significantly more than expected after LAQs.
In contrast, non-uncertain answers occur signifi-
cantly less (-), or aren?t significantly dependent (=).
Also, uncI occurs significantly more than expected
after DAQs. We hypothesize that LAQs and DAQs
are associated with more uncertainty because they
are harder questions requiring definitions or deep
reasoning. Not surprisingly, uncertain (and incor-
rect) answers occur significantly less than expected
after SAQs (easier fill-in-the-blank questions). Un-
certainty shows very weak dependencies on RPTs.
Table 2 shows that Depth1 is associated with more
correctness and less uncertainty overall. Both types
of correct answer occur significantly more than ex-
pected, but this dependency is stronger for nonuncC.
Both incorrect answers occur significantly less than
expected, but this dependency is stronger for uncI.
Dependency Obs. Exp. ?2
Depth# ? SANSWER 53.85
Depth1 ? uncC + 250 228 5.46
Depth1 ? uncI - 230 283 27.55
Depth1 ? nonuncC + 1661 1579 24.73
Depth1 ? nonuncI - 575 625 12.66
Depth2 ? uncC - 78 101 7.80
Depth2 ? uncI + 156 125 11.26
Depth2 ? nonuncC - 664 699 5.65
Depth2 ? nonuncI + 304 277 4.80
Depth3+ ? uncC = 58 57 0.05
Depth3+ ? uncI + 93 70 9.76
Depth3+ ? nonuncC - 344 391 15.66
Depth3+ ? nonuncI + 177 155 4.94
Table 2: Depth Dependencies (p?.05: critical
?2=12.59 (df=6); critical ?2=3.84 (df=1))
At Depths 2 and 3+, correct answers occur sig-
nificantly less than expected or show no signifi-
cance. Incorrect answers occur significantly more
than expected, and the dependencies are stronger for
uncI. We hypothesize that deeper depths are asso-
ciated with increased uncertainty and incorrectness
because they correspond to deeper knowledge gaps;
uncertainty here may also relate to a perceived lack
of cohesion between sub-topic and larger solution.
Table 3 shows Pushes have the same dependen-
cies as deeper depths (increased uncertainty and in-
correctness); however, here the uncI dependency is
only slightly stronger than nonuncI, which suggests
that increased uncertainty at deeper depths is more
reliably associated with remediation questions after
the Push. Although uncertainty shows only weak
43
dependencies on PopUps, after PopUpAdvs the uncI
dependency is strong, with uncI occurring more than
expected. We hypothesize that this dependency re-
lates to students losing track of the original ques-
tion/larger topic. Uncertainty shows only weak de-
pendencies on Advances. After NewTopLevels, in-
correct answers occur less than expected, but the de-
pendency is stronger for nonuncI. After SameGoals,
incorrect answers occur more than expected, but the
dependency is stronger for nonuncI. Compared with
the RPT results, the SameGoal results suggest stu-
dents feel increased uncertainty after timeouts.
Dependency Obs. Exp. ?2
TRANS ? SANSWER 190.97
Push ? uncC = 68 57 2.89
Push ? uncI + 100 70 16.37
Push ? nonuncC - 313 392 44.51
Push ? nonuncI + 193 155 14.13
PopUp ? uncC - 23 36 5.89
PopUp ? uncI - 32 45 4.68
PopUp ? nonuncC = 260 251 0.81
PopUp ? nonuncI + 117 99 4.47
PopUpAdv ? uncC = 8 13 2.50
PopUpAdv ? uncI + 32 17 16.22
PopUpAdv ? nonuncC - 76 93 7.72
PopUpAdv ? nonuncI = 44 37 1.89
Advance ? uncC = 217 205 1.70
Advance ? uncI - 223 254 9.06
Advance ? nonuncC + 1465 1416 8.66
Advance ? nonuncI - 530 560 4.51
NewTopLevel ? uncC = 53 54 0.04
NewTopLevel ? uncI - 49 67 6.47
NewTopLevel ? nonuncC + 463 375 57.33
NewTopLevel ? nonuncI - 80 148 47.63
SameGoal ? uncC = 17 21 0.70
SameGoal ? uncI + 43 25 14.24
SameGoal ? nonuncC - 92 152 44.25
SameGoal ? nonuncI + 92 56 31.43
Table 3: Transition Dependencies (p?.05: critical
?2=25.00 (df=15); critical ?2=3.84 (df=1))
4 Current Directions
We analyzed dependencies between uncertain stu-
dent answers and 3 automatically monitorable con-
texts. We plan to examine more contexts, such as
a Topic Repetition variable that tracks similar ques-
tions about a topic (e.g. gravity) across dialogues.
Our next step will be to use the significant de-
pendencies to develop system responses to uncer-
tain answers in these contexts. These responses will
be based both on our hypotheses about why uncer-
tainty is significantly associated with these contexts,
as well as on analyses of human tutor responses
in these contexts, using our human tutoring corpus,
which was collected with our first ITSPOKE corpus
using the same experimental procedure.
We also plan to investigate context dependencies
for other affective states, such as student frustration.
Acknowledgments
NSF (#0631930, #0354420 and #0328431) and
ONR (N00014-04-1-0108) support this research.
References
R. Cowie and R. R. Cornelius. 2003. Describing the
emotional states that are expressed in speech. Speech
Communication, 40:5?32.
A. Graesser, N. Person, and J. Magliano. 1995. Collabo-
rative dialog patterns in naturalistic one-on-one tutor-ing. Applied Cognitive Psychology, 9:495?522.
J. Klein, Y. Moon, and R. Picard. 2002. This computer
responds to user frustration: Theory, design, and re-sults. Interacting with Computers, 14:119?140.
C. M. Lee and S. Narayanan. 2005. Towards detect-
ing emotions in spoken dialogs. IEEE Transactions
on Speech and Audio Processing, 13(2), March.
D. Litman and K. Forbes-Riley. 2004. Predicting student
emotions in computer-human tutoring dialogues. In
Proc. ACL, pages 352?359.
D. J. Litman and K. Forbes-Riley. 2006. Correlations
between dialogue acts and learning in spoken tutoringdialogues. Natural Language Engineering, 12(2).
K. Liu and R. W. Picard. 2005. Embedded empathy
in continuous, interactive health assessment. In CHI
Workshop on HCI Challenges in Health Assessment.
H. Pon-Barry, K. Schultz, E. Bratt, B. Clark, and S. Pe-
ters. 2006. Responding to student uncertainty in spo-ken tutorial dialogue systems. International Journal
of Artificial Intelligence in Education, 16:171?194.
M. Rotaru and D. Litman. 2006. Exploiting discoursestructure for spoken dialogue performance analysis. In
Proceedings of EMNLP, Sydney, Australia.
K. VanLehn, P. W. Jordan, and C. P. Rose? et al 2002. Thearchitecture of Why2-Atlas: A coach for qualitativephysics essay writing. In Proceedings of ITS.
K. VanLehn, S. Siler, and C. Murray. 2003. Why doonly some events cause learning during human tutor-ing? Cognition and Instruction, 21(3):209?249.
44
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 193?200,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Dependencies between Student State and Speech Recognition  
Problems in Spoken Tutoring Dialogues 
 
 
Mihai Rotaru 
University of Pittsburgh 
Pittsburgh, USA 
mrotaru@cs.pitt.edu 
Diane J. Litman 
University of Pittsburgh 
Pittsburgh, USA 
litman@cs.pitt.edu 
 
  
 
Abstract 
Speech recognition problems are a reality 
in current spoken dialogue systems. In 
order to better understand these phenom-
ena, we study dependencies between 
speech recognition problems and several 
higher level dialogue factors that define 
our notion of student state: frustra-
tion/anger, certainty and correctness. We 
apply Chi Square (?2) analysis to a cor-
pus of speech-based computer tutoring 
dialogues to discover these dependencies 
both within and across turns. Significant 
dependencies are combined to produce 
interesting insights regarding speech rec-
ognition problems and to propose new 
strategies for handling these problems. 
We also find that tutoring, as a new do-
main for speech applications, exhibits in-
teresting tradeoffs and new factors to 
consider for spoken dialogue design. 
1 Introduction 
Designing a spoken dialogue system involves 
many non-trivial decisions. One factor that the 
designer has to take into account is the presence 
of speech recognition problems (SRP). Previous 
work (Walker et al, 2000) has shown that the 
number of SRP is negatively correlated with 
overall user satisfaction. Given the negative im-
pact of SRP, there has been a lot of work in try-
ing to understand this phenomenon and its impli-
cations for building dialogue systems. Most of 
the previous work has focused on lower level 
details of SRP: identifying components responsi-
ble for SRP (acoustic model, language model, 
search algorithm (Chase, 1997)) or prosodic 
characterization of SRP (Hirschberg et al, 2004). 
We extend previous work by analyzing the re-
lationship between SRP and higher level dia-
logue factors. Recent work has shown that dia-
logue design can benefit from several higher 
level dialogue factors: dialogue acts (Frampton 
and Lemon, 2005; Walker et al, 2001), prag-
matic plausibility (Gabsdil and Lemon, 2004). 
Also, it is widely believed that user emotions, as 
another example of higher level factor, interact 
with SRP but, currently, there is little hard evi-
dence to support this intuition. We perform our 
analysis on three high level dialogue factors: 
frustration/anger, certainty and correctness. Frus-
tration and anger have been observed as the most 
frequent emotional class in many dialogue sys-
tems (Ang et al, 2002) and are associated with a 
higher word error rate (Bulyko et al, 2005). For 
this reason, we use the presence of emotions like 
frustration and anger as our first dialogue factor. 
Our other two factors are inspired by another 
contribution of our study: looking at speech-
based computer tutoring dialogues instead of 
more commonly used information retrieval dia-
logues. Implementing spoken dialogue systems 
in a new domain has shown that many practices 
do not port well to the new domain (e.g. confir-
mation of long prompts (Kearns et al, 2002)). 
Tutoring, as a new domain for speech applica-
tions (Litman and Forbes-Riley, 2004; Pon-Barry 
et al, 2004), brings forward new factors that can 
be important for spoken dialogue design. Here 
we focus on certainty and correctness. Both fac-
tors have been shown to play an important role in 
the tutoring process (Forbes-Riley and Litman, 
2005; Liscombe et al, 2005). 
A common practice in previous work on emo-
tion prediction (Ang et al, 2002; Litman and 
Forbes-Riley, 2004) is to transform an initial 
finer level emotion annotation (five or more la-
bels) into a coarser level annotation (2-3 labels). 
We wanted to understand if this practice can im-
193
pact the dependencies we observe from the data. 
To test this, we combine our two emotion1 fac-
tors (frustration/anger and certainty) into a binary 
emotional/non-emotional annotation. 
To understand the relationship between SRP 
and our three factors, we take a three-step ap-
proach. In the first step, dependencies between 
SRP and our three factors are discovered using 
the Chi Square (?2) test. Similar analyses on hu-
man-human dialogues have yielded interesting 
insights about human-human conversations 
(Forbes-Riley and Litman, 2005; Skantze, 2005). 
In the second step, significant dependencies are 
combined to produce interesting insights regard-
ing SRP and to propose strategies for handling 
SRP. Validating these strategies is the purpose of 
the third step. In this paper, we focus on the first 
two steps; the third step is left as future work.  
Our analysis produces several interesting in-
sights and strategies which confirm the utility of 
the proposed approach. With respect to insights, 
we show that user emotions interact with SRP. 
We also find that incorrect/uncertain student 
turns have more SRP than expected. In addition, 
we find that the emotion annotation level affects 
the interactions we observe from the data, with 
finer-level emotions yielding more interactions 
and insights. 
In terms of strategies, our data suggests that 
favoring misrecognitions over rejections (by 
lowering the rejection threshold) might be more 
beneficial for our tutoring task ? at least in terms 
of reducing the number of emotional student 
turns. Also, as a general design practice in the 
spoken tutoring applications, we find an interest-
ing tradeoff between the pedagogical value of 
asking difficult questions and the system?s ability 
to recognize the student answer. 
2 Corpus 
The corpus analyzed in this paper consists of 95 
experimentally obtained spoken tutoring dia-
logues between 20 students and our system 
ITSPOKE (Litman and Forbes-Riley, 2004), a 
speech-enabled version of the text-based WHY2 
conceptual physics tutoring system (VanLehn et 
al., 2002). When interacting with ITSPOKE, stu-
dents first type an essay answering a qualitative 
physics problem using a graphical user interface. 
ITSPOKE then engages the student in spoken dia-
logue (using speech-based input and output) to 
correct misconceptions and elicit more complete 
                                                 
1 We use the term ?emotion? loosely to cover both affects 
and attitudes that can impact student learning. 
explanations, after which the student revises the 
essay, thereby ending the tutoring or causing an-
other round of tutoring/essay revision. For rec-
ognition, we use the Sphinx2 speech recognizer 
with stochastic language models. Because speech 
recognition is imperfect, after the data was col-
lected, each student utterance in our corpus was 
manually transcribed by a project staff member. 
An annotated excerpt from our corpus is shown 
in Figure 1 (punctuation added for clarity). The 
excerpts show both what the student said (the 
STD labels) and what ITSPOKE recognized (the 
ASR labels). The excerpt is also annotated with 
concepts that will be described next. 
2.1 Speech Recognition Problems (SRP) 
One form of SRP is the Rejection. Rejections 
occur when ITSPOKE is not confident enough in 
the recognition hypothesis and asks the student 
to repeat (Figure 1, STD3,4). For our ?2 analysis, 
we define the REJ variable with two values: Rej 
(a rejection occurred in the turn) and noRej (no 
rejection occurred in the turn). Not surprisingly, 
ITSPOKE also misrecognized some student turns. 
When ITSPOKE heard something different than 
what the student actually said but was confident 
in its hypothesis, we call this an ASR Misrecog-
nition (a binary version of the commonly used 
Word Error Rate) (Figure 1, STD1,2). Similarly, 
we define the ASR MIS variable with two val-
ues: AsrMis and noAsrMis. 
Semantic accuracy is more relevant for dia-
logue evaluation, as it does not penalize for word 
errors that are unimportant to overall utterance 
interpretation. In the case of form-based informa-
tion access spoken dialogue systems, computing 
semantic accuracy is straightforward (i.e. con-
cept accuracy = percentage of correctly recog-
nized concepts). In contrast, in the tutoring do-
main there are no clear forms with slots to be 
filled. We base our semantic accuracy on the 
?correctness? measure of the student turn. For 
each student turn, ITSPOKE interprets it and la-
bels its correctness with regard to whether the 
student correctly answered the tutor question (see 
the labels between square brackets in Figure 1). 
We define Semantic Misrecognition as cases 
where ITSPOKE was confident in its recognition 
hypothesis and the correctness interpretation of 
the recognition hypothesis is different from the 
correctness interpretation of the manual tran-
script (Figure 1, STD1). Similarly, we define the 
SEM MIS variable with two values: SemMis 
and noSemMis. The top part of Table 1 lists the 
distribution for our three SRP variables. 
194
3 Student State Annotation 
Our definition of student state includes annota-
tion on three dimensions (our three dialogue fac-
tors): Frustration/Anger/Hyperarticulation, Cer-
tainty and Correctness (see Table 1). The first 
two dimensions capture the student emotions and 
the last one the student correctness. 
ITSPOKE: Good. Apply the same force to an object of 
mass M3, which is three times the mass of M1. 
What is the magnitude M3's acceleration in terms of 
A1 [acceleration of the object M1]? 
STD1: third of a one (CERT=Uncertain, CRCT=C) 
[correct] 
ASR1: they're it but a one (AsrMis, SemMis) [incorrect] 
ITSPOKE: The acceleration of M3 [...] would be one-
third the acceleration of M1. [...] In general for the 
same force what remains constant? 
STD2: weight (CRCT=I) [incorrect] 
ASR2: great (AsrMis) [incorrect] 
ITSPOKE: Well... The product of the mass of a body 
and its acceleration remains constant for a given 
force. What is the relationship between a force act-
ing on the box, the mass of the box, and its accel-
eration? 
STD3: newton's second law (CERT=Uncertain, 
CRCT=PC) 
ASR3: the cancel the fire (Rej) 
ITSPOKE: Could you please repeat that? 
STD4: newton's second law (FAH=FrAng, CRCT=PC) 
ASR4: newton second long (Rej) 
Figure 1: Human-Computer Dialogue Excerpt 
The Frustration/Anger/Hyperarticulation 
dimension captures the perceived negative stu-
dent emotional response to the interaction with 
the system. Three labels were used to annotate 
this dimension: frustration-anger, hyperarticula-
tion and neutral. Similar to (Ang et al, 2002), 
because frustration and anger can be difficult to 
distinguish reliably, they were collapsed into a 
single label: frustration-anger (Figure 1, STD4). 
Often, frustration and anger is prosodically 
marked and in many cases the prosody used is 
consistent with hyperarticulation (Ang et al, 
2002). For this reason we included in this dimen-
sion the hyperarticulation label (even though hy-
perarticulation is not an emotion but a state). We 
used the hyperarticulation label for turns where 
no frustration or anger was perceived but never-
theless were hyperarticulated. For our interaction 
experiments we define the FAH variable with 
three values: FrAng (frustration-anger), Hyp 
(hyperarticulation) and Neutral. 
The Certainty dimension captures the per-
ceived student reaction to the questions asked by 
our computer tutor and her overall reaction to the 
tutoring domain (Liscombe et al, 2005). 
(Forbes-Riley and Litman, 2005) show that stu-
dent certainty interacts with a human tutor?s dia-
logue decision process (i.e. the choice of feed-
back). Four labels were used for this dimension: 
certain, uncertain (e.g. Figure 1, STD1), mixed 
and neutral. In a small number of turns, both cer-
tainty and uncertainty were expressed and these 
turns were labeled as mixed (e.g. the student was 
certain about a concept, but uncertain about an-
other concept needed to answer the tutor?s ques-
tion). For our interaction experiments we define 
the CERT variable with four values: Certain, 
Uncertain, Mixed and Neutral. 
 Vari-able Values 
Student turns 
(2334) 
Speech recognition problems 
 ASR MIS 
AsrMis 
noAsrMis 
25.4% 
74.6% 
 SEM MIS 
SemMis 
noSemMis 
5.7% 
94.3% 
 REJ Rej noRej 
7.0% 
93.0% 
Student state 
 FAH 
FrAng 
Hyp 
Neutral 
9.9% 
2.1% 
88.0% 
 CERT 
Certain 
Uncertain 
Mixed 
Neutral 
41.3% 
19.1% 
2.4% 
37.3% 
 CRCT 
C 
I 
PC 
UA 
63.3% 
23.3% 
6.2% 
7.1% 
 EnE Emotional Neutral 
64.8% 
35.2% 
Table 1: Variable distributions in our corpus. 
To test the impact of the emotion annotation 
level, we define the Emotional/Non-Emotional 
annotation based on our two emotional dimen-
sions: neutral turns on both the FAH and the 
CERT dimension are labeled as neutral2; all other 
turns were labeled as emotional. Consequently, 
we define the EnE variable with two values: 
Emotional and Neutral. 
Correctness is also an important factor of the 
student state. In addition to the correctness labels 
assigned by ITSPOKE (recall the definition of 
SEM MIS), each student turn was manually an-
notated by a project staff member in terms of 
their physics-related correctness. Our annotator 
used the human transcripts and his physics 
knowledge to label each student turn for various 
                                                 
2 To be consistent with our previous work, we label hyperar-
ticulated turns as emotional even though hyperarticulation is 
not an emotion. 
195
degrees of correctness: correct, partially correct, 
incorrect and unable to answer. Our system can 
ask the student to provide multiple pieces of in-
formation in her answer (e.g. the question ?Try 
to name the forces acting on the packet. Please, 
specify their directions.? asks for both the names 
of the forces and their direction). If the student 
answer is correct and contains all pieces of in-
formation, it was labeled as correct (e.g. ?grav-
ity, down?). The partially correct label was used 
for turns where part of the answer was correct 
but the rest was either incorrect (e.g. ?gravity, 
up?) or omitted some information from the ideal 
correct answer (e.g. ?gravity?). Turns that were 
completely incorrect (e.g. ?no forces?) were la-
beled as incorrect. Turns where the students did 
not answer the computer tutor?s question were 
labeled as ?unable to answer?. In these turns the 
student used either variants of ?I don?t know? or 
simply did not say anything. For our interaction 
experiments we defined the CRCT variable with 
four values: C (correct), I (incorrect), PC (par-
tially correct) and UA (unable to answer). 
Please note that our definition of student state 
is from the tutor?s perspective. As we mentioned 
before, our emotion annotation is for perceived 
emotions. Similarly, the notion of correctness is 
from the tutor?s perspective. For example, the 
student might think she is correct but, in reality, 
her answer is incorrect. This correctness should 
be contrasted with the correctness used to define 
SEM MIS. The SEM MIS correctness uses 
ITSPOKE?s language understanding module ap-
plied to recognition hypothesis or the manual 
transcript, while the student state?s correctness 
uses our annotator?s language understanding. 
All our student state annotations are at the turn 
level and were performed manually by the same 
annotator. While an inter-annotator agreement 
study is the best way to test the reliability of our 
two emotional annotations (FAH and CERT), 
our experience with annotating student emotions 
(Litman and Forbes-Riley, 2004) has shown that 
this type of annotation can be performed reliably. 
Given the general importance of the student?s 
uncertainty for tutoring, a second annotator has 
been commissioned to annotate our corpus for 
the presence or absence of uncertainty. This an-
notation can be directly compared with a binary 
version of CERT: Uncertain+Mixed versus Cer-
tain+Neutral. The comparison yields an agree-
ment of 90% with a Kappa of 0.68. Moreover, if 
we rerun our study on the second annotation, we 
find similar dependencies. We are currently 
planning to perform a second annotation of the 
FAH dimension to validate its reliability. 
We believe that our correctness annotation 
(CRCT) is reliable due to the simplicity of the 
task: the annotator uses his language understand-
ing to match the human transcript to a list of cor-
rect/incorrect answers. When we compared this 
annotation with the correctness assigned by 
ITSPOKE on the human transcript, we found an 
agreement of 90% with a Kappa of 0.79. 
4 Identifying dependencies using ?2 
To discover the dependencies between our vari-
ables, we apply the ?2 test. We illustrate our 
analysis method on the interaction between cer-
tainty (CERT) and rejection (REJ). The ?2 value 
assesses whether the differences between ob-
served and expected counts are large enough to 
conclude a statistically significant dependency 
between the two variables (Table 2, last column). 
For Table 2, which has 3 degrees of freedom ((4-
1)*(2-1)), the critical ?2 value at a p<0.05 is 7.81. 
We thus conclude that there is a statistically sig-
nificant dependency between the student cer-
tainty in a turn and the rejection of that turn. 
Combination  Obs. Exp. ?2
CERT ? REJ    11.45 
Certain ? Rej - 49 67 9.13 
Uncertain ? Rej + 43 31 6.15 
Table 2: CERT ? REJ interaction. 
If any of the two variables involved in a sig-
nificant dependency has more than 2 possible 
values, we can look more deeply into this overall 
interaction by investigating how particular values 
interact with each other. To do that, we compute 
a binary variable for each variable?s value in part 
and study dependencies between these variables. 
For example, for the value ?Certain? of variable 
CERT we create a binary variable with two val-
ues: ?Certain? and ?Anything Else? (in this case 
Uncertain, Mixed and Neutral). By studying the 
dependency between binary variables we can 
understand how the interaction works. 
Table 2 reports in rows 3 and 4 all significant 
interactions between the values of variables 
CERT and REJ. Each row shows: 1) the value 
for each original variable, 2) the sign of the de-
pendency, 3) the observed counts, 4) the ex-
pected counts and 5) the ?2 value. For example, 
in our data there are 49 rejected turns in which 
the student was certain. This value is smaller 
than the expected counts (67); the dependency 
between Certain and Rej is significant with a ?2 
value of 9.13. A comparison of the observed 
counts and expected counts reveals the direction 
196
(sign) of the dependency. In our case we see that 
certain turns are rejected less than expected (row 
3), while uncertain turns are rejected more than 
expected (row 4). On the other hand, there is no 
interaction between neutral turns and rejections 
or between mixed turns and rejections. Thus, the 
CERT ? REJ interaction is explained only by the 
interaction between Certain and Rej and the in-
teraction between Uncertain and Rej. 
5 Results - dependencies 
In this section we present all significant depend-
encies between SRP and student state both 
within and across turns. Within turn interactions 
analyze the contribution of the student state to 
the recognition of the turn. They were motivated 
by the widely believed intuition that emotion 
interacts with SRP. Across turn interactions look 
at the contribution of previous SRP to the current 
student state. Our previous work (Rotaru and 
Litman, 2005) had shown that certain SRP will 
correlate with emotional responses from the user. 
We also study the impact of the emotion annota-
tion level (EnE versus FAH/CERT) on the inter-
actions we observe. The implications of these 
dependencies will be discussed in Section 6. 
5.1 Within turn interactions 
For the FAH dimension, we find only one sig-
nificant interaction: the interaction between the 
FAH student state and the rejection of the current 
turn (Table 3). By studying values? interactions, 
we find that turns where the student is frustrated 
or angry are rejected more than expected (34 in-
stead of 16; Figure 1, STD4 is one of them). 
Similarly, turns where the student response is 
hyperarticulated are also rejected more than ex-
pected (similar to observations in (Soltau and 
Waibel, 2000)). In contrast, neutral turns in the 
FAH dimension are rejected less than expected. 
Surprisingly, FrAng does not interact with 
AsrMis as observed in (Bulyko et al, 2005) but 
they use the full word error rate measure instead 
of the binary version used in this paper. 
Combination  Obs. Exp. ?2
FAH ? REJ    77.92 
FrAng ? Rej + 34 16 23.61 
Hyp ? Rej + 16 3 50.76 
Neutral ? Rej - 113 143 57.90 
Table 3: FAH ? REJ interaction. 
Next we investigate how our second emotion 
annotation, CERT, interacts with SRP. All sig-
nificant dependencies are reported in Tables 2 
and 4. In contrast with the FAH dimension, here 
we see that the interaction direction depends on 
the valence. We find that ?Certain? turns have 
less SRP than expected (in terms of AsrMis and 
Rej). In contrast, ?Uncertain? turns have more 
SRP both in terms of AsrMis and Rej. ?Mixed? 
turns interact only with AsrMis, allowing us to 
conclude that the presence of uncertainty in the 
student turn (partial or overall) will result in ASR 
problems more than expected. Interestingly, on 
this dimension, neutral turns do not interact with 
any of our three SRP. 
Combination  Obs. Exp. ?2
CERT ? ASRMIS    38.41 
Certain ? AsrMis - 204 244 15.32 
Uncertain ? AsrMis + 138 112 9.46 
Mixed ? AsrMis + 29 13 22.27 
Table 4: CERT ? ASRMIS interaction. 
Finally, we look at interactions between stu-
dent correctness and SRP. Here we find signifi-
cant dependencies with all types of SRP (see Ta-
ble 5). In general, correct student turns have 
fewer SRP while incorrect, partially correct or 
UA turns have more SRP than expected. Partially 
correct turns have more AsrMis and SemMis 
problems than expected, but are rejected less 
than expected. Interestingly, UA turns interact 
only with rejections: these turns are rejected 
more than expected. An analysis of our corpus 
reveals that in most rejected UA turns the student 
does not say anything; in these cases, the sys-
tem?s recognition module thought the student 
said something but the system correctly rejects 
the recognition hypothesis. 
Combination  Obs. Exp. ?2
CRCT ? ASRMIS    65.17 
C ? AsrMis - 295 374 62.03 
I ? AsrMis + 198 137 45.95 
PC ? AsrMis + 50 37 5.9 
CRCT ? SEMMIS    20.44 
C ? SemMis + 100 84 7.83 
I ? SemMis - 14 31 13.09 
PC ? SemMis + 15 8 5.62 
CRCT ? REJ    99.48 
C ? Rej - 53 102 70.14 
I ? Rej + 84 37 79.61 
PC ? Rej - 4 10 4.39 
UA ? Rej + 21 11 9.19 
Table 5: Interactions between Correctness and SRP. 
The only exception to the rule is SEM MIS. 
We believe that SEM MIS behavior is explained 
by the ?catch-all? implementation in our system. 
In ITSPOKE, for each tutor question there is a list 
of anticipated answers. All other answers are 
197
treated as incorrect. Thus, it is less likely that a 
recognition problem in an incorrect turn will af-
fect the correctness interpretation (e.g. Figure 1, 
STD2: very unlikely to misrecognize the incor-
rect ?weight? with the anticipated ?the product of 
mass and acceleration?). In contrast, in correct 
turns recognition problems are more likely to 
screw up the correctness interpretation (e.g. mis-
recognizing ?gravity down? as ?gravity sound?). 
5.2 Across turn interactions 
Next we look at the contribution of previous SRP 
? variable name or value followed by (-1) ? to the 
current student state. Please note that there are 
two factors involved here: the presence of the 
SRP and the SRP handling strategy. In 
ITSPOKE, whenever a student turn is rejected, 
unless this is the third rejection in a row, the stu-
dent is asked to repeat using variations of ?Could 
you please repeat that??. In all other cases, 
ITSPOKE makes use of the available informa-
tion ignoring any potential ASR errors. 
Combination  Obs. Exp. ?2
ASRMIS(-1) ? FAH    7.64 
AsrMis(-1) ? FrAng -t 46 58 3.73 
AsrMis(-1) ? Hyp -t 7 12 3.52 
AsrMis(-1) ? Neutral + 527 509 6.82 
REJ(-1) ? FAH    409.31
Rej(-1) ? FrAng + 36 16 28.95 
Rej(-1) ? Hyp + 38 3 369.03
Rej(-1) ? Neutral - 88 142 182.9 
REJ(-1) ? CRCT    57.68 
Rej(-1) ? C - 68 101 31.94 
Rej(-1) ? I + 74 37 49.71 
Rej(-1) ? PC - 3 10 6.25 
Table 6: Interactions across turns (t ? trend, p<0.1). 
Here we find only 3 interactions (Table 6). We 
find that after a non-harmful SRP (AsrMis) the 
student is less frustrated and hyperarticulated 
than expected. This result is not surprising since 
an AsrMis does not have any effect on the nor-
mal dialogue flow. 
In contrast, after rejections we observe several 
negative events. We find a highly significant in-
teraction between a previous rejection and the 
student FAH state, with student being more frus-
trated and more hyperarticulated than expected 
(e.g. Figure 1, STD4). Not only does the system 
elicit an emotional reaction from the student after 
a rejection, but her subsequent response to the 
repetition request suffers in terms of the correct-
ness. We find that after rejections student an-
swers are correct or partially correct less than 
expected and incorrect more than expected. The 
REJ(-1) ? CRCT interaction might be explained 
by the CRCT ? REJ interaction (Table 5) if, in 
general, after a rejection the student repeats her 
previous turn. An annotation of responses to re-
jections as in (Swerts et al, 2000) (repeat, re-
phrase etc.) should  provide additional insights.  
We were surprised to see that a previous 
SemMis (more harmful than an AsrMis but less 
disruptive than a Rej) does not interact with the 
student state; also the student certainty does not 
interact with previous SRP. 
5.3 Emotion annotation level 
We also study the impact of the emotion annota-
tion level on the interactions we can observe 
from our corpus. In this section, we look at inter-
actions between SRP and our coarse-level emo-
tion annotation (EnE) both within and across 
turns. Our results are similar with the results of 
our previous work (Rotaru and Litman, 2005) on 
a smaller corpus and a similar annotation 
scheme. We find again only one significant in-
teraction: rejections are followed by more emo-
tional turns than expected (Table 7). The strength 
of the interaction is smaller than in previous 
work, though the results can not be compared 
directly. No other dependencies are present. 
Combination  Obs. Exp. ?2
REJ(-1) ? EnE    6.19 
Rej(-1) ? Emotional + 119 104 6.19 
Table 7: REJ(-1) ? EnE interaction. 
We believe that the REJ(-1) ? EnE interaction is 
explained mainly by the FAH dimension. Not 
only is there no interaction between REJ(-1) and 
CERT, but the inclusion of the CERT dimension 
in the EnE annotation decreases the strength of 
the interaction between REJ and FAH (the ?2 
value decreases from 409.31 for FAH to a mere 
6.19 for EnE). Collapsing emotional classes also 
prevents us from seeing any within turn interac-
tions. These observations suggest that what is 
being counted as an emotion for a binary emo-
tion annotation is critical its success. In our case, 
if we look at affect (FAH) or attitude (CERT) in 
isolation we find many interactions; in contrast, 
combining them offers little insight.  
6 Results ? insights & strategies 
Our results put a spotlight on several interesting 
observations which we discuss below. 
Emotions interact with SRP 
The dependencies between FAH/CERT and 
various SRP (Tables 2-4) provide evidence that 
user?s emotions interact with the system?s ability 
198
to recognize the current turn. This is a widely 
believed intuition with little empirical support so 
far. Thus, our notion of student state can be a 
useful higher level information source for SRP 
predictors. Similar to (Hirschberg et al, 2004), 
we believe that peculiarities in the acous-
tic/prosodic profile of specific student states are 
responsible for their SRP. Indeed, previous work 
has shown that the acoustic/prosodic information 
plays an important role in characterizing and 
predicting both FAH (Ang et al, 2002; Soltau 
and Waibel, 2000) and CERT (Liscombe et al, 
2005; Swerts and Krahmer, 2005). 
The impact of the emotion annotation level 
A comparison of the interactions yielded by 
various levels of emotion annotation shows the 
importance of the annotation level. When using a 
coarser level annotation (EnE) we find only one 
interaction. By using a finer level annotation, not 
only we can understand this interaction better but 
we also discover new interactions (five interac-
tions with FAH and CERT). Moreover, various 
state annotations interact differently with SRP. 
For example, non-neutral turns in the FAH di-
mension (FrAng and Hyp) will be always re-
jected more than expected (Table 3); in contrast, 
interactions between non-neutral turns in the 
CERT dimension and rejections depend on the 
valence (?certain? turns will be rejected less than 
expected while ?uncertain? will be rejected more 
than expected; recall Table 2). We also see that 
the neutral turns interact with SRP depending on 
the dimension that defines them: FAH neutral 
turns interact with SRP (Table 3) while CERT 
neutral turns do not (Tables 2 and 4). 
This insight suggests an interesting tradeoff 
between the practicality of collapsing emotional 
classes (Ang et al, 2002; Litman and Forbes-
Riley, 2004) and the ability to observe meaning-
ful interactions via finer level annotations. 
Rejections: impact and a handling strategy 
Our results indicate that rejections and 
ITSPOKE?s current rejection-handling strategy 
are problematic. We find that rejections are fol-
lowed by more emotional turns (Table 7). A 
similar effect was observed in our previous work 
(Rotaru and Litman, 2005). The fact that it gen-
eralizes across annotation scheme and corpus, 
emphasizes its importance. When a finer level 
annotation is used, we find that rejections are 
followed more than expected by a frustrated, an-
gry and hyperarticulated user (Table 6). More-
over, these subsequent turns can result in addi-
tional rejections (Table 3). Asking to repeat after 
a rejection does not also help in terms of correct-
ness: the subsequent student answer is actually 
incorrect more than expected (Table 6). 
These interactions suggest an interesting strat-
egy for our tutoring task: favoring misrecogni-
tions over rejections (by lowering the rejection 
threshold). First, since rejected turns are more 
than expected incorrect (Table 5), the actual rec-
ognized hypothesis for such turns turn is very 
likely to be interpreted as incorrect. Thus, ac-
cepting a rejected turn instead of rejecting it will 
have the same outcome in terms of correctness: 
an incorrect answer. In this way, instead of at-
tempting to acquire the actual student answer by 
asking to repeat, the system can skip these extra 
turn(s) and use the current hypothesis. Second, 
the other two SRP are less taxing in terms of 
eliciting FAH emotions (recall Table 6; note that 
a SemMis might activate an unwarranted and 
lengthy knowledge remediation subdialogue). 
This suggests that continuing the conversation 
will be more beneficial even if the system mis-
understood the student. A similar behavior was 
observed in human-human conversations through 
a noisy speech channel (Skantze, 2005). 
Correctness/certainty?SRP interactions 
We also find an interesting interaction between 
correctness/certainty and system?s ability to rec-
ognize that turn. In general correct/certain turns 
have less SRP while incorrect/uncertain turns 
have more SRP than expected. This observation 
suggests that the computer tutor should ask the 
right question (in terms of its difficulty) at the 
right time. Intuitively, asking a more complicated 
question when the student is not prepared to an-
swer it will increase the likelihood of an incor-
rect or uncertain answer. But our observations 
show that the computer tutor has more trouble 
recognizing correctly these types of answers. 
This suggests an interesting tradeoff between the 
tutor?s question difficulty and the system?s abil-
ity to recognize the student answer. This tradeoff 
is similar in spirit to the initiative-SRP tradeoff 
that is well known when designing information-
seeking systems (e.g. system initiative is often 
used instead of a more natural mixed initiative 
strategy, in order to minimize SRP). 
7 Conclusions 
In this paper we analyze the interactions between 
SRP and three higher level dialogue factors that 
define our notion of student state: frustra-
tion/anger/hyperarticulation, certainty and cor-
rectness. Our analysis produces several interest-
ing insights and strategies which confirm the 
199
utility of the proposed approach. We show that 
user emotions interact with SRP and that the 
emotion annotation level affects the interactions 
we observe from the data, with finer-level emo-
tions yielding more interactions and insights. 
We also find that tutoring, as a new domain 
for speech applications, brings forward new im-
portant factors for spoken dialogue design: cer-
tainty and correctness. Both factors interact with 
SRP and these interactions highlight an interest-
ing design practice in the spoken tutoring appli-
cations: the tradeoff between the pedagogical 
value of asking difficult questions and the sys-
tem?s ability to recognize the student answer (at 
least in our system). The particularities of the 
tutoring domain also suggest favoring misrecog-
nitions over rejections to reduce the negative im-
pact of asking to repeat after rejections. 
In our future work, we plan to move to the 
third step of our approach: testing the strategies 
suggested by our results. For example, we will 
implement a new version of ITSPOKE that never 
rejects the student turn. Next, the current version 
and the new version will be compared with re-
spect to users? emotional response. Similarly, to 
test the tradeoff hypothesis, we will implement a 
version of ITSPOKE that asks difficult questions 
first and then falls back to simpler questions. A 
comparison of the two versions in terms of the 
number of SRP can be used for validation. 
While our results might be dependent on the 
tutoring system used in this experiment, we be-
lieve that our findings can be of interest to practi-
tioners building similar voice-based applications. 
Moreover, our approach can be applied easily to 
studying other systems. 
Acknowledgements 
This work is supported by NSF Grant No. 
0328431. We thank Dan Bohus, Kate Forbes-
Riley, Joel Tetreault and our anonymous review-
ers for their helpful comments. 
References 
J. Ang, R. Dhillon, A. Krupski, A. Shriberg and A. 
Stolcke. 2002. Prosody-based automatic detection 
of annoyance and frustration in human-computer 
dialog. In Proc. of ICSLP. 
I. Bulyko, K. Kirchhoff, M. Ostendorf and J. Gold-
berg. 2005. Error-correction detection and response 
generation in a spoken dialogue system. Speech 
Communication, 45(3). 
L. Chase. 1997. Blame Assignment for Errors Made 
by Large Vocabulary Speech Recognizers. In Proc. 
of Eurospeech. 
K. Forbes-Riley and D. J. Litman. 2005. Using Bi-
grams to Identify Relationships Between Student 
Certainness States and Tutor Responses in a Spo-
ken Dialogue Corpus. In Proc. of SIGdial. 
M. Frampton and O. Lemon. 2005. Reinforcement 
Learning of Dialogue Strategies using the User's 
Last Dialogue Act. In Proc. of IJCAI Workshop on 
Know.&Reasoning in Practical Dialogue Systems. 
M. Gabsdil and O. Lemon. 2004. Combining Acoustic 
and Pragmatic Features to Predict Recognition 
Performance in Spoken Dialogue Systems. In Proc. 
of ACL. 
J. Hirschberg, D. Litman and M. Swerts. 2004. Pro-
sodic and Other Cues to Speech Recognition Fail-
ures. Speech Communication, 43(1-2). 
M. Kearns, C. Isbell, S. Singh, D. Litman and J. 
Howe. 2002. CobotDS: A Spoken Dialogue System 
for Chat. In Proc. of National Conference on Arti-
ficial Intelligence (AAAI). 
J. Liscombe, J. Hirschberg and J. J. Venditti. 2005. 
Detecting Certainness in Spoken Tutorial Dia-
logues. In Proc. of Interspeech. 
D. Litman and K. Forbes-Riley. 2004. Annotating 
Student Emotional States in Spoken Tutoring Dia-
logues. In Proc. of SIGdial Workshop on Discourse 
and Dialogue (SIGdial). 
H. Pon-Barry, B. Clark, E. O. Bratt, K. Schultz and S. 
Peters. 2004. Evaluating the effectiveness of Scot:a 
spoken conversational tutor. In Proc. of ITS Work-
shop on Dialogue-based Intellig. Tutoring Systems. 
M. Rotaru and D. Litman. 2005. Interactions between 
Speech Recognition Problems and User Emotions. 
In Proc. of Eurospeech. 
G. Skantze. 2005. Exploring human error recovery 
strategies: Implications for spoken dialogue sys-
tems. Speech Communication, 45(3). 
H. Soltau and A. Waibel. 2000. Specialized acoustic 
models for hyperarticulated speech. In Proc. of 
ICASSP. 
M. Swerts and E. Krahmer. 2005. Audiovisual Pros-
ody and Feeling of Knowing. Journal of Memory 
and Language, 53. 
M. Swerts, D. Litman and J. Hirschberg. 2000. Cor-
rections in Spoken Dialogue Systems. In Proc. of 
ICSLP. 
K. VanLehn, P. W. Jordan, C. P. Ros?, et al 2002. 
The Architecture of Why2-Atlas: A Coach for 
Qualitative Physics Essay Writing. In Proc. of In-
telligent Tutoring Systems (ITS). 
M. Walker, D. Litman, C. Kamm and A. Abella. 
2000. Towards Developing General Models of Us-
ability with PARADISE. Natural Language Engi-
neering. 
M. Walker, R. Passonneau and J. Boland. 2001. 
Quantitative and Qualitative Evaluation of Darpa 
Communicator Spoken Dialogue Systems. In Proc. 
of ACL. 
200
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 360?367,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
The Utility of a Graphical Representation of Discourse Structure in 
Spoken Dialogue Systems 
Mihai Rotaru 
University of Pittsburgh 
Pittsburgh, USA 
mrotaru@cs.pitt.edu 
Diane J. Litman 
University of Pittsburgh 
Pittsburgh, USA 
litman@cs.pitt.edu 
 
 
Abstract 
In this paper we explore the utility of the 
Navigation Map (NM), a graphical repre-
sentation of the discourse structure. We run 
a user study to investigate if users perceive 
the NM as helpful in a tutoring spoken dia-
logue system. From the users? perspective, 
our results show that the NM presence al-
lows them to better identify and follow the 
tutoring plan and to better integrate the in-
struction. It was also easier for users to 
concentrate and to learn from the system if 
the NM was present. Our preliminary 
analysis on objective metrics further 
strengthens these findings. 
1 Introduction 
With recent advances in spoken dialogue system 
technologies, researchers have turned their atten-
tion to more complex domains (e.g. tutoring 
(Litman and Silliman, 2004; Pon-Barry et al, 
2006), technical support (Acomb et al, 2007), 
medication assistance (Allen et al, 2006)). These 
domains bring forward new challenges and issues 
that can affect the usability of such systems: in-
creased task complexity, user?s lack of or limited 
task knowledge, and longer system turns. 
In typical information access dialogue systems, 
the task is relatively simple: get the information 
from the user and return the query results with 
minimal complexity added by confirmation dia-
logues. Moreover, in most cases, users have 
knowledge about the task. However, in complex 
domains things are different. Take for example 
tutoring. A tutoring dialogue system has to discuss 
concepts, laws and relationships and to engage in 
complex subdialogues to correct user misconcep-
tions. In addition, it is very likely that users of such 
systems are not familiar or are only partially famil-
iar with the tutoring topic. The length of system 
turns can also be affected as these systems need to 
make explicit the connections between parts of the 
underlying task. 
Thus, interacting with such systems can be char-
acterized by an increased user cognitive load asso-
ciated with listening to often lengthy system turns 
and the need to integrate the current information to 
the discussion overall (Oviatt et al, 2004). 
We hypothesize that one way to reduce the 
user?s cognitive load is to make explicit two pieces 
of information: the purpose of the current system 
turn, and how the system turn relates to the overall 
discussion. This information is implicitly encoded 
in the intentional structure of a discourse as pro-
posed in the Grosz & Sidner theory of discourse 
(Grosz and Sidner, 1986). 
Consequently, in this paper we propose using a 
graphical representation of the discourse structure 
as a way of improving the performance of com-
plex-domain dialogue systems (note that graphical 
output is required). We call it the Navigation Map 
(NM). The NM is a dynamic representation of the 
discourse segment hierarchy and the discourse seg-
ment purpose information enriched with several 
features (Section 3). To make a parallel with geog-
raphy, as the system ?navigates? with the user 
through the domain, the NM offers a cartographic 
view of the discussion. While a somewhat similar 
graphical representation of the discourse structure 
has been explored in one previous study (Rich and 
Sidner, 1998), to our knowledge we are the first to 
test its benefits (see Section 6). 
360
As a first step towards understanding the NM ef-
fects, here we focus on investigating whether users 
prefer a system with the NM over a system without 
the NM and, if yes, what are the NM usage pat-
terns. We test this in a speech based computer tutor 
(Section 2). We run a within-subjects user study in 
which users interacted with the system both with 
and without the NM (Section 4). 
Our analysis of the users? subjective evaluation 
of the system indicates that users prefer the version 
of the system with the NM over the version with-
out the NM on several dimensions. The NM pres-
ence allows the users to better identify and follow 
the tutoring plan and to better integrate the instruc-
tion. It was also easier for users to concentrate and 
to learn from the system if the NM was present. 
Our preliminary analysis on objective metrics fur-
ther strengthens these findings. 
2 ITSPOKE 
ITSPOKE (Litman and Silliman, 2004) is a state-
of-the-art tutoring spoken dialogue system for con-
ceptual physics. When interacting with ITSPOKE, 
users first type an essay answering a qualitative 
physics problem using a graphical user interface. 
ITSPOKE then engages the user in spoken dialogue 
(using head-mounted microphone input and speech 
output) to correct misconceptions and elicit more 
complete explanations, after which the user revises 
the essay, thereby ending the tutoring or causing 
another round of tutoring/essay revision. 
All dialogues with ITSPOKE follow a question-
answer format (i.e. system initiative): ITSPOKE 
asks a question, users answer and then the process 
is repeated. Deciding what question to ask, in what 
order and when to stop is hand-authored before-
hand in a hierarchical structure. Internally, system 
questions are grouped in question segments. 
In Figure 1, we show the transcript of a sample 
interaction with ITSPOKE. The system is discussing 
the problem listed in the upper right corner of the 
figure and it is currently asking the question Tu-
tor5. The left side of the figure shows the interac-
tion transcript (not available to the user at run-
time). The right side of the figure shows the NM 
which will be discussed in the next section. 
Our system behaves as follows. First, based on 
the analysis of the user essay, it selects a question 
segment to correct misconceptions or to elicit more 
complete explanations. Next the system asks every 
question from this question segment. If the user 
answer is correct, the system simply moves on to 
the next question (e.g. Tutor2?Tutor3). For incor-
rect answers there are two alternatives. For simple 
questions, the system will give out the correct an-
swer accompanied by a short explanation and 
move on to the next question (e.g. Tutor1?Tutor2). 
For complex questions (e.g. applying physics 
laws), ITSPOKE will engage into a remediation 
subdialogue that attempts to remediate user?s lack 
of knowledge or skills (e.g. Tutor4?Tutor5). The 
remediation subdialogue for each complex ques-
tion is specified in another question segment. 
Our system exhibits some of the issues we 
linked in Section 1 with complex-domain systems. 
Dialogues with our system can be long and com-
plex (e.g. the question segment hierarchical struc-
ture can reach level 6) and sometimes the system?s 
turn can be quite long (e.g. Tutor2). User?s reduced 
knowledge of the task is also inherent in tutoring.  
3 The Navigation Map (NM) 
We use the Grosz & Sidner theory of discourse 
(Grosz and Sidner, 1986) to inform our NM de-
sign. According to this theory, each discourse has a 
discourse purpose/intention. Satisfying the main 
discourse purpose is achieved by satisfying several 
smaller purposes/intentions organized in a hierar-
chical structure. As a result, the discourse is seg-
mented into discourse segments each with an asso-
ciated discourse segment purpose/intention. This 
theory has inspired several generic dialogue man-
agers for spoken dialogue systems (e.g. (Rich and 
Sidner, 1998)). 
The NM requires that we have the discourse 
structure information at runtime. To do that, we 
manually annotate the system?s internal representa-
tion of the tutoring task with discourse segment 
purpose and hierarchy information. Based on this 
annotation, we can easily construct the discourse 
structure at runtime. In this section we describe our 
annotation and the NM design choices we made. 
Figure 1 shows the state of the NM after turn Tu-
tor5 as the user sees it on the interface (NM line 
numbering is for exposition only). Note that Figure 
1 is not a screenshot of the actual system interface. 
The NM is the only part from the actual system 
interface. Figure 2 shows the NM after turn Tutor1. 
We manually annotated each system ques-
tion/explanation for its intention(s)/purpose(s). 
Note that some system turns have multiple inten-
361
tions/purposes thus multiple discourse segments 
were created for them. For example, in Tutor1 the 
system first identifies the time frames on which the 
analysis will be performed (Figure 1&2, NM2). 
Next, the system indicates that it will discuss about 
the first time frame (Figure 1&2, NM3) and then it 
asks the actual question (Figure 2, NM4).  
Thus, in addition to our manual annotation of 
the discourse segment purpose information, we 
manually organized all discourse segments from a 
question segment in a hierarchical structure that 
reflects the discourse structure. 
At runtime, while discussing a question seg-
ment, the system has only to follow the annotated 
hierarchy, displaying and highlighting the dis-
course segment purposes associated with the ut-
tered content. For example, while uttering Tutor1, 
the NM will synchronously highlight NM2, NM3 
and NM4. Remediation question segments (e.g. 
NM12) or explanations (e.g. NM5) activated by in-
correct answers are attached to the structure under 
the corresponding discourse segment. 
3.1 NM Design Choices 
In our graphical representation of the discourse 
structure, we used a left to right indented layout. In 
addition, we made several design choices to enrich 
the NM information content and usability. 
Figure 2. NM state after turn Tutor1 
Correct answers. In Figure 2 we show the state 
of the NM after uttering Tutor1. The current dis-
course segment purpose (NM4) indicates that the 
system is asking about the relationship between the 
two velocities. While we could have kept the same 
information after the system was done with this 
discourse segment, we thought that users will 
benefit from having the correct answer on the 
screen (recall NM4 in Figure 1). Thus, the NM was 
enhanced to display the correct answer after the 
system is done with each question. We extracted 
the correct answer from the system specifications 
for each question and manually created a new ver-
sion of the discourse segment purpose that includes 
this information. 
1
2
3
4
5
6
TUTOR1: To analyze this problem we will first de-
scribe the motion of the person and his keys while he
is holding them. Then we will look at the motion of 
the person and his keys after he lets go of them. 
  Let's begin by looking at the motion of the man and
his keys while he is holding them. 
  How does his velocity compare to that of his keys?
User answer1: man?s velocity is bigger [incorrect] 
TUTOR2: Well? The problem statement says that the
man is holding his keys motionless in front of his
face. That means they are both falling together and
are not moving with respect to each other. Hence their
velocities are the same. 
  Now let's see what happens when he releases them.
  To do so we will first find the individual forces
exerted on the man and his keys, then both net forces,
both accelerations, and finally we will describe and
compare the velocities. 
  So what are the forces exerted on the man after he
releases his keys? Please, specify their directions. 
User answer2: gravity vertically down [correct] 
TUTOR3: Good. Because there is just one force, grav-
ity's, it is trivial to determine the NET force. So what
is the direction of the NET force on the person? 
User answer3: down [correct] 
TUTOR4: Ok. Considering the net force, does the per-
son have an acceleration? If yes, specify its direction.
User answer4: the man has no acceleration [incorrect] 
TUTOR5: Well? To analyze the man's accelera-
tion we will use Newton's second law. What is
the definition of this law? 
Problem: Suppose a man is in a free-falling elevator and is 
holding his keys motionless right in front of his face.  He then 
lets go. What will be the position of the keys relative to the 
man's face as time passes?  Explain. 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
Figure 1. Transcript of a sample ITSPOKE speech interaction (left). The NM as the user sees it after turn Tutor5
362
Limited horizon. Since in our case the system 
drives the conversation (i.e. system initiative), we 
always know what questions would be discussed 
next. We hypothesized that by having access to 
this information, users will have a better idea of 
where instruction is heading, thus facilitating their 
understanding of the relevance of the current topic 
to the overall discussion. To prevent information 
overload, we only display the next discourse seg-
ment purpose at each level in the hierarchy (see 
Figure 1, NM14, NM16, NM17 and NM19; Figure 2, 
NM5); additional discourse segments at the same 
level are signaled through a dotted line. To avoid 
helping the students answer the current question in 
cases when the next discourse segment hints/de-
scribes the answer, each discourse segment has an 
additional purpose annotation that is displayed 
when the segment is part of the visible horizon.  
Auto-collapse. To reduce the amount of infor-
mation on the screen, discourse segments dis-
cussed in the past are automatically collapsed by 
the system. For example, in Figure 1, NM Line 3 is 
collapsed in the actual system and Lines 4 and 5 
are hidden (shown in Figure1 to illustrate our dis-
course structure annotation.). The user can expand 
nodes as desired using the mouse. 
Information highlight. Bold and italics font 
were used to highlight important information (what 
and when to highlight was manually annotated). 
For example, in Figure 1, NM2 highlights the two 
time frames as they are key steps in approaching 
this problem. Correct answers are also highlighted. 
We would like to reiterate that the goal of this 
study is to investigate if making certain types of 
discourse information explicitly available to the 
user provides any benefits. Thus, whether we have 
made the optimal design choices is of secondary 
importance. While, we believe that our annotation 
is relatively robust as the system questions follow a 
carefully designed tutoring plan, in the future we 
would like to investigate these issues. 
4 User Study 
We designed a user study focused primarily on 
user?s perception of the NM presence/absence. We 
used a within-subject design where each user re-
ceived instruction both with and without the NM. 
Each user went through the same experimental 
procedure: 1) read a short document of background 
material, 2) took a pretest to measure initial phys-
ics knowledge, 3) worked through 2 problems with 
ITSPOKE 4) took a posttest similar to the pretest, 5) 
took a NM survey, and 6) went through a brief 
open-question interview with the experimenter. 
In the 3rd step, the NM was enabled in only one 
problem. Note that in both problems, users did not 
have access to the system turn transcript. After 
each problem users filled in a system question-
naire in which they rated the system on various 
dimensions; these ratings were designed to cover 
dimensions the NM might affect (see Section 5.1). 
While the system questionnaire implicitly probed 
the NM utility, the NM survey from the 5th step 
explicitly asked the users whether the NM was use-
ful and on what dimensions (see Section 5.1) 
To account for the effect of the tutored problem 
on the user?s questionnaire ratings, users were ran-
domly assigned to one of two conditions. The users 
in the first condition (F) had the NM enabled in the 
first problem and disabled in the second problem, 
while users in the second condition (S) had the op-
posite. Thus, if the NM has any effect on the user?s 
perception of the system, we should see a decrease 
in the questionnaire ratings from problem 1 to 
problem 2 for F users and an increase for S users. 
Other factors can also influence our measure-
ments. To reduce the effect of the text-to-speech 
component, we used a version of the system with 
human prerecorded prompts. We also had to ac-
count for the amount of instruction as in our sys-
tem the top level question segment is tailored to 
what users write in the essay. Thus the essay 
analysis component was disabled; for all users, the 
system started with the same top level question 
segment which assumed no information in the es-
say. Note that the actual dialogue depends on the 
correctness of the user answers. After the dialogue, 
users were asked to revise their essay and then the 
system moved on to the next problem. 
The collected corpus comes from 28 users (13 in 
F and 15 in S). The conditions were balanced for 
gender (F: 6 male, 7 female; S: 8 male, 7 female). 
There was no significant differences between the 
two conditions in terms of pretest (p<0.63); in both 
conditions users learned (significant difference 
between pretest and posttest, p<0.01). 
5 Results 
5.1 Subjective metrics 
Our main resource for investigating the effect of 
the NM was the system questionnaires given after 
363
each problem. These questionnaires are identical 
and include 16 questions that probed user?s percep-
tion of ITSPOKE on various dimensions. Users 
were asked to answer the questions on a scale from 
1-5 (1 ? Strongly Disagree, 2 ? Disagree, 3 ? 
Somewhat Agree, 4 ? Agree, 5 ? Strongly Agree). 
If indeed the NM has any effect we should observe 
differences between the ratings of the NM problem 
and the noNM problem (i.e. the NM is disabled). 
Table 1 lists the 16 questions in the question-
naire order. The table shows for every question the 
average rating for all condition-problem combina-
tions (e.g. column 5: condition F problem 1 with 
the NM enabled). For all questions except Q7 and 
Q11 a higher rating is better. For Q7 and Q11 
(italicized in Table 1) a lower rating is better as 
they gauge negative factors (high level of concen-
tration and task disorientation). They also served as 
a deterrent for negligence while rating. 
To test if the NM presence has a significant ef-
fect, a repeated-measure ANOVA with between-
subjects factors was applied. The within-subjects 
factor was the NM presence (NMPres) and the 
between-subjects factor was the condition (Cond)1. 
The significance of the effect of each factor and 
their combination (NMPres*Cond) is listed in the 
table with significant and trend effects highlighted 
in bold (see columns 2-4). Post-hoc t-tests between 
the NM and noNM ratings were run for each con-
dition (?s?/?t?marks significant/trend differences). 
Results for Q1-6 
Questions Q1-6 were inspired by previous work 
on spoken dialogue system evaluation (e.g. 
(Walker et al, 2000)) and measure user?s overall 
perception of the system. We find that the NM 
presence significantly improves user?s perception 
of the system in terms of their ability to concen-
trate on the instruction (Q3), in terms of their incli-
nation to reuse the system (Q6) and in terms of the 
system?s matching of their expectations (Q4). 
There is a trend that it was easier for them to learn 
from the NM enabled version of the system (Q2). 
Results for Q7-13 
Q7-13 relate directly to our hypothesis that users 
                                                 
1 Since in this version of ANOVA the NM/noNM rat-
ings come from two different problems based on the 
condition, we also run an ANOVA in which the within-
subjects factor was the problem (Prob). In this case, the 
NM effect corresponds to an effect from Prob*Cond 
which is identical in significance with that of NMPres. 
benefit from access to the discourse structure in-
formation. These questions probe the user?s per-
ception of ITSPOKE during the dialogue. We find 
that for 6 out 7 questions the NM presence has a 
significant/trend effect (Table 1, column 2).  
Structure. Users perceive the system as having 
a structured tutoring plan significantly2 more in the 
NM problems (Q8). Moreover, it is significantly 
easier for them to follow this tutoring plan if the 
NM is present (Q11). These effects are very clear 
for F users where their ratings differ significantly 
between the first (NM) and the second problem 
(noNM). A difference in ratings is present for S 
users but it is not significant. As with most of the S 
users? ratings, we believe that the NM presentation 
order is responsible for the mostly non-significant 
differences. More specifically, assuming that the 
NM has a positive effect, the S users are asked to 
rate first the poorer version of the system (noNM) 
and then the better version (NM). In contrast, F 
users? task is easier as they already have a high 
reference point (NM) and it is easier for them to 
criticize the second problem (noNM). Other factors 
that can blur the effect of the NM are domain 
learning and user?s adaptation to the system. 
Integration. Q9 and Q10 look at how well users 
think they integrate the system questions in both a 
forward-looking fashion (Q9) and a backward 
looking fashion (Q10). Users think that it is sig-
nificantly easier for them to integrate the current 
system question to what will be discussed in the 
future if the NM is present (Q9). Also, if the NM is 
present, it is easier for users to integrate the current 
question to the discussion so far (Q10, trend). For 
Q10, there is no difference for F users but a sig-
nificant one for S users. We hypothesize that do-
main learning is involved here: F users learn better 
from the first problem (NM) and thus have less 
issues solving the second problem (noNM). In con-
trast, S users have more difficulties in the first 
problem (noNM), but the presence of the NM 
eases their task in the second problem. 
Correctness. The correct answer NM feature is 
useful for users too. There is a trend that it is easier 
for users to know the correct answer if the NM is 
present (Q13). We hypothesize that speech recog-
nition and language understanding errors are re-
                                                 
2 We refer to the significance of the NMPres factor (Ta-
ble 1, column 2). When discussing individual experi-
mental conditions, we refer to the post-hoc t-tests. 
364
sponsible for the non-significant NM effect on the 
dimension captured by Q12. 
Concentration. Users also think that the NM 
enabled version of the system requires less effort in 
terms of concentration (Q7). We believe that hav-
ing the discourse segment purpose as visual input 
allows the users to concentrate more easily on what 
the system is uttering. In many of the open ques-
tion interviews users stated that it was easier for 
them to listen to the system when they had the dis-
course segment purpose displayed on the screen. 
Results for Q14-16 
Questions Q14-16 were included to probe user?s 
post tutoring perceptions. We find a trend that in 
the NM problems it was easier for users to under-
stand the system?s main point (Q14). However, in 
terms of identifying (Q15) and correcting (Q16) 
problems in their essay the results are inconclusive. 
We believe that this is due to the fact that the essay 
interpretation component was disabled in this ex-
periment. As a result, the instruction did not match 
the initial essay quality. Nonetheless, in the open-
question interviews, many users indicated using 
the NM as a reference while updating their essay. 
In addition to the 16 questions, in the system 
questionnaire after the second problem users were 
asked to choose which version of the system they 
preferred the most (i.e. the first or the second prob-
lem version). 24 out 28 users (86%) preferred the 
NM enabled version. In the open-question inter-
view, the 4 users that preferred the noNM version 
(2 in each condition) indicated that it was harder 
for them to concurrently concentrate on the audio 
and the visual input (divided attention problem) 
and/or that the NM was changing too fast. 
To further strengthen our conclusions from the 
system questionnaire analysis, we would like to 
note that users were not asked to directly compare 
the two versions but they were asked to individu-
ally rate two versions which is a noisier process 
(e.g. users need to recall their previous ratings). 
The NM survey 
While the system questionnaires probed users? 
NM usage indirectly, in the second to last step in 
the experiments, users had to fill a NM survey 
Table 1. System questionnaire results 
Question
Overall NMPres Cond
NMPres*
Cond
1. The tutor increased my understanding of the subject 0.518 0.898 0.862 4.0 > 3.9 4.0 > 3.9
2. It was easy to learn from the tutor 0.100 0.813 0.947 3.9 > 3.6 3.9 > 3.5
3. The tutor helped me to concentrate 0.016 0.156 0.854 3.5 > 3.0 3.9 >t 3.4
4. The tutor worked the way I expected it to 0.034 0.886 0.157 3.5 > 3.4 3.9 >s 3.1
5. I enjoyed working with the tutor 0.154 0.513 0.917 3.5 > 3.2 3.7 > 3.4
6. Based on my experience using the tutor to learn physics, I 
would like to use such a tutor regularly
0.004 0.693 0.988 3.7 >s 3.2 3.5 >s 3.0
During the conversation with the tutor:
7. ... a high level of concentration is required to follow the tutor 0.004 0.534 0.545 3.5 <s 4.2 3.9 <t 4.3
8. ... the tutor had a clear and structured agenda behind its 
explanations
0.008 0.340 0.104 4.4 >s 3.6 4.3 > 4.1
9. ... it was easy to figure out where the tutor's instruction was 
leading me
0.017 0.472 0.593 4.0 >s 3.4 4.1 > 3.7
10. ... when the tutor asked me a question I knew why it was 
asking me that question 
0.054 0.191 0.054 3.5 ~ 3.5 4.3 >s 3.5
11. ... it was easy to loose track of where I was in the interaction 
with the tutor 
0.012 0.766 0.048 2.5 <s 3.5 2.9 < 3.0
12. ... I knew whether my answer to the tutor's question was 
correct or incorrect
0.358 0.635 0.804 3.5 > 3.3 3.7 > 3.4
13. ... whenever I answered incorrectly, it was easy to know the 
correct answer after the tutor corrected me
0.085 0.044 0.817 3.8 > 3.5 4.3 > 3.9
At the end of the conversation with the tutor:
14. ... it was easy to understand the tutor's main point 0.071 0.056 0.894 4.0 > 3.6 4.4 > 4.1
15. ... I knew what was wrong or missing from my essay 0.340 0.965 0.340 3.9 ~ 3.9 3.7 < 4.0
16. ... I knew how to modify my essay 0.791 0.478 0.327 4.1 > 3.9 3.7 < 3.8
P1       P2
NM     noNM
P2       P1
NM     noNM
Average rating
ANOVA F condition S condition
365
which explicitly asked how the NM helped them, if 
at all. The answers were on the same 1 to 5 scale. 
We find that the majority of users (75%-86%) 
agreed or strongly agreed that the NM helped them 
follow the dialogue, learn more easily, concentrate 
and update the essay. These findings are on par 
with those from the system questionnaire analysis. 
5.2 Objective metrics 
Our analysis of the subjective user evaluations 
shows that users think that the NM is helpful. We 
would like to see if this perceived usefulness is 
reflected in any objective metrics of performance. 
Due to how our experiment was designed, the ef-
fect of the NM can be reliably measured only in 
the first problem as in the second problem the NM 
is toggled3; for the same reason, we can not use the 
pretest/posttest information. 
Our preliminary investigation 4  found several 
dimensions on which the two conditions differed in 
the first problem (F users had NM, S users did 
not). We find that if the NM was present the inter-
action was shorter on average and users gave more 
correct answers; however these differences are not 
statistically significant (Table 2). In terms of 
speech recognition performance, we looked at two 
metrics: AsrMis and SemMis (ASR/Semantic Mis-
recognition). A user turn is labeled as AsrMis if the 
output of the speech recognition is different from 
the human transcript (i.e. a binary version of Word 
Error Rate). SemMis are AsrMis that change the 
correctness interpretation. We find that if the NM 
was present users had fewer AsrMis and fewer 
SemMis (trend for SemMis, p<0.09). 
In addition, a ?2 dependency analysis showed 
that the NM presence interacts significantly with 
both AsrMis (p<0.02) and SemMis (p<0.001), with 
fewer than expected AsrMis and SemMis in the 
                                                 
3 Due to random assignment to conditions, before the 
first problem the F and S populations are similar (e.g. no 
difference in pretest); thus any differences in metrics 
can be attributed to the NM presence/absence. However, 
in the second problem, the two populations are not simi-
lar anymore as they have received different forms of 
instruction; thus any difference has to be attributed to 
the NM presence/absence in this problem as well as to 
the NM absence/presence in the previous problem.  
4 Due to logging issues, 2 S users are excluded from this 
analysis (13 F and 13 S users remaining). We run the 
subjective metric analysis from Section 5.1 on this sub-
set and the results are similar. 
NM condition. The fact that in the second problem 
the differences are much smaller (e.g. 2% for 
AsrMis) and that the NM-AsrMis and NM-
SemMis interactions are not significant anymore, 
suggests that our observations can not be attributed 
to a difference in population with respect to sys-
tem?s ability to recognize their speech. We hy-
pothesize that these differences are due to the NM 
text influencing users? lexical choice. 
Metric F (NM) S (noNM) p 
# user turns 21.8 (5.3) 22.8 (6.5) 0.65 
% correct turns 72% (18%) 67% (22%) 0.59 
AsrMis 37% (27%) 46% (28%) 0.46 
SemMis 5% (6%) 12% (14%) 0.09 
Table 2. Average (standard deviation) for  
objective metrics in the first problem 
6 Related work 
Discourse structure has been successfully used in 
non-interactive settings (e.g. understanding spe-
cific lexical and prosodic phenomena (Hirschberg 
and Nakatani, 1996) , natural language generation 
(Hovy, 1993), essay scoring (Higgins et al, 2004) 
as well as in interactive settings (e.g. predic-
tive/generative models of postural shifts (Cassell et 
al., 2001), generation/interpretation of anaphoric 
expressions (Allen et al, 2001), performance mod-
eling (Rotaru and Litman, 2006)). 
In this paper, we study the utility of the dis-
course structure on the user side of a dialogue sys-
tem. One related study is that of (Rich and Sidner, 
1998). Similar to the NM, they use the discourse 
structure information to display a segmented inter-
action history (SIH): an indented view of the inter-
action augmented with purpose information. This 
paper extends over their work in several areas. The 
most salient difference is that here we investigate 
the benefits of displaying the discourse structure 
information for the users. In contrast, (Rich and 
Sidner, 1998) never test the utility of the SIH. 
Their system uses a GUI-based interaction (no 
speech/text input, no speech output) while we look 
at a speech-based system. Also, their underlying 
task (air travel domain) is much simpler than our 
tutoring task. In addition, the SIH is not always 
available and users have to activate it manually. 
Other visual improvements for dialogue-based 
computer tutors have been explored in the past 
(e.g. talking heads (Graesser et al, 2003)). How-
ever, implementing the NM in a new domain re-
quires little expertise as previous work has shown 
366
that na?ve users can reliably annotate the informa-
tion needed for the NM (Passonneau and Litman, 
1993). Our NM design choices should also have an 
equivalent in a new domain (e.g. displaying the 
recognized user answer can be the equivalent of 
the correct answers). Other NM usages can also be 
imagined: e.g. reducing the length of the system 
turns by removing text information that is implic-
itly represented in the NM. 
7 Conclusions & Future work 
In this paper we explore the utility of the Naviga-
tion Map, a graphical representation of the dis-
course structure. As our first step towards under-
standing the benefits of the NM, we ran a user 
study to investigate if users perceive the NM as 
useful. From the users? perspective, the NM pres-
ence allows them to better identify and follow the 
tutoring plan and to better integrate the instruction. 
It was also easier for users to concentrate and to 
learn from the system if the NM was present. Our 
preliminary analysis on objective metrics shows 
that users? preference for the NM version is re-
flected in more correct user answers and less 
speech recognition problems in the NM version. 
These findings motivate future work in under-
standing the effects of the NM. We would like to 
continue our objective metrics analysis (e.g. see if 
users are better in the NM condition at updating 
their essay and at answering questions that require 
combining facts previously discussed). We also 
plan to run an additional user study with a be-
tween-subjects experimental design geared towards 
objective metrics. The experiment will have two 
conditions: NM present/absent for all problems. 
The conditions will then be compared in terms of 
various objective metrics. We would also like to 
know which information sources represented in the 
NM (e.g. discourse segment purpose, limited hori-
zon, correct answers) has the biggest impact. 
Acknowledgements 
This work is supported by NSF Grants 0328431 
and 0428472. We would like to thank Shimei Pan, 
Pamela Jordan and the ITSPOKE group. 
References 
K. Acomb, J. Bloom, K. Dayanidhi, P. Hunter, P. 
Krogh, E. Levin and R. Pieraccini. 2007. Technical 
Support Dialog Systems: Issues, Problems, and Solu-
tions. In Proc. of Workshop on Bridging the Gap: 
Academic and Industrial Research in Dialog Technologies. 
J. Allen, G. Ferguson, B. N., D. Byron, N. Chambers, 
M. Dzikovska, L. Galescu and M. Swift. 2006. Ches-
ter: Towards a Personal Medication Advisor. Journal 
of Biomedical Informatics, 39(5). 
J. Allen, G. Ferguson and A. Stent. 2001. An architec-
ture for more realistic conversational systems. In 
Proc. of Intelligent User Interfaces. 
J. Cassell, Y. I. Nakano, T. W. Bickmore, C. L. Sidner 
and C. Rich. 2001. Non-Verbal Cues for Discourse 
Structure. In Proc. of ACL. 
A. Graesser, K. Moreno, J. Marineau, A. Adcock, A. 
Olney and N. Person. 2003. AutoTutor improves deep 
learning of computer literacy: Is it the dialog or the 
talking head? In Proc. of Artificial Intelligence in 
Education (AIED). 
B. Grosz and C. L. Sidner. 1986. Attentions, intentions 
and the structure of discourse. Computational Lin-
guistics, 12(3). 
D. Higgins, J. Burstein, D. Marcu and C. Gentile. 2004. 
Evaluating Multiple Aspects of Coherence in Student 
Essays. In Proc. of HLT-NAACL. 
J. Hirschberg and C. Nakatani. 1996. A prosodic analy-
sis of discourse segments in direction-giving mono-
logues. In Proc. of ACL. 
E. Hovy. 1993. Automated discourse generation using 
discourse structure relations. Articial Intelligence, 
63(Special Issue on NLP). 
D. Litman and S. Silliman. 2004. ITSPOKE: An intelli-
gent tutoring spoken dialogue system. In Proc. of 
HLT/NAACL. 
S. Oviatt, R. Coulston and R. Lunsford. 2004. When Do 
We Interact Multimodally? Cognitive Load and Mul-
timodal Communication Patterns. In Proc. of Interna-
tional Conference on Multimodal Interfaces. 
R. Passonneau and D. Litman. 1993. Intention-based 
segmentation: Human reliability and correlation with 
linguistic cues. In Proc. of ACL. 
H. Pon-Barry, K. Schultz, E. O. Bratt, B. Clark and S. 
Peters. 2006. Responding to Student Uncertainty in 
Spoken Tutorial Dialogue Systems. International 
Journal of Artificial Intelligence in Education, 16. 
C. Rich and C. L. Sidner. 1998. COLLAGEN: A Col-
laboration Manager for Software Interface Agents. 
User Modeling and User-Adapted Interaction, 8(3-4). 
M. Rotaru and D. Litman. 2006. Exploiting Discourse 
Structure for Spoken Dialogue Performance Analy-
sis. In Proc. of EMNLP. 
M. Walker, D. Litman, C. Kamm and A. Abella. 2000. 
Towards Developing General Models of Usability 
with PARADISE. Natural Language Engineering. 
367
Exceptionality and Natural Language Learning 
Mihai Rotaru Diane J. Litman 
Computer Science Department 
University of Pittsburgh 
Pittsburgh, PA 15260 
mrotaru, litman @cs.pitt.edu 
 
 
Abstract 
Previous work has argued that memory-based 
learning is better than abstraction-based learn-
ing for a set of language learning tasks. In this 
paper, we first attempt to generalize these re-
sults to a new set of language learning tasks 
from the area of spoken dialog systems and to 
a different abstraction-based learner. We then 
examine the utility of various exceptionality 
measures for predicting where one learner is 
better than the other. Our results show that 
generalization of previous results to our tasks 
is not so obvious and some of the exceptional-
ity measures may be used to characterize the 
performance of our learners. 
1 Introduction 
2 
                                                          
Our paper is a follow-up of the study done by Daele-
mans et al (1999) in which the authors show that keep-
ing exceptional training instances is useful for 
increasing generalization accuracy when natural lan-
guage learning tasks are involved. The tasks used in 
their experiments are: grapheme-phoneme conversion, 
part of speech tagging, prepositional phrase attachment 
and base noun phrase chunking. Their study provides 
empirical evidence that editing exceptional instances 
leads to a decrease in memory-based learner perform-
ance. Next, the memory-based learner is compared on 
the same tasks with a decision-tree learner and their 
results favor the memory-based learner. Moreover, the 
authors provide evidence that the performance of their 
memory-based learner is linked to its property of hold-
ing all instances (including exceptional ones) and gen-
eral properties of language learning tasks (difficultness 
in discriminating between noise and valid exceptions 
and sub-regularities for those tasks). 
We continue on the same track by investigating if 
their results hold on a different set of tasks. Our tasks 
come from the area of spoken dialog systems and have 
smaller datasets and more features (with many of the 
features being numeric, in contrast with the previous 
study that had none). We observe in our experiments 
with these tasks a much smaller exceptionality measure 
range compared with the previous study. Our results 
indicate that the previous results do not generalize to all 
our tasks. 
An additional goal of our research is to investigate a 
new topic by looking into whether exceptionality meas-
ures can be used to characterize the performance of our 
learners: a memory-based learner (IB1-IG) and a rule-
based learner (Ripper). Our results indicate that for 
some of the exceptionality measures we will examine, 
IB1-IG is better for predicting typical instances while 
Ripper is better for predicting exceptional instances. 
We will use the following conventions throughout 
the paper. The term ?exceptional? will be used to label 
instances that do not follow the rules that characterize 
the class they are part of (in language learning terms, 
they are ?bad? examples of their class rules). We will 
use ?typical? as the antonym of this term; it will label 
instances that are good examples of their class rules. 
The fact that an instance is typical should not be con-
fused with an exceptionality measure we will use that 
has the same name (typicality measure). 
Learning methods 
We will use in our study the same memory-based 
learner that was used in the previous study: IB1-IG. The 
abstraction-based learner used in the previous study was 
C5.0 (a commercial implementation of the C4.5 deci-
sion tree learner). In our study we will use a rule-based 
learner, Ripper. Although the two abstraction-based 
learners are different, they share many features (many 
techniques used in rule-based learning have been 
adapted from decision tree learning (Cohen, 1995))1.  
1 We used Ripper because its implementation was available 
and previous studies on our language learning tasks were per-
formed using Ripper  
2.1 IB1-IG 
Our memory-based learner is called IB1-IG and is part 
of TiMBL, a software package developed by the ILK 
Research Group, Tilburg University and the CNTS Re-
search Group, University of Antwerp. TiMBL is a col-
lection of memory-based learners that sit on top of the 
classic k-NN classification kernel with added metrics, 
algorithms, and extra functions.  
Memory-based reasoning is based on the hypothesis 
that humans, in order to react to a new situation, first 
compare the new situation with previously encountered 
situations (which reside in their memory), pick one or 
more similar situations, and react to the new one based 
on how they reacted to those similar situations.  This 
type of learning is also called lazy learning because the 
learner does not build a model from the training data. 
Instead, typically, the whole training set is stored. To 
predict the class for a new instance, the lazy learner 
compares it with stored instances using a similarity met-
ric and the new instance class is determined based on 
the classes of the most similar training instances. At the 
algorithm level, lazy learning algorithms are versions of 
k-nearest neighbor (k-NN) classifiers. 
IB1-IG is a k-NN classifier that uses a weighted 
overlap metric, where a feature weight is automatically 
computed as the Information Gain (IG) of that feature. 
The weighted overlap metric for two instances X and Y 
is defined as: 
?
=
=?
n
i
iii yxwYX
1
),(),( ?  (1) 
where: 
ii
ii
ii
ii
ii
yx
yx
minmax
yx
abs
yx
?
=
???
???
?
?
?
=
 if
 if
else numeric, if
1
0
)(
),(?  
Information gain is computed for every feature in 
isolation by computing the difference in uncertainty 
between situations with or without knowledge of the 
feature value (for more information, see Daelemans et 
al., 2001). These values describe the importance of that 
feature in predicting the class of an instance and are 
used as feature weights. 
 
2.2 
3 
3.1 
Ripper 
Ripper is a fast and effective rule-based learner devel-
oped by William Cohen (Cohen, 1995). The algorithm 
has an overfit-and-simplify learning strategy: first an 
initial rule set is devised by overfitting a part of the 
training set (called the growing set) and then this rule 
set is repeatedly simplified by applying pruning opera-
tors and testing the error reduction on another part of the 
training set (called the pruning set). Ripper produces a 
model consisting of an ordered set of if-then rules. 
There are several advantages to using rule-based 
learners. The most important one is the fact that people 
can understand relatively easy the model learned by a 
rule-based learner compared with the one learned by a 
decision-tree learner, neural network or memory-based 
learner. Also, domain knowledge can be incorporated in 
a rule-based learner by altering the type of rules it can 
learn. Finally, rule-based learners are relatively good at 
filtering the potential noise from the training set. But in 
the context of natural language learning tasks where 
distinguishing between noise and exceptions and sub-
regularities is very hard, this filtering may result in a 
decrease in accuracy. In contrast, memory-based learn-
ers, by keeping all instances around (including excep-
tional ones), may have higher classification accuracy for 
such tasks. 
Exceptionality measures 
One of the main disadvantages of memory-based learn-
ing is the fact that the entire training set is kept. This 
leads to serious time and memory performance draw-
backs if the training set is big enough. Moreover, to 
improve accuracy, one may want to have noisy in-
stances present in the training set pruned. To address 
these problems there has been a lot of work on trying to 
edit part of the training set without hampering the accu-
racy of the predictor. Two types of editing can be done. 
One can edit redundant regular instances (because the 
training set contains a lot of similar instances for that 
class) and/or unproductive instances (the ones that pre-
sent irregularities with respect to the training set space). 
There are many measures that capture both types of 
instances. We will use the ones from the previous study 
(typicality and class prediction strength) and a new one 
called local typicality. Even though these measures were 
devised with the purpose of editing part of the training 
set, they are used in our study and the previous study to 
point out instances that should not be removed, at least 
for language learning tasks. 
Typicality 
We will use the typicality definition from Daelemans et 
al. (1999) which is similar to the definition from Zhang 
(1992). In both cases, a typicality function is defined 
whose extremes correspond to exceptional and typical 
instances. The function requires a similarity measure 
which is defined in both cases as the inverse of the dis-
tance between two instances. The difference between 
the two implementations of typicality is that Zhang 
(1992) defines the distance as the Euclidian distance 
while Daelemans et al (1999) use the normalized 
weighted Manhattan distance from (1). Thus, our simi-
larity measure will be defined as: 
?
=
?=
n
i
iii yxwYXsim
1
)),(1(),( ?  
For every instance X, a subset of the dataset called 
family of X, Fam(X), is defined as being all instances 
from the dataset that have the same class as X. All re-
maining instances form the unrelated instances subset, 
Unr(X). Then, intra-concept similarity is defined as the 
average similarity between X and instances from 
Fam(X) and inter-concept similarity as the average 
similarity between X and instances from Unr(X). 
?
=
=
|)(|
1
))(,(
|)(|
1)(
XFam
i
iXFamXsimXFam
XIntra  
?
=
=
|)(|
1
))(,(
|)(|
1)(
XUnr
i
iXUnrXsimXUnr
XInter  
Finally, typicality of an instance X is defined as the 
ratio of its intra-concept and inter-concept similarity.  
)(
)()(
XInter
XIntraXTypicality =  
The typicality values are interpreted as follows: if 
the value is higher than 1, then that instance has an in-
tra-concept similarity higher than inter-concept similar-
ity, thus one can say that the instance is a good example 
of its class (it is a typical instance). A value less than 1 
implies the opposite: the instance is not a good example 
of its class (it is an exceptional instance). Values around 
1 are called by Zhang boundary instances since they 
seem to reside at the border between concepts. 
3.2 
3.3 
Class prediction strength 
Another measure used in the previous study is the class 
prediction strength (CPS). This measure tries to capture 
the ability of an instance to predict correctly the class of 
a new instance. We will employ the same CPS defini-
tion used in the previous study (the one proposed by 
Salzberg (1990)). In the context of k-NN, predicting the 
class means, typically, that the instance is the closest 
neighbor for a new instance. Thus the CPS function is 
defined as the ratio of the number of times our instance 
is the closest neighbor for an instance of the same class 
and the number of times our instance is the closest 
neighbor for another instance regardless of its class. A 
CPS value of 1 means that if our instance is to influence 
another instance class (by being its closest neighbor) its 
influence is good (in the sense that predicting the class 
using our instance class will result in an accurate predic-
tion). Thus our instance is a good predictor for our class, 
i.e. it is a typical instance. In contrast, a value of 0 indi-
cates a bad predictor for the class and thus labels an 
exception instance. A value of 0.5 will correspond to 
instances at the border between concepts. 
Unlike typicality, when computing CPS, we can en-
counter situations when its value is undefined (zero di-
vided by zero). This means that the instance is not the 
closest neighbor for any other instance. Since there is no 
clear interpretation of instance properties in this case, 
we will set its CPS value to a constant higher than 1 (no 
particular meaning of the value, just to recognize it in 
our graphs).  
Local typicality 
While CPS captures information very close to an in-
stance, typicality as defined by Zhang captures informa-
tion from the entire dataset. But this may not be the 
most desirable measure in cases such as those when a 
concept is made of at least two disjunctive clusters. 
Consider the example from Figure 1. For an instance in 
the center of cluster A1, its similarity with instances 
from the same cluster is very high but very low with 
instances from cluster A2. At the same time, its similar-
ity with instances from class B is somewhere between 
abo in-
stan ave 
com
arou
clus
of F
stan
we 
inst
sets
ity, 
inst
valu
stan
thei
of m
vici
neig
to th
sets
nish
and
puti
Fve two values. When everything is averaged, 
ce intra-concept and inter-concept similarity h
parable values thus leading to a typicality value 
nd 1 even if the instance is highly typical for the 
ter A1. 
A1
To address this problem, we changed the definition 
am(X) and Unr(X). Instead of considering all in-
ces from the dataset when building the two subsets, 
will be using only instances from a vicinity of our 
ance. The typicality computed using these new sub-
 will be called local typicality. To define the vicin-
we used again the similarity metric. When two 
ances are identical, their similarity has the maximum 
e which is the sum of all feature weights. An in-
ce is in the vicinity of another instance if and only if 
r similarity has a value higher than a given percent 
aximum similarity value (using this definition of 
nity instead of a specified number of nearest 
hbors, makes our exceptionality measure adaptive 
e density of the local neighborhood). For our data-
, a percent value of 90% yields the best results fur-
ing a measure that is different from both typicality 
 CPS. 
Like CPS, division by zero can appear when com-
ng local typicality. This means that inter-concept 
B
A2 
igure 1. Class distribution that causes flattening in typicality 
distribution  
similarity is zero and this can only happen if there is no 
instance with a different class in the vicinity of our in-
stance. In this case, if the intra-concept similarity is 
higher than 0 (there is at least one instance from the 
same class in the vicinity) we set the local typicality to a 
maximum value, while if the intra-concept similarity is 
0, then we set the typicality to a minimum value (no one 
in the vicinity of this instance is a good indication of an 
exceptional instance). When inter-concept similarity is 
higher than 0, we will set the local typicality to a mini-
mum value if its intra-concept similarity is 0 (so that we 
will not have a big gap between local typicality values). 
Minimum and maximum values are computed as values 
to the left and right of the local typicality interval for 
non-exceptional cases.   
We can rank our exceptionality measures by the 
level of information they capture (from most general to 
most local): typicality, local typicality and CPS. 
4 Language learning tasks 
The tasks we will be using in our study come from the 
area of spoken dialog systems (SDS). They were all 
designed as methods for potentially improving the dia-
log manager of a SDS system called TOOT (Litman and 
Pan, 2002).  This system provides access to train infor-
mation from the web via telephone and it was developed 
for the purpose of comparing differences in dialog strat-
egy. 
Our tasks are: (1) Identifying user corrections 
(ISCORR), (2) Identifying correction-aware sites 
(STATUS), (3) Identifying concept-level speech recog-
nition errors (CABIN) and (4) Identifying word-level 
speech recognition errors (WERBIN). The first task is a 
binary classification task that labels each user turn as to 
whether or not it is an attempt from the user to correct a 
prior system recognition failure. The second task is a 4-
way classification task that extends the previous one 
with whether or not the user is aware the system made a 
recognition error. The four classes are: normal user turn, 
user only tries to correct the system, user is only aware 
of a system recognition error, and user is both aware of 
and tries to correct the system error. The third and the 
fourth tasks are binary classification tasks that try to 
predict the system speech recognition accuracy when 
recognizing a user turn. CABIN measures a binary ver-
sion of the Concept Accuracy (percent of semantic con-
cepts recognized correctly) while WERBIN measures a 
binary version of the Word Error Rate (percent of words 
recognized incorrectly). 
Data for our tasks was gathered from a corpus of 
2,328 user turns from 152 dialogues between human 
subjects and TOOT. The features used to represent each 
user turn include prosodic information, information 
from the automatic speech recognizer, system condi-
tions and dialog history. Then, each user turn was la-
beled with respect to every classification task. Even 
though our classification tasks share the same data, 
there are clear differences between them. ISCORR and 
STATUS both deal with user corrections which is quite 
different from predicting speech recognition errors 
(handled in WERBIN and CABIN). Moreover, one will 
expect very little noise or no noise at all when manually 
annotating WERBIN and CABIN. For more information 
on our tasks and features, see (Litman et al, 2000; 
Hirschberg et al, 2001; Litman et al, 2001). 
There are a number of dimensions where our tasks 
differ from the tasks from the previous study. First of all 
our datasets are smaller (2,328 instances compared with 
at least 23,898). Second, the number of features used is 
much bigger than the previous study (141 compared 
with 4-11). Moreover, many features from our datasets 
are numeric while the previous study had none. These 
differences will also reflect on our exceptionality meas-
ures values. For example, the smallest range for typical-
ity in the previous study was between 0.43 and 10.57 
while for our tasks it is between 0.9 and 1.1. To explore 
these differences we varied the feature set used. Instead 
of using all the available features (this feature set is 
called All), we restricted the feature set by using only 
non-numeric features (Nonnum ? 22 features). The typi-
cality range increased when using this feature set (0.77-
1.45), but the number of features used was still larger 
than the previous study. For this reason, we next de-
vised two set of features with only 9 (First9) and 15 
features (First15). The features were selected based on 
their information gain (see section 2.1). 
Before proceeding with our results, there is one 
more thing we want to mention. At least half of our in-
stances have one or more missing values and while the 
Ripper implementation offered a way to handle them, 
there was no default handling of missing values in the 
IB1-IG implementation. Thus, we decided to replace 
missing values ourselves before presenting the datasets 
to our learners. In particular there are two types of miss-
ing values: genuine missing values (no value was pro-
vided; we will refer to them as missing values) and 
undefined values. Undefined values come from features 
that are not defined in that user turn (for example, in the 
first user turn, most of the dialog history features were 
undefined because there was no previous user turn).  
For symbolic features, we replaced missing and un-
defined values with a given string for missing values 
and another one for undefined values. For numeric fea-
tures, the problem was more complicated since the dis-
tance metric uses the difference between two numeric 
values and thus, the values used to fix the problem can 
influence the distance between instances. We experi-
mented with different replacement values: to the left and 
right of the interval boundaries for that features, both 
replacement values on one side of the interval or very 
far from the interval boundaries. All experiments with 
the values provided comparable results. For our experi-
ments, missing values were replaced with a value to the 
right of the interval for that feature and undefined val-
ues were replaced with a value to the left of that inter-
val. 
5 Results 
5.1 
In 5.1 we reproduce the editing and comparison experi-
ments from the previous study to see if their results gen-
eralize to our tasks. In 5.2, we move to our next goal: 
characterizing learners? performance using exceptional-
ity measures. Both learners were run using default pa-
rameters2. 
Natural language learning and memory-
based learning 
First, we performed the editing experiments from the 
previous study. The purpose of those experiments was 
to see the impact of editing exceptional and typical in-
stances on the accuracy of the memory-based learner. 
Since our datasets were small, unlike the previous study 
which performed editing only on the first train-test par-
tition of a 10-fold cross validation, we performed the 
editing experiment on all partitions of a 10-fold cross 
validation. For every fold, we edited 0, 1, 2, 5, 10, 20, 
30, 40 and 50% of the training set based on extreme 
values of all our exceptionality criteria. Accuracy after 
editing a given percent was averaged among all folds 
(there is a significant difference in accuracies among 
folds but all folds exhibit a similar trend with the aver-
age). Figure 2 shows our results for the ISCORR dataset 
79.0%
79.5%
80.0%
80.5%
81.0%
81.5%
82.0%
82.5%
83.0%
0 1 2 5 10 20 30 40 50
Percentage of instances removed
A
ve
ra
ge
 a
cc
ur
ac
y
High CPS
Low CPS
High Local Typ.
Low Local Typ.
High Typicality
Low Typicality
 
Figure 2. IB1-IG average accuracy after editing a given percent of the 
training set based on high and low extremes of all exceptionality 
measures (ISCORR dataset with all features) 
                                                          
2 We performed parameter tuning experiments for both predic-
tors: for every fold of a 10-fold cross validation, part of the 
training set was used as a validation set (for tuning parame-
ters). Our results indicate that the tuned parameters depend on 
the fold used and there was no clear gain to accuracy from 
tuning (in some cases there was even loss in accuracy). Inte-
grating tuned parameters with our leave-one-out experiments 
presents additional problems.  
using six types of editing (editing based on low and high 
value for all three criteria). In contrast with the previous 
study, where for all tasks even the smallest editing led 
to significant accuracy decreases, for our task there was 
no clear decrease in performance. Moreover, for some 
criteria (like low local-typicality) we can even see an 
initial increase in performance. Only after editing half 
of the training set is there a clear decrease in perform-
ance for all editing criteria on this task. 
Editing experiments for the other dataset-feature set 
combinations yield similar results. 
Next, we compared the memory-based learner with 
our abstraction-based learner on all tasks. Since the 
datasets were relatively small, we performed leave-one-
out cross validations. Table 1 summarizes our results. 
The baseline used is the majority class baseline. First, 
we run the predictors on all tasks using all features. In 
contrast with the previous study which favored the 
memory-based learner for almost all their tasks, our 
results favor IB1-IG for only two of the four tasks 
(ISCORR and STATUS). In Section 4, we mentioned 
that the typicality range for our tasks was very small 
compared with the previous study. Contrary to what we 
expected, the tasks where IB1-IG performed better were 
the ones with smaller typicality range. To investigate 
the typicality range impact on our predictors, we tried to 
make our datasets similar to the datasets from the previ-
ous study by tackling the feature set. We eliminated all 
numeric features (since the tasks from the previous 
study had none) and performed experiments on the tasks 
that had the less typicality range (again, ISCORR and 
STATUS). Again, when typicality range was increased, 
even though there were no numeric features, IB1-IG 
performed worse than Ripper. IB1-IG error rate in-
creased when using only non-numeric features for both 
tasks compared with the error rate when using all fea-
tures. This observation led us to assume that, at least for 
IB1-IG, some of the relevant features for classification 
were numeric and they were not present in our feature 
set. Thus, we selected two sets of features (First9 and 
First15) based on the features? relevance and performed 
the experiments again on the ISCORR dataset. We can 
 Error rate 
Data-Feat. set IB1-IG Ripper Baseline
Typicality 
range 
Iscorr-All 14.99% 16.15% 28.99% 0.94 - 1.06
Status-All 22.25% 23.71% 43.04% 0.96 - 1.10
Cabin-All 13.10% 12.11% 30.50% 0.90 - 1.12
Werbin-All 17.65% 11.90% 39.22% 0.90 - 1.10
Iscorr-Nonnum 17.01% 16.24% 28.99% 0.81 - 1.49
Status-Nonnum 23.93% 21.99% 43.04% 0.88 - 1.62
Iscorr-First9 17.78% 16.07% 28.99% 0.86 - 1.17
Iscorr-First15 14.69% 14.95% 28.99% 0.88 - 1.14
Table 1. IB1-IG, Ripper and majority class baseline error 
rate on some of our dataset-feature set combinations 
observe that as the number of relevant features is in-
creased, the error rate for both predictors and the typi-
cality range are decreasing and IB1-IG takes the lead 
when the First15 feature set is used. Our results indicate 
that the predictor that performs better depends on the 
task, the number of features and the type of features we 
use.  
To explore why the previous study?s results do not 
generalize in our case, we are planning to replicate these 
experiments on the dialog-act tagging task on the 
Switchboard corpus (a task more similar in size and 
feature types with the previous study than our tasks but 
still in the area of spoken dialog systems ? see Shriberg 
et al (1998)). 
5.2 Characterizing learners? performance 
using exceptionality measures 
The next goal of our study was to see if we can charac-
terize the performance of our predictors on various 
classes of instances defined by our exceptionality crite-
ria. In other words, we wanted to try to answer ques-
tions like: is IB1-IG better at predicting exceptional 
instances than Ripper? How about typical instances? 
Can we combine the two learners and select between 
them based on the instance exceptionality? 
To answer these questions, we performed the leave-
one-out experiments described above and recorded for 
every instance whether our predictors predicted it cor-
rectly or incorrectly. Next, we computed the exception-
ality of every instance using all three measures. Figure 3 
shows the exceptionality distribution using the typicality 
measure for the ISCORR dataset with all features3. The 
0
50
100
150
200
250
300
350
400
450
0.94 0.95 0.96 0.97 0.98 0.99 1.00 1.01 1.02 1.03 1.04 1.05 1.06
Typicality
Fr
eq
ue
nc
y
IB1-IG
Ripper
Full dataset
 
Figure 3. Typicality distribution for all instances, instances correctly 
predicted by IB1-IG and instances correctly predicted by Ripper 
(ISCORR dataset with all features) 
typicality distributions of all instances from the 
ISCORR dataset, of instances correctly predicted by 
IB1-IG, and of instances correctly predicted by Ripper 
are plotted in the figure. The graph shows that for this 
dataset there are a lot of boundary instances, very few 
exceptional instances and few typical instances. The 
typicality range for all our datasets (usually between 
0.85 and 1.15) is far less than the one from the previous 
study (0.43 up to 10 or even 3500). According to Zhang 
(1992) hard concepts are often characterized by small 
typicality spread. Moreover, small typicality spread is 
associated with low accuracy in predicting. 
                                                          
3 For other dataset-feature set combination graphs see: 
http://www.cs.pitt.edu/~mrotaru/exceptionality 
Figure 4 shows the same information as Figure 3, 
but instead of plotting the count, we plot the percentage 
of the instances with typicality between a given interval 
that have been correctly classified by one of the predic-
tors. We can observe that accuracy of both predictors 
increases with typicality. That is, the more typical the 
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0.94 0.95 0.96 0.97 0.98 0.99 1.00 1.01 1.02 1.03 1.04 1.05 1.06
Typicality
Co
rr
ec
tly
 p
re
di
ct
ed
 - 
pe
rc
en
ta
ge
 
IB1-IG
Ripper
 
Figure 4. Percent of instances predicted correctly by IB1-IG and Rip-
per based on instance typicality (ISCORR dataset with all features) 
instance, the more reliable the prediction; the more ex-
ceptional the instance, the more unreliable the predic-
tion. This observation holds for all our dataset-feature 
set combinations. It is not clear for the ISCORR dataset 
whether one predictor is better than the other based on 
the typicality. But for datasets CABIN and WERBIN 
where, overall, IB1-IG did worse than Ripper, the same 
graph (see Figure 5) shows that IB1-IG?s accuracy is 
worse than Ripper?s accuracy when predicting low typi-
cality instances4. Given the problems with typicality if 
the concepts we want to learn are clustered, we decided 
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0.89 0.91 0.93 0.94 0.96 0.98 1.00 1.02 1.04 1.06 1.07 1.09 1.11
T ypicality
Co
rr
ec
tly
 p
re
di
ct
ed
 - 
pe
rc
en
ta
ge
IB1-IG
Ripper
 
Figure 5. Percent of instances predicted correctly by IB1-IG and Rip-
per based on instance typicality (CABIN dataset with all features) 
                                                          
4 It was not our point to investigate statistical significance of 
this trend. As we will see later, this trend is powerful enough 
to yield interesting results when combining the predictors 
based on exceptionality measures. 
to investigate if this observation holds for other excep-
tionality measures. 
We continued the experiments on the other excep-
tionality measures hoping to get more insight into the 
trend observed for typicality. Indeed, Figure 6 (same as 
Figure 4  but using the CPS instead of typicality) shows 
the same trend: IB1-IG is worse than Ripper when pre-
dicting exceptional instances and it is better when pre-
dicting typical instances. The accuracy curves of the 
two predictors seem to cross at a CPS value of 0.5, 
which corresponds to boundary instances. Undefined 
CPS values (0/0) are assigned a value above 1 (the 
rightmost point on the graph). Ripper was the one that 
offered higher accuracy in predicting instances with 
undefined CPS value for almost all datasets (although 
not in Figure 6). The result holds for all our dataset-
feature set combinations. 
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0.00 0.08 0.17 0.25 0.34 0.42 0.51 0.59 0.68 0.76 0.85 0.93 1.02
CPS
Pr
ed
ic
te
d 
co
rr
ec
tly
 - 
pe
rc
en
ta
ge
IB1-IG
Ripper
 
Figure 6. Percent of instances predicted correctly by IB1-IG and Rip-
per based on instance CPS (ISCORR dataset with all features)5 
The experiments with local typicality yield the same 
results: Ripper constantly outperforms IB1-IG for ex-
ceptional instances and they switch places for typical 
instances (see Figure 7). Again, the accuracy curves 
cross at boundary instances (local typicality value of 1) 
and the same observation holds for all dataset-feature 
set combinations. 
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0.91 0.92 0.93 0.95 0.96 0.97 0.99 1.00 1.01 1.03 1.04 1.05 1.07
Local Typicality
Co
rr
ec
tly
 p
re
di
ct
ed
 - 
pe
rc
en
ta
ge
IB1-IG
Ripper
 
Figure 7. Percent of instances predicted correctly by IB1-IG and  
Ripper based on instance local typicality  
(ISCORR dataset with all features) 
                                                          
5 Abrupt movements in curves are caused by small number of 
instances in that class. We expect that a larger dataset will 
smooth our graphs. 
We computed what could be the reduction in error 
rate if we were to employ both predictors and decide 
between them based on the instance exceptionality 
measure. In other words, Ripper prediction was used for 
exceptional instances and for the left-hand side bound-
ary instances (CPS less than 0.5; typicality less than 1; 
local typicality less than 1); otherwise IB1-IG prediction 
was used. The lower bound of this reduction is when we 
perfectly know which of the predictors offer the correct 
prediction (in other words the error rate is the number of 
times both learners furnished wrong predictions). Figure 
8 plots the reduction in error rate achieved when decid-
ing between predictors based on typicality, CPS, local 
typicality and perfect discrimination. The reduction is 
relative to the best performer on that task. While dis-
criminating based on typicality offered no improvement 
relative to the best performer, CPS was able to con-
stantly achieve improvement and local typicality im-
proved in six out of eight cases. CPS improved the error 
rate of the best performer by decreasing it by 1.33% to 
3.18% (absolute percentage). In contrast with CPS, local 
typicality offered, for the cases when it improved the 
accuracy, more improvement decreasing the error rate 
by up to 4.94% (absolute percentage). A possible expla-
nation of this difference can be the fact that local typi-
cality captures much more information than CPS 
(vicinity-level information compared with information 
very close to the instance). 
-20%
-10%
0%
10%
20%
30%
40%
50%
60%
Is
co
rr
-
A
ll
St
at
us
-
A
ll
Ca
bi
n-
A
ll
W
er
bi
n-
A
ll
Is
co
rr
-
N
on
nu
m
St
at
us
-
N
on
nu
m
Is
co
rr
-
Fi
rs
t9
Is
co
rr
-
Fi
rs
t1
5
Typicality
CPS
Local Typicality
Perfect Discr.
 
Figure 8. Reduction in error rate relative to the best performer for 
typicality, CPS, local typicality and prefect discrimination 
In summary, all our exceptionality measures show 
the same trend in predicting ability: Ripper performs 
better than IB1-IG on exceptional instances while IB1-
IG performs better than Ripper on typical instances. 
While the fact that IB1-IG does better on typical in-
stances may be linked to its ability to handle sub-
regularities, we have no interpretation for the fact that 
Ripper does better on exceptional instances. We plan to 
address this by future work that will look at the distance 
between exceptional instances and the instances that 
generated the rule that made the correct prediction for 
those exceptional instances. 
5.3 Current directions 
The previous section showed that we can improve the 
overall accuracy on our datasets if we combine the pre-
diction generated by our learners based on the excep-
tionality measure of the new instance. Unfortunately, all 
our exceptionality measures require the class of the in-
stance. Moreover, for binary classification tasks, since 
all exceptionality criteria are a ratio, changing the in-
stance class will turn an exceptional instance into a 
typical instance. 
To move our results from offline to online, we con-
sidered interpolating the exceptionality value for an 
instance based on its neighbors? exceptionality values 
(the neighbors from the training set). We performed a 
very simple interpolation by using the exceptionality 
value of the closest neighbor (relative to equation (1)). 
While previous observations are not obvious anymore in 
online graphs (there is no clear crossing at boundary 
instances), there is a small improvement over the best 
predictor. Figure 9 shows that even for this simple in-
terpolation there is a small reduction in almost all cases 
in error rate relative to the best performer when using 
online CPS (interpolated CPS). 
-15%
-10%
-5%
0%
5%
10%
15%
20%
Is
co
rr
-A
ll
St
at
us
-A
ll
Ca
bi
n-
A
ll
W
er
bi
n-
A
ll
Is
co
rr
-
N
on
nu
m
St
at
us
-
N
on
nu
m
Is
co
rr
-F
irs
t9
Is
co
rr
-
Fi
rs
t1
5
Offline CPS
Online CPS
 
Figure 9. Reduction in error rate relative to the best performer for 
offline CPS and online CPS 
We are currently investigating more complicated in-
terpolation strategies like learning of a model from the 
training set that will predict the exceptionality value of 
an instance based on its closest neighbors. 
6 Conclusions 
In this paper we attempted to generalize the results of a 
previous study to a new set of language learning tasks 
from the area of spoken dialog systems. Our experi-
ments indicate that previous results do not generalize so 
obviously to the new tasks. Next, we showed that some 
exceptionality measures can be used as means to im-
prove the prediction accuracy on our tasks by combin-
ing the prediction of our learners based on measures of 
instance exceptionality. We observed that our memory-
based learner performs better than the rule-based learner 
on typical instances and they exchange places for excep-
tional instances. We also showed that there is potential 
for moving these results from offline to online by per-
forming a simple interpolation. Future work needs to 
address more complicated methods of interpolation, 
comparison between our method and other attempts to 
combine rule-based learning and memory-based learn-
ing (Domingos, 1996; Golding and Rosenbloom, 1991), 
comparison with ensemble methods, and whether the 
results from this paper generalize to other spoken dialog 
corpora. 
Acknowledgements 
We would like to thank Walter Daelemans and Antal 
van den Bosch for starting us on this work. 
References 
William Cohen. 1995. Fast effective rule induction. ICML. 
Walter Daelemans, Antal van den Bosch, and Jakub Zavrel. 
1999. Forgetting exceptions is harmful in language 
learning. Machine Learning 1999, 34 :11-43. 
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal 
van den Bosch. 2001. TiMBL: Tilburg Memory Based 
Learner, version 4.1, Reference Guide. ILK Technical 
Report ? ILK 01-04. 
Pedro Domingos. 1996. Unifying Instance-Based and Rule-
Based Induction. Machine Learning 1996, 24:141-168 
Andrew R. Golding and Paul S. Rosenbloom. 1991. Improving 
Rule-Based Systems Through Case-Based Reasoning. Proc. 
AAAI. 
Julia Hirschberg, Diane J. Litman, and Marc Swerts. 2001. 
Identifying User Corrections Automatically in Spoken 
Dialogue Systems. Proc. NAACL. 
Diane J. Litman, Julia Hirschberg, and Marc Swerts. 2000. 
Predicting Automatic Speech Recognition Performance 
Using Prosodic Cues. Proc. NAACL. 
Diane J. Litman, Julia Hirschberg, and Marc Swerts. 2001. 
Predicting User Reactions to System Error. Proc. ACL. 
Diane J. Litman, Shimei Pan. 2002. Designing and Evaluating 
an Adaptive Spoken Dialogue System. User Modeling and 
User-Adapted Interaction, 12(2/3):111-137. 
Salzberg, S. 1990. Learning with nested generalised 
exemplars. Kluwer Academic Publishers. 
Elizabeth Shriberg, Rebecca Bates, Paul Taylor, Andreas 
Stolcke, Klaus Ries, Daniel Jurafsky, Noah Coccaro, 
Rachel Martin, Marie Meteer, and Carol Van Ess-Dykema. 
1998. Can prosody aid the automatic classification of 
dialog acts in conversational speech?. Language and 
Speech 41:439?487. 
Jianping Zhang. 1992. Selecting typical instances in 
instance-based learning. Proc. ICML, 470-479. 
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 85?93,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Exploiting Discourse Structure for  
Spoken Dialogue Performance Analysis 
 
 
Mihai Rotaru 
University of Pittsburgh 
Pittsburgh, USA 
mrotaru@cs.pitt.edu 
Diane J. Litman 
University of Pittsburgh 
Pittsburgh, USA 
litman@cs.pitt.edu 
 
  
 
Abstract 
In this paper we study the utility of dis-
course structure for spoken dialogue per-
formance modeling. We experiment with 
various ways of exploiting the discourse 
structure: in isolation, as context infor-
mation for other factors (correctness and 
certainty) and through trajectories in the 
discourse structure hierarchy. Our corre-
lation and PARADISE results show that, 
while the discourse structure is not useful 
in isolation, using the discourse structure 
as context information for other factors 
or via trajectories produces highly predic-
tive parameters for performance analysis. 
1 Introduction 
Predictive models of spoken dialogue system 
(SDS) performance are an important tool for re-
searchers and practitioners in the SDS domain. 
These models offer insights on what factors are 
important for the success of a SDS and allow 
researchers to assess the performance of future 
system improvements without running additional 
costly user experiments. 
One of the most popular models of perform-
ance is the PARADISE framework proposed by 
(Walker et al, 2000). In PARADISE, a set of 
interaction parameters are measured in a SDS 
corpus, and then used in a multivariate linear 
regression to predict the target performance met-
ric. A critical ingredient in this approach is the 
relevance of the interaction parameters for the 
SDS success. A number of parameters that 
measure the dialogue efficiency (e.g. number of 
system/user turns, task duration) and the dia-
logue quality (e.g. recognition accuracy, rejec-
tions, helps) have been shown to be successful in 
(Walker et al, 2000). An extensive set of pa-
rameters can be found in (M?ller, 2005a). 
In this paper we study the utility of discourse 
structure as an information source for SDS per-
formance analysis. The discourse structure hier-
archy has been shown to be useful for other 
tasks: understanding specific lexical and pro-
sodic phenomena (Hirschberg and Nakatani, 
1996; Levow, 2004), natural language generation 
(Hovy, 1993), predictive/generative models of 
postural shifts (Cassell et al, 2001), and essay 
scoring (Higgins et al, 2004). 
We perform our analysis on a corpus of 
speech-based tutoring dialogues. A tutoring SDS 
(Litman and Silliman, 2004; Pon-Barry et al, 
2004) has to discuss concepts, laws and relation-
ships and to engage in complex subdialogues to 
correct student misconceptions. As a result, dia-
logues with such systems have a rich discourse 
structure. 
We perform three experiments to measure 
three ways of exploiting the discourse structure. 
In our first experiment, we test the predictive 
utility of the discourse structure in itself. For ex-
ample, we look at whether the number of pop-up 
transitions in the discourse structure hierarchy 
predicts performance in our system. 
The second experiment measures the utility of 
the discourse structure as contextual information 
for two types of student states: correctness and 
certainty. The intuition behind this experiment is 
that interaction events should be treated differ-
ently based on their position in the discourse 
structure hierarchy. For example, we test if the 
number of incorrect answers after a pop-up tran-
sition has a higher predictive utility than the total 
number of incorrect student answers. In contrast, 
the majority of the previous work either ignores 
this contextual information (M?ller, 2005a; 
Walker et al, 2000) or makes limited use of the 
85
discourse structure hierarchy by flattening it 
(Walker et al, 2001) (Section 5). 
As another way to exploit the discourse struc-
ture, in our third experiment we look at whether 
specific trajectories in the discourse structure are 
indicative of performance. For example, we test 
if two consecutive pushes in the discourse struc-
ture are correlated with higher learning. 
To measure the predictive utility of our inter-
action parameters, we focus primarily on corre-
lations with our performance metric (Section 4). 
There are two reasons for this. First, a significant 
correlation between an interaction parameter and 
the performance metric is a good indicator of the 
parameter?s relevance for PARADISE modeling. 
Second, correlations between factors and the per-
formance metric are commonly used in tutoring 
research to analyze the tutoring/learning process 
(Chi et al, 2001). 
Our correlation and PARADISE results show 
that, while the discourse structure is not useful in 
isolation, using the discourse structure as context 
information for other factors or via trajectories 
produces highly predictive parameters for per-
formance analysis. 
2 Annotation 
Our annotation for discourse structure and stu-
dent state has been performed on a corpus of 95 
experimentally obtained spoken tutoring dia-
logues between 20 students and our system 
ITSPOKE (Litman and Silliman, 2004).  
ITSPOKE is a speech-enabled version of the 
text-based Why2-Atlas conceptual physics tutor-
ing system (VanLehn et al, 2002). When inter-
acting with ITSPOKE, students first type an es-
say answering a qualitative physics problem us-
ing a graphical user interface. ITSPOKE then 
engages the student in spoken dialogue (using 
head-mounted microphone input and speech out-
put) to correct misconceptions and elicit more 
complete explanations, after which the student 
revises the essay, thereby ending the tutoring or 
causing another round of tutoring/essay revision. 
Each student went through the same procedure: 
1) read a short introductory material, 2) took a 
pretest to measure the initial physics knowledge, 
3) work through a set of 5 problems with 
ITSPOKE, and 4) took a posttest similar to the 
pretest. The resulting corpus had 2334 student 
turns and a comparable number of system turns. 
2.1 Discourse structure 
We base our annotation of discourse structure on 
the Grosz & Sidner theory of discourse structure 
(Grosz and Sidner, 1986). A critical ingredient of 
this theory is the intentional structure. According 
to the theory, each discourse has a discourse pur-
pose/intention. Satisfying the main discourse 
purpose is achieved by satisfying several smaller 
purposes/intentions organized in a hierarchical 
structure. As a result, the discourse is segmented 
in discourse segments each with an associated 
discourse segment purpose/intention. This theory 
has inspired several generic dialogue managers 
for spoken dialogue systems (Bohus and Rud-
nicky, 2003). 
 
Figure 1. The discourse structure and transition anno-
tation 
We automate our annotation of the discourse 
structure by taking advantage of the structure of 
the tutored information. A dialogue with 
ITSPOKE follows a question-answer format (i.e. 
system initiative): ITSPOKE asks a question, the 
student provides the answer and then the process 
is repeated. Deciding what question to ask, in 
what order and when to stop is hand-authored 
beforehand in a hierarchical structure that resem-
bles the discourse segment structure (see Figure 
1). Tutor questions are grouped in segments 
which correspond roughly to the discourse seg-
ments. Similarly to the discourse segment pur-
pose, each question segment has an associated 
tutoring goal or purpose. For example, in 
86
ITSPOKE there are question segments discuss-
ing about forces acting on the objects, others dis-
cussing about objects? acceleration, etc. 
In Figure 1 we illustrate ITSPOKE?s behavior 
and our discourse structure annotation. First, 
based on the analysis of the student essay, 
ITSPOKE selects a question segment to correct 
misconceptions or to elicit more complete expla-
nations. This question segment will correspond 
to the top level discourse segment (e.g. DS1). 
Next, ITSPOKE asks the student each question 
in DS1. If the student answer is correct, the sys-
tem moves on to the next question (e.g. Tu-
tor1?Tutor2). If the student answer is incorrect, 
there are two alternatives. For simple questions, 
the system will simply give out the correct an-
swer and move on to the next question (e.g. Tu-
tor3?Tutor4). For complex questions (e.g. apply-
ing physics laws), ITSPOKE will engage into a 
remediation subdialogue that attempts to reme-
diate the student?s lack of knowledge or skills. 
The remediation subdialogue is specified in an-
other question segment and corresponds to a new 
discourse segment (e.g DS2). The new discourse 
segment is dominated by the current discourse 
segment (e.g. DS2 dominated by DS1). Tutor2 
system turn is a typical example; if the student 
answers it incorrectly, ITSPOKE will enter dis-
course segment DS2 and go through its questions 
(Tutor3 and Tutor4). Once all the questions in 
DS2 have been answered, a heuristic determines 
whether ITSPOKE should ask the original ques-
tion again (Tutor2) or simply move on to the next 
question (Tutor5). 
To compute interaction parameters from the 
discourse structure, we focus on the transitions in 
the discourse structure hierarchy. For each sys-
tem turn we define a transition feature. This fea-
ture captures the position in the discourse struc-
ture of the current system turn relative to the 
previous system turn. We define six labels (see 
Table 1). NewTopLevel label is used for the first 
question after an essay submission (e.g. Tutor1). 
If the previous question is at the same level with 
the current question we label the current question 
as Advance (e.g. Tutor2,4). The first question in a 
remediation subdialogue is labeled as Push (e.g. 
Tutor3). After a remediation subdialogue is com-
pleted, ITSPOKE will pop up and it will either 
ask the original question again or move on to the 
next question. In the first case, we label the sys-
tem turn as PopUp. Please note that Tutor2 will 
not be labeled with PopUp because, in such 
cases, an extra system turn will be created be-
tween Tutor4 and Tutor5 with the same content as 
Tutor2. In addition, variations of ?Ok, back to the 
original question? are also included in the new 
system turn to mark the discourse segment 
boundary transition. If the system moves on to 
the next question after finishing the remediation 
subdialogue, we label the system turn as  
PopUpAdv (e.g. Tutor5). Note that while the 
sum of PopUp and PopUpAdv should be equal 
with Push, it is smaller in our corpus because in 
some cases ITSPOKE popped up more than one 
level in the discourse structure hierarchy. In case 
of rejections, the system question is repeated us-
ing variations of ?Could you please repeat that??. 
We label such cases as SameGoal (e.g. Tutor6). 
Discourse structure transitions 
  
Advance 
NewTopLevel 
PopUp 
PopUpAdv 
Push 
SameGoal 
53.4% 
13.5% 
9.2% 
3.5% 
14.5% 
5.9% 
Certainty 
  
Certain 
Uncertain 
Mixed 
Neutral 
41.3% 
19.1% 
2.4% 
37.3% 
Correctness 
  
Correct 
Incorrect 
Partially Correct 
Unable to Answer 
63.3% 
23.3% 
6.2% 
7.1% 
Table 1: Transition and student state distribution. 
Please note that each student dialogue has a 
specific discourse structure based on the dialogue 
that dynamically emerges based on the correct-
ness of her answers. For this reason, the same 
system question in terms of content may get a 
different transition label for different students. 
For example, in Figure 1, if the student would 
have answered Tutor2 correctly, the next tutor 
turn would have had the same content as Tutor5 
but the Advance label. Also, while a human an-
notation of the discourse structure will be more 
complex but more time consuming (Hirschberg 
and Nakatani, 1996; Levow, 2004), its advan-
tages are outweighed by the automatic nature of 
our discourse structure annotation. 
We would like to highlight that our transition 
annotation is domain independent and automatic. 
Our transition labels capture behavior like start-
ing a new dialogue (NewTopLevel), crossing 
discourse segment boundaries (Push, PopUp, 
PopUpAdv) and local phenomena inside a dis-
course segment (Advance, SameGoal). If the dis-
course structure information is available, the 
87
transition information can be automatically com-
puted using the procedure described above. 
2.2 Student state 
Because for our tutoring system student learning 
is the relevant performance metric, we hypothe-
size that information about student state in each 
student turn, in terms of correctness and cer-
tainty, will be an important indicator. For exam-
ple, a student being more correct and certain dur-
ing her interaction with ITSPOKE might be 
indicative of a higher learning gain. Also, 
previous studies have shown that tutoring 
specific parameters can improve the quality of 
SDS performance models that model the learning 
gain (Forbes-Riley and Litman, 2006).  
In our corpus, each student turn was manually 
labeled for correctness and certainty (Table 1). 
While our system assigns a correctness label to 
each student turn to plan its next move, we 
choose to use a manual annotation of correctness 
to eliminate the noise introduced by the auto-
matic speech recognition component and the 
natural language understanding component. A 
human annotator used the human transcripts and 
his physics knowledge to label each student turn 
for various degrees of correctness: correct, par-
tially correct, incorrect and unable to answer. 
?Unable to Answer? label was used for turns 
where the student did not answer the system 
question or used variants of ?I don?t know?. 
Previous work has shown that certainty plays 
an important role in the learning and tutoring 
process (Pon-Barry et al, 2006; VanLehn et al, 
2003). A human annotator listened to the dia-
logues between students and ITSPOKE and la-
beled each student turn for its perceived degree 
of certainness. Four labels were used: certain, 
uncertain, neutral and mixed (both certain and 
uncertain). To date, one annotator has labeled all 
student turns in our corpus1. 
3 Interaction parameters 
For each user, interaction parameters measure 
specific aspects of the dialogue with the system. 
We use our transition and student state annota-
tion to create two types of interaction parame-
                                                 
1 The agreement between the manual correctness an-
notation and the correctness assigned by ITSPOKE is 
90% (kappa of 0.79). In a preliminary agreement 
study, a second annotator labeled our corpus for a 
binary version of certainty (uncertainty versus other), 
resulting in a 90% inter-annotator agreement and a 
kappa of 0.68. 
ters: unigrams and bigrams. The difference be-
tween the two types of parameters is whether the 
discourse structure context is used or not. For 
each of our 12 labels (4 for correctness, 4 for 
certainty and 6 for discourse structure), we de-
rive two unigram parameters per student over the 
5 dialogues for that student: a total parameter 
and a percentage parameter. For example, for the 
?Incorrect? unigram we compute, for each stu-
dent, the total number of student turns labeled 
with ?Incorrect? (parameter Incorrect) and the 
percentage of such student turns out of all stu-
dent turns (parameter Incorrect%). For example, 
if we consider only the dialogue in Figure 1, In-
correct = 3 (Student2,3,5) and Incorrect% = 60% 
(3 out of 5). 
Bigram parameters exploit the discourse struc-
ture context. We create two classes of bigram 
parameters by looking at transition?student state 
bigrams and transition?transition bigrams. The 
transition?student state bigrams combine the in-
formation about the student state with the transi-
tion information of the previous system turn. Go-
ing back to Figure 1, the three incorrect answers 
will be distributed to three bigrams: Advance?
Incorrect (Tutor2?Student2), Push?Incorrect (Tu-
tor3?Student3) and PopUpAdv?Incorrect (Tutor5?
Student5). The transition?transition bigram looks 
at the transition labels of two consecutive system 
turns. For example, the Tutor4?Tutor5 pair will 
be counted as an Advance?PopUpAdv bigram. 
Similar to the unigrams, we compute a total 
parameter and a percentage parameter for each 
bigram. The percentage denominator is number 
of student turns for the transition?student state 
bigrams and the number of system turns minus 
one for the transition?transition bigram. In addi-
tion, for each bigram we compute a relative per-
centage parameter (bigram followed by %rel) by 
computing the percentage relative to the total 
number of times the transition unigram appears 
for that student. For example, we will compute 
the Advance?Incorrect %rel parameter by divid-
ing the number of Advance?Incorrect bigrams 
with the number of Advance unigrams (1 divided 
by 2 in Figure 1); this value will capture the per-
centage of times an Advance transition is fol-
lowed by an incorrect student answer. 
4 Results 
We use student learning as our evaluation metric 
because it is the primary metric for evaluating 
the performance of tutoring systems. Previous 
work (Forbes-Riley and Litman, 2006) has suc-
88
cessfully used student learning as the perform-
ance metric in the PARADISE framework. Two 
quantities are used to measure student learning: 
the pretest score and the posttest score. Both tests 
consist of 40 multiple-choice questions; the test?s 
score is computed as the percentage of correctly 
answered questions. The average score and stan-
dard deviation for each test are: pretest 0.47 
(0.17) and posttest 0.68 (0.17). 
We focus primarily on correlations between 
our interaction parameters and student learning. 
Because in our data the pretest score is signifi-
cantly correlated with the posttest score, we 
study partial Pearson?s correlations between our 
parameters and the posttest score that account for 
the pretest score. This correlation methodology is 
commonly used in the tutoring research (Chi et 
al., 2001). For each trend or significant correla-
tion we report the unigram/bigram, its average 
and standard deviation over all students, the 
Pearson?s Correlation Coefficient (R) and the 
statistical significance of R (p). 
First we report significant correlations for uni-
grams to test our first hypothesis. Next, for our 
second and third experiment, we report correla-
tions for transition?student state and transition?
transition parameters. Finally, we report our pre-
liminary results on PARADISE modeling. 
4.1 Unigram correlations 
In our first proposed experiment, we want to test 
the predictive utility of discourse structure in 
isolation. We compute correlations between our 
transition unigram parameters and learning. We 
find no trends or significant correlations. This 
result suggests that discourse structure in isola-
tion has no predictive utility. 
Here we also report all trends and significant 
correlations for student state unigrams as the 
baseline for contextual correlations to be pre-
sented in Section 4.2. We find only one signifi-
cant correlation (Table 2): students with a higher 
percentage of neutral turns (in terms of certainty) 
are negatively correlated with learning. We hy-
pothesize that this correlation captures the stu-
dent involvement in the tutoring process: more 
involved students will try harder thus expressing 
more certainty or uncertainty. In contrast, less 
involved students will have fewer certain/uncer-
tain/mixed turns and, in consequence, more neu-
tral turns. Surprisingly, student correctness does 
not significantly correlate with learning. 
Parameter Mean (SD) R. p 
Neutral % 37% (8%) -.47 .04 
Table 2: Trend and significant unigram correlations 
4.2 Transition?student state correlations 
For our second experiment, we need to determine 
the predictive utility of transition?student state 
bigram parameters. We find a large number of 
correlations for both transition?correctness bi-
grams and transition?certainty bigrams. 
Transition?correctness bigrams 
This type of bigram informs us whether ac-
counting for the discourse structure transition 
when looking at student correctness has any pre-
dictive value. We find several interesting trends 
and significant correlations (Table 3).  
The student behavior, in terms of correctness, 
after a PopUp or a PopUpAdv transition is very 
informative about the student learning process. 
In both situations, the student has just finished a 
remediation subdialogue and the system is pop-
ping up either by reasking the original question 
again (PopUp) or by moving on to the next ques-
tion (PopUpAdv). We find that after PopUp, the 
number of correct student answers is positively 
correlated with learning. In contrast, the number, 
the percentage and the relative percentage of in-
correct student answers are negatively correlated 
with learning. We hypothesize that this correla-
tion indicates whether the student took advantage 
of the additional learning opportunities offered 
by the remediation subdialogue. By answering 
correctly the original system question (PopUp?
Correct), the student demonstrates that she has 
absorbed the information from the remediation 
dialogue. This bigram is an indication of a suc-
cessful learning event. In contrast, answering the 
original system question incorrectly (PopUp?
Incorrect) is an indication of a missed learning 
opportunity; the more events like this happen the 
less the student learns. 
Parameter Mean (SD) R. p 
PopUp?Correct 7 (3.3) .45 .05 
PopUp?Incorrect 2 (1.8) -.42 .07 
PopUp?Incorrect % 1.6% (1.2%) -.46 .05 
PopUp?Incorrect %rel 17% (13%) -.39 .10 
PopUpAdv?Correct 2.5 (2) .43 .06 
PopUpAdv?Correct % 2% (1.3%) .52 .02 
NewTopLevel?Incorrect 2.3 (1.8) .56 .01 
NewTopLevel?Incorrect % 1.9% (1.4%) .49 .03 
NewTopLevel?Incorrect %rel 15% (12%) .51 .02 
Advance?Correct 40.5 (9.8) .45 .05 
Table 3: Trend and significant transition?correctness 
bigram correlations 
Similarly, being able to correctly answer the 
tutor question after popping up from a remedia-
tion subdialogue (PopUpAdv?Correct) is posi-
tively correlated with learning. Since in many 
cases, these system questions will make use of 
89
the knowledge taught in the remediation subdia-
logues, we hypothesize that this correlation also 
captures successful learning opportunities. 
Another set of interesting correlations is pro-
duced by the NewTopLevel?Incorrect bigram. 
We find that the number, the percentage and the 
relative percentage of times ITSPOKE starts a 
new essay revision dialogue that results in an 
incorrect student answer is positively correlated 
with learning. The content of the essay revision 
dialogue is determined based on ITSPOKE?s 
analysis of the student essay. We hypothesize 
that an incorrect answer to the first tutor question 
is indicative of the system?s picking of a topic 
that is problematic for the student. Thus, we see 
more learning in students for which more knowl-
edge gaps are discovered and addressed by 
ITSPOKE. 
Finally, we find the number of times the stu-
dent answers correctly after an advance transition 
is positively correlated with learning (the Ad-
vance?Correct bigram). We hypothesize that this 
correlation captures the relationship between 
students that advance without having major prob-
lems and a higher learning gains. 
Transition?certainty bigrams 
Next we look at the combination between the 
transition in the dialogue structure and the stu-
dent certainty (Table 4). These correlations offer 
more insight on the negative correlation between 
the Neutral % unigram parameter and student 
learning. We find that out of all neutral student 
answers, those that follow an Advance transi-
tions are negatively correlated with learning. 
Similar to the Neutral % correlation, we hy-
pothesize that Advance?Neutral correlations cap-
ture the lack of involvement of the student in the 
tutoring process. This might be also due to 
ITSPOKE engaging in teaching concepts that the 
student is already familiar with.  
Parameter Mean (SD) R. p 
Advance?Neutral 27 (8.3) -.40 .08 
Advance?Neutral % 21% (6%) -.62 .00 
Advance?Neutral %rel 38% (10%) -.73 .00 
SameGoal?Neutral %rel 35% (31%) .46 .05 
Table 4: Trend and significant transition?certainty 
bigram correlations 
In contrast, staying neutral in terms of cer-
tainty after a system rejection is positively corre-
lated with learning. These correlations show that 
based on their position in the discourse structure, 
neutral student answers will be correlated either 
negatively or positively with learning. 
Unlike student state unigram parameters 
which produce only one significant correlation, 
transition?student state bigram parameters pro-
duce a large number of trend and significant cor-
relations (14). This result suggests that exploiting 
the discourse structure as a contextual informa-
tion source can be beneficial for performance 
modeling. 
4.3 Transition?transition bigrams 
For our third experiment, we are looking at the 
transition?transition bigram correlations (Table 
5). These bigrams help us find trajectories of 
length two in the discourse structure that are as-
sociated with better student learning. Because 
our student state is domain dependent, translating 
the transition?student state bigrams to a new 
domain will require finding a new set of relevant 
factors to replace the student state. In contrast, 
because our transition information is domain in-
dependent, transition?transition bigrams can be 
easily implemented in a new domain.  
The Advance?Advance bigram covers situa-
tions where the student is covering tutoring ma-
terial without major knowledge gaps. This is be-
cause an Advance transition happens when the 
student either answers correctly or his incorrect 
answer can be corrected without going into a 
remediation subdialogue. Just like with the Ad-
vance?Correct correlation (recall Table 3), we 
hypothesize that these correlations links higher 
learning gains to students that cover a lot of ma-
terial without many knowledge gap.  
Parameter Mean (SD) R. p 
Advance?Advance 35 (9.1) .47 .04 
Push?Push 2.2 (1.7) .50 .03 
Push?Push % 1.8% (1.3%) .52 .02 
Push?Push %rel 11% (7%) .52 .02 
SameGoal?Push %rel 18% (23%) .49 .03 
Table 5: Trend and significant transition?transition 
bigram correlations 
The Push?Push bigrams capture another inter-
esting behavior. In these cases, the student incor-
rectly answers a question, entering a remediation 
subdialogue; she also incorrectly answers the 
first question in the remediation dialogue enter-
ing an even deeper remediation subdialogue. We 
hypothesize that these situations are indicative of 
big student knowledge gaps. In our corpus, we 
find that the more such big knowledge gaps are 
discovered and addressed by the system the 
higher the learning gain. 
The SameGoal?Push bigram captures another 
type of behavior after system rejections that is 
positively correlated with learning (recall the 
SameGoal?Neutral bigram, Table 4). In our pre-
vious work (Rotaru and Litman, 2006), we per-
90
formed an analysis of the rejected student turns 
and studied how rejections affect the student 
state. The results of our analysis suggested a new 
strategy for handling rejections in the tutoring 
domain: instead of rejecting student answers, a 
tutoring SDS should make use of the available 
information. Since the recognition hypothesis for 
a rejected student turn would be interpreted most 
likely as an incorrect answer thus activating a 
remediation subdialogue, the positive correlation 
between SameGoal?Push and learning suggests 
that the new strategy will not impact learning. 
Similar to the second experiment, the results 
of our third experiment are also positive: in con-
trast to transition unigrams, our domain inde-
pendent trajectories can produce parameters with 
a high predictive utility. 
4.4 PARADISE modeling 
Here we present our preliminary results on ap-
plying the PARADISE framework to model 
ITSPOKE performance. A stepwise multivariate 
linear regression procedure (Walker et al, 2000) 
is used to automatically select the parameters to 
be included in the model. Similar to (Forbes-
Riley and Litman, 2006), in order to model the 
learning gain, we use posttest as the dependent 
variable and force the inclusion of the pretest 
score as the first variable in the model. 
For the first experiment, we feed the model all 
transition unigrams. As expected due to lack of 
correlations, the stepwise procedure does not 
select any transition unigram parameter. The 
only variable in the model is pretest resulting in a 
model with a R2 of .22. 
For the second and third experiment, we first 
build a baseline model using only unigram pa-
rameters. The resulting model achieves an R2 of 
.39 by including the only significantly correlated 
unigram parameter: Neutral %. Next, we build a 
model using all unigram parameters and all sig-
nificantly correlated bigram parameters. The new 
model almost doubles the R2 to 0.75. Besides the 
pretest, the parameters included in the resulting 
model are (ordered by the degree of contribution 
from highest to lowest): Advance?Neutral %rel, 
and PopUp?Incorrect %. These results strengthen 
our correlation conclusions: discourse structure 
used as context information or as trajectories in-
formation is useful for performance modeling. 
Also, note that the inclusion of student certainty 
in the final PARADISE model provides addi-
tional support to a hypothesis that has gained a 
lot of attention lately: detecting and responding 
to student emotions has the potential to improve 
learning (Craig et al, 2004; Forbes-Riley and 
Litman, 2005; Pon-Barry et al, 2006). 
The performance of our best model is compa-
rable or higher than training performances re-
ported in previous work (Forbes-Riley and Lit-
man, 2006; M?ller, 2005b; Walker et al, 2001). 
Since our training data is relatively small (20 
data points) and overfitting might be involved 
here, in the future we plan to do a more in-depth 
evaluation by testing if our model generalizes on 
a larger ITSPOKE corpus we are currently anno-
tating. 
5 Related work 
Previous work has proposed a large number of 
interaction parameters for SDS performance 
modeling (M?ller, 2005a; Walker et al, 2000; 
Walker et al, 2001). Several information sources 
are being tapped to devise parameters classified 
by (M?ller, 2005a) in several categories: dia-
logue and communication parameters (e.g. dia-
logue duration, number of system/user turns), 
speech input parameters (e.g. word error rate, 
recognition/concept accuracy) and meta-
communication parameters (e.g. number of help 
request, cancel requests, corrections). 
But most of these parameters do not take into 
account the discourse structure information. A 
notable exception is the DATE dialogue act an-
notation from (Walker et al, 2001). The DATE 
annotation captures information on three dimen-
sions: speech acts (e.g. acknowledge, confirm), 
conversation domain (e.g. conversation- versus 
task-related) and the task model (e.g. subtasks 
like getting the date, time, origin, and destina-
tion). All these parameters can be linked to the 
discourse structure but flatten the discourse 
structure. Moreover, the most informative of 
these parameters (the task model parameters) are 
domain dependent. Similar approximations of the 
discourse structure are also common for other 
SDS tasks like predictive models of speech rec-
ognition problems (Gabsdil and Lemon, 2004). 
We extend over previous work in several ar-
eas. First, we exploit in more detail the hierarchi-
cal information in the discourse structure. We 
quantify this information by recording the dis-
course structure transitions. Second, in contrast 
to previous work, our usage of discourse struc-
ture is domain independent (the transitions). 
Third, we exploit the discourse structure as a 
contextual information source. To our knowl-
edge, previous work has not employed parame-
ters similar with our transition?student state bi-
91
gram parameters. Forth, via the transition?
transition bigram parameters, we exploit trajecto-
ries in the discourse structure as another domain 
independent source of information for perform-
ance modeling. Finally, similar to (Forbes-Riley 
and Litman, 2006), we are tackling a more prob-
lematic performance metric: the student learning 
gain. While the requirements for a successful 
information access SDS are easier to spell out, 
the same can not be said about tutoring SDS due 
to the current limited understanding of the hu-
man learning process. 
6 Conclusion 
In this paper we highlight the role of discourse 
structure for SDS performance modeling. We 
experiment with various ways of using the dis-
course structure: in isolation, as context informa-
tion for other factors (correctness and certainty) 
and through trajectories in the discourse structure 
hierarchy. Our correlation and PARADISE re-
sults show that, while the discourse structure is 
not useful in isolation, using the discourse struc-
ture as context information for other factors or 
via trajectories produces highly predictive pa-
rameters for performance analysis. Moreover, the 
PARADISE framework selects in the final model 
only discourse-based parameters ignoring pa-
rameters that do not use the discourse structure 
(certainty and correctness unigrams are ignored). 
Our significant correlations also suggest ways 
we should modify our system. For example, the 
PopUp?Incorrect negative correlations suggest 
that after a failed learning opportunity the system 
should not give out the correct answer but en-
gage in a secondary remediation subdialogue 
specially tailored for these situations. 
In the future, we plan to test the generality of 
our PARADISE model on other corpora and to 
compare models built using our interaction pa-
rameters against models based on parameters 
commonly used in previous work (M?ller, 
2005a). Testing if our results generalize to a hu-
man annotation of the discourse structure and 
automated models of certainty and correctness is 
also of importance. We also want to see if our 
results hold for performance metrics based on 
user satisfaction questionnaires; in the new 
ITSPOKE corpus we are currently annotating, 
each student also completed a user satisfaction 
survey (Forbes-Riley and Litman, 2006) similar 
to the one used in the DARPA Communicator 
multi-site evaluation (Walker et al, 2002).  
Our work contributes to both the computa-
tional linguistics domain and the tutoring do-
main. For the computational linguistics research 
community, we show that discourse structure is 
an important information source for SDS per-
formance modeling. Our analysis can be ex-
tended easily to other SDS. First, a similar auto-
matic annotation of the discourse structure can 
be performed in SDS that rely on dialogue man-
agers inspired by the Grosz & Sidner theory of 
discourse (Bohus and Rudnicky, 2003). Second, 
the transition?transition bigram parameters are 
domain independent. Finally, for the other suc-
cessful usage of discourse structure (transition?
student state bigrams) researchers have only to 
identify relevant factors and then combine them 
with the discourse structure information. In our 
case, we show that instead of looking at the user 
state in isolation (Forbes-Riley and Litman, 
2006), combining it with the discourse structure 
transition can generate informative interaction 
parameters. 
For the tutoring research community, we show 
that discourse structure, an important concept in 
computational linguistics theory, can provide 
useful insights regarding the learning process. 
The correlations we observe in our corpus have 
intuitive interpretations (successful/failed learn-
ing opportunities, discovery of deep student 
knowledge gaps, providing relevant tutoring). 
Acknowledgements 
This work is supported by NSF Grant No. 
0328431. We would like to thank Kate Forbes-
Riley, Joel Tetreault and our anonymous review-
ers for their helpful comments. 
References 
D. Bohus and A. Rudnicky. 2003. RavenClaw: 
Dialog Management Using Hierarchical Task 
Decomposition and an Expectation Agenda. In Proc. 
of Eurospeech. 
J. Cassell, Y. I. Nakano, T. W. Bickmore, C. L. 
Sidner and C. Rich. 2001. Non-Verbal Cues for 
Discourse Structure. In Proc. of ACL. 
M. T. H. Chi, S. A. Siler, H. Jeong, T. Yamauchi 
and R. G. Hausmann. 2001. Learning from human 
tutoring. Cognitive Science, 25. 
S. D. Craig, A. C. Graesser, J. Sullins and B. 
Gholson. 2004. Affect and learning: an exploratory 
look into the role affect in learning with AutoTutor. 
Journal of Educational Media, 29. 
92
K. Forbes-Riley and D. Litman. 2005. Using 
Bigrams to Identify Relationships Between Student 
Certainness States and Tutor Responses in a Spoken 
Dialogue Corpus. In Proc. of SIGdial. 
K. Forbes-Riley and D. Litman. 2006. Modelling 
User Satisfaction and Student Learning in a Spoken 
Dialogue Tutoring System with Generic, Tutoring, 
and User Affect Parameters. In Proc. of 
HLT/NAACL. 
M. Gabsdil and O. Lemon. 2004. Combining 
Acoustic and Pragmatic Features to Predict 
Recognition Performance in Spoken Dialogue 
Systems. In Proc. of ACL. 
B. Grosz and C. L. Sidner. 1986. Attentions, 
intentions and the structure of discourse. 
Computational Lingustics, 12(3). 
D. Higgins, J. Burstein, D. Marcu and C. Gentile. 
2004. Evaluating Multiple Aspects of Coherence in 
Student Essays. In Proc. of HLT-NAACL. 
J. Hirschberg and C. Nakatani. 1996. A prosodic 
analysis of discourse segments in direction-giving 
monologues. In Proc. of ACL. 
E. Hovy. 1993. Automated discourse generation 
using discourse structure relations. Articial 
Intelligence, 63(Special Issue on NLP). 
G.-A. Levow. 2004. Prosodic Cues to Discourse 
Segment Boundaries in Human-Computer Dialogue. 
In Proc. of SIGdial. 
D. Litman and S. Silliman. 2004. ITSPOKE: An 
intelligent tutoring spoken dialogue system. In Proc. 
of HLT/NAACL. 
S. M?ller. 2005a. Parameters for Quantifying the 
Interaction with Spoken Dialogue Telephone Services. 
In Proc. of SIGDial. 
S. M?ller. 2005b. Towards Generic Quality 
Prediction Models for Spoken Dialogue Systems - A 
Case Study. In Proc. of Interspeech. 
H. Pon-Barry, B. Clark, E. O. Bratt, K. Schultz and 
S. Peters. 2004. Evaluating the effectiveness of Scot:a 
spoken conversational tutor. In Proc. of ITS 
Workshop on Dialogue-based Intelligent Tutoring 
Systems. 
H. Pon-Barry, K. Schultz, E. O. Bratt, B. Clark and 
S. Peters. 2006. Responding to Student Uncertainty in 
Spoken Tutorial Dialogue Systems. International 
Journal of Artificial Intelligence in Education, 16. 
M. Rotaru and D. Litman. 2006. Dependencies 
between Student State and Speech Recognition 
Problems in Spoken Tutoring Dialogues. In Proc. of 
ACL. 
K. VanLehn, P. W. Jordan, C. P. Ros?, D. Bhembe, 
M. B?ttner, A. Gaydos, M. Makatchev, U. 
Pappuswamy, M. Ringenberg, A. Roque, S. Siler and 
R. Srivastava. 2002. The Architecture of Why2-Atlas: 
A Coach for Qualitative Physics Essay Writing. In 
Proc. of Intelligent Tutoring Systems (ITS). 
K. VanLehn, S. Siler, C. Murray, T. Yamauchi and 
W. B. Baggett. 2003. Why do only some events cause 
learning during human tutoring? Cognition and 
Instruction, 21(3). 
M. Walker, D. Litman, C. Kamm and A. Abella. 
2000. Towards Developing General Models of 
Usability with PARADISE. Natural Language 
Engineering. 
M. Walker, R. Passonneau and J. Boland. 2001. 
Quantitative and Qualitative Evaluation of Darpa 
Communicator Spoken Dialogue Systems. In Proc. of 
ACL. 
M. Walker, A. Rudnicky, R. Prasad, J. Aberdeen, 
E. Bratt, J. Garofolo, H. Hastie, A. Le, B. Pellom, A. 
Potamianos, R. Passonneau, S. Roukos, G. Sanders, S. 
Seneff and D. Stallard. 2002. DARPA Communicator: 
Cross-System Results for the 2001 Evaluation. In 
Proc. of ICSLP. 
 
 
93
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 178?187,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Discourse Structure and Performance Analysis: 
Beyond the Correlation 
 
Mihai Rotaru 
Textkernel B.V. 
Amsterdam, The Netherlands 
 
mich.rotaru@gmail.com 
Diane J. Litman 
University of Pittsburgh 
Pittsburgh, USA 
 
litman@cs.pitt.edu 
 
  
 
Abstract 
This paper is part of our broader investi-
gation into the utility of discourse struc-
ture for performance analysis. In our pre-
vious work, we showed that several in-
teraction parameters that use discourse 
structure predict our performance metric. 
Here, we take a step forward and show 
that these correlations are not only a sur-
face relationship. We show that redesign-
ing the system in light of an interpreta-
tion of a correlation has a positive impact. 
1 Introduction 
The success of a spoken dialogue system (SDS) 
depends on a large number of factors and the 
strategies employed to address them. Some of 
these factors are intuitive. For example, problems 
with automated speech recognition can derail a 
dialogue from the normal course: e.g. non-
understandings, misunderstandings, end-
pointing, etc. (e.g. (Bohus, 2007; Raux and Es-
kenazi, 2008)). The strategies used to handle or 
avoid these situations are also important and re-
searchers have experimented with many such 
strategies as there is no clear winner in all con-
texts (e.g. (Bohus, 2007; Singh et al, 2002)). 
However, other factors can only be inferred 
through empirical analyses. 
A principled approach to identifying important 
factors and strategies to handle them comes from 
performance analysis. This approach was pio-
neered by the PARADISE framework (Walker et 
al., 2000). In PARADISE, the SDS behavior is 
quantified in the form of interaction parameters: 
e.g. speech recognition performance, number of 
turns, number of help requests, etc. (M?ller, 
2005).These parameters are then used in a multi-
variate linear regression to predict a SDS per-
formance metric (e.g. task completion, user satis-
faction: (Singh et al, 2002)). Finally, SDS redes-
ign efforts are informed by the parameters that 
make it in the regression model. 
Conceptually, this equates to investigating two 
properties of interaction parameters: predictive-
ness and informativeness1. Predictiveness looks 
at the connection between the parameter and sys-
tem performance via predictive models (e.g. mul-
tivariate linear regression in PARADISE). Once 
the predictiveness is established, it is important 
to look at the parameter informativeness. Infor-
mally, informativeness looks at how much the 
parameter can help us improve the system. We 
already know that the parameter is predictive of 
performance. But this does not tell us if there is a 
causal link between the two. In fact, the main 
drive is not to prove a causal link but to show 
that the interaction parameter will inform a modi-
fication of the system and that this modification 
will improve the system. 
This paper is part of our broader investigation 
into the utility of discourse structure for per-
formance analysis. Although each dialogue has 
an inherent structure called the discourse struc-
ture (Grosz and Sidner, 1986), this information 
has received little attention in performance 
analysis settings. In our previous work (Rotaru 
and Litman, 2006), we established the predic-
tiveness of several interaction parameters derived 
from discourse structure. Here we take a step 
further and demonstrate the informativeness of 
these parameters. 
We show that one of the predictive discourse 
structure-based parameters (PopUp-Incorrect) 
informs a promising modification of our system. 
                                                 
1 Although this terminology is not yet established in the 
SDS community, the investigations behind these properties 
are a common practice in the field. 
178
We implement this modification and we compare 
it with the original version of the system through 
a user study. Our analyses indicate that the modi-
fication leads to objective improvements for our 
system (e.g. performance improvements for cer-
tain users but not at the population level and 
fewer system turns). 
2 Background 
ITSPOKE (Intelligent Tutoring Spoken Dia-
logue System) (Litman et al, 2006) is a speech-
enabled version of the text-based Why2-Atlas 
conceptual physics tutoring system (VanLehn et 
al., 2007). The interaction between ITSPOKE 
and users is mediated through a graphical web 
interface supplemented with a headphone-
microphone unit. ITSPOKE first analyzes a user 
typed essay response to a physics problem for 
mistakes and omissions. Then it engages in a 
spoken dialogue to remediate the identified prob-
lems. Finally, users revise their essay and 
ITSPOKE either does another round of tutor-
ing/essay revision if needed or moves on to the 
next problem. 
While for most information access SDS per-
formance is measured using task completion or 
user satisfaction, for the tutoring SDS the pri-
mary performance metric is learning. To measure 
learning, users take a knowledge test before and 
after interacting with ITSPOKE. The Normalized 
Learning Gain (NLG) is defined as (posttest-
pretest)/(1-pretest) and measures the percentage 
improvement relative to the perfect improve-
ment: an NLG of 0.0 means no improvement 
while an NLG of 1.0 means maximum improve-
ment. 
2.1 Discourse structure 
We use the Grosz & Sidner theory of discourse 
(Grosz and Sidner, 1986). According to this the-
ory, dialogue utterances naturally aggregate into 
discourse segments, with each segment having an 
associated purpose or intention. These segments 
are hierarchically organized forming the dis-
course structure hierarchy. This hierarchical as-
pect of dialogue has inspired several generic dia-
logue management frameworks (e.g. RavenClaw 
(Bohus, 2007)). We briefly describe our auto-
matic annotation of this hierarchy and its use 
through discourse transitions. A sample example 
is shown in Appendix 1. For more details see 
(Rotaru and Litman, 2006). 
Since dialogues with ITSPOKE follow a ?tu-
tor question - user answer - tutor response? for-
mat, which is hand-authored beforehand in a hi-
erarchical structure, we can easily approximate 
the discourse structure hierarchy. After the essay 
analysis, ITSPOKE selects a group of questions 
which are asked one by one. These questions 
form the top-level discourse segment (e.g. DS1 
in Appendix 1). For incorrect answers to more 
complex questions (e.g. applying physics laws), 
ITSPOKE will engage in a remediation subdia-
logue that attempts to remediate the student?s 
lack of knowledge or skills. These subdialogues 
form the embedded discourse segments (e.g. DS2 
in Appendix 2). 
We define six discourse transitions in the dis-
course structure hierarchy and use them to label 
each system turn. A NewTopLevel label is used 
for the first question after an essay submission. If 
the previous question is at the same level with 
the current question we label the current question 
as Advance. The first question in a remediation 
subdialogue is labeled as Push. After a remedia-
tion subdialogue is completed, ITSPOKE will 
pop up and a heuristic determines whether to ask 
again the question that triggered the remediation 
dialogue. Reasking is labeled as a PopUp, while 
moving on to the next question is labeled as 
PopUpAdv. Rejections due to speech problems or 
timeouts are labeled as SameGoal. 
Our transitions partially encode the hierarchi-
cal information of discourse structure: they cap-
ture the position of each system turn in this hier-
archy relative to the previous system turn. 
2.2 Discourse structure-based interaction  
parameters 
To derive interaction parameters, we look at 
transition?phenomena and transition?transition 
bigrams. The first type of bigrams is motivated 
by our intuition that dialogue phenomena related 
to performance are not uniformly important but 
have more weight depending on their position in 
the dialogue. For example, it is more important 
for users to be correct at specific places in the 
dialogue rather than overall in the dialogue. We 
use two phenomena related to performance in our 
system/domain: user correctness (e.g. correct, 
incorrect) and user certainty (e.g. uncertain, neu-
tral, etc.). For example, a PopUp-Incorrect event 
occurs whenever users are incorrect after being 
reasked the question that initially triggered the 
remediation dialogue. The second type of bi-
grams is motivated by our intuition that ?good? 
and ?bad? dialogues have different discourse 
structures. To compare two dialogues in terms of 
179
the discourse structure we look at consecutive 
transitions: e.g. Push-Push. 
For each bigram we compute 3 interaction pa-
rameters: a total (e.g. the number of PopUp-
Incorrect events), a percentage (e.g. the number 
of PopUp-Incorrect relative to the number of 
turns) and a relative percentage (e.g. the percent-
age of times a PopUp is followed by an incorrect 
answer). 
3 Predictiveness 
In (Rotaru and Litman, 2006), we demonstrate 
the predictiveness of several discourse structure-
based parameters. Here we summarize the results 
for parameters derived from the PopUp?Correct 
and PopUp?Incorrect bigrams (Table 1). These 
bigrams caught our attention as their predictive-
ness has intuitive interpretations and generalizes 
to other corpora. Predictiveness was measured by 
looking at correlations (i.e. univariate linear re-
gression) between our interaction parameters and 
learning2. We used a corpus of 95 dialogues from 
20 users (2334 user turns). For brevity, we report 
in Table 1 only the bigram, the best Pearson?s 
Correlation Coefficient (R) associated with pa-
rameters derived from that bigram and the statis-
tical significance of this coefficient (p).  
R p
PopUp-Correct 0.45 0.05
PopUp-Incorrect -0.46 0.05
Bigram
 
Table 1. Several discourse structure-based parameters 
significantly correlated with learning  
(for complete results see (Rotaru and Litman, 2006)) 
The two bigrams shed light into user?s learn-
ing patterns. In both cases, the student has just 
finished a remediation subdialogue and the sys-
tem is popping up by reasking the original ques-
tion again (a PopUp transition). We find that cor-
rect answers after a PopUp are positively corre-
lated with learning. In contrast, incorrect answers 
after a PopUp are negatively correlated with 
learning. We hypothesize that these correlations 
indicate whether the user took advantage of the 
additional learning opportunities offered by the 
remediation subdialogue. By answering correctly 
the original system question (PopUp?Correct), 
the user demonstrates that he/she has absorbed 
the information from the remediation dialogue. 
This bigram is an indication of a successful 
learning event. In contrast, answering the origi-
                                                 
2 As it is commonly done in the tutoring research (e.g. (Lit-
man et al, 2006)), we use partial Pearson?s correlations 
between our parameters and the posttest score that account 
for the pretest score. 
nal system question incorrectly (PopUp?
Incorrect) is an indication of a missed learning 
opportunity; the more such events happen the 
less the user learns. 
In  (Rotaru and Litman, 2006) we also demon-
strate that discourse structure is an important 
source for producing predictive parameters. In-
deed, we found that simple correctness parame-
ters (e.g. number of incorrect answers) are sur-
prisingly not predictive in our domain. In con-
trast, parameters that look at correctness at spe-
cific places in the discourse structure hierarchy 
are predictive (e.g. PopUp?Incorrect). 
4 Informativeness 
We investigate the informativeness of the 
PopUp?Incorrect bigram as in (Rotaru, 2008) we 
also show that its predictiveness generalizes to 
two other corpora. We need 3 things for this: an 
interpretation of the predictiveness (i.e. an inter-
pretation of the correlation), a new system strat-
egy derived from this interpretation and a valida-
tion of the strategy. 
As mentioned in Section 3, our interpretation 
of the correlation between PopUp?Incorrect 
events and learning is that these events signal 
failed learning opportunities. The remediation 
subdialogue is the failed learning opportunity: 
the system had a chance to correct user?s lack of 
knowledge and failed to achieve that. The more 
such events we see, the lesser the system per-
formance. 
How can we change the system in light of this 
interpretation? We propose to give additional 
explanations after a PopUp?Incorrect event as 
the new strategy. To arrive at this strategy, we 
hypothesized why the failed opportunity has oc-
curred. The simplest answer is that the user has 
failed to absorb the information from the reme-
diation dialogue. It is possible that the user did 
not understand the remediation dialogue and/or 
failed to make the connection between the reme-
diation dialogue and the original question. The 
current ITSPOKE strategy after a PopUp?
Incorrect is to give away the correct answer and 
move on. The negative correlations indicate that 
this strategy is not working. Thus, maybe it 
would be better if the system will engage in addi-
tional explanations to correct the user. If we can 
make the user understand, then we transform the 
failed learning opportunity into a successful 
learning opportunity. This will be equivalent to a 
PopUp?Correct event which we have seen is 
positively correlated with learning (Section 3). 
180
While other interpretation and hypotheses 
might also be true, our results (Section 5) show 
that the new strategy is successful. This validates 
the interpretation, the strategy and consequently 
the informativeness of the parameter. 
 
4.1 Modification 
To modify the system, we had to implement the 
new PopUp?Incorrect strategy: provide addi-
tional explanations rather than simply giving 
away the correct answer and moving on. But how 
to deliver the additional explanations? One way 
is to engage in an additional subdialogue. How-
ever, this was complicated by the fact that we did 
not know exactly what information to convey 
and/or what questions to ask. It was crucial that 
the information and/or the questions were on tar-
get due to the extra burden of the new subdia-
logue. 
Instead, we opted for a different implementa-
tion of the strategy: interrupt the conversation at 
PopUp?Incorrect events and offer the additional 
explanations in form of a webpage that the user 
will read (recall that ITSPOKE uses in addition a 
graphical web interface ? Section 2). Each poten-
tial PopUp?Incorrect event had an associated 
webpage that is displayed whenever the event 
occurs. Because the information was presented 
visually, users can choose which part to read, 
which meant that we did not have to be on target 
with our explanations. To return to the spoken 
dialogue, users pressed a button when done read-
ing the webpage. 
All webpages included several pieces of in-
formation we judged to be helpful. We included 
the tutor question, the correct answer and a text 
summary of the instruction so far and of the 
remediation subdialogue. We also presented a 
graphical representation of the discourse struc-
ture, called the Navigation Map. Our previous 
work (Rotaru and Litman, 2007) shows that users 
prefer this feature over not having it on many 
subjective dimensions related to understanding. 
Additional information not discussed by the sys-
tem was also included if applicable: intuitions 
and examples from real life, the purpose of the 
question with respect to the current problem and 
previous problems and/or possible pitfalls. See 
Appendix 2 for a sample webpage. 
The information we included in the PopUp?
Incorrect webpages has a ?reflective? nature. For 
example, we summarize and discuss the relevant 
instruction. We also comment on the connection 
between the current problem and previous prob-
lems. The value of ?reflective? information has 
been established previously e.g. (Katz et al, 
2003). 
All webpages and their content were created 
by one of the authors. All potential places for 
PopUp?Incorrect events (i.e. system questions) 
were identified and a webpage was authored for 
each question. There were 24 such places out of 
a total of 96 questions the system may ask during 
the dialogue. 
5 Results 
There are several ways to demonstrate the suc-
cess of the new strategy. First, we can investigate 
if the correlation between PopUp?Incorrect and 
learning is broken by the new strategy. Our re-
sults (5.2) show that this is true. Second, we can 
show that the new system outperforms the old 
system. However, this might not be the best way 
as the new PopUp?Incorrect strategy directly 
affects only people with PopUp?Incorrect events. 
In addition, its effect might depend on how many 
times it was activated. Indeed, we find no sig-
nificant effect of the new strategy in terms of 
performance at the population level. However, 
we find that the new strategy does produce a per-
formance improvement for users that ?needed? it 
the most: users with more PopUp?Incorrect 
events (5.3). 
We begin by describing the user study and 
then we proceed with our quantitative evalua-
tions. 
5.1 User study 
To test the effect of the new PopUp?Incorrect 
strategy, we designed and performed a between-
subjects study with 2 conditions. In the control 
condition (R) we used the regular version of 
ITSPOKE with the old PopUp?Incorrect strategy 
(i.e. give the current answer and move on). In the 
experimental condition (PI), we had the regular 
version of ITSPOKE with the new PopUp?
Incorrect strategy (i.e. give additional informa-
tion). 
The resulting corpus has 22 R users and 25 PI 
users and it is balanced for gender. There are 235 
dialogues and 3909 user turns. The experiment 
took 2? hours per user on average. 
5.2 Breaking the correlation 
The predictiveness of the PopUp?Incorrect bi-
gram (i.e. its negative correlation with learning) 
means that PopUp?Incorrect events signal lower 
performance. One way to validate the effective-
181
ness of the new PopUp?Incorrect strategy is to 
show that it breaks down this correlation. In 
other words, PopUp?Incorrect events no longer 
signal lower performance. Simple correlation 
does not guarantee that this is true because corre-
lation does not necessarily imply causality. 
In our experiment, this translates to showing 
that that PopUp?Incorrect bigram parameters are 
still correlated with learning for R students but 
the correlations are weaker for PI students. 
Table 2 shows these correlations. As in Table 1, 
we show only the bigrams for brevity. 
R p R p
PopUp-Correct 0.60 0.01 0.18 0.40
PopUp-Incorrect -0.65 0.01 -0.18 0.40
Bigram
R  users PI users
 
Table 2. Correlation with learning in each condition 
We find that the connection between user be-
havior after a PopUp transition and learning con-
tinues to be strong for R users. PopUp?Incorrect 
events continue to signal lower performance (i.e. 
a strong significant negative correlation of           
-0.65). PopUp?Correct events signal increased 
performance (i.e. a strong significant positive 
correlation of +0.60). The fact that these correla-
tions generalize across experiments/corpora fur-
ther strengthens the predictiveness of the 
PopUp?Incorrect parameters. 
PopUp-Incorrect (rel %)
N
LG
0% 20% 40% 60% 80%
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
 PI
 R
 
Figure 1. Correlations between a PopUp-Incorrect 
parameter and NLG 
In contrast, for PI users these correlations are 
much weaker with non-significant correlation 
coefficients of -0.18 and 0.18 respectively. In 
other words the new PopUp?Incorrect strategy 
breaks down the observed correlation: PopUp?
Incorrect events are no longer a good indicator of 
lower performance. 
It is interesting to visualize these correlations 
graphically. Figure 1 shows a scatter plot of the 
PopUp?Incorrect relative percentage parameter 
and NLG for each PI and R user. The regression 
lines for the correlation between PopUp?
Incorrect and NLG for PI and R are shown. The 
graph shows that users with less PopUp?
Incorrect events (e.g. less than 30% relative) tend 
to have a higher NLG (0.5 or higher) regardless 
of the condition. However, for users with more 
PopUp?Incorrect events, the behavior depends 
on the condition: R users (crosses) tend to have 
lower NLG (0.5 or lower) while PI users (cir-
cles) tend to cover the whole NLG spectrum (0.2 
to 0.73). Our next analysis will provide objective 
support for this observation. 
5.3 Performance improvements 
The simplest way to investigate the effect of the 
new PopUp?Incorrect strategy is to compare the 
two systems in terms of performance (i.e. learn-
ing). Table 3 shows in the second column the 
learning (NLG) in each condition. We find that 
the new strategy provides a small 0.02 perform-
ance improvement (0.48 vs. 0.46), but this effect 
is far from being significant. A one-way 
ANOVA test finds no significant effect of the 
condition on the NLG (F(1,45)=0.12, p<0.73). 
All Low High
PI 0.48 (0.19) 0.49 (0.21) 0.48 (0.17)
R 0.46 (0.19) 0.56 (0.13) 0.30 (0.18)
PI Split
 
Table 3. System performance (NLG) in each condi-
tion  
(averages and standard deviation in parentheses) 
There are several factors that contribute to this 
lack of significance. First, the new PopUp?
Incorrect strategy is only activated by users that 
have PopUp?Incorrect events. Including users 
without such events in our comparison could 
weaken the effect of the new strategy. Second, 
the impact of the new strategy might depend on 
how many times it was activated. This relates 
back to our hypothesis that that a PopUp?
Incorrect is an instance of a failed learning op-
portunity. If this is true and our new PopUp?
Incorrect strategy is effective, then we should see 
a stronger impact on PI users with a higher 
number of PopUp?Incorrect events compared 
with the similar R users. 
To test if the impact of the strategy depends on 
how many times it was engaged, we split users 
based on their PopUp?Incorrect (PISplit) behav-
ior into two subsets: Low and High. We used the 
182
mean split based on the PopUp?Incorrect relative 
percentage parameter (see the X axis in Figure 
1): users with a parameter value less than 30% go 
into the Low subset (15 PI and 14 R users) while 
the rest go into the High subset (10 PI and 8 R 
users). 
Results are shown in the third and the fourth 
columns in Table 3. To test the significance of 
the effect, we run a two-way factorial ANOVA 
with NLG as the dependent variable and two fac-
tors: PISplit (Low vs. High) and Condition (PI 
vs. R). We find a significant effect of the combi-
nation PISplit ? Condition (F(1,43)=5.13, 
p<0.03). This effect and the results of the post-
hoc tests are visualized in Figure 2. We find that 
PI users have a similar NLG regardless of their 
PopUp?Incorrect behavior while for R, High PI-
Split users learn less than Low PISplit users. 
Posthoc tests indicate that High PISplit R users 
learn significantly less than Low PISplit R users 
(p<0.01) and both categories of PI users 
(p<0.05). In other words, there is an inherent and 
significant performance gap between R users in 
the two subsets. The effect of the new PopUp?
Incorrect strategy is to bridge this gap and bring 
High PISplit users to the performance level of 
the Low PISplit users. This confirms that the new 
PopUp?Incorrect strategy is effective where it is 
most needed (i.e. High PISplit users). 
pi r
condition
0.1
0.2
0.3
0.4
0.5
0.6
0.7
N
LG
 L
 H
 
Figure 2. PISplit ? Condition effect on NLG 
(bars represent 95% confidence intervals) 
It is interesting to note that Low PISplit R us-
ers learn better than both categories of PI users 
although the differences are not significant. We 
hypothesize this happens because not all learning 
issues are signaled by PopUp?Incorrect events: a 
user might still have low learning even if he/she 
does not exhibit any PopUp?Incorrect events. 
Indeed, there are two PI users with a single 
PopUp?Incorrect event but with very low learn-
ing (NLG of 0.00 and 0.14 respectively). It is 
very likely that other things went wrong for these 
users rather than the activation of the new 
PopUp?Incorrect strategy (e.g. they might have 
other misconceptions that are not addressed by 
the remediation subdialogues). In fact, removing 
these two users results in identical NLG averages 
for the two low PISplit subsets. 
5.4 Dialogue duration 
We also wanted to know if the new PopUp?
Incorrect strategy has an effect on measures of 
dialogue duration. The strategy delivers addi-
tional explanations which can result in an in-
crease in the time users spend with the system 
(due to reading of the new instruction). Also, 
when designing tutoring systems researchers 
strive for learning efficiency: deliver increased 
learning as fast as possible.  
Total time
(min)
No. of sys. 
turns
PI 44.2 (6.2) 86.4 (6.8)
R 45.5 (5.7) 90.9 (9.3)  
Table 4. Dialogue duration metrics  
(averages and standard deviation in parentheses) 
We look at two shallow dialogue metrics: dia-
logue time and number of turns. Table 4 shows 
that, in fact, the dialogue duration is shorter for 
PI users on both metrics. A one way ANOVA 
finds a non-significant effect on dialogue time 
(F(1,45)=0.57, p<0.45) but a trend effect for 
number of system turns (F(1,45)=3.72, p<0.06). 
We hypothesize that 2 factors are at play here. 
First, the additional information activated by the 
new PopUp?Incorrect strategy might have a 
positive effect on users? correctness for future 
system questions especially on questions that 
discuss similar topics. As a result, the system has 
to correct the user less and, consequently, finish 
faster. Second, the average total time PI users 
spend reading the additional information is very 
small (about 2 minutes) compared to the average 
dialogue time. 
6 Related work 
Designing robust, efficient and usable spoken 
dialogue systems (SDS) is a complex process 
that is still not well understood by the SDS re-
search community (M?ller and Ward, 2008). 
Typically, a number of evaluation/performance 
183
metrics are used to compare multiple (versions 
of) SDS. But what do these metrics and the re-
sulting comparisons tell us about designing SDS? 
There are several approaches to answering this 
question, each requiring a different level of su-
pervision.  
One approach that requires little human super-
vision is to use reinforcement learning. In this 
approach, the dialogue is modeled as a (partially 
observable) Markov Decision Process (Levin et 
al., 2000; Young et al, 2007). A reward is given 
at the end of the dialogue (i.e. the evaluation 
metric) and the reinforcement learning process 
propagates back the reward to learn what the best 
strategy to employ at each step is. Other semi-
automatic approaches include machine learning 
and decision theoretic approaches (Levin and 
Pieraccini, 2006; Paek and Horvitz, 2004). How-
ever, these semi-automatic approaches are feasi-
ble only in small and limited domains though 
recent work has shown how more complex do-
mains can be modeled (Young et al, 2007). 
An approach that works on more complex 
domains but requires more human effort is 
through performance analysis: finding and tack-
ling factors that affect the performance (e.g. 
PARADISE (Walker et al, 2000)). Central to 
this approach is the quality of the interaction pa-
rameters in terms of predicting the performance 
metric (predictiveness) and informing useful 
modifications of the system (informativeness). 
An extensive set of parameters can be found in 
(M?ller, 2005). 
Our use of discourse structure for performance 
analysis extends over previous work in two im-
portant aspects. First, we exploit in more detail 
the hierarchical information in the discourse 
structure through the domain-independent con-
cept of discourse structure transitions. Most pre-
vious work does not use this information (e.g. 
(M?ller, 2005)) or, if used, it is flattened (Walker 
et al, 2001). Also, to our knowledge, previous 
work has not employed parameters similar to our 
transition?phenomena (transition?correctness in 
this paper) and transition?transition bigram pa-
rameters. In addition, several of these parameters 
are predictive (Rotaru and Litman, 2006). 
Second, in our work we also look at the in-
formativeness while most of the previous work 
stops at the predictiveness step. A notable excep-
tion is the work by (Litman and Pan, 2002). The 
factor they look at is user?s having multiple 
speech recognition problems in the dialogue. 
This factor is well known in the SDS field and it 
has been shown to be predictive of system per-
formance by previous work (e.g. (Walker et al, 
2000)). To test the informativeness of this factor, 
Litman and Pan propose a modification of the 
system in which the initiative and confirmation 
strategies are changed to more conservative set-
tings whenever the event is detected. Their re-
sults show that the modified version leads to im-
provements in terms of system performance (task 
completion). We extend over their work by look-
ing at a factor (PopUp?Incorrect) that was not 
known to be predictive of performance before-
hand. We discover this factor through our em-
pirical analyses of existing dialogues and we 
show that by addressing it (the new PopUp?
Incorrect strategy) we also obtain performance 
improvements (at least for certain users). In addi-
tion, we are looking at a performance metric for 
which significant improvements are harder to 
obtain with small system changes (e.g. (Graesser 
et al, 2003)). 
7 Conclusions 
In this paper we finalize our investigation into 
the utility of discourse structure for SDS per-
formance analysis (at least for our system). We 
use the discourse structure transition information 
in combination with other dialogue phenomena 
to derive a number of interaction parameters (i.e. 
transition?phenomena and transition?transition). 
Our previous work (Rotaru and Litman, 2006) 
has shown that these parameters are predictive of 
system performance. Here we take a step further 
and show that one of these parameters (the 
PopUp?Incorrect bigram) is also informative. 
From the interpretation of its predictiveness, we 
inform a promising modification of our system: 
offer additional explanations after PopUp?
Incorrect events. We implement this modifica-
tion and we compare it with the original system 
through a user study. We find that the modifica-
tion breaks down the negative correlation be-
tween PopUp?Incorrect and system performance. 
In addition, users that need the modification the 
most (i.e. users with more PopUp?Incorrect 
events) show significant improvement in per-
formance in the modified system over corre-
sponding users in the original system. However, 
this improvement is not strong enough to gener-
ate significant differences at the population level. 
Even though the additional explanations add ex-
tra time to the dialogue, overall we actually see a 
small reduction in dialogue duration. 
Our work has two main contributions. First, 
we demonstrate the utility of discourse structure 
184
for performance analysis. In fact, our other work 
(Rotaru and Litman, 2007) shows that discourse 
structure is also useful for other SDS tasks. Sec-
ond, to our knowledge, we are the first to show a 
complete application of the performance analysis 
methodology. We discover a new set of predic-
tive interaction parameters in our system and we 
show how our system can be improved in light of 
these findings. Consequently, we validate per-
formance analysis as an iterative, ?debugging? 
approach to dialogue design. By analyzing cor-
pora collected with an initial version of the sys-
tem, we can identify semi-automatically prob-
lems in the dialogue design. These problems in-
form a new version of the system which will be 
tested for performance improvements. In terms 
of design methodology for tutoring SDS, our re-
sults suggest the following design principle: ?do 
not give up but try other approaches?. In our 
case, we do not give up after a PopUp-Incorrect 
but give additional explanations. 
In the future, we would like to extend our 
work to other systems and domains. This should 
be relatively straightforward as the main ingredi-
ents, the discourse transitions, are domain inde-
pendent. 
Acknowledgments 
This work is supported by the NSF grants 
0328431 and 0428472. We would like to thank 
the ITSPOKE group. 
References 
D. Bohus. 2007. Error Awareness and Recovery in 
Conversational Spoken Language Interfaces. Ph.D. 
Dissertation, Carnegie Mellon University, School 
of Computer Science 
A. Graesser, K. Moreno, J. Marineau, A. Adcock, A. 
Olney and N. Person. 2003. AutoTutor improves 
deep learning of computer literacy: Is it the dialog 
or the talking head? In Proc. of Artificial Intelli-
gence in Education (AIED). 
B. Grosz and C. L. Sidner. 1986. Attentions, inten-
tions and the structure of discourse. Computational 
Linguistics, 12(3). 
S. Katz, D. Allbritton and J. Connelly. 2003. Going 
Beyond the Problem Given: How Human Tutors 
Use Post-Solution Discussions to Support Transfer. 
International Journal of Artificial Intelligence in 
Education (IJAIED), 13. 
E. Levin and R. Pieraccini. 2006. Value-based opti-
mal decision for dialogue systems. In Proc. of 
IEEE/ACL Workshop on Spoken Language Tech-
nology (SLT). 
E. Levin, R. Pieraccini and W. Eckert. 2000. A Sto-
chastic Model of Human Machine Interaction for 
Learning Dialog Strategies. IEEE Transactions on 
Speech and Audio Processing, 8:1. 
D. Litman and S. Pan. 2002. Designing and Evaluat-
ing an Adaptive Spoken Dialogue System. User 
Modeling and User-Adapted Interaction, 12(2/3). 
D. Litman, C. Rose, K. Forbes-Riley, K. VanLehn, D. 
Bhembe and S. Silliman. 2006. Spoken Versus 
Typed Human and Computer Dialogue Tutoring. 
International Journal of Artificial Intelligence in 
Education, 16. 
S. M?ller. 2005. Parameters for Quantifying the In-
teraction with Spoken Dialogue Telephone Services. 
In Proc. of SIGDial. 
S. M?ller and N. Ward. 2008. A Framework for 
Model-based Evaluation of Spoken Dialog Systems. 
In Proc. of Workshop on Discourse and Dialogue 
(SIGDial). 
T. Paek and E. Horvitz. 2004. Optimizing Automated 
Call Routing by Integrating Spoken Dialog Models 
with Queuing Models. In Proc. of HLT-NAACL. 
A. Raux and M. Eskenazi. 2008. Optimizing End-
pointing Thresholds using Dialogue Features in a 
Spoken Dialogue System. In Proc. of 9th SIGdial 
Workshop on Discourse and Dialogue. 
M. Rotaru. 2008. Applications of Discourse Structure 
for Spoken Dialogue Systems. Ph.D. Dissertation, 
University of Pittsburgh, Department of Computer 
Science 
M. Rotaru and D. Litman. 2006. Exploiting Discourse 
Structure for Spoken Dialogue Performance Analy-
sis. In Proc. of EMNLP. 
M. Rotaru and D. Litman. 2007. The Utility of a 
Graphical Representation of Discourse Structure 
in Spoken Dialogue Systems. In Proc. of ACL. 
S. Singh, D. Litman, M. Kearns and M. Walker. 2002. 
Optimizing Dialogue Management with Rein-
forcement Learning: Experiments with the NJFun 
System. Journal of Artificial Intelligence Research, 
(16). 
K. VanLehn, A. C. Graesser, G. T. Jackson, P. Jordan, 
A. Olney and C. P. Rose. 2007. When are tutorial 
dialogues more effective than reading? Cognitive 
Science, 31(1). 
M. Walker, D. Litman, C. Kamm and A. Abella. 2000. 
Towards Developing General Models of Usability 
with PARADISE. Natural Language Engineering. 
M. Walker, R. Passonneau and J. Boland. 2001. 
Quantitative and Qualitative Evaluation of Darpa 
Communicator Spoken Dialogue Systems. In Proc. 
of ACL. 
S. Young, J. Schatzmann, K. Weilhammer and H. Ye. 
2007. The Hidden Information State Approach to 
Dialog Management. In Proc. of ICASSP. 
185
 
 
 
DS 1 
TUTOR1: Consider Newton's laws applied to two 
objects that move together.  What three 
quantities does Newton's Second Law 
describe the relationship between? 
 Student answer1: correct (e.g. force, mass, accel.) 
TUTOR2: If two bodies are connected so that they move 
together and you know the acceleration of the 
first body, what is the acceleration of the 
second body? 
 Student answer2: incorrect (e.g. zero) 
 
 
 
 
 
 
 
 
 
 
 
 
 
TUTOR5: If a force acts on one body such 
that it moves, what happens to the second 
body? 
 Student answer5: incorrect but rejected (e.g. stays) 
TUTOR6: Could you please repeat that? 
? 
DS 2 
TUTOR3: If the two bodies always move 
together and one body speeds up, 
what happens to the other? 
 Student answer3: incorrect (e.g. lags behind) 
TUTOR4: The second body will speed up too. If 
the first body accelerates at a 
particular rate, will the second body 
accelerate at an equal or different 
rate? 
 Student answer4: correct (e.g. equal) 
ESSAY SUBMISSION & ANALYSIS 
 
Appendix 1. Automatic annotation of discourse structure hierarchy and of discourse structure transitions 
 
Discourse structure hierarchy annotation: DS1 is the top level discourse segment. Its purpose is 
to correct misconceptions in user?s essay and/or to elicit more complete explanations for the 
essay. DS2 is an embedded discourse segment which corresponds to the remediation subdia-
logue for question Tutor2. 
 
Discourse structure transition annotation: Each transition labels the system turn at the tip of the 
arrow (e.g. Tutor2 is labeled with Advance). Please note that Tutor2 will not be labeled with 
PopUp because, in such cases, an extra system turn will be created between Tutor4 and Tutor5 
with the same content as Tutor2. This extra turn also includes variations of ?Ok, back to the 
original question? to mark the discourse segment boundary transition. 
 
186
 
 
 
 
 
Appendix 2. Sample additional instructions webpage 
 
Problem discussed by ITSPOKE: Suppose a man is running in a straight line at constant speed. 
He throws a pumpkin straight up. Where will it land? Explain. 
 
Location in the dialogue: For this problem, ITSPOKE discusses what happens during three 
time frames: before pumpkin toss, during pumpkin toss and after pumpkin toss. ITSPOKE is 
currently discussing the forces and the net force on the pumpkin during the toss.  
 
187
