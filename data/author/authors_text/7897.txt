Unigram Language Model for Chinese Word Segmentation 
        Aitao Chen 
Yahoo! Inc. 
701 First Avenue 
Sunnyvale, CA 94089 
aitao@yahoo-inc.com 
Yiping Zhou 
Yahoo! Inc. 
701 First Avenue 
Sunnyvale, CA 94089 
zhouy@yahoo-inc.com
Anne Zhang 
Yahoo! Inc. 
701 First Avenue 
Sunnyvale, CA 94089
annezhangya@   
yahoo.com 
Gordon Sun 
Yahoo! Inc. 
701 First Avenue 
Sunnyvale, CA 94089 
gzsun@yahoo-inc.com
Abstract
This paper describes a Chinese word 
segmentation system based on unigram 
language model for resolving segmen-
tation ambiguities. The system is aug-
mented with a set of pre-processors and 
post-processors to extract new words in 
the input texts. 
1 Introduction
The Yahoo team participated in all four closed 
tasks and all four open tasks at the second inter-
national Chinese word segmentation bakeoff. 
2 System Description 
The underlying algorithm in our word segmenta-
tion system is the unigram language model in 
which words in a sentence are assumed to occur 
independently. For an input sentence, we exam-
ine all possible ways to segment the new sen-
tence with respect to the segmentation 
dictionary, and choose the segmentation of the 
highest probability, which is estimated based on 
the unigram model. 
Our system also has a few preprocessors and 
postprocessors. The main preprocessors include 
recognizers for extracting names of people, 
places and organizations, and recognizer for 
numeric expressions. The proper name recog-
nizers are built based on the maximum entropy 
model, and the numeric expression recognizer is 
built as a finite state automaton. The conditional 
maximum entropy model in our implementation 
is based on the one described in Section 2.5 in 
(Ratnaparkhi, 1998), and features are the same 
as those described in (Xue and Shen, 2003). 
One of the post-processing steps is to com-
bine single characters in the initial segmentation 
if each character in a sequence of characters oc-
curs in a word much more frequently than as a 
word on its own. The other post-processing pro-
cedure checks the segmentation of a text frag-
ment in the input text against the segmentation 
in the training data. If the segmentation pro-
duced by our system is different from the one in 
the training data, we will use the segmentation 
in the training data as the final segmentation. 
More details on the segmentation algorithm and 
the preprocessors and postprocessors can be 
found in (Chen, 2003). 
Our system processes a sentence independ-
ently. For an input sentence, the preprocessors 
are applied to the input sentence to extract nu-
meric expressions and proper names. The ex-
tracted numeric expressions and proper names 
are added to the segmentation dictionary, if they 
are not already in the dictionary. Then the input 
sentence is segmented into words. Finally the 
post-processing procedures are applied to the 
initial segmentation to produce the final seg-
mentation. Our system processes texts encoded 
in UTF-8; and it is used in all 8 tasks. 
3 Results 
Table 1 presents the results of the 10 official 
runs we submitted in all 8 tasks. 
Run id R P F R-
oov 
R-in
as-closed 0.955 0.934 0.947 0.468 0.978 
as-open 0.958 0.938 0.948 0.506 0.978 
138
cityu-closed 0.949 0.931 0.940 0.561 0.980 
cityu-open 0.952 0.937 0.945 0.608 0.980 
pku-closed 0.953 0.946 0.950 0.636 0.972 
pku-open-a 0.964 0.966 0.965 0.841 0.971 
msr-closed-a 0.969 0.952 0.960 0.379 0.985 
msr-closed-b 0.968 0.953 0.960 0.381 0.984 
msr-open-a 0.970 0.957 0.963 0.466 0.984 
msr-open-b 0.971 0.961 0.966 0.512 0.983 
Table 1: Summary of Yahoo official results. 
The first element in the run id is the corpus 
name, as referring to the Academia Sinica cor-
pus, cityu the City University of Hong Kong 
corpus, pku the Peking University Corpus, and 
msr the Microsoft Research corpus. The second 
element in the run id is the type of task, closed
or open. The second column shows the recall, 
the third column the precision, and the fourth 
column F-score. The last two columns present 
the recall of the out-of-vocabulary words and the 
recall of the words in the training data, respec-
tively.
3.1 Closed Tasks 
For the AS closed task run as-closed, we 
manually identified about 15 thousands person 
names and about 4 thousands place names from 
the AS training corpus. We then built a person 
name recognizer and a place name recognizer 
from the AS training data. All the name recog-
nizers we built are based on the maximum en-
tropy model. We also built a rule-based numeric 
expression recognizer implemented as a finite 
state automaton. 
The segmentation dictionary consists of the 
words in the training data with occurrence fre-
quency compiled from the training data. For 
each character, the probability that a character 
occurs in a word is also computed from the 
training data only. 
Each line of texts in the testing data set is 
processed independently. From an input line, 
first the person name recognizer and place name 
recognizer are used to extract person and place 
names; the numeric expression recognizer is 
used to extract numeric expressions. The ex-
tracted new proper names and new numeric ex-
pressions are added to the segmentation 
dictionary with a constant occurrence frequency 
of 0.5 before the input text is segmented. After 
the segmentation, a sequence of single charac-
ters is combined into a single unit if each of the 
characters in the sequence occurs much more 
frequently in a word than as a word on its own. 
The threshold of a character occurring in a word 
is set to 0.80. Also the quad-grams down to uni-
grams in the segmentation are checked against 
the training data. When a text fragment is seg-
mented in a different way by our system than in 
the training data, we use the segmentation of the 
text fragment in the training data as the final 
output. 
The runs cityu-closed and pku-closed are 
produced in the same way. We first manually 
identified the person names and place names in 
the training data, and then built name recogniz-
ers from the training data. The name recognizers 
and numeric expression recognizer are used first 
to extract proper names and numeric expressions 
before segmentation. The post-processing is also 
the same. 
Two runs, named msr-closed-a and msr-
closed-b, respectively, are submitted using the 
Microsoft Research corpus for the closed task. 
Unlike in the other three corpora, the numeric 
expressions are much more versatile, and there-
fore, more difficult to write regular expressions 
to identify them. We manually identified the 
numeric expressions, person names, place 
names, and organization names in the training 
data, and then built maximum entropy model-
based recognizers for extracting numeric expres-
sions and names of people, place, and organiza-
tions. Also the organization names in this corpus 
are not segmented into words like in the other 
three corpora. The organization name recognizer 
is word-based while the other three recognizers 
are character-based. The only difference be-
tween these two runs is that the run msr-closed-
b includes an organization name recognizer 
while the other run msr-closed-a does not. 
3.2 Open Tasks 
For the AS open task, we used a user diction-
ary and a person name recognizer and a place 
name recognizer, both trained on the combined 
AS corpus and the CITYU corpus. However, the 
base dictionary and word frequency counts are 
compiled from only the AS corpus. For the open 
run, we used the annotated AS corpus we ac-
quired from Academia Sinica. Also the phrase 
segmentation table is built from the AS training 
data only. The AS open run as-open was pro-
duced with the new person and place name rec-
ognizers and with the user dictionary. The 
139
performance of the open run is almost the same 
as that of the close run. 
The training data used in the CITYU open 
task is the same as in the closed task. We built a 
person name recognizer and a place name rec-
ognizer from the combined AS and CITYU cor-
pora. In training a recognizer, we only kept the 
sentences that contain at least one person or 
place name. The run cityu-open was produced 
with new person name and place name recog-
nizers trained on the combined corpora but with-
out user dictionary. The base dictionary and 
frequency counts are from the CITYU training 
data. We prepared a user dictionary for the 
CITYU open run but forgot to turn on this fea-
ture in the configuration file. We repeated the 
CITYU open run cityu-open with user diction-
ary. The recall is 0.959; precision is 0.953; and 
F-score is 0.956. 
For the PKU open task run pku-open-a, we 
trained our segmenter from the word-segmented 
People?s Daily corpus covering the period of 
January 1 through June 30, 1998. Our base dic-
tionary with word frequency counts, character 
counts, and phrase segmentation table are built 
from this larger training corpus of about 7 mil-
lion words. The words in this corpus are anno-
tated with part-of-speech categories. Both the 
names of people and the names of places are 
uniquely tagged in this corpus. We created a 
training set for person name recognizer by com-
bining the sentences in the People?s Daily cor-
pus that contain at least one person name with 
the sentences in the MSR training corpus that 
contain at least one person name. The person 
names in the MSR corpus were manually identi-
fied. From the combined training data for person 
names, we built a person name recognizer based 
on the maximum entropy model. The place 
name recognizer was built in the same way. The 
PKU open run pku-open-a was produced using 
the segmenter trained on the 6-month People?s 
Daily corpus with the new person and place 
name recognizer trained on the People?s Daily 
corpus and the MSR corpus. A user dictionary 
of about 100 thousand entries, most being 
proper names, was used in the PKU open run. 
The training data used for the MSR open 
runs is the same MSR training corpus. Our base 
dictionary, together with word frequency counts, 
and phrase segmentation table are built from the 
MSR training data only. The numeric expression 
recognizer is the same as the one used in the 
closed task. The person name recognizer and 
place name recognizer are the same as those 
used in the PKU open task. We built an organi-
zation name recognizer from the People?s Daily 
corpus where organization names are marked. 
For example, the text fragment ?[??/ns ???
/j]nt? is marked by a pair of brackets and tagged 
with ?nt? in the annotated People?s Daily cor-
pus. We extracted all the sentences containing at 
least one organization name and built a word-
based recognizer. The feature templates are the 
same as in person name or place name recog-
nizer. We submitted two MSR open task runs, 
named msr-open-a and msr-open-b, respec-
tively. The only difference between these two 
runs is that the first run msr-open-a did not in-
clude an organization name recognizer, while 
the run msr-open-b used the organization name 
recognizer built on the annotated People?s Daily 
corpus. Both runs were produced with a user 
dictionary, the new person name recognizer and 
new place name recognizer. The increase of F-
score from 0.963 to 0.966 is due to the organiza-
tion name recognizer. While the organization 
name recognizer correctly identified many or-
ganization names, it also generated many false 
positives. So the positive impact was offset by 
the false positives. 
At about 12 hours before the due time, we 
learned that multiple submissions for the same 
task are acceptable. A colleague of ours submit-
ted one PKU open run with the run id ?b? and 
one MSR open run with the run id ?c? in the 
bakeoff official results using a different word 
segmentation system without being tuning for 
the bakeoff. These two open runs are not dis-
cussed in this paper. 
4 Discussions 
The differences between our closed task runs 
and open task runs are rather small for both the 
AS corpus and the CITYU corpus. Our CITYU 
open run would be substantially better had we 
used our user dictionary. The open task run us-
ing the PKU corpus is much better than the 
closed task run. We performed a number of ad-
ditional evaluations in both the PKU closed task 
and the PKU open task. Table 2 below presents 
the evaluation results with different features ac-
tivated in our system. The PKU training corpus 
140
was used in all the experiments presented in Ta-
ble 2.  
Run Features R P F 
1 base-dict 0.9386 0.9095 0.9238 
2 1+num-expr 0.9411 0.9161 0.9285 
3 2+person+place 0.9440 0.9249 0.9343 
4 3+single-char 0.9404 0.9420 0.9412 
5 4+consistency-
checking 
0.9529 0.9464 0.9496 
Table 2: Results with different features applied 
in PKU closed task. 
    Table 3 presents the results with different fea-
tures applied in the PKU open task. The 6-
month annotated People?s Daily corpus was 
used in all the experiments shown in Table 3. 
Run Features R P F 
1 base-dict 0.9523 0.9503 0.9513 
2 1+user-dict 0.9534 0.9565 0.9549 
3 2+num-expr 0.9547 0.9605 0.9576 
4 3+person+place 0.9562 0.9647 0.9604 
5 4+single-char 0.9487 0.9650 0.9568 
6 5+consistency-
checking 
0.9637 0.9664 0.9650 
Table 3: Results with different features applied 
in PKU open task. 
In the features column, base-dict refers to the 
base dictionary built from the training data only; 
user-dict the additional user dictionary; num-
expr the numeric expression recognizer imple-
mented as a finite state automaton; person the 
person name recognizer; place the place name 
recognizer; single-char combining a sequence of 
single characters when each one of them occurs 
in words much more frequently than as a word 
on its own; and lastly consistency-checking 
checking segmentations against the training 
texts and choosing the segmentation in the train-
ing texts if the segmentation of a text fragment 
produced by our system is different from the one 
in the training data. The tables show the results 
with more and more features included. Each run 
in the both tables includes one or two new fea-
tures over the previous run.  The last run num-
bered 5 in Table 2 is our official PKU closed run 
labeled pku-closed in Table 1; and the last run 
numbered 6 in Table 3 is our official PKU open 
run labeled pku-open-a in Table 1. 
The F-score for our closed PKU task run is 
0.950 with all available features, while using the 
larger People?s Daily corpus as training data and 
its dictionary alone, the F-score is 0.9513. So a 
larger training data contributed significantly to 
the increase in performance in our PKU open 
task run. The user dictionary, the numeric ex-
pression recognizer, the person name recognizer, 
and the place name recognizer all contributed to 
the better performance of our PKU closed run 
and open run. Selectively combining sequence 
of single characters appreciably improved the 
precision while marginally decreased the recall 
in the PKU closed run. However, in the open 
task run, combining single characters did not 
result in better performance, probably because 
the new words recovered by combining single 
characters are already in our user dictionary for 
the open run. Finally consistency checking sub-
stantially improved the performance for both the 
closed run and the open run. 
5 Conclusion 
We presented a word segmentation system that 
uses unigram language model to select the most 
probable segmentation among all possible can-
didates for an input text. The system is aug-
mented with proper name recognizers, numeric 
expression recognizers, and post-processing 
modules to extract new words. Overall the rec-
ognizers and the post-processing modules sub-
stantially improved the baseline performance. 
The larger training data set used in the PKU 
open task also significantly increased the per-
formance of our PKU open run. The additional 
user dictionary is another major contributor to 
our better performance in the open tasks over 
the closed tasks. 
References 
Aitao Chen. 2003. Chinese Word Segmentation Us-
ing Minimal Linguistic Knowledge. In: Proceed-
ings of the Second SIGHAN Workshop on 
Chinese Language Processing. 
Nianwen Xue and Libin Shen. 2003. Chinese Word 
Segmentation as LMR Tagging. In: Proceedings of 
the Second SIGHAN Workshop on Chinese Lan-
guage Processing. 
Adwait Ratnaparkhi. 1998. Maximum Entropy Mod-
els for Natural Language Ambiguity Resolution.
Dissertation in Computer and Information Sci-
ence, University of Pennsylvania. 
141
A Two-Stage Approach to Chinese Part-of-Speech Tagging 
Aitao Chen 
Yahoo! Inc. 
701 First Avenue 
Sunnyvale, CA 94089 
aitao@yahoo-inc.com  
Ya Zhang 
Yahoo! Inc. 
701 First Avenue 
Sunnyvale, CA 94089 
yazhang@yahoo-inc.com  
Gordon Sun 
Yahoo! Inc. 
701 First Avenue 
Sunnyvale, CA 94089 
gzsun@yahoo-inc.com 
 
Abstract 
This paper describes a Chinese part-of-
speech tagging system based on the maxi-
mum entropy model. It presents a novel 
two-stage approach to using the part-of-
speech tags of the words on both sides of 
the current word in Chinese part-of-speech 
tagging. The system is evaluated on four 
corpora at the Fourth SIGHAN Bakeoff in 
the close track of the Chinese part-of-
speech tagging task. 
1 Introduction 
A part-of-speech tagger typically assigns a tag to 
each word in a sentence sequentially from left to 
right or in reverse order. When the words are 
tagged from left to right, the part-of-speech tags 
assigned to the previous words are available to the 
tagging of the current word, but not the tags of the 
following words. And when words are tagged from 
right to left, only the tags of the words on the right 
side are available to the tagging of the current 
word. We expect the use of the tags of the words 
on both sides of the current word should improve 
the tagging of the current word. In this paper, we 
present a novel two-stage approach to using the 
tags of the words on both sides of the current word 
in tagging the current word. We train two maxi-
mum entropy part-of-speech taggers on the same 
training data. The difference between the two tag-
gers is that the second tagger uses features involv-
ing the tags of the words on both sides of the cur-
rent word, while the first tagger uses the tags of 
only the previous words. Both taggers assign tags 
to words from left to right. In tagging a new sen-
tence, the first tagger is applied to the testing data, 
and then the second tagger is applied to the output 
of the first tagger to produce the final results.  
We participated in the Chinese part-of-speech 
tagging task at the Fourth International Chinese 
Language Processing Bakeoff. Our Chinese part-
of-speech taggers were trained only on the training 
data provided to the participants, and evaluated on 
four corpora in the close track of the part-of-speech 
tagging task. The words in both the training and 
testing data sets are already segmented into words. 
2 Maximum Entropy POS Tagger 
Maximum entropy model is a machine learning 
algorithm that has been applied to a range of natu-
ral language processing tasks, including part-of-
speech tagging (Ratnaparkhi, 1996). Our Chinese 
part-of-speech taggers are based on the maximum 
entropy model.  
2.1 Maximum Entropy Model 
The conditional maximum entropy model (Berger, 
et. al., 1996) has the form  
)),(exp()|( )(1 yxfxyp
k
kkxZ ?= ?  
where ?= y xypxZ )|()( is a normalization fac-
tor, and k? is a weight parameter associated with 
feature ).,( yxfk  In the context of part-of-speech 
tagging, y is the POS tag assigned to a word, and x 
represents the contextual information regarding the 
word in consideration, such as the surrounding 
words. A feature is a real-valued, typically binary, 
function. For example, we may define a binary fea-
ture which takes the value 1 if the current word of 
X is ?story? and its POS tag is ?NNS?; and 0 other-
wise. Given a set of training examples, the log 
likelihood of the model with Gaussian prior (Chen 
and Rosenfeld, 1999) has the form 
82
Sixth SIGHAN Workshop on Chinese Language Processing
constxypL
k
k
i
ii +?= ?? 2
2
)()(
2
)|(log)( ?
??  
Malouf (2002) compared iterative procedures such 
as Generalized Iterative Scaling (GIS) and Im-
proved Iterative Scaling (IIS) with numerical opti-
mization techniques like limited-memory BFGS 
(L-BFGS) for estimating the maximum entropy 
model parameters and found that L-BFGS outper-
forms the other methods.  The use of L-BFGS re-
quires the computation of the gradient of the log 
likelihood function. The first derivative with re-
spect to parameter k?  is given by 
2~ ),(),(
)(
?
?
?
? k
kpkp
k
yxfEyxfEL ??=
?
?  
where the first term kp fE~  is the feature expecta-
tion with the empirical model, and the second term 
kp fE is the feature expectation with respect to the 
model. In our model training, we used L-BFGS to 
estimate the model parameters by maximizing 
)(?L on the training data. 
 
2.2 Features 
The feature templates used in our part-of-speech 
taggers are presented in Table 1 and Table 2. 
 
Word 
2112 ,,,, ++?? iiiii wwwww  
1111
,211112
,
,,,
+?+?
+++???
iiiii
iiiiiiii
wwwww
wwwwwwww
 
Tag 
121, ??? iii ttt  
Word/Tag  
iiii wtwt 21 , ??  
Special FirstChar, LastChar, Length,  
ForeighWord 
Table 1: Feature templates used in the first stage 
POS tagger. 
 
Tag 
11211 ,, +?+++ iiiii ttttt  
Word/Tag 
21, ++ iiii twtw  
Table 2: Additional feature templates used in the 
second stage POS tagger. 
 
The features are grouped into four categories. The 
first category contains features involving word to-
kens only; the second category consists of features 
involving tags only; the third category has features 
involving both word tokens and tags. And the last 
category has four special features. In the feature 
templates, wi denotes the current word, wi-2 the 
second word to the left, wi-1 the previous word, 
wi+1 the next word, wi+2 the second word to the 
right of the current word, and ti denotes the part-of-
speech tag assigned to the word wi. The FirstChar 
refers to the initial character of a word, and the 
LastChar the final character of a word. The Length 
denotes the length of a word in terms of byte. And 
the feature ForeignWord indicates whether or not a 
word is a foreign word. Table 2 shows additional 
feature templates involving the part-of-speech tags 
of the following one or two words. The features 
involving the tags of the words in the right con-
texts are used only in the second maximum entropy 
POS tagger. Features are generated from the train-
ing data according to the feature templates pre-
sented in Table 1 and Table 2. 
2.3 Training Models 
The four training corpora we received for the Chi-
nese part-of-speech tagging task include the Aca-
demia Sinica corpus (CKIP), the City University 
of Hong Kong corpus (CityU), the National Chi-
nese Corpus (NCC), and the Peking University 
corpus (PKU).  The CKIP corpus and the CityU 
corpus contain texts in traditional Chinese, while 
the NCC corpus and the PKU corpus contain texts 
in simplified Chinese. The texts in all four training 
corpora are segmented into words according to 
different word segmentation guidelines. And the 
words in all training corpora are labeled with part-
of-speech tags using different tag sets.  
Two maximum entropy POS taggers were 
trained on each of the four corpora using our own 
implementation of the maximum entropy model. 
The first-stage POS tagger was trained with only 
the feature templates presented in Table 1, while 
the second-stage POS tagger with the feature tem-
plates presented in both Table 1 and Table 2. 
All the first-stage POS taggers, one for each 
corpus, were trained with the same feature tem-
plates shown in Table 1, and all the second-stage 
POS taggers were trained with the same feature 
templates shown in Table 1 and Table 2. The fea-
ture templates are not necessarily optimal for each 
individual corpus. For simplicity, we chose to ap-
ply the same feature templates to all four corpora. 
83
Sixth SIGHAN Workshop on Chinese Language Processing
The same parameter settings were applied in the 
training of all eight POS taggers. More specifi-
cally, no feature selection was performed. All fea-
tures, including features occurring just once in the 
training data, were retained. The sigma square 
2? was set to 5.0. And the training process was 
terminated when the ratio of the likelihood differ-
ence between the current iteration and the previous 
iteration over the likelihood of the current iteration 
is below the pre-defined threshold or the maximum 
number of iterations, which was set to 400, is 
reached. Both the first-stage POS tagger and the 
second-stage POS tagger were trained on the same 
corpus. 
2.4 Testing the Models 
The POS tagger assigns a part-of-speech tag to 
each word in a new sentence such that the tag se-
quence maximizes the probability p(Y|X), where X 
is the input sentence, and Y the POS tags assigned 
to X. The decoder implements the beam search 
procedure described in (Ratnaparkhi, 1996). At 
each word position, the decoder keeps the top n 
best tag sequences up to that position. The decoder 
also uses a word/tag dictionary, consisting of the 
words in the training data and the tags assigned to 
each word in the training data. During the decod-
ing phase, if a word in the new sentence is found in 
the training data, only the tags that are assigned to 
that word in the training corpus are considered. 
Otherwise, all the tags in the tag set are considered 
for a new word. So the tagger will not assign to a 
word, found in the training data, a tag that is never 
assigned to that word in the training data, even if 
that word should be assigned a new tag that was 
never assigned to the word in the training data. A 
word/tag dictionary is automatically built by col-
lecting all the words in the training corpus and the 
tags assigned to every word in the training corpus. 
The final output is produced in two steps. The 
first-stage POS tagger is applied on the testing data, 
and then the second-stage POS tagger is applied on 
the output of the first POS tagger. The second-
stage tagger uses features involving POS tags of 
the following one or two words. The features in-
volving the tags of following one or two words 
may be erroneous, since the tags assigned to the 
following one or two words by the first-stage tag-
ger may be incorrect. 
3 Evaluation Results 
Five corpora are provided for the Chinese part-of-
speech tagging task at the forth SIGHAN bakeoff. 
We selected four corpora, two in simplified Chi-
nese and two in traditional Chinese.  
  
Corpus Training size 
(tokens) 
Tagset 
size 
No. of tags 
per token 
type 
CityU 1,092,687 44 1.2587 
CKIP 721,551 60 1.1086 
NCC 535,023 60 1.0658 
PKU 1,116,754 103 1.1194 
Table 3:  Training corpus size. 
 
Table 3 shows the training corpus size, the tagset 
size, and the average number of tags per token type.  
The NCC tagset has 60 tags, but nine of the tags 
occurred only once in the training corpus. In all 
four corpora, most of the unique tokens have only 
a single tag. The percentage of token types having 
single tag is 83.29% in CityU corpus; 91.09 in 
CKIP corpus; 94.67 in NCC corpus; and 90.27% in 
PKU corpus. The proportion of token types having 
single tag in CityU corpus is much lower than in 
NCC corpus. In the NCC corpus, the organization 
names, location names, and a sequence of English 
words are all treated as single token, and these long 
single tokens are not ambiguous and are assigned 
to a single part-of-speech tag in the corpus.  
 
corpus Baseline  Testing 
size 
Token/tag OOV-R 
CityU 0.8433 184,314 0.0921 
CKIP 0.8865 91,071 0.0897 
NCC 0.9159 102,344 0.0527 
PKU 0.8805 156,407 0.0594 
Table 4: The testing data size and the baseline per-
formance. 
 
The baseline performance is computed by assign-
ing the most likely tag to each word in the testing 
data. When a word in the testing data is found in 
the training corpus, it is assigned the tag that is 
most frequently assigned to that word in the train-
ing corpus. A new word in the testing data is as-
signed the most frequent tag found in the training 
corpus, which is the common noun in all four cor-
pora. The baseline performances of the four testing 
84
Sixth SIGHAN Workshop on Chinese Language Processing
data sets are presented in Table 4, which also 
shows the percentage of new token/tag in the test-
ing data sets. 
    Our POS taggers are evaluated on four testing 
data sets, one corresponding to each training cor-
pus. We trained eight POS taggers, two on each 
training corpus, and submitted eight runs in total 
on the Chinese part-of-speech tagging task, two 
runs on each testing data set. The first run, labeled 
?a? in Table 5, is produced using the first-stage 
tagger, and the second run, labeled ?b? in Table 5, 
is the output of the second-stage tagger, which is 
applied to the output of the first tagger. For all of 
our runs, only the provided training data are used. 
Table 5 shows the official evaluation results of the 
eight runs we submitted in the close track. The 
third column, labeled ?Total-A?, shows the accu-
racy of the eight runs. The accuracy is the propor-
tion of correctly tagged words in a testing data set. 
Only one tag is assigned to every word in the test-
ing data set. The remaining three labels, ?IV-R?, 
?OOV-R?, and ?MT-R?, may be defined in The 
Fourth SIGHAN Bakeoff overview paper. 
 
 
Corpus Run 
ID 
Total-
A 
IV-R OOV-
R 
MT-R 
CityU a 0.8929 0.9367 0.4608 0.8705 
CityU b 0.8951 0.9389 0.4637 0.8745 
CKIP a 0.9286 0.9618 0.5875 0.9099 
CKIP b 0.9295 0.9629 0.5869 0.9123 
NCC a 0.9525 0.9717 0.6059 0.9135 
NCC b 0.9541 0.9738 0.5998 0.9195 
PKU a 0.9420 0.9648 0.5813 0.9148 
PKU b 0.9450 0.9679 0.5818 0.9252 
Table 5: Official evaluation results of eight runs in 
the close track of the Chinese part-of-speech tag-
ging task. 
4 Discussions 
A Chinese verb can function as a noun, and vice 
versa, without suffix change. In PKU corpus, a 
verb is labeled with the tag ?v?, and a verb that 
functions as a noun is labeled with the tag ?vn?. In 
the PKU-b run, almost half of the incorrectly 
tagged verbs (v) were tagged as verbal noun (vn), 
and slightly more than half of the incorrectly 
tagged verbal nouns (vn) were tagged as verb (v). 
The accuracy of our best runs on all four corpora 
is much higher than the baseline performance. On 
the PKU corpus, the accuracy is increased from the 
baseline performance of 0.8805 to 0.9450, an im-
provement of 7.33% over the baseline. The sec-
ond-stage tagging increased the accuracy on all 
four corpora. On the PKU corpus, the accuracy is 
increased by about 0.32% over the first-stage tag-
ging. The improvement may not seem to be large; 
however, it corresponds to an error reduction by 
5.4%.  
That the accuracy on the CityU corpus is the 
lowest among all four corpora is not surprising, 
given that the CityU testing data set has the highest 
out-of-vocabulary rate, and the CityU training cor-
pus has the highest average number of tags as-
signed to each token type. Furthermore, the CityU 
training corpus has the lowest percentage of tokens 
with only one tag. The POS tagging task on CityU 
corpus seems to be most challenging among the 
four corpora. 
5 Conclusions 
We have described a Chinese part-of-speech tagger 
with maximum entropy modeling. The tagger with 
rich lexical and morphological features signifi-
cantly outperforms the baseline system which as-
signs to a word the most likely tag assigned to that 
word in the training corpus. The use of features 
involving the part-of-speech tags of the following 
words further improves the performance of the 
tagger. 
References 
Adam L. Berger, Stephen A. Della Pietra, and Vincent 
J. Della Pietra. 1996. A Maximum Entropy Approach 
to Natural Language Processing. Computational Lin-
guistics, 22(1):39-71.  
Stanley F. Chen, and Ronald Rosenfeld. 1999. A Gaus-
sion Prior for Smoothing Maximum Entropy Models, 
Technical Report CMU-CS-99-108, Carnegie Mellon 
University. 
Rober Malouf. 2002. A Comparison of Algorithms for 
Maximum Entropy Parameter Estimation, Proceed-
ings of CoNLL-2002. 
Adwait Ratnaparkhi. 1996. A Maximum Entropy Model 
for Part-of-Speech Tagging, Proceedings of the Con-
ference on Empirical Methods in Natural Language 
Processing, pp 133-142.  
 
85
Sixth SIGHAN Workshop on Chinese Language Processing
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 173?176,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Chinese Named Entity Recognition with Conditional Probabilistic 
Models 
 
 
Aitao Chen 
Yahoo 
701 First Avenue 
Sunnyvale, CA 94089 
aitao@yahoo-inc.com 
Fuchun Peng 
Yahoo 
701 First Avenue 
Sunnyvale, CA 94089 
fuchun@yahoo-inc.com 
 
Roy Shan 
Yahoo 
701 First Avenue 
Sunnyvale, CA 94089 
                   rshan@yahoo-inc.com 
Gordon Sun 
Yahoo 
701 First Avenue 
Sunnyvale, CA 94089 
gzsun@yahoo-inc.com 
 
Abstract 
This paper describes the work on Chinese 
named entity recognition performed by 
Yahoo team at the third International 
Chinese Language Processing Bakeoff. 
We used two conditional probabilistic 
models for this task, including condi-
tional random fields (CRFs) and maxi-
mum entropy models. In particular, we 
trained two conditional random field rec-
ognizers and one maximum entropy rec-
ognizer for identifying names of people, 
places, and organizations in un-
segmented Chinese texts. Our best per-
formance is 86.2% F-score on MSRA 
dataset, and 88.53% on CITYU dataset. 
1 Introduction 
At the third International Chinese Language 
Processing Bakeoff, we participated in the closed 
test in the Named Entity Recognition (NER) task 
using the MSRA corpus and the CITYU corpus. 
The named entity types include person, place, 
and organization.  The training data consist of 
texts that are segmented into words with names 
of people, places, and organizations labeled. And 
the testing data consist of un-segmented Chinese 
texts, one sentence per line. 
There are many well known models for Eng-
lish named recognition, among which Condi-
tional Random Fields (Lafferty et al 2001) and 
maximum entropy models (Berger et al 2001) 
have achieved good performance in English in 
CoNLL NER tasks. To understand the perform-
ance of these two models on Chinese, we both 
models to Chinese NER task on MSRA data and 
CITYU data.  
2 Named Entity Recognizer 
2.1 Models 
We trained two named entity recognizers based 
on conditional random field and one based on 
maximum entropy model.  Both conditional ran-
dom field and maximum entropy models are ca-
pable of modeling arbitrary features of the input, 
thus are well suit for many language processing 
tasks. However, there exist significant differ-
ences between these two models. To apply a 
maximum entropy model to NER task, we have 
to first train a maximum entropy classifier to 
classify each individual word and then build a 
dynamic programming for sequence decoding. 
While in CRFs, these two steps are integrated 
together. Thus, in theory, CRFs are superior to 
maximum entropy models in sequence modeling 
problem and this will also confirmed in our Chi-
nese NER experiments. The superiority of CRFs 
on Chinese information processing was also 
demonstrated in word segmentation (Peng et al 
2004). However, the training speed of CRFs is 
much slower than that of maximum entropy 
models since training CRFs requires expensive 
forward-backward algorithm to compute the par-
tition function. 
 
173
We used Taku?s CRF package1  to train the first 
CRF recognizer, and the MALLET 2  package 
with BFGS optimization to train the second CRF 
recognizer. We used a C++ implementation3 of 
maximum entropy modeling and wrote our own 
second order dynamic programming for decod-
ing. 
 
2.2 Features 
The first CRF recognizer used the features C-2, C-
1, C0, C-1, C2, C-2C-1, C-1C0, C0C-1, C1C2, and C-
1C1, where C0 is the current character, C1 the next 
character, C2 the second character after C0, C-1 
the character preceding C0, and C-2 the second 
character before C0.  
The second CRF recognizer used the same set 
of basic features but the feature C2. In addition, 
the first CRF recognizer used the tag bigram fea-
ture, and the second CRF recognizer used word 
and character cluster features, obtained automati-
cally from the training data only with distribu-
tional word clustering (Tishby and Lee, 1993). 
The maximum entropy recognizer used the 
following unigram, bigram features, and type 
features: C-2, C-1, C0, C1, C2, C-4C-3, C-3C-2, C-2C-1, 
C-1C0, C0C1, C1C2, C2C3, C3C4, and T-2T-1. 
When using the first CRF package, we found 
the labeling scheme OBIE performs better than 
the OBIE scheme.  In the OBI scheme, the first 
character of a named entity is labeled as ?B?, the 
remaining characters, including the last character, 
are all labeled as ?I?. And any character that is 
not part of a named entity is labeled as ?O?. In 
the OBIE scheme, the last character of a named 
entity is labeled as ?E?. The other characters are 
labeled in the same way as in OBIE scheme. The 
first CRF recognizer used the OBIE labeling 
scheme, and the second CRF recognizer used the 
OBI scheme. 
We tried a window size of seven characters 
(three characters preceding the current character 
and three characters following the current char-
acter) with almost no difference in performance 
from using the window size of five characters. 
When a named entity occurs frequently in the 
training data, there is a very good chance that it 
will be recognized when appearing in the testing 
data. However, for entity names of rare occur-
rence, they are much harder to recognize in the 
                                                 
1 Available from http://chasen.org/~taku/software/CRF++ 
2 Available at http://mallet.cs.umass.edu 
3 Available at 
http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.htm 
testing data. Thus it may be beneficial to exam-
ine the testing data to identify the named entities 
that occur in the training data, and assign them 
the same label as in the training data. From the 
training data, we extracted the person names of 
at least three characters, the place names of at 
least four characters, and the organization names 
of at least four characters. We removed from the 
dictionary the named entities that are also com-
mon words. We did not include the short names 
in the dictionary because they may be part of 
long names. We produced a run first using one of 
the NER recognizers, and then replaced the la-
bels of a named entity assigned by a recognizer 
with the labels of the same named entity in the 
training data without considering the contexts.  
3 Results 
Run ID Precision Recall F-Score 
msra_a 91.22% 81.71% 86.20 
msra_b 88.43% 82.88% 85.56 
msra_f 88.45% 79.31% 83.63 
msra_g 86.61% 80.32% 83.35 
msra_r 87.48% 71.68% 78.80 
Table 1: Official results in the closed test of the 
NER task on MSRA corpus. 
 
Table 1 presents the official results of five runs 
in the closed test of the NER task on MSRA cor-
pus.  The first two runs, msra_a and msra_b, are 
produced using the first CRF recognizer; the next 
two runs, msra_f and msra_g, are produced using 
the second CRF recognizer which used randomly 
selected 90% of the MSRA training data. When 
we retrained the second CRF recognizer with the 
whole set of the MSRA training data, the overall 
F-Score is 85.00, precision 90.28%, and recall 
80.31%. The last run, msra_r, is produced using 
the MaxEnt recognizer.  
    The msra_a run used the set of basic features 
with a window size of five characters. Slightly 
over eight millions features are generated from 
the MSRA training data, excluding features oc-
curred only once. The training took 321 itera-
tions to complete.  The msra_b run is produced 
from the msra_a run by substituting the labels 
assigned by the recognizer to a named entity with 
the labels of the named entity in the training data 
if it occurs in the training data.  For example, in 
the MSRA training data, the text????? in 
the sentence ???????????  is 
tagged as a place name. The same entity also ap-
peared in MSRA testing data set. The first CRF 
recognizer failed to mark the text ????? as 
174
a place name instead it tagged ??? as a per-
son name. In post-processing, the text????
? in the testing data is re-tagged as a place name. 
As another example, the person name ??? 
appears both in the training data and in the test-
ing data. The first CRF recognizer failed to rec-
ognize it as a person name. In post-processing 
the text ??? is tagged as a person name be-
cause it appears in the training data as a person 
name. The text ??????????????
???? was correctly tagged as an organization 
name. It is not in the training data, but the texts 
??????, ?????????, and ????
??? are present in the training data and are all 
labeled as organization names. In our post-
processing, the correctly tagged organization 
name is re-tagged incorrectly as three organiza-
tion names. This is the main reason why the per-
formance of the organization name got much 
worse than that without post-processing. 
 
 Precision Recall F-score 
LOC 94.19% 87.14% 90.53 
ORG 83.59% 80.39% 81.96 
PER 92.35% 74.66% 82.57 
Table 2: The performance of the msra_a run bro-
ken down by entity type. 
 
 Precision Recall F-score 
LOC 93.09% 87.35% 90.13 
ORG 75.51% 78.51 76.98 
PER 91.52 79.27 84.95 
Table 3: The performance of the msra_b run bro-
ken down by entity type. 
 
Table 2 presents the performance of the msra_a 
run by entity type. Table 3 shows the perform-
ance of the msra_b run by entity type. While the 
post-processing improved the performance of 
person name recognition, but it degraded the per-
formance of organization name recognition. 
Overall the performance was worse than that 
without post-processing. In our development 
testing, we saw large improvement in organiza-
tion name recognition with post-processing.      
 
Run ID Precision Recall F-Score 
cityu_a 92.66% 84.75% 88.53 
cityu_b 92.42% 84.91% 88.50 
cityu_f 91.88% 82.31% 86.83 
cityu_g 91.64% 82.46% 86.81 
Table 4: Official results in the closed test of the 
NER task on CITYU corpus. 
 
Table 4 presents the official results of four runs 
in the closed test of the NER task on CITYU cor-
pus.  The first two runs, msra_a and msra_b, are 
produced using the first CRF recognizer; the next 
two runs, msra_f and msra_g, are produced using 
the second CRF recognizer. The system configu-
rations are the same as used on the MSRA cor-
pus. The cityu_b run is produced from cityu_a 
run with post-processing, and the cityu_g run 
produced from cityu_f run with post-processing. 
We used the whole set of CITYU to train the first 
CRF model, and 80% of the CITYU training data 
to train the second CRF model. No results on full 
training data are available at the time of submis-
sion. 
All the runs we submitted are based characters. 
We tried word-based approach but found it was 
not as effective as character-based approach.  
4 Discussions 
Table 4 is shows the confusion matrix of the la-
bels. The rows are the true labels and the col-
umns are the predicated labels. An entry at row x 
and column y in the table is the number of char-
acters that are predicated as y while the true label 
is x. Ideally, all entries except the diagonal 
should be zero.   
 
The table was obtained from the result of our 
development dataset for MSRA data, which are 
the last 9,364 sentences of the MSRA training 
data (we used the first 37,000 sentences for train-
ing in the model developing phase). As we can 
see, most of the errors lie in the first column, in-
dicating many of the entities labels are predi-
cated as O. This resulted low recall for entities. 
Another major error is on detecting the begin-
ning of ORG (B-O). Many of them are misla-
beled as O and beginning of location (B-L), re-
sulting low recall and low precision for ORG.  
 
 O B-L I-L B-O I-O B-P I-P 
O 406798 86 196 213 973 46 111 
B-L 463 5185 54 73 29 19 7 
I-L 852 25 6836 0 197 1 44 
B-O 464 141 3 2693 62 17 0 
I-O 1861 28 276 55 12626 2 39 
B-P 472 16 2 22 3 2998 8 
I-P 618 0 14 1 49 10 5502 
Table 4: Confusion matrix of on the MSRA de-
velopment dataset  
 
A second interesting thing to notice is the 
numbers presented in Table 2. They may suggest 
that person name recognition is more difficult 
175
than location name recognition, which is con-
trary to what we believe, since Chinese person 
names are short and have strict structure and they 
should be easier to recognize than both location 
and organization names. We examined the 
MSRA testing data and found out that 617 out 
1,973 person names occur in a single sentence as 
a list of person names. In this case, simple rule 
may be more effective. When we excluded the 
sentence with 617 person names, for person 
name recognition of our msra_a run, the F-score 
is 90.74, precision 93.44%, and recall 88.20%. 
Out of the 500 person names that were not rec-
ognized in our msra_a run, 340 occurred on the 
same line of 617 person names. 
5 Conclusions 
We applied Conditional Random Fields and 
maximum entropy models to Chinese NER tasks 
and achieved satisfying performance. Three sys-
tems with different implementations and differ-
ent features are reported. Overall, CRFs are su-
perior to maximum entropy models in Chinese 
NER tasks. Useful features include using BIOE 
tags instead of BIO tags and word and character 
clustering features.  
References 
Adam Berger, Stephen Della Pietra, and Vincent 
Della Pietra, A Maximum Entropy Approach to 
Natural Language Processing, Computational Lin-
guistics, 22 (1) 
John Lafferty, Andrew McCallum, and Fernando 
Pereira, Conditional random fields: Probabilistic 
models for segmenting and labeling sequence data. 
In: Proc. 18th International Conf. on Machine 
Learning, Morgan Kaufmann, San Francisco, CA 
(2001) 282?289 
Fuchun Peng, Fangfang Feng, and Andrew 
McCallum, Chinese Segmentation and New Word 
Detection using Conditional Random Fields, In 
Proceedings of The 20th International Conference 
on Computational Linguistics (COLING 2004) , 
pages 562-568, August 23-27, 2004, Geneva, Swit-
zerland 
Naftali Tishby and Lillian Lee, Distributional Cluster-
ing of English Words, In Proceedings of the 31st 
Annual Conference of Association for Computa-
tional Linguistics, pp 183--190, 1993. 
176
