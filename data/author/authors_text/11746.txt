Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 96?100,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Disambiguation of Preposition Sense Using Linguistically Motivated
Features
Stephen Tratz and Dirk Hovy
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Marina del Rey, CA 90292
{stratz,dirkh}@isi.edu
Abstract
In this paper, we present a supervised classifi-
cation approach for disambiguation of prepo-
sition senses. We use the SemEval 2007
Preposition Sense Disambiguation datasets to
evaluate our system and compare its results to
those of the systems participating in the work-
shop. We derived linguistically motivated fea-
tures from both sides of the preposition. In-
stead of restricting these to a fixed window
size, we utilized the phrase structure. Testing
with five different classifiers, we can report an
increased accuracy that outperforms the best
system in the SemEval task.
1 Introduction
Classifying instances of polysemous words into
their proper sense classes (aka sense disambigua-
tion) is potentially useful to any NLP application
that needs to extract information from text or build
a semantic representation of the textual information.
However, to date, disambiguation between preposi-
tion senses has not been an object of great study. In-
stead, most word sense disambiguation work has fo-
cused upon classifying noun and verb instances into
their appropriate WordNet (Fellbaum, 1998) senses.
Prepositions have mostly been studied in the con-
text of verb complements (Litkowski and Hargraves,
2007). Like instances of other word classes, many
prepositions are ambiguous, carrying different se-
mantic meanings (including notions of instrumental,
accompaniment, location, etc.) as in ?He ran with
determination?, ?He ran with a broken leg?, or ?He
ran with Jane?. As NLP systems take more and more
semantic content into account, disambiguating be-
tween preposition senses becomes increasingly im-
portant for text processing tasks.
In order to disambiguate different senses, most
systems to date use a fixed window size to derive
classification features. These may or may not be
syntactically related to the preposition in question,
resulting?in the worst case?in an arbitrary bag of
words. In our approach, we make use of the phrase
structure to extract words that have a certain syn-
tactic relation with the preposition. From the words
collected that way, we derive higher level features.
In 2007, the SemEval workshop presented par-
ticipants with a formal preposition sense dis-
ambiguation task to encourage the development
of systems for the disambiguation of preposition
senses (Litkowski and Hargraves, 2007). The train-
ing and test data sets used for SemEval have been re-
leased to the general public, and we used these data
to train and test our system. The SemEval work-
shop data consists of instances of 34 prepositions
in natural text that have been tagged with the ap-
propriate sense from the list of the common Eng-
lish preposition senses compiled by The Preposition
Project, cf. Litkowski (2005). The SemEval data
provides a natural method for comparing the per-
formance of preposition sense disambiguation sys-
tems. In our paper, we follow the task requirements
and can thus directly compare our results to the ones
from the study. For evaluation, we compared our re-
sults to those of the three systems that participated
in the task (MELB: Ye and Baldwin (2007); KU:
Yuret (2007); IRST: Popescu et al (2007)). We also
used the ?first sense? and the ?most frequent sense?
96
baselines (see section 3 and table 1). These baselines
are determined by the TPP listing and the frequency
in the training data, respectively. Our system beat
the baselines and outperformed the three participat-
ing systems.
2 Methodology
2.1 Data Preparation
We downloaded the test and training data provided
by the SemEval-2007 website for the preposition
sense disambiguation task. These are 34 separate
XML files?one for each preposition?, comprising
16557 training and 8096 test example sentences,
each sentence containing one example of the respec-
tive preposition.
What are your beliefs
<head>about</head> these emotions ?
The preposition is annotated by a head tag, and the
meaning of the preposition in question is given as
defined by TPP.
Each preposition had between 2 and 25 different
senses (on average 9.76). For the case of ?about?
these would be
1. on the subject of; concerning
2. so as to affect
3. used to indicate movement within a particular
area
4. around
5. used to express location in a particular place
6. used to describe a quality apparent in a person
We parsed the sentences using the Charniak
parser (Charniak, 2000). Note that the Charniak
parser?even though among the best availbale Eng-
lish parsers?occasionally fails to parse a sentence
correctly. This might result in an erroneous extrac-
tion, such as an incorrect or no word. However,
these cases are fairly rare, and we did not manually
correct this, but rather relied on the size of the data
to compensate for such an error.
After this preprocessing step, we were able to ex-
tract the features.
2.2 Feature Extraction
Following O?Hara and Wiebe (2003) and
Alam (2004), we assumed that there is a meaningful
connection between syntactically related words on
both sides of the preposition. We thus focused on
specific words that are syntactically related to the
preposition via the phrase structure. This has the
advantage that it is not limited to a certain window
size; phrases might stretch over dozens of words,
so the extracted word may occur far away from the
actual preposition. These words were chosen based
on a manual analysis of training data. Using Tregex
(Levy and Andrew, 2006), a utility for expressing
?regular expressions over trees?, we created a set
of rules to extract the words in question. Each rule
matched words that exhibited a specific relationship
with the preposition or were within a two word
window to cover collocations. An example rule is
given below.
IN > (PP < (V P < # = x& <
#!AUX))
This particular rule finds the head (denoted by x) of
a verb phrase that governs the prepositional phrase
containing the preposition, unless x is an auxiliary
verb. Tregex rules were used to identify the follow-
ing words for feature generation:
? the head verb/noun that immediately dominates
the preposition along with all of its modifying
determiners, quantifiers, numbers, and adjec-
tives
? the head verb/noun immediately dominated by
the preposition along with all of its modifying
determiners, quantifiers, numbers, and adjec-
tives
? the subject, negator, and object(s) of the imme-
diately dominating verb
? neighboring prepositional phrases dominated
by the same verb/noun (?sister? prepositional
phrases)
? words within 2 positions to the left or right of
the preposition
For each word extracted using these rules, we col-
lected the following items:
97
? the word itself
? lemma
? part-of-speech (both exact and conflated, e.g.
both ?VBD? and ?verb? for ?VBD?)
? all synonyms of the first WordNet sense
? all hypernyms of the first WordNet sense
? boolean indicator for capitalization
Each feature is a combination of the extraction
rule and the extracted item. The values the feature
can take on are binary: present or absent. For some
prepositions, this resulted in several thousand fea-
tures. In order to reduce computation time, we used
the following steps: For each preposition classifier,
we ranked the features using information gain (For-
man, 2003). From the resulting lists,we included at
most 4000 features. Thus not all classifiers used the
same features.
2.3 Classifier Training
We chose maximum entropy (Berger et al, 1996) as
our primary classifier, since it had been successfully
applied by the highest performing systems in both
the SemEval-2007 preposition sense disambiguation
task (Ye and Baldwin, 2007) and the general word
sense disambiguation task (Tratz et al, 2007). We
used the implementation provided by the Mallet ma-
chine learning toolkit (McCallum, 2002). For the
sake of comparison, we also built several other clas-
sifiers, including multinomial na??ve Bayes, SVMs,
kNN, and decision trees (J48) using the WEKA
toolkit (Witten, 1999). We chose the radial basis
function (RBF) kernel for the SVMs and left all
other parameters at their default values.
3 Results
We measured the accuracy of the classifiers over
the test set provided by SemEval-2007 and provided
these results in Table 1. It is notable that our system
produced good results with all classifiers: For three
of the classifiers, the accuracy is higher than MELB,
the winning system of the task. As expected, the
highest accuracy was achieved using the maximum
entropy classifier. Overall, our system outperformed
the winning system by 0.058, an 8 percent improve-
ment. A simple proportion test shows this to be sta-
tistically significant at 0.001. ??????
??????
??????
?? ??????
??????
??????
??????
??????
??????
??????
??????
??????
???????
??????????????
???????????????
?????????????????????
??????????????
???????????????????????????
????????????????
??????????????????????????
????????????????
Table 1: Accuracy results on SemEval data (with 4000
features)
Since our initial cutoff of 4000 features was ar-
bitrary, we reran our Maximum Entropy experiment
multiple times with different cutoffs. Accuracy con-
sistently increased as the feature limit was relaxed,
resulting in 0.764 accuracy at the 10k feature limit.
These results are displayed in Figure 1.
its modifying determiners, quantifiers, 
numbers, and adjectives
? the head verb/noun immediately domi-
nated by the preposition along with all of 
its modifying determiners, quantifiers, 
numbers, and adjectives
? the subject, negator, and object(s) of the 
immediately dominating verb
? neighboring prepositional phrases domi-
nated by the same verb/noun (?sister? 
prepositional phrases)
? words within 2 positions to the left  or right 
of the preposition
For words extracted using these rules, we col-
lected the following features: 
? the word itself
? lemma
? part-of-speech (both exact  and conflated 
(e.g. both 'VBD' and 'verb' for 'VBD'))
? synonyms of the first WordNet sense
? hypernyms of the first WordNet s nse
? boolean indicator for capitalization
This resulted in several thousand features for the 
prepositions. We used information gain (Foreman, 
2003) in order to find the highest ranking features 
of each class and limited our classifiers to the top 
4000 features in order to reduce computation time.
2.3 Classifier Training
We chose maximum entropy (Berger, 1996) as our 
primary classifier because the highest performing 
systems in both the SemEval-2007 preposition 
sense disambiguation task (Ye and Baldwin, 2007) 
and the general word sense dis mbiguation t sk 
(Tratz et al, 2007) used it. We used the implemen-
tation provided by the Mallet machine learning 
toolkit (McCallum, 2002). Then, for the sake of 
comparison, we also built several other classifiers 
including multinomial na?ve Bayes, SVMs, kNN, 
and decision trees (J48) using the WEKA toolkit 
(Witten, 1999). We chose the radial basis function 
(RBF) kernel for the SVMs and left all other pa-
rame ers at their default values.
3 Results
We measured the accuracy of the classifiers over 
the test  et provided by SemEval-2007 and pro-
vided these results in Table 1. It  is notable that  our 
system produced good results with all classifiers: 
For three of the classifiers, the accuracy is higher 
than MELB, the winning system of the task. As 
expected, the highest  accuracy was achieved using 
the maximum entropy classifier.
Overall, our system outperformed the winning 
system by 0.058, an 8 percent improvement. A 
simple proportion test  shows this to be statistically 
significant at 0.001.
System Accuracy
kNN 684
SVM (RBF Kernel) 692
J48 decision trees 712
Multinomial Na?ve Bayes 731
Maximum Entropy 751
Most Frequent Sense 396
IRST (Popescu et al, 2007) 496
KU (Yuret, 2007) 547
MELB (Ye and Baldwin, 2007) 693
Table 1. Accuracy results on SemEval-2007 data.
Since our initial cutoff of 4000 features was arbi-
trary, we reran our Maximum Entropy experiment 
multiple times with different cutoffs. Accuracy 
consistently increased as the feature limit was re-
laxed, resulting in 0.764 accuracy at  the 10k fea-
ture limit. These results re displayed in Figure 1.
Figure 1. Relationship between maximum feature limit 
and accuracy for the Maximum Entropy classifiers.
4 Related Work
The linguistic literature on prepositions and their 
use is copious and diverse. We restrict ourselves to 
the works that deal with preposition sense disam-
biguation in computational linguistics.
O'Hara and Wiebe (2003) make use of Penn 
Treebank (Marcus et  al., 1993) and FrameNet 
(Baker et  al., 1998) to classify prepositions. They 
show that  using high level features from the con-
text, such as semantic roles, significantly aids dis-
Figure 1: Maximum feature limit vs. accuracy for maxi-
mum entropy classifier
4 Related Work
The linguistic literature on prepositions and their use
is copious and div rse. We restrict our lves to the
systems that compet d in the SemEval 2007 Prepo-
sition Sense Disambiguation task. All three of the
systems within the framework of the SemEval task
used supervised learning algorithms, yet they dif-
fered widely in the data collection and model prepa-
ration.
98
Ye and Baldwin (2007) participated in the Sem-
Eval task using a maximum entropy classifier and
achieved the highest accuracy of the participating
systems. The features they extracted were similar
to the ones we used, including POS and WordNet
features, but they used a substantially larger word
window, taking seven words from each side of the
preposition. While they included many higher level
features, they state that the direct lexical context
(i.e., bag-of-words) features were the most effective
and account for the majority of features, while syn-
tactic and semantic features had relatively little im-
pact.
Yuret (2007) used a n-gram model based on word
substitution by synonyms or antonyms. While this
proved to be quite successful with content words, it
had considerable problems with prepositions, since
the number of synonyms and/or antonyms is fairly
limited.
Popescu et al (2007) take an interesting approach
which they call Chain Clarifying Relationship. They
are using a supervised algorithm to learn a regu-
lar language. They used the Charniak parser and
FrameNet information on the head, yet the features
they extract are generally not linguistically moti-
vated.
5 Discussion
Using the phrase structure allows for more freedom
in the choice of words for feature selection, yet still
guarantees to find words for which some syntactic
relation with the preposition holds. Extracting se-
mantic features from these words (hypernyms, syn-
onyms, etc.) allows for a certain degree of abstrac-
tion, and thus a high level comparison. O?Hara and
Wiebe (2003) also make use of high level features,
in their case the Penn Treebank (Marcus et al, 1993)
and FrameNet (Baker et al, 1998) to classify prepo-
sitions. They show that using high level features?
such as semantic roles?of words in the context sub-
stantially aids disambiguation efforts. They cau-
tion, however, that indiscriminately using colloca-
tions and neighboring words may yield high accu-
racy, but has the risk of overfitting. In order to mit-
igate this, they classify the features by their part of
speech. While we made use of collocation features,
we also took into account higher order aspects of the
context, such as the governing phrase, part of speech
type, and semantic class according to WordNet. All
other things being equal, this seems to increase per-
formance substantially.
As for the classifiers used, our results seem to
confirm that Maximum Entropy classifiers are very
well suited for disambiguation tasks. Other than
na??ve Bayes, they do not presuppose a conditional
independence between the features, which clearly
not always holds (quite contrary, the underlying syn-
tactic structure creates strong interdependencies be-
tween words and features). This, however, does not
satisfactory explain the ranking of the other classi-
fiers. One possible explanation could be the sensi-
tivity of for example decision trees to random noise.
Though we made use of information gain before
classification, there still seems to be a certain ten-
dency to split on features that are not optimal.
6 Conclusion
We showed that using a number of simple linguis-
tically motivated features can improve the accu-
racy of preposition sense disambiguation. Utilizing
widely used and freely available standard tools for
language processing and a set of simple rules, we
were able to extract these features easily and with
very limited preprocessing. Instead of taking a ?bag
of words? approach that focuses primarily upon the
words within a fixed window size, we focused on el-
ements that are related via the phrase structure. We
also included semantic information gathered from
WordNet about the extracted words. We compared
five different classifiers and demonstrated that they
all perform very well, using our selected feature set.
Several of them even outperformed the top system
at SemEval. Our best result was obtained using a
maximum entropy classifier, just as the best partici-
pating system, leading us to believe that our primary
advantage was our feature set. While the contribu-
tion of the direct context (+/-7 words) might have
a stronger effect than higher level features (Ye and
Baldwin, 2007), we conclude from our findings that
higher level features do make an important contribu-
tion. These results are very encouraging on several
levels, and demonstrate the close interaction of syn-
tax and semantics. Leveraging these types of fea-
tures effectively is a promising prospect for future
99
machine learning research in preposition sense dis-
ambiguation.
Acknowledgements
The authors would like to thank Eduard Hovy and
Gully Burns for invaluable comments and helpful
discussions.
References
Y.S. Alam. 2004. Decision Trees for Sense Disambigua-
tion of Prepositions: Case of Over. In HLT-NAACL
2004: Workshop on Computational Lexical Semantics,
pages 52?59.
C.F. Baker, C.J. Fillmore, and J.B. Lowe. 1998.
The Berkeley FrameNet Project. In Proceedings of
the 17th international conference on Computational
linguistics-Volume 1, pages 86?90. Association for
Computational Linguistics Morristown, NJ, USA.
A.L. Berger, V.J. Della Pietra, and S.A. Della Pietra.
1996. A maximum entropy approach to natural
language processing. Computational Linguistics,
22(1):39?71.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In ACM International Conference Proceeding Series,
volume 4, pages 132?139.
C. Fellbaum. 1998. WordNet: an electronic lexical
database. MIT Press USA.
G. Forman. 2003. An extensive empirical study of fea-
ture selection metrics for text classification. The Jour-
nal of Machine Learning Research, 3:1289?1305.
R. Levy and G. Andrew. 2006. Tregex and Tsurgeon:
tools for querying and manipulating tree data struc-
tures. In LREC 2006.
Ken Litkowski and Orin Hargraves. 2007. SemEval-
2007 Task 06: Word-Sense Disambiguation of Prepo-
sitions. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic.
Ken Litkowski. 2005. The preposition project.
http://www.clres.com/prepositions.html.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of Eng-
lish: the Penn TreeBank. Computational Linguistics,
19(2):313?330.
A.K. McCallum. 2002. MALLET: A Machine Learning
for Language Toolkit. 2002. http://mallet. cs. umass.
edu.
T. O?Hara and J. Wiebe. 2003. Preposition semantic
classification via Penn Treebank and FrameNet. In
Proceedings of CoNLL, pages 79?86.
Octavian Popescu, Sara Tonelli, and Emanuele Pianta.
2007. IRST-BP: Preposition Disambiguation based on
Chain Clarifying Relationships Contexts. In MELB-
YB: Preposition Sense Disambiguation Using Rich Se-
mantic Features, Prague, Czech Republic.
S. Tratz, A. Sanfilippo, M. Gregory, A. Chappell,
C. Posse, and P. Whitney. 2007. PNNL: A Supervised
Maximum Entropy Approach to Word Sense Disam-
biguation. In Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval-2007).
I.H. Witten. 1999. Weka: Practical Machine Learn-
ing Tools and Techniques with Java Implementations.
Dept. of Computer Science, University of Waikato,
University of Waikato, Dept. of Computer Science.
Patrick Ye and Timothy Baldwin. 2007. MELB-YB:
Preposition Sense Disambiguation Using Rich Seman-
tic Features. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic.
Deniz Yuret. 2007. Ku: Word sense disambiguation by
substitution. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic.
100
Coling 2010: Poster Volume, pages 454?462,
Beijing, August 2010
What?s in a Preposition?
Dimensions of Sense Disambiguation for an Interesting Word Class
Dirk Hovy, Stephen Tratz, and Eduard Hovy
Information Sciences Institute
University of Southern California
{dirkh, stratz, hovy}@isi.edu
Abstract
Choosing the right parameters for a word
sense disambiguation task is critical to
the success of the experiments. We ex-
plore this idea for prepositions, an of-
ten overlooked word class. We examine
the parameters that must be considered in
preposition disambiguation, namely con-
text, features, and granularity. Doing
so delivers an increased performance that
significantly improves over two state-of-
the-art systems, and shows potential for
improving other word sense disambigua-
tion tasks. We report accuracies of 91.8%
and 84.8% for coarse and fine-grained
preposition sense disambiguation, respec-
tively.
1 Introduction
Ambiguity is one of the central topics in NLP. A
substantial amount of work has been devoted to
disambiguating prepositional attachment, words,
and names. Prepositions, as with most other word
types, are ambiguous. For example, the word in
can assume both temporal (?in May?) and spatial
(?in the US?) meanings, as well as others, less
easily classifiable (?in that vein?). Prepositions
typically have more senses than nouns or verbs
(Litkowski and Hargraves, 2005), making them
difficult to disambiguate.
Preposition sense disambiguation (PSD) has
many potential uses. For example, due to the
relational nature of prepositions, disambiguating
their senses can help with all-word sense disam-
biguation. In machine translation, different senses
of the same English preposition often correspond
to different translations in the foreign language.
Thus, disambiguating prepositions correctly may
help improve translation quality.1 Coarse-grained
PSD can also be valuable for information extrac-
tion, where the sense acts as a label. In a recent
study, Hwang et al (2010) identified preposition
related features, among them the coarse-grained
PP labels used here, as the most informative fea-
ture in identifying caused-motion constructions.
Understanding the constraints that hold for prepo-
sitional constructions could help improve PP at-
tachment in parsing, one of the most frequent
sources of parse errors.
Several papers have successfully addressed
PSD with a variety of different approaches (Rudz-
icz and Mokhov, 2003; O?Hara and Wiebe, 2003;
Ye and Baldwin, 2007; O?Hara and Wiebe, 2009;
Tratz and Hovy, 2009). However, while it is often
possible to increase accuracy by using a differ-
ent classifier and/or more features, adding more
features creates two problems: a) it can lead to
overfitting, and b) while possibly improving ac-
curacy, it is not always clear where this improve-
ment comes from and which features are actually
informative. While parameter studies exist for
general word sense disambiguation (WSD) tasks
(Yarowsky and Florian, 2002), and PSD accuracy
has been steadily increasing, there has been no
exploration of the parameters of prepositions to
guide engineering decisions.
We go beyond simply improving accuracy to
analyze various parameters in order to determine
which ones are actually informative. We explore
the different options for context and feature se-
1See (Chan et al, 2007) for the relevance of word sense
disambiguation and (Chiang et al, 2009) for the role of
prepositions in MT.
454
lection, the influence of different preprocessing
methods, and different levels of sense granular-
ity. Using the resulting parameters in a Maximum
Entropy classifier, we are able to improve signif-
icantly over existing results. The general outline
we present can potentially be extended to other
word classes and improve WSD in general.
2 Related Work
Rudzicz and Mokhov (2003) use syntactic and
lexical features from the governor and the preposi-
tion itself in coarse-grained PP classification with
decision heuristics. They reach an average F-
measure of 89% for four classes. This shows that
using a very small context can be effective. How-
ever, they did not include the object of the prepo-
sition and used only lexical features for classifi-
cation. Their results vary widely for the different
classes.
O?Hara and Wiebe (2003) made use of a win-
dow size of five words and features from the
Penn Treebank (PTB) (Marcus et al, 1993) and
FrameNet (Baker et al, 1998) to classify prepo-
sitions. They show that using high level fea-
tures, such as semantic roles, significantly aid dis-
ambiguation. They caution that using colloca-
tions and neighboring words indiscriminately may
yield high accuracy, but has the risk of overfit-
ting. O?Hara and Wiebe (2009) show compar-
isons of various semantic repositories as labels for
PSD approaches. They also provide some results
for PTB-based coarse-grained senses, using a five-
word window for lexical and hypernym features in
a decision tree classifier.
SemEval 2007 (Litkowski and Hargraves,
2007) included a task for fine-grained PSD (more
than 290 senses). The best participating system,
that of Ye and Baldwin (2007), extracted part-of-
speech and WordNet (Fellbaum, 1998) features
using a word window of seven words in a Max-
imum Entropy classifier. Tratz and Hovy (2009)
present a higher-performing system using a set of
20 positions that are syntactically related to the
preposition instead of a fixed window size.
Though using a variety of different extraction
methods, contexts, and feature words, none of
these approaches explores the optimal configura-
tions for PSD.
3 Theoretical Background
The following parameters are applicable to other
word classes as well. We will demonstrate their
effectiveness for prepositions.
Analyzing the syntactic elements of preposi-
tional phrases, one discovers three recurring ele-
ments that exhibit syntactic dependencies and de-
fine a prepositional phrase. The first one is the
governing word (usually a noun, verb, or adjec-
tive)2, the preposition itself, and the object of the
preposition.
Prepositional phrases can be fronted (?In May,
prices dropped by 5%?), so that the governor (in
this case the verb ?drop?) occurs later in the sen-
tence. Similarly, the object can be fronted (con-
sider ?a dessert to die for?).
In the simplest version, we can do classification
based only on the preposition and the governor or
object alone.3 Furthermore, directly neighboring
words can influence the preposition, mostly two-
word prepositions such as ?out of? or ?because
of?.
To extract the words discussed above, one can
either employ a fixed window size, (which has
to be large enough to capture the words), or se-
lect them based on heuristics or parsing informa-
tion. The governor and object can be hard to ex-
tract if they are fronted, since they do not occur in
their unusual positions relative to the preposition.
While syntactically related words improve over
fixed-window-size approaches (Tratz and Hovy,
2009), it is not clear which words contribute most.
There should be an optimal context, i.e., the small-
est set of words that achieves the best accuracy. It
has to be large enough to capture all relevant infor-
mation, but small enough to avoid noise words.4
We surmise that earlier approaches were not uti-
lizing that optimal context, but rather include a lot
of noise.
Depending on the task, different levels of sense
granularity may be used. Fewer senses increase
the likelihood of correct classification, but may in-
2We will refer to the governing word, irrespective of
class, as governor.
3Basing classification on the preposition alone is not fea-
sible, because of the very polysemy we try to resolve.
4It is not obvious how much information a sister-PP can
provide, or the subject of the superordinate clause.
455
correctly conflate prepositions. A finer granular-
ity can help distinguish nuances and better fit the
different contexts. However, it might suffer from
sparse data.
4 Experimental Setup
We explore the different context types (fixed win-
dow size vs. selective), the influence of the words
in that context, and the preprocessing method
(heuristics vs. parsing) on both coarse and fine-
grained disambiguation. We use a most-frequent-
sense baseline. In addition, we compare to the
state-of-the-art systems for both types of granu-
larity (O?Hara and Wiebe, 2009; Tratz and Hovy,
2009). Their results show what has been achieved
so far in terms of accuracy, and serve as a second
measure for comparison beyond the baseline.
4.1 Model
We use the MALLET implementation (McCal-
lum, 2002) of a Maximum Entropy classifier
(Berger et al, 1996) to construct our models. This
classifier was also used by two state-of-the-art
systems (Ye and Baldwin, 2007; Tratz and Hovy,
2009). For fine-grained PSD, we train a separate
model for each preposition due to the high num-
ber of possible classes for each individual prepo-
sition. For coarse-grained PSD, we use a single
model for all prepositions, because they all share
the same classes.
4.2 Data
We use two different data sets from existing re-
sources for coarse and fine-grained PSD to make
our results as comparable to previous work as pos-
sible.
For the coarse-grained disambiguation, we use
data from the POS tagged version of the Wall
Street Journal (WSJ) section of the Penn Tree-
Bank. A subset of the prepositional phrases in
this corpus is labelled with a set of seven classes:
beneficial (BNF), direction (DIR), extent (EXT),
location (LOC), manner (MNR), purpose (PRP),
and temporal (TMP). We extract only those prepo-
sitions that head a PP labelled with such a class
(N = 35, 917). The distribution of classes is
highly skewed (cf. Figure 1). We compare the
PTB class distrib
Page 1
LOC TMP DIR MNR PRP EXT BNF
0
2000
4000
6000
8000
10000
12000
14000
16000
18000 16995
10332
5414
1781 1071 280 44
classes
fre
qu
en
cy
Figure 1: Distribution of Class Labels in the WSJ
Section of the Penn TreeBank.
results of this task to the findings of O?Hara and
Wiebe (2009).
For the fine-grained task, we use data from
the SemEval 2007 workshop (Litkowski and Har-
graves, 2007), separate XML files for the 34 most
frequent English prepositions, comprising 16, 557
training and 8096 test sentences, each instance
containing one example of the respective prepo-
sition. Each preposition has between two and 25
senses (9.76 on average) as defined by The Prepo-
sition Project (Litkowski and Hargraves, 2005).
We compare our results directly to the findings
from Tratz and Hovy (2009). As in the original
workshop task, we train and test on separate sets.
5 Results
In this section we show experimental results for
the influence of word extraction method (parsing
vs. POS-based heuristics), context, and feature se-
lection on accuracy. Each section compares the
results for both coarse and fine-grained granular-
ity. Accuracy for the coarse-grained task is in all
experiments higher than for the fine-grained one.
5.1 Word Extraction
In order to analyze the impact of the extraction
method, we compare parsing versus POS-based
heuristics for word extraction.
Both O?Hara and Wiebe (2009) and Tratz and
Hovy (2009) use constituency parsers to prepro-
cess the data. However, parsing accuracy varies,
456
and the problem of PP attachment ambiguity in-
creases the likelihood of wrong extractions. This
is especially troublesome in the present case,
where we focus on prepositions.5 We use the
MALT parser (Nivre et al, 2007), a state-of-the-
art dependency parser, to extract the governor and
object.
The alternative is a POS-based heuristics ap-
proach. The only preprocessing step needed is
POS tagging of the data, for which we used the
system of Shen et al (2007). We then use simple
heuristics to locate the prepositions and their re-
lated words. In order to determine the governor
in the absence of constituent phrases, we consider
the possible governing noun, verb, and adjective.
The object of the preposition is extracted as first
noun phrase head to the right. This approach is
faster than parsing, but has problems with long-
range dependencies and fronting of the PP (e.g.,
the PP appearing earlier in the sentence than its
governor). word selection
Page 1
MALT 84.4 94.0
84.8 90.9
84.8 91.8
extraction method fine coarse
Heuristics
MALT + Heuristics
Table 1: Accuracies (%) for Word-Extraction Us-
ing MALT Parser or Heuristics.
Interestingly, the extraction method does not
significantly affect the final score for fine-grained
PSD (see Table 1). The high score achieved when
using the MALT parse for coarse-grained PSD
can be explained by the fact that the parser was
originally trained on that data set. The good re-
sults we see when using heuristics-based extrac-
tion only, however, means we can achieve high-
accuracy PSD even without parsing.
5.2 Context
We compare the effects of fixed window size ver-
sus syntactically related words as context. Table 2
shows the results for the different types and sizes
of contexts.6
5Rudzicz and Mokhov (2003) actually motivate their
work as a means to achieve better PP attachment resolution.
6See also (Yarowsky and Florian, 2002) for experiments
on the effect of varying window size for WSD.
context
Page 1
91.6 80.4
92.0 81.4
91.6 79.8
91.0 78.7
80.7 78.9
94.2 56.9
94.0 84.8
Context coarse fine
2-word window
3-word window
4-word window
5-word window
Governor, prep
Prep, object
Governor, prep, object
Table 2: Accuracies (%) for Different Context
Types and Sizes
The results show that the approach using both
governor and object is the most accurate one. Of
the fixed-window-size approaches, three words to
either side works best. This does not necessarily
reflect a general property of that window size, but
can be explained by the fact that most governors
and objects occur within this window size.7 This
dista ce can vary from corpus to corpus, so win-
dow size would have to be determined individu-
ally for each task. The difference between using
governor and preposition versus preposition and
object between coarse and fine-grained classifica-
tion might reflect the annotation process: while
Litkowski and Hargraves (2007) selected exam-
ples based on a search for governors8, most anno-
tators in the PTB may have based their decision
of the PP label on the object that occurs in it. We
conclude that syntactically related words present a
better context for classification than fixed window
sizes.
5.3 Features
Having established the context we want to use, we
now turn to the details of extracting the feature
words from that context.9 Using higher-level fea-
tures instead of lexical ones helps accounting for
sparse training data (given an infinite amount of
data, we would not need to take any higher-level
7Based on such statistics, O?Hara and Wiebe (2003) ac-
tually set their window size to 5.
8Personal communication.
9As one reviewer pointed out, these two dimensions are
highly interrelated and influence each other. To examine the
effects, we keep one dimension constant while varying the
other.
457
features into account, since every case would be
covered). Compare O?Hara and Wiebe (2009).
Following the prepocessing, we use a set of
rules to select the feature words, and then gen-
erate feature values from them using a variety
of feature-generating functions.10 The word-
selection rules are listed below.
Word-Selection Rules
? Governor from the MALT parse
? Object from the MALT parse
? Heuristically determined object of the prepo-
sition
? First verb to the left of the preposition
? First verb/noun/adjective to the left of the
preposition
? Union of (First verb to the left, First
verb/noun/adjective to the left)
? First word to the left
The feature-generating functions, many of
which utilize WordNet (Fellbaum, 1998), are
listed below. To conserve space, curly braces are
used to represent multiple functions in a single
line. The name of each feature is the combination
of the word-selection rule and the output from the
feature-generating function.
WordNet-based Features
? {Hypernyms, Synonyms} for {1st, all}
sense(s) of the word
? All terms in the definitions (?glosses?) of the
word
? Lexicographer file names for the word
? Lists of all link types (e.g., meronym links)
associated with the word
? Part-of-speech indicators for the existence of
NN/VB/JJ/RB entries for the word
? All sentence frames for the word
? All {part, member, substance}-of holonyms
for the word
? All sentence frames for the word
Other Features
? Indicator that the word-finding rule found a
word
10Some words may be selected by multiple word-selection
rules. For example, the governor of the preposition may
be identified by the Governor from MALT parse rule, first
noun/verb/adjective to left, and the first word to the left rule.
? Capitalization indicator
? {Lemma, surface form} of the word
? Part-of-speech tag for the word
? General POS tag for the word (e.g. NNS?
NN, VBZ? VB)
? The {first, last} {two, three} letters of each
word
? Indicators for suffix types (e.g., de-
adjectival, de-nominal [non]agentive,
de-verbal [non]agentive)
? Indicators for a wide variety of other affixes
including those related to degree, number, or-
der, etc. (e.g., ultra-, poly-, post-)
? Roget?s Thesaurus divisions for the word
To establish the impact of each feature word on
the outcome, we use leave-one-out and only-one
evaluation.11 The results can be found in Table 3.
A word that does not perform well as the only at-
tribute may still be important in conjunction with
others. Conversely, leaving out a word may not
hurt performance, despite being a good single at-
tribute. word selection
Page 1
Word LOO LOO
92.1 80.1 84.3 78.9
93.4 94.2 84.9 56.3
92.0 77.9 85.0 62.1
92.1 78.7 84.3 78.5
92.1 78.4 84.5 81.0
92.0 78.8 84.4 77.2
91.9 93.0 84.9 56.8
91.8 ? 84.8 ?
coarse fine
Only Only
MALT governor
MALT object
Heuristics VB to left
Heur. NN/VB/ADJ to left
Heur. Governor Union
Heuristics word to left
Heuristics object
none
Table 3: Accuracies (%) for Leave-One-
Out (LOO) and Only-One Word-Extraction-Rule
Evaluation. none includes all words and serves for
comparison. Important words reduce accuracy for
LOO, but rank high when used as only rule.
Independent of the extraction method (MALT
parser or POS-based heuristics), the governor is
the most informative word. Combining several
heuristics to locate the governor is the best sin-
gle feature for fine-grained classification. The rule
looking only for a governing verb fails to account
11Since the feature words are not independent of one an-
other, neither of the two measures is decisive on its own.
458
full both
Page 1
Total Total Total Total
? ? 6 100.0 125 90.4 53 47.2
364 94.0 5 80.0 ? ? 74 93.2
23 69.6 78 65.4 ? ? 1 0.0
151 96.7 87 79.3 ? ? 7 71.4
53 79.2 841 92.5 of 1478 87.9 71 64.8
92 92.4 16 43.8 76 84.2 28 75.0
173 96.0 45 71.1 441 81.4 2287 90.8
? ? 5 80.0 58 91.4 15 53.3
? ? 58 70.7 out ? ? 90 68.9
50 80.0 358 93.9 ? ? 62 90.3
? ? 1 0.0 98 79.6 417 89.4
155 69.0 107 86.0 ? ? 6 83.3
84 100.0 232 84.5 per ? ? 3 100.0
? ? 2 50.0 82 65.9 ? ?
367 86.4 3078 92.0 ? ? 449 94.4
? ? 5 100.0 ? ? 2 0.0
? ? 420 91.7 208 48.1 364 69.0
20 90.0 384 83.3 ? ? 62 93.5
68 77.9 65 87.7 ? ? 3 100.0
? ? 94 71.3 to 572 89.7 3166 97.5
28 78.6 11 72.7 ? ? 55 65.5
29 100.0 4 100.0 102 97.1 2 100.0
? ? 1 0.0 ? ? 604 91.4
102 94.1 98 84.7 ? ? 2 50.0
? ? 45 64.4 ? ? 208 94.2
248 88.3 1341 87.5 up ? ? 20 75.0
down 153 81.7 16 56.2 ? ? 23 73.9
39 87.2 547 92.1 via ? ? 22 40.9
? ? 1 0.0 ? ? 1 100.0
478 82.4 1455 84.5 ? ? 3 33.3
578 85.5 1712 90.5 578 84.4 272 69.5
in 688 77.0 15706 95.0 ? ? 213 96.2
38 73.7 24 91.7 ? ? 69 63.8
297 86.2 415 80.0
Overall 8096 84.8 35917 91.8
fine coarse fine coarse
Prep Acc Acc Prep Acc Acc
aboard like
about near
above nearest
across next
after
against off
along on
alongside onto
amid
among outside
amongst over
around past
as
astride round
at since
atop than
because through
before throughout
behind till
below
beneath toward
beside towards
besides under
between underneath
beyond until
by
upon
during
except whether
for while
from with
within
inside without
into
Table 4: Accuracies (%) for Coarse and Fine-Grained PSD, Using MALT and Heuristics. Sorted by
preposition.
for noun governors, which consequently leads to
a slight improvement when left out.
Curiously, the word directly to the left is a bet-
ter single feature than the object (for fine-grained
classification). Leaving either of them out in-
creases accuracy, which implies that their infor-
mation can be covered by other words.
459
coarse both 2009
Page 1
Most Frequent Sense
f1 f1 f1
LOC 71.8 97.4 82.6 90.8 93.2 92.0 94.7 96.4 95.6
TMP 77.5 39.4 52.3 84.5 85.2 84.8 94.6 94.6 94.6
DIR 91.6 94.2 92.8 95.6 96.5 96.1 94.6 94.5 94.5
MNR 69.9 43.2 53.4 82.6 55.8 66.1 83.3 75.0 78.9
PRP 78.2 48.8 60.1 79.3 70.1 74.4 90.6 83.8 87.1
EXT 0.0 0.0 0.0 81.7 84.6 82.9 87.5 82.1 84.7
BNF 0.0 0.0 0.0 ? ? ? 75.0 34.1 46.9
O'Hara/Wiebe 2009 10-fold CV
Class prec rec prec rec prec rec
Table 5: Precision, Recall and F1 Results (%) for Coarse-Grained Classification. Comparison to O?Hara
and Wiebe (2009). Classes ordered by frequency
5.4 Comparison with Related Work
To situate our experimental results within the
body of work on PSD, we compare them to both
a most-frequent-sense baseline and existing work
for both granularities (see Table 6). The results
use a syntactically selective context of preposi-
tion, governor, object, and word to the left as
determined by combined extraction information
(POS tagging and parsing).
accuracies
Page 1
75.8 39.6  
89.3* 78.3**
93.9 84.8  
coarse fine
Baseline
Related Work
Our system
Table 6: Accuracies (%) for Different Classifi-
cations. Comparison with O?Hara and Wiebe
(2009)*, and Tratz and Hovy (2009)**.
Our system easily exceeds the baseline for both
coarse and fine-grained PSD (see Table 6). Com-
parison with related work shows that we achieve
an improvement of 6.5% over Tratz and Hovy
(2009), which is significant at p < .0001, and
of 4.5% over O?Hara and Wiebe (2009), which is
significant at p < .0001.
A detailed overview over all prepositions for
frequencies and accuracies of both coarse and
fine-grained PSD can be found in Table 4.
In addition to overall accuracy, O?Hara and
Wiebe (2009) also measure precision, recall and
F-measure for the different classes. They omitted
BNF because it is so infrequent. Due to different
training data and models, the two systems are not
strictly comparable, yet they provide a sense of
the general task difficulty. See Table 5. We note
that both systems perform better than the most-
frequent-sense baseline. DIR is reliably classified
using the baseline, while EXT and BNF are never
selected for any preposition. Our method adds
considerably to the scores for most classes. The
low score for BNF is mainly due to the low num-
ber of instances in the data, which is why it was
excluded by O?Hara and Wiebe (2009).
6 Conclusion
To get maximal accuracy in disambiguating
prepositions?and also other word classes?one
needs to consider context, features, and granular-
ity. We presented an evaluation of these parame-
ters for preposition sense disambiguation (PSD).
We find that selective context is better than
fixed window size. Within the context for prepo-
sitions, the governor (head of the NP or VP gov-
erning the preposition), the object of the prepo-
sition (i.e., head of the NP to the right), and the
word directly to the left of the preposition have
the highest influence.12 This corroborates the lin-
guistic intuition that close mutual constraints hold
between the elements of the PP. Each word syn-
tactically and semantically restricts the choice of
the other elements. Combining different extrac-
tion methods (POS-based heuristics and depen-
dency parsing) works better than either one in iso-
lation, though high accuracy can be achieved just
using heuristics. The impact of context and fea-
tures varies somewhat for different granularities.
12These will likely differ for other word classes.
460
Not surprisingly, we see higher scores for coarser
granularity than for the more fine-grained one.
We measured success in accuracy, precision, re-
call, and F-measure, and compared our results to
a most-frequent-sense baseline and existing work.
We were able to improve over state-of-the-art sys-
tems in both coarse and fine-grained PSD, achiev-
ing accuracies of 91.8% and 84.8% respectively.
Acknowledgements
The authors would like to thank Steve DeNeefe,
Victoria Fossum, and Zornitsa Kozareva for com-
ments and suggestions. StephenTratz is supported
by a National Defense Science and Engineering
fellowship.
References
Baker, C.F., C.J. Fillmore, and J.B. Lowe. 1998.
The Berkeley FrameNet Project. In Proceedings of
the 17th international conference on Computational
linguistics-Volume 1, pages 86?90. Association for
Computational Linguistics Morristown, NJ, USA.
Berger, A.L., V.J. Della Pietra, and S.A. Della Pietra.
1996. A maximum entropy approach to natural
language processing. Computational Linguistics,
22(1):39?71.
Chan, Y.S., H.T. Ng, and D. Chiang. 2007. Word sense
disambiguation improves statistical machine trans-
lation. In Annual Meeting ? Association For Com-
putational Linguistics, volume 45, pages 33?40.
Chiang, D., K. Knight, and W. Wang. 2009. 11,001
new features for statistical machine translation.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 218?226, Boulder, Colorado, June.
Association for Computational Linguistics.
Fellbaum, C. 1998. WordNet: an electronic lexical
database. MIT Press USA.
Hwang, J. D., R. D. Nielsen, and M. Palmer. 2010.
Towards a domain independent semantics: Enhanc-
ing semantic representation with construction gram-
mar. In Proceedings of the NAACL HLT Workshop
on Extracting and Using Constructions in Computa-
tional Linguistics, pages 1?8, Los Angeles, Califor-
nia, June. Association for Computational Linguis-
tics.
Litkowski, K. and O. Hargraves. 2005. The preposi-
tion project. ACL-SIGSEM Workshop on ?The Lin-
guistic Dimensions of Prepositions and Their Use in
Computational Linguistic Formalisms and Applica-
tions?, pages 171?179.
Litkowski, K. and O. Hargraves. 2007. SemEval-2007
Task 06: Word-Sense Disambiguation of Preposi-
tions. In Proceedings of the 4th International Work-
shop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic.
Marcus, M.P., M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: the Penn TreeBank. Computational Linguis-
tics, 19(2):313?330.
McCallum, A.K. 2002. MALLET: A Machine Learn-
ing for Language Toolkit. 2002. http://mallet. cs.
umass. edu.
Nivre, J., J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. Ku?bler, S. Marinov, and E. Marsi. 2007. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language En-
gineering, 13(02):95?135.
O?Hara, T. and J. Wiebe. 2003. Preposition semantic
classification via Penn Treebank and FrameNet. In
Proceedings of CoNLL, pages 79?86.
O?Hara, T. and J. Wiebe. 2009. Exploiting seman-
tic role resources for preposition disambiguation.
Computational Linguistics, 35(2):151?184.
Rudzicz, F. and S. A. Mokhov. 2003. To-
wards a heuristic categorization of prepo-
sitional phrases in english with word-
net. Technical report, Cornell University,
arxiv1.library.cornell.edu/abs/1002.1095-
?context=cs.
Shen, L., G. Satta, and A. Joshi. 2007. Guided learn-
ing for bidirectional sequence classification. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, volume 45, pages
760?767.
Tratz, S. and D. Hovy. 2009. Disambiguation of
preposition sense using linguistically motivated fea-
tures. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, Companion Volume: Student Re-
search Workshop and Doctoral Consortium, pages
96?100, Boulder, Colorado, June. Association for
Computational Linguistics.
Yarowsky, D. and R. Florian. 2002. Evaluating sense
disambiguation across diverse parameter spaces.
Natural Language Engineering, 8(4):293?310.
461
Ye, P. and T. Baldwin. 2007. MELB-YB: Preposition
Sense Disambiguation Using Rich Semantic Fea-
tures. In Proceedings of the 4th International Work-
shop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic.
462
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1783?1792, Dublin, Ireland, August 23-29 2014.
Adapting taggers to Twitter with not-so-distant supervision
Barbara Plank
1
, Dirk Hovy
1
, Ryan McDonald
2
and Anders S?gaard
1
Center for Language Technology, University of Copenhagen
1
Google Inc.
2
{bplank,dirkh}@cst.dk,ryanmcd@google.com,soegaard@hum.ku.dk
Abstract
We experiment with using different sources of distant supervision to guide unsupervised and
semi-supervised adaptation of part-of-speech (POS) and named entity taggers (NER) to Twitter.
We show that a particularly good source of not-so-distant supervision is linked websites. Specif-
ically, with this source of supervision we are able to improve over the state-of-the-art for Twitter
POS tagging (89.76% accuracy, 8% error reduction) and NER (F1=79.4%, 10% error reduction).
1 Introduction
Twitter contains a vast amount of information, including first stories and breaking news (Petrovic et al.,
2010), fingerprints of public opinions (Jiang et al., 2011) and recommendations of relevance to poten-
tially very small target groups (Benson et al., 2011). In order to automatically extract this information,
we need to be able to analyze tweets, e.g., determine the part-of-speech (POS) of words and recognize
named entities. Tweets, however, are notoriously hard to analyze (Foster et al., 2011; Eisenstein, 2013;
Baldwin et al., 2013). The challenges include dealing with variations in spelling, specific conventions
for commenting and retweeting, frequent use of abbreviations and emoticons, non-standard syntax, frag-
mented or mixed language, etc.
Gimpel et al. (2011) showed that we can induce POS tagging models with high accuracy on in-sample
Twitter data with relatively little annotation effort. Learning taggers for Twitter data from small amounts
of labeled data has also been explored by others (Ritter et al., 2011; Owoputi et al., 2013; Derczynski
et al., 2013). Hovy et al. (2014), on the other hand, showed that these models overfit their respective
samples and suffer severe drops when evaluated on out-of-sample Twitter data, sometimes performing
even worse than newswire models. This may be due to drift on Twitter (Eisenstein, 2013) or simply due
to the heterogeneous nature of Twitter, which makes small samples biased. So while existing systems
perform well on their own (in-sample) data sets, they over-fit the samples they were induced from, and
suffer on other (out-of-sample) Twitter data sets. This bias can, at least in theory, be corrected by learning
from additional unlabeled tweets. This is the hypothesis we explore in this paper.
We present a semi-supervised learning method that does not require additional labeled in-domain data
to correct sample bias, but rather leverages pools of unlabeled Twitter data. However, since taggers
trained on newswire perform poorly on Twitter data, we need additional guidance when utilizing the
unlabeled data. This paper proposes distant supervision to help our models learn from unlabeled data.
Distant supervision is a weakly supervised learning paradigm, where a knowledge resource is exploited
to gather (possible noisy) training instances (Mintz et al., 2009). Our basic idea is to can use linguistic
analysis of linked websites as a novel kind of distant supervision for learning how to analyze tweets. We
explore standard sources of distant supervision, such as Wiktionary for POS tagging, but we also propose
to use the linked websites of tweets with URLs as supervision. The intuition is that we can use websites
to provide a richer linguistic context for our tagging decisions. We exploit the fact that tweets with URLs
provide a one-to-one map between an unlabeled instance and the source of supervision, making this
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1783
1: X = {?x
i
, y
i
?}
N
i=1
labeled tweets
2: U = {?x
i
, w
i
?}
M
i=1
unlabeled tweet-website pairs
3: I iterations
4: k = 1000 pool size
5: v=train(X) base model
6: for i ? I do
7: for ?x, w? ? pool
k
(U) do
8: y?=predict(?x, w?;v)
9: X ? X ? {?y?,x?}
10: end for
11: v=train(X)
12: end for
13: return v
Figure 1: Semi-supervised learning with not-so-distant supervision, i.e. tweet-website pairs {?x
i
, w
i
?}.
SELF-TRAINING, WEB, DICT, DICT?WEB and WEB?DICT differ only in how predict() (line 8) is
implemented (cf. Section 2).
less distant supervision. Note that we use linked websites only for semi-supervised learning, but do not
require them at test time.
Our semi-supervised learning method enables us to learn POS tagging and NER models that perform
more robustly across different samples of tweets than existing approaches. We consider both the scenario
where a small sample of labeled Twitter data is available, and the scenario where only newswire data is
available. Training on a mixture of out-of-domain (WSJ) and in-domain (Twitter) data as well as unla-
beled data, we get the best reported results in the literature for both POS tagging and NER on Twitter. Our
tagging models are publicly available at https://bitbucket.org/lowlands/ttagger-nsd
2 Tagging with not-so-distant supervision
We assume that our labeled data is highly biased by domain differences (Jiang and Zhai, 2007), popula-
tion drift (Hand, 2006), or by our sample size simply being too small. To correct this bias, we want to use
unlabeled Twitter data. It is well-known that semi-supervised learning algorithms such as self-training
sometimes effectively correct model biases (McClosky et al., 2006; Huang et al., 2009). This paper
presents an augmented self-training algorithm that corrects model bias by exploiting unlabeled data and
not-so-distant supervision. More specifically, the idea is to use hyperlinks to condition tagging deci-
sions in tweets on a richer linguistic context than what is available in the tweets. This semi-supervised
approach gives state-of-the-art performance across available Twitter POS and NER data sets.
The overall semi-supervised learning algorithm is presented in Figure 1. The aim is to correct model
bias by predicting tag sequences on small pools of unlabeled tweets, and re-training the model across
several iterations to gradually correct model bias. Since information from hyperlinks will be important,
the unlabeled data U is a corpus of tweets containing URLs. We present a baseline and four system
proposals that only differ in their treatment of the predict() function.
In the SELF-TRAINING baseline, predict() corresponds to standard Viterbi inference on the unlabeled
Twitter data. This means, the current model v is applied to the tweets by disregarding the websites in
the tweet-website pairs, i.e., tagging x without considering w. Then the automatically tagged tweets are
added to the current pool of labeled data and the procedure is iterated (line 7-11 in Figure 1).
In the WEB method, we additionally use the information from the websites. The current model v
is used to predict tags for the pooled tweets and the website they linked to. For all the words that
occur both in a tweet and on the corresponding website, we then project the tag most frequently
assigned to those words on the website to their occurrences in the tweet. This enables us to basically
condition the tag decision for each such word on its accumulated context on the website. The assumption
of course being that the word in the tweet has the part-of-speech it most often has on the website linked to.
1784
Example Here is an example of a tweet that contains a URL:
(1) #Localization #job: Supplier / Project Manager - Localisation Vendor - NY, NY, United States
http://bit.ly/16KigBg #nlppeople
The words in the tweet are all common words, but they occur without linguistic context that could
help a tagging model to infer whether these words are nouns, verbs, named entities, etc. However, on the
website that the tweet refers to, all of these words occur in context:
(2) The Supplier/Project Manager performs the selection and maintenance . . .
For illustration, the Urbana-Champaign POS tagger
1
incorrectly tags Supplier in (1) as an adjective.
In (2), however, it gets the same word right and tags it as a noun. The tagging of (2) could potentially
help us infer that Supplier is also a noun in (1).
Obviously, the superimposition of tags in the WEB method may change the tag of a tweet word such
that it results in an unlikely tag sequence, as we will discuss later. Therefore we also implemented
type-constrained decoding (T?ackstr?om et al., 2013), i.e., prune the lattice such that the tweet words ob-
served on the website have one of the tags they were labeled with on the website (soft constraints), or,
alternatively, were forced during decoding to have the most frequent tags they were labeled with (hard
constraint decoding), thereby focusing on licensed sequences. However, none of these approaches per-
formed significantly better than the simple WEB approach on held-out data. This suggests that sequential
dependencies are less important for tagging Twitter data, which is of rather fragmented nature. Also, the
WEB approach allows us to override transitional probabilities that are biased by the observations we
made about the distribution of tags in our out-of-domain data.
Furthermore, we combine the not-so-distant supervision from linked websites (WEB) with supervision
from dictionaries (DICT). The idea here is to exploit the fact that many word types in a dictionary are
actually unambiguous, i.e., contain only a single tag. In particular, 93% of the word types in Wiktionary
2
are unambiguous. Wiktionary is a crowdsourced tag dictionary that has previously been used for mini-
mally supervised POS tagging (Li et al., 2012; T?ackstr?om et al., 2013). In the case of NER, we use a
gazetteer that combines information on PER, LOC and ORG from the KnownLists of the Illinois tagger.
3
For this gazetteer, 79% of the word types contained only a single named entity tag.
We experiment with a model that uses the dictionary only (DICT) and two ways to combine the two
sources. In the former setup, the current model is first applied to tag the tweets, then any token that
appears in the dictionary and is unambiguous is projected back to the tweet. The next two methods are
combinations of WEB and DICT: either first project the predicted tags from the website and then, in case
of conflicts, overrule predictions by the dictionary (WEB?DICT), or the other way around (DICT?WEB).
The intuition behind the idea of using linked websites as not-so-distant supervision is that while tweets
are hard to analyze (even for humans) because of the limited context available in 140 character messages,
tweets relate to real-world events, and Twitter users often use hyperlinks to websites to indicate what
real-world events their comments address. In fact, we observed that about 20% of tweets contain URLs.
The websites they link to are often newswire sites that provide more context and are written in a more
canonical language, and are therefore easier to process. Our analysis of the websites can then potentially
inform our analysis of the tweets. The tweets with the improved analyses can then be used to bootstrap
our tagging models using a self-training mechanism. Note that our method does not require tweets to
contain URLs at test time, but rather uses unlabeled tweets with URLs during training to build better
tagging models for tweets in general. At test time, these models can be applied to any tweet.
1
http://cogcomp.cs.illinois.edu/demo/pos/
2
http://en.wiktionary.org/ - We used the Wiktionary version derived by Li et al. (2012).
3
http://cogcomp.cs.illinois.edu/page/software_view/NETagger
1785
3 Experiments
3.1 Model
In our experiments we use a publicly available implementation of conditional random fields (CRF) (Laf-
ferty et al., 2001).
4
We use the features proposed by Gimpel et al. (2011), in particular features for word
tokens, a set of features that check for the presence of hyphens, digits, single quotes, upper/lowercase,
3 character prefix and suffix information. Moreover, we add Brown word cluster features that use 2
i
for
i ? 1, ..., 4 bitstring prefixes estimated from a large Twitter corpus (Owoputi et al., 2013), which is pub-
licly available.
5
We use a pool size of 1000 tweets. We experimented with other pool sizes {500,2000}
showing similar performance. The number of iterations i is set on the development data.
For NER on websites, we use the Stanford NER system (Finkel et al., 2005)
6
with POS tags from the
LAPOS tagger (Tsuruoka et al., 2011).
7
For POS we found it to be superior to use the current POS model
for re-tagging websites; for NER it was slightly better to use the Stanford NER tagger and thus off-line
NER tagging rather than retagging the websites in every iteration.
3.2 Data
In our experiments, we consider two scenarios, sometimes referred to as unsupervised and semi-
supervised domain adaptation (DA), respectively (Daum?e et al., 2010; Plank, 2011). In unsupervised
DA, we assume only (labeled) newswire data, in semi-supervised DA, we assume labeled data from both
domains, besides unlabeled target data, but the amount of labeled target data is much smaller than the
labeled source data. Most annotated corpora for English are newswire corpora. Some annotated Twitter
data sets have been made available recently, described next.
POS NER
train
WSJ (700k) REUTER-CONLL (Tjong Kim Sang and De Meulder, 2003) (200k)
GIMPEL-TRAIN (Owoputi et al., 2013) (14k) FININ-TRAIN (Finin et al., 2010) (170k)
dev
FOSTER-DEV (Foster et al., 2011) (3k) n/a
RITTER-DEV (Ritter et al., 2011) (2k) n/a
test
FOSTER-TEST (Foster et al., 2011) (2.8k) RITTER-TEST (Ritter et al., 2011) (46k)
GIMPEL-TEST (Gimpel et al., 2011) (7k) FININ-TEST (Finin et al., 2010) (51k)
HOVY-TEST (Hovy et al., 2014) FROMREIDE-TEST (Fromreide et al., 2014) (20k)
Table 1: Overview of data sets. Number in parenthesis: size in number of tokens.
Training data. An overview of the different data sets is given in Table 3.2. In our experiments, we
use the SANCL shared task
8
splits of the OntoNotes 4.0 distribution of the WSJ newswire annotations
as newswire training data for POS tagging.
9
For NER, we use the CoNLL 2003 data sets of annotated
newswire from the Reuters corpus.
10
The in-domain training POS data comes from Gimpel et al. (2011),
and the in-domain NER data comes from Finin et al. (2010) (FININ-TRAIN). These data sets are added
to the newswire sets when doing semi-supervised DA. Note that for NER, we thus do not rely on expert-
annotated Twitter data, but rely on crowdsourced annotations. We use MACE
11
(Hovy et al., 2013) to
resolve inter-annotator conflicts between turkers (50 iterations, 10 restarts, no confidence threshold). We
believe relying on crowdsourced annotations makes our set-up more robust across different samples of
Twitter data.
Development and test data. We use several evaluation sets for both tasks to prevent overfitting to a
specific sample. We use the (out-of-sample) development data sets from Ritter et al. (2011) and Foster
4
http://www.chokkan.org/software/crfsuite/
5
http://www.ark.cs.cmu.edu/TweetNLP/
6
http://http://nlp.stanford.edu/software/CRF-NER.shtml
7
http://www.logos.ic.i.u-tokyo.ac.jp/
?
tsuruoka/lapos/
8
https://sites.google.com/site/sancl2012/home/shared-task
9
LDC2011T03.
10
http://www.clips.ua.ac.be/conll2003/ner/
11
http://www.isi.edu/publications/licensed-sw/mace/
1786
et al. (2011). For NER, we simply use the parameters from our POS tagging experiments and thus do
not assume to have access to further development data. For both POS tagging and NER, we have three
test sets. For POS tagging, the ones used in Foster et al. (2011) (FOSTER-TEST) and Ritter et al. (2011)
(RITTER-TEST),
12
as well as the one presented in Hovy et al. (2014) (HOVY-TEST). For NER, we use
the data set from Ritter et al. (2011) and the two data sets from Fromreide et al. (2014) as test sets.
One is a manual correction of a held-out portion of FININ-TRAIN, named FININ-TEST; the other one
is referred to as FROMREIDE-TEST. Since the different POS corpora use different tag sets, we map all
of them corpora onto the universal POS tag set by Petrov et al. (2012). The data sets also differ in a
few annotation conventions, e.g., some annotate URLs as NOUN, some as X. Moreover, our newswire
tagger baselines tend to get Twitter-specific symbols such as URLs, hashtags and user accounts wrong.
Instead of making annotations more consistent across data sets, we follow Ritter et al. (2011) in using a
few post-processing rules to deterministically assign Twitter-specific symbols to their correct tags. The
major difference between the NER data sets is whether Twitter user accounts are annotated as PER. We
follow Finin et al. (2010) in doing so.
Unlabeled data We downloaded 200k tweet-website pairs from the Twitter search API over a period
of one week in August 2013 by searching for tweets that contain the string http and downloading the
content of the websites they linked to. We filter out duplicate tweets and restrict ourselves to websites
that contain more than one sentence (after removing boilerplate text, scripts, HTML, etc).
13
We also
require website and tweet to have at least one matching word that is not a stopword (as defined by the
NLTK stopword list).
14
Finally we restrict ourselves to pairs where the website is a subsite, because
website head pages tend to contain mixed content that is constantly updated. The resulting files are all
tokenized using the Twokenize tool.
15
Tweets were treated as one sentence, similar to the approaches in
Gimpel et al. (2011) and Owoputi et al. (2013); websites were processed by applying the Moses sentence
splitter.
16
The out-of-vocabulary (OOV) rates in Figure 2 show that in-domain training data reduces the number
of unseen words considerably, especially in the NER data sets. They also suggest that some evaluation
data sets share more vocabulary with our training data than others. In particular, we would expect better
performance on FOSTER-TEST than on RITTER-TEST and HOVY-TEST in POS tagging, as well as better
performance on FININ-TEST than on the other two NER test sets. In POS tagging, we actually do see
better results with FOSTER-TEST across the board, but in NER, FININ-TEST actually turns out to be the
hardest data set.
4 Results
4.1 POS results
Baselines We use three supervised CRF models as baselines (cf. the first part of Table 2). The first
supervised model is trained only on WSJ. This model does very well on FOSTER-DEV and FOSTER-
TEST, presumably because of the low OOV rates (Figure 2). The second supervised model is trained
only on GIMPEL-TRAIN; the third on the concatenation of WSJ and GIMPEL-TRAIN. While the second
baseline performs well on held-out data from its own sample (90.3% on GIMPEL-DEV), it performs
poorly across our out-of-sample test and development sets. Thus, it seems to overfit the sample of
tweets described in Gimpel et al. (2011). The third model trained on the concatenation of WSJ and
GIMPEL-TRAIN achieves the overall best baseline performance (88.4% macro-average accuracy). We
note that this is around one percentage point better than the best available off-the-shelf system for Twitter
(Owoputi et al., 2013) with an average accuracy of 87.5%.
12
Actually (Ritter et al., 2011) do cross-validation over this data, but we use the splits of Derczynski et al. (2013) for POS.
13
Using https://github.com/miso-belica/jusText
14
ftp://ftp.cs.cornell.edu/pub/smart/english.stop
15
https://github.com/brendano/ark-tweet-nlp
16
https://github.com/moses-smt/mosesdecoder/blob/master/scripts/ems/support/
split-sentences.perl
1787
Figure 2: Test set (type-level) OOV rates for POS (left) and NER (right).
l l
l
l l l
l l l l l l l
l l l l l l l l l l l l l l l l l l l l l l
0 5 10 15 20 25 30
88.5
89.0
89.5
90.0
DEV?avg wsj
iteration
accu
racy
l self?trainingWebDictWeb<DictDict<Web
l l
l l l
l l l l l l l
l l l l l l l l l l l l
l l
l l l l
0 5 10 15 20 25 30
88.8
89.0
89.2
89.4
89.6
89.8
90.0
90.2
DEV?avg wsj+gimpel
iteration
accu
racy
Figure 3: Learning curves on DEV-avg for systems trained on WSJ (left) and WSJ+GIMPEL (right) used
to set the hyperparameter i.
Learning with URLs The results of our approaches are presented in Table 2. The hyperparameter i
was set on the development data (cf. Figure 3). Note, again, that they do not require the test data to
contain URLs. First of all, naive self-training does not work: accuracy declines or is just around baseline
performance (Table 2 and Figure 3). In contrast, our augmented self-training methods with WEB or
DICT reach large improvements. In case we assume no target training data (train on WSJ only, i.e.
unsupervised DA), we obtain improvements of up to 9.1% error reduction. Overall the system improves
from 88.42% to 89.07%. This also holds for the second scenario, i.e. training on WSJ+GIMPEL-TRAIN
(semi-supervised DA, i.e., the case where we have some labeled target data, besides the pool of unlabeled
tweets) where we reach error reductions of up to 10%. Our technique, in other words, improves the
robustness of taggers, leading to much better performance on new samples of tweets.
4.2 NER results
For our NER results, cf. Table 3, we used the same feature models and parameter settings as those used for
POS tagging, except conditioning also on POS information. It is conceivable that other parameter settings
would have led to better results, but we did not want to assume the existence of in-domain development
data for this task. Our baselines are again supervised systems, as well as off-the-shelf systems. Our in-
1788
DEV-avg TEST TEST-avg
FOSTER HOVY RITTER
Baselines trained on
WSJ 88.82 91.87 87.01 86.38 88.42
GIMPEL-TRAIN 83.32 84.86 86.03 81.67 84.19
WSJ+GIMPEL-TRAIN 89.07 91.59 87.50 87.39 88.83
Systems trained on WSJ
SELF-TRAINING i = 25 85.52 91.80 86.72 85.90 88.14
DICT i = 25 85.61 92.08 87.63 85.68 88.46
WEB i = 25 85.27 92.47 87.30 86.60 88.79
DICT?WEB i = 25 86.11 92.61 87.70 86.69 89.00
WEB?DICT i = 25 86.15 92.57 88.12 86.51 89.07
max err.red 4.7% 9.1% 8.6% 2.3% 4.2%
Systems trained on WSJ+GIMPEL-TRAIN
SELF-TRAINING i = 27 89.12 91.83 86.88 87.43 88.71
DICT i = 27 89.43 92.22 88.38 87.69 89.43
WEB i = 27 89.82 92.43 87.43 88.21 89.36
DICT?WEB i = 27 90.04 92.43 88.38 88.48 89.76
WEB?DICT i = 27 90.04 92.40 87.99 88.39 89.59
max err.red 8.9% 10% 7.1% 8.6% 8.4%
Table 2: POS results.
house supervised baselines perform better than the available off-the-shelf systems, including the system
provided by Ritter et al. (2011) (TEST-avg of 54.2%). We report micro-average F
1
-scores over entity
types, computed using the publicly available evaluation script.
17
Our approaches again lead to substantial
error reductions of 8?13% across our NER evaluation data sets.
TEST TEST-avg
RITTER FROMREIDE FININ
Baseline trained on
CONLL+FININ-TRAIN 77.44 82.13 74.02 77.86
Systems trained on CONLL+FININ-TRAIN
SELF-TRAINING i = 27 78.63 82.88 74.89 78.80
DICT i = 27 65.24 69.1 65.45 66.60
WEB i = 27 78.29 83.82 74.99 79.03
DICT?WEB i = 27 78.53 83.91 75.83 79.42
WEB?DICT i = 27 65.97 69.92 65.86 67.25
err.red 9.1% 13.3% 8.0% 9.8%
Table 3: NER results.
5 Error analysis
The majority of cases where our taggers improve on the ARK tagger (Owoputi et al., 2013) seem to
relate to richer linguistic context. The ARK tagger incorrectly tags the sequence Man Utd as PRT-
NOUN, whereas our taggers correctly predict NOUN-NOUN. In a similar vein, our taggers correctly
predict the tag sequence NOUN-NOUN for Radio Edit, while the ARK tagger predicts NOUN-VERB.
However, some differences seem arbitrary. For example, the ARK tagger tags the sequence Nokia
17
http://www.cnts.ua.ac.be/conll2000/chunking/
1789
D5000 in FOSTER-TEST as NOUN-NUM. Our systems correctly predict NOUN-NOUN, but it is not
clear which analysis is better in linguistic terms. Our systems predict a sequence such as Love his version
to be VERB-PRON-NOUN, whereas the ARK tagger predicts VERB-DET-NOUN. Both choices seem
linguistically motivated.
Finally, some errors are made by all systems. For example, the word please in please, do that, for
example, is tagged as VERB by all systems. In FOSTER-TEST, this is annotated as X (which in the PTB
style was tagged as interjection UH). Obviously, please often acts as a verb, and while its part-of-speech
in this case may be debatable, we see please annotated as a verb in similar contexts in the PTB, e.g.:
(3) Please/VERB make/VERB me/PRON . . .
It is interesting to look at the tags that are projected from the websites to the tweets. Several of the
observed projections support the intuition that coupling tweets and the websites they link to enables us
to condition our tagging decisions on a richer linguistic context. Consider, for example Salmon-Safe,
initially predicted to be a NOUN, but after projection correctly analyzed as an ADJ:
Word Context Initial tag Projected tag
Salmon-Safe . . . parks NOUN ADJ
Snohomish . . . Bakery ADJ NOUN
toxic ppl r . . . NOUN ADJ
One of the most frequent projections is analyzing you?re, correctly, as a VERB rather than an ADV (if
the string is not split by tokenization).
One obvious limitation of the WEB-based models is that the projections apply to all occurrences of a
word. In rare cases, some words occur with different parts of speech in a single tweet, e.g., wish in:
(4) If I gave you one wish that will become true . What?s your wish ?... ? i wish i?ll get <num> wishes
from you :p <url>
In this case, our models enforce all occurrences of wish to, incorrectly, be verbs.
6 Related work
Previous work on tagging tweets has assumed labeled training data (Ritter et al., 2011; Gimpel et al.,
2011; Owoputi et al., 2013; Derczynski et al., 2013). Strictly supervised approaches to analyzing Twitter
has the weakness that labeled data quickly becomes unrepresentative of what people write on Twitter.
This paper presents results using no in-domain labeled data that are significantly better than several off-
the-shelf systems, as well as results leveraging a mixture of out-of-domain and in-domain labeled data
to reach new highs across several data sets.
Type-constrained POS tagging using tag dictionaries has been explored in weakly supervised settings
(Li et al., 2012), as well as for cross-language learning (Das and Petrov, 2011; T?ackstr?om et al., 2013).
Our type constraints in POS tagging come from tag dictionaries, but also from linked websites. The idea
of using linked websites as distant supervision is similar in spirit to the idea presented in Ganchev et
al. (2012) for search query tagging.
Ganchev et al. (2012), considering the problem of POS tagging search queries, tag search queries and
the associated snippets provided by the search engine, projecting tags from the snippets to the queries,
guided by click-through data. They do not incorporate tag dictionaries, but consider a slightly more
advanced matching of snippets and search queries, giving priority to n-gram matches with larger n.
Search queries contain limited contexts, like tweets, but are generally much shorter and exhibit less
spelling variation than tweets.
In NER, it is common to use gazetteers, but also dictionaries as distant supervision (Kazama and
Torisawa, 2007; Cucerzan, 2007). R?ud et al. (2011) consider using search engines for distant supervision
of NER of search queries. Their set-up is very similar to Ganchev et al. (2012), except they do not use
click-through data. They use the search engine snippets to generate feature representations rather than
projections. Want et al. (2013) also use distant supervision for NER, i.e., Wikipedia page view counts,
1790
applying their model to Twitter data, but their results are considerably below the state of the art. Also,
their source of supervision is not linked to the individual tweets in the way mentioned websites are.
In sum, our method is the first successful application of distant supervision to POS tagging and NER
for Twitter. Moreover, it is, to the best of our knowledge, the first paper that addresses both problems
using the same technique. Finally, our results are significantly better than state-of-the-art results in both
POS tagging and NER.
7 Conclusion
We presented a semi-supervised approach to POS tagging and NER for Twitter data that uses dictionaries
and linked websites as a source of not-so-distant (or linked) supervision to guide the bootstrapping. Our
approach outperforms off-the-shelf taggers when evaluated across various data sets, achieving average
error reductions across data sets of 5% on POS tagging and 10% on NER over state-of-the-art baselines.
References
Timothy Baldwin, Paul Cook, Marco Lui, Andrew MacKinlay, and Li Wang. 2013. How noisy social media text,
how diffrnt social media sources? In IJCNLP.
Edward Benson, Aria Haghighi, and Regina Barzilay. 2011. Event discovery in social media feeds. In ACL.
Silvia Cucerzan. 2007. Large-scale named entity disambiguation based on wikipedia data. In EMNLP-CoNLL.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-of-speech tagging with bilingual graph-based projections.
In ACL.
Hal Daum?e, Abhishek Kumar, and Avishek Saha. 2010. Frustratingly easy semi-supervised domain adaptation.
In ACL Workshop on Domain Adaptation for NLP.
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina Bontcheva. 2013. Twitter part-of-speech tagging for all:
overcoming sparse and noisy data. In RANLP.
Jacob Eisenstein. 2013. What to do about bad language on the internet. In NAACL.
Tim Finin, Will Murnane, Anand Karandikar, Nicholas Keller, Justin Martineau, and Mark Dredze. 2010. Anno-
tating named entities in Twitter data with crowdsourcing. In NAACL-HLT 2010 Workshop on Creating Speech
and Language Data with Amazon?s Mechanical Turk.
Jenny Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into infor-
mation extraction systems by Gibbs sampling. In ACL.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner, Josef Le Roux, Joakim Nivre, Deirde Hogan, and Josef van
Genabith. 2011. From news to comments: Resources and benchmarks for parsing the language of Web 2.0. In
IJCNLP.
Hege Fromreide, Dirk Hovy, and Anders S?gaard. 2014. Crowdsourcing and annotating ner for twitter #drift. In
LREC.
Kuzman Ganchev, Keith Hall, Ryan McDonald, and Slav Petrov. 2012. Using search-logs to improve query
tagging. In ACL.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-Speech Tagging for Twitter:
Annotation, Features, and Experiments. In ACL.
David Hand. 2006. Classifier technology and illusion of progress. Statistical Science, 21(1):1?15.
Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani, and Eduard Hovy. 2013. Learning whom to trust with
MACE. In NAACL.
Dirk Hovy, Barbara Plank, and Anders S?gaard. 2014. When pos datasets don?t add up: Combatting sample bias.
In LREC.
1791
Zhongqiang Huang, Mary Harper, and Slav Petrov. 2009. Self-training with products of latent variable grammars.
In EMNLP.
Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in NLP. In ACL.
Long Jiang, Mo Yo, Ming Zhou, Xiaohua Liu, and Tiejun Zhao. 2011. Target-dependent Twitter sentiment
classification. In ACL.
Jun?ichi Kazama and Kentaro Torisawa. 2007. Exploiting wikipedia as external knowledge for named entity
recognition. In EMNLP-CoNLL.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: probabilistic models
for segmenting and labeling sequence data. In ICML.
Shen Li, Jo?ao Grac?a, and Ben Taskar. 2012. Wiki-ly supervised part-of-speech tagging. In EMNLP.
David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In HLT-NAACL.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without
labeled data. In ACL.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A Smith. 2013.
Improved part-of-speech tagging for online conversational text with word clusters. In NAACL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In LREC.
Sasa Petrovic, Miles Osborne, and Victor Lavrenko. 2010. Streaming first story detection with application to
Twitter. In NAACL.
Barbara Plank. 2011. Domain Adaptation for Parsing. Ph.D. thesis, University of Groningen.
Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011. Named entity recognition in tweets: an experimental study. In
EMNLP.
Stefan R?ud, Massimiliano Ciaramita, Jens M?uller, and Hinrich Sch?utze. 2011. Piggyback: Using search engines
for robust cross-domain named entity recognition. In ACL.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan McDonald, and Joakim Nivre. 2013. Token and type con-
straints for cross-lingual part-of-speech tagging. TACL, 1:1?12.
Erik F Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-
independent named entity recognition. In In CoNLL.
Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichi Kazama. 2011. Learning with lookahead: can history-based
models rival globally optimized models? In CoNLL.
Chun-Kai Wang, Bo-June Hsu, Ming-Wei Chang, and Emre Kiciman. 2013. Simple and knowledge-intensive
generative model for named entity recognition. Technical report, Microsoft Research.
1792
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Tutorial Abstracts,
pages 11?13, Dublin, Ireland, August 23-29 2014.
Selection Bias, Label Bias, and Bias in Ground Truth
Anders S?gaard, Barbara Plank, and Dirk Hovy
Center for Language Technology
University of Copenhagen
soegaard@hum.ku.dk, {bplank|dhovy}@cst.dk
Introduction
Language technology is biased toward English newswire. In POS tagging, we get 97?98 words right out
of a 100 in English newswire, but results drop to about 8 out of 10 when running the same technology
on Twitter data. In dependency parsing, we are able to identify the syntactic head of 9 out of 10 words
in English newswire, but only 6?7 out of 10 in tweets. Replace references to Twitter with references to a
low-resource language of your choice, and the above sentence is still likely to hold true.
The reason for this bias is obviously that mainstream language technology is data-driven, based on
supervised statistical learning techniques, and annotated data resources are widely available for English
newswire. The situation that arises when applying off-the-shelf language technology, induced from
annotated newswire corpora, to something like Twitter, is a bit like when trying to predict elections from
Xbox surveys (Wang et al., 2013). Our induced models suffer from a data selection bias.
This is actually not the only way our data is biased. The available resources for English newswire
are the result of human annotators following specific guidelines. Humans err, leading to label bias, but
more importantly, annotation guidelines typically make debatable linguistic choices. Linguistics is not
an exact science, and we call the influence of annotation guidelines bias in ground truth.
In the tutorial, we present various case studies for each kind of bias, and show several methods that
can be used to deal with bias. This results in improved performance of NLP systems.
Selection Bias
The situation that arises when applying off-the-shelf language technology, induced from annotated
newswire corpora, to something like Twitter, is, as mentioned, a bit like when trying to predict elec-
tions from Xbox surveys. In the case of elections, however, we can correct most of the selection bias by
post-stratification or instance weighting (Wang et al., 2013). In language technology, the bias correction
problem is harder.
In the case of elections, you have a single output variable and various demographic observed variables.
All values taken by discrete variables at test time can be assumed to have been observed, and all values
observed at training time can be assumed to be seen at test time. In language technology, we typically
have several features only seen in training data and several features only seen in test data.
The latter observation has led to interest in bridging unseen words to known ones (Blitzer et al.,
2006; Turian et al., 2010), while the former has led to the development of learning algorithms that
prevent feature swamping (Sutton et al., 2006), i.e., that very predictive features prevents weights associ-
ated with less predictive, correlated features from being updated. Note, however, that post-stratification
(Smith, 1988) may prevent feature swamping, and that predictive approaches to bias correction (Roy-
all, 1988) may solve both problems. Instance weighting (Shimodaira, 2000), which is a generalization
of post-stratificiation, has received some interest in language technology (Jiang and Zhai, 2007; Foster
et al., 2011), but most work on domain adaptation in language technology has focused on predictive
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
11
approaches, i.e., semi-supervised learning (Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Mc-
Closky et al., 2010; Chen et al., 2011).
Selection bias introduces a bias in P (X). Note that, in theory, this should not hurt discriminative algo-
rithms trying to estimate P (Y |X), without estimating P (X), but in practice it still does. The inductive
bias of our algorithms and the size our samples make our models sensitive to selection bias (Zadrozny,
2004). Predictive approaches try to correct this bias by adding more (pseudo-labeled) data to the training
sample, while post-stratification and instance weighting reweigh the data to make P (X) similar to the
distribution observed in the population. As mentioned, this will never solve the problem with unseen
features, since you cannot up-weigh a null feature.
Semi-supervised learning can correct modest selection bias, but if the domain gap is too wide, our
initial predictions in the target domain will be poor, and semi-supervised learning is likely to increase
bias rather than decrease it. However, recent work has shown that semi-supervised learning can be
combined with distant supervision and correct bias in cases where semi-supervised learning algorithms
typically fail (Plank et al., 2014).
In the tutorial we illustrate these different approaches to selection bias correction, with discriminative
learning of POS taggers for English Twitter as our running example.
Label Bias
In most annotation projects, there is an initial stage, where the project managers compare annotators?
performance, compute agreement scores, select reliable annotators, adjudicate, and elaborate on anno-
tation guidelines, if necessary. Such procedures are considered necessary to correct for the individual
biases of the annotators (label bias). However, this is typically only for the first batches of data, and it
is well-known that even some of the most widely used annotated corpora (such as the Penn Treebank)
contain many errors (Dickinson and Meurers, 2003) in the form of inconsistent annotations of the same
n-grams.
Obviously, using non-expert annotators, e.g., through crowd-sourcing platforms, increase the label
bias considerably. One way to reduce this bias involves collecting several annotations for each datapoint
and averaging over them, which is often feasible because of the low cost of non-expert annotation. This
is called majority voting and is analogous to using ensembles of models to obtain more robust systems.
In the tutorial we discuss alternatives to averaging over annotators, incl., using EM to estimate anno-
tator confidence (Hovy et al., 2013), and joint learning of annotator competence and model parameters
(Raykar and Yu, 2012).
Bias in Ground Truth
In annotation projects, we use inter-annotator agreement measures and annotation guidelines to ensure
consistent annotations. However, annotation guidelines often make linguistically debatable and even
somewhat arbitrary decisions, and inter-annotator agreement is often less than perfect. Some annotators,
for example, may annotate socialin social media as a noun, others may annotate it as an adjective. In
this part of the tutorial, we discuss how to correct for the bias introduced by annotation guidelines. For
both label bias and bias in ground truth, we, again, use POS tagging for English Twitter as our running
example.
Evaluation
Once we accept our data is biased in different ways, we need to reconsider model evaluation. If our data
was selected in a biased way, say from a few editions of the Wall Street Journal, does significance over
data points make much sense? If our annotators have individual biases, can we no longer evaluate our
models on the data of one or two annotators? If the annotation guidelines introduce biases in ground truth,
can we somehow correct for that? In practice we typically do not have hundreds of datasets annotated by
different annotators using different annotation guidelines, but in the tutorial we present various ways of,
nevertheless, correcting for some of these biases.
12
Acknowledgements
This research is funded by the ERC Starting Grant LOWLANDS No. 313695.
References
John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence
learning. In EMNLP.
Minmin Chen, Killiang Weinberger, and John Blitzer. 2011. Co-training for domain adaptation. In NIPS.
Markus Dickinson and Detmar Meurers. 2003. Detecting errors in part-of-speech annotation. In EACL.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner, Josef Le Roux, Joakim Nivre, Deirde Hogan, and Josef van
Genabith. 2011. From news to comments: Resources and benchmarks for parsing the language of Web 2.0. In
IJCNLP.
Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani, and Eduard Hovy. 2013. Learning whom to trust with
MACE. In NAACL.
Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in NLP. In ACL.
David McClosky, Eugene Charniak, and Mark Johnson. 2010. Automatic domain adaptation for parsing. In
NAACL-HLT.
Barbara Plank, Dirk Hovy, Ryan McDonald, and Anders S?gaard. 2014. Adapting taggers to Twitter with not-so-
distant supervision. In COLING.
Vikas C. Raykar and Shipeng Yu. 2012. Eliminating Spammers and Ranking Annotators for Crowdsourced
Labeling Tasks. Journal of Machine Learning Research, 13:491?518.
Roi Reichart and Ari Rappoport. 2007. Self-training for enhancement and domain adaptation of statistical parsers
trained on small datasets. In ACL.
R Royall. 1988. The prediction approach to sampling theory. In Rao Krishnaiah, editor, Handbook of Statistics.
North-Holland.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency parsing and domain adaptation with LR models and parser
ensembles. In EMNLP-CoNLL.
Hidetoshi Shimodaira. 2000. Improving predictive inference under covariate shift by weighting the log-likelihood
function. Journal of Statistical Planning and Inference, 90:227?244.
T Smith. 1988. Post-stratification. The Statistician, 40.
Charles Sutton, Michael Sindelar, and Andrew McCallum. 2006. Reducing weight undertraining in structured
discriminative learning. In NAACL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for
semi-supervised learning. In ACL.
Wei Wang, David Rotschild, Sharad Goel, and Andrew Gelman. 2013. Forecasting elections with non-
representative polls. Forthcoming in International Journal of Forecasting.
Bianca Zadrozny. 2004. Learning and evaluating classifiers under sample selection bias. In ICML.
13
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1411?1416,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A Walk-based Semantically Enriched Tree Kernel
Over Distributed Word Representations
Shashank Srivastava1 Dirk Hovy2 Eduard Hovy1
(1) Carnegie Mellon University, Pittsburgh
(2) Center for Language Technology, University of Copenhagen, Denmark
{ssrivastava,hovy}@cmu.edu, mail@dirkhovy.com
Abstract
In this paper, we propose a walk-based graph
kernel that generalizes the notion of tree-
kernels to continuous spaces. Our proposed
approach subsumes a general framework for
word-similarity, and in particular, provides a
flexible way to incorporate distributed repre-
sentations. Using vector representations, such
an approach captures both distributional se-
mantic similarities among words as well as the
structural relations between them (encoded as
the structure of the parse tree). We show an ef-
ficient formulation to compute this kernel us-
ing simple matrix operations. We present our
results on three diverse NLP tasks, showing
state-of-the-art results.
1 Introduction
Capturing semantic similarity between sentences
is a fundamental issue in NLP, with applications in
a wide range of tasks. Previously, tree kernels based
on common substructures have been used to model
similarity between parse trees (Collins and Duffy,
2002; Moschitti, 2004; Moschitti, 2006b). These
kernels encode a high number of latent syntactic
features within a concise representation, and com-
pute the similarity between two parse trees based
on the matching of node-labels (words, POS tags,
etc.), as well as the overlap of tree structures. While
this is sufficient to capture syntactic similarity, it
does not capture semantic similarity very well, even
when using discrete semantic types as node labels.
This constrains the utility of many traditional
tree kernels in two ways: i) two sentences that
are syntactically identical, but have no semantic
similarity can receive a high matching score (see
Table 1, top) while ii) two sentences with only local
syntactic overlap, but high semantic similarity can
receive low scores (see Table 1, bottom).
tree pairs semantic syntactic score
?
?
high
?
?
low
love
we toys
crush
they puppies
kissed
she cat
gave
she
her
kiss
a
friend
feline
green little
her
Table 1: Traditional tree kernels do not capture se-
mantic similarity
In contrast, distributional vector representations
of words have been successful in capturing fine-
grained semantics, but lack syntactic knowledge.
Resources such as Wordnet, dictionaries and on-
tologies that encode different semantic perspectives
can also provide additional knowledge infusion.
In this paper, we describe a generic walk-based
graph kernel for dependency parse trees that sub-
sumes general notions of word-similarity, while
focusing on vector representations of words to
capture lexical semantics. Through a convolutional
framework, our approach takes into account the
distributional semantic similarities between words
in a sentence as well as the structure of the parse
tree. Our main contributions are:
1. We present a new graph kernel for NLP that ex-
tends to distributed word representations, and
diverse word similarity measures.
2. Our proposed approach provides a flexible
framework for incorporating both syntax and
semantics of sentence level constructions.
3. Our generic kernel shows state-of-the-art per-
formance on three eclectic NLP tasks.
1411
2 Related Work
Tree kernels in NLP Tree kernels have been ex-
tensively used to capture syntactic information about
parse trees in tasks such as parsing (Collins and
Duffy, 2002), NER (Wang et al, 2010; Cumby and
Roth, 2003), SRL (Moschitti et al, 2008) and rela-
tion extraction (Qian et al, 2008). These kernels are
based on the paradigm that parse trees are similar if
they contain many common substructures, consist-
ing of nodes with identical labels (Vishwanathan and
Smola, 2003; Collins and Duffy, 2002). Moschitti
(2006a) proposed a partial tree kernel that adds flex-
ibility in matching tree substructures. Croce et al
(2011) introduce a lexical semantic tree kernel that
incorporates continuous similarity values between
node labels, albeit with a different focus than ours
and would not match words with different POS. This
would miss the similarity of ?feline friend? and ?cat?
in our examples, as it requires matching the adjective
?feline? with ?cat?, and verb ?kissed? with ?kiss?.
Walk based kernels Kernels for structured data
derive from the seminal Convolution Kernel for-
malism by Haussler (1999) for designing kernels
for structured objects through local decompositions.
Our proposed kernel for parse trees is most closely
associated with the random walk-based kernels de-
fined by Gartner et al (2003) and Kashima et al
(2003). The walk-based graph kernels proposed by
Gartner et al (2003) count the common walks be-
tween two input graphs, using the adjacency matrix
of the product graph. This work extends to graphs
with a finite set of edge and node labels by appro-
priately modifying the adjacency matrix. Our kernel
differs from these kernels in two significant ways: (i)
Our method extends beyond label matching to con-
tinuous similarity metrics (this conforms with the
very general formalism for graph kernels in Vish-
wanathan et al (2010)). (ii) Rather than using the
adjacency matrix to model edge-strengths, we mod-
ify the product graph and the corresponding adja-
cency matrix to model node similarities.
3 Vector Tree Kernels
In this section, we describe our kernel and an al-
gorithm to compute it as a simple matrix multiplica-
tion formulation.
3.1 Kernel description
The similarity kernel K between two dependency
trees can be defined as:
K(T1, T2) =
?
h1?T1,h2?T2
len(h1)=len(h2)
k(h1, h2)
where the summation is over pairs of equal length
walks h1 and h2 on the trees T1 and T2 respec-
tively. The similarity between two n length walks,
k(h1, h2), is in turn given by the pairwise similari-
ties of the corresponding nodes vih in the respective
walks, measured via the node similarity kernel ?:
k(h1, h2) =
n?
i:1
?(vh1i , v
h2
i )
In the context of parse trees, nodes vh1i and v
h2
i cor-
respond to words in the two parse trees, and thus can
often be conveniently represented as vectors over
distributional/dependency contexts. The vector rep-
resentation allows us several choices for the node
kernel function ?. In particular, we consider:
1. Gaussian : ?(v1, v2) = exp
(
? ?v1?v2?
2
2?2
)
2. Positive-Linear: ?(v1, v2) = max(vT1 v2, 0)
3. Sigmoid: ?(v1, v2) =
(
1 + tanh(?vT1 v2)
)
/2
We note that the kernels above take strictly non-
negative values in [0, 1] (assuming word vector rep-
resentations are normalized). Non-negativity is nec-
essary, since we define the walk kernel to be the
product of the individual kernels. As walk kernels
are products of individual node-kernels, bounded-
ness by 1 ensures that the kernel contribution does
not grow arbitrarily for longer length walks.
The kernel function K puts a high similarity
weight between parse trees if they contain com-
mon walks with semantically similar words in corre-
sponding positions. Apart from the Gaussian kernel,
the other two kernels are based on the dot-product
of the word vector representations. We observe that
the positive-linear kernel defined above is not a Mer-
cer kernel, since the max operation makes it non-
positive semidefinite (PSD). However, this formu-
lation has desirable properties, most significant be-
ing that all walks with one or more node-pair mis-
matches are strictly penalized and add no score to
1412
the tree-kernel. This is a more selective condition
than the other two kernels, where mediocre walk
combinations could also add small contributions to
the score. The sigmoid kernel is also non-PSD, but
is known to work well empirically (Boughorbel et
al., 2005). We also observe while the summation in
the kernel is over equal length walks, the formalism
can allow comparisons over different length paths by
including self-loops at nodes in the tree.
With a notion of similarity between words that
defines the local node kernels, we need computa-
tional machinery to enumerate all pairs of walks
between two trees, and compute the summation
over products in the kernel K(T1, T2) efficiently.
We now show a convenient way to compute this as
a matrix geometric series.
3.2 Matrix Formulation for Kernel
Computation
Walk-based kernels compute the number of com-
mon walks using the adjacency matrix of the prod-
uct graph (Gartner et al, 2003). In our case, this
computation is complicated by the fact that instead
of counting common walks, we need to compute a
product of node-similarities for each walk. Since
we compute similarity scores over nodes, rather than
edges, the product for a walk of length n involves
n+ 1 factors.
However, we can still compute the tree kernel K
as a simple sum of matrix products. Given two trees
T (V,E) and T ?(V ?, E?), we define a modified prod-
uct graph G(Vp, Ep) with an additional ghost node
u added to the vertex set. The vertex and edge sets
for the modified product graph are given as:
Vp := {(vi1, vj1
?) : vi1 ? V, vj1
? ? V ?} ? u
Ep := {((vi1, vj1
?), (vi2, vj2
?)) : (vi1, vi2) ? E,
(vj1
?, vj2
?)) ? E?}
?
{(u, (vi1, vj1
?)) : vi1 ? V, vj1
? ? V ?}
The modified product graph thus has additional
edges connecting u to all other nodes. In our for-
mulation, u now serves as a starting location for all
random walks on G, and a k + 1 length walk of G
corresponds to a pair of k length walks on T and T ?.
We now define the weighted adjacency matrixW for
G, which incorporates the local node kernels.
W(vi1,vj1?),(vi2,vj2?) =
{
0 : ((vi1,vj1?),(vi2,vj2?)) /? Ep
?(vi2, vj2?) : otherwise
Wu,(vi1,vj1?) = ?(vi1, vj1
?)
W(v,u) = 0 ? v ? Vp
There is a straightforward bijective mapping from
walks on G starting from u to pairs of walks on T
and T ?. Restricting ourselves to the case when the
first node of a k + 1 length walk is u, the next k
steps allow us to efficiently compute the products of
the node similarities along the k nodes in the corre-
sponding k length walks in T and T ?. Given this ad-
jacency matrix for G, the sum of values of k length
walk kernels is given by the uth row of the (k+1)th
exponent of the weighted adjacency matrix (denoted
asW k+1). This corresponds to k+1 length walks on
G starting from u and ending at any node. Specif-
ically, Wu,(vi,v?j) corresponds to the sum of similar-
ities of all common walks of length n in T and T ?
that end in vi in T and v?j in T
?. The kernel K for
walks upto length N can now be calculated as :
K(T, T ?) =
|Vp|?
i
Su,i
where
S = W +W 2 + ...WN+1
We note that in out formulation, longer walks are
naturally discounted, since they involve products of
more factors (generally all less than unity).
The above kernel provides a similarity measure
between any two pairs of dependency parse-trees.
Depending on whether we consider directional re-
lations in the parse tree, the edge set Ep changes,
while the procedure for the kernel computation re-
mains the same. Finally, to avoid larger trees yield-
ing larger values for the kernel, we normalize the
kernel by the number of edges in the product graph.
4 Experiments
We evaluate the Vector Tree Kernel (VTK) on
three NLP tasks. We create dependency trees using
the FANSE parser (Tratz and Hovy, 2011), and
use distribution-based SENNA word embeddings
by Collobert et al (2011) as word representations.
These embeddings provide low-dimensional vector
1413
representations of words, while encoding distribu-
tional semantic characteristics. We use LibSVM for
classification. For sake of brevity, we only report
results for the best performing kernel.
We first consider the Cornell Sentence Polarity
dataset by Pang and Lee (2005). The task is to
identify the polarity of a given sentence. The
data consists of 5331 sentences from positive and
negative movie reviews. Many phrases denoting
sentiments are lexically ambiguous (cf. ?terribly
entertaining? vs ?terribly written?), so simple lexi-
cal approaches are not expected to work well here,
while syntactic context could help disambiguation.
Next, we try our approach on the MSR paraphrase
corpus. The data contains a training set of 4077
pairs of sentences, annotated as paraphrases and
non-paraphrases, and a test-set of 1726 sentence
pairs. Each instance consists of a pair of sentences,
so the VTK cannot be directly used by a kernel
machine for classification. Instead, we generate
16 kernel values based for each pair on different
parameter settings of the kernel, and feed these as
features to a linear SVM.
We finally look at the annotated Metaphor corpus
by (Hovy et al, 2013). The dataset consists of sen-
tences with specified target phrases. The task here is
to classify the target use as literal or metaphorical.
We focus on target phrases by upweighting walks
that pass through target nodes. This is done by
simply multiplying the corresponding entries in the
adjacency matrix by a constant factor.
5 Results
5.1 Sentence Polarity Dataset
Prec Rec F1 Acc
Albornoz et al0.63 ? ? 0.63
WNA+synsets 0.61 ? ? 0.61
WNA 0.53 ? ? 0.51
DSM 0.54 0.55 0.55 0.54
SSTK 0.49 0.48 0.48 0.49
VTK 0.65 0.58 0.62 0.67
Table 2: Results on Sentence Polarity dataset
On the polarity data set, Vector Tree Kernel
(VTK) significantly outperforms the state-of-the-art
method by Carrillo de Albornoz et al (2010), who
use a hybrid model incorporating databases of af-
fective lexicons, and also explicitly model the ef-
fect of negation and quantifiers (see Table 2). Lex-
ical approaches using pairwise semantic similarity
of SENNA embeddings (DSM), as well as Word-
net Affective Database-based (WNA) labels perform
poorly (Carrillo de Albornoz et al, 2010), showing
the importance of syntax for this particular problem.
On the other hand, a syntactic tree kernel (SSTK)
that ignores distributional semantic similarity be-
tween words, fails as expected.
5.2 MSR Paraphrase Dataset
Prec Rec F1 Acc
BASE 0.72 0.86 0.79 0.69
Zhang et al0.74 0.88 0.81 0.72
Qiu et al0.73 0.93 0.82 0.72
Malakasiotis 0.74 0.94 0.83 0.74
Finch 0.77 0.90 0.83 0.75
VTK 0.72 0.95 0.82 0.72
Table 3: Results on MSR Paraphrase corpus
On the MSR paraphrase corpus, VTK performs
competitively against state-of-the-art-methods. We
expected paraphrasing to be challenging to our
method, since it can involve little syntactic overlap.
However, data analysis reveals that the corpus gener-
ally contains sentence pairs with high syntactic sim-
ilarity. Results for this task are encouraging since
ours is a general approach, while other systems use
multiple task-specific features like semantic role la-
bels, active-passive voice conversion, and synonymy
resolution. In the future, incorporating such features
to VTK should further improve results for this task .
5.3 Metaphor Identification
Acc P R F1
CRF 0.69 0.74 0.50 0.59
SVM+DSM 0.70 0.63 0.80 0.71
SSTK 0.75 0.70 0.80 0.75
VTK 0.76 0.67 0.87 0.76
Table 4: Results on Metaphor dataset
On the Metaphor corpus, VTK improves the pre-
vious score by Hovy et al (2013), whose approach
uses an conjunction of lexical and syntactic tree ker-
nels (Moschitti, 2006b), and distributional vectors.
VTK identified several templates of metaphor usage
such as ?warm heart? and ?cold shoulder?. We look
towards approaches for automatedly mining such
metaphor patterns from a corpus.
6 Conclusion
We present a general formalism for walk-based
kernels to evaluate similarity of dependency trees.
1414
Our method generalizes tree kernels to take dis-
tributed representations of nodes as input, and cap-
ture both lexical semantics and syntactic structures
of parse trees. Our approach has tunable parame-
ters to look for larger or smaller syntactic constructs.
Our experiments shows state-of-the-art performance
on three diverse NLP tasks. The approach can gen-
eralize to any task involving structural and local sim-
ilarity, and arbitrary node similarity measures.
References
Sabri Boughorbel, Jean-Philippe Tarel, and Nozha Bouje-
maa. 2005. Conditionally positive definite kernels for
svm based image recognition. In ICME, pages 113?
116.
Jorge Carrillo de Albornoz, Laura Plaza, and Pablo
Gerva?s. 2010. A hybrid approach to emotional sen-
tence polarity and intensity classification. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning, pages 153?161. Associa-
tion for Computational Linguistics.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. In Pro-
ceedings of the 40th annual meeting on association for
computational linguistics, pages 263?270. Association
for Computational Linguistics.
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
Journal of Machine Learning Research, 12:2493?
2537.
Danilo Croce, Alessandro Moschitti, and Roberto Basili.
2011. Structured lexical similarity via convolution
kernels on dependency trees. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1034?1046. Association for
Computational Linguistics.
Chad Cumby and Dan Roth. 2003. On kernel methods
for relational learning. In In Proc. of the International
Conference on Machine Learning, pages 107?114.
Andrew Finch. 2005. Using machine translation evalu-
ation techniques to determine sentence-level semantic
equivalence. In In IWP2005.
Thomas Gartner, Peter Flach, and Stefan Wrobel. 2003.
On graph kernels: Hardness results and efficient al-
ternatives. In Proceedings of the Annual Conference
on Computational Learning Theory, pages 129?143.
Springer.
David Haussler. 1999. Convolution kernels on discrete
structures. Technical Report Technical Report UCS-
CRL-99-10, UC Santa Cruz.
Dirk Hovy, Shashank Srivastava, Sujay Kumar Jauhar,
Mrinmaya Sachan, Kartik Goyal, Huiying Li, Whit-
ney Sanders, and Eduard Hovy. 2013. Identifying
metaphorical word use with tree kernels. In Proceed-
ings of NAACL HLT, Meta4NLP Workshop.
Hisashi Kashima, Koji Tsuda, and Akihiro Inokuchi.
2003. Marginalized kernels between labeled graphs.
In Proceedings of the Twentieth International Con-
ference on Machine Learning, pages 321?328. AAAI
Press.
Prodromos Malakasiotis. 2009. Paraphrase recognition
using machine learning to combine similarity mea-
sures. In Proceedings of the ACL-IJCNLP 2009 Stu-
dent Research Workshop, ACLstudent ?09, pages 27?
35, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role labeling.
Computational Linguistics, 34(2):193?224.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow semantic parsing. In Proceedings
of the 42nd Annual Meeting on Association for Com-
putational Linguistics, pages 335?es. Association for
Computational Linguistics.
Alessandro Moschitti. 2006a. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Machine Learning: ECML 2006, pages 318?329.
Springer.
Alessandro Moschitti. 2006b. Making Tree Kernels
Practical for Natural Language Learning. In In Pro-
ceedings of the 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of the ACL.
Longhua Qian, Guodong Zhou, Fang Kong, Qiaoming
Zhu, and Peide Qian. 2008. Exploiting constituent
dependencies for tree kernel-based semantic relation
extraction. In Proceedings of the 22nd International
Conference on Computational Linguistics-Volume 1,
pages 697?704. Association for Computational Lin-
guistics.
Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2006.
Paraphrase recognition via dissimilarity significance
classification. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?06, pages 18?26, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language Processing,
pages 142?149.
1415
Stephen Tratz and Eduard Hovy. 2011. A fast, accu-
rate, non-projective, semantically-enriched parser. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, EMNLP ?11, pages
1257?1268, Stroudsburg, PA, USA. Association for
Computational Linguistics.
S. V. N. Vishwanathan and Alexander J. Smola. 2003.
Fast kernels for string and tree matching. In Advances
In Neural Information Processing Systems 15, pages
569?576. MIT Press.
S. V. N. Vishwanathan, Nicol N. Schraudolph, Risi Kon-
dor, and Karsten M. Borgwardt. 2010. Graph kernels.
J. Mach. Learn. Res., 99:1201?1242, August.
Xinglong Wang, Jun?ichi Tsujii, and Sophia Ananiadou.
2010. Disambiguating the species of biomedical
named entities using natural language parsers. Bioin-
formatics, 26(5):661?667.
Yitao Zhang and Jon Patrick. 2005. Paraphrase identi-
fication by text canonicalization. In In Proceedings
of the Australasian Language Technology Workshop
2005.
1416
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 185?193,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
When Did that Happen? ? Linking Events and Relations to Timestamps
Dirk Hovy*, James Fan, Alfio Gliozzo, Siddharth Patwardhan and Chris Welty
IBM T. J. Watson Research Center
19 Skyline Drive
Hawthorne, NY 10532
dirkh@isi.edu, {fanj,gliozzo,siddharth,welty}@us.ibm.com
Abstract
We present work on linking events and flu-
ents (i.e., relations that hold for certain
periods of time) to temporal information
in text, which is an important enabler for
many applications such as timelines and
reasoning. Previous research has mainly
focused on temporal links for events, and
we extend that work to include fluents
as well, presenting a common methodol-
ogy for linking both events and relations
to timestamps within the same sentence.
Our approach combines tree kernels with
classical feature-based learning to exploit
context and achieves competitive F1-scores
on event-time linking, and comparable F1-
scores for fluents. Our best systems achieve
F1-scores of 0.76 on events and 0.72 on flu-
ents.
1 Introduction
It is a long-standing goal of NLP to process natu-
ral language content in such a way that machines
can effectively reason over the entities, relations,
and events discussed within that content. The ap-
plications of such technology are numerous, in-
cluding intelligence gathering, business analytics,
healthcare, education, etc. Indeed, the promise
of machine reading is actively driving research in
this area (Etzioni et al 2007; Barker et al 2007;
Clark and Harrison, 2010; Strassel et al 2010).
Temporal information is a crucial aspect of this
task. For a machine to successfully understand
natural language text, it must be able to associate
time points and temporal durations with relations
and events it discovers in text.
?The first author conducted this research during an in-
ternship at IBM Research.
In this paper we present methods to estab-
lish links between events (e.g. ?bombing? or
?election?) or fluents (e.g. ?spouseOf? or ?em-
ployedBy?) and temporal expressions (e.g. ?last
Tuesday? and ?November 2008?). While previ-
ous research has mainly focused on temporal links
for events only, we deal with both events and flu-
ents with the same method. For example, consider
the sentence below
Before his death in October, Steve Jobs
led Apple for 15 years.
For a machine reading system processing this
sentence, we would expect it to link the fluent
CEO of (Steve Jobs, Apple) to time duration ?15
years?. Similarly we expect it to link the event
?death? to the time expression ?October?.
We do not take a strong ?ontological? position
on what events and fluents are, as part of our
task these distinctions are made a priori. In other
words, events and fluents are input to our tempo-
ral linking framework. In the remainder of this pa-
per, we also do not make a strong distinction be-
tween relations in general and fluents in particu-
lar, and use them interchangeably, since our focus
is only on the specific types of relations that rep-
resent fluents. While we only use binary relations
in this work, there is nothing in the framework
that would prevent the use of n-ary relations. Our
work focuses on accurately identifying temporal
links for eventual use in a machine reading con-
text.
In this paper, we describe a single approach that
applies to both fluents and events, using feature
engineering as well as tree kernels. We show that
we can achieve good results for both events and
fluents using the same feature space, and advocate
185
the versatility of our approach by achieving com-
petitive results on yet another similar task with a
different data set.
Our approach requires us to capture contextual
properties of text surrounding events, fluents and
time expressions that enable an automatic system
to detect temporal linking within our framework.
A common strategy for this is to follow standard
feature engineering methodology and manually
develop features for a machine learning model
from the lexical, syntactic and semantic analysis
of the text. A key contribution of our work in this
paper is to demonstrate a shallow tree-like repre-
sentation of the text that enables us to employ tree
kernel models, and more accurately detect tempo-
ral linking. The feature space represented by such
tree kernels is far larger than a manually engi-
neered feature space, and is capable of capturing
the contextual information required for temporal
linking.
The remainder of this paper goes into the de-
tails of our approach for temporal linking, and
presents empirical evidence for the effectiveness
of our approach. The contributions of this paper
can be summarized as follows:
1. We define a common methodology to link
events and fluents to timestamps.
2. We use tree kernels in combination with clas-
sical feature-based approaches to obtain sig-
nificant gains by exploiting context.
3. Empirical evidence illustrates that our
framework for temporal linking is very ef-
fective for the task, achieving an F1-score of
0.76 on events and 0.72 on fluents/relations,
as well as 0.65 for TempEval2, approaching
state-of-the-art.
2 Related Work
Most of the previous work on relation extraction
focuses on entity-entity relations, such as in the
ACE (Doddington et al 2004) tasks. Temporal
relations are part of this, but to a lesser extent.
The primary research effort in event temporality
has gone into ordering events with respect to one
another (e.g., Chambers and Jurafsky (2008)), and
detecting their typical durations (e.g., Pan et al
(2006)).
Recently, TempEval workshops have focused
on the temporal related issues in NLP. Some of
the TempEval tasks overlap with ours in many
ways. Our task is similar to task A and C of
TempEval-1 (Verhagen et al 2007) in the sense
that we attempt to identify temporal relation be-
tween events and time expressions or document
dates. However, we do not use a restricted set of
events, but focus primarily on a single temporal
relation tlink instead of named relations like BE-
FORE, AFTER or OVERLAP (although we show
that we can incorporate these as well). Part of our
task is similar to task C of TempEval-2 (Verha-
gen et al 2010), determining the temporal rela-
tion between an event and a time expression in
the same sentence. In this paper, we do apply our
system to TempEval-2 data and compare our per-
formance to the participating systems.
Our work is similar to that of Boguraev and
Ando (2005), whose research only deals with
temporal links between events and time expres-
sions (and does not consider relations at all). They
employ a sequence tagging model with manual
feature engineering for the task and achieved
state-of-the-art results on Timebank (Pustejovsky
et al 2003) data. Our task is slightly different be-
cause we include relations in the temporal linking,
and our use of tree kernels enables us to explore a
wider feature space very quickly.
Filatova and Hovy (2001) also explore tempo-
ral linking with events, but do not assume that
events and time stamps have been provided by an
external process. They used a heuristics-based ap-
proach to assign temporal expressions to events
(also relying on the proximity as a base case).
They report accuracy of the assignment for the
correctly classified events, the best being 82.29%.
Our best event system achieves an accuracy of
84.83%. These numbers are difficult to compare,
however, since accuracy does not efficiently cap-
ture the performance of a system on a task with so
many negative examples.
Mirroshandel et al(2011) describe the use of
syntactic tree kernels for event-time links. Their
results on TempEval are comparable to ours. In
contrast to them, we found, though, that syntactic
tree kernels alone do not perform as well as using
several flat tree representations.
3 Problem Definition
The task of linking events and relations to time
stamps can be defined as the following: given a set
of expressions denoting events or relation men-
186
tions in a document, and a set of time expressions
in the same document, find all instances of the
tlink relation between elements of the two input
sets. The existence of a tlink(e, t) means that e,
which is an event or a relation mention, occurs
within the temporal context specified by the time
expression t.
Thus, our task can be cast as a binary rela-
tion classification task: for each possible pair
of (event/relation, time) in a document, decide
whether there exists a link between the two, and
if so, express it in the data.
In addition, we make these assumptions about
the data:
1. There does not exist a timestamp for ev-
ery event/relation in a document. Although
events and relations typically have temporal
context, it may not be explicitly stated in a
document.
2. Every event/relation has at most one time ex-
pression associated with it. This is a simpli-
fying assumption, which in the case of rela-
tions we explore as future work.
3. Each temporal expression can be linked to
one or more events or relations. Since mul-
tiple events or relations may happen for a
given time, it is safe to assume that each tem-
poral expression can be linked to more than
one event/relation.
In general, the events/relations and their associ-
ated timestamps may occur within the same sen-
tence or may occur across different sentences. In
this paper, we focus on our effort and our evalua-
tion on the same sentence linking task.
In order to solve the problem of temporal link-
ing completely, however, it will be important to
also address the links that hold between entities
across sentences. We estimate, based on our data
set, that across sentence links account for 41% of
all correct event-time pairs in a document. For flu-
ents, the ratio is much higher, more than 80% of
the correct fluent-time links are across sentences.
One of the main obstacles for our approach in the
cross-sentence case is the very low ratio of posi-
tive to negative instances (3 : 100) in the set of all
pairs in a document. Most pairs are not linked to
one another.
4 Temporal Linking Framework
As previously mentioned, we approach the tem-
poral linking problem as a classification task. In
the framework of classification, we refer to each
pair of (event/relation, temporal expression) oc-
curring within a sentence as an instance. The goal
is to devise a classifier that separates positive (i.e.,
linked) instances from negative ones, i.e., pairs
where there is no link between the event/relation
and the temporal expression in question. The lat-
ter case is far more frequent, so we have an inher-
ent bias toward negative examples in our data.1
Note that the basis of the positive and nega-
tive links is the context around the target terms.
It is impossible even for humans to determine the
existence of a link based only on the two terms
without their context. For instance, given just two
words (e.g., ?said? and ?yesterday?) there is no
way to tell if it is a positive or a negative example.
We need the context to decide.
Therefore, we base our classification models on
contextual features drawn from lexical and syn-
tactic analyses of the text surrounding the target
terms. For this, we first define a feature-based
approach, then we improve it by using tree ker-
nels. These two subsections, plus the treatment
of fluent relations, are the main contributions of
this paper. In all of this work, we employ SVM
classifiers (Vapnik, 1995) for machine learning.
4.1 Feature Engineering
A manual analysis of development data provided
several intuitions about the kinds of features that
would be useful in this task. Based on this anal-
ysis and with inspiration from previous work (cf.
Boguraev and Ando (2005)) we established three
categories of features whose description follows.
Features describing events or relations. We
check whether the event or relation is phrasal, a
verb, or noun, whether it is present tense, past
tense, or progressive, the type assigned to the
event/relation by the UIMA type system used for
processing, and whether it includes certain trig-
ger words, such as reporting verbs (?said?, ?re-
ported?, etc.).
1Initially, we employed an instance filtering method to
address this, which proved to be ineffective and was subse-
quently left out.
187
Features describing temporal expressions.
We check for the presence of certain trigger words
(last, next, old, numbers, etc.) and the type of
the expression (DURATION, TIME, or DATE) as
specified by the UIMA type system.
Features describing context. We also in-
clude syntactic/structural features, such as testing
whether the relation/event dominates the temporal
expression, which one comes first in the sentence
order, and whether either of them is dominated
by a separate verb, preposition, ?that? (which of-
ten indicates a subordinate sentence) or counter-
factual nouns or verbs (which would negate the
temporal link).
It is not surprising that some of the most in-
formative features (event comes before tempo-
ral expression, time is syntactic child of event)
are strongly correlated with the baselines. Less
salient features include the test for certain words
indicating the event is a noun, a verb, and if so
which tense it has and whether it is a reporting
verb.
4.2 Tree Kernel Engineering
We expect that there exist certain patterns be-
tween the entities of a temporal link, which mani-
fest on several levels: some on the lexical level,
others expressed by certain sequences of POS
tags, NE labels, or other representations. Kernels
provide a principled way of expanding the number
of dimensions in which we search for a decision
boundary, and allow us to easily model local se-
quences and patterns in a natural way (Giuliano et
al., 2009). While it is possible to define a space
in which we find a decision boundary that sepa-
rates positive and negative instances with manu-
ally engineered features, these features can hardly
capture the notion of context as well as those ex-
plored by a tree kernel.
Tree Kernels are a family of kernel functions
developed to compute the similarity between tree
structures by counting the number of subtrees
they have in common. This generates a high-
dimensional feature space that can be handled ef-
ficiently using dynamic programming techniques
(Shawe-Taylor and Christianini, 2004). For our
purposes we used an implementation of the Sub-
tree and Subset Tree (SST) (Moschitti, 2006).
The advantages of using tree kernels are
two-fold: thanks to an existing implementation
(SVMlight with tree kernels, Moschitti (2004)), it
is faster and easier than traditional feature engi-
neering. The tree structure also allows us to use
different levels of representations (POS, lemma,
etc.) and combine their contributions, while at the
same time taking into account the ordering of la-
bels. We use POS, lemma, semantic type, and a
representation that replaces each word with a con-
catenation of its features (capitalization, count-
able, abstract/concrete noun, etc.).
We developed a shallow tree representation that
captures the context of the target terms, without
encoding too much structure (which may prevent
generalization). In essence, our tree structure in-
duces behavior somewhat similar to a string ker-
nel. In addition, we can model the tasks by pro-
viding specific markup on the generated tree. For
example, in our experiment we used the labels
EVENT (or equivalently RELATION) and TIME-
STAMP to mark our target terms. In order to re-
duce the complexity of this comparison, we focus
on the substring between event/relation and time
stamp and the rest of the tree structure is trun-
cated.
Figure 1 illustrates an example of the structure
described so far for both lemmas and POS tags
(note that the lowest level of the tree contains tok-
enized items, so their number can differ form the
actual words, as in ?attorney general?). Similar
trees are produced for each level of representa-
tions used, and for each instance (i.e., pair of time
expressions and event/relation). If a sentence con-
tains more than one event/relation, we create sep-
arate trees for each of them, which differ in the po-
sition of the EVENT/RELATION marks (at level
1 of the tree).
The tree kernel implicitly expands this struc-
ture into a number of substructures allowing us
to capture sequential patterns in the data. As we
will see, this step provides significant boosts to
the task performance.
Curiously, using a full-parse syntactic tree as
input representation did not help performance.
This is in line with our finding that syntactic re-
lations are less important than sequential patterns
(see also Section 5.2). Therefore we adopted the
?string kernel like? representation illustrated in
Figure 1.
188
Scores of supporters of detained Egyptian opposition leader Nur demonstrated outside the attorney general?s
office in Cairo last Saturday, demanding he be freed immediately.
BOW
TIME
TOK
saturday
TOK
last
TERM
TOK
cairo
TERM
TOK
in
TERM
TOK
office
TERM
TOK
attorney general
TERM
TOK
outside
EVENT
TOK
demonstrate
BOP
TIME
TOK
NNP
TOK
JJ
TERM
TOK
NNP
TERM
TOK
IN
TERM
TOK
NN
TERM
TOK
NNP
TERM
TOK
ADV
EVENT
TOK
VBD
Figure 1: Input Sentence and Tree Kernel Representations for Bag of Words (BOW) and POS tags (BOP)
5 Evaluation
We now apply our models to real world data, and
empirically demonstrate their effectiveness at the
task of temporal linking. In this section, we de-
scribe the data sets that were used for evaluation,
the baselines for comparison, parameter settings,
and the results of the experiments.
5.1 Benchmark
We evaluated our approach in 3 different tasks:
1. Linking Timestamps and Events in the IC
domain
2. Linking Timestamps and Relations in the IC
domain
3. Linking Events to Temporal Expressions
(TempEval-2, task C)
The first two data sets contained annotations
in the intelligence community (IC) domain, i.e.,
mainly news reports about terrorism. It com-
prised 169 documents. This dataset has been de-
veloped in the context of the machine reading pro-
gram (MRP) (Strassel et al 2010). In both cases
our goal is to develop a binary classifier to judge
whether the event (or relation) overlaps with the
time interval denoted by the timestamp. Success
of this classification can be measured by precision
and recall on annotated data.
We originally considered using accuracy as a
measure of performance, but this does not cor-
rectly reflect the true performance of the system:
given the skewed nature of the data (much smaller
number of positive examples), we could achieve a
high accuracy simply by classifying all instances
as negative, i.e., not assigning a time stamp at all.
We thus decided to report precision, recall and F1.
Unless stated otherwise, results were achieved via
10-fold cross-validation (10-CV).
The number of instances (i.e., pairs of event
and temporal expression) for each of the differ-
ent cases listed above was (in brackets the ratio of
positive to negative instances).
? events: 2046 (505 positive, 1541 negative)
? relations: 6526 (1847 positive, 4679 nega-
tive)
The size of the relation data set after filtering is
5511 (1847 positive, 3395 negative).
In order to increase the originally lower number
of event instances, we made use of the annotated
event-coreference as a sort of closure to add more
instances: if events A and B corefer, and there
is a link between A and time expression t, then
there is also a link between B and t. This was not
explicitly expressed in the data.
For the task at hand, we used gold standard
annotations for timestamps, events and relations.
The task was thus not the identification of these
objects (a necessary precursor and a difficult task
in itself), but the decision as to which events and
time expressions could and should be linked.
We also evaluated our system on TempEval-
2 (Verhagen et al 2010) for better comparison
189
to the state-of-the-art. TempEval-2 data included
the task of linking events to temporal expressions
(there called ?task C?), using several link types
(OVERLAP, BEFORE, AFTER, BEFORE-OR-
OVERLAP, OVERLAP-OR-AFTER). This is a
bit different from our settings as it required the
implementation of a multi-class classifier. There-
fore we trained three different binary classifiers
(using the same feature set) for the first three of
those types (for which there was sufficient train-
ing data) and we used a one-versus-all strategy to
distinguish positive from negative examples. The
output of the system is the category with the high-
est SVM decision score. Since we only use three
labels, we incur an error every time the gold la-
bel is something else. Note that this is stricter
than the evaluation in the actual task, which left
contestants with the option of skipping examples
their systems could not classify.
5.2 Baselines
Intuitively, one would expect temporal expres-
sions to be close to the event they denote, or even
syntactically related. In order to test this, we ap-
plied two baselines. In the first, each temporal ex-
pression was linked to the closest event (as mea-
sured in token distance). In the second, we at-
tached each temporal expression to its syntactic
head, if the head was an event. Results are re-
ported in Figure 2.
While these results are encouraging for our
task, it seems at first counter-intuitive that the
syntactic baseline does worse than the proximity-
based one. It does, however, reveal two facts:
events are not always synonymous with syntactic
units, and they are not always bound to tempo-
ral expressions through direct syntactic links. The
latter makes even more sense given that the links
can even occur across sentence boundaries. Pars-
ing quality could play a role, yet seems far fetched
to account for the difference.
More important than syntactic relations seem
to be sequential patterns on different levels, a fact
we exploit with the different tree representations
used (POS tags, NE types, etc.).
For relations, we only applied the closest-
relation baseline. Since relations consist of two or
more arguments that occur in different, often sep-
arated syntactic constituents, a syntactic approach
seems futile, especially given our experience with
events. Results are reported in Figure 3.
baseline comparison
Page 1
Precision Recall F1
0
20
40
60
80
100
35.0
63.0
45.048.0
88.0
62.063.0
75.4 68.376.6 76.5 76.2
Evaluation Measures Events
BL-parent BL-closest features +tree kernel
metric
%
Figure 2: Performance on events
System Accuracy
TRIOS 65%
this work 64.5%
JU-CSE, NCSU-indi
TRIPS, USFD2
all 63%
Table 1: Comparison to Best Systems in TempEval-2
5.3 Events
Figure 2 shows the improvements of the feature-
based approach over the two baseline, and the ad-
ditional gain obtained by using the tree kernel.
Both the features and tree kernels mainly improve
precision, while the tree kernel adds a small boost
in recall. It is remarkable, though, that the closest-
event baseline has a very high recall value. This
suggests that most of the links actually do occur
between items that are close to one another. For a
possible explanation for the low precision value,
see the error analysis (Section 5.5).
Using a two-tailed t-test, we compute the sig-
nificance in the difference between the F1-scores.
Both the feature-based and the tree kernel ap-
proach improvements are statistically significant
at p < 0.001 over the baseline scores.
Table 1 compares the performances of our sys-
tem to the state-of-the-art systems on TempEval-2
Data, task C, showing that our approach is very
competitive. The best systems there used sequen-
tial models. We attribute the competitive nature
of our results to the use of tree kernels, which en-
ables us to make use of contextual information.
5.4 Relations
In general, performance for relations is not as high
as for events (see Figure 3). The reason here is
two-fold: relations consist of two (or more) ele-
ments, which can be in various positions with re-
spect to one another and the temporal expression,
and each relation can be expressed in a number of
190
baseline comparison
Page 1
Precision Recall F1
0
10
20
30
40
50
60
70
80
90
100
35.0
24.0 29.0
63.1
80.6
70.470.8 74.0 72.2
Evaluation Metric Relations
BL-closest features +tree kernel
metric
%
Figure 3: Performance on relations/fluents
learning curves
Page 1
0 10 20 30 40 50 60 70 80 90 100
40
45
50
55
60
65
70
75
80
Learning Curves Relations
features w/ tree 
kernel
% of data
F1
 sc
or
e
Figure 4: Learning curves for relation models
different ways.
Again, we perform significance tests on the dif-
ference in F1 scores and find that our improve-
ments over the baseline are statistically significant
at p < 0.001. The improvement of the tree kernel
over the feature-based approach, however, are not
statistically significant at the same value.
The learning curve over parts of the training
data (exemplary shown here for relations, Figure
4)2 indicates that there is another advantage to us-
ing tree kernels: the approach can benefit from
more data. This is conceivably because it allows
the kernel to find more common subtrees in the
various representations the more examples it gets,
while the feature space rather finds more instances
that invalidate the expressiveness of features (i.e.,
it encounters positive and negative instances that
have very similar feature vectors). The curve sug-
gests that tree kernels could yield even better re-
sults with more data, while there is little to no ex-
pected gain using only features.
5.5 Error Analysis
Examining the misclassified examples in our data,
we find that both feature-based and tree-kernel
approaches struggle to correctly classify exam-
2The learning curve for events looks similar and is omit-
ted due to space constraints.
ples where time expression and event/relation are
immediately adjacent, but unrelated, as in ?the
man arrested last Tuesday told the police ...?,
where last Tuesday modifies arrested. It limits
the amount of context that is available to the tree
kernels, since we truncate the tree representations
to the words between those two elements. This
case closely resembles the problem we see in the
closest-event/relation baseline, which, as we have
seen, does not perform too well. In this case, the
incorrect event (?told?) is as close to the time ex-
pression as the correct one (?arrested?), resulting
in a false positive that affects precision. Features
capturing the order of the elements do not seem
help here, since the elements can be arranged in
any order (i.e., temporal expression before or af-
ter the event/relation). The only way to solve this
problem would be to include additional informa-
tion about whether a time expression is already
attached to another event/relation.
5.6 Ablations
To quantify the utility of each tree representation,
we also performed all-but-one ablation tests, i.e.,
left out each of the tree representations in turn, ran
10-fold cross-validation on the data and observed
the effect on F1. The larger the loss in F1, the
more informative the left-out-representation. We
performed ablations for both events and relations,
and found that the ranking of the representations
is the same for both.
In events and relations alike, leaving out POS
trees has the greatest effect on F1, followed by
the feature-bundle representation. Lemma and se-
mantic type representation have less of an impact.
We hypothesize that the former two capture un-
derlying regularities better by representing differ-
ent words with the same label. Lemmas in turn
are too numerous to form many recurring pat-
terns, and semantic type, while having a smaller
label alphabet, does not assign a label to every
word, thus creating a very sparse representation
that picks up more noise than signal.
In preliminary tests, we also used annotated
dependency trees as input to the tree kernel, but
found that performance improved when they were
left out. This is at odds with work that clearly
showed the value of syntactic tree kernels (Mir-
roshandel et al 2011). We identify two poten-
tial causes?either our setup was not capable of
correctly capturing and exploiting the information
191
from the dependency trees, or our formulation of
the task was not amenable to it. We did not inves-
tigate this further, but leave it to future work.
6 Conclusion and Future Work
We cast the problem of linking events and rela-
tions to temporal expressions as a classification
task using a combination of features and tree ker-
nels, with probabilistic type filtering. Our main
contributions are:
? We showed that within-sentence temporal
links for both events and relations can be ap-
proached with a common strategy.
? We developed flat tree representations and
showed that these produce considerable
gains, with significant improvements over
different baselines.
? We applied our technique without great ad-
justments to an existing data set and achieved
competitive results.
? Our best systems achieve F1 score of 0.76
on events and 0.72 on relations, and are ef-
fective at the task of temporal linking.
We developed the models as part of a machine
reading system and are currently evaluating it in
an end-to-end task.
Following tasks proposed in TempEval-2, we
plan to use our approach for across-sentence clas-
sification, as well as a similar model for linking
entities to the document creation date.
Acknowledgements
We would like to thank Alessandro Moschitti for
his help with the tree kernel setup, and the review-
ers who supplied us with very constructive feed-
back. Research supported in part by Air Force
Contract FA8750-09-C-0172 under the DARPA
Machine Reading Program.
References
Ken Barker, Bhalchandra Agashe, Shaw-Yi Chaw,
James Fan, Noah Friedland, Michael Glass, Jerry
Hobbs, Eduard Hovy, David Israel, Doo Soon Kim,
Rutu Mulkar-Mehta, Sourabh Patwardhan, Bruce
Porter, Dan Tecuci, and Peter Yeh. 2007. Learn-
ing by reading: A prototype system, performance
baseline and lessons learned. In Proceedings of
the 22nd National Conference for Artificial Intelli-
gence, Vancouver, Canada, July.
Branimir Boguraev and Rie Kubota Ando. 2005.
Timeml-compliant text analysis for temporal rea-
soning. In Proceedings of IJCAI, volume 5, pages
997?1003. IJCAI.
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. pages
789?797. Association for Computational Linguis-
tics.
Peter Clark and Phil Harrison. 2010. Machine read-
ing as a process of partial question-answering. In
Proceedings of the NAACL HLT Workshop on For-
malisms and Methodology for Learning by Reading,
Los Angeles, CA, June.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extrac-
tion program ? tasks, data and evaluation. In Pro-
ceedings of the LREC Conference, Canary Islands,
Spain, July.
Oren Etzioni, Michele Banko, and Michael Cafarella.
2007. Machine reading. In Proceedings of the
AAAI Spring Symposium Series, Stanford, CA,
March.
Elena Filatova and Eduard Hovy. 2001. Assigning
time-stamps to event-clauses. In Proceedings of
the workshop on Temporal and spatial information
processing, volume 13, pages 1?8. Association for
Computational Linguistics.
Claudio Giuliano, Alfio Massimiliano Gliozzo, and
Carlo Strapparava. 2009. Kernel methods for min-
imally supervised wsd. Computational Linguistics,
35(4).
Seyed A. Mirroshandel, Mahdy Khayyamian, and
Gholamreza Ghassem-Sani. 2011. Syntactic tree
kernels for event-time temporal relation learning.
Human Language Technology. Challenges for Com-
puter Science and Linguistics, pages 213?223.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow semantic parsing. In Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, pages 335?es. Associa-
tion for Computational Linguistics.
Alessandro Moschitti. 2006. Making tree kernels
practical for natural language learning. In Proceed-
ings of EACL, volume 6.
Feng Pan, Rutu Mulkar, and Jerry R. Hobbs. 2006.
Learning event durations from event descriptions.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 393?400. Association for Computa-
tional Linguistics.
James Pustejovsky, Patrick Hanks, Roser Saur?, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
192
Ferro, and Marcia Lazo. 2003. The TIMEBANK
Corpus. In Proceedings of Corpus Linguistics
2003, pages 647?656.
John Shawe-Taylor and Nello Christianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press.
Stephanie Strassel, Dan Adams, Henry Goldberg,
Jonathan Herr, Ron Keesing, Daniel Oblinger,
Heather Simpson, Robert Schrag, and Jonathan
Wright. 2010. The DARPA Machine Read-
ing Program-Encouraging Linguistic and Reason-
ing Research with a Series of Reading Tasks. In
Proceedings of LREC 2010.
Vladimir Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer, New York, NY.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Puste-
jovsky. 2007. Semeval-2007 task 15: Tempeval
temporal relation identification. In Proceedings of
the 4th International Workshop on Semantic Evalu-
ations, pages 75?80. Association for Computational
Linguistics.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 57?
62. Association for Computational Linguistics.
193
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 742?751,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Learning part-of-speech taggers with inter-annotator agreement loss
Barbara Plank, Dirk Hovy, Anders S?gaard
Center for Language Technology
University of Copenhagen, Denmark
Njalsgade 140, DK-2300 Copenhagen S
bplank@cst.dk,dirk@cst.dk,soegaard@hum.ku.dk
Abstract
In natural language processing (NLP) an-
notation projects, we use inter-annotator
agreement measures and annotation guide-
lines to ensure consistent annotations.
However, annotation guidelines often
make linguistically debatable and even
somewhat arbitrary decisions, and inter-
annotator agreement is often less than
perfect. While annotation projects usu-
ally specify how to deal with linguisti-
cally debatable phenomena, annotator dis-
agreements typically still stem from these
?hard? cases. This indicates that some er-
rors are more debatable than others. In this
paper, we use small samples of doubly-
annotated part-of-speech (POS) data for
Twitter to estimate annotation reliability
and show how those metrics of likely inter-
annotator agreement can be implemented
in the loss functions of POS taggers. We
find that these cost-sensitive algorithms
perform better across annotation projects
and, more surprisingly, even on data an-
notated according to the same guidelines.
Finally, we show that POS tagging mod-
els sensitive to inter-annotator agreement
perform better on the downstream task of
chunking.
1 Introduction
POS-annotated corpora and treebanks are collec-
tions of sentences analyzed by linguists accord-
ing to some linguistic theory. The specific choice
of linguistic theory has dramatic effects on down-
stream performance in NLP tasks that rely on syn-
tactic features (Elming et al., 2013). Variation
across annotated corpora in linguistic theory also
poses challenges to intrinsic evaluation (Schwartz
et al., 2011; Tsarfaty et al., 2012), as well as
for languages where available resources are mu-
tually inconsistent (Johansson, 2013). Unfortu-
nately, there is no grand unifying linguistic the-
ory of how to analyze the structure of sentences.
While linguists agree on certain things, there is
still a wide range of unresolved questions. Con-
sider the following sentence:
(1) @GaryMurphyDCU of @DemMattersIRL
will take part in a panel discussion on Octo-
ber 10th re the aftermath of #seanref . . .
While linguists will agree that in is a preposi-
tion, and panel discussion a compound noun, they
are likely to disagree whether will is heading the
main verb take or vice versa. Even at a more basic
level of analysis, it is not completely clear how to
assign POS tags to each word in this sentence: is
part a particle or a noun; is 10th a numeral or a
noun?
Some linguistic controversies may be resolved
by changing the vocabulary of linguistic theory,
e.g., by leaving out numerals or introducing ad
hoc parts of speech, e.g. for English to (Marcus
et al., 1993) or words ending in -ing (Manning,
2011). However, standardized label sets have
practical advantages in NLP (Zeman and Resnik,
2008; Zeman, 2010; Das and Petrov, 2011; Petrov
et al., 2012; McDonald et al., 2013).
For these and other reasons, our annotators
(even when they are trained linguists) often dis-
agree on how to analyze sentences. The strategy
in most previous work in NLP has been to monitor
and later resolve disagreements, so that the final
labels are assumed to be reliable when used as in-
put to machine learning models.
Our approach
Instead of glossing over those annotation disagree-
ments, we consider what happens if we embrace
the uncertainty exhibited by human annotators
742
when learning predictive models from the anno-
tated data.
To achieve this, we incorporate the uncertainty
exhibited by annotators in the training of our
model. We measure inter-annotator agreement on
small samples of data, then incorporate this in the
loss function of a structured learner to reflect the
confidence we can put in the annotations. This
provides us with cost-sensitive online learning al-
gorithms for inducing models from annotated data
that take inter-annotator agreement into consider-
ation.
Specifically, we use online structured percep-
tron with drop-out, which has previously been ap-
plied to POS tagging and is known to be robust
across samples and domains (S?gaard, 2013a). We
incorporate the inter-annotator agreement in the
loss function either as inter-annotator F1-scores
or as the confusion probability between annota-
tors (see Section 3 below for a more detailed de-
scription). We use a small amounts of doubly-
annotated Twitter data to estimate F1-scores and
confusion probabilities, and incorporate them dur-
ing training via a modified loss function. Specif-
ically, we use POS annotations made by two an-
notators on a set of 500 newly sampled tweets
to estimate our agreement scores, and train mod-
els on existing Twitter data sets (described be-
low). We evaluate the effect of our modified
training by measuring intrinsic as well as down-
stream performance of the resulting models on two
tasks, namely named entity recognition (NER) and
chunking, which both use POS tags as input fea-
tures.
2 POS-annotated Twitter data sets
The vast majority of POS-annotated resources
across languages contain mostly newswire text.
Some annotated Twitter data sets do exist for En-
glish. Ritter et al. (2011) present a manually an-
notated data set of 16 thousand tokens. They
do not report inter-annotator agreement. Gimpel
et al. (2011) annotated about 26 thousand tokens
and report a raw agreement of 92%. Foster et
al. (2011) annotated smaller portions of data for
cross-domain evaluation purposes. We refer to the
data as RITTER, GIMPEL and FOSTER below.
In our experiments, we use the RITTER splits
provided by Derczynski et al. (2013), and the
October splits of the GIMPEL data set, version
0.3. We train our models on the concatenation of
RITTER-TRAIN and GIMPEL-TRAIN and evaluate
them on the remaining data, the dev and test set
provided by Foster et al. (2011) as well as an in-
house annotated data set of 3k tokens (see below).
The three annotation efforts (Ritter et al., 2011;
Gimpel et al., 2011; Foster et al., 2011) all used
different tagsets, however, and they also differ in
tokenization, as well as a wide range of linguistic
decisions. We mapped all the three corpora to the
universal tagset provided by Petrov et al. (2012)
and used the same dummy symbols for numbers,
URLs, etc., in all the data sets. Following (Fos-
ter et al., 2011), we consider URLs, usernames
and hashtags as NOUN. We did not change the tok-
enization.
The data sets differ in how they analyze many of
the linguistically hard cases. Consider, for exam-
ple, the analysis of will you come out to in GIM-
PEL and RITTER (Figure 1, top). While Gimpel
et al. (2011) tag out and to as adpositions, Ritter
et al. (2011) consider them particles. What is the
right analysis depends on the compositionality of
the construction and the linguistic theory one sub-
scribes to.
Other differences include the analysis of abbre-
viations (PRT in GIMPEL; X in RITTER and FOS-
TER), colon (X in GIMPEL; punctuation in RIT-
TER and FOSTER), and emoticons, which can take
multiple parts of speech in GIMPEL, but are al-
ways X in RITTER, while they are absent in FOS-
TER. GIMPEL-TRAIN and RITTER-TRAIN are
also internally inconsistent. See the bottom of Fig-
ure 1 for examples and Hovy et al. (2014) for a
more detailed discussion on differences between
the data sets.
Since the mapping to universal tags could
potentially introduce errors, we also annotated
a data set directly using universal tags. We
randomly selected 200 tweets collected over the
span of one day, and had three annotators tag
this set. We split the data in such a way that
each annotator had 100 tweets: two annotators
had disjoint sets, the third overlapped 50 items
with each of the two others. In this way, we
obtained an initial set of 100 doubly-annotated
tweets. The annotators were not provided with
annotation guidelines. After the first round of
annotations, we achieved a raw agreement of
0.9, a Cohen?s ? of 0.87, and a Krippendorff?s
? of 0.87. We did one pass over the data to
adjudicate the cases where annotators disagreed,
743
. . .
will you come out to the
. . .GIMPEL VERB PRON VERB ADP ADP DET
RITTER VERB PRON VERB PRT PRT DET
RITTER
. . .
you/PRON come/VERB out/PRT to/PRT
. . .
it/PRON comes/VERB out/ADP nov/NOUN
GIMPEL
. . .
Advances/NOUN and/CONJ Social/NOUN Media/NOUN .../X
. . .
Journalists/NOUN and/CONJ Social/ADJ Media/NOUN experts/NOUN
Figure 1: Annotation differences between (top) and within (bottom) two available Twitter POS data sets.
or where they had flagged their choice as debat-
able. The final data set (lowlands.test),
referred below to as INHOUSE, contained 3,064
tokens (200 tweets) and is publicly available
at http://bitbucket.org/lowlands/
costsensitive-data/, along with the data
used to compute inter-annotator agreement scores
for learning cost-sensitive taggers, described in
the next section.
3 Computing agreement scores
Gimpel et al. (2011) used 72 doubly-annotated
tweets to estimate inter-annotator agreement, and
we also use doubly-annotated data to compute
agreement scores. We randomly sampled 500
tweets for this purpose. Each tweet was anno-
tated by two annotators, again using the univer-
sal tag set (Petrov et al., 2012). All annotators
were encouraged to use their own best judgment
rather than following guidelines or discussing dif-
ficult cases with each other. This is in contrast to
Gimpel et al. (2011), who used annotation guide-
lines. The average inter-annotator agreement was
0.88 for raw agreement, and 0.84 for Cohen?s ?.
Gimpel et al. (2011) report a raw agreement of
0.92.
We use two metrics to provide a more detailed
picture of inter-annotator agreement, namely
F1-scores between annotators on individual parts
of speech, and tag confusion probabilities, which
we derive from confusion matrices.
The F1-score relates to precision and recall
in the usual way, i.e, as the harmonic mean
between those two measure. In more detail, given
two annotators A
1
and A
2
, we say the precision
Figure 2: Inter-annotator F1-scores estimated
from 500 tweets.
of A
1
relative to A
2
with respect to POS tag T in
some data setX , denoted Prec
T
(A
1
(X), A
2
(X)),
is the number of tokens both A
1
and A
2
predict to
be T over the number of times A
1
predicts a token
to be T . Similarly, we define the recall with re-
spect to some tag T , i.e., Rec
T
(A
1
(X), A
2
(X)),
as the number of tokens both A
1
and A
2
predict
to be T over the number of times A
2
predicts
a token to be T . The only difference with
respect to standard precision and recall is that
the gold standard is replaced by a second anno-
tator, A
2
. Note that Prec
T
(A
1
(X), A
2
(X)) =
Rec
T
(A
2
(X), A
1
(X)). It follows from all of
the above that the F1-score is symmetrical, i.e.,
F1
T
(A
1
(X), A
2
(X)) = F1
T
(A
2
(X), A
1
(X)).
The inter-annotator F1-scores over the 12
POS tags in the universal tagset are presented in
Figure 2. It shows that there is a high agreement
for nouns, verbs and punctuation, while the agree-
744
Figure 3: Confusion matrix of POS tags obtained
from 500 doubly-annotated tweets.
ment is low, for instance, for particles, numerals
and the X tag.
We compute tag confusion probabilities
from a confusion matrix over POS tags like
the one in Figure 3. From such a matrix,
we compute the probability of confusing
two tags t
1
and t
2
for some data point x,
i.e. P ({A
1
(x), A
2
(x)} = {t
1
, t
2
}) as the
mean of P (A
1
(x) = t
1
, A
2
(x) = t
2
) and
P (A
1
(x) = t
2
, A
2
(x) = t
1
), e.g., the confusion
probability of two tags is the mean of the prob-
ability that annotator A
1
assigns one tag and A
2
another, and vice versa.
We experiment with both agreement scores (F1
and confusion matrix probabilities) to augment the
loss function in our learner. The next section de-
scribes this modification in detail.
4 Inter-annotator agreement loss
We briefly introduce the cost-sensitive perceptron
classifier. Consider the weighted perceptron loss
on our ith example ?x
i
, y
i
? (with learning rate ? =
1), L
w
(?x
i
, y
i
?):
?(sign(w ? x
i
), y
i
) max(0,?y
i
w ? x
i
)
In a non-cost-sensitive classifier, the weight
function ?(y
j
, y
i
) = 1 for 1 ? i ? N . The
1: X = {?x
i
, y
i
?}
N
i=1
with x
i
= ?x
1
i
, . . . , x
m
i
?
2: I iterations
3: w = ?0?
m
4: for iter ? I do
5: for 1 ? i ? N do
6: y? = arg max
y?Y
w ? ?(x
i
, y)
7: w? w+ ?(y?, y
i
)[?(x
i
, y
i
)??(x
i
, y?)]
8: w
?
+ = w
9: end for
10: end for
11: return w
?
/ = (N ? I)
Figure 4: Cost-sensitive structured perceptron (see
Section 3 for weight functions ?).
two cost-sensitive systems proposed only differ in
how we formulate ?(?, ?). In one model, the loss is
weighted by the inter-annotator F1 of the gold tag
in question. This boils down to
?(y
j
, y
i
) = F1
y
i
(A
1
(X), A
2
(X))
where X is the small sample of held-out data used
to estimate inter-annotator agreement. Note that
in this formulation, the predicted label is not taken
into consideration.
The second model is slightly more expressive
and takes both the gold and predicted tags into ac-
count. It basically weights the loss by how likely
the gold and predicted tag are to be mistaken for
each other, i.e., (the inverse of) their confusion
probability:
?(y
j
, y
i
)) = 1?P ({A
1
(X), A
2
(X)} = {y
j
, y
i
})
In both loss functions, a lower gamma value
means that the tags are more likely to be confused
by a pair of annotators. In this case, the update is
smaller. In contrast, the learner incurs greater loss
when easy tags are confused.
It is straight-forward to extend these cost-
sensitive loss functions to the structured percep-
tron (Collins, 2002). In Figure 4, we provide the
pseudocode for the cost-sensitive structured online
learning algorithm. We refer to the cost-sensitive
structured learners as F1- and CM-weighted be-
low.
5 Experiments
In our main experiments, we use structured per-
ceptron (Collins, 2002) with random corruptions
745
using a drop-out rate of 0.1 for regularization, fol-
lowing S?gaard (2013a). We use the LXMLS
toolkit implementation
1
with default parameters.
We present learning curves across iterations, and
only set parameters using held-out data for our
downstream experiments.
2
5.1 Results
Our results are presented in Figure 5. The top left
graph plots accuracy on the training data per iter-
ation. We see that CM-weighting does not hurt
training data accuracy. The reason may be that
the cost-sensitive learner does not try (as hard) to
optimize performance on inconsistent annotations.
The next two plots (upper mid and upper right)
show accuracy over epochs on in-sample evalua-
tion data, i.e., GIMPEL-DEV and RITTER-TEST.
Again, the CM-weighted learner performs better
than our baseline model, while the F1-weighted
learner performs much worse.
The interesting results are the evaluations on
out-of-sample evaluation data sets (FOSTER and
IN-HOUSE) - lower part of Figure 5. Here, both
our learners are competitive, but overall it is clear
that the CM-weighted learner performs best. It
consistently improves over the baseline and F1-
weighting. The former is much more expressive
as it takes confusion probabilities into account and
does not only update based on gold-label uncer-
tainty, as is the case with the F1-weighted learner.
5.2 Robustness across regularizers
Discriminative learning typically benefits from
regularization to prevent overfitting. The simplest
is the averaged perceptron, but various other meth-
ods have been suggested in the literature.
We use structured perceptron with drop-out, but
results are relatively robust across other regular-
ization methods. Drop-out works by randomly
dropping a fraction of the active features in each
iteration, thus preventing overfitting. Table 1
shows the results for using different regularizers,
in particular, Zipfian corruptions (S?gaard, 2013b)
and averaging. While there are minor differences
across data sets and regularizers, we observe that
the corresponding cell using the loss function sug-
gested in this paper (CM) always performs better
than the baseline method.
1
https://github.com/gracaninja/
lxmls-toolkit/
2
In this case, we use FOSTER-DEV as our development
data to avoid in-sample bias.
6 Downstream evaluation
We have seen that our POS tagging model im-
proves over the baseline model on three out-of-
sample test sets. The question remains whether
training a POS tagger that takes inter-annotator
agreement scores into consideration is also effec-
tive on downstream tasks. Therefore, we eval-
uate our best model, the CM-weighted learner,
in two downstream tasks: shallow parsing?also
known as chunking?and named entity recogni-
tion (NER).
For the downstream evaluation, we used the
baseline and CM models trained over 13 epochs,
as they performed best on FOSTER-DEV (cf. Fig-
ure 5). Thus, parameters were optimized only on
POS tagging data, not on the downstream evalu-
ation tasks. We use a publicly available imple-
mentation of conditional random fields (Lafferty
et al., 2001)
3
for the chunking and NER exper-
iments, and provide the POS tags from our CM
learner as features.
6.1 Chunking
The set of features for chunking include informa-
tion from tokens and POS tags, following Sha and
Pereira (2003).
We train the chunker on Twitter data (Ritter et
al., 2011), more specifically, the 70/30 train/test
split provided by Derczynski et al. (2013) for POS
tagging, as the original authors performed cross
validation. We train on the 70% Twitter data (11k
tokens) and evaluate on the remaining 30%, as
well as on the test data from Foster et al. (2011).
The FOSTER data was originally annotated for
POS and constituency tree information. We con-
verted it to chunks using publicly available conver-
sion software.
4
Part-of-speech tags are the ones
assigned by our cost-sensitive (CM) POS model
trained on Twitter data, the concatenation of Gim-
pel and 70% Ritter training data. We did not in-
clude the CoNLL 2000 training data (newswire
text), since adding it did not substantially improve
chunking performance on tweets, as also shown
in (Ritter et al., 2011).
The results for chunking are given in Ta-
ble 2. They show that using the POS tagging
model (CM) trained to be more sensitive to inter-
annotator agreement improves performance over
3
http://crfpp.googlecode.com
4
http://ilk.uvt.nl/team/sabine/
homepage/software.html
746
5 10 15 20 25
Epochs
74
75
76
77
78
79
80
81
82
Ac
cu
rac
y(
%)
TRAINING
BASELINE
F1
CM
5 10 15 20 25
Epochs
77.5
78.0
78.5
79.0
79.5
80.0
80.5
Ac
cu
rac
y(
%)
GIMPEL-DEV
BASELINE
F1
CM
5 10 15 20 25
Epochs
83.5
84.0
84.5
85.0
85.5
86.0
86.5
87.0
Ac
cu
rac
y(
%)
RITTER-TEST
BASELINE
F1
CM
5 10 15 20 25
Epochs
81.0
81.5
82.0
82.5
83.0
83.5
84.0
Ac
cu
rac
y(
%)
FOSTER-DEV
BASELINE
F1
CM
5 10 15 20 25
Epochs
82.5
83.0
83.5
84.0
84.5
85.0
Ac
cu
rac
y(
%)
FOSTER-TEST
BASELINE
F1
CM
5 10 15 20 25
Epochs
82.2
82.4
82.6
82.8
83.0
83.2
83.4
83.6
83.8
84.0
Ac
cu
rac
y(
%)
IN-HOUSE
BASELINE
F1
CM
Figure 5: POS accuracy for the three models: baseline, confusion matrix loss (CM) and F1-weighted
(F1) loss for increased number of training epochs. Top row: in-sample accuracy on training (left) and
in-sample evaluation datasets (center, right). Bottom row: out-of-sample accuracy on various data sets.
CM is robust on both in-sample and out-of-sample data.
RITTER-TEST
F1: All NP VP PP
BL 76.20 78.61 74.25 86.79
CM 76.42 79.07 74.98 86.19
FOSTER-TEST
F1: All NP VP PP
BL 68.49 70.73 60.56 86.50
CM 68.97 71.25 61.97 87.24
Table 2: Downstream results on chunking. Overall
F1 score (All) as well as F1 for NP, VP and PP.
the baseline (BL) for the downstream task of
chunking. Overall chunking F1 score improves.
More importantly, we report on individual scores
for NP, VP and PP chunks, where we see consis-
tent improvements for NPs and VPs (since both
nouns and verbs have high inter-annotator agree-
ment), while results on PP are mixed. This is to
be expected, since PP phrases involve adposition-
als (ADP) that are often confused with particles
(PRT), cf. Figure 3. Our tagger has been trained
to deliberately abstract away from such uncertain
cases. The results show that taking uncertainty in
POS annotations into consideration during train-
ing has a positive effect in downstream results. It
is thus better if we do not try to urge our models
to make a firm decision on phenomena that neither
747
BASELINE CM
Regularizer FOSTER-DEV FOSTER-TEST IN-HOUSE FOSTER-DEV FOSTER-TEST IN-HOUSE
Averaging 0.827 0.837 0.830 0.831 0.844 0.833
Drop-out 0.827 0.838 0.827 0.836 0.843 0.833
Zipfian 0.821 0.835 0.833 0.825 0.838 0.836
Table 1: Results across regularizers (after 13 epochs).
linguistic theories nor annotators do agree upon.
6.2 NER
In the previous section, we saw positive effects of
cost-sensitive POS tagging for chunking, and here
we evaluate it on another downstream task, NER.
For the named entity recognition setup, we use
commonly used features, in particular features
for word tokens, orthographic features like the
presence of hyphens, digits, single quotes, up-
per/lowercase, 3 character prefix and suffix infor-
mation. Moreover, we add Brown word cluster
features that use 2,4,6,8,..,16 bitstring prefixes es-
timated from a large Twitter corpus (Owoputi et
al., 2013).
5
For NER, we do not have access to carefully
annotated Twitter data for training, but rely on
the crowdsourced annotations described in Finin
et al. (2010). We use the concatenation of the
CoNLL 2003 training split of annotated data from
the Reuters corpus and the Finin data for training,
as in this case training on the union resulted in a
model that is substantially better than training on
any of the individual data sets. For evaluation, we
have three Twitter data set. We use the recently
published data set from the MSM 2013 challenge
(29k tokens)
6
, the data set of Ritter et al. (2011)
used also by Fromheide et al. (2014) (46k tokens),
as well as an in-house annotated data set (20k to-
kens) (Fromheide et al., 2014).
F1: RITTER MSM IN-HOUSE
BL 78.20 82.25 82.58
CM 78.30 82.00 82.77
Table 3: Downstream results for named entity
recognition (F1 scores).
Table 3 shows the result of using our POS mod-
els in downstream NER evaluation. Here we ob-
serve mixed results. The cost-sensitive model is
5
http://www.ark.cs.cmu.edu/TweetNLP/
6
http://oak.dcs.shef.ac.uk/msm2013/ie_
challenge/
able to improve performance on two out of the
three test sets, while being slightly below baseline
performance on the MSM challenge data. Note
that in contrast to chunking, POS tags are just one
of the many features used for NER (albeit an im-
portant one), which might be part of the reason
why the picture looks slightly different from what
we observed above on chunking.
7 Related work
Cost-sensitive learning takes costs, such as mis-
classification cost, into consideration. That is,
each instance that is not classified correctly during
the learning process may contribute differently to
the overall error. Geibel and Wysotzki (2003) in-
troduce instance-dependent cost values for the per-
ceptron algorithm and apply it to a set of binary
classification problems. We focus here on struc-
tured problems and propose cost-sensitive learn-
ing for POS tagging using the structured percep-
tron algorithm. In a similar spirit, Higashiyama
et al. (2013) applied cost-sensitive learning to the
structured perceptron for an entity recognition task
in the medical domain. They consider the dis-
tance between the predicted and true label se-
quence smoothed by a parameter that they esti-
mate on a development set. This means that the
entire sequence is scored at once, while we update
on a per-label basis.
The work most related to ours is the recent study
of Song et al. (2012). They suggest that some er-
rors made by a POS tagger are more serious than
others, especially for downstream tasks. They de-
vise a hierarchy of POS tags for the Penn tree-
bank tag set (e.g. the class NOUN contains NN,
NNS, NNP, NNPS and CD) and use that in an
SVM learner. They modify the Hinge loss that
can take on three values: 0, ?, 1. If an error oc-
curred and the predicted tag is in the same class as
the gold tag, a loss ? occurred, otherwise it counts
as full cost. In contrast to our approach, they let
the learner focus on the more difficult cases by oc-
curring a bigger loss when the predicted POS tag
748
is in a different category. Their approach is thus
suitable for a fine-grained tagging scheme and re-
quires tuning of the cost parameter ?. We tackle
the problem from a different angle by letting the
learner abstract away from difficult, inconsistent
cases as estimated from inter-annotator scores.
Our approach is also related to the literature
on regularization, since our cost-sensitive loss
functions are aimed at preventing over-fitting to
low-confidence annotations. S?gaard (2013b;
2013a) presented two theories of linguistic varia-
tion and perceptron learning algorithms that reg-
ularize models to minimize loss under expected
variation. Our work is related, but models varia-
tions in annotation rather than variations in input.
There is a large literature related to the issue of
learning from annotator bias. Reidsma and op den
Akker (2008) show that differences between anno-
tators are not random slips of attention but rather
different biases annotators might have, i.e. differ-
ent mental conceptions. They show that a classi-
fier trained on data from one annotator performed
much better on in-sample (same annotator) data
than on data of any other annotator. They propose
two ways to address this problem: i) to identify
subsets of the data that show higher inter-annotator
agreement and use only that for training (e.g. for
speaker address identification they restrict the data
to instances where at least one person is in the
focus of attention); ii) if available, to train sepa-
rate models on data annotated by different anno-
tators and combine them through voting. The lat-
ter comes at the cost of recall, because they de-
liberately chose the classifier to abstain in non-
consensus cases.
In a similar vein, Klebanov and Beigman (2009)
divide the instance space into easy and hard cases,
i.e. easy cases are reliably annotated, whereas
items that are hard show confusion and disagree-
ment. Hard cases are assumed to be annotated
by individual annotator?s coin-flips, and thus can-
not be assumed to be uniformly distributed (Kle-
banov and Beigman, 2009). They show that learn-
ing with annotator noise can have deteriorating ef-
fect at test time, and thus propose to remove hard
cases, both at test time (Klebanov and Beigman,
2009) and training time (Beigman and Klebanov,
2009).
In general, it is important to analyze the data
and check for label biases, as a machine learner is
greatly affected by annotator noise that is not ran-
dom but systematic (Reidsma and Carletta, 2008).
However, rather than training on subsets of data or
training separate models ? which all implicitly as-
sume that there is a large amount of training data
available ? we propose to integrate inter-annotator
biases directly into the loss function.
Regarding measurements for agreements, sev-
eral scores have been suggested in the literature.
Apart from the simple agreement measure, which
records how often annotators choose the same
value for an item, there are several statistics that
qualify this measure by adjusting for other fac-
tors, such as Cohen?s ? (Cohen and others, 1960),
the G-index score (Holley and Guilford, 1964), or
Krippendorff?s ? (Krippendorf, 2004). However,
most of these scores are sensitive to the label dis-
tribution, missing values, and other circumstances.
The measure used in this paper is less affected by
these factors, but manages to give us a good un-
derstanding of the agreement.
8 Conclusion
In NLP, we use a variety of measures to assess
and control annotator disagreement to produce ho-
mogenous final annotations. This masks the fact
that some annotations are more reliable than oth-
ers, and which is thus not reflected in learned pre-
dictors. We incorporate the annotator uncertainty
on certain labels by measuring annotator agree-
ment and use it in the modified loss function of
a structured perceptron. We show that this ap-
proach works well independent of regularization,
both on in-sample and out-of-sample data. More-
over, when evaluating the models trained with our
loss function on downstream tasks, we observe im-
provements on two different tasks. Our results
suggest that we need to pay more attention to an-
notator confidence when training predictors.
Acknowledgements
We would like to thank the anonymous review-
ers and Nathan Schneider for valuable comments
and feedback. This research is funded by the ERC
Starting Grant LOWLANDS No. 313695.
References
Eyal Beigman and Beata Klebanov. 2009. Learning
with annotation noise. In ACL.
Jacob Cohen et al. 1960. A coefficient of agreement
749
for nominal scales. Educational and psychological
measurement, 20(1):37?46.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In ACL.
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina
Bontcheva. 2013. Twitter part-of-speech tagging
for all: overcoming sparse and noisy data. In
RANLP.
Jakob Elming, Anders Johannsen, Sigrid Klerke,
Emanuele Lapponi, Hector Martinez, and Anders
S?gaard. 2013. Down-stream effects of tree-to-
dependency conversions. In NAACL.
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010.
Annotating named entities in Twitter data with
crowdsourcing. In NAACL-HLT 2010 Workshop on
Creating Speech and Language Data with Amazon?s
Mechanical Turk.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Josef Le Roux, Joakim Nivre, Deirde Hogan, and
Josef van Genabith. 2011. From news to comments:
Resources and benchmarks for parsing the language
of Web 2.0. In IJCNLP.
Hege Fromheide, Dirk Hovy, and Anders S?gaard.
2014. Crowdsourcing and annotating NER for Twit-
ter #drift. In Proceedings of LREC 2014.
Peter Geibel and Fritz Wysotzki. 2003. Perceptron
based learning with example dependent and noisy
costs. In ICML.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In ACL.
Shohei Higashiyama, Kazuhiro Seki, and Kuniaki Ue-
hara. 2013. Clinical entity recognition using
cost-sensitive structured perceptron for NTCIR-10
MedNLP. In NTCIR.
Jasper Wilson Holley and Joy Paul Guilford. 1964.
A Note on the G-Index of Agreement. Educational
and Psychological Measurement, 24(4):749.
Dirk Hovy, Barbara Plank, and Anders S?gaard. 2014.
When POS datasets don?t add up: Combatting sam-
ple bias. In Proceedings of LREC 2014.
Richard Johansson. 2013. Training parsers on incom-
patible treebanks. In NAACL.
Beata Klebanov and Eyal Beigman. 2009. From an-
notator agreement to noise models. Computational
Linguistics, 35(4):495?503.
Klaus Krippendorf, 2004. Content Analysis: An In-
troduction to Its Methodology, second edition, chap-
ter 11. Sage, Thousand Oaks, CA.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: prob-
abilistic models for segmenting and labeling se-
quence data. In ICML.
Christopher D Manning. 2011. Part-of-speech tag-
ging from 97% to 100%: is it time for some linguis-
tics? In Computational Linguistics and Intelligent
Text Processing, pages 171?189. Springer.
Mitchell Marcus, Mary Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuz-
man Ganchev, Keith Hall, Slav Petrov, Hao
Zhang, Oscar T?ackstr?om, Claudia Bedini, N?uria
Bertomeu Castell?o, and Jungmee Lee. 2013. Uni-
versal dependency annotation for multilingual pars-
ing. In ACL.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
NAACL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In LREC.
Dennis Reidsma and Jean Carletta. 2008. Reliabil-
ity measurement without limits. Computational Lin-
guistics, 34(3):319?326.
Dennis Reidsma and Rieks op den Akker. 2008. Ex-
ploiting ?subjective? annotations. In Workshop on
Human Judgements in Computational Linguistics,
COLING.
Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011.
Named entity recognition in tweets: an experimental
study. In EMNLP.
Roy Schwartz, Omri Abend, Roi Reichart, and Ari
Rappoport. 2011. Neutralizing linguistically prob-
lematic annotations in unsupervised dependency
parsing evaluation. In ACL.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In NAACL.
Anders S?gaard. 2013a. Part-of-speech tagging with
antagonistic adversaries. In ACL.
Anders S?gaard. 2013b. Zipfian corruptions for robust
pos tagging. In NAACL.
750
Hyun-Je Song, Jeong-Woo Son, Tae-Gil Noh, Seong-
Bae Park, and Sang-Jo Lee. 2012. A cost sensitive
part-of-speech tagging: differentiating serious errors
from minor errors. In ACL.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012. Cross-framework evaluation for statistical
parsing. In EACL.
Daniel Zeman and Philip Resnik. 2008. Cross-
language parser adaptation between related lan-
guages. In IJCNLP.
Daniel Zeman. 2010. Hard problems of tagset con-
version. In Proceedings of the Second International
Conference on Global Interoperability for Language
Resources.
751
Proceedings of NAACL-HLT 2013, pages 1120?1130,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Learning Whom to Trust with MACE
Dirk Hovy1 Taylor Berg-Kirkpatrick2 Ashish Vaswani1 Eduard Hovy3
(1) Information Sciences Institute, University of Southern California, Marina del Rey
(2) Computer Science Division, University of California at Berkeley
(3) Language Technology Institute, Carnegie Mellon University, Pittsburgh
{dirkh,avaswani}@isi.edu, tberg@cs.berkeley.edu, hovy@cmu.edu
Abstract
Non-expert annotation services like Amazon?s
Mechanical Turk (AMT) are cheap and fast
ways to evaluate systems and provide categor-
ical annotations for training data. Unfortu-
nately, some annotators choose bad labels in
order to maximize their pay. Manual iden-
tification is tedious, so we experiment with
an item-response model. It learns in an un-
supervised fashion to a) identify which an-
notators are trustworthy and b) predict the
correct underlying labels. We match perfor-
mance of more complex state-of-the-art sys-
tems and perform well even under adversarial
conditions. We show considerable improve-
ments over standard baselines, both for pre-
dicted label accuracy and trustworthiness es-
timates. The latter can be further improved
by introducing a prior on model parameters
and using Variational Bayes inference. Ad-
ditionally, we can achieve even higher accu-
racy by focusing on the instances our model is
most confident in (trading in some recall), and
by incorporating annotated control instances.
Our system, MACE (Multi-Annotator Compe-
tence Estimation), is available for download1.
1 Introduction
Amazon?s MechanicalTurk (AMT) is frequently
used to evaluate experiments and annotate data in
NLP (Callison-Burch et al, 2010; Callison-Burch
and Dredze, 2010; Jha et al, 2010; Zaidan and
Callison-Burch, 2011). However, some turkers try to
maximize their pay by supplying quick answers that
have nothing to do with the correct label. We refer to
1Available under http://www.isi.edu/
publications/licensed-sw/mace/index.html
this type of annotator as a spammer. In order to mit-
igate the effect of spammers, researchers typically
collect multiple annotations of the same instance so
that they can, later, use de-noising methods to infer
the best label. The simplest approach is majority
voting, which weights all answers equally. Unfor-
tunately, it is easy for majority voting to go wrong.
A common and simple spammer strategy for cate-
gorical labeling tasks is to always choose the same
(often the first) label. When multiple spammers
follow this strategy, the majority can be incorrect.
While this specific scenario might seem simple to
correct for (remove annotators that always produce
the same label), the situation grows more tricky
when spammers do not annotate consistently, but in-
stead choose labels at random. A more sophisticated
approach than simple majority voting is required.
If we knew whom to trust, and when, we could
reconstruct the correct labels. Yet, the only way
to be sure we know whom to trust is if we knew
the correct labels ahead of time. To address this
circular problem, we build a generative model of the
annotation process that treats the correct labels as
latent variables. We then use unsupervised learning
to estimate parameters directly from redundant
annotations. This is a common approach in the
class of unsupervised models called item-response
models (Dawid and Skene, 1979; Whitehill et al,
2009; Carpenter, 2008; Raykar and Yu, 2012).
While such models have been implemented in
other fields (e.g., vision), we are not aware of their
availability for NLP tasks (see also Section 6).
Our model includes a binary latent variable that
explicitly encodes if and when each annotator is
spamming, as well as parameters that model the
annotator?s specific spamming ?strategy?. Impor-
1120
tantly, the model assumes that labels produced by
an annotator when spamming are independent of
the true label (though, a spammer can still produce
the correct label by chance).
In experiments, our model effectively differenti-
ates dutiful annotators from spammers (Section 4),
and is able to reconstruct the correct label with high
accuracy (Section 5), even under extremely adver-
sarial conditions (Section 5.2). It does not require
any annotated instances, but is capable of including
varying levels of supervision via token constraints
(Section 5.2). We consistently outperform major-
ity voting, and achieve performance equal to that of
more complex state-of-the-art models. Additionally,
we find that thresholding based on the posterior la-
bel entropy can be used to trade off coverage for ac-
curacy in label reconstruction, giving considerable
gains (Section 5.1). In tasks where correct answers
are more important than answering every instance,
e.g., when constructing a new annotated corpus, this
feature is extremely valuable. Our contributions are:
? We demonstrate the effectiveness of our model
on real world AMT datasets, matching the ac-
curacy of more complex state-of-the-art sys-
tems
? We show how posterior entropy can be used to
trade some coverage for considerable gains in
accuracy
? We study how various factors affect perfor-
mance, including number of annotators, anno-
tator strategy, and available supervision
? We provide MACE (Multi-Annotator Compe-
tence Estimation), a Java-based implementa-
tion of a simple and scalable unsupervised
model that identifies malicious annotators and
predicts labels with high accuracy
2 Model
We keep our model as simple as possible so that it
can be effectively trained from data where annotator
quality is unknown. If the model has too many
parameters, unsupervised learning can easily pick
up on and exploit coincidental correlations in the
data. Thus, we make a modeling assumption that
keeps our parameterization simple. We assume that
an annotator always produces the correct label when
N
T
i
M
A
ij
S
ij
T
A
2
C
2
A
3
C
3
A
1
C
1
Figure 1: Graphical model: Annotator j produces
label Aij on instance i. Label choice depends on
instance?s true label Ti, and whether j is spam-
ming on i, modeled by binary variable Sij . N =
|instances|, M = |annotators|.
for i = 1 . . . N :
Ti ? Uniform
for j = 1 . . .M :
Sij ? Bernoulli(1? ?j)
if Sij = 0 :
Aij = Ti
else :
Aij ? Multinomial(?j)
Figure 2: Generative process: see text for descrip-
tion.
he tries to. While this assumption does not reflect
the reality of AMT, it allows us to focus the model?s
power where it?s important: explaining away labels
that are not correlated with the correct label.
Our model generates the observed annotations as
follows: First, for each instance i, we sample the
true label Ti from a uniform prior. Then, for each
annotator j we draw a binary variable Sij from a
Bernoulli distribution with parameter 1 ? ?j . Sij
represents whether or not annotator j is spamming
on instance i. We assume that when an annotator
is not spamming on an instance, i.e. Sij = 0, he
just copies the true label to produce annotation Aij .
If Sij = 1, we say that the annotator is spamming
on the current instance, and Aij is sampled from
a multinomial with parameter vector ?j . Note that
in this case the annotation Aij does not depend on
the true label Ti. The annotations Aij are observed,
1121
while the true labels Ti and the spamming indicators
Sij are unobserved. The graphical model is shown
in Figure 1 and the generative process is described
in Figure 2.
The model parameters are ?j and ?j . ?j specifies
the probability of trustworthiness for annotator j
(i.e. the probability that he is not spamming on
any given instance). The learned value of ?j will
prove useful later when we try to identify reliable
annotators (see Section 4). The vector ?j determines
how annotator j behaves when he is spamming. An
annotator can produce the correct answer even while
spamming, but this can happen only by chance since
the annotator must use the same multinomial param-
eters ?j across all instances. This means that we only
learn annotator biases that are not correlated with
the correct label, e.g., the strategy of the spammer
who always chooses a certain label. This contrasts
with previous work where additional parameters are
used to model the biases that even dutiful annotators
exhibit. Note that an annotator can also choose not
to answer, which we can naturally accommodate be-
cause the model is generative. We enhance our gen-
erative model by adding Beta and Dirichlet priors on
?j and ?j respectively which allows us to incorporate
prior beliefs about our annotators (section 2.1).
2.1 Learning
We would like to set our model parameters to
maximize the probability of the observed data, i.e.,
the marginal data likelihood:
P (A; ?, ?) =
X
T,S
h NY
i=1
P (Ti) ?
MY
j=1
P (Sij ; ?j) ? P (Aij |Sij , Ti; ?j)
i
where A is the matrix of annotations, S is the
matrix of competence indicators, and T is the vector
of true labels.
We maximize the marginal data likelihood using
Expectation Maximization (EM) (Dempster et al,
1977), which has successfully been applied to
similar problems (Dawid and Skene, 1979). We ini-
tialize EM randomly and run for 50 iterations. We
perform 100 random restarts, and keep the model
with the best marginal data likelihood. We smooth
the M-step by adding a fixed value ? to the fractional
counts before normalizing (Eisner, 2002). We find
that smoothing improves accuracy, but, overall,
learning is robust to varying ?, and set ? = 0.1num labels .
We observe, however, that the average annota-
tor proficiency is usually high, i.e., most annota-
tors answer correctly. The distribution learned by
EM, however, is fairly linear. To improve the cor-
relation between model estimates and true annotator
proficiency, we would like to add priors about the
annotator behavior into the model. A straightfor-
ward approach is to employ Bayesian inference with
Beta priors on the proficiency parameters, ?j . We
thus also implement Variational-Bayes (VB) train-
ing with symmetric Beta priors on ?j and symmet-
ric Dirichlet priors on the strategy parameters, ?j .
Setting the shape parameters of the Beta distribution
to 0.5 favors the extremes of the distribution, i.e.,
either an annotator tried to get the right answer, or
simply did not care, but (almost) nobody tried ?a lit-
tle?. With VB training, we observe improved corre-
lations over all test sets with no loss in accuracy. The
hyper-parameters of the Dirichlet distribution on ?j
were clamped to 10.0 for all our experiments with
VB training. Our implementation is similar to John-
son (2007), which the reader can refer to for details.
3 Experiments
We evaluate our method on existing annotated
datasets from various AMT tasks. However, we
also want to ensure that our model can handle
adversarial conditions. Since we have no control
over the factors in existing datasets, we create
synthetic data for this purpose.
3.1 Natural Data
In order to evaluate our model, we use the
datasets from (Snow et al, 2008) that use discrete
label values (some tasks used continuous values,
which we currently do not model). Since they
compared AMT annotations to experts, gold anno-
tations exist for these sets. We can thus evaluate
the accuracy of the model as well as the proficiency
of each annotator. We show results for word sense
disambiguation (WSD: 177 items, 34 annotators),
recognizing textual entailment (RTE: 800 items,
164 annotators), and recognizing temporal relation
(Temporal: 462 items, 76 annotators).
3.2 Synthetic Data
In addition to the datasets above, we generate
synthetic data in order to control for different
1122
factors. This also allows us to create a gold standard
to which we can compare. We generate data sets
with 100 items, using two or four possible labels.
For each item, we generate answers from 20
different annotators. The ?annotators? are functions
that return one of the available labels according
to some strategy. Better annotators have a smaller
chance of guessing at random.
For various reasons, usually not all annotators see
or answer all items. We thus remove a randomly
selected subset of answers such that each item is
only answered by 10 of the annotators. See Figure
3 for an example annotation of three items.
annotators
ite
m
s ? 0 0 1 ? 0 ? ? 0 ?
1 ? ? 0 ? 1 0 ? ? 0
? ? 0 ? 0 1 ? 0 ? 0
Figure 3: Annotations: 10 annotators on three items,
labels {1, 0}, 5 annotations/item. Missing annota-
tions marked ???
3.3 Evaluations
First, we want to know which annotators to trust.
We evaluate whether our model?s learned trustwor-
thiness parameters ?j can be used to identify these
individuals (Section 4).
We then compare the label predicted by our model
and by majority voting to the correct label. The
results are reported as accuracy (Section 5). Since
our model computes posterior entropies for each
instance, we can use this as an approximation for the
model?s confidence in the prediction. If we focus on
predictions with high confidence (i.e., low entropy),
we hope to see better accuracy, even at the price of
leaving some items unanswered. We evaluate this
trade-off in Section 5.1. In addition, we investigate
the influence of the number of spammers and their
strategy on the accuracy of our model (Section 5.2).
4 Identifying Reliable Annotators
One of the distinguishing features of the model
is that it uses a parameter for each annotator to
estimate whether or not they are spamming. Can
we use this parameter to identify trustworthy indi-
viduals, to invite them for future tasks, and block
untrustworthy ones?
RTE Temporal WSD
raw agreement 0.78 0.73 0.81
Cohen?s ? 0.70 0.80 0.13
G-index 0.76 0.73 0.81
MACE-EM 0.87 0.88 0.44
MACE-VB (0.5,0.5) 0.91 0.90 0.90
Table 1: Correlation with annotator proficiency:
Pearson ? of different methods for various data sets.
MACE-VB?s trustworthiness parameter (trained
with Variational Bayes with ? = ? = 0.5) corre-
lates best with true annotator proficiency.
It is natural to apply some form of weighting.
One approach is to assume that reliable annotators
agree more with others than random annotators.
Inter-annotator agreement is thus a good candidate
to weigh the answers. There are various measures
for inter-annotator agreement.
Tratz and Hovy (2010) compute the average
agreement of each annotator and use it as a weight
to identify reliable ones. Raw agreement can be
directly computed from the data. It is related to
majority voting, since it will produce high scores for
all members of the majority class. Raw agreement
is thus a very simple measure.
In contrast, Cohen?s ? corrects the agreement
between two annotators for chance agreement. It
is widely used for inter-annotator agreement in
annotation tasks. We also compute the ? values
for each pair of annotators, and average them for
each annotator (similar to the approach in Tratz and
Hovy (2010)). However, whenever one label is more
prevalent (a common case in NLP tasks), ? overesti-
mates the effect of chance agreement (Feinstein and
Cicchetti, 1990) and penalizes disproportionately.
The G-index (Gwet, 2008) corrects for the number
of labels rather than chance agreement.
We compare these measures to our learned trust-
worthiness parameters ?j in terms of their ability to
select reliable annotators. A better measure should
lend higher score to annotators who answer correctly
more often than others. We thus compare the ratings
of each measure to the true proficiency of each
annotator. This is the percentage of annotated items
the annotator answered correctly. Methods that can
identify reliable annotators should highly correlate
1123
to the annotator?s proficiency. Since the methods
use different scales, we compute Pearson?s ? for the
correlation coefficient, which is scale-invariant. The
correlation results are shown in Table 1.
The model?s ?j correlates much more strongly
with annotator proficiency than either ? or raw
agreement. The variant trained with VB performs
consistently better than standard EM training, and
yields the best results. This show that our model
detects reliable annotators much better than any
of the other measures, which are only loosely
correlated to annotator proficiency.
The numbers for WSD also illustrate the low ?
score resulting when all annotators (correctly) agree
on a small number of labels. However, all inter-
annotator agreement measures suffer from an even
more fundamental problem: removing/ignoring
annotators with low agreement will always improve
the overall score, irrespective of the quality of their
annotations. Worse, there is no natural stopping
point: deleting the most egregious outlier always
improves agreement, until we have only one anno-
tator with perfect agreement left (Hovy, 2010). In
contrast, MACE does not discard any annotators,
but weighs their contributions differently. We are
thus not losing information. This works well even
under adversarial conditions (see Section 5.2).
5 Recovering the Correct Answer
RTE Temporal WSD
majority 0.90 0.93 0.99
Raykar/Yu 2012 0.93 0.94 ?
Carpenter 2008 0.93 ? ?
MACE-EM/VB 0.93 0.94 0.99
MACE-EM@90 0.95 0.97 0.99
MACE-EM@75 0.95 0.97 1.0
MACE-VB@90 0.96 0.97 1.0
MACE-VB@75 0.98 0.98 1.0
Table 2: Accuracy of different methods on data sets
from (Snow et al, 2008). MACE-VB uses Varia-
tional Bayes training. Results @n use the n% items
the model is most confident in (Section 5.1). Results
below double line trade coverage for accuracy and
are thus not comparable to upper half.
The previous sections showed that our model reli-
ably identifies trustworthy annotators. However, we
also want to find the most likely correct answer. Us-
ing majority voting often fails to find the correct la-
bel. This problem worsens when there are more than
two labels. We need to take relative majorities into
account or break ties when two or more labels re-
ceive the same number of votes. This is deeply un-
satisfying.
Figure 2 shows the accuracy of our model on
various data sets from Snow et al (2008). The
model outperforms majority voting on both RTE
and Temporal recognition sets. It performs as well
as majority voting for the WSD task. This last set
is somewhat of an exception, though, since almost
all annotators are correct all the time, so majority
voting is trivially correct. Still, we need to ensure
that the model does not perform worse under such
conditions. The results for RTE and Temporal data
also rival those reported in Raykar and Yu (2012)
and Carpenter (2008), yet were achieved with a
much simpler model.
Carpenter (2008) models instance difficulty as
a parameter. While it seems intuitively useful to
model which items are harder than other, it increases
the parameter space more than our trustworthiness
variable. We achieve comparable performance with-
out modeling difficulty, which greatly simplifies
inference. The model of Raykar and Yu (2012) is
more similar to our approach, in that it does not
model item difficulty. However, it adds an extra step
that learns priors from the estimated parameters. In
our model, this is part of the training process. For
more details on both models, see Section 6.
5.1 Trading Coverage for Accuracy
Sometimes, we want to produce an answer for ev-
ery item (e.g., when evaluating a data set), and some-
times, we value good answers more than answering
all items (e.g., when developing an annotated
corpus). Jha et al (2010) have demonstrated how to
achieve better coverage (i.e., answer more items) by
relaxing the majority voting constraints. Similarly,
we can improve accuracy if we only select high qual-
ity annotations, even if this incurs lower coverage.
We provide a parameter in MACE that allows
users to set a threshold for this trade-off: the
model only returns a label for an instance if it is
sufficiently confident in its answer. We approximate
the model?s confidence by the posterior entropy of
1124
0$&((0
0$&(9%
PDMRULW\
Figure 4: Tradeoff between coverage and accuracy for RTE (left) and temporal (right). Lower thresholds
lead to less coverage, but result in higher accuracy.
each instance. However, entropy depends strongly
on the specific makeup of the dataset (number of
annotators and labels, etc.), so it is hard for the user
to set a specific threshold.
Instead of requiring an exact entropy value, we
provide a simple thresholding between 0.0 and 1.0
(setting the threshold to 1.0 will include all items).
After training, MACE orders the posterior entropies
for all instances and selects the value that covers
the selected fraction of the instances. The threshold
thus roughly corresponds to coverage. It then only
returns answers for instances whose entropy is
below the threshold. This procedure is similar to
precision/recall curves.
Jha et al (2010) showed the effect of varying the
relative majority required, i.e., requiring that at least
n out of 10 annotators have to agree to count an
item. We use that method as baseline comparison,
evaluating the effect on coverage and accuracy
when we vary n from 5 to 10.
Figure 4 shows the tradeoff between coverage
and accuracy for two data sets. Lower thresholds
produce more accurate answers, but result in lower
coverage, as some items are left blank. If we pro-
duce answers for all items, we achieve accuracies
of 0.93 for RTE and 0.94 for Temporal, but by
excluding just the 10% of items in which the model
is least confident, we achieve accuracies as high as
0.95 for RTE and 0.97 for Temporal. We omit the
results for WSD here, since there is little headroom
and they are thus not very informative. Using Varia-
tional Bayes inference consistently achieves higher
results for the same coverage than the standard im-
plementation. Increasing the required majority also
improves accuracy, although not as much, and the
loss in coverage is larger and cannot be controlled.
In contrast, our method allows us to achieve better
accuracy at a smaller, controlled loss in coverage.
5.2 Influence of Strategy, Number of
Annotators, and Supervision
Adverse Strategy We showed that our model
recovers the correct answer with high accuracy.
However, to test whether this is just a function of
the annotator pool, we experiment with varying
the trustworthiness of the pool. If most annotators
answer correctly, majority voting is trivially correct,
as is our model. What happens, however, if more
and more annotators are unreliable? While some
agreement can arise from randomness, majority
voting is bound to become worse?can our model
overcome this problem? We set up a second set of
experiments to test this, using synthetic data. We
choose 20 annotators and vary the amount of good
annotators among them from 0 to 10 (after which
the trivial case sets in). We define a good annotator
as one who answers correctly 95% of the time.2
Adverse annotators select their answers randomly or
always choose a certain value (minimal annotators).
These are two frequent strategies of spammers.
For different numbers of labels and varying
percentage of spammers, we measure the accuracy
2The best annotators on the Snow data sets actually found
the correct answer 100% of the time.
1125
a) random annotators b) minimal annotators
Figure 5: Influence of adverse annotator strategy on label accuracy (y-axis). Number of possible labels
varied between 2 (top row) and 4 (bottom row). Adverse annotators either choose at random (a) or always
select the first label (b). MACE needs fewer good annotators to recover the correct answer.
0$&((0
0$&(9%
PDMRULW\
Figure 6: Varying number of annotators: effect on prediction accuracy. Each point averaged over 10 runs.
Note different scale for WSD.
of our model and majority voting on 100 items,
averaged over 10 runs for each condition. Figure
5 shows the effect of annotator proficiency on both
majority voting and our method for both kinds of
spammers. Annotator pool strategy affects majority
voting more than our model. Even with few good
annotators, our model learns to dismiss the spam-
mers as noise. There is a noticeable point on each
graph where MACE diverges from the majority
voting line. It thus reaches good accuracy much
1126
faster than majority voting, i.e., with fewer good an-
notators. This divergence point happens earlier with
more label values when adverse annotators label
randomly. In general, random annotators are easier
to deal with than the ones always choosing the first
label. Note that in cases where we have a majority
of adversarial annotators, VB performs worse than
EM, since this condition violates the implicit as-
sumptions we encoded with the priors in VB. Under
these conditions, setting different priors to reflect
the annotator pool should improve performance.
Obviously, both of these pools are extremes: it is
unlikely to have so few good or so many malicious
annotators. Most pools will be somewhere in
between. It does show, however, that our model
can pick up on reliable annotators even under very
unfavorable conditions. The result has a practical
upshot: AMT allows us to require a minimum rating
for annotators to work on a task. Higher ratings
improve annotation quality, but delay completion,
since there are fewer annotators with high ratings.
The results in this section suggest that we can find
the correct answer even in annotator pools with low
overall proficiency. We can thus waive the rating
requirement and allow more annotators to work on
the task. This considerably speeds up completion.
Number of Annotators Figure 6 shows the effect
different numbers of annotators have on accuracy.
As we increase the number of annotators, MACE
and majority voting achieve better accuracy results.
We note that majority voting results level or even
drop when going from an odd to an even number.
In these cases, the new annotator does not improve
accuracy if it goes with the previous majority (i.e.,
going from 3:2 to 4:2), but can force an error when
going against the previous majority (i.e., from 3:2 to
3:3), by creating a tie. MACE-EM and MACE-VB
dominate majority voting for RTE and Temporal.
For WSD, the picture is less clear, where majority
voting dominates when there are fewer annotators.
Note that the differences are minute, though (within
1 percentage point). For very small pool sizes (< 3),
MACE-VB outperforms both other methods.
Amount of Supervision So far, we have treated
the task as completely unsupervised. MACE does
not require any expert annotations in order to
achieve high accuracy. However, we often have
annotations for some of the items. These annotated
data points are usually used as control items (by
removing annotators that answer them incorrectly).
If such annotated data is available, we would like
to make use of it. We include an option that lets
users supply annotations for some of the items,
and use this information as token constraints in the
E-step of training. In those cases, the model does
not need to estimate the correct value, but only has
to adjust the trust parameter. This leads to improved
performance.3
We explore for RTE and Temporal how per-
formance changes when we vary the amount of
supervision in increments of 5%.4 We average over
10 runs for each value of n, each time supplying an-
notations for a random set of n items. The baseline
uses the annotated label whenever supplied, other-
wise the majority vote, with ties split at random.
Figure 7 shows that, unsurprisingly, all methods
improve with additional supervision, ultimately
reaching perfect accuracy. However, MACE uses
the information more effectively, resulting in
higher accuracy for a given amount of supervision.
This gain is more pronounced when only little
supervision is available.
6 Related Research
Snow et al (2008) and Sorokin and Forsyth
(2008) showed that Amazon?s MechanicalTurk use
in providing non-expert annotations for NLP tasks.
Various models have been proposed for predicting
correct annotations from noisy non-expert annota-
tions and for estimating annotator trustworthiness.
These models divide naturally into two categories:
those that use expert annotations for supervised
learning (Snow et al, 2008; Bian et al, 2009), and
completely unsupervised ones. Our method falls
into the latter category because it learns from the
redundant non-expert annotations themselves, and
makes no use of expertly annotated data.
Most previous work on unsupervised models
belongs to a class called ?Item-response models?,
used in psychometrics. The approaches differ with
respect to which aspect of the annotation process
3If we had annotations for all items, accuracy would be per-
fect and require no training.
4Given the high accuracy for the WSD data set even in the
fully unsupervised case, we omit the results here.
1127
Figure 7: Varying the amount of supervision: effect on prediction accuracy. Each point averaged over 10
runs. MACE uses supervision more efficiently.
they choose to focus on, and the type of annotation
task they model. For example, many methods ex-
plicitly model annotator bias in addition to annotator
competence (Dawid and Skene, 1979; Smyth et al,
1995). Our work models annotator bias, but only
when the annotator is suspected to be spamming.
Other methods focus modeling power on instance
difficulty to learn not only which annotators are
good, but which instances are hard (Carpenter,
2008; Whitehill et al, 2009). In machine vision,
several models have taken this further by parameter-
izing difficulty in terms of complex features defined
on each pairing of annotator and annotation instance
(Welinder et al, 2010; Yan et al, 2010). While
such features prove very useful in vision, they are
more difficult to define for the categorical problems
common to NLP. In addition, several methods are
specifically tailored to annotation tasks that involve
ranking (Steyvers et al, 2009; Lee et al, 2011),
which limits their applicability in NLP.
The method of Raykar and Yu (2012) is most
similar to ours. Their goal is to identify and filter
out annotators whose annotations are not correlated
with the gold label. They define a function of the
learned parameters that is useful for identifying
these spammers, and then use this function to build
a prior. In contrast, we use simple priors, but incor-
porate a model parameter that explicitly represents
the probability that an annotator is spamming. Our
simple model achieves the same accuracy on gold
label predictions as theirs.
7 Conclusion
We provide a Java-based implementation, MACE,
that recovers correct labels with high accuracy, and
reliably identifies trustworthy annotators. In
addition, it provides a threshold to control the
accuracy/coverage trade-off and can be trained with
standard EM or Variational Bayes EM. MACE
works fully unsupervised, but can incorporate token
constraints via annotated control items. We show
that even small amounts help improve accuracy.
Our model focuses most of its modeling power
on learning trustworthiness parameters, which
are highly correlated with true annotator relia-
bility (Pearson ? 0.9). We show on real-world
and synthetic data sets that our method is more
accurate than majority voting, even under ad-
versarial conditions, and as accurate as more
complex state-of-the-art systems. Focusing on high-
confidence instances improves accuracy consider-
ably. MACE is freely available for download under
http://www.isi.edu/publications/
licensed-sw/mace/index.html.
Acknowledgements
The authors would like to thank Chris Callison-
Burch, Victoria Fossum, Stephan Gouws, Marc
Schulder, Nathan Schneider, and Noah Smith for
invaluable discussions, as well as the reviewers for
their constructive feedback.
1128
References
Jiang Bian, Yandong Liu, Ding Zhou, Eugene Agichtein,
and Hongyuan Zha. 2009. Learning to recognize re-
liable users and content in social media with coupled
mutual reinforcement. In Proceedings of the 18th in-
ternational conference on World wide web, pages 51?
60. ACM.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with amazon?s mechanical
turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, pages 1?12, Los Angeles,
June. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53, Uppsala, Sweden, July. Association for
Computational Linguistics.
Bob Carpenter. 2008. Multilevel Bayesian models of
categorical data annotation. Unpublished manuscript.
A. Philip Dawid and Allan M. Skene. 1979. Maximum
likelihood estimation of observer error-rates using the
EM algorithm. Applied Statistics, pages 20?28.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society. Series B (Methodological), 39(1):1?38.
Jason Eisner. 2002. An interactive spreadsheet for teach-
ing the forward-backward algorithm. In Proceed-
ings of the ACL-02 Workshop on Effective tools and
methodologies for teaching natural language process-
ing and computational linguistics-Volume 1, pages 10?
18. Association for Computational Linguistics.
Alvan R. Feinstein and Domenic V. Cicchetti. 1990.
High agreement but low kappa: I. the problems of
two paradoxes. Journal of Clinical Epidemiology,
43(6):543?549.
Kilem Li Gwet. 2008. Computing inter-rater reliabil-
ity and its variance in the presence of high agreement.
British Journal of Mathematical and Statistical Psy-
chology, 61(1):29?48.
Eduard Hovy. 2010. Annotation. A Tutorial. In 48th
Annual Meeting of the Association for Computational
Linguistics.
Mukund Jha, Jacob Andreas, Kapil Thadani, Sara Rosen-
thal, and Kathleen McKeown. 2010. Corpus creation
for new genres: A crowdsourced approach to pp at-
tachment. In Proceedings of the NAACL HLT 2010
Workshop on Creating Speech and Language Data
with Amazon?s Mechanical Turk, pages 13?20. Asso-
ciation for Computational Linguistics.
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 296?305.
Michael D. Lee, Mark Steyvers, Mindy de Young, and
Brent J. Miller. 2011. A model-based approach to
measuring expertise in ranking tasks. In L. Carlson,
C. Ho?lscher, and T.F. Shipley, editors, Proceedings of
the 33rd Annual Conference of the Cognitive Science
Society, Austin, TX. Cognitive Science Society.
Vikas C. Raykar and Shipeng Yu. 2012. Eliminating
Spammers and Ranking Annotators for Crowdsourced
Labeling Tasks. Journal of Machine Learning Re-
search, 13:491?518.
Padhraic Smyth, Usama Fayyad, Mike Burl, Pietro Per-
ona, and Pierre Baldi. 1995. Inferring ground truth
from subjective labelling of Venus images. Advances
in neural information processing systems, pages 1085?
1092.
Rion Snow, Brendan O?Connor, Dan Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it
good? Evaluating non-expert annotations for natural
language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 254?263. Association for Computational Lin-
guistics.
Alexander Sorokin and David Forsyth. 2008. Utility
data annotation with Amazon Mechanical Turk. In
IEEE Computer Society Conference on Computer Vi-
sion and Pattern Recognition Workshops, CVPRW ?08,
pages 1?8. IEEE.
Mark Steyvers, Michael D. Lee, Brent Miller, and
Pernille Hemmer. 2009. The wisdom of crowds in the
recollection of order information. Advances in neural
information processing systems, 23.
Stephen Tratz and Eduard Hovy. 2010. A taxonomy,
dataset, and classifier for automatic noun compound
interpretation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 678?687. Association for Computational
Linguistics.
Peter Welinder, Steve Branson, Serge Belongie, and
Pietro Perona. 2010. The multidimensional wisdom
of crowds. In Neural Information Processing Systems
Conference (NIPS), volume 6.
Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob
Bergsma, and Javier Movellan. 2009. Whose vote
should count more: Optimal integration of labels from
labelers of unknown expertise. Advances in Neural In-
formation Processing Systems, 22:2035?2043.
Yan Yan, Ro?mer Rosales, Glenn Fung, Mark Schmidt,
Gerardo Hermosillo, Luca Bogoni, Linda Moy, and
1129
Jennifer Dy. 2010. Modeling annotator expertise:
Learning when everybody knows a bit of something.
In International Conference on Artificial Intelligence
and Statistics.
Omar F. Zaidan and Chris Callison-Burch. 2011. Crowd-
sourcing translation: Professional quality from non-
professionals. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 1220?
1229, Portland, Oregon, USA, June. Association for
Computational Linguistics.
1130
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1466?1475,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Unsupervised Discovery of Domain-Specific Knowledge from Text
Dirk Hovy, Chunliang Zhang, Eduard Hovy
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Marina del Rey, CA 90292
{dirkh, czheng, hovy}@isi.edu
Anselmo Pen?as
UNED NLP and IR Group
Juan del Rosal 16
28040 Madrid, Spain
anselmo@lsi.uned.es
Abstract
Learning by Reading (LbR) aims at enabling
machines to acquire knowledge from and rea-
son about textual input. This requires knowl-
edge about the domain structure (such as en-
tities, classes, and actions) in order to do in-
ference. We present a method to infer this im-
plicit knowledge from unlabeled text. Unlike
previous approaches, we use automatically ex-
tracted classes with a probability distribution
over entities to allow for context-sensitive la-
beling. From a corpus of 1.4m sentences, we
learn about 250k simple propositions about
American football in the form of predicate-
argument structures like ?quarterbacks throw
passes to receivers?. Using several statisti-
cal measures, we show that our model is able
to generalize and explain the data statistically
significantly better than various baseline ap-
proaches. Human subjects judged up to 96.6%
of the resulting propositions to be sensible.
The classes and probabilistic model can be
used in textual enrichment to improve the per-
formance of LbR end-to-end systems.
1 Introduction
The goal of Learning by Reading (LbR) is to enable
a computer to learn about a new domain and then
to reason about it in order to perform such tasks as
question answering, threat assessment, and explana-
tion (Strassel et al, 2010). This requires joint efforts
from Information Extraction, Knowledge Represen-
tation, and logical inference. All these steps depend
on the system having access to basic, often unstated,
foundational knowledge about the domain.
Most documents, however, do not explicitly men-
tion this information in the text, but assume basic
background knowledge about the domain, such as
positions (?quarterback?), titles (?winner?), or ac-
tions (?throw?) for sports game reports. Without
this knowledge, the text will not make sense to the
reader, despite being well-formed English. Luckily,
the information is often implicitly contained in the
document or can be inferred from similar texts.
Our system automatically acquires domain-
specific knowledge (classes and actions) from large
amounts of unlabeled data, and trains a probabilis-
tic model to determine and apply the most infor-
mative classes (quarterback, etc.) at appropriate
levels of generality for unseen data. E.g., from
sentences such as ?Steve Young threw a pass to
Michael Holt?, ?Quarterback Steve Young finished
strong?, and ?Michael Holt, the receiver, left early?
we can learn the classes quarterback and receiver,
and the proposition ?quarterbacks throw passes to
receivers?.
We will thus assume that the implicit knowl-
edge comes in two forms: actions in the form of
predicate-argument structures, and classes as part of
the source data. Our task is to identify and extract
both. Since LbR systems must quickly adapt and
scale well to new domains, we need to be able to
work with large amounts of data and minimal super-
vision. Our approach produces simple propositions
about the domain (see Figure 1 for examples of ac-
tual propositions learned by our system).
American football was the first official evaluation
domain in the DARPA-sponsored Machine Reading
program, and provides the background for a number
1466
of LbR systems (Mulkar-Mehta et al, 2010). Sports
is particularly amenable, since it usually follows a
finite, explicit set of rules. Due to its popularity,
results are easy to evaluate with lay subjects, and
game reports, databases, etc. provide a large amount
of data. The same need for basic knowledge appears
in all domains, though. In music, musicians play in-
struments, in electronics, components constitute cir-
cuits, circuits use electricity, etc.
Teams beat teams
Teams play teams
Quarterbacks throw passes
Teams win games
Teams defeat teams
Receivers catch passes
Quarterbacks complete passes
Quarterbacks throw passes to receivers
Teams play games
Teams lose games
Figure 1: The ten most frequent propositions discovered
by our system for the American football domain
Our approach differs from verb-argument identi-
fication or Named Entity (NE) tagging in several re-
spects. While previous work on verb-argument se-
lection (Pardo et al, 2006; Fan et al, 2010) uses
fixed sets of classes, we cannot know a priori how
many and which classes we will encounter. We
therefore provide a way to derive the appropriate
classes automatically and include a probability dis-
tribution for each of them. Our approach is thus
less restricted and can learn context-dependent, fine-
grained, domain-specific propositions. While a NE-
tagged corpus could produce a general proposition
like ?PERSON throws to PERSON?, our method
enables us to distinguish the arguments and learn
?quarterback throws to receiver? for American foot-
ball and ?outfielder throws to third base? for base-
ball. While in NE tagging each word has only one
correct tag in a given context, we have hierarchical
classes: an entity can be correctly labeled as a player
or a quarterback (and possibly many more classes),
depending on the context. By taking context into
account, we are also able to label each sentence in-
dividually and account for unseen entities without
using external resources.
Our contributions are:
? we use unsupervised learning to train a model
that makes use of automatically extracted
classes to uncover implicit knowledge in the
form of predicate-argument propositions
? we evaluate the explanatory power, generaliza-
tion capability, and sensibility of the proposi-
tions using both statistical measures and human
judges, and compare them to several baselines
? we provide a model and a set of propositions
that can be used to improve the performance
of end-to-end LbR systems via textual enrich-
ment.
2 Methods
INPUT:
Steve Young threw a pass to Michael Holt
1. PARSE INPUT:
2. JOIN NAMES, EXTRACT PREDICATES:
NVN: Steve_Young throw pass 
NVNPN: Steve_Young throw pass to Michael_Holt
3. DECODE TO INFER PROPOSITIONS:
QUARTERBACK throw pass
QUARTERBACK throw pass to RECEIVER
Steve/NNP
Young/NNP
throw/VBD
pass/NN
a/DT
to/TO
Michael/NNP
Holt/NNP
nsubj
dobj
prep
nn
nn
pobjdet
Steve_Young    threw      a         pass       to    Michael_Holt
s1 s2 x1 s3 s4 s5
p1 p2 p3 p4 p5
quarterback      throw                pass          to         receiver
Figure 2: Illustrated example of different processing steps
Our running example will be ?Steve Young threw
a pass to Michael Holt?. This is an instance of the
underlying proposition ?quarterbacks throw passes
to receivers?, which is not explicitly stated in the
data. A proposition is thus a more general state-
ment about the domain than the sentences it de-
rives. It contains domain-specific classes (quarter-
back, receiver), as well as lexical items (?throw?,
?pass?). In order to reproduce the proposition,
given the input sentences, our system has to not
only identify the classes, but also learn when to
1467
abstract away from the lexical form to the ap-
propriate class and when to keep it (cf. Figure
2, step 3). To facilitate extraction, we focus on
propositions with the following predicate-argument
structures: NOUN-VERB-NOUN (e.g., ?quarter-
backs throw passes?), or NOUN-VERB-NOUN-
PREPOSITION-NOUN (e.g., ?quarterbacks throw
passes to receivers?. There is nothing, though, that
prevents the use of other types of structures as well.
We do not restrict the verbs we consider (Pardo et
al., 2006; Ritter et al, 2010)), which extracts a high
number of hapax structures.
Given a sentence, we want to find the most likely
class for each word and thereby derive the most
likely proposition. Similar to Pardo et al (2006), we
assume the observed data was produced by a process
that generates the proposition and then transforms
the classes into a sentence, possibly adding addi-
tional words. We model this as a Hidden Markov
Model (HMM) with bigram transitions (see Section
2.3) and use the EM algorithm (Dempster et al,
1977) to train it on the observed data, with smooth-
ing to prevent overfitting.
2.1 Data
We use a corpus of about 33k texts on Ameri-
can football, extracted from the New York Times
(Sandhaus, 2008). To identify the articles, we rely
on the provided ?football? keyword classifier. The
resulting corpus comprises 1, 359, 709 sentences
from game reports, background stories, and opin-
ion pieces. In a first step, we parse all documents
with the Stanford dependency parser (De Marneffe
et al, 2006) (see Figure 2, step 1). The output
is lemmatized (collapsing ?throws?, ?threw?, etc.,
into ?throw?), and marked for various dependen-
cies (nsubj, amod, etc.). This enables us to ex-
tract the predicate argument structure, like subject-
verb-object, or additional prepositional phrases (see
Figure 2, step 2). These structures help to sim-
plify the model by discarding additional words like
modifiers, determiners, etc., which are not essen-
tial to the proposition. The same approach is used
by (Brody, 2007). We also concatenate multi-
word names (identified by sequences of NNPs) with
an underscore to form a single token (?Steve/NNP
Young/NNP?? ?Steve Young?).
2.2 Deriving Classes
To derive the classes used for entities, we do not re-
strict ourselves to a fixed set, but derive a domain-
specific set directly from the data. This step is per-
formed simultaneously with the corpus generation
described above. We utilize three syntactic construc-
tions to identify classes, namely nominal modifiers,
copula verbs, and appositions, see below. This is
similar in nature to Hearst?s lexico-syntactic patterns
(Hearst, 1992) and other approaches that derive IS-
A relations from text. While we find it straightfor-
ward to collect classes for entities in this way, we
did not find similar patterns for verbs. Given a suit-
able mechanism, however, these could be incorpo-
rated into our framework as well.
Nominal modifier are common nouns (labeled
NN) that precede proper nouns (labeled NNP), as in
?quarterback/NN Steve/NNP Young/NNP?, where
?quarterback? is the nominal modifier of ?Steve
Young?. Similar information can be gained from ap-
positions (e.g., ?Steve Young, the quarterback of his
team, said...?), and copula verbs (?Steve Young is
the quarterback of the 49ers?). We extract those co-
occurrences and store the proper nouns as entities
and the common nouns as their possible classes. For
each pair of class and entity, we collect counts over
the corpus to derive probability distributions.
Entities for which we do not find any of the above
patterns in our corpus are marked ?UNK?. These
entities are instantiated with the 20 most frequent
classes. All other (non-entity) words (including
verbs) have only their identity as class (i.e., ?pass?
remains ?pass?).
The average number of classes per entity is 6.87.
The total number of distinct classes for entities is
63, 942. This is a huge number to model in our state
space.1 Instead of manually choosing a subset of the
classes we extracted, we defer the task of finding the
best set to the model.
We note, however, that the distribution of classes
for each entity is highly skewed. Due to the unsuper-
vised nature of the extraction process, many of the
extracted classes are hapaxes and/or random noise.
Most entities have only a small number of applicable
classes (a football player usually has one main posi-
1NE taggers usually use a set of only a few dozen classes at
most.
1468
tion, and a few additional roles, such as star, team-
mate, etc.). We handle this by limiting the number of
classes considered to 3 per entity. This constraint re-
duces the total number of distinct classes to 26, 165,
and the average number of classes per entity to 2.53.
The reduction makes for a more tractable model size
without losing too much information. The class al-
phabet is still several magnitudes larger than that for
NE or POS tagging. Alternatively, one could use ex-
ternal resources such as Wikipedia, Yago (Suchanek
et al, 2007), or WordNet++ (Ponzetto and Navigli,
2010) to select the most appropriate classes for each
entity. This is likely to have a positive effect on the
quality of the applicable classes and merits further
research. Here, we focus on the possibilities of a
self-contained system without recurrence to outside
resources.
The number of classes we consider for each entity
also influences the number of possible propositions:
if we consider exactly one class per entity, there will
be little overlap between sentences, and thus no gen-
eralization possible?the model will produce many
distinct propositions. If, on the other hand, we used
only one class for all entities, there will be similar-
ities between many sentences?the model will pro-
duce very few distinct propositions.
2.3 Probabilistic Model
INPUT:
Steve Young threw a pass to Michael Holt
PARSE:
INSTANCES:
Steve_Young throw pass 
Steve_Young throw pass to Michael_Holt
PROPOSITIONS:
Quarterback throw pass
Quarterback throw pass to receiver
Steve
Young
throw
pass
a
to
Michael
Holt
nsubj
dobj
prep
nn
nn
pobjdet
Steve_Young    threw      a         pass       to    Michael_Holt
s1 s2 x1 s3 s4 s5
p1 p2 p3 p4 p5
quarterback      throw                pass          to         receiver
Figure 3: Graphical model for the running example
We use a generative noisy-channel model to cap-
ture the joint probability of input sentences and their
underlying proposition. Our generative story of how
a sentence s (with words s1, ..., sn) was generated
assumes that a proposition p is generated as a se-
quence of classes p1, ..., pn, with transition proba-
bilities P (pi|pi?1). Each class pi generates a word
si with probability P (si|pi). We allow additional
words x in the sentence which do not depend on any
class in the proposition and are thus generated inde-
pendently with P (x) (cf. model in Figure 3).
Since we observe the co-occurrence counts of
classes and entities in the data, we can fix the emis-
sion parameter P (s|p) in our HMM. Further, we do
not want to generate sentences from propositions, so
we can omit the step that adds the additional words
x in our model. The removal of these words is re-
flected by the preprocessing step that extracts the
structure (cf. Section 2.1).
Our model is thus defined as
P (s,p) =P (p1) ?
n?
i=1
(
P (pi|pi?1) ? P (si|pi)
)
(1)
where si, pi denote the ith word of sentence s and
proposition p, respectively.
3 Evaluation
We want to evaluate how well our model predicts
the data, and how sensible the resulting propositions
are. We define a good model as one that generalizes
well and produces semantically useful propositions.
We encounter two problems. First, since we de-
rive the classes in a data-driven way, we have no
gold standard data available for comparison. Sec-
ond, there is no accepted evaluation measure for this
kind of task. Ultimately, we would like to evaluate
our model externally, such as measuring its impact
on performance of a LbR system. In the absence
thereof, we resort to several complementary mea-
sures, as well as performing an annotation task. We
derive evaluation criteria as follows. A model gener-
alizes well if it can cover (?explain?) all the sentences
in the corpus with a few propositions. This requires
a measure of generality. However, while a proposi-
tion such as ?PERSON does THING?, has excellent
generality, it possesses no discriminating power. We
also need the propositions to partition the sentences
into clusters of semantic similarity, to support effec-
tive inference. This requires a measure of distribu-
tion. Maximal distribution, achieved by assigning
every sentence to a different proposition, however,
is not useful either. We need to find an appropri-
ate level of generality within which the sentences
are clustered into propositions for the best overall
groupings to support inference.
To assess the learned model, we apply the mea-
sures of generalization, entropy, and perplexity (see
1469
Sections 3.2, 3.3, and 3.4). These measures can be
used to compare different systems. We do not at-
tempt to weight or combine the different measures,
but present each in its own right.
Further, to assess label accuracy, we use Ama-
zon?s Mechanical Turk annotators to judge the sen-
sibility of the propositions produced by each sys-
tem (Section 3.5). We reason that if our system
learned to infer the correct classes, then the resulting
propositions should constitute true, general state-
ments about that domain, and thus be judged as sen-
sible.2 This approach allows the effective annotation
of sufficient amounts of data for an evaluation (first
described for NLP in (Snow et al, 2008)).
3.1 Evaluation Data
With the trained model, we use Viterbi decoding to
extract the best class sequence for each example in
the data. This translates the original corpus sen-
tences into propositions. See steps 2 and 3 in Figure
2.
We create two baseline systems from the same
corpus, one which uses the most frequent class
(MFC) for each entity, and another one which uses
a class picked at random from the applicable classes
of each entity.
Ultimately, we are interested in labeling unseen
data from the same domain with the correct class,
so we evaluate separately on the full corpus and
the subset of sentences that contain unknown enti-
ties (i.e., entities for which no class information was
available in the corpus, cf. Section 2.2).
For the latter case, we select all examples con-
taining at least one unknown entity (labeled UNK),
resulting in a subset of 41, 897 sentences, and repeat
the evaluation steps described above. Here, we have
to consider a much larger set of possible classes per
entity (the 20 overall most frequent classes). The
MFC baseline for these cases is the most frequent
of the 20 classes for UNK tokens, while the random
baseline chooses randomly from that set.
3.2 Generalization
Generalization measures how widely applicable the
produced propositions are. A completely lexical ap-
2Unfortunately, if judged insensible, we can not infer
whether our model used the wrong class despite better options,
or whether we simply have not learned the correct label.
entropy
Page 1
full data set
unknown entities
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.04 0.01
0.12 0.09
0.25
0.66
Generalization
random
MFC
model
Figure 4: Generalization of models on the data sets
proach, at one extreme, would turn each sentence
into a separate proposition, thus achieving a gener-
alization of 0%. At the other extreme, a model that
produces only one proposition would generalize ex-
tremely well (but would fail to explain the data in
any meaningful way). Both are of course not desir-
able.
We define generalization as
g = 1?
|propositions|
|sentences|
(2)
The results in Figure 4 show that our model is
capable of abstracting away from the lexical form,
achieving a generalization rate of 25% for the full
data set. The baseline approaches do significantly
worse, since they are unable to detect similarities
between lexically different examples, and thus cre-
ate more propositions. Using a two-tailed t-test, the
difference between our model and each baseline is
statistically significant at p < .001.
Generalization on the unknown entity data set is
even higher (65.84%). The difference between the
model and the baselines is again statistically signif-
icant at p < .001. MFC always chooses the same
class for UNK, regardless of context, and performs
much worse. The random baseline chooses between
20 classes per entity instead of 3, and is thus even
less general.
3.3 Normalized Entropy
Entropy is used in information theory to measure
how predictable data is. 0 means the data is com-
pletely predictable. The higher the entropy of our
propositions, the less well they explain the data. We
are looking for models with low entropy. The ex-
treme case of only one proposition has 0 entropy:
1470
entropy
Page 1
full data set
unknown entities
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
1.00 1.000.99 0.99
0.89
0.50
Normalized Entropy
random
MFC
model
Figure 5: Entropy of models on the data sets
we know exactly which sentences are produced by
the proposition.
Entropy is directly influenced by the number of
propositions used by a system.3 In order to compare
different models, we thus define normalized entropy
as
HN =
?
n?
i=0
Pi ? logPi
log n
(3)
where Pi is the coverage of the proposition, or the
percentage of sentences explained by it, and n is the
number of distinct propositions.
The entropy of our model on the full data set is
relatively high with 0.89, see Figure 5. The best
entropy we can hope to achieve given the number
of propositions and sentences is actually 0.80 (by
concentrating the maximum probability mass in one
proposition). The model thus does not perform as
badly as the number might suggest. The entropy of
our model on unseen data is better, with 0.50 (best
possible: 0.41). This might be due to the fact that
we considered more classes for UNK than for regu-
lar entities.
3.4 Perplexity
Since we assume that propositions are valid sen-
tences in our domain, good propositions should have
a higher probability than bad propositions in a lan-
guage model. We can compute this using perplex-
3Note that how many classes we consider per entity influ-
ences how many propositions are produced (cf. Section 2.2),
and thus indirectly puts a bound on entropy.
entropy
Page 1
full data set unknown entities
50.00
51.00
52.00
53.00
54.00
55.00
56.00
57.00
58.00
59.00
60.00 59.52
57.0357.03 57.1556.84
54.92
Perplexity
random
MFC
model
Figure 6: Perplexity of models on the data sets
ity:4
perplexity(data) = 2
? logP (data)
n (4)
where P (data) is the product of the proposition
probabilities, and n is the number of propositions.
We use the uni-, bi-, and trigram counts of the
GoogleGrams corpus (Brants and Franz, 2006) with
simple interpolation to compute the probability of
each proposition.
The results in Figure 6 indicate that the proposi-
tions found by the model are preferable to the ones
found by the baselines. As would be expected, the
sensibility judgements for MFC and model5 (Tables
1 and 2, Section 3.5) are perfectly anti-correlated
(correlation coefficient ?1) with the perplexity for
these systems in each data set. However, due to the
small sample size, this should be interpreted cau-
tiously.
3.5 Sensibility and Label Accuracy
In unsupervised training, the model with the best
data likelihood does not necessarily produce the best
label accuracy. We evaluate label accuracy by pre-
senting subjects with the propositions we obtained
from the Viterbi decoding of the corpus, and ask
them to rate their sensibility. We compare the dif-
ferent systems by computing sensibility as the per-
centage of propositions judged sensible for each sys-
tem. Since the underlying probability distributions
are quite different, we weight the sensibility judge-
ment for each proposition by the likelihood of that
proposition. We report results for both aggregate
4Perplexity also quantifies the uncertainty of the resulting
propositions, where 0 perplexity means no uncertainty.
5We did not collect sensibility judgements for the random
baseline.
1471
accuracy
Page 1
System
90.16 92.13 69.35 70.57 88.84 90.37
94.28 96.55 70.93 70.45 93.06 95.16
100 most frequent random combined
Data set agg maj agg maj agg maj
full
baseline
model
Table 1: Percentage of propositions derived from labeling the full data set that were judged sensible
accuracy
Page 1
System
51.92 51.51 32.39 28.21 50.39 49.66
66.00 69.57 48.14 41.74 64.83 67.76
100 most frequent random combined
Data set agg maj agg maj agg maj
unknown
baseline
model
Table 2: Percentage of propositions derived from labeling unknown entities that were judged sensible
sensibility (using the total number of individual an-
swers), and majority sensibility, where each propo-
sition is scored according to the majority of annota-
tors? decisions.
The model and baseline propositions for the full
data set are both judged highly sensible, achieving
accuracies of 96.6% and 92.1% (cf. Table 1). While
our model did slightly better, the differences are not
statistically significant when using a two-tailed test.
The propositions produced by the model from un-
known entities are less sensible (67.8%), albeit still
significantly above chance level, and the baseline
propositions for the same data set (p < 0.01). Only
49.7% propositions of the baseline were judged sen-
sible (cf. Table 2).
3.5.1 Annotation Task
Our model finds 250, 169 distinct propositions,
the MFC baseline 293, 028. We thus have to restrict
ourselves to a subset in order to judge their sensi-
bility. For each system, we sample the 100 most
frequent propositions and 100 random propositions
found for both the full data set and the unknown enti-
ties6 and have 10 annotators rate each proposition as
sensible or insensible. To identify and omit bad an-
notators (?spammers?), we use the method described
in Section 3.5.2, and measure inter-annotator agree-
ment as described in Section 3.5.3. The details of
this evaluation are given below, the results can be
found in Tables 1 and 2.
The 200 propositions from each of the four sys-
6We omit the random baseline here due to size issues, and
because it is not likely to produce any informative comparison.
tems (model and baseline on both full and unknown
data set), contain 696 distinct propositions. We
break these up into 70 batches (Amazon Turk an-
notation HIT pages) of ten propositions each. For
each proposition, we request 10 annotators. Overall,
148 different annotators participated in our annota-
tion. The annotators are asked to state whether each
proposition represents a sensible statement about
American Football or not. A proposition like ?Quar-
terbacks can throw passes to receivers? should make
sense, while ?Coaches can intercept teams? does
not. To ensure that annotators judge sensibility and
not grammaticality, we format each proposition the
same way, namely pluralizing the nouns and adding
?can? before the verb. In addition, annotators can
state whether a proposition sounds odd, seems un-
grammatical, is a valid sentence, but against the
rules (e.g., ?Coaches can hit players?) or whether
they do not understand it.
3.5.2 Spammers
Some (albeit few) annotators on Mechanical Turk
try to complete tasks as quickly as possible with-
out paying attention to the actual requirements, in-
troducing noise into the data. We have to identify
these spammers before the evaluation. One way is
to include tests. Annotators that fail these tests will
be excluded. We use a repetition (first and last ques-
tion are the same), and a truism (annotators answer-
ing ?no? either do not know about football or just
answered randomly). Alternatively, we can assume
that good annotators, who are the majority, will ex-
hibit similar behavior to one another, while spam-
1472
mers exhibit a deviant answer pattern. To identify
those outliers, we compare each annotator?s agree-
ment to the others and exclude those whose agree-
ment falls more than one standard deviation below
the average overall agreement.
We find that both methods produce similar results.
The first method requires more careful planning, and
the resulting set of annotators still has to be checked
for outliers. The second method has the advantage
that it requires no additional questions. It includes
the risk, though, that one selects a set of bad annota-
tors solely because they agree with one another.
3.5.3 Agreement
agreement
Page 1
0.88 0.76 0.82
? 0.45 0.50 0.48
0.66 0.53 0.58
measure 100 most frequent random combined
agreement
G-index
Table 3: Agreement measures for different samples
We use inter-annotator agreement to quantify the
reliability of the judgments. Apart from the simple
agreement measure, which records how often an-
notators choose the same value for an item, there
are several statistics that qualify this measure by ad-
justing for other factors. One frequently used mea-
sure, Cohen?s ?, has the disadvantage that if there
is prevalence of one answer, ? will be low (or even
negative), despite high agreement (Feinstein and Ci-
cchetti, 1990). This phenomenon, known as the ?
paradox, is a result of the formula?s adjustment for
chance agreement. As shown by Gwet (2008), the
true level of actual chance agreement is realistically
not as high as computed, resulting in the counterin-
tuitive results. We include it for comparative rea-
sons. Another statistic, the G-index (Holley and
Guilford, 1964), avoids the paradox. It assumes that
expected agreement is a function of the number of
choices rather than chance. It uses the same general
formula as ?,
(Pa ? Pe)
(1? Pe)
(5)
where Pa is the actual raw agreement measured, and
Pe is the expected agreement. The difference with
? is that Pe for the G-index is defined as Pe = 1/q,
where q is the number of available categories, in-
stead of expected chance agreement. Under most
conditions, G and ? are equivalent, but in the case
of high raw agreement and few categories, G gives a
more accurate estimation of the agreement. We thus
report raw agreement, ?, and G-index.
Despite early spammer detection, there are still
outliers in the final data, which have to be accounted
for when calculating agreement. We take the same
approach as in the statistical spammer detection and
delete outliers that are more than one standard devi-
ation below the rest of the annotators? average.
The raw agreement for both samples combined is
0.82, G = 0.58, and ? = 0.48. The numbers show
that there is reasonably high agreement on the label
accuracy.
4 Related Research
The approach we describe is similar in nature to un-
supervised verb argument selection/selectional pref-
erences and semantic role labeling, yet goes be-
yond it in several ways. For semantic role label-
ing (Gildea and Jurafsky, 2002; Fleischman et al,
2003), classes have been derived from FrameNet
(Baker et al, 1998). For verb argument detec-
tion, classes are either semi-manually derived from
a repository like WordNet, or from NE taggers
(Pardo et al, 2006; Fan et al, 2010). This allows
for domain-independent systems, but limits the ap-
proach to a fixed set of oftentimes rather inappropri-
ate classes. In contrast, we derive the level of gran-
ularity directly from the data.
Pre-tagging the data with NE classes before train-
ing comes at a cost. It lumps entities together which
can have very different classes (i.e., all people be-
come labeled as PERSON), effectively allowing only
one class per entity. Etzioni et al (2005) resolve the
problem with a web-based approach that learns hi-
erarchies of the NE classes in an unsupervised man-
ner. We do not enforce a taxonomy, but include sta-
tistical knowledge about the distribution of possible
classes over each entity by incorporating a prior dis-
tribution P (class, entity). This enables us to gen-
eralize from the lexical form without restricting our-
selves to one class per entity, which helps to bet-
ter fit the data. In addition, we can distinguish sev-
eral classes for each entity, depending on the context
1473
(e.g., winner vs. quarterback). Ritter et al (2010)
also use an unsupervised model to derive selectional
predicates from unlabeled text. They do not assign
classes altogether, but group similar predicates and
arguments into unlabeled clusters using LDA. Brody
(2007) uses a very similar methodology to establish
relations between clauses and sentences, by cluster-
ing simplified propositions.
Pen?as and Hovy (2010) employ syntactic patterns
to derive classes from unlabeled data in the context
of LbR. They consider a wider range of syntactic
structures, but do not include a probabilistic model
to label new data.
5 Conclusion
We use an unsupervised model to infer domain-
specific classes from a corpus of 1.4m unlabeled
sentences, and applied them to learn 250k propo-
sitions about American football. Unlike previous
approaches, we use automatically extracted classes
with a probability distribution over entities to al-
low for context-sensitive selection of appropriate
classes.
We evaluate both the model qualities and sensibil-
ity of the resulting propositions. Several measures
show that the model has good explanatory power and
generalizes well, significantly outperforming two
baseline approaches, especially where the possible
classes of an entity can only be inferred from the
context.
Human subjects on Amazon?s Mechanical Turk
judged up to 96.6% of the propositions for the full
data set, and 67.8% for data containing unseen enti-
ties as sensible. Inter-annotator agreement was rea-
sonably high (agreement = 0.82, G = 0.58, ? =
0.48).
The probabilistic model and the extracted propo-
sitions can be used to enrich texts and support post-
parsing inference for question answering. We are
currently applying our method to other domains.
Acknowledgements
We would like to thank David Chiang, Victoria Fos-
sum, Daniel Marcu, and Stephen Tratz, as well as the
anonymous ACL reviewers for comments and sug-
gestions to improve the paper. Research supported
in part by Air Force Contract FA8750-09-C-0172
under the DARPA Machine Reading Program.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
ings of the 17th international conference on Computa-
tional linguistics-Volume 1, pages 86?90. Association
for Computational Linguistics Morristown, NJ, USA.
Thorsten Brants and Alex Franz, editors. 2006. The
Google Web 1T 5-gram Corpus Version 1.1. Number
LDC2006T13. Linguistic Data Consortium, Philadel-
phia.
Samuel Brody. 2007. Clustering Clauses for High-
Level Relation Detection: An Information-theoretic
Approach. In Annual Meeting-Association for Com-
putational Linguistics, volume 45, page 448.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC 2006. Citeseer.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society. Series B (Methodological), 39(1):1?38.
Oren Etzioni, Michael Cafarella, Doug. Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsuper-
vised named-entity extraction from the web: An exper-
imental study. Artificial Intelligence, 165(1):91?134.
James Fan, David Ferrucci, David Gondek, and Aditya
Kalyanpur. 2010. Prismatic: Inducing knowledge
from a large scale lexicalized relation resource. In
Proceedings of the NAACL HLT 2010 First Interna-
tional Workshop on Formalisms and Methodology for
Learning by Reading, pages 122?127, Los Angeles,
California, June. Association for Computational Lin-
guistics.
Alvan R. Feinstein and Domenic V. Cicchetti. 1990.
High agreement but low kappa: I. the problems of
two paradoxes. Journal of Clinical Epidemiology,
43(6):543?549.
Michael Fleischman, Namhee Kwon, and Eduard Hovy.
2003. Maximum entropy models for FrameNet classi-
fication. In Proceedings of EMNLP, volume 3.
Danies Gildea and Dan Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Kilem Li Gwet. 2008. Computing inter-rater reliabil-
ity and its variance in the presence of high agreement.
British Journal of Mathematical and Statistical Psy-
chology, 61(1):29?48.
1474
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of the
14th conference on Computational linguistics-Volume
2, pages 539?545. Association for Computational Lin-
guistics.
Jasper Wilson Holley and Joy Paul Guilford. 1964. A
Note on the G-Index of Agreement. Educational and
Psychological Measurement, 24(4):749.
Rutu Mulkar-Mehta, James Allen, Jerry Hobbs, Eduard
Hovy, Bernardo Magnini, and Christopher Manning,
editors. 2010. Proceedings of the NAACL HLT
2010 First International Workshop on Formalisms and
Methodology for Learning by Reading. Association
for Computational Linguistics, Los Angeles, Califor-
nia, June.
Thiago Pardo, Daniel Marcu, and Maria Nunes. 2006.
Unsupervised Learning of Verb Argument Structures.
Computational Linguistics and Intelligent Text Pro-
cessing, pages 59?70.
Anselmo Pen?as and Eduard Hovy. 2010. Semantic en-
richment of text with background knowledge. In Pro-
ceedings of the NAACL HLT 2010 First International
Workshop on Formalisms and Methodology for Learn-
ing by Reading, pages 15?23, Los Angeles, California,
June. Association for Computational Linguistics.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich Word Sense Disambiguation rivaling
supervised systems. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1522?1531. Association for Computational
Linguistics.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent
dirichlet alocation method for selectional preferences.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 424?434,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Evan Sandhaus, editor. 2008. The New York Times Anno-
tated Corpus. Number LDC2008T19. Linguistic Data
Consortium, Philadelphia.
Rion Snow, Brendan O?Connor, Dan Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it
good? Evaluating non-expert annotations for natural
language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 254?263. Association for Computational Lin-
guistics.
Stephanie Strassel, Dan Adams, Henry Goldberg,
Jonathan Herr, Ron Keesing, Daniel Oblinger, Heather
Simpson, Robert Schrag, and Jonathan Wright. 2010.
The DARPA Machine Reading Program-Encouraging
Linguistic and Reasoning Research with a Series of
Reading Tasks. In Proceedings of LREC 2010.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowledge.
In Proceedings of the 16th international conference on
World Wide Web, pages 697?706. ACM.
1475
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 323?328,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Models and Training for Unsupervised Preposition Sense Disambiguation
Dirk Hovy and Ashish Vaswani and Stephen Tratz and
David Chiang and Eduard Hovy
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Marina del Rey, CA 90292
{dirkh,avaswani,stratz,chiang,hovy}@isi.edu
Abstract
We present a preliminary study on unsu-
pervised preposition sense disambiguation
(PSD), comparing different models and train-
ing techniques (EM, MAP-EM with L0 norm,
Bayesian inference using Gibbs sampling). To
our knowledge, this is the first attempt at un-
supervised preposition sense disambiguation.
Our best accuracy reaches 56%, a significant
improvement (at p <.001) of 16% over the
most-frequent-sense baseline.
1 Introduction
Reliable disambiguation of words plays an impor-
tant role in many NLP applications. Prepositions
are ubiquitous?they account for more than 10% of
the 1.16m words in the Brown corpus?and highly
ambiguous. The Preposition Project (Litkowski and
Hargraves, 2005) lists an average of 9.76 senses
for each of the 34 most frequent English preposi-
tions, while nouns usually have around two (Word-
Net nouns average about 1.2 senses, 2.7 if monose-
mous nouns are excluded (Fellbaum, 1998)). Dis-
ambiguating prepositions is thus a challenging and
interesting task in itself (as exemplified by the Sem-
Eval 2007 task, (Litkowski and Hargraves, 2007)),
and holds promise for NLP applications such as
Information Extraction or Machine Translation.1
Given a sentence such as the following:
In the morning, he shopped in Rome
we ultimately want to be able to annotate it as
1See (Chan et al, 2007) for how using WSD can help MT.
in/TEMPORAL the morning/TIME he/PERSON
shopped/SOCIAL in/LOCATIVE
Rome/LOCATION
Here, the preposition in has two distinct meanings,
namely a temporal and a locative one. These mean-
ings are context-dependent. Ultimately, we want
to disambiguate prepositions not by and for them-
selves, but in the context of sequential semantic la-
beling. This should also improve disambiguation of
the words linked by the prepositions (here, morn-
ing, shopped, and Rome). We propose using un-
supervised methods in order to leverage unlabeled
data, since, to our knowledge, there are no annotated
data sets that include both preposition and argument
senses. In this paper, we present our unsupervised
framework and show results for preposition disam-
biguation. We hope to present results for the joint
disambiguation of preposition and arguments in a
future paper.
The results from this work can be incorporated
into a number of NLP problems, such as seman-
tic tagging, which tries to assign not only syntac-
tic, but also semantic categories to unlabeled text.
Knowledge about semantic constraints of preposi-
tional constructions would not only provide better
label accuracy, but also aid in resolving preposi-
tional attachment problems. Learning by Reading
approaches (Mulkar-Mehta et al, 2010) also cru-
cially depend on unsupervised techniques as the
ones described here for textual enrichment.
Our contributions are:
? we present the first unsupervised preposition
sense disambiguation (PSD) system
323
? we compare the effectiveness of various models
and unsupervised training methods
? we present ways to extend this work to prepo-
sitional arguments
2 Preliminaries
A preposition p acts as a link between two words, h
and o. The head word h (a noun, adjective, or verb)
governs the preposition. In our example above, the
head word is shopped. The object of the preposi-
tional phrase (usually a noun) is denoted o, in our
example morning and Rome. We will refer to h and
o collectively as the prepositional arguments. The
triple h, p, o forms a syntactically and semantically
constrained structure. This structure is reflected in
dependency parses as a common construction. In
our example sentence above, the respective struc-
tures would be shopped in morning and shopped in
Rome. The senses of each element are denoted by a
barred letter, i.e., p? denotes the preposition sense, h?
denotes the sense of the head word, and o? the sense
of the object.
3 Data
We use the data set for the SemEval 2007 PSD
task, which consists of a training (16k) and a test
set (8k) of sentences with sense-annotated preposi-
tions following the sense inventory of The Preposi-
tion Project, TPP (Litkowski and Hargraves, 2005).
It defines senses for each of the 34 most frequent
prepositions. There are on average 9.76 senses per
preposition. This corpus was chosen as a starting
point for our study since it allows a comparison with
the original SemEval task. We plan to use larger
amounts of additional training data.
We used an in-house dependency parser to extract
the prepositional constructions from the data (e.g.,
?shop/VB in/IN Rome/NNP?). Pronouns and num-
bers are collapsed into ?PRO? and ?NUM?, respec-
tively.
In order to constrain the argument senses, we con-
struct a dictionary that lists for each word all the
possible lexicographer senses according to Word-
Net. The set of lexicographer senses (45) is a higher
level abstraction which is sufficiently coarse to allow
for a good generalization. Unknown words are as-
sumed to have all possible senses applicable to their
respective word class (i.e. all noun senses for words
labeled as nouns, etc).
4 Graphical Model
ph o
p?h? o?
h o
p?h? o?
h o
p?h? o?
a)
b)
c)
Figure 1: Graphical Models. a) 1st order HMM. b)
variant used in experiments (one model/preposition,
thus no conditioning on p). c) incorporates further
constraints on variables
As shown by Hovy et al (2010), preposition
senses can be accurately disambiguated using only
the head word and object of the PP. We exploit this
property of prepositional constructions to represent
the constraints between h, p, and o in a graphical
model. We define a good model as one that reason-
ably constrains the choices, but is still tractable in
terms of the number of parameters being estimated.
As a starting point, we choose the standard first-
order Hidden Markov Model as depicted in Figure
1a. Since we train a separate model for each preposi-
tion, we can omit all arcs to p. This results in model
1b. The joint distribution over the network can thus
be written as
Pp(h, o, h?, p?, o?) = P (h?) ? P (h|h?) ? (1)
P (p?|h?) ? P (o?|p?) ? P (o|o?)
We want to incorporate as much information as
possible into the model to constrain the choices. In
Figure 1c, we condition p? on both h? and o?, to reflect
the fact that prepositions act as links and determine
324
their sense mainly through context. In order to con-
strain the object sense o?, we condition on h?, similar
to a second-order HMM. The actual object o is con-
ditioned on both p? and o?. The joint distribution is
equal to
Pp(h, o, h?, p?, o?) = P (h?) ? P (h|h?) ? (2)
P (o?|h?) ? P (p?|h?, o?) ? P (o|o?, p?)
Though we would like to also condition the prepo-
sition sense p? on the head word h (i.e., an arc be-
tween them in 1c) in order to capture idioms and
fixed phrases, this would increase the number of pa-
rameters prohibitively.
5 Training
The training method largely determines how well the
resulting model explains the data. Ideally, the sense
distribution found by the model matches the real
one. Since most linguistic distributions are Zipfian,
we want a training method that encourages sparsity
in the model.
We briefly introduce different unsupervised train-
ing methods and discuss their respective advantages
and disadvantages. Unless specified otherwise, we
initialized all models uniformly, and trained until the
perplexity rate stopped increasing or a predefined
number of iterations was reached. Note that MAP-
EM and Bayesian Inference require tuning of some
hyper-parameters on held-out data, and are thus not
fully unsupervised.
5.1 EM
We use the EM algorithm (Dempster et al, 1977) as
a baseline. It is relatively easy to implement with ex-
isting toolkits like Carmel (Graehl, 1997). However,
EM has a tendency to assume equal importance for
each parameter. It thus prefers ?general? solutions,
assigning part of the probability mass to unlikely
states (Johnson, 2007). We ran EM on each model
for 100 iterations, or until the perplexity stopped de-
creasing below a threshold of 10?6.
5.2 EM with Smoothing and Restarts
In addition to the baseline, we ran 100 restarts with
random initialization and smoothed the fractional
counts by adding 0.1 before normalizing (Eisner,
2002). Smoothing helps to prevent overfitting. Re-
peated random restarts help escape unfavorable ini-
tializations that lead to local maxima. Carmel pro-
vides options for both smoothing and restarts.
5.3 MAP-EM with L0 Norm
Since we want to encourage sparsity in our mod-
els, we use the MDL-inspired technique intro-
duced by Vaswani et al (2010). Here, the goal
is to increase the data likelihood while keeping
the number of parameters small. The authors use
a smoothed L0 prior, which encourages probabil-
ities to go down to 0. The prior involves hyper-
parameters ?, which rewards sparsity, and ?, which
controls how close the approximation is to the true
L0 norm.2 We perform a grid search to tune the
hyper-parameters of the smoothed L0 prior for ac-
curacy on the preposition against, since it has a
medium number of senses and instances. For HMM,
we set ?trans =100.0, ?trans =0.005, ?emit =1.0,
?emit =0.75. The subscripts trans and emit de-
note the transition and emission parameters. For
our model, we set ?trans =70.0, ?trans =0.05,
?emit =110.0, ?emit =0.0025. The latter resulted
in the best accuracy we achieved.
5.4 Bayesian Inference
Instead of EM, we can use Bayesian inference with
Gibbs sampling and Dirichlet priors (also known as
the Chinese Restaurant Process, CRP). We follow
the approach of Chiang et al (2010), running Gibbs
sampling for 10,000 iterations, with a burn-in pe-
riod of 5,000, and carry out automatic run selec-
tion over 10 random restarts.3 Again, we tuned the
hyper-parameters of our Dirichlet priors for accu-
racy via a grid search over the model for the prepo-
sition against. For both models, we set the concen-
tration parameter ?trans to 0.001, and ?emit to 0.1.
This encourages sparsity in the model and allows for
a more nuanced explanation of the data by shifting
probability mass to the few prominent classes.
2For more details, the reader is referred to Vaswani et al
(2010).
3Due to time and space constraints, we did not run the 1000
restarts used in Chiang et al (2010).
325
result table
Page 1
HMM
0.40 (0.40)
0.42 (0.42) 0.55 (0.55) 0.45 (0.45) 0.53 (0.53)
0.41 (0.41) 0.49 (0.49) 0.55 (0.56) 0.48 (0.49)
baseline Vanilla EM
EM, smoothed, 
100 random 
restarts
MAP-EM + 
smoothed L0 
norm
CRP, 10 random 
restarts
our model
Table 1: Accuracy over all prepositions w. different models and training. Best accuracy: MAP-
EM+smoothed L0 norm on our model. Italics denote significant improvement over baseline at p <.001.
Numbers in brackets include against (used to tune MAP-EM and Bayesian Inference hyper-parameters)
6 Results
Given a sequence h, p, o, we want to find the se-
quence of senses h?, p?, o? that maximizes the joint
probability. Since unsupervised methods use the
provided labels indiscriminately, we have to map the
resulting predictions to the gold labels. The pre-
dicted label sequence h?, p?, o? generated by the model
via Viterbi decoding can then be compared to the
true key. We use many-to-1 mapping as described
by Johnson (2007) and used in other unsupervised
tasks (Berg-Kirkpatrick et al, 2010), where each
predicted sense is mapped to the gold label it most
frequently occurs with in the test data. Success is
measured by the percentage of accurate predictions.
Here, we only evaluate p?.
The results presented in Table 1 were obtained
on the SemEval test set. We report results both
with and without against, since we tuned the hyper-
parameters of two training methods on this preposi-
tion. To test for significance, we use a two-tailed
t-test, comparing the number of correctly labeled
prepositions. As a baseline, we simply label all word
types with the same sense, i.e., each preposition to-
ken is labeled with its respective name. When using
many-to-1 accuracy, this technique is equivalent to a
most-frequent-sense baseline.
Vanilla EM does not improve significantly over
the baseline with either model, all other methods
do. Adding smoothing and random restarts increases
the gain considerably, illustrating how important
these techniques are for unsupervised training. We
note that EM performs better with the less complex
HMM.
CRP is somewhat surprisingly roughly equivalent
to EM with smoothing and random restarts. Accu-
racy might improve with more restarts.
MAP-EM with L0 normalization produces the
best result (56%), significantly outperforming the
baseline at p < .001. With more parameters (9.7k
vs. 3.7k), which allow for a better modeling of
the data, L0 normalization helps by zeroing out in-
frequent ones. However, the difference between
our complex model and the best HMM (EM with
smoothing and random restarts, 55%) is not signifi-
cant.
The best (supervised) system in the SemEval task
(Ye and Baldwin, 2007) reached 69% accuracy. The
best current supervised system we are aware of
(Hovy et al, 2010) reaches 84.8%.
7 Related Work
The semantics of prepositions were topic of a special
issue of Computational Linguistics (Baldwin et al,
2009). Preposition sense disambiguation was one of
the SemEval 2007 tasks (Litkowski and Hargraves,
2007), and was subsequently explored in a number
of papers using supervised approaches: O?Hara and
Wiebe (2009) present a supervised preposition sense
disambiguation approach which explores different
settings; Tratz and Hovy (2009), Hovy et al (2010)
make explicit use of the arguments for preposition
sense disambiguation, using various features. We
differ from these approaches by using unsupervised
methods and including argument labeling.
The constraints of prepositional constructions
have been explored by Rudzicz and Mokhov (2003)
and O?Hara and Wiebe (2003) to annotate the se-
mantic role of complete PPs with FrameNet and
Penn Treebank categories. Ye and Baldwin (2006)
explore the constraints of prepositional phrases for
326
semantic role labeling. We plan to use the con-
straints for argument disambiguation.
8 Conclusion and Future Work
We evaluate the influence of two different models (to
represent constraints) and three unsupervised train-
ing methods (to achieve sparse sense distributions)
on PSD. Using MAP-EM with L0 norm on our
model, we achieve an accuracy of 56%. This is a
significant improvement (at p <.001) over the base-
line and vanilla EM. We hope to shorten the gap to
supervised systems with more unlabeled data. We
also plan on training our models with EM with fea-
tures (Berg-Kirkpatrick et al, 2010).
The advantage of our approach is that the models
can be used to infer the senses of the prepositional
arguments as well as the preposition. We are cur-
rently annotating the data to produce a test set with
Amazon?s Mechanical Turk, in order to measure la-
bel accuracy for the preposition arguments.
Acknowledgements
We would like to thank Steve DeNeefe, Jonathan
Graehl, Victoria Fossum, and Kevin Knight, as well
as the anonymous reviewers for helpful comments
on how to improve the paper. We would also like
to thank Morgan from Curious Palate for letting us
write there. Research supported in part by Air Force
Contract FA8750-09-C-0172 under the DARPA Ma-
chine Reading Program and by DARPA under con-
tract DOI-NBC N10AP20031.
References
Tim Baldwin, Valia Kordoni, and Aline Villavicencio.
2009. Prepositions in applications: A survey and in-
troduction to the special issue. Computational Lin-
guistics, 35(2):119?149.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,
John DeNero, and Dan Klein. 2010. Painless Unsu-
pervised Learning with Features. In North American
Chapter of the Association for Computational Linguis-
tics.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Annual Meeting ? Association
For Computational Linguistics, volume 45, pages 33?
40.
David Chiang, Jonathan Graehl, Kevin Knight, Adam
Pauls, and Sujith Ravi. 2010. Bayesian inference
for Finite-State transducers. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 447?455. Association for
Computational Linguistics.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society. Series B (Methodological), 39(1):1?38.
Jason Eisner. 2002. An interactive spreadsheet for teach-
ing the forward-backward algorithm. In Proceed-
ings of the ACL-02 Workshop on Effective tools and
methodologies for teaching natural language process-
ing and computational linguistics-Volume 1, pages 10?
18. Association for Computational Linguistics.
Christiane Fellbaum. 1998. WordNet: an electronic lexi-
cal database. MIT Press USA.
Jonathan Graehl. 1997. Carmel Finite-state Toolkit.
ISI/USC.
Dirk Hovy, Stephen Tratz, and Eduard Hovy. 2010.
What?s in a Preposition? Dimensions of Sense Dis-
ambiguation for an Interesting Word Class. In Coling
2010: Posters, pages 454?462, Beijing, China, Au-
gust. Coling 2010 Organizing Committee.
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 296?305.
Ken Litkowski and Orin Hargraves. 2005. The prepo-
sition project. ACL-SIGSEM Workshop on ?The Lin-
guistic Dimensions of Prepositions and Their Use in
Computational Linguistic Formalisms and Applica-
tions?, pages 171?179.
Ken Litkowski and Orin Hargraves. 2007. SemEval-
2007 Task 06: Word-Sense Disambiguation of Prepo-
sitions. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic.
Rutu Mulkar-Mehta, James Allen, Jerry Hobbs, Eduard
Hovy, Bernardo Magnini, and Christopher Manning,
editors. 2010. Proceedings of the NAACL HLT
2010 First International Workshop on Formalisms and
Methodology for Learning by Reading. Association
for Computational Linguistics, Los Angeles, Califor-
nia, June.
Tom O?Hara and Janyce Wiebe. 2003. Preposi-
tion semantic classification via Penn Treebank and
FrameNet. In Proceedings of CoNLL, pages 79?86.
Tom O?Hara and Janyce Wiebe. 2009. Exploiting se-
mantic role resources for preposition disambiguation.
Computational Linguistics, 35(2):151?184.
327
Frank Rudzicz and Serguei A. Mokhov. 2003. Towards
a heuristic categorization of prepositional phrases in
english with wordnet. Technical report, Cornell
University, arxiv1.library.cornell.edu/abs/1002.1095-
?context=cs.
Stephen Tratz and Dirk Hovy. 2009. Disambiguation of
Preposition Sense Using Linguistically Motivated Fea-
tures. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, Companion Volume: Student Research Work-
shop and Doctoral Consortium, pages 96?100, Boul-
der, Colorado, June. Association for Computational
Linguistics.
Ashish Vaswani, Adam Pauls, and David Chiang. 2010.
Efficient optimization of an MDL-inspired objective
function for unsupervised part-of-speech tagging. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, pages 209?214. Association for Computational
Linguistics.
Patrick Ye and Tim Baldwin. 2006. Semantic role la-
beling of prepositional phrases. ACM Transactions
on Asian Language Information Processing (TALIP),
5(3):228?244.
Patrick Ye and Timothy Baldwin. 2007. MELB-YB:
Preposition Sense Disambiguation Using Rich Seman-
tic Features. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic.
328
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 377?382,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Experiments with crowdsourced re-annotation of a POS tagging data set
Dirk Hovy, Barbara Plank, and Anders S?gaard
Center for Language Technology
University of Copenhagen
Njalsgade 140, 2300 Copenhagen
{dirk|bplank}@cst.dk, soegaard@hum.ku.dk
Abstract
Crowdsourcing lets us collect multiple an-
notations for an item from several annota-
tors. Typically, these are annotations for
non-sequential classification tasks. While
there has been some work on crowdsourc-
ing named entity annotations, researchers
have largely assumed that syntactic tasks
such as part-of-speech (POS) tagging can-
not be crowdsourced. This paper shows
that workers can actually annotate sequen-
tial data almost as well as experts. Fur-
ther, we show that the models learned from
crowdsourced annotations fare as well as
the models learned from expert annota-
tions in downstream tasks.
1 Introduction
Training good predictive NLP models typically re-
quires annotated data, but getting professional an-
notators to build useful data sets is often time-
consuming and expensive. Snow et al (2008)
showed, however, that crowdsourced annotations
can produce similar results to annotations made
by experts. Crowdsourcing services such as Ama-
zon?s Mechanical Turk has since been successfully
used for various annotation tasks in NLP (Jha et
al., 2010; Callison-Burch and Dredze, 2010).
However, most applications of crowdsourcing
in NLP have been concerned with classification
problems, such as document classification and
constructing lexica (Callison-Burch and Dredze,
2010). A large part of NLP problems, however, are
structured prediction tasks. Typically, sequence
labeling tasks employ a larger set of labels than
classification problems, as well as complex inter-
actions between the annotations. Disagreement
among annotators is therefore potentially higher,
and the task of annotating structured data thus
harder.
Only a few recent studies have investi-
gated crowdsourcing sequential tasks; specifically,
named entity recognition (Finin et al, 2010; Ro-
drigues et al, 2013). Results for this are good.
However, named entities typically use only few la-
bels (LOC, ORG, and PER), and the data contains
mostly non-entities, so the complexity is manage-
able. The question of whether a more linguisti-
cally involved structured task like part-of-speech
(POS) tagging can be crowdsourced has remained
largely unaddressed.
1
In this paper, we investigate how well lay anno-
tators can produce POS labels for Twitter data. In
our setup, we present annotators with one word at
a time, with a minimal surrounding context (two
words to each side). Our choice of annotating
Twitter data is not coincidental: with the short-
lived nature of Twitter messages, models quickly
lose predictive power (Eisenstein, 2013), and re-
training models on new samples of more represen-
tative data becomes necessary. Expensive profes-
sional annotation may be prohibitive for keeping
NLP models up-to-date with linguistic and topical
changes on Twitter. We use a minimum of instruc-
tions and require few qualifications.
Obviously, lay annotation is generally less re-
liable than professional annotation. It is there-
fore common to aggregate over multiple annota-
tions for the same item to get more robust anno-
tations. In this paper we compare two aggrega-
tion schemes, namely majority voting (MV) and
MACE (Hovy et al, 2013). We also show how we
can use Wiktionary, a crowdsourced lexicon, to fil-
ter crowdsourced annotations. We evaluate the an-
notations in several ways: (a) by testing their ac-
curacy with respect to a gold standard, (b) by eval-
uating the performance of POS models trained on
1
One of the reviewers alerted us to an unpublished mas-
ters thesis, which uses pre-annotation to reduce tagging to
fewer multiple-choice questions. See Related Work section
for details.
377
the annotations across several existing data sets,
as well as (c) by applying our models in down-
stream tasks. We show that with minimal context
and annotation effort, we can produce structured
annotations of near-expert quality. We also show
that these annotations lead to better POS tagging
models than previous models learned from crowd-
sourced lexicons (Li et al, 2012). Finally, we
show that models learned from these annotations
are competitive with models learned from expert
annotations on various downstream tasks.
2 Our Approach
We crowdsource the training section of the data
from Gimpel et al (2011)
2
with POS tags. We use
Crowdflower,
3
to collect five annotations for each
word, and then find the most likely label for each
word among the possible annotations. See Figure
1 for an example. If the correct label is not among
the annotations, we are unable to recover the cor-
rect answer. This was the case for 1497 instances
in our data (cf. the token ?:? in the example).
We thus report on oracle score, i.e., the best label
sequence that could possibly be found, which is
correct except for the missing tokens. Note that
while we report agreement between the crowd-
sourced annotations and the crowdsourced anno-
tations, our main evaluations are based on models
learned from expert vs. crowdsourced annotations
and downstream applications thereof (chunking
and NER). We take care in evaluating our models
across different data sets to avoid biasing our
evaluations to particular annotations. All the data
sets used in our experiments are publicly available
at http://lowlands.ku.dk/results/.
x Z y
@USER NOUN,NOUN,X,NOUN,-,NOUN NOUN
: .,.,-,.,.,. X
I PRON,NOUN,PRON,NOUN,PRON,- PRON
owe VERB,VERB,-,VERB,VERB,VERB VERB
U PRON,X,-,NOUN,NOUN,PRON PRON
? = 0.9, 0.4, 0.2, 0.8, 0.8, 0.9
Figure 1: Five annotations per token, supplied by 6
different annotators (- = missing annotation), gold
label y. ? = competence values for each annotator.
2
http://www.ark.cs.cmu.edu/TweetNLP/
3
http://crowdflower.com
3 Crowdsourcing Sequential Annotation
In order to use the annotations to train models that
can be applied across various data sets, i.e., mak-
ing out-of-sample evaluation possible (see Section
5), we follow Hovy et al (2014) in using the uni-
versal tag set (Petrov et al, 2012) with 12 labels.
Figure 2: Screen shot of the annotation interface
on Crowdflower
Annotators were given a bold-faced word with
two words on either side and asked to select the
most appropriate tag from a drop down menu. For
each tag, we spell out the name of the syntactic
category, and provide a few example words.
See Figure 2 for a screenshot of the interface.
Annotators were also told that words can belong
to several classes, depending on the context. No
additional guidelines were given.
Only trusted annotators (in Crowdflower:
Bronze skills) that had answered correctly on 4
gold tokens (randomly chosen from a set of 20
gold tokens provided by the authors) were allowed
to submit annotations. In total, 177 individual
annotators supplied answers. We paid annotators
a reward of $0.05 for 10 tokens. The full data set
contains 14,619 tokens. Completion of the task
took slightly less than 10 days. Contributors were
very satisfied with the task (4.5 on a scale from 1
to 5). In particular, they felt instructions were clear
(4.4/5), and that the pay was reasonable (4.1/5).
4 Label Aggregation
After collecting the annotations, we need to aggre-
gate the annotations to derive a single answer for
each token. In the simplest scheme, we choose the
majority label, i.e., the label picked by most an-
notators. In case of ties, we select the final label
at random. Since this is a stochastic process, we
average results over 100 runs. We refer to this as
MAJORITY VOTING (MV). Note that in MV we
trust all annotators to the same degree. However,
crowdsourcing attracts people with different mo-
378
tives, and not all of them are equally reliable?
even the ones with Bronze level. Ideally, we would
like to factor this into our decision process.
We use MACE
4
(Hovy et al, 2013) as our sec-
ond scheme to learn both the most likely answer
and a competence estimate for each of the annota-
tors. MACE treats annotator competence and the
correct answer as hidden variables and estimates
their parameters via EM (Dempster et al, 1977).
We use MACE with default parameter settings to
give us the weighted average for each annotated
example.
Finally, we also tried applying the joint learn-
ing scheme in Rodrigues et al (2013), but their
scheme requires that entire sequences are anno-
tated by the same annotators, which we don?t have,
and it expects BIO sequences, rather than POS
tags.
Dictionaries Decoding tasks profit from the use
of dictionaries (Merialdo, 1994; Johnson, 2007;
Ravi and Knight, 2009) by restricting the number
of tags that need to be considered for each word,
also known as type constraints (T?ackstr?om et al,
2013). We follow Li et al (2012) in including
Wiktionary information as type constraints into
our decoding: if a word is found in Wiktionary,
we disregard all annotations that are not licensed
by the dictionary entry. If the word is not found in
Wiktionary, or if none of its annotations is licensed
by Wiktionary, we keep the original annotations.
Since we aggregate annotations independently
(unlike Viterbi decoding), we basically use Wik-
tionary as a pre-filtering step, such that MV and
MACE only operate on the reduced annotations.
5 Experiments
Each of the two aggregation schemes above pro-
duces a final label sequence y? for our training cor-
pus. We evaluate the resulting annotated data in
three ways.
1. We compare y? to the available expert annota-
tion on the training data. This tells us how similar
lay annotation is to professional annotation.
2. Ultimately, we want to use structured anno-
tations for supervised training, where annotation
quality influences model performance on held-out
test data. To test this, we train a CRF model
(Lafferty et al, 2001) with simple orthographic
features and word clusters (Owoputi et al, 2013)
4
http://www.isi.edu/publications/
licensed-sw/mace/
on the annotated Twitter data described in Gim-
pel et al (2011). Leaving out the dedicated test
set to avoid in-sample bias, we evaluate our mod-
els across three data sets: RITTER (the 10% test
split of the data in Ritter et al (2011) used in Der-
czynski et al (2013)), the test set from Foster et
al. (2011), and the data set described in Hovy et
al. (2014).
We will make the preprocessed data sets avail-
able to the public to facilitate comparison. In ad-
dition to a supervised model trained on expert an-
notations, we compare our tagging accuracy with
that of a weakly supervised system (Li et al, 2012)
re-trained on 400,000 unlabeled tweets to adapt to
Twitter, but using a crowdsourced lexicon, namely
Wiktionary, to constrain inference. We use param-
eter settings from Li et al (2012), as well as their
Wikipedia dump, available from their project web-
site.
5
3. POS tagging is often the first step for further
analysis, such as chunking, parsing, etc. We
test the downstream performance of the POS
models from the previous step on chunking and
NER. We use the models to annotate the training
data portion of each task with POS tags, and
use them as features in a chunking and NER
model. For both tasks, we train a CRF model
on the respective (POS-augmented) training set,
and evaluate it on several held-out test sets. For
chunking, we use the test sets from Foster et al
(2011) and Ritter et al (2011) (with the splits
from Derczynski et al (2013)). For NER, we use
data from Finin et al (2010) and again Ritter et al
(2011). For chunking, we follow Sha and Pereira
(2003) for the set of features, including token
and POS information. For NER, we use standard
features, including POS tags (from the previous
experiments), indicators for hyphens, digits,
single quotes, upper/lowercase, 3-character prefix
and suffix information, and Brown word cluster
features
6
with 2,4,8,16 bitstring prefixes estimated
from a large Twitter corpus (Owoputi et al, 2013).
We report macro-averages over all these data sets.
6 Results
Agreement with expert annotators Table 1
shows the accuracy of each aggregation compared
to the gold labels. The crowdsourced annotations
5
https://code.google.com/p/
wikily-supervised-pos-tagger/
6
http://www.ark.cs.cmu.edu/TweetNLP/
379
majority 79.54
MACE-EM 79.89
majority+Wiktionary 80.58
MACE-EM+Wiktionary 80.75
oracle 89.63
Table 1: Accuracy (%) of different annotations wrt
gold data
aggregated using MV agree with the expert anno-
tations in 79.54% of the cases. If we pre-filter the
data using Wiktionary, the agreement becomes
80.58%. MACE leads to higher agreement with
expert annotations under both conditions (79.89
and 80.75). The small difference indicates that
annotators are consistent and largely reliable,
thus confirming the Bronze-level qualification
we required. Both schemes cannot recover the
correct answer for the 1497 cases where none of
the crowdsourced labels matched the gold label,
i.e. y /? Z
i
. The best possible result either of them
could achieve (the oracle) would be matching all
but the missing labels, an agreement of 89.63%.
Most of the cases where the correct label was
not among the annotations belong to a small set
of confusions. The most frequent was mislabeling
?:? and ?. . .?, both mapped to X. Annotators
mostly decided to label these tokens as punctu-
ation (.). They also predominantly labeled your,
my and this as PRON (for the former two), and a
variety of labels for the latter, when the gold label
is DET.
RITTER FOSTER HOVY
Li et al (2012) 73.8 77.4 79.7
MV 80.5 81.6 83.7
MACE 80.4 81.7 82.6
MV+Wik 80.4 82.1 83.7
MACE+Wik 80.5 81.9 83.7
Upper bounds
oracle 82.4 83.7 85.1
gold 82.6 84.7 86.8
Table 2: POS tagging accuracies (%).
Effect on POS Tagging Accuracy Usually, we
don?t want to match a gold standard, but we
rather want to create new annotated training
data. Crowdsourcing matches our gold standard
to about 80%, but the question remains how useful
this data is when training models on it. After all,
inter-annotator agreement among professional an-
notators on this task is only around 90% (Gimpel
et al, 2011; Hovy et al, 2014). In order to evalu-
ate how much each aggregation scheme influences
tagging performance of the resulting model, we
train separate models on each scheme?s annota-
tions and test on the same four data sets. Table
2 shows the results. Note that the differences be-
tween the four schemes are insignificant. More
importantly, however, POS tagging accuracy us-
ing crowdsourced annotations are on average only
2.6% worse than gold using professional annota-
tions. On the other hand, performance is much
better than the weakly supervised approach by Li
et al (2012), which only relies on a crowdsourced
POS lexicon.
POS model from CHUNKING NER
MV 74.80 75.74
MACE 75.04 75.83
MV+Wik 75.86 76.08
MACE+Wik 75.86 76.15
Upper bounds
oracle 76.22 75.85
gold 79.97 75.81
Table 3: Downstream accuracy for chunking (l)
and NER (r) of models using POS.
Downstream Performance Table 3 shows the
accuracy when using the POS models trained
in the previous evaluation step. Note that we
present the average over the two data sets used
for each task. Note also how the Wiktionary con-
straints lead to improvements in downstream per-
formance. In chunking, we see that using the
crowdsourced annotations leads to worse perfor-
mance than using the professional annotations.
For NER, however, we find that some of the POS
taggers trained on aggregated data produce bet-
ter NER performance than POS taggers trained on
expert-annotated gold data. Since the only dif-
ference between models are the respective POS
features, the results suggest that at least for some
tasks, POS taggers learned from crowdsourced an-
notations may be as good as those learned from
expert annotations.
7 Related Work
There is considerable work in the literature on
modeling answer correctness and annotator com-
petence as latent variables (Dawid and Skene,
380
1979; Smyth et al, 1995; Carpenter, 2008; White-
hill et al, 2009; Welinder et al, 2010; Yan et al,
2010; Raykar and Yu, 2012). Rodrigues et al
(2013) recently presented a sequential model for
this. They estimate annotator competence as la-
tent variables in a CRF model using EM. They
evaluate their approach on synthetic and NER data
annotated on Mechanical Turk, showing improve-
ments over the MV baselines and the multi-label
model by Dredze et al (2009). The latter do not
model annotator reliability but rather model label
priors by integrating them into the CRF objective,
and re-estimating them during learning. Both re-
quire annotators to supply a full sentence, while
we use minimal context, which requires less anno-
tator commitment and makes the task more flexi-
ble. Unfortunately, we could not run those mod-
els on our data due to label incompatibility and
the fact that we typically do not have complete se-
quences annotated by the same annotators.
Mainzer (2011) actually presents an earlier pa-
per on crowdsourcing POS tagging. However, it
differs from our approach in several ways. It uses
the Penn Treebank tag set to annotate Wikipedia
data (which is much more canonical than Twitter)
via a Java applet. The applet automatically labels
certain categories, and only presents the users with
a series of multiple choice questions for the re-
mainder. This is highly effective, as it eliminates
some sources of possible disagreement. In con-
trast, we do not pre-label any tokens, but always
present the annotators with all labels.
8 Conclusion
We use crowdsourcing to collect POS annotations
with minimal context (five-word windows). While
the performance of POS models learned from
this data is still slightly below that of models
trained on expert annotations, models learned
from aggregations approach oracle performance
for POS tagging. In general, we find that the
use of a dictionary tends to make aggregations
more useful, irrespective of aggregation method.
For some downstream tasks, models using the
aggregated POS tags perform even better than
models using expert-annotated tags.
Acknowledgments
We would like to thank the anonymous review-
ers for valuable comments and feedback. This re-
search is funded by the ERC Starting Grant LOW-
LANDS No. 313695.
References
Chris Callison-Burch and Mark Dredze. 2010. Creat-
ing Speech and Language Data With Amazon?s Me-
chanical Turk. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk.
Bob Carpenter. 2008. Multilevel Bayesian models of
categorical data annotation. Technical report, Ling-
Pipe.
A. Philip Dawid and Allan M. Skene. 1979. Max-
imum likelihood estimation of observer error-rates
using the EM algorithm. Applied Statistics, pages
20?28.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Sta-
tistical Society. Series B (Methodological), 39(1):1?
38.
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina
Bontcheva. 2013. Twitter part-of-speech tagging
for all: overcoming sparse and noisy data. In
RANLP.
Mark Dredze, Partha Pratim Talukdar, and Koby Cram-
mer. 2009. Sequence learning from data with multi-
ple labels. In ECML/PKDD Workshop on Learning
from Multi-Label Data.
Jacob Eisenstein. 2013. What to do about bad lan-
guage on the internet. In NAACL.
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010.
Annotating named entities in Twitter data with
crowdsourcing. In NAACL-HLT 2010 Workshop on
Creating Speech and Language Data with Amazon?s
Mechanical Turk.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Josef Le Roux, Joakim Nivre, Deirde Hogan, and
Josef van Genabith. 2011. From news to comments:
Resources and benchmarks for parsing the language
of Web 2.0. In IJCNLP.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-Speech Tagging
for Twitter: Annotation, Features, and Experiments.
In ACL.
Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,
and Eduard Hovy. 2013. Learning whom to trust
with MACE. In NAACL.
Dirk Hovy, Barbara Plank, and Anders S?gaard. 2014.
When pos datasets don t add up: Combatting sample
bias. In LREC.
381
Mukund Jha, Jacob Andreas, Kapil Thadani, Sara
Rosenthal, and Kathleen McKeown. 2010. Corpus
creation for new genres: A crowdsourced approach
to pp attachment. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk. Association
for Computational Linguistics.
Mark Johnson. 2007. Why doesn?t EM find good
HMM POS-taggers. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL).
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: prob-
abilistic models for segmenting and labeling se-
quence data. In ICML.
Shen Li, Jo?ao Grac?a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In EMNLP.
Jacob Emil Mainzer. 2011. Labeling parts of
speech using untrained annotators on mechanical
turk. Master?s thesis, The Ohio State University.
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational linguistics,
20(2):155?171.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
NAACL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In LREC.
Sujith Ravi and Kevin Knight. 2009. Minimized Mod-
els for Unsupervised Part-of-Speech Tagging. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP. Association for Computational Lin-
guistics.
Vikas C. Raykar and Shipeng Yu. 2012. Eliminat-
ing Spammers and Ranking Annotators for Crowd-
sourced Labeling Tasks. Journal of Machine Learn-
ing Research, 13:491?518.
Alan Ritter, Sam Clark, Oren Etzioni, et al 2011.
Named entity recognition in tweets: an experimental
study. In EMNLP.
Filipe Rodrigues, Francisco Pereira, and Bernardete
Ribeiro. 2013. Sequence labeling with multiple an-
notators. Machine Learning, pages 1?17.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In NAACL.
Padhraic Smyth, Usama Fayyad, Mike Burl, Pietro Per-
ona, and Pierre Baldi. 1995. Inferring ground truth
from subjective labelling of Venus images. Ad-
vances in neural information processing systems,
pages 1085?1092.
Rion Snow, Brendan O?Connor, Dan Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?
Evaluating non-expert annotations for natural lan-
guage tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing. Association for Computational Linguistics.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. TACL, Mar(1):1?12.
Peter Welinder, Steve Branson, Serge Belongie, and
Pietro Perona. 2010. The multidimensional wisdom
of crowds. In NIPS.
Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob
Bergsma, and Javier Movellan. 2009. Whose vote
should count more: Optimal integration of labels
from labelers of unknown expertise. Advances in
Neural Information Processing Systems, 22:2035?
2043.
Yan Yan, R?omer Rosales, Glenn Fung, Mark Schmidt,
Gerardo Hermosillo, Luca Bogoni, Linda Moy, and
Jennifer Dy. 2010. Modeling annotator exper-
tise: Learning when everybody knows a bit of some-
thing. In International Conference on Artificial In-
telligence and Statistics.
382
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 482?487,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
How Well can We Learn
Interpretable Entity Types from Text?
Dirk Hovy
Center for Language Technology
University of Copenhagen
Njalsgade 140, 2300 Copenhagen
dirk@cst.dk
Abstract
Many NLP applications rely on type sys-
tems to represent higher-level classes.
Domain-specific ones are more informa-
tive, but have to be manually tailored to
each task and domain, making them in-
flexible and expensive. We investigate a
largely unsupervised approach to learning
interpretable, domain-specific entity types
from unlabeled text. It assumes that any
common noun in a domain can function as
potential entity type, and uses those nouns
as hidden variables in a HMM. To con-
strain training, it extracts co-occurrence
dictionaries of entities and common nouns
from the data. We evaluate the learned
types by measuring their prediction ac-
curacy for verb arguments in several do-
mains. The results suggest that it is pos-
sible to learn domain-specific entity types
from unlabeled data. We show significant
improvements over an informed baseline,
reducing the error rate by 56%.
1 Introduction
Many NLP applications, such as question answer-
ing (QA) or information extraction (IE), use type
systems to represent relevant semantic classes.
Types allow us to find similarities at a higher level
to group lexically different entities together. This
helps to filter out candidates that violate certain
constraints (e.g., in QA, if the intended answer
type is PERSON, we can ignore all candidate an-
swers with a different type), but is also used for
feature generation and fact-checking.
A central question is: where do the types
come from? Typically, they come from a hand-
constructed set. This has some disadvantages.
Domain-general types, such as named entities or
WordNet supersenses (Fellbaum, 1998), often fail
to capture critical domain-specific information (in
the medical domain, we might want ANTIBI-
OTIC, SEDATIVE, etc., rather than just ARTI-
FACT). Domain-specific types perform much bet-
ter (Ferrucci et al, 2010), but must be manually
adapted to each new domain, which is expensive.
Alternatively, unsupervised approaches (Ritter et
al., 2010) can be used to learn clusters of similar
words, but the resulting types (=cluster numbers)
are not human-interpretable, which makes analy-
sis difficult. Furthermore, it requires us to define
the number of clusters beforehand.
Ideally, we would like to learn domain-specific
types directly from data. To this end, pattern-
based approaches have long been used to induce
type systems (Hearst, 1992; Kozareva et al, 2008).
Recently, Hovy et al (2011) proposed an ap-
proach that uses co-occurrence patterns to find en-
tity type candidates, and then learns their appli-
cability to relation arguments by using them as la-
tent variables in a first-order HMM. However, they
only evaluate their method using human sensibil-
ity judgements for one domain. While this shows
that the types are coherent, it does not tell us much
about their applicability.
We extend their approach with three important
changes:
1. we evaluate the types by measuring accuracy
when using them in an extrinsic task,
2. we evaluate on more than one domain, and
3. we explore a variety of different models.
We measure prediction accuracy when us-
ing the learned types in a selectional restriction
task for frequent verbs. E.g., given the rela-
tion throw(X, pass) in the football domain, we
compare the model prediction to the gold data
X=QUARTERBACK. The results indicate that the
learned types can be used to in relation extraction
tasks.
482
Our contributions in this paper are:
? we empirically evaluate an approach to learn-
ing types from unlabeled data
? we investigate several domains and models
? the learned entity types can be used to predict
selectional restrictions with high accuracy
2 Related Work
In relation extraction, we have to identify the re-
lation elements, and then map the arguments to
types. We follow an open IE approach (Banko and
Etzioni, 2008) and use dependencies to identify
the elements. In contrast to most previous work
(Pardo et al, 2006; Yao et al, 2011; Yao et al,
2012), we have no pre-defined set of types, but
try to learn it along with the relations. Some ap-
proaches use types from general data bases such
as Wikipedia, Freebase, etc. (Yan et al, 2009;
Eichler et al, 2008; Syed and Viegas, 2010), side-
stepping the question how to construct those DBs
in the first place. We are less concerned with ex-
traction performance, but focus on the accuracy of
the learned type system by measuring how well it
performs in a prediction task.
Talukdar et al (2008) and Talukdar and Pereira
(2010) present graph-based approaches to the sim-
ilar problem of class-instance learning. While
this provides a way to discover types, it requires
a large graph that does not easily generalize to
new instances (transductive), since it produces no
predictive model. The models we use are trans-
ductive and can be applied to unseen data. Our
approach follows Hovy et al (2011). However,
they only evaluate one model on football by col-
lecting sensibility ratings from Mechanical Turk.
Our method provides extrinsic measures of perfor-
mance on several domains.
3 Model
Our goal is to find semantic type candidates in the
data, and apply them in relation extraction to see
which ones are best suited. We restrict ourselves
to verbal relations. We build on the approach by
Hovy et al (2011), which we describe briefly be-
low. It consists of two parts: extracting the type
candidates and fitting the model.
The basic idea is that semantic types are usu-
ally common nouns, often frequent ones from the
y
3
y
1
y
2
x
3
x
1
Montana throw ball
y
3
y
1
y
2
x
3
x
2
throw Montana ball
quarterback
player throw ball
throw
quarterback
player ball
a)
b)
Figure 1: Example of input sentence x and out-
put types for the HMM. Note that the verb type is
treated as observed variable.
domain at hand. Thus all common nouns are pos-
sible types, and can be used as latent variables in
an HMM. By estimating emission and transition
parameters with EM, we can learn the subset of
nouns to apply.
However, assuming the set of all common
nouns as types is intractable, and would not al-
low for efficient learning. To restrict the search
space and improve learning, we first have to learn
which types modify entities and record their co-
occurrence, and use this as dictionary.
Kleiman: professor:25, expert:13, (specialist:1)
Tilton: executive:37, economist:17, (chairman:4, presi-
dent:2)
Figure 2: Examples of dictionary entries with
counts. Types in brackets are not considered.
Dictionary Construction The number of com-
mon nouns in a domain is generally too high to
consider all of them for every entity. A com-
mon way to restrict the number of types is to pro-
vide a dictionary that lists all legal types for each
entity (Merialdo, 1994; Ravi and Knight, 2009;
T?ackstr?om et al, 2013). To construct this dictio-
nary, we collect for each entity (i.e., a sequence
of words labeled with NNP or NNPS tags) in our
data all common nouns (NN, NNS) that modify it.
These are
1. nominal modifiers (?judge Scalosi ...?),
2. appositions (?Tilton, a professor at ...?), and
3. copula constructions (?Finton, who is the in-
vestor ...?).
These modifications can be collected from the de-
pendency parse trees. For each entity, we store the
483
type candidates and their associated counts. See
Figure 2 for examples. We only consider types
observed more than 10 times. Any entity with-
out type information, as well as dictionary entities
with only singleton types are treated as unknown
tokens (?UNK?). We map UNK to the 50 most
common types in the dictionary. Verbs are con-
sidered to each have their own type, i.e., token and
label for verbs are the same.
We do not modify this step.
Original Model Hovy et al (2011) construct
a HMM using subject-verb-object (SVO) parse
triples as observations, and the type candidates as
hidden variables. Similar models have been used
in (Abney and Light, 1999; Pardo et al, 2006).
We estimate the free model parameters with EM
(Dempster et al, 1977), run for a fixed number of
iterations (30) or until convergence.
Note that Forward-backward EM has time com-
plexity of O(N
2
T ), where N is the number of
states, and T the number of time steps. T = 3 in
the model formulations used here, but N is much
larger than typically found in NLP tasks (see also
Table 3). The only way to make this tractable is
to restrict the free parameters the model needs to
estimate to the transitions.
The model is initialized by jointly normalizing
1
the dictionary counts to obtain the emission pa-
rameters, which are then fixed (except for the un-
known entities (P (word = UNK|type = ?)). Tran-
sition parameters are initialized uniformly (re-
stricted to potentially observable type sequences),
and kept as free parameters for the model to opti-
mize.
Common nouns can be both hidden variables
and observations in the model, so they act like an-
notated items: their legal types are restricted to the
identity. All entities are thus constrained by the
dictionary, as in (Merialdo, 1994). To further con-
strain the model, only the top three types of each
entity are considered. Since the type distribution
typically follows a Zipf curve, this still captures
most of the information.
1
This preserves the observed entity-specific distributions.
Under conditional normalization, the type candidates from
frequent entities tend to dominate those of infrequent entities.
I.e., the model favors an unlikely candidate for entity a if it is
frequent for entity b.
The model can be fully specified as
P (x,y) = P (y
1
)?P (x
1
|y
1
)
3
?
i=2
P (y
i
|y
i?1
)?P (x
i
|y
i
)
(1)
where x is an input triple of a verb and its argu-
ments, and y a sequence of types.
4 Extending the Model
The model used by Hovy et al (2011) was a sim-
ple first order HMM, with the elements in SVO or-
der (see Figure 3a). We observe two points: we al-
ways deal with the same number of elements, and
we have observed variables. We can thus move
from a sequential model to a general graphical
model by adding transitions and re-arranging the
structure.
Since we do not model verbs (they each have
their identity as type), they act like observed vari-
ables. We can thus move them in first position and
condition the subject on it (3b).
y
3
y
1
y
2
OVS
y
2
y
1
O
V
S
V
y
2
y
1
O
S
V
y
2
y
1
O
S
a) b)
c) d)
Figure 3: Original SVO. model (a), modified VSO
order (b), extension to general models (c and d)
By adding additional transitions, we can con-
strain the latent variables further. This is similar
to moving from a first to a second order HMM. In
contrast to the original model, we also distinguish
between unknown entities in the first and second
argument position.
The goal of these modifications is to restrict the
number of potential values for the argument po-
sitions. This allows us to use the models to type
individual instances. In contrast, the objective in
Hovy et al (2011) was to collect frequent relation
templates from a domain to populate a knowledge
base.
The modifications presented here extend to
484
Football Finances Law
system arg1 arg2 avg ?BL arg1 arg2 avg ?BL arg1 arg2 avg ?BL
baseline 0.28 0.26 0.27 ? 0.39 0.42 0.41 ? 0.37 0.32 0.35 ?
orig. 0.05 0.23 0.14 ?0.13 0.08 0.39 0.23 ?0.18 0.06 0.31 0.18 ?0.17
VSO, seq. 0.37 0.28 0.32 +0.05 0.38 0.45 0.41 0.0 0.45 0.37 0.41 +0.06
SVO, net 0.63 0.60 0.62 +0.35 0.55 0.63 0.59 +0.18 0.69 0.68 0.68 +0.33
VSO, net 0.66 0.58 0.62 +0.35 0.61 0.54 0.57 +0.16 0.71 0.62 0.66 +0.31
Table 1: Accuracy for most frequent sense baseline and different models on three domains. Italic num-
bers denote significant improvement over baseline (two-tailed t-test at p < 0.01). ?BL = difference to
baseline.
Football Finances Law
system arg1 arg2 avg arg1 arg2 avg arg1 arg2 avg
orig. 0.17 0.38 0.27 0.18 0.52 0.35 0.17 0.48 0.32
VSO, seq. 0.56 0.42 0.49 0.55 0.58 0.57 0.61 0.51 0.56
SVO, net 0.75 0.69 0.72 0.68 0.73 0.71 0.78 0.77 0.78
VSO, net 0.78 0.67 0.72 0.74 0.66 0.70 0.81 0.72 0.76
Table 2: Mean reciprocal rank for models on three domains.
verbs with more than two arguments, but in the
present paper, we focus on binary relations.
5 Experiments
Since the labels are induced dynamically from the
data, traditional precision/recall measures, which
require a known ground truth, are difficult to ob-
tain. Hovy et al (2011) measured sensibility by
obtaining human ratings and measuring weighted
accuracies over all relations. While this gives an
intuition of the general methodology, it is harder
to put in context. Here, we want to evaluate the
model?s performance in a downstream task. We
measure its ability to predict the correct types for
verbal arguments. We evaluate on three different
domains.
As test case, we use a cloze test, or fill-in-the-
blank. We select instances that contain a type-
candidate word in subject or object position and
replace that word with the unknown token. We can
then compare the model?s prediction to the origi-
nal word to measure accuracy.
5.1 Data
Like Yao et al (2012) and Hovy et al (2011), we
derive our data from the New York Times (NYT)
corpus (Sandhaus, 2008). It contains several years
worth of articles, manually annotated with meta-
data such as author, content, etc. Similar to Yao
et al (2012), we use articles whose content meta-
data field contains certain labels to distinguish data
from different domains. We use the labels Foot-
ball
2
, Law and Legislation, and Finances.
We remove meta-data and lists, tokenize, parse,
and lemmatize all articles. We then automatically
extract subject-verb-object (SVO) triples from the
parses, provided the verb is a full verb. Similarly
to (Pardo et al, 2006), we focus on the top 100
full verbs for efficiency reasons, though nothing
in our approach prevents us from extending it to
all verbs. For each domain, we select all instances
which have a potential type (common noun) in at
least one argument position. These serve as cor-
pus.
Football Finances Law
unique types 7,139 18,186 10,618
unique entities 38,282 27,528 12,782
Table 3: Statistics for the three domains.
As test data, we randomly select a subset of
1000 instances for each argument, provided they
contain one of the 50 most frequent types in sub-
ject or object position, such as player in ?player
throw pass?. This serves as gold data. We then
replace those types by UNK (i.e., we get ?UNK
throw pass?) and use this as test set for our model.
3
Table 3 shows that the domains vary with re-
2
The data likely differs from Hovy et al (2011).
3
We omit cases with two unknown arguments, since this
485
spect to the ratio of unique types to unique enti-
ties. Football uses many different entities (e.g.,
team and player names), but has few types (e.g.,
player positions), while the other domains use
more types, but fewer entities (e.g., company
names, law firms, etc.).
5.2 Evaluation
We run Viterbi decoding on each test set with our
trained model to predict the most likely type for
the unknown entities. We then compare these pre-
dictions to the type in the respective gold data and
compute the accuracy for each argument position.
As baseline, we predict the argument types most
frequently observed for the particular verb in train-
ing, e.g., predict PLAYER as subject of tackle in
football. We evaluate the influence of the different
model structures on performance.
6 Results
Table 1 shows the accuracy of the different mod-
els in the prediction task for the three different do-
mains. The low results of the informed baseline
indicate the task complexity.
We note that the original model, a bigram HMM
with SVO order (Figure 3a), fails to improve accu-
racy over the baseline (although its overall results
were judged sensible). Changing the input order
to VSO (Figure 3b) improves accuracy for both
arguments over SVO order and the baseline, albeit
not significantly. The first argument gains more,
since conditioning the subject type on the (unam-
biguous) verb is more constrained than starting out
with the subject. Conditioning the object directly
upon the subject creates sparser bigrams, which
capture ?who does what to whom?.
Moving from the HMMs to a general graphi-
cal model structure (Figures 3c and d) creates a
sparser distribution and significantly improves ac-
curacy across the board. Again, the position of the
verb makes a difference: in SVO order, accuracy
for the second argument is better, while in VSO
order, accuracy for the subject increases. This in-
dicates that direct conditioning on the verb is the
strongest predictor. Intuitively, knowing the verb
restricts the possible arguments much more than
knowing the arguments restrict the possible verbs
(the types of entities who can throw something are
becomes almost impossible to predict without further context,
even for humans (compare ?UNK make UNK?).
limited, but knowing that the subject is a quarter-
back still allows all kinds of actions).
We also compute the mean reciprocal rank
(MRR) for each condition (see Table 2). MRR de-
notes the inverse rank in the model?s k-best output
at which the correct answer occurs, i.e.,
1
k
. The
result gives us an intuition of ?how far off? the
model predictions are. Across domains, the cor-
rect answer is found on average among the top
two (rank 1.36). Note that since MRR require k-
best outputs, we cannot compute a measure for the
baseline.
7 Conclusion
We evaluated an approach to learning domain-
specific interpretable entity types from unlabeled
data. Type candidates are collected from patterns
and modeled as hidden variables in graphical mod-
els. Rather than using human sensibility judge-
ments, we evaluate prediction accuracy for selec-
tional restrictions when using the learned types in
three domains. The best model improves 35 per-
centage points over an informed baseline. On av-
erage, we reduce the error rate by 56%. We con-
clude that it is possible to learn interpretable type
systems directly from data.
Acknowledgements
The author would like to thank Victoria Fossum,
Eduard Hovy, Kevin Knight, and the anonymous
reviewers for their invaluable feedback.
References
Steven Abney and Marc Light. 1999. Hiding a seman-
tic hierarchy in a Markov model. In Proceedings
of the ACL Workshop on Unsupervised Learning in
Natural Language Processing, volume 67.
Michele Banko and Oren. Etzioni. 2008. The trade-
offs between open and traditional relation extraction.
Proceedings of ACL-08: HLT, pages 28?36.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Sta-
tistical Society. Series B (Methodological), 39(1):1?
38.
Kathrin Eichler, Holmer Hemsen, and G?unter Neu-
mann. 2008. Unsupervised relation extraction
from web documents. LREC. http://www. lrecconf.
org/proceedings/lrec2008.
Christiane Fellbaum. 1998. WordNet: an electronic
lexical database. MIT Press USA.
486
David Ferrucci, Eric Brown, Jennifer Chu-Carroll,
James Fan, David Gondek, Aditya A Kalyanpur,
Adam Lally, J William Murdock, Eric Nyberg, John
Prager, et al 2010. Building Watson: An overview
of the DeepQA project. AI magazine, 31(3):59?79.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th conference on Computational linguistics-
Volume 2, pages 539?545. Association for Compu-
tational Linguistics.
Dirk Hovy, Chunliang Zhang, Eduard Hovy, and
Anselmo Pe?nas. 2011. Unsupervised discovery of
domain-specific knowledge from text. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies, pages 1466?1475, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. Proceedings of
ACL-08: HLT, pages 1048?1056.
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational linguistics,
20(2):155?171.
Thiago Pardo, Daniel Marcu, and Maria Nunes. 2006.
Unsupervised Learning of Verb Argument Struc-
tures. Computational Linguistics and Intelligent
Text Processing, pages 59?70.
Sujith Ravi and Kevin Knight. 2009. Minimized
Models for Unsupervised Part-of-Speech Tagging.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the AFNLP, pages 504?512. Association
for Computational Linguistics.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A la-
tent dirichlet alocation method for selectional pref-
erences. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 424?434, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Evan Sandhaus, editor. 2008. The New York Times An-
notated Corpus. Number LDC2008T19. Linguistic
Data Consortium, Philadelphia.
Zareen Syed and Evelyne Viegas. 2010. A hybrid
approach to unsupervised relation discovery based
on linguistic analysis and semantic typing. In Pro-
ceedings of the NAACL HLT 2010 First Interna-
tional Workshop on Formalisms and Methodology
for Learning by Reading, pages 105?113. Associ-
ation for Computational Linguistics.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of the ACL.
Partha P. Talukdar and Fernando Pereira. 2010. Ex-
periments in graph-based semi-supervised learning
methods for class-instance acquisition. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 1473?1481.
Association for Computational Linguistics.
Partha P. Talukdar, Joseph Reisinger, Marcus Pas?ca,
Deepak Ravichandran, Rahul Bhagat, and Fernando
Pereira. 2008. Weakly-supervised acquisition of la-
beled class instances using graph random walks. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 582?
590. Association for Computational Linguistics.
Yulan Yan, Naoaki Okazaki, Yutaka Matsuo, Zhenglu
Yang, and Mitsuru Ishizuka. 2009. Unsupervised
relation extraction by mining wikipedia texts using
information from the web. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1021?1029. Association for
Computational Linguistics.
Limin Yao, Aria Haghighi, Sebastian Riedel, and An-
drew McCallum. 2011. Structured relation discov-
ery using generative models. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1456?1466. Association
for Computational Linguistics.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2012. Unsupervised relation discovery with sense
disambiguation. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics: Long Papers-Volume 1, pages 712?720.
Association for Computational Linguistics.
487
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 507?511,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Linguistically debatable or just plain wrong?
Barbara Plank, Dirk Hovy and Anders S?gaard
Center for Language Technology
University of Copenhagen, Denmark
Njalsgade 140, DK-2300 Copenhagen S
bplank@cst.dk,dirk@cst.dk,soegaard@hum.ku.dk
Abstract
In linguistic annotation projects, we typ-
ically develop annotation guidelines to
minimize disagreement. However, in this
position paper we question whether we
should actually limit the disagreements
between annotators, rather than embracing
them. We present an empirical analysis
of part-of-speech annotated data sets that
suggests that disagreements are systematic
across domains and to a certain extend also
across languages. This points to an un-
derlying ambiguity rather than random er-
rors. Moreover, a quantitative analysis of
tag confusions reveals that the majority of
disagreements are due to linguistically de-
batable cases rather than annotation errors.
Specifically, we show that even in the ab-
sence of annotation guidelines only 2% of
annotator choices are linguistically unmo-
tivated.
1 Introduction
In NLP, we often model annotation as if it re-
flected a single ground truth that was guided by
an underlying linguistic theory. If this was true,
the specific theory should be learnable from the
annotated data. However, it is well known that
there are linguistically hard cases (Zeman, 2010),
where no theory provides a clear answer, so an-
notation schemes commit to more or less arbi-
trary decisions. For example, in parsing auxil-
iary verbs may head main verbs, or vice versa,
and in part-of-speech (POS) tagging, possessive
pronouns may belong to the category of deter-
miners or the category of pronouns. This posi-
tion paper argues that annotation projects should
embrace these hard cases rather than pretend they
can be unambiguously resolved. Instead of using
overly specific annotation guidelines, designed to
minimize inter-annotator disagreement (Duffield
et al, 2007), and adjudicating between annotators
of different opinions, we should embrace system-
atic inter-annotator disagreements. To motivate
this, we present an empirical analysis showing
1. that certain inter-annotator disagreements are
systematic, and
2. that actual errors are in fact so infrequent as
to be negligible, even when linguists annotate
without guidelines.
The empirical analysis presented below relies
on text corpora annotated with syntactic cate-
gories or parts-of-speech (POS). POS is part of
most linguistic theories, but nevertheless, there
are still many linguistic constructions ? even very
frequent ones ? whose POS analysis is widely
debated. The following sentences exemplify some
of these hard cases that annotators frequently
disagree on. Note that we do not claim that both
analyses in each of these cases (1?3) are equally
good, but that there is some linguistic motivation
for either analysis in each case.
(1) Noam goes out tonight
NOUN VERB ADP/PRT ADV/NOUN
(2) Noam likes social media
NOUN VERB ADJ/NOUN NOUN
(3) Noam likes his car
NOUN VERB DET/PRON NOUN
To substantiate our claims, we first compare
the distribution of inter-annotator disagreements
across domains and languages, showing that most
disagreements are systematic (Section 2). This
suggests that most annotation differences derive
from hard cases, rather than random errors.
We then collect a corpus of such disagreements
and have experts mark which ones are due to ac-
tual annotation errors, and which ones reflect lin-
guistically hard cases (Section 3). The results
show that the majority of disagreements are due
507
to hard cases, and only about 20% of conflict-
ing annotations are actual errors. This suggests
that inter-annotator agreement scores often hide
the fact that the vast majority of annotations are
actually linguistically motivated. In our case, less
than 2% of the overall annotations are linguisti-
cally unmotivated.
Finally, in Section 4, we present an experiment
trying to learn a model to distinguish between hard
cases and annotation errors.
2 Annotator disagreements across
domains and languages
In this study, we had between 2-10 individual an-
notators with degrees in linguistics annotate dif-
ferent kinds of English text with POS tags, e.g.,
newswire text (PTB WSJ Section 00), transcripts
of spoken language (from a database containing
transcripts of conversations, Talkbank
1
), as well
as Twitter posts. Annotators were specifically not
presented with guidelines that would help them re-
solve hard cases. Moreover, we compare profes-
sional annotation to that of lay people. We in-
structed annotators to use the 12 universal POS
tags of Petrov et al (2012). We did so in or-
der to make comparison between existing data
sets possible. Moreover, this allows us to fo-
cus on really hard cases, as any debatable case in
the coarse-grained tag set is necessarily also part
of the finer-grained tag set.
2
For each domain,
we collected exactly 500 doubly-annotated sen-
tences/tweets. Besides these English data sets, we
also obtained doubly-annotated POS data from the
French Social Media Bank project (Seddah et al,
2012).
3
All data sets, except the French one, are
publicly available at http://lowlands.ku.
dk/.
We present disagreements as Hinton diagrams
in Figure 1a?c. Note that the spoken language data
does not include punctuation. The correlations
between the disagreements are highly significant,
with Spearman coefficients ranging from 0.644
1
http://talkbank.org/
2
Experiments with variation n-grams on WSJ (Dickinson
and Meurers, 2003) and the French data lead us to estimate
that the fine-to-coarse mapping of POS tags disregards about
20% of observed tag-pair confusion types, most of which re-
late to fine-grained verb and noun distinctions, e.g. past par-
ticiple versus past in ?[..] criminal lawyers speculated/VBD
vs. VBN that [..]?.
3
We mapped POS tags into the universal POS tags using
the mappings available here: https://code.google.
com/p/universal-pos-tags/
(spoken and WSJ) to 0.869 (spoken and Twit-
ter). Kendall?s ? ranges from 0.498 (Twitter and
WSJ) to 0.659 (spoken and Twitter). All diagrams
have a vaguely ?dagger?-like shape, with the blade
going down the diagonal from top left to bot-
tom right, and a slightly curved ?hilt? across the
counter-diagonal, ending in the more pronounced
ADP/PRT confusion cells.
Disagreements are very similar across all three
domains. In particular, adpositions (ADP) are con-
fused with particles (PRT) (as in the case of ?get
out?); adjectives (ADJ) are confused with nouns
(as in ?stone lion?); pronouns (PRON) are con-
fused with determiners (DET) (?my house?); nu-
merals are confused with adjectives, determiners,
and nouns (?2nd time?); and adjectives are con-
fused with adverbs (ADV) (?see you later?). In
Twitter, the X category is often confused with
punctuations, e.g., when annotating punctuation
acting as discourse continuation marker.
Our analyses show that a) experts disagree on
the known hard cases when freely annotating text,
and b) that these disagreements are the same
across text types. More surprisingly, though, we
also find that, as discussed next, c) roughly the
same disagreements are also observed when com-
paring the linguistic intuitions of lay people.
More specifically, we had lay annotators on the
crowdsourcing platform Crowdflower re-annotate
the training section of Gimpel et al (2011). They
collected five annotations per word. Only annota-
tors that had answered correctly on 4 gold items
(randomly chosen from a set of 20 gold items
provided by the authors) were allowed to submit
annotations. In total, 177 individual annotators
supplied answers. We paid annotators a reward
of $0.05 for 10 items. The full data set con-
tains 14,619 items and is described in further de-
tail in Hovy et al (2014). Annotators were satis-
fied with the task (4.5 on a scale from 1 to 5) and
felt that instructions were clear (4.4/5), and the pay
reasonable (4.1/5). The crowdsourced annotations
aggregated using majority voting agree with the
expert annotations in 79.54% of the cases. If we
pre-filter the data via Wiktionary and use an item-
response model (Hovy et al, 2013) rather than ma-
jority voting, the agreement rises to 80.58%.
Figure 2 presents the Hinton diagram of the dis-
agreements of lay people. Disagreements are very
similar to the disagreements between expert an-
notators, especially on Twitter data (Figure 1b).
508
a) b) c)
Figure 1: Hinton diagrams of inter-annotator disagreement on (a) excerpt from WSJ (Marcus et al,
1993), (b) random Twitter sample, and (c) pre-transcribed spoken language excerpts from talkbank.org
One difference is that lay people do not confuse
numerals very often, probably because they rely
more on orthographic cues than on distributional
evidence. The disagreements are still strongly cor-
related with the ones observed with expert anno-
tators, but at a slightly lower coefficient (with a
Spearman?s ? of 0.493 and Kendall?s ? of 0.366
for WSJ).
Figure 2: Disagreement between lay annotators
Lastly, we compare the disagreements of anno-
tators on a French social media data set (Seddah et
al., 2012), which we mapped to the universal POS
tag set. Again, we see the familiar dagger shape.
The Spearman coefficient with English Twitter is
0.288; Kendall?s ? is 0.204. While the correlation
is weaker across languages than across domains, it
remains statistically significant (p < 0.001).
3 Hard cases and annotation errors
In the previous section, we demonstrated that
some disagreements are consistent across domains
and languages. We noted earlier, though, that dis-
agreements can arise both from hard cases and
from annotation errors. This can explain some
Figure 3: Disagreement on French social media
of the variation. In this section, we investigate
what happens if we weed out obvious errors by
detecting annotation inconsistencies across a cor-
pus. The disagreements that remain are the truly
hard cases.
We use a modified version of the a priori algo-
rithm introduced in Dickinson and Meurers (2003)
to identify annotation inconsistencies. It works
by collecting ?variation n-grams?, i.e. the longest
sequence of words (n-gram) in a corpus that has
been observed with a token being tagged differ-
ently in another occurence of the same n-gram in
the same corpus. The algorithm starts off by look-
ing for unigrams and expands them until no longer
n-grams are found.
For each variation n-gram that we found in
WSJ-00, i.e, a word in various contexts and the
possible tags associated with it, we present anno-
tators with the cross product of contexts and tags.
Essentially, we ask for a binary decision: Is the tag
plausible for the given context?
We used 3 annotators with PhD degrees in lin-
guistics. In total, our data set contains 880 items,
509
i.e. 440 annotated confusion tag pairs. The raw
agreement was 86%. Figure 4 shows how truly
hard cases are distributed over tag pairs (dark gray
bars), as well as the proportion of confusions with
respect to a given tag pair that are truly hard cases
(light gray bars). The figure shows, for instance,
that the variation n-gram regarding ADP-ADV is
the second most frequent one (dark gray), and
approximately 70% of ADP-ADV disagreements
are linguistically hard cases (light gray). NOUN-
PRON disagreements are always linguistically de-
batable cases, while they are less frequent.
Figure 4: Relative frequency of hard cases
A survey of hard cases. To further test the idea
of there being truly hard cases that probably can-
not be resolved by linguistic theory, we presented
nine linguistics faculty members with 10 of the
above examples and asked them to pick their fa-
vorite analyses. In 8/10 cases, the faculty mem-
bers disagreed on the right analysis.
4 Learning to detect annotation errors
In this section, we examine whether we can learn
a classifier to distinguish between hard cases and
annotation errors. In order to do so, we train a clas-
sifier on the annotated data set containing 440 tag-
confusion pairs by relying only on surface form
features. If we balance the data set and perform 3-
fold cross-validation, a L2-regularized logistic re-
gression (L2-LR) model achieves an f
1
-score for
detecting errors at 70% (cf. Table 1), which is
above average, but not very impressive.
The two classes are apparently not easily sepa-
rable using surface form features, as illustrated in
f
1
HARD CASES ERRORS
L2-LR 73%(71-77) 70%(65-75)
NN 76%(76-77) 71%(68-72)
Table 1: Classification results
Figure 5: Hard cases and errors in 2d-PCA
the two-dimensional plot in Figure 5, obtained us-
ing PCA. The logistic regression decision bound-
ary is plotted as a solid, black line. This is prob-
ably also why the nearest neighbor (NN) classi-
fier does slightly better, but again, performance is
rather low. While other features may reveal that
the problem is in fact learnable, our initial experi-
ments lead us to conclude that, given the low ratio
of errors over truly hard cases, learning to detect
errors is often not worthwhile.
5 Related work
Juergens (2014) presents work on detecting lin-
guistically hard cases in the context of word
sense annotations, e.g., cases where expert an-
notators will disagree, as well as differentiat-
ing between underspecified, overspecified and
metaphoric cases. This work is similar to ours in
spirit, but considers a very different task. While
we also quantify the proportion of hard cases and
present an analysis of these cases, we also show
that disagreements are systematic.
Our work also relates to work on automatically
correcting expert annotations for inconsistencies
(Dickinson and Meurers, 2003). This work is
very different in spirit from our work, but shares
an interest in reconsidering expert annotations,
and we made use of their mining algorithm here.
There has also been recent work on adjudicat-
510
ing noisy crowdsourced annotations (Dawid and
Skene, 1979; Smyth et al, 1995; Carpenter, 2008;
Whitehill et al, 2009; Welinder et al, 2010; Yan
et al, 2010; Raykar and Yu, 2012; Hovy et al,
2013). Again, their objective is orthogonal to
ours, namely to collapse multiple annotations into
a gold standard rather than embracing disagree-
ments.
Finally, Plank et al (2014) use small samples of
doubly-annotated POS data to estimate annotator
reliability and show how those metrics can be im-
plemented in the loss function when inducing POS
taggers to reflect confidence we can put in annota-
tions. They show that not biasing the theory to-
wards a single annotator but using a cost-sensitive
learning scheme makes POS taggers more robust
and more applicable for downstream tasks.
6 Conclusion
In this paper, we show that disagreements between
professional or lay annotators are systematic and
consistent across domains and some of them are
systematic also across languages. In addition, we
present an empirical analysis of POS annotations
showing that the vast majority of inter-annotator
disagreements are competing, but valid, linguis-
tic interpretations. We propose to embrace such
disagreements rather than using annotation guide-
lines to optimize inter-annotator agreement, which
would bias our models in favor of some linguistic
theory.
Acknowledgements
We would like to thank the anonymous reviewers
for their feedback, as well as Djam?e Seddah for the
French data. This research is funded by the ERC
Starting Grant LOWLANDS No. 313695.
References
Bob Carpenter. 2008. Multilevel Bayesian models of
categorical data annotation. Technical report, Ling-
Pipe.
A. Philip Dawid and Allan M. Skene. 1979. Max-
imum likelihood estimation of observer error-rates
using the EM algorithm. Applied Statistics, pages
20?28.
Markus Dickinson and Detmar Meurers. 2003. Detect-
ing errors in part-of-speech annotation. In EACL.
Cecily Duffield, Jena Hwang, Susan Brown, Dmitriy
Dligach, Sarah Vieweg, Jenny Davis, and Martha
Palmer. 2007. Criteria for the manual grouping of
verb senses. In LAW.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-Speech Tagging
for Twitter: Annotation, Features, and Experiments.
In ACL.
Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,
and Eduard Hovy. 2013. Learning whom to trust
with MACE. In NAACL.
Dirk Hovy, Barbara Plank, and Anders S?gaard. 2014.
Experiments with crowdsourced re-annotation of a
POS tagging data set. In ACL.
David Juergens. 2014. An analysis of ambiguity in
word sense annotations. In LREC.
Mitchell Marcus, Mary Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In LREC.
Barbara Plank, Dirk Hovy, and Anders S?gaard. 2014.
Learning part-of-speech taggers with inter-annotator
agreement loss. In EACL.
Vikas C. Raykar and Shipeng Yu. 2012. Eliminat-
ing Spammers and Ranking Annotators for Crowd-
sourced Labeling Tasks. Journal of Machine Learn-
ing Research, 13:491?518.
Djam?e Seddah, Benoit Sagot, Marie Candito, Virginie
Mouilleron, and Vanessa Combet. 2012. The
French Social Media Bank: a treebank of noisy user
generated content. In COLING.
Padhraic Smyth, Usama Fayyad, Mike Burl, Pietro Per-
ona, and Pierre Baldi. 1995. Inferring ground truth
from subjective labelling of Venus images. In NIPS.
Peter Welinder, Steve Branson, Serge Belongie, and
Pietro Perona. 2010. The multidimensional wisdom
of crowds. In NIPS.
Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob
Bergsma, and Javier Movellan. 2009. Whose vote
should count more: Optimal integration of labels
from labelers of unknown expertise. In NIPS.
Yan Yan, R?omer Rosales, Glenn Fung, Mark Schmidt,
Gerardo Hermosillo, Luca Bogoni, Linda Moy, and
Jennifer Dy. 2010. Modeling annotator expertise:
Learning when everybody knows a bit of something.
In AIStats.
Daniel Zeman. 2010. Hard problems of tagset con-
version. In Proceedings of the Second International
Conference on Global Interoperability for Language
Resources.
511
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 1?11,
Dublin, Ireland, August 23-24 2014.
More or less supervised supersense tagging of Twitter
Anders Johannsen, Dirk Hovy, H
?
ector Mart??nez Alonso, Barbara Plank, Anders S?gaard
Center for Language Technology
University of Copenhagen, Denmark
Njalsgade 140
ajohannsen@hum.ku.dk, dirk@cst.dk, alonso@hum.ku.dk
plank@cst.dk, soegaard@hum.ku.dk
Abstract
We present two Twitter datasets annotated
with coarse-grained word senses (super-
senses), as well as a series of experiments
with three learning scenarios for super-
sense tagging: weakly supervised learn-
ing, as well as unsupervised and super-
vised domain adaptation. We show that
(a) off-the-shelf tools perform poorly on
Twitter, (b) models augmented with em-
beddings learned from Twitter data per-
form much better, and (c) errors can be
reduced using type-constrained inference
with distant supervision from WordNet.
1 Introduction
Supersense tagging (SST, Ciaramita and Altun,
2006) is the task of assigning high-level ontolog-
ical classes to open-class words (here, nouns and
verbs). It is thus a coarse-grained word sense dis-
ambiguation task. The labels are based on the lexi-
cographer file names for Princeton WordNet (Fell-
baum, 1998). They include 15 senses for verbs
and 26 for nouns (see Table 1). While WordNet
also provides catch-all supersenses for adjectives
and adverbs, these are grammatically, not seman-
tically motivated, and do not provide any higher-
level abstraction (recently, however, Tsvetkov et
al. (2014) proposed a semantic taxonomy for ad-
jectives). They will not be considered in this paper.
Coarse-grained categories such as supersenses
are useful for downstream tasks such as question-
answering (QA) and open relation extraction (RE).
SST is different from NER in that it has a larger set
of labels and in the absence of strong orthographic
cues (capitalization, quotation marks, etc.). More-
over, supersenses can be applied to any of the lex-
ical parts of speech and not only proper names.
Also, while high-coverage gazetteers can be found
for named entity recognition, the lexical resources
available for SST are very limited in coverage.
Twitter is a popular micro-blogging service,
which, among other things, is used for knowledge
sharing among friends and peers. Twitter posts
(tweets) announce local events, say talks or con-
certs, present facts about pop stars or program-
ming languages, or simply express the opinions of
the author on some subject matter.
Supersense tagging is relevant for Twitter, be-
cause it can aid e.g. QA and open RE. If someone
posts a message saying that some LaTeX module
now supports ?drawing trees?, it is important to
know whether the post is about drawing natural
objects such as oaks or pines, or about drawing
tree-shaped data representations.
This paper is, to the best of our knowledge, the
first work to address the problem of SST for Twit-
ter. While there exist corpora of newswire and
literary texts that are annotated with supersenses,
e.g., SEMCOR (Miller et al., 1994), no data is
available for microblogs or related domains. This
paper introduces two new data sets.
Furthermore, most, if not all, of previous work
on SST has relied on gold standard part-of-speech
(POS) tags as input. However, in a domain such
as Twitter, which has proven to be challenging
for POS tagging (Foster et al., 2011; Ritter et
al., 2011), results obtained under the assumption
of available perfect POS information are almost
meaningless for any real-life application.
In this paper, we instead use predicted POS tags
and investigate experimental settings in which one
or more of the following resources are available to
us:
? a large corpus of unlabeled Twitter data;
? Princeton WordNet (Fellbaum, 1998);
? SEMCOR (Miller et al., 1994); and
? a small corpus of Twitter data annotated with
supersenses.
We approach SST of Twitter using various de-
grees of supervision for both learning and domain
adaptation (here, from newswire to Twitter). In
1
weakly supervised learning, only unlabeled data
and the lexical resource WordNet are available to
us. While the quality of lexical resources varies,
this is the scenario for most languages. We present
an approach to weakly supervised SST based on
type-constrained EM-trained second-order HMMs
(HMM2s) with continuous word representations.
In contrast, when using supervised learning, we
can distinguish between two degrees of supervi-
sion for domain adaptation. For some languages,
e.g., Basque, English, Swedish, sense-annotated
resources exist, but these corpora are all limited
to newswire or similar domains. In such lan-
guages, unsupervised domain adaptation (DA)
techniques can be used to exploit these resources.
The setting does not presume labeled data from
the target domain. We use discriminative mod-
els for unsupervised domain adaptation, training
on SEMCOR and testing on Twitter.
Finally, we annotated data sets for Twitter, mak-
ing supervised domain adaptation (SU) exper-
iments possible. For supervised domain adapta-
tion, we use the annotated training data sets from
both the newswire and the Twitter domain, as well
as WordNet.
For both unsupervised domain adaptation and
supervised domain adaptation, we use structured
perceptron (Collins, 2002), i.e., a discriminative
HMM model, and search-based structured predic-
tion (SEARN) (Daume et al., 2009). We aug-
ment both the EM-trained HMM2, discrimina-
tive HMMs and SEARN with type constraints and
continuous word representations. We also exper-
imented with conditional random fields (Lafferty
et al., 2001), but obtained worse or similar results
than with the other models.
Contributions In this paper, we present two
Twitter data sets with manually annotated su-
persenses, as well as a series of experiments
with these data sets. These experiments cover
existing approaches to related tasks, as well as
some new methods. In particular, we present
type-constrained extensions of discriminative
HMMs and SEARN sequence models with con-
tinuous word representations that perform well.
We show that when no in-domain labeled data
is available, type constraints improve model
performance considerably. Our best models
achieve a weighted average F1 score of 57.1 over
nouns and verbs on our main evaluation data
set, i.e., a 20% error reduction over the most
frequent sense baseline. The two annotated Twit-
ter data sets are publicly released for download
at https://github.com/coastalcph/
supersense-data-twitter.
n.Tops n.object v.cognition
n.act n.person v.communication
n.animal n.phenomenon v.competition
n.artifact n.plant v.consumption
n.attribute n.possession v.contact
n.body n.process v.creation
n.cognition n.quantity v.emotion
n.communication n.relation v.motion
n.event n.shape v.perception
n.feeling n.state v.possession
n.food n.substance v.social
n.group n.time v.stative
n.location v.body v.weather
n.motive v.change
Table 1: The 41 noun and verb supersenses in
WordNet
2 More or less supervised models
This sections covers the varying degree of super-
vision of our systems as well as the usage of type
constraints as distant supervision.
2.1 Distant supervision
Distant supervision in these experiments was im-
plemented by only allowing a system to predict
a certain supersense for a given word if that su-
persense had either been observed in the training
data, or, for unobserved words, if the sense was
the most frequent sense in WordNet. If the word
did not appear in the training data nor in WordNet,
no filtering was applied. We refer to the distant-
supervision strategy as type constraints.
Distant supervision was implemented differ-
ently in SEARN and the HMM model. SEARN
decomposes sequential labelling into a series of
binary classifications. To constrain the labels we
simply pick the top-scoring sense for each token
from the allowed set. Structured perceptron uses
Viterbi decoding. Here we set the emission prob-
abilities for disallowed senses to negative infinity
and decode as usual.
2.2 Weakly supervised HMMs
The HMM2 model is a second-order hidden
Markov model (Mari et al., 1997; Thede and
Harper, 1999) using logistic regression to estimate
emission probabilities. In addition we constrain
2
w1
t
1
t
2
P(t
2
|t
1
)
P(w
1
|t
1
)
t
3
w
2
w
3
Figure 1: HMM2 with continuous word represen-
tations
the inference space of the HMM2 tagger using
type-level tag constraints derived from WordNet,
leading to roughly the model proposed by Li et
al. (2012), who used Wiktionary as a (part-of-
speech) tag dictionary. The basic feature model
of Li et al. (2012) is augmented with continuous
word representation features as shown in Figure 1,
and our logistic regression model thus works over
a combination of discrete and continuous variables
when estimating emission probabilities. We do 50
passes over the data as in Li et al. (2012).
We introduce two simplifications for the HMM2
model. First, we only use the most frequent senses
(k = 1) in WordNet as type constraints. The
most frequent senses seem to better direct the EM
search for a local optimum, and we see dramatic
drops in performance on held-out data when we
include more senses for the words covered by
WordNet. Second, motivated by computational
concerns, we only train and test on sequences of
(predicted) nouns and verbs, leaving out all other
word classes. Our supervised models performed
slightly worse on shortened sequences, and it is an
open question whether the HMM2 models would
perform better if we could train them on full sen-
tences.
2.3 Structured perceptron and SEARN
We use two approaches to supervised sequen-
tial labeling, structured perceptron (Collins, 2002)
and search-based structured prediction (SEARN)
(Daume et al., 2009). The structured perceptron
is a in-house reimplementation of Ciaramita and
Altun (2006).
1
SEARN performed slightly better
than structured perceptron, so we use it as our in-
house baseline in the experiments below. In this
section, we briefly explain the two approaches.
1
https://github.com/coastalcph/
rungsted
2.3.1 Structured perceptron (HMM)
Structured perceptron learning was introduced in
Collins (2002) and is an extension of the online
perceptron learning algorithm (Rosenblatt, 1958)
with averaging (Freund and Schapire, 1999) to
structured learning problems such as sequence la-
beling.
In structured perceptron for sequential labeling,
where we learn a function from sequences of data
points x
1
. . . x
n
to sequences of labels y
1
. . . y
n
,
we begin with a random weight vector w
0
initial-
ized to all zeros. This weight vector is used to
assign weights to transitions between labels, i.e.,
the discriminative counterpart of P (y
i+1
| y
i
), and
emissions of tokens given labels, i.e., the counter-
part of P (x
i
| y
i
). We use Viterbi decoding to de-
rive a best path
?
y through the correspondingm?n
lattice (with m the number of labels). Let the fea-
ture mapping ?(x,y) be a function from a pair
of sequences ?x,y? to all the features that fired
to make y the best path through the lattice for x.
Now the structured update for a sequence of data
points is simply ?(?(x,y)??(x,
?
y)), i.e., a fixed
positive update of features that fired to produce the
correct sequence of labels, and a fixed negative up-
date of features that fired to produce the best path
under the model. Note that if y =
?
y, no features
are updated.
2.3.2 SEARN
SEARN is a way of decomposing structured pre-
diction problems into search and history-based
classification. In sequential labeling, we decom-
pose the sequence of m tokens into m classifica-
tion problems, conditioning our labeling of the ith
token on the history of i ? 1 previous decisions.
The cost of a mislabeling at training time is de-
fined by a cost function over output structures. We
use Hamming loss rather than F
1
as our cost func-
tion, and we then use stochastic gradient descent
with quantile loss as a our cost-sensitive learning
algorithm. We use a publicly available implemen-
tation.
2
3 Experiments
We experiment with weakly supervised learning,
unsupervised domain adaptation, as well as su-
pervised domain adaptation, i.e., where our mod-
els are induced from hand-annotated newswire
and Twitter data. Note that in all our experiments,
2
http://hunch.net/
?
vw/
3
we use predicted POS tags as input to the system,
in order to produce a realistic estimate of SST per-
formance.
3.1 Data
Our experiments rely on combinations of available
resources and newly annotated Twitter data sets
made publicly available with this paper.
3.1.1 Available resources
Princeton WordNet (Fellbaum, 1998) is the main
resource for SST. The lexicographer file names
provide the label alphabet of the task, and the tax-
onomy defined therein is used not only in the base-
lines, but also as a feature in the discriminative
models. We use the WordNet 3.0 distribution.
SEMCOR (Miller et al., 1994) is a sense-
annotated corpus composed of 80% newswire and
20% literary text, using the sense inventory from
WordNet. SEMCOR comprises 23k distinct lem-
mas in 234k instances. We use the texts which
have full annotations, leaving aside the verb-only
texts (see Section 6).
We use a distributional semantic model in order
to incorporate distributional information as fea-
tures in our system. In particular, we use the
neural-network based models from (Mikolov et
al., 2013), also referred as word embeddings. This
model makes use of skip-grams (n-grams that do
not need to be consecutive) within a word window
to calculate continuous-valued vector representa-
tions from a recurrent neural network. These dis-
tributional models have been able to outperform
state of the art in the SemEval-2012 Task 2 (Mea-
suring degrees of relational similarity). We calcu-
late the embeddings from an in-house corpus of
57m English tweets using a window size 5 and
yielding vectors of 100 dimensions.
We also use the first 20k tweets of the 57m
tweets to train our HMM2 models.
3.1.2 Annotation
While an annotated newswire corpus and a high-
quality lexical resource already enable us to train,
we also need at least a small sample of anno-
tated tweets data to evaluate SST for Twitter. Fur-
thermore, if we want to experiment with super-
vised SST, we also need sufficient annotated Twit-
ter data to learn the distribution of sense tags.
This paper presents two data sets: (a) super-
sense annotations for the POS+NER-annotated
data set described in Ritter et al. (2011), which we
use for training, development and evaluation, us-
ing the splits proposed in Derczynski et al. (2013),
and (b) supersense annotations for a sample of 200
tweets, which we use for additional, out-of-sample
evaluation. We call these data sets RITTER-
{TRAIN,DEV,EVAL} and IN-HOUSE-EVAL, re-
spectively. The IN-HOUSE-EVAL dataset was
downloaded in 2013 and is a sample of tweets that
contain links to external homepages but are other-
wise unbiased. It was previously used (with part-
of-speech annotation) in (Plank et al., 2014). Both
data sets are made publicly available with this pa-
per.
Supersenses are annotated with in spans defined
by the BIO (Begin-Inside-Other) notation. To ob-
tain the Twitter data sets, we carried out an an-
notation task. We first pre-annotated all data sets
with WordNet?s most frequent senses. If the word
was not in WordNet and a noun, we assigned it the
sense n.person. All other words were labeled O.
Chains of nouns were altered to give every ele-
ment the sense of the head noun, and the BI tags
adjusted, i.e.:
Empire/B-n.loc State/B-n.loc Building/B-n.artifact
was changed to
Empire/B-n.artifact State/I-n.artifact Building/I-
n.artifact
For the RITTER data, three paid student an-
notators worked on different subsets of the pre-
annotated data. They were asked to correct mis-
takes in both the BIO notation and the assigned
supersenses. They were free to chose from the full
label set, regardless of the pre-annotation. While
the three annotators worked on separate parts, they
overlapped on a small part of RITTER-TRAIN (841
tokens). On this subset, we computed agreement
scores and annotation difficulties. The average
raw agreement was 0.86 and Cohen?s ? 0.77. The
majority of tokens received the O label by all an-
notators; this happended in 515 out of 841 cases.
Excluding these instances to evaluate the perfor-
mance on the more difficult content words, raw
agreement dropped to 0.69 and Cohen?s ? to 0.69.
The IN-HOUSE-EVAL data set was annotated
by two different annotators, namely two of the au-
thors of this article. Again, for efficiency reasons
they worked on different subsets of the data, with
an overlapping portion. Their average raw agree-
ment was 0.65 and their Cohen?s ? 0.62. For this
data set, we also compute F
1
, defined as usual as
the harmonic mean of recall and precision. To
4
compute this, we set one of the annotators as gold
data and the other as predicted data. However,
since F
1
is symmetrical, the order does not mat-
ter. The annotation F
1
gives us another estimate
of annotation difficulty. We present the figures in
Table 3.
3.2 Baselines
For most word sense disambiguation studies, pre-
dicting the most frequent sense (MFS) of a word
has been proven to be a strong baseline. Follow-
ing this, our MFS baseline simply predicts the su-
persense of the most frequent WordNet sense for
a tuple of a word and a part of speech. We use
the part of speech predicted by the LAPOS tagger
(Tsuruoka et al., 2011). Any word not in Word-
Net is labeled as noun.person, which is the most
frequent sense overall in the training data. After
tagging, we run a script to correct the BI tag pre-
fixes, as described above for the annotation ask.
We also compare to the performance of exist-
ing SST systems. In particular we use Sense-
Learner (Mihalcea and Csomai, 2005) as a base-
line, which produces estimates of the WordNet
sense for each word. For these predictions, we
retrieve the corresponding supersense. Finally,
we use a publicly available reimplementation of
Ciaramita and Altun (2006) by Michael Heilman,
which reaches comparable performance on gold-
tagged SEMCOR.
3
3.3 Model parameters
We use the feature model of Paa? and Reichartz
(2009) in all our models, except the weakly su-
pervised models. For the structured perceptron we
set the number of passes over the training data on
the held-out development data. The weakly super-
vised models use the default setting proposed in
Li et al. (2012). We have used the standard online
setup for SEARN, which only takes one pass over
the data.
The type of embedding is the same in all our
experiments. For a given word the embedding fea-
ture is a 100 dimensional vector, which combines
the embedding of the word with the embedding of
adjacent words. The feature combination f
e
for a
word w
t
is calculated as:
f
e
(w
t
) =
1
2
(e(w
t?1
) + e(w
t+1
))? 2e(w
t
),
3
http://www.ark.cs.cmu.edu/mheilman/
questions/SupersenseTagger-10-01-12.tar.
gz
where the factor of two is chosen heurestically to
give more weight to the current word.
We also set a parameter k on development data
for using the k-most frequent senses inWordNet
as type constraints. Our supervised models are
trained on SEMCOR+RITTER-TRAIN or simply
RITTER-TRAIN, depending on what gave us the
best performance on the held-out data.
4 Results
The results are presented in Table 2. We dis-
tinguish between three settings with various de-
grees of supervision: weakly supervised, which
uses no domain annotated information, but solely
relies on embeddings trained on unlabeled Twit-
ter data; unsupervised domain adaptation (DA),
which uses SemCor for supervised training; and
supervised domain adaptation (SU), which uses
annotated Twitter data in addition to the SemCor
data for training.
In each of the two domain adaptation settings,
SEARN and HMM are evaluated with type con-
straints as distant supervision, and without for
comparison. SEARN without embeddings or dis-
tant supervision serves as an in-house baseline.
In Table 3 we present the WordNet token cov-
erage of predicted nouns and verbs in the devel-
opment and evaluation data, as well as the inter-
annotator agreement F
1
scores.
All the results presented in Table 2 are
(weighted averaged) F
1
measures obtained on pre-
dicted POS tags. Note that these results are con-
siderably lower than results on supersense tagging
newswire (up to 80 F
1
) that assume gold standard
POS tags (Ciaramita and Altun, 2006; Paa? and
Reichartz, 2009).
The re-implementation of the state-of-the-art
system improves slightly upon the most frequent
sense baseline. SenseLearner does not seem to
capture the relevant information and does not
reach baseline performance. In other words, there
is no off-the-shelf tool for supersense tagging of
Twitter that does much better than assigning the
most frequent sense to predicted nouns and verbs.
Our weakly supervised model performs worse
than the most frequent sense baseline. This is a
negative result. It is, however, well-known from
the word sense disambiguation literature that the
MFS is a very strong baseline. Moreover, the EM
learning problem is hard because of the large la-
bel set and weak distributional evidence for super-
5
RITTER IN-HOUSE
DEV EVAL EVAL
Wordnet noun-verb
token coverage 83.72 70.22 41.18
Inter-annotator
agreement (F1) 81.01 69.15 61.57
Table 3: Properties of dataset.
senses.
The unsupervised domain adaptation and fully
supervised systems perform considerably better
than this baseline across the board. In the unsuper-
vised domain adaptation setup, we see huge im-
provements from using type constraints as distant
supervision. In the supervised setup, we only see
significant improvements adding type constraints
for the structured perceptron (HMM), but not for
search-based structured prediction (SEARN).
For all the data sets, there is still a gap between
model performance and human inter-annotator
agreement levels (see Table 3), leaving some room
for improvements. We hope that the release of the
data sets will help further research into this.
4.1 Coarse-grained evaluation
We also experimented with the more coarse-
grained classes proposed by Yuret and Yatbaz
(2010). Here our best model obtained an F
1
score
for mental concepts (nouns) of 72.3%, and 62.6%
for physical concepts, on RITTER-DEV. The over-
all F
1
score for verbs is 85.6%. The overall F
1
is
75.5%. Note that this result is not directly com-
parable to the figure (72.9%) reported in Yuret
and Yatbaz (2010), since they use different data
sets, exclude verbs and make different assump-
tions, e.g., relying on gold POS tags.
5 Error analysis
We have seen that inter-annotator agreements on
supersense annotation are reliable at above .60
but far from perfect. The Hinton diagram in Ta-
ble 2 presents the confusion matrix between our
annotators on IN-HOUSE-EVAL.
Errors in the prediction primarily stem from
two sources: out-of-vocabulary words and incor-
rect POS tags. Figure 3 shows the distribution of
senses over the words that were not contained in
either the training data, WordNet, or the Twitter
data used to learn the embeddings. The distribu-
tion follows a power law, with the most frequent
sense being noun.person, followed by noun.group,
and noun.artifact. The first two are related to NER
categories, namely PER and ORG, and can be ex-
pected, since Twitter users frequently talk about
new actors, musicians, and bands. Nouns of com-
munication are largely related to films, but also in-
clude Twitter, Facebook, and other forms of social
media. Note that verbs occur only towards the tail
end of the distribution, i.e., there are very few un-
known verbs, even in Twitter.
Overall, our models perform best on labels with
low lexical variability, such as quantities, states
and times for nouns, as well as consumption, pos-
session and stative for verbs. This is unsurprising,
since these classes have lower out-of-vocabulary
rates.
With regards to the differences between source
(SEMCOR) and target (Twitter) domains, we ob-
serve that the distribution of supersenses is al-
ways headed by the same noun categories like
noun.person or noun.group, but the frequency of
out-of-vocabulary stative verbs plummets in the
target domain, as some semantic types are more
closed class than others. There are for instance
fewer possibilities for creating new time units
(noun.time) or stative verbs like be than people or
company names (noun.person or noun.group, re-
spectively).
The weakly supervised model HMM2 has
higher precision (57% on RITTER-DEV) than re-
call (48.7%), which means that it often predicts
words to not belong to a semantic class. This
suggests an alternative strategy, which is to train
a model on sequences of purely non-O instances.
This would force the model to only predict O on
words that do not appear in the reduced sequences.
One important source of error seems to be un-
reliable part-of-speech tagging. In particular we
predict the wrong POS for 20-35% of the verbs
across the data sets, and for 4-6.5% of the nouns.
In the SEMCOR data, for comparability, we have
wrongly predicted tags for 6-8% of the anno-
tated tokens. Nevertheless, the error propaga-
tion of wrongly predicted nouns and verbs is par-
tially compensated by our systems, since they are
trained on imperfect input, and thus it becomes
possible for the systems to predict a noun super-
sense for a verb and viceversa. In our data we have
found e.g. that the noun Thanksgiving was incor-
rectly tagged as a verb, but its supersense was cor-
rectly predicted to be noun.time, and that the verb
guess had been mistagged as noun but the system
6
Resources Results
Token-level Type-level RITTER IN-HOUSE
SemCor Twitter Embeddings Type constraints DEV EVAL EVAL
General baselines
MFS - - - + 47.54 44.98 38.65
SENSELEARNER + - - - 14.61 26.24 22.81
HEILMAN + - - - 48.96 45.03 39.65
Weakly supervised systems
HMM2 - - - + 47.09 42.12 26.99
Unsupervised domain adaptation systems (DA)
SEARN (Baseline) + - - - 48.31 42.34 34.30
SEARN + - + - 52.45 48.30 40.22
SEARN + - + + 56.59 50.89 40.50
HMM + - + - 52.40 47.90 40.51
HMM + - + + 57.14 50.98 41.84
Supervised domain adaptation systems (SU)
SEARN (Baseline) + + - - 58.30 52.12 36.86
SEARN + + + - 63.05 57.09 42.37
SEARN + + + + 62.72 57.14 42.42
HMM + + + - 57.20 49.26 39.88
HMM + + + + 60.66 51.40 41.60
Table 2: Weighted F1 average over 41 supersenses.
7
Figure 2: Inter-annotator confusion matrix on TWITTER-EVAL.
0
0.1
0.2
0.3
0.4
noun.
person noun.
group
noun.
artifac
t
noun.
comm
unicat
ion
noun.
event
noun.
locatio
n
noun.
time noun.
act
noun.
food
noun.
attribu
te
noun.
relatio
n
verb.c
ogniti
on
verb.c
reatio
n
verb.e
motio
n
verb.m
otion
verb.p
ercept
ion
verb.s
tative
Figure 3: Sense distribution of OOV words.
8
still predicted the correct verb.cognition as super-
sense.
6 Related Work
There has been relatively little previous work on
supersense tagging, and to the best of our knowl-
edge, all of it has been limited to English newswire
and literature (SEMCOR and SENSEVAL).
The task of supersense tagging was first intro-
duced by Ciaramita and Altun (2006), who used
a structured perceptron trained and evaluated on
SEMCOR via 5-fold cross validation. Their eval-
uation included a held-out development set on
each fold that was used to estimate the number of
epochs. They used additional training data con-
taining only verbs. More importantly, they relied
on gold standard POS tags. Their overall F
1
score
on SEMCOR was 77.1. Reichartz and Paa? (Re-
ichartz and Paa?, 2008; Paa? and Reichartz, 2009)
extended this work, using a CRF model as well
as LDA topic features. They report an F
1
score
of 80.2, again relying on gold standard POS fea-
tures. Our implementation follows their setup and
feature model, but we rely on predicted POS fea-
tures, not gold standard features.
Supersenses provide information similar to
higher-level distributional clusters, but more in-
terpretable, and have thus been used as high-
level features in various tasks, such as preposi-
tion sense disambiguation, noun compound inter-
pretation, and metaphor detection (Ye and Bald-
win, 2007; Tratz and Hovy, 2010; Tsvetkov et al.,
2013). Princeton WordNet only provides a fully
developed taxonomy of supersenses for verbs and
nouns, but Tsvetkov et al. (2014) have recently
proposed an extension of the taxonomy to cover
adjectives. Outside of English, supersenses have
been annotated for Arabic Wikipedia articles by
Schneider et al. (2012).
In addition, a few researchers have tried to
solve coarse-grained word sense disambiguation
problems that are very similar to supersense tag-
ging. Kohomban and Lee (2005) and Kohom-
ban and Lee (2007) also propose to use lexicogra-
pher file identifers from Princeton WordNet senses
(supersenses) and, in addition, discuss how to re-
trieve fine-grained senses from those predictions.
They evaluate their model on all-words data from
SENSEEVAL-2 and SENSEEVAL-3. They use a
classification approach rather than structured pre-
diction.
Yuret and Yatbaz (2010) present a weakly unsu-
pervised approach to this problem, still evaluating
on SENSEVAL-2 and SENSEVAL-3. They focus
only on nouns, relying on gold part-of-speech, but
also experiment with a coarse-grained mapping,
using only three high level classes.
For Twitter, we are aware of little previous work
on word sense disambiguation. Gella et al. (2014)
present lexical sample word sense disambiguation
annotation of 20 target nouns on Twitter, but no
experimental results with this data. There has also
been related work on disambiguation to Wikipedia
for Twitter (Cassidy et al., 2012).
In sum, existing work on supersense tagging
and coarse-grained word sense disambiguation for
English has to the best of our knowledge all fo-
cused on newswire and literature. Moreover, they
all rely on gold standard POS information, making
previous performance estimates rather optimistic.
7 Conclusion
In this paper, we present two Twitter data sets with
manually annotated supersenses, as well as a se-
ries of experiments with these data sets. The data
is publicly available for download.
In this article we have provided, to the best
of our knowledge, the first supersense tagger for
Twitter. We have shown that off-the-shelf tools
perform poorly on Twitter, and we offer two
strategies?namely distant supervision and the us-
age of embeddings as features?that can be com-
bined to improve SST for Twitter.
We propose that distant supervision imple-
mented as type constraints during decoding is a
viable method to limit the mispredictions of su-
persenses by our systems, thereby enforcing pre-
dicted senses that a word has in WordNet. This ap-
proach compensates for the size limitations of the
training data and mitigates the out-of-vocabulary
effect, but is still subject to the coverage of Word-
Net; which is far from perfect for words coming
from high-variability sources such as Twitter.
Using distributional semantics as features in
form of word embeddings also improves the pre-
diction of supersenses, because it provides seman-
tic information for words, regardless of whether
they have been observed the training data. This
method does not require a hand-created knowl-
edge base like WordNet, and is a promising tech-
nique for domain adaptation of supersense tag-
ging.
9
References
Taylor Cassidy, Heng Ji, Lev-Arie Ratinov, Arkaitz Zu-
biaga, and Hongzhao Huang. 2012. Analysis and
enhancement of wikification for microblogs with
context expansion. In COLING, volume 12, pages
441?456.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger. In
Proc. of EMNLP, pages 594?602, Sydney, Australia,
July.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP.
Hal Daume, John Langford, and Daniel Marcu. 2009.
Search-based structured prediction. Machine Learn-
ing, pages 297?325.
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina
Bontcheva. 2013. Twitter part-of-speech tagging
for all: overcoming sparse and noisy data. In
RANLP.
Christiane Fellbaum. 1998. WordNet: an electronic
lexical database. MIT Press USA.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Josef Le Roux, Joakim Nivre, Deirde Hogan, and
Josef van Genabith. 2011. From news to comments:
Resources and benchmarks for parsing the language
of Web 2.0. In IJCNLP.
Yoav Freund and Robert Schapire. 1999. Large margin
classification using the perceptron algorithm. Ma-
chine Learning, 37:277?296.
Spandana Gella, Paul Cook, and Timothy Baldwin.
2014. One sense per tweeter and other lexical se-
mantic tales of Twitter. In EACL.
Upali Kohomban and Wee Lee. 2005. Learning se-
mantic classes for word sense disambiguation. In
ACL.
Upali Kohomban and Wee Lee. 2007. Optimizing
classifier performance in word sense disambiguation
by redefining word sense classes. In IJCAI.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: prob-
abilistic models for segmenting and labeling se-
quence data. In ICML.
Shen Li, Jo?ao Grac?a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In EMNLP.
Jean-Francois Mari, Jean-Paul Haton, and Abdelaziz
Kriouile. 1997. Automatic word recognition based
on second-order hidden Markov models. IEEE
Transactions on Speech and Audio Processing,
5(1):22?25.
Rada Mihalcea and Andras Csomai. 2005. Sense-
learner: Word sense disambiguation for all words in
unrestricted text. In Proceedings of the ACL 2005
on Interactive poster and demonstration sessions,
pages 53?56. Association for Computational Lin-
guistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory
Corrado, and Jeffrey Dean. 2013. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In NIPS.
George A. Miller, Martin Chodorow, Shari Landes,
Claudia Leacock, and Robert G. Thomas. 1994.
Using a semantic concordance for sense identifica-
tion. In Proceedings of the workshop on Human
Language Technology, pages 240?243. Association
for Computational Linguistics.
Gerhard Paa? and Frank Reichartz. 2009. Exploit-
ing semantic constraints for estimating supersenses
with CRFs. In Proc. of the Ninth SIAM Interna-
tional Conference on Data Mining, pages 485?496,
Sparks, Nevada, May.
Barbara Plank, Dirk Hovy, and Anders S?gaard. 2014.
Learning part-of-speech taggers with inter-annotator
agreement loss. In Proceedings of EACL.
Frank Reichartz and Gerhard Paa?. 2008. Estimating
Supersenses with Conditional Random Fields. In
Proceedings of ECMLPKDD.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: an ex-
perimental study. In EMNLP.
Frank Rosenblatt. 1958. The perceptron: a probabilis-
tic model for information storage and organization
in the brain. Psychological Review, 65(6):386?408.
Nathan Schneider, Behrang Mohit, Kemal Oflazer, and
Noah A Smith. 2012. Coarse lexical semantic an-
notation with supersenses: an arabic case study. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics, pages 253?
258. Association for Computational Linguistics.
Scott Thede and Mary Harper. 1999. A second-order
hidden Markov model for part-of-speech tagging. In
ACL.
Stephen Tratz and Eduard Hovy. 2010. Isi: automatic
classification of relations between nominals using a
maximum entropy classifier. In Proceedings of the
5th International Workshop on Semantic Evaluation,
pages 222?225. Association for Computational Lin-
guistics.
Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichi
Kazama. 2011. Learning with lookahead: can
history-based models rival globally optimized mod-
els? In CoNLL.
10
Yulia Tsvetkov, Elena Mukomel, and Anatole Gersh-
man. 2013. Cross-lingual metaphor detection us-
ing common semantic features. Meta4NLP 2013,
page 45.
Yulia Tsvetkov, Nathan Schneider, Dirk Hovy, Archna
Bhatia, Manaal Faruqui, and Chris Dyer. 2014.
Augmenting english adjective senses with super-
senses. In Proc. of LREC.
Patrick Ye and Timothy Baldwin. 2007. Melb-yb:
Preposition sense disambiguation using rich seman-
tic features. In Proceedings of the 4th International
Workshop on Semantic Evaluations, pages 241?244.
Association for Computational Linguistics.
Deniz Yuret and Mehmet Yatbaz. 2010. The noisy
channel model for unsupervised word sense disam-
biguation. Computational Linguistics, 36:111?127.
11
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 213?217,
Dublin, Ireland, August 23-24, 2014.
Copenhagen-Malm
?
o: Tree Approximations of Semantic Parsing Problems
Natalie Schluter
?
, Jakob Elming, Sigrid Klerke, H
?
ector Mart??nez Alonso, Dirk Hovy
Barbara Plank, Anders Johannsen, and Anders S?gaard
?
Dpt. of Computer Science Center for Language Technology
Malm?o University University of Copenhagen
natalie.schluter@mah.se {zmk867,skl,alonso}@hum.ku.dk
{dirk,bplank}@cst.dk,
{ajohannsen,soegaard}@hum.ku.dk
Abstract
In this shared task paper for SemEval-
2014 Task 8, we show that most se-
mantic structures can be approximated by
trees through a series of almost bijective
graph transformations. We transform in-
put graphs, apply off-the-shelf methods
from syntactic parsing on the resulting
trees, and retrieve output graphs. Us-
ing tree approximations, we obtain good
results across three semantic formalisms,
with a 15.9% error reduction over a state-
of-the-art semantic role labeling system on
development data. Our system came in 3/6
in the shared task closed track.
1 Introduction
Semantic analyses often go beyond tree-
structured representations, assigning multiple se-
mantic heads to nodes, some semantic formalisms
even tolerating directed cycles.
1
At the same
time, syntactic parsing is a mature field with effi-
cient, highly optimised decoding and learning al-
gorithms for tree-structured representations. We
present tree approximation algorithms that in com-
bination with a state-of-the-art syntactic parser
achieve competitive performance in semantic di-
graph parsing.
We investigate two kinds of tree approximation
algorithms that we will refer to as pruning algo-
rithms and packing algorithms. Our pruning al-
gorithms simply remove and reverse edges until
the graph is a tree; edge reversals are then undone
as a postprocessing step. Our packing algorithms,
on the other hand, carry out two bijective graph
This work is licenced under a Creative Commons Attribu-
tion 4.0 International License. Page numbers and proceed-
ings footer are added by the organizers. License details:
http://creativecommons.org/licenses/by/4.0/
1
For example, HPSG predicate-argument structures (Pol-
lard and Sag, 1994).
transformations to pack structural information into
new edge labels, making it possible to reconstruct
most of the structural complexity as a postprocess-
ing step. Specifically, we present a packing al-
gorithm that consists of two fully bijective graph
transformations, in addition to a further transfor-
mation that incurs only a small information loss.
We carry out experiments across three seman-
tic annotations of the Wall Street Journal section
of the Penn Treebank (Marcus et al., 1993), cor-
responding to simplified versions of the semantic
formalisms minimal recursion semantics (MRS)
(Copestake et al., 2005), Enju-style predicate-
argument structures (Miyao and Tsujii, 2003), and
Prague-style tectogrammar semantics (B?ohmov?a
et al., 2003). We show that pruning and pack-
ing algorithms lead to state-of-the-art performance
across these semantic formalisms using an off-the-
shelf syntactic dependency parser.
2 Related work
Sagae and Tsujii (2008) present a pruning algo-
rithm in their paper on transition-based parsing of
directed acyclic graphs (DAGs), which discards
the edges of longest span entering nodes. They
apply the dependency parser described in Sagae
and Tsujii (2007) to the tree representations. We
note that this algorithm is not sufficient to produce
trees in our case, where the input graphs are not
necessarily acyclic. It does correspond roughly to
our LONGEST-EDGE baseline, which removes the
longest edge in cycles, in addition to flow reversal.
Sagae and Tsujii (2008) also present a shift-
reduce automaton approach to parsing DAGs. In
their paper, they report a labeled F1-score of
88.7% on the PAS dataset (see Section 3), while
we obtain 89.1%, however the results are thus not
directly comparable due to different data splits.
2
2
We obtained code to run this as a baseline, but were un-
able to, due to memory leaks, caused by subsets of our data,
and on the subsets of data that actually parsed, recall was very
213
The shared task organizers of the Broad-
coverage Semantic Dependency Parsing task at
SemEval-2014
3
also presented a pruning-based
baseline system. They eliminate re-entrancies in
the graph by removing dependencies to nodes with
multiple incoming edges. Of these edges, they
again keep the shortest. They incorporate all sin-
gleton nodes by attaching nodes to the immedi-
ately following node or to a virtual root - in case
the singleton is sentence-final. Finally, they inte-
grate fragments by subordinating remaining nodes
with in-degree 0 to the root node. They apply the
parser described in Bohnet (2010), also used be-
low, to the resulting trees. This system obtained
a labeled F1-score of 54.7% on the PAS dataset.
The performance of their pruning algorithm was
also considerably lower than our algorithms on the
other datasets considered below.
3 Tree approximations
This section describes two approaches to approxi-
mating graphs by trees, namely pruning and pack-
ing. Pruning optimizes the number of ?good?
edges in trees (Section 3.1), while packing trans-
forms graphs into trees by means of a pipeline of
operations which are 99.6% reversible (see Fig-
ure 1); that is, almost no information from the
original graphs is lost in the trees (Section 3.2).
Under both approaches, we first introduce arti-
ficial root nodes to the graphs and append them
to the word list. Graphs may initially be discon-
nected. We connect all weakly connected com-
ponents as follows. We first identify a most im-
portant node in each weakly connected compo-
nent, which we will refer to as the root. This root
is taken to be the first node with the ?top? fea-
ture from the data, if one exists. If none exists,
then the node with highest degree is chosen as the
?root?. (Note that the ?root? of each non-singleton
connected component is marked as a ?top? node
in the inverse transformation.) The root of each
non-singleton weakly connected component is at-
tached as a dependent of the artificial root node
with a special new label for the corresponding
edge. Also, each disconnected node is attached
as a dependent of the node to the right of it, with
a distinct special new label. It is these connected
graphs that we take to be the input in the following
low, suggesting that maybe the decoding algorithm was tuned
to a specific planarization of the complex graphs.
3
http://alt.qcri.org/semeval2014/task8/
two subsections describing our graph pruning and
packing algorithms.
3.1 Graph pruning
Our PRUNING algorithm removes a small number
of edges in the semantic graphs to be able to repre-
sent them as trees. The average edge counts from
the training data (see Section 4.1) indicate that the
potential edge loss in pruning is relatively small
(5.7% in the worst case). In this approach, two
transformations on the connected semantic graphs
are carried out: pruning and flow reversal.
Pruning. The input digraph may contain under-
lying undirected cycles. We break these cycles
by iteratively removing the longest edge from the
node with the fewest predecessors (lowest depth)
in the digraph. The resulting underlying undi-
rected graph is a tree.
Depth-first flow reversal. We then carry out
depth-first traversal of the resulting underlying
undirected tree, reversing the direction of edges
from the leaves upwards, as needed, until reach-
ing the root. Any reversed edge?s label is given a
special prefix, so that this reversal can be undone
in a post-processing step.
Following the above two transformations, we train
our parsers on the transformed semantic annota-
tions and output graphs such as the one in Fig-
ure 1a.
3.2 Graph packing
Our PACKING algorithm consists of a pipeline of
four graph transformations. The two major trans-
formations are for coordination and generalised
long-distance dependencies, being both parallel
path inducing constructions. The transformations
are both linguistically and topologically inspired
by the f-structure annotated c-structures in Lex-
ical Functional Grammar and f-structure parsing
via off-the-shelf dependency parsers (Schluter and
Van Genabith, 2009). We further ensure the defin-
ing tree property that every node is connected by a
unique path from the root, by carrying out flow re-
versal when necessary. Finally remaining parallel
paths are broken according to an heuristic on path
locality.
Coordination. In some semantic representa-
tions of coordination, individual conjunct nodes
may all dominate a same argument, or be domi-
nated by a same head. In both these cases, paral-
lel paths are generated. The same structures may
214
a)
b)
c)
Figure 1: Example of pruned (top), packed (middle), and original (bottom) semantic graph. (Sentence
22002004 from the PAS dataset.)
be represented if the head or arguments are ?fac-
tored out?. To do this, we remove all edges from
conjuncts towards a same argument (resp. from
a shared head to each conjunct), and introduce a
new edge from the root of the coordination sub-
tree towards this argument (resp. from a shared
head to the root of the coordination subtree). The
new edges receive a special prefix to facilitate ap-
plying the inverse transformation.
Breadth-first flow reversal. Unlike our pruning
algorithm, there is not yet any clear distinct path
from the root to the all nodes (as there are not
leaves yet). After carrying out the coordination
transformation, we carry out a breadth-first search
on the graph to direct flow away from the root, and
again, reversed edges? labels are given a special
prefix. As we do this, we test resulting nodes to
see if there are any parallel paths leading to them.
If so, these paths may be transformed immediately
according to the following transformation.
Generalized long-distance dependencies.
Long-distance dependencies are represented
in f-structure annotated c-structures by path
equations. This gives a tree representation of
parallel paths, at least one of which is exactly
one edge long. Given two parallel paths p
1
and
p
2
in the graph, where p
1
= (v
1
, l, v
n
) and p
2
=
(v
1
, l
1
, v
2
), (v
2
, l
2
, v
3
), . . . , (v
n?1
, l
n?1
, v
n
), we
remove the last edge of p
2
and augment p
1
?s label
with the representation l
1
: l
2
: ? ? ? : l
n?1
of p
2
. p
1
becomes (v
1
, l and l
1
: l
2
: ? ? ? : l
n?1
, v
n
), indi-
cating that v
n
is also the child (with dependency
label l
n?1
) of the node found by travelling (from
v
1
) down an l
1
labelled edge, followed by an l
2
labelled edge, and so on until the child of the l
n?2
labelled edge is found.
Maximum average locality heuristic. Follow-
ing these transformations, there may still be paral-
lel paths in the graph: those not parallel to a single
edge. We remove ?worst? re-entrant edges using
the simple heuristic that the path with the lowest
average edge span should be conserved entirely.
These removed edges clearly cannot be recovered
after transformation.
Our parsers are trained on the output graphs of
these four transformations such as the one in Fig-
ure 1b. We observe the main difference between
PRUNING and PACKING: coordination and long-
distance dependencies. For example, PACKING
keeps the edge between the conjunction and the
first conjunct, which is pruned away in PRUNING.
Such a difference provides a partial explanation
for the lower recall of PRUNING vis-`a-vis PACK-
ING (see Section 4.5).
4 Experiments
4.1 Data
The three datasets are semantic annotations of the
WSJ section of the Penn Treebank of English. The
average sentence length, which is also the aver-
age number of dependency edges in the tree ap-
proximations that we use to induce our semantic
parsers, is 22.93. The three semantic formalisms
are slightly richer, and the average number of
edges in the PAS-annotated treebank is 24.32. For
DM, the average number of edges is 23.77, and
for DM it is 23.33. While the pruning-based ap-
proaches thus suffers from a modest information
loss, throwing out 5.7% of the edges in the worst
215
case, this is not the case for packing. The re-
versibility of the packed representations is given
by the score upper bound in the last row in Ta-
ble 1. We use the dataset splits of the SemEval
2014 shared task.
4.2 Model
For both our pruning and packing models, we use
the Mate parser (Bohnet, 2010)
4
with default pa-
rameters to learn our parsing models. The Mate
parser is trained on the output of the transforma-
tion pipeline on Sections 00-19 of the three se-
mantically annotated WSJ datasets. Some models
use Brown clusters generated from Sections 00-
19 only. This does not solve OOV problems, but
allows of slightly better generalisation across dis-
tributionally similar words in the training data.
4.3 Baselines
We use the SemEval 2014 shared task baseline
(SIMPLE-PRUNE; see Section 2), as well as the
LONGEST-EDGE baseline, also mentioned above.
The latter is our strongest baseline system. It is
very similar to PRUNING, in doing both edge prun-
ing and flow reversal, but the pruning step only
removes the longest edge rather than considering
node depth. Our third baseline is the Mate seman-
tic role labeling learner (SRL-DEP) (Bj?orkelund
et al., 2009), which uses predicted syntactic parses
as input; for this, we use the syntactic parses made
available in the SemEval 2014 shared task for
replicability.
Approach Cl DM PAS PCEDT Av
Systems
PRUNING
NO 86.6 88.8 72.7 82.7
YES 86.9 89.1 72.5 82.8
PACKING
NO 85.8 88.7 71.8 82.1
YES 86.1 88.7 72.9 82.6
Baselines
SIMPLE-PRUNE 54.7 50.9 67.8 57.8
LONGEST-EDGE 83.8 88.9 66.1 79.6
SRL-DEP 79.5 82.4 70.1 77.4
Upper bound
PACKING 99.9 99.5 99.5 99.6
Table 1: Labelled F1-score results on development
data, with and without use of Brown clusters (Cl).
4.4 Results
The results are presented in Tables 1 through 3,
where the system evaluations for the SemEval task
are marked with asterisks in Table 2. We note that
all our approaches do considerably better than our
4
https://code.google.com/p/mate-tools/
Approach metric DM PAS PCEDT Av
Systems
PACKING PREC 84.8 87.7 71.2 81.2
(W/ TOP) REC 84.0 88.4 68.6 80.3
F1 84.4 88.0 69.9 80.8
?
PREC 85.4 87.9 70.8 81.4
(W/O TOP) REC 84.6 88.6 68.8 80.7
F1 85.0 88.3 69.9 81.1
PRUNING PREC 87.2 91.3 72.8 83.8
(W/ TOP) REC 80.2 81.3 62.8 74.8
F1 83.6 86.0 67.4 79.0
?
PREC 87.2 91.3 72.8 83.8
(W/O TOP) REC 85.1 85.1 68.0 79.4
F1 86.2 88.1 70.3 81.5
Table 2: Labelled results on test data, with and
without evaluation of top nodes. The scores with
asterisks correspond to the output evaluated in the
SemEval task.
Approach metric DM PAS PCEDT Av
Systems
PACKING PREC 86.8 89.1 84.8 86.9
(W/ TOP) REC 86.0 89.8 81.8 85.9
F1 86.4 89.4 83.2 86.3
PREC 87.5 89.4 85.4 87.4
(W/O TOP) REC 86.7 90.1 83.0 86.6
F1 87.1 89.7 84.2 87.0
PRUNING PREC 89.2 92.6 88.2 90.0
(W/ TOP) REC 82.0 82.5 76.1 80.2
F1 85.4 87.3 81.7 84.8
PREC 89.2 92.6 88.2 90.0
(W/O TOP) REC 87.1 86.3 82.4 85.3
F1 88.1 89.3 85.2 87.5
Table 3: Unlabelled results on test data, with and
without evaluation of top nodes.
three baselines. The error reduction of our best
system over the SRL system across all three for-
malisms is 24.2%, and the error reduction over
the more competitive pruning baseline LONGEST-
EDGE is 15.9%. As mentioned in Section 2, these
results seem to promise better performance than
current DAG parsing models. Note from the re-
sults in Table 2 that, as expected, PRUNING leads
to higher precision than PACKING at the expense
of recall.
4.5 Error Analysis
We observe that pruning leads to high precision,
while our packing algorithm gives us much bet-
ter recall. This is not surprising, since our packed
representations introduce new labels, making it
harder to generalize at training time. On the other
hand, pruning approaches suffer in recall, simply
because edges are thrown away in preprocessing
the data.
216
5 Conclusions
In this paper, we experimented with using tree ap-
proximation algorithms to reduce semantic struc-
tures to trees and use off-the-shelf structured pre-
diction techniques to train semantic parsers. Our
approximation algorithms include both pruning
and packing algorithms, i.e., algorithms that try
to reduce graphs to trees optimally, as well as al-
gorithms that pack information about graphs into
trees from which we later recover the richer struc-
tures. Using these tree approximation algorithms,
we obtain 15.9% error reductions over a state-of-
the-art SRL system.
References
Anders Bj?orkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Proc.
of CoNLL: Shared Task, pages 43?48, Boulder, CO,
USA.
Alena B?ohmov?a, Jan Haji?c, Eva Haji?cov?a, and Barbora
Hladk?a. 2003. The Prague Dependency Treebank:
A three-level annotation scenario. In Anne Abeill?e,
editor, Treebanks: Building and Using Syntacti-
cally Annotated Corpora, pages 103?127. Kluwer,
Netherlands.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proc. of
COLING, pages 89?97, Beijing, China.
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan
Sag. 2005. Minimal recursion semantics. Research
on Language and Computation, 3:281?332.
Mitchell Marcus, Mary Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Yusuke Miyao and Jun?ichi Tsujii. 2003. Probabilis-
tic modeling of argument structures including non-
local dependencies. In Proc. of RANLP, pages 79?
85, Borovets, Bulgaria.
Carl Pollard and Ivan Sag. 1994. Head-driven phrase
structure grammar. University of Chicago Press.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency
parsing and domain adaptation with LR models
and parser ensembles. In Proc. of CoNLL Shared
task session of EMNLP-CoNLL, pages 1044?1050,
Prague, Czech Republic.
Kenji Sagae and Jun?ichi Tsujii. 2008. Shift-reduce
dependency DAG parsing. In Proc. of COLING,
pages 753?760, Manchester, UK.
Natalie Schluter and Josef Van Genabith. 2009. De-
pendency parsing resources for French. In Proc. of
NODALIDA, pages 166?173, Odense, Denmark.
217
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 82?90,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Unsupervised Mining of Lexical Variants from Noisy Text
Stephan Gouws?, Dirk Hovy and Donald Metzler
stephan@ml.sun.ac.za, {dirkh, metzler}@isi.edu
USC Information Sciences Institute
Marina del Rey, CA
90292, USA
Abstract
The amount of data produced in user-
generated content continues to grow at a stag-
gering rate. However, the text found in these
media can deviate wildly from the standard
rules of orthography, syntax and even seman-
tics and present significant problems to down-
stream applications which make use of this
noisy data. In this paper we present a novel
unsupervised method for extracting domain-
specific lexical variants given a large volume
of text. We demonstrate the utility of this
method by applying it to normalize text mes-
sages found in the online social media service,
Twitter, into their most likely standard English
versions. Our method yields a 20% reduction
in word error rate over an existing state-of-the-
art approach.
1 Introduction
The amount of data produced in user-generated con-
tent, e.g. in online social media, and from machine-
generated sources such as optical character recog-
nition (OCR) and automatic speech recognition
(ASR), surpasses that found in more traditional me-
dia by orders of magnitude and continues to grow
at a staggering rate. However, the text found in
these media can deviate wildly from the standard
rules of orthography, syntax and even semantics and
present significant problems to downstream applica-
tions which make use of this ?noisy? data. In social
?This work was done while the first author was a visiting
student at ISI from the MIH Media Lab at Stellenbosch Univer-
sity, South Africa.
media this noise might result from the need for so-
cial identity, simple spelling errors due to high in-
put cost associated with the device (e.g. typing on
a mobile phone), space constraints imposed by the
specific medium or even a user?s location (Gouws et
al., 2011). In machine-generated texts, noise might
result from imperfect inputs, imperfect conversion
algorithms, or various degrees of each.
Recently, several works have looked at the pro-
cess of normalizing these ?noisy? types of text into
more standard English, or in other words, to convert
the various forms of idiosyncratic spelling and writ-
ing errors found in these media into what would nor-
mally be considered standard English orthography.
Many of these works rely on supervised methods
which share the common burden of requiring train-
ing data in the form of noisy input and clean output
pairs. The problem with developing large amounts
of annotated training data is that it is costly and re-
quires annotators with sufficient expertise. However,
the volume of data that is available in these media
makes this a suitable domain for applying semi- and
even fully unsupervised methods.
One interesting observation is that these noisy
out-of-vocabulary (OOV) words are typically
formed through some semi-deterministic process
which doesn?t render them completely indiscernible
at a lexical level from the original words they are
meant to represent. We therefore refer to these OOV
tokens as lexical variants of the clean in-vocabulary
(IV) tokens they are derived from. For instance,
in social media ?2morrow? ?2morow? and ?2mrw?
still share at least some lexical resemblance with
?tomorrow?, due to the fact that it is mainly the
82
Figure 1: A plot of the OOV distribution found in Twit-
ter. Also indicated is the potential for using (OOV,most-
likely-IV) training pairs found on this curve for either
exception dictionary entries (the most frequent pairs),
or for learning lexical transformations (the long tail).
The threshold between the two (vertical bar) is domain-
specific.
result of a phonetic transliteration procedure. Also,
?computer? and ?conpu7er? share strong lexical
overlap, and might be the result of noise in the OCR
process.
As with many aspects of NLP, the distribution of
these OOV tokens resemble a power law distribution
(see Figure 1 for the OOV distribution in Twitter).
Thus, some words are commonly converted to some
OOV representation (e.g. domain-specific abbrevia-
tions in social media, or words which are commonly
incorrectly detected in OCR) and these account for
most of the errors, with the rest making up the long
tail. If one could somehow automatically extract a
list of all the domain-specific OOV tokens found in
a collection of texts, along with the most likely clean
word (or words) each represents, then this could play
a key role in for instance normalizing individual
messages. Very frequent (noisy, clean) pairs at the
head of the distribution could be used for extracting
common domain-specific abbreviations, and word-
pairs in the long tail may be used as input to learn-
ing algorithms for automatically learning the types
of transformations found in these media, as shown
in Figure 1.
For example, taking Twitter as our target domain,
examples for learning common exception pairs may
include ?gf ???girlfriend?. For learning types of lex-
ical transformations, one might learn from ?think-
ing???thinkin? and ?walking???walkin? that ?ng?
could go to ?n? (known as ?g-clipping?).
In this paper we present a novel unsupervised
method for extracting an approximation to such a
domain-specific list of (noisy, clean) pairs, given
only a large volume of representative text. We fur-
thermore demonstrate the utility of this method by
applying it to normalize text messages found in the
online social media service, Twitter, into their most
likely standard English versions.
The primary contributions of this paper are:
? We present an unsupervised method that mines
(noisy, clean) pairs and requires only large
amounts of domain-specific noisy data
? We demonstrate the utility of this method by in-
corporating it into a standard method for noisy
text normalization, which results in a signifi-
cant reduction in the word error rate compared
to the original method.
2 Training Pair Mining
Given a large corpus of noisy text, our challenge is to
automatically mine pairs of domain-specific lexical
variants that can be used as training data for a va-
riety of natural language processing tasks. The key
challenge is how to develop an effective approach
that is both domain-specific and robust to noisy cor-
pora. Our proposed approach requires nothing more
than a large ?common English? corpus (e.g., a large
newswire corpus) and a large corpus of domain text
(e.g., a large corpus of Twitter data, a query log,
OCR output, etc.). Using these two sources of ev-
idence, the approach mines domain-specific lexical
variants in a fully unsupervised manner.
Before describing the details of our approach, we
first describe the characteristics that we would like
the mined lexical variants to have. First, the variants
should be semantically related to each other. Pairs
of words that are lexically similar, but semantically
unrelated are not of particular interest since such
pairs can be found using basic edit distance-based
approaches. Second, the variants should be domain-
specific. Variants that capture common English lexi-
cal variations (e.g., ?running? and ?run?) can be cap-
tured using standard normalization procedures, such
83
Figure 2: Flow chart illustrating our procedure for mining
pairs of lexical variants.
as stemming. Instead, we are interested in identify-
ing domain-specific variations (e.g., ?u? and ?you?
in the SMS and Twitter domains) that cannot eas-
ily be handled by existing approaches. Finally, the
variants should be lexically similar, by definition.
Hence, ideal variants will be domain-specific, lex-
ically similar, and semantically related.
To mine such variants we synthesize ideas from
natural language processing and large-scale text
mining to derive a novel mining procedure. Our pro-
cedure can be divided into three atomic steps. First
we identify semantically similar pairs, then we filter
out common English variants, and finally we rescore
the resulting list based on lexical similarity (see Fig-
ure 2). The remainder of this section describes the
complete details of each of these steps.
2.1 Identifying Semantically Similar Pairs
The first step of our mining procedure harvests se-
mantically similar pairs of terms from both the com-
mon English corpus and the domain corpus. There
are many different ways to measure semantic relat-
edness. In this work, we use distributional similar-
ity as our measure of semantic similarity. However,
since we are taking a fully unsupervised approach,
we do not know a priori which pairs of terms may
be related to each other. Hence, we must compute
the semantic similarity between all possible pairs of
terms within the lexicon. To solve this computa-
tionally challenging task, we use a large-scale all-
pairs distributional similarity approach similar to the
one originally proposed by Pasca and Dienes (Pasca
and Dienes, 2005). Our implementation, which
makes use of Hadoop?s MapReduce distributed pro-
gramming paradigm, can efficiently compute all-
pairs distributional similarity over very large corpora
(e.g., the Twitter pairs we use later were mined from
a corpus of half a billion Twitter messages).
Using a similar strategy as Pasca and Dienes, we
define term contexts as the bigrams that appear to
the left and to the right of a given word (Pasca and
Dienes, 2005). Following standard practice, the con-
textual vectors are weighted according to pointwise
mutual information and the similarity between the
vectors is computed using the cosine similarity met-
ric (Lin and Pantel, 2001; Bhagat and Ravichandran,
2008). It is important to note that there are many
other possible ways to compute distributional and
semantic similarity, and that just about any approach
can be used within our framework. The approach
used here was chosen because we had an existing
implementation. Indeed, other approaches may be
more apt for other data sets and tasks.
This approach is applied to both the common En-
glish corpus and the domain corpus. This yields two
sets of semantically (distributionally) similar word
pairs that will ultimately be used to distill unsuper-
vised lexical variants.
2.2 Filtering Common English Variants
Given these two sets of semantically similar word
pairs, the next step in our procedure is designed to
identify the domain-specific pairs by filtering out the
common English variants. The procedure that we
follow is very simple, yet highly effective. Given
the semantically similar word pairs harvested from
the domain corpus, we eliminate all of the pairs that
are also found in the semantically similar common
English pairs.
Any type of ?common English? corpus can be
used for this purpose, depending on the task. How-
ever, we found that a large corpus of newswire ar-
ticles tends to work well. Most of the semanti-
cally similar word pairs harvested from such a cor-
pus are common lexical variants and synonyms. By
eliminating these common variants from the har-
vested domain corpus pairs, we are left with only
the domain-specific semantically similar word pairs.
2.3 Lexical Similarity-Based Re-ordering
The first step of our mining procedure identified
semantically similar term pairs using distributional
similarity, while the second identified those that
were domain-specific by filtering out common En-
glish variants. The third, and final, step of our pro-
cedure re-orders the output of the second step to ac-
count for lexical similarity.
For each word pair (from the second step of our
procedure), we compute two scores: 1) a seman-
84
tic similarity score, and 2) a lexical similarity score.
The final score of the pair is then simply the prod-
uct of the two scores. In this work, we use the
cosine similarity score as our semantic similarity
score, since it is already computed during the first
step of our procedure.
In the social media domain, as in the mobile tex-
ting domain, compressed writing schemes typically
involve deleting characters or replacing one or more
characters with some other characters. For example,
users might delete vowels (?tomorrow???tmrrw?),
or replace ?ph? with its phonetic equivalent ?f ?,
as in ?phone???fone?. We make use of a subse-
quence similarity function (Lodhi et al, 2002) which
can still capture the structural overlap (in the form
of string subsequences) between the remaining un-
changed letters in the noisy word and the original
clean word from which it was derived. In this work
we use a subsequence length of 2, but as with the
other steps in our procedure, this one is purpose-
fully defined in a general way. Any semantic sim-
ilarity score, lexical similarity score, and combina-
tion function can be used in practice.
The output of the entire procedure is a scored list
of word pairs that are semantically related, domain-
specific, and lexically similar, thereby exhibiting the
characteristics that we initially defined as important.
We treat these (scored) pairs as pseudo training data
that has been derived in a fully unsupervised manner.
We anticipate that these pairs will serve as powerful
training data for a variety of tasks, such as noisy text
normalization, which we will return to in Section 3.
2.4 Example and Error Analysis
As an illustrative example of this procedure in prac-
tice, Table 1 shows the actual output of our system
for each step of the mining procedure. To generate
this example, we used a corpus of 2GB of English
news articles as our ?common English? corpus and
a corpus of approximately 500 million Twitter mes-
sages as our domain corpus. In this way, our goal
is to identify Twitter-specific lexical variants, which
we will use in the next section to normalize noisy
Twitter messages.
Column (A) of the table shows that our distribu-
tional similarity approach is capable of identifying
a variety of semantically similar terms in the Twit-
ter corpus. However, the list contains a large num-
Rank Precision
P@50 0.90
P@100 0.88
Table 2: Precision at 50 and 100 of the induced exception
dictionary.
ber of common English variants that are not spe-
cific to Twitter. Column (B) shows the outcome of
eliminating all of the pairs that were found in the
newswire corpus. Many of the common pairs have
been eliminated and the list now contains mostly
Twitter-specific variants. Finally, Column (C) shows
the result of re-ordering the domain-specific pairs to
account for lexical similarity.
In our specific case, the output of step 1 yielded
a list of roughly 3.3M potential word variants. Fil-
tering out common English variants reduced this to
about 314K pairs. In order to estimate the quality of
the list we computed the precision at 50 and at 100
for which the results are shown in Table 2. Further-
more, we find that up to position 500 the pairs are
still of reasonable quality. Thereafter, the number of
errors start to increase noticeably. In particular, we
find that the most common types of errors are
1. Number-related: e.g. ?30? and ?30pm? (due to
incorrect tokenization), or ?5800? and ?5530?;
2. Lemma-related: e.g. ?incorrect? and ?incor-
rectly?; and
3. Negations: e.g. ?could? and ?couldnt?.
Performance can thus be improved by making
use of better tokenization, lemmatizing words, fil-
tering out common negations and filtering out pairs
of numbers.
Still, the resulting pairs satisfy all of our de-
sired qualities rather well, and hence we hypothesize
would serve as useful training data for a number of
different Twitter-related natural language processing
tasks. Indeed, we will now describe one such possi-
ble application and empirically validate the utility of
the automatically mined pairs.
85
(A) (B) (C)
i? you u? you ur? your
my? the seeking? seeks wit? with
u? you 2? to to? too
is? was lost? won goin? going
a? the q? que kno? know
i? we f*ck? hell about? bout
my? your feat? ft wat? what
and? but bday? birthday jus? just
seeking? seeks ff? followfriday talkin? talking
me? you yang? yg gettin? getting
2? to wit? with doin? doing
am? was a? my so? soo
are? were are? r you? your
lost? won amazing? awesome dnt? dont
he? she til? till bday? birthday
q? que fav? favorite nothin? nothing
it? that mostly? partly people? ppl
f*ck? hell northbound? southbound lil? little
can? could hung? toned sayin? saying
im? its love? miss so? sooo
Table 1: Column (A) shows the highest weighted distributionally similar terms harvested from a large Twitter corpus.
Column (B) shows which pairs from (A) remain after filtering out distributionally similar word pairs mined from a
large news corpus. Column (C) shows the effect of reordering the pairs from (B) using a string similarity kernel.
3 Deriving A Common Exception
Dictionary for Text Normalization as a
Use Case for Mining Lexical Variants
As discussed in Section 1, these training pairs may
aid methods which attempt to normalize noisy text
by translating from the ill-formed text into stan-
dard English. Since the OOV distribution in noisy
text mostly resemble a power law distribution (see
Figure 1), one may use the highest scoring train-
ing pairs to induce ?exception dictionaries? (lists of
(noisy word)?(most likely clean word)) of the most
common domain-specific abbreviations found in the
text.
We will demonstrate the utility of our derived
pairs in one specific use case, namely inducing a
domain-specific exception dictionary to augment a
vanilla normalization method. We leave the sec-
ond proposed use-case, namely using pairs in the
long tail for learning transformation rules, for future
work.
We evaluate the first use case in Section 4.
3.1 Baseline Normalization Method
We make use of a competitive heuristic text nor-
malization method over Twitter data as a baseline,
and compare its accuracy to an augmented method
which makes use of an automatically induced excep-
tion dictionary (using the method described in Sec-
tion 2) as a first step, before resorting to the same
baseline method as a ?back-off? for words not found
in the dictionary.
As we point out in Section 5, there are various
metaphors within which the noisy text normalization
problem has been approached. In general, however,
the problem of noisy text normalization may be ap-
proached by using a three step process (Gouws et al,
2011):
1. In the out-of-vocabulary (OOV) detection
step, we detect unknown words which are can-
didates for normalization
2. In the candidate selection step, we find the
weighted lists of most likely candidates (from
a list of in-vocabulary (IV) words) for the OOV
words and group them into a confusion set. The
86
confusion sets are then appended to one another
to create a confusion- network or lattice
3. Finally, in the decoding step, we use a lan-
guage model to rescore the confusion network,
and then find the most likely posterior path
(Viterbi path) through this network.
The words at each node in the resulting posterior
Viterbi path represents the words of the hypothe-
sized original clean sentence.
In this work, we reimplement the method de-
scribed in Contractor (2010) as our baseline method.
We next describe the details of this method in the
context of the framework presented above. See
(Gouws et al, 2011) for more details.
OOV DETECTION is a crucial part of the nor-
malizaton process, since false-positives will result
in undesirable attempts to ?correct? IV words, hence
bringing down the method?s accuracy. We imple-
ment OOV detection as a simple lexicon-lookup pro-
cedure, with heuristics for handling specific out-of-
vocabulary-but-valid tokens such as hash tags and
@usernames.
CANDIDATE SELECTION involves comparing
an unknown OOV word to a list of words which
are deemed in-vocabulary, and producing a top-K
ranked list with candidate words and their estimated
probabilities of relevance as output. This process re-
quires a function with which to compute the simi-
larity or alternatively, distance, between two words.
More traditional string-similarity functions like the
simple Lehvenshtein string edit distance do not fare
too well in this domain.
We implement the IBM-similarity (Contractor et
al., 2010) which employs a slightly more advanced
similarity function. It finds the length of the longest
common subsequence (LCS) between two strings s1
and s2, normalized by the edit distance (ED) be-
tween the consonants in each string (referred to as
the ?consonant skeleton? (CS)), thus
sim(s1, s2) =
LCS(s1, s2)
ED(CS(s1),CS(s2))
Finally, the DECODING step takes an input word
lattice (lattice of concatenated, weighted confusion
sets), and produces a new lattice by incorporating
the probabilities from an n-gram language model
with the prior probabilities in the lattice to produce a
reranked posterior lattice. The most likely (Viterbi)
path through this lattice represents the decoded clean
output. We use SRI-LM (Stolcke, 2002) for this.
3.2 Augmenting the Baseline: Our Method
In order to demonstrate the utility of the mined lex-
ical variant pairs, we first construct a (noisy, clean)
lookup table from the mined pairs. We (arbitrarily)
use the 50 mined pairs with the highest overall com-
bined score (see Section 2.3) for the exception dic-
tionary. For each pair, we map the OOV term (noisy
and typically shorter) to the IV term (clean and usu-
ally longer). The exception lookup list is then used
to augment the baseline method (see Section 3.1) in
the following way: When the method encounters a
new word, it first checks to see if the word is in the
exception dictionary. If it is, we normalize to the
value in the dictionary. If it is not, we pass the ill-
formed word to the baseline method to proceed as
normal.
4 Evaluation
4.1 Dataset
We make use of the Twitter dataset discussed in
Han (2011). It consists of a random sampling of 549
English tweets, annotated by three independent an-
notators. All OOV words were pre-identified and the
annotators were requested to determine the standard
form (gold standard) for each ill-formed word.
4.2 Evaluation Metrics
In this study, we are interested in measuring the
quality of our mined training pairs by evaluating its
utility on an external task: Using the training pairs
to induce a (noisy?clean) exception dictionary to
augment the working of a standard noisy text nor-
malization system. Hence, our focus is entirely on
the accuracy of the candidate selection procedure as
defined in Section 3.1. We compute this accuracy
in terms of the word error rate (WER), defined as
the number of token substitutions, insertions or dele-
tions one has to make to turn the system output into
the gold standard, normalized by the total number of
tokens in the output. In order to remove the possi-
ble bias introduced by our very basic OOV-detection
87
Method WER % Change
Naive baseline 10.7% ?
IBM-baseline 7.8% ?27.1%
Our method 5.6% ?47.7%
Table 3: Word error rate (WER, lower is better) results
of our method against a naive baseline and the much
stronger IBM-baseline (Contractor et al, 2010). We also
show the relative change in WER for our method and the
IBM-baseline compared to the naive baseline.
mechanism, we evaluate the output of all systems
only on the oracle pairs. Oracle pairs are defined as
the (input,system-output,gold) pairs where input and
gold do not match. In other words, we remove the
possible confounding impact of imperfect OOV de-
tection on the accuracy of the normalization process
by assuming a perfect OOV-detection step.
4.3 Discussion of Results
The results of our experiments are displayed in Ta-
ble 3. It is important to note that the focus is not
on achieving the best WER compared to other sys-
tems (although we achieve very competitive scores),
but to evaluate the added utility of integrating an
exception dictionary which is based purely on the
mined (noisy, clean) pairs with an already competi-
tive baseline method.
The ?naive baseline? shows the results if we make
no changes to the input tokens for all oracle pairs.
Therefore it reflects the total level of errors that are
present in the corpus.
The IBM-method is seen to reduce the amount of
errors by a substantial 27.1%. However, the aug-
mented method results in a further 20.6% reduction
in errors, for a total reduction of 47.7% of all er-
rors in the dataset, compared to the IBM-baseline?s
27.1%.
Since we replace matches in the dictionary indis-
criminately, and since the dictionary comprise those
pairs that typically occur most frequently in the cor-
pus from which they were mined, it is important to
note that if these pairs are of poor quality, then their
sheer frequency will drive the overall system accu-
racy down. Therefore, the accuracy of these pairs
are strongly reflected in the WER performance of
the augmented method.
Noisy Clean % Oracle Pairs
u you 8.7
n and 1.4
ppl people 1
da the 1
w with 0.7
cuz because 0.5
y why 0.5
yu you 0.5
lil little 0.5
dat that 0.5
wat what 0.4
tha the 0.4
kno know 0.4
r are 0.4
Table 4: Error analysis for all (noisy, clean) normaliza-
tions missed by the vanilla IBM-baseline method, but in-
cluded in the top-50 pairs used for constructing the ex-
ception dictionary. We also show the percentage of all
oracle pairs that are corrected by including each pair in
an exception dictionary.
Table 4 shows the errors missed by the IBM-
baseline, but contained in the mined exception dic-
tionary. We also show each pair?s frequency of oc-
currence in the oracle pairs (hence its contribution
towards lowering WER).
5 Related work
To the best of our knowledge, we are the first to ad-
dress the problem of mining pairs of lexical variants
from noisy text in an unsupervised and purely sta-
tistical manner that does not require aligned noisy
and clean messages. To obtain aligned clean and
noisy text without annotated data implies the use
of some normalizing method first. Yvon (2010)
presents one such approach, where they generate ex-
ception dictionaries from their finite-state system?s
normalized output. However, their method is still
trained on annotated training pairs, and hence su-
pervised. A related direction is ?transliteration min-
ing? (Jiampojamarn et al, 2010) which aims to au-
tomatically obtain bilingual lists of names written in
different scripts. They also employ string-similarity
measures to find similar string pairs written in differ-
ent scripts. However, their input data is constrained
88
to Wikipedia articles written in different languages,
whereas we impose no constrains on our input data,
and merely require a large collection thereof.
Noisy text normalization, on the other hand, has
recently received a lot of focus. Most works con-
strue the problem in the metaphors of either ma-
chine translation (MT) (Bangalore et al, 2002;
Aw et al, 2006; Kaufmann and Kalita, 2010),
spelling correction (Choudhury et al, 2007; Cook
and Stevenson, 2009), or automated speech recog-
nition (ASR) (Kobus et al, 2008). For our evalua-
tion, we developed an implementation of Contrac-
tor (2010) which works on the same general ap-
proach as Han (2011).
6 Conclusions and Future Work
The ability to automatically extract lexical variants
from large noisy corpora has many practical appli-
cations, including noisy text normalization, query
spelling suggestion, fixing OCR errors, and so on.
This paper developed a novel methodology for au-
tomatically mining such pairs from a large domain-
specific corpus. The approach makes use of distri-
butional similarity for measuring semantic similar-
ity, a novel approach for filtering common English
pairs by comparing against pairs mined from a large
news corpus, and a substring similarity measure for
re-ordering the pairs according to their lexical simi-
larity.
To demonstrate the utility of the method, we used
automatically mined pairs to construct an unsuper-
vised exception dictionary, that was used in con-
junction with a string similarity measure, to form
a highly effective hybrid noisy text normalization
technique. By exploiting the properties of the power
law distribution, the exception dictionary can suc-
cessfully correct a large number of cases, while the
heuristic string similarity-based approach handled
many of the less common test cases from the tail of
the distribution. The hybrid approach showed sub-
stantial reductions in WER (around 20%) versus the
string similarity approach, hence validating our pro-
posed approach.
For future work we are interested in exploiting the
(noisy, clean) pairs contained in the long tail as input
to learning algorithms for acquiring domain-specific
lexical transformations.
Acknowledgments
Stephan Gouws would like to thank MIH Holdings
Ltd. for financial support during the course of this
work.
References
A.T. Aw, M. Zhang, J. Xiao, and J. Su. 2006. A phrase-
based statistical model for SMS text normalization. In
Proceedings of the COLING/ACL on Main conference
poster sessions, pages 33?40. Association for Compu-
tational Linguistics.
S. Bangalore, V. Murdock, and G. Riccardi. 2002.
Bootstrapping bilingual data using consensus transla-
tion for a multilingual instant messaging system. In
Proceedings of the 19th International Conference on
Computational Linguistics Volume 1, pages 1?7. As-
sociation for Computational Linguistics.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL-08: HLT, pages 674?
682, Columbus, Ohio, June. Association for Computa-
tional Linguistics.
M. Choudhury, R. Saraf, V. Jain, A. Mukherjee, S. Sarkar,
and A. Basu. 2007. Investigation and modeling of the
structure of texting language. International Journal on
Document Analysis and Recognition, 10(3):157?174.
D. Contractor, T.A. Faruquie, and L.V. Subramaniam.
2010. Unsupervised cleansing of noisy text. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 189?196.
Association for Computational Linguistics.
P. Cook and S. Stevenson. 2009. An unsupervised model
for text message normalization. In Proceedings of the
Workshop on Computational Approaches to Linguis-
tic Creativity, pages 71?78. Association for Computa-
tional Linguistics.
S. Gouws, D. Metzler, C. Cai, and E. Hovy. 2011. Con-
textual Bearing on Linguistic Variation in Social Me-
dia. In Proceedings of the ACL-11 Workshop on Lan-
guage in Social Media. Association for Computational
Linguistics.
Bo Han and Timothy Baldwin. 2011. Lexical Normal-
isation of Short Text Messages: Makn Sens a #twit-
ter. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies. Association for Compu-
tational Linguistics.
S. Jiampojamarn, K. Dwyer, S. Bergsma, A. Bhargava,
Q. Dou, M.Y. Kim, and G. Kondrak. 2010. Translit-
eration generation and mining with limited training
89
resources. In Proceedings of the 2010 Named Enti-
ties Workshop, pages 39?47. Association for Compu-
tational Linguistics.
M. Kaufmann and J. Kalita. 2010. Syntactic Normaliza-
tion of Twitter Messages. In International Conference
on Natural Language Processing, Kharagpur, India.
C. Kobus, F. Yvon, and G. Damnati. 2008. Normal-
izing SMS: are two metaphors better than one? In
Proceedings of the 22nd International Conference on
Computational Linguistics-Volume 1, pages 441?448.
Association for Computational Linguistics.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question-answering. Nat. Lang. Eng.,
7:343?360, December.
H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini,
and C. Watkins. 2002. Text classification using string
kernels. The Journal of Machine Learning Research,
2:419?444.
Marius Pasca and Pter Dienes. 2005. Aligning needles
in a haystack: Paraphrase acquisition across the web.
In Robert Dale, Kam-Fai Wong, Jian Su, and Oi Yee
Kwong, editors, Natural Language Processing IJC-
NLP 2005, volume 3651 of Lecture Notes in Computer
Science, pages 119?130. Springer Berlin / Heidelberg.
A. Stolcke. 2002. SRILM-an extensible language mod-
eling toolkit. In Proceedings of the International Con-
ference on Spoken Language Processing, volume 2,
pages 901?904. Citeseer.
F. Yvon. 2010. Rewriting the orthography of sms mes-
sages. Journal of Natural Language Engineering,
16(02):133?159.
90
NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 31?38,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Exploiting Partial Annotations with EM Training
Dirk Hovy, Eduard Hovy
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Marina del Rey, CA 90292
{dirkh, hovy}@isi.edu
Abstract
For many NLP tasks, EM-trained HMMs are
the common models. However, in order to es-
cape local maxima and find the best model, we
need to start with a good initial model. Re-
searchers suggested repeated random restarts
or constraints that guide the model evolu-
tion. Neither approach is ideal. Restarts are
time-intensive, and most constraint-based ap-
proaches require serious re-engineering or ex-
ternal solvers. In this paper we measure the ef-
fectiveness of very limited initial constraints:
specifically, annotations of a small number of
words in the training data. We vary the amount
and distribution of initial partial annotations,
and compare the results to unsupervised and
supervised approaches. We find that partial
annotations improve accuracy and can reduce
the need for random restarts, which speeds up
training time considerably.
1 Introduction
While supervised learning methods achieve good
performance in many NLP tasks, they are inca-
pable of dealing with missing annotations. For most
new problems, however, missing data is the norm,
which makes it impossible to train supervised mod-
els. Unsupervised learning techniques can make
use of unannotated data and are thus well-suited for
these problems.
For sequential labeling tasks (POS-tagging, NE-
recognition), EM-trained HMMs are the most com-
mon unsupervised model. However, running vanilla
forward-backward-EM leads to mediocre results,
due to various properties of the training method
(Johnson, 2007). Running repeated restarts with
random initialization can help escape local maxima,
but in order to find the global optimum, we need to
run a great number (100 or more) of them (Ravi and
Knight, 2009; Hovy et al, 2011). However, there
is another solution. Various papers have shown that
the inclusion of some knowledge greatly enhances
performance of unsupervised systems. They intro-
duce constraints on the initial model and the param-
eters. This directs the learning algorithm towards a
better parameter configuration. Types of constraints
include ILP-based methods (Chang et al, 2007;
Chang et al, 2008; Ravi and Knight, 2009), and pos-
terior regularization (Grac?a et al, 2007; Ganchev et
al., 2010). While those approaches are powerful and
yield good results, they require us to reformulate the
constraints in a certain language, and either use an
external solver, or re-design parts of the maximiza-
tion step. This is time-consuming and requires a cer-
tain expertise.
One of the most natural ways of providing con-
straints is to annotate a small amount of data. This
can either be done manually, or via simple heuris-
tics, for example, if some words? parts of speech
are unambiguous. This can significantly speed up
learning and improve accuracy of the learned mod-
els. These partial annotations are a common tech-
nique for semi-supervised learning. It requires no
changes to the general framework, or the use of ex-
ternal solvers.
While this well-known, it is unclear exactly how
much annotation, and annotation of what, is most ef-
fective to improve accuracy. To our knowledge, no
paper has investigated this aspect empirically. We
31
Inputs: I went to the show
walk on water
Partial Annotations: I went to the:DET show:NN
walk on:sense5 water
Figure 1: In partial annotation, words are replaced by
their label
explore the use of more unlabeled data vs. partial
annotation of a small percentage. For the second
case, we investigate how much annotation we need
to achieve a particular accuracy, and what the best
distribution of labels is. We test our approach on
a POS-tagging and word sense disambiguation task
for prepositions.
We find that using partial annotations improves
accuracy and reduces the effect of random restarts.
This indicates that the same accuracy can be reached
with fewer restarts, which speeds up training time
considerably.
Our contributions are:
? we show how to include partial annotations in
EM training via parameter tying
? we show how the amounts and distribution of
partial annotations influence accuracy
? we evaluate our method on an existing data set,
comparing to both supervised and unsupervised
methods on two tasks
2 Preliminaries
2.1 Partial Annotations
When training probabilistic models, more con-
straints generally lead to improved accuracy. The
more knowledge we can bring to bear, the more we
constrain the number of potential label sequences
the training algorithm has to consider. They also
help us to find a good initial model: it has to explain
those fixed cases.
The purest form of unsupervised learning as-
sumes the complete lack of annotation. However,
in many cases, we can use prior knowledge to label
words in context based on heuristics. It is usually
not the case that all labels apply to all observations.
If we know the alphabet of labels we use, we of-
ten also know which labels are applicable to which
observations. This is encoded in a dictionary. For
POS-tagging, it narrows the possible tags for each
word?irrespective of context?down to a manageable
set. Merialdo (1994) showed how the amount of
available dictionary information is correlated with
performance. However, dictionaries list all applica-
ble labels per word, regardless of context. We can
often restrict the applicable label for an observation
in a specific context even more. We extend this to
include constraints applied to some, but not all in-
stances. This allows us to restrict the choice for an
observation to one label. We substitute the word in
case by a special token with just one label. Based on
simple heuristics, we can annotate individual words
in the training data with their label. For example, we
can assume that ?the? is always a determiner. This
is a unigram constraint. We can expand those con-
straints to include a wider context. In a sentence like
?I went to the show?, we know that NN is the only
applicable tag for ?show?, even if a dictionary lists
the possible tags NN and VB. In fact, we can make
that assumption for all words with a possible POS
tag of NN that follow ?the?. This is an n-gram con-
straint.
Partial annotations provide local constraints.
They arise from a number of different cases:
? simple heuristics that allow the disambiguation
of some words in context (such as words after
?the? being nouns)
? when we can leverage annotated data from a
different task
? manual labeling of a few instances
While the technique is mainly useful for problems
where only few labeled examples are available, we
make use of a corpus of annotated data. This allows
us to control the effect of the amount and type of
annotated data on accuracy.
We evaluate the impact of partial annotations on
two tasks: preposition sense disambiguation and
POS tagging.
2.2 Preposition Sense Disambiguation
Prepositions are ubiquitous and highly ambiguous.
Disambiguating prepositions is thus a challenging
and interesting task in itself (see SemEval 2007 task,
32
(Litkowski and Hargraves, 2007)). There are three
elements in the syntactic structure of prepositional
phrases, namely the head word h (usually a noun,
verb, or adjective), the preposition p, and the object
of the preposition, o. The triple (h, p, o) forms a
syntactically and semantically constrained structure.
This structure is reflected in dependency parses as a
common construction.
Tratz and Hovy (2009) show how to use the de-
pendency structure to solve it. Their method out-
performed the previous state-of-the-art (which used
a window-based approach) by a significant margin.
Hovy et al (2011) showed how the sequential na-
ture of the problem can be exploited in unsupervised
learning. They present various sequential models
and training options. They compare a standard bi-
gram HMM and a very complex model that is de-
signed to capture mutual constraints. In contrast to
them, we use a trigram HMM, but move the preposi-
tion at the end of the observed sequence, to condition
it on the previous words. As suggested there, we use
EM with smoothing and random restarts.
2.3 Unsupervised POS-tagging
Merialdo (1994) introduced the task of unsupervised
POS tagging using a dictionary. For each word,
we know the possible labels in general. The model
has to learn the labels in context. Subsequent work
(Johnson, 2007; Ravi and Knight, 2009; Vaswani et
al., 2010) has expanded on this in various ways, with
accuracy between 86% and 96%. In this paper, we
do not attempt to beat the state of the art, but rather
test whether our constraints can be applied to a dif-
ferent task and data set.
3 Methodology
3.1 Data
For PSD, we use the SemEval task data. It con-
sists of a training (16k) and a test set (8k) of sen-
tences with sense-annotated prepositions following
the sense inventory of The Preposition Project, TPP
(Litkowski, 2005). It defines senses for each of the
34 most frequent English prepositions. There are on
average 9.76 senses per preposition (between 2 and
25). We combine training and test and use the an-
notations from the training data to partially label our
corpus. The test data remains unlabeled. We use the
WordNet lexicographer senses as labels for the argu-
ments. It has 45 labels for nouns, verbs, and adjec-
tives and is thus roughly comparable to the prepo-
sitions sense granularity. It also allows us to con-
struct a dictionary for the arguments from WordNet.
Unknown words are assumed to have all possible
senses applicable to their respective word class (i.e.
all noun senses for words labeled as nouns, etc). We
assume that pronouns other than ?it? refer to people.
For the POS-tagged data, we use the Brown cor-
pus. It contains 57k sentences and about 1, 16m
words. We assume a simplified tag set with 38 tags
and a dictionary that lists all possible tags for each
word. For the partial annotations, we label every oc-
currence of ?the?, ?a?, and ?an? as DET, and the next
word with possible tag NN as NN. Additional con-
straints label all prepositions as ?P? and all forms of
?be? as ?V?. We train on the top two thirds and test
on the last third.
For both data sets, we converted all words to
lower case and replaced numbers by ?@?.
3.2 Models
w
1
w
2
l
1
l
2
walk water on
w
3
l
3
Figure 2: PSD: Trigram HMM with preposition as last
element
For POS-tagging, we use a standard bigram
HMM without back-off.
For PSD, we use a trigram HMM, but move the
preposition at the end of the observed sequence, to
condition it on the previous words (see Figure 2).
Since not all prepositions have the same set of la-
bels, we train individual models for each preposi-
tion. We can thus learn different parameter settings
for the different prepositions.
We use EM with smoothing and random restarts
to train our models. For smoothing,  is added to
each fractional count before normalization at each
iteration to prevent overfitting (Eisner, 2002a). We
33
set  to 0.01. We stop training after 40 iterations,
or if the perplexity change between iterations was
less than 0.0001. We experimented with different
numbers of random restarts (none, 10, 50, and 100).
3.3 Dealing with Partial Annotations
The most direct way to constrain a specific word to
only one label is to substitute it for a special to-
ken that has only that label. If we have a partially
annotated example ?walk on-sense5 water? as in-
put (see Figure 1), we add an emission probability
P (word = label |tag = label) to our model.
However, this is problematic in two ways. Firstly,
we have effectively removed a great number of
instances where ?on? should be labeled ?sense5 ?
from our training data, and replaced them with an-
other token: there are now fewer instances from
which we collect C(on|sense5 ). The fractional
counts for our transition parameters are not af-
fected by this, but the counts for emission param-
eter are skewed. We thus essentially siphon prob-
ability mass from P (on|sense5 ) and move it to
P (on : sense5 |sense5 ). Since the test data never
contains labels such as sense5 , our partial annota-
tions have moved a large amount of probability mass
to a useless parameter: we are never going to use
P (on : sense5 |sense5 ) during inference!
Secondly, since EM tends to find uniform distri-
butions (Johnson, 2007), other, rarer labels will also
have to receive some probability. The counts for la-
bels with partial annotations are fixed, so in order to
use the rare labels (for which we have no partial an-
notations), their emission counts need to come from
unlabeled instances. Say sense1 is a label for which
we have no partial annotations. Every time EM col-
lects emission counts from a word ?on? (and not a
labeled version ?on:sensen?), it assigns some of it
to P (on|sense1 ). Effectively, we thus assign too
much probability mass to the emission of the word
from rare labels.
The result of these two effects is the inverse of
what we want: our model will use the label with
the least partial annotations (i.e., a rare label) dis-
proportionately often during inference, while the la-
bels for which we had partial annotations are rarely
used. The resulting annotation has a low accuracy.
We show an example of this in Section 5.
The solution to this problem is simple: param-
eter tying. We essentially have to link each par-
tial annotation to the original word that we replaced.
The observed word ?on? and the partial annotation
?on : sense5 ? should behave the same way during
training. This way, our emission probabilities for
the word ?on? given a label (say, ?sense5 ?) take
the information from the partial annotations into ac-
count. This technique is also described in Eisner
(2002b) for a phonological problem with similar
properties. Technically, the fractional counts we col-
lect for C(on : sense5 |sense5 ) should also count
for C(on|sense5 ). By tying the two parameters to-
gether, we achieve exactly that. This way, we can
prevent probability mass from being siphoned away
from the emission probability of the word, and an
undue amount of probability mass from being as-
signed to rare labels.
4 Experiments
4.1 How Much Annotation Is Needed?
In order to test the effect of partial annotations on
accuracy, we built different training sets. We varied
the amount of partial annotations from 0 to 65% in
increments of 5%. The original corpus we use con-
tains 67% partial annotations, so we were unable to
go beyond this number. We created the different cor-
pora by randomly removing the existing annotations
from our corpus. Since this is done stochastically,
we ran 5 trials for each batch and averaged the re-
sults.
We also test the effect more unsupervised data has
on the task. Theoretically, unsupervised methods
should be able to exploit additional training data. We
use 27k examples extracted from the prepositional
attachment corpus from Ratnaparkhi et al (1994).
4.2 What Kind of Annotation Is Needed?
We can assume that not only the quantity, but also
the distribution of the partial annotations makes a
difference. Given that we can only annotate a cer-
tain percentage of the data, how should we best dis-
tribute those annotations among instances to max-
imize accuracy? In order to test this, we hold the
amount of annotated data fixed, but vary the labels
we use. We choose one sense and annotate only the
instances that have that sense, while leaving the rest
unlabeled.
34
Ideally, one would like to examine all subsets of
annotations, from just a single annotation to all but
one instances of the entire training data. This would
cover the spectrum from unsupervised to supervised.
It is unlikely that there is a uniform best number that
holds for all problems within this immense search
space. Rather, we explore two very natural cases,
and compare them to the unsupervised case, for var-
ious numbers of random restarts:
1. all partial annotations are of the same sense
2. one labeled example of each sense
5 Results
System Acc. (%)
semi-supervised w/o param tying 4.73
MFS baseline 40.00
unsupervised (Hovy et al, 2011) 55.00
semi-supervised, no RR 63.18
semi-supervised, 10 RR 63.12
semi-supervised, 50 RR 63.16
semi-supervised, 100 RR 63.22
semi-supervised, addtl. data, no RR 62.67
semi-supervised, addtl. data, 10 RR 62.47
semi-supervised, addtl. data, 50 RR 62.58
semi-supervised, addtl. data, 100 RR 62.58
supervised (Hovy et al, 2010) 84.50
Table 1: Accuracy of various PSD systems. Baseline is
most frequent sense.
Table 1 shows the results for the PSD systems we
tested. Since not all test sets are the same size, we re-
port the weighted average over all prepositions. For
significance tests, we use two-tailed t-tests over the
difference in accuracy at p < 0.001.
The difference between our models and the base-
line as well as the best unsupervised models in
Hovy et al (2011) are significant. The low accu-
racy achieved without parameter tying underscores
the importance of this technique. We find that the
differences between none and 100 random restarts
are not significant if partial annotations are used.
Presumably, the partial annotations provide a strong
enough constraint to overcome the effect of the ran-
dom initializations. I.e., the fractional counts from
the partial annotations overwhelm any initial param-
eter settings and move the model to a more advanta-
geous position in the state space. The good accuracy
for the case with no restarts corroborates this.
50
55
60
65
70
75
80
85
90
95
100
0 5 10 15 20 25 30 35 40 45 50 55 60 65
a
c
c
u
r
a
c
y
 
(
%
)
amount of annotated prepositions (%)
Figure 3: Accuracy for PSD systems improves linearly
with amount of partial annotations. Accuracies above
dotted line improve significantly (at p < 0.001) over un-
supervised approach (Hovy et al, 2011)
Figure 3 shows the effect of more partial anno-
tations on PSD accuracy. Using no annotations at
all, just the dictionary, we achieve roughly the same
results as reported in Hovy et al (2011). Each incre-
ment of partial annotations increases accuracy. At
around 27% annotated training examples, the differ-
ence starts to be significant. This shows that unsu-
pervised training methods can benefit from partial
annotations. When adding more unsupervised data,
we do not see an increase in accuracy. In this case,
the algorithm failed to make use of the additional
training data. This might be because the two data
sets were not heterogenous enough, or because the
number of emission parameters grew faster than the
amount of available training examples. A possible,
yet somewhat unsatisfying explanation is that when
we increase the overall training data, we reduce the
percentage of labeled data (here to 47%; the result
was comparable to the one observed in our ablation
studies). It seems surprising, though, that the model
does not benefit from the additional data1. More ag-
gressive smoothing might help alleviate that prob-
lem.
The results on the distribution of partial annota-
tion are shown in Figure 4. Using only the most
1Note that similar effects were observed by (Smith and Eis-
ner, 2005; Goldwater and Griffiths, 2007).
35
010
20
30
40
50
60
70
80
90
100
all 1st 2nd 3rd 4th 5th one each
53.55
48.77
44.71
43.00
45.65
49.69
63.12
a
c
c
u
r
a
c
y
 
(
%
)
senses used
Figure 4: Labeling one example of each sense yields bet-
ter results than all examples of any one sense. Senses
ordered by frequency
frequent sense, accuracy drops to 49.69%. While
this is better than the baseline which simply assigns
this sense to every instance, it is a steep drop. We
get better results using just one annotated example
of each sense (53.55%).
System Acc. (%)
(Merialdo, 1994) 86.60
random baseline 62.46
unsupervised, no RR 82.77
semi-supervised, DET+NN 88.51
semi-supervised, DET+NN+P 88.97
semi-supervised, DET+NN+P+V 87.07
Table 2: Accuracy of various POS systems. Random
baseline averaged over 10 runs.
The results for POS tagging confirm our previ-
ous findings. The random baseline chooses for each
word one of the possible tags. We averaged the re-
sults over 10 runs. The difference in accuracy be-
tween both the baseline and the unsupervised ap-
proach as well as the unsupervised approach and any
of the partial annotations are significant. However,
the drop in accuracy when adding the last heuris-
tic points to a risk: partial annotation with heuris-
tics can introduce errors and offset the benefits of
the constraints. Careful selection of the right heuris-
tics and the tradeoff between false positives they in-
troduce and true positives they capture can alleviate
this problem.
6 Related Research
Unsupervised methods have great appeal for
resource-poor languages and new tasks. They have
been applied to a wide variety of sequential label-
ing tasks, such as POS tagging, NE recognition, etc.
The most common training technique is forward-
backward EM. While EM is guaranteed to improve
the data likelihood, it can get stuck in local max-
ima. Merialdo (1994) showed how the the initialized
model influences the outcome after a fixed number
of iterations. The importance is underscored suc-
cinctly by Goldberg et al (2008). They experiment
with various constraints.
The idea of using partial annotations has been
explored in various settings. Druck et al (2008)
present an approach to label features instead of
instances for discriminative probabilistic models,
yielding substantial improvements. They also study
the effectiveness of labeling features vs. labeling in-
stances. Rehbein et al (2009) study the utility of
partial annotations as precursor to further, human
annotation. Their experiments do not extend to un-
supervised training. Tsuboi et al (2008) used data
that was not full annotated. However, their setting
is in principle supervised, only few words are miss-
ing. Instead of no labels, those words have a limited
number of possible alternatives. This works well for
tasks with a small label alphabet or data where anno-
tators left multiple options for some words. In con-
trast, we start out with unannotated data and assume
that some words can be labeled. Gao et al (2010)
present a successful word alignment approach that
uses partial annotations. These are derived from
human annotation or heuristics. Their method im-
proves BLEU, but requires some modification of the
EM framework.
7 Conclusion and Future Work
It is obvious, and common knowledge, that provid-
ing some annotation to an unsupervised algorithm
will improve accuracy and learning speed. Surpris-
ingly, however, our literature search did not turn up
any papers stating exactly how and to what degree
the improvements appear. We therefore selected a
36
very general training method, EM, and a simple ap-
proach to include partial annotations in it using pa-
rameter tying. This allows us to find more stable
starting points for sequential labeling tasks than ran-
dom or uniform initialization. We find that we would
need a substantial amount of additional unlabeled
data in order to boost accuracy. In contrast, we can
get significant improvements by partially annotating
some instances (around 27%). Given that we can
only annotate a certain percentage of the data, it is
best to distribute those annotations among all appli-
cable senses, rather than focus on one. This obviates
the need for random restarts and speeds up training.
This work suggests several interesting new av-
enues to explore. Can one integrate this procedure
into a large-scale human annotation effort to ob-
tain a kind of active learning, suggesting which in-
stances to annotate next, until appropriate stopping
criteria are satisfied (Zhu et al, 2008)? Can one
determine upper bounds for the number of random
restarts given the amount of annotations?
Acknowledgements
We would like to thank Victoria Fossum, Kevin
Knight, Zornitsa Kozareva, and Ashish Vaswani for
invaluable discussions and advice. We would also
like to thank the reviewers who provided us with
helpful feedback and suggestions. Research sup-
ported in part by Air Force Contract FA8750-09-C-
0172 under the DARPA Machine Reading Program.
References
Ming-Wei Chang, Lev Ratinov, and Dan Roth.
2007. Guiding semi-supervision with constraint-
driven learning. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 280?287, Prague, Czech Republic. Associ-
ation for Computational Linguistics.
Ming-Wei Chang, Lev Ratinov, Nicholas Rizzolo, and
Dan Roth. 2008. Learning and inference with con-
straints. In Proceedings of the 23rd national confer-
ence on Artificial intelligence, pages 1513?1518.
Gregory Druck, Gideon Mann, and Andrew McCallum.
2008. Learning from labeled features using gener-
alized expectation criteria. In Proceedings of the
31st annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 595?602. ACM.
Jason Eisner. 2002a. An interactive spreadsheet for
teaching the forward-backward algorithm. In Pro-
ceedings of the ACL-02 Workshop on Effective tools
and methodologies for teaching natural language pro-
cessing and computational linguistics-Volume 1, pages
10?18. Association for Computational Linguistics.
Jason Eisner. 2002b. Parameter estimation for prob-
abilistic finite-state transducers. In Proceedings of
the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 1?8. Association for Com-
putational Linguistics.
Kuzman Ganchev, Joa?o Grac?a, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. The Journal of Machine
Learning Research, 11:2001?2049.
Qin Gao, Nguyen Bach, and Stephan Vogel. 2010. A
semi-supervised word alignment algorithm with par-
tial manual alignments. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation and
MetricsMATR, pages 1?10. Association for Computa-
tional Linguistics.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
Em can find pretty good hmm pos-taggers (when given
a good start). In Proceedings of ACL.
Sharon Goldwater and Thomas Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech tag-
ging. In ANNUAL MEETING-ASSOCIATION FOR
COMPUTATIONAL LINGUISTICS, volume 45, page
744.
Joa?o Grac?a, Kuzman Ganchev, and Ben Taskar. 2007.
Expectation maximization and posterior constraints.
Advances in Neural Information Processing Systems,
20:569?576.
Dirk Hovy, Stephen Tratz, and Eduard Hovy. 2010.
What?s in a Preposition? Dimensions of Sense Dis-
ambiguation for an Interesting Word Class. In Coling
2010: Posters, pages 454?462, Beijing, China, Au-
gust. Coling 2010 Organizing Committee.
Dirk Hovy, Ashish Vaswani, Stephen Tratz, David Chi-
ang, and Eduard Hovy. 2011. Models and training
for unsupervised preposition sense disambiguation. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 323?328, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 296?305.
Ken Litkowski and Orin Hargraves. 2007. SemEval-
2007 Task 06: Word-Sense Disambiguation of Prepo-
sitions. In Proceedings of the 4th International
37
Workshop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic.
Ken Litkowski. 2005. The preposition project.
http://www.clres.com/prepositions.html.
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational linguistics,
20(2):155?171.
Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos.
1994. A maximum entropy model for prepositional
phrase attachment. In Proceedings of the workshop on
Human Language Technology, pages 250?255. Asso-
ciation for Computational Linguistics.
Sujith Ravi and Kevin Knight. 2009. Minimized models
for unsupervised part-of-speech tagging. In Proceed-
ings of the Joint Conference of the 47th Annual Meet-
ing of the ACL and the 4th International Joint Confer-
ence on Natural Language Processing of the AFNLP:
Volume 1-Volume 1, pages 504?512. Association for
Computational Linguistics.
Ines Rehbein, Josef Ruppenhofer, and Caroline
Sporleder. 2009. Assessing the benefits of par-
tial automatic pre-labeling for frame-semantic
annotation. In Proceedings of the Third Linguistic
Annotation Workshop, pages 19?26. Association for
Computational Linguistics.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 354?362.
Association for Computational Linguistics.
Stephen Tratz and Dirk Hovy. 2009. Disambiguation of
Preposition Sense Using Linguistically Motivated Fea-
tures. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, Companion Volume: Student Research Work-
shop and Doctoral Consortium, pages 96?100, Boul-
der, Colorado, June. Association for Computational
Linguistics.
Yuta Tsuboi, Hisashi Kashima, Hiroki Oda, Shinsuke
Mori, and Yuji Matsumoto. 2008. Training condi-
tional random fields using incomplete annotations. In
Proceedings of the 22nd International Conference on
Computational Linguistics, volume 1, pages 897?904.
Association for Computational Linguistics.
Ashish Vaswani, Adam Pauls, and David Chiang. 2010.
Efficient optimization of an mdl-inspired objective
function for unsupervised part-of-speech tagging. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, pages 209?214. Association for Computational
Linguistics.
Jingbo Zhu, Huizhen Wang, and Eduard Hovy. 2008.
Multi-criteria-based strategy to stop active learning
for data annotation. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics-
Volume 1, pages 1129?1136. Association for Compu-
tational Linguistics.
38
Proceedings of the First Workshop on Metaphor in NLP, pages 52?57,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
Identifying Metaphorical Word Use with Tree Kernels
Dirk Hovy1 Shashank Srivastava2 Sujay Kumar Jauhar2 Mrinmaya Sachan2
Kartik Goyal2 Huiying Li2 Whitney Sanders2 Eduard Hovy2
(1) ISI, University of Southern California, Marina del Rey
(2) LTI, Carnegie Mellon University, Pittsburgh
dirkh@isi.edu, {shashans,sjauhar,mrinmays,kartikgo,huiyingl,wsanders,hovy}@cs.cmu.edu
Abstract
A metaphor is a figure of speech that refers
to one concept in terms of another, as in ?He
is such a sweet person?. Metaphors are ubiq-
uitous and they present NLP with a range
of challenges for WSD, IE, etc. Identifying
metaphors is thus an important step in lan-
guage understanding. However, since almost
any word can serve as a metaphor, they are
impossible to list. To identify metaphorical
use, we assume that it results in unusual se-
mantic patterns between the metaphor and its
dependencies. To identify these cases, we use
SVMs with tree-kernels on a balanced corpus
of 3872 instances, created by bootstrapping
from available metaphor lists.1 We outper-
form two baselines, a sequential and a vector-
based approach, and achieve an F1-score of
0.75.
1 Introduction
A metaphor is a figure of speech used to transfer
qualities of one concept to another, as in ?He is
such a sweet person?. Here, the qualities of ?sweet?
(the source) are transferred to a person (the target).
Traditionally, linguistics has modeled metaphors as
a mapping from one domain to another (Lakoff and
Johnson, 1980).
Metaphors are ubiquitous in normal language and
present NLP with a range of challenges. First, due to
their very nature, they cannot be interpreted at face
value, with consequences for WSD, IE, etc. Second,
metaphors are very productive constructions, and
almost any word can be used metaphorically (e.g.,
1Available at http://www.edvisees.cs.cmu.edu/
metaphordata.tar.gz
?This is the Donald Trump of sandwiches.?). This
property makes them impossible to pre-define or
list. Third, repeated use of a metaphor eventu-
ally solidifies it into a fixed expression with the
metaphorical meaning now accepted as just another
sense, no longer recognized as metaphorical at all.
This gradient makes it hard to determine a boundary
between literal and metaphorical use of some ex-
pressions. Identifying metaphors is thus a difficult
but important step in language understanding.2
Since many words can be productively used as
new metaphors, approaches that try to identify
them based on lexical features alone are bound to
be unsuccessful. Some approaches have therefore
suggested considering distributional properties
and ?abstractness? of the phrase (Turney et al,
2011). This nicely captures the contextual nature
of metaphors, but their ubiquity makes it impossible
to find truly ?clean? data to learn the separate
distributions of metaphorical and literal use for
each word. Other approaches have used pre-defined
mappings from a source to a target domain, as in
?X is like Y?, e.g., ?emotions are like temperature?
(Mason, 2004). These approaches tend to do well
on the defined mappings, but they do not generalize
to new, creative metaphors. It is doubtful that it
is feasible to list all possible mappings, so these
approaches remain brittle.
In contrast, we do not assume any predefined
mappings. We hypothesize instead that if we inter-
preted every word literally, metaphors will manifest
themselves as unusual semantic compositions.
Since these compositions most frequently occur
2Shutova (2010) distinguishes between metaphor identifica-
tion (which she calls recognition) and interpretation. We are
solely concerned with the former.
52
in certain syntactic relations, they are usually con-
sidered semantic preference violations; e.g., in the
metaphorical ?You will have to eat your words?, the
food-related verb heads a noun of communication.
In contrast, with the literal sense of ?eat? in ?You
will have to eat your peas?, it heads a food noun.
This intuition is the basis of the approaches in
(Iverson and Helmreich, 1991; Krishnakumaran
and Zhu, 2007; Baumer et al, 2010; Turney et
al., 2011).3 We generalize this intuition beyond
preference selections of verbs and relational nouns.
Given enough labeled examples of a word, we
expect to find distinctive differences in the compo-
sitional behavior of its literal and metaphorical uses
in certain preferred syntactic relationships. If we
can learn to detect such differences/anomalies, we
can reliably identify metaphors. Since we expect
these patterns in levels other than the lexical level,
the approach expands well to creative metaphors.
The observation that the anomaly tends to occur
between syntactically related words makes depen-
dency tree kernels a natural fit for the problem. Tree
kernels have been successfully applied to a wide
range of NLP tasks that involve (syntactic) relations
(Culotta and Sorensen, 2004; Moschitti, 2006; Qian
et al, 2008; Giuliano et al, 2009; Mirroshandel et
al., 2011).
Our contributions in this paper are:
? we annotate and release a corpus of 3872 in-
stances for supervised metaphor classification
? we are the first to use tree kernels for metaphor
identification
? our approach achieves an F1-score of 0.75, the
best score of of all systems tested.
2 Data
2.1 Annotation
We downloaded a list of 329 metaphor examples
from the web4. For each expression, we extracted
sentences from the Brown corpus that contained
the seed (see Figure 1 for an example). To decide
3A similar assumption can be used to detect the literal/non-
literal uses of idioms (Fazly et al, 2009).
4http://www.metaphorlist.com and http://
www.macmillandictionaryblog.com
whether a particular instance is used metaphorically,
we set up an annotation task on Amazon Mechanical
Turk (AMT).
Annotators were asked to decide whether a
highlighted expression in a sentence was used
metaphorically or not (see Figure 2 for a screen-
shot). They were prompted to think about whether
the expression was used in its original meaning.5
In some cases, it is not clear whether an expression
is used metaphorically or not (usually in short
sentences such as ?That?s sweet?), so annotators
could state that it was not possible to decide. We
paid $0.09 for each set of 10 instances.
Each instance was annotated by 7 annotators.
Instances where the annotators agreed that it was
impossible to tell whether it is a metaphor or not
were discarded. Inter-annotator agreement was
0.57, indicating a difficult task. In order to get the
label for each instance, we weighted the annotator?s
answers using MACE (Hovy et al, 2013), an
implementation of an unsupervised item-response
model. This weighted voting produces more reliable
estimates than simple majority voting, since it is
capable of sorting out unreliable annotators. The
final corpus consisted of 3872 instances, 1749 of
them labeled as metaphors.
Figure 2: Screenshot of the annotation interface on Ama-
zon?s Mechanical Turk
We divided the data into training, dev, and test
sets, using a 80-10-10 split. All results reported
here were obtained on the test set. Tuning and
development was only carried out on the dev set.
2.2 Vector Representation of Words
The same word may occur in a literal and a
metaphorical usage. Lexical information alone is
5While this is somewhat imprecise and not always easy to
decide, it proved to be a viable strategy for untrained annotators.
53
A bright idea.
? Peter is the bright , sympathetic guy when you ?re doing a deal , ? says one agent . yes
Below he could see the bright torches lighting the riverbank . no
Her bright eyes were twinkling . yes
Washed , they came out surprisingly clear and bright . no
Figure 1: Examples of a metaphor seed, the matching Brown sentences, and their annotations
thus probably not very helpful. However, we would
like to capture semantic aspects of the word and
represent it in an expressive way. We use the exist-
ing vector representation SENNA (Collobert et al,
2011) which is derived from contextual similarity.
In it, semantically similar words are represented
by similar vectors, without us having to define
similarity or looking at the word itself. In initial
tests, these vectors performed better than binary
vectors straightforwardly derived from features of
the word in context.
2.3 Constructing Trees
a) b) c)
like
I people
the sweet in
Boston
NNS
DT JJ IN
n.group
O adj.all O
NNP n.location
VB
PRP
v.emotion
O
Figure 3: Graphic demonstration of our approach. a) de-
pendency tree over words, with node of interest labeled.
b) as POS representation. c) as supersense representation
The intuition behind our approach is that
metaphorical use differs from literal use in certain
syntactic relations. For example, the only difference
between the two sentences ?I like the sweet people
in Boston? and ?I like the sweet pies in Boston? is
the head of ?sweet?. Our assumption is that?given
enough examples?certain patterns emerge (e.g.,
that ?sweet? in combination with food nouns is
literal, but is metaphorical if governed by a noun
denoting people).
We assume that these patterns occur on different
levels, and mainly between syntactically related
words. We thus need a data representation to
capture these patterns. We borrow its structure from
dependency trees, and the different levels from
various annotations. We parse the input sentence
with the FANSE parser (Tratz and Hovy, 2011)6. It
provides the dependency structure, POS tags, and
other information.
To construct the different tree representations,
we replace each node in the tree with its word,
lemma, POS tag, dependency label, or supersense
(the WordNet lexicographer name of the word?s
first sense (Fellbaum, 1998)), and mark the word
in question with a special node. See Figure 3 for
a graphical representation. These trees are used in
addition to the vectors.
This approach is similar to the ones described in
(Moschitti et al, 2006; Qian et al, 2008; Hovy et
al., 2012).
2.4 Classification Models
A tree kernel is simply a similarity matrix over tree
instances. It computes the similarity between two
trees T1, T2 based on the number of shared subtrees.
We want to make use of the information en-
coded in the different tree representations during
classification, i.e., a forest of tree kernels. We thus
combine the contributions of the individual tree
representation kernels via addition. We use kernels
over the lemma, POS tag, and supersense tree
representations, the combination which performed
best on the dev set in terms of accuracy.
We use the SVMlight TK implementation by
Moschitti (2006).7 We left most parameters set
to default values, but tuned the weight of the
contribution of the trees and the cost factor on the
dev set. We set the multiplicative constant for the
trees to 2.0, and the cost factor for errors on positive
examples to 1.7.
6http://www.isi.edu/publications/
licensed-sw/fanseparser/index.html
7http://disi.unitn.it/moschitti/
Tree-Kernel.htm
54
If we assume any word can be used metaphori-
cally, we ultimately want to label every word in a
sentence, so we also evaluate a sequential model, in
this case a CRF. We use CRFsuite (Okazaki, 2007)8
to implement the CRF, and run it with averaged
perceptron. While the CRF produces labels for
every word, we only evaluate on the words that
were annotated in our corpus (to make it maximally
comparable), and use the same representations
(lemma, POS and SST) of the word and its parent
as features as we did for the SVM. Training method
and feature selection were again tuned on the dev
set to maximize accuracy.
3 Experiments
system acc P R F1
BLall 0.49 0.49 1.0 0.66
BLmost freq. class 0.70 0.66 0.65 0.65
CRF 0.69? 0.74? 0.50 0.59
SVMvector?only 0.70? 0.63? 0.80 0.71
SVM+tree 0.75? 0.70? 0.80 0.75?
Table 1: Accuracy, precision, recall, and F1 for various
systems on the held-out test set. Values significantly bet-
ter than baseline at p < .02 are marked ? (two-tailed t-
test).
We compare the performance of two baselines,
the CRF model, vanilla SVM, and SVM with tree
kernels and report accuracy, precision, recall, and
F1 (Table 1).
The first baseline (BLall) labels every instance
as metaphor. Its accuracy and precision reflect the
metaphor ratio in the data, and it naturally achieves
perfect recall. This is a rather indiscriminate
approach and not very viable in practice, so we
also apply a more realistic baseline, labeling each
word with the class it received most often in the
training data (BLmost freq. class ). This is essentially
like assuming that every word has a default class.
Accuracy and precision for this baseline are much
better, although recall naturally suffers.
The CRF improves in terms of accuracy and
precision, but lacks the high recall the baseline
has, resulting in a lower F1-score. It does yield
8http://www.chokkan.org/software/
crfsuite/
the highest precision of all models, though. So
while not capturing every metaphor in the data, it is
usually correct if it does label a word as metaphor.
SVMlight allows us to evaluate the performance
of a classification using only the vector representa-
tion (SVMvector?only). This model achieves better
accuracy and recall than the CRF, but is less precise.
Accuracy is the same as for the most-frequent-
class baseline, indicating that the vector-based
SVM learns to associate a class with each lexical
item. Once we add the tree kernels to the vector
(SVM+tree), we see considerable gains in accuracy
and precision. This confirms our hypothesis that
metaphors are not only a lexical phenomenon, but
also a product of the context a word is used in. The
contextual interplay with their dependencies creates
patterns that can be exploited with tree kernels.
We note that the SVM with tree kernels is the only
system whose F1 significantly improves over the
baseline (at p < .02).
Testing with one tree representation at a time,
we found the various representations differ in terms
of informativeness. Lemma, POS, and supersense
performed better than lexemes or dependency labels
(when evaluated on the dev set) and were thus used
in the reported system. Combining more than one
representation in the same tree to form compound
leaves (e.g. lemma+POS, such as ?man-NN?)
performed worse in all combinations tested. We
omit further details here, since the combinatorics of
these tests are large and yield only little insight.
Overall, our results are similar to comparable
methods on balanced corpora, and we encourage
the evaluation of other methods on our data set.
4 Related Work
There is plenty of research into metaphors. While
many are mainly interested in their general proper-
ties (Shutova, 2010; Nayak, 2011), we focus on the
ones that evaluate their results empirically.
Gedigian et al (2006) use a similar approach
to identify metaphors, but focus on frames. Their
corpus is with about 900 instances relatively small.
They improve over the majority baseline, but only
report accuracy. Both their result and the baseline
are in the 90s, which might be due to the high
number of metaphors (about 90%). We use a larger,
55
more balanced data set. Since accuracy can be
uninformative in cases of unbalanced data sets, we
also report precision, recall, and F1.
Krishnakumaran and Zhu (2007) also use se-
mantic relations between syntactic dependencies
as basis for their classification. They do not aim to
distinguish literal and metaphorical use, but try to
differentiate various types of metaphors. They use a
corpus of about 1700 sentences containing different
metaphors, and report a precision of 0.70, recall of
0.61 (F1 = 0.65), and accuracy of 0.58.
Birke and Sarkar (2006) and Birke and Sarkar
(2007) present unsupervised and active learning
approaches to classifying metaphorical and literal
expressions, reporting F1 scores of 0.54 and 0.65,
outperforming baseline approaches. Unfortunately,
as they note themselves, their data set is ?not large
enough to [...] support learning using a supervised
learning method? (Birke and Sarkar, 2007, 22),
which prevents a direct comparison.
Similarly to our corpus construction, (Shutova et
al., 2010) use bootstrapping from a small seed set.
They use an unsupervised clustering approach to
identify metaphors and report a precision of 0.79,
beating the baseline system by a wide margin. Due
to the focus on corpus construction, they cannot
provide recall or F1. Their approach considers only
pairs of a single verbs and nouns, while we allow
for any syntactic combination.
Tree kernels have been applied to a wide va-
riety of NLP tasks (Culotta and Sorensen, 2004;
Moschitti et al, 2006; Qian et al, 2008; Hovy et
al., 2012). They are specifically adept in capturing
long-range syntactic relationships. In our case, we
use them to detect anomalies in syntactic relations.
5 Conclusion
Under the hypothesis that the metaphorical use of a
word creates unusual patterns with its dependencies,
we presented the first tree-kernel based approach
to metaphor identification. Syntactic dependencies
allow us to capture those patterns at different
levels of representations and identify metaphorical
use more reliably than non-kernel methods. We
outperform two baselines, a sequential model, and
purely vector-based SVM approaches, and reach an
F1 of 0.75. Our corpus is available for download
at http://www.edvisees.cs.cmu.edu/
metaphordata.tar.gz and we encourage the
research community to evaluate other methods on it.
Acknowledgements
The authors would like to thank the reviewers for
helping us clarify several points and giving con-
structive input that helped to improve the quality of
this paper. This work was (in part) supported by
the Intelligence Advanced Research Projects Activ-
ity (IARPA) via Department of Defense US Army
Research Laboratory contract number W911NF-12-
C-0025. The U.S. Government is authorized to re-
produce and distribute reprints for Governmental
purposes notwithstanding any copyright annotation
thereon. Disclaimer: The views and conclusions
contained herein are those of the authors and should
not be interpreted as necessarily representing the of-
ficial policies or endorsements, either expressed or
implied, of IARPA, DoD/ARL, or the U.S. Govern-
ment.
References
Eric P.S. Baumer, James P. White, and Bill Tomlinson.
2010. Comparing semantic role labeling with typed
dependency parsing in computational metaphor identi-
fication. In Proceedings of the NAACL HLT 2010 Sec-
ond Workshop on Computational Approaches to Lin-
guistic Creativity, pages 14?22. Association for Com-
putational Linguistics.
Julia Birke and Anoop Sarkar. 2006. A clustering ap-
proach for the nearly unsupervised recognition of non-
literal language. In Proceedings of EACL, volume 6,
pages 329?336.
Julia Birke and Anoop Sarkar. 2007. Active learning for
the identification of nonliteral language. In Proceed-
ings of the Workshop on Computational Approaches
to Figurative Language, pages 21?28. Association for
Computational Linguistics.
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
Journal of Machine Learning Research, 12:2493?
2537.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, page 423. Association for Com-
putational Linguistics.
56
Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised type and token identification
of idiomatic expressions. Computational Linguistics,
35(1):61?103.
Christiane Fellbaum. 1998. WordNet: an electronic lexi-
cal database. MIT Press USA.
Matt Gedigian, John Bryant, Srini Narayanan, and Bran-
imir Ciric. 2006. Catching metaphors. In Proceedings
of the 3rd Workshop on Scalable Natural Language
Understanding, pages 41?48.
Claudio Giuliano, Alfio Massimiliano Gliozzo, and Carlo
Strapparava. 2009. Kernel methods for minimally su-
pervised wsd. Computational Linguistics, 35(4).
Dirk Hovy, James Fan, Alfio Gliozzo, Siddharth Patward-
han, and Christopher Welty. 2012. When Did that
Happen? ? Linking Events and Relations to Times-
tamps. In Proceedings of EACL.
Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,
and Eduard Hovy. 2013. Learning Whom to trust with
MACE. In Proceedings of NAACL HLT.
Eric Iverson and Stephen Helmreich. 1991. Non-literal
word sense identification through semantic network
path schemata. In Proceedings of the 29th annual
meeting on Association for Computational Linguistics,
pages 343?344. Association for Computational Lin-
guistics.
Saishuresh Krishnakumaran and Xiaojian Zhu. 2007.
Hunting elusive metaphors using lexical resources. In
Proceedings of the Workshop on Computational ap-
proaches to Figurative Language, pages 13?20. Asso-
ciation for Computational Linguistics.
George Lakoff and Mark Johnson. 1980. Metaphors we
live by, volume 111. University of Chicago Press.
Zachary J. Mason. 2004. CorMet: a computational,
corpus-based conventional metaphor extraction sys-
tem. Computational Linguistics, 30(1):23?44.
Seyed A. Mirroshandel, Mahdy Khayyamian, and Gho-
lamreza Ghassem-Sani. 2011. Syntactic tree ker-
nels for event-time temporal relation learning. Human
Language Technology. Challenges for Computer Sci-
ence and Linguistics, pages 213?223.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2006. Tree kernel engineering for proposition
re-ranking. MLG 2006, page 165.
Alessandro Moschitti. 2006. Making Tree Kernels Prac-
tical for Natural Language Learning. In In Proceed-
ings of the 11th Conference of the European Chapter
of the Association for Computational Linguistics.
Sushobhan Nayak. 2011. Towards a grounded model
for ontological metaphors. In Student Research Work-
shop, pages 115?120.
Naoaki Okazaki. 2007. CRFsuite: a fast implementation
of Conditional Random Fields (CRFs).
Longhua Qian, Guodong Zhou, Fang Kong, Qiaoming
Zhu, and Peide Qian. 2008. Exploiting constituent
dependencies for tree kernel-based semantic relation
extraction. In Proceedings of the 22nd International
Conference on Computational Linguistics-Volume 1,
pages 697?704. Association for Computational Lin-
guistics.
Ekaterina Shutova, Lin Sun, and Anna Korhonen. 2010.
Metaphor identification using verb and noun cluster-
ing. In Proceedings of the 23rd International Confer-
ence on Computational Linguistics, pages 1002?1010.
Association for Computational Linguistics.
Ekaterina Shutova. 2010. Models of metaphor in nlp. In
Proceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 688?697.
Association for Computational Linguistics.
Stephen Tratz and Eduard Hovy. 2011. A fast, accurate,
non-projective, semantically-enriched parser. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 1257?1268. As-
sociation for Computational Linguistics.
Peter D. Turney, Yair Neuman, Dan Assaf, and Yohai
Cohen. 2011. Literal and metaphorical sense iden-
tification through concrete and abstract context. In
Proceedings of the 2011 Conference on the Empirical
Methods in Natural Language Processing, pages 680?
690.
57
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 1?10,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
What?s in a p-value in NLP?
Anders S?gaard, Anders Johannsen, Barbara Plank, Dirk Hovy and Hector Martinez
Center for Language Technology
University of Copenhagen
soegaard@hum.ku.dk
Abstract
In NLP, we need to document that our pro-
posed methods perform significantly bet-
ter with respect to standard metrics than
previous approaches, typically by re-
porting p-values obtained by rank- or
randomization-based tests. We show that
significance results following current re-
search standards are unreliable and, in ad-
dition, very sensitive to sample size, co-
variates such as sentence length, as well as
to the existence of multiple metrics. We
estimate that under the assumption of per-
fect metrics and unbiased data, we need a
significance cut-off at ?0.0025 to reduce
the risk of false positive results to <5%.
Since in practice we often have consider-
able selection bias and poor metrics, this,
however, will not do alone.
1 Introduction
In NLP, we try to improve upon state of the art
language technologies, guided by experience and
intuition, as well as error analysis from previous
experiments, and research findings often consist in
system comparisons showing that System A is bet-
ter than System B.
Effect size, i.e., one system?s improvements
over another, can be seen as a random variable.
If the random variable follows a known distribu-
tion, e.g., a normal distribution, we can use para-
metric tests to estimate whether System A is bet-
ter than System B. If it follows a normal dis-
tribution, we can use Student?s t-test, for exam-
ple. Effect sizes in NLP are generally not nor-
mally distributed or follow any of the other well-
studied distributions (Yeh, 2000; S?gaard, 2013).
The standard significance testing methods in NLP
are therefore rank- or randomization-based non-
parametric tests (Yeh, 2000; Riezler and Maxwell,
2005; Berg-Kirkpatrick et al., 2012). Specifi-
cally, most system comparisons across words, sen-
tences or documents use bootstrap tests (Efron and
Tibshirani, 1993) or approximate randomization
(Noreen, 1989), while studies that compare perfor-
mance across data sets use rank-based tests such as
Wilcoxon?s test.
The question we wish to address here is: how
likely is a research finding in NLP to be false?
Naively, we would expect all reported findings to
be true, but significance tests have their weak-
nesses, and sometimes researchers are forced
to violate test assumptions and basic statistical
methodology, e.g., when there is no one estab-
lished metric, when we can?t run our models on
full-length sentences, or when data is biased. For
example, one such well-known bias from the tag-
ging and parsing literature is what we may refer to
as the WSJ FALLACY. This is the false belief that
performance on the test section of the Wall Street
Journal (WSJ) part of the English Penn treebank
is representative for performance on other texts in
English. In other words, it is the belief that our
samples are always representative. However, (the
unawareness of) selection bias is not the only rea-
son research findings in NLP may be false.
In this paper, we critically examine significance
results in NLP by simulations, as well as running
a series of experiments comparing state-of-the-art
POS taggers, dependency parsers, and NER sys-
tems, focusing on the sensitivity of p-values to var-
ious factors.
Specifically, we address three important factors:
Sample size. When system A is reported to be
better than system B, this may not hold across do-
mains (cf. WSJ FALLACY). More importantly,
though, it may not even hold on a sub-sample of
the test data, or if we added more data points to
the test set. Below, we show that in 6/10 of our
POS tagger evaluations, significant effects become
insignificant by (randomly) adding more test data.
1
Covariates. Sometimes we may bin our results
by variables that are actually predictive of the out-
come (covariates) (Simmons et al., 2011). In some
subfields of NLP, such as machine translation or
(unsupervised) syntactic parsing, for example, it
is common to report results that only hold for sen-
tences up to some length. If a system A is reported
to be better than a system B on sentences up to
some length, A need not be better than B, neither
for a different length nor in general, since sentence
length may actually be predictive of A being better
than B.
Multiple metrics. In several subfields of NLP,
we have various evaluation metrics. However, if
a system A is reported to be better than a system
B with respect to some metric M
1
, it need not be
better with respect to some other metric M
2
. We
show that even in POS tagging it is sometimes the
case that results are significant with respect to one
metric, but not with respect to others.
While these caveats should ideally be avoided
by reporting significance over varying sample
sizes and multiple metrics, some of these effects
also stem from the p-value cut-off chosen in the
NLP literature. In some fields, p-values are re-
quired to be much smaller, e.g., in physics, where
the 5   criterion is used, and maybe we should also
be more conservative in NLP?
We address this question by a simulation of the
interaction of type 1 and type 2 error in NLP and
arrive at an estimate that more than half of research
findings in NLP with p < 0.05 are likely to be
false, even with a valid metric and in the absence
of selection bias. From the same simulations, we
propose a new cut-off level at 0.0025 or smaller
for cases where the metric can be assumed to be
valid, and where there is no selection bias.
1
We
briefly discuss what to do in case of selection bias
or imperfect metrics.
Note that we do not discuss false discovery rate
control or family wise error rate procedures here.
While testing with different sample sizes could
be be considered multiple hypothesis testing, as
pointed out by one of our anonymous reviewers,
NLP results should be robust across sample sizes.
Note that the p < 0.0025 cut-off level corresponds
1
In many fields, including NLP, it has become good prac-
tice to report actual p-values, but we still need to understand
how significance levels relate to the probability that research
findings are false, to interpret such values. The fact that we
propose a new cut-off level for the ideal case with perfect
metrics and no bias does not mean that we do not recommend
reporting actual p-values.
to a Bonferroni correction for a family of m = 20
hypotheses.
Our contributions
Several authors have discussed significance test-
ing in NLP before us (Yeh, 2000; Riezler and
Maxwell, 2005; Berg-Kirkpatrick et al., 2012), but
while our discussion touches on many of the same
topics, this paper is to the best of our knowledge
the first to:
a) show experimentally how sensitive p-values
are to sample size, i.e., that in standard NLP
experiments, significant effects may actually
disappear by adding more data.
b) show experimentally that multiple metrics
and the use of covariates in evaluation in-
crease the probability of positive test results.
c) show that even under the assumption of per-
fect metrics and unbiased data, as well as our
estimates of type 1 and 2 error in NLP, you
need at least p < 0.0025 to reduce the prob-
ability of a research finding being false to be
< 5%.
2 Significance testing in NLP
Most NLP metric for comparing system outputs
can be shown to be non-normally distributed
(S?gaard, 2013) and hence, we generally cannot
use statistical tests that rely on such an assump-
tion, e.g., Student?s t-test. One alternative to such
tests are non-parametric rank-based tests such as
Wilcoxon?s test. Rank-based tests are sometimes
used in NLP, and especially when the number of
observations is low, e.g., when evaluating perfor-
mance across data sets, such tests seem to be the
right choice (Demsar, 2006; S?gaard, 2013). The
draw-back of rank-based tests is their relatively
weak statistical power. When we reduce scores to
ranks, we throw away information, and rank-based
tests are therefore relatively conservative, poten-
tially leading to high type 2 error rate ( , i.e., the
number of false negatives over trials). An alterna-
tive, however, are randomization-based tests such
as the bootstrap test (Efron and Tibshirani, 1993)
and approximate randomization (Noreen, 1989),
which are the de facto standards in NLP. In this
paper, we follow Berg-Kirkpatrick et al. (2012) in
focusing on the bootstrap test. The bootstrap test is
non-parametric and stronger than rank-based test-
ing, i.e., introduces fewer type 2 errors. For small
samples, however, it does so at the expense of a
2
higher type 1 error (?, i.e., the number of false
positives). The reason for this is that for the boot-
strap test to work, the original sample has to cap-
ture most of the variation in the population. If the
sample is very small, though, this is likely not the
case. Consequently, with small sample sizes, there
is a risk that the calculated p-value will be arti-
ficially low?simply because the bootstrap sam-
ples are too similar. In our experiments below, we
make sure only to use bootstrap when sample size
is > 200, unless otherwise stated. In our experi-
ments, we average across 3 runs for POS and NER
and 10 runs for dependency parsing.
DOMAIN #WORDS TASKS
POS Dep. NER
CONLL 2007
Bio 4k ?
Chem 5k ?
SWITCHBOARD 4
Spoken 162k ?
ENGLISH WEB TREEBANK
Answers 29k ? ?
Emails 28k ? ?
Newsgrs 21k ? ?
Reviews 28k ? ?
Weblogs 20k ? ?
WSJ 40k ? ?
FOSTER
Twitter 3k ?
CONLL 2003
News 50k ?
Table 1: Evaluation data.
3 Experiments
Throughout the rest of the paper, we use four run-
ning examples: a synthetic toy example and three
standard experimental NLP tasks, namely POS
tagging, dependency parsing and NER. The toy
example is supposed to illustrate the logic behind
our reasoning and is not specific to NLP. It shows
how likely we are to obtain a low p-value for the
difference in means when sampling from exactly
the same (Gaussian) distributions. For the NLP
setups (2-4), we use off-the-shelf models or avail-
able runs, as described next.
3.1 Models and data
We use pre-trained models for POS tagging and
dependency parsing. For NER, we use the output
of the best performing systems from the CoNLL
2003 shared task. In all three NLP setups, we
compare the outcome of pairs of systems. The
data sets we use for each of the NLP tasks are
listed in Table 1 (Nivre et al., 2007a; Foster et
Figure 1: Accuracies of LAPOS VS. STANFORD
across 10 data sets.
al., 2011; Tjong Kim Sang and De Meulder, 2003,
LDC99T42; LDC2012T13).
POS tagging. We compare the performance
of two state-of-the-art newswire taggers across 10
evaluation data sets (see Table 1), namely the LA-
POS tagger (Tsuruoka et al., 2011) and the STAN-
FORD tagger (Toutanova et al., 2003), both trained
on WSJ00?18. We use the publicly available pre-
trained models from the associated websites.
2
Dependency parsing. Here we compare the
pre-trained linear SVM MaltParser model for En-
glish (Nivre et al., 2007b) to the compositional
vector grammar model for the Stanford parser
(Socher et al., 2013). For this task, we use the sub-
set of the POS data sets that comes with Stanford-
style syntactic dependencies (cf. Table 1), exclud-
ing the Twitter data set which we found too small
to produce reliable results.
NER. We use the publicly available runs of
the two best systems from the CoNLL 2003
shared task, namely FLORIAN (Florian et al.,
2003) and CHIEU-NG (Chieu and Ng, 2003).
3
3.2 Standard comparisons
POS tagging. Figure 1 shows that the LAPOS
tagger is marginally better than STANFORD on
macro-average, but it is also significantly better? If
we use the bootstrap test over tagging accuracies,
the difference between the two taggers is only sig-
nificant (p < 0.05) in 3/10 cases (see Table 2),
namely SPOKEN, ANSWERS and REVIEWS. In
two of these cases, LAPOS is significantly better
2
http://www.logos.ic.i.u-tokyo.ac.jp/
?
tsuruoka/lapos/ and http://nlp.stanford.
edu/software/tagger.shtml
3
http://www.cnts.ua.ac.be/conll2003/
ner/
3
TA (b) UA (b) SA (b) SA(w)
Bio 0.3445 0.0430 0.3788 0.9270
Chem 0.3569 0.2566 0.4515 0.9941
Spoken <0.001 <0.001 <0.001 <0.001
Answers <0.001 0.0143 <0.001 <0.001
Emails 0.2020 <0.001 0.1622 0.0324
Newsgrs 0.3965 0.0210 0.1238 0.6602
Reviews 0.0020 0.0543 0.0585 0.0562
Weblogs 0.2480 0.0024 0.2435 0.9390
WSJ 0.4497 0.0024 0.2435 0.9390
Twitter 0.4497 0.0924 0.1111 0.7853
Table 2: POS tagging p-values across tagging ac-
curacy (TA), accuracy for unseen words (UA) and
sentence-level accuracy (SA) with bootstrap (b)
and Wilcoxon (w) (p < 0.05 gray-shaded).
LAS UAS
Answers 0.020 <0.001
Emails 0.083 <0.001
Newsgroups 0.049 <0.001
Reviews <0.001 <0.001
Weblogs <0.001 <0.001
WSJ <0.001 <0.001
Table 3: Parsing p-values (MALT-LIN
VS. STANFORD-RNN) across LAS and UAS
(p < 0.05 gray-shaded).
than STANFORD, but in one case it is the other way
around. If we do a Wilcoxon test over the results
on the 10 data sets, following the methodology
in Demsar (2006) and S?gaard (2013), the differ-
ence, which is ?0.12% on macro-average, is not
significant (p ? 0.1394). LAPOS is thus not sig-
nificantly better than STANFORD across data sets,
but as we have already seen, it is significantly bet-
ter on some data sets. So if we allow ourselves
to cherry-pick our data sets and report significance
over word-level tagging accuracies, we can at least
report significant improvements across a few data
sets.
Dependency parsing. Using the bootstrap test
over sentences, we get the p-values in Table 3.
We see that differences are always significant
wrt. UAS, and in most cases wrt. LAS.
NER. Here we use the macro-f
1
as our stan-
dard metric. FLORIAN is not significantly bet-
ter than CHIEU-NG with p < 0.05 as our cut-
off (p ? 0.15). The two systems were also re-
ported to have overlapping confidence intervals in
the shared task.
3.3 p-values across metrics
In several NLP subfields, multiple metrics are in
use. This happens in dependency parsing where
multiple metrics (Schwartz et al., 2011; Tsarfaty
et al., 2012) have been proposed in addition to un-
labeled and labeled attachment scores, as well as
exact matches. Perhaps more famously, in ma-
chine translation and summarization it is com-
mon practice to use multiple metrics, and there
exists a considerable literature on that topic (Pa-
pineni et al., 2002; Lin, 2004; Banerjee and Lavie,
2005; Clark et al., 2011; Rankel et al., 2011).
Even in POS tagging, some report tagging ac-
curacies, tagging accuracies over unseen words,
macro-averages over sentence-level accuracies, or
number of exact matches.
The existence of several metrics is not in it-
self a problem, but if researchers can cherry-pick
their favorite metric when reporting results, this
increases the a priori chance of establishing sig-
nificance. In POS tagging, most papers report sig-
nificant improvements over tagging accuracy, but
some report significant improvements over tag-
ging accuracy of unknown words, e.g., Denis and
Sagot (2009) and Umansky-Pesin et al. (2010).
This corresponds to the situation in psychology
where researchers cherry-pick between several de-
pendent variables (Simmons et al., 2011), which
also increases the chance of finding a significant
correlation.
Toy example. We draw two times 100 val-
ues from identical (0, 1)-Gaussians 1000 times
and calculate a t-test for two independent sam-
ples. This corresponds to testing the effect size
between two systems on a 1000 randomly cho-
sen test sets with N = 100. Since we are sam-
pling from the same distribution, the chance of
p < ? should be smaller than ?. In our simula-
tion, the empirical chance of obtaining p < 0.01
is .8%, and the chance of obtaining p < 0.05 is
4.8%, as expected. If we simulate a free choice
between two metrics by introducing choice be-
tween a pair of samples and a distorted copy of
that pair (inducing random noise at 10%), simu-
lating the scenario where we have a perfect metric
and a suboptimal metric, the chance of obtaining
p < 0.05 is 10.0%. We see a significant correla-
tion (p < 0.0001) between Pearson?s ? between
the two metrics, and the p-value. The less the two
metrics are correlated, the more likely we are to
obtain p < 0.05. If we allow for a choice between
two metrics, the chance of finding a significant dif-
ference increases considerably. If the two metrics
are identical, but independent (introducing a free
choice between two pairs of samples), we have
4
P (A_B) = P (A) + P (B)  P (A)P (B), hence
the chance of obtaining p < 0.01 is 1.9%, and the
chance of obtaining p < 0.05 is 9.75%.
POS tagging. In our POS-tagging experiments,
we saw a significant improvement in 3/10 cases
following the standard evaluation methodology
(see Table 2). If we allow for a choice between
tagging accuracy and sentence-level accuracy, we
see a significant improvement in 4/10 cases, i.e.,
for 4/10 data sets the effect is significance wrt. at
least one metric. If we allow for a free choice be-
tween all three metrics (TA, UA, and SA), we ob-
serve significance in 9/10 cases. This way the ex-
istence of multiple metrics almost guarantees sig-
nificant differences. Note that there are only two
data sets (Answers and Spoken), where all metric
differences appear significant.
Dependency parsing. While there are multi-
ple metrics in dependency parsing (Schwartz et
al., 2011; Tsarfaty et al., 2012), we focus on
the two standard metrics: labeled (LAS) and un-
labeled attachment score (UAS) (Buchholz and
Marsi, 2006). If we just consider the results in
Table 3, i.e., only the comparison of MALT-LIN
VS. STANFORD-RNN, we observe significant im-
provements in all cases, if we allow for a free
choice between metrics. Bod (2000) provides a
good example of a parsing paper evaluating mod-
els using different metrics on different test sets.
Chen et al. (2008), similarly, only report UAS.
NER. While macro-f
1
is fairly standard in
NER, we do have several available multiple met-
rics, including the unlabeled f
1
score (collapsing
all entity types), as well as the f
1
scores for each
of the individual entity types (see Derczynski and
Bontcheva (2014) for an example of only report-
ing f
1
for one entity type). With macro-f
1
and
f
1
for the individual entity types, we observe that,
while the average p-value for bootstrap tests over
five runs is around 0.15, the average p-value with a
free choice of metrics is 0.02. Hence, if we allow
for a free choice of metrics, FLORIAN comes out
significantly better than CHIEU-NG.
3.4 p-values across sample size
We now show that p-values are sensitive to sam-
ple size. While it is well-known that studies with
low statistical power have a reduced chance of
detecting true effects, studies with low statistical
power are also more likely to introduce false pos-
itives (Button et al., 2013). This, combined with
the fact that free choice between different sample
Figure 2: The distribution of p-values with (above)
and without (below) multiple metrics.
Figure 3: POS tagging p-values varying sample
sizes (p < 0.05 shaded).
sizes also increases the chance of false positives
(Simmons et al., 2011), is a potential source of er-
ror in NLP.
Toy example. The plot in Figure 2 shows the
distribution of p-values across 1000 bootstrap tests
(above), compared to the distribution of p-values
with a free choice of four sample sizes. It is clear
that the existence of multiple metrics makes the
probability of a positive result much higher.
POS tagging. The same holds for POS tag-
ging. We plot the p-values across various sample
sizes in Figure 3. Note that even when we ignore
the smallest sample size (500 words), where re-
sults may be rather unreliable, it still holds that for
Twitter, Answers, Newsgrs, Reviews, Weblogs and
WSJ, i.e., more than half of the data sets, a sig-
nificant result (p < 0.05) becomes insignificant
by increasing the sample size. This shows how
unreliable significance results in NLP with cut-off
p < 0.05 are.
5
Figure 4: Parsing p-values varying sample sizes
(p < 0.05 shaded)
Figure 5: NER p-values varying sample sizes (p <
0.05 shaded)
Dependency parsing. We performed simi-
lar experiments with dependency parsers, seeing
much the same picture. Our plots are presented in
Figure 4. We see that while effect sizes are al-
ways significant wrt. UAS, LAS differences be-
come significant when adding more data in 4/6
cases. An alternative experiment is to see how
often a bootstrap test at a particular sample size
comes out significant. The idea is to sample, say,
10% of the test data 100 times and report the ra-
tio of positive results. We only present the results
for MALT-LIN VS. STANFORD-RNN in Table 4,
but the full set of results (including comparisons of
more MaltParser and Stanford parser models) are
made available at http://lowlands.ku.dk.
For MALT-LIN VS. STANFORD-RNN differ-
ences on the full Emails data set are consistently
insignificant, but on small sample sizes we do get
significant test results in more than 1/10 cases. We
see the same picture with Newsgrs and Reviews.
On Weblogs and WSJ, the differences on the full
data sets are consistently significant, but here we
see that the test is underpowered at small sam-
ple sizes. Note that we use bootstrap tests over
sentences, so results with small samples may be
somewhat unreliable. In sum, these experiments
show how small sample sizes not only increase the
chance of false negatives, but also the chance of
false positives (Button et al., 2013).
NER. Our plots for NER are presented in Fig-
ure 5. Here, we see significance at small sam-
ple sizes, but the effect disappears with more data.
This is an example of how underpowered studies
may introduce false positives (Button et al., 2013).
3.5 p-values across covariates
Toy example. If we allow for a choice between
two subsamples, using a covariate to single out a
subset of the data, the chance of finding a signifi-
cant difference increases. Even if we let the subset
be a random 50-50 split, the chance of obtaining
p < 0.01 becomes 2.7%, and the chance of obtain-
ing p < 0.05 is 9.5%. If we allow for both a choice
of dependent variables and a random covariate, the
chance of obtaining p < 0.01 is 3.7%, and the
chance of obtaining p < 0.05 is 16.2%. So iden-
tical Gaussian variables will appear significantly
different in 1/6 cases, if our sample size is 100,
and if we are allowed a choice between two iden-
tical, but independent dependent variables, and a
choice between two subsamples provided by a ran-
dom covariate.
POS We see from Figure 6 that p-values are
also very sensitive to sentence length cut-offs. For
instance, LAPOS is significantly (p < 0.05) bet-
ter than STANFORD on sentences shorter than 16
words in EMAILS, but not on sentences shorter
than 14 words. On the other hand, when longer
sentences are included, e.g., up to 22 words, the
effect no longer appears significant. On full sen-
tence length, four differences seem significant, but
if we allow ourselves to cherry-pick a maximum
sentence length, we can observe significant differ-
ences in 8/10 cases.
Figure 6: POS tagging p-values varying sentence
length (p < 0.05 shaded)
We observe similar results in Dependency
parsing and NER when varying sentence length,
but do not include them here for space rea-
sons. The results are available at http://
lowlands.ku.dk. We also found that other
covariates are used in evaluations of dependency
parsers and NER systems. In dependency pars-
ing, for example, parsers can either be evaluated
6
N Emails Newsgrs Reviews Weblogs WSJ
LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS
10% 14 % 100 % 9 % 100 % 33% 100 % 42 % 99 % 28 % 75 %
25% 15 % 100 % 23 % 100 % 52% 100 % 68 % 100 % 27 % 98 %
50% 19 % 100 % 25 % 100 % 78% 100 % 100 % 100 % 60 % 100 %
75% 22 % 100 % 41 % 100 % 97% 100 % 100 % 100 % 80 % 100 %
100% 0 % 100 % 36 % 100 % 100% 100 % 100 % 100 % 100 % 100 %
Table 4: Ratio of positive results (p < 0.05) for MALT-LIN VS. STANFORD-RNN at sample sizes (N )
.
on naturally occurring text such as in our experi-
ments or at tailored test suites, typically focusing
on hard phenomena (Rimell et al., 2009). While
such test suites are valuable resources, cf. Man-
ning (2011), they do introduce free choices for re-
searchers, increasing the a priori chance of posi-
tive results. In NER, it is not uncommon to leave
out sentences without any entity types from eval-
uation data. This biases evaluation toward high
recall systems, and the choice between including
them or not increases chances of positive results.
4 How likely are NLP findings to be
false?
The previous sections have demonstrated how
many factors can contribute to reporting an erro-
neously significant result. Given those risks, it is
natural to wonder how likely we are as a field to
report false positives. This can be quantified by
the positive predictive value (PPV), or probability
that a research finding is true. PPV is defined as
(1  )R
R  R+?
(1)
The PPV depends on the type 1 and 2 error rates
(? and  ) and the ratio of true relations over null
relations in the field (R) (Ioannidis, 2005).
R. The likelihood that a research finding is true
depends on the ratio of true relations over null re-
lations in the field, usually denoted R (Ioannidis,
2005). Out of the systems that researchers in the
field would test out (not rejecting them a priori),
how many of them are better than the current state
of the art? The a priori likelihood of a relation be-
ing true, i.e., a new system being better than state
of the art, is R/(R+1). Note that while the space
of reasonably motivated methods may seem big to
researchers in the field, there is often more than
one method that is better than the current state of
the art. Obviously, as the state of the art improves,
R drops. On the other hand, if R becomes very
low, researchers are likely to move on to new ap-
plications where R is higher.
The type 1 error rate (?) is also known as the
false positive rate, or the likelihood to accept a
non-significant result. Since our experiments are
fully automated and deterministic, and precision
usually high, the type 1 error rate is low in NLP.
What is not always appreciated in the field is that
this should lead us to expect true effects to be
highly significant with very low p-values, much
like in physics. The type 2 error rate ( ) is the
false negative rate, i.e., the likelihood that a true
relation is never found. This factors into the recall
of our experimental set-ups.
So what values should we use to estimate PPV?
Our estimate for R (how often reasonable hy-
potheses lead to improvements over state of the
art) is around 0.1. This is based on a sociolog-
ical rather than an ontological argument. With
? = 0.05 and R = 0.1, researchers get positive
results inR+(1 R)? cases, i.e.,? 1/7 cases. If
researchers needed to test more than 7 approaches
to ?hit the nail?, they would never get to write pa-
pers. With ? = 0.05, and   set to 0.5, we find that
the probability of a research finding being true ?
given there is no selection bias and with perfectly
valid metrics ? is just 50%:
PPV =
(1  )R
R  R+?
=
0.5?0.1
0.1 0.05+0.05
=
0.05
0.1
= 0.5
(2)
In other words, if researchers do a perfect experi-
ment and report p < 0.05, the chance of that find-
ing being true is the chance of seeing tail when
flipping a coin. With p < 0.01, the chance is 5/6,
i.e., the chance of not getting a 3 when rolling a
die. Of course these parameters are somewhat ar-
bitrary. Figure 7 shows PPV for various values of
?.
In the experiments in Section 3, we consistently
used the standard p-value cut-off of 0.05. How-
ever, our experiments have shown that significance
results at this threshold are unreliable and very
sensitive to the choice of sample size, covariates,
or metrics. Based on the curves in Figure 7, we
7
Figure 7: PPV for different ? (horizontal line is PPV for p = 0.05, vertical line is ? for PPV=0.95).
could propose a p-value cut-off at p < 0.0025.
This is the cut-off that ? in the absence of bias and
with perfect metrics ? gives us the level of con-
fidence we expect as a research community, i.e.,
PPV = 0.95. Significance results would thus be
more reliable and reduce type 1 error.
5 Discussion
Incidentally, the p < 0.0025 cut-off also leads to
a 95% chance of seeing the same effect on held-
out test data in Berg-Kirkpatrick et al. (2012) (see
their Table 1, first row). The caveat is that this
holds only in the absence of bias and with perfect
metrics. In reality, though, our data sets are of-
ten severely biased (Berg-Kirkpatrick et al., 2012;
S?gaard, 2013), and our metrics are far from per-
fect (Papineni et al., 2002; Lin, 2004; Banerjee
and Lavie, 2005; Schwartz et al., 2011; Tsarfaty et
al., 2012). Here, we discuss how to address these
challenges.
Selection bias. The WSJ FALLACY (Section
1) has been widely discussed in the NLP litera-
ture (Blitzer et al., 2006; Daume III, 2007; Jiang
and Zhai, 2007; Plank and van Noord, 2011). But
if our test data is biased, how do we test whether
System A performs better than System B in gen-
eral? S?gaard (2013) suggests to predict signif-
icance across data sets. This only assumes that
data sets are randomly chosen, e.g., not all from
newswire corpora. This is also standard practice in
the machine learning community (Demsar, 2006).
Poor metrics. For tasks such as POS tagging
and dependency parsing, our metrics are subopti-
mal (Manning, 2011; Schwartz et al., 2011; Tsar-
faty et al., 2012). System A and System B may
perform equally well as measured by some met-
ric, but contribute very differently to downstream
tasks. Elming et al. (2013) show how parsers
trained on different annotation schemes lead to
very different downstream results. This suggests
that being wrong with respect to a gold standard,
e.g., choosing NP analysis over a ?correct? DP
analysis, may in some cases lead to better down-
stream performance. See the discussion in Man-
ning (2011) for POS tagging. One simple ap-
proach to this problem is to report results across
available metrics. If System A improves over Sys-
tem B wrt. most metrics, we obtain significance
against the odds. POS taggers and dependency
parsers should also be evaluated by their impact
on downstream performance, but of course down-
stream tasks may also introduce multiple metrics.
6 Conclusion
In sum, we have shown that significance results
with current research standards are unreliable, and
we have provided a more adequate p-value cut-off
under the assumption of perfect metrics and unbi-
8
ased data. In the cases where these assumptions
cannot be met, we suggest reporting significance
results across datasets wrt. all available metrics.
Acknowledgements
We would like to thank the anonymous review-
ers, as well as Jakob Elming, Matthias Gondan,
and Natalie Schluter for invaluable comments and
feedback. This research is funded by the ERC
Starting Grant LOWLANDS No. 313695.
References
Satanjeev Banerjee and Alon Lavie. 2005. ME-
TEOR: an automatic metric for MT evaluation with
improved correlation with human judgments. In
ACL Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for MT and/or Summarization.
Taylor Berg-Kirkpatrick, David Burkett, and Dan
Klein. 2012. An empirical investigation of statis-
tical significance in nlp. In EMNLP.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
Rens Bod. 2000. Parsing with the shortest derivation.
In COLING.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
Shared Task on Multilingual Dependency Parsing.
In CoNLL.
Katherine Button, John Ioannidis, Claire Mokrysz,
Brian Nosek, Jonathan Flint, Emma Robinson, and
Marcus Munafo. 2013. Power failure: why small
sample size undermines the reliability of neuro-
science. Nature Reviews Neuroscience, 14:365?376.
Wenliang Chen, Youzheng Wu, and Hitoshi Isahara.
2008. Learning Reliable Information for Depen-
dency Parsing Adaptation. In COLING.
Hai Leong Chieu and Hwee Tou Ng. 2003. Named en-
tity recognition with a maximum entropy approach.
In CoNLL.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: controlling for opti-
mizer instability. In ACL.
Hal Daume III. 2007. Frustratingly easy domain adap-
tation. In ACL.
Janez Demsar. 2006. Statistical comparisons of clas-
sifiers over multiple data sets. Journal of Machine
Learning Research, 7:1?30.
Pascal Denis and Beno??t Sagot. 2009. Coupling an
annotated corpus and a morphosyntactic lexicon for
state-of-the-art pos tagging with less human effort.
In PACLIC.
Leon Derczynski and Kalina Bontcheva. 2014.
Passive-aggressive sequence labeling with discrim-
inative post-editing for recognising person entities
in tweets. In EACL.
Bradley Efron and Robert Tibshirani. 1993. An intro-
duction to the bootstrap. Chapman & Hall, Boca
Raton, FL.
Jakob Elming, Anders Johannsen, Sigrid Klerke,
Emanuele Lapponi, Hector Martinez Alonso, and
Anders S?gaard. 2013. Down-stream effects of
tree-to-dependency conversions. In NAACL.
Radu Florian, Abe Ittycheriah, Hongyan Jing, and
Tong Zhang. 2003. Named entity recognition
through classifier combination. In CoNLL.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Josef Le Roux, Joakim Nivre, Deirde Hogan, and
Josef van Genabith. 2011. From news to comments:
Resources and benchmarks for parsing the language
of Web 2.0. In IJCNLP.
John Ioannidis. 2005. Why most published research
findings are false. PLoS Medicine, 2(8):696?701.
Jing Jiang and ChengXiang Zhai. 2007. Instance
weighting for domain adaptation in NLP. In ACL.
Chin-Yew Lin. 2004. ROUGE: a package for auto-
matic evaluation of summaries. In WAS.
Chris Manning. 2011. Part-of-speech tagging from
97% to 100%: Is it time for some linguistics? In
CICLing.
Joakim Nivre, Johan Hall, Sandra K?ubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007a. The CoNLL 2007 Shared Task on
Dependency Parsing. In EMNLP-CoNLL.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?ulsen Eryigit, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007b. MaltParser:
a language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Eric Noreen. 1989. Computer intensive methods for
testing hypotheses. Wiley.
Kishore Papineni, Salim Roukus, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In ACL, pages
311?318, Philadelphia, Pennsylvania.
Barbara Plank and Gertjan van Noord. 2011. Effective
measures of domain similarity for parsing. In ACL.
Peter Rankel, John Conroy, Eric Slud, and Dianne
O?Leary. 2011. Ranking human and machine sum-
marization systems. In EMNLP.
9
Stefan Riezler and John Maxwell. 2005. On some pit-
falls in automatic evaluation and significance test-
ing for MT. In ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization.
Laura Rimell, Stephen Clark, and Mark Steedman.
2009. Unbounded dependency recovery for parser
evaluation. In EMNLP.
Roy Schwartz, and Omri Abend, Roi Reichart, and
Ari Rappoport. 2011. Neutralizing linguisti-
cally problematic annotations in unsupervised de-
pendency parsing evaluation. In ACL.
Joseph Simmons, Leif Nelson, and Uri Simonsohn.
2011. False-positive psychology: undisclosed flexi-
bility in data collection and analysis allows present-
ing anything as significant. Psychological Science,
22(11):1359?1366.
Richard Socher, John Bauer, Chris Manning, and An-
drew Ng. 2013. Parsing with compositional vector
grammars. In ACL.
Anders S?gaard. 2013. Estimating effect size across
datasets. In NAACL.
Erik F Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
In CoNLL.
Kristina Toutanova, Dan Klein, Chris Manning, and
Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In
NAACL.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012. Cross-framework evaluation for statistical
parsing. In EACL.
Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichi
Kazama. 2011. Learning with lookahead: can
history-based models rival globally optimized mod-
els? In CoNLL.
Shulamit Umansky-Pesin, Roi Reichart, and Ari Rap-
poport. 2010. A multi-domain web-based algorithm
for POS tagging of unknown words. In COLING.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In ACL.
10
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 2?7,
Baltimore, Maryland, USA. June 27, 2014.
c?2014 Association for Computational Linguistics
Robust Cross-Domain Sentiment Analysis for Low-Resource Languages
Jakob Elming Dirk Hovy Barbara Plank
Centre for Language Technology
University of Copenhagen
zmk867@hum.ku.dk,{dirk,bplank}@cst.dk
Abstract
While various approaches to domain adap-
tation exist, the majority of them requires
knowledge of the target domain, and ad-
ditional data, preferably labeled. For a
language like English, it is often feasible
to match most of those conditions, but in
low-resource languages, it presents a prob-
lem. We explore the situation when nei-
ther data nor other information about the
target domain is available. We use two
samples of Danish, a low-resource lan-
guage, from the consumer review domain
(film vs. company reviews) in a sentiment
analysis task. We observe dramatic perfor-
mance drops when moving from one do-
main to the other. We then introduce a
simple offline method that makes models
more robust towards unseen domains, and
observe relative improvements of more
than 50%.
1 Introduction
Sentiment analysis, the task of determining the
polarity of a text, is a valuable tool for gather-
ing information from the vast amount of opin-
ionated text produced today. It is actively used
in reputation management and consumer assess-
ment (Amig?o et al., 2012; Amig?o et al., 2013).
While supervised approaches achieve reasonable
performance (Mohammad et al., 2013), they are
typically highly domain-dependent. In fact, mov-
ing from one (source) domain to a different (tar-
get) domain will often lead to severe performance
drops (Blitzer et al., 2007; Daum?e et al., 2010).
This is mainly due to the models overfitting the
source (training) data, both in terms of its la-
bel and word distribution. The task of overcom-
ing this tendency is known as domain adaptation
(DA) (Blitzer et al., 2007; Daum?e et al., 2010).
There are three different approaches to DA: in
Supervised DA, labeled training data for the target
domain exists, in Unsupervised DA, data for the
target domain exists, but it is unlabeled. A third,
less investigated scenario is Blind DA: the target
domain is not known at all in advance. Super-
vised DA effectively counteracts domain-bias by
including labeled data from the target domain dur-
ing training, thus preventing overfitting to both the
label and the word distribution of the source. Un-
supervised methods usually rely either on external
data, in the form of gazetteers, dictionaries, or on
unlabeled data from the target domain. While they
do not prevent overfitting to the source domain?s
label distribution, the additional data acts as a reg-
ularizer by introducing a larger vocabulary.
However, both cases presuppose that we already
know the target domain and have data from it. In
many real-world settings, these conditions are not
met, especially when dealing with low-resource
languages. We thus need to regularize our models
independent of the possible target domains. Ef-
fectively, this means that we need to prevent our
models from memorizing the observed label distri-
bution, and from putting too much weight on fea-
tures that are predictive in the source domain, but
might not even be present in the target domain.
In this paper, we investigate sentiment analysis
for Danish, a low-resource language, and therefore
approach it as a Blind DA problem. We perform
experiments on two types of domains, namely re-
views for movies and companies. The challenge
lies in the fact that the label distribution (posi-
tive, negative, neutral) changes dramatically when
moving from one domain to the other, and many
highly predictive words in the company domain
(e.g., ?reliable?) are unlikely to carry over to the
movie domain, and vice versa. To the best of our
knowledge, this is the first study to perform senti-
ment analysis for Danish, a low-resource language
where relevant resources like polarity dictionaries
2
are hard to come by.
We present a simple offline-learning version in-
spired by previous work on corruptions (S?gaard,
2013), which also addresses the sparsity of avail-
able training data. Our method introduces a rela-
tive improvement on out-of-domain performance
by up to 54%.
2 Robust Learning
The main idea behind robust learning is to steer the
model away from overfitting the source domain.
Overfitting can occur either by
1. putting too much weight on certain features
(which might not be present in the target do-
main), or
2. over-using certain labels (since the label dis-
tribution on the target domain might differ).
One approach that has been proven to re-
duce overfitting is data corruption, also known as
dropout training (S?gaard and Johannsen, 2012;
S?gaard, 2013), which is a way of regularizing
the model by randomly leaving out features. In-
tuitively, this approach can be viewed as coercing
the learning algorithm to rely on more general, but
less consistent features. Rather than learning to
mainly trust the features that are highly predictive
for the given training data, the algorithm is encour-
aged to use the less predictive features, since the
highly predictive features might be deleted by the
corruption. Most prior work on dropout regular-
ization (S?gaard and Johannsen, 2012; Wang and
Manning, 2012; S?gaard, 2013) has used online
corruptions, i.e., the specific dropout function is
integrated into the learning objective and thus tied
to the specific learner. Here, we propose a simple
approximation, i.e., a wrapper function that cor-
rupts instances in an off-line fashion based on the
weights learned from a base model. The advan-
tage is that it can be used for any learning func-
tion, thereby abstracting away from the underlying
learner.
2.1 Our approach
Our off-line feature corruption algorithm works as
follows:
1. train an uncorrupted (base) model,
2. create k copies of the training data instances,
3. corrupt copies based on the feature weights of
the base model and an exponential function
(described below), and
4. train a new model on the corrupted training
data.
The advantages of this algorithm compared to
online corruption are
1. it is a wrapper method, so it becomes very
easy to move to a different learning algo-
rithm, and
2. corruption is done based on knowledge from
a full, uncorrupted model, which provides a
better picture of the overfitting.
This comes, however, at the cost of longer training
times, but in a low-resource language training time
is less of an issue.
Specifically, multiple copies of the training data
are used in the corrupted training stage. This re-
sults in each data point appearing in different, cor-
rupted versions, as visualized in Figure 1. The
copying process retains more of the information in
the training data, since it is unlikely that the same
feature is deleted in each copy. In our experiments,
we used k=5. Larger values of k resulted in longer
training times without improving performance.
1 11 1Original
1 1 11 1 111
1
1
1
Corrupted Copies?!?!?111
Figure 1: Example of an original feature vector
and its multiple corrupted copies.
We experiment with a random and a biased
corruption approach. The first approach (S?gaard
and Johannsen, 2012) does not utilize the feature
weight information from the base model, but ran-
domly deletes 10% of the features. We use this
approach to test whether an effect is merely the
result of deleting features.
The biased approach, on the other hand, tar-
gets the most predictive features in the base model
for deletion. We use a function that increases
the probability of deleting a feature exponentially
3
0	 ?
25	 ?
50	 ?
75	 ?
100	 ?
-??0.33	 ? -??0.23	 ? -??0.13	 ? -??0.03	 ? 0.07	 ? 0.17	 ? 0.27	 ? 0.37	 ?
%	 ?
Feature	 ?weight	 ?
Figure 2: The corruption function conditioning the
probability of deleting a feature in a positive in-
stance on its weight in the Scope baseline model.
with its model weight. That is, a highly predic-
tive feature (with a high weight in the model) will
be more likely to be deleted. A feature with a
low weight, on the other hand, has a much lower
chance of being deleted. Figure 2 visualizes the
exponential corruption function used. The func-
tion assigns the lowest weighted feature of the
model zero likelihood of deletion, and the highest
weighted feature a 0.9 likelihood of deletion. In
order to mainly corrupt the highly predictive fea-
tures, the exponential function is shifted to an area
with a steeper gradient. That is, instead of scal-
ing to the exponential function between 0 and 1, it
is scaled to the area between -3 and 2 (parameters
set experimentally on the development set). The
corruption probability p
cor
of deleting a feature f
given a category c is defined as
p
cor
(f |c) =
exp(
w(f |c)?w
min
(c)
w
max
(c)?w
min
(c)
?5?3)?exp(?3)
exp(2)?exp(?3)
? 0.9
(1)
with w(f |c) being the weight of f given the in-
stance category c in the model, and w
min
(c) and
w
max
(c) being the lowest and highest weights of
the model respectively for category c.
3 Experiments
Our experiments use Danish reviews from two do-
mains: movies and companies. The specifications
of the data sets are listed in Table 1 and Figure 3.
The two data sets differ considerably in data size
and label distribution.
DOMAIN SPLIT REVIEWS WORDS
Scope Train 8,718 749,952
Dev 1,198 107,351
Test 2,454 210,367
Total 12,370 1,067,670
Trustpilot Train 170,137 7,180,160
Dev 23,958 1,000,443
Test 48,252 2,040,956
Total 242,347 10,221,559
Table 1: Overview of data set and split sizes in
number of reviews and number of words.
3.1 Data preparation
The movie reviews are downloaded from a Dan-
ish movie website, www.scope.dk. They con-
tain reviews of 829 movies, each rated on a scale
from 1 to 6 stars. The company reviews are
downloaded from a Danish consumer review web-
site, www.trustpilot.dk. They consist of re-
views of 19k companies, each rated between 1 and
5 stars.
Similar to prior work on sentiment analy-
sis (Blitzer et al., 2007), the star ratings are binned
into the three standard categories; positive, neu-
tral, and negative. For the Scope data, a 6 star rat-
ing is considered positive, a 3 or 4 rating neutral,
and a 1 star rating negative. 2 and 5 star ratings are
excluded to retain more distinct categories. For the
Trustpilot data, 5 star reviews are categorized as
positive, 3 stars as neutral, and 1 star as negative.
Similar to Scope data, 2 and 4 stars are excluded.
0%
25%
50%
75%
100%
scope trustpilot
84.85%
27.36%
5.40%
60.85%
9.75%11.79%
negative neutral positive
Figure 3: Label distribution in the two data sets.
Apart from the difference in size, the two data
sets also differ in the distribution of categories (see
Figure 3). This means that a majority label base-
line estimated from one would perform horribly on
4
- N-gram presence for token lengths 1 to 4
- Skip-grams (n-gram with one middle word replaced by *) presence for token lengths 3 to 4
- Character n-gram presence for entire document string for token lengths 1 to 5
- Brown clusters (Brown et al., 1992; Liang, 2005) estimated on the source training data
- Number of words with only upper case characters
- Number of contiguous sequences of question marks, exclamation marks, or both
- Presence of question mark or exclamation mark in last word
- Number of words with characters repeated more than two times e.g. ?sooooo?
- Number of negated contexts using algorithm described in the text
- Most positive, most negative, or same amount of polar words according to a sentiment lexicon
Table 2: Feature set description.
the other domain. For instance, the majority base-
line on Scope (assigning neutral to all instances)
achieves a 5% accuracy on Trustpilot data. Sim-
ilarly, the Trustpilot majority baseline obtains an
accuracy of 27% on Scope data by always assign-
ing positive.
We choose not to balance the data sets, in keep-
ing with the blind DA setup. Knowing the target
label distribution can help greatly, but we can as-
sume no prior knowledge about that. In fact, the
difference in label distribution is one of the ma-
jor challenges when predicting on out-of-domain
data.
3.2 Features
The features we use (described in Table 2) are
inspired by the top performing system from the
SemEval-2013 task on Twitter sentiment analy-
sis (Mohammad et al., 2013).
One main difference is that Mohammad et al.
(2013) had several high-quality sentiment lexicons
at their disposal, shown to be effective. Working
with a low-resource language, we only have ac-
cess to a single lexicon created by an MA student
(containing 2248 positive and 4736 negative word
forms). Our lexicon features are therefore simpler,
i.e., based on whether words are considered pos-
itive or negative in the lexicon, as opposed to the
score-based features in Mohammad et al. (2013).
We adopted the simple negation scope reso-
lution algorithm directly from Mohammad et al.
(2013). Anything appearing in-between a negation
token
1
and the first following punctuation mark is
considered a negated context. This works well for
English, but Danish has different sentence adver-
bial placement, so the negation may also appear
1
We use the following negation markers: ikke, ingen, in-
tet, ingenting, aldrig, hverken. n?ppe.
after the negated constituent. This simple algo-
rithm is therefore less likely to be beneficial in a
Danish system. We plan to extend the system for
better negation handling in future work.
3.3 Corruption
The corruption happens at the feature-instance
level. When we refer to the deletion of a feature
in the following, it does not mean the deletion of
this feature throughout the training data, but the
deletion of a single instance in a feature vector (cf.
Figure 1).
Corrupting the Scope data deleted 9.24% of all
feature instances in the training data. Most fea-
tures are deleted from positive instances (16.7%
of all features) and least from the majority neutral
instances (6.5% of all features). Only 9.4% of the
minority class negative are deleted.
For Trustpilot, the corruption deleted 11.73%
of the feature instances. The pattern is the same
here, though more extreme. The majority positive
class has the fewest features removed (2.2%), the
minority class neutral has 22.8% of its features
deleted, and the negative class has an overwhelm-
ing 35.6% of its features deleted.
The fact that the corruption function does not
take the weight distribution of the individual la-
bels into account, and therefore corrupts the data
of some labels much more than others, does prove
to be a problem. We will get back to this in the
results section.
4 Results
Table 3 shows the results of the experiments. We
report both accuracy and the average f-score for
positive and negative instances (AF).
AF is the official SemEval-2013 metric (Nakov
et al., 2013). It offers a more detailed insight into
5
In-domain Out-of-domain
System Dev set Test set Dev set Test set
Acc. AF Acc. AF Acc. AF Acc. AF
Scope baseline 84.2 75.6 82.4 72.1 35.5 43.3 36.0 44.3
Scope random corrupt 83.1 72.9 82.7 72.8 35.7 43.9 36.2 44.5
Scope biased corrupt 82.7 72.6 81.5 70.6 55.5 48.6 55.5 44.9
Trustpilot baseline 94.8 91.8 94.3 91.2 39.9 45.0 39.9 46.2
Trustpilot random corrupt 94.8 91.7 94.4 91.4 39.8 45.6 40.0 46.0
Trustpilot biased corrupt 93.7 89.0 93.4 89.5 43.6 45.7 43.4 44.7
Table 3: Evaluation on development and test sets measured in accuracy (Acc.) and the average f-score
for positive and negative instances (AF).
the model?s performance on the two ?extreme?
classes, but it is highly skewed, since it ignores the
neutral label. As we have seen in our data, this
can make up the majority of the instances. Ac-
curacy has the advantage that it provides a clear
picture of how often the system makes a correct
prediction, but can be harder to interpret when the
data sets are highly skewed in favor of one class.
The results show that randomly corrupting the
data (cf. S?gaard and Johannsen (2012), Sec. 5)
does not have much influence on the model. Per-
formance on in- and out-of-domain data is similar
to the baseline system. This indicates that we can
not just delete any features to help domain adapta-
tion.
The biased corruption model, on the other hand,
makes informed choices about deleting features.
As expected, this leads to a drop on in-domain
data, since we are underfitting the model. Con-
sidering that the algorithm is targeting the most
important features for this particular domain, the
drop is relatively small, though. The percentage
of features deleted is roughly the same as the 10%
for the random system (see section 3.3).
With the exception of AF on Trustpilot test,
our biased corruption approach always increases
out-of-domain performance. The increase is es-
pecially notable when the model is trained on the
small domain, Scope. On both test and develop-
ment, the corruption approach increases accuracy
more than 50%. On the AF measure, the increase
is smaller, which indicates that most of the in-
crease stems from the neutral category. On the
test set, the f-score for positive labels increases
from 49.1% to 71.2%, neutral increases from
13.5% to 18.4%, but negative decreases from
39.4% to 27.5%. The fact that f-score decreases on
negative indicates that the corruption algorithm
is too aggressive for this category. We previously
saw that this was the category where 35% of the
features are deleted.
The lower degree of overfitting in the corrupted
model is also reflected in the overall label distri-
bution. For the Scope system, the training data
has a negative/neutral/positive distribution (in per-
centages) of 27/61/12. The baseline predictions
on the Trustpilot data has a very similar distribu-
tion of 30/63/7, while the corrupted system results
in a very different distribution of 52/35/13, which
is more similar to the Trustpilot gold distribution
of 85/5/10. The KL divergence between the base-
line system and the Trustpilot data is 1.26, while
for the corrupted system it is 0.46.
5 Related Work
There is a large body of prior work on sen-
timent analysis (Pang and Lee, 2008), ranging
from work on well-edited newswire data using
the MPQA corpus (Wilson et al., 2005), to Ama-
zon reviews (Blitzer et al., 2007), blogs (Kessler
et al., 2010) and user-generated content such as
tweets (Mohammad et al., 2013). All of these
studies worked with English, while this study ? to
the best of our knowledge ? is the first to present
results for Danish.
As far as we are aware of, the only related work
on Danish is Hardt and Wulff (2012). In their ex-
ploratory paper, they investigate whether user pop-
ulations differ systematically in the way they ex-
press sentiment, finding that positive ratings are
far more common in U.S. reviews than in Danish
ones. However, their paper focuses on a quantita-
tive analysis and a single domain (movie reviews),
while we build an actual sentiment classification
system that performs well across domains.
Data corruption has been used for other NLP
6
tasks (S?gaard and Johannsen, 2012; S?gaard,
2013). Our random removal setup is basi-
cally an offline version of the approach presented
in (S?gaard and Johannsen, 2012). Their online
algorithm removes a random subset of the features
in each iteration and was successfully applied to
cross-domain experiments on part-of-speech tag-
ging and document classification. S?gaard (2013)
presents a follow-up online approach that takes
the weights of the current model into considera-
tion, regularizing the most predictive features. Our
biased approach is inspired by this, but has the ad-
vantage that it abstracts away from the underlying
learner.
6 Discussion and Future Work
We investigate cross-domain sentiment analysis
for a low-resource language, Danish. We observe
that performance drops precipitously when train-
ing on one domain and evaluating on the other. We
presented a robust offline-learning approach that
deletes features proportionate to their predictive-
ness. Applied to blind domain adaptation, this cor-
ruption method prevents overfitting to the source
domain, and results in relative improvements of
more than 50%.
In the future, we plan to experiment with in-
tegrating the weight distribution of a label into
the corruption function in order to prevent over-
corrupting of certain labels.
Acknowledgments
We would like to thank Daniel Hardt for host-
ing the Copenhagen Sentiment Analysis Work-
shop and making the data sets available. The last
two authors are supported by the ERC Starting
Grant LOWLANDS No. 313695.
References
Enrique Amig?o, Adolfo Corujo, Julio Gonzalo, Edgar
Meij, and Maarten de Rijke. 2012. Overview of
RepLab 2012: Evaluating Online Reputation Man-
agement Systems. In CLEF.
Enrique Amig?o, Jorge Carrillo de Albornoz, Irina
Chugur, Adolfo Corujo, Julio Gonzalo, Tamara
Mart??n, Edgar Meij, Maarten de Rijke, and Dami-
ano Spina. 2013. Overview of RepLab 2013: Eval-
uating Online Reputation Monitoring Systems. In
CLEF.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In ACL.
P.F. Brown, P.V. Desouza, R.L. Mercer, V.J. DellaPi-
etra, and J.C. Lai. 1992. Class-based n-gram mod-
els of natural language. Computational linguistics,
18(4):467?479.
Hal Daum?e, Abhishek Kumar, and Avishek Saha.
2010. Frustratingly easy semi-supervised domain
adaptation. In ACL Workshop on Domain Adapta-
tion for NLP.
Daniel Hardt and Julie Wulff. 2012. What is the mean-
ing of 5 *?s? An investigation of the expression and
rating of sentiment. In Proceedings of KONVENS
2012.
Jason S. Kessler, Miriam Eckert, Lyndsie Clark, and
Nicolas Nicolov. 2010. The 2010 ICWSM JDPA
sentiment corpus for the automotive domain. In
ICWSM-DWC.
Percy Liang. 2005. Semi-supervised learning for nat-
ural language. Ph.D. thesis, Massachusetts Institute
of Technology.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the State-
of-the-Art in Sentiment Analysis of Tweets. In
SemEval-2013.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM).
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Anders S?gaard and Anders Johannsen. 2012. Ro-
bust learning in random subspaces: Equipping nlp
for oov effects. In COLING.
Anders S?gaard. 2013. Part-of-speech tagging with
antagonistic adversaries. In ACL.
Sida Wang and Christopher D Manning. 2012. Fast
dropout training for logistic regression. In NIPS
workshop on log-linear models.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In EMNLP.
7
