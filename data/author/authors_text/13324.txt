Proceedings of the ACL 2010 Conference Short Papers, pages 49?54,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
The Prevalence of Descriptive Referring Expressions
in News and Narrative
Raquel Herva?s
Departamento de Ingenieria
del Software e Inteligenc??a Artificial
Universidad Complutense de Madrid
Madrid, 28040 Spain
raquelhb@fdi.ucm.es
Mark Alan Finlayson
Computer Science and
Artificial Intelligence Laboratory
Massachusetts Institute of Technology
Cambridge, MA, 02139 USA
markaf@mit.edu
Abstract
Generating referring expressions is a key
step in Natural Language Generation. Re-
searchers have focused almost exclusively
on generating distinctive referring expres-
sions, that is, referring expressions that
uniquely identify their intended referent.
While undoubtedly one of their most im-
portant functions, referring expressions
can be more than distinctive. In particular,
descriptive referring expressions ? those
that provide additional information not re-
quired for distinction ? are critical to flu-
ent, efficient, well-written text. We present
a corpus analysis in which approximately
one-fifth of 7,207 referring expressions in
24,422 words of news and narrative are de-
scriptive. These data show that if we are
ever to fully master natural language gen-
eration, especially for the genres of news
and narrative, researchers will need to de-
vote more attention to understanding how
to generate descriptive, and not just dis-
tinctive, referring expressions.
1 A Distinctive Focus
Generating referring expressions is a key step in
Natural Language Generation (NLG). From early
treatments in seminal papers by Appelt (1985)
and Reiter and Dale (1992) to the recent set
of Referring Expression Generation (REG) Chal-
lenges (Gatt et al, 2009) through different corpora
available for the community (Eugenio et al, 1998;
van Deemter et al, 2006; Viethen and Dale, 2008),
generating referring expressions has become one
of the most studied areas of NLG.
Researchers studying this area have, almost
without exception, focused exclusively on how
to generate distinctive referring expressions, that
is, referring expressions that unambiguously iden-
tify their intended referent. Referring expres-
sions, however, may be more than distinctive. It
is widely acknowledged that they can be used to
achieve multiple goals, above and beyond distinc-
tion. Here we focus on descriptive referring ex-
pressions, that is, referring expressions that are not
only distinctive, but provide additional informa-
tion not required for identifying their intended ref-
erent. Consider the following text, in which some
of the referring expressions have been underlined:
Once upon a time there was a man, who had
three daughters. They lived in a house and
their dresses were made of fabric.
While a bit strange, the text is perfectly well-
formed. All the referring expressions are distinc-
tive, in that we can properly identify the referents
of each expression. But the real text, the opening
lines to the folktale The Beauty and the Beast, is
actually much more lyrical:
Once upon a time there was a rich merchant,
who had three daughters. They lived in a
very fine house and their gowns were made
of the richest fabric sewn with jewels.
All the boldfaced portions ? namely, the choice
of head nouns, the addition of adjectives, the use
of appositive phrases ? serve to perform a descrip-
tive function, and, importantly, are all unneces-
sary for distinction! In all of these cases, the au-
thor is using the referring expressions as a vehi-
cle for communicating information about the ref-
erents. This descriptive information is sometimes
new, sometimes necessary for understanding the
text, and sometimes just for added flavor. But
when the expression is descriptive, as opposed to
distinctive, this additional information is not re-
quired for identifying the referent of the expres-
sion, and it is these sorts of referring expressions
that we will be concerned with here.
49
Although these sorts of referring expression
have been mostly ignored by researchers in this
area1, we show in this corpus study that descrip-
tive expressions are in fact quite prevalent: nearly
one-fifth of referring expressions in news and nar-
rative are descriptive. In particular, our data,
the trained judgments of native English speakers,
show that 18% of all distinctive referring expres-
sions in news and 17% of those in narrative folk-
tales are descriptive. With this as motivation, we
argue that descriptive referring expressions must
be studied more carefully, especially as the field
progresses from referring in a physical, immedi-
ate context (like that in the REG Challenges) to
generating more literary forms of text.
2 Corpus Annotation
This is a corpus study; our procedure was there-
fore to define our annotation guidelines (Sec-
tion 2.1), select texts to annotate (2.2), create an
annotation tool for our annotators (2.3), and, fi-
nally, train annotators, have them annotate refer-
ring expressions? constituents and function, and
then adjudicate the double-annotated texts into a
gold standard (2.4).
2.1 Definitions
We wrote an annotation guide explaining the dif-
ference between distinctive and descriptive refer-
ring expressions. We used the guide when train-
ing annotators, and it was available to them while
annotating. With limited space here we can only
give an outline of what is contained in the guide;
for full details see (Finlayson and Herva?s, 2010a).
Referring Expressions We defined referring
expressions as referential noun phrases and their
coreferential expressions, e.g., ?John kissed Mary.
She blushed.?. This included referring expressions
to generics (e.g., ?Lions are fierce?), dates, times,
and numbers, as well as events if they were re-
ferred to using a noun phrase. We included in each
referring expression all the determiners, quan-
tifiers, adjectives, appositives, and prepositional
phrases that syntactically attached to that expres-
sion. When referring expressions were nested, all
the nested referring expressions were also marked
separately.
Nuclei vs. Modifiers In the only previous cor-
pus study of descriptive referring expressions, on
1With the exception of a small amount of work, discussed
in Section 4.
museum labels, Cheng et al (2001) noted that de-
scriptive information is often integrated into refer-
ring expressions using modifiers to the head noun.
To study this, and to allow our results to be more
closely compared with Cheng?s, we had our an-
notators split referring expressions into their con-
stituents, portions called either nuclei or modifiers.
The nuclei were the portions of the referring ex-
pression that performed the ?core? referring func-
tion; the modifiers were those portions that could
be varied, syntactically speaking, independently of
the nuclei. Annotators then assigned a distinctive
or descriptive function to each constituent, rather
than the referring expression as a whole.
Normally, the nuclei corresponded to the head
of the noun phrase. In (1), the nucleus is the token
king, which we have here surrounded with square
brackets. The modifiers, surrounded by parenthe-
ses, are The and old.
(1) (The) (old) [king] was wise.
Phrasal modifiers were marked as single modi-
fiers, for example, in (2).
(2) (The) [roof] (of the house) collapsed.
It is significant that we had our annotators mark
and tag the nuclei of referring expressions. Cheng
and colleagues only mentioned the possibility that
additional information could be introduced in the
modifiers. However, O?Donnell et al (1998) ob-
served that often the choice of head noun can also
influence the function of a referring expression.
Consider (3), in which the word villain is used to
refer to the King.
The King assumed the throne today.(3)
I don?t trust (that) [villain] one bit.
The speaker could have merely used him to re-
fer to the King?the choice of that particular head
noun villain gives us additional information about
the disposition of the speaker. Thus villain is de-
scriptive.
Function: Distinctive vs. Descriptive As al-
ready noted, instead of tagging the whole re-
ferring expression, annotators tagged each con-
stituent (nuclei and modifiers) as distinctive or de-
scriptive.
The two main tests for determining descriptive-
ness were (a) if presence of the constituent was
unnecessary for identifying the referent, or (b) if
50
the constituent was expressed using unusual or os-
tentatious word choice. If either was true, the con-
stituent was considered descriptive; otherwise, it
was tagged as distinctive. In cases where the con-
stituent was completely irrelevant to identifying
the referent, it was tagged as descriptive. For ex-
ample, in the folktale The Princess and the Pea,
from which (1) was extracted, there is only one
king in the entire story. Thus, in that story, the
king is sufficient for identification, and therefore
the modifier old is descriptive. This points out the
importance of context in determining distinctive-
ness or descriptiveness; if there had been a room-
ful of kings, the tags on those modifiers would
have been reversed.
There is some question as to whether copular
predicates, such as the plumber in (4), are actually
referring expressions.
(4) John is the plumber
Our annotators marked and tagged these construc-
tions as normal referring expressions, but they
added an additional flag to identify them as cop-
ular predicates. We then excluded these construc-
tions from our final analysis. Note that copular
predicates were treated differently from apposi-
tives: in appositives the predicate was included in
the referring expression, and in most cases (again,
depending on context) was marked descriptive
(e.g., John, the plumber, slept.).
2.2 Text Selection
Our corpus comprised 62 texts, all originally writ-
ten in English, from two different genres, news
and folktales. We began with 30 folktales of dif-
ferent sizes, totaling 12,050 words. These texts
were used in a previous work on the influence of
dialogues on anaphora resolution algorithms (Ag-
garwal et al, 2009); they were assembled with an
eye toward including different styles, different au-
thors, and different time periods. Following this,
we matched, approximately, the number of words
in the folktales by selecting 32 texts from Wall
Street Journal section of the Penn Treebank (Mar-
cus et al, 1993). These texts were selected at ran-
dom from the first 200 texts in the corpus.
2.3 The Story Workbench
We used the Story Workbench application (Fin-
layson, 2008) to actually perform the annotation.
The Story Workbench is a semantic annotation
program that, among other things, includes the
ability to annotate referring expressions and coref-
erential relationships. We added the ability to an-
notate nuclei, modifiers, and their functions by
writing a workbench ?plugin? in Java that could
be installed in the application.
The Story Workbench is not yet available to the
public at large, being in a limited distribution beta
testing phase. The developers plan to release it as
free software within the next year. At that time,
we also plan to release our plugin as free, down-
loadable software.
2.4 Annotation & Adjudication
The main task of the study was the annotation of
the constituents of each referring expression, as
well as the function (distinctive or descriptive) of
each constituent. The system generated a first pass
of constituent analysis, but did not mark functions.
We hired two native English annotators, neither of
whom had any linguistics background, who cor-
rected these automatically-generated constituent
analyses, and tagged each constituent as descrip-
tive or distinctive. Every text was annotated by
both annotators. Adjudication of the differences
was conducted by discussion between the two an-
notators; the second author moderated these dis-
cussions and settled irreconcilable disagreements.
We followed a ?train-as-you-go? paradigm, where
there was no distinct training period, but rather
adjudication proceeded in step with annotation,
and annotators received feedback during those ses-
sions.
We calculated two measures of inter-annotator
agreement: a kappa statistic and an f-measure,
shown in Table 1. All of our f-measures indicated
that annotators agreed almost perfectly on the lo-
cation of referring expressions and their break-
down into constituents. These agreement calcu-
lations were performed on the annotators? original
corrected texts.
All the kappa statistics were calculated for two
tags (nuclei vs. modifier for the constituents, and
distinctive vs. descriptive for the functions) over
both each token assigned to a nucleus or modifier
and each referring expression pair. Our kappas in-
dicate moderate to good agreement, especially for
the folktales. These results are expected because
of the inherent subjectivity of language. During
the adjudication sessions it became clear that dif-
ferent people do not consider the same information
51
as obvious or descriptive for the same concepts,
and even the contexts deduced by each annotators
from the texts were sometimes substantially dif-
ferent.
Tales Articles Total
Ref. Exp. (F1) 1.00 0.99 0.99
Constituents (F1) 0.99 0.98 0.98
Nuc./Mod. (?) 0.97 0.95 0.96
Const. Func. (?) 0.61 0.48 0.54
Ref. Exp. Func. (?) 0.65 0.54 0.59
Table 1: Inter-annotator agreement measures
3 Results
Table 2 lists the primary results of the study. We
considered a referring expression descriptive if
any of its constituents were descriptive. Thus,
18% of the referring expressions in the corpus
added additional information beyond what was re-
quired to unambiguously identify their referent.
The results were similar in both genres.
Tales Articles Total
Texts 30 32 62
Words 12,050 12,372 24,422
Sentences 904 571 1,475
Ref. Exp. 3,681 3,526 7,207
Dist. Ref. Exp. 3,057 2,830 5,887
Desc. Ref. Exp. 609 672 1,281
% Dist. Ref. 83% 81% 82%
% Desc. Ref. 17% 19% 18%
Table 2: Primary results.
Table 3 contains the percentages of descriptive
and distinctive tags broken down by constituent.
Like Cheng?s results, our analysis shows that de-
scriptive referring expressions make up a signif-
icant fraction of all referring expressions. Al-
though Cheng did not examine nuclei, our results
show that the use of descriptive nuclei is small but
not negligible.
4 Relation to the Field
Researchers working on generating referring ex-
pressions typically acknowledge that referring ex-
pressions can perform functions other than distinc-
tion. Despite this widespread acknowledgment,
researchers have, for the most part, explicitly ig-
nored these functions. Exceptions to this trend
Tales Articles Total
Nuclei 3,666 3,502 7,168
Max. Nuc/Ref 1 1 1
Dist. Nuc. 95% 97% 96%
Desc. Nuc. 5% 3% 4%
Modifiers 2,277 3,627 5,904
Avg. Mod/Ref 0.6 1.0 0.8
Max. Mod/Ref 4 6 6
Dist. Mod. 78% 81% 80%
Desc. Mod. 22% 19% 20%
Table 3: Breakdown of Constituent Tags
are three. First is the general study of aggregation
in the process of referring expression generation.
Second and third are corpus studies by Cheng et al
(2001) and Jordan (2000a) that bear on the preva-
lence of descriptive referring expressions.
The NLG subtask of aggregation can be used
to imbue referring expressions with a descriptive
function (Reiter and Dale, 2000, ?5.3). There is a
specific kind of aggregation called embedding that
moves information from one clause to another in-
side the structure of a separate noun phrase. This
type of aggregation can be used to transform two
sentences such as ?The princess lived in a castle.
She was pretty? into ?The pretty princess lived in
a castle?. The adjective pretty, previously a cop-
ular predicate, becomes a descriptive modifier of
the reference to the princess, making the second
text more natural and fluent. This kind of ag-
gregation is widely used by humans for making
the discourse more compact and efficient. In or-
der to create NLG systems with this ability, we
must take into account the caveat, noted by Cheng
(1998), that any non-distinctive information in a
referring expression must not lead to confusion
about the distinctive function of the referring ex-
pression. This is by no means a trivial problem
? this sort of aggregation interferes with refer-
ring and coherence planning at both a local and
global level (Cheng and Mellish, 2000; Cheng et
al., 2001). It is clear, from the current state of the
art of NLG, that we have not yet obtained a deep
enough understanding of aggregation to enable us
to handle these interactions. More research on the
topic is needed.
Two previous corpus studies have looked at
the use of descriptive referring expressions. The
first showed explicitly that people craft descrip-
tive referring expressions to accomplish different
52
goals. Jordan and colleagues (Jordan, 2000b; Jor-
dan, 2000a) examined the use of referring expres-
sions using the COCONUT corpus (Eugenio et
al., 1998). They tested how domain and discourse
goals can influence the content of non-pronominal
referring expressions in a dialogue context, check-
ing whether or not a subject?s goals led them to in-
clude non-referring information in a referring ex-
pression. Their results are intriguing because they
point toward heretofore unexamined constraints,
utilities and expectations (possibly genre- or style-
dependent) that may underlie the use of descriptive
information to perform different functions, and are
not yet captured by aggregation modules in partic-
ular or NLG systems in general.
In the other corpus study, which partially in-
spired this work, Cheng and colleagues analyzed
a set of museum descriptions, the GNOME cor-
pus (Poesio, 2004), for the pragmatic functions of
referring expressions. They had three functions
in their study, in contrast to our two. Their first
function (marked by their uniq tag) was equiv-
alent to our distinctive function. The other two
were specializations of our descriptive tag, where
they differentiated between additional information
that helped to understand the text (int), or ad-
ditional information not necessary for understand-
ing (attr). Despite their annotators seeming to
have trouble distinguishing between the latter two
tags, they did achieve good overall inter-annotator
agreement. They identified 1,863 modifiers to
referring expressions in their corpus, of which
47.3% fulfilled a descriptive (attr or int) func-
tion. This is supportive of our main assertion,
namely, that descriptive referring expressions, not
only crucial for efficient and fluent text, are ac-
tually a significant phenomenon. It is interest-
ing, though, that Cheng?s fraction of descriptive
referring expression was so much higher than ours
(47.3% versus our 18%). We attribute this sub-
stantial difference to genre, in that Cheng stud-
ied museum labels, in which the writer is space-
constrained, having to pack a lot of information
into a small label. The issue bears further study,
and perhaps will lead to insights into differences
in writing style that may be attributed to author or
genre.
5 Contributions
We make two contributions in this paper.
First, we assembled, double-annotated, and ad-
judicated into a gold-standard a corpus of 24,422
words. We marked all referring expressions,
coreferential relations, and referring expression
constituents, and tagged each constituent as hav-
ing a descriptive or distinctive function. We wrote
an annotation guide and created software that al-
lows the annotation of this information in free text.
The corpus and the guide are available on-line in a
permanent digital archive (Finlayson and Herva?s,
2010a; Finlayson and Herva?s, 2010b). The soft-
ware will also be released in the same archive
when the Story Workbench annotation application
is released to the public. This corpus will be useful
for the automatic generation and analysis of both
descriptive and distinctive referring expressions.
Any kind of system intended to generate text as
humans do must take into account that identifica-
tion is not the only function of referring expres-
sions. Many analysis applications would benefit
from the automatic recognition of descriptive re-
ferring expressions.
Second, we demonstrated that descriptive refer-
ring expressions comprise a substantial fraction
(18%) of the referring expressions in news and
narrative. Along with museum descriptions, stud-
ied by Cheng, it seems that news and narrative are
genres where authors naturally use a large num-
ber of descriptive referring expressions. Given that
so little work has been done on descriptive refer-
ring expressions, this indicates that the field would
be well served by focusing more attention on this
phenomenon.
Acknowledgments
This work was supported in part by the Air
Force Office of Scientific Research under grant
number A9550-05-1-0321, as well as by the
Office of Naval Research under award number
N00014091059. Any opinions, findings, and con-
clusions or recommendations expressed in this pa-
per are those of the authors and do not necessarily
reflect the views of the Office of Naval Research.
This research is also partially funded the Span-
ish Ministry of Education and Science (TIN2009-
14659-C03-01) and Universidad Complutense de
Madrid (GR58/08). We also thank Whitman
Richards, Ozlem Uzuner, Peter Szolovits, Patrick
Winston, Pablo Gerva?s, and Mark Seifter for their
helpful comments and discussion, and thank our
annotators Saam Batmanghelidj and Geneva Trot-
ter.
53
References
Alaukik Aggarwal, Pablo Gerva?s, and Raquel Herva?s.
2009. Measuring the influence of errors induced by
the presence of dialogues in reference clustering of
narrative text. In Proceedings of ICON-2009: 7th
International Conference on Natural Language Pro-
cessing, India. Macmillan Publishers.
Douglas E. Appelt. 1985. Planning English referring
expressions. Artificial Intelligence, 26:1?33.
Hua Cheng and Chris Mellish. 2000. Capturing the in-
teraction between aggregation and text planning in
two generation systems. In INLG ?00: First interna-
tional conference on Natural Language Generation
2000, pages 186?193, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Hua Cheng, Massimo Poesio, Renate Henschel, and
Chris Mellish. 2001. Corpus-based np modifier
generation. In NAACL ?01: Second meeting of
the North American Chapter of the Association for
Computational Linguistics on Language technolo-
gies 2001, pages 1?8, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Hua Cheng. 1998. Embedding new information into
referring expressions. In ACL-36: Proceedings of
the 36th Annual Meeting of the Association for Com-
putational Linguistics and 17th International Con-
ference on Computational Linguistics, pages 1478?
1480, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Barbara Di Eugenio, Johanna D. Moore, Pamela W.
Jordan, and Richmond H. Thomason. 1998. An
empirical investigation of proposals in collabora-
tive dialogues. In Proceedings of the 17th inter-
national conference on Computational linguistics,
pages 325?329, Morristown, NJ, USA. Association
for Computational Linguistics.
Mark A. Finlayson and Raquel Herva?s. 2010a. Anno-
tation guide for the UCM/MIT indications, referring
expressions, and coreference corpus (UMIREC cor-
pus). Technical Report MIT-CSAIL-TR-2010-025,
MIT Computer Science and Artificial Intelligence
Laboratory. http://hdl.handle.net/1721.1/54765.
Mark A. Finlayson and Raquel Herva?s. 2010b.
UCM/MIT indications, referring expres-
sions, and coreference corpus (UMIREC
corpus). Work product, MIT Computer Sci-
ence and Artificial Intelligence Laboratory.
http://hdl.handle.net/1721.1/54766.
Mark A. Finlayson. 2008. Collecting semantics in
the wild: The Story Workbench. In Proceedings of
the AAAI Fall Symposium on Naturally-Inspired Ar-
tificial Intelligence, pages 46?53, Menlo Park, CA,
USA. AAAI Press.
Albert Gatt, Anja Belz, and Eric Kow. 2009. The
TUNA-REG challenge 2009: overview and evalu-
ation results. In ENLG ?09: Proceedings of the 12th
European Workshop on Natural Language Genera-
tion, pages 174?182, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Pamela W. Jordan. 2000a. Can nominal expressions
achieve multiple goals?: an empirical study. In ACL
?00: Proceedings of the 38th Annual Meeting on As-
sociation for Computational Linguistics, pages 142?
149, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Pamela W. Jordan. 2000b. Influences on attribute se-
lection in redescriptions: A corpus study. In Pro-
ceedings of CogSci2000, pages 250?255.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: the penn treebank. Compu-
tational Linguistics, 19(2):313?330.
Michael O?Donnell, Hua Cheng, and Janet Hitze-
man. 1998. Integrating referring and informing in
NP planning. In Proceedings of COLING-ACL?98
Workshop on the Computational Treatment of Nom-
inals, pages 46?56.
Massimo Poesio. 2004. Discourse annotation and
semantic annotation in the GNOME corpus. In
DiscAnnotation ?04: Proceedings of the 2004 ACL
Workshop on Discourse Annotation, pages 72?79,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Ehud Reiter and Robert Dale. 1992. A fast algorithm
for the generation of referring expressions. In Pro-
ceedings of the 14th conference on Computational
linguistics, Nantes, France.
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge Univer-
sity Press.
Kees van Deemter, Ielka van der Sluis, and Albert Gatt.
2006. Building a semantically transparent corpus
for the generation of referring expressions. In Pro-
ceedings of the 4th International Conference on Nat-
ural Language Generation (Special Session on Data
Sharing and Evaluation), INLG-06.
Jette Viethen and Robert Dale. 2008. The use of spa-
tial relations in referring expressions. In Proceed-
ings of the 5th International Conference on Natural
Language Generation.
54
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 20?24,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
Detecting Multi-Word Expressions improves Word Sense Disambiguation
Mark Alan Finlayson & Nidhi Kulkarni
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
Cambridge, MA, 02139, USA
{markaf,nidhik}@mit.edu
Abstract
Multi-Word Expressions (MWEs) are preva-
lent in text and are also, on average, less poly-
semous than mono-words. This suggests that
accurate MWE detection should lead to a non-
trivial improvement in Word Sense Disam-
biguation (WSD). We show that a straight-
forward MWE detection strategy, due to Ar-
ranz et al (2005), can increase a WSD al-
gorithm?s baseline f-measure by 5 percentage
points. Our measurements are consistent with
Arranz?s, and our study goes further by us-
ing a portion of the Semcor corpus containing
12,449 MWEs - over 30 times more than the
approximately 400 used by Arranz. We also
show that perfect MWE detection over Sem-
cor only nets a total 6 percentage point in-
crease in WSD f-measure; therefore there is
little room for improvement over the results
presented here. We provide our MWE detec-
tion algorithms, along with a general detection
framework, in a free, open-source Java library
called jMWE.
Multi-word expressions (MWEs) are prevalent
in text. This is important for the classic task of
Word Sense Disambiguation (WSD) (Agirre and Ed-
monds, 2007), in which an algorithm attempts to as-
sign to each word in a text the appropriate entry from
a sense inventory. A WSD algorithm that cannot cor-
rectly detect the MWEs that are listed in its sense in-
ventory will not only miss those sense assignments,
it will also spuriously assign senses to MWE con-
stituents that themselves have sense entries, dealing
a double-blow to WSD performance.
Beyond this penalty, MWEs listed in a sense in-
ventory also present an opportunity to WSD algo-
rithms - they are, on average, less polysemous than
mono-words. In Wordnet 1.6, multi-words have an
average polysemy of 1.07, versus 1.53 for mono-
words. As a concrete example, consider sentence
She broke the world record. In Wordnet 1.6 the
lemma world has nine different senses and record
has fourteen, while the MWE world record has only
one. If a WSD algorithm correctly detects MWEs,
it can dramatically reduce the number of possible
senses for such sentences.
Measure Us Arranz
Number of MWEs 12,449 382
Fraction of MWEs 7.4% 9.4%
WSD impr. (Best v. Baseline) 0.016F1 0.012F1
WSD impr. (Baseline v. None) 0.033F1 -
WSD impr. (Best v. None) 0.050F1 -
WSD impr. (Perfect v. None) 0.061F1 -
Table 1: Improvement of WSD f-measures over an
MWE-unaware WSD strategy for various MWE detec-
tion strategies. Baseline, Best, and Perfect refer to the
MWE detection strategy used in the WSD preprocess.
With this in mind, we expected that accurate
MWE detection will lead to a small yet non-trivial
improvement in WSD performance, and this is in-
deed the case. Table 1 summarizes our results. In
particular, a relatively straightforward MWE detec-
tion strategy, here called the ?best? strategy and due
to Arranz et al (2005), yielded a 5 percentage point
improvement1 in WSD f-measure. We also mea-
sured an improvement similar to that of Arranz when
1For example, if the WSD algorithm has an f-measure of
20
moving from a Baseline MWE detection strategy to
the Best strategy, namely, 1.6 percentage points to
their 1.2.
We performed our measurements over the brown1
and brown2 concordances2 of the Semcor cor-
pus (Fellbaum, 1998), which together contain
12,449 MWEs, over 30 times as many as the approx-
imately 400 contained in the portion of the XWN
corpus used by Arranz. We also measured the im-
provement for WSD f-measure for Baseline and Per-
fect MWE detection strategies. These strategies
improved WSD f-measure by 3.3 and 6.1 percent-
age points, respectively, showing that the relatively
straightforward Best MWE detection strategy, at 5.0
percentage points, leaves little room for improve-
ment.
1 MWE Detection Algorithms by Arranz
Arranz et al describe their TALP Word Sense Dis-
ambiguation system in (Castillo et al, 2004) and
(Arranz et al, 2005). The details of the WSD pro-
cedure are not critical here; what is important is
that their preprocessing system attempted to detect
MWEs that could later be disambiguated by the
WSD algorithm. This preprocessing occurred as a
pipeline that tokenized the text, assigned a part-of-
speech tag, and finally determined a lemma for each
stemmable word. This information was then passed
to a MWE candidate identifier3 whose output was
then filtered by an MWE selector. The resulting list
of MWEs, along with all remaining tokens, were
then passed into the WSD algorithm for disambigua-
tion.
The MWE identifier-selector pair determined
what combinations of tokens were marked as
MWEs. It considered only continuous (i.e., unbro-
ken) sequences of tokens whose order matched the
order of the constituents of the associated MWE en-
try in Wordnet. Because of morphological variation,
not all sequences of tokens are in base form; the
main function of the candidate identifier, therefore,
0.6, then a 5 percentage point increase yields an f-measure of
0.65.
2The third concordance, brownv, only has verbs marked, so
we did not test on it.
3Arranz calls the candidate identification stage the MWE de-
tector; we have renamed it because we take ?detection? to be the
end-to-end process of marking MWEs.
was to determine what morphological variation was
allowed for a particular MWE entry. They identified
and tested four different strategies:
1. None - no morphological variation allowed, all
MWEs must be in base form
2. Pattern - allows morphological variation ac-
cording to a set of pre-defined patterns
3. Form - a morphological variant is allowed if it
is observed in Semcor
4. All - all morphological variants allowed
The identification procedure produced a list of
candidate MWEs. These MWEs were then filtered
by the MWE selection process, which used one of
two strategies:
1. Longest Match, Left-to-Right - starting from
the left to right, selects the longest multi-word
expression found
2. Semcor - selects the multi-word expression
whose tokens have the maximum probability
of participating in an MWE, according to mea-
surements over Semcor
Arranz identified the None/Longest-Match-Left-
to-Right strategy as the Baseline, noting that this was
the most common strategy for MWE-aware WSD
algorithms. For this strategy the only MWE can-
didates allowed were those already in base form
(None), followed by resolution of conflicts by select-
ing the MWEs that started farthest to the left, choos-
ing the longest in case of ties (Longest-Match-Left-
to-Right);
Arranz?s Best strategy was Pattern/Semcor,
namely, allowing candidate MWEs to vary mor-
phologically according to a pre-defined set of syn-
tactic patterns (Pattern), followed by selecting the
most likely MWE based on examination of to-
ken frequencies in the Semcor corpus (Semcor).
They ran their detection strategies over a subset of
the sense-disambiguated glosses of the eXtended
WordNet (XWN) corpus (Moldovan and Novischi,
2004). They selected all glosses whose sense-
disambiguated words were all marked as ?gold?
quality, namely, reviewed by a human annota-
tor. Over this set of words, their WSD sys-
tem achieved 0.617F1 (0.622p/0.612r) when using
the Baseline MWE detection strategy, and 0.629F1
(0.638p/0.620r) when using the Best strategy.
21
2 Extension of Results
We implemented both the Baseline and Best MWE-
detection strategies, and used them as preprocessors
for a simple WSD algorithm, namely, the Most-
Frequent Sense algorithm. This algorithm simply
chooses, for each identified base form, the most fre-
quent sense in the sense inventory. We chose this
strategy, instead of re-implementing Arranz?s strat-
egy, for two reasons. First, our purpose was to study
the improvement MWE-detection provides to WSD
in general, not to a specific WSD algorithm. We
wished to show that, to the first order, MWE detec-
tion improves WSD irrespective of the WSD algo-
rithm chosen. Using a different algorithm than Ar-
ranz?s supports this claim. Second, for those wish-
ing to further this work, or build upon it, the Most-
Frequent-Sense strategy is easily implemented.
We used JSemcor (Finlayson, 2008a) to inter-
face with the Semcor data files. We used Word-
net version 1.6 with the original version of Sem-
cor4. Each token in each sentence in the brown1
and brown2 concordances of Semcor was assigned a
part of speech tag calculated using the Stanford Java
NLP library (Toutanova et al, 2003), as well as a
set of lemmas calculated using the MIT Java Word-
net Interface (Finlayson, 2008b). This data was the
input to each MWE detection strategy.
There was one major difference between our de-
tector implementations and Arranz, stemming from
a major difference between XWN and Semcor:
Semcor contains a large number of proper nouns,
whereas XWN glosses contain almost none. There-
fore our detector implementations included a simple
proper noun MWE detector, which marked all un-
broken runs of tokens tagged as proper nouns as a
proper noun MWE. This proper noun detector was
run first, before the Baseline and Best detectors, and
the proper noun MWEs detected took precedence
over the MWEs detected in later stages.
Baseline MWE Detection This MWE detec-
tion strategy was called None/Longest-Match-Left-
4The latest version of Wordnet is 3.0, but Semcor has not
been manually updated for Wordnet versions later than 1.6. Au-
tomatically updated versions of Semcor are available, but they
contain numerous errors resulting from deleted sense entries,
and the sense assignments and multi-word identifications have
not been adjusted to take into account new entries. Therefore
we decided to use versions 1.6 for both Wordnet and Semcor.
to-Right by Arranz; we implemented it in four
stages. First, we detected proper nouns, as de-
scribed. Second, for each sentence, the strategy
used the part of speech tags and lemmas to iden-
tify all possible consecutive MWEs, using a list ex-
tracted from WordNet 1.6 and Semcor 1.6. The
only restriction was that at least one token identi-
fied as part of the MWE must share the basic part of
speech (e.g., noun, verb, adjective, or adverb) with
the part of speech of the MWE. As noted, tokens that
were identified as being part of a proper noun MWE
were not included in this stage. In the third stage,
we removed all non-proper-noun MWEs that were
inflected?this corresponds to Arranz?s None stage.
In our final stage, any conflicts were resolved by
choosing the MWE with the leftmost token. For two
conflicting MWEs that started at the same token, the
longest MWE was chosen. This corresponds to Ar-
ranz?s Longest-Match-Left-to-Right selection.
Best MWE Detection This MWE detection strat-
egy was called Pattern/Semcor by Arranz, and we
also implemented this strategy in four stages. The
first and second stages were the same as the Baseline
strategy, namely, detection of proper nouns followed
by identification of continuous MWEs. The third
stage kept only MWEs whose morphological inflec-
tion matched one of the inflection rules described
by Arranz (Pattern). The final stage resolved any
conflicts by choosing the MWE whose constituent
tokens occur most frequently in Semcor as an MWE
rather than a sequence of monowords (Arranz?s Sem-
cor selection).
Word Sense Disambiguation No special tech-
nique was required to chain the Most-Frequent
Sense WSD algorithm with the MWE detection
strategies. We measured the performance of the
WSD algorithm using no MWE detection, the Base-
line detection, the Best detection, and Perfect detec-
tion5. These results are shown in Table 2.
Our improvement from Baseline to Best was ap-
proximately the same as Arranz?s: 1.7 percentage
points to their 1.2. We attribute the difference to the
much worse performance of our Baseline detection
algorithm: our Baseline MWE detection f-measure
was 0.552, compared their 0.740. The reason for this
5Perfect detection merely returned the MWEs identified in
Semcor
22
Measure Arranz et al (2005) Finlayson & Kulkarni
Corpus eXtended WordNet (XWN) 2.0 Semcor 1.6 (brown1 & brown2)
Number of Tokens (non-punctuation) 8,493 376,670
Number of Open-Class Tokens 5,133 196,852
Number of Open-Class Monowords 4,332 168,808
Number of Open-Class MWEs 382 12,449
Number of Tokens in Open-Class MWEs 801 28,044
Number of Open-Class Words (mono & multi) 4,714 181,257
Fraction MWEs 9.4% 7.4%
MWE Detection, Baseline 0.740F1 (0.765p/0.715r) 0.552F1 (0.452p/0.708r)
MWE Detection, Best 0.811F1 (0.806p/0.816r) 0.856F1 (0.874p/0.838r)
WSD, MWE-unaware - 0.579F1 (0.572p/0.585r)
WSD, Baseline MWE Detection 0.617F1 (0.622p/0.612r) 0.612F1 (0.614p/0.611r)
WSD, Best MWE Detection 0.629F1 (0.638p/0.620r) 0.629F1 (0.630p/0.628r)
WSD, Perfect MWE Detection - 0.640F1 (0.642p/0.638r)
WSD Improvement, Baseline vs. Best 0.012F1 (0.016p/0.008r) 0.016F1 (0.016p/0.017r)
WSD Improvement, Baseline vs. None - 0.033F1 (0.042p/0.025r)
WSD Improvement, Best vs. None - 0.050F1 (0.058p/0.043r)
WSD Improvement, Perfect vs. None - 0.061F1 (0.070p/0.053r)
Table 2: All the relevant numbers for the study. For purposes of comparison we recalculated the token counts for the
gold-annotated portion of the XWN corpus, and found discrepancies with Arranz?s reported values. They reported
1300 fully-gold-annotated glosses containing 397 MWEs; we found 1307 glosses containing 382 MWEs. The table
contains our token counts, but Arranz?s actual MWE detection and WSD f-measures, precisions, and recalls.
striking difference in Baseline performance seems to
be that, in the XWN glosses, a much higher fraction
of the MWEs are already in base form (e.g., nouns
in glosses are preferentially expressed as singular).
To encourage other researchers to build upon our
results, we provide our implementation of these
two MWE detection strategies, along with a gen-
eral MWE detection framework and numerous other
MWE detectors, in the form of a free, open-source
Java library called jMWE (Finlayson and Kulkarni,
2011a). Furthermore, to allow independent verifi-
cation of our results, we have placed all the source
code and data required to run these experiments
in an online repository (Finlayson and Kulkarni,
2011b).
3 Contributions
We have shown that accurately detecting multi-
word expressions allows a non-trivial improvement
in word sense disambiguation. Our Baseline, Best,
and Perfect MWE detection strategies show a 3.3,
5.1, and 6.1 percentage point improvement in WSD
f-measure. Our Baseline-to-Best improvement is
comparable with that measured by Arranz, the dif-
ference being due to more prevalent base-form
MWEs between XWN glosses and Semcor. The
very small improvement of the Perfect strategy over
the Best shows that, at least for Wordnet over texts
with an MWE distribution similar to Semcor, there
is little to be gained even from a highly-sophisticated
MWE detector. We have provided these two MWE
detection algorithms in a free, open-source Java li-
brary called jMWE.
Acknowledgments
This work was supported in part by the Air Force
Office of Scientific Research under grant num-
ber A9550-05-1-0321, and the Defense Advanced
Research Projects Agency under contract number
FA8750-10-1-0076. Thanks to Michael Fay for
helpful comments.
23
References
Eneko Agirre and Philip Edmonds. 2007. Word Sense
Disambiguation. Text, Speech, and Language Tech-
nology. Springer-Verlag, Dordrecht, The Netherlands.
Victoria Arranz, Jordi Atserias, and Mauro Castillo.
2005. Multiwords and word sense disambiguation.
In Alexander Gelbukh, editor, Proceedings of the
Sixth International Conference on Intelligent Text Pro-
cessing and Computational Linguistics (CICLING),
volume 3406 of Lecture Notes in Computer Sci-
ence (LNCS), pages 250?262, Mexico City, Mexico.
Springer-Verlag.
Mauro Castillo, Francis Real, Jordi Asterias, and German
Rigau. 2004. The TALP systems for disambiguat-
ing WordNet glosses. In Rada Mihalcea and Phil Ed-
monds, editors, Proceedings of Senseval-3: Third In-
ternational Workshop on the Evaluation of Systems for
the Semantic Analysis of Text, pages 93?96. Associa-
tion for Computational Linguistics.
Christiane Fellbaum. 1998. Wordnet: An Electronic Lex-
ical Database. MIT Press, Cambridge, MA.
Mark Alan Finlayson and Nidhi Kulkarni. 2011a.
jMWE, version 1.0.0.
http://projects.csail.mit.edu/jmwe
http://hdl.handle.net/1721.1/62793.
Mark Alan Finlayson and Nidhi Kulkarni. 2011b. Source
code and data for MWE?2011 papers.
http://hdl.handle.net/1721.1/62792.
Mark Alan Finlayson. 2008a. JSemcor, version 1.0.0.
http://projects.csail.mit.edu/jsemcor.
Mark Alan Finlayson. 2008b. JWI: The MIT Java Word-
net Interface, version 2.1.5.
http://projects.csail.mit.edu/jwi.
Dan Moldovan and Adrian Novischi. 2004. Word
sense disambiguation of WordNet glosses. Computer
Speech and Language, 18:301?317.
Kristina Toutanova, Daniel Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-
of-speech tagging with a cyclic dependency network.
pages 252?259. Proceedings of the Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics
(HLT-NAACL).
24
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 122?124,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
jMWE: A Java Toolkit for Detecting Multi-Word Expressions
Nidhi Kulkarni & Mark Alan Finlayson
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
Cambridge, MA, 02139, USA
{nidhik,markaf}@mit.edu
Abstract
jMWE is a Java library for implementing and
testing algorithms that detect Multi-Word Ex-
pression (MWE) tokens in text. It provides (1)
a detector API, including implementations of
several detectors, (2) facilities for construct-
ing indices of MWE types that may be used
by the detectors, and (3) a testing framework
for measuring the performance of a MWE de-
tector. The software is available for free down-
load.
jMWE is a Java library for constructing and test-
ing Multi-Word Expression (MWE) token detectors.
The original goal of the library was to detect tokens
(instances) of MWE types in a token stream, given a
list of types such as those that can be extracted from
an electronic dictionary such as WordNet (Fellbaum,
1998). The purpose of the library is not to discover
new MWE types, but rather find instances of a set of
given types in a given text. The library also supports
MWE detectors that are not list-based.
The functionality of the library is basic, but it is
a necessary foundation for any system that wishes
to use MWEs in later stages of language processing.
It is a natural complement to software for discover-
ing MWE types, such as mwetoolkit (Ramisch et
al., 2010) or the NSP package (Banerjee and Peder-
sen, 2003). jMWE is available online for free down-
load (Finlayson and Kulkarni, 2011a).
1 Library Facilities
Detector API The core of the library is the detector
API. The library defines a detector interface which
provides a single method for detecting MWE tokens
in a list of individual tokens; anyone interested in
taking advantage of jMWE?s testing infrastructure or
writing their own MWE token detection algorithm
need only implement this interface. jMWE pro-
vides several baseline MWE token detection strate-
gies. Also provided are detector filters, which apply
a specific constraint to, or resolve conflicts in, the
output another detector.
MWE Index jMWE also provides classes for con-
structing, storing, and accessing indices of valid
MWE types. An MWE index allows an algorithm
to retrieve a list of MWE types given a single word
token and part of speech. The index also lists how
frequently, in a particular concordance, a set of to-
kens appears as a particular MWE type rather than
as independent words. To facilitate construction
of indices, jMWE provides bindings to the MIT
Java Wordnet Interface (JWI) (Finlayson, 2008b)
and JSemcor (Finlayson, 2008a), as well as classes
which extract all MWE types from those resources
and write them to disk.
Test Harness The linchpin of jMWE?s testing in-
frastructure is a test harness that runs an MWE de-
tector over a given corpus and measures its precision
and recall. The library comes with default bindings
for running detectors over the Semcor corpus or any
other corpus that can be mounted with the JSemcor
library. Nevertheless, jMWE is not restricted to run-
ning tests over Semcor, or even restricted to using
JSemcor for interfacing with a corpus: a detector can
be run over any corpus whose MWE instances have
been marked can be analyzed, merely by implement-
ing four interfaces. Also included in the testing in-
122
frastructure are a number of error detectors, which
analyze the detailed output of the test harness to
identify common MWE token detection errors. The
library includes implementation for twelve standard
error types.
2 Detection Algorithms
Preprocessing To run an MWE detector over a text
the text must, at a minimum, be tokenized. jMWE
does not include facilities to do this; tokenization
must be done via an external library. Most detec-
tion strategies also require tokens to be tagged with
a part of speech and lemmatized. This information
is also not provided directly by jMWE, but there are
bindings in the library for using JWI and the Stan-
ford POS Tagger (Toutanova et al, 2003) to tag and
lemmatize a set of texts, provided those texts can be
accessed via the JSemcor library.
2.1 Detector Types
MWE token Detectors can be split into at least three
types: Basic Detectors, Filters, and Resolvers. Per-
formance of selected combinations of these detec-
tors are given in Table 1.
Basic Detectors that fall into this category use an
MWE index, or other source of information, to de-
tect MWE tokens in a stream of tokens. jMWE in-
cludes several implementations of basic detectors,
including the following:
(1) Exhaustive: Given a MWE type index, finds all
possible MWE tokens regardless of inflection, order,
or continuity.
(2) Consecutive: Given a MWE type index, finds all
MWE tokens whose constituent tokens occur with-
out other tokens interspersed.
(3) Simple Proper Noun: Finds all continuous se-
quences of proper noun tokens, and marks them as
proper noun MWE tokens.
Filters These MWE detectors apply a particular fil-
ter to the output of another, wrapped, detector. Only
MWE tokens from the wrapped detector that pass
the filter are returned. Examples of implemented fil-
ters are:
(1) In Order: Only returns MWE tokens whose con-
stituent tokens are in the same order as the con-
stituents listed in the MWE type?s definition.
(2) No Inflection: Removes inflected MWE tokens.
(3) Observed Inflection: Returns base form MWEs,
as well as those whose inflection has been observed
in a specified concordance.
(4) Pattern Inflection: Only return MWE tokens
whose inflection matches a pre-defined set of part
of speech patterns. We used the same rules as those
found in (Arranz et al, 2005) with two additional
rules related to Verb-Particle MWEs.
Resolvers Like filters, these wrap another MWE
detector; they resolve conflicts between identified
MWE tokens. A conflict occurs when two identified
MWE tokens share a constituent. Examples include:
(1) Longest-Match-Left-to-Right: For a set of con-
flicting MWE tokens, picks the one that starts earli-
est. If all of the conflicting MWE tokens start at the
same point, picks the longest.
(2) Observed Probability: For a set of conflicting
MWE tokens, picks the one whose constituents have
most often been observed occurring as an MWE to-
ken rather than as isolated words.
(3) Variance Minimizing: For a set of conflicting
MWE tokens, picks the MWE token with the fewest
interstitial spaces.
Detector F1 (precision/recall)
Exhaustive
+Proper Nouns
0.197F1 (0.110p/0.919r)
Consecutive
+Proper Nouns
0.631F1 (0.472p/0.950r)
Consecutive
+Proper Nouns
+No Inflection
+Longest-Match-L-to-R
0.593F1 (0.499p/0.731r)
Consecutive
+Proper Nouns
+Pattern Inflection
+More Frequent As MWE
0.834F1 (0.835p/0.832r)
Table 1: F-measures for select detectors, run over Sem-
cor 1.6 brown1 and brown2 concordances using MWEs
drawn from WordNet 1.6. The code for generating this
table is available at (Finlayson and Kulkarni, 2011b)
Acknowledgments
This work was supported in part by the AFOSR un-
der grant number A9550-05-1-0321, and DARPA
under award FA8750-10-1-0076.
123
References
Victoria Arranz, Jordi Atserias, and Mauro Castillo.
2005. Multiwords and word sense disambiguation. In
Alexander Gelbukh, editor, Proceedings of the Sixth
International Conference on Intelligent Text Process-
ing and Computational Linguistics (CICLING 2005),
volume 3406 in Lecture Notes in Computer Sci-
ence (LNCS), pages 250?262, Mexico City, Mexico.
Springer-Verlag.
Satanjeev Banerjee and Ted Pedersen. 2003. The de-
sign, implementation, and use of the ngram statistics
package. In Alexander Gelbukh, editor, Proceedings
of the Fourth International Conference on Intelligent
Text Processing and Computational Linguistics (CI-
CLING 2003), volume 2588 in Lecture Notes in Com-
puter Science (LNCS), pages 370?381, Mexico City,
Mexico. Springer-Verlag.
http://ngram.sourceforge.net.
Christiane Fellbaum. 1998. Wordnet: An Electronic Lex-
ical Database. MIT Press, Cambridge, MA.
Mark Alan Finlayson and Nidhi Kulkarni. 2011a.
jMWE:, version 1.0.0.
http://projects.csail.mit.edu/jmwe
http://hdl.handle.net/1721.1/62793.
Mark Alan Finlayson and Nidhi Kulkarni. 2011b. Source
code and data for MWE?2011 papers.
http://hdl.handle.net/1721.1/62792.
Mark Alan Finlayson. 2008a. JSemcor, version 1.0.0.
http://projects.csail.mit.edu/jsemcor.
Mark Alan Finlayson. 2008b. JWI: The MIT Java Word-
net Interface, version 2.1.5.
http://projects.csail.mit.edu/jwi.
Carlos Ramisch, Aline Villavicencio, and Christian
Boitet. 2010. Multiword expressions in the wild? the
mwetoolkit comes in handy. In Chu-Ren Huang and
Daniel Jurafsky, editors, Proceedings of the Twenty-
Third International Conference on Computational Lin-
guistics (COLING 2010): Demonstrations, volume 23,
pages 57?60, Beijing, China.
http://mwetoolkit.sourceforge.net.
Kristina Toutanova, Daniel Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network. In
Proceedings of the Human Language Technology Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics (HLT-NAACL),
pages 252?259, Edmonton, Canada.
124
