Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 641?648,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Concept Unification of Terms in Different Languages for IR 
 
Qing Li,  Sung-Hyon Myaeng 
Information & Communications 
University, Korea 
{liqing,myaeng}@icu.ac.kr
Yun Jin  
Chungnam National 
University, Korea 
wkim@cnu.ac.kr 
Bo-yeong Kang 
Seoul National University, 
Korea 
comeng99@snu.ac.kr 
 
  
 
Abstract 
Due to the historical and cultural reasons, 
English phases, especially the proper 
nouns and new words, frequently appear 
in Web pages written primarily in Asian 
languages such as Chinese and Korean. 
Although these English terms and their 
equivalences in the Asian languages refer 
to the same concept, they are erroneously 
treated as independent index units in tra-
ditional Information Retrieval (IR). This 
paper describes the degree to which the 
problem arises in IR and suggests a novel 
technique to solve it. Our method firstly 
extracts an English phrase from Asian 
language Web pages, and then unifies the 
extracted phrase and its equivalence(s) in 
the language as one index unit. Experi-
mental results show that the high preci-
sion of our conceptual unification ap-
proach greatly improves the IR perform-
ance. 
1 Introduction 
The mixed use of English and local languages 
presents a classical problem of vocabulary mis-
match in monolingual information retrieval 
(MIR). The problem is significant especially in 
Asian language because words in the local lan-
guages are often mixed with English words. Al-
though English terms and their equivalences in a 
local language refer to the same concept, they are 
erroneously treated as independent index units in 
traditional MIR. Such separation of semantically 
identical words in different languages may limit 
retrieval performance. For instance, as shown in 
Figure 1, there are three kinds of Chinese Web 
pages containing information related with 
?Viterbi Algorithm (?????)?.  The first 
case contains ?Viterbi Algorithm? but not its 
Chinese equivalence ???????. The second 
 
 
Figure 1.  Three Kinds of Web Pages 
contains ??????? but not ?Viterbi Algo-
rithm?. The third has both of them. A user would 
expect that a query with either ?Viterbi Algo-
rithm? or ??????? would retrieve all of 
these three groups of Chinese Web pages. Oth-
erwise some potentially useful information will 
be ignored.  
Furthermore, one English term may have sev-
eral corresponding terms in a different language. 
For instance, Korean words ?????, ?????, 
and ????? are found in local Web pages, 
which all correspond to the English word ?digi-
tal? but are in different forms because of differ-
ent phonetic interpretations. Establishing an 
equivalence class among the three Korean words 
and the English counterpart is indispensable. By 
doing so, although the query is ?????, the 
Web pages containing ?????, ????? or 
?digital? can be all retrieved. The same goes to 
Chinese terms. For example, two same semantic 
Chinese terms ????? and ????? corre-
spond to one English term ?Viterbi?. There 
should be a semantic equivalence relation be-
tween them. 
Although tracing the original English term 
from a term in a native language by back trans-
literation (Jeong et al, 1999) is a good way to 
build such mapping, it is only applicable to the 
words that are amenable for transliteration based 
on the phoneme. It is difficult to expand the 
method to abbreviations and compound words. 
641
Since English abbreviations frequently appear in 
Korean and Chinese texts, such as 
??????? (WTO)? in Korean, ?????
?? (WTO)? in Chinese, it is essential in IR to 
have a mapping between these English abbrevia-
tions and the corresponding words. The same 
applies to the compound words like ???? 
(Seoul National University)? in Korean, ????
(mad cow disease)? in Chinese. Realizing the 
limitation of the transliteration, we present a way 
to extract the key English phrases in local Web 
pages and conceptually unify them with their 
semantically identical terms in the local language.  
2 Concept Unification 
The essence of the concept unification of terms 
in different languages is similar to that of the 
query translation for cross-language information 
retrieval (CLIR) which has been widely explored 
(Cheng et al, 2004; Cao and Li, 2002; Fung et 
al., 1998; Lee, 2004; Nagata et al, 2001; Rapp, 
1999; Zhang et al, 2005; Zhang and Vine, 2004).  
For concept unification in index, firstly key Eng-
lish phrases should be extracted from local Web 
pages. After translating them into the local lan-
guage, the English phrase and their translation(s) 
are treated as the same index units for IR. Differ-
ent from previous work on query term translation 
that aims at finding relevant terms in another 
language for the target term in source language, 
conceptual unification requires a high translation 
precision. Although the fuzzy Chinese transla-
tions (e.g. ??? (virus), ???  (designer?s 
name), ???? (computer virus)) of  English 
term ?CIH? can enhance the CLIR performance 
by the ?query expansion? gain (Cheng et al, 
2004), it does not work in the conceptual unifica-
tion of terms in different languages for IR.   
While there are lots of additional sources to be 
utilized for phrase translation (e.g., anchor text, 
parallel or comparable corpus), we resort to the 
mixed language Web pages which are the local 
Web pages with some English words, because 
they are easily obtainable and frequently self-
refresh.  
Observing the fact that English words some-
times appear together with their equivalence in a 
local language in Web texts as shown in Figure 1, 
it is possible to mine the mixed language search-
result pages obtained from Web search engines 
and extract proper translations for these English 
words that are treated as queries. Due to the lan-
guage nature of Chinese and Korean, we inte-
grate the phoneme and semanteme instead of 
statistical information alone to pick out the right 
translation from the search-result pages. 
3 Key Phrase Extraction 
Since our intention is to unify the semantically 
identical words in different languages and index 
them together, the primary task is to decide what 
kinds of key English phrases in local Web pages 
are necessary to be conceptually unified.  
In (Jeong et al, 1999), it extracts the Korean 
foreign words for concept unification based on 
statistical information. Some of the English 
equivalences of these Korean foreign words, 
however, may not exist in the Korean Web pages. 
Therefore, it is meaningless to do the cross-
language concept unification for these words. 
The English equivalence would not benefit any 
retrieval performance since no local Web pages 
contain it, even if the search system builds a se-
mantic class among both local language and 
English for these words. In addition, the method 
for detecting Korean foreign words may bring 
some noise. The Korean terms detected as for-
eign words sometimes are not meaningful. 
Therefore, we do it the other way around by 
choosing the English phrases from the local Web 
pages based on a certain selection criteria.  
Instead of extracting all the English phrases in 
the local Web pages, we only select the English 
phrases that occurred within the special marks 
including quotation marks and parenthesis. Be-
cause English phrases within these markers re-
veal their significance in information searching 
to some extent. In addition, if the phrase starts 
with some stemming words (e.g., for, as) or in-
cludes some special sign, it is excluded as the 
phrases to be translated.  
4 Translation of English Phrases 
In order to translate the English phrases extracted, 
we query the search engine with English phrases 
to retrieve the local Web pages containing them. 
For each document returned, only the title and 
the query-biased summary are kept for further 
analysis. We dig out the translation(s) for the 
English phrases from these collected documents.  
4.1 Extraction of Candidates for Selection 
After querying the search engine with the Eng-
lish phrase, we can get the snippets (title and 
summary) of Web texts in the returned search-
result pages as shown in Figure 1. The next step 
then is to extract translation candidates within a 
window of a limited size, which includes the 
642
English phrase, in the snippets of Web texts in 
the returned search-result pages. Because of the 
agglutinative nature of the Chinese and Korean 
languages, we should group the words in the lo-
cal language into proper units as translation can-
didates, instead of treating each individual word 
as candidates. There are two typical ways: one is 
to group the words based on their co-occurrence 
information in the corpus (Cheng et al, 2004), 
and the other is to employ all sequential combi-
nations of the words as the candidates (Zhang 
and Vine, 2004). Although the first reduces the 
number of candidates, it risks losing the right 
combination of words as candidates. We adopt 
the second in our approach, so that,  return to the 
aforementioned example in Figure 1, if there are 
three Chinese characters (???) within the pre-
defined window, the translation candidates for 
English phrases ?Viterbi? are  ???,???, ???, 
??? ?, ????, and ?????. The number of 
candidates in the second method, however, is 
greatly increased by enlarging the window size 
k . Realizing that the number of words, n , avail-
able in the window size, k , is generally larger 
than the predefined maximum length of candi-
date, m ,  it is unreasonable to use all adjacent 
sequential combinations of available words 
within the window size k . Therefore, we tune 
the method as follows: 
1. If n m? , all adjacent sequential combina-
tions of words within the window are treated as 
candidates 
2. If n m> , only adjacent sequential combina-
tions of which the word number is less than m  
are regarded as candidates. For example, if we 
set n  to 4 and m  to 2, the window ? 1 2 3 4w w w w ? 
consists of four words. Therefore, only ? 1 2w w ?, 
? 2 3w w ?, ? 3 4w w ?, ? 1w ?, ? 2w ?? ? 3w ?, ? 4w ? are 
employed as the candidates for final translation 
selection.  
Based on our experiments, this tuning method 
achieves the same performance while reducing 
the candidate size greatly. 
4.2 Selection of candidates  
The final step is to select the proper candidate(s) 
as the translation(s) of the key English phrase. 
We present a method that considers the statistical, 
phonetic and semantic features of the English 
candidates for selection.  
Statistical information such as co-occurrence, 
Chi-square, mutual information between the 
English term and candidates helps distinguish the 
right translation(s). Using Cheng?s Chi-square 
method (Cheng et al, 2004), the probability to 
find the right translation for English specific 
term is around 30% in the top-1 case and 70% in 
the top-5 case. Since our goal is to find the corre-
sponding counterpart(s) of the English phrase to 
treat them as one index unit in IR, the accuracy 
level is not satisfactory. Since it seems difficult 
to improve the precision solely through variant 
statistical methods, we also consider semantic 
and phonetic information of candidates besides 
the statistical information. For example, given 
the English Key phrase ?Attack of the clones?, 
the right Korean translation ??????? is far 
away from the top-10 selected by Chi-square 
method (Cheng et al, 2004). However, based on 
the semantic match of ???? and ?Attack?, and 
the phonetic match of ???? and ?clones?, we 
can safely infer they are the right translation. The 
same rule applies to the Chinese translation  ??
??????, where ????? is phonetically 
match for ?clones? and ???? semantically cor-
responds to ?attack?.  
 In selection step, we first remove most of the 
noise candidates based on the statistical method 
and re-rank the candidates based on the semantic 
and phonetic similarity. 
4.3 Statistical model 
There are several statistical models to rank the 
candidates.  Nagata (2001) and Huang (2005) use 
the frequency of co-occurrence and the textual 
distance, the number of words between the Key 
phrase and candidates in texts to rank the candi-
dates, respectively. Although the details of the 
methods are quite different, both of them share 
the same assumption that the higher co-
occurrence between candidates and the Key 
phrase, the more possible they are the right trans-
lations for each other. In addition, they observed 
that most of the right translations for the Key 
phrase are close to it in the text, especially, right 
after or before the key phrase (e.g. ? ?  
?????(FBI)???). Zhang (2004) sug-
gested a statistical model based on the frequency 
of co-occurrence and the length of the candidates. 
In the model, since the distance between the key 
phrase and a candidate is not considered, the 
right translation located far away from the key 
phrase also has a chance to be selected. We ob-
serve, however, that such case is very rare in our 
study, and most of right translations are located 
within 5~8 words. The distance information is a 
valuable factor to be considered.  
643
In our statistical model, we consider the fre-
quency, length and location of candidates to-
gether. The intuition is that if the candidate is the 
right translation, it tends to co-occur with the key 
phrase frequently; its location tends to be close to 
the key phrase; and the longer the candidates? 
length, the higher the chance to be the right 
translation. The formula to calculate the ranking 
score for a candidate is as follows: 
1
( ) ( , )( , ) (1 )
max max
ki k i
FL i
len Freq len
len c d q cw q c ? ?
?
= ? + ? ?
?
 
where ( , )k id q c  is the word distance between the 
English phrase  q  and the candidate ic  in the k-
th occurrence of candidate in the search-result 
pages. If  q  is adjacent to ic  , the word distance 
is one. If there is one word between them, it is 
counted as two and so forth.  ?  is the coefficient 
constant, and maxFreq len?  is the max reciprocal of 
( , )k id q c  among all the candidates. ( )ilen c  is the 
number of characters in the candidate ic .  
4.4 Phonetic and semantic model 
Phonetic and semantic match: There has been 
some related work on extracting term translation 
based on the transliteration model (Kang and 
Choi, 2002; Kang and Kim, 2000). Different 
from transliteration that attempts to generate 
English transliteration given a foreign word in 
local language, our approach is a kind a match 
problem since we already have the candidates 
and aim at selecting the right candidates as the 
final translation(s) for the English key phrase. 
While the transliteration method is partially 
successful, it suffers form the problem that trans-
literation rules are not applied consistently. The 
English key phrase for which we are looking for 
the translation sometimes contains several words 
that may appear in a dictionary as an independent 
unit. Therefore, it can only be partially matched 
based on the phonetic similarity, and the rest part 
may be matched by the semantic similarity in 
such situation. Returning to the above example, 
?clone? is matched with ???? by phonetic 
similarity. ?of? and ?attack? are matched with 
??? and ???? respectively by semantic simi-
larity. The objective is to find a set of mappings 
between the English word(s) in the key phrase 
and the local language word(s) in candidates, 
which maximize the sum of the semantic and 
phonetic mapping weights. We call the sum as 
SSP (Score of semanteme and phoneme). The 
higher SSP value is, the higher the probability of 
the candidate to be the right translation.  
The solution for a maximization problem can 
be found using an exhaustive search method. 
However, the complexity is very high in practice 
for a large number of pairs to be processed.  As 
shown in Figure 2, the problem can be repre-
sented as a bipartite weighted graph matching 
problem. Let the English key phrase, E, be repre-
sented as a sequence of tokens 1,..., mew ew< > , and 
the candidate in local language, C, be repre-
sented as a sequence of tokens 1,..., ncw cw< > . 
Each English and candidate token is represented 
as a graph vertex. An edge ( , )i jew cw  is formed 
with the weight ( , )i jew cw?  calculated as the av-
erage of normalized semantic and phonetic val-
ues, whose calculation details are explained be-
low. In order to balance the number of vertices 
on both sides, we add the virtual vertex (vertices) 
with zero weight on the side with less number of 
vertices. The SSP is calculated:  
n
( )
i=1
SSP=argmax ( , )i ikw ew???
 
where ?  is a permutation of {1, 2, 3, ?, n}. It 
can be solved by the Kuhn-Munkres algorithm 
(also known as Hungarian algorithm) with poly-
nomial time complexity (Munkres, 1957).  
 
 
Figure 2. Matching based on the semanteme and 
phoneme 
Phonetic & Semantic Weights: If two lan-
guages have a close linguistic relationship such 
as English and French, cognate matching (Davis, 
1997) is typically employed to translate the un-
translatable terms. Interestingly, Buckley et al, 
(2000) points out that ?English query words are 
treated as potentially misspelled French words? 
and attempts to treat English words as variations 
of French words according to lexicographical 
rules.  However, when two languages are very 
distinct, e.g., English?Korean, English?Chinese, 
transliteration from English words is utilized for 
cognate matching. 
Phonetic weight is the transliteration probabil-
ity between English and candidates in local lan-
guage. We adopt the method in (Jeong et al, 
1999) with some adjustments. In essence, we 
compute the probabilities of particular English 
?? ?? ?
The of Clones Attack
644
key phrase EW given a candidate in the local 
language CW.  
1 1
1 1 1
( , ) ( ,..., , ,..., )
1( ,..., , ,..., ) log ( | ) ( | )
phoneme phoneme m k
phoneme n k j j j j
j
EW CW e e c c
g g c c P g g P c g
n
? ?
? ?
=
= = ?
 
where the English phrase consists of a string of 
English alphabets 1,..., me e , and the candidate in 
the local language is comprised of  a string of 
phonetic elements. 1,..., kc c . For Korean language, 
the phonetic element is the Korean alphabets 
such as ???, ???, ??? , ??? and etc. For Chi-
nese language, the phonetic elements mean the 
elements of ?pinying?.  ig  is a pronunciation unit 
comprised of one or more English alphabets 
( e.g., ?ss? for ???, a Korean alphabet ).  
The first term in the product corresponds to 
the transition probability between two states in 
HMM and the second term to the output prob-
ability for each possible output that could corre-
spond to the state, where the states are all possi-
ble distinct English pronunciation units for the 
given Korean or Chinese word. Because the dif-
ference between Korean/Chinese and English 
phonetic systems makes the above uni-gram 
model almost impractical in terms of output 
quality, bi-grams are applied to substitute the 
single alphabet in the above equation. Therefore, 
the phonetic weight should be calculated as:  
1 1 1 1
1( , ) log ( | ) ( | )phoneme j j j j j j j j
j
E C P g g g g P c c g g
n
? + ? + += ?
where 1 1( | )j j j jP c c g g+ +  is computed from the 
training corpus as the ratio between the fre-
quency of 1j jc c +  in the candidates, which were 
originated from 1j jg g + in English words, to the 
frequency of 1j jg g + . If 1j =  or j n= , 1jg ?  or 
1jg + , 1jc +  is substituted with a space marker.  
The semantic weight is calculated from the bi-
lingual dictionary. The current bilingual diction-
ary we employed for the local languages are Ko-
rean-English WorldNet and LDC Chinese-
English dictionary with additional entries in-
serted manually. The weight relies on the degree 
of overlaps between an English translation and 
the candidate  
semanteme
No. of  overlapping unitsw (E,C)= argmax
total No. of   units  
 
For example, given the English phrase ?Inha 
University? and its candidate ???? (Inha 
University), ?University? is translated into 
?????, therefore, the semantic weight be-
tween ?University? and ??? is about 0.33 be-
cause only one third of the full translation is 
available in the candidate. 
Due to the range difference between phonetic 
and semantic weights, we normalized them by 
dividing the maximum phonetic and semantic 
weights in each pair of the English phrase and a 
candidate if the maximum is larger than zero.  
The strategy for us to pick up the final transla-
tion(s) is distinct on two different aspects from 
the others. If the SSP values of all candidates are 
less than the threshold, the top one obtained by 
statistical model is selected as the final transla-
tion. Otherwise, we re-rank the candidates ac-
cording to the SSP value. Then we look down 
through the new rank list and draw a ?virtual? 
line if there is a big jump of SSP value. If there is 
no big jump of SSP values, the ?virtual? line is 
drawn at the bottom of the new rank list. Instead 
of the top-1 candidate, the candidates above the 
?virtual? line are all selected as the final transla-
tions. It is because that an English phrase may 
have more than one correct translation in the lo-
cal language. Return to the previous example, the 
English term ?Viterbi? corresponds to two Chi-
nese translations ????? and ?????. The 
candidate list based on the statistical information 
is ???, ??, ??, ???,?,????. We 
then calculate the SSP value of these candidates 
and re-rank the candidates whose SSP values are 
larger than the threshold which we set to 0.3. 
Since the SSP value of ????(0.91)? and ??
??(0.91)? are both larger than the threshold 
and there is no big jump, both of them are se-
lected as the final translation.  
5 Experimental Evaluation  
Although the technique we developed has values 
in their own right and can be applied for other 
language engineering fields such as query trans-
lation for CLIR, we intend to understand to what 
extent monolingual information retrieval effec-
tiveness can be increased when relevant terms in 
different language are treated as one unit while 
indexing. We first examine the translation preci-
sion and then study the impact of our approach 
for monolingual IR. 
We crawls the web pages of a specific domain 
(university & research) by WIRE crawler pro-
vided by center of Web Research, university of 
Chile (http://www.cwr.cl/projects/WIRE/). Cur-
rently, we have downloaded 32 sites with 5,847 
645
Korean Web pages and 74 sites with 13,765 Chi-
nese Web pages. 232 and 746 English terms 
were extracted from Korean Web pages and Chi-
nese Web pages, respectively.  The accuracy of 
unifying semantically identical words in different 
languages is dependant on the translation per-
formance. The translation results are shown in 
table 1.  As it can be observed, 77% of English 
terms from Korean web pages and 83% of Eng-
lish terms from Chinese Web pages can be 
strictly translated into accurate Korean and Chi-
nese, respectively. However, additional 15% and 
14% translations contained at least one Korean 
and Chinese translations, respectively.  The er-
rors were brought in by containing additional 
related information or incomplete translation. For 
instance, the English term ?blue chip? is trans-
lated into ???(blue chip)?, ???? (a kind of 
stock)?. However, another acceptable translation 
???? (a kind of stock)? is ignored.  An ex-
ample for incomplete translation is English 
phrase ? SIGIR 2005? which only can be trans-
late into ?????????? (international 
conference of computer information retrieval? 
ignoring the year.  
 
Korean Chinese  
No. % No. % 
Exactly correct 179 77% 618 83% 
At least one is 
correct but not all 35 15% 103 14% 
Wrong translation 18 8% 25 3% 
Total 232 100% 746 100% 
Table 1. Translation performance 
We also compare our approach with two well-
known translation systems. We selected 200 
English words and translate them into Chinese 
and Korean by these systems.  Table2 and Table 
3 show the results in terms of the top 1, 3, 5 in-
clusion rates for Korean and Chinese translation, 
respectively. ?Exactly and incomplete? transla-
tions are all regarded as the right translations.  
?LiveTrans? and ?Google? represent the systems 
against which we compared the translation abil-
ity. Google provides a machine translation func-
tion to translate text such as Web pages. Al-
though it works pretty well to translate sentences, 
it is ineligible for short terms where only a little 
contextual information is available for translation. 
LiveTrans (Cheng et al, 2004) provided by the 
WKD lab in Academia Sinica is the first un-
known word translation system based on web-
mining. There are two ways in this system to 
translate words: the fast one with lower precision 
is based on the ?chi-square? method ( 2? ) and the 
smart one with higher precision is based on ?con-
text-vector? method (CV) and ?chi-square? 
method ( 2? ) together. ?ST? and ?ST+PS? repre-
sent our approaches based on statistic model and 
statistic model plus phonetic and semantic model, 
respectively.   
 
 Top -1 Top-3 Top -5
Google 56% NA NA 
?Fast? 
2? 37% 43% 53.5%Live 
Trans ?Smart? 
2? +CV 
42% 49% 60% 
ST(dk=1) 28.5 % 41% 47% 
ST 39 % 46.5% 55.5%
Our 
Methods
ST+PS 93% 93% 93% 
Table 2. Comparison (Chinese case) 
 
 Top -1 Top-3 Top -5
Google 44% NA NA 
?Fast? 
2? 28% 37.5% 45% Live 
Trans ?Smart? 
2? +CV 
24.5% 44% 50% 
ST(dk=1) 26.5 % 35.5% 41.5%
ST 32 % 40% 46.5%
Our 
Methods
ST+PS 89% 89.5% 89.5%
Table 3. Comparison  (Korean case) 
 
Even though the overall performance of Li-
veTrans? combined method ( 2? +CV) is better 
than the simple method ( 2? ) in both Table 2 and 
3, the same doesn?t hold for each individual. For 
instance, ?Jordan?  is the English translation of 
Korean term   ?????, which  ranks 2nd and 
5th in ( 2? )  and ( 2? +CV), respectively. The con-
text-vector sometimes misguides the selection.  
In our two-step selection approach, the final 
selection would not be diverted by the false sta-
tistic information. In addition, in order to exam-
ine the contribution of distance information in 
the statistical method, we ran our experiments 
based on statistical method (ST) with two differ-
ent conditions. In the first case, we set  ( , )k id q c  to 
1, that is, the location information of all candi-
dates is ignored. In the second case, ( , )k id q c  is 
calculated based on the real textual distance of 
the candidates. As in both Table 2 and Table 3, 
the later case shows better performance. 
As shown in both Table 2 and Table 3, it can 
be observed that ?ST+PS? shows the best per-
formance, then followed by ?LiveTrans (smart)?, 
?ST?, ?LiveTrans(fast)?, and ?Google?. The sta-
646
tistical methods seem to be able to give a rough 
estimate for potential translations without giving 
high precision. Considering the contextual words 
surrounding the candidates and the English 
phrase can further improve the precision but still 
less than the improvement made by the phonetic 
and semantic information in our approach. High 
precision is very important to the practical appli-
cation of the translation results. The wrong trans-
lation sometimes leads to more damage to its 
later application than without any translation 
available.  For instance, the Chinese translation 
of ?viterbi? is ???(algorithm)? by LiveTrans 
(fast). Obviously, treating ?Viterbi? and  ??? 
(algorithm)?as one index unit is not acceptable.    
We ran monolingual retrieval experiment to 
examine the impact of our concept unification on 
IR. The retrieval system is based on the vector 
space model with our own indexing scheme to 
which the concept unification part was added. 
We employed the standard tf idf?  scheme for 
index term weighting and idf  for query term 
weighting. Our experiment is based on KT-SET 
test collection (Kim et al, 1994). It contains 934 
documents and 30 queries together with rele-
vance judgments for them. 
In our index scheme, we extracted the key 
English phrases in the Korean texts, and trans-
lated them. Each English phrases and its equiva-
lence(s) in Korean is treated as one index unit. 
The baseline against which we compared our 
approach applied a relatively simple indexing 
technique. It uses a dictionary that is Korean-
English WordNet, to identify index terms. The 
effectiveness of the baseline scheme is compara-
ble with other indexing methods (Lee and Ahn, 
1999). While there is a possibility that an index-
ing method with a full morphological analysis 
may perform better than our rather simple 
method, it would also suffer from the same prob-
lem, which can be alleviated by concept unifica-
tion approach. As shown in Figure 3, we ob-
tained 14.9 % improvement based on mean aver-
age 11-pt precision. It should be also noted that 
this result was obtained even with the errors 
made by the unification of semantically identical 
terms in different languages. 
6 Conclusion 
In this paper, we showed the importance of the 
unification of semantically identical terms in dif-
ferent languages for Asian monolingual informa-
tion retrieval, especially Chinese and Korean. 
Taking the utilization of the high translation ac-
curacy of our previous work, we successfully 
unified the most semantically identical terms in 
the corpus.  This is along the line of work where 
researchers attempt to index documents with 
concepts rather than words. We would extend 
our work along this road in the future. 
 
Recall
0.0 .2 .4 .6 .8 1.0
P
re
ci
si
on
0.0
.2
.4
.6
.8
1.0
Baseline
Conceptual Unification
 
Figure 3. Korean Monolingual IR 
Reference 
Buckley, C., Mitra, M., Janet, A. and Walz, C.C.. 
2000.  Using Clustering and Super Concepts within 
SMART: TREC 6. Information Processing & 
Management. 36(1): 109-131. 
Cao, Y. and Li., H.. 2002.  Base Noun Phrase Transla-
tion Using Web Data and the EM Algorithm. In 
Proc. of. the 19th COLING. 
Cheng, P.,  Teng, J., Chen, R., Wang, J., Liu,W., 
Chen, L.. 2004.  Translating Specific Queries with 
Web Corpora for Cross-language Information Re-
trieval. In Proc. of ACM SIGIR. 
Davis, M.. 1997. New Experiments in Cross-language 
Text Retrieval at NMSU's Computing Research 
Lab. In Proc. Of TREC-5. 
Fung, P. and Yee., L.Y.. 1998. An IR Approach for 
Translating New Words from Nonparallel, Compa-
rable Texts. In Proc. of  COLING/ACL-98. 
Huang, F., Zhang, Y. and Vogel, S.. 2005. Mining 
Key Phrase Translations from Web Corpora, In 
Proc. of the Human Language Technologies Con-
ference (HLT-EMNLP).   
Jeong, K. S., Myaeng, S. H., Lee, J. S., Choi, K. S.. 
1999. Automatic identification and back-
transliteration of foreign words for information re-
trieval. Information Processing & Management. 
35(4): 523-540. 
Kang, B. J., and Choi, K. S. 2002. Effective Foreign 
Word Extraction for Korean Information Retrieval. 
Information Processing & Management, 38(1): 91-
109. 
647
Kang, I. H. and Kim, G. C.. 2000. English-to-Korean 
Transliteration using Multiple Unbounded Over-
lapping Phoneme Chunks. In Proc. of COLING . 
Kim, S.-H. et al. 1994. Development of the Test Set 
for Testing Automatic Indexing. In Proc. of the 
22nd KISS Spring Conference. (in Korean).  
Lee, J, H. and Ahn, J. S.. 1996. Using N-grams for 
Korean Test Retrieval. In Proc. of SIGIR. 
Lee, J. S.. 2004.  Automatic Extraction of Translation 
Phrase Enclosed within Parentheses using Bilin-
gual Alignment Method. In Proc. of the 5th China-
Korea Joint Symposium on Oriental Language 
Processing and Pattern Recognition. 
Munkres, J.. 1957. Algorithms for the Assignment 
and Transportation Problems. J. Soc. Indust. Appl. 
Math., 5 (1957).  
Nagata, M., Saito, T., and Suzuki, K.. 2001. Using the 
Web as a Bilingual Dictionary. In Proc. of ACL 
'2001 DD-MT Workshop. 
Rapp, R.. 1999. Automatic Identification of Word 
Translations from Unrelated English and German 
corpora. In Proc. of ACL. 
Zhang, Y., Huang, F. and Vogel, S.. 2005. Mining 
Translations of OOV Terms from the Web through 
Cross-lingual Query Expansion, In Proc. of ACM 
SIGIR-05. 
Zhang, Y. and Vines, P.. 2004. Using the Web for 
Automated Translation Extraction in Cross-
Language Information Retrieval. In Proc. of ACM 
SIGIR-04. 
648
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 206?214,
Beijing, August 2010
Simplicity is Better: Revisiting Single Kernel PPI Extraction 
Sung-Pil Choi 
Information Technology Laboratory 
Korea Institute of Science and Technol-
ogy Information 
spchoi@kisti.re.kr 
Sung-Hyon Myaeng 
Department of Computer Science 
Korea Advanced Institute of Science and 
Technology 
myaeng@kaist.ac.kr 
 
Abstract 
It has been known that a combination of 
multiple kernels and addition of various 
resources are the best options for im-
proving effectiveness of kernel-based 
PPI extraction methods. These supple-
ments, however, involve extensive ker-
nel adaptation and feature selection 
processes, which attenuate the original 
benefits of the kernel methods. This pa-
per shows that we are able to achieve 
the best performance among the state-
of-the-art methods by using only a sin-
gle kernel, convolution parse tree kernel. 
In-depth analyses of the kernel reveal 
that the keys to the improvement are the 
tree pruning method and consideration 
of tree kernel decay factors. It is note-
worthy that we obtained the perfor-
mance without having to use any addi-
tional features, kernels or corpora. 
1 Introduction 
Protein-Protein Interaction (PPI) Extraction 
refers to an automatic extraction of the interac-
tions between multiple protein names from nat-
ural language sentences using linguistic features 
such as lexical clues and syntactic structures. A 
sentence may contain multiple protein names 
and relations, i.e., multiple PPIs. For example, 
the sentence in Fig.1 contains a total of six pro-
tein names of varying word lengths and three 
explicit interactions (relations). The interaction 
type between phosphoprotein and the acronym 
P in the parentheses is ?EQUAL.? A longer pro-
tein name phosphoprotein of vesicular stomati-
tis virus is related to nucleocapsid protein via 
?INTERACT? relation. Like the first PPI, nuc-
leocapsid protein is equivalent to the abbre-
viated term N.  
It is not straightforward to extract PPIs from 
a sentence or textual segment. There may be 
multiple protein names and their relationships, 
which are intertwined in a sentence. An interac-
tion type may be expressed in a number of dif-
ferent ways.  
 
Figure 1. An example sentence containing mul-
tiple PPIs involving different names of varying 
scopes and relations1  
 
A significant amount of efforts have been 
devoted to kernel-based approaches to PPI ex-
tractions (PPIE) as well as relation extractions2 
(Zhang et al, 2006; Pyysalo et al, 2008; Guo-
Dong et al, 2007; Zhang et al, 2008; Airola et 
al., 2008; Miwa et al, 2009). They include 
word feature kernels, parse tree kernels, and 
graph kernels. One of the benefits of using a 
kernel method is that it can keep the original 
                                                 
1 BioInfer, Sentence ID:BioInfer.d10.s0 
2 Relation extraction has been studied massively with the 
help of the ACE (www.nist.gov/tac) competition work-
shop and its corpora. The ACE corpora contain valuable 
information showing the traits of target entities (e.g., ent-
ity types, roles) for relation extraction in single sentences. 
Since all target entities are of the same type, protein 
name, in PPIE, however, we cannot use relational infor-
mation that exists among entity types. This makes PPIE 
more challenging.  
206
formation of target objects such as parse trees, 
not requiring extensive feature engineering for 
learning algorithms (Zelenko et al, 2003).  
In an effort to improve the performance of 
PPIE, researchers have developed not only new 
kernels but also methods for combining them 
(GuoDong et al, 2007; Zhang et al, 2008; Air-
ola et al, 2008; Miwa et al, 2009a; Miwa et al, 
2009b). While the intricate ways of combing 
various kernels and using extra resources have 
played the role of establishing strong baseline 
performance for PPIE, however, they are 
viewed as another form of engineering efforts. 
After all, one of the reasons the kernel methods 
have become popular is to avoid such engineer-
ing efforts. 
Instead, we focus on a state-of-the-art kernel 
and investigate how it can be best utilized for 
enhanced performance. We show that even with 
a single kernel, convolution parse tree kernel in 
this case, we can achieve superior performance 
in PPIE by devising an appropriate preprocess-
ing and factor adjustment method. The keys to 
the improvement are tree pruning and consider-
ation of a tree kernel decay factor, which are 
independent of the machine learning model 
used in this paper. The main contribution of our 
work is the extension and application of the 
particular convolution tree kernel method for 
PPIE, which gives a lesson that a deep analysis 
and a subsequent extension of a kernel for max-
imal performance can override the gains ob-
tained from engineering additional features or 
combining other kernels. 
The remaining part of the paper is organized 
as follows. In section 2, we survey the existing 
approaches. Section 3 introduces the parse tree 
kernel model and its algorithm. Section 4 ex-
plains the performance improving factors ap-
plied to the parse tree kernel. The architecture 
of our system is introduced in section 5. Section 
6 shows the improvements in effectiveness in 
multiple PPI corpora and finally we conclude 
our work in section 7. 
2 Related Work 
In recent years, numerous studies have at-
tempted to extract PPI automatically from text. 
Zhou and He (2008) classified various PPIE 
approaches into three categories: linguistic, 
rule-based and machine learning and statistical 
methods. 
Linguistic approaches involve constructing 
special grammars capable of syntactically ex-
pressing the interactions in sentences and then 
applying them to the language analyzers such as 
part-of-speech taggers, chunkers and parsers to 
extract PPIs. Based on the level of linguistic 
analyses, we can divide the linguistic approach-
es into two categories: shallow parsing (Seki-
mizu et al, 1998; Gondy et al, 2003) and full 
parsing methods (Temkin & Gilder, 2003; Ni-
kolai et al, 2004). 
Rule-based approaches use manually defined 
sets of lexical patterns and find text segments 
that match the patterns. Blaschke et al (1996) 
built a set of lexical rules based on clue words 
denoting interactions. Ono et al (2001) defined 
a group of lexical and syntactic interaction pat-
terns, embracing negative expressions, and ap-
plied them to extract PPIs from documents 
about ?Saccharomyces cerevisiae? and ?Esche-
richia coli?. Recently, Fundel et al (2007) pro-
posed a PPI extraction model based on more 
systematic rules using a dependency parser.  
Machine learning and statistical approaches 
have been around for a while but have recently 
become a dominant approach for PPI extraction. 
These methods involve building supervised or 
semi-supervised models based on training sets 
and various feature extraction methods (An-
drade & Valencia, 1998; Marcotte et al, 2001; 
Craven & Kumlien, 1999). Among them, ker-
nel-based methods have been studied extensive-
ly in recent years. Airola et al (2008) attempted 
to extract PPIs using a graph kernel by convert-
ing dependency parse trees into the correspond-
ing dependency graphs.  
Miwa et al (2009a) utilized multiple kernels 
such as word feature kernels, parse tree kernels, 
and even graph kernels in order to improve the 
performance of PPI extraction. Their experi-
ments based on five PPI corpora, however, 
showed that combining multiple kernels gave 
only minor improvements compared to other 
methods. To further improve the performance 
of the multiple kernel system, the same group 
combined multiple corpora to exploit additional 
features for a modified SVM model (Miwa et 
al., 2009b). While they achieved the best per-
formance in PPI extraction, it was possible only 
207
with additional kernels and corpora from which 
additional features were extracted.  
Unlike the aforementioned approaches trying 
to use all possible resources for performance 
enhancement, this paper aims at maximizing the 
performance of PPIE using only a single kernel 
without any additional resources. Without lo-
wering the performance, we attempt to stick to 
the initial benefits of the kernel methods: sim-
plicity and modularity (Shawe-Taylor & Cris-
tianini, 2004).  
3 Convolution Parse Tree Kernel 
Model for PPIE 
The main idea of a convolution parse tree ker-
nel is to sever a parse tree into its sub-trees and 
transfer it as a point in a vector space in which 
each axis denotes a particular sub-tree in the 
entire set of parse trees. If this set contains M 
unique sub-trees, the vector space becomes M-
dimensional. The similarity between two parse 
trees can be obtained by computing the inner 
product of the two corresponding vectors, 
which is the output of the parse tree kernel. 
There are two types of parse tree kernels of 
different forms of sub-trees: one is SubTree 
Kernel (STK) proposed by Vishwanathan and 
Smola (2003), and the other is SubSet Tree 
Kernel (SSTK) developed by Collins and Duffy 
(2001). In STK, each sub-tree should be a com-
plete tree rooted by a specific node in the entire 
tree and ended with leaf nodes. All the sub-trees 
must obey the production rules of the syntactic 
grammar. Meanwhile, SSTK can have any 
forms of sub-trees in the entire parse tree given 
that they should obey the production rules. It 
was shown that SSTK is much superior to STK 
in many tasks (Moschitti, 2006). He also intro-
duced a fast algorithm for computing a parse 
tree kernel and showed its beneficial effects on 
the semantic role labeling problem.  
A parse tree kernel can be computed by the 
following equation: 
             
                                             (1) 
where Ti is i
th parse tree and n1 and n2 are nodes 
in NT, the set of the entire nodes of T. ? 
represents a tree kernel decay factor, which will 
be explained later, and ? decides the way the 
tree is severed. Finally ?(n1, n2, ?, ?) counts the 
number of the common sub-trees of the two 
parse trees rooted by n1 and n2. Figure 2 shows 
the algorithm. 
In this algorithm, the get_children_number 
function returns the number of the direct child 
nodes of the current node in a tree. The function 
named get_node_value gives the value of a 
node such as part-of-speeches, phrase tags and 
words. The get_production_rule function finds 
the grammatical rule of the current node and its 
children by inspecting their relationship. 
 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
FUNCTION delta(TreeNode n1, TreeNode n2, ?, ?) 
n1 = one node of T1;  n2 = one node of T2; 
? = tree kernel decay factor;  ? = tree division me-
thod; 
BEGIN 
nc1 = get_children_number(n1);   
nc2 = get_children_number(n2); 
IF nc1 EQUAL 0 AND nc2 EQUAL 0 THEN     
nv1 = get_node_value(n1);   
nv2 = get_node_value(n2);  
IF nv1 EQUAL nv2 THEN RETURN 1; 
ENDIF 
np1 = get_production_rule(n1);   
np2 = get_production_rule(n2); 
IF np1 NOT EQUAL np2 THEN RETURN 0; 
 
IF np1 EQUAL np2 AND nc1 EQUAL 1  
AND nc2 EQUAL 1 THEN 
        RETURN ?; 
END IF 
 
mult_delta = 1; 
FOR I = 1 TO nc1 
nch1 = I
th child of n1;   nch2 = I
th child of n2; 
mult_delta = mult_delta ?  
(? + delta(nch1, nch2, ?, ?)); 
END FOR 
RETURN ? ? mult_delta; 
END 
Figure 2. ? (n1, n2, ?, ?) algorithm 
4 Performance Improving Factors 
4.1 Tree Pruning Methods 
Tree pruning for relation extraction was firstly 
introduced by Zhang et al (2006) and also re-
ferred to as ?tree shrinking task? for removing 
less related contexts. They suggested five types 
of the pruning methods and later invented two 
more in Zhang et al (2008). Among them, the 
path-enclosed tree (PT) method was shown to 
give the best result in the relation extraction 
task based on ACE corpus. We opted for this 
pruning method in our work.  
208
Figure 3 shows how the PT method prunes a 
tree. To focus on the pivotal context, it pre-
serves only the syntactic structure encompass-
ing the two proteins at hand and the words in 
between them (the part enclosed by the dotted 
lines). Without pruning, all the words like addi-
tion, increased and activity would intricately 
participate in deciding the interaction type of 
this sentence. 
 
Figure 3. Path-enclosed Tree (PT) Method 
 
Another important effect of the tree pruning 
is its ability to separate features when two or 
more interactions exist in a sentence. As in Fig-
ure 1, each interaction involves its unique con-
text even though a sentence has multiple inte-
ractions. With tree pruning, it is likely to extract 
context-sensitive features by ignoring external 
features. 
4.2 Tree Kernel Decay Factor 
Collins and Duffy (2001) addressed two prob-
lems of the parse tree kernel. The first one is 
that its kernel value tends to be largely domi-
nated by the size of two input trees. If they are 
large in size, it is highly probable for the kernel 
to accumulate a large number of overlapping 
counts in computing their similarity. Secondly, 
the kernel value of two identical parse trees can 
become overly large while the value of two dif-
ferent parse trees is much tiny in general. These 
two aspects can cause a trouble during a train-
ing phase because pairs of large parse trees that 
are similar to each other are disproportionately 
dominant. Consequently, the resulting models 
could act like nearest neighbor models (Collins 
and Duffy, 2001). 
To alleviate the problems, Collins and Duffy 
(2001) introduced a scalability parameter called 
decay factor, 0 < ? ? 1 which scales the relative 
importance of tree fragments with their sizes as 
in line 33 of Fig. 2. Based on the algorithm, a 
decay factor decreases the degree of contribu-
tion of a large sub-tree exponentially in kernel 
computation. Figure 4 illustrates both the way a 
tree kernel is computed and the effect of a de-
cay factor. In the figure, T1 and T2 share four 
common sub-trees (S1, S2, S3, S5). Let us assume 
that there are only two trees in a training set and 
only five unique sub-trees exist. Then each tree 
can be expressed by a vector whose elements 
are the number of particular sub-trees. Kernel 
value is obtained by computing the inner prod-
uct of the two vectors. As shown in the figure, 
S1 is a large sub-sub-trees, S1, S2 S3, and S4, two 
of which (S2, and S3) are duplicated in the inner 
product computation. It is highly probable for 
large sub-trees to contain many smaller sub-
trees, which lead to an over-estimated similarity 
value between two parse trees. As mentioned 
above, therefore, it is necessary to rein those 
large sub-trees with respect to their sizes in 
computing kernel values by using decay factors. 
In this paper, we treat the decay factor as one of 
the important optimization parameters for a PPI 
extraction task. 
Figure 4. The effect of decaying in comparing two trees. n(?) denotes #unique subtrees in a tree. 
209
5 Experimental Results 
In order to show the superiority of the simple 
kernel based method using the two factors used 
in this paper, compared to the resent results for 
PPIE using additional resources, we ran a series 
of experiments using the same PPI corpora 
cited in the literature. In addition, we show that 
the method is robust especially for cross-corpus 
experiments where a classifier is trained and 
tested with entirely different corpora.  
5.1 Evaluation Corpora 
To evaluate our approach for PPIE, we used 
?Five PPI Corpora3? organized by Pyysalo et al 
(2008). It contains five different PPI corpora: 
AImed, BioInfer, HPRD50, IEPA and LLL. 
They have been combined in a unified XML 
format and ?binarized? in case of involving 
multiple interaction types.  
Table 1. Five PPI Corpora 
 
Table 1 shows the size of each corpus in 
?Five PPI Corpora.? As mentioned before, a 
sentence can have multiple interactions, which 
results in the gaps between the number of sen-
tences and the sum of the number of instances. 
Negative instances have been automatically 
generated by enumerating sentences with mul-
tiple proteins but not having interactions be-
tween them (Pyysalo et al, 2008).  
5.2 Evaluation Settings 
In order to parse each sentence, we used Char-
niak Parser4. For kernel-based learning, we ex-
panded the original libsvm 2.895 (Chang & Lin, 
2001) so that it has two additional kernels in-
cluding parse tree kernel and composite kernel6 
along with four built-in kernels7 
Our experiment uses both macro-averaged 
and micro-averaged F-scores. Macro-averaging 
                                                 
3 http://mars.cs.utu.fi/PPICorpora/eval-standard.html 
4 http://www.cs.brown.edu/people/ec/#software 
5 http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 
6 A kernel combining built-in kernels and parse tree kernel 
7 Linear, polynomial, radial basis function, sigmoid ker-
nels 
computes F-scores for all the classes indivi-
dually and takes average of the scores. On the 
other hand, micro-averaging enumerates both 
positive results and negative results on the 
whole without considering the score of each 
class and computes total F-score.  
In 10-fold cross validation, we apply the 
same split used in Airola et al, (2008), Miwa et 
al., (2009a) and Miwa et al, (2009b) for com-
parisons. Also, we empirically estimate the re-
gularization parameters of SVM (C-values) by 
conducting 10-fold cross validation on each 
training data. We do not adjust the SVM thre-
sholds to the optimal value as in Airola et al, 
(2008) and Miwa et al, (2009a).  
5.3 PPI Extraction Performance 
Table 2 shows the best scores of our system. 
The optimal decay factor varies with each cor-
pus. In LLL, the optimal decay factor is 0.28 
indicating that the shortage of data has forced 
our system to normalize parse trees more inten-
sively with a strong decay factor in kernel com-
putation in order to cover various syntactic 
structures.  
 
 
DF AC ma-P ma-R ma-F ?ma-F 
A 0.6 83.6 
72.8 
(55.0) 
62.1 
(68.8) 
67.0 
(60.8) 
4.5 
(6.6) 
B 0.5 79.8 
74.5 
(65.7) 
70.9 
(71.1) 
72.6 
(68.1) 
2.7 
(3.2) 
H 0.7 74.5 
75.3 
(68.5) 
71.0 
(76.1) 
73.1 
(70.9) 
10.2 
(10.3) 
I 0.6 74.2 
74.1 
(67.5) 
72.2 
(78.6) 
73.1 
(71.7) 
6.0 
(7.8) 
L 0.2 82.2 
83.2 
(77.6) 
81.2 
(86.0) 
82.1 
(80.1) 
10.4 
(14.1) 
 
Table 2. The highest results of the proposed 
system w.r.t. decay factors. DF: Decay Factor, 
AC: accuracy, ma-F: macro-averaged F1, ?ma-F: 
standard deviation of F-scores in CV. A:AIMed, 
B:BioInfer, H:HPRD50, I:IEPA, L:LLL. The 
numbers in parentheses refer to the scores of 
Miwa et al, (2009a).  
 
Our system outperforms the previous results 
as in Table 2. Even using rich feature vectors 
including Bag-Of-Words and shortest path trees 
                                                 
8 It was determined by increasing it by 0.1 progressively 
through 10-fold cross validation. 
 
AIMed BioInfer HPRD50 IEPA LLL 
#Sentence 1,955 1,100 145 486 77 
#Positive  1,000 2,534 163 335 164 
#Negative  4,834 7,132 270 482 166 
210
generated from multiple corpora, Miwa et al, 
(2009b) reported 64.0% and 66.7% in AIMed 
and BioInfer, respectively. Our system, howev-
er, produced 67.0% in AIMed and 72.6% in 
BioInfer with a single parse tree kernel. We did 
not have to perform any intensive feature gen-
eration tasks using various linguistic analyzers 
and more importantly, did not use any addition-
al corpora for training as done in Miwa et al, 
(2009b). While the performance differences are 
not very big, we argue that obtaining higher 
performance values is significant because the 
proposed system did not use any of the addi-
tional efforts and resources.  
To investigate the effect of the scaling para-
meter of the parse tree kernel in PPI extraction, 
we measure how the performance changes as 
the decay factor varies (Figure 5). It is obvious 
that the decay factor influences the overall per-
formance of PPI extraction. Especially, the F-
scores of the small-scale corpora such as 
HPRD50 and LLL are influenced by the decay 
factor. The gaps between the best and worst 
scores in LLL and HPRD50 are 19.1% and 
5.2%, respectively. The fluctuation in F-scores 
of the large-scale corpora (AIMed, BioInfer, 
IEPA) is not so extreme, which seems to stem 
from the abundance in syntactic and lexical 
forms that reduce the normalizing effect of the 
decay factor. The increase in the decay factor 
leads to the increase in the precision values of 
all the corpora except for LLL. The phenome-
non is fairly plausible because the decreased 
normalization power causes the system to com-
pute the tree similarities more intensively and 
therefore it classifies each instance in a strict 
and detailed manner. On the contrary, the recall 
values slightly decrease with respect to the de-
cay factor, which indicates that the tree pruning 
(PT) has already conducted the normalization 
process to reduce the sparseness problem in 
each corpus. 
Most importantly, along with tree pruning, 
decay factor could boost the performance of our 
system by controlling the rigidness of the parse 
tree kernel in PPI extraction. 
Table 3 shows the results of the cross-corpus 
evaluation to measure the generalization power 
of our system as conducted in Airola et al, 
(2008) and Miwa et al, (2009a). Miwa et al, 
(2009b) executed a set of combinatorial expe-
riments by mixing multiple corpora and pre-
sented their results. Therefore, it is not reasona-
ble to compare our results with them due to the 
size discrepancy between training corpora. 
Nevertheless, we will compare our results with 
their approaches in later based on AIMed cor-
pus. 
As seen in Table 3, our system outperforms 
the existing approaches in almost all pairs of 
corpora. In particular, in the multiple corpora-
based evaluations aimed at AIMed which has 
been frequently used as a standard set in PPI 
extraction, our approach shows prominent re-
sults compared with others. While other ap-
proaches showed the performance ranging from 
33.3% to 60.8%, our approach achieved much 
higher scores between 55.9% and 67.0%. More 
specific observations are: 
(1) Our PPIE method trained on any corpus ex-
cept for IEPA outperforms the other approaches 
regardless of the test corpus only with a few 
exceptions with IEPA and LLL. 
(2) Even when using LLL or HPRD50, two 
smallest corpora, as training sets, our system 
performs well with every other corpus for test-
ing. It indicates that our approach is much less 
vulnerable to the sizes of training corpora than 
other methods. 
(3) The degree of score fluctuation of our sys-
tem across different testing corpora is much 
smaller than other regardless of the training da-
ta set. When trained on LLL, for example, the 
range for our system (55.9% ~ 82.1%) is small-
er than the others (38.6% ~ 83.2% and 33.3% ~ 
76.8%). 
(4) The cross-corpus evaluation reveals that our 
method outperforms the others significantly. 
This is more visibly shown especially when the 
large-scale corpora (AIMed and BioInfer) are 
used.  
(5) PPI extraction model trained on AIMed 
shows lower scores in IEPA and LLL as com-
pared with other methods, which could trigger 
further investigation. 
In order to convince ourselves further the su-
periority of the proposed method, we compare 
it with other previously reported approaches.  
Table 4 lists the macro-averaged precision, re-
call and F-scores of the nine approaches tested 
on AIMed. While the experimental settings are 
different as reported in the literature, they are 
quite close in terms of the numbers of positive 
and negative documents. 
211
As seen in the table, the proposed method is 
superior to all the others in F-scores. The im-
provement in precision (12.8%) is most signifi-
cant, especially in comparison with the work of 
Miwa et al, (2009b), which used multiple cor-
pora (AIMed + IEPA) for training and com-
bined various kernels such as bag-of-words, 
parse trees and graphs. It is natural that the re-
call value is lower since a less number of pat-
terns (features) must have been learned. What?s 
important is that the proposed method has a 
higher or at least comparable overall perfor-
mance without additional resources.  
Our approach is significantly better than that 
of Airola et al, (2008), which employed two 
different forms of graph kernels to improve the 
initial model. Since they did not use multiple 
corpora for training, the comparison shows the 
direct benefit of using the extension of the ker-
nel. 
6 Conclusion and Future Works 
To improve the performance of PPIE, recent 
research activities have had a tendency of in-
creasing the complexity of the systems by com-
bining various methods and resources. In this 
paper, however, we argue that by paying more  
Training 
corpora 
Systems 
F-Scores in the test corpora 
AIMed BioInfer HPRD50 IEPA LLL 
AIMed 
Our System 67.0  64.2  72.9  59.0  62.7  
(Miwa et al, 2009a) 60.8  53.1  68.3  68.1  73.5  
(Airola et al, 2008) 56.4  47.1  69.0  67.4  74.5  
BioInfer 
Our System 65.2  72.6  71.9  72.9  78.4  
(Miwa et al, 2009a) 49.6  68.1  68.3  71.4  76.9  
(Airola et al, 2008) 47.2  61.3  63.9  68.0  78.0  
HPRD50 
Our System 63.1  65.5  73.1  69.3  73.7  
(Miwa et al, 2009a) 43.9  48.6  70.9  67.8  72.2  
(Airola et al, 2008) 42.2  42.5  63.4  65.1  67.9  
IEPA 
Our System 57.8  66.1  66.3  73.1  78.4  
(Miwa et al, 2009a) 40.4  55.8  66.5  71.7  83.2  
(Airola et al, 2008) 39.1  51.7  67.5  75.1  77.6  
LLL 
Our System 55.9  64.4  69.4  71.4  82.1  
(Miwa et al, 2009a) 38.6  48.9  64.0  65.6  83.2  
(Airola et al, 2008) 33.3  42.5  59.8  64.9  76.8  
Table 3. Macro-averaged F1 scores in cross-corpora evaluation. Rows and columns correspond to 
the training and test corpora, respectively. We parallel our results with other recently reported re-
sults. All the split methods in 10-fold CV are the same for fair comparisons. 
    
Figure 5. Performance variation with respect to decay factor in Five PPI Corpora. Macro-
averaged F1 (left), Precision (middle), Recall (right) evaluated by 10-fold CV 
212
attention to a single model and adjusting para-
meters more carefully, we can obtain at least 
comparable performance if not better. 
This paper indicates that a well-tuned parse 
tree kernel based on decay factor can achieve 
the superior performance in PPIE when it is 
preprocessed by the path-enclosed tree pruning 
method. It was shown in a series of experiments 
that our system produced the best scores in sin-
gle corpus evaluation as well as cross-corpora 
validation in comparison with other state-of-
the-art methods. Contribution points of this pa-
per are as follows: 
(1) We have shown that the benefits of using 
additional resources including richer features 
can be obtained by tuning a single tree kernel 
method with tree pruning and decaying factors. 
(2) We have newly found that the decay factor 
influences precision enhancement of PPIE and 
hence its overall performance as well. 
(3) We have also revealed that the parse tree 
kernel method equipped with decay factors 
shows superior generalization power even with 
small corpora while presenting significant per-
formance increase on cross-corpora experi-
ments. 
As a future study, we leave experiments with 
training the classifier with multiple corpora and 
deeper analysis of what aspects of the corpora 
gave different magnitudes of the improvements. 
Acknowledgment 
We want to thank the anonymous reviewers 
for their valuable comments. This work has 
been supported in part by KRCF Grant, the Ko-
rean government. 
Reference 
Airola, A., Pyysalo, S., Bjorne, J., Pahikkala, T., 
Ginter, F. & Salakoski, T. (2008). All-paths graph 
kernel for protein-protein interaction extraction 
with evaluation of cross-corpus learning. BMC 
Bioinformatics, 9(S2), doi:10.1186/1471-2105-9-
S11-S2. 
Andrade, M. A. & Valencia, A. (1998). Automatic 
extraction of keywords from scientific text: appli-
cation to the knowledge domain of protein fami-
lies. Bioinformatics, 14(7), 600-607. 
Blaschke, C., Andrade, M., Ouzounis, C. & Valencia, 
A. (1999). Automatic extraction of biological in-
formation from scientific text: protein-protein in-
teractions. Proc. Int. Conf. Intell. Syst. Mol. Biol., 
(pp. 60-67). 
Bunescu, R., Ge, R., Kate, R., Marcotte, E., Mooney, 
R., Ramani, A. & Wong, Y. (2005).  Comparative 
Experiments on Learning Information Extractors 
for Proteins and their Interactions. Artif. Intell. 
Med., Summarization and Information Extraction 
from Medical Documents, 33, 139-155 
Collins, M. & Duffy, N. (2001). Convolution Ker-
nels for Natural Language. NIPS-2001, (pp. 625-
632). 
Craven, M. & Kumlien, J. (1999). Constructing bio-
logical knowledge bases by extracting informa-
tion from text sources. Proceedings of the 7th In-
ternational conference on intelligent systems for 
molecular biology, (pp.77-86), Heidelberg, Ger-
many. 
Ding, J., Berleant, D., Nettleton, D. & Wurtele, E. 
(2002). Mining MEDLINE: abstracts, sentences, 
or phrases?. Proceedings of PSB'02, (pp. 326-337) 
Erkan, G., Ozgur, A., & Radev, D. R., (2007). Semi-
supervised classification for extracting protein in-
  POS NEG ma-P ma-R ma-F ?F 
Our System 1,000 4,834 72.8 62.1 67.0 4.5 
(Miwa et al, 2009b) 1,000 4,834 60.0 71.9 65.2 
 
(Miwa et al, 2009a) 1,000 4,834 58.7 66.1 61.9 7.4 
(Miwa et al, 2008) 1,005 4,643 60.4 69.3 61.5 
 
(Miyao et al, 2008) 1,059 4,589 54.9 65.5 59.5 
 
(Giuliano et al, 2006) - - 60.9 57.2 59.0 
 
(Airola et al, 2008) 1,000 4,834 52.9 61.8 56.4 5.0 
(S? tre et al, 2007) 1,068 4,563 64.3 44.1 52.0 
 
(Erkan et al, 2007) 951 4,020 59.6 60.7 60.0 
 
(Bunescu & Mooney, 2005) - - 65.0 46.4 54.2 
 
Table 4. Comparative results in AIMed. The number of positive instances (POS) and negative in-
stances (NEG) and macro-averaged precision (ma-P), recall (ma-R) and F1-score (ma-F) are shown.  
213
teraction sentences using dependency parsing. In 
EMNLP 2007. 
Fundel, K., K?ffner, R. & Zimmer, R. (2007). RelEx 
? Relation extraction using dependency parse 
trees. Bioinformatics, 23, 365-371. 
Giuliano, C., Lavelli, A., Romano, L., (2006). Ex-
ploiting Shallow Linguistic Information for Rela-
tion Extraction From Biomedical Literature. Pro-
ceedings of the 11th Conference of the European 
Chapter of the Association for Computational 
Linguistics. 
Gondy, L., Hsinchun C. & Martinez Jesse D. (2003). 
A shallow parser based on closed-class words to 
capture relations in biomedical text. J. Biomed. 
Informatics. 36(3), 145-158. 
GuoDong, Z., Min, Z., Dong, H. J. & QiaoMing, Z. 
(2007). Tree Kernel-based Relation Extraction 
with Context-Sensitive Structured Parse Tree In-
formation. Proceedings of the 2007 Joint Confe-
rence on Empirical Methods in Natural Language 
Processing and Computational Natural Language 
Learning, Prague, (pp. 728?736) 
Marcotte, E. M., Xenarios, I. & Eisenberg D. (2001). 
Mining literature for protein-protein interactions. 
Bioinformatics, 17(4), 359-363. 
Miwa, M., S? tre, R., Miyao, Y. & Tsujii J. (2009a). 
Protein-protein interaction extraction by leverag-
ing multiple kernels and parsers. International 
Journal of Medical Informatics, 78(12), e39-e46. 
Miwa, M., S? tre, R., Miyao, Y. & Tsujii J. (2009b). 
A Rich Feature Vector for Protein-Protein Inte-
raction Extraction from Multiple Corpora. Pro-
ceedings of the 2009 Conference on Empirical 
Methods in Natural Language Processing, (pp. 
121-130) 
Miwa, M., S? tre, R., Miyao, Y., Ohta,  T., & Tsujii, 
J. (2008). Combining multiple layers of syntactic 
information for protein-protein interaction extrac-
tion. In Proceedings of the Third International 
Symposium on Semantic Mining in Biomedicine 
(SMBM 2008), (pp. 101?108) 
Miyao, Y., S? tre, R., Sagae, K., Matsuzaki, T., & 
Tsujii, J. (2008). Task-oriented evaluation of syn-
tactic parsers and their representations. Proceed-
ings of the 45th Meeting of the Association for 
Computational Linguistics (ACL?08:HLT). 
Moschitti, A. (2006). Making tree kernels practical 
for natural language learning.  Proceedings of 
EACL?06, Trento, Italy. 
 
Nikolai, D., Anton, Y., Sergei, E., Svetalana, N., 
Alexander, N. & llya, M. (2004). Extracting hu-
man protein interactions from MEDLINE using a 
full-sentence parser.  Bioinformatics, 20(5), 604-
611. 
Ono, T., Hishigaki, H., Tanigam, A. & Takagi, T. 
(2001). Automated extraction of information on 
protein-protein interactions from the biological li-
terature. Bioinformatics, 17(2), 155-161. 
Pyysalo, S., Airola, A., Heimonen, J., Bj?rne, J., 
Ginter, F. & Salakoski, T. (2008).  Comparative 
analysis of five protein-protein interaction corpo-
ra. BMC Bioinformatics, 9(S6), 
doi:10.1186/1471-2105-9-S3-S6. 
S? tre, R., Sagae, K., & Tsujii, J. (2007). Syntactic 
features for protein-protein interaction extraction. 
In LBM 2007 short papers. 
Sekimizu, T., Park H. S. & Tsujii J. (1998). Identify-
ing the interaction between genes and gene prod-
ucts based on frequently seen verbs in MEDLINE 
abstracts. Workshop on genome informatics, vol. 
9, (pp. 62-71). 
Shawe-Taylor, J., Cristianini, N., (2004). Kernel 
Methods for Pattern Analysis, Cambridge Univer-
sity Press. 
Temkin, J. M. & Gilder, M. R. (2003). Extraction of 
protein interaction information from unstructured 
text using a context-free grammar. Bioinformatics, 
19(16), 2046-2053. 
Vishwanathan, S. V. N., Smola, A. J. (2003). Fast 
Kernels for String and Tree Matching.  Advances 
in Neural Information Processing Systems, 15, 
569-576, MIT Press. 
Zhang, M., GuoDong, Z. & Aiti, A. (2008). Explor-
ing syntactic structured features over parse trees 
for relation extraction using kernel methods. In-
formation Processing and Management, 44, 687-
701 
Zhang, M., Zhang, J., Su, J. & Zhou, G. (2006). A 
Composite Kernel to Extract Relations between 
Entities with both Flat and Structured Features. 
21st International Conference on Computational 
Linguistics and 44th Annual Meeting of the ACL, 
(pp.825-832). 
Zhou, D. & He, Y. (2008). Extracting interactions 
between proteins from the literature. Journal of 
Biomedical Informatics, 41, 393-407. 
214
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1464?1472,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
 
 
Detecting Experiences from Weblogs 
 
 
Keun Chan Park, Yoonjae Jeong and Sung Hyon Myaeng 
Department of Computer Science 
Korea Advanced Institute of Science and Technology 
{keunchan, hybris, myaeng}@kaist.ac.kr 
 
 
  
Abstract 
Weblogs are a source of human activity know-
ledge comprising valuable information such as 
facts, opinions and personal experiences. In 
this paper, we propose a method for mining 
personal experiences from a large set of web-
logs. We define experience as knowledge em-
bedded in a collection of activities or events 
which an individual or group has actually un-
dergone. Based on an observation that expe-
rience-revealing sentences have a certain lin-
guistic style, we formulate the problem of de-
tecting experience as a classification task us-
ing various features including tense, mood, as-
pect, modality, experiencer, and verb classes. 
We also present an activity verb lexicon con-
struction method based on theories of lexical 
semantics. Our results demonstrate that the ac-
tivity verb lexicon plays a pivotal role among 
selected features in the classification perfor-
mance and shows that our proposed method 
outperforms the baseline significantly. 
1 Introduction 
In traditional philosophy, human beings are 
known to acquire knowledge mainly by reason-
ing and experience. Reasoning allows us to draw 
a conclusion based on evidence, but people tend 
to believe it firmly when they experience or ob-
serve it in the physical world. Despite the fact 
that direct experiences play a crucial role in mak-
ing a firm decision and solving a problem, 
people often resort to indirect experiences by 
reading written materials or asking around. 
Among many sources people resort to, the Web 
has become the largest one for human expe-
riences, especially with the proliferation of web-
logs.  
While Web documents contain various types 
of information including facts, encyclopedic 
knowledge, opinions, and experiences in general, 
personal experiences tend to be found in weblogs 
more often than other web documents like news 
articles, home pages, and scientific papers. As 
such, we have begun to see some research efforts 
in mining experience-related attributes such as 
time, location, topic, and experiencer, and their 
relations from weblogs (Inui et al, 2008; Kura-
shima et al, 2009).  
Mined experiences can be of practical use in 
wide application areas. For example, a collection 
of experiences from the people who visited a 
resort area would help planning what to do and 
how to do things correctly without having to 
spend time sifting through a variety of resources 
or rely on commercially-oriented sources. 
Another example would be a public service de-
partment gleaning information about how a park 
is being used at a specific location and time. 
Experiences can be recorded around a frame 
like ?who did what, when, where, and why? al-
though opinions and emotions can be also linked. 
Therefore attributes such as location, time, and 
activity and their relations must be extracted by 
devising a method for selecting experience-
containing sentences based on verbs that have a 
particular linguistics case frame or belong to a 
?do? class (Kurashima et al, 2009). However, 
this kind of method may extract the following 
sentences as containing an experience: 
[1] If Jason arrives on time, I?ll buy him a drink. 
[2] Probably, she will laugh and dance in his funeral. 
[3] Can anyone explain what is going on here? 
[4] Don?t play soccer on the roads! 
None of the sentences contain actual experiences 
because hypotheses, questions, and orders have 
not actually happened in the real world. For ex-
perience mining, it is important to ensure a sen-
tence mentions an event or passes a factuality 
test to contain experience (Inui et al, 2008).  
In this paper, we focus on the problem of de-
tecting experiences from weblogs. We formulate 
1464
  
Class Examples 
State like, know, believe 
Activity run, swim, walk 
Achievement recognize, realize 
Accomplishment 
paint (a picture), 
build (a house) 
Table 1. Vendler class examples 
the problem as a classification task using various 
linguistic features including tense, mood, aspect, 
modality, experiencer, and verb classes.  
Based on our observation that experience-
revealing sentences tend to have a certain lin-
guistic style (Jijkoun et al, 2010), we investigate 
on the roles of various features. The ability to 
detect experience-revealing sentences should be 
a precursor for ensuring the quality of extracting 
various elements of actual experiences. 
Another issue addressed in this paper is au-
tomatic construction of a lexicon for verbs re-
lated to activities and events. While there have 
been well-known studies about classifying verbs 
based on aspectual features (Vendler, 1967), 
thematic roles and selectional restrictions (Fill-
more, 1968; Somers, 1987; Kipper et al, 2008), 
valence alternations and intuitions (Levin, 1993) 
and conceptual structures (Fillmore and Baker, 
2001), we found that none of the existing lexical 
resources such as Framenet (Baker et al, 2003) 
and Verbnet (Kipper et al, 2008) are sufficient 
for identifying experience-revealing verbs. We 
introduce a method for constructing an activi-
ty/event verb lexicon based on Vendler?s theory 
and statistics obtained by utilizing a web search 
engine.  
We define experience as knowledge embed-
ded in a collection of activities or events which 
an individual or group has actually undergone1. It 
can be subjective as in opinions as well as objec-
tive, but our focus in this article lies in objective 
knowledge. The following sentences contain ob-
jective experiences: 
[5] I ran with my wife 3 times a week until we 
moved to Washington, D.C. 
[6] Jane and I hopped on a bus into the city centre. 
[7] We went to a restaurant near the central park. 
Whereas sentences like the following contain 
subjective knowledge: 
[8] I like your new style. You?re beautiful! 
[9] The food was great, the interior too.  
Subject knowledge has been studied extensively 
for various functions such as identification, po-
                                                 
1 http://en.wikipedia.org/wiki/Experience_(disambiguation) 
larity detection, and holder extraction under the 
names of opinion mining and sentiment analysis 
(Pang and Lee, 2008). 
In summary, our contribution lies in three as-
pects: 1) conception of experience detection, 
which is a precursor for experience mining, and 
specific related tasks that can be tackled with a 
high performance machine learning based solu-
tion; 2) examination and identification of salient 
linguistic features for experience detection; 3) a 
novel lexicon construction method with identifi-
cation of key features to be used for verb classi-
fication.  
The remainder of the paper is organized as fol-
lows. Section 2 presents our lexicon construction 
method with experiments. Section 3 describes 
the experience detection method, including expe-
rimental setup, evaluation, and results. In Section 
4, we discuss related work, before we close with 
conclusion and future work in Section 5.  
2 Lexicon Construction 
Since our definition of experience is based on 
activities and events, it is critical to determine 
whether a sentence contains a predicate describ-
ing an activity or an event. To this end, it is quite 
conceivable that a lexicon containing activity / 
event verbs would play a key role.  Given that 
our ultimate goal is to extract experiences from a 
large amount of weblogs, we opt for increased 
coverage by automatically constructing a lexicon 
rather than high precision obtainable by manual-
ly crafted lexicon.  
Based on the theory of Vendler (1967), we 
classify a given verb or a verb phrase into one of 
the two categories: activity and state. We consid-
er all the verbs and verb phrases in WordNet 
(Fellbaum, 1998) which is the largest electronic 
lexical database. In addition to the linguistic 
schemata features based on Vendler?s theory, we 
used thematic role features and an external 
knowledge feature.   
2.1 Background 
Vendler (1967) proposes that verb meanings can 
be categorized into four basic classes, states, ac-
tivities, achievements, and accomplishments, de-
pending on interactions between the verbs and 
their aspectual and temporal modifiers. Table 1 
shows some examples for the classes.  
Vendler (1967) and Dowty (1979) introduce 
linguistic schemata that serve as evidence for the 
classes. 
1465
  
Linguistic 
Schemata 
bs prs prp pts ptp 
No schema  ? ? ? ? ? 
Progressive   ?   
Force ?     
Persuade ?     
Stop   ?   
For ? ? ? ? ? 
Carefully ? ? ? ? ? 
Table 2. Query matrix. The ??? indicates that the 
query is applied. No Schema indicates that no 
schema is applied when the word itself is a query. 
bs, prs, prp, pts, ptp correspond to base form, 
present simple (3rd person singular), present par-
ticiple, past simple and past participle, respect-
fully.  
Below are the six schemata we chose because 
they can be tested automatically: progressive, 
force, persuade, stop, for, and carefully (An aste-
risk denotes that the statement is awkward). 
? States cannot occur in progressive tense: 
John is running. 
John is liking.* 
? States cannot occur as complements of 
force and persuade: 
John forced harry to run. 
John forced harry to know.* 
John persuaded harry to know.* 
? Achievements cannot occur as comple-
ments of stop: 
John stopped running. 
John stopped realizing.* 
? Achievements cannot occur with time ad-
verbial for: 
John ran for an hour. 
John realized for an hour.* 
? State and achievement cannot occur with 
adverb carefully: 
John runs carefully. 
John knows carefully.* 
The schemata are not perfect because verbs can 
shift classes due to various contextual factors 
such as arguments and senses. However, a verb 
certainly has its fundamental class that is its most 
natural category at least in its dominant use.  
The four classes can further be grouped into 
two genuses: a genus of processes going on in 
time and the other that refers to non-processes. 
Activity and accomplishment belong to the for-
mer whereas state and achievement belong to the 
latter. As can be seen in table 1, states are rather 
immanent operations and achievements are those 
occur in a single moment or operations related to 
perception level. On the other hand, activity and 
accomplishment are processes (transeunt opera-
tions) in traditional philosophy. We henceforth 
call the first genus activity and the latter state. 
Our aim is to classify verbs into the two genuses. 
2.2 Features based on Linguistic Schemata 
We developed a relatively simple computational 
testing method for the schemata. Assuming that 
an awkward expression like, ?John is liking 
something? won?t occur frequently, for example, 
we generated a co-occurrence based test for the 
first linguistic schema using the Web as a corpus. 
By issuing a search query, ((be OR am OR is OR 
was OR were OR been) and ? ing) where ??? 
represents the verb at hand, to a search engine, 
we can get an estimate about how the verb is 
likely to belong to state. A test can be generated 
for each of the schemata in a similar way. 
For completeness, we considered all the verb 
forms (i.e., 3rd person singular present, present 
participle, simple past, past participle) available. 
However, some of the patterns cannot be applied 
to some forms. For example, other forms except 
the base form cannot come as a complement of 
force (e.g., force to runs.*). Therefore, we 
created a query matrix which represents all query 
patterns we have applied, in table 2.  
Based on the query matrix in table 2, we is-
sued queries for all the verbs and verb phrases 
from WordNet to a search engine. We used the 
Google news archive search for two reasons. 
First, since news articles are written rather for-
mally compared to weblogs and other web pages, 
the statistics obtained for a test would be more 
reliable. Second, Google provides an advanced 
option to retrieve snippets containing the query 
word. Normally, a snippet is composed of 3~5 
sentences.  
The basic statistics we consider are hit count, 
candidate sentence count and correct sentence 
count which we use the notations Hij(w), Sij(w), 
and Cij(w), respectfully, where w is a word, i the 
linguistic schema and j the verb form from the 
query matrix in table 2. Hij(w) was directly ga-
thered from the Google search engine. Sij(w) is 
the number of sentences containing the word w 
in the search result snippets. Cij(w) is the number 
of correct sentences matching the query pattern 
among the candidate sentences. For example, the 
progressive schema for a verb ?build? can re-
trieve the following sentences. 
[10]   ?, New-York, is building one of the largest ? 
[11]   Is building an artifact? 
1466
  
?Building? in the first example is a progressive 
verb, but the one in second is a noun, which does 
not satisfy the linguistic schema. For a POS and 
grammatical check of a candidate sentence, we 
used the Stanford POS tagger (Toutanova et al, 
2003) and Stanford dependency parser (Klein 
and Manning, 2003).  
For each linguistic schema, we derived three 
features: Absolute hit ratio, Relative hit ratio and 
Valid ratio for which we use the notations Ai(w), 
Ri(w) and Vi(w), respectfully, where w is a word 
and i a linguistic schema. The index j for summa-
tions represents the j-th verb form. They are 
computed as follows. 
 
( )
( )
( )
( )
( )
( )
( )
( )
( )
*
ijj
i
i
ijj
i
No Schemej
ijj
i
ijj
H w
A w
H
H w
R w
H w
C w
V w
S w
=
=
=
?
?
?
?
?
 (1) 
Absolute hit ratio is computes the extent to 
which the target word w occurs with the i-th 
schema over all occurrences of the schema. The 
denominator is the hit count of wild card ?*? 
matching any single word with the schema pat-
tern from Google (e.g., H1(*), the progressive 
test hit count is 3.82 ? 108). Relative hit ratio 
computes the extent to which the target word w 
occurs with the i-th schema over all occurrences 
of the word. The denominator is the sum of all 
verb forms. Valid ratio means the fraction of cor-
rect sentences among candidate sentences. The 
weight of a linguistic schema increases as the 
valid ratio gets high. With the three different 
ratios, Ai(w), Ri(w) and Vi(w), for each test, we 
can generate a total of 18 features.  
2.3 Features based on case frames 
Since the hit count via Google API sometimes 
returns unreliable results (e.g., when the query 
becomes too long in case of long verb phrases), 
we also consider additional features. While our 
initial observation indicated that the existing lex-
ical resources would not be sufficient for our 
goal, it occurred to us that the linguistic theory 
behind them would be worth exploring as gene-
rating additional features for categorizing verbs 
for the two classes. Consider the following ex-
amples:  
[12]   John(D) believed(V) the story(O). 
[13]   John(A) hit(V) him(O) with a bat(I). 
The subject of a state verb is dative (D) as in [12] 
whereas the subject for an action verb takes the 
agent (A) role. In addition, a verb with the in-
strument (I) role tends to be an action verb. From 
these observations, we can use the distribution of 
cases (thematic roles) for a verb in a corpus. Ac-
tivity verbs are expected to have high frequency 
of agent and instrument roles than state verbs. 
Although a verb may have more than one case 
frame, it is possible to determine which thematic 
roles used more dominantly. 
We utilize two major resources of lexical se-
mantics, Verbnet (Kipper et al, 2008) based on 
the theory of Levin (1993), and Framenet (Baker 
et al, 2003), which is based on Fillmore (1968). 
Levin (1993) demonstrated that syntactic alterna-
tions can be the basis for groupings of verbs se-
mantically and accord reasonably well with lin-
guistic intuitions. Verbnet provides 274 verb 
classes with 23 thematic roles covering 3,769 
verbs based on their alternation behaviors with 
thematic roles annotated. Framenet defines 978 
semantic frames with 7,124 unique semantic 
roles, covering 11,583 words including verbs, 
nouns, adverbs, etc.  
Using Verbnet alne does not suit our needs 
because it has a relatively small number of ex-
ample sentences. Framenet contains a much larg-
er number of examples but the vast number of 
semantic roles presents a problem. In order to get 
meaningful distributions for a manageable num-
ber of thematic roles, we used Semlink (Loper et 
al., 2007) that provides a mapping between Fra-
menet and Verbnet and uses a total of 23 themat-
ic roles of Verbnet for the annotated corpora of 
the two resources. By the mapping, we obtained 
distributions of the thematic roles for 2,868 
unique verbs that exist in both of the resources. 
For example, the verb ?construct? has high fre-
quencies with agent, material and product roles.  
2.4 Features based on how-to instructions 
Ryu et al (2010) presented a method for extract-
ing action steps for how-to goals from eHow2 a 
website containing a large number of how-to in-
structions. The authors attempted to extract ac-
tions comprising a verb and some ingredients 
like an object entity from the documents based 
on syntactic patterns and a CRF based model.  
Since each extracted action has its probability, 
we can use the value as a feature for state / activ-
ity verb classification. However, a verb may ap-
pear in different contexts and can have multiple 
                                                 
2 http://www.ehow.com 
1467
  
Feature 
ME SVM 
Prec. Recall Prec. Recall 
All 43 68% 50% 83% 75% 
Top 30 72% 52% 83% 75% 
Top 20 83% 76% 85% 77% 
Top 10 89% 88% 91% 78% 
Table 3. Classification Performance 
Class Examples 
Activity 
act, battle, build, carry, chase, 
drive, hike, jump, kick, sky 
dive, tap dance, walk, ? 
State 
admire, believe, know, like, 
love, ? 
Table 4. Classified Examples 
probability values. To generate a single value for 
a verb, we combine multiple probability values 
using the following sigmoid function:  
 
1
( )
1
( )
w
t
dd D
E w
e
t P w
?
?
=
+
=?
 (2) 
Evidence of a word w being an action in eHow is 
denoted as E(w) where variable t is the sum of 
individual action probability values in Dw the set 
of documents from which the word w has been 
extracted as an action. The higher probability a 
word gets and the more frequent the word has 
been extracted as an action, the more evidence 
we get.  
2.5 Classification 
For training, we selected 80 seed verbs from 
Dowty?s list (1979) which are representative 
verbs for each Vendler (1967) class. The selec-
tion was based on the lack of word sense ambi-
guity.  
One of our classifiers is based on Maximum 
Entropy (ME) models that implement the intui-
tion that the best model will be the one that is 
consistent with the set of constraints imposed by 
the evidence, but otherwise is as uniform as 
possible (Berger et al, 1996). ME models are 
widely used in natural language processing tasks 
for its flexibility to incorporate a diverse range of 
features. The other one is based on Support Vec-
tor Machine (Chang and Lin, 2001) which is the 
state-of-the-art algorithm for many classification 
tasks.  We used RBF kernel with the default set-
tings (Hsu et al, 2009) because it is been known 
to show moderate performance using multiple 
feature compositions. 
The features we considered are a total of 42 
real values: 18 from linguistic schemata, 23 the-
matic role distributions, and one from eHow. In 
order to examine which features are discrimina-
tive for the classification, we used two well 
known feature selection methods, Chi-square and 
information gain.  
2.6 Results 
Table 3 shows the classification performance 
values for different feature selection methods. 
The evaluation was done on the training data 
with 10-fold cross validation.  
Note that the precision and recall are macro-
averaged values across the two classes, activity 
and state. The most discriminative features were 
absolute ratio and relative ratio in conjunction 
with the force, stop, progressive, and persuade 
schemata, the role distribution of experiencer, 
and the eHow evidence.  
It is noteworthy that eHow evidence and the 
distribution of experiencer got into the top 10. 
Other thematic roles did not perform well be-
cause of the data sparseness. Only a few roles 
(e.g., experience, agent, topic, location) among 
the 23 had frequency values other than 0 for 
many verbs. Data sparseness affected the linguis-
tic schemata as well. Many of the verbs had zero 
hit counts for the for and carefully schemata. It is 
also interesting that the validity ratio Vi(w) was 
not shown to be a good feature-generating statis-
tic. 
We finally trained our model with the top 10 
features and classified all WordNet verbs and 
verb phrases. For actual construction of the lex-
icon, 11,416 verbs and verb phrases were classi-
fied into the two classes roughly equally. We 
randomly sampled 200 items and examined how 
accurately the classification was done. A total of 
164 items were correctly classified, resulting in 
82% accuracy. Some examples from the classifi-
cation are shown in table 4. 
A further analysis of the results show that 
most of the errors occurred with domain-specific 
verbs (e.g., ablactate, alkalify, and transaminate 
in chemistry) and multi-word verb phrases (e.g., 
turn a nice dime; keep one?s shoulder to the 
wheel). Since many features are computed based 
on Web resources, rare verbs cannot be classified 
correctly when their hit rations are very low. The 
domain-specific words rarely appear in Framenet 
or e-how, either. 
3 Experience Detection 
As mentioned earlier, experience-revealing sen-
tences tend to have a certain linguistic style. 
1468
  
Having converted the problem of experience de-
tection for sentences to a classification task, we 
focus on the extent to which various linguistic 
features contribute to the performance of the bi-
nary classifier for sentences. We also explain the 
experimental setting for evaluation, including the 
classifier and the test corpus. 
3.1 Linguistic features 
In addition to the verb class feature available in 
the verb lexicon constructed automatically, we 
used tense, mood, aspect, modality, and expe-
riencer features.  
Verb class: The feature comes directly from 
the lexicon since a verb has been classified into a 
state or activity verb. The predicate part of the 
sentence to be classified for experience is looked 
up in the lexicon without sense disambiguation.  
Tense: The tense of a sentence is important 
since an experience-revealing sentence tends to 
use past and present tense. Future tenses are not 
experiences in most cases. We use POS tagging 
(Toutanova et al, 2003) for tense determination, 
but since the Penn tagset provides no future 
tenses, they are determined by exploiting modal 
verbs such as ?will? and future expressions such 
?going to?.  
Mood: It is one of distinctive forms that are 
used to signal the modal status of a sentence. We 
consider three mood categories: indicative, im-
perative and subjunctive. We determine the 
mood of a sentence by a small set of heuristic 
rules using the order of POS occurrences and 
punctuation marks. 
Aspect: It defines the temporal flow of a verb 
in the activity or state. Two categories are used: 
progressive and perfective. This feature is deter-
mined by the POS of the predicate in a sentence. 
Modality: In linguistics, modals are expres-
sions broadly associated with notions of possibil-
ity. While modality can be classified at a fine 
level (e.g., epistemic and deontic), we simply 
determine whether or not a sentence includes a 
modal marker that is involved in the main predi-
cate of the sentence. In other words, this binary 
feature is determined based on the existence of a 
model verb like ?can?, ?shall?, ?must?, and ?may? 
or a phrase like ?have to? or ?need to?. The de-
pendency parser is used to ensure a modal mark-
er is indeed associated with the main predicate.  
Experiencer: A sentence can or cannot be 
treated as containing an experience depending on 
the subject or experiencer of the verb (note that 
this is different from the experiencer role in a 
case frame). Consider the following sentences: 
[14]   The stranger messed up the entire garden. 
[15]   His presence messed up the whole situation. 
The first sentence is considered an experience 
since the subject is a person. However, the 
second sentence with the same verb is not, be-
cause the subject is a non-animate abstract con-
cept. That is, a non-animate noun can hardly 
constitute an experience. In order to make a dis-
tinction, we use the dependency parser and a 
named-entity recognizer (Finkel et al, 2005) that 
can recognize person pronouns and person names.  
3.2 Classification 
To train our classifier, we first crawled weblogs 
from Wordpress3, one of the most popular blog 
sites in use today. Worpress provides an interface 
to search blog posts with queries. In selecting 
experience-containing blog pots, we used loca-
tion names such as Central Park, SOHO, Seoul 
and general place names such as airport, subway 
station, and restaurant because blog posts with 
some places are expected to describe experiences 
rather than facts or thoughts. 
We crawled 6,000 blog posts. After deleting 
non-English and multi-media blog posts for 
which we could not obtain any meaningful text 
data, the number became 5,326. We randomly 
sampled 1,000 sentences4 and asked three anno-
tators to judge whether or not individual sen-
tences are considered containing an experience 
based on our definition. For maximum accuracy, 
we decided to use only those sentences all the 
three annotators agreed, resulting in a total of 
568 sentences.  
While we tested several classifiers, we chose 
to use two different classifiers based on SVM 
and Logistic Regression for the final experimen-
tal results because they showed the best perfor-
mance. 
3.3 Results 
For comparison purposes, we take the method of 
Kurashima et al (2005) as our baseline because 
the method was used in subsequent studies (Ku-
rashima et al, 2006; Kurashima et al, 2009) 
where experience attributes are extracted. We 
briefly describe the method and present how we 
implemented it.  
The method first extracts all verbs and their 
dependent phrasal unit from candidate sentences.  
                                                 
3 http://wordpress.com 
4 It was due to the limited human resources, but when we 
increased the number at a later stage, the performance in-
crease was almost negligible.  
1469
  
Feature 
Logistic  
Regression 
SVM 
Prec. Recall Prec. Recall 
Baseline 32.0% 55.1% 25.3% 44.4% 
Lexicon 77.5% 76.0% 77.5% 76.0% 
Tense 75.1% 75.1% 75.1% 75.1% 
Mood 75.8% 60.3% 75.8% 60.3% 
Aspect 26.7% 51.7% 26.7% 51.7% 
Modality 79.8% 70.5% 79.8% 70.5% 
Experiencer 54.3% 53.5% 54.3% 53.5% 
All included 91.9% 91.7% 91.7% 91.4% 
Table 5. Experience Detection Performance 
The candidate goes through three filters before it 
is treated as experience-containing sentence. 
First, the candidates that do not have an objective 
case (Fillmore, 1968) are eliminated because 
their definition of experience as ?action + object?. 
This was done by identifying the object-
indicating particle (case marker) in Japanese. 
Next, the candidates belonging to ?become? and 
?be? statements based on Japanese verb types are 
filtered out. Finally, the candidate sentences in-
cluding a verb that indicates a movement are 
eliminated because the main interest was to iden-
tify an activity in a place.  
Although their definition of experience is 
somewhat different from ours (i.e., ?action + ob-
ject?), they used the method to generate candi-
date sentences from which various experience 
attributes are extracted. From this perspective, 
the method functioned like our experience detec-
tion. Put differently, the definition and the me-
thod by which it is determined were much cruder 
than the one we are using, which seems close to 
our general understanding.5 
The three filtering steps were implemented as 
follows. We used the dependency parser for ex-
tracting objective cases using the direct object 
relation. The second step, however, could not be 
applied because there is no grammatical distinc-
tion among ?do, be, become? statements in Eng-
lish. We had to alter this step by adopting the 
approach of Inui et al (2008). The authors pro-
pose a lexicon of experience expression by col-
lecting hyponyms from a hierarchically struc-
tured dictionary. We collected all hyponyms of 
words ?do? and ?act?, from WordNet (Fellbaum, 
1998). Lastly, we removed all the verbs that are 
under the hierarchy of ?move? from WordNet.  
We not only compared our results with the 
baseline in terms of precision and recall but also 
                                                 
5 This is based on our observation that the three annotators 
found their task of identifying experience sentences not 
difficulty, resulting in a high degree of agreements.  
Feature 
Logistic  
Regression 
SVM 
Prec. Recall Prec. Recall 
Baseline 32.0% 55.1% 25.3% 44.4% 
-Lexicon 84.6% 84.6% 83.1% 81.2% 
-Tense 87.3% 87.1% 86.8% 86.5% 
-Mood 89.5% 89.5% 89.3% 89.2% 
-Aspect 90.8% 90.5% 89.0% 88.6% 
-Modality 89.5% 89.5% 82.8% 82.8% 
-Experiencer 91.5% 91.4% 91.1% 90.8% 
All included 91.9% 91.7% 91.7% 91.4% 
Table 6. Experience Detection Performance 
without Individual Features 
evaluated individual features for their importance 
in experience detection (classification). The 
evaluation was conducted with 10-fold cross va-
lidation. The results are shown in table 5.  
The performance, especially precision, of the 
baseline is much lower than those of the others. 
The method devised for Japanese doesn?t seem 
suitable for English. It seems that the linguistic 
styles shown in experience expressions are dif-
ferent from each other. In addition, the lexicon 
we constructed for the baseline (i.e., using the 
WordNet) contains more errors than our activity 
lexicon for activity verbs. Some hyponyms of an 
activity verb may not be activity verbs. (e.g., 
?appear? is a hyponym of ?do?).  
There is almost no difference between the Lo-
gistic Regression and SVM classifiers for our 
methods although SVM was inferior for the 
baseline. The performance for the best case with 
all the features included is very promising, 
closed to   92% precision and recall. Among the 
features, the lexicon, i.e., verb classes, gave the 
best result when each is used alone, followed by 
modality, tense, and mood. Aspect was the worst 
but close to the baseline. This result is very en-
couraging for the automatic lexicon construction 
work because the lexicon plays a pivotal role in 
the overall performance. 
In order to see the effect of including individ-
ual features in the feature set, precision and re-
call were measured after eliminating a particular 
feature from the full set. The results are shown in 
table 6. Although the absence of the lexicon fea-
ture hurt the performance most badly, still the 
performance was reasonably high (roughly 84 % 
in precision and recall for the Logistic Regres-
sion case). Similar to table 5, the aspect and ex-
perience features were the least contributors as 
the performance drops are almost negligible.  
1470
  
4 Related Work 
Experience mining in its entirety is a relatively 
new area where various natural language 
processing and text mining techniques can play a 
significant role. While opinion mining or senti-
ment analysis, which can be considered an im-
portant part of experience mining, has been stu-
died quite extensively (see Pang and Lee?s excel-
lent survey (2008)), another sub-area, factuality 
analysis, begins to gain some popularity (Inui et 
al., 2008; Saur?, 2008). Very few studies have 
focused explicitly on extracting various entities 
that constitute experiences (Kurashima et al, 
2009) or detecting experience-containing parts of 
text although many NLP research areas such as 
named entity recognition and verb classification 
are strongly related. The previous work on expe-
rience detection relies on a handcrafted lexicon. 
There have been a number of studies for verb 
classification (Fillmore, 1968; Vendler, 1967; 
Somers, 1982; Levin, 1993; Fillmore and Baker, 
2001; Kipper et al, 2008) that are essential for 
construction of an activity verb lexicon, which in 
turn is important for experience detection. Most 
similar to our work was done by Siegel and 
McKeown (2000), who attempted to categorize 
verbs into state or event classes based on 14 tests 
similar to those of Vendler?s. They attempted to 
compute co-occurrence statistics from a corpus. 
The event class, however, includes activity, ac-
complishment, and achievement. Similarly, Za-
crone and Lenci (2008) attempted to categorize 
verbs in Italian into the four Vendler classes us-
ing the Vendler tests by using a tagged corpus. 
They focused on existence of arguments such as 
subject and object that should co-occur with the 
linguistic features in the tests. 
The main difference between the previous 
work and ours lies in the goal and scope of the 
work. Since our work is specifically geared to-
ward domain-independent experience detection, 
we attempted to maximize the coverage by using 
all the verbs in WordNet, as opposed to the verbs 
appearing in a particular domain-specific corpus 
(e.g., medicine domain) as done in the previous 
work. Another difference is that while we are not 
limited to a particular domain, we did not use 
extensive human-annotated corpus other than 
using the 80 seed verbs and existing lexical re-
sources. 
5 Conclusion and Future Work 
We defined experience detection as an essential 
task for experience mining, which is restated as 
determining whether individual sentences con-
tain experience or not. Viewing the task as a 
classification problem, we focused on identifica-
tion and examination of various linguistic fea-
tures such as verb class, tense, aspect, mood, 
modality, and experience, all of which were 
computed automatically. For verb classes, in par-
ticular, we devised a method for classifying all 
the verbs and verb phrases in WordNet into the 
activity and state classes. The experimental re-
sults show that verb and verb phrase classifica-
tion method is reasonably accurate with 91% 
precision and 78% recall with manually con-
structed gold standard consisting of 80 verbs and 
82% accuracy for a random sample of all the 
WordNet entries. For experience detection, the 
performance was very promising, closed to 92% 
in precision and recall when all the features were 
used. Among the features, the verb classes, or the 
lexicon we constructed, contributed the most. 
In order to increase the coverage even further 
and reduce the errors in lexicon construction, i.e., 
verb classification, caused by data sparseness, we 
need to devise a different method, perhaps using 
domain specific resources.  
Given that experience mining is a relatively 
new research area, there are many areas to ex-
plore. In addition to refinements of our work, our 
next step is to develop a method for representing 
and extracting actual experiences from expe-
rience-revealing sentences. Furthermore, consi-
dering that only 13% of the blog data we 
processed contain experiences, an interesting 
extension is to apply the methodology to extract 
other types of knowledge such as facts, which 
are not necessarily experiences.  
Acknowledgments 
This research was supported by the IT R&D pro-
gram of MKE/KEIT under grant KI001877 [Lo-
cational/Societal Relation-Aware Social Media 
Service Technology], and by the MKE (The 
Ministry of Knowledge Economy), Korea, under 
the ITRC (Information Technology Research 
Center) support program supervised by the NIPA 
(National IT Industry Promotion Agency) [NI-
PA-2010-C1090-1011-0008]. 
Reference 
Eiji Aramaki, Yasuhide Miura, Masatsugu Tonoike, 
Tomoko Ohkuma, Hiroshi Mashuichi, and Kazuhi-
ko Ohe. 2009. TEXT2TABLE: Medical Text 
Summarization System based on Named Entity 
1471
  
Recognition and Modality Identification. In Pro-
ceedings of the Workshop on BioNLP. 
Collin F. Baker, Charles J. Fillmore, and Beau Cronin. 
2003. The Structure of the Framenet Database. In-
ternational Journal of Lexicography. 
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A Mximum Entropy 
Approach to Natural Language Processing. Com-
putational Linguistics. 
Chih-Chung Chang and Chih-Jen Lin. 2001. 
LIBSVM : a Library for Support Vector Machines. 
http://www.csie.ntu.edu.tw/~cjlin/libsvm. 
David R. Dowty. 1979. Word meaning and Montague 
Grammar. Reidel, Dordrecht. 
Christiane Fellbaum. 1998. WordNet: An Electronic 
Lexical Database. MIT Press. 
Charles J. Fillmore. 1968. The Case for Case. In Bach 
and Harms (Ed.): Universals in Linguistic Theory.  
Charles J. Fillmore and Collin F. Baker. 2001. Frame 
Semantics for Text Understanding. In Proceedings 
of WordNet and Other Lexical Resources Work-
shop, NAACL. 
Jenny R. Finkel, Trond Grenager, and Christopher D. 
Manning. 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs 
Sampling. In Proceedings of ACL. 
Chih-Wei Hsu, Chih-Chung Chang, and Chih-Jen Lin. 
2009. A Practical Guide to Support Vector Classi-
fication. http://www.csie.ntu.edu.tw/~cjlin/libsvm. 
Kentaro Inui, Shuya Abe, Kazuo Hara, Hiraku Morita, 
Chitose Sao, Megumi Eguchi, Asuka Sumida, Koji 
Murakami, and Suguru Matsuyoshi. 2008. Expe-
rience Mining: Building a Large-Scale Database of 
Personal Experiences and Opinions from Web 
Documents. In Proceedings of the International 
Conference on Web Intelligence. 
Valentin Jijkoun, Maarten de Rijke, Wouter Weer-
kamp, Paul Ackermans and Gijs Geleijnse. 2010. 
Mining User Experiences from Online Forums: An 
Exploration. In Proceedings of NAACL HLT Work-
shop on Computational Linguistics in a World of 
Social Media. 
Karin Kipper, Anna Korhonen, Neville Ryant, and 
Martha Palmer. 2008. A Large-scale Classification 
of English Verbs. Language Resources and Evalu-
ation Journal. 
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of ACL. 
Takeshi Kurashima, Ko Fujimura, and Hidenori Oku-
da. 2009. Discovering Association Rules on Expe-
riences from Large-Scale Blog Entries. In Proceed-
ings of ECIR. 
Takeshi Kurashima, Taro Tezuka, and Katsumi Tana-
ka. 2005. Blog Map of Experiences: Extracting and 
Geographically Mapping Visitor Experiences from 
Urban Blogs. In Proceedings of WISE.  
Takeshi Kurashima, Taro Tezuka, and Katsumi Tana-
ka. 2006. Mining and Visualizing Local Expe-
riences from Blog Entries. In Proceedings of 
DEXA. 
John Lafferty, Andew McCallum, and Fernando Pe-
reira. 2001. Conditional Random Fields: Probabil-
istic Models for Segmenting and Labeling Se-
quence Data. In Proceedings of ICML. 
Beth Levin. 1993. English verb classes and alterna-
tions: A Preliminary investigation. University of 
Chicago press. 
Edward Loper, Szu-ting Yi, and Martha Palmer. 2007. 
Combining Lexical Resources: Mapping Between 
PropBank and Verbnet. In Proceedings of the In-
ternational Workshop on Computational Linguis-
tics. 
Bo Pang and Lillian Lee. 2008. Opinion Mining and 
Sentiment Analysis, Foundations and Trends in In-
formation Retrieval. 
Jihee Ryu, Yuchul Jung, Kyung-min Kim and Sung H. 
Myaeng. 2010. Automatic Extraction of Human 
Activity Knowledge from Method-Describing Web 
Articles. In Proceedings of the 1st Workshop on Au-
tomated Knowledge Base Construction. 
Roser Saur?. 2008. A Factuality Profiler for Eventuali-
ties in Text. PhD thesis, Brandeis University. 
Eric V. Siegel and Kathleen R. McKeown. 2000. 
Learing Methods to Combine Linguistic Indicators: 
Improving Aspectual Classification and Revealing 
Linguistic Insights. In Computational Linguistics. 
Harold L. Somers. 1987. Valency and Case in Com-
putational Linguistics. Edinburgh University Press. 
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-
of-Speech Tagging with a Cyclic Dependency 
Network. In Proceedings of HLT-NAACL. 
Zeno Vendler. 1967. Linguistics in Philosophy. Cor-
nell University Press.  
Alessandra Zarcone and Alessandro Lenci. 2008. 
Computational Models of Event Type Classifica-
tion in Context. In Proceedings of LREC. 
 
1472
