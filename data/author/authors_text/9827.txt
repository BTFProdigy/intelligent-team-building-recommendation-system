Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 96?99,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Quantifying Ellipsis in Dialogue:  an index of mutual understanding   Marcus Colman, Arash Eshghi and Patrick G. T. Healey Interaction, Media and Communication Research Group Queen Mary, University of London E1 4NS UK {marcus, arash, ph}@dcs.qmul.ac.uk        Abstract 
This paper presents a coding protocol that al-lows na?ve users to annotate dialogue tran-scripts for anaphora and ellipsis. Cohen's kappa statistic demonstrates that the protocol is sufficiently robust in terms of reliability. It is proposed that quantitative ellipsis data may be used as an index of mutual-engagement. Current and potential uses of ellipsis coding are described. 
1. Introduction Spontaneously generated dialogue, whether natu-rally occurring or task-oriented, rarely sticks to accepted rules of grammar or even politeness. In-terruptions, ungrammatical utterances and grunts or other noises are found in the majority of contri-butions in dialogue corpora. One reason for this is the ubiquitous use of ellipsis; the omission of words or phrases from a contribution which can be inferred or extracted from previous contributions. Ellipsis is optional; the full constituent could serve communication as well as the elliptical version. Where ellipsis occurs across speakers i.e., one par-ticipant makes (elliptical) use of another?s contri-bution, it provides a direct index of the mutual-accessibility of the current conversational context (cf. Healey et. al. 2007; Eshghi and Healey, 2007).      In some cases elliptical contributions are obvi-ous, as in the polar response 'yeah', signifying that a question has been heard, understood and consid-
ered; however, there are degrees of complexity that would seem to require a close understanding of what another participant is referring to. It is this issue of mutual-accessibility or 'grounding' that we propose can be investigated through the quantifica-tion of elliptical phenomena. These phenomena are, we propose, also related to the way referring expressions can contract over repeated use.  (e.g.  Schober and Clark, 1989; Wilkes-Gibbs and Clark, 1992). The approach taken in Clark et al's 'col-laborative theory' is that as mutual understanding increases, dialogue contributions become shorter as referring terms become part of the common ground. Clark and Krych (2004) note that various elliptical phrases can be used to establish common ground, from continuers ('uh-huh', 'yeah') or as-sessments ('gosh') to establishing shared attention through deictic expressions such as 'this', 'that', 'here' and 'there'.    Healey et al (2007) demonstrated the basic con-cept and viability of quantifying ellipsis phenom-ena as a quantitative index of mutual-accessibility of context. They showed that the frequency of use of cross-speaker elliptical expressions in online chat varies systematically depending on whether communication is ?local? i.e. within a single chat room or ?remote?.  However, the coding of ellipsis in this study did not follow an explicit protocol. It relied mainly on the distinctions made by Fernan-dez et al (2004)  but specific measures of reliabil-ity and validity were not calculated.    
96
 Figure 1. ?Anaphora? decision chart   In this paper we present an ellipsis coding protocol that provides a set of coding categories and we re-port the inter-rater reliability scores that have been obtained with it. In order to simplify coding and increase reliability, categories suggested by Fer-nandez et al have been collapsed into broader ones. It should be pointed out that we are not, in general, trying to produce an accurate or definitive analysis of ellipsis. The protocol is rather the prod-uct of contending with the compromise between robust coding categories and linguistic elegance. The categories presented here are generally or-dered in terms of occurrence in order to assist the coder. A contribution to dialogue may contain more than one type of elliptical utterance; contri-butions are not assigned to one mutually exclusive category. Rather, coders are able to use the proto-col to label any part of a dialogue that is elliptical. 2. The Ellipsis Protocol  The protocol is designed as a tool for coding one aspect of dialogue, developed with the intention    
 Figure 2. ?Answers? decision chart  that users with no specific knowledge of linguistics can use it. As can be seen from Figures 1-4, it con-sists of four binary branching decision trees that are applied to each contribution in an interaction. Full instructions for use of the protocol have also been written and are available from the authors. 3. Inter-rater reliability In order to demonstrate reliability between coders, two coders (one computer scientist, one psycholo-gist) applied the ellipsis protocol to a sample of task oriented dialogue. This was taken from the HCRC Map Task corpus (Anderson et al 1991); a series of dialogues in which one participant at-tempts to describe a route on a fictional map to another. The longest of these dialogues was chosen to be coded (transcript Q1NC1) which consisted of 446 turns and 5533 words. Cohen's kappa was cal-culated using the procedure outlined in Howell (1994); see Carletta (1996) for a discussion of the use of kappa in dialogue coding. Kappa in this in-stance was .81, which shows very high reliability, even by conservative standards (Krippendorff,  
97
 Figure 3. ?Questions? decision chart  1980). Table 1 below presents a breakdown of the instances of categories that were agreed upon. Table 1 shows the total number and approximate percentage of agreements. Also given, '1.dis' and '2.dis' are the number of observed instances by coders one and two respectively identified but dis-puted for that particular category. The total number of elliptical or non-elliptical instances coded, from single words or phrases to entire turns was 624; of these, 100 (16%) were disagreed upon and 78 in-stances (12.5%) were agreed to contain no ellipti-cal phenomena (no ellipsis disagreements = 50). Some categories have very low frequencies; how-ever, previous work suggests that these categories are necessary. To some extent this table shows the limitations of the kappa statistic; coder agreement varies considerably across these categories.  
 Figure 4. ?Statements? decision chart     Endophor Cataphor Exaphor Vague Anaphor Total 119 2 8 33 % 19 .03 1.3 5.3 1.dis 12 1 1 20 2.dis 10 3 17 6  Polar Answer Acknowledge Prompted NSU Ans. Un-prompted NSU Ans. Total 113 78 1 7 % 18.1 12.5 0.2 1.1 1.dis 7 15 0 1 2.dis 5 9 1 5  Sluice Clarification Ellipsis Check NSU Query Total 2 7 20 27 % .03 1.1 3.2 4.3 1.dis 0 0 2 5 2.dis 2 2 0 2  Rejection Modification Continua-tion Sentential Ellipsis Total 2 1 13 13 % .03 .002 2.1 2.1 1.dis 1 0 3 10 2.dis 4 0 3 3 Table 1. Total agreements by category  
98
4. Discussion  Although mutual-accessibility of context is funda-mental to communication, there has not been a re-liable method for observing or measuring it. The ellipsis protocol presented here thus provides a useful step in this direction. It gives a standardised coding scheme that can quantify the extent to which speakers can directly access the constituents of each other?s turns.     In previous work there have been several differ-ent attempts to define taxonomies of elliptical or context dependent utterances. For example, non-sentential utterances (NSUs), e.g. Schlangen and Lascarides (2003); Fernandez and Ginzburg (2002); Fernandez, Ginzburg and Lappin (2007). One issue with these previous approaches is the lack of reliability data; a statistic such as Cohen?s kappa is needed in order to demonstrate that a tax-onomy or coding scheme can be reliably applied between independent coders. Carletta et al (1997) presented a reliable coding scheme for the classifi-cation of dialogue moves; although there are over-laps between their categories and ours, the questions used in the scheme are intended to estab-lish solely the function of an utterance and impor-tantly, not whether the utterance is elliptical. The protocol presented here achieves a high level of reliability for some of these context dependent phenomena without requiring specific prior knowl-edge of the relevant linguistic theory.    Further work will code a sample from the BNC (Burnard, 2000) in order to allow comparisons with previous taxonomies. The HCRC map task corpus has previously been examined in terms of various features of dialogue, e.g. Dialogue Games Analysis (Kowtko et al 1991) and disfluencies (Lickley and Bard, 1998). Ongoing work will de-velop this through coding the entire HCRC map task corpus; providing data on how ellipsis varies over different conditions such as medium, familiar-ity and task role. Acknowledgments Thanks go to the HCRC group for providing the map task data. Thanks also to Jackie Folkes and Greg Mills for help and advice.  
References  Anderson, A., Bader. M., Bard, E., Boyle, E., Doherty, G. M., Garrod, S., Isard, S., Kowtko, J., McAllister, J., Miller, J., Sotillo, C., Thompson, H. S. and Wein-ert, R. (1991). The HCRC Map Task Corpus. Lan-guage and Speech, 34, 351-366. Burnard, L. (2000). Reference Guide for the British Na-tional Corpus (World Edition). Oxford University Computing Services. Carletta, J. (1996). Assessing agreement on classifica-tion tasks: the kappa statistic. Computational Lin-guistics, 22(2): 249-254. Clark, H. H. and Krych, M. A. (2004). Speaking while monitoring addressees for understanding. Journal of Memory and Language, 50, 62-81. Eshghi, A. and Healey, P. G. T. (2007). Collective states of understanding. Proceedings of the 8th SIGdial workshop on discourse and dialogue. pp 2-9. Fernandez, R. and Ginzburg, J. (2002). Non-sentential utterances: a corpus study. Traitement automatique des languages: dialogue, 43(2), 13-42. Fernandez, R., Ginzburg J and Lappin, S, (2004). Clas-sifying ellipsis in dialogue: a machine learning ap-proach. Proceedings of the 20th international conference on computational linguistics. pp 240-246. Fernandez, R. , Ginzburg J and Lappin, S. (2007). Clas-sifying non-sentential utterances in dialogue: a ma-chine learning approach. Computational Linguistics 33(3), 397-427. Healey, P. G. T., White, G., Eshghi, A. and Light, A. (2007). Communication Spaces. Computer Supported Co-operative Work, 2007. Howell, D. C. (1997). Statistical Methods for Psychol-ogy. Duxbury Press. Kowtko, J. C., Isard, S. D. and Doherty, G. M. (1991). Conversational games within dialogue. Proceedings of the espirit workshop on discourse coherence, 1991. Krippendorff, K. (1980). Content Analysis: an introduc-tion to its methodology. Beverly Hills: Sage Publica-tions. Lickley, R. and Bard, E. (1998). When can listeners detect disfluency in spontaneous speech? Language and Speech, 41. Schlangen, D. and Lascarides, A. (2003). The interpre-tation of non-sentential utterances in dialogue. Pro-ceedings of the 4th SIGdial workshop on discourse and dialogue, 2003. Schober, M. F. and Clark, H. H. (1989). Understanding by addressees and overhearers. Cognitive Psychol-ogy, 21, 211-232. Wilkes-Gibbs, D. and Clark, H. H. (1992). Coordinating beliefs in conversation. Journal of Memory and Lan-guage, 31, 183-194. 
99
Incremental Semantic Construction in a Dialogue System?
Matthew Purver, Arash Eshghi, Julian Hough
Interaction, Media and Communication
School of Electronic Engineering and Computer Science, Queen Mary University of London
{mpurver, arash, jhough}@eecs.qmul.ac.uk
Abstract
This paper describes recent work on the DynDial project? towards incremental semantic inter-
pretation in dialogue. We outline our domain-general grammar-based approach, using a variant of
Dynamic Syntax integrated with Type Theory with Records and a Davidsonian event-based seman-
tics. We describe a Java-based implementation of the parser, used within the Jindigo framework to
produce an incremental dialogue system capable of handling inherently incremental phenomena such
as split utterances, adjuncts, and mid-sentence clarification requests or backchannels.
1 Introduction
Many dialogue phenomena seem to motivate an incremental view of language processing: for example,
a participant?s ability to change hearer/speaker role mid-sentence to produce or interpret backchannels,
or complete or continue an utterance (see e.g. Yngve, 1970; Lerner, 2004, amongst many others). Much
recent research in dialogue systems has pursued this line, resulting in frameworks for incremental di-
alogue processing (Schlangen and Skantze, 2009) and systems capable of mid-utterance backchannels
(Skantze and Schlangen, 2009) or utterance completions (DeVault et al, 2009; Bu? et al, 2010).
However, to date there has been little focus on semantics, with the systems produced either operating
in domains in which semantic representation is not required (Skantze and Schlangen, 2009), or using
variants of domain-specific canned lexical or phrasal matching (Bu? et al, 2010). Our intention is to
extend this work to finer-grained and more domain-general notions of grammar and semantics, by using
an incremental grammatical framework, Dynamic Syntax (DS, Kempson et al, 2001) together with the
structured semantic representation provided by Type Theory with Records (TTR, see e.g. Cooper, 2005).
(a)
A: I want to go to . . .
B: Uh-huh
A: . . . Paris by train.
(b)
A: I want to go to Paris . . .
B: Uh-huh
A: . . . by train.
(c)
A: I want to go to Paris.
B: OK. When do you . . .
A: By train.
Figure 1: Examples of motivating incremental dialogue phenomena
One aim is to deal with split utterances, both when the antecedent is inherently incomplete (see Fig-
ure 1(a)) and potentially complete (even if not intended as such ? Figure 1(b)). This involves producing
representations which are as complete as possible ? i.e contain all structural and semantic information
so far conveyed ? on a word-by-word basis, so that in the event of an interruption or a hesitation, the
system can act accordingly (by producing backchannels or contentful responses as above); but that can
be further incremented in the event of a continuation by the user.
Importantly, this ability should be available not only when an initial contribution is intended and/or
treated as incomplete (as in Figure 1(b)), but also when it is in fact complete, but is still subsequently
extended (Figure 1(c)). Treating A?s two utterances as distinct, with separate semantic representations,
must require high-level processes of ellipsis reconstruction to interpret the final fragment ? for example,
treating it as the answer to an implicit question raised by A?s initial sentence (Ferna?ndez et al, 2004). If,
?The authors were supported by the Dynamics of Conversational Dialogue project (DynDial ? ESRC-RES-062-23-0962).
We thank Shalom Lappin, Tim Fernando, Yo Sato, our project colleagues and the anonymous reviewers for helpful comments.
365
instead, we can treat such fragments as continuations which merely add directly to the existing represen-
tation, the task is made easier and the relevance of the two utterances to each other becomes explicit.
2 Dynamic Syntax (DS) and Type Theory with Records (TTR)
Our approach is a grammar-based one, as our interest is in using domain-general techniques that are
capable of fine-grained semantic representation. Dynamic Syntax (DS) provides an inherently incre-
mental grammatical framework which dispenses with an independent level of syntax, instead expressing
grammaticality via constraints on the word-by-word monotonic growth of semantic structures. In DS?s
original form, these structures are trees with nodes corresponding to terms in the lambda calculus; nodes
are decorated with labels expressing their semantic type and formula, and beta-reduction determines the
type and formula at a mother node from those at its daughters (Figure 2(a)). Trees can be partial, with
nodes decorated with requirements for future development; lexical actions (corresponding to words) and
computational actions (general capabilities) are defined as operations on trees which satisfy and/or add
requirements; and grammaticality of a word sequence is then defined as satisfaction of all requirements
(tree completeness) via the application of its associated actions ? see Kempson et al (2001) for details.
Previous work in DS has shown how this allows a treatment of split utterances and non-sentential
fragments (e.g. clarifications) as extensions of the semantic trees so far constructed, either directly or via
the addition of ?linked? trees (Purver and Kempson, 2004; Gargett et al, 2009).
(a) Ty(t),arrive(john)
Ty(e),
john
Ty(e ? t),
?x.arrive(x)
(b)
[
x=john : e
p=arrive(x) : t
]
[
x=john : e
]
?r :
[
x : e
]
[
x=r.x : e
p=arrive(x) : t
]
(c)
?
?
e=now : es
x=john : e
p=arrive(e,x) : t
?
?
[
x=john : e
]
?r :
[
x : e
]
?
?
e=now : es
x=r.x : e
p=arrive(e,x) : t
?
?
Figure 2: A simple DS tree for ?john arrives?: (a) original DS, (b) DS+TTR, (c) event-based
2.1 Extensions
More recent work in DS has started to explore the use of TTR to extend the formalism, replacing the
atomic semantic type and FOL formula node labels with more complex record types, and thus providing
a more structured semantic representation. Purver et al (2010) provide a sketch of one way to achieve
this and explain how it can be used to incorporate pragmatic information such as participant reference
and illocutionary force. As shown in Figure 2(b) above, we use a slightly different variant here: node
record types are sequences of typed labels (e.g. [x : e] for a label x of type e), with semantic content
expressed by use of manifest types (e.g. [x=john : e] where john is a singleton subtype of e).
We further adopt an event-based semantics along Davidsonian lines (Davidson, 1980). As shown
in Figure 2(c), we include an event term (of type es) in the representation: this allows tense and aspect
to be expressed (although Figure 2(c) shows only a simplified version using the current time now).
It also permits a straightforward analysis of optional adjuncts as extensions of an existing semantic
representation; extensions which predicate over the event term already in the representation. Adding
fields to a record type results in a more fully specified record type which is still a subtype of the original:
?
?
?
e=now : es
x=john : e
p=arrive(e,x) : t
?
?
?
7?
?
?
?
?
?
e=now : es
x=john : e
p=arrive(e,x) : t
p?=today(e) : t
?
?
?
?
?
?john arrives? 7? ?john arrives today?
Figure 3: Optional adjuncts as leading to TTR subtypes
366
3 Implementation
The resulting framework has been implemented in Java, following the formal details of DS as per (Kemp-
son et al, 2001; Cann et al, 2005, inter alia). This implementation, DyLan,1 includes a parser and gener-
ator for English, which take as input a set of computational actions, a lexicon and a set of lexical actions
(instructions for partial tree update); these are specified separately in text files in the IF-THEN-ELSE
procedural (meta-)language of DS, allowing any pre-written grammar to be loaded. Widening or chang-
ing its coverage, i.e. extending the system with new analyses of various linguistic phenomena, thus do
not involve modification or extension of the Java program, but only the lexicon and action specifications.
The current coverage includes a small lexicon, but a broad range of structures: complementation, relative
clauses, adjuncts, tense, pronominal and ellipsis construal, all in interaction with quantification.
3.1 The parsing process
Given a sequence of words (w1, w2, ..., wn), the parser starts from the axiom tree T0 (a requirement
to construct a complete tree of type t), and applies the corresponding lexical actions (a1, a2, . . . , an),
optionally interspersing general computational actions (which can apply whenever their preconditions
are met). More precisely: we define the parser state at step i as a set of partial trees Si. Beginning with
the singleton axiom state S0 = {T0}, for each word wi:
1. Apply all lexical actions ai corresponding to wi to each partial tree in Si?1. For each application
that succeeds (i.e. the tree satisfies the action preconditions), add resulting (partial) tree to Si.
2. For each tree in Si, apply all possible sequences of computational actions and add the result to Si.
If at any stage the state Si is empty, the parse has failed and the string is deemed ungrammatical. If the
final state Sn contains a complete tree (all requirements satisfied), the string is grammatical and its root
node will provide the full sentence semantics; partial trees provide only partial semantic specifications.2
3.2 Graph representations
Sato (2010) shows how this procedure can be modelled as a directed acyclic graph, rooted at T0, with
individual partial trees as nodes, connected by edges representing single actions. While Sato uses this to
model the search process, we exploit it (in a slightly modified form) to represent the linguistic context
available during the parse ? important in DS for ellipsis and pronominal construal. Details are given in
(Cann et al, 2007; Gargett et al, 2009), but three general mechanisms are available: 1) copying formulae
from some tree in context (used for e.g. anaphora and strict VP ellipsis); 2) rerunning actions in context
(for e.g. sloppy VP-ellipsis and fragment corrections); and 3) directly extending/augmenting the current
tree (used for most fragment types in (Ferna?ndez, 2006)). For any partial tree, then, the context available
to the parser must include not only the tree itself, but the sequence of actions and previous partial trees
which have gone into its construction. The parse graph (which we call the tree graph) provides exactly
this information, via the shortest path back to the root from the current node.
However, we can also take a coarser-grained view via a graph which we term the state graph; here,
nodes are states Si and edges the sets of action sequences connecting them. This subsumes the tree graph,
with state nodes containing possibly many tree-graph nodes; and here, nodes have multiple outgoing
edges only when multiple word hypotheses are present. This corresponds directly to the input word graph
(often called a word lattice) available from a speech recognizer, allowing close integration in a dialogue
system ? see below. We also see this as a suitable structure with which to begin to model phenomena
such as hesitation and self-repair: as edges are linear action sequences, intended to correspond to the
time-linear psycholinguistic processing steps involved, such phenomena may be analysed as building
further edges from suitable departure points earlier in the graph.3
1DyLan is short for Dynamics of Language. Available from http://dylan.sourceforge.net/.
2Note that only a subset of possible computational actions can apply to any given tree; together with a set of heuristics on
possible application order, and the merging of identical trees produced by different sequences, this helps reduce complexity.
3There are similarities to chart parsing here: the tree graph edges spanning a state graph edge could be seen as corresponding
to chart edges spanning a substring, with the tree nodes in the state Si as the agenda. However, the lack of a notion of syntactic
constituency means no direct equivalent for the active/passive edge distinction; a detailed comparison is still to be carried out.
367
4 Dialogue System
The DyLan parser has now been integrated into a working dialogue system by implementation as an
Interpreter module in the Java-based incremental dialogue framework Jindigo (Skantze and Hjal-
marsson, 2010). Jindigo follows Schlangen and Skantze (2009)?s abstract architecture specification and
is specifically designed to handle units smaller than fully sentential utterances; one of its specific imple-
mentations is a travel agent system, and our module integrates semantic interpretation into this.
As set out by Schlangen and Skantze (2009)?s specification, our Interpreter?s essential compo-
nents are a left buffer (LB), processor and right buffer (RB). Incremental units (IUs) of various types are
posted from the RB of one module to the LB of another; for our module, the LB-IUs are ASR word hy-
potheses, and after processing, domain-level concept frames are posted as RB-IUs for further processing
by a downstream dialogue manager. The input IUs are provided as updates to a word lattice, and new
edges are passed to the DyLan parser which produces a state graph as described above in 3.1 and 3.2:
new nodes are new possible parse states, with new edges the sets of DS actions which have created them.
These state nodes are then used to create Jindigo domain concept frames by matching against the TTR
record types available (see below), and these are posted to the RB as updates to the state graph (lattice
updates in Jindigo?s terminology).
Crucial in Schlangen and Skantze (2009)?s model is the notion of commitment: IUs are hypotheses
which can be revoked at any time until they are committed by the module which produces them. Our
module hypothesizes both parse states and associated domain concepts (although only the latter are
outputs); these are committed when their originating word hypotheses are committed (by ASR) and a
type-complete subtree is available; other strategies are possible and are being investigated.
4.1 Mapping TTR record types to domain concepts incrementally
Our Interpreter module matches TTR record types to domain concept frames via a simple XML
matching specification; TTR fields map to particular concepts in the domain depending on their se-
mantic type (e.g. go events map to Trip concepts, and the entity of manifest type paris maps to the
City[paris] concept). As the tree and parse state graphs are maintained, incremental sub-sentential
extensions can produce TTR subtypes and lead to enrichment of the associated domain concept.
User: I want to go to Paris . . .
?
?
?
?
?
?
?
?
?
?
?
?
e=,e17,P resentState : es
e1=,e19,FutureAccomp : es
x1=Paris : e
p2=to(e1,x1) : t
x=speaker : e
p1=go(e1,x) : t
p?=want(e,x,p1) : t
?
?
?
?
?
?
?
?
?
?
?
?
Trip(to : City[paris])
User: . . . from London
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
e=,e17,P resentState : es
e1=,e19,FutureAccomp : es
x1=Paris : e
p2=to(e1,x1) : t
x2=London : e
p3=from(e1,x2) : t
x=speaker : e
p1=go(e1,x) : t
p?=want(e,x,p1) : t
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Trip(from : City[london],
to : City[paris])
Figure 4: Incremental construction of a TTR record type over two turns
Figure 4 illustrates this process for a user continuation; the initial user utterance is parsed to produce
a TTR record type, with a corresponding domain concept ? a valid incremental unit to post in the RB.
The subsequent user continuation ?from London? extends the parser state graph, producing a new TTR
subtype (in this case via the DS apparatus of an adjoining linked tree (Cann et al, 2005)), and a more
368
fully specified concept (with a further argument slot filled) as output.
System behaviour between these two user contributions will depend on the committed status of the
input, and perhaps some independent prosody-based judgement of whether a turn is finished (Skantze
and Schlangen, 2009). An uncommitted input might be responded to with a backchannel (Yngve, 1970);
commitment might lead to the system beginning processing and starting to respond more substantively.
However, in either case, the maintenance of the parse state graph allows the user continuation to be
treated as extending a parse tree, subtyping the TTR record type, and finally mapping to a fully satisfied
domain concept frame that can be committed.
5 Conclusions
We have implemented an extension of the Dynamic Syntax framework, integrated with Type Theory with
Records, which provides structured semantic representations suitable for use in a dialogue system, and
which does so incrementally, producing well-defined partial representations on a word-by-word basis.
This has been integrated into a working Jindigo dialogue system, capable of incremental behaviour such
as mid-sentence backchannels and utterance continuations, which will be demonstrated at the conference.
The coverage of the parser is currently limited, but work is in progress to widen it; the possibility of using
grammar induction to learn lexical actions from real corpora is also being considered for future projects.
We are also actively pursuing possbilities for tighter integration of TTR and DS, with the aim of unifying
syntactic and semantic incremental construction.
References
Bu?, O., T. Baumann, and D. Schlangen (2010). Collaborating on utterances with a spoken dialogue system using
an ISU-based approach to incremental dialogue management. In Proceedings of the SIGDIAL 2010 Conference.
Cann, R., R. Kempson, and L. Marten (2005). The Dynamics of Language. Oxford: Elsevier.
Cann, R., R. Kempson, and M. Purver (2007). Context and well-formedness: the dynamics of ellipsis. Research
on Language and Computation 5(3), 333?358.
Cooper, R. (2005). Records and record types in semantic theory. Journal of Logic and Computation 15(2), 99?112.
Davidson, D. (1980). Essays on Actions and Events. Oxford, UK: Clarendon Press.
DeVault, D., K. Sagae, and D. Traum (2009). Can I finish? learning when to respond to incremental interpretation
results in interactive dialogue. In Proceedings of the SIGDIAL 2009 Conference.
Ferna?ndez, R. (2006). Non-Sentential Utterances in Dialogue: Classification, Resolution and Use. Ph. D. thesis,
King?s College London, University of London.
Ferna?ndez, R., J. Ginzburg, H. Gregory, and S. Lappin (2004). SHARDS: Fragment resolution in dialogue. In
H. Bunt and R. Muskens (Eds.), Computing Meaning, Volume 3. Kluwer Academic Publishers. To appear.
Gargett, A., E. Gregoromichelaki, R. Kempson, M. Purver, and Y. Sato (2009). Grammar resources for modelling
dialogue dynamically. Cognitive Neurodynamics 3(4), 347?363.
Kempson, R., W. Meyer-Viol, and D. Gabbay (2001). Dynamic Syntax: The Flow of Language Understanding.
Blackwell.
Lerner, G. H. (2004). Collaborative turn sequences. In Conversation analysis: Studies from the first generation,
pp. 225?256. John Benjamins.
Purver, M., E. Gregoromichelaki, W. Meyer-Viol, and R. Cann (2010). Splitting the ?I?s and crossing the ?You?s:
Context, speech acts and grammar. In Aspects of Semantics and Pragmatics of Dialogue. SemDial 2010, 14th
Workshop on the Semantics and Pragmatics of Dialogue.
Purver, M. and R. Kempson (2004). Incremental context-based generation for dialogue. In Proceedings of the 3rd
International Conference on Natural Language Generation (INLG04).
Sato, Y. (2010). Local ambiguity, search strategies and parsing in Dynamic Syntax. In E. Gregoromichelaki,
R. Kempson, and C. Howes (Eds.), The Dynamics of Lexical Interfaces. CSLI. to appear.
Schlangen, D. and G. Skantze (2009). A general, abstract model of incremental dialogue processing. In Proceed-
ings of the 12th Conference of the European Chapter of the ACL (EACL 2009).
Skantze, G. and A. Hjalmarsson (2010). Towards incremental speech generation in dialogue systems. In Proceed-
ings of the SIGDIAL 2010 Conference.
Skantze, G. and D. Schlangen (2009). Incremental dialogue processing in a micro-domain. In Proceedings of the
12th Conference of the European Chapter of the ACL (EACL 2009).
Yngve, V. H. (1970). On getting a word in edgewise. In Papers from the 6th regional meeting of the Chicago
Linguistic Society.
369
Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 94?103,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Incremental Grammar Induction from Child-Directed
Dialogue Utterances?
Arash Eshghi
Interaction Lab
Heriot-Watt University
Edinburgh, United Kingdom
eshghi.a@gmail.com
Julian Hough and Matthew Purver
Cognitive Science Research Group
Queen Mary University of London
London, United Kingdom
{julian.hough, mpurver}@eecs.qmul.ac.uk
Abstract
We describe a method for learning an in-
cremental semantic grammar from data in
which utterances are paired with logical
forms representing their meaning. Work-
ing in an inherently incremental frame-
work, Dynamic Syntax, we show how
words can be associated with probabilistic
procedures for the incremental projection
of meaning, providing a grammar which
can be used directly in incremental prob-
abilistic parsing and generation. We test
this on child-directed utterances from the
CHILDES corpus, and show that it results
in good coverage and semantic accuracy,
without requiring annotation at the word
level or any independent notion of syntax.
1 Introduction
Human language processing has long been
thought to function incrementally, both in pars-
ing and production (Crocker et al, 2000; Fer-
reira, 1996). This incrementality gives rise to
many characteristic phenomena in conversational
dialogue, including unfinished utterances, inter-
ruptions and compound contributions constructed
by more than one participant, which pose prob-
lems for standard grammar formalisms (Howes et
al., 2012). In particular, examples such as (1) sug-
gest that a suitable formalism would be one which
defines grammaticality not in terms of licensing
strings, but in terms of constraints on the semantic
construction process, and which ensures this pro-
cess is common between parsing and generation.
(1) A: I burnt the toast.
? We are grateful to Ruth Kempson for her support and
helpful discussions throughout this work. We also thank
the CMCL?2013 anonymous reviewers for their constructive
criticism. This work was supported by the EPSRC, RISER
project (Ref: EP/J010383/1), and in part by the EU, FP7
project, SpaceBook (Grant agreement no: 270019).
B: But did you burn . . .
A: Myself? Fortunately not.
[where ?did you burn myself?? if uttered by
the same speaker is ungrammatical]
One such formalism is Dynamic Syntax (DS)
(Kempson et al, 2001; Cann et al, 2005); it
recognises no intermediate layer of syntax, but
instead reflects grammatical constraints via con-
straints on the word-by-word incremental con-
struction of meaning, underpinned by attendant
concepts of underspecification and update.
Eshghi et al (2013) describe a method for in-
ducing a probabilistic DS lexicon from sentences
paired with DS semantic trees (see below) repre-
senting not only their meaning, but their function-
argument structure with fine-grained typing infor-
mation. They apply their method only to an ar-
tificial corpus generated using a known lexicon.
Here, we build on that work to induce a lexi-
con from real child-directed utterances paired with
less structured Logical Forms in the form of TTR
Record Types (Cooper, 2005), thus providing less
supervision. By assuming only the availability of a
small set of general compositional semantic opera-
tions, reflecting the properties of the lambda calcu-
lus and the logic of finite trees, we ensure that the
lexical entries learnt include the grammatical con-
straints and corresponding compositional seman-
tic structure of the language. Our method exhibits
incrementality in two senses: incremental learn-
ing, with the grammar being extended and refined
as each new sentence becomes available; resulting
in an inherently incremental, probabilistic gram-
mar for parsing and production, suitable for use
in state-of-the-art incremental dialogue systems
(Purver et al, 2011) and for modelling human-
human dialogue.
94
?Ty(t)
?Ty(e),
?
?Ty(e ? t)
??
?john?
?Ty(t)
Ty(e),
john
?Ty(e ? t),
?
??
?upset?
?Ty(t)
Ty(e),
john ?Ty(e ? t)
?Ty(e),
?
Ty(e ? (e ? t)),
?y?x.upset?(x)(y)
??
?mary?
Ty(t),?,
upset?(john?)(mary?)
Ty(e),
john
Ty(e ? t),
?x.upset?(x)(mary?)
Ty(e),
mary?
Ty(e ? (e ? t)),
?y?x.upset?(x)(y)
Figure 1: Incremental parsing in DS producing semantic trees: ?John upset Mary?
2 Background
2.1 Grammar Induction and Semantics
We can view existing grammar induction meth-
ods along a spectrum from supervised to unsu-
pervised. Fully supervised methods take a parsed
corpus as input, pairing sentences with syntactic
trees and words with their syntactic categories, and
generalise over the phrase structure rules to learn
a grammar which can be applied to a new set of
data. Probabilities for production rules sharing a
LHS category can be estimated, producing a gram-
mar suitable for probabilistic parsing and disam-
biguation e.g. a PCFG (Charniak, 1996). While
such methods have shown great success, they pre-
suppose detailed prior linguistic information and
are thus inadequate as human grammar learning
models. Fully unsupervised methods, on the other
hand, proceed from unannotated raw data; they
are thus closer to the human language acquisition
setting, but have seen less success. In its pure
form ?positive data only, without bias? unsu-
pervised learning is computationally too complex
(?unlearnable?) in the worst case (Gold, 1967).
Successful approaches involve some prior learning
or bias (see (Clark and Lappin, 2011)) e.g. a set
of known lexical categories, a probability distri-
bution bias (Klein and Manning, 2005) or a semi-
supervised method with shallower (e.g. POS-tag)
annotation (Pereira and Schabes, 1992).
Another point on the spectrum is lightly su-
pervised learning: providing information which
constrains learning but with little or no lexico-
syntactic detail. One possibility is the use of se-
mantic annotation, using sentence-level proposi-
tional Logical Forms (LF). It seems more cogni-
tively plausible, as the learner can be said to be
able to understand, at least in part, the meaning
of what she hears from evidence gathered from
(1) her perception of her local, immediate environ-
ment given appropriate biases on different patterns
of individuation of entities and relationships be-
tween them, and (2) helpful interaction, and joint
focus of attention with an adult (see e.g. (Saxton,
1997)). Given this, the problem she is faced with
is one of separating out the contribution of each
individual linguistic token to the overall meaning
of an uttered linguistic expression (i.e. decompo-
sition), while maintaining and generalising over
several such hypotheses acquired through time as
she is exposed to more utterances involving each
token.
This has been successfully applied in Combi-
natorial Categorial Grammar (CCG) (Steedman,
2000), as it tightly couples compositional seman-
tics with syntax (Zettlemoyer and Collins, 2007;
Kwiatkowski et al, 2010; Kwiatkowski et al,
2012); as CCG is a lexicalist framework, grammar
learning involves inducing a lexicon assigning to
each word its syntactic and semantic contribution.
Moreover, the grammar is learnt incrementally, in
the sense that the learner collects data over time
and does the learning sentence by sentence.
Following this approach, Eshghi et al (2013)
outline a method for inducing a DS grammar
from semantic LFs. This brings an added di-
mension of incrementality: not only is learning
sentence-by-sentence incremental, but the gram-
mar learned is inherently word-by-word incre-
mental (see section 2.2 below). However, their
method requires a higher degree of supervision
than (Kwiatkowski et al, 2012): the LFs assumed
are not simply flat semantic formulae, but full DS
semantic trees (see e.g. Fig. 1) containing infor-
mation about the function-argument structure re-
95
quired for their composition, in addition to fine
grained type and formula annotations. Further,
they test their method only on artificial data cre-
ated using a known, manually-specified DS gram-
mar. In contrast, in this paper we provide an
approach which can learn from LFs without any
compositional structure information, and test it on
real language data; thus providing the first prac-
tical learning system for an explicitly incremental
grammar that we are aware of.
2.2 Dynamic Syntax (DS)
Dynamic Syntax (Kempson et al, 2001; Cann et
al., 2005) is a parsing-directed grammar formal-
ism, which models the word-by-word incremental
processing of linguistic input. Unlike many other
formalisms, DS models the incremental building
up of interpretations without presupposing or in-
deed recognising an independent level of syntactic
processing. Thus, the output for any given string
of words is a purely semantic tree representing
its predicate-argument structure; tree nodes cor-
respond to terms in the lambda calculus, deco-
rated with labels expressing their semantic type
(e.g. Ty(e)) and formula, with beta-reduction de-
termining the type and formula at a mother node
from those at its daughters (Figure 1).
These trees can be partial, containing unsatis-
fied requirements for node labels (e.g. ?Ty(e) is a
requirement for future development to Ty(e)), and
contain a pointer ? labelling the node currently
under development. Grammaticality is defined as
parsability: the successful incremental construc-
tion of a tree with no outstanding requirements (a
complete tree) using all information given by the
words in a sentence. The complete sentential LF
is then the formula decorating the root node ? see
Figure 1. Note that in these trees, leaf nodes do
not necessarily correspond to words, and may not
be in linear sentence order; syntactic structure is
not explicitly represented, only the structure of se-
mantic predicate-argument combination.
2.2.1 Actions in DS
The parsing process is defined in terms of condi-
tional actions: procedural specifications for mono-
tonic tree growth. These include general structure-
building principles (computational actions), puta-
tively independent of any particular natural lan-
guage, and language-specific actions associated
with particular lexical items (lexical actions). The
latter are what we learn from data here.
Computational actions These form a small,
fixed set, which we assume as given here. Some
merely encode the properties of the lambda cal-
culus and the logical tree formalism itself, LoFT
(Blackburn and Meyer-Viol, 1994) ? these we
term inferential actions. Examples include THIN-
NING (removal of satisfied requirements) and
ELIMINATION (beta-reduction of daughter nodes
at the mother). These actions are language-
independent, cause no ambiguity, and add no new
information to the tree; as such, they apply non-
optionally whenever their preconditions are met.
Other computational actions reflect the fun-
damental predictivity and dynamics of the DS
framework. For example, *-ADJUNCTION in-
troduces a single unfixed node with underspec-
ified tree position (replacing feature-passing or
type-raising concepts for e.g. long-distance depen-
dency); and LINK-ADJUNCTION builds a paired
(?linked?) tree corresponding to semantic con-
junction (licensing relative clauses, apposition and
more). These actions represent possible parsing
strategies and can apply optionally whenever their
preconditions are met. While largely language-
independent, some are specific to language type
(e.g. INTRODUCTION-PREDICTION in the form
used here applies only to SVO languages).
Lexical actions The lexicon associates words
with lexical actions; like computational actions,
these are sequences of tree-update actions in an
IF..THEN..ELSE format, and composed of ex-
plicitly procedural atomic tree-building actions
such as make (creates a new daughter node),
go (moves the pointer), and put (decorates the
pointed node with a label). Figure 2 shows an ex-
ample for a proper noun, John. The action checks
whether the pointed node (marked as ?) has a re-
quirement for type e; if so, it decorates it with type
e (thus satisfying the requirement), formula John?
and the bottom restriction ???? (meaning that the
node cannot have any daughters). Otherwise the
action aborts, i.e. the word ?John? cannot be parsed
in the context of the current tree.
Graph-based Parsing & Generation These ac-
tions define the parsing process. Given a sequence
of words (w1, w2, ..., wn), the parser starts from
the axiom tree T0 (a requirement to construct a
complete propositional tree, ?Ty(t)), and applies
the corresponding lexical actions (a1, a2, . . . , an),
optionally interspersing computational actions.
96
Action Input tree Output tree
John
IF ?Ty(e)
THEN put(Ty(e))
put(Fo(John?)
put(????)
ELSE ABORT
?Ty(t)
?Ty(e),
?
?Ty(e ? t)
?John??? ?Ty(t)
Ty(e), ?Ty(e)
John?, ????,?
?Ty(e ? t)
Figure 2: Lexical action for the word ?John?
T0
T1intro
T2pred
T3
link-adj
T4*-adj
T5
john
abort
T6
john
?john?
T7
thin
T8
comp
T9
pred
T10
link-adj
T11
thin
T12
comp
T13
likes
abort
abort
?likes?
Figure 3: DS parsing as a graph: actions (edges) are transitions between partial trees (nodes).
This parsing process can be modelled as a di-
rected acyclic graph (DAG) rooted at T0, with par-
tial trees as nodes, and computational and lexi-
cal actions as edges (i.e. transitions between trees)
(Sato, 2011). Figure 3 shows an example: here,
intro, pred and *adj correspond to the computa-
tional actions INTRODUCTION, PREDICTION and
*-ADJUNCTION respectively; and ?john? is a lex-
ical action. Different DAG paths represent dif-
ferent parsing strategies, which may succeed or
fail depending on how the utterance is continued.
Here, the path T0?T3 will succeed if ?John? is the
subject of an upcoming verb (?John upset Mary?);
T0 ? T4 will succeed if ?John? turns out to be a
left-dislocated object (?John, Mary upset?).
This incrementally constructed DAG makes up
the entire parse state at any point. The right-
most nodes (i.e. partial trees) make up the current
maximal semantic information; these nodes with
their paths back to the root (tree-transition actions)
make up the linguistic context for ellipsis and
pronominal construal (Purver et al, 2011). Given
a conditional probability distribution P (a|w, T )
over possible actions a given a word w and (some
set of features of) the current partial tree T , we can
parse probabilistically, constructing the DAG in a
best-first, breadth-first or beam parsing manner.
Generation uses exactly the same actions and
structures, and can be modelled on the same DAG
with the addition only of a goal tree; partial
trees are checked for subsumption of the goal
at each stage. The framework therefore inher-
ently provides both parsing and generation that
are word-by-word incremental and interchange-
able, commensurate with psycholinguistic results
(Lombardo and Sturt, 1997; Ferreira and Swets,
2002) and suitable for modelling dialogue (Howes
et al, 2012). While standard grammar formalisms
can of course also be used with incremental pars-
ing or generation algorithms (Hale, 2001; Collins
and Roark, 2004; Clark and Curran, 2007), their
string-based grammaticality and lack of inherent
parsing-generation interoperability means exam-
ples such as (1) remain problematic.
3 Method
Our task here is to learn an incremental DS gram-
mar; following Kwiatkowski et al (2012), we
assume as input a set of sentences paired with
their semantic LFs. Eshghi et al (2013) outline a
method for inducing DS grammars from semantic
DS trees (e.g. Fig. 1), in which possible lexical en-
tries are incrementally hypothesized, constrained
by subsumption of the target tree for the sentence.
Here, however, this structured tree information is
not available to us; our method must therefore con-
strain hypotheses via compatibility with the sen-
tential LF, represented as Record Types of Type
Theory with Records (TTR).
3.1 Type Theory with Records (TTR)
Type Theory with Records (TTR) is an exten-
sion of standard type theory shown useful in se-
mantics and dialogue modelling (Cooper, 2005;
Ginzburg, 2012). It is also used for representing
97
non-linguistic context such as the visual percep-
tion of objects (Dobnik et al, 2012), suggesting
potential for embodied learning in future work.
Some DS variants have incorporated TTR as the
semantic LF representation (Purver et al, 2011;
Hough and Purver, 2012; Eshghi et al, 2012).
Here, it can provide us with the mechanism we
need to constrain hypotheses in induction by re-
stricting them to those which lead to subtypes of
the known sentential LF.
In TTR, logical forms are specified as record
types (RTs), sequences of fields of the form [ l : T ]
containing a label l and a type T . RTs can be wit-
nessed (i.e. judged true) by records of that type,
where a record is a sequence of label-value pairs
[ l = v ], and [ l = v ] is of type [ l : T ] just in case
v is of type T .
R1 :
?
?
l1 : T1
l2=a : T2
l3=p(l2) : T3
?
? R2 :
[
l1 : T1
l2 : T2?
]
R3 : []
Figure 4: Example TTR record types
Fields can be manifest, i.e. given a singleton
type e.g. [ l : Ta ] where Ta is the type of which
only a is a member; here, we write this using the
syntactic sugar [ l=a : T ]. Fields can also be de-
pendent on fields preceding them (i.e. higher) in
the record type ? see R1 in Figure 4. Importantly
for us here, the standard subtyping relation ? can
be defined for record types: R1 ? R2 if for all
fields [ l : T2 ] in R2, R1 contains [ l : T1 ] where
T1 ? T2. In Figure 4, R1 ? R2 if T2 ? T2? , and
both R1 and R2 are subtypes of R3.
Following Purver et al (2011), we assume
that DS tree nodes are decorated not with simple
atomic formulae but with RTs, and correspond-
ing lambda abstracts representing functions from
RT to RT (e.g. ?r : [ l1 : T1 ].[ l2=r.l1 : T1 ] where
r.l1 is a path expression referring to the label l1
in r) ? see Figure 5. The equivalent of conjunc-
tion for linked trees is now RT extension (concate-
nation modulo relabelling ? see (Cooper, 2005;
Ferna?ndez, 2006)). TTR?s subtyping relation now
allows a record type at the root node to be in-
ferred for any partial tree, and incrementally fur-
ther specified via subtyping as parsing proceeds
(Hough and Purver, 2012).
We assume a field head in all record types, with
this corresponding to the DS tree node type. We
also assume a neo-Davidsonian representation of
?, T y(t),
?
?
?
x=john : e
e=arrive : es
p=subj(e,x) : t
head=p : t
?
?
?
Ty(e),
[
x=john : e
head=x : e
]
Ty(e ? t),
?r :
[
head : e
]
.
?
?
?
x=r.head : e
e=arrive : es
p=subj(e,x) : t
head=p : t
?
?
?
Figure 5: DS-TTR tree
predicates, with fields corresponding to the event
and to each semantic role; this allows all available
semantic information to be specified incrementally
via strict subtyping (e.g. providing the subj() field
when subject but not object has been parsed) ? see
Figure 5 for an example.
3.2 Problem Statement
Our induction procedure now assumes as input:
? a known set of DS computational actions.
? a set of training examples of the form
?Si, RTi?, where Si = ?w1 . . . wn? is a sen-
tence of the language and RTi ? henceforth
referred to as the target RT ? is the record
type representing the meaning of Si.
The output is a grammar specifying the possi-
ble lexical actions for each word in the corpus.
Given our data-driven approach, we take a prob-
abilistic view: we take this grammar as associat-
ing each word w with a probability distribution ?w
over lexical actions. In principle, for use in pars-
ing, this distribution should specify the posterior
probability p(a|w, T ) of using a particular action
a to parse a word w in the context of a particular
partial tree T . However, here we make the sim-
plifying assumption that actions are conditioned
solely on one feature of a tree, the semantic type
Ty of the currently pointed node; and that actions
apply exclusively to one such type (i.e. ambiguity
of type implies multiple actions). This simplifies
our problem to specifying the probability p(a|w).
In traditional DS terms, this is equivalent to as-
suming that all lexical actions have a simple IF
clause of the form IF ?Ty(X); this is true of
most lexical actions in existing DS grammars (see
Fig. 2), but not all. Our assumption may there-
fore lead to over-generation ? inducing actions
which can parse some ungrammatical strings ? we
must rely on the probabilities learned to make such
98
parses unlikely, and evaluate this in Section 4.
Given this, our focus here is on learning the THEN
clauses of lexical actions: sequences of DS atomic
actions such as go, make, and put (Fig. 2), but now
with attendant posterior probabilities. We will
henceforth refer to these sequences as lexical hy-
potheses. We first describe how we construct lexi-
cal hypotheses from individual training examples;
we then show how to generalise over these, while
incrementally estimating corresponding probabil-
ity distributions.
3.3 Hypothesis construction
DS is strictly monotonic: actions can only extend
the current (partial) tree Tcur, deleting nothing ex-
cept satisfied requirements. Thus, we can hypoth-
esise lexical actions by incrementally exploring
the space of all monotonic, well-formed exten-
sions T of Tcur, whose maximal semantics R is
a supertype of (extendible to) the target RT (i.e.
R ? RT ). This gives a bounded space described
by a DAG equivalent to that of section 2.2.1: nodes
are trees; edges are possible extensions; paths start
from Tcur and end at any tree with LF RT . Edges
may be either known computational actions or
new lexical hypotheses. The space is further con-
strained by the properties of the lambda-calculus
and the modal tree logic LoFT (not all possible
trees and extensions are well-formed).1
Hypothesising increments In purely semantic
terms, the hypothesis space at any point is the pos-
sible set of TTR increments from the current LF
R to the target RT . We can efficiently compute
and represent these possible increments using a
type lattice (see Figure 6),2 which can be con-
structed for the whole sentence before processing
each training example. Each edge is a RTR repre-
senting an increment from one RT, Rj , to another,
Rj+1, such that Rj ? RI = Rj+1 (where ? rep-
resents record type intersection (Cooper, 2005));
possible parse DAG paths must correspond to
some path through this lattice.
Hypothesising tree structure These DAG paths
can now be hypothesised with the lattice as a con-
straint: hypothesising possible sequences of ac-
1We also prevent arbitrary type-raising by restricting the
types allowed, taking the standard DS assumption that noun
phrases have semantic type e (rather than a higher type as in
Generalized Quantifier theory) and common nouns their own
type cn, see Cann et al (2005), chapter 3 for details.
2Clark (2011) similarly use a concept lattice relating
strings to their contexts in syntactic grammar induction.
Ri : []
R11 :
[
a : b
]
R12 :
[
c : d
]
R12 :
[
e : f
]
R21 :
[
a : b
c : d
]
R22 :
[
a : b
e : f
]
R22 :
[
c : d
e : f
]
RT :
?
?
a : b
c : d
e : f
?
?
Figure 6: RT extension hypothesis lattice
tions which extend the tree to produce the required
semantic increment, while the increments them-
selves constitute a search space of their own which
we explore by traversing the lattice.
The lexical hypotheses comprising these DAG
paths are divide into two general classes: (1) tree-
building hypotheses, which hypothesise appropri-
ately typed daughters to compose a given node;
and (2) content hypotheses, which decorate leaf
nodes with appropriate formulae from Ri (non-
leaf nodes then receive their content via beta-
reduction/extension of daughters).
Tree-building can be divided into two general
options: functional decomposition (corresponding
to the addition of daughter nodes with appropri-
ate types and formulae which will form a suitable
mother node by beta-reduction); and type exten-
sion (corresponding to the adjunction of a linked
tree whose LF will extend that of the current tree,
see Sec. 3.1 above). The availability of the former
is constrained by the presence of suitable depen-
dent types in the LF (e.g. in Fig. 5, p = subj(e, x)
depends on the fields with labels x and e, and
could therefore be hypothesised as the body of a
function with x and/or e as argument). The latter is
more generally available, but constrained by shar-
ing of a label between the resulting linked trees.
Figure 7 shows an example: a template for
functional decomposition hypotheses, extending a
node with some type requirement ?Ty(X) with
daughter nodes which can combine to satisfy that
requirement ? here, of types Y and Y ? X.
Specific instantiations are limited to a finite set of
types: e.g. X = e ? t and Y = e is allowed,
but higher types for Y are not. We implement
these constraints by packaging together permitted
sequences of tree updates as macros, and using
these macros to hypothesise DAG paths commen-
surate with the lattice.
Finally, semantic content decorations (as se-
99
IF ?Ty(X)
THEN make(??0?); go(??0?)
put(?Ty(Y )); go(???)
make(??1?); go(??1?)
put(?Ty(Y ? X)); go(?)
ELSE ABORT
Figure 7: Tree-building hypothesis
quences of put operations) are hypothesised for
the leaf nodes of the tree thus constructed; these
are now determined entirely by the tree structure
so far hypothesised and the target LF RT .
3.4 Probabilistic Grammar Estimation
This procedure produces, for each training sen-
tence ?w1 . . . wn?, all possible sequences of ac-
tions that lead from the axiom tree T0 to a tree
with the target RT as its semantics. These must
now be split into n sub-sequences, hypothesising
a set of word boundaries to form discrete word hy-
potheses; and a probability distribution estimated
over this (large) word hypothesis space to provide
a grammar that can be useful in parsing. For this,
we apply the procedure of Eshghi et al (2013).
For each training sentence S = ?w1 . . . wn?,
we have a set HT of possible Hypothesis Tuples
(sequences of word hypotheses), each of the form
HTj = ?hj1 . . . h
j
n?, where hji is the word hypoth-
esis for wi in HTj . We must estimate a prob-
ability distribution ?w over hypotheses for each
word w, where ?w(h) is the posterior probability
p(h|w) of a given word hypothesis h being used to
parse w. Eshghi et al (2013) define an incremen-
tal version of Expectation-Maximisation (Demp-
ster et al, 1977) for use in this setting.
Re-estimation At any point, the Expectation
step assigns each hypothesis tuple HTj a proba-
bility based on the current estimate ??w:
p(HTj|S) =
n
?
i=1
p(hji |wi) =
n
?
i=1
??wi(h
j
i ) (2)
The Maximisation step then re-estimates
p(h|w) as the normalised sum of the probabilities
of all observed tuples HTj which contain h,w:
???w(h) =
1
Z
?
{j|h,w?HTj}
n
?
i=1
??wi(h
j
i ) (3)
where Z is the appropriate normalising constant
summed over all the HTj?s.
Incremental update The estimate of ?w is now
updated incrementally at each training example:
the new estimate ?Nw is a weighted average of the
previous estimate ?N?1w and the new value from
the current example ???w from equation (3):
?Nw (h) =
N ? 1
N ?
N?1
w (h) +
1
N ?
??
w(h) (4)
?e.not(aux|do(v|have(pro|he, det|a(x,n|hat(x)), e), e), e)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
e=have : es
p3=not(e) : t
p2=do-aux(e) : t
r :
?
?
x : e
p=hat(x) : t
head=x : e
?
?
x2=?(r.head,r) : e
x1=he : e
p1=object(e,x2) : t
p=subject(e,x1) : t
head=e : es
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 8: Conversion of LFs from FOL to TTR.
For the first training example, a uniform distribu-
tion is assumed; when subsequent examples pro-
duce new previously unseen hypotheses these are
assigned probabilities uniformly distributed over a
held-out probability mass.
4 Experimental Setup
Corpus We tested our approach on a section
of the Eve corpus within CHILDES (MacWhin-
ney, 2000), a series of English child-directed ut-
terances, annotated with LFs by Kwiatkowski et
al. (2012) following Sagae et al (2004)?s syntactic
annotation. We convert these LFs into semanti-
cally equivalent RTs; e.g. Fig 8 shows the conver-
sion to a record type for ?He doesn?t have a hat?.
Importantly, our representations remove all
part-of-speech or syntactic information; e.g. the
subject, object and indirect object predicates func-
tion as purely semantic role information express-
ing an event?s participants. This includes e.g.
do-aux(e) in (8), which is taken merely to rep-
resent temporal/aspectual information about the
event, and could be part of any word hypothesis.
From this corpus we selected 500 short
utterance-record type pairs. The minimum utter-
ance length in this set is 1 word, maximum 7,
mean 3.7; it contains 1481 word tokens of 246
types, giving a type:token ratio of 6.0). We use the
first 400 for training and 100 for testing; the test
set alo has a mean utterance length of 3.7 words,
and contains only words seen in training.
Evaluation We evaluate our learner by compar-
ing the record type semantic LFs produced using
the induced lexicon against the gold standard LFs,
calculating precision, recall and f-score using a
method similar to Allen et al (2008).
100
Coverage % Precision Recall F-Score
Top-1 59 0.548 0.549 0.548
Top-2 85 0.786 0.782 0.782
Top-3 92 0.854 0.851 0.851
Table 1: Results: parse coverage & accuracy using
the top N hypotheses induced in training.
Each field has a potential score in the range
[0,1]. A method maxMapping(R1, R2) con-
structs a mapping from fields in R1 to those in R2
to maximise alignment, with fields that map com-
pletely scoring a full 1, and partially mapped fields
receiving less, depending on the proportion of the
R1 field?s representation that subsumes its mapped
R2 field;e.g. a unary predicate field in RT2 such
as
[
p=there(e) : t
]
could score a maximum of
3 - 1 for correct type t, 1 for correct predicate
there and 1 for the subsumption of its argument
e; we use the total to normalise the final score.
The potential maximum for any pair is therefore
the number of fields in R1 (including those in em-
bedded record types). So, for hypothesis H and
goal record type G, with NH and NG fields re-
spectively:
(5) precision = maxMapping(H,G)/NH
recall = maxMapping(H,G)/NG
5 Results
Table 1 shows that the grammar learned achieves
both good parsing coverage and semantic accu-
racy. Using the top 3 lexical hypotheses induced
from training, 92% of test set utterances receive a
parse, and average LF f-score reaches 0.851.
We manually inspected the learned lexicon for
instances of ambiguous words to assess the sys-
tem?s ability to disambiguate (e.g. the word ??s?
(is) has three different senses in our corpus: (1)
auxiliary, e.g. ?the coffee?s coming?; (2) verb
predicating NP identity, e.g. ?that?s a girl?; and
(3) verb predicating location, e.g. ?where?s the
pencil?). From these the first two were in the top
3 hypotheses (probabilities p=0.227 and p=0.068).
For example, the lexical entry learned for (2) is
shown in Fig. 9.
However, less common words fared worse: e.g.
the double object verb ?put?, with only 3 tokens,
had no correct hypothesis in the top 5. Given suffi-
cient frequency and variation in the token distribu-
tions, our method appears successful in inducing
the correct incremental grammar. However, the
complexity of the search space also limits the pos-
sibility of learning from larger record types, as the
space of possible subtypes used for hypothesising
IF ?Ty(e ? t)
THEN make(??0?); go(??0?)
put(?Ty(e))
go(??0?)
make(??1?); go(??1?)
put(Ty(e ? (e ? t)))
put(Fo(
?r1 :
[
head : e
]
?r2 :
[
head : e
]
.
?
?
?
?
?
?
?
?
x1=r1.head : e
x2=r2.head : e
e=eq : es
p1=subj(e,x2) : t
p2=obj(e,x1) : t
head=e : t
?
?
?
?
?
?
?
?
))
put(????)
ELSE ABORT
Figure 9: Action learned for second sense of ?is?
tree structure grows exponentially with the num-
ber of fields in the type. Therefore, when learning
from longer, more complicated sentences, we may
need to bring in further sources of bias to constrain
our hypothesis process further (e.g. learning from
shorter sentences first).
6 Conclusions
We have outlined a novel method for the induc-
tion of a probabilistic grammar in an inherently in-
cremental and semantic formalism, Dynamic Syn-
tax, compatible with dialogue phenomena such
as compound contributions and with no indepen-
dent level of syntactic phrase structure. Assum-
ing only general compositional mechanisms, our
method learns from utterances paired with their
logical forms represented as TTR record types.
Evaluation on a portion of the CHILDES corpus
of child-directed dialogue utterances shows good
coverage and semantic accuracy, which lends sup-
port to viewing it as a plausible, yet idealised, lan-
guage acquisition model.
Future work planned includes refining the
method outlined above for learning from longer
utterances, and then from larger corpora e.g. the
Groningen Meaning Bank (Basile et al, 2012),
which includes more complex structures. This will
in turn enable progress towards large-scale incre-
mental semantic parsers and allow further investi-
gation into semantically driven language learning.
101
References
James F. Allen, Mary Swift, and Will de Beaumont.
2008. Deep Semantic Analysis of Text. In Johan
Bos and Rodolfo Delmonte, editors, Semantics in
Text Processing. STEP 2008 Conference Proceed-
ings, volume 1 of Research in Computational Se-
mantics, pages 343?354. College Publications.
Valerio Basile, Johan Bos, Kilian Evang, and Noortje
Venhuizen. 2012. Developing a large semantically
annotated corpus. In Proceedings of the Eight In-
ternational Conference on Language Resources and
Evaluation (LREC 2012), pages 3196?3200, Istan-
bul, Turkey.
Patrick Blackburn and Wilfried Meyer-Viol. 1994.
Linguistics, logic and finite trees. Logic Journal
of the Interest Group of Pure and Applied Logics,
2(1):3?29.
Ronnie Cann, Ruth Kempson, and Lutz Marten. 2005.
The Dynamics of Language. Elsevier, Oxford.
Eugene Charniak. 1996. Statistical Language Learn-
ing. MIT Press.
Stephen Clark and James Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Alexander Clark and Shalom Lappin. 2011. Linguistic
Nativism and the Poverty of the Stimulus. Wiley-
Blackwell.
Alexander Clark. 2011. A learnable representation for
syntax using residuated lattices. In Philippe Groote,
Markus Egg, and Laura Kallmeyer, editors, Formal
Grammar, volume 5591 of Lecture Notes in Com-
puter Science, pages 183?198. Springer Berlin Hei-
delberg.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of the 42ndMeeting of the ACL, pages 111?118,
Barcelona.
Robin Cooper. 2005. Records and record types in se-
mantic theory. Journal of Logic and Computation,
15(2):99?112.
Matthew Crocker, Martin Pickering, and Charles
Clifton, editors. 2000. Architectures and Mecha-
nisms in Sentence Comprehension. Cambridge Uni-
versity Press.
A.P. Dempster, N.M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical So-
ciety. Series B (Methodological), 39(1):1?38.
Simon Dobnik, Robin Cooper, and Staffan Larsson.
2012. Modelling language, action, and perception in
type theory with records. In Proceedings of the 7th
International Workshop on Constraint Solving and
Language Processing (CSLP12), pages 51?63.
Arash Eshghi, Julian Hough, Matthew Purver, Ruth
Kempson, and Eleni Gregoromichelaki. 2012. Con-
versational interactions: Capturing dialogue dynam-
ics. In S. Larsson and L. Borin, editors, From Quan-
tification to Conversation: Festschrift for Robin
Cooper on the occasion of his 65th birthday, vol-
ume 19 of Tributes, pages 325?349. College Publi-
cations, London.
Arash Eshghi, Matthew Purver, and Julian Hough.
2013. Probabilistic induction for an incremental se-
mantic grammar. In Proceedings of the 10th In-
ternational Conference on Computational Seman-
tics (IWCS 2013) ? Long Papers, pages 107?118,
Potsdam, Germany, March. Association for Compu-
tational Linguistics.
Raquel Ferna?ndez. 2006. Non-Sentential Utterances
in Dialogue: Classification, Resolution and Use.
Ph.D. thesis, King?s College London, University of
London.
Fernanda Ferreira and Benjamin Swets. 2002. How
incremental is language production? evidence from
the production of utterances requiring the compu-
tation of arithmetic sums. Journal of Memory and
Language, 46:57?84.
Victor Ferreira. 1996. Is it better to give than to do-
nate? Syntactic flexibility in language production.
Journal of Memory and Language, 35:724?755.
Jonathan Ginzburg. 2012. The Interactive Stance:
Meaning for Conversation. Oxford University
Press.
E. Mark Gold. 1967. Language identification in the
limit. Information and Control, 10(5):447?474.
John Hale. 2001. A probabilistic Earley parser as
a psycholinguistic model. In Proceedings of the
2nd Conference of the North American Chapter of
the Association for Computational Linguistics, Pitts-
burgh, PA.
Julian Hough and Matthew Purver. 2012. Process-
ing self-repairs in an incremental type-theoretic di-
alogue system. In Proceedings of the 16th SemDial
Workshop on the Semantics and Pragmatics of Di-
alogue (SeineDial), pages 136?144, Paris, France,
September.
Christine Howes, Matthew Purver, Rose McCabe,
Patrick G. T. Healey, and Mary Lavelle. 2012.
Predicting adherence to treatment for schizophrenia
from dialogue transcripts. In Proceedings of the
13th Annual Meeting of the Special Interest Group
on Discourse and Dialogue (SIGDIAL 2012 Confer-
ence), pages 79?83, Seoul, South Korea, July. Asso-
ciation for Computational Linguistics.
Ruth Kempson,WilfriedMeyer-Viol, and Dov Gabbay.
2001. Dynamic Syntax: The Flow of Language Un-
derstanding. Blackwell.
102
Dan Klein and Christopher D. Manning. 2005. Nat-
ural language grammar induction with a genera-
tive constituent-context mode. Pattern Recognition,
38(9):1407?1419.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, andMark Steedman. 2010. Inducing probabilis-
tic CCG grammars from logical form with higher-
order unification. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1223?1233, Cambridge, MA, Oc-
tober. Association for Computational Linguistics.
Tom Kwiatkowski, Sharon Goldwater, Luke Zettle-
moyer, and Mark Steedman. 2012. A proba-
bilistic model of syntactic and semantic acquisition
from child-directed utterances and their meanings.
In Proceedings of the Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL).
Vincenzo Lombardo and Patrick Sturt. 1997. Incre-
mental processing and infinite local ambiguity. In
Proceedings of the 1997 Cognitive Science Confer-
ence.
Brian MacWhinney. 2000. The CHILDES Project:
Tools for Analyzing Talk. Lawrence Erlbaum As-
sociates, Mahwah, New Jersey, third edition.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed cor-
pora. In Proceedings of the 30th Annual Meeting
of the Association for Computational Linguistics,
pages 128?135, Newark, Delaware, USA, June. As-
sociation for Computational Linguistics.
Matthew Purver, Arash Eshghi, and Julian Hough.
2011. Incremental semantic construction in a di-
alogue system. In J. Bos and S. Pulman, editors,
Proceedings of the 9th International Conference on
Computational Semantics, pages 365?369, Oxford,
UK, January.
Kenji Sagae, Brian MacWhinney, and Alon Lavie.
2004. Adding syntactic annotations to transcripts of
parent-child dialogs. In Proceedings of the 4th In-
ternational Conference on Language Resources and
Evaluation (LREC), pages 1815?1818, Lisbon.
Yo Sato. 2011. Local ambiguity, search strate-
gies and parsing in Dynamic Syntax. In E. Gre-
goromichelaki, R. Kempson, and C. Howes, editors,
The Dynamics of Lexical Interfaces. CSLI Publica-
tions.
Matthew Saxton. 1997. The contrast theory of nega-
tive input. Journal of Child Language, 24(1):139?
161.
Mark Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, MA.
Luke Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In Proceedings of the Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL).
103
