Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 390?399, Prague, June 2007. c?2007 Association for Computational Linguistics
Part-of-speech Tagging for Middle English through Alignment and
Projection of Parallel Diachronic Texts
Taesun Moon and Jason Baldridge
Department of Linguistics
University of Texas at Austin
1 University Station B5100
Austin, TX 78712-0198 USA
tsmoon, jbaldrid@mail.utexas.edu
Abstract
We demonstrate an approach for inducing a
tagger for historical languages based on ex-
isting resources for their modern varieties.
Tags from Present Day English source text
are projected to Middle English text using
alignments on parallel Biblical text. We
explore the use of multiple alignment ap-
proaches and a bigram tagger to reduce the
noise in the projected tags. Finally, we train
a maximum entropy tagger on the output of
the bigram tagger on the target Biblical text
and test it on tagged Middle English text.
This leads to tagging accuracy in the low
80?s on Biblical test material and in the 60?s
on other Middle English material. Our re-
sults suggest that our bootstrapping meth-
ods have considerable potential, and could
be used to semi-automate an approach based
on incremental manual annotation.
1 Introduction
Annotated corpora of historical texts provide an im-
portant resource for studies of syntactic variation
and change in diachronic linguistics. For example,
the Penn-Helsinki Parsed Corpus of Middle English
(PPCME) (Kroch and Taylor, 2000) has been used
to show the existence of syntactic dialectal differ-
ences between northern and southern Middle En-
glish (Kroch et al, 2000) and to examine the syn-
tactic evolution of the English imperative construc-
tion (Han, 2000). However, their utility rests on their
having coverage of a significant amount of annotated
material from which to draw patterns for such stud-
ies, and creating resources such as the PPCME re-
quire significant time and cost to produce. Corpus
linguists interested in diachronic language studies
thus need efficient ways to produce such resources.
One approach to get around the annotation bottle-
neck is to use semi-automation. For example, when
producing part-of-speech tags for the Tycho Brahe
corpus of Historical Portuguese (Britto et al, 2002),
a set of seed sentences was manually tagged, and the
Brill tagger (Brill, 1995) was then trained on those
and consequently used to tag other sentences. The
output was inspected for errors, the tagger was re-
trained and used again to tag new sentences, for sev-
eral iterations.
We also seek to reduce the human effort involved
in producing part-of-speech tags for historical cor-
pora. However, our approach does so by leveraging
existing resources for a language?s modern varieties
along with parallel diachronic texts to produce accu-
rate taggers. This general technique has worked well
for bilingual bootstrapping of language processing
resources for one language based on already avail-
able resources from the other. The first to explore
the idea were Yarowsky and Ngai (2001), who in-
duced a part-of-speech tagger for French and base
noun phrase detectors for French and Chinese via
transfer from English resources. They built a highly
accurate POS tagger by labeling English text with an
existing tagger (trained on English resources), align-
ing that text with parallel French, projecting the au-
tomatically assigned English POS tags across these
alignments, and then using the automatically labeled
French text to train a new French tagger. This tech-
390
nique has since been used for other languages and
tasks, e.g. morphological analysis (Yarowsky et al,
2001), fine-grained POS tagging for Czech (Dra?bek
and Yarowsky, 2005), and tagging and inducing syn-
tactic dependencies for Polish (Ozdowska, 2006).
This methodology holds great promise for pro-
ducing tools and annotated corpora for processing
diachronically related language pairs, such as Mod-
ern English to Middle or Old English. Historical
languages suffer from a paucity of machine readable
text, inconsistencies in orthography, and grammati-
cal diversity (in the broadest sense possible). This
diversity is particularly acute given that diachronic
texts of a given language encompass texts and gen-
res spanning across centuries or millenia with a
plethora of extra-linguistic influences to complicate
the data. Furthermore, even in historically contem-
poraneous texts, possible dialectal variations further
amplify the differences in already idiosyncratic or-
thographies and syntactic structure.
The present study goes further than Britto et al
(2002) by fully automating the alignment, POS tag
induction, and noise elimination process. It is able to
utilize the source language to a greater degree than
the previously mentioned studies that attempted lan-
guage neutrality; that is, it directly exploits the ge-
netic similarity between the source and target lan-
guage. Some amount of surface structural similarity
between a diachronic dialect and its derivatives is to
be expected, and in the case of Middle English and
Modern English, such similarities are not negligible.
The automation process is further aided through
the use of two versions of the Bible, which obviates
the need for sentence alignment. The modern Bible
is tagged using the C&C maximum entropy tagger
(Curran and Clark, 2003), and these tags are trans-
ferred from source to target through high-confidence
alignments aquired from two alignment approaches.
A simple bigram tagger is trained from the resulting
target texts and then used to relabel the same texts as
Middle English training material for the C&C tag-
ger. This tagger utilizes a rich set of features and a
wider context, so it can exploit surface similarities
between the source and target language. By train-
ing it with both the original (Modern English) Penn
Treebank Wall Street Journal (WSJ) material and
our automatically tagged Middle English Wycliffe
material, we achieve an accuracy of 84.8% on pre-
dicting coarse tags, improving upon a 63.4% base-
line of training C&C on the WSJ sentences alone.
Furthermore, we show that the bootstrapped tagger
greatly reduces the error rate on out-of-domain, non-
Biblical Middle English texts.
2 Data
English provides an ideal test case for our study be-
cause of the existence of publically accessible di-
achronic texts of English and their translations in
electronic format and because of the availability of
the large, annotated Penn-Helsinki Parsed Corpus of
Middle English. The former allows us to create a
POS tagger via alignment and projection; the latter
allows us to evaluate the tagger on large quantities
of human-annotated tags.
2.1 The Bible as a parallel corpus
We take two versions of the Bible as our parallel cor-
pus. For modern English, we utilize the NET Bible1.
For Middle English (ME), we utilize John Wycliffe?s
Bible2. The first five lines of Genesis in both Bibles
are shown in Figure 1.
The Bible offers some advantages beyond its
availability. All its translations are numbered, fa-
cilitating assessment of accuracy for sentence align-
ment models. Also, the Bible is quite large for
a single text: approximately 950,000 words for
Wycliffe?s version and 860,000 words for the NET
bible. Finally, Wycliffe?s Bible was released in the
late 14th century, a period when the transition of En-
glish from a synthetic to analytical language was
finalized. Hence, word order was much closer to
Modern English and less flexible than Old English;
also, nominal case distinctions were largely neutral-
ized, though some verbal inflections such as dis-
tinctions for the first and second person singular in
the present tense were still in place (Fennell, 2001).
This places Wycliffe?s Bible as far back as possible
without introducing extreme nominal and verbal in-
flections in word alignment.
The two Bibles were cleaned and processed for
the present task and then examined for levels of
correspondence. The two texts were compared for
1The New English Translation Bible, which may be down-
loaded from http://www.bible.org/page.php?page id=3086.
2Available for download at:
http://wesley.nnu.edu/biblical studies/wycliffe.
391
1 In the beginning God created the heavens and the earth.
2 Now the earth was without shape and empty, and darkness was over the surface of the watery
deep, but the Spirit of God was moving over the surface of the water.
3 God said, ?Let there be light.? And there was light!
4 God saw that the light was good, so God separated the light from the darkness.
5 God called the light day and the darkness night. There was evening, and there was morning,
marking the first day.
1 In the bigynnyng God made of nouyt heuene and erthe.
2 Forsothe the erthe was idel and voide, and derknessis weren on the face of depthe; and the Spiryt
of the Lord was borun on the watris.
3 And God seide, Liyt be maad, and liyt was maad.
4 And God seiy the liyt, that it was good, and he departide the liyt fro derknessis; and he clepide
the liyt,
5 dai, and the derknessis, nyyt. And the euentid and morwetid was maad, o daie.
Figure 1: The first five verses of Genesis the NET Bible (top) and Wycliffe?s Bible (below).
whether there were gaps in the chapters and whether
one version had more chapters over the other. If dis-
crepancies were found, the non-corresponding chap-
ters were removed. Next, because we assume sen-
tences are already aligned in our approach, discrep-
ancies in verses between the two Bibles were culled.
A total of some two hundred lines were removed
from both Bibles. This processing resulted in a total
of 67 books3, with 920,000 words for the Wycliffe
Bible and 840,000 words for the NET Bible.
2.2 The Penn-Helsinki Parsed Corpus of
Middle English
The Penn-Helsinki Parsed Corpus of Middle En-
glish is a collection of text samples derived from
manuscripts dating 1150?1500 and composed dur-
ing the same period or earlier. It is based on and
expands upon the Diachronic Part of the Helsinki
Corpus of English Texts. It contains approximately
1,150,000 words of running text from 55 sources.
The texts are provided in three forms: raw, POS
tagged, and parsed.
Among the texts included are portions of the
Wycliffe Bible. They comprise partial sections of
Genesis and Numbers from the Old Testament and
John I.1?XI.56 from the New Testament. In total,
366 books shared by the churches and one book from the
Apocrypha. A comparison of the two Bibles revealed that
the NET Bible contained the Apocrypha, but only Baruch was
shared between the two versions.
the sections of Wycliffe annotated in PPCME have
some 25,000 words in 1,845 sentences. This was
used as part of the test material. It is important to
note that there are significant spelling differences
from the full Wycliffe text that we use for alignment
? this is a common issue with early writings that
makes building accurate taggers for them more diffi-
cult than for the clean and consistent, edited modern
texts typically used to evaluate taggers.
2.3 Tagsets
The PPCME uses a part-of-speech tag set that has
some differences from that used for the Penn Tree-
bank, on which modern English taggers are gener-
ally trained. It has a total of 84 word tags compared
to the widely used Penn Treebank tag set?s 36 word
tags.4 One of the main reasons for the relative diver-
sity of the PPCME tag set is that it maintains distinc-
tions between the do, have, and be verbs in addition
to non-auxiliary verbs. The tag set is further com-
plicated by the fact that composite POS tags are al-
lowed as in another D+OTHER, midnyght ADJ+N,
or armholes N+NS.
To measure tagging accuracy, we consider two
different tag sets: PTB, and COARSE. A measure-
ment of accuracy is not possible with a direct com-
parison to the PPMCE tags since our approach la-
4In our evaluations, we collapse the many different punctu-
ation tags down to a single tag, PUNC.
392
bels target text in Middle English with tags from
the Penn Treebank. Therefore, with PTB, all non-
corresponding PPCME tags were conflated if neces-
sary and mapped to the Penn Treebank tag set. Be-
tween the two sets, only 8 tags, EX, FW, MD, TO, VB,
VBD, VBN, VBP, were found to be fully identical.
In cases where tags from the two sets denoted the
same category/subcategory, one was simply mapped
to the other. When a PPCME tag made finer dis-
tinctions than a related Penn tag and could be con-
sidered a subcategory of that tag, it was mapped ac-
cordingly. For example, the aforementioned auxil-
iary verb tags in the PPMCE were all mapped to cor-
responding subcategories of the larger VB tag group,
a case in point being the mapping of the perfect par-
ticiple of have HVN to VBN, a plain verbal partici-
ple. For COARSE, the PTB tags were even further
reduced to 15 category tags,5 which is still six more
than the core consensus tag set used in Yarkowsky
and Ngai (2001). Specifically, COARSE was mea-
sured by comparing the first letter of each tag. For
example, NN and NNS are conflated to N.
2.4 Penn Treebank Release 3
The POS tagged Wall Street Journal, sections 2 to
21, from the Penn Treebank Release 3 (Marcus et
al., 1994) was used to train a Modern English tagger
to automatically tag the NET Bible. It was also used
to enhance the maximum likelihood estimates of a
bigram tagger used to label the target text.
3 Approach
Our approach involves three components: (1) pro-
jecting tags from Modern English to Middle English
through alignment; (2) training a bigram tagger; and
(3) bootstrapping the C&C tagger on Middle En-
glish texts tagged by the bigram tagger. This section
describes these components in detail.
3.1 Bootstrapping via alignment
Yarowsky and Ngai (2001) were the first to propose
the use of parallel texts to bootstrap the creation of
taggers. The approach first requires an alignment
to be induced between the words of the two texts;
5Namely, adjective, adverb, cardinal number, complemen-
tizer/preposition, conjunction, determiner, existential there, for-
eign word, interjection, infinitival to, modal, noun, pronoun,
verb, and wh-words.
tags are then projected from words of the source lan-
guage to words of the target language. This natu-
rally leads to the introduction of noise in the target
language tags. Yarowsky and Ngai deal with this
by (a) assuming that each target word can have at
most two tags and interpolating the probability of
tags given a word between the probabilities of the
two most likely tags for that word and (b) interpo-
lating between probabilities for tags projected from
1-to-1 alignments and those from 1-to-n alignments.
Each of these interpolated probabilities is parame-
terized by a single variable; however, Yarowsky and
Ngai do not provide details for how the two param-
eter values were determined/optimized.
Here, we overcome much of the noise by using
two alignment approaches, one of which exploits
word level similarities (present in genetically de-
rived languages such as Middle English and Present
Day English) and builds a bilingual dictionary be-
tween them. We also fill in gaps in the alignment
by using a bigram tagger that is trained on the noisy
tags and then used to relabel the entire target text.
The C&C tagger (Curran and Clark, 2003) was
trained on the Wall Street Journal texts in the Penn
Treebank and then used to tag the NET Bible (the
source text). The POS tags were projected from the
source to the Wycliffe Bible based on two alignment
approaches, the Dice coefficient and Giza++, as de-
scribed below.
3.1.1 Dice alignments
A dictionary file is built using the variation of
the Dice Coefficient (Dice (1945)) used by Kay and
Ro?scheisen (1993):
D(v,w) = 2cNA(v) + NB(w)
? ?
Here, c is the number of cooccurring positions and
NT (x) is the number of occurrences of word x in
corpus T . c is calculated only once for redundant
occurrences in an aligned sentence pair. For exam-
ple, it is a given that the will generally occur more
than once in each aligned sentence. However, even if
the occurs more than once in each of the sentences in
aligned pair sA and sB, c is incremented only once.
v and w are placed in the word alignment table if
they exceed the threshold value ?, which is an em-
pirically determined, heuristic measure.
393
The dictionary was structured to establish a sur-
jective relation from the target language to the
source language. Therefore, no lexeme in the
Wycliffe Bible was matched to more than one lex-
eme in the NET Bible. The Dice Coefficient was
modified so that for a given target word v
Dv = arg max
w
D(v,w)
would be mapped to a corresponding word from the
source text, such that the Dice Coefficient would be
maximized. Dictionary entries were further culled
by removing (v,w) pairs whose maximum Dice Co-
efficient was lower than the ? threshold, for which
we used the value 0.5. Finally, each word which had
a mapping from the target was sequentially mapped
to a majority POS tag. For example, the word like
which had been assigned four different POS tags,
IN, NN, RB, VB, by the C&C tagger in the NET
Bible was only mapped to IN since the pairings of
the two occurred the most frequently. The result is
a mapping from one or more target lexemes to a
source lexeme to a majority POS tag. In the case
of like, two words from the target, as and lijk, were
mapped thereto and to the majority tag IN.
Later, we will refer to the Wycliffe text (partially)
labeled with tags projected using the Dice coeffi-
cient as DICE 1TO1.
3.1.2 GIZA++ alignments
Giza++ (Och and Ney, 2003) was also used to de-
rive 1-to-n word alignments between the NET Bible
and the Wycliffe Bible. This produces a tagged ver-
sion of the Wycliffe text which we will refer to as
GIZA 1TON. In our alignment experiment, we used
a combination of IBM Model 1, Model 3, Model 4,
and an HMM model in configuring Giza++.
GIZA 1TON was further processed to remove
noise from the transferred tag set by creating a 1-to-1
word alignment: each word in the target Middle En-
glish text was given its majority tag based on the as-
signment of tags to GIZA 1TON as a whole. We call
this version of the tagged Wycliffe text GIZA 1TO1.
3.2 Bigram tagger
Note that because the projected tags in the Wycliffe
materials produced from the alignments are incom-
plete, there are words in the target text which have
no tag. Nonetheless, a bigram tagger can be trained
from maximum likelihood estimates for the words
and tag sequences which were successfully pro-
jected. This serves two functions: (1) it creates a
useable bigram tagger and (2) the bigram tagger can
be used to fill in the gaps so that the more powerful
C&C tagger can be trained on the target text.
A bigram tagger selects the most likely tag se-
quence T for a word sequence W by:
arg max
T
P (T |W ) = P (W |T )P (T )
Computing these terms requires knowing the transi-
tion probabilities P (ti|ti?1) and the emission proba-
bilities P (wi|ti). We use straightforward maximum
likelihood estimates from data with projected tags:
P (ti|ti?1) =
f(ti?1, ti)
f(ti?1)
P (wi|ti) =
f(wi, ti)
f(ti)
Estimates for unseen events were obtained
through add-one smoothing.
In order to diversify the maximum likelihood es-
timates and provide robustness against the errors
of any one alignment method, we concatenate sev-
eral tagged versions of the Wycliffe Bible with tags
projected from each of our methods (DICE 1TO1,
GIZA 1TON, and GIZA 1TO1) and the NET Bible
(and its tags from the C&C tagger).
3.3 Training C&C on projected tags
The bigram tagger learned from the aligned text has
very limited context and cannot use rich features
such as prefixes and suffixes of words in making its
predictions. In contrast, the C&C tagger, which is
based on that of Ratnaparkhi (1996), utilizes a wide
range of features and a larger contextual window in-
cluding the previous two tags and the two previous
and two following words. However, the C&C tagger
cannot train on texts which are not fully tagged for
POS, so we use the bigram tagger to produce a com-
pletely labeled version of the Wycliffe text and train
the C&C tagger on this material. The idea is that
even though it is training on imperfect material, it
will actually be able to correct many errors by virtue
of its greater discriminitive power.
394
Evaluate on Evaluate on
PPCME Wycliffe PPCME Test
Model PTB COARSE PTB COARSE
(a) Baseline, tag NN 9.0 17.7 12.6 20.1
(b) C&C, trained on gold WSJ 56.2 63.4 56.2 62.3
(c) Bigram, trained on DICE 1TO1 and GIZA 1TON 68.0 73.1 43.9 49.8
(d) Bigram, trained on DICE 1TO1 and GIZA 1TO1 74.8 80.5 58.0 63.9
(e) C&C, trained on BOOTSTRAP (920k words) 78.8 84.1 61.3 67.8
(f) C&C, trained on BOOTSTRAP and WSJ and NET 79.5 84.8 61.9 68.5
(g) C&C, trained on (gold) PPCME Wycliffe (25k words) n/a n/a 71.0 76.0
(h) C&C, trained on (gold) PPCME training set (327k words) 95.9 96.9 93.7 95.1
Figure 2: Tagging results. See section 4 for discussion.
We will refer to the version of the Wycliffe text
(fully) tagged in this way as BOOTSTRAP.
4 Experiments
The M3 and M34 subsections6 of the Penn Helsinki
corpus were chosen for testing since it is not only
from the same period as the Wycliffe Bible but since
it also includes portions of the Wycliffe Bible. A
training set of 14 texts comprising 330,000 words
was selected to train the C&C tagger and test the
cost necessary to equal or exceed the automatic im-
plementation. The test set consists of 4 texts with
110,000 words. The sample Wycliffe Bible with the
gold standard tags has some 25,000 words.
The results of the various configurations are given
in Figure 2, and are discussed in detail below.
4.1 Baselines
We provide two baselines. The first is the result of
giving every word the common tag NN . The sec-
ond baseline was established by directly applying
the C&C tagger, trained on the Penn Treebank, to
the PPCME data. The results are given in lines (a)
and (b) of Figure 2 for the first and second baselines,
respectively. As can be seen, the use of the Mod-
ern English tagger already provides a strong starting
point for both evaluation sets.
6Composition dates and manuscript dates for M3 are 1350-
1420. The composition dates for M34 are the same but the
manuscripts date 1420-1500
4.2 Bigram taggers
In section 3.1, we discuss three versions of the
Wycliffe target text labeled with tags projected
across alignments from the NET Bible. The
most straightforward of these were DICE 1TO1 and
GIZA 1TON which directly use the alignments from
the methods. Training a bigram tagger on these
two sources leads to a large improvement over the
C&C baseline on the PPCME Wycliffe sentences,
as can be seen by comparing line (c) to line (b)
in Figure 2. However, performance drops on the
PPCME Test sentences, which come from different
domains than the bigram tagger?s automatically pro-
duced Wycliffe training material. This difference is
likely to do good estimates of P (wi|ti), but poor es-
timates of P (ti|ti?1) due to the noise introduced in
GIZA 1TON.
More conservative tags projection is thus likely
to have a large effect on the out-of-domain perfor-
mance of the learned taggers. To test this, we trained
a bigram tagger on DICE 1TO1 and the more con-
servative GIZA 1TO1 projection. This produces fur-
ther gains for the PPCME Wycliffe, and enormous
improvements on the PPCME Test data (see line (d)
of Figure 2). This result confirms that conservativity
beats wild guessing (at the risk of reduced coverage)
for bootstrapping taggers in this way. This is very
much in line with the methodology of Yarowksy and
Ngai (2001), who project a small number of tags out
of all those predicted by alignment. They achieve
this restriction by directly adjusting the probabality
mass assigned to projected tags; we do it by using
two versions of the target text with tags projected in
395
two different 1-to-1 ways.
4.3 Bootstrapping the C&C tagger
As described in section 3.3, a bigram tagger trained
on DICE 1TO1 and GIZA 1TO1 (i.e., the tagger of
line (d)), was used to relabel the entire Wycliffe tar-
get text to produce training material for C&C, which
we call BOOTSTRAP. The intention is to see whether
the more powerful tagger can bootstrap off imper-
fect tags and take advantage of its richer features to
produce a more accurate tagger. As can be seen in
row (e) of Figure 2, it provides a 3-4% gain across
the board over the bigram tagger which produced its
training material (row (d)).
We also considered whether using all available
(non-PPCME) training material would improve tag-
ging accuracy by training C&C on BOOTSTRAP,
the Modern English Wall Street Journal (from the
Penn Treebank), and the automatically tagged NET
text7 It did produce slight gains on both test sets
over C&C trained on BOOTSTRAP alone. This is
likely due to picking up some words that survived
unchanged to the Modern English. Of course, the
utility of modern material used directly in this man-
ner will likely vary a great deal depending on the
distance between the two language variants. What is
perhaps most interesting is that adding the modern
material did not hurt performance.
4.4 Upperbounds
It is apparent from the results that there is a strong
domain effect on the performance of both the bigram
and C&C taggers which have been trained on auto-
matically projected tags. There is thus a question of
how well we could ever hope to perform on PPCME
Test given perfect tags from the Wycliffe texts. To
test this, C&C was trained on the PPCME version of
Wycliffe, which has human annotated standard tags,
and then applied on the PPCME test set. We also
compare this to training on PPCME texts which are
similar to those in PPCME Test.
The results, given in lines (g) and (h) of Figure
2, indicate that there is a likely performance cap on
non-Biblical texts when bootstrapping from parallel
Biblical texts. The results in line (h) also show that
the non-Biblical texts are more difficult, even with
7This essentially is partial self-training since C&C trained
on WSJ was used to produce the NET tags.
gold training material. This is likely due to the wide
variety of authors and genres contained in these texts
? in a sense, everything is slightly out-of-domain.
4.5 Learning curves with manual annotation
The upperbounds raise two questions. One is
whether the performance gap between (g) and (h) in
Figure 2 on PPCME Test is influenced by the signif-
icant difference in the size of their training sets. The
other is how much gold-standard PPCME training
material would be needed to match the performance
of our best bootstrapped tagger (line (f)). This is a
natural question to ask, as it hits at the heart of the
utility of our essentially unsupervised approach ver-
sus annotating target texts manually.
To examine the cost of manually annotating the
target language as compared to our unsupervised
method, the C&C tagger was also trained on ran-
domly selected sets of sentences from PPCME (dis-
joint from PPCME Test). Accuracy was measured
on PPCME Wycliffe and Test for a range of training
set sizes, sampled at exponentially increasing values
(25, 50, 100, . . . , 12800). Though we trained on and
predicted the full tagset used by the PPCME, it was
evaluated on PTB to give an accurate comparison.8
The learning curves on both test sets are shown
in Figure 3. The accuracy of the C&C tagger in-
creases rapidly, and the accuracy exceeds our auto-
mated method on PPCME Test with just 50 labeled
sentences and on the PPCME Wycliffe with 400 ex-
amples. This shows the domain of the target text is
served much better with the projection approach.
To see how much gold-standard PPCME Wycliffe
material is necessary to beat our best bootstrapped
tagger, we trained the tagger as in (g) of Figure 2
with varying amounts of material. Roughly 600 la-
beled sentences were required to beat the perfor-
mance of 61.9%/68.5% (line (f), on both metrics).
These learning curves suggest that when the do-
main for which one wishes to produce a tagger is
significantly different from the aligned text one has
available (in this and in many cases, the Bible), then
labeling a small number of examples by hand is a
quite reasonable approach (provided random sam-
pling is used). However, if one is not careful, con-
siderable effort could be put into labeling sentences
8Evaluation with the full PPCME set produces accuracy fig-
ures about 1% lower.
396
 60
 65
 70
 75
 80
 85
 90
 95
 100
 0  2000  4000  6000  8000  10000  12000  14000
Ac
cu
ra
cy
Number of sentences
PPCME Wycliffe
PPCME Test
Figure 3: Learning curve showing the accuracy for
PTB tags of the C&C tagger on both Bible and Test
as it is given more gold-standard PPCME training
sentences.
that are not optimal overall (imagine getting unlucky
and starting out by manually annotating primarily
Wycliffe sentences). The automated methods we
present here start producing good taggers immedi-
ately, and there is much room for improving them
further. Additionally, they could be used to aid man-
ual annotation by proposing high-confidence labels
even before any annotation has begun.
5 Related work
Despite the fact that the Bible has been translated
into many languages and that it constitutes a solid
source for studies in NLP with a concentration on
machine translation or parallel text processing, the
number of studies involving the Bible is fairly lim-
ited. A near exhaustive list is Chew et al(2006),
Melamed(1998), Resnik et al(1999), and Yarowsky
et al(2001).
Yarowsky and Ngai (2001) is of central rele-
vance to this study. The study describes an unsu-
pervised method for inducing a monolingual POS
tagger, base noun-phrase bracketer, named-entity
tagger and morphological analyzers from training
based on parallel texts, among many of which the
Bible was included. This is particularly useful given
that no manually annotated data is necessary in the
target language and that it works for two languages
from different families such as French and Chinese.
In the case of POS tagging, only the results for
English-French are given and an accuracy of 96% is
achieved. Even though this accuracy figure is based
on a reduced tag set smaller than the COARSE used
in this study, it is still a significant increase over that
achieved here. However, their method had the ad-
vantage of working in a domain that overlaps with
the training data for their POS tagger. Second, the
the French tag set utilized in that study is consider-
ably smaller than the Penn Helsinki tag set, a possi-
ble source of greater noise due to its size.
Dra?bek and Yarowsky (2005) create a fine-
grained tagger for Czech and French by enriching
the tagset for parallel English text with additional
morphological information, which, though not di-
rectly attested by the impoverished English morpho-
logical system (e.g. number on adjectives), typically
does appear in other languages.
6 Conclusion
The purpose of the study was to implement a POS
tagger for diachronic texts of maximal accuracy with
minimal cost in terms of labor, regardless of the
shortcuts taken. Such taggers are the building blocks
in the design of higher level tools which depend
on POS data such as morphological analyzers and
parsers, all of which are certain to contribute to di-
achronic language studies and genetic studies of lan-
guage change.
We showed that using two conservative methods
for projecting tags through alignment significantly
improves bigram POS tagging accuracies over a
baseline of applying a Modern English tagger to
Middle English text. Results were improved further
by training a more powerful maximum entropy tag-
ger on the predictions of the bootstrapped bigram
tagger, and we observed a further, small boost by
using Modern English tagged material in addition to
the projected tags when training the maximum en-
tropy tagger.
Nonetheless, our results show that there is still
much room for improvement. A manually annotated
training set of 400?800 sentences surpassed our best
bootstrapped tagger. However, it should be noted
that the learning curve approach was based on do-
main neutral, fully randomized, incremental texts,
which are not easily replicated in real world appli-
cations. The domain effect is particularly evident in
397
training on the sample Wycliffe and tagging on the
test PPCME set. Of course, our approach can be in-
tegrated with one based on annotation by using our
bootstrapped taggers to perform semi-automated an-
notation, even before the first human-annotated tag
has been labeled.
It is not certain how our method would fare on the
far more numerous parallel diachronic texts which
do not come prealigned. It is also questionable
whether it would still be robust on texts predating
Middle English, which might as well be written in
a foreign language when compared to Modern En-
glish. These are all limitations that need to be ex-
plored in the future.
Immediate improvements can be sought for the al-
gorithms themselves. By restricting the mapping of
words to only one POS tag in the Wycliffe Bible,
this seriously handicapped the utility of a bigram
tagger. It should be relatively straightforward to
transfer the probability mass of multiple POS tags
in a modern text to corresponding words in a di-
achronic text and include this modified probability
in the bigram tagger. When further augmented for
automatic parameter adjustment with the forward-
backward algorithm, accuracy rates might increase
further. Furthermore, different algorithms might be
better able to take advantage of similarities in or-
thography and syntactic structure when constructing
word alignment tables. Minimum Edit Distance al-
gorithms seem particularly promising in this regard.
Finally, it is evident that the utility of the Bible
as a potential resource of parallel texts has largely
gone untapped in NLP research. Considering that
it has probably been translated into more languages
than any other single text, and that this richness
of parallelism holds not only for synchrony but di-
achrony, its usefulness would apply not only to the
most immediate concern of building language tools
for many of the the world?s underdocumented lan-
guages, but also to cross-linguistic studies of un-
precedented scope at the level of language genera.
This study shows that despite the fact that any two
Bibles are rarely in a direct parallel relation, stan-
dard NLP methods can be applied with success.
References
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: a case study
in part-of-speech tagging. Computational Linguistics,
21(4):543?565.
Helena Britto, Marcelo Finger, and Charlotte Galves,
2002. Computational and linguistic aspects of the
construction of The Tycho Brahe Parsed Corpus of
Historical Portuguese. Tu?bingen: Narr.
Peter A. Chew, Steve J. Verzi, Travis L. Bauer, and
Jonathan T. McClain. 2006. Evaluation of the bible
as a resource for cross-language information retrieval.
In Proceedings of the Workshop on Multilingual Lan-
guage Resources and Interoperability, Sydney, July
2006, pages 68?74.
James R Curran and Stephen Clark. 2003. Investigat-
ing gis and smoothing for maximum entropy taggers.
In Proceedings of the 11th Meeting of the European
Chapter of the Association for Computational Linguis-
tics (EACL-03).
Lee R. Dice. 1945. Measures of the amount of eco-
logic association between species. Journal of Ecology,
26:297?302.
Elliott Franco Dra?bek and David Yarowsky. 2005. In-
duction of fine-grained part-of-speech taggers via clas-
sifier combination and crosslingual projection. In Pro-
ceedings of the ACL Workshop on Building and Us-
ing Parallel Texts, pages 49?56, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Barbara A. Fennell. 2001. A History of English: A Soci-
olinguistic Approach. Blackwell, Oxford.
Chung-Hye Han, 2000. The Evolution of Do-Support In
English Imperatives, pages 275?295. Oxford Univer-
sity Press.
Martin Kay and Martin Ro?scheisen. 1993. Text-
translation alignment. Computational Linguistics,
19(1):121?142.
Anthony Kroch and Ann Taylor. 2000. Penn-helsinki
parsed corpus of middle english, second edition.
Anthony Kroch, Ann Taylor, and Donald Ringe. 2000.
The middle english verb-second constraint: A case
study in language contact and language change. Ams-
terdam Studies in the Theory and History of Linguistic
Science Series, 4:353?392.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313?330.
398
Dan I. Melamed. 1998. Manual annotation of transla-
tion equivalence: The blinker project. In Technical
Report 98-07, Institute for Research in Cognitive Sci-
ence, Philadelphia.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Sylwia Ozdowska. 2006. Projecting pos tags and syntac-
tic dependencies from english and french to polish in
aligned corpora. In EACL 2006 Workshop on Cross-
Language Knowledge Induction.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Eric Brill and Ken-
neth Church, editors, Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pages 133?142. Association for Computational
Linguistics, Somerset, New Jersey.
Philip Resnik, Mari Broman Olsen, and Mona Diab.
1999. The bible as a parallel corpus: Annotating the
?book of 2000 tongues?. Computers and the Humani-
ties, 33(1?2):129?153.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual pos taggers and np bracketers via robust pro-
jection across aligned corpora. In NAACL ?01: Sec-
ond meeting of the North American Chapter of the As-
sociation for Computational Linguistics on Language
technologies 2001, pages 1?8, Morristown, NJ, USA.
Association for Computational Linguistics.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001. Inducing multilingual text analysis tools via
robust projection across aligned corpora. In HLT
?01: Proceedings of the first international conference
on Human language technology research, pages 1?8,
Morristown, NJ, USA. Association for Computational
Linguistics.
Appendix
Figure 4 provides the full mapping from PPCME
tags to the Penn Treebank Tags used in our evalu-
ation.
PPCME?PTB PPCME?PTB
ADJR?JJR N?NN
ADJS?JJS N$?NN
ADV?RB NEG?RB
ADVR?RBR NPR?NNP
ADVS?RBS NPR$?NNP
ALSO?RB NPRS?NNPS
BAG?VBG NPRS$?NNPS
BE?VB NS?NNS
BED?VBD NS$?NNS
BEI?VB NUM?CD
BEN?VBN NUM$?CD
BEP?VBZ ONE?PRP
C?IN ONE$?PRP$
CODE?CODE OTHER?PRP
CONJ?CC OTHER$?PRP
D?DT OTHERS?PRP
DAG?VBG OTHERS$?PRP
DAN?VBN P?IN
DO?VB PRO?PRP
DOD?VBD PRO$?PRP$
DOI?VB Q?JJ
DON?VBN Q$?JJ
DOP?VBP QR?RBR
E S?E S QS?RBS
ELSE?RB RP?RB
EX?EX SUCH?RB
FOR?IN TO?TO
FOR+TO?IN VAG?VBG
FP?CC VAN?VBN
FW?FW VB?VB
HAG?VBG VBD?VBD
HAN?VBN VBI?VB
HV?VB VBN?VBN
HVD?VBD VBP?VBP
HVI?VB WADV?WRB
HVN?VBN WARD?WARD
HVP?VBP WD?WDT
ID?ID WPRO?WP
INTJ?UH WPRO$?WP$
MAN?PRP WQ?IN
MD?MD X?X
MD0?MD
Figure 4: Table of mappings from PPCME tags to
Penn Treebank Tags.
399
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 668?677,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Unsupervised morphological segmentation and clustering with document
boundaries
Taesun Moon, Katrin Erk, and Jason Baldridge
Department of Linguistics
University of Texas at Austin
1 University Station B5100
Austin, TX 78712-0198 USA
{tsmoon,katrin.erk,jbaldrid}@mail.utexas.edu
Abstract
Many approaches to unsupervised mor-
phology acquisition incorporate the fre-
quency of character sequences with re-
spect to each other to identify word stems
and affixes. This typically involves heuris-
tic search procedures and calibrating mul-
tiple arbitrary thresholds. We present a
simple approach that uses no thresholds
other than those involved in standard ap-
plication of ?2 significance testing. A
key part of our approach is using docu-
ment boundaries to constrain generation of
candidate stems and affixes and clustering
morphological variants of a given word
stem. We evaluate our model on English
and the Mayan language Uspanteko; it
compares favorably to two benchmark sys-
tems which use considerably more com-
plex strategies and rely more on experi-
mentally chosen threshold values.
1 Introduction
Unsupervised morphology acquisition attempts to
learn from raw corpora one or more of the follow-
ing about the written morphology of a language:
(1) the segmentation of the set of word types in a
corpus (Creutz and Lagus, 2007), (2) the cluster-
ing of word types in a corpus based on some notion
of morphological relatedness (Schone and Juraf-
sky, 2000), (3) the generation of out-of-vocabulary
items which are morphologically related to other
word types in the corpus (Yarowsky et al, 2001).
We take a novel approach to segmenting words
and clustering morphologically related words.
The approach uses no parameters that need to
be tuned on data. The two main ideas of the
approach are (a) the filtering of affixes by sig-
nificant co-occurrence, and (b) the integration of
knowledge of document boundaries when gener-
ating candidate stems and affixes and when clus-
tering morphologically related words. The main
application that we envision for our approach is
to produce interlinearized glossed texts for under-
resourced/endangered languages (Palmer et al,
2009). Thus, we strive to eliminate hand-tuned
parameters to enable documentary linguists to use
our model as a preprocessing step for their manual
analysis of stems and affixes. To require a docu-
mentary linguist?who is likely to have little to no
knowledge of NLP methods?to tune parameters is
unfeasible. Additionally, data-driven exploration
of parameter settings is unlikely to be reliable in
language documentation since datasets typically
are quite small. To be relevant in this context, a
model needs to produce useful results out of the
box.
Constraining learning by using document
boundaries has been used quite effectively in un-
supervised word sense disambiguation (Yarowsky,
1995). Many applications in information retrieval
are built on the statistical correlation between doc-
uments and terms. However, we are unaware of
cases where knowledge of document boundaries
has been used for unsupervised learning for mor-
phology. The intuition behind our approach is very
simple: if two words in a single document are
very similar in terms of orthography, then the two
words are likely to be related morphologically. We
measure how integrating these assumptions into
our model at different stages affects performance.
We define a simple pipeline model. After gen-
erating candidate stems and affixes (possibly con-
strained by document boundaries), a ?2 test based
on global corpus counts filters out unlikely affixes.
Mutually consistent affix pairs are then clustered
to form affix groups. These in turn are used to
build morphologically related word clusters, pos-
sibly constrained by evidence from co-occurence
of word forms in documents. Following Schone
and Jurafsky (2000), clusters are evaluated for
668
whether they capture inflectional paradigms using
CELEX (Baayen et al, 1993).
We are unaware of other work on morphology
using ?2 tests despite its wide application across
many disciplines.1 This may be due to the large
degree of noise found in the candidate affix sets
induced through other candidate generation meth-
ods. The ?2 test has two standard thresholds?a
significance threshold and a lower bound on ob-
served counts. These are the only manually set
parameters we require?and we in fact use the
widely accepted standard values for these thresh-
olds without varying them in our experiments.
This is a significant improvement over other ap-
proaches that typically require a number of arbi-
trary thresholds and parameters yet provide little
intuitive justification for them. (We give examples
of these in ?3.)
We evaluate our approach on two languages,
English and Uspanteko, and compare its per-
formance to two benchmark systems, Morfessor
(Creutz and Lagus, 2007) and Linguistica (Gold-
smith, 2001). English is commonly used in other
studies and permits the use of CELEX as a gold
standard for evaluation. Uspanteko is an endan-
gered Mayan language for which we have a set of
interlinearized glossed texts (IGT) (Pixabaj et al,
2007; Palmer et al, 2009). IGT provides word-
by-word morpheme segmenation, which we use
to create a synthetic gold standard. In addition
to evaluation against this standard, Telma Kaan
Pixabaj?a Mayan linguist who helped create the
annotated corpus?reviewed by hand 100 word
clusters produced by our system, Morfessor and
Linguistica. Note that because English is suffixal
and Uspanteko is both prefixal and suffixal, we use
a slightly modified model for Uspanteko.
The approach introduced in this paper compares
favorably to Linguistica and Morfessor, two mod-
els that employ much more complex strategies and
rely on experimentally-tuned language/corpus-
specific parameters. In our evaluation, document
boundary awareness greatly benefits precision for
small datasets, blocking acquisition of spurious af-
fixes. For large datasets, global candidate genera-
tion outperforms document-aware candidate gen-
eration at the task of filtering out spurious stems,
but document-aware clustering improves preci-
sion. These findings are promising for the applica-
tion of this approach to under-resourced languages
1Monson (2004) suggests, but does not actually use, ?2.
like Uspanteko.
2 Unsupervised morphology acquisition
Unsupervised morphology acquisition aims to
model one or more of three properties of writ-
ten morphology: segmentation, clustering around
a common stem, and generation of new word
forms with productive affixes. Intuitively, there are
straightforward, but non-trivial, challenges that
arise when evaluating a model. One large chal-
lenge is distinguishing derivational from inflec-
tional morphology. Most approaches deal with to-
kens without considering context. Since inflec-
tional morphology is virtually always driven by
syntax and word context, such approaches are un-
able to learn only inflectional morphology or only
derivational morphology. Even approaches which
take context into consideration (Schone and Juraf-
sky, 2000; Baroni et al, 2002; Freitag, 2005) can-
not learn specifically for one or the other.
In addition, the evaluation of both segmentation
and clustering involves arbitrary judgment calls.
Concerning segmentation, should altimeter and
altitude be one morpheme or two? (The sam-
ple English gold standard for MorphoChallenge
2009 provides alti+meter but altitude.) Similar is-
sues arise when evaluating clusters of related word
forms if inflection and derivation are not distin-
guished. Does atheism belong to the same cluster
as theism? Where is the frequency cutoff point be-
tween a productive derivational morpheme and an
unproductive one? Yet, many studies have eval-
uated their segmentations and clusters by going
over their results word by word, cluster by cluster
and judging by sight whether some segmentation
or clustering is good (e.g., Goldsmith (2001)).
Like Schone and Jurafsky (2001), we build clus-
ters that will have both inflectionally and deriva-
tionally related stems and evaluate them with re-
spect to a gold standard of only inflectionally re-
lated stems.
3 Related work
There is a diverse body of existing work on unsu-
pervised morphology acquisition. We summarize
previous work, emphasizing some of its more ar-
bitrary and ad hoc aspects.
Letter successor variety. Letter successor va-
riety (LSV) models (Hafer and Weiss, 1974;
Gaussier, 1999; Bernhard, 2005; Bordag, 2005;
669
Keshava and Pitler, 2005; Hammarstro?m, 2006;
Dasgupta and Ng, 2007; Demberg, 2007) use the
hypothesis that there is less certainty when pre-
dicting the next character at morpheme bound-
aries. LSV has several issues that require fine pa-
rameter tuning. For example, Hafer and Weiss
(1974) counts how many types of characters ap-
pear after some initial string (the successor count)
and how many types of characters appear before
some final string (the predecessor count). A suc-
cessful criterion for segmenting a word was if the
predecessor count for the second part was greater
than 17 and the successor count for the first part
was greater than 5. Other studies have similar data
specific parameters and restrictions.
MDL and Bayesian models. Minimum descrip-
tion length (MDL) models (Goldsmith, 2001;
Creutz and Lagus, 2002; Creutz and Lagus, 2004;
Goldsmith, 2006; Creutz and Lagus, 2007) try to
segment words by maximizing the probability of
a training corpus subject to a penalty based on
the size of hypothesized morpheme lexicons they
build on the basis of the segmentations. While the-
oretically elegant, a pure implementation on real
data results in descriptions that do not reflect ac-
tual morphology. Creutz and Lagus (2005) re-
port that, ?frequent word forms remain unsplit,
whereas rare word forms are excessively split.? In
the end, every MDL approach uses probabilisti-
cally motivated refinements that restrict the ten-
dency of raw MDL to generate descriptions that
do not fit linguistic notions of morphology. De-
spite the sophistication of the models in this group,
there are many parameters that need to be set, and
heuristic search procedures are crucial for their
success (Goldwater, 2007). Snover et al (2002)
present a Bayesian model that uses a prior distribu-
tion to refine disjoint clusters of morphologically
related words. It disposes with parameter setting
by selecting the highest ranking hypothesis.
Context aware approaches. A word?s mor-
phology is strongly influenced by its syntactic and
semantic context. Schone and Jurafsky (2000) at-
tempts to cluster morphologically related words
starting with an unrefined trie search (but with a
parameter of minimum possible stem length and
an upper bound on potential affix candidates) that
is constrained by semantic similarity in a word
context vector space. Schone and Jurafsky (2001)
builds on this approach, but adds more ad hoc
parameters to handle circumfixation. Baroni et
al. (2002) takes a similar approach but uses edit
distance to cluster words that are similar but do
not necessarily share a long, contiguous substring.
They remove noise by constraining cluster mem-
bership with mutual information derived semantic
similarity. Freitag (2005) uses a mutual informa-
tion derived measure to learn the syntactic simi-
larity between words and clusters them. Then he
derives finite state machines across words in dif-
ferent clusters and refines them through a graph
walk algorithm. This group is the only one to eval-
uate against CELEX (Schone and Jurafsky, 2000;
Schone and Jurafsky, 2001; Freitag, 2005).
Others. Some other models require input such
as POS tables and lexicons and use a wider range
of information about the corpus (Yarowsky and
Wicentowski, 2000; Yarowsky et al, 2001; Chan,
2006). Because of the knowledge dependence of
these models, they are able to properly induce
inflectional morphology, as opposed to the stud-
ies cited above. Snyder and Barzilay (2008) uses
a set of aligned phrases across related languages
to learn how to segment words with a Bayesian
model and is otherwise fully unsupervised.
4 Model2
Our goal is to generate conflation sets: sets of
word types that are related through either inflec-
tional or derivational morphology (Schone and Ju-
rafsky, 2000). Solving this task requires learning
how individual types are segmented (though the
segmentation itself is not evaluated). For present
purposes, we assume that the affixal pattern of the
language is known: whether it is prefixal, suffixal,
or both. To simplify presentation, we discuss a
model that captures suffixes only. Our approach is
a four stage process:
1. Candidate Generation: generate candidate
stems and affixes using an orthographically
defined data structure (a trie)
2. Candidate Filtering: filter candidate affixes
using the statistical significance for pairs of
affixes based on their co-occurence counts
with shared stems
3. Affix Clustering: cluster significant affix pairs
into affix groups
2The code implementing the model is available from
http://comp.ling.utexas.edu/earl
670
4. Word Clustering: form conflation sets based
on affix clusters
The first and last stages are particularly prone to
noise, which has necessitated many of the thresh-
olds and heuristics employed in previous work.
We hypothesize that naturally occuring document
boundaries provide a strong constraint that should
reduce this noise, and we test that hypothesis by
using it in those stages.
Our intuition comes from an observation by
Yarowsky (1995) regarding multiple tokens of
words in documents. He tabulates the applicabil-
ity of using document boundaries to disambiguate
word senses, which measures how often a given
word occurs more than twice in the same docu-
ment. For ten potentially ambiguous words, he
counts how often they occur more than once in
some document and finds that if the words do oc-
cur, they do so multiple times in 50.1% of these
documents, on average. His counts ignored mor-
phological variation, and it is likely the applica-
bility measure would have increased considerably:
if a content word is used more than once in some
text, it is likely to be repeated in different syntactic
contexts, requiring the word to be inflected or to be
derived for a different part-of-speech category. 3
For stage one, we build separate tries for each
document rather than a trie for the entire corpus.
This should reduce the chance that orthographi-
cally similar but morphologically unrelated word
pairs lead to bad candidates by reducing the search
space for words which share a stem to a local doc-
ument. For example, assuage and assume are both
likely to occur in a large corpus and suggest that
there is a stem assu with affixes -age and -me.
They are less likely to occur together in many dif-
ferent documents that form the corpus, whereas
assume, assumed, and assuming are. We refer to
this document constrained candidate generation as
CandGen-D, and to the unconstrained generation
(a single trie for all documents) as CandGen-G.
For stage four, documents are used to constrain
potential membership of words in clusters: all
pairs of words in a cluster must have occured to-
gether in some document. We refer to document-
constrained clustering as Clust-D and the uncon-
strained global clustering as Clust-G.
3For example, in just this one paragraph we have
{document,documents}, {measure, measures}, {occur, oc-
curs, occuring}, and {word, words}.
4.1 Candidate generation
Given a document or collection of documents, we
use tries (prefix trees) to identify potential stems
and affixes and collect statistics for co-occurrences
between affixes and between affixes and stems.
a
b c
d $
Figure 1
A trie G, like the example
on the right, can be iden-
tified with the set of all
words on paths from the
root to any leaf, in the case
of the example figure the
set G = {abd, ab$, ac}.
(We use $ to denote an
empty affix.) Given a trie
G over alphabet L, we de-
fine the set of trunks of G
as all paths from the root to a branching point:
Tr(G) = {w ? L+ |?a, b ? L, x
1
, x
2
? L
?
:
a 6= b ? wax
1
, wbx
2
? G}
Also, we define the set of branches of a trunk t ?
Tr(G) as the paths from its branching points to the
leaves:
Br(t,G) = {x ? L+ | tx ? G}
In our example, {a, ab} are the trunks, with
Br(a, G) = {bd, b$, c} and Br(ab, G) = {d, $}.
When we use a trie to induce stems and affixes,
all induced stems will be trunks, and all induced
affixes will be branches.
From a given trie, we induce a set of stem can-
didates and affix candidates. A simple criterion is
used: if a trunk is longer than all of its branches,
the trunk is a stem candidate and its branches are
affix candidates. So, the set of stem candidates for
a trie G, CStem(G), is the set of trunks t ? Tr(G)
such that |t| > |b| for all b ? Br(t,G).
Given a stem candidate s ? CStem(G), its set of
affix candidates CAff(s,G) is identical to its set of
branches. (To talk about the sets of stem and affix
candidates for a whole trie G or a set of tries, we
write CAff(G), StC(G), CAff, and CStem.) The
count of an affix candidate b ? CAff is the number
of stem candidates with which it occurs:
count(b) =
?
G
|{s ? CStem(G) | b ? CAff(s,G)}|
For Fig. 1, the set of stem candidates is {ab} (since
some branches of the trunk a are longer than the
671
trunk itself). The matching set of affix candidates
is CAff(ab, G) = {d, $}, each with a count of one.
An affix rule candidate is an unordered pair of
affix candidates {b
1
, b
2
}. It states that any stem
occurring with b
1
can also occur with b
2
. Affix
rules implement the assumption that all produc-
tive affixes will cooccur with other productive af-
fixes and that these will form a coherent group.
The rule candidates for a given stem candidate
s ? CStem(G) are:
CRule(s,G) =
{
{b
1
, b
2
} ? CAff(s,G) | b
1
6= b
2
}
For example, the single stem candidate ab in
Fig. 1 has one rule candidate, {d, $}. We also use
CRule(G) for the rule candidates of a trie G across
all stems, and CRule for the union of rule candi-
dates in a set of tries.
The count of a rule candidate r={b
1
, b
2
} in a
trie is the number of stem candidates it appears
with:
count(r) =
?
G
|{s ? CStem(G) | r ? CRule(s,G)}|
We also use CAff(s) for the set of affix candidates
of stem s across several tries, and CRule(s) for the
set of rule candidates of a stem s across several
tries.
Document-specific versus global candidate gen-
eration. CandGen-D defines separate tries for
every document in the corpus and induces stem,
affix and rule candidates for each document.
CandGen-G instead induces these candidates for
a global trie over all the words in the corpus.
From the perspective of the formalism laid out
above, the only difference is that CandGen-D
has as many tries G
i
as there are documents i
and CandGen-G has only one G. This simple
difference leads to different candidate sets and
counts over their occurrences. For example, say
two documents contain the pair putt/putts and
another contains bogey/bogeys. With CandGen-
D, count($)=3, count(s)=3, and count($, s)=2.
For the same documents, CandGen-G would pro-
duce count($)=2 and count(s)=2 since putt/putts
would have occurred only once in the global trie.
Also, consider a rare pair such as aard-
vark/aardvarks where each word is found in a dif-
ferent document. The pair would be identified
by CandGen-G but not by CandGen-D. The pair
would contribute a count of one to count($, s) in
CandGen-G but not in CandGen-D. So, CandGen-
G can provide better coverage, but it is also more
likely to identify noisy candidates, such as as-
suage/assumed, than CandGen-D.
4.2 Candidate filtering
The sets of candidates CStem,CAff,CRule is ex-
pected to be noisy since the only basis for gener-
ating them was strings that share a large portion of
their substrings. One way of filtering candidates is
to find affix candidates whose co-occurence with
other candidates is not statistically significant.
We measure correlation between candidate af-
fixes b
1
, b
2
in a candidate rule with the paired
?
2 test. By using ?2, we only consider pairwise
correlation between affixes, rather than attempting
global inference. Global consistency of affix sets
is not ensured, and as such the approach is sus-
ceptible to the multiple comparisons problem. We
still opt for this approach for its simplicity and be-
cause global inference is problematic due to data
sparseness.
Correlation between b
1
and b
2
is determined by
the following contingency table:4
b
1
? b
1
b
2
O
11
O
12
? b
2
O
21
O
22
Based on the significance testing, we define the set
of valid rules PairRule as those for which the ?2
test is significant at p < 0.05. Thus, affix can-
didates not significantly correlated with any other
affix in CAff are discarded.
4.3 Affix clustering
The previous stage produces a set of pairs of af-
fixes that are significantly correlated. However,
inflectional paradigms rarely contain just two af-
fixes, so we would like to group together affix
pairs into larger affix sets to improve generaliza-
tion. We use a bottom up, minimum distance clus-
tering for valid affix pairs (rules). We do not as-
sume that cluster membership is exclusive. For
example, it would not make sense to determine
that the null affix -$ can belong to only one cluster.
Therefore, we produce non-disjoint affix clusters.
A valid cluster of affixes is a maximal set of af-
fixes forming pairwise valid rules: Aff ? CAff is a
valid cluster of affixes iff
4where O
11
= count({b
1
, b
2
}), O
12
= count(b
2
) ?
O
11
, O
21
= count(b
1
)?O
11
, O
22
= N?O
11
?O
12
?O
21
and N =
P
b?CAff count(b). See table (1) for examples.
672
ed ?ed
ing 10273 21853
?ing 27120 4119332
(a) ?2 = 352678
le ?le
s 122 132945
?s 936 4044575
(b) ?2 = 239.132
ed ?ed
ing 2651 1310
?ing 1490 150848
(c) ?2 = 65101.6
le ?le
s 20 12073
?s 198 144008
(d) ?2 = 0.631, p = 0.427
Table 1: Affix counts in contingency tables for the valid pair ed/ing and spurious pair le/s according to
CandGen-D in (a) and (b) and according to CandGen-G in (c) and (d). ?2 test values are given under
each table. Data is from NYT. Total affix token counts induced through CandGen-D and CandGen-G
are N=4178578 and N=156299, respectively. A total of 2054 and 3739 affix types were induced for
CandGen-D and CandGen-G, respectively showing that CandGen-G does have better coverage though
it might have more noise.
1. ?b
1
, b
2
? Aff : {b
1
, b
2
} ? PairRule, and
2. If b ? CAff with ?b? ? Aff : {b, b?} ?
PairRule, then b ? Aff.
The set of all valid affix clusters is GroupRule.
This formulation does not rule out the existence
of clusters with affixes in common.
4.4 Word clustering
We next cluster word forms into morphologically
related groups. Our model assumes two word
forms to be morphologically related iff (1) they oc-
curred in the same trie G, (2) they have a trunk s in
common that is a stem in Stem(G), and (3) their af-
fixes under this stem s are members in a common
valid affix cluster in GroupRule. Hence a single
stem s can be involved in at most |GroupRule| con-
flation sets, one for each valid affix cluster. Again,
the only distinction between clustering with a
global trie (Clust-G) and clustering with several
tries from the documents in a corpus (Clust-D) is
that the former has only one trie.
We define the conflation set for a given stem s ?
Stem and valid affix cluster Aff ? GroupRule as
Wd(s,Aff) = {sb
1
, sb
2
| b
1
, b
2
? Aff ?
?G.s ? Stem(G) ? b
1
, b
2
? CAff(s,G)}
One issue that needs clarification is when the
candidate generation and clustering stages use dif-
ferent strategies, i.e. the models CandGen-D
+Clust-G and CandGen-G +Clust-D. This sim-
ply means that the statistics, and thus the valid
GroupRule, are derived from either CandGen-D or
CandGen-G.
4.5 Induction for languages that are both
prefixal and affixal
The above approach would not fit a language that
is prefixal and suffixal. Assuming we have in-
duced separate conflation sets over a prefix trie and
a suffix trie, we merge clusters between the two if
they have at least one word form in common. For-
mally, given a set of prefix conflation sets PCS and
a set of suffix conflation sets SCS, the final set of
conflation sets CS is:
CS = {p ? s |p ? PCS, s ? SCS ? p ? s 6= ?}
5 Data
We apply our method on English and Uspanteko,
an endangered Mayan language.
Learning corpora. For English, we use two
subsets of the NYTimes portion in the Gigaword
corpus which we will call NYT and MINI-NYT.
NYT in the current study is the complete collec-
tion of articles in the New York Times from June,
2002. NYT has 10K articles, 88K types and 9M
tokens. MINI-NYT is a subset of NYT with 190
articles, 15K types and 187K tokens.
The Uspanteko text, USP has 29 distinct texts,
7K types, and 50K tokens. The texts are from
OKMA (Pixabaj et al, 2007) and the segmenta-
tion and labels of the interlinear glossed text anno-
tations were checked for consistency and cleaned
up (Palmer et al, 2009). All counts are for lower-
cased, punctuation-removed word forms.
CELEX. The CELEX lexical database (Baayen
et al, 1993) has been built for Dutch, English and
German and provides detailed entries that list and
analyze the morphological properties of words,
among other information. Using CELEX, we eval-
uate on types rather than tokens. The performance
of the model is based on how many of the words it
judges to be morphologically related overlap with
the entries in CELEX. Following previous work
(Schone and Jurafsky, 2000; Schone and Jurafsky,
673
2001; Freitag, 2005), we evaluate on inflectional
clusters only, using the CELEX file listing clusters
of inflectional variants. 5
6 Experiments and evaluation
We outline our evaluation methodology, baselines,
benchmarks and results, and discuss the results.
6.1 Evaluation metric
Schone and Jurafsky (2000) give definitions for
correct (C), inserted (I), and deleted (D) words
in model-derived conflation sets in relation to a
gold standard. Their formulation does not allow
for multiple cluster membership of words. We ex-
tend the definition to incorporate this fact about the
data. Let w be a word form. We write X
w
for the
clusters induced by the model that contain w, and
Y
w
for gold standard clusters containing w. X
w
and Y
w
only count words which occurred in both
model and gold standard clusters. Then
C =
?
w
?
X
w
?
Y
w
(|X
w
? Y
w
|/|Y
w
|)
I =
?
w
?
X
w
?
Y
w
(|X
w
? (X
w
? Y
w
)|/|Y
w
|)
D =
?
w
?
X
w
?
Y
w
(|Y
w
? (X
w
? Y
w
)|/|Y
w
|)
Based on these definitions, we formulate preci-
sion (P ), recall (R), and the f -score (F ) as: P =
C/(C+I), R = C/(C+D), F = (2PR)/(P+R).
USP evaluation We use two different means to
evaluate the performance on USP. One is the
f -score derived from the above section with re-
spect to a standard that was automatically gen-
erated from the morpheme segment tiers of the
OKMA IGT. We generated the standard by taking
non-hyphenated segments as the stem and cluster-
ing words with shared stems.
We also had an expert in Uspanteko manually
evaluate a random subset (N = 100) of the model
output to compensate for any failings in the stan-
dard. The evaluator determined a dominant stem
for a cluster and identified words which were not
related to that stem. We measured accuracy and
5CELEX does have a second file listing words and their
breakup into constituent morphemes for both derivation and
inflection, but its use would have required additional process-
ing that could introduce errors.
0 10 20 30 40 50 60 70 80 90 100
40
50
60
70
80
90
100
precision
re
ca
ll
 
 
mini?NYT
NYT
Usp?S
Usp?P
Figure 2: Precision/recall graph for baseline ex-
periments on English, prefix USP (Usp-P) and suf-
fix USP (Usp-S).
full cluster accuracy6 for the expert evaluations
(table 4).
We experimented on Uspanteko with three dif-
ferent assumptions: (1) it is only prefixal; (2) it is
only suffixal; (3) it is both prefixal and suffixal.
We applied the assumptions of only prefixal or
only suffixal to LINGUISTICA as well. The rele-
vant results are given row headers in tables with a
corresponding +P(prefix) or +S(suffix).
6.2 Baselines and benchmarks
In a set of baselines, we put words which share
the first k characters into the same cluster. We
do this for NYT, MINI-NYT, and USP in a pre-
fix tree, and for USP in suffix tree (using the last k
characters). We set the values of 0 < k < max,
where max is the length of the longest string, and
plot the results in a precision-recall graph (Fig. 2).
Low k corresponds to high recall and low preci-
sion while high k shows the opposite. The contrast
in morphological patterns for each language can
also be seen. Because Uspanteko is morpholog-
ically complex with suffixes and prefixes, a very
simple strategy cannot achieve high recall as op-
posed to English where it is possible to retrieve all
variants with a simple prefix tree.
We use Linguistica (Goldsmith, 2001) and Mor-
fessor (Creutz and Lagus, 2007) as benchmarks.
We used the default settings for these programs.
Note that comparison with these tools is not com-
6Given a model cluster C
i
and the ?misses? for each clus-
ter M
i
, accuracy is measured as 1/N
P
i
(|C
i
|?|M
i
|)/(|C
i
|)
where N is the sample size. Full cluster accuracy is the num-
ber of clusters that did not have any misses over N .
674
MINI-NYT NYT
P R F P R F
LINGUISTICA 64.30 93.34 76.15 47.50 88.33 61.77
MORFESSOR 45.2 87.8 59.7 63.6 69.2 66.3
CandGen-D + Clust-G 69.41 91.42 78.91 46.00 79.81 58.36
CandGen-D + Clust-D 83.47 80.36 81.89 59.02 74.50 65.86
CandGen-G + Clust-G 73.44 88.72 80.36 61.81 82.98 70.85
CandGen-G + Clust-D 88.34 77.95 82.82 77.71 70.24 73.79
Table 2: Results on English for all models in precision(P), recall(R), f -score(F) for each data set.
pletely fair. Morfessor only generates segmenta-
tions. We therefore processed Morfessor output
by clustering words by assuming that the longest
segment in any segmentation is the stem and eval-
uated this instead. Linguistica produces stems and
associated suffixes so the clusters naturally follow
from this output. However, Linguistica only infers
either prefix or suffix patterns.
6.3 Results and discussion
The results on English are in table 2 with ?2 test
criteria of p<0.05 and each cell in the contingency
table >5. CandGen-G +Clust-D had the best f -
score, and easily beats the benchmarks.
This is different from our expectation that
awareness of document boundaries at all stages
(i.e., CandGen-D +Clust-D) would show the best
results. The discrepancy is especially marked for
the larger NYT. One important reason for this is
the affix criterion itself: trunks must be longer than
branches. Consider again the sample contingency
tables in Table 1 that were derived from NYT
through CandGen-D and CandGen-G. We had as-
sumed at the outset that CandGen-D would be bet-
ter able to filter out noise and would be sparser, but
results show the opposite. The reason is that that
short words in a global lexicon are more likely to
share trunks with longer, unrelated words. This
ensures that short word forms rarely generate can-
didate affixes. Longer words which are less likely
to have spurious long branches generate the bulk
of candidate suffixes and stems. This is born out
by the stems that were associated with the spuri-
ous suffix pair le/s: CandGen-G has cliente, cripp,
crumb, daniel, ender, label, mccord, nag, oval,
sear, stubb, whipp. CandGen-D has crumb, hand,
need, sing, tab, trick, trip. The word forms that
are associated with le/s through the CandGen-D
strategy are crumble/crumbs, handle/hands, . . . .
Compare this with the word forms associated with
the search strategy CandGen-G such as clien-
tele/clientes, cripple/crips, . . . . The majority of
them are not common English words; they are
most probably proper names such as LaBelle and
Searle. Furthermore, there is no item among the
stems from the CandGen-G search where concate-
nating the stems le and s would result in both word
forms being a common noun or verb as is the
case with the stems from the CandGen-D search
where all concatenated word forms are common
English words. Though CandGen-G finds spuri-
ous stems, the counts for the spurious affix pair are
suppressed (see table 1) because it is a type count
rather than a token count. This results in le/s be-
ing properly excluded as a rule. This explains why
CandGen-D has worse precision in general than
CandGen-G.
The affix criterion has other minor issues. One
is that it ignores the few cases where stems are
shorter than affixes, such as the very common
words be, do, go.7 Assuming that the longest
productive inflectional suffix in English is -ing8,
the criterion would correctly find stem candidates
for -ing only when the stem is longer than 3 or
4 letters. Another is that the criterion, when
combined with CandGen-D, generates candidates
from the/them/then/their/these which cooccur fre-
quently in documents. This is not an issue when
the criterion is applied in CandGen-G.
Nonetheless, results show that when data sizes
are small, as with USP (Table 3) and MINI-NYT,
awareness of document boundaries at the candi-
date generation stage is beneficial to precision.
7The exclusion of such words in a token based evaluation
as opposed to a type based evaluation would heavily penalize
our approach. We are not aware, however, of any prior work
in unsupervised morphology that evaluates over tokens.
8with occasional gemination of final consonant such as
occur ? occurring
675
P R F
Ca-D + Cl-D 70.51 44.35 54.45
Ca-G + Cl-G 70.00 46.87 56.15
Ca-D + Cl-D + S 88.58 45.21 59.86
Ca-D + Cl-G + S 85.03 44.75 58.64
Ca-G + Cl-D + S 90.34 45.48 60.50
Ca-G + Cl-G + S 84.54 46.03 59.60
Ca-D + Cl-D + P 93.84 47.90 63.42
Ca-D + Cl-G + P 89.94 47.38 62.06
Ca-G + Cl-D + P 95.42 47.89 63.78
Ca-G + Cl-G + P 92.03 50.01 64.80
LINGUISTICA + S 81.14 47.60 60.00
LINGUISTICA + P 84.15 52.00 64.28
MORFESSOR 28.12 62.28 38.75
Table 3: Performance of models on automatically
generated USP evaluation set. P: Prefix only, S:
Suffix only. If there is no indication of S or P, it
means model attempted to learn both
Acc. FAcc. Avg. Sz.
Ca-G + Cl-G 98.5 79.0 2.94
LINGUISTICA 96.0 85.0 2.64
MORFESSOR 85.3 55.0 4.8
Table 4: Human expert evaluated accuracy (Acc.)
and full cluster accuracy (FAcc.) of models on
USP and average cluster size in words (Avg. Sz.)
However, it seems that CandGen-G has better cov-
erage no matter the size of the corpus, which
explains why coupling it with Clust-D produces
overall better scores. Clust-D does provide a use-
ful added constraint to mere orthographic similar-
ity (i.e. shared trunks in a trie).
A worrisome aspect of the results is that perfor-
mance degrades for large data sets (this is also true
for Linguistica). However, it also hints that this
method might work well for under-resourced lan-
guages. We surmise that since productive suffixes
do not suffer from sparsity, even a small data set
provides sufficient evidence to reach reliable con-
clusions about the productive morphology of some
language. Increasing the size of the data merely
increases the counts of spurious affixes and poses
problems for a relative simple measure such as
the ?2 test. A similar result was shown in Creutz
and Lagus (2005) where f -score performance of
their segmentation method improved as more data
was provided then decreased as the input exceeded
250K tokens in English. Their method showed
continued improvement with increased data for
Finnish. This hints that more data is beneficial
for morphologically complex languages but not
for morphologically impoverished languages.
Finally, it is also encouraging that the manual
evaluation (Table 4) shows very high accuracy, as
judged by a documentary linguist. Both our model
and Linguistica perform very well under this eval-
uation.
7 Conclusion
We have presented a novel approach to unsuper-
vised morphology acquisition that uses a very
simple pipeline and does not use any thresholds
other than standard ones associated with the ?2
test. The model relies on document boundaries
and correlation tests for filtering spurious stems
and affixes. The model compares favorably to
Linguistica and Morfessor, two models that em-
ploy much more complex strategies and rely on
fine-tuned parameters. We found that the use of
document boundaries is especially beneficial with
small datasets, which is promising for the applica-
tion of this model to under-resourced languages.
For large datasets, global candidate generation
outperformed document-aware candidate genera-
tion at the task of filtering out spurious stems,
but document-aware clustering does improve pre-
cision and overall performance.
In this paper we have addressed one aspect of
morphology acquisition, segmentation and clus-
tering. Extending the approach is straightforward,
for example, substituting more sophisticated data
structures or statistical tests for the current ones.
In particular, we will move from the use of doc-
ument boundaries to a flexible notion of textual
distance to estimate likelihood of morphological
relatedness.
Acknowledgments
This work is funded by NSF grant BCS 06651988
?Reducing Annotation Effort in the Documenta-
tion of Languages using Machine Learning and
Active Learning.? Thanks to Alexis Palmer, Telma
Kaan Pixabaj, Elias Ponvert, and the anonymous
reviewers.
676
References
R. H. Baayen, R. Piepenbrock, and H. van Rijn. 1993.
The CELEX lexical database on CD-ROM. Linguis-
tic Data Consortium, Philadelphia, PA.
M. Baroni, J. Matiasek, and H. Trost. 2002. Unsu-
pervised discovery of morphologically related words
based on orthographic and semantic similarity. In
ACL ?02 workshop on Morphological and phonolog-
ical learning, pages 48?57.
D. Bernhard. 2005. Unsupervised morphological seg-
mentation based on segment predictability and word
segments alignment. In Proceedings of Morpho
Challenge 2005, pages 18?27.
S. Bordag. 2005. Two-step approach to unsupervised
morpheme segmentation. In Proceedings of Morpho
Challenge 2005, pages 23?27.
E. Chan. 2006. Learning Probabilistic Paradigms for
Morphology in a Latent Class Model. In ACL SIG-
PHON ?06, pages 69?78.
M. Creutz and K. Lagus. 2002. Unsupervised dis-
covery of morphemes. In ACL ?02 workshop on
Morphological and phonological learning-Volume
6, pages 21?30.
M. Creutz and K. Lagus. 2004. Induction of a simple
morphology for highly-inflecting languages. In ACL
SIGPHON ?04, pages 43?51.
M. Creutz and K. Lagus. 2005. Inducing the morpho-
logical lexicon of a natural language from unanno-
tated text. In AKRR ?05, pages 106?113.
M. Creutz and K. Lagus. 2007. Unsupervised models
for morpheme segmentation and morphology learn-
ing. ACM Trans. Speech Lang. Process., 4(1):3.
S. Dasgupta and V. Ng. 2007. High-performance,
language-independent morphological segmentation.
In NAACL-HLT, pages 155?163.
V. Demberg. 2007. A language-independent unsu-
pervised model for morphological segmentation. In
ACL ?07, volume 45, page 920.
D. Freitag. 2005. Morphology induction from term
clusters. In CoNLL ?05.
E. Gaussier. 1999. Unsupervised learning of deriva-
tional morphology from inflectional lexicons. In
ACL workshop on Unsupervised Methods in Natu-
ral Language Learning.
J. Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Comp. Ling.,
27(2):153?198.
J. Goldsmith. 2006. An algorithm for the unsupervised
learning of morphology. Natural Language Engi-
neering, 12(04):353?371.
S.J. Goldwater. 2007. Nonparametric Bayesian mod-
els of lexical acquisition. Ph.D. thesis, Brown Uni-
versity.
M.A. Hafer and S.F. Weiss. 1974. Word Segmentation
by Letter Successor Varieties. Information Storage
and Retrieval, 10:371?385.
H. Hammarstro?m. 2006. A naive theory of affixation
and an algorithm for extraction. In ACL SIGPHON
?06, pages 79?88, June.
S. Keshava and E. Pitler. 2005. A simpler, intuitive
approach to morpheme induction. In Proceedings of
Morpho Challenge 2005, pages 28?32.
C. Monson. 2004. A framework for unsupervised nat-
ural language morphology induction. In Proceed-
ings of the Student Workshop at ACL, volume 4.
Alexis Palmer, Taesun Moon, and Jason Baldridge.
2009. Evaluating automation strategies in language
documentation. In Proceedings of the NAACL HLT
2009 Workshop on Active Learning for Natural Lan-
guage Processing, pages 36?44, Boulder, CO.
T.C. Pixabaj, M.A. Vicente Me?ndez, M. Vicente
Me?ndez, and O.A. Damia?n. 2007. Text collections
in Four Mayan Languages. Archived in The Archive
of the Indigenous Languages of Latin America.
P. Schone and D. Jurafsky. 2000. Knowledge-free in-
duction of morphology using latent sematic analysis.
In CoNLL-2000 and LLL-2000.
P. Schone and D. Jurafsky. 2001. Knowledge-free
induction of inflectional morphologies. In NAACL
?01, pages 1?9.
M.G. Snover, G.E. Jarosz, and M.R. Brent. 2002. Un-
supervised learning of morphology using a novel di-
rected search algorithm: taking the first step. In ACL
?02 workshop on Morphological and phonological
learning, pages 11?20.
B. Snyder and R. Barzilay. 2008. Unsupervised multi-
lingual learning for morphological segmentation. In
ACL ?08.
D. Yarowsky and R. Wicentowski. 2000. Minimally
supervised morphological analysis by multimodal
alignment. In ACL ?00, pages 207?216.
D. Yarowsky, G. Ngai, and R. Wicentowski. 2001.
Inducing multilingual text analysis tools via robust
projection across aligned corpora. In HLT ?01.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In ACL ?95,
pages 189?196.
677
Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 36?44,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Evaluating Automation Strategies in Language Documentation
Alexis Palmer, Taesun Moon, and Jason Baldridge
Department of Linguistics
The University of Texas at Austin
Austin, TX 78712
{alexispalmer,tsmoon,jbaldrid}@mail.utexas.edu
Abstract
This paper presents pilot work integrating ma-
chine labeling and active learning with human
annotation of data for the language documen-
tation task of creating interlinearized gloss
text (IGT) for the Mayan language Uspanteko.
The practical goal is to produce a totally an-
notated corpus that is as accurate as possible
given limited time for manual annotation. We
describe ongoing pilot studies which examine
the influence of three main factors on reduc-
ing the time spent to annotate IGT: sugges-
tions from a machine labeler, sample selection
methods, and annotator expertise.
1 Introduction
Languages are dying at the rate of two each month.
By the end of this century, half of the approxi-
mately 6000 extant spoken languages will cease to
be transmitted effectively from one generation of
speakers to the next (Crystal, 2000). Under this
immense time pressure, documentary linguists seek
to preserve a record of endangered languages while
there are still communities of speakers to work with.
Many language documentation projects target lan-
guages about which our general linguistic knowl-
edge is nonexistent or much less than for more
widely-spoken languages. The vast majority of these
are individual or small-group endeavors on small
budgets with little or no institutional guidance by
the greater documentary linguistic community. The
focus in such projects is often first on collection of
data (documentation), with a following stage of lin-
guistic analysis and description. A key part of the
analysis process, detailed linguistic annotation of the
recorded texts, is a time-consuming and tedious task
usually occurring late in the project, if it occurs at
all.
Text annotation typically involves producing in-
terlinearized glossed text (IGT), labeling for mor-
phology, parts-of-speech, etc., which greatly facil-
itates further exploration and analysis of the lan-
guage. The following is IGT for the phrase xelch
li from the Mayan language Uspanteko:1
(1) x-
COM-
el
salir
-ch
-DIR
li
DEM
Spanish: ?Salio entonces.? English:?Then he left.?
The levels of analysis include morpheme segmenta-
tion, transliteration of stems, and labeling of stems
and morphemes with tags, some corresponding to
parts-of-speech and others to semantic distinctions.
There is no single standard format for IGT. The
IGT systems developed by documentation projects
tend to be idiosyncratic: they may be linguistically
well-motivated and intuitive, but they are unlikely to
be compatible or interchangeable with systems de-
veloped by other projects. They may lack internal
consistency as well. Nonetheless, IGT in a read-
ily accessible format is an important resource that
can be used fruitfully by linguists to examine hy-
potheses on novel data (e.g. Xia and Lewis (2007;
2008), Lewis and Xia (2008)). Furthermore, it can
be used by educators and language activists to create
curriculum material for mother language education
and promote the survival of the language.
Despite the urgent need for such resources, IGT
annotations are time consuming to create entirely by
hand, and both human and financial resources are
extremely limited in this domain. Thus, language
1KEY: COM=completive aspect, DEM=demonstrative,
DIR=directional
36
documentation presents an interesting test case and
an ideal context for use of machine labeling and ac-
tive learning. This paper describes a series of ex-
periments designed to assess this promise in a re-
alistic documentation context: creation of IGT for
the Mayan language Uspanteko. We systematically
compare varying degrees of machine involvement in
the development of IGT, from minimally involved
situations where examples for tagging are selected
sequentially to active learning situations where the
machine learner selects samples for human tagging
and suggests labels. We also discuss the challenges
faced by linguists in having to learn, transcribe, ana-
lyze, and annotate a language almost simultaneously
and discuss whether machine involvement reduces
or compounds those challenges.
In the experiments, two documentary linguists an-
notate IGT for Uspanteko texts using different lev-
els of support from a machine learned classifier. We
consider the interaction of three main conditions: (1)
sequential, random, or uncertainty sampling for re-
questing labels from an annotator, (2) suggestions or
no suggestions from a machine labeler, and (3) ex-
pert versus non-expert annotator. All annotator deci-
sions are timed, enabling the actual time cost of an-
notation to be measured within the context of each
condition. This paper describes the Uspanteko data
set we adapted for the experiments, expands on the
choices described above, and reports on preliminary
results from our ongoing annotation experiments.
2 Data: Uspanteko IGT
This section describes the Uspanteko corpus used
for the experiments, our clean-up of the corpus, and
the specific task?labeling part-of-speech and gloss
tags?addressed by the experiments.
2.1 OKMA Uspanteko corpus
Our primary dataset is a corpus of texts (Pixabaj et
al., 2007) in the Mayan language Uspanteko that
were collected, transcribed, translated (into Span-
ish) and annotated as part of the OKMA language
documentation project.2 Uspanteko, a member of
the K?ichee? branch of the Mayan language family,
is spoken by approximately 1320 people in central
Guatemala (Richards, 2003).
2http://www.okma.org
The corpus contains 67 texts, 32 of them glossed.
Four textual genres are represented in the glossed
portion of the corpus: oral histories (five texts) usu-
ally have to do with the history of the village and the
community, personal experience texts (five texts) re-
count events from the lives of individual people in
the community, and stories (twenty texts) are pri-
marily folk stories and children?s stories. The corpus
also contains one recipe and one advice text in which
a speaker discusses what the community should be
doing to better preserve and protect the environment.
The transcriptions are based on spoken data, with
attendant dysfluencies, repetitions, false starts, and
incomplete sentences. Of the 284,455 words, 74,298
are segmented and glossed. This is a small dataset
by computational linguistics standards but rather
large for a documentation project.
2.2 Interlinearized Glossed Text
Once recordings have been made, the next tasks are
typically to produce translations and transcription of
the audio. Transcription is a complex and difficult
process, often involving the development of an or-
thography for the language in parallel. The product
of the transcription is raw text like the Uspanteko
sample shown below (text 068, clauses 283-287):
Non li in yolow rk?il kita?
tinch?ab?ex laj inyolj iin, si no ke
laj yolj jqaaj tinch?ab?ej i non qe li
xk?am rib? chuwe, non qe li lajori
non li iin yolow rk?ilaq.3
Working with the transcription, the translation, and
any previously-attained knowledge about the lan-
guage, the linguist next makes decisions about the
division of words into morphemes and the contribu-
tions made by individual morphemes to the meaning
of the word or of the sentence. IGT efficiently brings
together and presents all of this information.
In the traditional four-line IGT format, mor-
phemes appear on one line and glosses for those
morphemes on the next. The gloss line includes both
labels for grammatical morphemes (e.g. PL or COM)
and translations of stems (e.g. salir or ropa). See
the following example from Uspanteko:4
3Spanish: Solo asi yo aprendi con e?l. No le hable en el
idioma mio. Si no que en el idioma su papa? le hablo. Y solo asi
me fui acostumbrando. Solo asi ahora yo platico con ellos.
4KEY: E1S=singular first person ergative, INC=incompletive,
PART=particle, PREP=preposition, PRON=pronoun, NEG=negation,
37
(2) Kita? tinch?ab?ej laj inyolj iin
(3) kita?
NEG
PART
t-in-ch?abe-j
INC-E1S-hablar-SC
TAM-PERS-VT-SUF
laj
PREP
PREP
in-yolj
E1S-idioma
PERS-S
iin
yo
PRON
?No le hablo en mi idioma.?
(?I don?t speak to him in my language.?)
Most commonly, IGT is presented in a four-tier
format. The first tier (2) is the raw, unannotated
text. The second (first line of (3)) is the same text
with each word morphologically segmented. The
third tier (second line of (3)), the gloss line, is a
combination of Spanish translations of the Uspan-
teko stems and gloss tags representing the grammat-
ical information encoded by affixes and stand-alone
morphemes. The fourth tier (fourth line of (3)) is a
translation in the target language of documentation.
Some interlinear texts include other project-
defined tiers. OKMA uses a fifth tier (third line of
(3)), described as the word-class line. This line is
a mix of traditional POS tags, positional labels (e.g.
suffix, prefix), and broader linguistic categories like
TAM for tense-aspect-mood.
2.3 Cleaning up the OKMA annotations
The OKMA annotations were created using Shoe-
box,5 a standard tool used by documentary linguists
for lexicon management and IGT creation. To de-
velop a corpus suitable for these studies, it was nec-
essary to put considerable effort into normalizing
the original OKMA source annotations. Varied lev-
els of linguistic training of the original annotators
led to many inconsistencies in the original annota-
tions. Also, Shoebox (first developed in 1987) uses
a custom, pre-XML whitespace delimited data for-
mat, making normalization especially challenging.
Finally, not all of the texts are fully annotated. Al-
most half of the 67 texts are just transcriptions, sev-
eral texts are translated but not further analyzed, and
several others are only partially annotated at text
level, clause level, word level, or morpheme level. It
was thus necessary to identify complete texts for use
in our experiments. Some missing labels in nearly-
complete texts were filled in by the expert annotator.
A challenge for representing IGT in a machine-
readable format is maintaining the links between
S=sustantivo (noun), SC=category suffix, SUF=suffix,
TAM=tense/aspect/mood, VT=transitive verb
5http://www.sil.org/computing/shoebox/
the source text morphemes in the second tier and
the morpheme-by-morpheme glosses in the third
tier. The standard Shoebox output format, for ex-
ample, enforces these links through management of
the number of spaces between items in the output.
To address this, we converted the cleaned annota-
tions into IGT-XML (Palmer and Erk, 2007) with
help from the Shoebox/Toolbox interfaces provided
in the Natural Language Toolkit (Robinson et al,
2007). Automating the transformation from Shoe-
box format to IGT-XML?s hierarchical format re-
quired cleaning up tier-to-tier alignment and check-
ing segmentation in some cases where morphemes
and glosses were misaligned, as in (5) below.6
(4) Non li in yolow rk?il
(5) Non
DEM
DEM
li
DEM
DEM
in
yo
PRON
yolow
platicar
VI
r-k?il
AP
SUF
E3s.-SR
PERS SREL
?Solo asi yo aprendi con e?l.?
Here, the number of elements in the morpheme tier
(first line of (5)) does not match the number of el-
ements in the gloss tier (second line of (5)). The
problem is a misanalysis of yolow: it should be
segmented yol-ow with the gloss platicar-AP.
Automating this transformation has the advantage of
identifying such inconsistencies and errors.
There also were many low-level issues that had
to be handled, such as checking and enforcing con-
sistency of tags. For example, the tag E3s. in the
gloss tier of (5) is a typo; the correct tag is E3S. The
annotation tool used in these studies does not allow
such inconsistencies to occur.
2.4 Target labels
There are two main tasks in producing IGT: word
segmentation (determination of stems and affixes)
and glossing each segment. Stems and affixes each
get a different type of gloss: the gloss of a stem is
typically its translation whereas the gloss of an affix
is a label indicating its grammatical role. The addi-
tional word-class line provides part-of-speech infor-
mation for the stems, such as VT for salir.
Complete prediction of segmentation, gloss trans-
lations and labels is our ultimate goal for aiding IGT
6KEY: AP=antipassive, DEM=demonstrative, E3S=singular third
person ergative, PERS=person marking, SR/SREL=relational noun,
VI=intransitive verb
38
creation with automation. Here, we study the poten-
tial for improving annotation efficiency for the more
limited task of predicting the gloss label for each af-
fix and the part-of-speech label for each stem. Thus,
the experiments aim to produce a single label for
each morpheme. We assume that words have been
pre-segmented and we ignore the gloss translations.
The target representation in these studies is an ad-
ditional tier which combines gloss labels for affixes
and stand-alone morphemes with part-of-speech la-
bels for stems. Example (6) repeats the clause in (4),
adding this new combined tier. Stem labels are given
in bold text, and affix labels in plain text.
(6) Non li in yolow rk?il
(7) Non
DEM
li
DEM
in
PRON
yol-ow
VI-AP
r-k?il
E3S-SR
?Solo asi yo aprendi con e?l.?
A simple procedure was used to create the new tier.
For each morpheme, if a gloss label (such as DEM
or E3S) appears on the gloss line (second line of
(3)), we select that label. If what appears is a stem
translation, we instead select the part-of-speech la-
bel from the next tier down (third line of (3)).
In the entire corpus, sixty-nine different labels
appear in this combined tier. The following table
shows the five most common part-of-speech labels
(left) and the five most common gloss labels (right).
The most common label, S, accounts for 11.3% of
the tokens in the corpus.
S noun 7167 E3S sg.3p. ergative 3433
ADV adverb 6646 INC incompletive 2835
VT trans. verb 5122 COM completive 2586
VI intrans. verb 3638 PL plural 1905
PART particle 3443 SREL relational noun 1881
3 Integrated annotation and automation
The experimental framework described in this sec-
tion is designed to model and evaluate real-time inte-
gration of human annotation, active learning strate-
gies, and output from machine-learned classifiers.
The task is annotation of morpheme-segmented texts
from a language documentation project (sec. 2).
3.1 Tools and resources
Integrating automated support and human annota-
tion in this context requires careful coordination of
three components: 1) presenting examples to the an-
notator and storing the annotations, 2) training and
evaluation of tagging models using data labeled by
the annotator, and 3) selecting new examples for an-
notation. The processes are managed and coordi-
nated using the OpenNLP IGT Editor.7 The anno-
tation component of the tool, and in particular the
user interface, is built on the Interlinear Text Editor
(Lowe et al, 2004).
For tagging we use a strong but simple standard
classifier. There certainly are many other modeling
strategies that could be used, for example a condi-
tional random field (as in Settles and Craven (2008)),
or a model that deals differently with POS labels and
morpheme gloss labels. Nonetheless, a documen-
tary linguistics project would be most likely to use a
straightforward, off-the-shelf labeler, and our focus
is on exploring different annotation approaches in a
realistic documentation setting rather than building
an optimal classifier. To that end, we use a standard
maximum entropy classifier which predicts the label
for a morpheme based on the morpheme itself plus
a window of two morphemes before and after. Stan-
dard features used in part-of-speech taggers are ex-
tracted from the morpheme to help with predicting
labels for previously unseen stems and morphemes.
3.2 Annotators and annotation procedures
A practical goal of these studies is to explore best
practices for using automated support to create fully-
annotated texts of the highest quality possible within
fixed resource limits. For producing IGT, one of the
most valuable resources is the time of a linguist with
language-specific expertise. Documentary projects
may also (or instead) have access to a trained lin-
guist without prior experience in the language. We
compare results from two annotators with different
levels of exposure to the language. Both are trained
linguists who specialize in language documentation
and have extensive field experience.8
The first, henceforth referred to as the expert
annotator, has worked extensively on Uspanteko,
including writing a grammar of the language and
7http://igt.sourceforge.net/
8It should be noted that these are pilot studies. With just
two annotators, the annotation comparisons are suggestive but
not conclusive. Even so, this scenario accurately reflects the
resource limitations encountered in documentation projects.
39
contributing to the publication of an Uspanteko-
Spanish dictionary (A?ngel Vicente Me?ndez, 2007).
She is a native speaker of K?ichee?, a closely-related
Mayan language. The second annotator, the non-
expert annotator, is a doctoral student in language
documentation with no prior experience with Us-
panteko and only limited previous knowledge of
Mayan languages. Throughout the annotation pro-
cess, the non-expert annotator relied heavily on the
Uspanteko-Spanish dictionary. Both annotators are
fluent speakers of Spanish, the target translation and
glossing language for the OKMA texts.
In many annotation projects, labeling of training
data is done with reference to a detailed annotation
manual. In the language documentation context, a
more usual situation is for the annotator(s) to work
from a set of agreed-upon conventions but without
strict annotation guidelines. This is not because doc-
umentary linguists lack motivation or discipline but
simply because many aspects of the language are un-
known and the analysis is constantly changing.
In the absence of explicit written annotation
guidelines, we use an annotation training process for
the annotators to learn the OKMA annotation con-
ventions. Two seed sets of ten clauses each were se-
lected to be used both for human annotation training
and for initial classifier training. The first ten clauses
of the first text in the training data were used to seed
model training for the sequential selection cases (see
3.4). The second set of ten were randomly selected
from the entire corpus and used to seed model train-
ing for both random and uncertainty sampling.
These twenty clauses were used to provide initial
guidance to the annotators. With the aid of a list of
possible labels and the grammatical categories they
correspond to, each annotator was asked to label the
seed clauses, and these labels were compared to the
gold standard labels. Annotators were told which
labels were correct and which were incorrect, and
the process was repeated until all morphemes were
correctly labeled. In some cases during this training
phase, the correct label for a morpheme was sup-
plied to the annotator after several incorrect guesses.
3.3 Suggesting labels
We consider two situations with respect to the con-
tribution of the classifier: a suggest condition in
which the labels predicted by the machine learner
are shown to the annotator as she begins labeling a
selected clause, and a no-suggest condition in which
the annotator does not see the predicted labels.
In the suggest cases, the annotator is shown the la-
bel assigned the greatest likelihood by the tagger as
well as a list of several highly-likely labels, ranked
according to likelihood. To be included on this list,
a label must be assigned a probability greater than
half that of the most-likely label. In the no-suggest
cases, the annotator has access to a list of the la-
bels previously seen in the training data for a given
morpheme, ranked in order of frequency of occur-
rence with the morpheme in question; this is similar
to the input an annotator gets while glossing texts in
Shoebox/Toolbox. Specifically, Shoebox/Toolbox
presents previously seen glosses and labels for a
given morpheme in alphabetic order.
3.4 Sample selection
We consider three methods of selecting examples
for annotation?sequential (seq), random (rand), and
uncertainty sampling (al)?and the performance of
each method in both the suggest and the no-suggest
setups. For uncertainty sampling, we measure un-
certainty of a clause as the average entropy per mor-
pheme (i.e., per labeling decision).
3.5 Measuring annotation cost
Not all examples take the same amount of effort to
annotate. Even so, the bulk of the literature on active
learning assumes some sort of unit cost to determine
the effectiveness of different sample selection strate-
gies. Examples of unit cost measurements include
the number of documents in text classification, the
number of sentences in part-of-speech tagging (Set-
tles and Craven, 2008), or the number of constituents
in parsing (Hwa, 2000). These measures are conve-
nient for performing active learning simulations, but
awareness has grown that they are not truly repre-
sentative measures of the actual cost of annotation
(Haertel et al, 2008a; Settles et al, 2008), with Ngai
and Yarowsky (2000) being an early exception to the
unit-cost approach. Also, Baldridge and Osborne
(2004) use discriminants in parse selection, which
are annotation decisions that they later showed cor-
relate with timing information (Baldridge and Os-
borne, 2008).
The cost of annotation ultimately comes down to
40
money. Since annotator pay may be variable but will
(under standard assumptions) be constant for a given
annotator, the best approximation of likely cost sav-
ings is to measure the time taken to annotate under
different levels of automated support. This is es-
pecially important in sample selection and its inter-
action with automated suggestions: active learning
seeks to find more informative examples, and these
will most likely involve more difficult decisions, de-
creasing annotation quality and/or increasing anno-
tation time (Hachey et al, 2005). Thus, we measure
cost in terms of the time taken by each annotator on
each example. This allows us to measure the actual
time taken to produce a given labeled data set, and
thus compare the effectiveness of different levels of
automated support plus their interaction with anno-
tators of different levels of expertise.
Recent work shows that paying attention to pre-
dicted annotation cost in sample selection itself can
increase the effectiveness of active learning (Settles
et al, 2008; Haertel et al, 2008b). Though we have
not explored cost-sensitive selection here, the sce-
nario described here is an appropriate test ground for
it: in fact, the results of our experiments, reported in
the next section, provide strong evidence for a real
natural language annotation task that active learning
selection with cost-sensitivity is indeed sub-optimal.
4 Discussion
This section presents and discusses preliminary re-
sults from the ongoing annotation experiments. The
Uspanteko corpus was split into training, develop-
ment, and held-out test sets, roughly 50%, 25%,
and 25%. Specifically, the training set of 21 texts
contains 38802 words, the development set of 5
texts contains 16792 words, and the held-out test
set, 6 texts, contains 18704 words. These are small
datasets, but the size is realistic for computational
work on endangered languages.
When measuring the performance of annotators,
factors like fatigue, frustration, and especially the
annotator?s learning process must be considered.
Annotators improve as they see more examples (es-
pecially the non-expert annotator). To minimize the
impact of the annotator?s learning process on the re-
sults, annotation is done in rounds. Each round con-
sists of ten clauses from each of the six experimental
0 10 20 30 40 50
0
10
20
30
40
Number of Annotation Rounds
Seco
nds 
per M
orph
eme
Non?expertExpert
Figure 1: Average annotation time (in seconds per mor-
pheme) over annotation rounds, averaged over all six con-
ditions for each annotator.
cases for each annotator. The newly-labeled clauses
are then added to the labeled training data, and a new
tagging model is trained on the updated training set
and evaluated on the development set. Both annota-
tors have completed fifty-one rounds of annotation
so far, labeling 510 clauses for each of the six ex-
perimental conditions. The average number of mor-
phemes labeled is 3059 per case. Because the anno-
tation experiments are ongoing, we discuss results in
terms of the trends seen thus far.
4.1 Annotator speed
The expert annotator showed a small increase in
speed after an initial familiarization period, and the
non-expert showed a dramatic increase. Figure 1
plots the number of seconds taken per morpheme
over the course of annotation, averaged over all six
conditions for each annotator. The slowest, fastest,
and mean rates, in seconds per morpheme, for the
expert annotator were 12.60, 1.89, and 4.14, respec-
tively. For the non-expert, they were 59.71, 1.90,
and 8.03.
4.2 Accuracy of model on held-out data
Table 1 provides several measures of the current
state of annotation in all 12 conditions after 51
rounds of annotation. The sixth column, labeled
41
Anno Suggest Select Time (sec) #Morphs Model Accuracy Total Accuracy of Annotation
NonExp N Seq 23739.79 3314 63.28 63.92
NonExp N Rand 22721.11 2911 68.36 68.69
NonExp N AL 23755.71 2911 68.26 67.84
NonExp Y Seq 21514.05 2887 66.55 66.89
NonExp Y Rand 22189.68 3002 68.41 68.73
NonExp Y AL 25731.57 2750 67.63 67.30
Exp N Seq 11862.39 3354 61.15 61.88
Exp N Rand 11665.10 3043 64.60 64.91
Exp N AL 13894.14 3379 66.74 66.47
Exp Y Seq 11758.74 2892 61.12 61.48
Exp Y Rand 11426.85 2979 60.13 60.57
Exp Y AL 16253.40 3296 63.30 63.15
Table 1: After 51 rounds of annotation: ModelAcc=accuracy on development set, TotalAnnoAcc=accuracy of fully-labeled corpus
ModelAcc, shows the accuracy of models on the
development data. This represents a unit cost as-
sumption at the clause level: measured this way, the
results would suggest that the non-expert was best
served by random selection, with no effect from ma-
chine suggestions. For the expert, they suggest ac-
tive learning without suggestions is best, and that
suggestions actually hurt effectiveness.
4.3 Accuracy of fully-labeled corpus
We are particularly concerned with the question of
how to develop a fully-labeled corpus with the high-
est level of accuracy, given a finite set of resources.
Thus, we combine the portion of the training set la-
beled by the human annotator with the results of tag-
ging the remainder of the training set with the model
trained on those annotations. The rightmost column
of Table 1, labeled Total Accuracy of Annotation,
shows the accuracy of the fully labeled training set
(part human, part machine labels) after 51 rounds.
These accuracies parallel the model accuracies: ran-
dom selection is best for the non-expert annotator,
and uncertainty selection is best for the expert.
Since this tagging task involves labeling mor-
phemes, a clause cost assumption is not ideal?e.g.,
active learning tends to select longer clauses and
thereby obtains more labels. To reflect this, a sub-
clause cost can help: here we use the number of
morphemes annotated. The column labeled Tokens
in Table 2 shows the total accuracy achieved in each
condition when human annotation ceases at 2750
morphemes. The figure in parentheses is the cumu-
lative annotation time at the morpheme cut-off point.
Here, the non-expert does best: he took great care
with the annotations and was clearly not tempted to
Anno Suggest Select Time Tokens (time)
(11427 sec) (2750 morphs)
NonExp N Seq 55.01 59.80 (21678 secs)
NonExp N Rand 59.95 68.68 (22069 secs)
NonExp N AL 59.86 67.70 (22879 secs)
NonExp Y Seq 60.27 66.79 (21053 secs)
NonExp Y Rand 62.96 68.38 (21194 secs)
NonExp Y AL 59.18 67.30 (25732 secs)
Exp N Seq 61.21 59.18 (10110 secs)
Exp N Rand 64.92 64.42 (10683 secs)
Exp N AL 65.72 65.74 (11826 secs)
Exp Y Seq 61.47 61.47 (11436 secs)
Exp Y Rand 60.57 61.16 (10934 secs)
Exp Y AL 61.54 62.87 (13957 secs)
Table 2: For given cost, accuracy of fully-labeled corpus.
accept erroneous suggestions from the machine la-
beler. In contrast, the expert does seem to have ac-
cepted many bad machine suggestions.
Morpheme unit cost is more fine-grained than
clause-level cost, but it hides the fact that the ex-
pert annotator needed far less time to produce a cor-
pus of higher overall labeled quality than the non-
expert. This can be seen in the Time column of
Table 2, which gives the total annotation accuracy
when 11427 seconds are alloted for human label-
ing. The expert annotator achieved the highest accu-
racy for total labeling of the training set using active
learning without machine label suggestions. Active
learning helps the non-expert as well, but his best
condition is random selection with machine labels.
4.4 Annotator accuracy by round
Active learning clearly selects harder examples that
hurt the non-expert?s performance. To see this
clearly, we measured the accuracy of the annotators?
labels for each round of each experimental setup,
42
0 10 20 30 40 50
50
60
70
80
90
100
Number of Annotation Rounds
Sing
le R
oun
d Ac
cura
cy
Suggest + UncertaintySuggest + RandomSuggest + SequentialNo?suggest + UncertaintyNo?suggest + RandomNo?suggest + Sequential
0 10 20 30 40 50
50
60
70
80
90
100
Number of Annotation Rounds
Sing
le R
oun
d Ac
cura
cy
Suggest + UncertaintySuggest + RandomSuggest + SequentialNo?suggest + UncertaintyNo?suggest + RandomNo?suggest + Sequential
(a) (b)
Figure 2: Single round accuracy per round for each experiment type by: (a) non-expert annotator, (b) expert annotator
given in Fig. 2. It is not clear at this stage whether
the tag suggestions by the machine labeler are help-
ful to human annotation. It is useful to compare the
cases where the machine learner is not involved in
example selection (i.e. random and sequential) to
uncertainty sampling, which does involve the ma-
chine learner. One thing that is apparent is that when
active learning is used to select samples for annota-
tion, both the expert and non-expert annotator have
a harder time providing correct tags. A point of con-
trast between the expert and non-expert is that the
non-expert generally outperforms the expert on label
accuracy in the non-active learning scenarios. The
non-expert was very careful with his labeling deci-
sions, but also much slower than the expert. In the
end, speedier annotation rates allowed the expert an-
notator to achieve higher accuracies in less time.
5 Conclusion
We have described a set of ongoing pilot experi-
ments designed to test the utility of machine label-
ing and active learning in the context of documen-
tary linguistics. The production of IGT is a realistic
annotation scenario which desperately needs label-
ing efficiency improvements. Our preliminary re-
sults suggest that both machine labeling and active
learning can increase the effectiveness of annotators,
but they interact quite strongly with the expertise of
the annotators. In particular, though active learn-
ing works well with the expert annotator, for a non-
expert annotator it seems that random selection is
a better choice. However, we stress that our anno-
tation experiments are ongoing. Active learning is
often less effective early in the learning curve, es-
pecially when automated label suggestions are pro-
vided, because the model is not yet accurate enough
to select truly useful examples, nor to suggest labels
for them reliably (Baldridge and Osborne, 2004).
Thus, we expect automation via uncertainty sam-
pling and/or suggestion may gather momentum and
outpace random selection and/or no suggestions by
wider margins as annotation continues.
Acknowledgments
This work is funded by NSF grant BCS 06651988
?Reducing Annotation Effort in the Documentation
of Languages using Machine Learning and Active
Learning.? Thanks to Katrin Erk, Nora England,
Michel Jacobson, and Tony Woodbury; and to anno-
tators Telma Kaan Pixabaj and Eric Campbell. Fi-
nally, thanks to the anonymous reviewers for valu-
able feedback.
43
References
Miguel A?ngel Vicente Me?ndez. 2007. Diccionario bil-
ingu?e Uspanteko-Espan?ol. Cholaj Tzijb?al li Uspan-
teko. Okma y Cholsamaj, Guatemala.
Jason Baldridge and Miles Osborne. 2004. Active learn-
ing and the total cost of annotation. In Proceedings of
Empirical Approaches to Natural Language Process-
ing (EMNLP).
Jason Baldridge and Miles Osborne. 2008. Active learn-
ing and logarithmic opinion pools for HPSG parse se-
lection. Natural Language Engineering, 14(2):199?
222.
David Crystal. 2000. Language Death. Cambridge Uni-
versity Press, Cambridge.
Ben Hachey, Beatrice Alex, and Markus Becker. 2005.
Investigating the effects of selective sampling on the
annotation task. In Proceedings of the 9th Conference
on Computational Natural Language Learning, Ann
Arbor, MI.
Robbie Haertel, Eric Ringger, Kevin Seppi, James Car-
roll, and McClanahan Peter. 2008a. Assessing the
costs of sampling methods in active learning for anno-
tation. In Proceedings of ACL-08: HLT, Short Papers,
pages 65?68, Columbus, Ohio, June. Association for
Computational Linguistics.
Robbie A. Haertel, Kevin D. Seppi, Eric K. Ringger, and
James L. Carroll. 2008b. Return on investment for
active learning. In Proceedings of the NIPS Workshop
on Cost-Sensitive Learning. ACL Press.
Rebecca Hwa. 2000. Sample selection for statistical
grammar induction. In Proceedings of the 2000 Joint
SIGDAT Conference on EMNLP and VLC, pages 45?
52, Hong Kong, China, October.
William Lewis and Fei Xia. 2008. Automatically iden-
tifying computationally relevant typological features.
In Proceedings of IJCNLP-2008, Hyderabad, India.
John Lowe, Michel Jacobson, and Boyd Michailovsky.
2004. Interlinear text editor demonstration and projet
archivage progress report. In 4th EMELD workshop
on Linguistic Databases and Best Practice, Detroit,
MI.
Grace Ngai and David Yarowsky. 2000. Rule Writing or
Annotation: Cost-efficient Resource Usage for Base
Noun Phrase Chunking. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics, pages 117?125, Hong Kong.
Alexis Palmer and Katrin Erk. 2007. IGT-XML: An
XML format for interlinearized glossed text. In Pro-
ceedings of the Linguistic Annotation Workshop (LAW-
07), ACL07, Prague.
Telma Can Pixabaj, Miguel Angel Vicente Me?ndez,
Mar??a Vicente Me?ndez, and Oswaldo Ajcot Damia?n.
2007. Text collections in Four Mayan Languages.
Archived in The Archive of the Indigenous Languages
of Latin America.
Michael Richards. 2003. Atlas lingu???stico de Guatemala.
Servipresna, S.A., Guatemala.
Stuart Robinson, Greg Aumann, and Steven Bird. 2007.
Managing fieldwork data with Toolbox and the Natu-
ral Language Toolki t. Language Documentation and
Conservation, 1:44?57.
Burr Settles and Mark Craven. 2008. An analysis of
active learning strategies for sequence labeling tasks.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1070?1079, Honolulu, Hawaii, October. Association
for Computational Linguistics.
Burr Settles, Mark Craven, and Lewis Friedland. 2008.
Active learning with real annotation costs. In Proceed-
ings of the NIPS Workshop on Cost-Sensitive Learn-
ing, pages 1069?1078. ACL Press.
Fei Xia and William Lewis. 2007. Multilingual struc-
tural projection across interlinear text. In Proceedings
of HLT/NAACL 2007, Rochester, NY.
Fei Xia and William Lewis. 2008. Repurposing theoreti-
cal linguistic data for tool development antd search. In
Proceedings of IJCNLP-2008, Hyderabad, India.
44
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 196?206,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Crouching Dirichlet, Hidden Markov Model:
Unsupervised POS Tagging with Context Local Tag Generation
Taesun Moon, Katrin Erk, and Jason Baldridge
Department of Linguistics
University of Texas at Austin
1 University Station B5100
Austin, TX 78712-0198 USA
{tsmoon,katrin.erk,jbaldrid}@mail.utexas.edu
Abstract
We define the crouching Dirichlet, hidden
Markov model (CDHMM), an HMM for part-
of-speech tagging which draws state prior dis-
tributions for each local document context.
This simple modification of the HMM takes
advantage of the dichotomy in natural lan-
guage between content and function words. In
contrast, a standard HMM draws all prior dis-
tributions once over all states and it is known
to perform poorly in unsupervised and semi-
supervised POS tagging. This modification
significantly improves unsupervised POS tag-
ging performance across several measures on
five data sets for four languages. We also show
that simply using different hyperparameter
values for content and function word states in
a standard HMM (which we call HMM+) is
surprisingly effective.
1 Introduction
Hidden Markov Models (HMMs) are simple, ver-
satile, and widely-used generative sequence models.
They have been applied to part-of-speech (POS) tag-
ging in supervised (Brants, 2000), semi-supervised
(Goldwater and Griffiths, 2007; Ravi and Knight,
2009) and unsupervised (Johnson, 2007) training
scenarios. Though discriminative models achieve
better performance in both semi-supervised (Smith
and Eisner, 2005) and supervised (Toutanova et al,
2003) learning, there has been only limited work on
unsupervised discriminative sequence models (e.g.,
on synthetic data and protein sequences (Xu et al,
2006)), and none to POS tagging.
The tagging accuracy of purely unsupervised
HMMs is far below that of supervised and semi-
supervised HMMs; this is unsurprising as it is still
not well understood what kind of structure is being
found by an unconstrained HMM (Headden III et al,
2008). However, HMMs are fairly simple directed
graphical models, and it is straightforward to ex-
tend them to define alternative generative processes.
This also applies to linguistically motivated HMMs
for recovering states and sequences that correspond
more closely to those implicitly defined by linguists
when they label sentences with parts-of-speech.
One way in which a basic HMM?s structure is a
poor model for POS tagging is that there is no inher-
ent distinction between (open-class) content words
and (closed-class) function words. Here, we propose
two extensions to the HMM. The first, HMM+, is a
very simple modification where two different hyper-
parameters are posited for content states and func-
tion states, respectively. The other is the crouch-
ing Dirichlet, hidden Markov model (CDHMM), an
extended HMM that captures this dichotomy based
on the statistical evidence that comes from context.
Content states display greater variance across lo-
cal context (e.g. sentences, paragraphs, documents),
and we capture this variance by adding a component
to the model for content states that is based on la-
tent Dirichlet alocation (Blei et al, 2003). This ex-
tension is in some ways similar to the LDAHMM
of Griffiths et al (2005). Both models are compos-
ite in that two distributions do not mix with each
other. Unlike the LDAHMM, the generation of con-
tent states is folded into the CDHMM process.
We compare the HMM+ and CDHMM against a
basic HMM and LDAHMM on POS tagging on a
more extensive and diverse set of languages than
previous work in monolingual unsupervised POS
tagging: four languages from three families (Ger-
manic: English and German; Romance: Portuguese;
196
and Mayan: Uspanteko). The CDHMM easily out-
performs all other models, including HMM+, across
three measures (accuracy, F-score, and variation
of information) for unsupervised POS tagging on
most data sets. However, the HMM+ is surpris-
ingly competitive, outperforming the basic HMM
and LDAHMM, and rivaling or even passing the
CDHMM on some measures and data sets.
2 Background
The Bayesian formulation for a basic HMM (Gold-
water and Griffiths, 2007) is:
?t|? ? Dir(?)
?t|? ? Dir(?)
wi|ti = t ? Mult(?t)
ti|ti?1 = t ? Mult(?t)
Dir is the conjugate Dirichlet prior to Mult (a multi-
nomial distribution). The state transitions are gen-
erated by Mult(?t) whose prior ?t is generated by
Dir(?) with a symmetric (i.e. uniform) hyperparam-
eter ?. Emissions are generated by Mult(?t) with
a prior ?t generated by Dir(?) with a symmetric
hyperparameter ?. Hyperparameter values smaller
than one encourage posteriors that are peaked, with
smaller values increasing this concentration. It is
not necessary that the hyperparameters be symmet-
ric, but this is a common approach when one wants
to be na??ve about the data. This is particularly ap-
propriate in unsupervised POS tagging with regard
to novel data since there won?t be a priori grounds
for favoring certain distributions over others.
There is considerable work on extensions to
HMM-based unsupervised POS tagging (see ?6),
but here we concentrate on the LDAHMM (Grif-
fiths et al, 2005), which models topics and state
sequences jointly. The model is a composite of a
probabilistic topic model and an HMM in which a
single state is allocated for words generated from
the topic model. A strength of this model is that it
is able to use less supervision than previous topic
models since it does not require a stopword list.
While the topic model component still uses the bags-
of-words assumption, the joint model infers which
words are more likely to carry topical content and
which words are more likely to contribute to the
local sequence. This model is competitive with a
standard topic model, and its output is also compet-
itive when compared with a standard HMM. How-
ever, Griffiths et al (2005) note that the topic model
component inevitably loses some finer distinctions
with respect to parts-of-speech. Though many con-
tent states such as adjectives, verbs, and nouns can
vary a great deal across documents, the topic state
groups these words together. This leads to assign-
ment of word tokens to clusters that are a poorer fit
for POS tagging. This paper shows that a model that
conflates the LDAHMM topics with content states
can significantly improve POS tagging.
3 Models
We aim to model the fact that in many languages
words can generally be grouped into function words
and content words and that these groups often
have significantly different distributions. There are
few function words and they appear frequently,
while there are many content words appearing infre-
quently. Another difference in distribution is often
implied in information retrieval by the use of stop-
word filters and tf-idf values to remove or reduce the
influence of words which occur frequently but have
low variance (i.e. their global probability is similar
to their local probability in a document).
A difference in distribution is also revealed when
the parts-of-speech are known. When no smoothing
parameters are added, the joint probability of a word
that is not ?the? or ?a? occurring with a DT tag (in
the Penn Treebank) is almost always zero. Similarly
peaked distributions are observed for other function
categories such as MD and CC. On the other hand,
the joint probability of any word occurring with NN
is much less likely to be zero and the distribution is
much less likely to be peaked.
We attempt to account for these two distributional
properties?that certain words have higher variance
across contexts (e.g. a document) and that certain
tags have more peaked emission distributions?in a
sequence model. To do this, we define the crouching
Dirichlet, hidden Markov model1 (CDHMM). This
model, like LDAHMM, captures items of high vari-
ance across contexts, but it does so without losing
1We call our model a ?crouching Dirichlet? model since it
involves a Dirichlet prior that generates distributions for certain
states as if it were ?crouching? on the side.
197
wi
?
?
?
?
ti? ? ? ? ? ?
?
?
?
?
Figure 1: Graphical representation of relevant vari-
ables and dependencies at a given time step i. Ob-
served word wi is dependent on hidden state ti.
Edges to priors ?, ?, ? may or may not be activated
depending on the value of ti. The edge to transition
prior ? is always activated. Hyperparameters to pri-
ors are represented by dots. See ?3.1 for details.
sequence distinctions, namely, a given word?s lo-
cal function via its part-of-speech. We also define
the HMM+, a simple adaptation of a basic HMM
which accounts for the latter property by using dif-
ferent priors for emissions from content and function
states.
3.1 CDHMM
The CDHMM incorporates an LDA-like module to
its graphical structure in order to capture words
and tags which have high variance across contexts.
Such tags correspond to content states. Like the
LDAHMM, the model is composite in that distribu-
tions over a single random variable are composed
of several different distribution functions which de-
pend on the value of the underlying variable.
We posit the following model (see fig. 1 for a dia-
gram of dependencies and all variables involved at a
single time step). We observe a sequence of tokens
w=(w1, . . . , wN ) that we assume is generated by
an underlying state sequence t=(t1, . . . , tN ) over a
state alphabet T with first order Markov dependen-
cies. T is a union of disjoint content states C and
function states F . In this composite model, the pri-
ors for the emission and transition for each step in
the sequence depend on whether state t at step i is
t?C or t?F . If t?C , the word emission is depen-
dent on ? (the content word prior) and the state tran-
sition is dependent on ? (the ?topic? prior) and ? (the
transition prior). If t?F , the word emission proba-
bility is dependent on ? (the function word prior)
and the state transition on ? (again, the transition
prior). Therefore, if t?F , the transition and emis-
sion structure is identical to the standard Bayesian
HMM.
To elaborate, three prior distributions are defined
globally for this model: (1) ?t, the transition prior
such that p(t?|t, ?t) = ?t?|t (2) ?t, the function word
prior such that p(w|t, ?t) = ?w|t (3) ?t, the content
word prior such that p(w|t, ?t) = ?w|t. Locally for
each context d (documents in our case), we define
?d, the topic prior such that p(t|?d) = ?t|d for t?C .
The generative story is as follows:
1. For each state t?T
(a) Draw a distribution over states ?t ?
Dir(?)
(b) If t?C , draw a distribution over words
?t ? Dir(?)
(c) If t?F , draw a distribution over words
?t ? Dir(?)
2. For each context d
(a) Draw a distribution ?d ? Dir(?) over
states t?C
(b) For each word wi in d
i. draw ti from ?ti?1 ? ?d
ii. if ti?C , then draw wi from ?ti , else
draw wi from ?ti
For each context d, we draw a prior distribution
?d?formally identical to the LDA topic prior?that
is defined only for the states t?C . This prior is then
used to weight the draws for states at each word,
from ?ti?1 ? ?d, where we have defined the vector
valued operation ? as follows:
(?ti?1 ? ?d)ti =
{
1
Z ?ti|ti?1 ? ?ti|d ti?C
1
Z ?ti|ti?1 ti?F
where (?ti?1 ? ?d)ti is the element corresponding to
state ti in the vector ?ti?1 ? ?d. Z is a normalization
constant such that the probability mass sums to one.
198
p(ti|t?i,w) ?
?
?
?
?
?
Nwi|ti+?
Nti+W?
Nti|di+?
Ndi+C?
?
Nti|ti?1+?
??
Nti+1|ti+I[ti?1=ti=ti+1]+?
?
Nti+T?+I[ti=ti?1]
ti ? C
Nwi|ti+?
Nti+W?
?
Nti|ti?1+?
??
Nti+1|ti+I[ti?1=ti=ti+1]+?
?
Nti+T?+I[ti=ti?1]
ti ? F
Figure 2: Conditional distribution for ti in the CDHMM.
The important thing to note is that the draw for
states at each word is proportional to a composite
of (a) the product of the individual elements of the
topic and transition priors when ti?C and (b) the
transition priors when ti?F . The draw is propor-
tional to the product of topic and transition priors
when ti?C because we have made a product of ex-
perts (PoE) factorization assumption (Hinton, 2002)
for tractability and to reduce the size of our model.
Without such an assumption, the transition parame-
ters would lie in a partitioned space of size O(|C|4)
as opposed to O(|T |2) for the current model. Fur-
thermore, this combination of a composite hidden
state space with a product of experts assumption al-
lows us to capture high variance for certain states.
To summarize, the CDHMM is a composite
model where both the observed token and the hidden
state variable are composite distributions. For the
hidden state, this means that there is a ?topical? ele-
ment with high variance across contexts that is em-
bedded in the state sequence for a subset of events.
We embed this element through a PoE assumption
where transitions into content states are modeled as
a product of the transition probability and the local
probability of the content state.
Inference. We use a Gibbs sampler (Gao and
Johnson, 2008) to learn the parameters of this and
all other models under consideration. In this infer-
ence regime, two distributions are of particular in-
terest. One is the posterior density and the other is
the conditional distribution, neither of which can be
learned in closed form.
Letting ? = (?, ?, ?, ?) and h = (?, ?, ?, ?), the
posterior density is given as
p(?|w, t;h) ? p(w, t|?)p(?;h)
Note that p(w, t|?) is equal to
D
?
d
Nd
?
i
(
?wi|ti?ti|d?ti|ti?1
)I[ti?C]
(
?wi|ti?ti|ti?1
)I[ti?F ] (1)
where I[?] is the indicator function, D is the number
of documents in the corpus and Nd is the number of
tokens in document d.
Another important measure is the conditional dis-
tribution which is conditioned on all the random
variables except the hidden state variable of interest
and which is derived by integrating out the priors:
p(ti|t?i,w;h) ? p(ti|t?i;h)p(wi|t,w?i;h) (2)
where t?i is the joint random variable t without ti
and w?i is w without wi.
There are two well-known approaches to conduct-
ing Gibbs sampling for HMMs. The default method
is to sample ? based on the posterior, then sample
each ti based on the conditional distribution. An-
other approach is to sample directly from the con-
ditional distribution without sampling from the pos-
terior since the conditional distribution incorporates
the posterior through integration. This is called a
collapsed Gibbs sampler, which is the method em-
ployed for the models in this study.
The full conditional distribution for tag transitions
for the Gibbs sampler is given in Figure 2. At each
time step, we decrement all counts for the current
value of ti, sample a new value for ti from a multino-
mial proportional to the conditional distribution and
assign that value to ti. ?, ? are the hyperparameters
for the word emission priors of the content states and
function states, respectively. ? is the hyperparame-
ter for the state transition priors. ? is the hyperpa-
rameter for the state prior given that it is in some
context d. Note that we have overridden notation so
199
that C and T here refer to the size of the alphabet.
W is the size of the vocabulary. Notation such as
Nti|ti?1 refers to the counts of the events indicated
by the subscript, minus the current token and tag un-
der consideration. Nti|ti?1 is the number of times ti
has occurred after ti?1 minus the tag for wi. Nwi|ti
is the number of times wi has occurred with ti minus
the current value. Nti and Ndi are the counts for the
given tag and document minus the current value.
In its broad outline, the CDHMM is not much
more complicated than an HMM since the decompo-
sition (eqn. 1) is nearly identical to that of an HMM
with the exception that conditional probabilities for
a subset of the states?the content states?are local.
An inference algorithm can be derived that involves
no more than adding a single term to the standard
MCMC algorithm for HMMs (see Figure 2).
3.2 HMM+
The CDHMM explicitly posits two different types
of states: function states and content states. Hav-
ing made this distinction, there is a very simple way
to capture the difference in emission distributions
for function and content states within an otherwise
standard HMM: posit different hyperparameters for
the two types. One type has a small hyperparame-
ter to model a sparse distribution for function words
and the other has a relatively large hyperparameter
to model a distribution with broader support. This
extension, which we refer to as HMM+, provides an
important benchmark to compare with the CDHMM
to see how much is gained by its additional ability to
model the fact that function words occur frequently
but have low variance across contexts.
As with the CDHMM, we use Gibbs sampling to
estimate the model parameters while holding the two
different hyperparameters fixed. The conditional
distribution for tag transitions for this model is iden-
tical to that in fig. 2 except that it does not have the
second term Nti|di+?Ndi+C? in the first case where ti?C .
We are not aware of a published instance of such
an extension to the HMM?which our results show
to be surprisingly effective. Goldwater and Griffiths
(2007) posits different hyperparameters for individ-
ual states, but not for different groups of states.
corpus tokens docs avg. tags
WSJ 974254 1801 541 43
Brown 797328 343 2325 80
Tiger 447079 1090 410 58
Floresta 197422 1956 101 19
Uspanteko 70125 29 2418 83
Table 2: Number of tokens, documents, average to-
kens per document and total tag types for each cor-
pus.
4 Data and Experiments
Data. We use five datasets from four languages
(English, German, Portuguese, Uspanteko) for eval-
uating POS tagging performance.
? English: the Brown corpus (Francis et al, 1982)
and the Wall Street Journal portion of the Penn
Treebank (Marcus et al, 1994).
? German: the Tiger corpus (Brants et al, 2002).
? Portuguese: the full Bosque subset of the Floresta
corpus (Afonso et al, 2002).
? Uspanteko (an endangered Mayan language of
Guatemala): morpheme-segmented and POS-
tagged texts collected and annotated by the
OKMA language documentation project (Pixabaj
et al, 2007); we use the cleaned-up version de-
scribed in Palmer et al (2009).
Table 2 provides the statistics for these corpora.
We lowercase all words, do not remove any punc-
tuation or hapax legomena, and we do not replace
numerals with a single identifier. Due to the nature
of the models, document boundaries are retained.
Evaluation We report values for three evaluation
metrics on all five corpora, using their full tagsets.
? Accuracy: We use a greedy search algorithm to
map each unsupervised tag to a gold label such
that accuracy is maximized. We evaluate on a
1-to-1 mapping between unsupervised tags and
gold labels, as well as many-to-1 (M-to-1), cor-
responding to the evaluation mappings used in
Johnson (2007). The 1-to-1 mapping provides a
stricter evaluation. The many-to-one mapping, on
the other hand, may be more adequate as unsu-
pervised tags tend to be more fine-grained than
200
Model Accuracy Pairwise P/R Scores VI1-to-1 M-to-1 P R F
W
SJ
(50
) HMM 0.34 (0.01) 0.49 (0.03) 0.51 (0.03) 0.19 (0.01) 0.28 (0.01) 3.72 (0.08)
LDAHMM 0.30 (0.04) 0.45 (0.04) 0.25 (0.07) 0.27 (0.03) 0.26 (0.04) 3.64 (0.14)
HMM+ 0.42 (0.04) 0.46 (0.05) 0.24 (0.03) 0.49 (0.03) 0.32 (0.03) 2.65 (0.15)
CDHMM 0.44 (0.01) 0.58 (0.02) 0.31 (0.01) 0.43 (0.03) 0.36 (0.02) 2.73 (0.08)
B
ro
w
n
(50
) HMM 0.32 (0.01) 0.50 (0.02) 0.60 (0.02) 0.18 (0.00) 0.28 (0.01) 3.82 (0.05)
LDAHMM 0.28 (0.06) 0.41 (0.08) 0.25 (0.10) 0.28 (0.05) 0.25 (0.05) 3.71 (0.21)
HMM+ 0.43 (0.06) 0.48 (0.07) 0.29 (0.05) 0.50 (0.04) 0.37 (0.05) 2.63 (0.19)
CDHMM 0.48 (0.02) 0.62 (0.02) 0.32 (0.03) 0.54 (0.04) 0.40 (0.03) 2.48 (0.06)
Ti
ge
r
(50
) HMM 0.29 (0.02) 0.49 (0.02) 0.49 (0.04) 0.14 (0.01) 0.22 (0.02) 3.91 (0.06)
LDAHMM 0.31 (0.04) 0.50 (0.04) 0.26 (0.07) 0.24 (0.02) 0.25 (0.04) 3.51 (0.11)
HMM+ 0.41 (0.08) 0.44 (0.05) 0.25 (0.05) 0.58 (0.10) 0.35 (0.06) 2.70 (0.25)
CDHMM 0.47 (0.01) 0.61 (0.02) 0.45 (0.01) 0.58 (0.03) 0.50 (0.02) 2.72 (0.04)
U
sp
.
(50
) HMM 0.36 (0.01) 0.49 (0.02) 0.39 (0.01) 0.18 (0.00) 0.25 (0.00) 3.63 (0.04)
LDAHMM 0.35 (0.02) 0.47 (0.02) 0.26 (0.04) 0.23 (0.03) 0.24 (0.02) 3.52 (0.09)
HMM+ 0.32 (0.02) 0.35 (0.03) 0.12 (0.02) 0.52 (0.05) 0.20 (0.02) 3.13 (0.06)
CDHMM 0.39 (0.02) 0.50 (0.02) 0.16 (0.02) 0.39 (0.03) 0.23 (0.02) 3.00 (0.06)
Fl
o
r.
(50
) HMM 0.30 (0.01) 0.58 (0.03) 0.62 (0.05) 0.18 (0.01) 0.28 (0.01) 3.51 (0.06)
LDAHMM 0.36 (0.06) 0.59 (0.04) 0.55 (0.10) 0.29 (0.07) 0.38 (0.08) 3.22 (0.15)
HMM+ 0.35 (0.04) 0.52 (0.02) 0.28 (0.04) 0.43 (0.06) 0.34 (0.04) 2.58 (0.07)
CDHMM 0.36 (0.01) 0.64 (0.02) 0.37 (0.02) 0.27 (0.01) 0.31 (0.01) 2.73 (0.05)
Table 1: Evaluation on WSJ, Brown, Tiger, Floresta and Uspanteko for models with 50 states. For VI, lower
is better
gold part-of-speech tags. In particular, they tend
to form semantically coherent sub-classes of gold
parts of speech.
? Pairwise Precision and Recall: Viewing tagging
as a clustering task over tokens, we evaluate pair-
wise precision (P ) and recall (R) between the
model tag sequence (M ) and gold tag sequence
(G) by counting the true positives (tp), false pos-
itives (fp) and false negatives (fn) between the
two and setting P = tp/(tp + fp) and R =
tp/(tp+ fn). tp is the number of token pairs that
share a tag in M as well as in G, fp is the number
token pairs that share the same tag in M but have
different tags in G, and fn is the number token
pairs assigned a different tag in M but the same
in G (Meila, 2007). We also provide the f -score
which is the harmonic mean of P and R.
? Variation of Information (VI): The variation of
information is an information theoretic metric
that measures the amount of information lost and
gained in going from tag sequenceM toG (Meila,
2007). It is defined as V I(M,G) = H(M) +
H(G) ? 2I(M,G) where H denotes entropy and
I mutual information. Goldwater and Griffiths
(2007) noted that this measure can point out mod-
els that have more consistent errors in the form
of lower VI, even when accuracy figures are the
same.
We also report learning curves on M-to-1 with ge-
ometrically increasing training set sizes of 8, 16, 32,
64, 128, 256, 512, 1024, and all documents, or as
many as possible given the corpus.
5 Experiments
In this section we discuss our parameter settings and
experimental results.
5.1 Models and Parameters
We compare four different models:
? HMM: a standard HMM
? HMM+: an HMM in which the hyperparameters
for the word emissions are asymmetric, such that
content states have different word emission priors
compared to function states.
? LDAHMM: an HMM with a distinguished state
that generates words from a topic model (Griffiths
et al, 2005)
201
WSJ Brown Tiger Floresta Uspanteko
20
30
40
50
HMM+
a
cc
0.
0
0.
1
0.
2
0.
3
0.
4
0.
5
0.
6
WSJ Brown Tiger Floresta Uspanteko
LDAHMM
a
cc
0.
0
0.
1
0.
2
0.
3
0.
4
0.
5
0.
6
WSJ Brown Tiger Floresta Uspanteko
CDHMM
a
cc
0.
0
0.
1
0.
2
0.
3
0.
4
0.
5
0.
6
Figure 3: Averaged many-to-one accuracy on the full tagset for the models HMM+, LDAHMM, CDHMM
when the number of states is set at 20, 30, 40 and 50 states.
? CDHMM: our HMM with context-based emis-
sions, where the context used is the document
We implemented all of these models, ensuring per-
formance differences are due to the models them-
selves rather than implementation details.
For all models, the transition hyperparameters ?
are set to 0.1. For the LDAHMM and HMM all emis-
sion hyperparameters are set to 0.0001. These fig-
ures are the MCMC settings that provided the best
results in Johnson (2007). For the models that distin-
guish content and function states (HMM+, CDHMM),
we fixed the number of content states at 5 and set the
function state emission hyperparameters ? = 0.0001
and the content state emission hyperparameters ? =
0.1. For the models with an LDA or LDA-like com-
ponent (LDAHMM, CDHMM), we set the topic or
content-state hyperparameter ? = 1.
For decoding, we use maximum posterior decod-
ing to obtain a single sample after the required burn-
in, as has been done in other unsupervised HMM
experiments. We use this sample for evaluation.
5.2 Results
Results for all models on the full tagset are provided
in table 1.2 Each number is the mean accuracy of
ten randomly initialized samples after a single chain
burn-in of 1000 iterations. The model with a sta-
tistically significant (p < 0.05) best score for each
measure and data set is given in plain bold. In cases
2Similar results are obtained with reduced tagsets, as is com-
monly done in other work on unsupervised POS-tagging.
where the differences for the best models are not sig-
nificantly different from each other, but are signifi-
cantly better from the others, the top model scores
are given in bold italic.
CDHMM is extremely strong on the accuracy met-
ric: it wins or ties for all datasets for both 1-to-1 and
M-to-1 measures. For pairwise f -score, it obtains
the best score for two datasets (WSJ and Tiger), and
ties with HMM+ on Brown (we return to Uspanteko
and Floresta below in an experiment that varies the
number of states). For VI, HMM+ and CDHMM both
easily outperform the other models, with CDHMM
winning Brown and Uspanteko and HMM+ winning
Floresta.
In the case of Uspanteko, the absolute difference
in mean performance between models is smaller
overall but still significant. This is due to the reduced
variance between samples for all models. This is
striking because the non-CDHMM models have much
higher standard deviation on other corpora but have
sharply reduced standard deviation only for Uspan-
teko. The most likely explanation is that the Uspan-
teko corpus is much smaller than the other corpora.3
Nonetheless, CDHMM comes out strongest on most
measures.
A simple baseline for accuracy is to choose the
most frequent tag for all tokens; this gives accura-
cies of 0.14 (WSJ), 0.14 (Brown), 0.21 (Tiger), 0.20
3which is interesting in itself since the weak law of large
numbers implies that sample standard deviation decreases with
sample size, which in our case is the number of tokens rather
than the 10 samples under discussion
202
Model Accuracy P/R Scores VI1-to-1 M-to-1 P R F
U
sp
.
(10
0) HMM 0.36 (0.01) 0.58 (0.01) 0.56 (0.02) 0.16 (0.00) 0.25 (0.01) 3.53 (0.04)
LDAHMM 0.35 (0.01) 0.58 (0.02) 0.45 (0.04) 0.17 (0.01) 0.24 (0.01) 3.46 (0.06)
HMM+ 0.35 (0.02) 0.41 (0.02) 0.18 (0.01) 0.36 (0.03) 0.24 (0.01) 3.25 (0.08)
CDHMM 0.40 (0.01) 0.59 (0.01) 0.25 (0.02) 0.27 (0.02) 0.26 (0.01) 3.05 (0.03)
Fl
o
r.
(20
) HMM 0.31 (0.02) 0.48 (0.03) 0.40 (0.03) 0.21 (0.01) 0.28 (0.02) 3.54 (0.10)
LDAHMM 0.35 (0.06) 0.46 (0.06) 0.27 (0.07) 0.45 (0.08) 0.33 (0.05) 3.10 (0.10)
HMM+ 0.37 (0.04) 0.50 (0.03) 0.30 (0.02) 0.45 (0.06) 0.36 (0.03) 2.62 (0.06)
CDHMM 0.44 (0.02) 0.55 (0.02) 0.30 (0.01) 0.53 (0.03) 0.39 (0.02) 2.39 (0.07)
Table 3: Evaluation for Uspanteko and Floresta. Experiments in this table use state sizes that correspond
more closely to the size of the tag sets in the respective corpora.
(Floresta), and 0.11 (Uspanteko). Clearly, all of the
models easily outperform this baseline.
Number of states. Figure 3 shows the change in
accuracy for the different models for different cor-
pora when the overall number of states is varied
between 20 and 50. The figure shows results for
M-to-1. All models with the exception of HMM+
show improvements as the number of states is in-
creased. This brings up the valid concern (Clark,
2003; Johnson, 2007) that a model could posit a
very large number of states and obtain high M-to-
1 scores. However, it is neither the case here nor
in any of the studies we cite. Furthermore, as is
strongly suggested with HMM+, it does not seem as
if all models will benefit from assuming a large num-
ber of states.
Looking at the results by number of states on VI
and f -score for CDHMM(Figure 5), it is clear that
Floresta displays the reverse pattern of all other data
sets where performance monotonically deteriorates
as state sizes are increased. Though the exact reason
is unknown, we believe it is partially due to the fact
that Floresta has 19 tags. We therefore wondered
whether positing a state size that more closely ap-
proximated the size of the gold tag set performs bet-
ter. Since the discrepancy is greatest for Uspanteko
and Floresta, we present tabulated results for exper-
iments with state settings of 100 and 20 states re-
spectively (table 3). With the exception of VI (where
lower is better) for Uspanteko, the scores generally
improve when the model state size is closer to the
gold size. M-to-1 goes down for Floresta when 20
states are posited, but this is to be expected since this
score is defined, to a certain extent, to do better with
WSJ Brown Tiger Floresta Uspanteko
20
30
40
50
F?SCORE
f?
sc
or
e
0.
0
0.
1
0.
2
0.
3
0.
4
0.
5
WSJ Brown Tiger Floresta Uspanteko
VI
vi
0
1
2
3
4
Figure 5: f -score and VI for CDHMM by number of
states
larger models.
Variance. As we average performance figures
over ten runs for each model, it is also instructive
to consider standard deviation across runs. Standard
deviation is lowest for the CDHMM models and the
vanilla HMM. Standard deviation is high for HMM+
and LDAHMM. This is not surprising for LDAHMM,
since it has fifty topic parameters in addition to the
number of states posited, and random initial condi-
tions would have greater effect on the outcome than
for the other models. It is unexpected, however, that
HMM+ has high variance over different chains. The
model shares the large content emission hyperpa-
rameter ? = 0.1 with CDHMM. At this point, it can
only be assumed that the additional LDA component
acts as a regularization factor for CDHMM and re-
duced the volatility in having a large emission hy-
perparameter.
203
0 1 2 3 4 5 6
0.
3
0.
4
0.
5
0.
6
Brown WSJ Tiger
UspantekoFloresta
0 1 2 3 4 5 6 7 8
0.
3
0.
4
0.
5
0.
6
0 1 2 3 4 5 6 7 8
0.
3
0.
4
0.
5
0.
6
0 1 2 3 4 5 6 7 8
0.
3
0.
4
0.
5
0.
6
0 1 2
0.
3
0.
4
0.
5
0.
6
hmm
hmm+
ldahmm
Figure 4: Learning curves on M-to-1 evaluation. The staples at each point represent two standard deviations.
Learning curves We present learning curves on
different sizes of subcorpora in Figure 4. The graphs
are box plots of the full M-1 accuracy figures on
10 randomly initialized training runs for seven sub-
corpora in Brown, nine in WSJ, Tiger, Floresta and
three in Uspanteko.
Comparing the graphs, the performance of HMM+
shows the strongest improvement for English and
German data as the amount of training data in-
creases. Also, it is evident that CDHMM posts con-
sistent performance gains across data sets as it trains
on more data. This stands in opposition to HMM and
LDAHMM which do not seem able to take advantage
of more information for WSJ and Floresta. This
suggests that performance for CDHMM and HMM+
could improve if the training corpora were aug-
mented with out-of-corpus raw data. One exception
to the consistent improvement over increased data is
the performance of the models on Uspanteko, which
uniformly flatline. One reason might be that the tags
are labeled over segmented morphemes instead of
words like the other corpora. Another could be that
Uspanteko has a relatively large number of tags in a
very small corpus.
6 Related work
Unsupervised POS tagging is an active area of re-
search. Most recent work has involved HMMs.
Given that an unconstrained HMM is not well under-
stood in POS tagging, much work has been done on
examining the mechanism and the properties of the
HMM as applied to natural language data (Johnson,
2007; Gao and Johnson, 2008; Headden III et al,
2008). Conversely, there has also been work focused
on improving the HMM as an inference procedure
that looked at POS tagging as an example (Graca et
al., 2009; Liang and Klein, 2009). Nonparametric
HMMs for unsupervised POS tag induction (Snyder
et al, 2008; Van Gael et al, 2009) have seen partic-
ular activity due to the fact that model size assump-
tions are unnecessary and it lets the data ?speak for
itself.?
There is also work on alternative unsupervised
models that are not HMMs (Schu?tze, 1993; Abend
et al, 2010; Reichart et al, 2010b) as well as re-
search on improving evaluation of unsupervised tag-
gers (Frank et al, 2009; Reichart et al, 2010a).
Though they did not concentrate on unsupervised
methods, Haghighi and Klein (2006) conducted an
unsupervised experiment that utilized certain to-
ken features (e.g. character suffixes of 3 or less,
204
has initial capital, etc.; the features themselves are
from Smith and Eisner (2005)) to learn parameters
in an undirected graphical model which was the
equivalent of an HMM in directed models. It was
also the first study to posit the one-to-one evalua-
tion criterion which has been repeated extensively
since (Johnson, 2007; Headden III et al, 2008;
Graca et al, 2009).
Finkel et al (2007) is an interesting variant of un-
supervised POS tagging where a parse tree is as-
sumed and POS tags are induced from this structure
non-parametrically. It is the converse of unsuper-
vised parsing which assumes access to a tagged cor-
pus and induces a parsing model.
Other models more directly influenced or closely
parallel our work. Griffiths et al (2005) is the work
that inspired the current approach where a set of
states is designated to capture variance across con-
texts. The primary goal of that model was to induce
a topic model given data that had not been filtered
of noise in the form of function words. As such,
distinguishing between topic states such that they
model different syntactic states was not attempted,
and we have seen in sec. 3 that such an extension is
not entirely straightforward.4 Boyd-Graber and Blei
(2009) has some parallels to our model in that a hid-
den variable over topics is distributed according to
a normalized product between a context prior and a
syntactic prior. However, it assumes a much greater
amount of information than we do in that a parse tree
as well as (possibly) POS tags are taken as observed.
The model has a very different goal from ours as
well, which is to infer a syntactically informed topic
model. Teichert and Daume? III (2010) is another
study with close similarities to our own. This study
models distinctions between closed class words and
open class words within a modified HMM. It is un-
clear from their formulation how the distinction be-
tween open class and closed class words is learned.
There is also extensive literature on learning se-
quence structure from unlabeled text (Smith and
Eisner, 2005; Goldberg et al, 2008; Ravi and
Knight, 2009) which assume access to a tag dic-
tionary. Goldwater and Griffiths (2007) deserves
mention for examining a semi-supervised model
4We tested a variant of LDAHMM in which more than one
state can generate topics. It did not achieve good results.
that sampled emission hyperparameters for each
state rather than a single symmetric hyperparame-
ter. They showed that this outperformed a symmet-
ric model. An interesting heuristic model is Zhao
and Marcus (2009) that uses a seed set of closed
class words to classify open class words.
7 Conclusion
We have shown that a hidden Markov model that
allocates a subset of the states to have distribu-
tions conditioned on localized domains can signif-
icantly improve performance in unsupervised part-
of-speech tagging. We have also demonstrated that
significant performance gains are possible simply
by setting a different emission hyperparameter for
a subgroup of the states. It is encouraging that these
results hold for both models not just on the WSJ but
across a diverse set of languages and measures.
We believe our proposed extensions to the HMM
are a significant contribution to the general HMM
and unsupervised POS tagging literature in that both
can be implemented with minimum modification
of existing MCMC inferred HMMs, have (nearly)
equivalent run times, produce output that is easy to
interpret since they are based on a generative frame-
work, and bring about considerable performance im-
provements at the same time.
Acknowledgments
The authors would like to thank Elias Ponvert and
the anonymous reviewers. This work was supported
by a grant from the Morris Memorial Trust Fund of
the New York Community Trust.
References
O. Abend, R. Reichart, and A. Rappoport. 2010. Im-
proved unsupervised POS induction through prototype
discovery. In Proceedings of ACL, pages 1298?1307.
S. Afonso, E. Bick, R. Haber, and D. Santos. 2002. Flo-
resta sinta?(c)tica?: a treebank for Portuguese. In Pro-
ceedings of LREC, pages 1698?1703.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
Dirichlet alocation. The Journal of Machine Learning
Research, 3:993?1022.
J. L. Boyd-Graber and D. Blei. 2009. Syntactic topic
models. In Proceedings of NIPS, pages 185?192.
205
S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith.
2002. The TIGER treebank. In Proceedings of the
Workshop on Treebanks and Linguistic Theories.
T. Brants. 2000. TnT: a statistical part-of-speech tag-
ger. In Proceedings of conference on Applied natural
language processing, pages 224?231.
A. Clark. 2003. Combining distributional and morpho-
logical information for part of speech induction. In
Proceedings of EACL, pages 59?66.
J. R. Finkel, T. Grenager, and C. D. Manning. 2007. The
infinite tree. In Proceedings of ACL, pages 272?279.
W.N. Francis, H. Kuc?era, and A.W. Mackie. 1982. Fre-
quency analysis of English usage: Lexicon and gram-
mar. Houghton Mifflin Harcourt.
S. Frank, S. Goldwater, and F. Keller. 2009. Evaluating
models of syntactic category acquisition without using
a gold standard. In Proceedings of CogSci.
J. Gao and M. Johnson. 2008. A comparison of Bayesian
estimators for unsupervised Hidden Markov Model
POS taggers. In Proceedings of EMNLP, pages 344?
352.
Y. Goldberg, M. Adler, and M. Elhadad. 2008. EM
can find pretty good HMM POS-taggers (when given
a good start). In Proceedings of ACL, pages 746?754.
S. Goldwater and T. L. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
Proceedings of ACL, pages 744?751.
J. Graca, K. Ganchev, B. Taskar, and F. Pereira. 2009.
Posterior vs parameter sparsity in latent variable mod-
els. In Proceedings of NIPS, pages 664?672.
T. L. Griffiths, M. Steyvers, D. M. Blei, and J. M. Tenen-
baum. 2005. Integrating topics and syntax. In Pro-
ceedings of NIPS, pages 537?544.
A. Haghighi and D. Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
HLT/NAACL, pages 320?327.
W. P. Headden III, D. McClosky, and E. Charniak.
2008. Evaluating unsupervised part-of-speech tagging
for grammar induction. In Proceedings of COLING,
pages 329?336.
G.E. Hinton. 2002. Training products of experts by min-
imizing contrastive divergence. Neural Computation,
14(8):1771?1800.
M. Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers. In Proceedings of EMNLP-CoNLL,
pages 296?305.
P. Liang and D. Klein. 2009. Online EM for unsuper-
vised models. In Proceedings of HLT/NAACL, pages
611?619.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1994. Building a large annotated corpus of English:
The Penn Treebank. Comp. ling., 19(2):313?330.
M. Meila. 2007. Comparing clusterings?an informa-
tion based distance. Journal of Multivariate Analysis,
98(5):873?895.
A. Palmer, T. Moon, and J. Baldridge. 2009. Evaluat-
ing automation strategies in language documentation.
In Proceedings of the NAACL-HLT 2009 Workshop
on Active Learning for Natural Language Processing,
pages 36?44.
T. C. Pixabaj, M. A. Vicente Me?ndez, M. Vicente
Me?ndez, and O. A. Damia?n. 2007. Text Collections in
Four Mayan Languages. Archived in The Archive of
the Indigenous Languages of Latin America.
S. Ravi and K. Knight. 2009. Minimized models for
unsupervised part-of-speech tagging. In Proceedings
of ACL and AFNLP, pages 504?512.
R. Reichart, O. Abend, and A. Rappoport. 2010a. Type
level clustering evaluation: New measures and a POS
induction case study. In Proceedings of CoNLL, pages
77?87.
R. Reichart, R. Fattal, and A. Rappoport. 2010b. Im-
proved unsupervised POS induction using intrinsic
clustering quality and a Zipfian constraint. In Proceed-
ings of CoNLL, pages 57?66.
H. Schu?tze. 1993. Part-of-speech induction from scratch.
In Proceedings of ACL, pages 251?258.
N.A. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In Pro-
ceedings of ACL, pages 354?362.
B. Snyder, T. Naseem, J. Eisenstein, and R. Barzilay.
2008. Unsupervised multilingual learning for POS
tagging. In Proceedings of EMNLP, pages 1041?
1050.
A.R. Teichert and H. Daume? III. 2010. Unsupervised
Part of Speech Tagging Without a Lexicon. In NIPS
Workshop on Grammar Induction, Representation of
Language and Language Learning 2010.
K. Toutanova, D. Klein, C. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proceedings of
NAACL, pages 173?180.
J. Van Gael, A. Vlachos, and Z. Ghahramani. 2009. The
infinite HMM for unsupervised PoS tagging. In Pro-
ceedings of EMNLP, pages 678?687.
L. Xu, D. Wilkinson, F. Southey, and D. Schuurmans.
2006. Discriminative unsupervised learning of struc-
tured predictors. In Proceedings of ICML, pages
1057?1064.
Q. Zhao and M. Marcus. 2009. A simple unsuper-
vised learner for POS disambiguation rules given only
a minimal lexicon. In Proceedings of EMNLP, pages
688?697.
206
