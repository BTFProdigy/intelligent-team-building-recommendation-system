Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 561?570, Prague, June 2007. c?2007 Association for Computational Linguistics
Generating Lexical Analogies Using Dependency Relations
Andy Chiu, Pascal Poupart, and Chrysanne DiMarco
David R. Cheriton School of Computer Science
University of Waterloo
Waterloo, Ontario, Canada
{pachiu,ppoupart,cdimarco}@uwaterloo.ca
Abstract
A lexical analogy is a pair of word-pairs
that share a similar semantic relation. Lex-
ical analogies occur frequently in text and
are useful in various natural language pro-
cessing tasks. In this study, we present a
system that generates lexical analogies au-
tomatically from text data. Our system dis-
covers semantically related pairs of words
by using dependency relations, and applies
novel machine learning algorithms to match
these word-pairs to form lexical analogies.
Empirical evaluation shows that our system
generates valid lexical analogies with a pre-
cision of 70%, and produces quality output
although not at the level of the best human-
generated lexical analogies.
1 Introduction
Analogy discovery and analogical reasoning are ac-
tive research areas in a multitude of disciplines, in-
cluding philosophy, psychology, cognitive science,
linguistics, and artificial intelligence. A type of anal-
ogy that is of particular interest in natural language
processing is lexical analogy. A lexical analogy is a
pair of word-pairs that share a similar semantic rela-
tion. For example, the word-pairs (dalmatian, dog)
and (trout, fish) form a lexical analogy because dal-
matian is a subspecies of dog just as trout is a sub-
species of fish, and the word-pairs (metal, electric-
ity) and (air, sound) form a lexical analogy because
in both cases the initial word serves as a conductor
for the second word. Lexical analogies occur fre-
quently in text and are useful in various natural lan-
guage processing tasks. For example, understanding
metaphoric language such as ?the printer died? re-
quires the recognition of implicit lexical analogies,
in this case between (printer, malfunction) and (per-
son, death). Lexical analogies also have applica-
tions in word sense disambiguation, information ex-
traction, question-answering, and semantic relation
classification (see (Turney, 2006)).
In this study, we present a novel system for gen-
erating lexical analogies directly from a text cor-
pus without relying on dictionaries or other seman-
tic resources. Our system uses dependency relations
to characterize pairs of semantically related words,
then compares the similarity of their semantic rela-
tions using two machine learning algorithms. We
also present an empirical evaluation that shows our
system generates valid lexical analogies with a pre-
cision of 70%. Section 2 provides a list of defini-
tions, notations, and necessary background materi-
als. Section 3 describes the methods used in our
system. Section 4 presents our empirical evalua-
tion. Section 5 reviews selected related work. Fi-
nally, Section 6 concludes the paper with suggested
future work and a brief conclusion.
2 Definitions
A word-pair is a pair of entities, where each entity
is a single word or a multi-word named entity. The
underlying relations of a word-pair (w1, w2) are the
semantic relations1 between w1 and w2. For exam-
1Here ?semantic relations? include both classical relations
such as synonymy and meronymy, and non-classical relations
as defined by Morris and Hirst (2004).
561
ple, the underlying relations of (poet, poem) include
produces, writes, enjoys, and understands. A lexical
analogy is a pair of word-pairs that share at least one
identical or similar underlying relation.
A key linguistic formalism we use is dependency
grammar (Tesnie`re, 1959). A dependency gram-
mar describes the syntactic structure of a sentence
in a manner similar to the familiar phrase-structure
grammar. However, unlike phrase-structure gram-
mars which associate each word of a sentence to
the syntactic phrase in which the word is contained,
a dependency grammar associates each word to its
syntactic superordinate as determined by a set of
rules. Each pair of depending words is called a
dependency. Within a dependency, the word being
depended on is called the governor, and the word
depending on the governor is called the dependent.
Each dependency is also labelled with the syntac-
tic relation between the governor and the dependent.
Dependency grammars require that each word of a
sentence have exactly one governor, except for one
word called the head word which has no governor at
all. A proposition p that is governor to exactly one
word w1 and dependent of exactly one word w2 is
often collapsed (Lin and Pantel, 2001); that is, the
two dependencies involving p are replaced by a sin-
gle dependency between w1 and w2 labelled p.
The dependency structure of a sentence can be
concisely represented by a dependency tree, in
which each word is a node, each dependent is a child
of its governor, and the head word is the root. A de-
pendency path is an undirected path through a de-
pendency tree, and a dependency pattern is a depen-
dency path with both ends replaced by slots (Lin and
Pantel, 2001). Figure 1 illustrates various depen-
dency structures of the sentence, rebels fired rockets
at a military convoy, after each word is lemmatized.
3 Methods
We consider lexical analogy generation as a se-
quence of two key problems: data extraction and
relation-matching. Data extraction involves the
identification and extraction of pairs of semantically
related words, as well as features that characterize
their relations. Relation-matching involves match-
ing word-pairs with similar features to form lexi-
cal analogies. We describe our methods for solving
these two problems in the following subsections.
3.1 Data Extraction
Extracting Word-Pairs
To identify semantically related words, we rely
on the assumption that highly syntactically related
words also tend to be semantically related ? a hy-
pothesis that is supported by works such as Levin?s
(1993) study of English verbs. As such, the de-
pendency structure of a sentence can be used to ap-
proximate the semantic relatedness between its con-
stituent words. Our system uses a dependency parser
to parse the input text into a set of dependency trees,
then searches through these trees to extract depen-
dency paths satisfying the following constraints:
1. The path must be of the form noun-verb-noun.
2. One of the nouns must be the subject of the
clause to which it belongs.
Each of these paths is then turned into a word-pair
by taking its two nouns. The path constraints that we
use are suggested by the subject-verb-object (SVO)
pattern commonly used in various relation extraction
algorithms. However, our constraints allow signifi-
cantly more flexibility than the SVO pattern in two
important aspects. First, our constraints allow an
arbitrary relation between the verb and the second
noun, not just the object relation. Hence, word-pairs
can be formed from a clause?s subject and its loca-
tion, time, instrument, and other arguments, which
are clearly semantically related to the subject. Sec-
ondly, searching in the space of dependency trees in-
stead of raw text data means that we are able to find
semantically related words that are not necessarily
adjacent to each other in the sentence.
It is important to note that, although these con-
straints improve the precision of our system and tend
to identify effectively the most relevant word-pairs,
they are not strictly necessary. Our system would be
fully functional using alternative sets of constraints
tailored for specific applications, or even with no
constraints at all.
Using the sentence in Figure 1 as an example, our
system would extract the dependency paths ?rebel
subj
? fire
obj
? rocket? and ?rebel
subj
? fire at? con-
voy?, and would thus generate the word-pairs (rebel,
rocket) and (rebel, convoy).
562
Figure 1: Dependency structures of ?rebels fired rockets at a military convoy? after lemmatization
Extracting Features
Recall that each word-pair originates from a de-
pendency path. The path, and in particular the mid-
dle verb, provides a connection between the two
words of the word-pair, and hence is a good in-
dication of their semantic relation. Therefore, for
each word-pair extracted, we also extract the depen-
dency pattern derived from the word-pair?s depen-
dency path as a feature for the word-pair. We further
justify this choice of feature by noting that the use
of dependency patterns have previously been shown
to be effective at characterizing lexico-syntactic re-
lations (Lin and Pantel, 2001; Snow et al, 2004).
Using Figure 1 as an example again, the depen-
dency patterns ?
subj
? fire
obj
? ? and ?
subj
?
fire at? ? would be extracted as a feature of (rebel,
rocket) and (rebel, convoy), respectively.
Filtering
Word-pairs and features extracted using only de-
pendency relations tend to be crude in several as-
pects. First, they contain a significant amount of
noise, such as word-pairs that have no meaningful
underlying relations. Noise comes from grammati-
cal and spelling mistakes in the original input data,
imperfect parsing, as well as the fact that depen-
dency structure only approximates semantic related-
ness. Secondly, some of the extracted word-pairs
contain underlying relations that are too general or
too obscure for the purpose of lexical analogy gen-
eration. For example, consider the word-pair (com-
pany, right) from the sentence ?the company exer-
cised the right to terminate his contract?. The two
words are clearly semantically related, however the
relation (have or entitled-to) is very general and it
is difficult to construct satisfying lexical analogies
from the word-pair. Lastly, some features are also
subject to the same problem. The feature ?
subj
?
say
obj
? ?, for example, has very little characteri-
zation power because almost any pair of words can
occur with this feature.
In order to retain only the most relevant word-
pairs and features, we employ a series of refining
filters. All of our filters rely on the occurrence
statistics of the word-pairs and features. Let W =
{wp1, wp2, ..., wpn} be the set of all word-pairs and
F = {f1, f2, ..., fm} the set of all features. Let Fwp
be the set of features of word-pair wp, and let Wf
be the set of word-pairs associated with feature f .
Let O(wp) be the total number of occurrences of
word-pair wp, O(f) be the total number of occur-
rences of feature f , and O(wp, f) be the number of
occurrences of word-pair wp with feature f . The
following filters are used:
1. Occurrence filter: Eliminate word-pair wp if
O(wp) is less than some constant Kf1 , and
eliminate feature f if O(f) is less than some
constant Kf2 . This filter is inspired by the sim-
ple observation that valid word-pairs and fea-
tures tend to occur repeatedly.
2. Generalization filter: Eliminate feature f if
|Wf | is greater than some constant Kf3 . This
filter ensures that features associated with too
many word-pairs are not kept. A feature that
occurs with many word-pairs tend to describe
overly general relations. An example of such
a feature is ?
subj
? say
obj
? ?, which in
our experiment occurred with several thousand
word-pairs while most features occurred with
less than a hundred.
563
3. Data sufficiency filter: Eliminate word-pair wp
if |Fwp| is less than some constant Kf4 . This
filter ensures that all word-pairs have sufficient
features to be compared meaningfully.
4. Entropy filter: Eliminate word-pair wp if its
normalized entropy is greater than some con-
stant Kf5 . We compute a word-pair?s entropy
by considering it as a distribution over features,
in a manner that is analogous to the feature en-
tropy defined in (Turney, 2006). Specifically,
the normalized entropy of a word-pair wp is:
?
?
f?Fwp p(f |wp) log (p(f |wp))
log |Fwp|
where p(f |wp) = O(wp,f)O(wp) is the conditional
probability of f occurring in the context of wp.
The normalized entropy of a word-pair ranges
from zero to one, and is at its highest when the
distribution of the word-pair?s occurrences over
its features is the most random. The justifica-
tion behind this filter is that word-pairs with
strong underlying relations tend to have just a
few dominant features that characterize those
relations, whereas word-pairs that have many
non-dominant features tend to have overly gen-
eral underlying relations that can be character-
ized in many different ways.
3.2 Relation-Matching
Central to the problem of relation-matching is that of
a relational similarity function: a function that com-
putes the degree of similarity between two word-
pairs? underlying relations. Given such a function,
relation-matching reduces to simply computing the
relational similarity between every pair of word-
pairs, and outputting the pairs scoring higher than
some threshold Kth as lexical analogies. Our sys-
tem incorporates two relational similarity functions,
as discussed in the following subsections.
Latent Relational Analysis
The baseline algorithm that we use to compute
relational similarity is a modified version of Latent
Relational Analysis (LRA) (Turney, 2006), that con-
sists of the following steps:
1. Construct an n-by-m matrix A such that the
ith row maps to word-pair wpi, the jth column
maps to feature fj , and Ai,j = O(wpi, fj).
2. Reduce the dimensionality of A to a con-
stant Ksvd using Singular Value Decomposi-
tion (SVD) (Golub and van Loan, 1996). SVD
produces a matrix A? of rank Ksvd that is the
best approximation of A among all matrices of
rank Ksvd. The use of SVD to compress the
feature space was pioneered in Latent Semantic
Analysis (Deerwester et al, 1990) and has be-
come a popular technique in feature-based sim-
ilarity computation. The compressed space is
believed to be a semantic space that minimizes
artificial surface differences.
3. The relational similarity between two word-
pairs is the cosine measure of their correspond-
ing row vectors in the reduced feature space.
Specifically, let A?i denote the ith row vector of
A?, then the relational similarity between word-
pairs wpi1 and wpi2 is:
A?i1 ? A?i2?
?
?A?i1
?
?
?
2
+
?
?
?A?i2
?
?
?
2
The primary difference between our algorithm
and LRA is that LRA also includes each word?s
synonyms in the computation. Synonym inclusion
greatly increases the size of the problem space,
which leads to computational issues for our system
as it operates at a much larger scale than previous
work in relational similarity. Turney?s (2006) exten-
sive evaluation of LRA on SAT verbal analogy ques-
tions, for example, involves roughly ten thousand re-
lational similarity computations2. In contrast, our
system typically requires millions of relational sim-
ilarity computations because every pair of extracted
word-pairs needs to be compared. We call our algo-
rithm LRA-S (LRA Without Synonyms) to differen-
tiate it from the original LRA.
Similarity Graph Traversal
While LRA has been shown to perform well in
computing relational similarity, it suffers from two
2The study evaluated 374 SAT questions, each involving 30
pairwise comparisons, for a total of 11220 relational similarity
computations.
564
limitations. First, the use of SVD is difficult to inter-
pret from an analytical point of view as there is no
formal analysis demonstrating that the compressed
space really corresponds to a semantic space. Sec-
ondly, even LRA-S does not scale up well to large
data sets due to SVD being an expensive operation
? computing SVD is in generalO(mn?min(m,n))
(Koyuturk et al, 2005), where m, n are the number
of matrix rows and columns, respectively.
To counter these limitations, we propose an alter-
native algorithm for computing relational similarity
? Similarity Graph Traversal (SGT). The intuition
behind SGT is as follows. Suppose we know that
wp1 and wp2 are relationally similar, and that wp2
and wp3 are relationally similar. Then, by transi-
tivity, wp1 and wp3 are also likely to be relation-
ally similar. In other words, the relational similar-
ity between two word-pairs can be reinforced by
other word-pairs through transitivity. The actual al-
gorithm involves the following steps:
1. Construct a similarity graph as follows. Each
word-pair corresponds to a node in the graph.
An edge exists from wp1 to wp2 if and only
if the cosine measure of the two word-pairs?
feature vectors is greater than or equal to some
threshold Ksgt, in which case, the cosine mea-
sure is assigned as the strength of the edge.
2. Define a similarity path of length k, or k-
path, from wp1 to wp2 to be a directed acyclic
path of length k from wp1 to wp2, and de-
fine the strength s(p) of a path p to be the
product of the strength of all of the path?s
edges. Denote the set of all k-paths from wp1
to wp2 as P(k,wp1, wp2), and denote the sum
of the strength of all paths in P(k,wp1, wp2)
as S(k,wp1, wp2).
3. The relational similarity between word-pairs
wpi1 and wpi2 is:
?1S(1, wp1, wp2) +
?2S(2, wp1, wp2) +
. . .
?KlS(Kl, wp1, wp2)
where Kl is the maximum path length to con-
sider, and ?1, . . ., ?Kl are weights that are
learned using least-squares regression on a
small set of hand-labelled lexical analogies.
A natural concern for SGT is that relational simi-
larity is not always transitive, and hence some paths
may be invalid. For example, although (teacher,
student) is relationally similar to both (shepherd,
sheep) and (boss, employee), the latter two word-
pairs are not relationally similar. The reason that
this is not a problem for SGT is because truly simi-
lar word-pairs tend to be connected by many transi-
tive paths, while invalid paths tend to occur in iso-
lation. As such, while a single path may not be in-
dicative, a collection of many paths likely signifies
a true common relation. The weights in step 3 en-
sure that SGT assigns a high similarity score to two
word-pairs only if there are sufficiently many tran-
sitive paths (which are sufficiently strong) between
them.
Analogy Filters
As a final step in both LSA-R and SGT, we fil-
ter out lexical analogies of the form (w1,w2) and
(w1,w3), as such lexical analogies tend to express
the near-synonymy between w2 and w3 more than
they express the relational similarity between the
two word-pairs. We also keep only one permuta-
tion of each lexical analogy: (w1,w2) and (w3,w4),
(w3,w4) and (w1,w2), (w2,w1) and (w4,w3), and
(w4,w3) and (w2,w1) are different permutations of
the same lexical analogy.
4 Evaluation
Our evaluation consisted of two parts. First, we eval-
uated the performance of the system, using LRA-S
for relation-matching. Then, we evaluated the SGT
algorithm, in particular, how it compares to LRA-S.
4.1 System Evaluation
Experimental Setup
We implemented our system in Sun JDK 1.5. We
also used MXTerminator (Reynar and Ratnaparkhi,
1997) for sentence segmentation, MINIPAR (Lin,
1993) for lemmatization and dependency parsing,
and MATLAB3 for SVD computation. The exper-
iment was conducted on a 2.1 GHz processor, with
3http://www.mathworks.com
565
the exception of SVD computation which was car-
ried out in MATLAB running on a single 2.4 GHz
processor within a 64-processor cluster. The input
corpus consisted of the following collections in the
Text Retrieval Conference Dataset4: AP Newswire
1988?1990, LA Times 1989?1990, and San Jose
Mercury 1991. In total, 1196 megabytes of text data
were used for the experiment. Table 1 summarizes
the running times of the experiment.
Process Time
Sentence Segmentation 20 min
Dependency Parsing 2232 min
Data Extraction 138 min
Relation-Matching 65 min
Table 1: Experiment Running Times
The parameter values selected for the experiment
are listed in Table 2. The filter parameters were se-
lected mostly through trial-and-error ? various pa-
rameter values were tried and filtration results exam-
ined. We used a threshold valueKth = 0.80 to gener-
ate the lexical analogies, but the evaluation was per-
formed at ten different thresholds from 0.98 to 0.80
in 0.02 decrements.
Kf1 Kf2 Kf3 Kf4 Kf5 Ksvd
35 10 100 10 0.995 600
Table 2: Experiment Parameter Values
Evaluation Protocol
An objective evaluation of our system is difficult
for two reasons. First, lexical analogies are by defi-
nition subjective; what constitutes a ?good? lexical
analogy is debatable. Secondly, there is no gold
standard of lexical analogies to which we can com-
pare. For these reasons, we adopted a subjective
evaluation protocol that involved human judges rat-
ing the quality of the lexical analogies generated.
Such a manual evaluation protocol, however, meant
that it was impractical to evaluate the entire output
set (which was well in the thousands). Instead, we
evaluated random samples from the output and in-
terpolated the results.
4http://trec.nist.gov/
In total, 22 human judges participated in the eval-
uation. All judges were graduate or senior under-
graduate students in English, Sociology, or Psychol-
ogy, and all were highly competent English speak-
ers. Each judge was given a survey containing 105
lexical analogies, 100 of which were randomly sam-
pled from our output, and the remaining five were
sampled from a control set of ten human-generated
lexical analogies. All entries in the control set were
taken from the Verbal Analogy section of the Stan-
dard Aptitude Test5 and represented the best possi-
ble lexical analogies. The judges were instructed to
grade each lexical analogy with a score from zero to
10, with zero representing an invalid lexical analogy
(i.e., when the two word-pairs share no meaningful
underlying relation) and ten representing a perfect
lexical analogy. To minimize inter-judge subjectiv-
ity, all judges were given detailed instructions con-
taining the definition and examples of lexical analo-
gies. In all, 1000 samples out of the 8373 generated
were graded, each by at least two different judges.
We evaluated the output at ten threshold values,
from 0.98 to 0.80 in 0.02 decrements. For each
threshold, we collected all samples down to that
threshold and computed the following metrics:
1. Coverage: The number of lexical analogies
generated at the current threshold over the
number of lexical analogies generated at the
lowest threshold (8373).
2. Precision: The proportion of samples at the
current threshold that scored higher than three.
These are considered valid lexical analogies.
Note that this is significantly more conservative
than the survey scoring. Wewant to ensure very
poor lexical analogies were excluded, even if
they were ?valid? according to the judges.
3. Quality: The average score of all samples at the
current threshold, divided by ten to be in the
same scale as the other metrics.
4. Goodness: The proportion of samples at the
current threshold that scored within 10% of the
average score of the control set. These are con-
sidered human quality.
5http://www.collegeboard.com/
566
Note that recall was not an evaluation metric be-
cause there does not exist a method to determine the
true number of lexical analogies in the input corpus.
Result
Table 3 summarizes the result of the control set,
and Figure 2:Left summarizes the result of the lex-
ical analogies our system generated. Table 4 lists
some good and some poor lexical analogies our sys-
tem generated, along with some of their shared fea-
tures.
Coverage Precision Quality Goodness
N/A 1.00 0.97 0.90
Table 3: Result of the Control Set
As Figure 2 shows, our system performed fairly
well, generating valid lexical analogies with a preci-
sion around 70%. The quality of the generated lex-
ical analogies was reasonable, although not at the
level of human-generation. On the other hand, a
small portion (19% at the highest threshold) of our
output was of very high quality, comparable to the
best human-generated lexical analogies.
Our result also showed that there was a correspon-
dence between the score our system assigned to each
generated lexical analogy and its quality. Precision,
quality, and goodness all declined steadily toward
lower thresholds: precision 0.70?0.66, quality 0.54?
0.49, and goodness 0.19?0.14.
Error Analysis
Despite our aggressive filtration of irrelevant
word-pairs and features, noise was still the most sig-
nificant problem in our output. Most low-scoring
samples contained at least one word-pair that did not
have a meaningful and clear underlying relation; for
examples, (guy, ball) and (issue, point). As men-
tioned, noise originated from mistakes in the input
data, errors in sentence segmentation and parsing,
as well as mismatches between dependencies and
semantic relatedness. An example of the latter in-
volved the frequent usage of the proposition ?of ?
in various constructs. In the sentence ?the com-
pany takes advantage of the new legislation?, for
example, the dependency structure associates com-
pany with advantage, whereas the semantic relation
clearly lies between company and legislation. All
three of our evaluation metrics (precision, quality,
and goodness) were negatively affected by noise.
Polysemic words, as well as words which were
heavily context-dependent, also posed a problem.
For example, one of the lexical analogies generated
in the experiment was (resolution, house) and (leg-
islation, senate). This lexical analogy only makes
sense if ?house? is recognized as referring to the
House of Representatives, which is often abbrevi-
ated as ?the House? in news articles. Polysemy also
negatively affected all three of our evaluation met-
rics, although to a lesser extent for precision.
Finally, our system had difficulties differentiat-
ing semantic relations of different granularity. The
underlying relations of (relation, country) and (tie,
united states), for example, are similar, yet they do
not form a good lexical analogy because the rela-
tions are at different levels of granularity (countries
in general in the former, and a particular country
in the latter). Undifferentiated granularity affected
quality and goodness, but it did not have a signifi-
cant effect on precision.
4.2 SGT Evaluation
To evaluate how SGT compares to LRA-S, we
repeated the experiment using SGT for relation-
matching. We set Kl (maximum path length) to 3,
andKsgt (cosine threshold) to 0.2; these values were
again determined largely through trial-and-error. To
train SGT, we used 90 lexical analogies graded by
human judges from the previous experiment. In or-
der to facilitate a fair comparison to LRA-S, we se-
lected Kth values that allowed SGT to generate the
same number of lexical analogies as LRA-S did at
each threshold interval.
Running on the same 2.1 GHz processor, SGT
finished in just over eight minutes, which is almost
a magnitude faster than LRA-S? 65 minutes. SGT
also used significantly less memory, as the similar-
ity graph was efficiently stored in an adjacency list.
The sets of lexical analogies generated by the two
algorithms were quite similar, overlapping approxi-
mately 50% at all threshold levels.
The significant overlap between SGT and LRA-S?
outputs allowed us to evaluate SGT using the sam-
ples collected from the previous surveys instead of
conducting a new round of human grading. Specifi-
cally, we identified previously graded samples that
567
Figure 2: System Evaluation Results
Good Examples Shared Features
(vietnam, cambodia) and (iraq, kuwait)
subj
? invade
obj
? ,
subj
? pull out
of
?
(building, office) and (museum, collection)
subj
? house
obj
? ,
subj
? consolidate
obj
?
(stock market, rally) and (student, march)
subj
? stage
obj
?
(researcher, experiment) and (doctor, surgery)
subj
? perform
obj
?
(gainer, loser) and (decline, advance)
subj
? outnumber
obj
?
(book, shelf ) and (picture, wall) with? line
subj
? ,
subj
? remain on?
(blast, car) and (sanction, economy)
subj
? damage
obj
? ,
by
? destroy
subj
?
Poor Examples Shared Features
(president, change) and (bush, legislation)
subj
? veto
obj
?
(charge, death) and (lawsuit, federal court)
subj
? file in?
(relation, country) and (tie, united states)
obj
? severe
subj
?
(judge, term) and (member, life)
subj
? sentence to?
(issue, point) and (stock, cent)
subj
? be down? ,
subj
? be
up
?
Table 4: Examples of Good and Poor Lexical Analogies Generated
had also been generated by SGT, and used these
samples as the evaluation data points for SGT. At the
lowest threshold (where 8373 lexical analogies were
generated), we were able to reuse 533 samples out
of the original 1000 samples. Figure 2:Right sum-
marizes the performance of the system using SGT
for relation-matching.
As the figure shows, SGT performed very simi-
larly to LRA-S. Both SGT?s precision and quality
scores were slightly higher than LRA-S, but the dif-
ferences were very small and hence were likely due
to sample variation. The goodness scores between
the two algorithms were also comparable. In the
case of SGT, however, the score fluctuated instead
of monotonically decreased. We attribute the fluctu-
ation to the smaller sample size.
As the samples were drawn exclusively from the
portion of SGT?s output that overlapped with LRA-
S? output, we needed to ensure that the samples were
not strongly biased and that the reported result was
not better than SGT?s actual performance. To val-
idate the result, we conducted an additional experi-
ment involving a single human judge. The judge was
given a survey with 50 lexical analogies, 25 of which
were sampled from the overlapping portion of SGT
and LRA-S? outputs, and 25 from lexical analogies
generated only by SGT. Table 5 summarizes the re-
sult of this experiment. As the table demonstrates,
568
the results from the two sets were comparable with
small differences. Moreover, the differences were
in favour of the SGT-only portion. Therefore, either
there was no sampling bias at all, or the sampling
bias negatively affected the result. As such, SGT?s
actual performance was at least as good as reported,
and may have been slightly higher.
Precision Quality Goodness
Overlap 0.76 0.56 0.28
SGT-Only 0.88 0.62 0.2
Table 5: Overlap vs. SGT-Only
We conclude that SGT is indeed a viable alter-
native to LRA-S. SGT generates lexical analogies
that are of the same quality as LRA-S, while be-
ing significantly faster and more scalable. On the
other hand, an obvious limitation of SGT is that it is
a supervised algorithm requiring manually labelled
training data. We claim this is not a severe limitation
because there are only a few variables to train (i.e.,
the weights), hence only a small set of training data
is required. Moreover, a supervised algorithm can
be advantageous in some situations; for example, it
is easier to tailor SGT to a particular input corpus.
5 Related Work
The study of analogy in the artificial intelligence
community has historically focused on computa-
tional models of analogy-making. French (2002)
and Hall (1989) provide two of the most complete
surveys of such models. Veale (2004; 2005) gen-
erates lexical analogies from WordNet (Fellbaum,
1998) and HowNet (Dong, 1988) by dynamically
creating new type hierarchies from the semantic
information stored in these lexicons. Unlike our
corpus-based generation system, Veale?s algorithms
are limited by the lexicons in which they oper-
ate, and generally are only able to generate near-
analogies such as (Christian, Bible) and (Muslim,
Koran). Turney?s (2006) Latent Relational Analy-
sis is a corpus-based algorithm that computes the re-
lational similarity between word-pairs with remark-
ably high accuracy. However, LRA is focused solely
on the relation-matching problem, and by itself is in-
sufficient for lexical analogy generation.
6 Conclusion and Future Work
We have presented a system that is, to the best of our
knowledge, the first system capable of generating
lexical analogies from unstructured text data. Em-
pirical evaluation shows that our system performed
fairly well, generating valid lexical analogies with
a precision of about 70%. The quality of the gen-
erated lexical analogies was reasonable, although
not at the level of human performance. As part
of the system, we have also developed a novel al-
gorithm for computing relational similarity that ri-
vals the performance of the current state-of-the-art
while being significantly faster and more scalable.
One of our immediate tasks is to complement depen-
dency patterns with additional features. In particu-
lar, we expect semantic features such as word defini-
tions from machine-readable dictionaries to improve
our system?s ability to differentiate between differ-
ent senses of polysemic words, as well as different
granularities of semantic relations. We also plan to
take advantage of our system?s flexibility and relax
the constraints on dependency paths so as to gen-
erate more-varied lexical analogies, e.g., analogies
involving verbs and adjectives.
A potential application of our system, and the
original inspiration for this research, would be to
use the system to automatically enrich ontologies
by spreading semantic relations between lexical ana-
logues. For example, if words w1 and w2 are related
by relation r, and (w1, w2) and (w3, w4) form a lex-
ical analogy, then it is likely that w3 and w4 are also
related by r. A dictionary of lexical analogies there-
fore would allow an ontology to grow from a small
set of seed relations. In this way, lexical analogies
become bridges through which semantic relations
flow in a sea of ontological concepts.
Acknowledgments
We thank the reviewers of EMNLP 2007 for valu-
able comments and suggestions. This work was sup-
ported in part by the Ontario Graduate Scholarship
Program, Ontario Innovation Trust, Canada Foun-
dation for Innovation, and the Natural Science and
Engineering Research Council of Canada.
569
References
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science, 41:391?
407.
Dong Zhen Dong. 1988. What, how and who? Pro-
ceedings of the International Symposium on Electronic
Dictionaries. Tokyo, Japan.
Christine Fellbaum, editor. 1998. WordNet ? An Elec-
tronic Lexical Database. MIT Press.
Robert French. 2002. The computational modeling
of analogy-making. Trends in Cognitive Sciences,
6(5):200?205.
Gene Golub and Charles van Loan. 1996. Matrix Com-
putations. Johns Hopkins University Press, third edi-
tion.
Rogers Hall. 1989. Computational approaches to ana-
logical reasoning: A comparative analysis. Artificial
Intelligence, 39:39?120.
Mehmet Koyuturk, Ananth Grama, and Naren Ramakr-
ishnan. 2005. Compression, clustering, and pattern
discovery in very high-dimensional discrete-attribute
data sets. IEEE Transactions on Knowledge and Data
Engineering, 17(4):447?461.
Beth Levin 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press.
Dekang Lin. 1993. Principle-based parsing without
overgeneration. Proceedings of the 31st Annual Meet-
ing on ACL, pp 112?120. Columbus, USA.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4):343?360.
Jane Morris and Graeme Hirst. 2004. Non-classical lex-
ical semantic relations. Proceedings of the Compu-
tational Lexical Semantics Workshop at HLT-NAACL
2004, pp 46?51. Boston, USA.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
maximum entropy approach to identifying sentence
boundaries. Proceedings of the 5th Conference on Ap-
plied Natural Language Processing, pp 16?19. Wash-
ington, USA.
Gerard Salton, A. Wong, and C.S. Yang. 1975. A vector
space model for automatic indexing. Communications
of the ACM, 13(11):613?620.
Rion Snow, Daniel Jurafsky, and Andrew Ng. 2004.
Learning syntactic patterns for automatic hypernym
discovery. Proceedings of the 2004 Neural Infor-
mation Processing Systems Conference. Vancouver,
Canada.
Lucien Tesnie`re. 1959. E?le?ments de Syntaxe Structurale.
Librairie C. Klincksieck, Paris.
Peter D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379?416.
Tony Veale, Jer Hayes, and Nuno Seco. 2004. The Bible
is the Christian Koran: Discovering simple analogical
compounds. Proceedings of the Workshop on Com-
putational Creativity in 2004 European Conference on
Case-Based Reasoning. Madrid, Spain.
Tony Veale. 2005. Analogy generation with HowNet.
Proceedings of the 2005 International Joint Confer-
ence on Artificial Intelligence. Edinburgh, Scotland.
570
A Design Methodology for a Biomedical Literature Indexing Tool
Using the Rhetoric of Science
Robert E. Mercer
University of Western Ontario,
London, Ontario, N6A 5B7
mercer@csd.uwo.ca
Chrysanne Di Marco
University of Waterloo,
Waterloo, Ontario, N2L 3G1
cdimarco@uwaterloo.ca
Abstract
Literature indexing tools provide re-
searchers with a means to navigate
through the network of scholarly scientific
articles in a subject domain. We propose
that more effective indexing tools may be
designed using the links between articles
provided by citations.
With the explosion in the amount of sci-
entific literature and with the advent of ar-
tifacts requiring more sophisticated index-
ing, a means to provide more information
about the citation relation in order to give
more intelligent control to the navigation
process is warranted. In order to navigate
a citation index in this more-sophisticated
manner, the citation index must provide
not only the citation-link information, but
also must indicate the function of the cita-
tion. The design methodology of an in-
dexing tool for scholarly biomedical lit-
erature which uses the rhetorical context
surrounding the citation to provide the ci-
tation function is presented. In particular,
we discuss how the scientific method is re-
flected in scientific writing and how this
knowledge can be used to decide the pur-
pose of a citation.
1 Introduction
1.1 The aim of citation indexing
Indexing tools, such as CiteSeer (Bollacker et al, 1999),
play an important role in the scientific endeavour by
providing researchers with a means to navigate through
the network of scholarly scientific papers using the con-
nections provided by citations. Citations relate articles
within a research field by linking together works whose
methods and results are in some way mutually relevant.
Customarily, authors include citations in their papers to
indicate works that are foundational in their field, back-
ground for their own work, or representative of comple-
mentary or contradictory research. Another researcher
may then use the presence of citations to locate articles
she needs to know about when entering a new field or to
read in order to keep track of progress in a field where she
is already well-established. But, with the explosion in the
amount of scientific literature, a means to provide more
information in order to give more intelligent control to the
navigation process is warranted. A user normally wants
to navigate more purposefully than ?Find all articles cit-
ing a source article?. Rather, the user may wish to know
whether other experiments have used similar techniques
to those used in the source article, or whether other works
have reported conflicting experimental results. In order to
navigate a citation index in this more-sophisticated man-
ner, the citation index must contain not only the citation-
link information, but also must indicate the function of
the citation in the citing article.
The goal of our research project is the design and im-
plementation of an indexing tool for scholarly biomedical
literature which uses the text surrounding the citation to
provide information about the binary relation between the
two papers connected by a citation. In particular, we are
interested in how the scientific method structures the way
in which ideas, results, theories, etc. are presented in sci-
entific writing and how the style of presentation indicates
the purpose of citations, that is, what the relationship is
between the cited and citing papers.
Our interest in the connection between scientific lit-
erature (our focus), ontologies, and databases is that the
content and structure of each of these three repositories
of scientific knowledge has its foundations in the method
of science. Our purpose here is twofold: to make explicit
our design methodology for an indexing tool that uses
                                            Association for Computational Linguistics.
                   Linking Biological Literature, Ontologies and Databases, pp. 77-84.
                                                HLT-NAACL 2004 Workshop: Biolink 2004,
the rhetoric of science as its foundation to see whether the
ideas that underly our methodology can cross-fertilize the
enquiry into the other two areas, and to discuss the tool
itself with the purpose of making known that there exists
a working tool which can assist the development of other
projects.
A citation may be formally defined as a portion of a
sentence in a citing document which references another
document or a set of other documents collectively. For
example, in sentence 1 below, there are two citations:
the first citation is Although the 3-D structure. . . progress,
with the set of references (Eger et al, 1994; Kelly, 1994);
the second citation is it was shown. . . submasses with the
single reference (Coughlan et al, 1986).
(1) Although the 3-D structure analysis by x-ray
crystallography is still in progress (Eger et al,
1994; Kelly, 1994), it was shown by electron
microscopy that XO consists of three submasses
(Coughlan et al, 1986).
A citation index enables efficient retrieval of docu-
ments from a large collection?a citation index consists
of source items and their corresponding lists of biblio-
graphic descriptions of citing works. The use of citation
indexing of scientific articles was invented by Dr. Eugene
Garfield in the 1950s as a result of studies on problems
of medical information retrieval and indexing of biomed-
ical literature. Dr. Garfield later founded the Institute
for Scientific Information (ISI), whose Science Citation
Index (Garfield, no date) is now one of the most popu-
lar citation indexes. Recently, with the advent of digi-
tal libraries, Web-based indexing systems have begun to
appear (e.g., ISI?s ?Web of Knowledge?, CiteSeer (Bol-
lacker et al, 1999)).
Authors of scientific papers normally include citations
in their papers to indicate works that are connected in an
important way to their paper. Thus, a citation connect-
ing the source document and a citing document serves
one of many functions. For example, one function is that
the citing work gives some form of credit to the work
reported in the source article. Another function is to
criticize previous work. Other functions include: foun-
dational works in their field, background for their own
work, works which are representative of complementary
or contradictory research.
The aim of citation analysis studies has been to cate-
gorize and, ultimately, to classify the function of scien-
tific citations automatically. Many citation classification
schemes have been developed, with great variance in the
number and nature of categories used. Garfield (1965)
was the first to define a classification scheme, while
Finney (1979) was the first to suggest that a citation clas-
sifier could be automated. Other classification schemes
include those by Cole (1975), Duncan, Anderson, and
McAleese (1981), Frost (1979), Lipetz (1965), Moravc-
sik and Murugesan (1975), Peritz (1983), Small (1978),
Spiegel-Ro?sing (1977), and Weinstock (1971). Within
this representative group of classification schemes, the
number of categories ranges from four to 26. Examples
of these categories include a contrastive, supportive, or
corrective relationship between citing and cited works.
But, the author?s purpose for including a citation is not
apparent in the citation per se. Determining the nature
of the exact relationship between a citing and cited paper
often requires some level of understanding of the text in
which the citation is embedded.
1.2 Citation indexing in biomedical literature
analysis
In the biomedical field, we believe that the usefulness
of automated citation classification in literature indexing
can be found in both the larger context of managing entire
databases of scientific articles or for specific information-
extraction problems. On the larger scale, database cura-
tors need accurate and efficient methods for building new
collections by retrieving articles on the same topic from
huge general databases. Simple systems (e.g., (Andrade
and Valencia, 1998), (Marcotte et al, 2001)) consider
only keyword frequencies in measuring article similarity.
More-sophisticated systems, such as the Neighbors utility
(Wilbur and Coffee, 1994), may be able to locate articles
that appear to be related in some way (e.g., finding related
Medline abstracts for a set of protein names (Blaschke et
al., 1999)), but the lack of specific information about the
nature and validity of the relationship between articles
may still make the resulting collection a less-than-ideal
resource for subsequent analysis. Citation classification
to indicate the nature of the relationships between articles
in a database would make the task of building collections
of related articles both easier and more accurate. And, the
existence of additional knowledge about the nature of the
linkages between articles would greatly enhance naviga-
tion among a space of documents to retrieve meaningful
information about the related content.
A specific problem in information extraction that may
benefit from the use of citation categorization involves
mining the literature for protein-protein interactions (e.g.,
(Blaschke et al, 1999), (Marcotte et al, 2001), (Thomas
et al, 2000)). Currently, even the most-sophisticated sys-
tems are not yet capable of dealing with all the difficult
problems of resolving ambiguities and detecting hidden
knowledge. For example, Blaschke et al?s system (1999)
is able to handle fairly complex problems in detecting
protein-protein interactions, including constructing the
network of protein interactions in cell-cycle control, but
important implicit knowledge is not recognized. In the
case of cell-cycle analysis for Drosophila, their system is
able to determine that relationships exist between Cak,
Cdk7, CycH, and Cdk2: Cak inhibits/phosphorylates
Cdk7, Cak activates/phosphorylates Cdk2, Cdk7 phos-
phorylates Cdk2, CycH phosphorylates Cak and CycH
phosphorylates Cdk2. However, the system is not able
to detect that Cak is actually a complex formed by Cdk7
and CycH, and that the Cak complex regulates Cdk2.
While the earlier literature describes inter-relationships
among these proteins, the recognition of the generaliza-
tion in their structure, i.e., that these proteins are part
of a complex, is contained only in more-recent articles:
?There is an element of generalization implicit in later
publications, embodying previous, more dispersed find-
ings. A clear improvement here would be the generation
of associated weights for texts according to their level
of generality? (Blaschke et al, 1999). Citation catego-
rization could provide just these kind of ?ancestral? re-
lationships between articles?whether an article is foun-
dational in the field or builds directly on closely related
work?and, if automated, could be used in forming col-
lections of articles for study that are labelled with ex-
plicit semantic and rhetorical links to one another. Such
collections of semantically linked articles might then be
used as ?thematic? document clusters (cf. Wilbur (2002))
to elicit much more meaningful information from docu-
ments known to be closely related.
An added benefit of having citation categories avail-
able in text corpora used for studies such as extract-
ing protein-protein interactions is that more, and more-
meaningful, information may be obtained. In a potential
application for our research, Blaschke et al (1999) noted
that they were able to discover many more protein-protein
interactions when including in the corpus those articles
found to be related by the Neighbors facility (Wilbur and
Coffee, 1994) (285 versus only 28 when relevant protein
names alone were used in building the corpus). Lastly,
very difficult problems in scientific and biomedical infor-
mation extraction that involve aspects of deep-linguistic
meaning may be resolved through the availability of cita-
tion categorization in curated texts: synonym detection,
for example, may be enhanced if different names for the
same entity occur in articles that can be recognized as
being closely related in the scientific research process.
2 Our Guiding Principles
2.1 Scientific writing and the rhetoric of science
The automated labelling of citations with a specific ci-
tation function requires an analysis of the linguistic fea-
tures in the text surrounding the citation, coupled with
a knowledge of the author?s pragmatic intent in placing
the citation at that point in the text. The author?s pur-
pose for including citations in a research article reflects
the fact that researchers wish to communicate their results
to their scientific community in such a way that their re-
sults, or knowledge claims, become accepted as part of
the body of scientific knowledge. This persuasive na-
ture of the scientific research article, how it contributes to
making and justifying a knowledge claim, is recognized
as the defining property of scientific writing by rhetori-
cians of science, e.g., (Gross, 1996), (Gross et al, 2002),
(Hyland, 1998), (Myers, 1991). Style (lexical and syntac-
tic choice), presentation (organization of the text and dis-
play of the data), and argumentation structure are noted as
the rhetorical means by which authors build a convincing
case for their results.
Our approach to automated citation classification is
based on the detection of fine-grained linguistics cues in
scientific articles that help to communicate these rhetori-
cal stances and thereby map to the pragmatic purpose of
citations. As part of our overall research methodology,
our goal is to map the various types of pragmatic cues
in scientific articles to rhetorical meaning. Our previous
work has described the importance of discourse cues in
enhancing inter-article cohesion signalled by citation us-
age (Mercer and Di Marco, 2003), (Di Marco and Mercer,
2003). We have also been investigating another class of
pragmatic cues, hedging cues, (Mercer, Di Marco, and
Kroon, 2004), that are deeply involved in creating the
pragmatic effects that contribute to the author?s knowl-
edge claim by linking together a mutually supportive net-
work of researchers within a scientific community.
2.2 Results of our previous studies
In our preliminary study (Mercer and Di Marco, 2003),
we analyzed the frequency of the cue phrases from
(Marcu, 1997) in a set of scholarly scientific articles. We
reported strong evidence that these cue phrases are used
in the citation sentences and the surrounding text with
the same frequency as in the article as a whole. In sub-
sequent work (Di Marco and Mercer, 2003), we analyzed
the same dataset of articles to begin to catalogue the fine-
grained discourse cues that exist in citation contexts. This
study confirmed that authors do indeed have a rich set
of linguistic and non-linguistic methods to establish dis-
course cues in citation contexts.
Another type of linguistic cue that we are studying is
related to hedging effects in scientific writing that are
used by an author to modify the affect of a ?knowledge
claim?. Hedging in scientific writing has been exten-
sively studied by Hyland (1998), including cataloging the
pragmatic functions of the various types of hedging cues.
As Hyland (1998) explains, ?[Hedging] has subsequently
been applied to the linguistic devices used to qualify a
speaker?s confidence in the truth of a proposition, the kind
of caveats like I think, perhaps, might, and maybe which
we routinely add to our statements to avoid commitment
to categorical assertions. Hedges therefore express tenta-
tiveness and possibility in communication, and their ap-
propriate use in scientific discourse is critical (p. 1)?.
The following examples illustrate some of the ways in
which hedging may be used to deliberately convey an atti-
tude of uncertainty or qualifification. In the first example,
the use of the verb suggested hints at the author?s hesi-
tancy to declare the absolute certainty of the claim:
(2) The functional significance of this modulation
is suggested by the reported inhibition of MeSo-
induced differentiation in mouse erythroleukemia
cells constitutively expressing c-myb.
In the second example, the syntactic structure of the sen-
tence, a fronted adverbial clause, emphasizes the effect
of qualification through the rhetorical cue Although. The
subsequent phrase, a certain degree, is a lexical modifier
that also serves to limit the scope of the result:
(3) Although many neuroblastoma cell lines show
a certain degree of heterogeneity in terms of neu-
rotransmitter expression and differentiative po-
tential, each cell has a prevalent behavior in re-
sponse to differentiation inducers.
In Mercer (2004), we showed that the hedging cues pro-
posed by Hyland occur more frequently in citation con-
texts than in the text as a whole. With this information
we conjecture that hedging cues are an important aspect
of the rhetorical relations found in citation contexts and
that the pragmatics of hedges may help in determining
the purpose of citations.
We investigated this hypothesis by doing a frequency
analysis of hedging cues in citation contexts in a corpus
of 985 biology articles. We obtained statistically signifi-
cant results (summarized in Table 1 indicating that hedg-
ing is used more frequently in citation contexts than the
text as a whole. Given the presumption that writers make
stylistic and rhetorical choices purposefully, we propose
that we have further evidence that connections between
fine-grained linguistic cues and rhetorical relations exist
in citation contexts.
Table 1 shows the proportions of the various types
of sentences that contain hedging cues, broken down by
hedging-cue category (verb or nonverb cues), according
to the different sections in the articles (background, meth-
ods, results and discussion, conclusions). For all but one
combination, citation sentences are more likely to contain
hedging cues than would be expected from the overall fre-
quency of hedge sentences (  ). Citation ?window?
sentences (i.e., sentences in the text close to a citation)
generally are also significantly ( 	
  ) more likely to
contain hedging cues than expected, though for certain
combinations (methods, verbs and nonverbs; res+disc,
verbs) the difference was not significant.
Tables 2, 3, and 4 summarize the occurrence of hedg-
ing cues in citation ?contexts? (a citation sentence and the
surrounding citation window). Table 5 shows the propor-
tion of hedge sentences that either contain a citation, or
fall within a citation window; Table 5 suggests (last 3-
column column) that the proportion of hedge sentences
containing citations or being part of citation windows is
at least as great as what would be expected just by the
distribution of citation sentences and citation windows.
Table 1 indicates (statistically significant) that in most
cases the proportion of hedge sentences in the cita-
tion contexts is greater than what would be expected
by the distribution of hedge sentences. Taken together,
these conditional probabilities support the conjecture that
hedging cues and citation contexts correlate strongly. Hy-
land (1998) has catalogued a variety of pragmatic uses of
hedging cues, so it is reasonable to speculate that these
uses can be mapped to the rhetorical meaning of the text
surrounding a citation, and from thence to the function of
the citation.
3 Our Design Methodology
3.1 The Tool
The indexing tool that we are designing enhances a stan-
dard citation index by labelling each citation with the
function of that citation. That is, given an agreed-upon
set of citation functions, our tool will categorize a cita-
tion automatically into one of these functional categories.
To accomplish this automatic categorization we are using
a decision tree: given a set of features, which combina-
tions of features map to which citation function. Our cur-
rent focus is the biomedical literature, but we are certain
that our tool can be used for the experimental sciences.
We are not certain whether the tool can be generalized
beyond this corpus (Frost, 1979).
In the following we describe in more detail the three as-
pects of our design methodology: the research program,
the tool implementation, and its evaluation. Our basic
assumption is that citations form links to other articles
for much the same purpose and in much the same way as
links to other parts of the same article. These intra-textual
and inter-textual linkages are made to create a coherent
presentation to convince the reader that the content of the
article is of value. The presentation is made cohesive by
use of linguistic and stylistic devices that have been cata-
logued by rhetoricians and which we believe may be de-
tected by automated means.
The research program will
 develop a catalogue of linguistic and non-
linguistic cues that capture both the linguistic
and stylistic techniques as well as the extensive
body of knowledge that has accumulated about
the rhetoric of science and how science is written
about;
Table 1: Proportion of sentences containing hedging cues, by type of sentence and hedging cue category.
Verb Cues Nonverb Cues All Cues
Cite Wind All Cite Wind All Cite Wind All
background 0.15 0.11 0.13 0.13 0.13 0.12 0.25 0.22 0.24
methods 0.09 0.06 0.06 0.05 0.04 0.04 0.14 0.10 0.09
res+disc 0.22 0.16 0.16 0.15 0.14 0.14 0.32 0.27 0.27
conclusions 0.29 0.22 0.20 0.18 0.19 0.15 0.42 0.36 0.32
Table 2: Number and proportion of citation contexts containing a hedging cue, by section and location of hedging cue.
Contexts Sentences Windows
# % # % # %
background 3361 0.33 2575 0.25 2679 0.26
methods 1089 0.18 801 0.14 545 0.09
res+disc 7257 0.44 5366 0.32 4660 0.28
conclusions 338 0.58 245 0.42 221 0.38
 develop computationally realizable methods to
detect these cues;
 connect these cues to rhetorical relations; and
 organize the knowledge that these rhetorical rela-
tions represent as features in a decision tree that
produces the intended function of the citation.
Our purpose in using a decision tree is three-fold.
Firstly, the decision tree gives us ready access to the
citation-function decision rules. Secondly, we aim to
have a working indexing tool whenever we add more
knowledge to the categorization process. This goal ap-
pears very feasible given our design choice to use a
decision tree: adding more knowledge only refines the
decision-making procedure of the previous version. And
thirdly, as we gain more experience (currently, we are
building the decision tree by hand), we intend to use ma-
chine learning techniques to enhance our tool by inducing
a decision tree.
3.2 The Research Program
Our basic assumption is that the rhetorical relations that
will provide the information that will allow the tool to cat-
egorize the citations in a biomedical article are evident to
the reader through the use of surface linguistic cues, cues
which are linguistically-based but require some knowl-
edge that is not directly derivable from the text, and some
cues which are known to the culture of scientific readers-
writers because of the practice of science and how this
practice influences communication through the writing.
We rely on the notion that rhetorical information is
realized in linguistic ?cues? in the text, some of which,
although not all, are evident in surface features (cf. Hy-
land (1998) on surface hedging cues in scientific writing).
Since we anticipate that many such cues will map to the
same rhetorical features that give evidence of the text?s
argumentative and pragmatic meaning, and that the inter-
action of these cues will likely influence the text?s overall
rhetorical effect, the formal rhetorical relation (cf. (Mann
and Thompson, 1988)) appears to be the appropriate fea-
ture for the basis of the decision tree. So, our long-term
goal is to map between the textual cues and rhetorical re-
lations. Having noted that many of the cue words in the
prototype are discourse cues, and with two recent impor-
tant works linking discourse cues and rhetorical relations
((Knott, 1996; Marcu, 1997)), we began our investigation
of this mapping with discourse cues. We have some early
results that show that discourse cues are used extensively
with citations and that some cues appear much more fre-
quently in the citation context than in the full text (Mercer
and Di Marco, 2003). Another textual device is the hedg-
ing cue, which we are currently investigating (Mercer, Di
Marco, and Kroon, 2004).
Although our current efforts focus on cue words which
are connected to organizational effects (discourse cues),
and writer intent (hedging cues), we are also interested
in other types of cues that are associated more closely
to the purpose and method of science. For example, the
scientific method is, more or less, to establish a link to
previous work, set up an experiment to test an hypothe-
sis, perform the experiment, make observations, then fi-
nally compile and discuss the importance of the results of
the experiment. Scientific writing reflects this scientific
method and its purpose: one may find evidence even at
the coarsest granularity of the IMRaD structure in scien-
tific articles. At a finer granularity, we have many target-
Table 3: Proportion of citation contexts containing a verbal hedging cue, by section and location of hedging cue.
Contexts Sentences Windows
# % # % # %
background 1967 0.19 1511 0.15 1479 0.15
methods 726 0.12 541 0.09 369 0.06
res+disc 4858 0.29 3572 0.22 2881 0.17
conclusions 227 0.39 168 0.29 139 0.24
Table 4: Proportion of citation contexts containing a nonverb hedging cue, by section and location of hedging cue.
Contexts Sentences Windows
# % # % # %
background 1862 0.18 1302 0.13 1486 0.15
methods 432 0.07 295 0.05 198 0.03
res+disc 3751 0.23 2484 0.15 2353 0.14
conclusions 186 0.32 107 0.18 111 0.19
ted words to convey the notions of procedure, observa-
tion, reporting, supporting, explaining, refining, contra-
dicting, etc. More specifically, science categorizes into
taxonomies or creates polarities. Scientific writing then
tends to compare and contrast or refine. Not surpris-
ingly, the morphology of scientific terminology exhibits
comparison and contrasting features, for example, exo-
and endo-. Science needs to measure, so scientific writ-
ing contains measurement cues by referring to scales (0?
100), or using comparatives (larger, brighter, etc.). Ex-
periments are described as a sequence of steps, so this is
an implicit method cue.
Since the inception of the formal scientific article in
the seventeenth century, the process of scientific discov-
ery has been inextricably linked with the actions of writ-
ing and publishing the results of research. Rhetoricians
of science have gradually moved from a purely descrip-
tive characterization of the science genre to full-fledged
field studies detailing the evolution of the scientific arti-
cle. During the first generation of rhetoricians of science,
e.g., (Myers, 1991), (Gross, 1996), (Fahnestock, 1999),
the persuasive nature of the scientific article, how it con-
tributes to making and justifying a knowledge claim, was
recognized as the defining property of scientific writing.
Style (lexical and syntactic choice), presentation (orga-
nization of the text and display of the data), and argu-
mentation structure were noted as the rhetorical means
by which authors build a convincing case for their results.
Recently, second-generation rhetoricians of science (e.g.,
(Hyland, 1998), (Gross et al, 2002)) have begun to me-
thodically analyze large corpora of scientific texts with
the purpose of cataloguing specific stylistic and rhetorical
features that are used to create the pragmatic effects that
contribute to the author?s knowledge claim. One particu-
lar type of pragmatic effect, hedging, is especially com-
mon in scientific writing and can be realized through a
wide variety of linguistic choices.
To catalogue these cues and to propose a mapping from
these cues to rhetorical relations, we suggest a research
program that consists of two phases. One phase is theory-
based: we are applying our knowledge from computa-
tional linguistics and the rhetoric of science to develop a
set of principles that guide the development of rules. An-
other phase is data-driven. This phase will use machine-
learning techniques to induce a decision tree.
Our two approaches are guided by a number of factors.
Firstly, the initial set of 35 categories ((Garzone, 1996),
(Garzone and Mercer, 2000)) were developed by combin-
ing and adding to the previous work from the information
science community with a preliminary manual study of
citations in biochemistry and physics articles. Secondly,
our next stages, cataloguing linguistic cues, will require
manual work by rhetoricians. Thirdly, and perhaps most
importantly, one group of cues is not found in the text,
but is rather a set of cultural guidelines that are accepted
by the scientific community for which the article is being
written. Lastly, we are interested not in the connection
between the citation functions and these cues per se, but
rather the citation functions and the rhetorical relations
that are signalled by the cues.
3.3 The Tool Implementation
Concerning the features on which the decision tree makes
its decisions, we have started with a simple, yet fully
automatic prototype (Garzone, 1996) which takes jour-
nal articles as input and classifies every citation found
therein. Its decision tree is very shallow, using only sets
of cue-words and polarity switching words (not, however,
Table 5: Proportion of hedge sentences that contain citations or are part of a citation window, by section and hedging
cue category.
Verb Cues Nonverb Cues All Cues
Cite Wind None Cite Wind None Cite Wind None
background 0.52 0.23 0.25 0.47 0.28 0.25 0.49 0.26 0.26
methods 0.25 0.16 0.59 0.20 0.15 0.65 0.23 0.16 0.61
res+disc 0.26 0.19 0.55 0.21 0.19 0.60 0.23 0.19 0.58
conclusions 0.16 0.14 0.70 0.14 0.16 0.70 0.15 0.14 0.71
etc.), some simple knowledge about the IMRaD struc-
ture1 of the article together with some simple syntactic
structure of the citation-containing sentence. The proto-
type uses 35 citation categories. In addition to having
a design which allows for easy incorporation of more-
sophisticated knowledge, it also gives flexibility to the
tool: categories can be easily coalesced to give users a
tool that can be tailored to a variety of uses.
Although we anticipate some small changes to the
number of categories due to category refinement, the ma-
jor modifications to the decision tree will be driven by
a more-sophisticated set of features associated with each
citation. When investigating a finer granularity of the IM-
RaD structure, we came to realize that the structure of
scientific writing at all levels of granularity was founded
on rhetoric, which involves both argumentation structure
as well as stylistic choices of words and syntax. This was
the motivation for choosing the rhetoric of science as our
guiding principle.
3.4 Evaluation of the Tool
Finally, as for our prototype system, at each stage of de-
velopment the tool will be evaluated:
 A test set of citations will be developed and
will be initially manually categorized by humans
knowledgeable in the scientific field that the arti-
cles represent.
 Of most essential interest, the classification accu-
racy of the citation-indexing tool will be evalu-
ated: we propose to use a combination of statisti-
cal testing and validation by human experts.
 In addition, we would like to assess the tool?s util-
ity in real-world applications such as database cu-
ration for studies in biomedical literature analy-
sis. We have suggested earlier that there may be
many uses of this tool, so a significant aspect of
the value of our tool will be its ability to enhance
other research projects.
1The corpus of biomedical papers all have the standard In-
troduction, Methods, Results, and Discussion or a slightly mod-
ified version in which Results and Discussion are merged.
4 Conclusions and Future Work
The purposeful nature of citation function is a feature of
scientific writing which can be exploited in a variety of
ways. We anticipate more-informative citation indexes as
well as more-intelligent database curation. Additionally,
sophisticated information extraction may be enhanced
when better selection of the dataset is enabled. For ex-
ample, synonym detection in a corpus of papers may be
made more tractable when the corpus is comprised of re-
lated papers derived from navigating a space of linked
citations.
In this paper we have motivated our approach to devel-
oping a literature-indexing tool that computes the func-
tions of citations. We have proposed that the function of a
citation may be determined by analyzing the rhetorical in-
tent of the text that surrounds it. This analysis is founded
on the guiding principle that the scientific method is in-
trinsic to scientific writing.
Our early investigations have determined that linguis-
tic cues and citations are related in important ways. Our
future work will be to map these linguistic cues to rhetor-
ical relations and other pragmatic functions so that this
information can then be used to determine the purpose of
citations.
Acknowledgements
We thank Mark Garzone and Fred Kroon for their par-
ticipation in this project. Our research has been finan-
cially supported by the Natural Sciences and Engineering
Research Council of Canada and by the Universities of
Western Ontario and Waterloo.
References
Miguel A. Andrade and Alfonso Valencia. 1998. Au-
tomatic Extraction of Keywords from Scientific Text:
Application to the Knowledge Domain of Protein Fam-
ilies. Bioinformatics, 14(7):600?607.
Christian Blaschke, Miguel A. Andrade, Christos Ouzou-
nis, and Alfonso Valencia. 1999. Automatic Extrac-
tion of Biological Information from Scientific Text:
Protein-Protein Interactions. International Conference
on Intelligent Systems for Molecular Biology (ISMB
1999), 60?67.
B. Bollacker, S. Lawrence, and C.L. Giles. 1999. A Sys-
tem for Automatic Personalized Tracking of Scientific
Literature on the Web. In Digital Libraries 99?The
Fourth ACM Conference on Digital Libraries, 105?
113. ACM Press, New York.
S. Cole. 1975. The Growth of Scientific Knowledge:
Theories of Deviance as a Case Study. In The Idea of
Social Structure: Papers in Honor of Robert K. Mer-
ton, 175?220. Harcourt Brace Jovanovich, New York.
Chrysanne Di Marco and Robert E. Mercer. 2003. To-
ward a Catalogue of Citation-related Rhetorical Cues
in Scientific Texts. In Proceedings of the Pacific
Association for Computational Linguistics (PACLING
2003) Conference. Halifax, Canada, August 2003.
E.B. Duncan, F.D. Anderson, and R. McAleese. 1981.
Qualified Citation Indexing: Its Relevance to Educa-
tional Technology. In Information Retrieval in Edu-
cational Technology: Proceedings of the First Sympo-
sium on Information Retrieval in Educational Technol-
ogy, 70?79. University of Aberdeen.
Jeanne Fahnestock. 1999. Rhetorical Figures in Science.
Oxford University Press.
B. Finney. 1979. The Reference Characteristics of Sci-
entific Texts. Master?s thesis, The City University of
London.
C. Frost. 1979. The Use of Citations in Literary Re-
search: A Preliminary Classification of Citation Func-
tions. Library Quarterly, 49:399?414.
Eugene Garfield. 1965. Can Citation Indexing Be au-
tomated? In M.E. Stevens et al, editors, Statistical
Association Methods for Mechanical Documentation
(NBS Misc. Pub. 269). National Bureau of Standards,
Washington, DC.
Eugene Garfield. Information, Power, and the Science
Citation Index. In Essays of an Information Scientist,
Volume 1, 1962?1973, Institute for Scientific Infor-
mation.
Mark Garzone. 1996. Automated Classification of Ci-
tations using Linguistic Semantic Grammars. M.Sc.
Thesis, The University of Western Ontario.
Mark Garzone and Robert E. Mercer. 2000. To-
wards an Automated Citation Classifier. In AI?2000,
Proceedings of the 13th Biennial Conference of the
CSCSI/SCEIO, Lecture Notes in Artificial Intelligence,
1822:337?346, H.J. Hamilton (ed.). Springer-Verlag.
A.G. Gross. 1996. The Rhetoric of Science. Harvard
University Press.
A.G. Gross, J.E. Harmon, and M. Reidy. 2002. Commu-
nicating Science: The Scientific Article from the 17th
Century to the Present. Oxford University Press.
M.A.K. Halliday and Ruqaiya Hasan. 1976. Cohesion in
English. Longman Group Limited.
Ken Hyland. 1998. Hedging in Scientific Research Arti-
cles. John Benjamins Publishing Company.
Alistair Knott. 1996. A Data-driven Methodology for
Motivating a Set of Coherence Relations. Ph.D. thesis,
University of Edinburgh.
B.A. Lipetz. 1965. Problems of Citation Analysis: Criti-
cal Review. American Documentation, 16:381?390.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: Toward a Functional
Theory of Text Organization. Text, 8(3):243?281.
Edward M. Marcotte, Ioannis Xenarios, and David Eisen-
berg. 2001. Mining Literature for Protein-Protein In-
teractions. Bioinformatics, 17(4):359?363.
Daniel Marcu. 1997. The Rhetorical Parsing, Summa-
rization, and Generation of Natural Language Texts.
Ph.D. thesis, University of Toronto.
Robert E. Mercer and Chrysanne Di Marco. 2003. The
Importance of Fine-grained Cue Phrases in Scientific
Citations. In AI?2003, Proceedings of the 16th Confer-
ence of the CSCSI/SCEIO, 550?556. Edmonton, Al-
berta, 11?13 June 2003.
Robert E. Mercer, Chrysanne Di Marco, and Frederick
Kroon. 2004. The Frequency of Hedging Cues in Cita-
tion Contexts in Scientific Writing. Submitted to Con-
ference of the Canadian Society for the Computational
Studies of Intelligence (CSCSI 2004).
M.J. Moravscik and P. Murugesan. 1975. Some Results
on the Function and Quality of Citations. Social Stud-
ies of Science, 5:86?92.
Greg Myers. 1991. Writing Biology. University of Wis-
consin Press.
B.C. Peritz. 1983. A Classification of Citation Roles for
the Social Sciences and Related Fields. Scientomet-
rics, 5:303?312.
H. Small. 1978. Cited Documents as Concept Symbols.
Social Studies of Science, 8(3):327?340.
I. Spiegel-Ro?sing. 1977. Science Studies: Bibliometric
and Content Analysis. Social Studies of Science, 7:97?
113.
James Thomas, David Milward, Christos Ouzounis,
Stephen Pulman, and Mark Carroll. 2000. Automatic
Extraction of Protein Interactions from Scientific Ab-
stracts. In Proceedings of the 5th Pacific Symposium
on Biocomputing (PSB 2000), 538-549.
M. Weinstock. 1971. Citation Indexes. In Encyclopae-
dia of Library and Information Science, 5:16?40. Mar-
cel Dekkar, New York.
W. John Wilbur. 2002. A Thematic Analysis of the AIDS
Literature. In Proceedings of the 7th Pacific Sympo-
sium on Biocomputing (PSB 2004), 386-397.
W.J. Wilbur and L. Coffee. 1994. The Effectiveness of
Document Neighboring in Search Enhancement. In-
formation Processing Management, 30:253?266.
