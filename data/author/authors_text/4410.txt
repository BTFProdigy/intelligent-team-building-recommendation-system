A Relational Syntax-Semantics Interface Based on Dependency Grammar
Ralph Debusmann Denys Duchier? Alexander Koller Marco Kuhlmann Gert Smolka Stefan Thater
Saarland University, Saarbr?cken, Germany ?LORIA, Nancy, France
{rade|kuhlmann|smolka}@ps.uni-sb.de, duchier@loria.fr, {koller|stth}@coli.uni-sb.de
Abstract
We propose a syntax-semantics interface that
realises the mapping between syntax and se-
mantics as a relation and does not make func-
tionality assumptions in either direction. This
interface is stated in terms of Extensible De-
pendency Grammar (XDG), a grammar formal-
ism we newly specify. XDG?s constraint-based
parser supports the concurrent flow of informa-
tion between any two levels of linguistic rep-
resentation, even when only partial analyses are
available. This generalises the concept of under-
specification.
1 Introduction
A key assumption of traditional syntax-semantics
interfaces, starting with (Montague, 1974), is that
the mapping from syntax to semantics is functional,
i. e. that once we know the syntactic structure of a
sentence, we can deterministically compute its se-
mantics.
Unfortunately, this assumption is typically not
justified. Ambiguities such as of quantifier scope
or pronominal reference are genuine semantic am-
biguities; that is, even a syntactically unambigu-
ous sentence can have multiple semantic readings.
Conversely, a common situation in natural language
generation is that one semantic representation can
be verbalised in multiple ways. This means that the
relation between syntax and semantics is not func-
tional at all, but rather a true m-to-n relation.
There is a variety of approaches in the litera-
ture on syntax-semantics interfaces for coping with
this situation, but none of them is completely sat-
isfactory. One way is to recast semantic ambiguity
as syntactic ambiguity by compiling semantic dis-
tinctions into the syntax (Montague, 1974; Steed-
man, 1999; Moortgat, 2002). This restores function-
ality, but comes at the price of an artificial blow-
up of syntactic ambiguity. A second approach is to
assume a non-deterministic mapping from syntax
to semantics as in generative grammar (Chomsky,
1965), but it is not always obvious how to reverse
the relation, e. g. for generation. For LFG, the oper-
ation of functional uncertaintainty allows for a re-
stricted form of relationality (Kaplan and Maxwell
III, 1988). Finally, underspecification (Egg et al,
2001; Gupta and Lamping, 1998; Copestake et al,
2004) introduces a new level of representation,
which can be computed functionally from a syntac-
tic analysis and encapsulates semantic ambiguity in
a way that supports the enumeration of all semantic
readings by need.
In this paper, we introduce a completely rela-
tional syntax-semantics interface, building upon the
underspecification approach. We assume a set of
linguistic dimensions, such as (syntactic) immedi-
ate dominance and predicate-argument structure; a
grammatical analysis is a tuple with one component
for each dimension, and a grammar describes a set
of such tuples. While we make no a priori function-
ality assumptions about the relation of the linguistic
dimensions, functional mappings can be obtained
as a special case. We formalise our syntax-seman-
tics interface using Extensible Dependency Gram-
mar (XDG), a new grammar formalism which gen-
eralises earlier work on Topological Dependency
Grammar (Duchier and Debusmann, 2001).
The relational syntax-semantics interface is sup-
ported by a parser for XDG based on constraint pro-
gramming. The crucial feature of this parser is that
it supports the concurrent flow of possibly partial in-
formation between any two dimensions: once addi-
tional information becomes available on one dimen-
sion, it can be propagated to any other dimension.
Grammaticality conditions and preferences (e. g. se-
lectional restrictions) can be specified on their nat-
ural level of representation, and inferences on each
dimension can help reduce ambiguity on the oth-
ers. This generalises the idea of underspecifica-
tion, which aims to represent and reduce ambiguity
through inferences on a single dimension only.
The structure of this paper is as follows: in Sec-
tion 2, we give the general ideas behind XDG, its
formal definition, and an overview of the constraint-
based parser. In Section 3, we present the relational
syntax-semantics interface, and go through exam-
ples that illustrate its operation. Section 4 shows
how the semantics side of our syntax-semantics in-
terface can be precisely related to mainstream se-
mantics research. We summarise our results and
point to further work in Section 5.
2 Extensible Dependency Grammar
This section presents Extensible Dependency
Grammar (XDG), a description-based formalism
for dependency grammar. XDG generalizes previ-
ous work on Topological Dependency Grammar
(Duchier and Debusmann, 2001), which focussed
on word order phenomena in German.
2.1 XDG in a Nutshell
XDG is a description language over finite labelled
graphs. It is able to talk about two kinds of con-
straints on these structures: The lexicon of an XDG
grammar describes properties local to individual
nodes, such as valency. The grammar?s principles
express constraints global to the graph as a whole,
such as treeness. Well-formed analyses are graphs
that satisfy all constraints.
An XDG grammar allows the characterisation
of linguistic structure along several dimensions of
description. Each dimension contains a separate
graph, but all these graphs share the same set of
nodes. Lexicon entries synchronise dimensions by
specifying the properties of a node on all dimen-
sions at once. Principles can either apply to a single
dimension (one-dimensional), or constrain the rela-
tion of several dimensions (multi-dimensional).
Consider the example in Fig. 1, which shows an
analysis for a sentence of English along two dimen-
sions of description, immediate dominance (ID) and
linear precedence (LP). The principles of the under-
lying grammar require both dimensions to be trees,
and the LP tree to be a ?flattened? version of the ID
tree, in the sense that whenever a node v is a tran-
sitive successor of a node u in the LP tree, it must
also be a transitive successor of u in the ID tree. The
given lexicon specifies the potential incoming and
required outgoing edges for each word on both di-
mensions. The word does, for example, accepts no
incoming edges on either dimension and must there-
fore be at the root of both the ID and the LP tree. It is
required to have outgoing edges to a subject (subj)
and a verb base form (vbse) in the ID tree, needs
fillers for a subject (sf) and a verb complement field
(vcf) in the LP tree, and offers an optional field for
topicalised material (tf). All these constraints are
satisfied by the analysis, which is thus well-formed.
2.2 Formalisation
Formally, an XDG grammar is built up of dimen-
sions, principles, and a lexicon, and characterises a
set of well-formed analyses.
A dimension is a tuple D = (Lab,Fea,Val,Pri) of
a set Lab of edge labels, a set Fea of features, a set
Val of feature values, and a set of one-dimensional
s
u
b
j
v
b
s
e
o
b
j
what does John eat
s
f
v
c
f
what does John eat
t
f
word inID outID inLP outLP
what {obj?} {} {tf?} {}
does {} {subj,vbse} {} {tf?,sf,vcf}
John {subj?,obj?} {} {sf?,of?} {}
eat {vbse?} {obj} {vcf?} {}
Figure 1: XDG analysis of ?what does John eat?
principles Pri. A lexicon for the dimension D is a
set Lex ? Fea ? Val of total feature assignments (or
lexical entries). A D-structure, representing an anal-
ysis on dimension D, is a triple (V,E,F) of a set V
of nodes, a set E ?V ?V ?Lab of directed labelled
edges, and an assignment F : V ? (Fea ? Val) of
lexical entries to nodes. V and E form a graph. We
write StrD for the set of all possible D-structures.
The principles characterise subsets of StrD that have
further dimension-specific properties, such as being
a tree, satisfying assigned valencies, etc. We assume
that the elements of Pri are finite representations of
such subsets, but do not go into details here; some
examples are shown in Section 3.2.
An XDG grammar ((Labi,Feai,Vali,Prii)ni=1,Pri,
Lex) consists of n dimensions, multi-dimensional
principles Pri, and a lexicon Lex. An XDG analysis
(V,Ei,Fi)ni=1 is an element of Ana = Str1??? ??Strn
where all dimensions share the same set of nodes V .
Multi-dimensional principles work just like one-
dimensional principles, except that they specify
subsets of Ana, i. e. couplings between dimensions
(e. g. the flattening principle between ID and LP in
Section 2.1). The lexicon Lex ? Lex1 ? ?? ? ? Lexn
constrains all dimensions at once. An XDG analysis
is licenced by Lex iff (F1(w), . . . ,Fn(w)) ? Lex for
every node w ?V .
In order to compute analyses for a given input, we
model it as a set of input constraints (Inp), which
again specify a subset of Ana. The parsing prob-
lem for XDG is then to find elements of Ana that
are licenced by Lex and consistent with Inp and
Pri. Note that the term ?parsing problem? is tradi-
tionally used only for inputs that are sequences of
words, but we can easily represent surface realisa-
tion as a ?parsing? problem in which Inp specifies a
semantic dimension; in this case, a ?parser? would
compute analyses that contain syntactic dimensions
from which we can read off a surface sentence.
2.3 Constraint Solver
The parsing problem of XDG has a natural read-
ing as a constraint satisfaction problem (CSP) (Apt,
2003) on finite sets of integers; well-formed anal-
yses correspond to the solutions of this problem.
The transformation, whose details we omit due to
lack of space, closely follows previous work on ax-
iomatising dependency parsing (Duchier, 2003) and
includes the use of the selection constraint to effi-
ciently handle lexical ambiguity.
We have implemented a constraint solver for
this CSP using the Mozart/Oz programming system
(Smolka, 1995; Mozart Consortium, 2004). This
solver does a search for a satisfying variable assign-
ment. After each case distinction (distribution), it
performs simple inferences that restrict the ranges
of the finite set variables and thus reduce the size
of the search tree (propagation). The successful
leaves of the search tree correspond to XDG anal-
yses, whereas the inner nodes correspond to partial
analyses. In these cases, the current constraints are
too weak to specify a complete analysis, but they
already express that some edges or feature values
must be present, and that others are excluded. Partial
analyses will play an important role in Section 3.3.
Because propagation operates on all dimensions
concurrently, the constraint solver can frequently
infer information about one dimension from infor-
mation on another, if there is a multi-dimensional
principle linking the two dimensions. These infer-
ences take place while the constraint problem is be-
ing solved, and they can often be drawn before the
solver commits to any single solution.
Because XDG allows us to write grammars with
completely free word order, XDG solving is an NP-
complete problem (Koller and Striegnitz, 2002).
This means that the worst-case complexity of the
solver is exponential, but the average-case complex-
ity for the hand-crafted grammars we experimented
with is often better than this result suggests. We
hope there are useful fragments of XDG that would
guarantee polynomial worst-case complexity.
3 A Relational Syntax-Semantics Interface
Now that we have the formal and processing frame-
works in place, we can define a relational syntax-
semantics interface for XDG. We will first show
how we encode semantics within the XDG frame-
work. Then we will present an example grammar
(including some principle definitions), and finally
go through an example that shows how the rela-
tionality of the interface, combined with the con-
currency of the constraint solver, supports the flow
of information between different dimensions.
3.1 Representing Meaning
We represent meaning within XDG on two dimen-
sions: one for predicate-argument structure (PA),
every student reads a book
s
u
b
j
d
e
t
o
b
j
d
e
t
every student reads a book
a
g
a
r
g
p
a
t
a
r
g
i. ID-tree ii. PA-structure
s
every student reads a book
s
r
r
s
every student reads a book
s
r
r
iii. scope trees
Figure 2: Two analyses for the sentence ?every stu-
dent reads a book.?
and one for scope (SC). The function of the PA di-
mension is to abstract over syntactic idiosyncrasies
such as active-passive alternations or dative shifts,
and to make certain semantic dependencies e. g. in
control constructions explicit; it deals with concepts
such as agent and patient, rather than subject and ob-
ject. The purpose of the SC dimension is to reflect
the structure of a logical formula that would repre-
sent the semantics, in terms of scope and restriction.
We will make this connection explicit in Section 4.
In addition, we assume an ID dimension as above.
We do not include an LP dimension only for ease of
presentation; it could be added completely orthogo-
nally to the three dimensions we consider here.
While one ID structure will typically correspond
to one PA structure, each PA structure will typically
be consistent with multiple SC structures, because
of scope ambiguities. For instance, Fig. 2 shows the
unique ID and PA structures for the sentence ?Ev-
ery student reads a book.? These structures (and the
input sentence) are consistent with the two possi-
ble SC-structures shown in (iii). Assuming a David-
sonian event semantics, the two SC trees (together
with the PA-structure) represent the two readings of
the sentence:
? ?e.?x.student(x) ??y.book(y)? read(e,x,y)
? ?e.?y.book(y)??x.student(x) ? read(e,x,y)
3.2 A Grammar for a Fragment of English
The lexicon for an XDG grammar for a small frag-
ment of English using the ID, PA, and SC dimensions
is shown in Fig. 3. Each row in the table specifies a
(unique) lexical entry for each part of speech (deter-
miner, common noun, proper noun, transitive verb
and preposition); there is no lexical ambiguity in
this grammar. Each column specifies a feature. The
meaning of the features will be explained together
inID outID inPA outPA inSC outSC
DET {subj?,obj?,pcomp?} {det!} {ag?,pat?,arg?} {quant!} {r?,s?,a?} {r!,s!}
CN {det?} {prep?} {quant?} {mod?} {r?,s?,a?} {}
PN {subj?,obj?,pcomp?} {prep?} {ag?,pat?,arg?} {mod?} {r?,s?,a?} {r?,s!}
TV {} {subj!,obj!,prep?} {} {ag!,pat!, instr?} {r?,s?,a?} {}
PREP {prep?} {pcomp!} {mod?, instr?} {arg!} {r?,s?,a?} {a!}
link codom contradom
DET {quant 7? {det}} {quant 7? {r}} {}
CN,PN {mod 7? {prep}} {} {mod 7? {a}}
TV {ag 7? {subj},pat 7? {obj}, instr 7? {prep}} {} {ag 7? {s},pat 7? {s}, instr 7? {a}}
PREP {arg 7? {pcomp}} {} {arg 7? {s}}
Figure 3: The example grammar fragment
with the principles that use them.
The ID dimension uses the edge labels LabID =
{det,subj,obj,prep,pcomp} resp. for determined
common noun,1 subject, object, preposition, and
complement of a preposition. The PA dimension
uses LabPA = {ag,pat,arg,quant,mod, instr}, resp.
for agent, patient, argument of a modifier, common
noun pertaining to a quantifier, modifier, and instru-
ment; and SC uses LabSC = {r,s,a} resp. for restric-
tion and scope of a quantifier, and for an argument.
The grammar also contains three one-dimen-
sional principles (tree, dag, and valency), and
three multi-dimensional principles (linking, co-
dominance, and contra-dominance).
Tree and dag principles. The tree principle re-
stricts ID and SC structures to be trees, and the
dag principle restricts PA structures to be directed
acyclic graphs.
Valency principle. The valency principle, which
we use on all dimensions, states that the incom-
ing and outgoing edges of each node must obey the
specifications of the in and out features. The possi-
ble values for each feature ind and outd are subsets
of Labd ? {!,?,?}. `! specifies a mandatory edge
with label `, `? an optional one, and `? zero or more.
Linking principle. The linking principle for di-
mensions d1,d2 constrains how dependents on d1
may be realised on d2. It assumes a feature linkd1,d2
whose values are functions that map labels from
Labd1 to sets of labels from Labd2 , and is specified
by the following implication:
v
l
?d1 v
? ? ?l? ? linkd1,d2(v)(l) : v
l?
?d2 v
?
Our grammar uses this principle with the link fea-
ture to constrain the realisations of PA-dependents in
the ID dimension. In Fig. 2, the agent (ag) of reads
must be realised as the subject (subj), i. e.
1We assume on all dimensions that determiners are the
heads of common nouns. This makes for a simpler relationship
between the syntactic and semantic dimensions.
reads ag?PA every ? reads
subj
? ID every
Similarly for the patient and the object. There
is no instrument dependent in the example, so this
part of the link feature is not used. An ergative verb
would use a link feature where the subject realises
the patient; Control and raising phenomena can also
be modelled, but we cannot present this here.
Co-dominance principle. The co-dominance
principle for d1,d2 relates edges in d1 to dominance
relations in the same direction in d2. It assumes a
feature codomd1,d2 mapping labels in Labd1 to sets
of labels in Labd2 and is specified as
v
l
?d1 v
? ? ?l? ? codomd1,d2(v)(l) : v
l?
???d2v
?
Our grammar uses the co-dominance principle on
dimension PA and SC to express, e. g., that the
propositional contribution of a noun must end up in
the restriction of its determiner. For example, for the
determiner every of Fig. 2 we have:
every quant? PA student ? every
r
???SCstudent
Contra-dominance principle. The contra-domi-
nance principle is symmetric to the co-dominance
principle, and relates edges in d1 to dominance
edges into the opposite direction in d2. It assumes
a feature contradomd1,d2 mapping labels of Labd1 to
sets of labels from Labd2 and is specified as
v
l
?d1 v
? ?
?l? ? contradomd1,d2(v)(l) : v?
l?
???d2v
Our grammar uses the contra-dominance principle
on dimensions PA and SC to express, e. g., that pred-
icates must end up in the scope of the quantifiers
whose variables they refer to. Thus, for the transi-
tive verb reads of Fig. 2, we have:
reads ag?PA every ? every
s
???SCreads
reads pat?PA a ? a
s
???SCreads
Mary saw a student with a book
a
g
p
a
t
q
u
a
n
t
a
r
g
q
u
a
n
t
Mary saw a student with a book
s
s
r
s
r
Mary saw a student with a book
s
u
b
j
o
b
j
d
e
t
a
r
g
d
e
t
p
r
e
p
Mary saw a student with a book
a
g
p
a
t
q
u
a
n
t
a
r
g
q
u
a
n
t
Mary saw a student with a book
s
r
s
r
Mary saw a student with a book
s
u
b
j
o
b
j
d
e
t
a
r
g
d
e
t
i
n
s
t
r
a
s
Mary saw a student with a book
a
g
p
a
t
a
r
g
a
r
g
q
u
a
n
t
Mary saw a student with a book
s
s
r
s
r
Mary saw a student with a book
s
u
b
j
o
b
j
d
e
t
a
r
g
d
e
t
m
o
d
p
r
e
p
a
i. Partial analysis
ii. verb attachment
iii. noun attachment
ID
PA
SC
Figure 4: Partial description (left) and two solutions (right) for ?Mary saw a student with a book.?
3.3 Syntax-Semantics Interaction
It is important to note at this point that the syntax-
semantics interface we have defined is indeed re-
lational. Each principle declaratively specifies a set
of admissible analyses, i. e. a relation between the
structures for the different dimensions, and the anal-
yses that the complete grammar judges grammatical
are simply those that satisfy all principles. The role
of the lexicon is to provide the feature values which
parameterise the principles defined above.
The constraint solver complements this relation-
ality by supporting the use of the principles to move
information between any two dimensions. If, say,
the left-hand side of the linking principle is found to
be satisfied for dimension d1, a propagator will infer
the right-hand side and add it to dimension d2. Con-
versely, if the solver finds that the right-hand side
must be false for d2, the negation of the left-hand
side is inferred for d1. By letting principles interact
concurrently, we can make some very powerful in-
ferences, as we will demonstrate with the example
sentence ?Mary saw a student with a book,? some
partial analyses for which are shown in Fig. 4.
Column (i) in the figure shows the state after the
constraint solver finishes its initial propagation, at
the root of the search tree. Even at this point, the va-
lency and treeness principles have conspired to es-
tablish an almost complete ID-structure. By the link-
ing principle, the PA-structure has been determined
similarly closely. The SC-structure is still mostly un-
determined, but by the co- and contra-dominance
principles, the solver has already established that
some nodes must dominate others: A dotted edge
with label s in the picture means that the solver
knows there must be a path between these two nodes
which starts with an s-edge. In other words, the
solver has computed a large amount of semantic in-
formation from an incomplete syntactic analysis.
Now imagine some external source tells us that
with is a mod-child of student on PA, i. e. the anal-
ysis in (iii). This information could come e. g. from
a statistical model of selectional preferences, which
will judge this edge much more probable than an
instr-edge from the verb to the preposition (ii).
Adding this edge will trigger additional inferences
through the linking principle, which can now infer
that with is a prep-child of student on ID. In the other
direction, the solver will infer more dominances on
SC. This means that semantic information can be
used to disambiguate syntactic ambiguities, and se-
mantic information such as selectional preferences
can be stated on their natural level of representation,
rather than be forced into the ID dimension directly.
Similarly, the introduction of new edges on SC
could trigger a similar reasoning process which
would infer new PA-edges, and thus indirectly also
new ID-edges. Such new edges on SC could come
from inferences with world or discourse knowledge
(Koller and Niehren, 2000), scope preferences, or
interactions with information structure (Duchier and
Kruijff, 2003).
4 Traditional Semantics
Our syntax-semantics interface represents seman-
tic information as graphs on the PA and SC dimen-
sions. While this looks like a radical departure from
traditional semantic formalisms, we consider these
graphs simply an alternative way of presenting more
traditional representations. We devote the rest of the
paper to demonstrating that a pair of a PA and a SC
structure can be interpreted as a Montague-style for-
mula, and that a partial analysis on these two di-
mensions can be seen as an underspecified semantic
description.
4.1 Montague-style Interpretation
In order to extract a standard type-theoretic expres-
sion from an XDG analysis, we assign each node v
two semantic values: a lexical value L(v) represent-
ing the semantics of v itself, and a phrasal value
P(v) representing the semantics of the entire SC-
subtree rooted at v. We use the SC-structure to de-
termine functor-argument relationships, and the PA-
structure to establish variable binding.
We assume that nodes for determiners and proper
names introduce unique individual variables (?in-
dices?). Below we will write ??v?? to refer to the in-
dex of the node v, and we write ?` to refer to the
node which is the `-child of the current node in the
appropriate dimension (PA or SC). The semantic lex-
icon is defined as follows; ?L(w)? should be read as
?L(v), where v is a node for the word w?.
L(a) = ?P?Q?e.?x(P(x)?Q(x)(e))
L(book) = book?
L(with) = ?P?x.(with?(???arg??)(x)?P(x))
L(reads) = read?(???pat??)(???ag??)
Lexical values for other determiners, common
nouns, and proper names are defined analogously.
Note that we do not formally distinguish event
variables from individual variables. In particular,
L(with) can be applied to either nouns or verbs,
which both have type ?e, t?.
We assume that no node in the SC-tree has more
than one child with the same edge label (which our
grammar guarantees), and write n(`1, . . . , `k) to in-
dicate that the node n has SC-children over the edge
labels `1, . . . , `k. The phrasal value for n is defined
(in the most complex case) as follows:
P(n(r,s)) = L(n)(P(?r))(? ??n??.P(?s))
This rule implements Montague?s rule of quan-
tification (Montague, 1974); note that ? ??n?? is a
binder for the variable ??n??. Nodes that have no
s-children are simply functionally applied to the
phrasal semantics of their children (if any).
By way of example, consider the left-hand SC-
structure in Fig. 2. If we identify each node by the
word it stands for, we get the following phrasal
@
@
every
?
@
@
a
?
@
@
read var
var
student
book
r
s
s
r
every student reads a book
r
r
s
s
every student reads a book
a
g
a
r
g
p
a
t
a
r
g
Figure 5: A partial SC-structure and its correspond-
ing CLLS description.
value for the root of the tree:
L(a)(L(book))(?x.L(every)(L(student)
(?y.read?(y)(x)))),
where we write x for ??a?? and y for ??every??. The
arguments of read? are x and y because every and
a are the arg and pat children of reads on the PA-
structure. After replacing the lexical values by their
definitions and beta-reduction, we obtain the fa-
miliar representation for this semantic reading, as
shown in Section 3.1.
4.2 Underspecification
It is straightforward to extend this extraction of
type-theoretic formulas from fully specified XDG
analyses to an extraction of underspecified seman-
tic descriptions from partial XDG analyses. We will
briefly demonstrate this here for descriptions in the
CLLS framework (Egg et al, 2001), which sup-
ports this most easily. Other underspecification for-
malisms could be used too.
Consider the partial SC-structure in Fig. 5, which
could be derived by the constraint solver for the
sentence from Fig. 2. We can obtain a CLLS con-
straint from it by first assigning to each node of
the SC-structure a lexical value, which is now a part
of the CLLS constraint (indicated by the dotted el-
lipses). Because student and book are known to be r-
daughters of every and a on SC, we plug their CLLS
constraints into the r-holes of their mothers? con-
straints. Because we know that reads must be dom-
inated by the s-children of the determiners, we add
the two (dotted) dominance edges to the constraint.
Finally, variable binding is represented by the bind-
ing constraints drawn as dashed arrows, and can be
derived from PA exactly as above.
5 Conclusion
In this paper, we have shown how to build a fully re-
lational syntax-semantics interface based on XDG.
This new grammar formalism offers the grammar
developer the possibility to represent different kinds
of linguistic information on separate dimensions
that can be represented as graphs. Any two dimen-
sions can be linked by multi-dimensional principles,
which mutually constrain the graphs on the two di-
mensions. We have shown that a parser based on
concurrent constraint programming is capable of in-
ferences that restrict ambiguity on one dimension
based on newly available information on another.
Because the interface we have presented makes
no assumption that any dimension is more ?basic?
than another, there is no conceptual difference be-
tween parsing and generation. If the input is the sur-
face sentence, the solver will use this information
to compute the semantic dimensions; if the input is
the semantics, the solver will compute the syntactic
dimensions, and therefore a surface sentence. This
means that we get bidirectional grammars for free.
While the solver is reasonably efficient for many
(hand-crafted) grammars, it is an important goal
for the future to ensure that it can handle large-
scale grammars imported from e.g. XTAG (XTAG
Research Group, 2001) or induced from treebanks.
One way in which we hope to achieve this is to iden-
tify fragments of XDG with provably polynomial
parsing algorithms, and which contain most useful
grammars. Such grammars would probably have to
specify word orders that are not completely free,
and we would have to control the combinatorics
of the different dimensions (Maxwell and Kaplan,
1993). One interesting question is also whether dif-
ferent dimensions can be compiled into a single di-
mension, which might improve efficiency in some
cases, and also sidestep the monostratal vs. multi-
stratal distinction.
The crucial ingredient of XDG that make rela-
tional syntax-semantics processing possible are the
declaratively specified principles. So far, we have
only given some examples for principle specifi-
cations; while they could all be written as Horn
clauses, we have not committed to any particular
representation formalism. The development of such
a representation formalism will of course be ex-
tremely important once we have experimented with
more powerful grammars and have a stable intuition
about what principles are needed.
At that point, it would also be highly interest-
ing to define a (logic) formalism that generalises
both XDG and dominance constraints, a fragment of
CLLS. Such a formalism would make it possible to
take over the interface presented here, but use dom-
inance constraints directly on the semantics dimen-
sions, rather than via the encoding into PA and SC
dimensions. The extraction process of Section 4.2
could then be recast as a principle.
Acknowledgements
We thank Markus Egg for many fruitful discussions
about this paper.
References
K. Apt. 2003. Principles of Constraint Programming.
Cambridge University Press.
N. Chomsky. 1965. Aspects of the Theory of Syntax.
MIT Press, Cambridge, MA.
A. Copestake, D. Flickinger, C. Pollard, and I. Sag.
2004. Minimal recursion semantics. an introduction.
Journal of Language and Computation. To appear.
D. Duchier and R. Debusmann. 2001. Topological de-
pendency trees: A constraint-based account of linear
precedence. In ACL 2001, Toulouse.
D. Duchier and G.-J. M. Kruijff. 2003. Information
structure in topological dependency grammar. In
EACL 2003.
D. Duchier. 2003. Configuration of labeled trees un-
der lexicalized constraints and principles. Research
on Language and Computation, 1(3?4):307?336.
M. Egg, A. Koller, and J. Niehren. 2001. The Constraint
Language for Lambda Structures. Logic, Language,
and Information, 10:457?485.
V. Gupta and J. Lamping. 1998. Efficient linear logic
meaning assembly. In COLING/ACL 1998.
R. M. Kaplan and J. T. Maxwell III. 1988. An algorithm
for functional uncertainty. In COLING 1988, pages
297?302, Budapest/HUN.
A. Koller and J. Niehren. 2000. On underspecified
processing of dynamic semantics. In Proceedings of
COLING-2000, Saarbr?cken.
A. Koller and K. Striegnitz. 2002. Generation as depen-
dency parsing. In ACL 2002, Philadelphia/USA.
J. T. Maxwell and R. M. Kaplan. 1993. The interface
between phrasal and functional constraints. Compu-
tational Linguistics, 19(4):571?590.
R. Montague. 1974. The proper treatment of quantifica-
tion in ordinary english. In Richard Thomason, editor,
Formal Philosophy. Selected Papers of Richard Mon-
tague, pages 247?271. Yale University Press, New
Haven and London.
M. Moortgat. 2002. Categorial grammar and formal se-
mantics. In Encyclopedia of Cognitive Science. Na-
ture Publishing Group, MacMillan. To appear.
Mozart Consortium. 2004. The Mozart-Oz website.
http://www.mozart-oz.org/.
G. Smolka. 1995. The Oz Programming Model. In
Computer Science Today, Lecture Notes in Computer
Science, vol. 1000, pages 324?343. Springer-Verlag.
M. Steedman. 1999. Alternating quantifier scope in
CCG. In Proc. 37th ACL, pages 301?308.
XTAG Research Group. 2001. A lexicalized tree adjoin-
ing grammar for english. Technical Report IRCS-01-
03, IRCS, University of Pennsylvania.
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 507?514,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Mildly Non-Projective Dependency Structures
Marco Kuhlmann
Programming Systems Lab
Saarland University
Germany
kuhlmann@ps.uni-sb.de
Joakim Nivre
V?xj? University and
Uppsala University
Sweden
nivre@msi.vxu.se
Abstract
Syntactic parsing requires a fine balance
between expressivity and complexity, so
that naturally occurring structures can be
accurately parsed without compromising
efficiency. In dependency-based parsing,
several constraints have been proposed that
restrict the class of permissible structures,
such as projectivity, planarity, multi-pla-
narity, well-nestedness, gap degree, and
edge degree. While projectivity is gener-
ally taken to be too restrictive for natural
language syntax, it is not clear which of the
other proposals strikes the best balance be-
tween expressivity and complexity. In this
paper, we review and compare the different
constraints theoretically, and provide an ex-
perimental evaluation using data from two
treebanks, investigating how large a propor-
tion of the structures found in the treebanks
are permitted under different constraints.
The results indicate that a combination of
the well-nestedness constraint and a para-
metric constraint on discontinuity gives a
very good fit with the linguistic data.
1 Introduction
Dependency-based representations have become in-
creasingly popular in syntactic parsing, especially
for languages that exhibit free or flexible word or-
der, such as Czech (Collins et al, 1999), Bulgarian
(Marinov and Nivre, 2005), and Turkish (Eryig?it
and Oflazer, 2006). Many practical implementa-
tions of dependency parsing are restricted to pro-
jective structures, where the projection of a head
word has to form a continuous substring of the
sentence. While this constraint guarantees good
parsing complexity, it is well-known that certain
syntactic constructions can only be adequately rep-
resented by non-projective dependency structures,
where the projection of a head can be discontinu-
ous. This is especially relevant for languages with
free or flexible word order.
However, recent results in non-projective depen-
dency parsing, especially using data-driven meth-
ods, indicate that most non-projective structures
required for the analysis of natural language are
very nearly projective, differing only minimally
from the best projective approximation (Nivre and
Nilsson, 2005; Hall and Nov?k, 2005; McDon-
ald and Pereira, 2006). This raises the question
of whether it is possible to characterize a class of
mildly non-projective dependency structures that is
rich enough to account for naturally occurring syn-
tactic constructions, yet restricted enough to enable
efficient parsing.
In this paper, we review a number of propos-
als for classes of dependency structures that lie
between strictly projective and completely unre-
stricted non-projective structures. These classes
have in common that they can be characterized in
terms of properties of the dependency structures
themselves, rather than in terms of grammar for-
malisms that generate the structures. We compare
the proposals from a theoretical point of view, and
evaluate a subset of them empirically by testing
their representational adequacy with respect to two
dependency treebanks: the Prague Dependency
Treebank (PDT) (Hajic? et al, 2001), and the Danish
Dependency Treebank (DDT) (Kromann, 2003).
The rest of the paper is structured as follows.
In section 2, we provide a formal definition of de-
pendency structures as a special kind of directed
graphs, and characterize the notion of projectivity.
In section 3, we define and compare five different
constraints on mildly non-projective dependency
structures that can be found in the literature: pla-
narity, multiplanarity, well-nestedness, gap degree,
and edge degree. In section 4, we provide an ex-
perimental evaluation of the notions of planarity,
well-nestedness, gap degree, and edge degree, by
507
investigating how large a proportion of the depen-
dency structures found in PDT and DDT are al-
lowed under the different constraints. In section 5,
we present our conclusions and suggestions for fur-
ther research.
2 Dependency graphs
For the purposes of this paper, a dependency graph
is a directed graph on the set of indices correspond-
ing to the tokens of a sentence. We write ?n? to refer
to the set of positive integers up to and including n.
Definition 1 A dependency graph for a sentence
x D w1; : : : ; wn is a directed graph1
G D .V I E/; where V D ?n? and E  V  V .
Throughout this paper, we use standard terminol-
ogy and notation from graph theory to talk about
dependency graphs. In particular, we refer to the
elements of the set V as nodes, and to the elements
of the set E as edges. We write i ! j to mean that
there is an edge from the node i to the node j (i.e.,
.i; j / 2 E), and i ! j to mean that the node i
dominates the node j , i.e., that there is a (possibly
empty) path from i to j . For a given node i , the set
of nodes dominated by i is the yield of i . We use
the notation .i/ to refer to the projection of i : the
yield of i , arranged in ascending order.
2.1 Dependency forests
Most of the literature on dependency grammar and
dependency parsing does not allow arbitrary de-
pendency graphs, but imposes certain structural
constraints on them. In this paper, we restrict our-
selves to dependency graphs that form forests.
Definition 2 A dependency forest is a dependency
graph with two additional properties:
1. it is acyclic (i.e., if i ! j , then not j ! i);
2. each of its nodes has at most one incoming
edge (i.e., if i ! j , then there is no node k
such that k ? i and k ! j ).
Nodes in a forest without an incoming edge are
called roots. A dependency forest with exactly one
root is a dependency tree.
Figure 1 shows a dependency forest taken from
PDT. It has two roots: node 2 (corresponding to the
complementizer proto) and node 8 (corresponding
to the final punctuation mark).
1We only consider unlabelled dependency graphs.
1 2 3 5 64 7 8
Nen? proto zapot?eb? uzav?rat nov? n?jemn? smlouvy .
contractsleasenewsignneededis-not therefore .
?It is therefore not needed to sign new lease contracts.?
Figure 1: Dependency forest for a Czech sentence
from the Prague Dependency Treebank
Some authors extend dependency forests by a
special root node with position 0, and add an edge
.0; i/ for every root node i of the remaining graph
(McDonald et al, 2005). This ensures that the ex-
tended graph always is a tree. Although such a
definition can be useful, we do not follow it here,
since it obscures the distinction between projectiv-
ity and planarity to be discussed in section 3.
2.2 Projectivity
In contrast to acyclicity and the indegree constraint,
both of which impose restrictions on the depen-
dency relation as such, the projectivity constraint
concerns the interaction between the dependency
relation and the positions of the nodes in the sen-
tence: it says that the nodes in a subtree of a de-
pendency graph must form an interval, where an
interval (with endpoints i and j ) is the set
?i; j ? WD f k 2 V j i  k and k  j g :
Definition 3 A dependency graph is projective, if
the yields of its nodes are intervals.
Since projectivity requires each node to dominate a
continuous substring of the sentence, it corresponds
to a ban on discontinuous constituents in phrase
structure representations.
Projectivity is an interesting constraint on de-
pendency structures both from a theoretical and
a practical perspective. Dependency grammars
that only allow projective structures are closely
related to context-free grammars (Gaifman, 1965;
Obre?bski and Gralin?ski, 2004); among other things,
they have the same (weak) expressivity. The pro-
jectivity constraint also leads to favourable pars-
ing complexities: chart-based parsing of projective
dependency grammars can be done in cubic time
(Eisner, 1996); hard-wiring projectivity into a de-
terministic dependency parser leads to linear-time
parsing in the worst case (Nivre, 2003).
508
3 Relaxations of projectivity
While the restriction to projective analyses has a
number of advantages, there is clear evidence that
it cannot be maintained for real-world data (Zeman,
2004; Nivre, 2006). For example, the graph in
Figure 1 is non-projective: the yield of the node 1
(marked by the dashed rectangles) does not form
an interval?the node 2 is ?missing?. In this sec-
tion, we present several proposals for structural
constraints that relax projectivity, and relate them
to each other.
3.1 Planarity and multiplanarity
The notion of planarity appears in work on Link
Grammar (Sleator and Temperley, 1993), where
it is traced back to Mel?c?uk (1988). Informally,
a dependency graph is planar, if its edges can be
drawn above the sentence without crossing. We
emphasize the word above, because planarity as
it is understood here does not coincide with the
standard graph-theoretic concept of the same name,
where one would be allowed to also use the area
below the sentence to disentangle the edges.
Figure 2a shows a dependency graph that is pla-
nar but not projective: while there are no crossing
edges, the yield of the node 1 (the set f1; 3g) does
not form an interval.
Using the notation linked.i; j / as an abbrevia-
tion for the statement ?there is an edge from i to j ,
or vice versa?, we formalize planarity as follows:
Definition 4 A dependency graph is planar, if it
does not contain nodes a; b; c; d such that
linked.a; c/ ^ linked.b; d/ ^ a < b < c < d :
Yli-Jyr? (2003) proposes multiplanarity as a gen-
eralization of planarity suitable for modelling de-
pendency analyses, and evaluates it experimentally
using data from DDT.
Definition 5 A dependency graph G D .V I E/ is
m-planar, if it can be split into m planar graphs
G1 D .V I E1/; : : : ;Gm D .V I Em/
such that E D E1]  ]Em. The planar graphs Gi
are called planes.
As an example of a dependency forest that is 2-
planar but not planar, consider the graph depicted in
Figure 2b. In this graph, the edges .1; 4/ and .3; 5/
are crossing. Moving either edge to a separate
graph partitions the original graph into two planes.
1 2 3
(a) 1-planar
1 2 3 4 5
(b) 2-planar
Figure 2: Planarity and multi-planarity
3.2 Gap degree and well-nestedness
Bodirsky et al (2005) present two structural con-
straints on dependency graphs that characterize
analyses corresponding to derivations in Tree Ad-
joining Grammar: the gap degree restriction and
the well-nestedness constraint.
A gap is a discontinuity in the projection of a
node in a dependency graph (Pl?tek et al, 2001).
More precisely, let i be the projection of the
node i . Then a gap is a pair .jk ; jkC1/ of nodes
adjacent in i such that jkC1   jk > 1.
Definition 6 The gap degree of a node i in a de-
pendency graph, gd.i/, is the number of gaps in i .
As an example, consider the node labelled i in the
dependency graphs in Figure 3. In Graph 3a, the
projection of i is an interval (.2; 3; 4/), so i has gap
degree 0. In Graph 3b, i D .2; 3; 6/ contains a
single gap (.3; 6/), so the gap degree of i is 1. In
the rightmost graph, the gap degree of i is 2, since
i D .2; 4; 6/ contains two gaps (.2; 4/ and .4; 6/).
Definition 7 The gap degree of a dependency
graph G, gd.G/, is the maximum among the gap
degrees of its nodes.
Thus, the gap degree of the graphs in Figure 3
is 0, 1 and 2, respectively, since the node i has the
maximum gap degree in all three cases.
The well-nestedness constraint restricts the posi-
tioning of disjoint subtrees in a dependency forest.
Two subtrees are called disjoint, if neither of their
roots dominates the other.
Definition 8 Two subtrees T1;T2 interleave, if
there are nodes l1; r1 2 T1 and l2; r2 2 T2 such
that l1 < l2 < r1 < r2. A dependency graph is
well-nested, if no two of its disjoint subtrees inter-
leave.
Both Graph 3a and Graph 3b are well-nested.
Graph 3c is not well-nested. To see this, let T1
be the subtree rooted at the node labelled i , and
let T2 be the subtree rooted at j . These subtrees
interleave, as T1 contains the nodes 2 and 4, and T2
contains the nodes 3 and 5.
509
ji
1 2 3 5 64
(a) gd D 0, ed D 0, wnC
j
i
1 2 3 5 64
(b) gd D 1, ed D 1, wnC
j
i
1 2 3 5 64
(c) gd D 2, ed D 1, wn 
Figure 3: Gap degree, edge degree, and well-nestedness
3.3 Edge degree
The notion of edge degree was introduced by Nivre
(2006) in order to allow mildly non-projective struc-
tures while maintaining good parsing efficiency in
data-driven dependency parsing.2
Define the span of an edge .i; j / as the interval
S..i; j // WD ?min.i; j /;max.i; j /? :
Definition 9 Let G D .V I E/ be a dependency
forest, let e D .i; j / be an edge in E, and let Ge
be the subgraph of G that is induced by the nodes
contained in the span of e.
 The degree of an edge e 2 E, ed.e/, is the
number of connected components c in Ge
such that the root of c is not dominated by
the head of e.
 The edge degree of G, ed.G/, is the maximum
among the degrees of the edges in G.
To illustrate the notion of edge degree, we return
to Figure 3. Graph 3a has edge degree 0: the only
edge that spans more nodes than its head and its de-
pendent is .1; 5/, but the root of the connected com-
ponent f2; 3; 4g is dominated by 1. Both Graph 3b
and 3c have edge degree 1: the edge .3; 6/ in
Graph 3b and the edges .2; 4/, .3; 5/ and .4; 6/ in
Graph 3c each span a single connected component
that is not dominated by the respective head.
3.4 Related work
Apart from proposals for structural constraints re-
laxing projectivity, there are dependency frame-
works that in principle allow unrestricted graphs,
but provide mechanisms to control the actually per-
mitted forms of non-projectivity in the grammar.
The non-projective dependency grammar of Ka-
hane et al (1998) is based on an operation on de-
pendency trees called lifting: a ?lift? of a tree T is
the new tree that is obtained when one replaces one
2We use the term edge degree instead of the original simple
term degree from Nivre (2006) to mark the distinction from
the notion of gap degree.
or more edges .i; k/ in T by edges .j ; k/, where
j ! i . The exact conditions under which a cer-
tain lifting may take place are specified in the rules
of the grammar. A dependency tree is acceptable,
if it can be lifted to form a projective graph.3
A similar design is pursued in Topological De-
pendency Grammar (Duchier and Debusmann,
2001), where a dependency analysis consists of
two, mutually constraining graphs: the ID graph
represents information about immediate domi-
nance, the LP graph models the topological struc-
ture of a sentence. As a principle of the grammar,
the LP graph is required to be a lift of the ID graph;
this lifting can be constrained in the lexicon.
3.5 Discussion
The structural conditions we have presented here
naturally fall into two groups: multiplanarity, gap
degree and edge degree are parametric constraints
with an infinite scale of possible values; planarity
and well-nestedness come as binary constraints.
We discuss these two groups in turn.
Parametric constraints With respect to the
graded constraints, we find that multiplanarity is
different from both gap degree and edge degree
in that it involves a notion of optimization: since
every dependency graph is m-planar for some suf-
ficiently large m (put each edge onto a separate
plane), the interesting question in the context of
multiplanarity is about the minimal values for m
that occur in real-world data. But then, one not
only needs to show that a dependency graph can be
decomposed into m planar graphs, but also that this
decomposition is the one with the smallest number
of planes among all possible decompositions. Up
to now, no tractable algorithm to find the minimal
decomposition has been given, so it is not clear how
to evaluate the significance of the concept as such.
The evaluation presented by Yli-Jyr? (2003) makes
use of additional constraints that are sufficient to
make the decomposition unique.
3We remark that, without restrictions on the lifting, every
non-projective tree has a projective lift.
510
1 2 3 5 64
(a) gd D 2, ed D 1
1 2 3 54
(b) gd D 1, ed D 2
Figure 4: Comparing gap degree and edge degree
The fundamental difference between gap degree
and edge degree is that the gap degree measures the
number of discontinuities within a subtree, while
the edge degree measures the number of interven-
ing constituents spanned by a single edge. This
difference is illustrated by the graphs displayed in
Figure 4. Graph 4a has gap degree 2 but edge de-
gree 1: the subtree rooted at node 2 (marked by
the solid edges) has two gaps, but each of its edges
only spans one connected component not domi-
nated by 2 (marked by the squares). In contrast,
Graph 4b has gap degree 1 but edge degree 2: the
subtree rooted at node 2 has one gap, but this gap
contains two components not dominated by 2.
Nivre (2006) shows experimentally that limiting
the permissible edge degree to 1 or 2 can reduce the
average parsing time for a deterministic algorithm
from quadratic to linear, while omitting less than
1% of the structures found in DDT and PDT. It
can be expected that constraints on the gap degree
would have very similar effects.
Binary constraints For the two binary con-
straints, we find that well-nestedness subsumes
planarity: a graph that contains interleaving sub-
trees cannot be drawn without crossing edges, so
every planar graph must also be well-nested. To see
that the converse does not hold, consider Graph 3b,
which is well-nested, but not planar.
Since both planarity and well-nestedness are
proper extensions of projectivity, we get the fol-
lowing hierarchy for sets of dependency graphs:
projective Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 160?167,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Mildly Context-Sensitive Dependency Languages
Marco Kuhlmann
Programming Systems Lab
Saarland University
Saarbr?cken, Germany
kuhlmann@ps.uni-sb.de
Mathias M?hl
Programming Systems Lab
Saarland University
Saarbr?cken, Germany
mmohl@ps.uni-sb.de
Abstract
Dependency-based representations of natu-
ral language syntax require a fine balance
between structural flexibility and computa-
tional complexity. In previous work, several
constraints have been proposed to identify
classes of dependency structures that are well-
balanced in this sense; the best-known but
also most restrictive of these is projectivity.
Most constraints are formulated on fully spec-
ified structures, which makes them hard to in-
tegrate into models where structures are com-
posed from lexical information. In this paper,
we show how two empirically relevant relax-
ations of projectivity can be lexicalized, and
how combining the resulting lexicons with a
regular means of syntactic composition gives
rise to a hierarchy of mildly context-sensitive
dependency languages.
1 Introduction
Syntactic representations based on word-to-word de-
pendencies have a long tradition in descriptive lin-
guistics. Lately, they have also been used in many
computational tasks, such as relation extraction (Cu-
lotta and Sorensen, 2004), parsing (McDonald et al,
2005), and machine translation (Quirk et al, 2005).
Especially in recent work on parsing, there is a par-
ticular interest in non-projective dependency struc-
tures, in which a word and its dependents may be
spread out over a discontinuous region of the sen-
tence. These structures naturally arise in the syntactic
analysis of languages with flexible word order, such
as Czech (Vesel? et al, 2004). Unfortunately, most
formal results on non-projectivity are discouraging:
While grammar-driven dependency parsers that are
restricted to projective structures can be as efficient
as parsers for lexicalized context-free grammar (Eis-
ner and Satta, 1999), parsing is prohibitively expen-
sive when unrestricted forms of non-projectivity are
permitted (Neuhaus and Br?ker, 1997). Data-driven
dependency parsing with non-projective structures is
quadratic when all attachment decisions are assumed
to be independent of one another (McDonald et al,
2005), but becomes intractable when this assumption
is abandoned (McDonald and Pereira, 2006).
In search of a balance between structural flexibility
and computational complexity, several authors have
proposed constraints to identify classes of non-projec-
tive dependency structures that are computationally
well-behaved (Bodirsky et al, 2005; Nivre, 2006).
In this paper, we focus on two of these proposals:
the gap-degree restriction, which puts a bound on
the number of discontinuities in the region of a sen-
tence covered by a word and its dependents, and the
well-nestedness condition, which constrains the ar-
rangement of dependency subtrees. Both constraints
have been shown to be in very good fit with data from
dependency treebanks (Kuhlmann and Nivre, 2006).
However, like all other such proposals, they are for-
mulated on fully specified structures, which makes it
hard to integrate them into a generative model, where
dependency structures are composed from elemen-
tary units of lexicalized information. Consequently,
little is known about the generative capacity and com-
putational complexity of languages over restricted
non-projective dependency structures.
160
Contents of the paper In this paper, we show how
the gap-degree restriction and the well-nestedness
condition can be captured in dependency lexicons,
and how combining such lexicons with a regular
means of syntactic composition gives rise to an infi-
nite hierarchy of mildly context-sensitive languages.
The technical key to these results is a procedure
to encode arbitrary, even non-projective dependency
structures into trees (terms) over a signature of local
order-annotations. The constructors of these trees
can be read as lexical entries, and both the gap-de-
gree restriction and the well-nestedness condition
can be couched as syntactic properties of these en-
tries. Sets of gap-restricted dependency structures
can be described using regular tree grammars. This
gives rise to a notion of regular dependency lan-
guages, and allows us to establish a formal relation
between the structural constraints and mildly con-
text-sensitive grammar formalisms (Joshi, 1985): We
show that regular dependency languages correspond
to the sets of derivations of lexicalized Linear Con-
text-Free Rewriting Systems (lcfrs) (Vijay-Shanker
et al, 1987), and that the gap-degree measure is the
structural correspondent of the concept of ?fan-out?
in this formalism (Satta, 1992). We also show that
adding the well-nestedness condition corresponds
to the restriction of lcfrs to Coupled Context-Free
Grammars (Hotz and Pitsch, 1996), and that regu-
lar sets of well-nested structures with a gap-degree
of at most 1 are exactly the class of sets of deriva-
tions of Lexicalized Tree Adjoining Grammar (ltag).
This result generalizes previous work on the relation
between ltag and dependency representations (Ram-
bow and Joshi, 1997; Bodirsky et al, 2005).
Structure of the paper The remainder of this pa-
per is structured as follows. Section 2 contains some
basic notions related to trees and dependency struc-
tures. In Section 3 we present the encoding of depen-
dency structures as order-annotated trees, and show
how this encoding allows us to give a lexicalized re-
formulation of both the gap-degree restriction and the
well-nestedness condition. Section 4 introduces the
notion of regular dependency languages. In Section 5
we show how different combinations of restrictions
on non-projectivity in these languages correspond
to different mildly context-sensitive grammar for-
malisms. Section 6 concludes the paper.
2 Preliminaries
Throughout the paper, we write ?n? for the set of all
positive natural numbers up to and including n. The
set of all strings over a set A is denoted by A, the
empty string is denoted by ", and the concatenation
of two strings x and y is denoted either by xy, or,
where this is ambiguous, by x  y.
2.1 Trees
In this paper, we regard trees as terms. We expect the
reader to be familiar with the basic concepts related
to this framework, and only introduce our particular
notation. Let ? be a set of labels. The set of (finite,
unranked) trees over ? is defined recursively by the
equation T? ? f .x/ j  2 ?; x 2 T ? g. The set
of nodes of a tree t 2 T? is defined as
N..t1    tn//? f"g [ f iu j i 2 ?n?; u 2 N.ti / g :
For two nodes u; v 2 N.t/, we say that u governs v,
and write u E v, if v can be written as v D ux, for
some sequence x 2 N. Note that the governance
relation is both reflexive and transitive. The converse
of government is called dependency, so u E v can
also be read as ?v depends on u?. The yield of a
node u 2 N.t/, buc, is the set of all dependents of u
in t : buc ? f v 2 N.t/ j u E v g. We also use the
notations t .u/ for the label at the node u of t , and
t=u for the subtree of t rooted at u. A tree language
over ? is a subset of T? .
2.2 Dependency structures
For the purposes of this paper, a dependency structure
over ? is a pair d D .t; x/, where t 2 T? is a tree,
and x is a list of the nodes in t . We write D? to
refer to the set of all dependency structures over ? .
Independently of the governance relation in d , the
list x defines a total order on the nodes in t ; we
write u  v to denote that u precedes v in this order.
Note that, like governance, the precedence relation is
both reflexive and transitive. A dependency language
over ? is a subset of D? .
Example. The left half of Figure 1 shows how we
visualize dependency structures: circles represent
nodes, arrows represent the relation of (immediate)
governance, the left-to-right order of the nodes repre-
sents their order in the precedence relation, and the
dotted lines indicate the labelling. 
161
a b c d e f
2
11
1
1
hf; 0i
he; 01i
ha; 012i
hc ; 0i
hd ; 10i
hb ; 01i
Figure 1: A projective dependency structure
3 Lexicalizing the precedence relation
In this section, we show how the precedence relation
of dependency structures can be encoded as, and
decoded from, a collection of node-specific order
annotations. Under the assumption that the nodes of
a dependency structure correspond to lexemic units,
this result demonstrates how word-order information
can be captured in a dependency lexicon.
3.1 Projective structures
Lexicalizing the precedence relation of a dependency
structure is particularly easy if the structure under
consideration meets the condition of projectivity. A
dependency structure is projective, if each of its
yields forms an interval with respect to the prece-
dence order (Kuhlmann and Nivre, 2006).
In a projective structure, the interval that corre-
sponds to a yield buc decomposes into the singleton
interval ?u; u?, and the collection of the intervals that
correspond to the yields of the immediate dependents
of u. To reconstruct the global precedence relation,
it suffices to annotate each node u with the relative
precedences among the constituent parts of its yield.
We represent this ?local? order as a string over the
alphabet N0, where the symbol 0 represents the sin-
gleton interval ?u; u?, and a symbol i ? 0 represents
the interval that corresponds to the yield of the i th
direct dependent of u. An order-annotated tree is a
tree labelled with pairs h; !i, where  is the label
proper, and ! is a local order annotation. In what
follows, we will use the functional notations .u/
and !.u/ to refer to the label and order annotation
of u, respectively.
Example. Figure 1 shows a projective dependency
structure together with its representation as an order-
annotated tree. 
We now present procedures for encoding projec-
tive dependency structures into order-annotated trees,
and for reversing this encoding.
Encoding The representation of a projective depen-
dency structure .t; x/ as an order-annotated tree can
be computed in a single left-to-right sweep over x.
Starting with a copy of the tree t in which every
node is annotated with the empty string, for each new
node u in x, we update the order annotation of u
through the assignment !.u/? !.u/ 0 . If u D vi
for some i 2 N (that is, if u is an inner node), we
also update the order annotation of the parent v of u
through the assignment !.v/? !.v/  i .
Decoding To decode an order-annotated tree t , we
first linearize the nodes of t into a sequence x, and
then remove all order annotations. Linearization pro-
ceeds in a way that is very close to a pre-order traver-
sal of the tree, except that the relative position of
the root node of a subtree is explicitly specified in
the order annotation. Specifically, to linearize an or-
der-annotated tree, we look into the local order !.u/
annotated at the root node of the tree, and concatenate
the linearizations of its constituent parts. A symbol i
in !.u/ represents either the singleton interval ?u; u?
(i D 0), or the interval corresponding to some direct
dependent ui of u (i ? 0), in which case we pro-
ceed recursively. Formally, the linearization of u is
captured by the following three equations:
lin.u/ D lin0.u; !.u//
lin0.u; i1    in/ D lin00.u; i1/    lin00.u; in/
lin00.u; i/ D if i D 0 then u else lin.ui/
Both encoding and decoding can be done in time
linear in the number of nodes of the dependency
structure or order-annotated tree.
3.2 Non-projective structures
It is straightforward to see that our representation of
dependency structures is insufficient if the structures
under consideration are non-projective. To witness,
consider the structure shown in Figure 2. Encoding
this structure using the procedure presented above
yields the same order-annotated tree as the one shown
in Figure 1, which demonstrates that the encoding is
not reversible.
162
a b c de f
1
2
1
1
1
ha; h01212ii
hc; h0ii
he; h0; 1ii
hf ; h0ii
hb ; h01; 1ii
hd ; h1; 0ii
Figure 2: A non-projective dependency structure
Blocks In a non-projective dependency structure,
the yield of a node may be spread out over more than
one interval; we will refer to these intervals as blocks.
Two nodes v;w belong to the same block of a node u,
if all nodes between v and w are governed by u.
Example. Consider the nodes b; c; d in the struc-
tures depicted in Figures 1 and 2. In Figure 1, these
nodes belong to the same block of b. In Figure 2,
the three nodes are spread out over two blocks of b
(marked by the boxes): c and d are separated by a
node (e) not governed by b. 
Blocks have a recursive structure that is closely re-
lated to the recursive structure of yields: the blocks of
a node u can be decomposed into the singleton ?u; u?,
and the blocks of the direct dependents of u. Just as
a projective dependency structure can be represented
by annotating each yield with an order on its con-
stituents, an unrestricted structure can be represented
by annotating each block.
Extended order annotations To represent orders
on blocks, we extend our annotation scheme as fol-
lows. First, instead of a single string, an annotation
!.u/ now is a tuple of strings, where the kth com-
ponent specifies the order among the constituents of
the kth block of u. Second, instead of one, the an-
notation may now contain multiple occurrences of
the same dependent; the kth occurrence of i in !.u/
represents the kth block of the node ui .
We write !.u/k to refer to the kth component of
the order annotation of u. We also use the notation
.i#k/u to refer to the kth occurrence of i in !.u/,
and omit the subscript when the node u is implicit.
Example. In the annotated tree shown in Figure 2,
!.b/1 D .0#1/.1#1/, and !.b/2 D .1#2/. 
Encoding To encode a dependency structure .t; x/
as an extended order-annotated tree, we do a post-
order traversal of t as follows. For a given node u, let
us represent a constituent of a block of u as a triple
i W ?vl ; vr ?, where i denotes the node that contributes
the constituent, and vl and vr denote the constituent?s
leftmost and rightmost elements. At each node u, we
have access to the singleton block 0 W ?u; u?, and the
constituent blocks of the immediate dependents of u.
We say that two blocks i W ?vl ; vr ?; j W ?wl ; wr ? can
be merged, if the node vr immediately precedes the
node wl . The result of the merger is a new block ij W
?vl ; wr ? that represents the information that the two
merged constituents belong to the same block of u.
By exhaustive merging, we obtain the constituent
structure of all blocks of u. From this structure, we
can read off the order annotation !.u/.
Example. The yield of the node b in Figure 2 de-
composes into 0 W ?b; b?, 1 W ?c; c?, and 1 W ?d; d ?.
Since b and c are adjacent, the first two of these con-
stituents can be merged into a new block 01 W ?b; c?;
the third constituent remains unchanged. This gives
rise to the order annotation h01; 1i for b. 
When using a global data-structure to keep track
of the constituent blocks, the encoding procedure can
be implemented to run in time linear in the number
of blocks in the dependency structure. In particular,
for projective dependency structures, it still runs in
time linear in the number of nodes.
Decoding To linearize the kth block of a node u,
we look into the kth component of the order anno-
tated at u, and concatenate the linearizations of its
constituent parts. Each occurrence .i#k/ in a com-
ponent of !.u/ represents either the node u itself
(i D 0), or the kth block of some direct dependent ui
of u (i ? 0), in which case we proceed recursively:
lin.u; k/ D lin0.u; !.u/k/
lin0.u; i1    in/ D lin00.u; i1/    lin00.u; in/
lin00.u; .i#k/u/ D if i D 0 then u else lin.ui; k/
The root node of a dependency structure has only
one block. Therefore, to linearize a tree t , we only
need to linearize the first block of the tree?s root node:
lin.t/ D lin."; 1/.
163
Consistent order annotations Every dependency
structure over? can be encoded as a tree over the set
? ?, where ? is the set of all order annotations.
The converse of this statement does not hold: to be
interpretable as a dependency structure, tree structure
and order annotation in an order-annotated tree must
be consistent, in the following sense.
Property C1: Every annotation !.u/ in a tree t
contains all and only the symbols in the collection
f0g [ f i j ui 2 N.t/ g, i.e., one symbol for u, and
one symbol for every direct dependent of u.
Property C2: The number of occurrences of a
symbol i ? 0 in !.u/ is identical to the number of
components in the annotation of the node ui . Further-
more, the number of components in the annotation
of the root node is 1.
With this notion of consistency, we can prove the
following technical result about the relation between
dependency structures and annotated trees. We write
? .s/ for the tree obtained from a tree s 2 T??
by re-labelling every node u with .u/.
Proposition 1. For every dependency structure
.t; x/ over ? , there exists a tree s over ? ? such
that ? .s/ D t and lin.s/ D x. Conversely, for
every consistently order-annotated tree s 2 T?? ,
there exists a uniquely determined dependency struc-
ture .t; x/ with these properties. 
3.3 Local versions of structural constraints
The encoding of dependency structures as order-an-
notated trees allows us to reformulate two constraints
on non-projectivity originally defined on fully speci-
fied dependency structures (Bodirsky et al, 2005) in
terms of syntactic properties of the order annotations
that they induce:
Gap-degree The gap-degree of a dependency
structure is the maximum over the number of dis-
continuities in any yield of that structure.
Example. The structure depicted in Figure 2 has
gap-degree 1: the yield of b has one discontinuity,
marked by the node e, and this is the maximal number
of discontinuities in any yield of the structure. 
Since a discontinuity in a yield is delimited by two
blocks, and since the number of blocks of a node u
equals the number of components in the order anno-
tation of u, the following result is obvious:
Proposition 2. A dependency structure has gap-de-
gree k if and only if the maximal number of compo-
nents among the annotations !.u/ is k C 1. 
In particular, a dependency structure is projective iff
all of its annotations consist of just one component.
Well-nestedness The well-nestedness condition
constrains the arrangement of subtrees in a depen-
dency structure. Two subtrees t=u1; t=u2 interleave,
if there are nodes v1l ; v
1
r 2 t=u1 and v
2
l ; v
2
r 2 t=u2
such that v1l  v
2
l  v
1
r  v
2
r . A dependency struc-
ture is well-nested, if no two of its disjoint subtrees
interleave. We can prove the following result:
Proposition 3. A dependency structure is well-
nested if and only if no annotation !.u/ contains
a substring i    j    i    j , for i; j 2 N. 
Example. The dependency structure in Figure 1 is
well-nested, the structure depicted in Figure 2 is not:
the subtrees rooted at the nodes b and e interleave.
To see this, notice that b  e  d  f . Also notice
that !.a/ contains the substring 1212. 
4 Regular dependency languages
The encoding of dependency structures as order-an-
notated trees gives rise to an encoding of dependency
languages as tree languages. More specifically, de-
pendency languages over a set ? can be encoded
as tree languages over the set ?  ?, where ? is
the set of all order annotations. Via this encoding,
we can study dependency languages using the tools
and results of the well-developed formal theory of
tree languages. In this section, we discuss depen-
dency languages that can be encoded as regular tree
languages.
4.1 Regular tree grammars
The class of regular tree languages, REGT for short,
is a very natural class with many characterizations
(G?cseg and Steinby, 1997): it is generated by regular
tree grammars, recognized by finite tree automata,
and expressible in monadic second-order logic. Here
we use the characterization in terms of grammars.
Regular tree grammars are natural candidates for the
formalization of dependency lexicons, as each rule
in such a grammar can be seen as the specification of
a word and the syntactic categories or grammatical
functions of its immediate dependents.
164
Formally, a (normalized) regular tree grammar is
a construct G D .NG ; ?G ; SG ; PG/, in which NG
and ?G are finite sets of non-terminal and termi-
nal symbols, respectively, SG 2 NG is a dedicated
start symbol, and PG is a finite set of productions
of the form A ! .A1   An/, where  2 ?G ,
A 2 NG , and Ai 2 NG , for every i 2 ?n?. The (di-
rect) derivation relation associated to G is the binary
relation)G on the set T?G[NG defined as follows:
t 2 T?G[NG t=u D A .A! s/ 2 PG
t )G t ?u 7! s?
Informally, each step in a derivation replaces a non-
terminal-labelled leaf by the right-hand side of a
matching production. The tree language generated
by G is the set of all terminal trees that can eventu-
ally be derived from the trivial tree formed by its start
symbol: L.G/ D f t 2 T?G j SG )

G t g.
4.2 Regular dependency grammars
We call a dependency language regular, if its encod-
ing as a set of trees over ? ? forms a regular tree
language, and write REGD for the class of all regular
dependency languages. For every regular dependency
language L, there is a regular tree grammar with ter-
minal alphabet ? ? that generates the encoding
of L. Similar to the situation with individual struc-
tures, the converse of this statement does not hold:
the consistency properties mentioned above impose
corresponding syntactic restrictions on the rules of
grammars G that generate the encoding of L.
Property C10: The !-component of every pro-
ductionA! h; !i.A1   An/ inG contains all and
only symbols in the set f0g [ f i j i 2 ?n? g.
Property C20: For every non-terminal X 2 NG ,
there is a uniquely determined integer dX such that
for every production A ! h; !i.A1   An/ in G,
dAi gives the number of occurrences of i in !, dA
gives the number of components in !, and dSG D 1.
It turns out that these properties are in fact sufficient
to characterize the class of regular tree grammars that
generate encodings of dependency languages. In but
slight abuse of terminology, we will refer to such
grammars as regular dependency grammars.
Example. Figure 3 shows a regular tree grammar
that generates a set of non-projective dependency
structures with string language f anbn j n  1 g. 
a b b baa
B
B
B
S
A
A
S ! ha; h01ii.B/ j ha; h0121ii.A;B/
A ! ha; h0; 1ii.B/ j ha; h01; 21ii.A;B/
B ! hb; h0ii
Figure 3: A grammar for a language in REGD.1/
5 Structural constraints and formal power
In this section, we present our results on the genera-
tive capacity of regular dependency languages, link-
ing them to a large class of mildly context-sensitive
grammar formalisms.
5.1 Gap-restricted dependency languages
A dependency language L is called gap-restricted, if
there is a constant cL  0 such that no structure in L
has a gap-degree higher than cL. It is plain to see that
every regular dependency language is gap-restricted:
the gap-degree of a structure is directly reflected in
the number of components of its order annotations,
and every regular dependency grammar makes use of
only a finite number of these annotations. We write
REGD.k/ to refer to the class of regular dependency
languages with a gap-degree bounded by k.
Linear Context-Free Rewriting Systems Gap-re-
stricted dependency languages are closely related
to Linear Context-Free Rewriting Systems (lcfrs)
(Vijay-Shanker et al, 1987), a class of formal sys-
tems that generalizes several mildly context-sensitive
grammar formalisms. An lcfrs consists of a regular
tree grammar G and an interpretation of the terminal
symbols of this grammar as linear, non-erasing func-
tions into tuples of strings. By these functions, each
tree in L.G/ can be evaluated to a string.
Example. Here is an example for a function:
f .hx11 ; x
2
1i; hx
1
2i/ D hax
1
1 ; x
1
2x
2
1i
This function states that in order to compute the pair
of strings that corresponds to a tree whose root node
is labelled with the symbol f , one first has to com-
pute the pair of strings corresponding to the first child
165
of the root node (hx11 ; x
2
1i) and the single string cor-
responding to the second child (hx12i), and then con-
catenate the individual components in the specified
order, preceded by the terminal symbol a. 
We call a function lexicalized, if it contributes ex-
actly one terminal symbol. In an lcfrs in which all
functions are lexicalized, there is a one-to-one cor-
respondence between the nodes in an evaluated tree
and the positions in the string that the tree evaluates
to. Therefore, tree and string implicitly form a depen-
dency structure, and we can speak of the dependency
language generated by a lexicalized lcfrs.
Equivalence We can prove that every regular de-
pendency grammar can be transformed into a lexi-
calized lcfrs that generates the same dependency
language, and vice versa. The basic insight in this
proof is that every order annotation in a regular de-
pendency grammar can be interpreted as a compact
description of a function in the corresponding lcfrs.
The number of components in the order-annotation,
and hence, the gap-degree of the resulting depen-
dency language, corresponds to the fan-out of the
function: the highest number of components among
the arguments of the function (Satta, 1992).1 A tech-
nical difficulty is caused by the fact that lcfrs can
swap components: f .hx11 ; x
2
1i/ D hax
2
1 ; x
1
1i. This
commutativity needs to be compiled out during the
translation into a regular dependency grammar.
We write LLCFRL.k/ for the class of all depen-
dency languages generated by lexicalized lcfrs with
a fan-out of at most k.
Proposition 4. REGD.k/ D LLCFRL.k C 1/ 
In particular, the class REGD.0/ of regular depen-
dency languages over projective structures is exactly
the class of dependency languages generated by lexi-
calized context-free grammars.
Example. The gap-degree of the language generated
by the grammar in Figure 3 is bounded by 1. The
rules for the non-terminal A can be translated into
the following functions of an equivalent lcfrs:
fha;h0;1ii.hx
1
1i/ D ha; x
1
1i
fha;h01;21ii.hx
1
1 ; x
2
1i; hx
1
2i/ D hax
1
1 ; x
1
2x
2
1i
The fan-out of these functions is 2. 
1More precisely, gap-degree D fan-out   1.
5.2 Well-nested dependency languages
The absence of the substring i    j    i    j in the
order annotations of well-nested dependency struc-
tures corresponds to a restriction to ?well-bracketed?
compositions of sub-structures. This restriction is
central to the formalism of Coupled-Context-Free
Grammar (ccfg) (Hotz and Pitsch, 1996).
It is straightforward to see that every ccfg can
be translated into an equivalent lcfrs. We can also
prove that every lcfrs obtained from a regular depen-
dency grammar with well-nested order annotations
can be translated back into an equivalent ccfg. We
write REGDwn.k/ for the well-nested subclass of
REGD.k/, and LCCFL.k/ for the class of all depen-
dency languages generated by lexicalized ccfgs with
a fan-out of at most k.
Proposition 5. REGDwn.k/ D LCCFL.k C 1/ 
As a special case, Coupled-Context-Free Grammars
with fan-out 2 are equivalent to Tree Adjoining Gram-
mars (tags) (Hotz and Pitsch, 1996). This enables
us to generalize a previous result on the class of de-
pendency structures generated by lexicalized tags
(Bodirsky et al, 2005) to the class of generated de-
pendency languages, LTAL.
Proposition 6. REGDwn.1/ D LTAL 
6 Conclusion
In this paper, we have presented a lexicalized refor-
mulation of two structural constraints on non-pro-
jective dependency representations, and shown that
combining dependency lexicons that satisfy these
constraints with a regular means of syntactic com-
position yields classes of mildly context-sensitive
dependency languages. Our results make a signif-
icant contribution to a better understanding of the
relation between the phenomenon of non-projectivity
and notions of formal power.
The close link between restricted forms of non-
projective dependency languages and mildly context-
sensitive grammar formalisms provides a promising
starting point for future work. On the practical side,
it should allow us to benefit from the experience
in building parsers for mildly context-sensitive for-
malisms when addressing the task of efficient non-
projective dependency parsing, at least in the frame-
166
work of grammar-driven parsing. This may even-
tually lead to a better trade-off between structural
flexibility and computational efficiency than that ob-
tained with current systems. On a more theoretical
level, our results provide a basis for comparing a va-
riety of formally rather distinct grammar formalisms
with respect to the sets of dependency structures that
they can generate. Such a comparison may be empir-
ically more adequate than one based on traditional
notions of generative capacity (Kallmeyer, 2006).
Acknowledgements We thank Guido Tack, Stefan
Thater, and the anonymous reviewers of this paper
for their detailed comments. The work of the authors
is funded by the German Research Foundation.
References
Manuel Bodirsky, Marco Kuhlmann, and Mathias M?hl.
2005. Well-nested drawings as models of syntactic
structure. In Tenth Conference on Formal Grammar
and Ninth Meeting on Mathematics of Language, Edin-
burgh, Scotland, UK.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In 42nd Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 423?429, Barcelona, Spain.
Jason Eisner and Giorgio Satta. 1999. Efficient parsing
for bilexical context-free grammars and head automa-
ton grammars. In 37th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
457?464, College Park, Maryland, USA.
Ferenc G?cseg and Magnus Steinby. 1997. Tree lan-
guages. In Grzegorz Rozenberg and Arto Salomaa,
editors, Handbook of Formal Languages, volume 3,
pages 1?68. Springer-Verlag, New York, USA.
G?nter Hotz and Gisela Pitsch. 1996. On parsing coupled-
context-free languages. Theoretical Computer Science,
161:205?233.
Aravind K. Joshi. 1985. Tree adjoining grammars: How
much context-sensitivity is required to provide reason-
able structural descriptions? In David R. Dowty, Lauri
Karttunen, and Arnold M. Zwicky, editors, Natural Lan-
guage Parsing, pages 206?250. Cambridge University
Press, Cambridge, UK.
Laura Kallmeyer. 2006. Comparing lexicalized grammar
formalisms in an empirically adequate way: The notion
of generative attachment capacity. In International
Conference on Linguistic Evidence, pages 154?156,
T?bingen, Germany.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly non-
projective dependency structures. In 21st International
Conference on Computational Linguistics and 44th An-
nual Meeting of the Association for Computational Lin-
guistics (COLING-ACL) Main Conference Poster Ses-
sions, pages 507?514, Sydney, Australia.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Eleventh Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL), pages 81?88, Trento, Italy.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan
Hajic?. 2005. Non-projective dependency parsing using
spanning tree algorithms. In Human Language Technol-
ogy Conference (HLT) and Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 523?530, Vancouver, British Columbia, Canada.
Peter Neuhaus and Norbert Br?ker. 1997. The complexity
of recognition of linguistically adequate dependency
grammars. In 35th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 337?343,
Madrid, Spain.
Joakim Nivre. 2006. Constraints on non-projective depen-
dency parsing. In Eleventh Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL), pages 73?80, Trento, Italy.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically informed
phrasal smt. In 43rd Annual Meeting of the Association
for Computational Linguistics (ACL), pages 271?279,
Ann Arbor, USA.
Owen Rambow and Aravind K. Joshi. 1997. A for-
mal look at dependency grammars and phrase-structure
grammars. In Leo Wanner, editor, Recent Trends in
Meaning-Text Theory, volume 39 of Studies in Lan-
guage, Companion Series, pages 167?190. John Ben-
jamins, Amsterdam, The Netherlands.
Giorgio Satta. 1992. Recognition of linear context-free
rewriting systems. In 30th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
89?95, Newark, Delaware, USA.
Katerina Vesel?, Jir?i Havelka, and Eva Hajic?ova. 2004.
Condition of projectivity in the underlying depen-
dency structures. In 20th International Conference on
Computational Linguistics (COLING), pages 289?295,
Geneva, Switzerland.
K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi.
1987. Characterizing structural descriptions produced
by various grammatical formalisms. In 25th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 104?111, Stanford, California, USA.
167
Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 121?126,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Extended cross-serial dependencies in Tree Adjoining Grammars
Marco Kuhlmann and Mathias M?hl
Programming Systems Lab
Saarland University
Saarbr?cken, Germany
{kuhlmann|mmohl}@ps.uni-sb.de
Abstract
The ability to represent cross-serial depen-
dencies is one of the central features of
Tree Adjoining Grammar (TAG). The class
of dependency structures representable by
lexicalized TAG derivations can be captured
by two graph-theoretic properties: a bound
on the gap degree of the structures, and a
constraint called well-nestedness. In this
paper, we compare formalisms from two
strands of extensions to TAG in the context
of the question, how they behave with re-
spect to these constraints. In particular, we
show that multi-component TAG does not
necessarily retain the well-nestedness con-
straint, while this constraint is inherent to
Coupled Context-Free Grammar (Hotz and
Pitsch, 1996).
1 Introduction
The ability to assign ?limited cross-serial depen-
dencies? to the words in a sentence is a hallmark
of mildly context-sensitive grammar formalisms
(Joshi, 1985). In the case of TAG, an exact def-
inition of this ability can be given in terms of
two graph-theoretic properties of the dependency
structures induced by TAG derivations: the gap de-
gree restriction and the well-nestedness constraint
(Bodirsky et al, 2005).
Gap degree and well-nestedness can be seen as
the formal correspondents of what Joshi (1985)
refers to as ?a limited amount of cross-serial depen-
dencies? and ?the nesting properties as in the case
of context-free grammars.? More specifically, the
gap degree of a dependency structure counts the
number of discontinuities in a dependency subtree,
while well-nestedness constrains the positions of
disjoint subtrees relative to one another. The depen-
dency structures that correspond to the derivations
in a lexicalized TAG are well-nested, and their gap
degree is at most 1.
In the present paper, we compare formalisms
from two strands of extensions to TAG in the con-
text of the question, what classes of dependency
structures they are able to induce.
We are particularly interested in formalisms that
induce only well-nested dependency structures.
This interest is motivated by two observations:
First, well-nestedness is interesting as a generaliza-
tion of projectivity (Marcus, 1967)?while more
than 23% of the 73 088 dependency structures in
the Prague Dependency Treebank of Czech (Ha-
jic? et al, 2001) are non-projective, only 0.11%
are not well-nested (Kuhlmann and Nivre, 2006).
Second, well-nestedness is interesting for process-
ing. Specifically, parsers for well-nested grammar
formalisms are not confronted with the ?crossing
configurations? that make the universal recogni-
tion problem of Linear Context-Free Rewriting Sys-
tems NP-complete (Satta, 1992). In summary, it
appears that well-nestedness can strike a successful
balance between empirical coverage and computa-
tional tractability. If this is true, then a formalism
that has the well-nestedness constraint hardwired
is preferable over one that has not.
The results of this paper can be summarized
as follows: Derivations in lexicalized multi-com-
ponent TAGs (Weir, 1988; Kallmeyer, 2005), in
which a single adjunction adds a set of elemen-
tary trees, either induce exactly the same depen-
dency structures as TAG, or induce all structures
of bounded gap degree, even non-well-nested ones.
This depends on the decision whether one takes
?lexicalized? to mean ?one lexical anchor per tree?,
or ?one lexical anchor per tree set?. In contrast,
multi-foot extensions of TAG (Abe, 1988; Hotz
and Pitsch, 1996), where a single elementary tree
may have more than one foot node, only induce
well-nested dependency structures of bounded gap
degree. Thus, from the dependency point of view,
they constitute the structurally more conservative
extension of TAG.
121
2 Dependency structures for TAG
We start with a presentation of the dependency
view on TAG that constitutes the basis for our work,
and introduce the relevant terminology. The main
objective of this section is to provide intuitions; for
the formal details, see Bodirsky et al (2005).
2.1 The dependency view on TAG
Let s D w1   wn be a sentence (a sequence of
tokens). By a dependency structure for s, we mean
a tuple .W;!;/, where W D fw1; : : : ; wng, and
! D f .wi ; wj / 2 W W j wj depends on wi g
 D f .wi ; wj / 2 W W j i < j g
To interpret a grammar formalism as a specifica-
tion for a set of dependency structures, we need to
assign meaning to the relation ?depends? in terms
of this formalism. For TAG, this can be done based
on the Fundamental Hypothesis that ?every syntac-
tic dependency is expressed locally within a single
elementary tree? (Frank, 2002). More specifically,
a derivation in a (strongly) lexicalized TAG can
be viewed as a dependency structure as follows:
The set W contains the (occurences of) lexical an-
chors involved in the derivation. For two anchors
wi ; wj 2 W , wi ! wj if the elementary tree an-
chored at wj was substituted or adjoined into the
tree anchored at wi . We then have wi  wj if wi
precedes wj in the yield of the derived tree cor-
responding to the derivation. Notice that the rela-
tion ! in such a dependency structure is almost
exactly the derivation tree of the underlying TAG
derivation; the only difference is that elementary
trees have been replaced by their lexical anchors.
Figure 1 shows a TAG grammar together with a
dependency structure induced by a derivation of
this grammar. Tokens in the derived string are rep-
resented by labelled nodes; the solid arcs between
the nodes represent the dependencies.
2.2 Gap degree and well-nestedness
An interesting feature of the dependency structure
shown in Figure 1 is that it violates a standard
constraint on dependency structures known as pro-
jectivity (Marcus, 1967). We introduce some termi-
nology for non-projective dependency structures:
A set T  W is convex, if for no two tokens
w1; w2 2 T , there exists a token w from W   T
such that w1  w  w2. The cover of T , C.T /,
is the smallest convex set that contains T . For
w 2 W , we write #w for the set of tokens in the
S;
a T D
B C
T ;
a T D
B ? C
B;
b
C ;
c
D;
d
.
a1 a2 b2 b1 c1 c2 d2 d1
Figure 1: TAG grammar for anbncndn, and a de-
pendency structure induced by this grammar
subtree rooted at w (including w itself). A gap in
#w is a largest convex set in C.#w/ #w. The gap
degree of w, gd.w/, is the number of gaps in #w.
The gaps in #w partition #w into gd.w/ 1 largest
convex blocks; we write #iw to refer to the i-th
of these blocks, counted from left to right (with
respect to ). The gap degree of a dependency
structure is the maximum over the gap degrees of its
subtrees; we writeDg for the set of all dependency
structures with a gap degree of at most g.
The gap degree provides a quantitative measure
for the non-projectivity of dependency structures.
Well-nestedness is a qualitative property: it con-
strains the relative positions of disjoint subtrees.
Let w1; w2 2 W such that #w1 and #w2 are dis-
joint. Four tokens w11 ; w
2
1 2 #w1, w
1
2 ; w
2
2 2 #w2
interleave, if w11  w
1
2  w
2
1  w
2
2 . A depen-
dency structure is well-nested, if it does not contain
interleaving tokens. We write Dwn for the set of all
well-nested dependency structures.
For illustration, consider again the dependency
structure shown in Figure 1. It has gap degree 1:
a2 is the only token w for which #w is not convex;
the set fb1; c1g forms a gap in #a2. The structure
is also well-nested. In contrast, the structure shown
in the right half of Figure 2 is not well-nested; the
tokens b; c; d; e interleave. Bodirsky et al (2005)
show that TAG induces precisely the set Dwn \D1.
3 Multi-component extensions
Multi-component TAG (MCTAG) extends TAG with
the ability to adjoin a whole set of elementary trees
(components) simultaneously. To answer the ques-
tion, whether this extension also leads to an ex-
tended class of dependency structures, we first need
to decide how we want to transfer the Fundamental
Hypothesis (Frank, 2002) to MCTAGs.
122
A;
a B1 C1 B2 C2
8
?
?
<
?
?
:
B;1
b
B;2
D
9
>
>
=
>
>
;
8
?
?
<
?
?
:
C ;1
c
C ;2
E
9
>
>
=
>
>
;
D;
d
E;
e a b c d e
Figure 2: An MCTAG and a not well-nested dependency structure derived by it.
3.1 One anchor per component
If we commit to the view that each component of
a tree set introduces a separate lexical anchor and
its syntactic dependencies, the dependency struc-
tures induced by MCTAG are exactly the structures
induced by TAG. In particular, each node in the
derivation tree, and therefore each token in the
dependency tree, corresponds to a single elemen-
tary tree. As Kallmeyer (2005) puts it, one can
then consider an MCTAG as a TAG G ?where cer-
tain derivation trees in G are disallowed since they
do not satisfy certain constraints.? The ability of
MCTAG to perform multiple adjunctions simultane-
ously allows one to induce more complex sets of
dependency structures?each individual structure
is limited as in the case of standard TAG.
3.2 One anchor per tree set
If, on the other hand, we take a complete tree set
as the level on which syntactic dependencies are
specified, MCTAGs can induce a larger class of de-
pendency structures. Under this perspective, tokens
in the dependency structure correspond not to in-
dividual components, but to tree sets (Weir, 1988).
For each token w, #w then contains the lexical an-
chors of all the subderivations starting in the tree set
corresponding to w. As there can be a gap between
each two of these subderivations, the gap degree
of the induced dependency structures is bounded
only by the maximal number of components per
tree set. At the same time, even non-well-nested
structures can be induced; an example is shown in
Figure 2. Here, #b is distributed over the compo-
nents rooted at B1 and B2, and #c is distributed
over C1 and C2. The elementary tree rooted at A
arranges the substitution sites such that b; c; d; e in-
terleave. Note that the MCTAG used in this example
is heavily restricted: it is tree-local and does not
even use adjunction. This restricted form suffices
to induce non-well-nested dependency structures.
4 Multi-foot extensions
A second way to extend TAG, orthogonal to the
multi-component approach, is to allow a single el-
ementary tree to have more than one foot node.
For this kind of extension, the Fundamental Hy-
pothesis does not need to be re-interpreted. Prob-
ably the most prominent multi-foot extension of
TAG is Ranked Node Rewriting Grammar (RNRG)
(Abe, 1988); however, the properties that we are
interested in here can be easier investigated in a
notational variant of RNRG, Coupled Context-Free
Grammar (Hotz and Pitsch, 1996).
Terminology Multi-foot formalisms require a
means to specify which foot node gets what ma-
terial in an adjunction. To do so, they use ranked
symbols. A ranked alphabet is a pair ? D .?; Proceedings of the 12th Conference of the European Chapter of the ACL, pages 460?468,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Dependency trees and the strong generative capacity of CCG
Alexander Koller
Saarland University
Saarbr?cken, Germany
koller@mmci.uni-saarland.de
Marco Kuhlmann
Uppsala University
Uppsala, Sweden
marco.kuhlmann@lingfil.uu.se
Abstract
We propose a novel algorithm for extract-
ing dependencies from the derivations of
a large fragment of CCG. Unlike earlier
proposals, our dependency structures are
always tree-shaped. We then use these de-
pendency trees to compare the strong gen-
erative capacities of CCG and TAG and
obtain surprising results: Both formalisms
generate the same languages of derivation
trees ? but the mechanisms they use to
bring the words in these trees into a linear
order are incomparable.
1 Introduction
Combinatory Categorial Grammar (CCG; Steed-
man (2001)) is an increasingly popular grammar
formalism. Next to being theoretically well-mo-
tivated due to its links to combinatory logic and
categorial grammar, it is distinguished by the avail-
ability of efficient open-source parsers (Clark and
Curran, 2007), annotated corpora (Hockenmaier
and Steedman, 2007; Hockenmaier, 2006), and
mechanisms for wide-coverage semantic construc-
tion (Bos et al, 2004).
However, there are limits to our understanding
of the formal properties of CCG and its relation
to other grammar formalisms. In particular, while
it is well-known that CCG belongs to a family of
mildly context-sensitive formalisms that all gener-
ate the same string languages (Vijay-Shanker and
Weir, 1994), there are few results about the strong
generative capacity of CCG. This makes it difficult
to gauge the similarities and differences between
CCG and other formalisms in how they model lin-
guistic phenomena such as scrambling and relat-
ive clauses (Hockenmaier and Young, 2008), and
hampers the transfer of algorithms from one form-
alism to another.
In this paper, we propose a new method for deriv-
ing a dependency tree from a CCG derivation tree
for PF-CCG, a large fragment of CCG. We then
explore the strong generative capacity of PF-CCG
in terms of dependency trees. In particular, we cast
new light on the relationship between CCG and
other mildly context-sensitive formalisms such as
Tree-Adjoining Grammar (TAG; Joshi and Schabes
(1997)) and Linear Context-Free Rewrite Systems
(LCFRS; Vijay-Shanker et al (1987)). We show
that if we only look at valencies and ignore word
order, then the dependency trees induced by a PF-
CCG grammar form a regular tree language, just
as for TAG and LCFRS. To our knowledge, this is
the first time that the regularity of CCG?s deriva-
tional structures has been exposed. However, if we
take the word order into account, then the classes
of PF-CCG-induced and TAG-induced dependency
trees are incomparable; in particular, CCG-induced
dependency trees can be unboundedly non-project-
ive in a way that TAG-induced dependency trees
cannot.
The fact that all our dependency structures are
trees brings our approach in line with the emerging
mainstream in dependency parsing (McDonald et
al., 2005; Nivre et al, 2007) and TAG derivation
trees. The price we pay for restricting ourselves to
trees is that we derive fewer dependencies than the
more powerful approach by Clark et al (2002). In-
deed, we do not claim that our dependencies are lin-
guistically meaningful beyond recording the way in
which syntactic valencies are filled. However, we
show that our dependency trees are still informative
enough to reconstruct the semantic representations.
The paper is structured as follows. In Section 2,
we introduce CCG and the fragment PF-CCG that
we consider in this paper, and compare our contri-
bution to earlier research. In Section 3, we then
show how to read off a dependency tree from a
CCG derivation. Finally, we explore the strong
generative capacity of CCG in Section 4 and con-
clude with ideas for future work.
460
mer
np : we?
L
em Hans
np : Hans?
L
es huus
np : house?
L
h?lfed
((s\np)\np)/vp : help?
L
aastriche
vp\np : paint?
L
((s\np)\np)\np : ?x. help?(paint?(x))
F
(s\np)\np : help? (paint?(house?))
B
s\np : help? (paint?(house?)) Hans?
B
s : help? (paint?(house?)) Hans? we?
B
Figure 1: A PF-CCG derivation
2 Combinatory Categorial Grammars
We start by introducing the Combinatory Categorial
Grammar (CCG) formalism. Then we introduce
the fragment of CCG that we consider in this paper,
and discuss some related work.
2.1 CCG
Combinatory Categorial Grammar (Steedman,
2001) is a grammar formalism that assigns categor-
ies to substrings of an input sentence. There are
atomic categories such as s and np; and if A and B
are categories, then A\B and A/B are functional
categories representing a constituent that will have
category A once it is combined with another con-
stituent of type B to the left or right, respectively.
Each word is assigned a category by the lexicon;
adjacent substrings can then be combined by com-
binatory rules. As an example, Steedman and Bald-
ridge?s (2009) analysis of Shieber?s (1985) Swiss
German subordinate clause (das) mer em Hans es
huus h?lfed aastriiche (?(that) we help Hans paint
the house?) is shown in Figure 1.
Intuitively, the arguments of a functional cat-
egory can be thought of as the syntactic valencies
of the lexicon entry, or as arguments of a func-
tion that maps categories to categories. The core
combinatory mechanism underlying CCG is the
composition and application of these functions. In
their most general forms, the combinatory rules of
(forward and backward) application and compos-
ition can be written as in Figure 2. The symbol |
stands for an arbitrary (forward or backward) slash;
it is understood that the slash before each Bi above
the line is the same as below. The rules derive state-
ments about triples w ` A : f , expressing that the
substring w can be assigned the category A and the
semantic representation f ; an entire string counts
as grammatical if it can be assigned the start cat-
egory s. In parallel to the combination of substrings
by the combinatory rules, their semantic represent-
ations are combined by functional composition.
We have presented the composition rules of CCG
in their most general form. In the literature, the
special cases for n = 0 are called forward and
backward application; the cases for n > 0 where
the slash before Bn is the same as the slash be-
fore B are called composition of degree n; and
the cases where n > 0 and the slashes have dif-
ferent directions are called crossed composition of
degree n. For instance, the F application that com-
bines h?lfed and aastriche in Figure 1 is a forward
crossed composition of degree 1.
2.2 PF-CCG
In addition to the composition rules introduced
above, CCG also allows rules of substitution and
type-raising. Substitution is used to handle syn-
tactic phenomena such as parasitic gaps; type-rais-
ing allows a constituent to serve syntactically as a
functor, while being used semantically as an argu-
ment. Furthermore, it is possible in CCG to restrict
the instances of the rule schemata in Figure 2?for
instance, to say that the application rule may only
be used for the case A = s. We call a CCG gram-
mar pure if it does not use substitution, type-raising,
or restricted rule schemata. Finally, the argument
categories of a CCG category may themselves be
functional categories; for instance, the category of
a VP modifier like passionately is (s\np)\(s\np).
We call a category that is either atomic or only has
atomic arguments a first-order category, and call a
CCG grammar first-order if all categories that its
lexicon assigns to words are first-order.
In this paper, we only consider CCG grammars
that are pure and first-order. This fragment, which
we call PF-CCG, is less expressive than full CCG,
but it significantly simplifies the definitions in Sec-
tion 3. At the same time, many real-world CCG
grammars do not use the substitution rule, and type-
raising can be compiled into the grammar in the
sense that for any CCG grammar, there is an equi-
valent CCG grammar that does not use type-raising
and assigns the same semantic representations to
461
(a,A, f) is a lexical entry
a ` A : f
L
v ` A/B : ?x. f(x) w ` B |Bn | . . . |B1 : ?y1, . . . , yn. g(y1, . . . , yn)
vw ` A |Bn | . . . |B1 : ?y1, . . . , yn. f(g(y1, . . . , yn))
F
v ` B |Bn | . . . |B1 : ?y1, . . . , yn. g(y1, . . . , yn) w ` A\B : ?x. f(x)
vw ` A |Bn | . . . |B1 : ?y1, . . . , yn. f(g(y1, . . . , yn))
B
Figure 2: The generalized combinatory rules of CCG
each string. On the other hand, the restriction to
first-order grammars is indeed a limitation in prac-
tice. We take the work reported here as a first step
towards a full dependency-tree analysis of CCG,
and discuss ideas for generalization in the conclu-
sion.
2.3 Related work
The main objective of this paper is the definition
of a novel way in which dependency trees can
be extracted from CCG derivations. This is sim-
ilar to Clark et al (2002), who aim at capturing
?deep? dependencies, and encode these into annot-
ated lexical categories. For instance, they write
(npi\npi)/(s\npi) for subject relative pronouns to
express that the relative pronoun, the trace of the
relative clause, and the modified noun phrase are
all semantically the same. This means that the rel-
ative pronoun has multiple parents; in general, their
dependency structures are not necessarily trees. By
contrast, we aim to extract only dependency trees,
and achieve this by recording only the fillers of syn-
tactic valencies, rather than the semantic dependen-
cies: the relative pronoun gets two dependents and
one parent (the verb whose argument the modified
np is), just as the category specifies. So Clark et
al.?s and our dependency approach represent two
alternatives of dealing with the tradeoff between
simple and expressive dependency structures.
Our paper differs from the well-known results
of Vijay-Shanker and Weir (1994) in that they es-
tablish the weak equivalence of different grammar
formalisms, while we focus on comparing the deriv-
ational structures. Hockenmaier and Young (2008)
present linguistic motivations for comparing the
strong generative capacities of CCG and TAG, and
the beginnings of a formal comparison between
CCG and spinal TAG in terms of Linear Indexed
Grammars.
3 Induction of dependency trees
We now explain how to extract a dependency tree
from a PF-CCG derivation. The basic idea is to
associate, with every step of the derivation, a cor-
responding operation on dependency trees, in much
the same way as derivation steps can be associated
with operations on semantic representations.
3.1 Dependency trees
When talking about a dependency tree, it is usually
convenient to specify its tree structure and the lin-
ear order of its nodes separately. The tree structure
encodes the valency structure of the sentence (im-
mediate dominance), whereas the linear precedence
of the words is captured by the linear order.
For the purposes of this paper, we represent a
dependency tree as a pair d = (t, s), where t is a
ground term over some suitable alphabet, and s is
a linearization of the nodes (term addresses) of t,
where by a linearization of a set S we mean a list of
elements of S in which each element occurs exactly
once (see also Kuhlmann and M?hl (2007)). As
examples, consider
(f(a, b), [1, ?, 2]) and (f(g(a)), [1 ? 1, ?, 1]) .
These expressions represent the dependency trees
d1 =
a f b
and d2 =
a f g
.
Notice that it is because of the separate specifica-
tion of the tree and the order that dependency trees
can become non-projective; d2 is an example.
A partial dependency tree is a pair (t, s) where t
is a term that may contain variables, and s is a
linearization of those nodes of t that are not labelled
with variables. We restrict ourselves to terms in
which each variable appears exactly once, and will
also prefix partial dependency trees with ?-binders
to order the variables.
462
e = (a,A |Am ? ? ? |A1) is a lexical entry
a ` A |Am ? ? ? |A1 : ?x1, . . . , xm. (e(x1, . . . , xm), [?])
L
v ` A |Am ? ? ? |A1/B : ?x, x1, . . . , xm. d w ` B |Bn ? ? ? |B1 : ?y1, . . . , yn. d
?
vw ` A |Am ? ? ? |A1 |Bn ? ? ? |B1 : ?y1, . . . , yn, x1, . . . , xm. d[x := d
? ]F
F
w ` B |Bn ? ? ? |B1 : ?y1, . . . , yn. d
? v ` A |Am ? ? ? |A1\B : ?x, x1, . . . , xm. d
wv ` A |Am ? ? ? |A1 |Bn ? ? ? |B1 : ?y1, . . . , yn, x1, . . . , xm. d[x := d
? ]B
B
Figure 3: Computing dependency trees in CCG derivations
3.2 Operations on dependency trees
Let t be a term, and let x be a variable in t. The
result of the substitution of the term t? into t for x
is denoted by t[x := t? ]. We extend this opera-
tion to dependency trees as follows. Given a list
of addresses s, let xs be the list of addresses ob-
tained from s by prefixing every address with the
address of the (unique) node that is labelled with x
in t. Then the operations of forward and backward
concatenation are defined as
(t, s)[x := (t?, s?) ]F = (t[x := t
? ], s ? xs?) ,
(t, s)[x := (t?, s?) ]B = (t[x := t
? ], xs? ? s) .
The concatenation operations combine two given
dependency trees (t, s) and (t?, s?) into a new tree
by substituting t? into t for some variable x of t,
and adding the (appropriately prefixed) list s? of
nodes of t? either before or after the list s of nodes
of t. Using these two operations, the dependency
trees d1 and d2 from above can be written as fol-
lows. Let da = (a, [?]) and db = (b, [?]).
d1 = (f(x, y), [?])[x := da ]F [ y := db ]F
d2 = (f(x), [?])[x := (g(y), [?]) ]F [ y := da ]B
Here is an alternative graphical notation for the
composition of d2:
f g
y
2
6
4 y :=
a
3
7
5
B
=
a f g
In this notation, nodes that are not marked with
variables are positioned (indicated by the dotted
projection lines), while the (dashed) variable nodes
dangle unpositioned.
3.3 Dependency trees for CCG derivations
To encode CCG derivations as dependency trees,
we annotate each composition rule of PF-CCG with
instructions for combining the partial dependency
trees for the substrings into a partial dependency
tree for the larger string. Essentially, we now com-
bine partial dependency trees using forward and
backward concatenation rather than combining se-
mantic representations by functional composition
and application. From now on, we assume that the
node labels in the dependency trees are CCG lex-
icon entries, and represent these by just the word
in them.
The modified rules are shown in Figure 3. They
derive statements about triples w ` A : p, where w
is a substring, A is a category, and p is a lambda
expression over a partial dependency tree. Each
variable of p corresponds to an argument category
in A, and vice versa. Rule L covers the base case:
the dependency tree for a lexical entry e is a tree
with one node for the item itself, labelled with e,
and one node for each of its syntactic arguments,
labelled with a variable. Rule F captures forward
composition: given two dependency trees d and d?,
the new dependency tree is obtained by forward
concatenation, binding the outermost variable in d.
Rule B is the rule for backward composition. The
result of translating a complete PF-CCG derivation
? in this way is always a dependency tree without
variables; we call it d(?).
As an example, Figure 4 shows the construc-
tion for the derivation in Figure 1. The induced
dependency tree looks like this:
mer em Hans es huus h?lfed aastriche
For instance, the partial dependency tree for the
lexicon entry of aastriiche contains two nodes: the
root (with address ?) is labelled with the lexicon
entry, and its child (address 1) is labelled with the
463
mer
(mer, [?])
L
em Hans
(Hans, [?])
L
es huus
(huus, [?])
L
h?lfed
?x, y, z. (h?lfed(x, y, z), [?])
L
aastriiche
?w. (aastriiche(w), [?])
L
?w, y, z. (h?lfed(aastriiche(w), y, z), [?, 1])
F
?y, z. (h?lfed(aastriiche(huus), y, z), [11, ?, 1])
B
?z. (h?lfed(aastriiche(huus),Hans, z), [2, 11, ?, 1])
B
(h?lfed(aastriiche(huus),Hans,mer), [3, 2, 11, ?, 1])
B
Figure 4: Computing a dependency tree for the derivation in Figure 1
variable x. This tree is inserted into the tree from
h?lfed by forward concatenation. The variable w is
passed on into the new dependency tree, and later
filled by backward concatenation to huus. Passing
the argument slot of aastriiche to h?lfed to be filled
on its left creates a non-projectivity; it corresponds
to a crossed composition in CCG terms. Notice
that the categories derived in Figure 1 mirror the
functional structure of the partial dependency trees
at each step of the derivation.
3.4 Semantic equivalence
The mapping from derivations to dependency trees
loses some information: different derivations may
induce the same dependency tree. This is illus-
trated by Figure 5, which provides two possible
derivations for the phrase big white rabbit, both
of which induce the same dependency tree. Espe-
cially in light of the fact that our dependency trees
will typically contain fewer dependencies than the
DAGs derived by Clark et al (2002), one could ask
whether dependency trees are an appropriate way
of representing the structure of a CCG derivation.
However, at the end of the day, the most import-
ant information that can be extracted from a CCG
derivation is the semantic representation it com-
putes; and it is possible to reconstruct the semantic
representation of a derivation ? from d(?) alone. If
we forget the word order information in the depend-
ency trees, the rules F and B in Figure 3 are merely
?-expanded versions of the semantic construction
rules in Figure 2. This means that d(?) records
everything we need to know about constructing the
semantic representation: We can traverse it bottom-
up and apply the lexical semantic representation
of each node to those of its subterms. So while
the dependency trees obliterate some information
in the CCG derivations (particularly its associative
structure), they are indeed appropriate represent-
ations because they record all syntactic valencies
and encode enough information to recompute the
semantics.
4 Strong generative capacity
Now that we know how to see PF-CCG derivations
as dependency trees, we can ask what sets of such
trees can be generated by PF-CCG grammars. This
is the question about the strong generative capa-
city of PF-CCG, measured in terms of dependency
trees (Miller, 2000). In this section, we give a
partial answer to this question: We show that the
sets of PF-CCG-induced valency trees (dependency
trees without their linear order) form regular tree
languages, but that the sets of dependency trees
themselves are irregular. This is in contrast to other
prominent mildly context-sensitive grammar form-
alisms such as Tree Adjoining Grammar (TAG;
Joshi and Schabes (1997)) and Linear Context-
Free Rewrite Systems (LCFRS; Vijay-Shanker et
al. (1987)), in which both languages are regular.
4.1 CCG term languages
Formally, we define the language of all dependency
trees generated by a PF-CCG grammar G as the set
LD(G) = { d(?) | ? is a derivation of G } .
Furthermore, we define the set of valency trees to
be the set of just the term parts of each d(?):
LV (G) = { t | (t, s) ? LD(G) } .
By our previous assumption, the node labels of a
valency tree are CCG lexicon entries.
We will now show that the valency tree lan-
guages of PF-CCG grammars are regular tree lan-
guages (G?cseg and Steinby, 1997). Regular tree
languages are sets of trees that can be generated
by regular tree grammars. Formally, a regular tree
grammar (RTG) is a construct ? = (N,?, S, P ),
where N is an alphabet of non-terminal symbols,
? is an alphabet of ranked term constructors called
terminal symbols, S ? N is a distinguished start
symbol, and P is a finite set of production rules of
the form A ? ?, where A ? N and ? is a term
over ? and N , where the nonterminals can be used
464
big
np/np
white
np/np
np/np
rabbit
np
np big white rabbit
big
np/np
white
np/np
rabbit
np
np/np
np
Figure 5: Different derivations may induce the same dependency tree
as constants. The grammar ? generates trees from
the start symbol by successively expanding occur-
rences of nonterminals using production rules. For
instance, the grammar that contains the productions
S ? f(A,A), A ? g(A), and A ? a generates
the tree language { f(gm(a), gn(a)) | m,n ? 0 }.
We now construct an RTG ? (G) that generates
the set of valency trees of a PF-CCG G. For the
terminal alphabet, we choose the lexicon entries:
If e = (a,A | B1 . . . | Bn, f) is a lexicon entry of
G, we take e as an n-ary term constructor. We also
take the atomic categories of G as our nonterminal
symbols; the start category s of G counts as the
start symbol. Finally, we encode each lexicon entry
as a production rule: The lexicon entry e above
encodes to the rule A? e(Bn, . . . , B1).
Let us look at our running example to see how
this works. Representing the lexicon entries as just
the words for brevity, we can write the valency tree
corresponding to the CCG derivation in Figure 4
as t0 = h?lfed(aastriiche(huus),Hans,mer); here
h?lfed is a ternary constructor, aastriiche is unary,
and all others are constants. Taking the lexical
categories into account, we obtain the RTG with
s? h?lfed(vp, np, np)
vp? aastriiche(np)
np? huus | Hans | mer
This grammar indeed generates t0, and all other
valency trees induced by the sample grammar.
More generally, LV (G) ? L(? (G)) because
the construction rules in Figure 3 ensure that if
a node v becomes the i-th child of a node u in
the term, then the result category of v?s lexicon
entry equals the i-th argument category of u?s lex-
icon entry. This guarantees that the i-th nonter-
minal child introduced by the production for u can
be expanded by the production for v. The con-
verse inclusion can be shown by reconstructing,
for each valency tree t, a CCG derivation ? that
induces t. This construction can be done by ar-
ranging the nodes in t into an order that allows
us to combine every parent in t with its children
using only forward and backward application. The
CCG derivation we obtain for the example is shown
in Figure 6; it is a derivation for the sentence
das mer em Hans h?lfed es huus aastriiche, using
the same lexicon entries. Together, this shows that
L(? (G)) = LV (G). Thus:
Theorem 1 The sets of valency trees generated by
PF-CCG are regular tree languages. 2
By this result, CCG falls in line with context-free
grammars, TAG, and LCFRS, whose sets of deriva-
tional structures are all regular (Vijay-Shanker et
al., 1987). To our knowledge, this is the first time
the regular structure of CCG derivations has been
exposed. It is important to note that while CCG
derivations themselves can be seen as trees as well,
they do not always form regular tree languages
(Vijay-Shanker et al, 1987). Consider for instance
the CCG grammar from Vijay-Shanker and Weir?s
(1994) Example 2.4, which generates the string lan-
guage anbncndn; Figure 7 shows the derivation of
aabbccdd. If we follow this derivation bottom-up,
starting at the first c, the intermediate categories
collect an increasingly long tail of\a arguments; for
longer words from the language, this tail becomes
as long as the number of cs in the string. The in-
finite set of categories this produces translates into
the need for an infinite nonterminal alphabet in an
RTG, which is of course not allowed.
4.2 Comparison with TAG
If we now compare PF-CCG to its most promin-
ent mildly context-sensitive cousin, TAG, the reg-
ularity result above paints a suggestive picture: A
PF-CCG valency tree assigns a lexicon entry to
each word and says which other lexicon entry fills
each syntactic valency. In this respect, it is the
analogue of a TAG derivation tree (in which the
lexicon entries are elementary trees), and we just
saw that PF-CCG and TAG generate the same tree
languages. On the other hand, CCG and TAG are
weakly equivalent (Vijay-Shanker and Weir, 1994),
i.e. they generate the same linear word orders. So
one could expect that CCG and TAG also induce
the same dependency trees. Interestingly, this is
not the case.
465
mer
np
L
em Hans
np
L
h?lfed
s\np\np/vp
L
es huus
np
L
aastriiche
vp\np
L
vp
B
s\np\np
F
s\np
B
s
B
Figure 6: CCG derivation reconstructed from the dependency tree from Figure 4 using only applications
We know from the literature that those depend-
ency trees that can be constructed from TAG deriva-
tion trees are exactly those that are well-nested and
have a block-degree of at most 2 (Kuhlmann and
M?hl, 2007). The block-degree of a node u in a de-
pendency tree is the number of ?blocks? into which
the subtree below u is separated by intervening
nodes that are not below u, and the block-degree
of a dependency tree is the maximum block-degree
of its nodes. So for instance, the dependency tree
on the right-hand side of Figure 8 has block-degree
two. It is also well-nested, and can therefore be
induced by TAG derivations.
Things are different for the dependency trees that
can be induced by PF-CCG. Consider the left-hand
dependency tree in Figure 8, which is induced by
a PF-CCG derivation built from words with the
lexical categories a/a, b\a, b\b, and a. While
this dependency tree is well-nested, it has block-
degree three: The subtree below the leftmost node
consists of three parts. More generally, we can in-
sert more words with the categories a/a and b\b
in the middle of the sentence to obtain depend-
ency trees with arbitrarily high block-degrees from
this grammar. This means that unlike for TAG-
induced dependency trees, there is no upper bound
on the block-degree of dependency trees induced
by PF-CCG?as a consequence, there are CCG
dependency trees that cannot be induced by TAG.
On the other hand, there are also dependency
trees that can be induced by TAG, but not by PF-
CCG. The tree on the right-hand side of Figure 8
is an example. We have already argued that this
tree can be induced by a TAG. However, it con-
tains no two adjacent nodes that are connected by
a/a b\a a/a b\b a b\b 1 2 3 4
Figure 8: The divergence between CCG and TAG
an edge; and every nontrivial PF-CCG derivation
must combine two adjacent words at least at one
point during the derivation. Therefore, the tree
cannot be induced by a PF-CCG grammar. Further-
more, it is known that all dependency languages
that can be generated by TAG or even, more gener-
ally, by LCRFS, are regular in the sense of Kuhl-
mann and M?hl (2007). One crucial property of
regular dependency languages is that they have a
bounded block-degree; but as we have seen, there
are PF-CCG dependency languages with unboun-
ded block-degree. Therefore there are PF-CCG
dependency languages that are not regular. Hence:
Theorem 2 The sets of dependency trees gener-
ated by PF-CCG and TAG are incomparable. 2
We believe that these results will generalize to
full CCG. While we have not yet worked out the
induction of dependency trees from full CCG, the
basic rule that CCG combines adjacent substrings
should still hold; therefore, every CCG-induced
dependency tree will contain at least one edge
between adjacent nodes. We are thus left with
a very surprising result: TAG and CCG both gener-
ate the same string languages and the same sets of
valency trees, but they use incomparable mechan-
isms for linearizing valency trees into sentences.
4.3 A note on weak generative capacity
As a final aside, we note that the construction for
extracting purely applicative derivations from the
terms described by the RTG has interesting con-
sequences for the weak generative capacity of PF-
CCG. In particular, it has the corollary that for any
PF-CCG derivation ? over a string w, there is a per-
mutation of w that can be accepted by a PF-CCG
derivation that uses only application?that is, every
string language L that can be generated by a PF-
CCG grammar has a context-free sublanguage L?
such that all words in L are permutations of words
in L?.
This means that many string languages that we
commonly associate with CCG cannot be generated
466
aa/d
L
a
a/d
L
b
b
L
b
b
L
c
s\a/t\b
L
s\a/t
B
c
t\a\b
L
s\a\a\b
F
s\a\a
B
s\a/d
B
d
d
L
s\a
F
s/d
B
d
d
L
s
F
Figure 7: The CCG derivation of aabbccdd using Example 2.4 in Vijay-Shanker and Weir (1994)
by PF-CCG. One such language is anbncndn. This
language is not itself context-free, and therefore
any PF-CCG grammar whose language contains it
also contains permutations in which the order of
the symbols is mixed up. The culprit for this among
the restrictions that distinguish PF-CCG from full
CCG seems to be that PF-CCG grammars must
allow all instances of the application rules. This
would mean that the ability of CCG to generate non-
context-free languages (also linguistically relevant
ones) hinges crucially on its ability to restrict the
allowable instances of rule schemata, for instance,
using slash types (Baldridge and Kruijff, 2003).
5 Conclusion
In this paper, we have shown how to read deriva-
tions of PF-CCG as dependency trees. Unlike pre-
vious proposals, our view on CCG dependencies
is in line with the mainstream dependency parsing
literature, which assumes tree-shaped dependency
structures; while our dependency trees are less in-
formative than the CCG derivations themselves,
they contain sufficient information to reconstruct
the semantic representation. We used our new de-
pendency view to compare the strong generative
capacity of PF-CCG with other mildly context-
sensitive grammar formalisms. It turns out that
the valency trees generated by a PF-CCG grammar
form regular tree languages, as in TAG and LCFRS;
however, unlike these formalisms, the sets of de-
pendency trees including word order are not regular,
and in particular can be more non-projective than
the other formalisms permit. Finally, we found
new formal evidence for the importance of restrict-
ing rule schemata for describing non-context-free
languages in CCG.
All these results were technically restricted to
the fragment of PF-CCG, and one focus of future
work will be to extend them to as large a fragment
of CCG as possible. In particular, we plan to extend
the lambda notation used in Figure 3 to cover type-
raising and higher-order categories. We would then
be set to compare the behavior of wide-coverage
statistical parsers for CCG with statistical depend-
ency parsers.
We anticipate that our results about the strong
generative capacity of PF-CCG will be useful to
transfer algorithms and linguistic insights between
formalisms. For instance, the CRISP generation
algorithm (Koller and Stone, 2007), while specified
for TAG, could be generalized to arbitrary gram-
mar formalisms that use regular tree languages?
given our results, to CCG in particular. On the
other hand, we find it striking that CCG and TAG
generate the same string languages from the same
tree languages by incomparable mechanisms for
ordering the words in the tree. Indeed, the exact
characterization of the class of CCG-inducable de-
pendency languages is an open issue. This also
has consequences for parsing complexity: We can
understand why TAG and LCFRS can be parsed in
polynomial time from the bounded block-degree
of their dependency trees (Kuhlmann and M?hl,
2007), but CCG can be parsed in polynomial time
(Vijay-Shanker and Weir, 1990) without being re-
stricted in this way. This constitutes a most inter-
esting avenue of future research that is opened up
by our results.
Acknowledgments. We thank Mark Steedman,
Jason Baldridge, and Julia Hockenmaier for valu-
able discussions about CCG, and the reviewers for
their comments. The work of Alexander Koller
was funded by a DFG Research Fellowship and the
Cluster of Excellence ?Multimodal Computing and
Interaction?. The work of Marco Kuhlmann was
funded by the Swedish Research Council.
467
References
Jason Baldridge and Geert-Jan M. Kruijff. 2003.
Multi-modal Combinatory Categorial Grammar. In
Proceedings of the Tenth EACL, Budapest, Hungary.
Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-
coverage semantic representations from a CCG
parser. In Proceedings of the 20th COLING, Geneva,
Switzerland.
Stephen Clark and James Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4).
Stephen Clark, Julia Hockenmaier, and Mark Steed-
man. 2002. Building deep dependency structures
with a wide-coverage CCG parser. In Proceedings
of the 40th ACL, Philadelphia, USA.
Ferenc G?cseg and Magnus Steinby. 1997. Tree lan-
guages. In Rozenberg and Salomaa (Rozenberg and
Salomaa, 1997), pages 1?68.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
Julia Hockenmaier and Peter Young. 2008. Non-local
scrambling: the equivalence of TAG and CCG re-
visited. In Proceedings of TAG+9, T?bingen, Ger-
many.
Julia Hockenmaier. 2006. Creating a CCGbank and
a wide-coverage CCG lexicon for German. In Pro-
ceedings of COLING/ACL, Sydney, Australia.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
Adjoining Grammars. In Rozenberg and Salomaa
(Rozenberg and Salomaa, 1997), pages 69?123.
Alexander Koller and Matthew Stone. 2007. Sentence
generation as planning. In Proceedings of the 45th
ACL, Prague, Czech Republic.
Marco Kuhlmann and Mathias M?hl. 2007. Mildly
context-sensitive dependency languages. In Pro-
ceedings of the 45th ACL, Prague, Czech Republic.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT/EMNLP.
Philip H. Miller. 2000. Strong Generative Capacity:
The Semantics of Linguistic Formalism. University
of Chicago Press.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?lsen Eryigit, Sandra K?bler, Svetoslav
Marinov, and Erwin Marsi. 2007. MaltParser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Grzegorz Rozenberg and Arto Salomaa, editors. 1997.
Handbook of Formal Languages. Springer.
Stuart Shieber. 1985. Evidence against the context-
freeness of natural language. Linguistics and Philo-
sophy, 8:333?343.
Mark Steedman and Jason Baldridge. 2009. Combin-
atory categorial grammar. In R. Borsley and K. Bor-
jars, editors, Non-Transformational Syntax. Black-
well. To appear.
Mark Steedman. 2001. The Syntactic Process. MIT
Press.
K. Vijay-Shanker and David Weir. 1990. Polynomial
time parsing of combinatory categorial grammars.
In Proceedings of the 28th ACL, Pittsburgh, USA.
K. Vijay-Shanker and David J. Weir. 1994. The equi-
valence of four extensions of context-free grammars.
Mathematical Systems Theory, 27(6):511?546.
K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi.
1987. Characterizing structural descriptions pro-
duced by various grammatical formalisms. In Pro-
ceedings of the 25th ACL, Stanford, CA, USA.
468
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 478?486,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Treebank Grammar Techniques for Non-Projective Dependency Parsing
Marco Kuhlmann
Uppsala University
Uppsala, Sweden
marco.kuhlmann@lingfil.uu.se
Giorgio Satta
University of Padua
Padova, Italy
satta@dei.unipd.it
Abstract
An open problem in dependency parsing
is the accurate and efficient treatment of
non-projective structures. We propose to
attack this problem using chart-parsing
algorithms developed for mildly context-
sensitive grammar formalisms. In this pa-
per, we provide two key tools for this ap-
proach. First, we show how to reduce non-
projective dependency parsing to parsing
with Linear Context-Free Rewriting Sys-
tems (LCFRS), by presenting a technique
for extracting LCFRS from dependency
treebanks. For efficient parsing, the ex-
tracted grammars need to be transformed
in order to minimize the number of nonter-
minal symbols per production. Our second
contribution is an algorithm that computes
this transformation for a large, empirically
relevant class of grammars.
1 Introduction
Dependency parsing is the task of predicting the
most probable dependency structure for a given
sentence. One of the key choices in dependency
parsing is about the class of candidate structures
for this prediction. Many parsers are confined to
projective structures, in which the yield of a syn-
tactic head is required to be continuous. A major
benefit of this choice is computational efficiency:
an exhaustive search over all projective structures
can be done in cubic, greedy parsing in linear time
(Eisner, 1996; Nivre, 2003). A major drawback of
the restriction to projective dependency structures
is a potential loss in accuracy. For example, around
23% of the analyses in the Prague Dependency
Treebank of Czech (Hajic? et al, 2001) are non-
projective, and for German and Dutch treebanks,
the proportion of non-projective structures is even
higher (Havelka, 2007).
The problem of non-projective dependency pars-
ing under the joint requirement of accuracy and
efficiency has only recently been addressed in the
literature. Some authors propose to solve it by tech-
niques for recovering non-projectivity from the out-
put of a projective parser in a post-processing step
(Hall and Nov?k, 2005; Nivre and Nilsson, 2005),
others extend projective parsers by heuristics that
allow at least certain non-projective constructions
to be parsed (Attardi, 2006; Nivre, 2007). McDon-
ald et al (2005) formulate dependency parsing as
the search for the most probable spanning tree over
the full set of all possible dependencies. However,
this approach is limited to probability models with
strong independence assumptions. Exhaustive non-
projective dependency parsing with more powerful
models is intractable (McDonald and Satta, 2007),
and one has to resort to approximation algorithms
(McDonald and Pereira, 2006).
In this paper, we propose to attack non-project-
ive dependency parsing in a principled way, us-
ing polynomial chart-parsing algorithms developed
for mildly context-sensitive grammar formalisms.
This proposal is motivated by the observation that
most dependency structures required for the ana-
lysis of natural language are very nearly projective,
differing only minimally from the best projective
approximation (Kuhlmann and Nivre, 2006), and
by the close link between such ?mildly non-project-
ive? dependency structures on the one hand, and
grammar formalisms with mildly context-sensitive
generative capacity on the other (Kuhlmann and
M?hl, 2007). Furthermore, as pointed out by Mc-
Donald and Satta (2007), chart-parsing algorithms
are amenable to augmentation by non-local inform-
ation such as arity constraints and Markovization,
and therefore should allow for more predictive stat-
istical models than those used by current systems
for non-projective dependency parsing. Hence,
mildly non-projective dependency parsing prom-
ises to be both efficient and accurate.
478
Contributions In this paper, we contribute two
key tools for making the mildly context-sensitive
approach to accurate and efficient non-projective
dependency parsing work.
First, we extend the standard technique for ex-
tracting context-free grammars from phrase-struc-
ture treebanks (Charniak, 1996) to mildly con-
text-sensitive grammars and dependency treebanks.
More specifically, we show how to extract, from
a given dependency treebank, a lexicalized Linear
Context-Free Rewriting System (LCFRS) whose
derivations capture the dependency analyses in the
treebank in the same way as the derivations of
a context-free treebank grammar capture phrase-
structure analyses. Our technique works for arbit-
rary, even non-projective dependency treebanks,
and essentially reduces non-projective dependency
to parsing with LCFRS. This problem can be solved
using standard chart-parsing techniques.
Our extraction technique yields a grammar
whose parsing complexity is polynomial in the
length of the sentence, but exponential in both a
measure of the non-projectivity of the treebank and
the maximal number of dependents per word, re-
flected as the rank of the extracted LCFRS. While
the number of highly non-projective dependency
structures is negligible for practical applications
(Kuhlmann and Nivre, 2006), the rank cannot eas-
ily be bounded. Therefore, we present an algorithm
that transforms the extracted grammar into a nor-
mal form that has rank 2, and thus can be parsed
more efficiently. This contribution is important
even independently of the extraction procedure:
While it is known that a rank-2 normal form of
LCFRS does not exist in the general case (Rambow
and Satta, 1999), our algorithm succeeds for a large
and empirically relevant class of grammars.
2 Preliminaries
We start by introducing dependency trees and
Linear Context-Free Rewriting Systems (LCFRS).
Throughout the paper, for positive integers i and j ,
we write ?i; j ? for the interval f k j i  k  j g,
and use ?n? as a shorthand for ?1; n?.
2.1 Dependency Trees
Dependency parsing is the task to assign depend-
ency structures to a given sentence w. For the
purposes of this paper, dependency structures are
edge-labelled trees. More formally, let w be a sen-
tence, understood as a sequence of tokens over
some given alphabet T , and let L be an alphabet
of edge labels. A dependency tree for w is a con-
structD D .w;E; /, where E forms a rooted tree
(in the standard graph-theoretic sense) on the set
?jwj?, and  is a total function that assigns every
edge in E a label in L. Each node of D represents
a (position of a) token in w.
Example 1 Figure 2 shows a dependency tree for
the sentence A hearing is scheduled on the issue
today, which consists of 8 tokens and the edges
f .2; 1/; .2; 5/; .3; 2/; .3; 4/; .4; 8/; .5; 7/; .7; 6/ g.
The edges are labelled with syntactic functions
such as sbj for ?subject?. The root node is marked
by a dotted line. 
Let u be a node of a dependency treeD. A node u0
is a descendant of u, if there is a (possibly empty)
path from u to u0. A block of u is a maximal
interval of descendants of u. The number of blocks
of u is called the block-degree of u. The block-
degree of a dependency tree is the maximum among
the block-degrees of its nodes. A dependency tree
is projective, if its block-degree is 1.
Example 2 The tree shown in Figure 2 is not
projective: both node 2 (hearing) and node 4
(scheduled) have block-degree 2. Their blocks are
f 2 g; f 5; 6; 7 g and f 4 g; f 8 g, respectively.
2.2 LCFRS
Linear Context-Free Rewriting Systems (LCFRS)
have been introduced as a generalization of sev-
eral mildly context-sensitive grammar formalisms.
Here we use the standard definition of LCFRS
(Vijay-Shanker et al, 1987) and only fix our nota-
tion; for a more thorough discussion of this formal-
ism, we refer to the literature.
Let G be an LCFRS. Recall that each nonter-
minal symbol A of G comes with a positive integer
called the fan-out of A, and that a production p
of G has the form
A! g.A1; : : : ; Ar/ I g.Ex1; : : : ; Exr/ D E? ;
whereA;A1; : : : ; Ar are nonterminals with fan-out
f; f1; : : : ; fr , respectively, g is a function symbol,
and the equation to the right of the semicolon spe-
cifies the semantics of g. For each i 2 ?r?, Exi is
an fi -tuple of variables, and E? D h?1; : : : ; f? i is a
tuple of strings over the variables on the left-hand
side of the equation and the alphabet of terminal
symbols in which each variable appears exactly
once. The production p is said to have rank r ,
fan-out f , and length j?1jC   C j f? jC .f  1/.
479
3 Grammar Extraction
We now explain how to extract an LCFRS from a
dependency treebank, in very much the same way
as a context-free grammar can be extracted from a
phrase-structure treebank (Charniak, 1996).
3.1 Dependency Treebank Grammars
A simple way to induce a context-free grammar
from a phrase-structure treebank is to read off the
productions of the grammar from the trees. We will
specify a procedure for extracting, from a given
dependency treebank, a lexicalized LCFRS G that
is adequate in the sense that for every analysis D
of a sentencew in the treebank, there is a derivation
tree of G that is isomorphic to D, meaning that
it becomes equal to D after a suitable renaming
and relabelling of nodes, and has w as its derived
string. Here, a derivation tree of an LCFRS G is
an ordered tree such that each node u is labelled
with a production p of G, the number of children
of u equals the rank r of p, and for each i 2 ?r?,
the i th child of u is labelled with a production that
has as its left-hand side the i th nonterminal on the
right-hand side of p.
The basic idea behind our extraction procedure
is that, in order to represent the compositional struc-
ture of a possibly non-projective dependency tree,
one needs to represent the decomposition and relat-
ive order not of subtrees, but of blocks of subtrees
(Kuhlmann and M?hl, 2007). We introduce some
terminology. A component of a node u in a de-
pendency tree is either a block B of some child u0
of u, or the singleton interval that contains u; this
interval will represent the position in the string that
is occupied by the lexical item corresponding to u.
We say that u0 contributes B , and that u contrib-
utes ?u; u? to u. Notice that the number of com-
ponents that u0 contributes to its parent u equals
the block-degree of u0. Our goal is to construct
for u a production of an LCFRS that specifies how
each block of u decomposes into components, and
how these components are ordered relative to one
another. These productions will make an adequate
LCFRS, in the sense defined above.
3.2 Annotating the Components
The core of our extraction procedure is an efficient
algorithm that annotates each node u of a given de-
pendency tree with the list of its components, sor-
ted by their left endpoints. It is helpful to think of
this algorithm as of two independent parts, one that
1: Function Annotate-L.D/
2: for each u of D, from left to right do
3: if u is the first node of D then
4: b WD the root node of D
5: else
6: b WD the lca of u and its predecessor
7: for each u0 on the path b   u do
8: left?u0? WD left?u0?  u
Figure 1: Annotation with components
annotates each node u with the list of the left end-
points of its components (Annotate-L) and one
that annotates the corresponding right endpoints
(Annotate-R). The list of components can then
be obtained by zipping the two lists of endpoints
together in linear time.
Figure 1 shows pseudocode for Annotate-L;
the pseudocode for Annotate-R is symmetric. We
do a single left-to-right sweep over the nodes of the
input treeD. In each step, we annotate all nodes u0
that have the current node u as the left endpoint of
one of their components. Since the sweep is from
left to right, this will get us the left endpoints of u0
in the desired order. The nodes that we annotate are
the nodes u0 on the path between u and the least
common ancestor (lca) b of u and its predecessor,
or the path from the root node to u, in case that u
is the leftmost node of D.
Example 3 For the dependency tree in Figure 2,
Annotate-L constructs the following lists left?u?
of left endpoints, for u D 1; : : : ; 8:
1; 1  2  5; 1  3  4  5  8; 4  8; 5  6; 6; 6  7; 8
The following Lemma establishes the correctness
of the algorithm:
Lemma 1 Let D be a dependency tree, and let u
and u0 be nodes of D. Let b be the least common
ancestor of u and its predecessor, or the root node
in case that u is the leftmost node of D. Then u is
the left endpoint of a component of u0 if and only
if u0 lies on the path from b to u. 
Proof It is clear that u0 must be an ancestor of u.
If u is the leftmost node of D, then u is the left
endpoint of the leftmost component of all of its
ancestors. Now suppose that u is not the leftmost
node of D, and let Ou be the predecessor of u. Dis-
tinguish three cases: If u0 is not an ancestor of Ou,
then Ou does not belong to any component of u0;
therefore, u is the left endpoint of a component
480
of u0. If u0 is an ancestor of Ou but u0 ? b, then Ou
and u belong to the same component of u0; there-
fore, u is not the left endpoint of this component.
Finally, if u0 D b, then Ou and u belong to different
components of u0; therefore, u is the left endpoint
of the component it belongs to. 
We now turn to an analysis of the runtime of the
algorithm. Let n be the number of components
of D. It is not hard to imagine an algorithm that
performs the annotation task in time O.n logn/:
such an algorithm could construct the components
for a given node u by essentially merging the list of
components of the children of u into a new sorted
list. In contrast, our algorithm takes time O.n/.
The crucial part of the analysis is the assignment
in line 6, which computes the least common an-
cestor of u and its predecessor. Using markers for
the path from the root node to u, it is straightfor-
ward to implement this assignment in time O.jj/,
where  is the path b   u. Now notice that, by our
correctness argument, line 8 of the algorithm is ex-
ecuted exactly n times. Therefore, the sum over the
lengths of all the paths  , and hence the amortized
time of computing all the least common ancest-
ors in line 6, is O.n/. This runtime complexity is
optimal for the task we are solving.
3.3 Extraction Procedure
We now describe how to extend the annotation al-
gorithm into a procedure that extracts an LCFRS
from a given dependency tree D. The basic idea is
to transform the list of components of each node u
of D into a production p. This transformation will
only rename and relabel nodes, and therefore yield
an adequate derivation tree. For the construction
of the production, we actually need an extended
version of the annotation algorithm, in which each
component is annotated with the node that contrib-
uted it. This extension is straightforward, and does
not affect the linear runtime complexity.
Let D be a dependency tree for a sentence w.
Consider a single node u of D, and assume that u
has r children, and that the block-degree of u is f .
We construct for u a production p with rank r
and fan-out f . For convenience, let us order the
children of u, say by their leftmost descendants,
and let us write ui for the i th child of u according
to this order, and fi for the block-degree of ui ,
i 2 ?r?. The production p has the form
L! g.L1; : : : ; Lr/ I g.Ex1; : : : ; Exr/ D E? ;
where L is the label of the incoming edge of u
(or the special label root in case that u is the root
node of D) and for each i 2 ?r?: Li is the label of
the incoming edge of ui ; Exi is a fi -tuple of vari-
ables of the form xi;j , where j 2 ?fi ?; and E? is
an f -tuple that is constructed in a single left-to-
right sweep over the list of components computed
for u as follows. Let k 2 ?fi ? be a pointer to a cur-
rent segment of E?; initially, k D 1. If the current
component is not adjacent (as an interval) to the
previous component, we increase k by one. If the
current component is contributed by the child ui ,
i 2 ?r?, we add the variable xi;j to ?k , where j
is the number of times we have seen a component
contributed by ui during the sweep. Notice that
j 2 ?fi ?. If the current component is the (unique)
component contributed by u, we add the token cor-
responding to u to ?k . In this way, we obtain a
complete specification of how the blocks of u (rep-
resented by the segments of the tuple E?) decompose
into the components of u, and of the relative order
of the components. As an example, Figure 2 shows
the productions extracted from the tree above.
3.4 Parsing the Extracted Grammar
Once we have extracted the grammar for a depend-
ency treebank, we can apply any parsing algorithm
for LCFRS to non-projective dependency parsing.
The generic chart-parsing algorithm for LCFRS
runs in timeO.jP j  jwjf .rC1//, where P is the set
of productions of the input grammar G, w is the in-
put string, r is the maximal rank, and f is the max-
imal fan-out of a production inG (Seki et al, 1991).
For a grammar G extracted by our technique, the
number f equals the maximal block-degree per
node. Hence, without any further modification, we
obtain a parsing algorithm that is polynomial in the
length of the sentence, but exponential in both the
block-degree and the rank. This is clearly unaccept-
able in practical systems. The relative frequency
of analyses with a block-degree  2 is almost neg-
ligible (Havelka, 2007); the bigger obstacle in ap-
plying the treebank grammar is the rank of the res-
ulting LCFRS. Therefore, in the remainder of the
paper, we present an algorithm that can transform
the productions of the input grammar G into an
equivalent set of productions with rank at most 2,
while preserving the fan-out. This transformation,
if it succeeds, yields a parsing algorithm that runs
in time O.jP j  r  jwj3f /.
481
1A 2hearing 3is 4scheduled 5on 6the 7issue 8today
nmod sbj
root node
vc
pp
nmod
np
tmp
nmod! g1 g1 D hAi
sbj! g2.nmod; pp/ g2.hx1;1i; hx2;1i/ D hx1;1 hearing; x2;1i
root! g3.sbj; vc/ g3.hx1;1; x1;2i; hx2;1; x2;2i/ D hx1;1 is x2;1 x1;2 x2;2i
vc! g4.tmp/ g4.hx1;1i/ D hscheduled; x1;1i
pp! g5.np/ g5.hx1;1i/ D hon x1;1i
nmod! g6 g6 D hthei
np! g7.nmod/ g7.hx1;1i/ D hx1;1 issuei
tmp! g8 g8 D htodayi
Figure 2: A dependency tree, and the LCFRS extracted for it
4 Adjacency
In this section we discuss a method for factorizing
an LCFRS into productions of rank 2. Before start-
ing, we get rid of the ?easy? cases. A production p
is connected if any two strings ?i , j? in p?s defini-
tion share at least one variable referring to the same
nonterminal. It is not difficult to see that, when p is
not connected, we can always split it into new pro-
ductions of lower rank. Therefore, throughout this
section we assume that LCFRS only have connec-
ted productions. We can split p into its connected
components using standard methods for finding the
strongly connected components of an undirected
graph. This can be implemented in time O.r  f /,
where r and f are the rank and the fan-out of p,
respectively.
4.1 Adjacency Graphs
Let p be a production with length n and fan-out f ,
associated with function a g. The set of positions
of p is the set ?n?. Informally, each position rep-
resents a variable or a lexical element in one of the
components of the definition of g, or else a ?gap?
between two of these components. (Recall that n
also accounts for the f   1 gaps in the body of g.)
Example 4 The set of positions of the production
for hearing in Figure 2 is ?4?: 1 for variable x1, 2
for hearing, 3 for the gap, and 4 for y1. 
Let i1; j1; i2; j2 2 ?n?. An interval ?i1; j1? is ad-
jacent to an interval ?i2; j2? if either j1 D i2   1
(left-adjacent) or i1 D j2 C 1 (right-adjacent). A
multi-interval, or m-interval for short, is a set v of
pairwise disjoint intervals such that no interval in v
is adjacent to any other interval in v. The fan-out
of v, written f .v/, is defined as jvj.
We use m-intervals to represent the nonterminals
and the lexical element heading p. The i th nonter-
minal on the right-hand side of p is represented by
the m-interval obtained by collecting all the pos-
itions of p that represent a variable from the i th
argument of g. The head of p is represented by the
m-interval containing the associated position. Note
that all these m-intervals are pairwise disjoint.
Example 5 Consider the production for is in
Figure 2. The set of positions is ?5?. The
first nonterminal is represented by the m-inter-
val f ?1; 1?; ?4; 4? g, the second nonterminal by
f ?3; 3?; ?5; 5? g, and the lexical head by f ?2; 2? g. 
For disjoint m-intervals v1; v2, we say that v1 is
adjacent to v2, denoted by v1 ! v2, if for every
interval I1 2 v1, there is an interval I2 2 v2 such
that I1 is adjacent to I2. Adjacency is not symmet-
ric: if v1 D f ?1; 1?; ?4; 4? g and v2 D f ?2; 2? g, then
v2 ! v1, but not vice versa.
Let V be some collection of pairwise disjoint
m-intervals representing p as above. The ad-
jacency graph associated with p is the graph
G D .V;!G/ whose vertices are the m-intervals
in V , and whose edges!G are defined by restrict-
ing the adjacency relation! to the set V .
For m-intervals v1; v2 2 V , the merger of v1
and v2, denoted by v1 ? v2, is the (uniquely
determined) m-interval whose span is the union
of the spans of v1 and v2. As an example, if
v1 D f ?1; 1?; ?3; 3? g and v2 D f ?2; 2? g, then
v1 ? v2 D f ?1; 3? g. Notice that the way in which
we defined m-intervals ensures that a merging oper-
ation collapses all adjacent intervals. The proof of
the following lemma is straightforward and omitted
for space reasons:
482
1: Function Factorize.G D .V;!G//
2: R WD ;;
3: while!G ? ; do
4: choose .v1; v2/ 2 !G ;
5: R WD R [ f .v1; v2/ g;
6: V WD V   f v1; v2 g [ f v1 ? v2 g;
7: !G WD f .v; v0/ j v; v0 2 V; v ! v0 g;
8: if jV j D 1 then
9: output R and accept;
10: else
11: reject;
Figure 3: Factorization algorithm
Lemma 2 If v1 ! v2, then f .v1 ? v2/  f .v2/.
4.2 The Adjacency Algorithm
Let G D .V;!G/ be some adjacency graph, and
let v1!G v2. We can derive a new adjacency
graph from G by merging v1 and v2. The resulting
graph G0 has vertices V 0 D V  f v1; v2 g[ f v1?
v2 g and set of edges!G0 obtained by restricting
the adjacency relation ! to V 0. We denote the
derive relation as G ).v1;v2/ G
0.
Informally, ifG represents some LCFRS produc-
tion p and v1; v2 represent nonterminals A1; A2,
thenG0 represents a production p0 obtained from p
by replacing A1; A2 with a fresh nonterminal A. A
new production p00 can also be constructed, expand-
ing A into A1; A2, so that p0; p00 together will be
equivalent to p. Furthermore, p0 has a rank smaller
than the rank of p and, from Lemma 2, A does not
increase the overall fan-out of the grammar.
In order to simplify the notation, we adopt the
following convention. Let G ).v1;v2/ G
0 and
let v!G v1, v ? v2. If v!G0 v1 ? v2, then
edges .v; v1/ and .v; v1 ? v2/ will be identified,
and we say that G0 inherits .v; v1 ? v2/ from G.
If v 6!G0 v1?v2, then we say that .v; v1/ does not
survive the derive step. This convention is used for
all edges incident upon v1 or v2.
Our factorization algorithm is reported in Fig-
ure 3. We start from an adjacency graph repres-
enting some LCFRS production that needs to be
factorized. We arbitrarily choose an edge e of the
graph, and push it into a set R, in order to keep
a record of the candidate factorization. We then
merge the two m-intervals incident to e, and we
recompute the adjacency relation for the new set
of vertices. We iterate until the resulting graph has
an empty edge set. If the final graph has one one
vertex, then we have managed to factorize our pro-
duction into a set of productions with rank at most
two that can be computed from R.
Example 6 Let V D f v1; v2; v3 g with v1 D
f ?4; 4? g, v2 D f ?1; 1?; ?3; 3? g, and v3 D
f ?2; 2?; ?5; 5? g. Then !G D f .v1; v2/ g. After
merging v1; v2 we have a new graph G with V D
f v1 ? v2; v3 g and!G D f .v1 ? v2; v3/ g. We
finally merge v1 ? v2; v3 resulting in a new graph
G with V D f v1 ? v2 ? v3 g and!G D ;. We
then accept and stop. 
4.3 Mathematical Properties
We have already argued that, if the algorithm ac-
cepts, then a binary factorization that does not
increase the fan-out of the grammar can be built
from R. We still need to prove that the algorithm
answers consistently on a given input, despite of
possibly different choices of edges at line 4. We do
this through several intermediate results.
A derivation for an adjacency graph G is a se-
quence of edges d D he1; : : : ; eni, n  1, such
that G D G0 and Gi 1 )ei Gi for every i with
1  i  n. For short, we write G0 )d Gn.
Two derivations for G are competing if one is a
permutation of the other.
Lemma 3 If G )d1 G1 and G )d2 G2 with d1
and d2 competing derivations, then G1 D G2.
Proof We claim that the statement of the lemma
holds for jd1j D 2. To see this, let G )e1
G01 )e2 G1 and G )e2 G
0
2 )e1 G2 be valid
derivations. We observe that G1 and G2 have the
same set of vertices. Since the edges of G1 and G2
are defined by restricting the adjacency relation to
their set of vertices, our claim immediately follows.
The statement of the lemma then follows from
the above claim and from the fact that we can al-
ways obtain the sequence d2 starting from d1 by
repeatedly switching consecutive edges. 
We now consider derivations for the same adja-
cency graph that are not competing, and show that
they always lead to isomorphic adjacency graphs.
Two graphs are isomorphic if they become equal
after some suitable renaming of the vertices.
Lemma 4 The out-degree of G is bounded by 2.
Proof Assume v!G v1 and v!G v2, with v1 ?
v2, and let I 2 v. I must be adjacent to some in-
terval I1 2 v1. Without loss of generality, assume
that I is left-adjacent to I1. I must also be adja-
cent to some interval I2 2 v2. Since v1 and v2
483
are disjoint, I must be right-adjacent to I2. This
implies that I cannot be adjacent to an interval in
any other m-interval v0 of G. 
A vertex v of G such that v!G v1 and v!G v2
is called a bifurcation.
Example 7 Assume v D f ?2; 2? g, v1 D
f ?3; 3?; ?5; 5? g, v2 D f ?1; 1? g with v!G v1 and
v!G v2. The m-interval v? v1 D f ?2; 3?; ?5; 5? g
is no longer adjacent to v2. 
The example above shows that, when choosing one
of the two outgoing edges in a bifurcation for mer-
ging, the other edge might not survive. Thus, such
a choice might lead to distinguishable derivations
that are not competing (one derivation has an edge
that is not present in the other). As we will see (in
the proof of Theorem 1), bifurcations are the only
cases in which edges might not survive a merging.
Lemma 5 Let v be a bifurcation of G with outgo-
ing edges e1; e2, and let G )e1 G1, G )e2 G2.
Then G1 and G2 are isomorphic.
Proof (Sketch) Assume e1 has the form
v!G v1 and e2 has the form v!G v2. Let
also VS be the set of vertices shared by G1 and
G2. We show that the statement holds under the
isomorphism mapping v ? v1 and v2 in G1 to v1
and v ? v2 in G2, respectively.
When restricted to VS , the graphs G1 and G2
are equal. Let us then consider edges from G1 and
G2 involving exactly one vertex in VS . We show
that, for v0 2 VS , v0!G1 v ? v1 if and only if
v0!G2 v1. Consider an arbitrary interval I
0 2 v0.
If v0!G1 v?v1, then I
0 must be adjacent to some
interval I1 2 v ? v1. If I1 2 v1 we are done.
Otherwise, I1 must be the concatenation of two
intervals I1v and I1v1 with I1v 2 v and I1v1 2
v1. Since v!G2 v2, I1v is also adjacent to some
interval in v2. However, v0 and v2 are disjoint.
Thus I 0 must be adjacent to I1v1 2 v1. Conversely,
if v0!G2 v1, then I
0 must be adjacent to some
interval I1 2 v1. Because v0 and v are disjoint, I 0
must also be adjacent to some interval in v ? v1.
Using very similar arguments, we can conclude
that G1 and G2 are isomorphic when restricted to
edges with at most one vertex in VS .
Finally, we need to consider edges from G1 and
G2 that are not incident upon vertices in VS . We
show that v ? v1!G1 v2 only if v1!G2 v ? v2;
a similar argument can be used to prove the con-
verse. Consider an arbitrary interval I1 2 v?v1. If
v ? v1!G1 v2, then I1 must be adjacent to some
interval I2 2 v2. If I1 2 v1 we are done. Other-
wise, I1 must be the concatenation of two adjacent
intervals I1v and I1v1 with I1v 2 v and I1v1 2 v1.
Since I1v is also adjacent to some interval I 02 2 v2
(here I 02 might as well be I2), we conclude that
I1v1 2 v1 is adjacent to the concatenation of I1v
and I 02, which is indeed an interval in v? v2. Note
that our case distinction is exhaustive. We thus
conclude that v1!G2 v ? v2.
A symmetrical argument can be used to show
that v2!G1 v ? v1 if and only if v ? v2!G2 v1,
which concludes our proof. 
Theorem 1 Let d1 and d2 be derivations for G,
describing two different computations c1 and c2 of
the algorithm of Figure 3 on input G. Computation
c1 is accepting if and only if c2 is accepting.
Proof First, we prove the claim that if e is not an
edge outgoing from a bifurcation vertex, then in the
derive relation G )e G0 all of the edges of G but
e and its reverse are inherited by G0. Let us write
e in the form v1!G v2. Obviously, any edge of
G not incident upon v1 or v2 will be inherited by
G0. If v!G v2 for some m-interval v ? v1, then
every interval I 2 v is adjacent to some interval
in v2. Since v and v1 are disjoint, I will also be
adjacent to some interval in v1?v2. Thus we have
v!G0 v1 ? v2. A similar argument shows that
v!G v1 implies v!G0 v1 ? v2.
If v2!G v for some v ? v1, then every in-
terval I 2 v2 is adjacent to some interval in v.
From v1!G v2 we also have that each interval
I12 2 v1 ? v2 is either an interval in v2 or else
the concatenation of exactly two intervals I1 2 v1
and I2 2 v2. (The interval I2 cannot be adjacent
to more than an interval in v1, because v2!G v).
In both cases I12 is adjacent to some interval in
v, and hence v1 ? v2!G0 v. This concludes the
proof of our claim.
Let d1, d2 be as in the statement of the the-
orem, with G )d1 G1 and G )d2 G2. If d1
and d2 are competing, then the theorem follows
from Lemma 3. Otherwise, assume that d1 and d2
are not competing. From our claim above, some
bifurcation vertices must appear in these deriva-
tions. Let us reorder the edges in d1 in such a way
that edges outgoing from a bifurcation vertex are
processed last and in some canonical order. The
resulting derivation has the form dd 01, where d
0
1
involves the processing of all bifurcation vertices.
We can also reorder edges in d2 to obtain dd 02,
where d 02 involves the processing of all bifurcation
484
not context-free 102 687 100.00%
not binarizable 24 0.02%
not well-nested 622 0.61%
Table 1: Properties of productions extracted from
the CoNLL 2006 data (3 794 605 productions)
vertices in exactly the same order as in d 01, but with
possibly different choices for the outgoing edges.
Let G )d Gd )d 01 G
0
1 and G )d Gd )d 02
G02. Derivations dd
0
1 and d1 are competing. Thus,
by Lemma 3, we haveG01 D G1. Similarly, we can
conclude that G02 D G2. Since bifurcation vertices
in d 01 and in d
0
2 are processed in the same canonical
order, from repeated applications of Lemma 5 we
have that G01 and G
0
2 are isomorphic. We then con-
clude that G1 and G2 are isomorphic as well. The
statement of the theorem follows immediately. 
We now turn to a computational analysis of the
algorithm of Figure 3. Let G be the representation
of an LCFRS production p with rank r . G has
r vertices and, following Lemma 4, O.r/ edges.
Let v be an m-interval of G with fan-out fv. The
incoming and outgoing edges for v can be detected
in time O.fv/ by inspecting the 2  fv endpoints of
v. Thus we can compute G in time O.jpj/.
The number of iterations of the while cycle in the
algorithm is bounded by r , since at each iteration
one vertex of G is removed. Consider now an
iteration in which m-intervals v1 and v2 have been
chosen for merging, with v1!G v2. (These m-
intervals might be associated with nonterminals
in the right-hand side of p, or else might have
been obtained as the result of previous merging
operations.) Again, we can compute the incoming
and outgoing edges of v1?v2 in time proportional
to the number of endpoints of such an m-interval.
By Lemma 2, this number is bounded by O.f /, f
the fan-out of the grammar. We thus conclude that
a run of the algorithm on G takes time O.r  f /.
5 Discussion
We have shown how to extract mildly context-
sensitive grammars from dependency treebanks,
and presented an efficient algorithm that attempts
to convert these grammars into an efficiently par-
seable binary form. Due to previous results (Ram-
bow and Satta, 1999), we know that this is not
always possible. However, our algorithm may fail
even in cases where a binarization exists?our no-
tion of adjacency is not strong enough to capture
all binarizable cases. This raises the question about
the practical relevance of our technique.
In order to get at least a preliminary answer to
this question, we extracted LCFRS productions
from the data used in the 2006 CoNLL shared task
on data-driven dependency parsing (Buchholz and
Marsi, 2006), and evaluated how large a portion
of these productions could be binarized using our
algorithm. The results are given in Table 1. Since it
is easy to see that our algorithm always succeeds on
context-free productions (productions where each
nonterminal has fan-out 1), we evaluated our al-
gorithm on the 102 687 productions with a higher
fan-out. Out of these, only 24 (0.02%) could not be
binarized using our technique. We take this number
as an indicator for the usefulness of our result.
It is interesting to compare our approach
with techniques for well-nested dependency trees
(Kuhlmann and Nivre, 2006). Well-nestedness is
a property that implies the binarizability of the
extracted grammar; however, the classes of well-
nested trees and those whose corresponding pro-
ductions can be binarized using our algorithm are
incomparable?in particular, there are well-nested
productions that cannot be binarized in our frame-
work. Nevertheless, the coverage of our technique
is actually higher than that of an approach that
relies on well-nestedness, at least on the CoNLL
2006 data (see again Table 1).
We see our results as promising first steps in a
thorough exploration of the connections between
non-projective and mildly context-sensitive pars-
ing. The obvious next step is the evaluation of our
technique in the context of an actual parser.
As a final remark, we would like to point out
that an alternative technique for efficient non-pro-
jective dependency parsing, developed by G?mez
Rodr?guez et al independently of this work, is
presented elsewhere in this volume.
Acknowledgements We would like to thank
Ryan McDonald, Joakim Nivre, and the anonym-
ous reviewers for useful comments on drafts of this
paper, and Carlos G?mez Rodr?guez and David J.
Weir for making a preliminary version of their pa-
per available to us. The work of the first author
was funded by the Swedish Research Council. The
second author was partially supported by MIUR
under project PRIN No. 2007TJNZRE_002.
485
References
Giuseppe Attardi. 2006. Experiments with a mul-
tilanguage non-projective dependency parser. In
Tenth Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 166?170, New
York, USA.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency pars-
ing. In Tenth Conference on Computational Natural
Language Learning (CoNLL), pages 149?164, New
York, USA.
Eugene Charniak. 1996. Tree-bank grammars. In 13th
National Conference on Artificial Intelligence, pages
1031?1036, Portland, Oregon, USA.
Jason Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In 16th In-
ternational Conference on Computational Linguist-
ics (COLING), pages 340?345, Copenhagen, Den-
mark.
Carlos G?mez-Rodr?guez, David J. Weir, and John
Carroll. 2009. Parsing mildly non-projective de-
pendency structures. In Twelfth Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL), Athens, Greece.
Jan Hajic?, Barbora Vidova Hladka, Jarmila Panevov?,
Eva Hajic?ov?, Petr Sgall, and Petr Pajas. 2001.
Prague Dependency Treebank 1.0. Linguistic Data
Consortium, 2001T10.
Keith Hall and V?clav Nov?k. 2005. Corrective mod-
elling for non-projective dependency grammar. In
Ninth International Workshop on Parsing Technolo-
gies (IWPT), pages 42?52, Vancouver, Canada.
Jir?? Havelka. 2007. Beyond projectivity: Multilin-
gual evaluation of constraints and measures on non-
projective structures. In 45th Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 608?615, Prague, Czech Republic.
Marco Kuhlmann and Mathias M?hl. 2007. Mildly
context-sensitive dependency languages. In 45th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 160?167, Prague, Czech
Republic.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly
non-projective dependency structures. In 21st In-
ternational Conference on Computational Linguist-
ics and 44th Annual Meeting of the Association for
Computational Linguistics (COLING-ACL), Main
Conference Poster Sessions, pages 507?514, Sydney,
Australia.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Eleventh Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL), pages 81?88, Trento, Italy.
Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency
parsing. In Tenth International Conference on Pars-
ing Technologies (IWPT), pages 121?132, Prague,
Czech Republic.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Human Lan-
guage Technology Conference (HLT) and Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP), pages 523?530, Vancouver,
Canada.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In 43rd Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 99?106, Ann Arbor, USA.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Eighth International
Workshop on Parsing Technologies (IWPT), pages
149?160, Nancy, France.
Joakim Nivre. 2007. Incremental non-projective
dependency parsing. In Human Language Tech-
nologies: The Conference of the North American
Chapter of the Association for Computational Lin-
guistics (HLT-NAACL), pages 396?403, Rochester,
NY, USA.
Owen Rambow and Giorgio Satta. 1999. Independent
parallelism in finite copying parallel rewriting sys-
tems. Theoretical Computer Science, 223(1?2):87?
120.
Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii,
and Tadao Kasami. 1991. On Multiple Context-
Free Grammars. Theoretical Computer Science,
88(2):191?229.
K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi.
1987. Characterizing structural descriptions pro-
duced by various grammatical formalisms. In 25th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 104?111, Stanford,
CA, USA.
486
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 539?547,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Optimal Reduction of Rule Length
in Linear Context-Free Rewriting Systems
Carlos Go?mez-Rodr??guez1, Marco Kuhlmann2, Giorgio Satta3 and David Weir4
1 Departamento de Computacio?n, Universidade da Corun?a, Spain (cgomezr@udc.es)
2 Department of Linguistics and Philology, Uppsala University, Sweden (marco.kuhlmann@lingfil.uu.se)
3 Department of Information Engineering, University of Padua, Italy (satta@dei.unipd.it)
4 Department of Informatics, University of Sussex, United Kingdom (davidw@sussex.ac.uk)
Abstract
Linear Context-free Rewriting Systems
(LCFRS) is an expressive grammar formalism
with applications in syntax-based machine
translation. The parsing complexity of an
LCFRS is exponential in both the rank
of a production, defined as the number of
nonterminals on its right-hand side, and a
measure for the discontinuity of a phrase,
called fan-out. In this paper, we present
an algorithm that transforms an LCFRS
into a strongly equivalent form in which
all productions have rank at most 2, and
has minimal fan-out. Our results generalize
previous work on Synchronous Context-Free
Grammar, and are particularly relevant for
machine translation from or to languages that
require syntactic analyses with discontinuous
constituents.
1 Introduction
There is currently considerable interest in syntax-
based models for statistical machine translation that
are based on the extraction of a synchronous gram-
mar from a corpus of word-aligned parallel texts;
see for instance Chiang (2007) and the references
therein. One practical problem with this approach,
apart from the sheer number of the rules that result
from the extraction procedure, is that the parsing
complexity of all synchronous formalisms that we
are aware of is exponential in the rank of a rule,
defined as the number of nonterminals on the right-
hand side. Therefore, it is important that the rules
of the extracted grammar are transformed so as to
minimise this quantity. Not only is this beneficial in
terms of parsing complexity, but smaller rules can
also improve a translation model?s ability to gener-
alize to new data (Zhang et al, 2006).
Optimal algorithms exist for minimising the size
of rules in a Synchronous Context-Free Gram-
mar (SCFG) (Uno and Yagiura, 2000; Zhang et al,
2008). However, the SCFG formalism is limited
to modelling word-to-word alignments in which a
single continuous phrase in the source language is
aligned with a single continuous phrase in the tar-
get language; as defined below, this amounts to
saying that SCFG have a fan-out of 2. This re-
striction appears to render SCFG empirically inad-
equate. In particular, Wellington et al (2006) find
that the coverage of a translation model can increase
dramatically when one allows a bilingual phrase to
stretch out over three rather than two continuous
substrings. This observation is in line with empir-
ical studies in the context of dependency parsing,
where the need for formalisms with higher fan-out
has been observed even in standard, single language
texts (Kuhlmann and Nivre, 2006).
In this paper, we present an algorithm that com-
putes optimal decompositions of rules in the for-
malism of Linear Context-Free Rewriting Systems
(LCFRS) (Vijay-Shanker et al, 1987). LCFRS was
originally introduced as a generalization of sev-
eral so-called mildly context-sensitive grammar for-
malisms. In the context of machine translation,
LCFRS is an interesting generalization of SCFG be-
cause it does not restrict the fan-out to 2, allow-
ing productions with arbitrary fan-out (and arbitrary
rank). Given an LCFRS, our algorithm computes a
strongly equivalent grammar with rank 2 and min-
539
imal increase in fan-out.1 In this context, strong
equivalence means that the derivations of the orig-
inal grammar can be reconstructed using some sim-
ple homomorphism (c.f. Nijholt, 1980). Our contri-
bution is significant because the existing algorithms
for decomposing SCFG, based on Uno and Yagiura
(2000), cannot be applied to LCFRS, as they rely
on the crucial property that components of biphrases
are strictly separated in the generated string: Given a
pair of synchronized nonterminal symbols, the ma-
terial derived from the source nonterminal must pre-
cede the material derived from the target nontermi-
nal, or vice versa. The problem that we solve has
been previously addressed by Melamed et al (2004),
but in contrast to our result, their algorithm does not
guarantee an optimal (minimal) increase in the fan-
out of the resulting grammar. However, this is essen-
tial for the practical applicability of the transformed
grammar, as the parsing complexity of LCFRS is ex-
ponential in both the rank and the fan-out.
Structure of the paper The remainder of the pa-
per is structured as follows. Section 2 introduces the
terminology and notation that we use for LCFRS.
In Section 3, we present the technical background
of our algorithm; the algorithm itself is discussed
in Section 4. Section 5 concludes the paper by dis-
cussing related work and open problems.
General notation The set of non-negative integers
is denoted by N. For i, j ? N, we write [i, j] to
denote the interval { k ? N | i ? k ? j }, and use
[i] as a shorthand for [1, i]. Given an alphabet V , we
write V ? for the set of all (finite) strings over V .
2 Preliminaries
We briefly summarize the terminology and notation
that we adopt for LCFRS; for detailed definitions,
see Vijay-Shanker et al (1987).
2.1 Linear, non-erasing functions
Let V be an alphabet. For natural numbers r ? 0
and f, f1, . . . , fr ? 1, a function
g : (V ?)f1 ? ? ? ? ? (V ?)fr ? (V ?)f
1Rambow and Satta (1999) show that without increasing
fan-out it is not always possible to produce even weakly equiv-
alent grammars.
is called a linear, non-erasing function over V of
type f1 ? ? ? ? ? fr ? f , if it can be defined by an
equation of the form
g(?x1,1, . . . , x1,f1?, . . . , ?xr,1, . . . , xr,fr?) = ?g ,
where ?g = ??g,1, . . . , ?g,f ? is an f -tuple of strings
over the variables on the left-hand side of the equa-
tion and symbols in V that contains exactly one oc-
currence of each variable. We call the value r the
rank of g, the value f its fan-out, and write ?(g)
and ?(g), respectively, to denote these quantities.
Note that, if we assume the variables on the left-
hand side of the defining equation of g to be named
according to the specific schema given above, then g
is uniquely determined by ?g.
2.2 Linear context-free rewriting systems
A linear context-free rewriting system (LCFRS)
is a construct G = (VN , VT , P, S), where: VN is
an alphabet of nonterminal symbols in which each
symbol A ? VN is associated with a value ?(A),
called its fan-out; VT is an alphabet of terminal
symbols; S ? N is a distinguished start symbol with
?(S) = 1; and P is a set of productions of the form
p : A? g(B1, B2, . . . , Br) ,
where A,B1, . . . , Br ? VN , and g is a linear, non-
erasing function over the terminal alphabet VT of
type ?(B1) ? ? ? ? ? ?(Br) ? ?(A). In a deriva-
tion of an LCFRS, the production p can be used to
transform a sequence of r tuples of strings, gener-
ated by the nonterminals B1, . . . , Br, into a single
?(A)-tuple of strings, associated with the nonter-
minal A. The values ?(g) and ?(g) are called the
rank and fan-out of p, respectively, and we write
?(p) and ?(p), respectively, to denote these quan-
tities. The rank and fan-out of G, written ?(G)
and ?(G), respectively, are the maximum rank and
fan-out among all of its productions. Given that
?(S) = 1, a derivation will associate S with a set of
one-component tuples of strings over VT ; this forms
the string language generated by G.
Example 1 The following LCFRS generates the
string language { anbncndn | n ? N }. We only
specify the set of productions; the remaining com-
540
ponents of the grammar are obvious from that.
S ? g1(R) g1(?x1,1, x1,2?) = ?x1,1x1,2?
R? g2(R) g2(?x1,1, x1,2?) = ?ax1,1b, cx1,2d?
R? g3 g3 = ??, ??
The functions g1 and g2 have rank 1; the function g3
has rank 0. The functions g2 and g3 have fan-out 2;
the function g1 has fan-out 1. 2
3 Technical background
The general idea behind our algorithm is to replace
each production of an LCFRS with a set of ?shorter?
productions that jointly are equivalent to the original
production. Before formalizing this idea, we first in-
troduce a specialized representation for the produc-
tions of an LCFRS.
We distinguish between occurrences of symbols
within a string by exploiting two different notations.
Let ? = a1a2 ? ? ? an be a string. The occurrence ai
in ? can be denoted by means of its position index
i ? [n], or else by means of its two (left and right)
endpoints, i?1 and i; here, the left (right) endpoint
denotes a boundary between occurrence ai and the
previous (subsequent) occurrence, or the beginning
(end) of the string ?. Similarly, a substring ai ? ? ? aj
of ? with i ? j can be denoted by the positions
i, i+ 1, . . . , j of its occurrences, or else by means of
its left and right endpoints, i? 1 and j.
3.1 Production representation
For the remainder of this section, let us fix an
LCFRS G = (VN , VT , P, S) and a production
p : A ? g(B1, . . . , Br) of G, with g defined as
in Section 2.1. We define
|p| = ?(g) +
?(g)?
i=1
|?g,i|.
Let $ be a fresh symbol that does not occur inG. We
define the characteristic string of the production p
as
?(p) = ?g,1$ ? ? ? $?g,?(g) ,
and the variable string of p as the string ?N (p) ob-
tained from ?(p) by removing all the occurrences of
symbols in VT .
Example 2 We will illustrate the concepts intro-
duced in this section using the concrete production
p0 : A? g(B1, B2, B3), where
?g = ?x1,1ax2,1x1,2, x3,1bx3,2? .
In this case, we have
?(p0) = x1,1ax2,1x1,2$x3,1bx3,2 , and
?N (p0) = x1,1x2,1x1,2$x3,1x3,2 . 2
Let I be an index set, I ? [r]. Consider the set B of
occurrences Bi in the right-hand side of p such that
i ? I .2 We define the position set of B, denoted
by ?B, as the set of all positions 1 ? j ? |?N (p)|
such that the jth symbol in ?N (p) is a variable of the
form xi,h, for i ? I and some h ? 1.
Example 3 Some position sets of p0 are
?{B1} = {1, 3} ,?{B2} = {2} ,?{B3} = {5, 6} .
2
A position set ?B can be uniquely expressed as the
union of f ? 1 intervals [l1 + 1, r1], . . . , [lf + 1, rf ]
such that ri?1 < li for every 1 < i ? f . Thus we
define the set of endpoints of ?B as
?B = { lj | j ? [f ] } ? { rj | j ? [f ] } .
The quantity f is called the fan-out of ?B, writ-
ten ?(?B). Notice that the fan-out of a position set
?{B} does not necessarily coincide with the fan-out
of the non-terminal B in the underlying LCFRS. A
set with 2f endpoints always corresponds to a posi-
tion set of fan-out f .
Example 4 For our running example, we have
?{B1} = {0, 1, 2, 3}, ?{B2} = {1, 2}, ?{B3} =
{4, 6}. Consequently, the fan-out of ?{B1} is 2, and
the fan-out of ?{B2} and ?{B3} is 1. Notice that the
fan-out of the non-terminal B3 is 2. 2
We drop B from ?B and ?B whenever this set is
understood from the context or it is not relevant.
Given a set of endpoints ? = {i1, . . . , i2f} with
i1 < ? ? ? < i2f , we obtain its corresponding position
set by calculating the closure of ?, defined as
[?] = ?fj=1[i2j?1 + 1, i2j ] .
2To avoid clutter in our examples, we abuse the notation by
not making an explicit distinction between nonterminals and oc-
currences of nonterminals in productions.
541
3.2 Reductions
Assume that r > 2. The reduction of p by the non-
terminal occurrencesBr?1, Br is the ordered pair of
productions (p1, p2) that is defined as follows. Let
?1, . . . , ?n be the maximal substrings of ?(p) that
contain only variables xi,j with r ? 1 ? i ? r and
terminal symbols, and at least one variable. Then
p1 : A? g1(B1, . . . , Br?2, X) and
p2 : X ? g2(Br?1, Br) ,
where X is a fresh nonterminal symbol, the char-
acteristic string ?(p1) is the string obtained from
?(p) by replacing each substring ?i by the vari-
able xr?1,i, and the characteristic string ?(p2) is the
string ?1$ ? ? ? $?n.
Note that the defining equations of neither g1
nor g2 are in the specific form discussed in Sec-
tion 2.1; however, they can be brought into this form
by a consistent renaming of the variables. We will
silently assume this renaming to take place.
Example 5 The reduction of p0 by the nonterminal
occurrences B2 and B3 has p1 : A ? g1(B1, X)
and p2 : X ? g2(B2, B3) with
?(p1) = x1,1x2,1x1,2$x2,2
?(p2) = ax2,1$x3,1bx3,2
or, after renaming and in standard notation,
g1(?x1,1, x1,2?, ?x2,1, x2,2?) = ?x1,1x2,1x1,2, x2,2?
g2(?x1,1?, ?x2,1, x2,2?) = ?ax1,1, x2,1bx2,2? .2
It is easy to check that a reduction provides us with a
pair of productions that are equivalent to the original
production p, in terms of generative capacity, since
g1(B1, . . . , Br?2, g2(Br?1, Br)) = g(B1, . . . , Br)
for all tuples of strings generated from the nontermi-
nalsB1, . . . , Br, respectively. Note also that the fan-
out of production p1 equals the fan-out of p. How-
ever, the fan-out of p2 (the value n) may be greater
than the fan-out of p, depending on the way vari-
ables are arranged in ?(p). Thus, a reduction does
not necessarily preserve the fan-out of the original
production. In the worst case, the fan-out of p2 can
be as large as ?(Br?1) + ?(Br).
1: Function NAIVE-BINARIZATION(p)
2: result? ?;
3: currentProd? p;
4: while ?(currentProd) > 2 do
5: (p1, p2)? any reduction of currentProd;
6: result? result ? p2;
7: currentProd? p1;
8: return result ? currentProd;
Figure 1: The naive algorithm
We have defined reductions only for the last two
occurrences of nonterminals in the right-hand side of
a production p. However, it is easy to see that we can
also define the concept for two arbitrary (not neces-
sarily adjacent) occurrences of nonterminals, at the
cost of making the notation more complicated.
4 The algorithm
Let G be an LCFRS with ?(G) = f and ?(G) = r,
and let f ? ? f be a target fan-out. We will now
present an algorithm that computes an equivalent
LCFRS G? of fan-out at most f ? whose rank is at
most 2, if such an LCFRS exists in the first place.
The algorithm works by exhaustively reducing all
productions in G.
4.1 Naive algorithm
Given an LCFRS production p, a naive algorithm
to compute an equivalent set of productions whose
rank is at most 2 is given in Figure 1. By ap-
plying this algorithm to all the productions in the
LCFRSG, we can obtain an equivalent LCFRS with
rank 2. We will call such an LCFRS a binarization
of G.
The fan-out of the obtained LCFRS will depend
on the nonterminals that we choose for the reduc-
tions in line 5. It is not difficult to see that, in the
worst case, the resulting fan-out can be as high as
d r2e ? f . This occurs when we choose d r2e nonter-minals with fan-out f that have associated variables
in the string ?N (p) that do not occur at consecutive
positions.
The algorithm that we develop in Section 4.3 im-
proves on the naive algorithm in that it can be ex-
ploited to find a sequence of reductions that results
in a binarization of G that is optimal, i.e., leads to
542
an LCFRS with minimal fan-out. The algorithm is
based on a technical concept called adjacency.
4.2 Adjacency
Let p be some production in the LCFRS G, and let
?1,?2 be sets of endpoints, associated with some
sets of nonterminal occurrences in p. We say that?1
and ?2 overlap if the intersection of their closures
is nonempty, that is, if [?1]? [?2] 6= ?. Overlapping
holds if and only if the associated sets of nontermi-
nal occurrences are not disjoint. If ?1 and ?2 do
not overlap, we define their merge as
?(?1,?2) = (?1 ??2) \ (?1 ??2) .
It is easy to see that [?(?1,?2)] = [?1] ? [?2].
We say that ?1 and ?2 are adjacent for a given fan-
out f , written ?1 ?f ?2, if ?1 and ?2 do not
overlap, and ?([?(?1,?2)]) ? f .
Example 6 For the production p0 from Example 2,
we have ?(?{B1},?{B2}) = {0, 3}, showing that?{B1} ?1 ?{B2}. Similarly, we have
?(?{B1},?{B3}) = {0, 1, 2, 3, 4, 6} ,
showing that ?{B1} ?3 ?{B3}, but that neither?{B1} ?2 ?{B3} nor ?{B1} ?1 ?{B3} holds. 2
4.3 Bounded binarization algorithm
The adjacency-based binarization algorithm is given
in Figure 2. It starts with a working set contain-
ing the endpoint sets corresponding to each non-
terminal occurrence in the input production p. Re-
ductions of p are only explored for nonterminal oc-
currences whose endpoint sets are adjacent for the
target fan-out f ?, since reductions not meeting this
constraint would produce productions with fan-out
greater than f ?. Each reduction explored by the al-
gorithm produces a new endpoint set, associated to
the fresh nonterminal that it introduces, and this new
endpoint set is added to the working set and poten-
tially used in further reductions.
From the definition of the adjacency relation?f ,
it follows that at lines 9 and 10 of BOUNDED-
BINARIZATION we only pick up reductions for p
that do not exceed the fan-out bound of f ?. This
implies soundness for our algorithm. Completeness
means that the algorithm fails only if there exists no
binarization for p of fan-out not greater than f ?. This
1: Function BOUNDED-BINARIZATION(p, f ?)
2: workingSet? ?;
3: agenda? ?;
4: for all i from 1 to ?(p) do
5: workingSet? workingSet ? {?{Bi}};
6: agenda? agenda ? {?{Bi}};
7: while agenda 6= ? do
8: ?? pop some endpoint set from agenda;
9: for all ?1 ? workingSet with ?1 ?f ? ? do
10: ?2 = ?(?,?1);
11: if ?2 /? workingSet then
12: workingSet? workingSet ? {?2};
13: agenda? agenda ? {?2};
14: if ?{B1,B2,...,B?(p))} ? workingSet then
15: return true;
16: else
17: return false;
Figure 2: Algorithm to compute a bounded binarization
property is intuitive if one observes that our algo-
rithm is a specialization of standard algorithms for
the computation of the closure of binary relations.
A formal proof of this fact is rather long and te-
dious, and will not be reported here. We notice that
there is a very close similarity between algorithm
BOUNDED-BINARIZATION and the deduction pro-
cedure proposed by Shieber et al (1995) for parsing.
We discuss this more at length in Section 5.
Note that we have expressed the algorithm as a
decision function that will return true if there exists
a binarization of p with fan-out not greater than f ?,
and false otherwise. However, the algorithm can
easily be modified to return a reduction producing
such a binarization, by adding to each endpoint set
? ? workingSet two pointers to the adjacent end-
point sets that were used to obtain it. If the algorithm
is successful, the tree obtained by following these
pointers from the final endpoint set ?{B1,...,B?(p)} ?workingSet gives us a tree of reductions that will
produce a binarization of p with fan-out not greater
than f ?, where each node labeled with the set ?{Bi}
corresponds to the nonterminal Bi, and nodes la-
beled with other endpoint sets correspond to the
fresh nonterminals created by the reductions.
543
4.4 Implementation
In order to implement BOUNDED-BINARIZATION,
we can represent endpoint sets in a canonical way
as 2f ?-tuples of integer positions in ascending order,
and with some special null value used to fill posi-
tions for endpoint sets with fan-out strictly smaller
than f ?. We will assume that the concrete null value
is larger than any other integer.
We also need to provide some appropriate repre-
sentation for the set workingSet, in order to guar-
antee efficient performance for the membership test
and the insertion operation. Both operations can be
implemented in constant time if we represent work-
ingSet as an (2?f ?)-dimensional table with Boolean
entries. Each dimension is indexed by values in
[0, n] plus our special null value; here n is the length
of the string ?N (p), and thus n = O(|p|). However,
this has the disadvantage of using space ?(n2f ?),
even in case workingSet is sparse, and is affordable
only for quite small values of f ?. Alternatively, we
can more compactly represent workingSet as a trie
data structure. This representation has size certainly
smaller than 2f ? ? q, where q is the size of the set
workingSet. However, both membership and inser-
tion operations take now an amount of time O(2f ?).
We now analyse the time complexity of algorithm
BOUNDED-BINARIZATION for inputs p and f ?. We
first focus on the while-loop at lines 7 to 13. As
already observed, the number of possible endpoint
sets is bounded by O(n2f ?). Furthermore, because
of the test at line 11, no endpoint set is ever inserted
into the agenda variable more than once in a sin-
gle run of the algorithm. We then conclude that our
while-loop cycles a number of times O(n2f ?).
We now focus on the choice of the endpoint set
?1 in the inner for-loop at lines 9 to 13. Let us fix ?
as in line 8. It is not difficult to see that any ?1 with
?1 ?f ? ? must satisfy
?(?) + ?(?1)? |? ??1| ? f ?. (1)
Let I ? ?, and consider all endpoint sets ?1 with
? ??1 = I . Given (1), we also have
?(?1) ? f ? + |I| ? ?(?). (2)
This means that, for each ? coming out of the
agenda, at line 9 we can choose all endpoint sets ?1
such that ?1 ?f ? ? by performing the following
steps:
? arbitrarily choose a set I ? ?;
? choose endpoints in set ?1\I subject to (2);
? test whether ?1 belongs to workingSet and
whether ?, ?1 do not overlap.
We claim that, in the above steps, the number
of involved endpoints does not exceed 3f ?. To
see this, we observe that from (2) we can derive
|I| ? ?(?) + ?(?1) ? f ?. The total number
of (distinct) endpoints in a single iteration step is
e = 2?(?) + 2?(?1) ? |I|. Combining with the
above inequality we have
e ? 2?(?) + 2?(?1)? ?(?)? ?(?1) + f ?
= ?(?) + ?(?1) + f ? ? 3f ? ,
as claimed. Since each endpoint takes values in
the set [0, n], we have a total of O(n3f ?) different
choices. For each such choice, we need to clas-
sify an endpoint as belonging to either ?\I , ?1\I ,
or I . This amounts to an additional O(33f ?) dif-
ferent choices. Overall, we have a total number of
O((3n)3f ?) different choices. For each such choice,
the test for membership in workingSet for ?1 takes
constant time in case we use a multi-dimensional ta-
ble, or else O(|p|) in case we use a trie. The ad-
jacency test and the merge operations can easily be
carried out in time O(|p|).
Putting all of the above observations together, and
using the already observed fact that n = O(|p|),
we can conclude that the total amount of time re-
quired by the while-loop at lines 7 to 13 is bounded
byO(|p| ? (3|p|)3f ?), both under the assumption that
workingSet is represented as a multi-dimensional ta-
ble or as a trie. This is also a bound on the running
time of the whole algorithm.
4.5 Minimal binarization of a complete LCFRS
The algorithm defined in Section 4.3 can be used
to binarize an LCFRS in such a way that each rule
in the resulting binarization has the minimum pos-
sible fan-out. This can be done by applying the
BOUNDED-BINARIZATION algorithm to each pro-
duction p, until we find the minimum value for the
544
1: Function MINIMAL-BINARIZATION(G)
2: pb = ? {Set of binarized productions}
3: for all production p of G do
4: f ? = fan-out(p);
5: while not BOUNDED-BINARIZATION(p, f ?)
do
6: f ? = f ? + 1;
7: add result of BOUNDED-BINARIZATION(p,
f ?) to pb; {We obtain the tree from
BOUNDED-BINARIZATION as explained in
Section 4.3 and use it to binarize p}
8: return pb;
Figure 3: Minimal binarization by sequential search
bound f ? for which this algorithm finds a binariza-
tion. For a production with rank r and fan-out f ,
we know that this optimal value of f ? must be in
the interval [f, d r2e ? f ] because binarizing a pro-duction cannot reduce its fan-out, and the NAIVE-
BINARIZATION algorithm seen in Section 4.1 can
binarize any production by increasing fan-out to
d r2e ? f in the worst case.
The simplest way of finding out the optimal value
of f ? for each production is by a sequential search
starting with ?(p) and going upwards, as in the algo-
rithm in Figure 3. Note that the upper bound d r2e ? fthat we have given for f ? guarantees that the while-
loop in this algorithm always terminates.
In the worst case, we may need f ? (d r2e ? 1) + 1executions of the BOUNDED-BINARIZATION algo-
rithm to find the optimal binarization of a production
in G. This complexity can be reduced by changing
the strategy to search for the optimal f ?: for exam-
ple, we can perform a binary search within the inter-
val [f, d r2e ? f ], which lets us find the optimal bina-rization in blog(f ? (d r2e?1)+1)c+1 executions ofBOUNDED-BINARIZATION. However, this will not
result in a practical improvement, since BOUNDED-
BINARIZATION is exponential in the value of f ? and
the binary search will require us to run it on val-
ues of f ? larger than the optimal in most cases. An
intermediate strategy between the two is to apply
exponential backoff to try the sequence of values
f?1+2i (for i = 0, 1, 2 . . .). When we find the first
i such that BOUNDED-BINARIZATION does not fail,
if i > 0, we apply the same strategy to the interval
[f?1+2i?1, f?2+2i], and we repeat this method to
shrink the interval until BOUNDED-BINARIZATION
does not fail for i = 0, giving us our optimal f ?.
With this strategy, the amount of executions of the
algorithm that we need in the worst case is
1
2(dlog(?)e+ dlog(?)e
2) + 1 ,
where ? = f ? (d r2e ? 1) + 1, but we avoid usingunnecessarily large values of f ?.
5 Discussion
To conclude this paper, we now discuss a number of
aspects of the results that we have presented, includ-
ing various other pieces of research that are particu-
larly relevant to this paper.
5.1 The tradeoff between rank and fan-out
The algorithm introduced in this paper can be used
to transform an LCFRS into an equivalent form
with rank 2. This will result into a more effi-
ciently parsable LCFRS, since rank exponentially
affects parsing complexity. However, we must take
into account that parsing complexity is also influ-
enced by fan-out. Our algorithm guarantees a min-
imal increase in fan-out. In practical cases it seems
such an increase is quite small. For example, in
the context of dependency parsing, both Go?mez-
Rodr??guez et al (2009) and Kuhlmann and Satta
(2009) show that all the structures in several well-
known non-projective dependency treebanks are bi-
narizable without any increase in their fan-out.
More in general, it has been shown by Seki et al
(1991) that parsing of LCFRS can be carried out in
time O(n|pM |), where n is the length of the input
string and pM is the production in the grammar with
largest size.3 Thus, there may be cases in which one
has to find an optimal tradeoff between rank and fan-
out, in order to minimize the size of pM . This re-
quires some kind of Viterbi search over the space of
all possible binarizations, constructed as described
at the end of Subsection 4.3, for some appropriate
value of the fan-out f ?.
3The result has been shown for the formalism of multiple
context-free grammars (MCFG), but it also applies to LCFRS,
which are a special case of MCFG.
545
5.2 Extension to general LCFRS
This paper has focussed on string-based LCFRS.
As discussed in Vijay-Shanker et al (1987), LCFRS
provide a more general framework where the pro-
ductions are viewed as generating a set of abstract
derivation trees. These trees can be used to specify
how structures other than tuples of strings are com-
posed. For example, LCFRS derivation trees can be
used to specify how the elementary trees of a Tree
Adjoining Grammar can be composed to produced
derived tree. However, the results in this paper also
apply to non-string-based LCFRS, since by limit-
ing attention to the terminal string yield of whatever
structures are under consideration, the composition
operations can be defined using the string-based ver-
sion of LCFRS that is discussed here.
5.3 Similar algorithmic techniques
The NAIVE-BINARIZATION algorithm given in Fig-
ure 1 is not novel to this paper: it is similar to
an algorithm developed in Melamed et al (2004)
for generalized multitext grammars, a formalism
weakly equivalent to LCFRS that has been intro-
duced for syntax-based machine translation. How-
ever, the grammar produced by our algorithm has
optimal (minimal) fan-out. This is an important im-
provement over the result in (Melamed et al, 2004),
as this quantity enters into the parsing complexity
of both multitext grammars and LCFRS as an expo-
nential factor, and therefore must be kept as low as
possible to ensure practically viable parsing.
Rank reduction is also investigated in Nesson
et al (2008) for synchronous tree-adjoining gram-
mars, a synchronous rewriting formalism based on
tree-adjoining grammars Joshi and Schabes (1992).
In this case the search space of possible reductions
is strongly restricted by the tree structures specified
by the formalism, resulting in simplified computa-
tion for the reduction algorithms. This feature is not
present in the case of LCFRS.
There is a close parallel between the technique
used in the MINIMAL-BINARIZATION algorithm
and deductive parsing techniques as proposed by
Shieber et al (1995), that are usually implemented
by means of tabular methods. The idea of exploit-
ing tabular parsing in production factorization was
first expressed in Zhang et al (2006). In fact, the
particular approach presented here has been used
to improve efficiency of parsing algorithms that use
discontinuous syntactic models, in particular, non-
projective dependency grammars, as discussed in
Go?mez-Rodr??guez et al (2009).
5.4 Open problems
The bounded binarization algorithm that we have
presented has exponential run-time in the value of
the input fan-out bound f ?. It remains an open ques-
tion whether the bounded binarization problem for
LCFRS can be solved in deterministic polynomial
time. Even in the restricted case of f ? = ?(p), that
is, when no increase in the fan-out of the input pro-
duction is allowed, we do not know whether p can be
binarized using only deterministic polynomial time
in the value of p?s fan-out. However, our bounded
binarization algorithm shows that the latter problem
can be solved in polynomial time when the fan-out
of the input LCFRS is bounded by some constant.
Whether the bounded binarization problem can
be solved in polynomial time in the value of the
input bound f ? is also an open problem in the re-
stricted case of synchronous context-free grammars,
a special case of an LCFRS of fan-out two with
a strict separation between the two components of
each nonterminal in the right-hand side of a produc-
tion, as discussed in the introduction. An interesting
analysis of this restricted problem can be found in
Gildea and Stefankovic (2007).
Acknowledgements The work of Carlos Go?mez-
Rodr??guez was funded by Ministerio de Educacio?n
y Ciencia and FEDER (HUM2007-66607-C04) and
Xunta de Galicia (PGIDIT07SIN005206PR, IN-
CITE08E1R104022ES, INCITE08ENA305025ES,
INCITE08PXIB302179PR and Rede Galega de
Procesamento da Linguaxe e Recuperacio?n de Infor-
macio?n). The work of Marco Kuhlmann was funded
by the Swedish Research Council. The work of
Giorgio Satta was supported by MIUR under project
PRIN No. 2007TJNZRE 002. We are grateful to an
anonymous reviewer for a very detailed review with
a number of particularly useful suggestions.
546
References
David Chiang. 2007. Hierarchical phrase-
based translation. Computational Linguistics,
33(2):201?228.
Daniel Gildea and Daniel Stefankovic. 2007. Worst-
case synchronous grammar rules. In Human Lan-
guage Technologies 2007: The Conference of the
North American Chapter of the Association for
Computational Linguistics; Proceedings of the
Main Conference, pages 147?154. Association
for Computational Linguistics, Rochester, New
York.
Carlos Go?mez-Rodr??guez, David J. Weir, and John
Carroll. 2009. Parsing mildly non-projective de-
pendency structures. In Twelfth Conference of the
European Chapter of the Association for Compu-
tational Linguistics (EACL). To appear.
A. K. Joshi and Y. Schabes. 1992. Tree adjoining
grammars and lexicalized grammars. In M. Nivat
and A. Podelsky, editors, Tree Automata and Lan-
guages. Elsevier, Amsterdam, The Netherlands.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly
non-projective dependency structures. In 21st
International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics (COLING-
ACL), Main Conference Poster Sessions, pages
507?514. Sydney, Australia.
Marco Kuhlmann and Giorgio Satta. 2009. Tree-
bank grammar techniques for non-projective de-
pendency parsing. In Twelfth Conference of the
European Chapter of the Association for Compu-
tational Linguistics (EACL). To appear.
I. Dan Melamed, Benjamin Wellington, and Gior-
gio Satta. 2004. Generalized multitext gram-
mars. In 42nd Annual Meeting of the Association
for Computational Linguistics (ACL), pages 661?
668. Barcelona, Spain.
Rebecca Nesson, Giorgio Satta, and Stuart M.
Shieber. 2008. Optimal k-arization of syn-
chronous tree-adjoining grammar. In Proceedings
of ACL-08: HLT, pages 604?612. Association for
Computational Linguistics, Columbus, Ohio.
A. Nijholt. 1980. Context-Free Grammars: Cov-
ers, Normal Forms, and Parsing, volume 93.
Springer-Verlag, Berlin, Germany.
Owen Rambow and Giorgio Satta. 1999. Indepen-
dent parallelism in finite copying parallel rewrit-
ing systems. Theoretical Computer Science,
223(1?2):87?120.
Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii,
and Tadao Kasami. 1991. On Multiple Context-
Free Grammars. Theoretical Computer Science,
88(2):191?229.
Stuart M. Shieber, Yves Schabes, and Fernando
Pereira. 1995. Principles and implementation of
deductive parsing. Journal of Logic Program-
ming, 24(1?2):3?36.
Takeaki Uno and Mutsunori Yagiura. 2000. Fast al-
gorithms to enumerate all common intervals of
two permutations. Algorithmica, 26(2):290?309.
K. Vijay-Shanker, David J. Weir, and Aravind K.
Joshi. 1987. Characterizing structural descrip-
tions produced by various grammatical for-
malisms. In 25th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL), pages
104?111. Stanford, CA, USA.
Benjamin Wellington, Sonjia Waxmonsky, and
I. Dan Melamed. 2006. Empirical lower bounds
on the complexity of translational equivalence. In
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics (COLING-
ACL), pages 977?984. Sydney, Australia.
Hao Zhang, Daniel Gildea, and David Chiang.
2008. Extracting synchronous grammar rules
from word-level alignments in linear time. In
22nd International Conference on Computational
Linguistics (Coling), pages 1081?1088. Manch-
ester, England, UK.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Human Language Technol-
ogy Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 256?263. New York, USA.
547
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 73?76,
Paris, October 2009. c?2009 Association for Computational Linguistics
An Improved Oracle for Dependency Parsing with Online Reordering
Joakim Nivre?? Marco Kuhlmann? Johan Hall?
?Uppsala University, Department of Linguistics and Philology, SE-75126 Uppsala
?V?xj? University, School of Mathematics and Systems Engineering, SE-35195 V?xj?
E-mail: FIRSTNAME.LASTNAME@lingfil.uu.se
Abstract
We present an improved training strategy
for dependency parsers that use online re-
ordering to handle non-projective trees.
The new strategy improves both efficiency
and accuracy by reducing the number of
swap operations performed on non-project-
ive trees by up to 80%. We present state-of-
the-art results for five languages with the
best ever reported results for Czech.
1 Introduction
Recent work on dependency parsing has resulted in
considerable progress with respect to both accuracy
and efficiency, not least in the treatment of discon-
tinuous syntactic constructions, usually modeled
by non-projective dependency trees. While non-
projective dependency relations tend to be rare in
most languages (Kuhlmann and Nivre, 2006), it
is not uncommon that up to 25% of the sentences
have a structure that involves at least one non-pro-
jective relation, a relation that may be crucial for
an adequate analysis of predicate-argument struc-
ture. This makes the treatment of non-projectivity
central for accurate dependency parsing.
Unfortunately, parsing with unrestricted non-pro-
jective structures is a hard problem, for which exact
inference is not possible in polynomial time except
under drastic independence assumptions (McDon-
ald and Satta, 2007), and most data-driven parsers
therefore use approximate methods (Nivre et al,
2006; McDonald et al, 2006). One recently ex-
plored approach is to perform online reordering
by swapping adjacent words of the input sentence
while building the dependency structure. Using this
technique, the system of Nivre (2009) processes
unrestricted non-projective structures with state-of-
the-art accuracy in observed linear time.
The normal procedure for training a transition-
based parser is to use an oracle that predicts an
optimal transition sequence for every dependency
tree in the training set, and then approximate this
oracle by a classifier. In this paper, we show that
the oracle used for training by Nivre (2009) is sub-
optimal because it eagerly swaps words as early
as possible and therefore makes a large number of
unnecessary transitions, which potentially affects
both efficiency and accuracy. We propose an altern-
ative oracle that reduces the number of transitions
by building larger structures before swapping, but
still handles arbitrary non-projective structures.
2 Background
The fundamental reason why sentences with non-
projective dependency trees are hard to parse is that
they contain dependencies between non-adjacent
substructures. The basic idea in online reordering
is to allow the parser to swap input words so that
all dependency arcs can be constructed between
adjacent subtrees. This idea is implemented in the
transition system proposed by Nivre (2009). The
first three transitions of this system (LEFT-ARC,
RIGHT-ARC, and SHIFT) are familiar from many
systems for transition-based dependency parsing
(Nivre, 2008). The only novelty is the SWAP trans-
ition, which permutes two nodes by moving the
second-topmost node from the stack back to the
input buffer while leaving the top node on the stack.
To understand how we can parse sentences with
non-projective dependency trees, in spite of the
fact that dependencies can only be added between
nodes that are adjacent on the stack, note that, for
any sentence x with dependency tree G, there is
always some permutation x? of x such thatG is pro-
jective with respect to x?. There may be more than
one such permutation, but Nivre (2009) defines the
canonical projective order <G for x given G as
the order given by an inorder traversal of G that
respects the order < between a node and its direct
dependents. This is illustrated in Figure 1, where
the words of a sentence with a non-projective tree
73
ROOT Who did you send the letter to ?
0 6 1 2 3 4 5 7 8
ROOT
NMOD
P
VG
SUBJ
OBJ2
OBJ1
DET
Figure 1: Dependency tree for an English sentence with projective order annotation.
have been annotated with their positions in the pro-
jective order; reading the words in this order gives
the permuted string Did you send the letter who to?
3 Training Oracles
In order to train classifiers for transition-based pars-
ing, we need a training oracle, that is, a function
that maps every dependency tree T in the training
set to a transition sequence that derives T . While
every complete transition sequence determines a
unique dependency tree, the inverse does not neces-
sarily hold. This also means that it may be possible
to construct different training oracles. For simple
systems that are restricted to projective dependency
trees, such differences are usually trivial, but for
a system that allows online reordering there may
be genuine differences that can affect both the effi-
ciency and accuracy of the resulting parsers.
3.1 The Old Oracle
Figure 2 defines the original training oracle ?1 pro-
posed by Nivre (2009). This oracle follows an
eager reordering strategy; it predicts SWAP in every
configuration where this is possible. The basic in-
sight in this paper is that, by postponing swaps and
building as much of the tree structure as possible
before swapping, we can significantly decrease the
length of the transition sequence for a given sen-
tence and tree. This may benefit the efficiency of
the parser trained using the oracle, as each trans-
ition takes a certain time to predict and to execute.
Longer transition sequences may also be harder to
learn than shorter ones, which potentially affects
the accuracy of the parser.
3.2 A New Oracle
While it is desirable to delay a SWAP transition
for as long as possible, it is not trivial to find the
right time point to actually do the swap. To see
this, consider the dependency tree in Figure 1. In a
parse of this tree, the first configuration in which
swapping is possible is when who6 and did1 are the
two top nodes on the stack. In this configuration we
can delay the swap until did has combined with its
subject you by means of a RIGHT-ARC transition,
but if we do not swap in the second configuration
where this is possible, we eventually end up with
the stack [ROOT0,who6, did1, send3, to7]. Here we
cannot attach who to to by means of a LEFT-ARC
transition and get stuck.
In order to define the new oracle, we introduce
an auxiliary concept. Consider a modification of
the oracle ?1 from Figure 2 that cannot predict
SWAP transitions. This oracle will be able to pro-
duce valid transition sequences only for projective
target trees; for non-projective trees, it will fail to
reconstruct all dependency arcs. More specifically,
a parse with this oracle will end up in a configur-
ation in which the set of constructed dependency
arcs forms a set of projective dependency trees, not
necessarily a single such tree. We call the elements
of this set the maximal projective components of
the target tree. To illustrate the notion, we have
drawn boxes around nodes in the same component
in Figures 1.
Based on the concept of maximal projective com-
ponents, we define a new training oracle ?2, which
delays swapping as long as the next node in the
input is in the same maximal projective compon-
ent as the top node on the stack. The definition
of the new oracle ?2 is identical to that of ?1 ex-
cept that the third line is replaced by ?SWAP if
c = ([?|i, j], [k|?], Ac), j <G i, and MPC(j) 6=
MPC(k)?, where MPC(i) is the maximal project-
ive component of node i. As a special case, ?2
predicts SWAP if j <G i and the buffer B is empty.
74
?1(c) =
?
????
????
LEFT-ARCl if c = ([?|i, j], B,Ac), (j, l, i)?A and Ai ? Ac
RIGHT-ARCl if c = ([?|i, j], B,Ac), (i, l, j)?A and Aj ? Ac
SWAP if c = ([?|i, j], B,Ac) and j <G i
SHIFT otherwise
Figure 2: Training oracle ?1 for an arbitrary target tree G = (Vx, A), following the notation of Nivre
(2009), where c = (?,B,Ac) denotes a configuration c with stack ?, input buffer B and arc set Ac. We
write Ai to denote the subset of A that only contains the outgoing arcs of the node i. (Note that Ac is the
arc set in configuration c, while A is the arc set in the target tree G.)
For example, in extracting the transition se-
quence for the target tree in Figure 1, the new oracle
will postpone swapping of did when you is the next
node in the input, but not postpone when the next
node is send. We can show that a parser informed
by the new training oracle can always proceed to
a terminal configuration, and still derive all (even
non-projective) dependency trees.
4 Experiments
We now test the hypothesis that the new training
oracle can improve both the accuracy and the ef-
ficiency of a transition-based dependency parser.
Our experiments are based on the same five data
sets as Nivre (2009). The training sets vary in size
from 28,750 tokens (1,534 sentences) for Slovene
to 1,249,408 tokens (72,703 sentences) for Czech,
while the test sets all consist of about 5,000 tokens.
4.1 Number of Transitions
For each language, we first parsed the training set
with both the old and the new training oracle. This
allowed us to compare the number of SWAP trans-
itions needed to parse all sentences with the two
oracles, shown in Table 1. We see that the reduction
is very substantial, ranging from 55% (for Czech)
to almost 84% (for Arabic). While this difference
does not affect the asymptotic complexity of pars-
ing, it may reduce the number of calls to the classi-
fier, which is where transition-based parsers spend
most of their time.
4.2 Parsing Accuracy
In order to assess whether the reduced number of
SWAP transitions also has a positive effect on pars-
ing accuracy, we trained two parsers for each of
the five languages, one for each oracle. All sys-
tems use SVM classifiers with a polynomial kernel
with features and parameters optimized separately
for each language and training oracle. The train-
ing data for these classifiers consist only of the
sequences derived by the oracles, which means that
the parser has no explicit notion of projective order
or maximal projective components at parsing time.
Table 2 shows the labeled parsing accuracy of the
parsers measured by the overall attachment score
(AS), as well as labeled precision, recall and (bal-
anced) F-score for non-projective dependencies.1
For comparison, we also give results for the two
best performing systems in the original CoNLL-X
shared task, Malt (Nivre et al, 2006) and MST (Mc-
Donald et al, 2006), as well as the combo system
MSTMalt, (Nivre and McDonald, 2008).
Looking first at the overall labeled attachment
score, we see that the new training oracle consist-
ently gives higher accuracy than the old one, with
differences of up to 0.5 percentage points (for Ar-
abic and Slovene), which is substantial given that
the frequency of non-projective dependencies is
only 0.4?1.9%. Because the test sets are so small,
none of the differences is statistically significant
(McNemar?s test, ? = .05), but the consistent im-
provement over all languages nevertheless strongly
suggests that this is a genuine difference.
In relation to the state of the art, we note that
the parsers with online reordering significantly out-
perform Malt and MST on Czech and Slovene,
and MST on Turkish, and have significantly lower
scores than the combo system MSTMalt only for
Arabic and Danish. For Czech, the parser with
the new oracle actually has the highest attachment
score ever reported, although the difference with
respect to MSTMalt is not statistically significant.
Turning to the scores for non-projective depend-
encies, we again see that the new oracle consist-
ently gives higher scores than the old oracle, with
1These metrics are not meaningful for Arabic as the test
set only contains 11 non-projective dependencies.
75
Arabic Czech Danish Slovene Turkish
Old (?1) 1416 57011 8296 2191 2828
New (?2) 229 26208 1497 690 1253
Reduction (%) 83.8 55.0 82.0 68.5 55.7
Table 1: Number of SWAP transitions for the old (?1) and new (?2) training oracle.
Arabic Czech Danish Slovene Turkish
System AS AS P R F AS P R F AS P R F AS P R F
Old (?1) 67.2 82.5 74.7 72.9 73.8 84.2 30.0 30.0 30.0 75.2 33.3 26.4 29.5 64.7 12.5 11.4 11.9
New (?2) 67.5 82.7 79.3 71.0 79.3 84.3 38.2 32.5 35.1 75.7 60.6 27.6 37.9 65.0 14.3 13.2 13.7
Malt 66.7 78.4 76.3 57.9 65.8 84.8 45.8 27.5 34.4 70.3 45.9 20.7 25.1 65.7 16.7 9.2 11.9
MST 66.9 80.2 60.5 61.7 61.1 84.8 54.0 62.5 57.9 73.4 33.7 26.4 29.6 63.2 ? 11.8 ?
MSTMalt 68.6 82.3 63.9 69.2 66.1 86.7 63.0 60.0 61.5 75.9 31.6 27.6 29.5 66.3 11.1 9.2 10.1
Table 2: Labeled attachment score (AS) overall; precision (P), recall (R) and balanced F-score (F) for
non-projective dependencies. Old = ?1; New = ?2; Malt = Nivre et al (2006), MST = McDonald et al
(2006), MSTMalt = Nivre and McDonald (2008).
the single exception that the old one has marginally
higher recall for Czech. Moreover, the reordering
parser with the new oracle has higher F-score than
any other system for all languages except Danish.
Especially the result for Czech, with 79.3% preci-
sion and 71.0% recall, is remarkably good, making
the parser almost as accurate for non-projective de-
pendencies as it is for projective dependencies. It
seems likely that the good results for Czech are due
to the fact that Czech has the highest percentage of
non-projective structures in combination with the
(by far) largest training set.
5 Conclusion
We have presented a new training oracle for the
transition system originally presented in Nivre
(2009). This oracle postpones swapping as long as
possible but still fulfills the correctness criterion.
Our experimental results show that the new training
oracle can reduce the necessary number of swaps
by more than 80%, and that parsers trained in this
way achieve higher precision and recall on non-
projective dependency arcs as well as higher at-
tachment score overall. The results are particularly
good for languages with a high percentage of non-
projective dependencies, with an all-time best over
all metrics for Czech.
An interesting theoretical question is whether
the new oracle defined in this paper is optimal with
respect to minimizing the number of swaps. The an-
swer turns out to be negative, and it is possible to re-
duce the number of swaps even further by general-
izing the notion of maximal projective components
to maximal components that may be non-projective.
However, the characterization of these generalized
maximal components is non-trivial, and is therefore
an important problem for future research.
References
Marco Kuhlmann and Joakim Nivre. 2006. Mildly
non-projective dependency structures. In Proceed-
ings of the COLING/ACL 2006 Main Conference
Poster Sessions, pages 507?514.
Ryan McDonald and Giorgio Satta. 2007. On the
complexity of non-projective data-driven depend-
ency parsing. In Proceedings of IWPT, pages 122?
131.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a
two-stage discriminative parser. In Proceedings of
CoNLL, pages 216?220.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency pars-
ers. In Proceedings of ACL, pages 950?958.
Joakim Nivre, Johan Hall, Jens Nilsson, G?lsen Ery-
ig?it, and Svetoslav Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vector
machines. In Proceedings of CoNLL, pages 221?
225.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34:513?553.
Joakim Nivre. 2009. Non-projective dependency pars-
ing in expected linear time. In Proceedings of ACL-
IJCNLP.
76
Tree-Adjoining Grammars Are Not
Closed Under Strong Lexicalization
Marco Kuhlmann?
Uppsala University
Giorgio Satta??
University of Padua
A lexicalized tree-adjoining grammar is a tree-adjoining grammar where each elementary tree
contains some overt lexical item. Such grammars are being used to give lexical accounts of
syntactic phenomena, where an elementary tree defines the domain of locality of the syntactic
and semantic dependencies of its lexical items. It has been claimed in the literature that for
every tree-adjoining grammar, one can construct a strongly equivalent lexicalized version. We
show that such a procedure does not exist: Tree-adjoining grammars are not closed under strong
lexicalization.
1. Introduction
Many contemporary linguistic theories give lexical accounts of syntactic phenomena,
where complex syntactic structures are analyzed as the combinations of elementary
structures taken from a finite lexicon. In the computational linguistics community, this
trend has been called lexicalization, and has been extensively investigated since the
1990s. From a mathematical perspective, the main question that arises in the context of
lexicalization is whether the restriction of a given class of grammars to lexicalized form
has any impact on the generative or computational properties of the formalism.
As a simple example, consider the class of context-free grammars (CFGs). Recall
that a CFG is in Greibach normal form if the right-hand side of every rule in the gram-
mar starts with a terminal symbol, representing an overt lexical item. Although several
procedures for casting a CFG in Greibach normal form exist, all of them substantially
alter the structure of the parse trees of the source grammar. In technical terms, these
procedures provide a weak lexicalization of the source grammar (because the string
language is preserved) but not a strong lexicalization (because the sets of parse trees
that the two grammars assign to the common string language are not the same). Strong
lexicalization is highly relevant for natural language processing, however, where the
parse tree assigned by a grammar represents the syntactic analysis of interest, and is
used by other modules such as semantic interpretation or translation. In this article, we
investigate the problem of strong lexicalization.
? Department of Linguistics and Philology, Box 635, 75126 Uppsala, Sweden.
E-mail: marco.kuhlmann@lingfil.uu.se.
?? Department of Information Engineering, via Gradenigo 6/A, 35131 Padova, Italy.
E-mail: satta@dei.unipd.it.
Submission received: 16 July 2011; accepted for publication: 10 September 2011.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 3
Two important results about strong lexicalization have been obtained by Schabes
(1990). The first result is that CFGs are not closed under strong lexicalization. (The
author actually shows a stronger result involving a formalism called tree substitution
grammar, as will be discussed in detail in Section 3.) Informally, this means that we
cannot cast a CFG G in a special form in which each rule has an overt lexical item in its
right-hand side, under the restriction that the new grammar generates exactly the same
set of parse trees as G. As a special case, this entails that no procedure can cast a CFG
in Greibach normal form, under the additional condition that the generated parse trees
are preserved.
The second result obtained by Schabes concerns the relation between CFGs and the
class of tree-adjoining grammars (TAGs) (Joshi, Levy, and Takahashi 1975; Joshi and
Schabes 1997). A TAG consists of a finite set of elementary trees, which are phrase
structure trees of unbounded depth, and allows for the combination of these trees by
means of two operations called substitution and adjunction (described in more detail in
the next section). A lexicalized TAG is one where each elementary tree contains at least
one overt lexical item called the anchor of the tree; the elementary tree is intended to
encapsulate the syntactic and semantic dependencies of its anchor. Because CFG rules
can be viewed as elementary trees of depth one, and because context-free rewriting can
be simulated by the substitution operation defined for TAGs, we can view any CFG as
a special TAG. Under this view, one can ask whether lexicalized TAGs can provide a
strong lexicalization of CFGs. Schabes? second result is that this is indeed the case. This
means that, given a CFG G, one can always construct a lexicalized TAG generating the
same set of parse trees as G, and consequently the same string language.
Following from this result, there arose the possibility of establishing a third result,
stating that TAGs are closed under strong lexicalization. Schabes (1990) states that this
is the case, and provides an informal argument to justify the claim. The same claim
still appears in two subsequent publications (Joshi and Schabes 1992, 1997), but no
precise proof of it has appeared until now. We speculate that the claim could be due
to the fact that adjunction is more powerful than substitution with respect to weak
generative capacity. It turns out, however, that when it comes to strong generative
capacity, adjunction also shares some of the restrictions of substitution. This observation
leads to the main result of this article: TAGs are not closed under strong lexicalization.
In other words, there are TAGs that lack a strongly equivalent lexicalized version.
In the same line of investigation, Schabes and Waters (1995) introduce a restricted
variant of TAG called tree insertion grammars (TIGs). This formalism severely restricts
the adjunction operation originally defined for TAGs, in such a way that the class of
generated string languages, as well as the class of generated parse trees, are the same
as those of CFGs. Schabes and Waters then conjecture that TIGs are closed under strong
lexicalization. In this article we also disprove their conjecture.
2. Preliminaries
We assume familiarity with the TAG formalism; for a survey, we refer the reader to
Joshi and Schabes (1997). We briefly introduce here the basic terminology and notation
for TAG that we use in this article.
2.1 Basic Definitions
A TAG is a rewriting system that derives trees starting from a finite set of elementary
trees. Elementary trees are trees of finite but arbitrary depth, with internal nodes labeled
618
Kuhlmann and Satta TAGs Are Not Closed Under Strong Lexicalization
with nonterminal symbols and frontier nodes labeled with terminal and nonterminal
symbols. Each elementary tree is either an initial tree or else an auxiliary tree. Initial
trees serve as the starting point for derivations, and may combine with other trees by
means of an operation called substitution. Tree substitution replaces a node labeled
with a nonterminal A in the frontier of some target tree with an initial tree whose
root is labeled with A. The nodes that are the target of the substitution operation are
identified by a down arrow (?). The substitution operation is illustrated in the left half
of Figure 1.
Auxiliary trees are elementary trees in which a special node in the frontier has the
same nonterminal label as the root node. This special node is called the foot node and is
identified by an asterisk (?). Auxiliary trees may combine with other trees by means of
an operation called adjunction. The adjunction operation entails splitting some target
tree at an internal node with label A, and inserting an auxiliary tree whose root (and
foot) node is labeled with A. The adjunction operation is illustrated in the right half of
Figure 1.
A derivation in a TAG can be specified by a derivation tree d; this is a rooted
tree whose nodes are labeled with (instances of) elementary trees, and whose edges
are labeled with (addresses of) nodes at which substitution or adjunction takes place.
More specifically, an edge v ?u v? in d represents the information that the elementary
tree at v? is substituted at or adjoined into node u of the elementary tree at v. When
we combine the elementary trees of our TAG as specified by d, we obtain a (unique)
phrase structure tree called the derived tree associated with d, which we denote
as t(d).
We use the symbol ? as a variable ranging over elementary trees, ? as a variable
ranging over initial trees, and ? as a variable ranging over auxiliary trees. We also
use the symbols u and v as variables ranging over nodes of generic trees (elementary,
derived, or derivation trees). For an elementary tree ?, a derivation tree d is said to have
type ? if the root node of d is labeled with ?. A derivation tree d is called sentential if d is
of some type ?, and the root node of ? is labeled with the start symbol of the grammar,
denoted as S.
A node u in an elementary tree ?may be annotated with an adjunction constraint,
which for purposes here is a label in the set {NA,OA}. The label NA denotes Null
Adjunction, forbidding adjunction at u; the label OA denotes Obligatory Adjunction,
forcing adjunction at u. A derivation tree d is called saturated if, at each node v of d
there is an arc v ?u v?, for some v?, for every node u of the elementary tree at v that
requires substitution or is annotated with an OA constraint.
For a TAGG, we denote by T(G) the set of all the derived trees t such that t = t(d) for
some sentential and saturated derivation tree d obtained in G. Each such derived tree is
Figure 1
Combination operations in TAG.
619
Computational Linguistics Volume 38, Number 3
(uniquely) associated with a string y(t) called the yield of t, obtained by concatenating
all terminal symbols labeling the frontier of t, from left to right. The string language
generated by G is the set
L(G) = { y(t) | t ? T(G) }
A TAG G is said to be finitely ambiguous if, for every string w ? L(G), the subset of
those trees in T(G) that have w as their yield is finite.
An elementary tree ? of G is called useless if ? never occurs in a sentential and sat-
urated derivation tree of G, that is, if no sentential and saturated derivation of G uses ?.
A grammar G is called reduced if none of its elementary trees is useless. Throughout
this article we shall assume that the grammars that we deal with are reduced.
2.2 Lexicalization
In a tree, a node labeled with a terminal symbol is called a lexical node. A TAG is
called lexicalized if each of its elementary trees has at least one lexical node. Observe
that a lexicalized grammar cannot generate the empty string, denoted by ?, because
every derived tree yields at least one lexical element. Similarly, a lexicalized grammar
is always finitely ambiguous, because the length of the generated strings provides an
upper bound on the size of the associated derived trees. Let G and G? be two subclasses
of the class of all TAGs. We say that G? strongly lexicalizes G, if, for every grammar
G ? G that is finitely ambiguous and that satisfies ? 	? L(G), there exists a lexicalized
grammar G? ? G? such that T(G?) = T(G). We also say that G is closed under strong
lexicalization if the class G strongly lexicalizes itself.
Using this terminology, we can now restate the two main results obtained by
Schabes (1990) about strong lexicalization for subclasses of TAGs, already mentioned in
the Introduction. The first result states that the class of CFGs is not closed under strong
lexicalization. Here we view a CFG as a special case of a TAG using only substitution
and elementary trees of depth one. Informally, this means that we cannot cast a CFG
G in a special form in which each rule has an overt lexical item in its right-hand side,
under the restriction that the new grammar generates exactly the same tree set as G. The
second result is that the class of TAGs strongly lexicalizes the class of tree substitution
grammars (TSGs). The latter class is defined as the class of all TAGs that use substitution
as the only tree combination operation, and thus includes all context-free grammars.
This means that, given a TSG or a CFG G, we can always construct a TAG that is
lexicalized and that generates exactly the same tree set as G.
3. Tree Substitution Grammars Are Not Closed Under Strong Lexicalization
Before turning to our main result in Section 4, we find it useful to technically revisit the
related result for TSGs.
Theorem 1
Tree substitution grammars are not closed under strong lexicalization.
To prove this result, Schabes (1990) uses a proof by contradiction: The author considers a
specific TSGG1, reported in Figure 2. It is not difficult to see thatG1 is finitely ambiguous
and that ? 	? L(G1). The author then assumes that G1 can be lexicalized by another TSG,
620
Kuhlmann and Satta TAGs Are Not Closed Under Strong Lexicalization
Figure 2
The counterexample tree substitution grammar G1.
and derives a contradiction. We provide here an alternative, direct proof of Theorem 1.
This alternative proof will be generalized in Section 4 to obtain the main result of this
article.
We use the following notation. For a derived tree t and a terminal symbol a, we
write Nodes(a, t) to denote the set of all nodes in t that are labeled with a. Furthermore,
for a node u of t we write depth(u, t) to denote the length of the unique path from the
root node of t leading to u.
3.1 Intuition
In order to convey the basic idea behind Schabes?s proof and our alternative version
herein, we first consider a specific candidate grammar for the lexicalization of G1. For
example, one might think that the following TSG G?1 lexicalizes G1:
This grammar is obtained from G1 by taking the lexicalized tree ?1, as well as every
elementary tree that can be obtained by substituting ?1 into the non-lexicalized tree ?2.
The grammar G?1 only generates a subset of the trees generated by G1, however. The
following tree, for example, cannot be generated by G?1:
To see this, we reason as follows. Consider a lexical node v in an elementary tree ?
of G?1, and let t be a tree obtained by substituting some elementary tree into ?. Because
substitution takes place at the frontier of ?, depth(v, t) must be the same as depth(v,?).
More generally, the depth of a lexical node in an elementary tree ? is the same in all trees
derived starting from ?. Because the maximal depth of a lexical node in an elementary
621
Computational Linguistics Volume 38, Number 3
tree of G?1 is 2, we deduce that every tree generated by G
?
1 contains a lexical node with
depth at most 2. In contrast, all lexical nodes in the tree t1 have depth 3. Therefore the
tree t1 is not generated by G
?
1.
3.2 Main Part
We now generalize this argument to arbitrary candidate grammars. For this, we are
interested in the following class G1 of all (reduced) TSGs that derive a subset of the trees
derived by G1:
G1 = {G | G is a TSG, T(G) ? T(G1) }
For a grammar G ? G1, we define the d-index of G as the maximum in N ? {?} of the
minimal depths of a-labeled nodes in trees derived by G:
d-index(G) = max
t?T(G)
min
v?Nodes(a,t)
depth(v, t)
Note that, for two grammars G,G? ? G1, T(G) = T(G?) implies that G and G? have the
same d-index. This means that two grammars in G1 with different d-indices cannot gen-
erate the same tree language. Then Theorem 1 directly follows from the two statements
in the next lemma.
Lemma 1
The grammar G1 has infinite d-index. Every lexicalized grammar in G1 has finite
d-index.
Proof
The first statement is easy to verify: Using longer and longer derivations, the mini-
mal depth of an a-labeled node in the corresponding tree can be pushed beyond any
bound.
To prove the second statement, let G be a lexicalized grammar in G1, and let
t ? T(G). The tree t is derived starting from some initial tree; call this tree ?. Because G
is lexicalized, at least one of the a-labeled nodes in Nodes(a, t) is contributed by ?. Let va
be any such node in t, and let ua be the node of ? that corresponds to va. Remember
that the only tree combination operation allowed in a TSG derivation is substitution.
Because substitution can only take place at the frontier of a derived tree, we must
conclude that depth(va, t) = depth(ua,?). There are only finitely many initial trees in G,
therefore depth(ua,?) must be upper bounded by some constant depending only on G,
and the same must hold for depth(va, t). Lastly, because t has been arbitrarily chosen in
T(G), we must conclude that d-index(G) is finite. 
3.3 Lexicalization of Tree Substitution Grammars
What we have just seen is that lexicalized TSGs are unable to derive the tree structures
generated by the grammar G1 in Figure 2. This is essentially because tree substitution
cannot stretch the depth of a lexical node in an elementary tree. In contrast, tree adjunc-
tion allows the insertion of additional structure at internal nodes of elementary trees,
622
Kuhlmann and Satta TAGs Are Not Closed Under Strong Lexicalization
and enables TAGs to provide a strong lexicalization of TSGs. For example, the following
TAG G??1 lexicalizes G1.
Note that this grammar looks almost like G?1, except that adjunction now is allowed at
internal nodes, and substitution nodes have become foot nodes. The following deriva-
tion tree witnesses that the tree t1 can be derived in G
??
1 . We write 0 to denote the root
node of an elementary tree, and 1 to denote its leftmost child.
?6 ?0 ?1 ?0 ?1 ?1 ?1
Schabes (1990) provides a general procedure for constructing a lexicalized TAG for a
given context-free grammar.
4. Tree-Adjoining Grammars Are Not Closed Under Strong Lexicalization
In this section we develop the proof of the main result of this article.
Theorem 2
Tree-adjoining grammars are not closed under strong lexicalization.
4.1 Proof Idea
The basic idea underlying the proof of Theorem 2 is essentially the same as the one used
in the proof of Theorem 1 in Section 3. Some discussion of this issue is in order at this
point. In the previous section, we have seen that adjunction, in contrast to substitution,
allows the insertion of additional structure at internal nodes of elementary trees, and
enables TAGs to provide a strong lexicalization of TSGs. One might now be tempted to
believe that, because the depth-based argument that we used in the proof of Lemma 1
can no longer be applied to TAGs, they might be closed under strong lexicalization.
There is a perspective under which adjunction quite closely resembles substitution,
however. Let us first look at substitution as an operation on the yield of the derived
tree. Under this view, substitution is essentially context-free rewriting: It replaces a non-
terminal symbol in the yield of a derived tree with a new string consisting of terminals
and nonterminals, representing the yield of the tree that is substituted. Under the same
perspective, adjunction is more powerful than tree substitution, as is well known. But
just as substitution can be seen as context-free rewriting on tree yields, adjunction can
be seen as context-free rewriting on the paths of trees: It replaces a nonterminal symbol
in some path of a derived tree with a string representing the spine of the tree that is
adjoined?the unique path from the root node of the tree to the foot node.
This observation gives us the following idea for how to lift the proof of Theorem 1
to TAGs. We will specify a TAG G2 such that the paths of the derived trees of G2 encode
in a string form the derived trees of the counterexample grammar G1. This encoding
is exemplified in Figure 3. Each internal node of a derived tree of G1 is represented in
623
Computational Linguistics Volume 38, Number 3
Figure 3
A derived tree of G1, and the corresponding encoding, drawn from left to right. Every
internal node of the original tree is represented by a pair of matching brackets [S (, )S].
The correspondence is indicated by the numerical subscripts.
the spine of the corresponding derived tree of G2 as a pair of matching brackets. By
our encoding, any TAG generating trees from T(G2) will have to exploit adjunction at
nodes in the spine of its elementary trees, and will therefore be subject to essentially the
same restrictions as the grammar G1 which used substitution at nodes in the yield. This
will allow us to lift our argument from Lemma 1. The only difference is that instead of
working with the actual depth of a lexical node in a tree t ? T(G2), we will now need
to work with the depth of the node in the encoded tree. As will be explained later, this
measure can be recovered as the excess of left parentheses over right parentheses in the
spine above the lexical node.
4.2 Preliminaries
As alreadymentioned, our proof of Theorem 2 follows the same structure as our proof of
Theorem 1. As our counterexample grammar, we use the grammar G2 given in Figure 4;
this grammar generates the encodings of the derived trees of G1 that we discussed
previously. Note that the left parenthesis symbol ?(? and the right parenthesis symbol
?)? are nonterminal symbols. As with the grammar G1 before, it is not difficult to see
that G2 is finitely ambiguous and that ? /? L(G2).
Figure 4
The counterexample TAG G2.
624
Kuhlmann and Satta TAGs Are Not Closed Under Strong Lexicalization
Grammar G2 derives trees that we call right spinal: Each node in such a tree has
at most two children, and the left child of every node with two children is always a
leaf node. The path from the root node of a right spinal tree t to the rightmost leaf
of t is called spine. To save some space, in the following we write right spinal trees
horizontally and from left to right, as already done in Figure 3. Thus the grammar G2
can alternatively be written as follows:
For a node u in a right spinal tree derived by G2, we define
c(u) =
?
?
?
?
?
+1 if u is labeled with (
0 if u is labeled with S or a
?1 if u is labeled with )
We exploit this function to compute the excess of left parentheses over right parentheses
in a sequence of nodes, and write:
excess(?u1, . . . ,un?) =
n
?
i=1
c(ui)
Let t be some right spinal tree in T(G2), and let v be some node in t. Assume that
?u1, . . . ,un = v? is the top?down sequence of all the nodes in the path from t?s root u1
to v. We write excess(v, t) as a shorthand notation for excess(?u1, . . . ,un?). If ?u1, . . . ,un?
is the top?down sequence of all the nodes in the spine of t, we also write excess(t) as a
short hand notation for excess(?u1, . . . ,un?).
It is easy to prove by induction that, for each tree t ? T(G2), the excess of the
sequence of nodes in the spine of t is always zero. Thus, we omit the proof of the
following statement.
Lemma 2
Every derived tree t ? T(G2) is a right spinal tree, and excess(t) = 0.
In order to get a better understanding of the construction used in the following
proofs, it is useful at this point to come back to our discussion of the relation between
that construction and the construction presented in Section 3. We observe that for each
tree t1 generated by G1 there is a tree t2 ? T(G2) such that the sequence of labels in t2?s
spine encodes t1, following the scheme exemplified in Figure 3. Using such encoding,
we can establish a bijection between the a-labeled nodes in the frontier of t1 and the
a-labeled nodes in the frontier of t2. Furthermore, if v1 in t1 and v2 in t2 are two
nodes related by such a correspondence, then it is not difficult to see that depth(v1, t1) =
excess(v2, t2).
4.3 Intuition
Before we give the actual proof of Theorem 2, let us attempt to get some intuition
about why our counterexample grammar G2 cannot be strongly lexicalized by some
625
Computational Linguistics Volume 38, Number 3
other TAG. One might think that the following TAG G?2 is a lexicalized version
of G2:
This grammar is obtained from G2 by taking the lexicalized tree ?3 (repeated here
as ?5), as well as all trees that can be obtained by adjoining ?3 into some non-lexicalized
elementary tree. G?2 does not generate all trees generated by G2, however. The following
tree t2 for example is not generated by G
?
2:
Note that this tree is the encoded version of the counterexample tree t1 from the previous
section (cf. Figure 3).
To see that t2 is not generated by G
?
2, we reason as follows. Consider a lexical node u
in an elementary tree ? of G?2, and let t be a tree obtained by adjoining some elementary
tree into ?. Although this adjunction increases the depth of u, it does not increase its
excess, as it adds a balanced sequence of parentheses into the spine of ?. More generally,
the excess of a lexical node in an elementary ? is constant in all trees derived starting
from ?. From this we conclude that every tree generated by G?2 contains a lexical node
with excess at most 2; this is the maximal excess of a lexical node in an elementary tree
of G?2. In contrast, all lexical nodes in the tree t2 have excess 3. This shows that t2 is not
generated by G?2.
4.4 Main Part
In what follows, we consider the class G2 of (reduced) TAGs that generate subsets of the
trees derived by G2:
G2 = {G | G is a TAG, T(G) ? T(G2) }
For a grammar G ? G2, we define the e-index of G as the maximum in N ? {?} of the
minimal excess of a-labeled nodes in trees derived by G:
e-index(G) = max
t?T(G)
min
v?Nodes(a,t)
excess(v, t)
As we will see, the notion of e-index plays exactly the same role as the notion of d-index
in Section 3.
626
Kuhlmann and Satta TAGs Are Not Closed Under Strong Lexicalization
There is one last obstacle that we need to overcome. For TSGs we noted (in the proof
of Lemma 1) that the minimal depth of lexical nodes in a derived tree t is bounded by
the minimal depth of lexical nodes in the elementary tree ? from which t was derived.
For the TAGs in G2, the situation is not quite as simple, as an adjunction of an auxiliary
tree ? into an elementary tree ? might affect the excess of a lexical node of ?. It turns
out, however, that this potential variation in the excess of a lexical node of ? is bounded
by a grammar-specific constant. This observation is expressed in the following lemma.
It is the correspondent of Lemma 4 in Knuth?s paper on parenthesis languages (Knuth
1967), and is proved in essentially the same way. Recall that a derivation tree d is of
type ?, ? some elementary tree, if d is derived starting from ?.
Lemma 3
Let G ? G2. For each elementary tree ? of G, there exists a number e(?) such that, for
every saturated derivation tree d of type ?, excess(t(d)) = e(?).
Proof
Because ? is not useless, we can find at least one sentential and saturated derivation tree
of G that contains an occurrence of ?. Let d be any such derivation tree, and let v be any
node of d labeled with ?. Let d1 be the subtree of d rooted at v. Observe that t(d1) must
be a spinal tree. We then let e(?) = excess(t(d1)).
If d1 is the only derivation tree of type ? available inG, then we are done. Otherwise,
let d2 	= d1 be some derivation tree of type ? occurring within some other sentential
and saturated derivation tree of G. We can replace d1 with d2 in d at v to obtain a new
sentential and saturated derivation tree d? 	= d. Every derived tree in T(G) must be a
right spinal tree: This follows from the assumption that G ? G2 and from Lemma 2. We
can then write
excess(t(d?)) = excess(t(d))? excess(t(d1))+ excess(t(d2))
Because excess(t(d)) = 0 and excess(t(d?)) = 0 (by Lemma 2), we conclude that
excess(t(d2)) = excess(t(d1)) = e(?)

Using Lemma 3, we can now prove the following result.
Lemma 4
The grammarG2 has infinite e-index. Every lexicalized grammar in G2 has finite e-index.
Proof
As in the case of Lemma 1, the first statement is easy to verify and we omit its proof. To
prove the second statement, let G ? G2. Let ? be the set of all elementary trees of G, and
let s be the maximal number of nodes in an elementary tree in ?. We show that
e-index(G) ? k , where k = s+ s ?max
???
|e(?)|
Note that k is a constant that only depends on G.
627
Computational Linguistics Volume 38, Number 3
Let d be a sentential and saturated derivation tree of G. It has the following shape:
Here ? is some initial tree, m ? 0, each ui is a node of ? at which a tree combination
operation takes place, each ?i is an elementary tree, and each di is a derivation tree of
type ?i that is a subtree of d. According to this derivation tree, the derived tree t(d) is
obtained by substituting or adjoining the derived trees t(di) at the respective nodes ui
of ?.
Because G is lexicalized, at least one a-labeled node on the frontier of t(d) is con-
tributed by ?. Let va be any such node, and let ua be the node of ? that corresponds
to va. The quantity excess(va, t(d)), representing the excess of the path in t(d) from its
root to the node va, can be computed as follows. Let ?u?1, . . . ,u
?
n = ua? be the top?down
sequence of nodes in the path from the root node of ? to ua. For each i with 1 ? i ? n
we define
c?(u?i ) =
{
excess(t(dj)) if u
?
i = uj for some 1 ? j ? m
c(u?i ) otherwise
Because G ? G2 and because t(d) is a right spinal tree (Lemma 2), we can write
excess(va, t(d)) =
n
?
i=1
c?(u?i )
By Lemma 3, we have excess(t(dj)) = e(?j), for each jwith 1 ? j ? m. We can then write
excess(va, t(d)) ? n+
m
?
i=1
|e(?i)| ? s+ s ?max
???
|e(?)| = k
Thus, every derived tree t in T(G) contains at least one node va in its frontier such that
excess(va, t) ? k. Therefore, e-index(G) ? k. 
Two grammars in G2 that have a different e-index cannot generate the same tree lan-
guage, thus we have concluded the proof of Theorem 2.
5. Tree Insertion Grammars Are Not Closed Under Strong Lexicalization
As mentioned earlier Schabes and Waters (1995) introduce a restricted variant of TAG
called TIG. The essential restriction in that formalism is the absence of wrapping trees,
which are trees derived starting from auxiliary trees with overt lexical material on both
sides of the foot node. Schabes and Waters (1995, Section 5.1.4) conjecture that the class
of all TIGs is closed under strong lexicalization.
628
Kuhlmann and Satta TAGs Are Not Closed Under Strong Lexicalization
It is easy to see that the counterexample grammar G2 that we gave in Figure 4
does not derive wrapping trees; this means that G2 actually is a TIG. Using the proof
of Section 4, we then obtain the following result.
Theorem 3
Tree insertion grammars are not closed under strong lexicalization.
In fact, we have even proved the stronger result that the class of TAGs does not lexicalize
the class of TIGs.
6. Conclusion
We have shown that, in contrast to what has been claimed in the literature, TAGs are not
closed under strong lexicalization: The restriction to lexicalized TAGs involves a loss in
strong generative capacity.
In this article we have only considered TAGs with Null Adjunction and Obligatory
Adjunction constraints. A third kind of adjunction constraint that has been used in the
literature is Selective Adjunction, where a set of trees is provided thatmay be adjoined at
some node. It is not difficult to see that the proofs of Lemma 3, Lemma 4, and Theorem 3
still hold if Selective Adjunction constraints are used.
Our result triggers a number of follow-up questions. First, are TAGs closed under
weak lexicalization, defined in Section 1? We know that, in the case of CFGs, this ques-
tion can be answered affirmatively, because Greibach normal form is a special case of
lexicalized form, and for every CFG there is a weakly equivalent grammar in Greibach
normal form. But to our knowledge, no comparable result exists for TAG. Second, if
TAGs cannot strongly lexicalize themselves, what would a grammar formalism look
like that is capable of providing strong lexicalization for TAGs?
Acknowledgments
We are grateful to Aravind Joshi for
discussion on previous versions of this
article and for helping us in shaping
the text in the Introduction of the
current version. We also acknowledge
three anonymous reviewers for their
helpful comments.
References
Joshi, Aravind K., Leon S. Levy, and
Masako Takahashi. 1975. Tree Adjunct
Grammars. Journal of Computer and
System Sciences, 10(2):136?163.
Joshi, Aravind K. and Yves Schabes. 1992.
Tree-adjoining grammars and lexicalized
grammars. In Maurice Nivat and
Andreas Podelski, editors, Tree Automata
and Languages. North-Holland,
Amsterdam, pages 409?431.
Joshi, Aravind K. and Yves Schabes. 1997.
Tree-adjoining grammars. In Grzegorz
Rozenberg and Arto Salomaa, editors,
Handbook of Formal Languages, volume 3.
Springer, Berlin, pages 69?123.
Knuth, Donald E. 1967. A characterization
of parenthesis languages. Information
and Control, 11(3):269?289.
Schabes, Yves. 1990.Mathematical and
Computational Aspects of Lexicalized
Grammars. Ph.D. thesis, University of
Pennsylvania, Philadelphia.
Schabes, Yves and Richard C. Waters.
1995. Tree insertion grammar:
A cubic-time parsable formalism that
lexicalizes context-free grammars
without changing the trees produced.
Computational Linguistics, 21(4):479?513.
629

Mildly Non-Projective Dependency Grammar
Marco Kuhlmann*
Uppsala University
Syntactic representations based on word-to-word dependencies have a long-standing tradition
in descriptive linguistics, and receive considerable interest in many applications. Nevertheless,
dependency syntax has remained something of an island from a formal point of view. Moreover,
most formalisms available for dependency grammar are restricted to projective analyses, and
thus not able to support natural accounts of phenomena such as wh-movement and cross?serial
dependencies. In this article we present a formalism for non-projective dependency grammar
in the framework of linear context-free rewriting systems. A characteristic property of our
formalism is a close correspondence between the non-projectivity of the dependency trees
admitted by a grammar on the one hand, and the parsing complexity of the grammar on the
other. We show that parsing with unrestricted grammars is intractable. We therefore study two
constraints on non-projectivity, block-degree and well-nestedness. Jointly, these two constraints
define a class of ?mildly? non-projective dependency grammars that can be parsed in polynomial
time. An evaluation on five dependency treebanks shows that these grammars have a good
coverage of empirical data.
1. Introduction
Syntactic representations based on word-to-word dependencies have a long-standing
tradition in descriptive linguistics. Since the seminal work of Tesnie`re (1959), they
have become the basis for several linguistic theories, such as Functional Generative
Description (Sgall, Hajic?ova?, and Panevova? 1986), Meaning?Text Theory (Mel?c?uk 1988),
and Word Grammar (Hudson 2007). In recent years they have also been used for a wide
range of practical applications, such as information extraction, machine translation, and
question answering. We ascribe the widespread interest in dependency structures to
their intuitive appeal, their conceptual simplicity, and in particular to the availability of
accurate and efficient dependency parsers for a wide range of languages (Buchholz and
Marsi 2006; Nivre et al 2007).
Although there exist both a considerable practical interest and an extensive lin-
guistic literature, dependency syntax has remained something of an island from a
formal point of view. In particular, there are relatively few results that bridge between
dependency syntax and other traditions, such as phrase structure or categorial syntax.
? Department of Linguistics and Philology, Box 635, 751 26 Uppsala, Sweden.
E-mail: marco.kuhlmann@lingfil.uu.se.
Submission received: 17 December 2009; revised submission received: 3 April 2012; accepted for publication:
24 May 2012.
doi:10.1162/COLI a 00125
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 2
Figure 1
Nested dependencies and cross?serial dependencies.
This makes it hard to gauge the similarities and differences between the paradigms,
and hampers the exchange of linguistic resources and computational methods. An
overarching goal of this article is to bring dependency grammar closer to the mainland
of formal study.
One of the few bridging results for dependency grammar is thanks to Gaifman
(1965), who studied a formalism that we will refer to as Hays?Gaifman grammar, and
proved it to be weakly equivalent to context-free phrase structure grammar. Although
this result is of fundamental importance from a theoretical point of view, its practical
usefulness is limited. In particular, Hays?Gaifman grammar is restricted to projective
dependency structures, which is similar to the familiar restriction to contiguous con-
stituents. Yet, non-projective dependencies naturally arise in the analysis of natural
language. One classic example of this is the phenomenon of cross?serial dependencies
in Dutch. In this language, the nominal arguments of verbs that also select an infinitival
complement occur in the same order as the verbs themselves:
(i) dat Jan1 Piet2 Marie3 zag1 helpen2 lezen3 (Dutch)
that Jan Piet Marie saw help read
?that Jan saw Piet help Marie read?
In German, the order of the nominal arguments instead inverts the verb order:
(ii) dass Jan1 Piet2 Marie3 lesen3 helfen2 sah1 (German)
that Jan Piet Marie read help saw
Figure 1 shows dependency trees for the two examples.1 The German linearization
gives rise to a projective structure, where the verb?argument dependencies are nested
within each other, whereas the Dutch linearization induces a non-projective structure
with crossing edges. To account for such structures we need to turn to formalisms more
expressive than Hays?Gaifman grammars.
In this article we present a formalism for non-projective dependency grammar
based on linear context-free rewriting systems (LCFRSs) (Vijay-Shanker, Weir, and Joshi
1987; Weir 1988). This framework was introduced to facilitate the comparison of various
1 We draw the nodes of a dependency tree as circles, and the edges as arrows pointing towards the
dependent (away from the root node). Following Hays (1964), we use dotted lines to help us keep
track of the positions of the nodes in the linear order, and to associate nodes with lexical items.
356
Kuhlmann Mildly Non-Projective Dependency Grammar
grammar formalisms, including standard context-free grammar, tree-adjoining gram-
mar (Joshi and Schabes 1997), and combinatory categorial grammar (Steedman and
Baldridge 2011). It also comprises, among others, multiple context-free grammars (Seki
et al 1991), minimalist grammars (Michaelis 1998), and simple range concatenation
grammars (Boullier 2004).
The article is structured as follows. In Section 2 we provide the technical back-
ground to our work; in particular, we introduce our terminology and notation for linear
context-free rewriting systems. An LCFRS generates a set of terms (formal expressions)
which are interpreted as derivation trees of objects from some domain. Each term also
has a secondary interpretation under which it denotes a tuple of strings, representing
the string yield of the derived object. In Section 3 we introduce the central notion of a
lexicalized linear context-free rewriting system, which is an LCFRS in which each rule
of the grammar is associated with an overt lexical item, representing a syntactic head
(cf. Schabes, Abeille?, and Joshi 1988 and Schabes 1990). We show that this property gives
rise to an additional interpretation under which each term denotes a dependency tree
on its yield. With this interpretation, lexicalized LCFRSs can be used as dependency
grammars.
In Section 4 we show how to acquire lexicalized LCFRSs from dependency tree-
banks. This works in much the same way as the extraction of context-free grammars
from phrase structure treebanks (cf. Charniak 1996), except that the derivation trees of
dependency trees are not immediately accessible in the treebank. We therefore present
an efficient algorithm for computing a canonical derivation tree for an input depen-
dency tree; from this derivation tree, the rules of the grammar can be extracted in a
straightforward way. The algorithm was originally published by Kuhlmann and Satta
(2009). It produces a restricted type of lexicalized LCFRS that we call ?canonical.? In
Section 5 we provide a declarative characterization of this class of grammars, and show
that every lexicalized LCFRS is (strongly) equivalent to a canonical one, in the sense that
it induces the same set of dependency trees.
In Section 6 we present a simple parsing algorithm for LCFRSs. Although the
runtime of this algorithm is polynomial in the length of the sentence, the degree of
the polynomial depends on two grammar-specific measures called fan-out and rank.
We show that even in the restricted case of canonical grammars, parsing is an NP-
hard problem. It is important therefore to keep the fan-out and the rank of a grammar
as low as possible, and much of the recent work on LCFRSs has been devoted to
the development of techniques that optimize parsing complexity in various scenarios
Go?mez-Rodr??guez and Satta 2009; Go?mez-Rodr??guez et al 2009; Kuhlmann and Satta
2009; Gildea 2010; Go?mez-Rodr??guez, Kuhlmann, and Satta 2010; Sagot and Satta 2010;
and Crescenzi et al 2011).
In this article we explore the impact of non-projectivity on parsing complexity. In
Section 7 we present the structural correspondent of the fan-out of a lexicalized LCFRS,
a measure called block-degree (or gap-degree) (Holan et al 1998). Although there is
no theoretical upper bound on the block-degree of the dependency trees needed for
linguistic analysis, we provide evidence from several dependency treebanks showing
that, from a practical point of view, this upper bound can be put at a value of as low as 2.
In Section 8 we study a second constraint on non-projectivity called well-nestedness
(Bodirsky, Kuhlmann, and Mo?hl 2005), and show that its presence facilitates tractable
parsing. This comes at the cost of a small loss in coverage on treebank data. Bounded
block-degree and well-nestedness jointly define a class of ?mildly? non-projective
dependency grammars that can be parsed in polynomial time.
Section 9 summarizes our main contributions and concludes the article.
357
Computational Linguistics Volume 39, Number 2
2. Technical Background
We assume basic familiarity with linear context-free rewriting systems (see, e.g., Vijay-
Shanker, Weir, and Joshi 1987 and Weir 1988) and only review the terminology and
notation that we use in this article.
A linear context-free rewriting system (LCFRS) is a structure G = (N,?, P, S)
where N is a set of nonterminals, ? is a set of function symbols, P is a finite set of
production rules, and S ? N is a distinguished start symbol. Rules take the form
A0 ? f (A1, . . . , Am) (1)
where f is a function symbol and the Ai are nonterminals. Rules are used for rewriting
in the same way as in a context-free grammar, with the function symbols acting as
terminals. The outcome of the rewriting process is a set T(G) of terms, tree-formed
expressions built from function symbols. Each term is then associated with a string
yield, more specifically a tuple of strings. For this, every function symbol f comes with
a yield function that specifies how to compute the yield of a term f (t1, . . . , tm) from the
yields of its subterms ti. Yield functions are defined by equations
f (?x1,1, . . . , x1,k1?, . . . , ?xm,1, . . . , xm,km?) = ??1, . . . ,?k0? (2)
where the tuple on the right-hand side consists of strings over the variables on the
left-hand side and some given alphabet of yield symbols, and contains exactly one
occurrence of each variable. For a yield function f defined by an equation of this form,
we say that f is of type k1 ? ? ? km ? k0, denoted by f : k1 ? ? ? km ? k0. To guarantee that
the string yield of a term is well-defined, each nonterminal A is associated with a
fan-out ?(A) ? 1, and it is required that for every rule (1),
f : ?(A1) ? ? ??(Am) ? ?(A0)
In Equation (2), the values m and k0 are called the rank and the fan-out of f , respectively.
The rank and the fan-out of an LCFRS are the maximal rank and fan-out of its yield
functions.
Example 1
Figure 2 shows an example of an LCFRS for the language { ?anbncndn? | n ? 0 }.
Equation (2) is uniquely determined by the tuple on the right-hand side of the
equation. We call this tuple the template of the yield function f , and use it as the
canonical function symbol for f . This gives rise to a compact notation for LCFRSs,
Figure 2
An LCFRS that generates the yield language { ?anbncndn? | n ? 0 }.
358
Kuhlmann Mildly Non-Projective Dependency Grammar
illustrated in the right column of Figure 2. In this notation, to save some subscripts,
we use the following shorthands for variables: x and x1 for x1,1; x2 for x1,2; x3 for x1,3;
y and y1 for x2,1; y2 for x2,2; y3 for x2,3.
3. Lexicalized LCFRSs as Dependency Grammars
Recall the following examples for verb?argument dependencies in German and Dutch
from Section 1:
(iii) dass Jan1 Piet2 Marie3 lesen3 helfen2 sah1 (German)
that Jan Piet Marie read help saw
(iv) dat Jan1 Piet2 Marie3 zag1 helpen2 lezen3 (Dutch)
that Jan Piet Marie saw help read
?that Jan saw Piet help Marie read?
Figure 3 shows the production rules of two linear context-free rewriting systems (one for
German, one for Dutch) that generate these examples. The grammars are lexicalized in
the sense that each of their yield functions is associated with a lexical item, such as sah or
zag (cf. Schabes, Abeille?, and Joshi 1988 and Schabes 1990). Productions with lexicalized
yield functions can be read as dependency rules. For example, the rules
V ? ?x y sah?(N, V) (German) V ? ?x y1 zag y2?(N, V) (Dutch)
can be read as stating that the verb to see requires two dependents, one noun (N) and
one verb (V). Based on this reading, every term generated by a lexicalized LCFRS
does not only yield a tuple of strings, but also induces a dependency tree on these
strings: Each parent?child relation in the term represents a dependency between the
associated lexical items (cf. Rambow and Joshi 1997). Thus every lexicalized LCFRS can
be reinterpreted as a dependency grammar. To illustrate the idea, Figure 4 shows (the
tree representations of) two terms generated by the grammars G1 and G2, together with
the dependency trees induced by them. Note that these are the same trees that we gave
for (iii) and (iv) in Figure 1.
Our goal for the remainder of this section is to make the notion of induction formally
precise. To this end we will reinterpret the yield functions of lexicalized LCFRSs as
operations on dependency trees.
Figure 3
Lexicalized linear context-free rewriting systems.
359
Computational Linguistics Volume 39, Number 2
Figure 4
Lexicalized linear context-free rewriting systems induce dependency trees.
3.1 Dependency Trees
By a dependency tree, we mean a pair (w, D), where w is a tuple of strings, and D is
a tree-shaped graph whose nodes correspond to the occurrences of symbols in w, and
whose edges represent dependency relations between these occurrences. We identify
occurrences in w by pairs (i, j) of integers, where i indexes the component of w that
contains the occurrence, and j specifies the linear position of the occurrence within
that component. We can then formally define a dependency graph for a tuple of
strings
w = ?a1,1 ? ? ? a1,n1 , . . . , ak,1 ? ? ? ak,nk?
as a directed graph G = (V, E) where
V = { (i, j) | 1 ? i ? k, 1 ? j ? ni } and E ? V ? V
We use u and v as variables for nodes, and denote edges (u, v) as u ? v. A dependency
tree D for w is a dependency graph for w in which there exists a root node r such
that for any node u, there is exactly one directed path from r to u. A dependency
tree is called simple if w consists of a single string w. In this case, we write the de-
pendency tree as (w, D), and identify occurrences by their linear positions j in w, with
1 ? j ? |w|.
Example 2
Figure 5 shows examples of dependency trees. In pictures of such structures we use
dashed boxes to group nodes that correspond to occurrences from the same tuple
360
Kuhlmann Mildly Non-Projective Dependency Grammar
Figure 5
Dependency trees.
component; however, we usually omit the box when there is only one component.
Writing Di as Di = (Vi, Ei) we have:
V1 = {(1, 1)} E1 = {}
V2 = {(1, 1), (1, 2)} E2 = {(1, 1) ? (1, 2)}
V3 = {(1, 1), (2, 1)} E3 = {(1, 1) ? (2, 1)}
V4 = {(1, 1), (3, 1)} E4 = {(1, 1) ? (3, 1)}
We use standard terminology from graph theory for dependency trees and the
relations between their nodes. In particular, for a node u, the set of descendants of u,
which we denote by 
u, is the set of nodes that can be reached from u by following a
directed path consisting of zero or more edges. We write u < v to express that the node u
precedes the node v when reading the yield from left to right. Formally, precedence is
the lexicographical order on occurrences:
(i1, j1) < (i2, j2) if and only if either i1 < i2 or (i1 = i2 and j1 < j2)
3.2 Operations on Dependency Trees
A yield function f is called lexicalized if its template contains exactly one yield symbol,
representing a lexical item; this symbol is then called the anchor of f . With every
lexicalized yield function f we associate an operation f ? on dependency trees as follows.
Let w1, . . . , wm, w be tuples of strings such that
f (w1, . . . , wm) = w
and let Di be a dependency tree for wi. By the definition of yield functions, every
occurrence u in an input tuple wi corresponds to exactly one occurrence in the output
tuple w; we denote this occurrence by u?. Let G be the dependency graph for w that
has an edge u? ? v? whenever there is an edge u ? v in some Di, and no other edges.
Because f is lexicalized, there is exactly one occurrence r in the output tuple w that does
not correspond to any occurrence in some wi; this is the occurrence of the anchor of f .
Let D be the dependency tree for w that is obtained by adding to the graph G all edges
of the form r ? r?i, where ri is the root node of Di. By this construction, the occurrence r
of the anchor becomes the root node of D, and the root nodes of the input dependency
trees Di become its dependents. We then define
f ?((w1, D1), . . . , (wm, Dm)) = (w, D)
361
Computational Linguistics Volume 39, Number 2
Figure 6
Operations on dependency trees.
Example 3
We consider a concrete application of an operation on dependency trees, illustrated in
Figure 6. In this example we have
f = ?x1 b, y x2? w1 = ?a, e? w2 = ?c d? w = f (w1, w2) = ?a b, c d e?
and the dependency trees D1, D2 are defined as
D1 = ({(1, 1), (2, 1)}, {(1, 1) ? (2, 1)}) D2 = ({(1, 1), (1, 2)}, {(1, 1) ? (1, 2)})
We show that f ?((w1, D1), (w2, D2)) = (w, D), where D = (V, E) with
V = {(1, 1), (1, 2), (2, 1), (2, 2), (2, 3)}
E = {(1, 1) ? (2, 3), (1, 2) ? (1, 1), (1, 2) ? (2, 1), (2, 1) ? (2, 2)}
The correspondences between the occurrences u in the input tuples and the occur-
rences u? in the output tuple are as follows:
for w1: (1, 1) = (1, 1) , (2, 1) = (2, 3) for w2: (1, 1) = (2, 1) , (1, 2) = (2, 2)
By copying the edges from the input dependency trees, we obtain the intermediate
dependency graph G = (V, E?) for w, where
E? = {(1, 1) ? (2, 3), (2, 1) ? (2, 2)}
The occurrence r of the anchor b of f in w is (1, 2); the nodes of G that correspond to
the root nodes of D1 and D2 are r?1 = (1, 1) and r?2 = (2, 1). The dependency tree D is
obtained by adding the edges r ? r?1 and r ? r?2 to G.
4. Extraction of Dependency Grammars
We now show how to extract lexicalized linear context-free rewriting systems from
dependency treebanks. To this end, we adapt the standard technique for extracting
context-free grammars from phrase structure treebanks (Charniak 1996).
Our technique was originally published by Kuhlmann and Satta (2009). In recent
work, Maier and Lichte (2011) have shown how to unify it with a similar technique
for the extraction of range concatenation grammars from discontinuous constituent
structures, due to Maier and S?gaard (2008). To simplify our presentation we restrict
our attention to treebanks containing simple dependency trees.
362
Kuhlmann Mildly Non-Projective Dependency Grammar
Figure 7
A dependency tree and one of its construction trees.
To extract a lexicalized LCFRS from a dependency treebank we proceed as follows.
First, for each dependency tree (w, D) in the treebank, we compute a construction tree,
a term t over yield functions that induces (w, D). Then we collect a set of production
rules, one rule for each node of the construction trees. As an example, consider Fig-
ure 7, which shows a dependency tree with one of its construction trees. (The analysis
is taken from Ku?bler, McDonald, and Nivre [2009].) From this construction tree we
extract the following rules. The nonterminals (in bold) represent linear positions of
nodes.
1 ? ?A? 5 ? ?on x?(7)
2 ? ?x hearing, y?(1, 5) 6 ? ?the?
3 ? ?x1 is y1 x2 y2?(2, 4) 7 ? ?x issue?(6)
4 ? ?scheduled, x?(8) 8 ? ?today?
Rules like these can serve as the starting point for practical systems for data-driven,
non-projective dependency parsing (Maier and Kallmeyer 2010).
Because the extraction of rules from construction trees is straightforward, the prob-
lem that we focus on in this section is how to obtain these trees in the first place. Our
procedure for computing construction trees is based on the concept of ?blocks.?
4.1 Blocks
Let D be a dependency tree. A segment of D is a contiguous, non-empty sequence
of nodes of D, all of which belong to the same component of the string yield. Thus
a segment contains its endpoints, as well as all nodes between the endpoints in the
precedence order. For a node u of D, a block of u is a longest segment consisting of
descendants of u. This means that the left endpoint of a block of u either is the first node
in its component, or is preceded by a node that is not a descendant of u. A symmetric
property holds for the right endpoint.
Example 4
Consider the node 2 of the dependency tree in Figure 7. The descendants of 2 fall into
two blocks, marked by the dashed boxes: 1 2 and 5 6 7.
363
Computational Linguistics Volume 39, Number 2
We use u and v as variables for blocks. Extending the precedence order on nodes,
we say that a block u precedes a block v, denoted by u < v, if the right endpoint of u
precedes the left endpoint of v.
4.2 Computing Canonical Construction Trees
To obtain a canonical construction tree t for a dependency tree (w, D) we label each
node u of D with a yield function f as follows. Let w be the tuple consisting of the blocks
of u, in the order of their precedence, and let w1, . . . , wm be the corresponding tuples for
the children of u. We may view blocks as strings of nodes. Taking this view, we compute
the (unique) yield function g with the property that
g(w1, . . . , wm) = w
The anchor of g is the node u, the rank of g corresponds to the number of children
of u, the variables in the template of g represent the blocks of these children, and the
components of the template represent the blocks of u. To obtain f , we take the template
of g and replace the occurrence of u with the corresponding lexical item.
Example 5
Node 2 of the dependency tree shown in Figure 7 has two children, 1 and 5. We have
w = ?1 2, 5 6 7? w1 = ?1? w2 = ?5 6 7? g = ?x 2, y? f = ?x hearing, y?
Note that in order to properly define f we need to assume some order on the
children of u. The function g (and hence the construction tree t) is unique up to the
specific choice of this order. In the following we assume that children are ordered from
left to right based on the position of their leftmost descendants.
4.3 Computing the Blocks of a Dependency Tree
The algorithmically most interesting part of our extraction procedure is the computation
of the yield function g. The template of g is uniquely determined by the left-to-right
sequence of the endpoints of the blocks of u and its children. An efficient algorithm that
can be used to compute these sequences is given in Table 1.
4.3.1 Description. We start at a virtual root node ? (line 1) which serves as the parent
of the real root node. For each node next in the precedence order of D, we follow the
shortest path from the current node current to next. To determine this path, we compute
the lowest common ancestor lca of the two nodes (lines 4?5), using a set of markings
on the nodes. At the beginning of each iteration of the for loop in line 2, all ancestors of
current (including the virtual root node ?) are marked; therefore, we find lca by going
upwards from next to the first node that is marked. To restore the loop invariant, we
then unmark all nodes on the path from current to lca (lines 6?9). Each time we move
down from a node to one of its children (line 12), we record the information that next
is the left endpoint of a block of current. Symmetrically, each time we move up from a
node to its parent (lines 8 and 17), we record the information that next ? 1 is the right
endpoint of a block of current. The while loop in lines 15?18 takes us from the last node
of the dependency tree back to the node ?.
364
Kuhlmann Mildly Non-Projective Dependency Grammar
Table 1
Computing the blocks of a simple dependency tree.
Input: a string w and a simple dependency tree D for w
1: current ? ?; mark current
2: for each node next of D from 1 to |w| do
3: lca ? next; stack ? []
4: while lca is not marked do loop 1
5: push lca to stack; lca ? the parent of lca
6: while current = lca do loop 2
7:  next ? 1 is the right endpoint of a block of current
8:  move up from current to the parent of current
9: unmark current; current ? the parent of current
10: while stack is not empty do loop 3
11: current ? pop stack; mark current
12:  move down from the parent of current to current
13:  next is the left endpoint of a block of current
14:  arrive at next; at this point, current = next
15: while current = ? do loop 4
16:  |w| is the right endpoint of a block of current
17:  move up from current to the parent of current
18: unmark current; current ? the parent of current
4.3.2 Runtime Analysis. We analyze the runtime of our algorithm. Let m be the total
number of blocks of D. Let us write ni for the total number of iterations of the ith while
loop, and let n = n1 + n2 + n3 + n4. Under the reasonable assumption that every line in
Table 1 can be executed in constant time, the runtime of the algorithm clearly is in O(n).
Because each iteration of loop 2 and loop 4 determines the right endpoint of a block, we
have n2 + n4 = m. Similarly, as each iteration of loop 3 fixes the left endpoint of a block,
we have n3 = m. To determine n1, we note that every node that is pushed to the auxiliary
stack in loop 1 is popped again in loop 3; therefore, n1 = n3 = m. Putting everything
together, we have n = 3m, and we conclude that the runtime of the algorithm is in O(m).
Note that this runtime is asymptotically optimal for the task we are considering.
5. Canonical Grammars
Our extraction technique produces a restricted type of lexicalized linear context-free
rewriting system that we will refer to as ?canonical.? In this section we provide a
declarative characterization of these grammars, and show that every lexicalized LCFRS
is equivalent to a canonical one.
5.1 Definition of Canonical Grammars
We are interested in a syntactic characterization of the yield functions that can occur
in extracted grammars. We give such a characterization in terms of four properties,
stated in the following. We use the following terminology and notation. Consider a
yield function
f : k1 ? ? ? km ? k , f = ??1, . . . ,?k?
For variables x, y we write x <f y to state that x precedes y in the template of f , that
is, in the string ?1 ? ? ??k. Recall that, in the context of our extraction procedure, the
365
Computational Linguistics Volume 39, Number 2
components in the template of f represent the blocks of a node u, and the variables in
the template represent the blocks of the children of u. For a variable xi,j we call i the
argument index and j the component index of the variable.
Property 1
For all 1 ? i1, i2 ? m, if i1 < i2 then xi1,1 <f xi2,1.
This property is an artifact of our decision to order the children of a node from left
to right based on the position of their leftmost descendants. A variable with argument
index i represents a block of the ith child of u in that order. An example of a yield
function that does not have Property 1 is ?x2,1 x1,1?, which defines a kind of ?reverse
concatenation operation.?
Property 2
For all 1 ? i ? m and 1 ? j1, j2 ? ki, if j1 < j2 then xi,j1 <f xi,j2 .
This property reflects that, in our extraction procedure, the variable xi,j represents the
jth block of the ith child of u, where the blocks of a node are ordered from left to right
based on their precedence. An example of a yield function that violates the property
is ?x1,2 x1,1?, which defines a kind of swapping operation. In the literature on LCFRSs
and related formalisms, yield functions with Property 2 have been called monotone
(Michaelis 2001; Kracht 2003), ordered (Villemonte de la Clergerie 2002; Kallmeyer
2010), and non-permuting (Kanazawa 2009).
Property 3
No component ?h is the empty string.
This property, which is similar to ?-freeness as known from context-free grammars,
has been discussed for multiple context-free grammars (Seki et al 1991, Property N3
in Lemma 2.2) and range concatenation grammars (Boullier 1998, Section 5.1). For our
extracted grammars it holds because each component ?h represents a block, and blocks
are always non-empty.
Property 4
No component ?h contains a substring of the form xi,j1xi,j2 .
This property, which does not seem to have been discussed in the literature before, is a
reflection of the facts that variables with the same argument index represent blocks of
the same child node, and that these blocks are longest segments of descendants.
A yield function with Properties 1?4 is called canonical. An LCFRS is canonical if
all of its yield functions are canonical.
Lemma 1
A lexicalized LCFRS is canonical if and only if it can be extracted from a dependency
treebank using the technique presented in Section 4.
Proof
We have already argued for the ?only if? part of the claim. To prove the ?if? part, it
suffices to show that for every canonical, lexicalized yield function f , one can construct
366
Kuhlmann Mildly Non-Projective Dependency Grammar
a dependency tree such that the construction tree extracted for this dependency tree
contains f . This is an easy exercise. 
We conclude by noting that Properties 2?4 are also shared by the treebank grammars
extracted from constituency treebanks using the technique by Maier and S?gaard (2008).
5.2 Equivalence Between General and Canonical Grammars
Two lexicalized LCFRSs are called strongly equivalent if they induce the same set of
dependency trees. We show the following equivalence result:
Lemma 2
For every lexicalized LCFRS G one can construct a strongly equivalent lexicalized
LCFRS G? such that G? is canonical.
Proof
Our proof of this lemma uses two normal-form results about multiple context-free
grammars: Michaelis (2001, Section 2.4) provides a construction that transforms a mul-
tiple context-free grammar into a weakly equivalent multiple context-free grammar in
which all rules satisfy Property 2, and Seki et al (1991, Lemma 2.2) present a corre-
sponding construction for Property 3. Whereas both constructions are only quoted to
preserve weak equivalence, we can verify that, in the special case where the input
grammar is a lexicalized LCFRS, they also preserve the set of induced dependency trees.
To complete the proof of Lemma 2, we show that every lexicalized LCFRS can be cast
into normal forms that satisfy Property 1 and Property 4. It is not hard then to combine
the four constructions into a single one that simultaneously establishes all properties of
canonical yield functions. 
Lemma 3
For every lexicalized LCFRS G one can construct a strongly equivalent lexicalized
LCFRS G? such that G? only contains yield functions which satisfy Property 1.
Proof
The proof is very simple. Intuitively, Property 1 enforces a canonical naming of the
arguments of yield functions. To establish it, we determine, for every yield function f ,
a permutation ? that renames the argument indices of the variables occurring in the
template of f in such a way that the template meets Property 1. This renaming gives rise
to a modified yield function f?. We then replace every rule A ? f (A1, . . . , Am) with the
modified rule A ? f?(A?(1), . . . , A?(m) ). 
Lemma 4
For every lexicalized LCFRS G one can construct a strongly equivalent lexicalized
LCFRS G? such that G? only contains yield functions which satisfy Property 4.
Proof
The idea behind our construction of the grammar G? is perhaps best illustrated by an
example. Imagine that the grammar G generates the term t shown in Figure 8a. The yield
function f1 = ?x1 c x2 x3? at the root node of that term violates Property 4, as its template
contains the offending substring x2 x3. We set up G? in such a way that instead of t it
generates the term t? shown in Figure 8b in which f1 is replaced with the yield function
367
Computational Linguistics Volume 39, Number 2
Figure 8
The transformation implemented by the construction of the grammar G? in Lemma 4.
f ?1 = ?x1 c x2?. To obtain f ?1 from f1 we reduce the offending substring x2 x3 to the single
variable x2. In order to ensure that t and t? induce the same dependency tree (shown in
Figure 8c), we then adapt the function f2 = ?x1 b, y, x2? at the first child of the root node:
Dual to the reduction, we replace the two-component sequence y, x2 in the template of f2
with the single component y x2; in this way we get f ?2 = ?x1 b, y x2?.
Because adaptation operations may introduce new offending substrings, we need a
recursive algorithm to compute the rules of the grammar G?. Such an algorithm is given
in Table 2. For every rule A ? f (A1, . . . , Am) of G we construct new rules
(A, g) ? f ?((A1, g1), . . . , (Am, gm))
where g and the gi are yield functions encoding adaptation operations. As an example,
the adaptation of the function f2 in the term t may be encoded into the adaptor function
?x1, x2 x3?. The function f ?2 can then be written as the composition of this function and f2:
f ?2 = ?x1, x2 x3? ? f2 = ?x1, x2 x3?(?x1 b, y, x2?) = ?x1 b, y x2?
The yield function f ? and the adaptor functions gi are computed based on the template
of the g-adapted yield function f , that is, the composed function g ? f . In Table 2 we write
this as f ? = reduce(f, g) and gi = adapt(f, g, i), respectively. Let us denote the template of
the adapted function g ? f by ?. An i-block of ? is a maximal, non-empty substring of
some component of ? that consists of variables with argument index i. To compute the
template of gi we read the i-blocks of ? from left to right and rename the variables by
changing their argument indices from i to 1. To compute the template of f ? we take the
Table 2
Computing the production rules of an LCFRS in which all yield functions satisfy Property 4.
Input: a linear context-free rewriting system G = (N,?, P, S)
1: P? ? ?; agenda ? {(S, ?x?)}; chart ? ?
2: while agenda is not empty
3: remove some (A, g) from agenda
4: if (A, g) /? chart then
5: add (A, g) to chart
6: for each rule A ? f (A1, . . . , Am) ? P do
7: f ? reduce(f, g); gi ? adapt(f, g, i) (1 ? i ? m)
8: for each i from 1 to m do
9: add (Ai, gi) to agenda
10: add (A, g) ? f ?((A1, g1), . . . , (Am, gm)) to P?
368
Kuhlmann Mildly Non-Projective Dependency Grammar
template ? and replace the jth i-block with the variable xi,j, for all argument indices i
and component indices j.
Our algorithm is controlled by an agenda and a chart, both containing pairs of
the form (A, g), where A is a nonterminal of G and g is an adaptor function. These
pairs also constitute the nonterminals of the new grammar G?. The fan-out of a non-
terminal is the fan-out of g. The agenda is initialized with the pair (S, ?x?) where ?x?
is the identity function; this pair also represents the start symbol of G?. To see that
the algorithm terminates, one may observe that the fan-out of every nonterminal (A, g)
added to the agenda is upper-bounded by the fan-out of A. Hence, there are only finitely
many pairs (A, g) that may occur in the chart, and a finite number of iterations of the
while-loop. 
We conclude by noting that when constructing a canonical grammar, one needs to
be careful about the order in which the individual constructions (for Properties 1?4) are
combined. One order that works is
Property 3 < Property 4 < Property 2 < Property 1
6. Parsing and Recognition
Lexicalized linear context-free rewriting systems are able to account for arbitrarily non-
projective dependency trees. This expressiveness comes with a price: In this section we
show that parsing with lexicalized LCFRSs is intractable, unless we are willing to restrict
the class of grammars.
6.1 Parsing Algorithm
To ground our discussion of parsing complexity, we present a simple bottom?up parsing
algorithm for LCFRSs, specified as a grammatical deduction system (Shieber, Schabes,
and Pereira 1995). Several similar algorithms have been described in the literature (Seki
et al 1991; Bertsch and Nederhof 2001; Kallmeyer 2010). We assume that we are given a
grammar G = (N,?, P, S) and a string w = a1 ? ? ? an ? V? to be parsed.
Item form. The items of the deduction system take the form
[A, l1, r1, . . . , lk, rk]
where A ? N with ?(A) = k, and the remaining components are indices identifying the
left and right endpoints of pairwise non-overlapping substrings of w. More formally,
0 ? lh ? rh ? n, and for all h, h? with h = h?, either rh ? lh? or rh? ? lh. The intended
interpretation of an item of this form is that A derives a term t ? T(G) that yields the
specified substrings of w, that is,
A ??G t and yield(t) = ?al1+1 ? ? ? ar1 , . . . , alk+1 ? ? ? ark?
Goal item. The goal item is [S, 0, n]. By this item, there exists a term that can be derived
from the start symbol S and yields the full string ?w?.
369
Computational Linguistics Volume 39, Number 2
Inference rules. The inference rules of the deduction system are defined based on the
rules in P. Each production rule
A ? f (A1, . . . , Am) with f : k1 ? ? ? km ? k , f = ??1, . . . ,?k?
is converted into a set of inference rules of the form
[
A1, l1,1, r1,1, . . . , l1,k1 , r1,k1
]
? ? ?
[
Am, lm,1, rm,1, . . . , lm,km , rm,km
]
[
A, l1, r1, . . . , lk, rk
]
(3)
Each such rule is subject to the following constraints. Let 1 ? h ? k, v ? V?, 1 ? i ? m,
and 1 ? j ? ki. We write ?(l, v) = r to assert that r = l + |v| and that v is the substring
of w between indices l and r.
If ?h = v then ?(lh, v) = rh (c1)
If v xi,j is a prefix of ?h then ?(lh, v) = li,j (c2)
If xi,j v is a suffix of ?h then ?(ri,j, v) = rh (c3)
If xi,j v xi?,j? is an infix of ?h then ?(ri,j, v) = li?,j? (c4)
These constraints ensure that the substrings corresponding to the premises of the
inference rule can be combined into the substrings corresponding to the conclusion by
means of the yield function f .
Based on the deduction system, a tabular parser for LCFRSs can be implemented
using standard dynamic programming techniques. This parser will compute a packed
representation of the set of all derivation trees that the grammar G assigns to the
string w. Such a packed representation is often called a shared forest (Lang 1994). In
combination with appropriate semirings, the shared forest is useful for many tasks in
syntactic analysis and machine learning (Goodman 1999; Li and Eisner 2009).
6.2 Parsing Complexity
We are interested in an upper bound on the runtime of the tabular parser that we have
just presented. We can see that the parser runs in time O(|G||w|c), where |G| denotes
the size of some suitable representation of the grammar G, and c denotes the maximal
number of instantiations of an inference rule (cf. McAllester 2002). Let us write c( f ) for
the specialization of c to inference rules for productions with yield function f . We refer
to this value as the parsing complexity of f (cf. Gildea 2010). Then to show an upper
bound on c it suffices to show an upper bound on the parsing complexities of the yield
functions that the parser has to handle. An obvious such upper bound is
c( f ) ? 2k +
m
?
i=1
2ki
Here we imagine that we could choose each endpoint in Equation (3) independently of
all the others. By virtue of the constraints, however, some of the endpoints cannot be
chosen freely; in particular, some of the substrings may be adjacent. In general, to show
370
Kuhlmann Mildly Non-Projective Dependency Grammar
an upper bound c(f ) ? b we specify a strategy for choosing b endpoints, and then argue
that, given the constraints, these choices determine the remaining endpoints.
Lemma 5
For a yield function f : k1 ? ? ? km ? k we have
c( f ) ? k +
m
?
i=1
ki
Proof
We adopt the following strategy for choosing endpoints: For 1 ? i ? k, choose the
value of lh. Then, for 1 ? i ? m and 1 ? j ? ki, choose the value of ri,j. It is not hard
to see that these choices suffice to determine all other endpoints. In particular, each left
endpoint li?,j? will be shared either with the left endpoint lh of some component (by
constraint c2), or with some right endpoint ri,j (by constraint c4). 
6.3 Universal Recognition
The runtime of our parsing algorithm for LCFRSs is exponential in both the rank and the
fan-out of the input grammar. One may wonder whether there are parsing algorithms
that can be substantially faster. We now show that the answer to this question is likely
to be negative even if we restrict ourselves to canonical lexicalized LCFRSs. To this end
we study the universal recognition problem for this class of grammars.
The universal recognition problem for a class of linear context-free rewriting
systems is to decide, given a grammar G from the class in question and a string w,
whether G yields ?w?. A straightforward algorithm for solving this problem is to first
compute the shared forest for G and w, and to return ?yes? if and only if the shared
forest is non-empty. Choosing appropriate data structures, the emptiness of shared
forests can be decided in linear time and space with respect to the size of the forest.
Therefore, the computational complexity of universal recognition is upper-bounded by
the complexity of constructing the shared forest. Conversely, parsing cannot be faster
than universal recognition.
In the next three lemmas we prove that the universal recognition problem for
canonical lexicalized LCFRSs is NP-complete unless we restrict ourselves to a class of
grammars where both the fan-out and the rank of the yield functions are bounded by
constants. Lemma 6, which shows that the universal recognition problem of lexicalized
LCFRSs is in NP, distinguishes lexicalized LCFRSs from general LCFRSs, for which the
universal recognition problem is known to be PSPACE-complete (Kaji et al 1992). The
crucial difference between general and lexicalized LCFRSs is the fact that in the latter,
the size of the generated terms is bounded by the length of the input string. Lemma 7
and Lemma 8, which establish two NP-hardness results for lexicalized LCFRSs, are
stronger versions of the corresponding results for general LCFRSs presented by Satta
(1992), and are proved using similar reductions. They show that the hardness results
hold under significant restrictions of the formalism: to lexicalized form and to canonical
yield functions. Note that, whereas in Section 5.2 we have shown that every lexicalized
LCFRS is equivalent to a canonical one, the normal form transformation increases the
size of the original grammar by a factor that is at least exponential in the fan-out.
Lemma 6
The universal recognition problem of lexicalized LCFRSs is in NP.
371
Computational Linguistics Volume 39, Number 2
Proof
Let G be a lexicalized LCFRS, and let w be a string. To test whether G yields ?w?, we
guess a term t ? T(G) and check whether t yields ?w?. Let |t| denote the length of some
string representation of t. Since the yield functions of G are lexicalized, |t| ? |w||G|. Note
that we have
|t| ? |w||G| ? |w|2 + 2|w||G|+ |G|2 = (|w|+ |G|)2
Using a simple tabular algorithm, we can verify in time O(|w||G|) whether a candidate
term t belongs to T(G). It is then straightforward to compute the string yield of t in time
O(|w||G|). Thus we have a nondeterministic polynomial-time decider for the universal
recognition problem. 
For the following two lemmas, recall the decision problem 3SAT, which is known
to be NP-complete. An instance of 3SAT is a Boolean formula ? in conjunctive normal
form where each clause contains exactly three literals, which may be either variables or
negated variables. We write m for the number of distinct variables that occur in ?, and n
for the number of clauses. In the proofs the index i will always range over values from 1
to m, and the index j will range over values from 1 to n.
In order to make the grammars in the following reductions more readable, we use
yield functions with more than one lexical anchor. Our use of these yield functions
is severely restricted, however, and each of our grammars can be transformed into a
proper lexicalized LCFRS without affecting the correctness or polynomial size of the
reductions.
Lemma 7
The universal recognition problem for canonical lexicalized LCFRSs with unbounded
fan-out and rank 1 is NP-hard.
Proof
To prove this claim, we provide a polynomial-time reduction of 3SAT. The basic idea is
to use the derivations of the grammar to guess truth assignments for the variables, and
to use the feature of unbounded fan-out to ensure that the truth assignment satisfies all
clauses.
Let ? be an instance of 3SAT. We construct a canonical lexicalized LCFRS G and a
string w as follows. Let M denote the m ? n matrix with entries Mi,j = (vi, cj), that is,
entries in the same row share the same variable, and entries in the same column share
the same clause. We set up G in such a way that each of its derivations simulates a row-
wise iteration over M. Before visiting a new row, the derivation chooses a truth value
for the corresponding variable, and sticks to that choice until the end of the row. The
string w takes the form
w = w1 $ ? ? ? $ wn where wj = cj,1 ? ? ? cj,m cj,1 ? ? ? cj,m
This string is built up during the iteration over M in a column-wise fashion, where each
column corresponds to one component of a tuple with fan-out n. More specifically, for
each entry (vi, cj), the derivation generates one of two strings, denoted by ?i,j and ??i,j:
?i,j = cj,i ? ? ? cj,m cj,1 ? ? ? cj,i ??i,j = cj,i
372
Kuhlmann Mildly Non-Projective Dependency Grammar
The string ?i,j is generated only if vi can be used to satisfy cj under the hypothesized
truth assignment. By this construction, every successful derivation of G represents a
truth assignment that satisfies ?. Conversely, using a satisfying truth assignment for ?,
we will be able to construct a derivation of G that yields w.
To see how the traversal of the matrix M can be implemented by the grammar G,
consider the grammar fragment in Figure 9. Each of the rules specifies one possible step
of the iteration for the pair (vi, cj) under the truth assignment vi = true; rules with left-
hand side Fi,j (not shown here) specify possible steps under the assignment vi = false. 
Lemma 8
The universal recognition problem for canonical lexicalized LCFRSs with unbounded
rank and fan-out 2 is NP-hard.
Proof
We provide another polynomial-time reduction of 3SAT to a grammar G and a string w,
again based on the matrix M mentioned in the previous proof. Also as in the previous
reduction, we set up the grammar G to simulate a row-wise iteration over M. The major
difference this time is that the entries of M are not visited during one long rank 1
derivation, but during mn rather short fan-out 2 subderivations. The string w is
w = w
,1 ? ? ?w,m $ w,1 ? ? ?w,n
where w
,i = ai,1 ? ? ? ai,n bi,1 ? ? ? bi,n and w,j = c1,j ? ? ? cm,j c1,j ? ? ? cm,j
During the traversal of M, for each entry (vi, cj), we generate a tuple consisting of two
substrings of w. The right component of the tuple consists of one the two strings ?i,j
and ??i,j mentioned previously. As before, the string ?i,j is generated only if vi can be
used to satisfy cj under the hypothesized truth assignment. The left component consists
of one of two strings, denoted by ?i,j and ??i,j:
?i,1 = ai,1 ? ? ? ai,n bi,1 ?i,j = bi,j (1 < j) ??i,n = ai,n bi,1 ? ? ? bi,n ??i,j = ai,j (j < n)
These strings are generated to represent the truth assignments vi = true and vi = false,
respectively. By this construction, each substring w
,i can be derived in exactly one of
two ways, ensuring a consistent truth assignment for all subderivations that are linked
to the same variable vi.
Figure 9
A fragment of the grammar used in the proof of Lemma 7.
373
Computational Linguistics Volume 39, Number 2
The grammar G is defined as follows. There is one rather complex rule to rewrite
the start symbol S; this rule sets up the general topology of w. Let I be the m ? n matrix
with entries Ii,j = (j ? 1)m + i. Define x1 to be the sequence of variables of the form xh,1,
where the argument index i is taken from a row-wise reading of the matrix I; in this
case, the argument indices in x will simply go up from 1 to mn. Now define x2 to be the
sequence of variables of the form xh,2, where h is taken from a column-wise reading of
the matrix I. Then S can be expanded with the rule
S ? ?x1 $x2?(V1,1, . . . , V1,n, . . . , Vm,1, . . . , Vm,n)
Note that there is one nonterminal Vi,j for each variable?clause pair (vi, cj). These non-
terminals can be rewritten using the following rules:
Vi,1 ? ??i,1, x?(Ti,1 ) Vi,j ? ??i,j, x?(Ti,j)
Vi,n ? ???i,n, x?(Fi,n) Vi,j ? ???i,j, x?(Fi,j)
The remaining rules rewrite the nonterminals Ti,j and Fi,j:
Ti,j ? ??i,j? (if vi occurs in cj) Ti,j ? ???i,j?
Fi,j ? ??i,j? (if v?i occurs in cj) Fi,j ? ???i,j?
It is not hard to see that both G and w can be constructed in polynomial time. 
7. Block-Degree
To obtain efficient parsing, we would like to have grammars with as low a fan-out as
possible. Therefore it is interesting to know how low we can go without losing too much
coverage. In lexicalized LCFRSs extracted from dependency treebanks, the fan-out of a
grammar has a structural correspondence in the maximal number of blocks per subtree,
a measure known as ?block-degree.? In this section we formally define block-degree,
and evaluate grammar coverage under different bounds on this measure.
7.1 Definition of Block-Degree
Recall the concept of ?blocks? that was defined in Section 4.2. The block-degree of a
node u of a dependency tree D is the number of distinct blocks of u. The block-degree
of D is the maximal block-degree of its nodes.2
Example 6
Figure 10 shows two non-projective dependency trees. For D1, consider the node 2. The
descendants of 2 fall into two blocks, marked by the dashed boxes. Because this is the
maximal number of blocks per node in D1, the block-degree of D1 is 2. Similarly, we can
verify that the block-degree of the dependency tree D2 is 3.
2 We note that, instead of counting the blocks of each node, one may also count the gaps between these
blocks and define the ?gap-degree? of a dependency tree (Holan et al 1998).
374
Kuhlmann Mildly Non-Projective Dependency Grammar
Figure 10
Block-degree.
A dependency tree is projective if its block-degree is 1. In a projective dependency
tree, each subtree corresponds to a substring of the underlying tuple of strings. In a non-
projective dependency tree, a subtree may span over several, discontinuous substrings.
7.2 Computing the Block-Degrees
Using a straightforward extension of the algorithm in Table 1, the block-degrees of all
nodes of a dependency tree D can be computed in time O(m), where m is the total
number of blocks. To compute the block-degree of D, we simply take the maximum
over the degrees of each node. We can also adapt this procedure to test whether D is
projective, by aborting the computation as soon as we discover that some node has
more than one block. The runtime of this test is linear in the number of nodes of D.
7.3 Block-Degree in Extracted Grammars
In a lexicalized LCFRS extracted from a dependency treebank, there is a one-to-one
correspondence between the blocks of a node u and the components of the template
of the yield function f extracted for u. In particular, the fan-out of f is exactly the
block-degree of u. As a consequence, any bound on the block-degree of the trees in
the treebank translates into a bound on the fan-out of the extracted grammar. This has
consequences for the generative capacity of the grammars: As Seki et al (1991) show,
the class of LCFRSs with fan-out k > 1 can generate string languages that cannot be
generated by the class of LCFRSs with fan-out k ? 1.
It may be worth emphasizing that the one-to-one correspondence between blocks
and tuple components is a consequence of two characteristic properties of extracted
grammars (Properties 3 and 4), and does not hold for non-canonical lexicalized
LCFRSs.
Example 7
The following term induces a two-node dependency tree with block-degree 1, but
contains yield functions with fan-out 2: ?a x1 x2?(?b, ??). Note that the yield functions
in this term violate both Property 3 and Property 4.
7.4 Coverage on Dependency Treebanks
In order to assess the consequences of different bounds on the fan-out, we now evaluate
the block-degree of dependency trees in real-world data. Specifically, we look into five
375
Computational Linguistics Volume 39, Number 2
dependency treebanks used in the 2006 CoNLL shared task on dependency parsing
(Buchholz and Marsi 2006): the Prague Arabic Dependency Treebank (Hajic? et al 2004),
the Prague Dependency Treebank of Czech (Bo?hmova? et al 2003), the Danish Depen-
dency Treebank (Kromann 2003), the Slovene Dependency Treebank (Dz?eroski et al
2006), and the Metu-Sabanc? treebank of Turkish (Oflazer et al 2003). The full data used
in the CoNLL shared task also included treebanks that were produced by conversion
of corpora originally annotated with structures other than dependencies, which is a
potential source of ?noise? that one has to take into account when interpreting any
findings. Here, we consider only genuine dependency treebanks. More specifically, our
statistics concern the training sections of the treebanks that were set off for the task. For
similar results on other data sets, see Kuhlmann and Nivre (2006), Havelka (2007), and
Maier and Lichte (2011).
Our results are given in Table 3. For each treebank, we list the number of rules
extracted from that treebank, as well as the number of corresponding dependency trees.
We then list the number of rules that we lose if we restrict ourselves to rules with fan-
out = 1, or rules with fan-out ? 2, as well as the number of dependency trees that we
lose because their construction trees contain at least one such rule. We count rule tokens,
meaning that two otherwise identical rules are counted twice if they were extracted
from different trees, or from different nodes in the same tree.
By putting the bound at fan-out 1, we lose between 0.74% (Arabic) and 1.75%
(Slovene) of the rules, and between 11.16% (Arabic) and 23.15% (Czech) of the trees
in the treebanks. This loss is quite substantial. If we instead put the bound at fan-out
? 2, then rule loss is reduced by between 94.16% (Turkish) and 99.76% (Arabic), and
tree loss is reduced by between 94.31% (Turkish) and 99.39% (Arabic). This outcome
is surprising. For example, Holan et al (1998) argue that it is impossible to give a
theoretical upper bound for the block-degree of reasonable dependency analyses of
Czech. Here we find that, if we are ready to accept a loss of as little as 0.02% of the
rules extracted from the Prague Dependency Treebank, and up to 0.5% of the trees, then
such an upper bound can be set at a block-degree as low as 2.
8. Well-Nestedness
The parsing of LCFRSs is exponential both in the fan-out and in the rank of the
grammars. In this section we study ?well-nestedness,? another restriction on the non-
projectivity of dependency trees, and show how enforcing this constraint allows us to
restrict our attention to the class of LCFRSs with rank 2.
Table 3
Loss in coverage under the restriction to yield functions with fan-out = 1 and fan-out ? 2.
fan-out = 1 fan-out ? 2
rules trees rules trees rules trees
Arabic 5,839 1,460 411 163 1 1
Czech 1,322,111 72,703 22,283 16,831 328 312
Danish 99,576 5,190 1,229 811 11 9
Slovene 30,284 1,534 530 340 14 11
Turkish 62,507 4,997 924 580 54 33
376
Kuhlmann Mildly Non-Projective Dependency Grammar
8.1 Definition of Well-Nestedness
Let D be a dependency tree, and let u and v be nodes of D. The descendants of u
and v overlap, denoted by 
u  
v, if there exist nodes ul, ur ? 
u and vl, vr ? 
v
such that
ul < vl < ur < vr or vl < ul < vr < ur
A dependency tree D is called well-nested if for all pairs of nodes u, v of D

u  
v implies that 
u ? 
v = ?
In other words, 
u and 
v may overlap only if u is an ancestor of v, or v is an ancestor
of u. If this implication does not hold, then D is called ill-nested.
Example 8
Figure 11 shows three non-projective dependency trees. Both D1 and D2 are well-nested:
D1 does not contain any overlapping sets of descendants at all. In D2, although 
1
and 
2 overlap, it is also the case that 
1 ? 
2. In contrast, D3 is ill-nested, as

2  
3 but 
2 ? 
3 = ?
The following lemma characterizes well-nestedness in terms of blocks.
Lemma 9
A dependency tree is ill-nested if and only if it contains two sibling nodes u, v and blocks
u1,u2 of u and v1,v2 of v such that
u1 < v1 < u2 < v2 (4)
Proof
Let D be a dependency tree. Suppose that D contains a configuration of the form (4).
This configuration witnesses that the sets 
u and 
v overlap. Because u, v are siblings,

u ? 
v = ?. Therefore we conclude that D is ill-nested. Conversely now, suppose
that D is ill-nested. In this case, there exist two nodes u and v such that

u  
v and 
u ? 
v = ? (?)
Figure 11
Well-nestedness and ill-nestedness.
377
Computational Linguistics Volume 39, Number 2
Here, we may assume u and v to be siblings: otherwise, we may replace either u or v
with its parent node, and property (?) will continue to hold. Because 
u  
v, there
exist descendants ul, ur ? 
u and vl, vr ? 
v such that
ul < vl < ur < vr or vl < ul < vr < ur
Without loss of generality, assume that we have the first case. The nodes ul and ur belong
to different blocks of u, say u1 and u2; and the nodes vl and vr belong to different blocks
of v, say v1 and v2. Then it is not hard to verify Equation (4). 
Note that projective dependency trees are always well-nested; in these structures,
every node has exactly one block, so configuration (4) is impossible. For every k > 1,
there are both well-nested and ill-nested dependency trees with block-degree k.
8.2 Testing for Well-Nestedness
Based on Lemma 9, testing whether a dependency tree D is well-nested can be done in
time linear in the number of blocks in D using a simple subsequence test as follows. We
run the algorithm given in Table 1, maintaining a stack s[u] for every node u. The first
time we make a down step to u, we push u to the stack for the parent of u; every other
time, we pop the stack for the parent until we either find u as the topmost element, or the
stack becomes empty. In the latter case, we terminate the computation and report that D
is ill-nested; if the computation can be completed without any stack ever becoming
empty, we report that D is well-nested.
To show that the algorithm is sound, suppose that some stack s[p] becomes empty
when making a down step to some child v of p. In this case, the node v must have been
popped from s[p] when making a down step to some other child u of p, and that child
must have already been on the stack before the first down step to v. This witnesses the
existence of a configuration of the form in Equation (4).
8.3 Well-Nestedness in Extracted Grammars
Just like block-degree, well-nestedness can be characterized in terms of yield functions.
Recall the notation x <f y from Section 5.1. A yield function
f : k1 ? ? ? km ? k , f = ??1, . . . ,?k?
is ill-nested if there are argument indices 1 ? i1, i2 ? m with i1 = i2 and component
indices 1 ? j1, j?1 ? ki1 , 1 ? j2, j?2 ? ki2 such that
xi1,j1 <f xi2,j2 <f xi1,j?1 <f xi2,j?2 (5)
Otherwise, we say that f is well-nested. As an immediate consequence of Lemma 9, a
restriction to well-nested dependency trees translates into a restriction to well-nested
yield functions in the extracted grammars. This puts them into the class of what
Kanazawa (2009) calls ?well-nested multiple context-free grammars.?3 These grammars
3 Kanazawa (2009) calls a multiple context-free grammar well-nested if each of its rules is non-deleting,
non-permuting (our Property 2), and well-nested according to (5).
378
Kuhlmann Mildly Non-Projective Dependency Grammar
have a number of interesting properties that set them apart from general LCFRSs; in
particular, they have a standard pumping lemma (Kanazawa 2009). The yield languages
generated by well-nested multiple context-free grammars form a proper subhierarchy
within the languages generated by general LCFRSs (Kanazawa and Salvati 2010). Per-
haps the most prominent subclass of well-nested LCFRSs is the class of tree-adjoining
grammars (Joshi and Schabes 1997).
Similar to the situation with block-degree, the correspondence between structural
well-nestedness and syntactic well-nestedness is tight only for canonical grammars.
For non-canonical grammars, syntactic well-nestedness alone does not imply structural
well-nestedness, nor the other way around.
8.4 Coverage on Dependency Treebanks
To estimate the coverage of well-nested grammars, we extend the evaluation presented
in Section 7.4. Table 4 shows how many rules and trees in the five dependency treebanks
we lose if we restrict ourselves to well-nested yield functions with fan-out ? 2. The
losses reported in Table 3 are repeated here for comparison. Although the coverage
of well-nested rules is significantly smaller than the coverage of rules without this
requirement, rule loss is still reduced by between 92.65% (Turkish) and 99.51% (Arabic)
when compared to the fan-out = 1 baseline.
8.5 Binarization of Well-Nested Grammars
Our main interest in well-nestedness comes from the following:
Lemma 10
The universal recognition problem for well-nested lexicalized LCFRS with fan-out k and
unbounded rank can be decided in time
O
(
|G| ? |w|2k+2
)
To prove this lemma, we will provide an algorithm for the binarization of well-
nested lexicalized LCFRSs. In the context of LCFRSs, a binarization is a procedure for
transforming a grammar into an equivalent one with rank at most 2. Binarization,
either explicit at the level or the grammar or implicit at the level of some parsing
algorithm, is essential for achieving efficient recognition algorithms, in particular the
usual cubic-time algorithms for context-free grammars. Note that our binarization only
Table 4
Loss in coverage under the restriction to yield functions with fan-out = 1, fan-out ? 2,
and to well-nested yield functions with fan-out ? 2 (last column).
fan-out = 1 fan-out ? 2 + well-nested
rules trees rules trees rules trees rules trees
Arabic 5,839 1,460 411 163 1 1 2 2
Czech 1,322,111 72,703 22,283 16,831 328 312 407 382
Danish 99,576 5,190 1,229 811 11 9 17 15
Slovene 30,284 1,534 530 340 14 11 17 13
Turkish 62,507 4,997 924 580 54 33 68 43
379
Computational Linguistics Volume 39, Number 2
preserves weak equivalence; in effect, it reduces the universal recognition problem for
well-nested lexicalized LCFRSs to the corresponding problem for well-nested LCFRSs
with rank 2. Many interesting semiring computations on the original grammar can be
simulated on the binarized grammar, however. A direct parsing algorithm for well-
nested dependency trees has been presented by Go?mez-Rodr??guez, Carroll, and Weir
(2011).
The binarization that we present here is a special case of the binarization proposed
by Go?mez-Rodr??guez, Kuhlmann, and Satta (2010). They show that every well-nested
LCFRS can be transformed (at the cost of a linear size increase) into a weakly equivalent
one in which all yield functions are either constants (that is, have rank 0) or binary
functions of one of two types:
?x1, . . . , xk1 y1, . . . , yk2? : k1 k2 ? (k1 + k2 ? 1) (concatenation) (6)
?x1, . . . , xj y1, . . . , yk2 xj+1, . . . , xk1? : k1 k2 ? (k1 + k2 ? 2) (wrapping) (7)
A concatenation function takes a k1-tuple and a k2-tuple and returns the (k1 + k2 ? 1)-
tuple that is obtained by concatenating the two arguments. The simplest concatenation
function is the standard concatenation operation ?x y?. We will write conc : k1 k2 to refer
to a concatenation function of the type given in Equation (6). By counting endpoints, we
see that the parsing complexity of concatenation functions is
c(conc : k1 k2) ? 2k1 + 2k2 ? 1
A wrapping function takes a k1-tuple (for some k1 ? 2) and a k2-tuple and returns the
(k1 + k2 ? 2)-tuple that is obtained by ?wrapping? the first argument around the second
argument, filling some gap in the former. The simplest function of this type is ?x1 y x2?,
which wraps a 2-tuple around a 1-tuple. We write wrap : k1 k2 j to refer to a wrapping
function of the type given in Equation (7). The parsing complexity is
c(wrap : k1 k2 j) ? 2k1 + 2k2 ? 2 (for all choices of j)
The constants of the binarized grammar have the form ???, ??, ??, and ?a?, where a is the
anchor of some yield function of the original grammar.
8.5.1 Parsing Complexity. Before presenting the actual binarization, we determine the
parsing complexity of the binarized grammar. Because the binarization preserves the
fan-out of the original grammar, and because in a grammar with fan-out k, for con-
catenation functions conc : k1 k2 we have k1 + k2 ? 1 ? k and for wrapping functions
wrap : k1 k2 j we have k1 + k2 ? 2 ? k, we can rewrite the general parsing complexities as
c(conc : k1 k2) ? 2k1 + 2k2 ? 1 = 2(k1 + k2 ? 1) + 1 ? 2k + 1
c(wrap : k1 k2 j) ? 2k1 + 2k2 ? 2 = 2(k1 + k2 ? 2) + 2 ? 2k + 2
Thus the maximal parsing complexity in the binarized grammar is 2k + 2; this is
achieved by wrapping operations. This gives the bound stated in Lemma 10.
380
Kuhlmann Mildly Non-Projective Dependency Grammar
Figure 12
Binarization of well-nested LCFRSs (complex cases).
8.5.2 Binarization. We now turn to the actual binarization. Consider a rule
A ? f (A1, . . . , Am)
where f is not already a concatenation function, wrapping function, or constant. We
decompose this rule into up to three rules
A ? f ?(B, C) B ? f1(B1, . . . , Bm1 ) C ? f2(C1, . . . , Cm2 )
as follows. We match the template of f against one of three cases, shown schematically
in Figure 12. In each case we select a concatenation or wrapping function f ? (shown in
the right half of the figure), and split up the template of f into two parts defining yield
functions f1 and f2, respectively. In Figure 12, f1 is drawn shaded, and f2 is drawn non-
shaded.4 The split of f partitions the variables that occur in the template, in the sense
4 In order for these parts to make well-defined templates, we will in general need to rename the variables.
We leave this renaming implicit here.
381
Computational Linguistics Volume 39, Number 2
that if for some argument index 1 ? i ? m, either f1 or f2 contains any variable with
argument index i, then it contains all such variables. The two sequences
B1, . . . , Bm1 and C1, . . . , Cm2 are obtained from A1, . . . , Am
by collecting the nonterminal Ai if the variables with argument index i belong to the
template of f1 and f2, respectively. The nonterminals B and C are fresh nonterminals. We
do not create rules for f1 and f2 if they are identity functions.
Example 9
We illustrate the binarization by showing how to transform the rule
A ? ?x1 a x2 y1, y2, y3 x3?(A1, A2)
The template ?x1 a x2 y1, y2, y3 x3? is complex and matches Case 3 in Figure 12, because
its first component starts with the variable x1 and its last component ends with the
variable x3. We therefore split the template into two smaller parts ?x1 a x2, x3? and
?y1, y2, y3?. The function ?y1, y2, y3? is an identity. We therefore create two rules:
A ? f ?1(X, A2) , f ?1 = wrap : 2 3 1 = ?x1 y1, y2, y3 x2? X ? ?x1 a x2, x3?(A1)
Note that the index j for the wrapping function was chosen to be j = 2 because there
were more component boundaries between x2 and x3 than between x1 and x2. The
template ?x1 a x2, x3? requires further decomposition according to Case 3. This time, the
two smaller parts are the identity function ?x1, x2, x3? and the constant ?a?. We therefore
create the following rules:
X ? f ?2(A1, Y) , f ?2 = wrap : 3 1 1 = ?x1 y x2, x3? Y ? ?a?
At this point, the transformation ends.
8.5.3 Correctness. We need to show that the fan-out of the binarized grammar does not
exceed the fan-out of the original grammar. We reason as follows. Starting from some
initial yield function f0 : k1 ? ? ? km ? k, each step of the binarization decomposes some
yield function f into two new yield functions f1, f2. Let us denote the fan-outs of the
three functions by h, h1, h2, respectively. We have
h = h1 + h2 ? 1 in Case 1 and Case 2 (8)
h = h1 + h2 ? 2 in Case 3 (9)
From Equation (8) it is clear that in Case 1 and Case 2, both h1 and h2 are upper-
bounded by h. In Case 3 we have h1 ? 2, which together with Equation (9) implies
that h2 ? h. However, h1 is upper-bounded by h only if h2 ? 2; if h2 = 1, then h1 may
be greater than h. As an example, consider the decomposition of ?x1 a x2? (fan-out 1) into
the wrapping function ?x1, x2? (fan-out 2) and the constant ?a? (fan-out 1). But because
in Case 3 the index j is chosen to maximize the number of component boundaries
between the variables xi,j and xi,j+1, the assumption h2 = 1 implies that each of the h1
components of f1 contains at least one variable with argument index i?if there were
382
Kuhlmann Mildly Non-Projective Dependency Grammar
a component without such a variable, then the two variables that surrounded that com-
ponent would have given rise to a different choice of j. Hence we deduce that h1 ? ki.
9. Conclusion
In this article, we have presented a formalism for non-projective dependency grammar
based on linear context-free rewriting systems, along with a technique for extracting
grammars from dependency treebanks. We have shown that parsing with the full class
of these grammars is intractable. Therefore, we have investigated two constraints on the
non-projectivity of dependency trees, block-degree and well-nestedness. Jointly, these
two constraints define a class of ?mildly? non-projective dependency grammars that
can be parsed in polynomial time.
Our results in Sections 7 and 8 allow us to relate the formal power of an LCFRS
to the structural properties of the dependency structures that it induces. Although we
have used this relation to identify a class of dependency grammars that can be parsed
in polynomial time, it also provides us with a new perspective on the question about
the descriptive adequacy of a grammar formalism. This question has traditionally been
discussed on the basis of strong and weak generative capacity (Bresnan et al 1982;
Huybregts 1984; Shieber 1985). A notion of generative capacity based on dependency
trees makes a useful addition to this discussion, in particular when comparing
formalisms for which no common concept of strong generative capacity exists. As an
example for a result in this direction, see Koller and Kuhlmann (2009).
We have defined the dependency trees that an LCFRS induces by means of a
compositional mapping on the derivations. While we would claim that compositionality
is a generally desirable property, the particular notion of induction is up for discussion.
In particular, our interpretation of derivations may not always be in line with how the
grammar producing these derivations is actually used. One formalism for which such a
mismatch between derivation trees and dependency trees has been pointed out is tree-
adjoining grammar (Rambow, Vijay-Shanker, and Weir 1995; Candito and Kahane 1998).
Resolving this mismatch provides an interesting line of future work.
One aspect that we have not discussed here is the linguistic adequacy of block-
degree and well-nestedness. Each of our dependency grammars is restricted to a finite
block-degree. As a consequence of this restriction, our dependency grammars are not
expressive enough to capture linguistic phenomena that require unlimited degrees
of non-projectivity, such as the ?scrambling? in German subordinate clauses (Becker,
Rambow, and Niv 1992). The question whether it is reasonable to assume a bound
on the block-degree of dependency trees, perhaps for some performance-based reason,
is open. Likewise, it is not clear whether well-nestedness is a ?natural? constraint on
dependency analyses (Chen-Main and Joshi 2010; Maier and Lichte 2011).
Although most of the results that we have presented in this article are of a theo-
retical nature, some of them have found their way into practical systems. In particular,
the extraction technique from Section 4 is used by the data-driven dependency parser
of Maier and Kallmeyer (2010).
Acknowledgments
The author gratefully acknowledges
financial support from The
German Research Foundation
(Sonderforschungsbereich 378,
project MI 2) and The Swedish Research
Council (diary no. 2008-296).
References
Becker, Tilman, Owen Rambow, and
Michael Niv. 1992. The derivational
generative power of formal systems,
or: Scrambling is beyond LCFRS. IRCS
Report 92-38, University of Pennsylvania,
Philadelphia, PA.
383
Computational Linguistics Volume 39, Number 2
Bertsch, Eberhard and Mark-Jan Nederhof.
2001. On the complexity of some
extensions of RCG parsing. In Proceedings
of the Seventh International Workshop on
Parsing Technologies (IWPT), pages 66?77,
Beijing.
Bodirsky, Manuel, Marco Kuhlmann, and
Mathias Mo?hl. 2005. Well-nested
drawings as models of syntactic structure.
In Proceedings of the 10th Conference on
Formal Grammar (FG) and Ninth Meeting
on Mathematics of Language (MOL),
pages 195?203, Edinburgh.
Bo?hmova?, Alena, Jan Hajic?, Eva Hajic?ova?,
and Barbora Hladka?. 2003. The Prague
Dependency Treebank: A three-level
annotation scenario. In Abeille?, Anne,
editor. Treebanks: Building and Using Parsed
Corpora. Kluwer Academic Publishers,
Dordrecht, chapter 7, pages 103?127.
Boullier, Pierre. 1998. Proposal for a natural
language processing syntactic backbone.
Rapport de recherche 3342, INRIA
Rocquencourt, Paris, France.
Boullier, Pierre. 2004. Range Concatenation
Grammars. In Harry C. Bunt, John Carroll,
and Giorgio Satta, editors, New
Developments in Parsing Technology,
volume 23 of Text, Speech and Language
Technology. Kluwer Academic Publishers,
Dordrecht, pages 269?289.
Bresnan, Joan, Ronald M. Kaplan, Stanley
Peters, and Annie Zaenen. 1982.
Cross-serial dependencies in Dutch.
Linguistic Inquiry, 13(4):613?635.
Buchholz, Sabine and Erwin Marsi. 2006.
CoNLL-X shared task on multilingual
dependency parsing. In Proceedings of the
Tenth Conference on Computational Natural
Language Learning (CoNLL), pages 149?164,
New York, NY.
Candito, Marie-He?le`ne and Sylvain Kahane.
1998. Can the TAG derivation tree
represent a semantic graph? An answer
in the light of Meaning-Text Theory.
In Proceedings of the Fourth Workshop on
Tree Adjoining Grammars and Related
Formalisms (TAG+), pages 21?24,
Philadelphia, PA.
Charniak, Eugene. 1996. Tree-bank
grammars. In Proceedings of the
13th National Conference on Artificial
Intelligence (AAAI) and Eighth Innovative
Applications of Artificial Intelligence
Conference (IAAI), volume 2,
pages 1031?1036, Portland, OR.
Chen-Main, Joan and Aravind K. Joshi.
2010. Unavoidable ill-nestedness in
natural language and the adequacy of
tree local-MCTAG induced dependency
structures. In Proceedings of the Tenth
International Conference on Tree Adjoining
Grammars and Related Formalisms (TAG+),
New Haven, CT. Available at http://dx.
doi.org/10.1093/logcom/exs012.
Crescenzi, Pierluigi, Daniel Gildea, Andrea
Marino, Gianluca Rossi, and Giorgio Satta.
2011. Optimal head-driven parsing
complexity for linear context-free
rewriting systems. In Proceedings of the
49th Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 450?459, Portland, OR.
Dz?eroski, Sas?o, Tomaz? Erjavec, Nina
Ledinek, Petr Pajas, Zdenek Z?abokrtsky,
and Andreja Z?ele. 2006. Towards a
Slovene dependency treebank. In Fifth
International Conference on Language
Resources and Evaluations (LREC),
pages 1388?1391, Genoa.
Gaifman, Haim. 1965. Dependency systems
and phrase-structure systems. Information
and Control, 8(3):304?337.
Gildea, Daniel. 2010. Optimal parsing
strategies for linear context-free rewriting
systems. In Proceedings of Human Language
Technologies: The 2010 Annual Conference of
the North American Chapter of the Association
for Computational Linguistics (NAACL),
pages 769?776, Los Angeles, CA.
Go?mez-Rodr??guez, Carlos, John Carroll, and
David J. Weir. 2011. Dependency parsing
schemata and mildly non-projective
dependency parsing. Computational
Linguistics, 37(3):541?586.
Go?mez-Rodr??guez, Carlos, Marco
Kuhlmann, and Giorgio Satta. 2010.
Efficient parsing of well-nested linear
context-free rewriting systems. In
Proceedings of Human Language Technologies:
The 2010 Annual Conference of the North
American Chapter of the Association for
Computational Linguistics (NAACL),
pages 276?284, New Haven, CT.
Go?mez-Rodr??guez, Carlos, Marco
Kuhlmann, Giorgio Satta, and David J.
Weir. 2009. Optimal reduction of rule
length in linear context-free rewriting
systems. In Proceedings of Human Language
Technologies: The 2009 Annual Conference of
the North American Chapter of the Association
for Computational Linguistics (NAACL),
pages 539?547, Boulder, CO.
Go?mez-Rodr??guez, Carlos and Giorgio
Satta. 2009. An optimal-time binarization
algorithm for linear context-free rewriting
systems with fan-out two. In Proceedings
of the Joint Conference of the 47th Annual
384
Kuhlmann Mildly Non-Projective Dependency Grammar
Meeting of the Association for Computational
Linguistics (ACL) and the Fourth
International Joint Conference on Natural
Language Processing of the Asian Federation
of Natural Language Processing (IJCNLP),
pages 985?993, Singapore.
Goodman, Joshua. 1999. Semiring parsing.
Computational Linguistics, 25(4):573?605.
Hajic?, Jan, Otakar Smrz?, Petr Zema?nek,
Jan S?naidauf, and Emanuel Bes?ka. 2004.
Prague Arabic Dependency Treebank:
Development in data and tools. In
Proceedings of the International Conference on
Arabic Language Resources and Tools,
pages 110?117, Cairo.
Havelka, Jir???. 2007. Beyond projectivity:
Multilingual evaluation of constraints and
measures on non-projective structures.
In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 608?615, Prague.
Hays, David G. 1964. Dependency theory:
A formalism and some observations.
Language, 40(4):511?525.
Holan, Toma?s?, Vladislav Kubon?, Karel Oliva,
and Martin Pla?tek. 1998. Two useful
measures of word order complexity.
In Proceedings of the Workshop on
Processing of Dependency-Based Grammars,
pages 21?29, Montre?al.
Hudson, Richard. 2007. Language Networks.
The New Word Grammar. Oxford University
Press, Oxford.
Huybregts, Riny. 1984. The weak inadequacy
of context-free phrase structure grammars.
In Ger de Haan, Mieke Trommelen, and
Wim Zonneveld, editors, Van periferie naar
kern. Foris, Dordrecht, pages 81?99.
Joshi, Aravind K. and Yves Schabes. 1997.
Tree-Adjoining Grammars. In Grzegorz
Rozenberg and Arto Salomaa, editors,
Handbook of Formal Languages, volume 3.
Springer, Berlin, pages 69?123.
Kaji, Yuichi, Ryuichi Nakanishi, Hiroyuki
Seki, and Tadao Kasami. 1992. The
universal recognition problems for
multiple context-free grammars and for
linear context-free rewriting systems.
IEICE Transactions on Information and
Systems, E75-D(1):78?88.
Kallmeyer, Laura. 2010. Parsing Beyond
Context-Free Grammars. Springer, Berlin.
Kanazawa, Makoto. 2009. The pumping
lemma for well-nested multiple
context-free languages. In Developments
in Language Theory. Proceedings of the
13th International Conference, DLT 2009,
volume 5583 of Lecture Notes in Computer
Science, pages 312?325, Stuttgart.
Kanazawa, Makoto and Sylvain Salvati.
2010. The copying power of well-nested
multiple context-free grammars.
In Adrian-Horia Dediu, Henning Fernau,
and Carlos Mart??n-Vide, editors, Language
and Automata Theory and Applications.
Proceedings of the 4th International
Conference, LATA 2010, volume 6031
of Lecture Notes in Computer Science,
pages 344?355, Trier.
Koller, Alexander and Marco Kuhlmann.
2009. Dependency trees and the
strong generative capacity of CCG. In
Proceedings of the 12th Conference of the
European Chapter of the Association for
Computational Linguistics (EACL),
pages 460?468, Athens.
Kracht, Marcus. 2003. The Mathematics
of Language, volume 63 of Studies in
Generative Grammar. Mouton de
Gruyter, Paris.
Kromann, Matthias Trautner. 2003. The
Danish Dependency Treebank and
the underlying linguistic theory. In
Proceedings of the Second Workshop on
Treebanks and Linguistic Theories (TLT),
pages 217?220, Va?xjo?.
Ku?bler, Sandra, Ryan McDonald, and
Joakim Nivre. 2009. Dependency
Parsing. Synthesis Lectures on Human
Language Technologies. Morgan and
Claypool.
Kuhlmann, Marco and Joakim Nivre.
2006. Mildly non-projective dependency
structures. In Proceedings of the
21st International Conference on
Computational Linguistics (COLING) and
44th Annual Meeting of the Association
for Computational Linguistics (ACL)
Main Conference Poster Sessions,
pages 507?514, Sydney.
Kuhlmann, Marco and Giorgio Satta.
2009. Treebank grammar techniques for
non-projective dependency parsing. In
Proceedings of the 12th Conference of the
European Chapter of the Association for
Computational Linguistics (EACL),
pages 478?486, Athens.
Lang, Bernard. 1994. Recognition can be
harder than parsing. Computational
Intelligence, 10(4):486?494.
Li, Zhifei and Jason Eisner. 2009.
First- and second-order expectation
semirings with applications to
minimum-risk training on translation
forests. In Proceedings of the 2009
Conference on Empirical Methods in
Natural Language Processing (EMNLP),
pages 40?51, Singapore.
385
Computational Linguistics Volume 39, Number 2
Maier, Wolfgang and Laura Kallmeyer. 2010.
Discontinuity and non-projectivity: Using
mildly context-sensitive formalisms for
data-driven parsing. In Proceedings of the
Tenth International Conference on Tree
Adjoining Grammars and Related Formalisms
(TAG+), New Haven, CT.
Maier, Wolfgang and Timm Lichte. 2011.
Characterizing discontinuity in constituent
treebanks. In Philippe de Groote, Markus
Egg, and Laura Kallmeyer, editors,
Formal Grammar. Proceedings of the 14th
International Conference, FG 2009, Revised
Selected Papers, volume 5591 of Lecture
Notes in Computer Science, pages 167?182,
Bordeaux.
Maier, Wolfgang and Anders S?gaard. 2008.
Treebanks and mild context-sensitivity.
In Proceedings of the 13th Conference on
Formal Grammar (FG), pages 61?76,
Hamburg.
McAllester, David. 2002. On the complexity
analysis of static analyses. Journal of the
Association for Computing Machinery,
49(4):512?537.
Mel?c?uk, Igor. 1988. Dependency Syntax:
Theory and Practice. State University
of New York Press, Albany, NY.
Michaelis, Jens. 1998. Derivational
minimalism is mildly context-sensitive.
In Logical Aspects of Computational
Linguistics, Third International Conference,
LACL 1998, Selected Papers, volume 2014
of Lecture Notes in Computer Science,
pages 179?198, Grenoble.
Michaelis, Jens. 2001. On Formal Properties
of Minimalist Grammars. Ph.D. thesis,
Universita?t Potsdam, Potsdam,
Germany.
Nivre, Joakim, Johan Hall, Sandra Ku?bler,
Ryan McDonald, Jens Nilsson, Sebastian
Riedel, and Deniz Yuret. 2007. The CoNLL
2007 shared task on dependency parsing.
In Proceedings of the Joint Conference on
Empirical Methods in Natural Language
Processing (EMNLP) and Computational
Natural Language Learning (CoNLL),
pages 915?932, Prague.
Oflazer, Kemal, Bilge Say, Dilek Zeynep
Hakkani-Tu?r, and Go?khan Tu?r. 2003.
Building a Turkish treebank. In Abeille?,
Anne, editor. Treebanks: Building and
Using Parsed Corpora. Kluwer Academic
Publishers, Dordrecht, chapter 15,
pages 261?277.
Rambow, Owen and Aravind K. Joshi.
1997. A formal look at dependency
grammars and phrase-structure
grammars, with special consideration
of word-order phenomena. In Leo Wanner,
editor, Recent Trends in Meaning-Text
Theory, volume 39 of Studies in Language,
Companion Series. John Benjamins,
Amsterdam, pages 167?190.
Rambow, Owen, K. Vijay-Shanker, and
David J. Weir. 1995. D-Tree grammars.
In Proceedings of the 33rd Annual Meeting
of the Association for Computational
Linguistics (ACL), pages 151?158,
Cambridge, MA.
Sagot, Beno??t and Giorgio Satta. 2010.
Optimal rank reduction for linear
context-free rewriting systems with
fan-out two. In Proceedings of the 48th
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 525?533, Uppsala.
Satta, Giorgio. 1992. Recognition of
linear context-free rewriting systems.
In Proceedings of the 30th Annual
Meeting of the Association for
Computational Linguistics (ACL),
pages 89?95, Newark, DE.
Schabes, Yves. 1990. Mathematical and
Computational Aspects of Lexicalized
Grammars. Ph.D. thesis, University
of Pennsylvania, Philadelphia, PA.
Schabes, Yves, Anne Abeille?, and
Aravind K. Joshi. 1988. Parsing
strategies with ?lexicalized? grammars:
Application to tree adjoining grammars.
In Proceedings of the Twelfth International
Conference on Computational Linguistics
(COLING), pages 578?583, Budapest.
Seki, Hiroyuki, Takashi Matsumura,
Mamoru Fujii, and Tadao Kasami.
1991. On Multiple Context-Free
Grammars. Theoretical Computer
Science, 88(2):191?229.
Sgall, Petr, Eva Hajic?ova?, and Jarmila
Panevova?. 1986. The Meaning of the
Sentence in Its Semantic and Pragmatic
Aspects. Springer, Berlin.
Shieber, Stuart M. 1985. Evidence against
the context-freeness of natural language.
Linguistics and Philosophy, 8(3):333?343.
Shieber, Stuart M., Yves Schabes, and
Fernando Pereira. 1995. Principles
and implementation of deductive
parsing. Journal of Logic Programming,
24(1?2):3?36.
Steedman, Mark and Jason Baldridge.
2011. Combinatory categorial grammar.
In Robert D. Borsley and Kersti Bo?rjars,
editors, Non-Transformational Syntax:
Formal and Explicit Models of Grammar.
Wiley-Oxford, Blackwell, chapter 5,
pages 181?224.
386
Kuhlmann Mildly Non-Projective Dependency Grammar
Tesnie`re, Lucien. 1959. E?le?ments de syntaxe
structurale. Klinksieck, Paris.
Vijay-Shanker, K., David J. Weir, and
Aravind K. Joshi. 1987. Characterizing
structural descriptions produced by
various grammatical formalisms.
In Proceedings of the 25th Annual Meeting
of the Association for Computational
Linguistics (ACL), pages 104?111,
Stanford, CA.
Villemonte de la Clergerie, E?ric. 2002.
Parsing mildly context-sensitive
languages with thread automata.
In Proceedings of the 19th International
Conference on Computational Linguistics
(COLING), pages 1?7, Taipei.
Weir, David J. 1988. Characterizing Mildly
Context-Sensitive Grammar Formalisms.
Ph.D. thesis, University of Pennsylvania,
Philadelphia, PA.
387

Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 276?284,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Efficient Parsing of Well-Nested Linear Context-Free Rewriting Systems
Carlos G?mez-Rodr?guez1, Marco Kuhlmann2, and Giorgio Satta3
1Departamento de Computaci?n, Universidade da Coru?a, Spain, cgomezr@udc.es
2Department of Linguistics and Philology, Uppsala University, Sweden, marco.kuhlmann@lingfil.uu.se
3Department of Information Engineering, University of Padua, Italy, satta@dei.unipd.it
Abstract
The use of well-nested linear context-free
rewriting systems has been empirically moti-
vated for modeling of the syntax of languages
with discontinuous constituents or relatively
free word order. We present a chart-based pars-
ing algorithm that asymptotically improves the
known running time upper bound for this class
of rewriting systems. Our result is obtained
through a linear space construction of a binary
normal form for the grammar at hand.
1 Introduction
Since its earliest years, one of the main goals of
computational linguistics has been the modeling of
natural language syntax by means of formal gram-
mars. Following results by Huybregts (1984) and
Shieber (1985), special attention has been given to
formalisms that enlarge the generative power of con-
text-free grammars, but still remain below the full
generative power of context-sensitive grammars. On
this line of investigation, mildly context-sensitive
grammar formalisms have been introduced (Joshi,
1985), including, among several others, the tree ad-
joining grammars (TAGs) of Joshi et al (1975).
Linear context-free rewriting system (LCFRS), in-
troduced by Vijay-Shanker et al (1987), is a mildly
context-sensitive formalism that allows the deriva-
tion of tuples of strings, i.e., discontinuous phrases.
This feature has been used to model phrase structure
treebanks with discontinuous constituents (Maier and
S?gaard, 2008), as well as to map non-projective de-
pendency trees into discontinuous phrase structures
(Kuhlmann and Satta, 2009).
Informally, in an LCFRS G, each nonterminal can
generate string tuples with a fixed number of compo-
nents. The fan-out of G is defined as the maximum
number of tuple components generated by G. During
a derivation of an LCFRS, tuple components gener-
ated by the nonterminals in the right-hand side of
a production are concatenated to form new tuples,
possibly adding some terminal symbols. The only re-
striction applying to these generalized concatenation
operations is linearity, that is, components cannot be
duplicated or deleted.
The freedom in the rearrangement of components
has specific consequences in terms of the computa-
tional and descriptional complexity of LCFRS. Even
for grammars with bounded fan-out, the universal
recognition problem is NP-hard (Satta, 1992), and
these systems lack Chomsky-like normal forms for
fixed fan-out (Rambow and Satta, 1999) that are es-
pecially convenient in tabular parsing. This is in con-
trast with other mildly context-sensitive formalisms,
and TAG in particular: TAGs can be parsed in poly-
nomial time both with respect to grammar size and
string size, and they can be cast in normal forms
having binary derivation trees only.
It has recently been argued that LCFRS might be
too powerful for modeling languages with discontin-
uous constituents or with relatively free word order,
and that additional restrictions on the rearrangement
of components might be needed. More specifically,
analyses of both dependency and constituency tree-
banks (Kuhlmann and Nivre, 2006; Havelka, 2007;
Maier and Lichte, 2009) have shown that rearrange-
ments of argument tuples almost always satisfy the
so-called well-nestedness condition, a generalization
276
of the standard condition on balanced brackets. This
condition states that any two components x1, x2 of
some tuple will never be composed with any two
components y1, y2 of some other tuple in such a way
that a ?crossing? configuration is realized.
In this paper, we contribute to a better understand-
ing of the formal properties of well-nested LCFRS.
We show that, when fan-out is bounded by any inte-
ger ? ? 1, these systems can always be transformed,
in an efficient way, into a specific normal form with
no more than two nonterminals in their productions?
right-hand sides. On the basis of this result, we
then develop an efficient parsing algorithm for well-
nested LCFRS, running in timeO(? ? |G| ? |w|2?+2),
where G and w are the input grammar and string,
respectively. Well-nested LCFRS with fan-out ? = 2
are weakly equivalent to TAG, and our complex-
ity result reduces to the well-known upper bound
O(|G| ? |w|6) for this class. For ? > 2, our upper
bound is asymptotically better than the one obtained
from existing parsing algorithms for general LCFRS
or equivalent formalisms (Seki et al, 1991).
Well-nested LCFRS are generatively equivalent
to (among others) coupled context-free grammars
(CCFG), introduced by Hotz and Pitsch (1996).
These authors also provide a normal form and de-
velop a parsing algorithm for CCFGs. One difference
with respect to our result is that the normal form for
CCFGs allows more than two nonterminals to appear
in the right-hand side of a production, even though no
nonterminal may contribute more than two tuple com-
ponents. Also, the construction in (Hotz and Pitsch,
1996) results in a blow-up of the grammar that is ex-
ponential in its fan-out, and the parsing algorithm that
is derived runs in time O(4? ? |G| ? |w|2?+2). Our
result is therefore a considerable asymptotic improve-
ment over the CCFG result, both with respect to the
normal form construction and the parsing efficiency.
Finally, under a practical perspective, our parser is a
simple chart-based algorithm, while the algorithm in
(Hotz and Pitsch, 1996) involves two passes and is
considerably more complex to analyze and to imple-
ment than ours.
Kanazawa and Salvati (2010) mention a normal
form for well-nested multiple context-free grammars.
Structure In Section 2, we introduce LCFRS and
the class of well-nested LCFRS that is the focus of
this paper. In Section 3, we discuss the parsing com-
plexity of LCFRS, and show why grammars using
our normal form can be parsed efficiently. Section 4
presents the transformation of a well-nested LCFRS
into the normal form. Section 5 concludes the paper.
2 Linear Context-Free Rewriting Systems
We write [n] to denote the set of positive integers up
to and including n: [n] = {1, . . . , n}.
2.1 Linear, non-erasing functions
Let ? be an alphabet. For integers m ? 0 and
k1, . . . , km, k ? 1, a total function
f : (??)k1 ? ? ? ? ? (??)km ? (??)k
is called a linear, non-erasing function over ? with
type k1 ? ? ? ? ? km ? k, if it can be defined by an
equation of the form
f(?x1,1, . . . , x1,k1?, . . . , ?xm,1, . . . , xm,km?) = ~? ,
where ~? is a k-tuple of strings over the variables on
the left-hand side of the equation and ? with the
property that each variable occurs in ~? exactly once.
The values m and k are called the rank and the fan-
out of f , and denoted by ?(f) and ?(f).
2.2 Linear Context-Free Rewriting Systems
For the purposes of this paper, a linear context-free
rewriting system, henceforth LCFRS, is a construct
G = (N,T, P, S), where N is an alphabet of nonter-
minal symbols in which each symbol A is associated
with a positive integer ?(A) called its fan-out, T is
an alphabet of terminal symbols, S ? N is a distin-
guished start symbol with ?(S) = 1; and P is a finite
set of productions of the form
p = A? f(A1, . . . , Am) ,
where m ? 0, A,A1, . . . , Am ? N , and f is a linear,
non-erasing function over the terminal alphabet T
with type ?(A1)? ? ? ? ??(Am)? ?(A), called the
composition operation associated with p. The rank
of G and the fan-out of G are defined as the maximal
rank and fan-out of the composition operations of G,
and are denoted by ?(G) and ?(G).
The sets of derivation trees of G are the smallest
indexed family of sets DA, A ? N , such that, if
p = A? f(A1, . . . , Am)
277
N = {S,R} , T = {a, b, c, d} , P = { p1 = S ? f1(R), p2 = R? f2(R), p3 = R? f3 } ,
where: f1(?x1,1, x1,2?) = ?x1,1 x1,2? , f2(?x1,1, x1,2?) = ?a x1,1 b, c x1,2 d? , f3 = ??, ?? .
Figure 1: An LCFRS that generates the string language { anbncndn | n ? 0 }.
is a production of G and ti ? DAi for all i ? [m],
then t = p(t1, . . . , tm) ? DA. By interpreting pro-
ductions as their associated composition operations
in the obvious way, a derivation tree t ? DA evalu-
ates to a ?(A)-tuple of strings over T ; we denote this
tuple by val(t). The string language generated by G,
denoted by L(G), is then defined as
L(G) = {w ? T ? | t ? DS , ?w? = val(t) } .
Two LCFRS are called weakly equivalent, if they
generate the same string language.
Example Figure 1 shows a sample LCFRS G with
?(G) = 1 and ?(G) = 2. The sets of its deriva-
tion trees are DR = { pn2 (p3) | n ? 0 } and
DS = { p1(t) | t ? DR }. The string language
generated by G is { anbncndn | n ? 0 }.
2.3 Characteristic strings
In the remainder of this paper, we use the following
convenient syntax for tuples of strings. Instead of
?v1, . . . , vk? , we write v1 $ ? ? ? $ vk ,
using the $-symbol to mark the component bound-
aries. We call this the characteristic string of the tu-
ple, and an occurrence of the symbol $ a gap marker.
We also use this notation for composition operations.
For example, the characteristic string of the operation
f(?x1,1, x1,2?, ?x2,1?) = ?a x1,1 x2,1, x1,2 b?
is a x1,1 x2,1 $ x1,2 b. If we assume the variables on
the left-hand side of an equation to be named ac-
cording to the schema used in Section 2.1, then the
characteristic string of a composition operation deter-
mines that operation completely. We will therefore
freely identify the two, and write productions as
p = A? [v1 $ ? ? ? $ vk](A1, . . . , Am) ,
where the string inside the brackets is the charac-
teristic string of some composition operation. The
substrings v1, . . . , vk are called the components of
the characteristic string. Note that the character-
istic string of a composition operation with type
k1 ? ? ? ? ? km ? k is a sequence of terminal
symbols, gap markers, and variables from the set
{xi,j | i ? [m], j ? [ki] } in which the number of
gap markers is k?1, and each variable occurs exactly
once. When in the context of such a composition op-
eration we refer to ?a variable of the form xi,j?, then
it will always be the case that i ? [m] and j ? [ki].
The identification of composition operations and
their characteristic strings allows us to construct new
operations by string manipulations: if, for example,
we delete some variables from a characteristic string,
then the resulting string still defines a composition
operation (after a suitable renaming of the remaining
variables, which we leave implicit).
2.4 Canonical LCFRS
To simplify our presentation, we will assume that
LCFRS are given in a certain canonical form. Intu-
itively, this canonical form requires the variables in
the characteristic string of a composition operation
to be ordered in a certain way.
Formally, the defining equation of a composition
operation f with type k1 ? ? ? ? ? km ? k is called
canonical, if (i) the sequence obtained from f by
reading variables of the form xi,1 from left to right
has the form x1,1 ? ? ?xm,1; and (ii) for each i ? [m],
the sequence obtained from f by reading variables
of the form xi,j from left to right has the form
xi,1 ? ? ?xi,ki . An LCFRS is called canonical, if each
of its composition operations is canonical.
We omit the proof that every LCFRS can be trans-
formed into a weakly equivalent canonical LCFRS.
However, we point out that both the normal form and
the parsing algorithm that we present in this paper
can be applied also to general LCFRS. This is in con-
trast to some left-to-right parsers in the literature on
LCFRS and equivalent formalisms (de la Clergerie,
2002; Kallmeyer and Maier, 2009), which actually
depend on productions in canonical form.
2.5 Well-nested LCFRS
We now characterize the class of well-nested LCFRS
that are the focus of this paper. Well-nestedness
was first studied in the context of dependency gram-
mars (Kuhlmann and M?hl, 2007). Kanazawa (2009)
278
defines well-nested multiple context-free grammars,
which are weakly equivalent to well-nested LCFRS.
A composition operation is called well-nested, if it
does not contain a substring of the form
xi,i1 ? ? ?xj,j1 ? ? ?xi,i2 ? ? ?xj,j2 , where i 6= j .
For example, the operation x1,1 x2,1$x2,2 x1,2 is well-
nested, while x1,1 x2,1 $ x1,2 x2,2 is not. An LCFRS
is called well-nested, if it contains only well-nested
composition operations.
The class of languages generated by well-nested
LCFRS is properly included in the class of languages
generated by general LCFRS; see Kanazawa and Sal-
vati (2010) for further discussion.
3 Parsing LCFRS
We now discuss the parsing complexity of LCFRS,
and motivate our interest in a normal form for well-
nested LCFRS.
3.1 General parsing schema
A bottom-up, chart-based parsing algorithm for the
class of (not necessarily well-nested) LCFRS can be
defined by using the formalism of parsing schemata
(Sikkel, 1997). The parsing schemata approach con-
siders parsing as a deduction process (as in Shieber
et al (1995)), generating intermediate results called
items. Starting with an initial set of items obtained
from each input sentence, a parsing schema defines
a set of deduction steps that can be used to infer
new items from existing ones. Each item contains
information about the sentence?s structure, and a suc-
cessful parsing process will produce at least one final
item containing a full parse for the input.
The item set used by our bottom-up algorithm to
parse an input string w = a1 ? ? ? an with an LCFRS
G = (N,T, P, S) will be
I = {[A, (l1, r1), . . . , (lk, rk)] | A ? N ?
0 ? li ? ri ? n ?i ? [k]},
where an item [A, (l1, r1), . . . , (lk, rk)] can be inter-
preted as the set of those derivation trees t ? DA
of G for which
val(t) = al1+1 ? ? ? ar1 $ ? ? ? $ alk+1 ? ? ? ark .
The set of final items is thus F = {[S, (0, n)]}, con-
taining full derivation trees that evaluate to w.
For simplicity of definition of the sets of initial
items and deduction steps, let us assume that pro-
ductions of rank > 0 in our grammar do not contain
terminal symbols in their right-hand sides. This can
be easily achieved from a starting grammar by cre-
ating a nonterminal Aa for each terminal a ? T , a
corresponding rank-0 production pa = Aa ? [a](),
and then changing each occurrence of a in the char-
acteristic string of a production to the single variable
associated with the fan-out 1 nonterminal Aa. With
this, our initial item set for a string a1 ? ? ? an will be
H = {[Aai , (i? 1, i)] | i ? [n]} ,
and each production p = A0 ? f(A1, . . . , Am) of
G (excluding the ones we created for the terminals)
will produce a deduction step of the form given in
Figure 2a, where the indexes are subject to the fol-
lowing constraints, imposed by the semantics of f .
1. If the kth component of the characteristic string
of f starts with xi,j , then l0,k = li,j .
2. If the kth component of the characteristic string
of f ends with xi,j , then r0,k = ri,j .
3. If xi,jxi?,j? is an infix of the characteristic string
of f , then ri,j = li?,j? .
4. If the kth component of the characteristic string
of f is the empty string, then l0,k = r0,k.
3.2 General complexity
The time complexity of parsing LCFRS with respect
to the length of the input can be analyzed by counting
the maximum number of indexes that can appear in
an instance of the inference rule above. Although the
total number of indexes is
?m
i=0 2 ? ?(Ai), some of
these indexes are equated by the constraints.
To count the number of independent indexes, con-
sider all the indexes of the form l0,i (corresponding to
the left endpoints of each component of the character-
istic string of f ) and those of the form rj,k for j > 0
(corresponding to the right endpoints of each vari-
able in the characteristic string). By the constraints
above, these indexes are mutually independent, and it
is easy to check that any other index is equated to one
of these: indexes r0,i are equated to the index rj,k
corresponding to the last variable xj,k of the ith com-
ponent of the characteristic string, or to l0,i if there
is no such variable; while indexes lj,k with j > 0
are equated to an index l0,i if the variable xj,k is at
the beginning of a component of the characteristic
string, or to an index rj?,k?(j? > 1) if the variable xj,k
follows another variable xj?,k? .
279
[A1, (l1,1, r1,1), . . . , (l1,?(A1), r1,?(A1))] ? ? ? [Am, (lm,1, rm,1), . . . , (lm,?(Am), rm,?(Am))]
[A0, (l0,1, r0,1), . . . , (l0,?(A0), r0,?(A0))]
(a) The general rule for a parsing schema for LCFRS
[B, (l1, r1), . . . , (lm, rm)] [C, (l
?
1, r
?
1), . . . (l
?
n, r
?
n)]
[A, (l1, r1), . . . , (lm, r
?
1), . . . (l
?
n, r
?
n)]
rm = l?1
(b) Deduction step for concatenation
[B, (l1, r1), . . . , (lm, rm)] [C, (l
?
1, r
?
1), . . . (l
?
n, r
?
n)]
[A, (l1, r1), . . . , (li, r
?
1), . . . (l
?
n, ri+1), . . . , (lm, rm)]
ri = l?1, r
?
n = li+1
(c) Deduction step for wrapping
Figure 2: Deduction steps for parsing LCFRS.
Thus, the parsing complexity (Gildea, 2010) of a
production p = A0 ? f(A1, . . . , Am) is determined
by ?(A0) l-indexes and
?
i?[m] ?(Ai) r-indexes, for
a total complexity of
O(|w|?(A0)+
?
i?[m] ?(Ai))
where |w| is the length of the input string. The pars-
ing complexity of an LCFRS will correspond to the
maximum parsing complexity among its productions.
Note that this general complexity matches the result
given by Seki et al (1991).
In an LCFRS of rank ? and fan-out ?, the maxi-
mum possible parsing complexity is O(|w|?(?+1)),
obtained by applying the above expression to a pro-
duction of rank ? and where each nonterminal has fan-
out ?. The asymptotic time complexity of LCFRS
parsing is therefore exponential both in its rank and
its fan-out. This means that it is interesting to trans-
form LCFRS into equivalent forms that reduce their
rank while preserving the fan-out. For sets of LCFRS
that can be transformed into a binary form (i.e., such
that all its rules have rank at most 2), the ? factor in
the complexity is reduced to a constant, and complex-
ity is improved to O(|w|3?) (see G?mez-Rodr?guez
et al (2009) for further discussion). Unfortunately,
it is known by previous results (Rambow and Satta,
1999) that it is not always possible to convert an
LCFRS into such a binary form without increasing
the fan-out. However, we will show that it is always
possible to build such a binarization for well-nested
LCFRS. Combining this result with the inference
rule and complexity analysis given above, we would
obtain a parser for well-nested LCFRS running in
O(|w|3?) time. But the construction of our binary
normal form additionally restricts binary composition
operations in the binarized LCFRS to be of two spe-
cific forms, concatenation and wrapping, which fur-
ther improves the parsing complexity to O(|w|2?+2),
as we will see below.
3.3 Concatenation and wrapping
A composition operation is called a concatenation
operation, if its characteristic string has the form
x1,1 $ ? ? ? $ x1,m x2,1 $ ? ? ? $ x2,n ,
where m,n ? 1. Intuitively, such an operation corre-
sponds to the bottom-up combination of two adjacent
discontinuous constituents into one. An example of
a concatenation operation is the binary parsing rule
used by the standard CKY parser for context-free
grammars, which combines continuous constituents
(represented as 1-tuples of strings in the LCFRS nota-
tion). In the general case, a concatenation operation
will take an m-tuple and an n-tuple and return an
(m + n ? 1)-tuple, as the joined constituents may
have gaps that will also appear in the resulting tuple.
If we apply the general parsing rule given in Fig-
ure 2a to a production A? conc(B,C), where conc
is a concatenation operation, then we obtain the de-
duction step given in Figure 2b. This step uses 2m
different l- and r-indexes, and 2n? 1 different l?-
and r?-indexes (excluding l?1 which must equal rm),
for a total of 2m+2n?1 = 2(m+n?1)+1 indexes.
Since m+ n? 1 is the fan-out of the nonterminal A,
we conclude that the maximum number of indexes in
the step associated with a concatenation operation in
an LCFRS of fan-out ? is 2?+ 1.
280
before: p
? ? ?
t1 tm
after: p?
q
? ? ?
tq,1 tq,mq
r
? ? ?
tr,1 tr,mr
Figure 3: Transformation of derivation trees
A linear, non-erasing function is called a wrapping
operation, if its characteristic string has the form
x1,1 $ ? ? ? $ x1,i x2,1 $ ? ? ? $ x2,n x1,i+1 $ ? ? ? $ x1,m ,
where m,n ? 1 and i ? [m? 1]. Intuitively, such an
operation wraps the tuple derived from a nontermi-
nal B around the tuple derived from a nonterminal C,
filling the ith gap in the former. An example of a
wrapping operation is the adjunction of an auxiliary
tree in tree-adjoining grammar. In the general case, a
wrapping operation will take an m-tuple and an n-tu-
ple and return an (m + n ? 2)-tuple of strings: the
gaps of the argument tuples appear in the obtained
tuple, except for one gap in the tuple derived from B
which is filled by the tuple derived from C.
By applying the general parsing rule in Figure 2a
to a production A ? wrapi(B,C), where wrapi is
a wrapping operation, then we obtain the deduction
step given in Figure 2c. This step uses 2m different l-
and r-indexes, and 2n? 2 different l?- and r?-indexes
(discounting l?1 and r
?
n which are equal to other in-
dexes), for a total of 2m+2n?2 = 2(m+n?2)+2
indexes. Since the fan-out of A is m + n ? 2, this
means that a wrapping operation needs at most 2?+2
indexes for an LCFRS of fan-out ?.
From this, we conclude that an LCFRS of fan-
out ? in which all composition operations are ei-
ther concatenation operations, wrapping operations,
or operations of rank 0 or 1, can be parsed in time
O(|w|2?+2). In particular, nullary and unary compo-
sition operations do not affect this worst-case com-
plexity, since their associated deduction steps can
never have more than 2? indexes.
4 Transformation
We now show how to transform a well-nested LCFRS
into the normal form that we have just described.
4.1 Informal overview
Consider a production p = A ? f(A1, . . . , Am),
where m ? 2 and f is neither a concatenation nor a
wrapping operation. We will construct new produc-
tions p?, q, r such that every derivation that uses p can
be rewritten into a derivation that uses the new pro-
ductions, and the new productions do not license any
other derivations. Formally, this can be understood as
implementing a tree transformation, where the input
trees are derivations of the original grammar, and the
output trees are derivations of the new grammar. The
situation is illustrated in Figure 3. The tree on top
represents a derivation in the original grammar; this
derivation starts with the rewriting of the nontermi-
nal A using the production p, and continues with the
subderivations t1, . . . , tm. The tree at the bottom rep-
resents a derivation in the transformed grammar. This
derivation starts with the rewriting ofA using the new
production p?, and continues with two independent
subderivations that start with the new productions q
and r, respectively. The sub-derivations t1, . . . , tm
have been partitioned into two sequences
t1,1, . . . , t1,m1 and t2,1, . . . , t2,m2 .
The new production p? will be either a concatenation
or a wrapping operation, and the rank of both q and r
will be strictly smaller than the rank of p. The trans-
formation will continue with q and r, unless these
have rank one. By applying this strategy exhaustively,
we will thus eventually end up with a grammar that
only has productions with rank at most 2, and in
which all productions with rank 2 are either concate-
nation or wrapping operations.
4.2 Constructing the composition operations
To transform the production p, we first factorize the
composition operation f associated with p into three
new composition operations f ?, g, h as follows. Re-
call that we represent composition operations by their
characteristic strings.
In the following, we will assume that no charac-
teristic string starts or ends with a gap marker, or
contains immediate repetitions of gap markers. This
281
property can be ensured, without affecting the asymp-
totic complexity, by adding intermediate steps to the
transformation that we report here; we omit the de-
tails due to space reasons. When this property holds,
we are left with the following two cases. Let us call a
sequence of variables joint, if it contains all and only
variables associated with a given nonterminal.
Case 1 f = x1 f1 x2 ? ? ?xk?1 fk?1 xk f? ,
where k ? 1, x1, . . . , xk are joint variables, and the
suffix f? contains at least one variable. Let
g = x1 f1 x2 ? ? ?xk?1 fk?1 xk ,
let h = f?, and let f ? = conc. As f is well-nested,
both g and h define well-nested composition opera-
tions. By the specific segmentation of f , the ranks of
these operations are strictly smaller than the rank of f .
Furthermore, we have ?(f) = ?(g) + ?(h)? 1 .
Case 2 f = x1 f1 x2 ? ? ?xk?1 fk?1 xk ,
where k ? 2, x1, . . . , xk are joint variables, and there
exist at least one i such that the sequence fi contains
at least one variable. Choose an index j as follows:
if there is at least one i such that fi contains at least
one variable and one gap marker, let j be the minimal
such i; otherwise, let j be the minimal i such that fi
contains at least one variable. Now, let
g = x1 f1 x2 ? ? ?xj $ xj+1 ? ? ?xk?1 fk?1 xk ,
let h = fj , and let f ? = wrapj . As in Case 1, both g
and h define well-nested composition operations
whose ranks are strictly smaller than the rank of f .
Furthermore, we have ?(f) = ?(g) + ?(h)? 2 .
Note that at most one of the two cases can apply
to f . Furthermore, since f is well-nested, it is also
true that at least one of the two cases applies. This
is so because for two distinct nonterminals Ai, Ai? ,
either all variables associated with Ai? precede the
leftmost variable associated with Ai, succeed the
rightmost variable associated with Ai, or are placed
between two variables associated with Ai without an-
other variable associated with Ai intervening. (Here,
we have left out the symmetric cases.)
4.3 Constructing the new productions
Based on the composition operations, we now con-
struct three new productions p?, q, r as follows. LetB
and C be two fresh nonterminals with ?(B) = ?(g)
and ?(C) = ?(h), and let p? = A ? f ?(B,C).
The production p? rewrites A into B and C and
combines the two subderivations that originate at
these nonterminals using either a concatenation or a
wrapping operation. Now, let Aq,1, . . . , Aq,mq and
Ar,1, . . . , Ar,mr be the sequences of nonterminals
that are obtained from the sequence A1, . . . , Am by
deleting those nonterminals that are not associated
with any variable in g or h, respectively. Then, let
q = B ? g(Aq,1, . . . , Aq,mq) and
r = C ? h(?Ar,1, . . . , Ar,mr) .
4.4 Example
We now illustrate the transformation using the con-
crete production p = A? f(A1, A2, A3), where
f = x1,1 x2,1 $ x1,2 $ x3,1 .
Note that this operation has rank 3 and fan-out 3.
The composition operations are constructed as fol-
lows. The operation f matches the pattern of Case 1,
and hence induces the operations
g1 = x1,1 x2,1 $ x1,2 , h1 = $ x3,1 , f ?1 = conc .
The productions constructed from these are
p?1 = A? conc(B1, C1) ,
q1 = B1 ? g1(A1, A2) , r1 = C1 ? h1(A3) .
where B1 and C1 are fresh nonterminals with fan-
out 2. The production r1 has rank one, so it does not
require any further transformations. The transforma-
tion thus continues with q1. The operation g1 matches
the pattern of Case 2, and induces the operations
g2 = x1,1 $ x1,2 , h2 = x2,1$ , f ?2 = wrap1 .
The productions constructed from these are
p?2 = B1 ? wrap1(B2, C2) ,
q2 = B2 ? g2(A1) , r2 = C2 ? h2(A2) ,
where B2 and C2 are fresh nonterminals with fan-
out 2. At this point, the transformation terminates.
We can now delete p from the original grammar, and
replace it with the productions {p?1, r1, p
?
2, q2, r2}.
4.5 Correctness
To see that the transformation is correct, we need to
verify that each production of the original grammar
is transformed into a set of equivalent normal-form
productions, and that the fan-out of the new grammar
does not exceed the fan-out of the old grammar.
For the first point, we note that the transformation
preserves well-nestedness, decreases the rank of a
production, and is always applicable as long as the
282
rank of a production is at most 2 and the production
does not use a concatenation or wrapping operation.
That the new productions are equivalent to the old
ones in the sense of Figure 3 can be proved by induc-
tion on the length of a derivation in the original and
the new grammar, respectively.
Let us now convince ourselves that the fan-out of
the new grammar does not exceed the fan-out of the
old grammar. This is clear in Case 1, where
?(f) = ?(g) + ?(h)? 1
implies that both ?(g) ? ?(f) and ?(h) ? ?(f).
For Case 2, we reason as follows. The fan-out of the
operation h, being constructed from an infix of the
characteristic string of the original operation f , is
clearly bounded by the fan-out of f . For g, we have
?(g) = ?(f)? ?(h) + 2 ,
Now suppose that the index j was chosen according
to the first alternative. In this case, ?(h) ? 2, and
?(g) ? ?(f)? 2 + 2 = ?(f) .
For the case where j was chosen according to the
second alternative, ?(f) < k (since there are no
immediate repetitions of gap markers), ?(h) = 1,
and ?(g) ? k. If we assume that each nonterminal
is productive, then this means that the underlying
LCFRS has at least one production with fan-out k or
more; therefore, the fan-out of g does not increase
the fan-out of the original grammar.
4.6 Complexity
To conclude, we now briefly discuss the space com-
plexity of the normal-form transformation. We mea-
sure it in terms of the length of a production, defined
as the length of its string representation, that is, the
string A? [v1 $ ? ? ? $ vk](A1, . . . , Am) .
Looking at Figure 3, we note that the normal-form
transformation of a production p can be understood
as the construction of a (not necessarily complete)
binary-branching tree whose leaves correspond to the
productions obtained by splitting the characteristic
string of p and whose non-leaf nodes are labeled with
concatenation and wrapping operations. By construc-
tion, the sum of the lengths of leaf-node productions
is O(|p|). Since the number of inner nodes of a bi-
nary tree with n leaves is bounded by n ? 1, we
know that the tree hasO(?(p)) inner nodes. As these
nodes correspond to concatenation and wrapping
operations, each inner-node production has length
O(?(p)). Thus, the sum of the lengths of the produc-
tions created from |p| is O(|p|+ ?(p)?(p)). Since
the rank of a production is always smaller than its
length, this is reduced to O(|p|?(p)).
Therefore, the size of the normal-form transfor-
mation of an LCFRS G of fan-out ? is O(?|G|) in
the worst case, and linear space in practice, since
the fan-out is typically bounded by a small integer.
Taking the normal-form transformation into account,
our parser therefore runs in timeO(? ? |G| ? |w|2?+2)
where |G| is the original grammar size.
5 Conclusion
In this paper, we have presented an efficient parsing
algorithm for well-nested linear context-free rewrit-
ing systems, based on a new normal form for this
formalism. The normal form takes up linear space
with respect to grammar size, and the algorithm is
based on a bottom-up process that can be applied
to any LCFRS, achieving O(? ? |G| ? |w|2?+2) time
complexity when applied to LCFRS of fan-out ?
in our normal form. This complexity is an asymp-
totic improvement over existing results for this class,
both from parsers specifically geared to well-nested
LCFRS or equivalent formalisms (Hotz and Pitsch,
1996) and from applying general LCFRS parsing
techniques to the well-nested case (Seki et al, 1991).
The class of well-nested LCFRS is an interest-
ing syntactic formalism for languages with discon-
tinuous constituents, providing a good balance be-
tween coverage of linguistic phenomena in natu-
ral language treebanks (Kuhlmann and Nivre, 2006;
Maier and Lichte, 2009) and desirable formal prop-
erties (Kanazawa, 2009). Our results offer a further
argument in support of well-nested LCFRS: while
the complexity of parsing general LCFRS depends
on two dimensions (rank and fan-out), this bidimen-
sional hierarchy collapses into a single dimension
in the well-nested case, where complexity is only
conditioned by the fan-out.
Acknowledgments G?mez-Rodr?guez has been
supported by MEC/FEDER (HUM2007-66607-C04)
and Xunta de Galicia (PGIDIT07SIN005206PR, Re-
des Galegas de PL e RI e de Ling. de Corpus, Bolsas
Estad?as INCITE/FSE cofinanced). Kuhlmann has
been supported by the Swedish Research Council.
283
References
?ric Villemonte de la Clergerie. 2002. Parsing mildly
context-sensitive languages with thread automata. In
19th International Conference on Computational Lin-
guistics (COLING), pages 1?7, Taipei, Taiwan.
Daniel Gildea. 2010. Optimal parsing strategies for linear
context-free rewriting systems. In Human Language
Technologies: The Eleventh Annual Conference of the
North American Chapter of the Association for Compu-
tational Linguistics, Los Angeles, USA.
Carlos G?mez-Rodr?guez, Marco Kuhlmann, Giorgio
Satta, and David J. Weir. 2009. Optimal reduction
of rule length in linear context-free rewriting systems.
In Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages 539?547,
Boulder, CO, USA.
Jir?? Havelka. 2007. Beyond projectivity: Multilin-
gual evaluation of constraints and measures on non-
projective structures. In 45th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
608?615.
G?nter Hotz and Gisela Pitsch. 1996. On parsing coupled-
context-free languages. Theoretical Computer Science,
161(1?2):205?233.
Riny Huybregts. 1984. The weak inadequacy of context-
free phrase structure grammars. In Ger de Haan, Mieke
Trommelen, and Wim Zonneveld, editors, Van periferie
naar kern, pages 81?99. Foris, Dordrecht, The Nether-
lands.
Aravind K. Joshi, Leon S. Levy, and Masako Takahashi.
1975. Tree Adjunct Grammars. Journal of Computer
and System Sciences, 10(2):136?163.
Aravind K. Joshi. 1985. Tree Adjoining Grammars: How
much context-sensitivity is required to provide reason-
able structural descriptions? In Natural Language
Parsing, pages 206?250. Cambridge University Press.
Laura Kallmeyer and Wolfgang Maier. 2009. An incre-
mental Earley parser for simple range concatenation
grammar. In Proceedings of the 11th International Con-
ference on Parsing Technologies (IWPT 2009), pages
61?64. Association for Computational Linguistics.
Makoto Kanazawa and Sylvain Salvati. 2010. The copy-
ing power of well-nested multiple context-free gram-
mars. In Fourth International Conference on Language
and Automata Theory and Applications, Trier, Ger-
many.
Makoto Kanazawa. 2009. The pumping lemma for well-
nested multiple context-free languages. In Develop-
ments in Language Theory. 13th International Confer-
ence, DLT 2009, Stuttgart, Germany, June 30?July 3,
2009. Proceedings, volume 5583 of Lecture Notes in
Computer Science, pages 312?325.
Marco Kuhlmann and Mathias M?hl. 2007. Mildly
context-sensitive dependency languages. In 45th An-
nual Meeting of the Association for Computational Lin-
guistics (ACL), pages 160?167.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly non-
projective dependency structures. In 21st International
Conference on Computational Linguistics and 44th An-
nual Meeting of the Association for Computational Lin-
guistics (COLING-ACL), Main Conference Poster Ses-
sions, pages 507?514, Sydney, Australia.
Marco Kuhlmann and Giorgio Satta. 2009. Treebank
grammar techniques for non-projective dependency
parsing. In Twelfth Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL), pages 478?486, Athens, Greece.
Wolfgang Maier and Timm Lichte. 2009. Characterizing
discontinuity in constituent treebanks. In 14th Confer-
ence on Formal Grammar, Bordeaux, France.
Wolfgang Maier and Anders S?gaard. 2008. Treebanks
and mild context-sensitivity. In 13th Conference on
Formal Grammar, pages 61?76, Hamburg, Germany.
Owen Rambow and Giorgio Satta. 1999. Independent
parallelism in finite copying parallel rewriting systems.
Theoretical Computer Science, 223(1?2):87?120.
Giorgio Satta. 1992. Recognition of Linear Context-
Free Rewriting Systems. In 30th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 89?95, Newark, DE, USA.
Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, and
Tadao Kasami. 1991. On Multiple Context-Free Gram-
mars. Theoretical Computer Science, 88(2):191?229.
Stuart M. Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive pars-
ing. Journal of Logic Programming, 24(1?2):3?36.
Stuart M. Shieber. 1985. Evidence against the context-
freeness of natural language. Linguistics and Philoso-
phy, 8(3):333?343.
Klaas Sikkel. 1997. Parsing Schemata: A Framework
for Specification and Analysis of Parsing Algorithms.
Springer.
K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi.
1987. Characterizing structural descriptions produced
by various grammatical formalisms. In 25th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 104?111, Stanford, CA, USA.
284
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 534?543,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
The Importance of Rule Restrictions in CCG
Marco Kuhlmann
Dept. of Linguistics and Philology
Uppsala University
Uppsala, Sweden
Alexander Koller
Cluster of Excellence
Saarland University
Saarbr?cken, Germany
Giorgio Satta
Dept. of Information Engineering
University of Padua
Padua, Italy
Abstract
Combinatory Categorial Grammar (CCG)
is generally construed as a fully lexicalized
formalism, where all grammars use one and
the same universal set of rules, and cross-
linguistic variation is isolated in the lexicon.
In this paper, we show that the weak gener-
ative capacity of this ?pure? form of CCG is
strictly smaller than that of CCG with gram-
mar-specific rules, and of other mildly con-
text-sensitive grammar formalisms, includ-
ing Tree Adjoining Grammar (TAG). Our
result also carries over to a multi-modal
extension of CCG.
1 Introduction
Combinatory Categorial Grammar (CCG) (Steed-
man, 2001; Steedman and Baldridge, 2010) is an
expressive grammar formalism with formal roots
in combinatory logic (Curry et al, 1958) and links
to the type-logical tradition of categorial grammar
(Moortgat, 1997). It has been successfully used for
a wide range of practical tasks, such as data-driven
parsing (Hockenmaier and Steedman, 2002; Clark
and Curran, 2007), wide-coverage semantic con-
struction (Bos et al, 2004), and the modelling of
syntactic priming (Reitter et al, 2006).
It is well-known that CCG can generate lan-
guages that are not context-free (which is neces-
sary to capture natural languages), but can still
be parsed in polynomial time. Specifically, Vijay-
Shanker and Weir (1994) identified a version of
CCG that is weakly equivalent to Tree Adjoining
Grammar (TAG) (Joshi and Schabes, 1997) and
other mildly context-sensitive grammar formalisms,
and can generate non-context-free languages such
as anbncn. The generative capacity of CCG is com-
monly attributed to its flexible composition rules,
which allow it to model more complex word orders
that context-free grammar can.
The discussion of the (weak and strong) gener-
ative capacity of CCG and TAG has recently been
revived (Hockenmaier and Young, 2008; Koller and
Kuhlmann, 2009). In particular, Koller and Kuhl-
mann (2009) have shown that CCGs that are pure
(i.e., they can only use generalized composition
rules, and there is no way to restrict the instances
of these rules that may be used) and first-order
(i.e., all argument categories are atomic) can not
generate anbncn. This shows that the generative
capacity of at least first-order CCG crucially relies
on its ability to restrict rule instantiations, and is at
odds with the general conception of CCG as a fully
lexicalized formalism, in which all grammars use
one and the same set of universal rules. A question
then is whether the result carries over to pure CCG
with higher-order categories.
In this paper, we answer this question to the pos-
itive: We show that the weak generative capacity of
general pure CCG is still strictly smaller than that
of the formalism considered by Vijay-Shanker and
Weir (1994); composition rules can only achieve
their full expressive potential if their use can be
restricted. Our technical result is that every lan-
guage L that can be generated by a pure CCG has
a context-free sublanguage L0  L such that every
string in L is a permutation of a string in L0, and
vice versa. This means that anbncn, for instance,
cannot be generated by pure CCG, as it does not
have any (non-trivial) permutation-equivalent sub-
languages. Conversely, we show that there are still
languages that can be generated by pure CCG but
not by context-free grammar.
We then show that our permutation language
lemma also holds for pure multi-modal CCG as
defined by Baldridge and Kruijff (2003), in which
the use of rules can be controlled through the lex-
icon entries by assigning types to slashes. Since
this extension was intended to do away with
the need for grammar-specific rule restrictions, it
comes as quite a surprise that pure multi-modal
534
CCG in the style of Baldridge and Kruijff (2003) is
still less expressive than the CCG formalism used
by Vijay-Shanker and Weir (1994). This means that
word order in CCG cannot be fully lexicalized with
the current formal tools; some ordering constraints
must be specified via language-specific combina-
tion rules and not in lexicon entries. On the other
hand, as pure multi-modal CCG has been success-
fully applied to model the syntax of a variety of
natural languages, another way to read our results
is as contributions to a discussion about the exact
expressiveness needed to model natural language.
The remainder of this paper is structured as fol-
lows. In Section 2, we introduce the formalism
of pure CCG that we consider in this paper, and
illustrate the relevance of rule restrictions. We then
study the generative capacity of pure CCG in Sec-
tion 3; this section also presents our main result. In
Section 4, we show that this result still holds for
multi-modal CCG. Section 5 concludes the paper
with a discussion of the relevance of our findings.
2 Combinatory Categorial Grammar
We start by providing formal definitions for cat-
egories, syntactic rules, and grammars, and then
discuss the relevance of rule restrictions for CCG.
2.1 Categories
Given a finite set A of atomic categories, the set of
categories over A is the smallest set C such that
A  C , and .x=y/; .xny/ 2 C whenever x; y 2 C .
A category x=y represents a function that seeks a
string with category y to the right (indicated by the
forward slash) and returns a new string with cat-
egory x; a category xny instead seeks its argument
to the left (indicated by the backward slash). In
the remainder of this paper, we use lowercase sans-
serif letters such as x; y; z as variables for categor-
ies, and the vertical bar j as a variable for slashes.
In order to save some parentheses, we understand
slashes as left-associative operators, and write a
category such as .x=y/nz as x=ynz.
The list of arguments of a category c is defined
recursively as follows: If c is atomic, then it has no
arguments. If c D xjy for some categories x and y,
then the arguments of c are the slashed category jy,
plus the arguments of x. We number the arguments
of a category from outermost to innermost. The
arity of a category is the number of its arguments.
The target of a category c is the atomic category
that remains when stripping c of its arguments.
x=y y ) x forward application >
y xny ) x backward application <
x=y y=z ) x=z forward harmonic composition >B
ynz xny ) xnz backward harmonic composition <B
x=y ynz ) xnz forward crossed composition >B
y=z xny ) x=z backward crossed composition <B
Figure 1: The core set of rules of CCG.
2.2 Rules
The syntactic rules of CCG are directed versions
of combinators in the sense of combinatory logic
(Curry et al, 1958). Figure 1 lists a core set of
commonly assumed rules, derived from functional
application and the B combinator, which models
functional composition. When talking about these
rules, we refer to the premise containing the argu-
ment jy as the primary premise, and to the other
premise as the secondary premise of the rule.
The rules in Figure 1 can be generalized into
composition rules of higher degrees. These are
defined as follows, where n  0 and ? is a variable
for a sequence of n arguments.
x=y y? ) x? generalized forward composition >n
y? xny ) x? generalized backward composition <n
We call the value n the degree of the composition
rule. Note that the rules in Figure 1 are the special
cases for n D 0 and n D 1.
Apart from the core rules given in Figure 1, some
versions of CCG also use rules derived from the S
and T combinators of combinatory logic, called
substitution and type-raising, the latter restricted
to the lexicon. However, since our main point of
reference in this paper, the CCG formalism defined
by Vijay-Shanker and Weir (1994), does not use
such rules, we will not consider them here, either.
2.3 Grammars and Derivations
With the set of rules in place, we can define a
pure combinatory categorial grammar (PCCG) as
a construct G D .A;?;L; s/, where A is an alpha-
bet of atomic categories, s 2 A is a distinguished
atomic category called the final category, ? is a
finite set of terminal symbols, and L is a finite rela-
tion between symbols in ? and categories over A,
called the lexicon. The elements of the lexicon L
are called lexicon entries, and we represent them
using the notation  ` x, where  2 ? and x
is a category over A. A category that occurs in a
lexicon entry is called a lexical category.
535
A derivation in a grammar G can be represen-
ted as a derivation tree as follows. Given a string
w 2 ?, we choose a lexicon entry for each oc-
currence of a symbol in w, line up the respective
lexical categories from left to right, and apply ad-
missible rules to adjacent pairs of categories. After
the application of a rule, only the conclusion is
available for future applications. We iterate this
process until we end up with a single category. The
string w is called the yield of the resulting deriva-
tion tree. A derivation tree is complete, if the last
category is the final category of G. The language
generated by G, denoted by L.G/, is formed by
the yields of all complete derivation trees.
2.4 Degree Restrictions
Work on CCG generally assumes an upper bound
on the degree of composition rules that can be used
in derivations. We also employ this restriction, and
only consider grammars with compositions of some
bounded (but arbitrary) degree n  0.1 CCG with
unbounded-degree compositions is more express-
ive than bounded-degree CCG or TAG (Weir and
Joshi, 1988).
Bounded-degree grammars have a number of
useful properties, one of which we mention here.
The following lemma rephrases Lemma 3.1 in
Vijay-Shanker and Weir (1994).
Lemma 1 For every grammar G, every argument
in a derivation ofG is the argument of some lexical
category of G.
As a consequence, there is only a finite number
of categories that can occur as arguments in some
derivation. In the presence of a bound on the degree
of composition rules, this implies the following:
Lemma 2 For every grammar G, there is a finite
number of categories that can occur as secondary
premises in derivations of G.
Proof. The arity of a secondary premise c can be
written as mC n, where m is the arity of the first
argument of the corresponding primary premise,
and n is the degree of the rule applied. Since each
argument is an argument of some lexical category
of G (Lemma 1), and since n is assumed to be
bounded, both m and n are bounded. Hence, there
is a bound on the number of choices for c. 
Note that the number of categories that can occur
as primary premises is generally unbounded even
in a grammar with bounded degree.
1For practical grammars, n  4.
2.5 Rule Restrictions
The rule set of pure CCG is universal: the differ-
ence between the grammars of different languages
should be restricted to different choices of categor-
ies in the lexicon. This is what makes pure CCG
a lexicalized grammar formalism (Steedman and
Baldridge, 2010). However, most practical CCG
grammars rely on the possibility to exclude or re-
strict certain rules. For example, Steedman (2001)
bans the rule of forward crossed composition from
his grammar of English, and stipulates that the rule
of backward crossed composition may be applied
only if both of its premises share the common tar-
get category s, representing sentences. Exclusions
and restrictions of rules are also assumed in much
of the language-theoretic work on CCG. In partic-
ular, they are essential for the formalism used in
the aforementioned equivalence proof for CCG and
TAG (Vijay-Shanker and Weir, 1994).
To illustrate the formal relevance of rule restric-
tions, suppose that we wanted to write a pure CCG
that generates the language
L3 D f anbncn j n  1 g ,
which is not context-free. An attempt could be
G1 D .f s; a; b; c g; f a; b; c g; L; s/ ,
where the lexicon L is given as follows:
a ` a , b ` s=cna , b ` b=cna ,
b ` s=c=bna , b ` s=c=bna , c ` c .
From a few sample derivations like the one given
in Figure 2a, we can convince ourselves that G1
generates all strings of the form anbncn, for any
n  1. However, a closer inspection reveals that it
also generates other, unwanted strings?in partic-
ular, strings of the form .ab/ncn, as witnessed by
the derivation given in Figure 2b.
Now suppose that we would have a way to only
allow those instances of generalized composition in
which the secondary premise has the form b=c=bna
or b=cna. Then the compositions
b=c=b b=c
b=c=c >
1 and s=c=b b=c
s=c=c >
1
would be disallowed, and it is not hard to see
that G1 would generate exactly anbncn.
As we will show in this paper, our attempt to
capture L3 with a pure CCG grammar failed not
only because we could not think of one: L3 cannot
be generated by any pure CCG.
536
a...................
a
a...........
a
a...
a
b...
s=c=bna
b.......
b=c=bna
b...............
b=cna
c.......................
c
c...........................
c
c...............................
c
<0
s=c=b
>3
s=c=c=bna
<0
s=c=c=b
>2
s=c=c=cna
<0
s=c=c=c
>0
s=c=c
>0
s=c
>0
s
(a) Derivation of the string aaabbbccc.
a...........
a
b...........
s=c=bna
a...
a
b...
b=c=bna
a...
a
b...
b=cna
c...........
c
c...................
c
c.......................
c
<0
s=c=b
<0
b=c=b
<0
b=c
>1
b=c=c
>0
b=c
>1
s=c=c
>0
s=c
>0
s
(b) Derivation of the string abababccc.
Figure 2: Two derivations of the grammar G1.
3 The Generative Capacity of Pure CCG
We will now develop a formal argument showing
that rule restrictions increase the weak generative
capacity of CCG. We will first prove that pure CCG
is still more expressive than context-free grammar.
We will then spend the rest of this section working
towards the result that pure CCG is strictly less
expressive than CCG with rule restrictions. Our
main technical result will be the following:
Theorem 1 Every language that can be generated
by a pure CCG has a Parikh-equivalent context-free
sublanguage.
Here, two languages L and L0 are called Parikh-
equivalent if every string in L is the permutation
of a string in L0 and vice versa.
3.1 CFG ? PCCG
Proposition 1 The class of languages generated
by pure CCG properly includes the class of context-
free languages.
Proof. To see the inclusion, it suffices to note that
pure CCG when restricted to application rules is
the same as AB-grammar, the classical categorial
formalism investigated by Ajdukiewicz and Bar-
Hillel (Bar-Hillel et al, 1964). This formalism is
weakly equivalent to context-free grammar.
To see that the inclusion is proper, we can go
back to the grammarG1 that we gave in Section 2.5.
We have already discussed that the language L3 is
included inL.G1/. We can also convince ourselves
that all strings generated by the grammar G1 have
an equal number of as, bs and cs. Consider now
the regular language R D abc. From our ob-
servations, it follows that L.G1/\R D L3. Since
context-free languages are closed under intersec-
tion with regular languages, we find that L.G1/
can be context-free only if L3 is. Since L3 is not
context-free, we therefore conclude that L.G1/ is
not context-free, either. 
Two things are worth noting. First, our result shows
that the ability of CCG to generate non-context-free
languages does not hinge on the availability of sub-
stitution and type-raising rules: The derivations
of G1 only use generalized compositions. Neither
does it require the use of functional argument cat-
egories: The grammarG1 is first-order in the sense
of Koller and Kuhlmann (2009).
Second, it is important to note that if the com-
position degree n is restricted to 0 or 1, pure CCG
actually collapses to context-free expressive power.
This is clear for n D 0 because of the equivalence
to AB grammar. For n D 1, observe that the arity
of the result of a composition is at most as high as
537
that of each premise. This means that the arity of
any derived category is bounded by the maximal
arity of lexical categories in the grammar, which
together with Lemma 1 implies that there is only
a finite set of derivable categories. The set of all
valid derivations can then be simulated by a con-
text-free grammar. In the presence of rules with
n  2, the arities of derived categories can grow
unboundedly.
3.2 Active and Inactive Arguments
In the remainder of this section, we will develop
the proof of Theorem 1, and use it to show that the
generative capacity of PCCG is strictly smaller than
that of CCG with rule restrictions. For the proof,
we adopt a certain way to view the information
flow in CCG derivations. Consider the following
instance of forward harmonic composition:
a=b b=c ) a=c
This rule should be understood as obtaining its con-
clusion a=c from the primary premise a=b by the
removal of the argument =b and the subsequent
transfer of the argument =c from the secondary
premise. With this picture in mind, we will view
the two occurrences of =c in the secondary premise
and in the conclusion as two occurrences of one
and the same argument. Under this perspective,
in a given derivation, an argument has a lifespan
that starts in a lexical category and ends in one
of two ways: either in the primary or in the sec-
ondary premise of a composition rule. If it ends
in a primary premise, it is because it is matched
against a subcategory of the corresponding second-
ary premise; this is the case for the argument =b
in the example above. We will refer to such argu-
ments as active. If an argument ends its life in a
secondary premise, it is because it is consumed as
part of a higher-order argument. This is the case
for the argument =c in the secondary premise of
the following rule instance:
a=.b=c/ b=c=d ) a=d
(Recall that we assume that slashes are left-associ-
ative.) We will refer to such arguments as inactive.
Note that the status of an argument as either active
or inactive is not determined by the grammar, but
depends on a concrete derivation.
The following lemma states an elementary prop-
erty in connection with active and inactive argu-
ments, which we will refer to as segmentation:
Lemma 3 Every category that occurs in a CCG
derivation has the general form a??, where a is an
atomic category, ? is a sequence of inactive argu-
ments, and ? is a sequence of active arguments.
Proof. The proof is by induction on the depth of a
node in the derivation. The property holds for the
root (which is labeled with the final category), and
is transferred from conclusions to premises. 
3.3 Transformation
The fundamental reason for why the example gram-
mar G1 from Section 2.5 overgenerates is that in
the absence of rule restrictions, we have no means
to control the point in a derivation at which a cat-
egory combines with its arguments. Consider the
examples in Figure 2: It is because we cannot en-
sure that the bs finish combining with the other bs
before combining with the cs that the undesirable
word order in Figure 2b has a derivation. To put
it as a slogan: Permuting the words allows us to
saturate arguments prematurely.
In this section, we show that this property applies
to all pure CCGs. More specifically, we show that,
in a derivation of a pure CCG, almost all active
arguments of a category can be saturated before
that category is used as a secondary premise; at
most one active argument must be transferred to
the conclusion of that premise. Conversely, any
derivation that still contains a category with at least
two active arguments can be transformed into a
new derivation that brings us closer to the special
property just characterized.
We formalize this transformation by means of a
system of rewriting rules in the sense of Baader and
Nipkow (1998). The rules are given in Figure 3. To
see how they work, let us consider the first rule, R1;
the other ones are symmetric. This rules states that,
whenever we see a derivation in which a category
of the form x=y (here marked as A) is combined
with a category of the form y?=z (marked as B),
and the result of this combination is combined with
a category of the form z (C), then the resulting
category can also be obtained by ?rotating? the de-
rivation to first saturate =z by combining B with C,
and only then do the combination with A. When ap-
plying these rotations exhaustively, we end up with
a derivation in which almost all active arguments of
a category are saturated before that category is used
as a secondary premise. Applying the transform-
ation to the derivation in Figure 2a, for instance,
yields the derivation in Figure 2b.
We need the following result for some of the
lemmas we prove below. We call a node in a deriv-
538
A x=y B y?=z
x?=z C z
x?
R1
H) x=y
y?=z z
y?
x?
B y?=z A xny
x?=z C z
x?
R2
H)
y?=z z
y? xny
x?
C z
A x=y B y?nz
x?nz
x?
R3
H) x=y
z y?nz
y?
x?
C z
B y?nz A xny
x?nz
x?
R4
H)
z y?nz
y? xny
x?
Figure 3: Rewriting rules used in the transformation. Here,  represents a (possibly empty) sequence of
arguments, and ? represents a sequence of arguments in which the first (outermost) argument is active.
ation critical if its corresponding category contains
more than one active argument and it is the second-
ary premise of a rule. We say that u is a highest
critical node if there is no other critical node whose
distance to the root is shorter.
Lemma 4 If u is a highest critical node, then we
can apply one of the transformation rules to the
grandparent of u.
Proof. Suppose that the category at u has the form
y?=z, where =z is an active argument, and the first
argument in ? is active as well. (The other possible
case, in which the relevant occurrence has the form
y?nz, can be treated symmetrically.) Since u is a
secondary premise, it is involved in an inference of
one of the following two forms:
x=y y?=z
x?=z
y?=z xny
x?=z
Since u is a highest critical node, the conclusion
of this inference is not a critical node itself; in
particular, it is not a secondary premise. Therefore,
the above inferences can be extended as follows:
x=y y?=z
x?=z z
x?
y?=z xny
x?=z z
x?
These partial derivations match the left-hand side of
the rewriting rules R1 and R2, respectively. Hence,
we can apply a rewriting rule to the derivation. 
We now show that the transformation is well-
defined, in the sense that it terminates and trans-
forms derivations of a grammar G into new deriva-
tions of G.
Lemma 5 The rewriting of a derivation tree ends
after a finite number of steps.
Proof. We assign natural numbers to the nodes
of a derivation tree as follows. Each leaf node
is assigned the number 0. For an inner node u,
which corresponds to the conclusion of a composi-
tion rule, let m; n be the numbers assigned to the
nodes corresponding to the primary and second-
ary premise, respectively. Then u is assigned the
number 1C 2mCn. Suppose now that we have as-
sociated premise A with the number x, premise B
with the number y, and premise C with the num-
ber z. It is then easy to verify that the conclusion
of the partial derivation on the left-hand side of
each rule has the value 3 C 4x C 2y C z, while
the conclusion of the right-hand side has the value
2C 2x C 2y C z. Thus, each step decreases the
value of a derivation tree under our assignment by
the amount 1C 2x. Since this value is positive for
all choices of x, the rewriting ends after a finite
number of steps. 
To convince ourselves that our transformation does
not create ill-formed derivations, we need to show
that none of the rewriting rules necessitates the use
of composition operations whose degree is higher
than the degree of the operations used in the ori-
ginal derivation.
Lemma 6 Applying the rewriting rules from the
top down does not increase the degree of the com-
position operations.
Proof. The first composition rule used in the left-
hand side of each rewriting rule has degree j?j C 1,
the second rule has degree j j; the first rule used in
the right-hand side has degree j j, the second rule
has degree j?jC j j. To prove the claim, it suffices
to show that j j  1. This is a consequence of the
following two observations.
1. In the category x? , the arguments in  occur
on top of the arguments in ?, the first of which is
active. Using the segmentation property stated in
Lemma 3, we can therefore infer that  does not
contain any inactive arguments.
539
2. Because we apply rules top-down, premise B
is a highest critical node in the derivation (by
Lemma 4). This means that the category at
premise C contains at most one active argument;
otherwise, premise C would be a critical node
closer to the root than premise B. 
We conclude that, if we rewrite a derivation d of G
top-down until exhaustion, then we obtain a new
valid derivation d 0. We call all derivations d 0 that
we can build in this way transformed. It is easy to
see that a derivation is transformed if and only if it
contains no critical nodes.
3.4 Properties of Transformed Derivations
The special property established by our transform-
ation has consequences for the generative capacity
of pure CCG. In particular, we will now show that
the set of all transformed derivations of a given
grammar yields a context-free language. The cru-
cial lemma is the following:
Lemma 7 For every grammar G, there is some
k  0 such that no category in a transformed
derivation of G has arity greater than k.
Proof. The number of inactive arguments in the
primary premise of a rule does not exceed the num-
ber of inactive arguments in the conclusion. In
a transformed derivation, a symmetric property
holds for active arguments: Since each second-
ary premise contains at most one active argument,
the number of active arguments in the conclusion
of a rule is not greater than the number of act-
ive arguments in its primary premise. Taken to-
gether, this implies that the arity of a category that
occurs in a transformed derivation is bounded by
the sum of the maximal arity of a lexical category
(which bounds the number of active arguments),
and the maximal arity of a secondary premise
(which bounds the number of inactive arguments).
Both of these values are bounded in G. 
Lemma 8 The yields corresponding to the set of
all transformed derivations of a pure CCG form a
context-free language.
Proof. Let G be a pure CCG. We construct a con-
text-free grammar GT that generates the yields of
the set of all transformed derivations of G.
As the set of terminals of GT , we use the set of
terminals ofG. To form the set of nonterminals, we
take all categories that can occur in a transformed
derivation of G, and mark each argument as either
?active? (C) or ?inactive? ( ), in all possible ways
that respect the segmentation property stated in
Lemma 3. Note that, because of Lemma 7 and
Lemma 1, the set of nonterminals is finite. As the
start symbol, we use s, the final category of G.
The set of productions of GT is constructed as
follows. For each lexicon entry  ` c of G, we in-
clude all productions of the form x !  , where x
is some marked version of c. These productions
represent all valid guesses about the activity of the
arguments of c during a derivation of G. The re-
maining productions encode all valid instantiations
of composition rules, keeping track of active and
inactive arguments to prevent derivations with crit-
ical nodes. More specifically, they have the form
x? ! x=yC y? or x? ! y? xnyC ,
where the arguments in the y-part of the secondary
premise are all marked as inactive, the sequence ?
contains at most one argument marked as active,
and the annotations of the left-hand side nonter-
minal are copied over from the corresponding an-
notations on the right-hand side.
The correctness of the construction ofGT can be
proved by induction on the length of a transformed
derivation of G on the one hand, and the length of
a derivation of GT on the other hand. 
3.5 PCCG ? CCG
We are now ready to prove our main result, repeated
here for convenience.
Theorem 1 Every language that can be generated
by a pure CCG grammar has a Parikh-equivalent
context-free sublanguage.
Proof. Let G be a pure CCG, and let LT be the
set of yields of the transformed derivations of G.
Inspecting the rewriting rules, it is clear that every
string of L.G/ is the permutation of a string in LT :
the transformation only rearranges the yields. By
Lemma 8, we also know that LT is context-free.
Since every transformed derivation is a valid deriv-
ation of G, we have LT  L.G/. 
As an immediate consequence, we find:
Proposition 2 The class of languages generated
by pure CCG cannot generate all languages that
can be generated by CCG with rule restrictions.
Proof. The CCG formalism considered by Vijay-
Shanker and Weir (1994) can generate the non-con-
text-free language L3. However, the only Parikh-
equivalent sublanguage of that language isL3 itself.
From Theorem 1, we therefore conclude that L3
cannot be generated by pure CCG. 
540
In the light of the equivalence result established
by Vijay-Shanker and Weir (1994), this means that
pure CCG cannot generate all languages that can
be generated by TAG.
4 Multi-Modal CCG
We now extend Theorem 1 to multi-modal CCG.
We will see that at least for a popular version
of multi-modal CCG, the B&K-CCG formalism
presented by Baldridge and Kruijff (2003), the
proof can be adapted quite straightforwardly. This
means that even B&K-CCG becomes less express-
ive when rule restrictions are disallowed.
4.1 Multi-Modal CCG
The term ?multi-modal CCG? (MM-CCG) refers to
a family of extensions to CCG which attempt to
bring some of the expressive power of Categorial
Type Logic (Moortgat, 1997) into CCG. Slashes in
MM-CCG have slash types, and rules can be restric-
ted to only apply to arguments that have slashes
of the correct type. The idea behind this extension
is that many constraints that in ordinary CCG can
only be expressed in terms of rule restrictions can
now be specified in the lexicon entries by giving
the slashes the appropriate types.
The most widely-known version of multi-modal
CCG is the formalism defined by Baldridge and
Kruijff (2003) and used by Steedman and Baldridge
(2010); we refer to it as B&K-CCG. This formalism
uses an inventory of four slash types, f?;;?;  g,
arranged in a simple type hierarchy: ? is the most
general type,  the most specific, and  and ? are
in between. Every slash in a B&K-CCG lexicon is
annotated with one of these slash types.
The combinatory rules in B&K-CCG, given in
Figure 4, are defined to be sensitive to the slash
types. In particular, slashes with the types ? and 
can only be eliminated by harmonic and crossed
compositions, respectively.2 Thus, a grammar
writer can constrain the application of harmonic
and crossed composition rules to certain categor-
ies by assigning appropriate types to the slashes
of this category in the lexicon. Application rules
apply to slashes of any type. As before, we call
an MM-CCG grammar pure if it only uses applic-
ation and generalized compositions, and does not
provide means to restrict rule applications.
2Our definitions of generalized harmonic and crossed com-
position are the same as the ones used by Hockenmaier and
Young (2008), but see the discussion in Section 4.3.
x=?y y ) x forward application
y xn?y ) x backward application
x=?y y=?z? ) x=?z? forward harmonic composition
x=y ynz? ) xnz? forward crossed composition
yn?z? xn?y ) xn?z? backward harmonic composition
y=z? xny ) x=z? backward crossed composition
Figure 4: Rules in B&K-CCG.
4.2 Rule Restrictions in B&K-CCG
We will now see what happens to the proof of The-
orem 1 in the context of pure B&K-CCG. There
is only one point in the entire proof that could be
damaged by the introduction of slash types, and
that is the result that if a transformation rule from
Figure 3 is applied to a correct derivation, then the
result is also grammatical. For this, it must not
only be the case that the degree on the composition
operations is preserved (Lemma 6), but also that
the transformed derivation remains consistent with
the slash types. Slash types make the derivation
process sensitive to word order by restricting the
use of compositions to categories with the appropri-
ate type, and the transformation rules permute the
order of the words in the string. There is a chance
therefore that a transformed derivation might not
be grammatical in B&K-CCG.
We now show that this does not actually happen,
for rule R3; the other three rules are analogous.
Using s1; s2; s3 as variables for the relevant slash
types, rule R3 appears in B&K-CCG as follows:
z
x=s1y yjs2w?ns3z
xjs2w?ns3z
xjs2w?
R3
H) x=s1y
z yjs2w?ns3z
yjs2w?
xjs2w?
Because the original derivation is correct, we know
that, if the slash of w is forward, then s1 and s2 are
subtypes of ?; if the slash is backward, they are
subtypes of . A similar condition holds for s3 and
the first slash in  ; if  is empty, then s3 can be
anything because the second rule is an application.
After the transformation, the argument =s1y is
used to compose with yjs2w? . The direction of
the slash in front of the w is the same as before,
so the (harmonic or crossed) composition is still
compatible with the slash types s1 and s2. An
analogous argument shows that the correctness of
combining ns3z with  carries over from the left to
the right-hand side. Thus the transformation maps
grammatical derivations into grammatical deriva-
tions. The rest of the proof in Section 3 continues
to work literally, so we have the following result:
541
Theorem 2 Every language that can be generated
by a pure B&K-CCG grammar contains a Parikh-
equivalent context-free sublanguage.
This means that pure B&K-CCG is just as unable
to generate L3 as pure CCG is. In other words,
the weak generative capacity of CCG with rule
restrictions, and in particular that of the formalism
considered by Vijay-Shanker and Weir (1994), is
strictly greater than the generative capacity of pure
B&K-CCG?although we conjecture (but cannot
prove) that pure B&K-CCG is still more expressive
than pure non-modal CCG.
4.3 Towards More Expressive MM-CCGs
To put the result of Theorem 2 into perspective, we
will now briefly consider ways in which B&K-CCG
might be modified in order to obtain a pure multi-
modal CCG that is weakly equivalent to CCG in
the style of Vijay-Shanker and Weir (1994). Such
a modification would have to break the proof in
Section 4.2, which is harder than it may seem at
first glance. For instance, simply assuming a more
complex type system will not do it, because the
arguments ns3z and =s1y are eliminated using the
same rules in the original and the transformed deriv-
ations, so if the derivation step was valid before, it
will still be valid after the transformation. Instead,
we believe that it is necessary to make the composi-
tion rules sensitive to the categories inside ? and 
instead of only the arguments ns3z and =s1y, and
we can see two ways how to do this.
First, one could imagine a version of multi-
modal CCG with unary modalities that can be used
to mark certain category occurrences. In such an
MM-CCG, the composition rules for a certain slash
type could be made sensitive to the presence or
absence of unary modalities in ?. Say for instance
that the slash type s1 in the modalized version of
R3 in Section 4.2 would require that no category in
the secondary argument is marked with the unary
modality ??, but ? contains a category marked
with ??. Then the transformed derivation would
be ungrammatical.
A second approach concerns the precise defin-
ition of the generalized composition rules, about
which there is a surprising degree of disagreement.
We have followed Hockenmaier and Young (2008)
in classifying instances of generalized forward
composition as harmonic if the innermost slash of
the secondary argument is forward and crossed if
it is backward. However, generalized forward com-
position is sometimes only accepted as harmonic
if all slashes of the secondary argument are for-
ward (see e.g. Baldridge (2002) (40, 41), Steedman
(2001) (19)). At the same time, based on the prin-
ciple that CCG rules should be derived from proofs
of Categorial Type Logic as Baldridge (2002) does,
it can be argued that generalized composition rules
of the form x=y y=znw ) x=znw, which we
have considered as harmonic, should actually be
classified as crossed, due to the presence of a slash
of opposite directionality in front of the w. This
definition would break our proof. Thus our res-
ult might motivate further research on the ?correct?
definition of generalized composition rules, which
might then strengthen the generative capacity of
pure MM-CCG.
5 Conclusion
In this paper, we have shown that the weak generat-
ive capacity of pure CCG and even pure B&K-CCG
crucially depends on the ability to restrict the ap-
plication of individual rules. This means that these
formalisms cannot be fully lexicalized, in the sense
that certain languages can only be described by
selecting language-specific rules.
Our result generalizes Koller and Kuhlmann?s
(2009) result for pure first-order CCG. Our proof
is not as different as it looks at first glance, as
their construction of mapping a CCG derivation to
a valency tree and back to a derivation provides a
different transformation on derivation trees. Our
transformation is also technically related to the nor-
mal form construction for CCG parsing presented
by Eisner (1996).
Of course, at the end of the day, the issue that is
more relevant to computational linguistics than a
formalism?s ability to generate artificial languages
such as L3 is how useful it is for modeling natural
languages. CCG, and multi-modal CCG in partic-
ular, has a very good track record for this. In this
sense, our formal result can also be understood as
a contribution to a discussion about the expressive
power that is needed to model natural languages.
Acknowledgments
We have profited enormously from discussions with
Jason Baldridge and Mark Steedman, and would
also like to thank the anonymous reviewers for their
detailed comments.
542
References
Franz Baader and Tobias Nipkow. 1998. Term Rewrit-
ing and All That. Cambridge University Press.
Jason Baldridge and Geert-Jan M. Kruijff. 2003.
Multi-modal Combinatory Categorial Grammar.
In Proceedings of the Tenth Conference of the
European Chapter of the Association for Compu-
tational Linguistics (EACL), pages 211?218, Bud-
apest, Hungary.
Jason Baldridge. 2002. Lexically Specified Deriva-
tional Control in Combinatory Categorial Grammar.
Ph.D. thesis, University of Edinburgh.
Yehoshua Bar-Hillel, Haim Gaifman, and Eli Shamir.
1964. On categorial and phrase structure gram-
mars. In Language and Information: Selected Es-
says on their Theory and Application, pages 99?115.
Addison-Wesley.
Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-
coverage semantic representations from a CCG
parser. In Proceedings of the 20th International
Conference on Computational Linguistics (COL-
ING), pages 176?182, Geneva, Switzerland.
Stephen Clark and James Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4).
Haskell B. Curry, Robert Feys, and William Craig.
1958. Combinatory Logic. Volume 1. Studies in
Logic and the Foundations of Mathematics. North-
Holland.
Jason Eisner. 1996. Efficient normal-form parsing
for combinatory categorial grammar. In Proceed-
ings of the 34th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 79?86,
Santa Cruz, CA, USA.
Julia Hockenmaier and Mark Steedman. 2002. Gen-
erative models for statistical parsing with Combin-
atory Categorial Grammar. In Proceedings of the
40th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 335?342, Phil-
adelphia, USA.
Julia Hockenmaier and Peter Young. 2008. Non-local
scrambling: the equivalence of TAG and CCG revis-
ited. In Proceedings of the 9th Internal Workshop on
Tree Adjoining Grammars and Related Formalisms
(TAG+9), T?bingen, Germany.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
Adjoining Grammars. In Grzegorz Rozenberg and
Arto Salomaa, editors, Handbook of Formal Lan-
guages, volume 3, pages 69?123. Springer.
Alexander Koller and Marco Kuhlmann. 2009. De-
pendency trees and the strong generative capacity of
CCG. In Proceedings of the Twelfth Conference of
the European Chapter of the Association for Compu-
tational Linguistics (EACL), pages 460?468, Athens,
Greece.
Michael Moortgat. 1997. Categorial type logics. In
Handbook of Logic and Language, chapter 2, pages
93?177. Elsevier.
David Reitter, Julia Hockenmaier, and Frank Keller.
2006. Priming effects in combinatory categorial
grammar. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 308?316, Sydney, Australia.
Mark Steedman and Jason Baldridge. 2010. Combin-
atory categorial grammar. In R. Borsley and K. Bor-
jars, editors, Non-Transformational Syntax. Black-
well. Draft 7.0, to appear.
Mark Steedman. 2001. The Syntactic Process. MIT
Press.
K. Vijay-Shanker and David J. Weir. 1994. The equi-
valence of four extensions of context-free grammars.
Mathematical Systems Theory, 27(6):511?546.
David J. Weir and Aravind K. Joshi. 1988. Combinat-
ory categorial grammars: Generative power and rela-
tionship to linear context-free rewriting systems. In
Proceedings of the 26th Annual Meeting of the As-
sociation for Computational Linguistics, pages 278?
285, Buffalo, NY, USA.
543
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 673?682,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Dynamic Programming Algorithms
for Transition-Based Dependency Parsers
Marco Kuhlmann
Dept. of Linguistics and Philology
Uppsala University, Sweden
marco.kuhlmann@lingfil.uu.se
Carlos G?mez-Rodr?guez
Departamento de Computaci?n
Universidade da Coru?a, Spain
cgomezr@udc.es
Giorgio Satta
Dept. of Information Engineering
University of Padua, Italy
satta@dei.unipd.it
Abstract
We develop a general dynamic programming
technique for the tabulation of transition-based
dependency parsers, and apply it to obtain
novel, polynomial-time algorithms for parsing
with the arc-standard and arc-eager models. We
also show how to reverse our technique to ob-
tain new transition-based dependency parsers
from existing tabular methods. Additionally,
we provide a detailed discussion of the con-
ditions under which the feature models com-
monly used in transition-based parsing can be
integrated into our algorithms.
1 Introduction
Dynamic programming algorithms, also known as
tabular or chart-based algorithms, are at the core of
many applications in natural language processing.
When applied to formalisms such as context-free
grammar, they provide polynomial-time parsing al-
gorithms and polynomial-space representations of
the resulting parse forests, even in cases where the
size of the search space is exponential in the length
of the input string. In combination with appropri-
ate semirings, these packed representations can be
exploited to compute many values of interest for ma-
chine learning, such as best parses and feature expec-
tations (Goodman, 1999; Li and Eisner, 2009).
In this paper, we follow the line of investigation
started by Huang and Sagae (2010) and apply dy-
namic programming to (projective) transition-based
dependency parsing (Nivre, 2008). The basic idea,
originally developed in the context of push-down
automata (Lang, 1974; Tomita, 1986; Billot and
Lang, 1989), is that while the number of computa-
tions of a transition-based parser may be exponential
in the length of the input string, several portions of
these computations, when appropriately represented,
can be shared. This can be effectively implemented
through dynamic programming, resulting in a packed
representation of the set of all computations.
The contributions of this paper can be summarized
as follows. We provide (declarative specifications of)
novel, polynomial-time algorithms for two widely-
used transition-based parsing models: arc-standard
(Nivre, 2004; Huang and Sagae, 2010) and arc-eager
(Nivre, 2003; Zhang and Clark, 2008). Our algorithm
for the arc-eager model is the first tabular algorithm
for this model that runs in polynomial time. Both
algorithms are derived using the same general tech-
nique; in fact, we show that this technique is applica-
ble to all transition-parsing models whose transitions
can be classified into ?shift? and ?reduce? transitions.
We also show how to reverse the tabulation to de-
rive a new transition system from an existing tabular
algorithm for dependency parsing, originally devel-
oped by G?mez-Rodr?guez et al (2008). Finally, we
discuss in detail the role of feature information in
our algorithms, and in particular the conditions under
which the feature models traditionally used in transi-
tion-based dependency parsing can be integrated into
our framework.
While our general approach is the same as the one
of Huang and Sagae (2010), we depart from their
framework by not representing the computations of
a parser as a graph-structured stack in the sense of
Tomita (1986). We instead simulate computations
as in Lang (1974), which results in simpler algo-
rithm specifications, and also reveals deep similari-
ties between transition-based systems for dependency
parsing and existing tabular methods for lexicalized
context-free grammars.
673
2 Transition-Based Dependency Parsing
We start by briefly introducing the framework of
transition-based dependency parsing; for details, we
refer to Nivre (2008).
2.1 Dependency Graphs
Let w D w0   wn 1 be a string over some fixed
alphabet, where n  1 and w0 is the special token
root. A dependency graph for w is a directed graph
G D .Vw ; A/, where Vw D f0; : : : ; n   1g is the set
of nodes, and A  Vw  Vw is the set of arcs. Each
node in Vw encodes the position of a token in w, and
each arc in A encodes a dependency relation between
two tokens. To denote an arc .i; j / 2 A, we write
i ! j ; here, the node i is the head, and the node j is
the dependent. A sample dependency graph is given
in the left part of Figure 2.
2.2 Transition Systems
A transition system is a structure S D .C; T; I; Ct /,
where C is a set of configurations, T is a finite set
of transitions, which are partial functions t WC * C ,
I is a total initialization function mapping each input
string to a unique initial configuration, and Ct  C
is a set of terminal configurations.
The transition systems that we investigate in this
paper differ from each other only with respect to
their sets of transitions, and are identical in all other
aspects. In each of them, a configuration is de-
fined relative to a string w as above, and is a triple
c D .; ?; A/, where  and ? are disjoint lists of
nodes from Vw , called stack and buffer, respectively,
and A  Vw  Vw is a set of arcs. We denote the
stack, buffer and arc set associated with c by .c/,
?.c/, and A.c/, respectively. We follow a standard
convention and write the stack with its topmost ele-
ment to the right, and the buffer with its first element
to the left; furthermore, we indicate concatenation
in the stack and in the buffer by a vertical bar. The
initialization function maps each string w to the ini-
tial configuration .??; ?0; : : : ; jwj   1?;;/. The set of
terminal configurations contains all configurations of
the form .?0?; ??; A/, where A is some set of arcs.
Given an input string w, a parser based on S pro-
cesses w from left to right, starting in the initial con-
figuration I.w/. At each point, it applies one of
the transitions, until at the end it reaches a terminal
.; i j?;A/ ` . ji; ?; A/ .sh/
. ji jj; ?;A/ ` . jj; ?;A [ fj ! ig/ .la/
. ji jj; ?;A/ ` . ji; ?; A [ fi ! j g/ .ra/
Figure 1: Transitions in the arc-standard model.
configuration; the dependency graph defined by the
arc set associated with that configuration is then re-
turned as the analysis for w. Formally, a computation
of S on w is a sequence  D c0; : : : ; cm, m  0, of
configurations (defined relative to w) in which each
configuration is obtained as the value of the preced-
ing one under some transition. It is called complete
whenever c0 D I.w/, and cm 2 Ct . We note that a
computation can be uniquely specified by its initial
configuration c0 and the sequence of its transitions,
understood as a string over T . Complete computa-
tions, where c0 is fixed, can be specified by their
transition sequences alone.
3 Arc-Standard Model
To introduce the core concepts of the paper, we first
look at a particularly simple model for transition-
based dependency parsing, known as the arc-stan-
dard model. This model has been used, in slightly
different variants, by a number of parsers (Nivre,
2004; Attardi, 2006; Huang and Sagae, 2010).
3.1 Transition System
The arc-standard model uses three types of transi-
tions: Shift (sh) removes the first node in the buffer
and pushes it to the stack. Left-Arc (la) creates a
new arc with the topmost node on the stack as the
head and the second-topmost node as the dependent,
and removes the second-topmost node from the stack.
Right-Arc (ra) is symmetric to Left-Arc in that it
creates an arc with the second-topmost node as the
head and the topmost node as the dependent, and
removes the topmost node.
The three transitions can be formally specified as
in Figure 1. The right half of Figure 2 shows a com-
plete computation of the arc-standard transition sys-
tem, specified by its transition sequence. The picture
also shows the contents of the stack over the course of
the computation; more specifically, column i shows
the stack .ci / associated with the configuration ci .
674
root This news had little effect on the markets
0
1
0 0
1
2
0
2
0
2
3
0
3
0
3
0
3
0
3
54 4
5
0
3
5
0
3
5
0
3
5
6 6 6
77
8
0
3
5
6
8
0
3
5
6
0
3
5
0
3
0
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17
sh sh sh la sh la sh sh la sh sh sh la ra ra ra ra
1 2
0
Figure 2: A dependency tree (left) and a computation generating this tree in the arc-standard system (right).
3.2 Push Computations
The key to the tabulation of transition-based depen-
dency parsers is to find a way to decompose com-
putations into smaller, shareable parts. For the arc-
standard model, as well as for the other transition
systems that we consider in this paper, we base our
decomposition on the concept of push computations.
By this, we mean computations
 D c0; : : : ; cm ; m  1 ;
on some input string w with the following properties:
(P1) The initial stack .c0/ is not modified during
the computation, and is not even exposed after the
first transition: For every 1  i  m, there exists a
non-empty stack i such that .ci / D .c0/ji .
(P2) The overall effect of the computation is to
push a single node to the stack: The stack .cm/ can
be written as .cm/ D .c0/jh, for some h 2 Vw .
We can verify that the computation in Figure 2 is
a push computation. We can also see that it contains
shorter computations that are push computations; one
example is the computation 0 D c1; : : : ; c16, whose
overall effect is to push the node 3. In Figure 2, this
computation is marked by the zig-zag path traced
in bold. The dashed line delineates the stack .c1/,
which is not modified during 0.
Every computation that consists of a single sh tran-
sition is a push computation. Starting from these
atoms, we can build larger push computations by
means of two (partial) binary operations fla and fra,
defined as follows. Let 1 D c10; : : : ; c1m1 and
2 D c20; : : : ; c2m2 be push computations on the
same input string w such that c1m1 D c20. Then
fra.1; 2/ D c10; : : : ; c1m1 ; c21; : : : ; c2m2 ; c ;
where c is obtained from c2m2 by applying the ra
transition. (The operation fla is defined analogously.)
We can verify that fra.1; 2/ is another push com-
putation. For instance, with respect to Figure 2,
fra.1; 2/ D 0. Conversely, we say that the push
computation 0 can be decomposed into the subcom-
putations 1 and 2, and the operation fra.
3.3 Deduction System
Building on the compositional structure of push com-
putations, we now construct a deduction system (in
the sense of Shieber et al (1995)) that tabulates the
computations of the arc-standard model for a given
input string w D w0   wn 1. For 0  i  n, we
shall write ?i to denote the buffer ?i; : : : ; n 1?. Thus,
?0 denotes the full buffer, associated with the initial
configuration I.w/, and ?n denotes the empty buffer,
associated with a terminal configuration c 2 Ct .
Item form. The items of our deduction system
take the form ?i; h; j ?, where 0  i  h < j  n.
The intended interpretation of an item ?i; h; j ? is:
For every configuration c0 with ?.c0/ D ?i , there
exists a push computation  D c0; : : : ; cm such that
?.cm/ D j? , and .cm/ D .c0/jh.
Goal. The only goal item is ?0; 0; n?, asserting
that there exists a complete computation for w.
Axioms. For every stack  , position i < n and
arc set A, by a single sh transition we obtain the
push computation .; ?i ; A/; . ji; ?iC1; A/. There-
fore we can take the set of all items of the form
?i; i; i C 1? as the axioms of our system.
Inference rules. The inference rules parallel the
composition operations fla and fra. Suppose that
we have deduced the items ?i; h1; k? and ?k; h2; j ?,
where 0  i  h1 < k  h2 < j  n. The
item ?i; h1; k? asserts that for every configuration c10
675
Item form: ?i; h; j ? , 0  i  h < j  jwj Goal: ?0; 0; jwj? Axioms: ?i; i; i C 1?
Inference rules:
?i; h1; k? ?k; h2; j ?
?i; h2; j ?
.laI h2 ! h1/
?i; h1; k? ?k; h2; j ?
?i; h1; j ?
.raI h1 ! h2/
Figure 3: Deduction system for the arc-standard model.
with ?.c10/ D ?i , there exists a push computation
1 D c10; : : : ; c1m1 such that ?.c1m1/ D ?k , and
.c1m1/ D .c10/jh1. Using the item ?k; h2; j ?,
we deduce the existence of a second push compu-
tation 2 D c20; : : : ; c2m2 such that c20 D c1m1 ,
?.c2m2/ D j? , and .c2m2/ D .c10/jh1jh2. By
means of fra, we can then compose 1 and 2 into a
new push computation
fra.1; 2/ D c10; : : : ; c1m1 ; c21; : : : ; c2m2 ; c :
Here, ?.c/ D j? , and .c/ D .c10/jh1. Therefore,
we may generate the item ?i; h1; j ?. The inference
rule for la can be derived analogously.
Figure 3 shows the complete deduction system.
3.4 Completeness and Non-Ambiguity
We have informally argued that our deduction sys-
tem is sound. To show completeness, we prove the
following lemma: For all 0  i  h < j  jwj and
every push computation  D c0; : : : ; cm on w with
?.c0/ D ?i , ?.cm/ D j? and .cm/ D .c0/jh, the
item ?i; h; j ? is generated. The proof is by induction
on m, and there are two cases:
m D 1. In this case,  consists of a single sh transi-
tion, h D i , j D i C 1, and we need to show that the
item ?i; i; i C 1? is generated. This holds because this
item is an axiom.
m  2. In this case,  ends with either a la or a ra
transition. Let c be the rightmost configuration in 
that is different from cm and whose stack size is one
larger than the size of .c0/. The computations
1 D c0; : : : ; c and 2 D c; : : : ; cm 1
are both push computations with strictly fewer tran-
sitions than  . Suppose that the last transition in 
is ra. In this case, ?.c/ D ?k for some i < k < j ,
.c/ D .c0/jh with h < k, ?.cm 1/ D j? , and
.cm 1/ D .c0/jhjh0 for some k  h0 < j . By
induction, we may assume that we have generated
items ?i; h; k? and ?k; h0; j ?. Applying the inference
rule for ra, we deduce the item ?i; h; j ?. An analo-
gous argument can be made for fla.
Apart from being sound and complete, our deduc-
tion system also has the property that it assigns at
most one derivation to a given item. To see this,
note that in the proof of the lemma, the choice of c
is uniquely determined: If we take any other con-
figuration c0 that meets the selection criteria, then
the computation  02 D c
0; : : : ; cm 1 is not a push
computation, as it contains c as an intermediate con-
figuration, and thereby violates property P1.
3.5 Discussion
Let us briefly take stock of what we have achieved
so far. We have provided a deduction system capable
of tabulating the set of all computations of an arc-
standard parser on a given input string, and proved
the correctness of this system relative to an interpre-
tation based on push computations. Inspecting the
system, we can see that its generic implementation
takes space in O.jwj3/ and time in O.jwj5/.
Our deduction system is essentially the same as the
one for the CKY algorithm for bilexicalized context-
free grammar (Collins, 1996; G?mez-Rodr?guez et
al., 2008). This equivalence reveals a deep correspon-
dence between the arc-standard model and bilexical-
ized context-free grammar, and, via results by Eisner
and Satta (1999), to head automata. In particular,
Eisner?s and Satta?s ?hook trick? can be applied to
our tabulation to reduce its runtime to O.jwj4/.
4 Adding Features
The main goal with the tabulation of transition-based
dependency parsers is to obtain a representation
based on which semiring values such as the high-
est-scoring computation for a given input (and with
it, a dependency tree) can be calculated. Such com-
putations involve the use of feature information. In
this section, we discuss how our tabulation of the arc-
standard system can be extended for this purpose.
676
?i; h1; kI hx2; x1i; hx1; x3i? W v1 ?k; h2; j I hx1; x3i; hx3; x4i? W v2
?i; h1; j I hx2; x1i; hx1; x3i? W v1 C v2 C hx3; x4i  E?ra
.ra/
?i; h; j I hx2; x1i; hx1; x3i? W v
?j; j; j C 1I hx1; x3i; hx3; wj i? W hx1; x3i  E?sh
.sh/
Figure 4: Extended inference rules under the feature model ? D hs1:w; s0:wi. The annotations indicate how to calculate
a candidate for an update of the Viterbi score of the conclusion using the Viterbi scores of the premises.
4.1 Scoring Computations
For the sake of concreteness, suppose that we want
to score computations based on the following model,
taken from Zhang and Clark (2008). The score of a
computation  is broken down into a sum of scores
score.t; ct / for combinations of a transition t in the
transition sequence associated with  and the config-
uration ct in which t was taken:
score./ D
X
t2
score.t; ct / (1)
The score score.t; ct / is defined as the dot product of
the feature representation of ct relative to a feature
model ? and a transition-specific weight vector E?t :
score.t; ct / D ?.ct /  E?t
The feature model ? is a vector h1; : : : ; ni of
elementary feature functions, and the feature rep-
resentation ?.c/ of a configuration c is a vector
Ex D h1.c/; : : : ; n.c/i of atomic values. Two ex-
amples of feature functions are the word form associ-
ated with the topmost and second-topmost node on
the stack; adopting the notation of Huang and Sagae
(2010), we will write these functions as s0:w and
s1:w, respectively. Feature functions like these have
been used in several parsers (Nivre, 2006; Zhang and
Clark, 2008; Huang et al, 2009).
4.2 Integration of Feature Models
To integrate feature models into our tabulation of
the arc-standard system, we can use extended items
of the form ?i; h; j I ExL; ExR? with the same intended
interpretation as the old items ?i; h; j ?, except that
the initial configuration of the asserted computations
 D c0; : : : ; cm now is required to have the feature
representation ExL, and the final configuration is re-
quired to have the representation ExR:
?.c0/ D ExL and ?.cm/ D ExR
We shall refer to the vectors ExL and ExR as the left-
context vector and the right-context vector of the
computation  , respectively.
We now need to change the deduction rules so that
they become faithful to the extended interpretation.
Intuitively speaking, we must ensure that the feature
values can be computed along the inference rules.
As a concrete example, consider the feature model
? D hs1:w; s0:wi. In order to integrate this model
into our tabulation, we change the rule for ra as in
Figure 4, where x1; : : : ; x4 range over possible word
forms. The shared variable occurrences in this rule
capture the constraints that hold between the feature
values of the subcomputations 1 and 2 asserted
by the premises, and the computations fra.1; 2/
asserted by the conclusion. To illustrate this, suppose
that 1 and 2 are as in Figure 2. Then the three
occurrences of x3 for instance encode that
?s0:w?.c6/ D ?s1:w?.c15/ D ?s0:w?.c16/ D w3 :
We also need to extend the axioms, which cor-
respond to computations consisting of a single sh
transition. The most conservative way to do this is
to use a generate-and-test technique: Extend the ex-
isting axioms by all valid choices of left-context and
right-context vectors, that is, by all pairs ExL; ExR such
that there exists a configuration c with ?.c/ D ExL
and ?.sh.c// D ExR. The task of filtering out use-
less guesses can then be delegated to the deduction
system.
A more efficient way is to only have one axiom, for
the case where c D I.w/, and to add to the deduction
system a new, unary inference rule for sh as in Fig-
ure 4. This rule only creates items whose left-context
vector is the right-context vector of some other item,
which prevents the generation of useless items. In
the following, we take this second approach, which
is also the approach of Huang and Sagae (2010).
677
?i; h; j I hx2; x1i; hx1; x3i? W .p; v/
?j; j; j C 1I hx1; x3i; hx3; wj i? W .p C ; /
.sh/ , where  D hx1; x3i  E?sh
?i; h1; kI hx2; x1i; hx1; x3i? W .p1; v1/ ?k; h2; j I hx1; x3i; hx3; x4i? W .p2; v2/
?i; h1; j I hx2; x1i; hx1; x3i? W .p1 C v2 C Transactions of the Association for Computational Linguistics, 1 (2013) 267?278. Action Editor: Brian Roark.
Submitted 3/2013; Published 7/2013. c?2013 Association for Computational Linguistics.
Efficient Parsing for Head-Split Dependency Trees
Giorgio Satta
Dept. of Information Engineering
University of Padua, Italy
satta@dei.unipd.it
Marco Kuhlmann
Dept. of Linguistics and Philology
Uppsala University, Sweden
marco.kuhlmann@lingfil.uu.se
Abstract
Head splitting techniques have been success-
fully exploited to improve the asymptotic
runtime of parsing algorithms for project-
ive dependency trees, under the arc-factored
model. In this article we extend these tech-
niques to a class of non-projective dependency
trees, called well-nested dependency trees with
block-degree at most 2, which has been previ-
ously investigated in the literature. We define a
structural property that allows head splitting for
these trees, and present two algorithms that im-
prove over the runtime of existing algorithms
at no significant loss in coverage.
1 Introduction
Much of the recent work on dependency parsing has
been aimed at finding a good balance between ac-
curacy and efficiency. For one end of the spectrum,
Eisner (1997) showed that the highest-scoring pro-
jective dependency tree under an arc-factored model
can be computed in timeO.n3/, where n is the length
of the input string. Later work has focused on mak-
ing projective parsing viable under more expressive
models (Carreras, 2007; Koo and Collins, 2010).
At the same time, it has been observed that for
many standard data sets, the coverage of projective
trees is far from complete (Kuhlmann and Nivre,
2006), which has led to an interest in parsing al-
gorithms for non-projective trees. While non-project-
ive parsing under an arc-factored model can be done
in time O.n2/ (McDonald et al, 2005), parsing with
more informed models is intractable (McDonald and
Satta, 2007). This has led several authors to investig-
ate ?mildly non-projective? classes of trees, with the
goal of achieving a balance between expressiveness
and complexity (Kuhlmann and Nivre, 2006).
In this article we focus on a class of mildly non-
projective dependency structures called well-nested
dependency trees with block-degree at most 2. This
class was first introduced by Bodirsky et al (2005),
who showed that it corresponds, in a natural way, to
the class of derivation trees of lexicalized tree-adjoin-
ing grammars (Joshi and Schabes, 1997). While there
are linguistic arguments against the restriction to this
class (Maier and Lichte, 2011; Chen-Main and Joshi,
2010), Kuhlmann and Nivre (2006) found that it has
excellent coverage on standard data sets. Assum-
ing an arc-factored model, well-nested dependency
trees with block-degree  2 can be parsed in time
O.n7/ using the algorithm of Go?mez-Rodr??guez et
al. (2011). Recently, Pitler et al (2012) have shown
that if an additional restriction called 1-inherit is im-
posed, parsing can be done in time O.n6/, without
any additional loss in coverage on standard data sets.
Standard context-free parsing methods, when adap-
ted to the parsing of projective trees, provide O.n5/
time complexity. The O.n3/ time result reported by
Eisner (1997) has been obtained by exploiting more
sophisticated dynamic programming techniques that
?split? dependency trees at the position of their heads,
in order to save bookkeeping. Splitting techniques
have also been exploited to speed up parsing time
for other lexicalized formalisms, such as bilexical
context-free grammars and head automata (Eisner
and Satta, 1999). However, to our knowledge no at-
tempt has been made in the literature to extend these
techniques to non-projective dependency parsing.
In this article we leverage the central idea from
Eisner?s algorithm and extend it to the class of well-
nested dependency trees with block-degree at most 2.
267
We introduce a structural property, called head-split,
that allows us to split these trees at the positions of
their heads. The property is restrictive, meaning that
it reduces the class of trees that can be generated.
However, we show that the restriction to head-split
trees comes at no significant loss in coverage, and it
allows parsing in timeO.n6/, an asymptotic improve-
ment of one order of magnitude over the algorithm
by Go?mez-Rodr??guez et al (2011) for the unrestric-
ted class. We also show that restricting the class of
head-split trees by imposing the already mentioned
1-inherit property does not cause any additional loss
in coverage, and that parsing for the combined class
is possible in time O.n5/, one order of magnitude
faster than the algorithm by Pitler et al (2012) for
the 1-inherit class without the head-split condition.
The above results have consequences also for the
parsing of other related formalisms, such as the
already mentioned lexicalized tree-adjoining gram-
mars. This will be discussed in the final section.
2 Head Splitting
To introduce the basic idea of this article, we briefly
discuss in this section two well-known algorithms for
computing the set of all projective dependency trees
for a given input sentence: the na??ve, CKY-style
algorithm, and the improved algorithm with head
splitting, in the version of Eisner and Satta (1999).1
CKY parsing The CKY-style algorithm works in
a pure bottom-up way, building dependency trees
by combining subtrees. Assuming an input string
w D a1    an, n  1, each subtree t is represented
by means of a finite signature ?i; j; h?, called item,
where i; j are the boundary positions of t ?s span over
w and h is the position of t?s root. This is the only
information we need in order to combine subtrees
under the arc-factored model. Note that the number
of possible signatures is O.n3/.
The main step of the algorithm is displayed in
Figure 1(a). Here we introduce the graphical conven-
tion, used throughout this article, of representing a
subtree by a shaded area, with an horizontal line in-
dicating the spanned fragment of the input string, and
of marking the position of the head by a bullet. The
illustrated step attaches a tree with signature ?k; j; d ?
1Eisner (1997) describes a slightly different algorithm.
(a)
ah ad
i k j
)
ah ad
i j
(b)
ah ad
k
)
ah ad
(c)
ah ad
j
)
ah ad
j
Figure 1: Basic steps for (a) the CKY-style algorithm
and (b, c) the head splitting algorithm.
as a dependent of a tree with signature ?i; k; h?. There
can be O.n5/ instantiations of this step, and this is
also the running time of the algorithm.
Eisner?s algorithm Eisner and Satta (1999) im-
prove over the CKY algorithm by reducing the num-
ber of position records in an item. They do this by
?splitting? each tree into a left and a right fragment,
so that the head is always placed at one of the two
boundary positions of a fragment, as opposed to be-
ing placed at an internal position. In this way items
need only two indices. Left and right fragments can
be processed independently, and merged afterwards.
Let us consider a right fragment t with head ah.
Attachment at t of a right dependent tree with head
ad is now performed in two steps. The first step at-
taches a left fragment with head ad , as in Figure 1(b).
This results in a new type of fragment/item that has
both heads ah and ad placed at its boundaries. The
second step attaches a right fragment with head ad ,
as in Figure 1(c). The number of possible instanti-
ations of these steps, and the asymptotic runtime of
the algorithm, is O.n3/.
In this article we extend the splitting technique to
the class of well-nested dependency trees with block-
degree at most 2. This amounts to defining a fac-
torization for these trees into fragments, each with
its own head at one of its boundary positions, along
with some unfolding of the attachment operation into
intermediate steps. While for projective trees head
splitting can be done without any loss in coverage,
for the extended class head splitting turns out to be
a proper restriction. The empirical relevance of this
will be discussed in ?7.
268
3 Head-Split Trees
In this section we introduce the class of well-nested
dependency trees with block-degree at most 2, and
define the subclass of head-split dependency trees.
3.1 Preliminaries
For non-negative integers i; j we write ?i; j ? to de-
note the set fi; iC1; : : : ; j g; when i > j , ?i; j ? is the
empty set. For a string w D a1    an, where n  1
and each ai is a lexical token, and for i; j 2 ?0; n?
with i  j , we write wi;j to denote the substring
aiC1    aj of w; wi;i is the empty string.
A dependency tree t over w is a directed tree
whose nodes are a subset of the tokens ai in w and
whose arcs encode a dependency relation between
two nodes. We write ai ! aj to denote the arc
.ai ; aj / in t ; here, the node ai is the head, and the
node aj is the dependent. If each token ai , i 2 ?1; n?,
is a node of t , then t is called complete. Sometimes
we write tai to emphasize that tree t is rooted in node
ai . If ai is a node of t , we also write t ?ai ? to denote
the subtree of t composed by node ai as its root and
all of its descendant nodes.
The nodes of t uniquely identify a set of max-
imal substrings of w, that is, substrings separated
by tokens not in t . The sequence of such substrings,
ordered from left to right, is the yield of t , written
yd.t/. Let ai be some node of t . The block-degree
of ai in t , written bd.ai ; t /, is defined as the number
of string components of yd.t ?ai ?/. The block-degree
of t , written bd.t/, is the maximal block-degree of
its nodes. Tree t is non-projective if bd.t/ > 1.
Tree t is well-nested if, for each node ai of t and for
every pair of outgoing dependencies ai ! ad1 and
ai ! ad2 , the string components of yd.t ?ad1 ?/ and
yd.t ?ad2 ?/ do not ?interleave? in w. More precisely,
it is required that, if some component of yd.t ?adi ?/,
i 2 ?1; 2?, occurs in w in between two components
s1; s2 of yd.t ?adj ?/, j 2 ?1; 2? and j ? i , then allcomponents of yd.t ?adi ?/ occur in between s1; s2.
Throughout this article, whenever we consider a
dependency tree t we always implicitly assume that
t is over w, that t has block-degree at most 2, and
that t is well-nested. Let tai be such a tree, with
bd.ai ; tai / D 2. We call the portion of w in between
the two substrings of yd.tai / the gap of tai , denoted
by gap.tai /.
ah ad4ad3ad2ad1
m.tah/
Figure 2: Example of a node ah with block-degree 2 in a
non-projective, well-nested dependency tree tah . Integerm.tah/, defined in ?3.2, is also marked.
Example 1 Figure 2 schematically depicts a well-
nested tree tah with block-degree 2; we have marked
the root node ah and its dependent nodes adi . Foreach node adi , a shaded area highlights t ?adi ?. Wehave bd.ah; tah/ D bd.ad1 ; tah/ D bd.ad4 ; tah/ D
2 and bd.ad2 ; tah/ D bd.ad3 ; tah/ D 1. 
3.2 The Head-Split Property
We say that a dependency tree t has the head-split
property if it satisfies the following condition. Let
ah ! ad be any dependency in t with bd.ah; t / D
bd.ad ; t / D 2. Whenever gap.t ?ad ?/ contains ah, it
must also contain gap.t ?ah?/. Intuitively, this means
that if yd.t ?ad ?/ ?crosses over? the lexical token ah in
w, then yd.t ?ad ?/ must also ?cross over? gap.t ?ah?/.
Example 2 Dependency ah ! ad1 in Figure 3 viol-
ates the head-split condition, since yd.t ?ad1 ?/ crosses
over the lexical token ah inw, but does not cross over
gap.t ?ah?/. The remaining outgoing dependencies of
ah trivially satisfy the head-split condition, since the
child nodes have block-degree 1. 
Let tah be a dependency tree satisfying the head-
split property and with bd.ah; tah/ D 2. We specify
below a construction that ?splits? tah with respect to
the position of the head ah in yd.tah/, resulting in
two dependency trees sharing the root ah and having
all of the remaining nodes forming two disjoint sets.
Furthermore, the resulting trees have block-degree at
most 2.
ahad1 ad2 ad3
Figure 3: Arc ah ! ad1 violates the head-split condition.
269
(a)
ah ad4ad3
(b)
ahad2ad1 m.tah/
Figure 4: Lower tree (a) and upper tree (b) fragments for
the dependency tree in Figure 2.
Let yd.tah/ D hwi;j ; wp;qi and assume that ah
is placed within wi;j . (A symmetric construction
should be used in case ah is placed withinwp;q .) The
mirror image of ah with respect to gap.tah/, written
m.tah/, is the largest integer in ?p; q? such that there
are no dependencies linking nodes in wi;h 1 and
nodes in wp;m.tah / and there are no dependencieslinking nodes in wh;j and nodes in wm.tah /;q . It isnot hard to see that such an integer always exists,
since tah is well-nested.
We classify every dependent ad of ah as being
an ?upper? dependent or a ?lower? dependent of
ah, according to the following conditions: (i) If
d 2 ?i; h   1? [ ?m.tah/C 1; q?, then ad is an upper
dependent of ah. (ii) If d 2 ?hC 1; j ? [ ?p;m.tah/?,
then ad is a lower dependent of ah.
The upper tree of tah is the dependency tree
rooted in ah and composed of all dependencies
ah ! ad in tah with ad an upper dependent of
ah, along with all subtrees tah ?ad ? rooted in those
dependents. Similarly, the lower tree of tah is the
dependency tree rooted in ah and composed of all
dependencies ah ! ad in tah with ad a lower de-
pendent of ah, along with all subtrees tah ?ad ? rooted
in those dependents. As a general convention, in this
article we write tU;ah and tL;ah to denote the upper
and the lower trees of tah , respectively. Note that, in
some degenerate cases, the set of lower or upper de-
pendents may be empty; then tU;ah or tL;ah consists
of the root node ah only.
Example 3 Consider the tree tah displayed in Fig-
ure 2. Integer m.tah/ denotes the boundary between
the right component of yd.tah ?ad4 ?/ and the right
component of yd.tah ?ad1 ?/. Nodes ad3 and ad4 are
lower dependents, and nodes ad1 and ad2 are upper
dependents. Trees tL;ah and tU;ah are displayed in
Figure 4 (a) and (b), respectively. 
The importance of the head-split property can be
informally explained as follows. Let ah ! ad be a
dependency in tah . When we take apart the upper and
the lower trees of tah , the entire subtree tah ?ad ? ends
up in either of these two fragments. This allows us to
represent upper and lower fragments for some head
independently of the other, and to freely recombine
them. More formally, our algorithms will make use
of the following three properties, stated here without
any formal proof:
P1 Trees tU;ah and tL;ah are well-nested, have block-
degree  2, and satisfy the head-split property.
P2 Trees tU;ah and tL;ah have their head ah always
placed at one of the boundaries in their yields.
P3 Let t 0U;ah and t 00L;ah be the upper and lower treesof distinct trees t 0ah and t 00ah , respectively. If m.t 0ah/ Dm.t 00ah/, then there exists a tree tah such that tU;ah D
t 0U;ah and tL;ah D t 00L;ah .
4 Parsing Items
Let w D a1    an, n  1, be the input string. We
need to compactly represent trees that span substrings
of w by recording only the information that is needed
to combine these trees into larger trees during the
parsing process. We do this by associating each
tree with a signature, called item, which is a tuple
?i; j ; p; q; h?X , where h 2 ?1; n? identifies the token
ah, i; j with 0  i  j  n identify a substringwi;j ,
and p; q with j < p  q  n identify a substring
wp;q . We also use the special setting p D q D  .
The intended meaning is that each item repres-
ents some tree tah . If p; q ?   then yd.tah/ D
hwi;j ; wp;qi. If p; q D   then
yd.tah/ D
8
<?
:?
hwi;j i if h 2 ?i C 1; j ?
hwh;h; wi;j i if h < i
hwi;j ; wh;hi if h > j C 1
The two cases h < i and h > j C 1 above will
be used when the root node ah of tah has not yet
collected all of its dependents.
Note that h 2 fi; j C 1g is not used in the
definition of item. This is meant to avoid differ-
ent items representing the same dependency tree,
270
which is undesired for the specification of our al-
gorithm. As an example, items ?i; j ; ; ; i C 1?X
and ?i C 1; j ; ; ; i C 1?X both represent a depend-
ency tree taiC1 with yd.taiC1/ D hwi;j i. This and
other similar cases are avoided by the ban against
h 2 fi; j C 1g, which amounts to imposing some
normal form for items. In our example, only item
?i; j ; ; ; i C 1?X is a valid signature.
Finally, we distinguish among several item types,
indicated by the value of subscript X . These types
are specific to each parsing algorithm, and will be
defined in later sections.
5 Parsing of Head-Split Trees
We present in this section our first tabular algorithm
for computing the set of all dependency trees for an
input sentence w that have the head-split property,
under the arc-factored model. Recall that tai denotes
a tree with root ai , and tL;ai and tU;ai are the lower
and upper trees of tai . The steps of the algorithm
are specified by means of deduction rules over items,
following the approach of Shieber et al (1995).
5.1 Basic Idea
Our algorithm builds trees step by step, by attaching
a tree tah0 as a dependent of a tree tah and creatingthe new dependency ah ! ah0 . Computationally,
the worst case for this operation is when both tah
and tah0 have a gap; then, for each tree we need tokeep a record of the four boundaries, along with the
position of the head, as done by Go?mez-Rodr??guez et
al. (2011). However, if we are interested in parsing
trees that satisfy the head-split property, we can avoid
representing a tree with a gap by means of a single
item. We instead follow the general idea of ?2 for
projective parsing, and use different items for the
upper and the lower trees of the source tree.
When we need to attach tah0 as an upper dependentof tah , defined as in ?3.2, we perform two consecutive
steps. First, we attach tL;ah0 to tU;ah , resulting in anew intermediate tree t1. As a second step, we attach
tU;ah0 to t1, resulting in a new tree t2 which is tU;ahwith tah0 attached as an upper dependent, as desired.Both steps are depicted in Figure 5; here we introduce
the convention of indicating tree grouping through
a dashed line. A symmetric procedure can be used
to attach tah0 as a lower dependent to tL;ah . The
ah
tU;ah
ah0
tL;ah0
+
t1
(a)
t1
ah0 ah
ah0
tU;ah0
+
t2
(b)
Figure 5: Two step attachment of tah0 at tU;ah : (a) attach-ment of tL;ah0 ; (b) attachment of tU;ah0 .
correctness of the two step approach follows from
properties P1 and P3 in ?3.2.
By property P2 in ?3.2, in both steps above the
lexical heads ah and ah0 can be read from the bound-
aries of the involved trees. Then these steps can be
implemented more efficiently than the na??ve method
of attaching tah0 to tah in a single step. A more de-tailed computational analysis will be provided in ?5.7.
To simplify the presentation, we restrict the use of
head splitting to trees with a gap and parse trees with
no gap with the na??ve method; this does not affect
the computational complexity.
5.2 Item Types
We distinguish five different types of items, indicated
by the subscriptX 2 f0;L;U; =L; =U g, as described
in what follows.
 If X D 0, we have p D q D   and yd.ah/ is
specified as in ?4.
 If X D L, we use the item to represent some
lower tree. We have therefore p; q ?   and
h 2 fi C 1; qg.
 If X D U , we use the item to represent some
upper tree. We have therefore p; q ?   and
h 2 fj; p C 1g.
 If X D =L or X D =U , we use the item to
represent some intermediate step in the parsing
process, in which only the lower or upper tree of
some dependent has been collected by the head
ah, and we are still missing the upper (=U ) or
the lower (=L) tree.
271
We further specialize symbol =U by writing =U<
(=U>) to indicate that the missing upper tree should
have its head to the left (right) of its gap. We also use
=L< and =L> with a similar meaning.
5.3 Item Normal Form
It could happen that our algorithm produces items of
type 0 that do not satisfy the normal form condition
discussed in ?4. To avoid this problem, we assume
that every item of type 0 that is produced by the
algorithm is converted into an equivalent normal form
item, by means of the following rules:
?i; j ; ; ; i ?0
?i   1; j ; ; ; i ?0 (1)
?i; j ; ; ; j C 1?0
?i; j C 1; ; ; j C 1?0 (2)
5.4 Items of Type 0
We start with deduction rules that produce items of
type 0. As already mentioned, we do not apply the
head splitting technique in this case.
The next rule creates trees with a single node, rep-
resenting the head, and no dependents. The rule is
actually an axiom (there is no antecedent) and the
statement i 2 ?1; n? is a side condition.
?i   1; i ; ; ; i ?0
?
i 2 ?1; n? (3)
The next rule takes a tree headed in ah0 and makes
it a dependent of a new head ah. This rule imple-
ments what has been called the ?hook trick?. The first
side condition enforces that the tree headed in ah0
has collected all of its dependents, as discussed in ?4.
The second side condition enforces that no cycle is
created. We also write ah ! ah0 to indicate that a
new dependency is created in the parse forest.
?i; j ; ; ; h0?0
?i; j ; ; ; h?0
8
<
:
h0 2 ?i C 1; j ?
h 62 ?i C 1; j ?
ah ! ah0
(4)
The next two rules combine gap-free dependents
of the same head ah.
?i; k; ; ; h?0 ?k; j ; ; ; h?0
?i; j ; ; ; h?0 (5)
?i; h; ; ; h?0 ?h   1; j ; ; ; h?0
?i; j ; ; ; h?0 (6)
We need the special case in (6) to deal with the con-
catenation of two items that share the head ah at the
concatenation point. Observe the apparent mismatch
in step (6) between index h in the first antecedent
and index h   1 in the second antecedent. This is
because in our normal form, both the first and the
second antecedent have already incorporated a copy
of the shared head ah.
The next two rules collect a dependent of ah that
wraps around the dependents that have already been
collected. As already discussed, this operation is
performed by two successive steps: We first collect
the lower tree and then the upper tree. We present
the case in which the shared head of the two trees is
placed at the left of the gap. The case in which the
head is placed at the right of the gap is symmetric.
?i 0; j 0; ; ; h?0
?i; i 0; j 0; j ; i C 1?L
?i; j ; ; ; h?=U<
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 63?72,
Dublin, Ireland, August 23-24, 2014.
SemEval 2014 Task 8:
Broad-Coverage Semantic Dependency Parsing
Stephan Oepen
??
, Marco Kuhlmann
?
, Yusuke Miyao
?
, Daniel Zeman
?
,
Dan Flickinger
?
, Jan Haji
?
c
?
, Angelina Ivanova
?
, and Yi Zhang
?
?
University of Oslo, Department of Informatics
?
Potsdam University, Department of Linguistics
?
Link?ping University, Department of Computer and Information Science
?
National Institute of Informatics, Tokyo
?
Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics
?
Stanford University, Center for the Study of Language and Information
?
Nuance Communications Aachen GmbH
sdp-organizers@emmtee.net
Abstract
Task 8 at SemEval 2014 defines Broad-
Coverage Semantic Dependency Pars-
ing (SDP) as the problem of recovering
sentence-internal predicate?argument rela-
tionships for all content words, i.e. the se-
mantic structure constituting the relational
core of sentence meaning. In this task
description, we position the problem in
comparison to other sub-tasks in compu-
tational language analysis, introduce the se-
mantic dependency target representations
used, reflect on high-level commonalities
and differences between these representa-
tions, and summarize the task setup, partic-
ipating systems, and main results.
1 Background and Motivation
Syntactic dependency parsing has seen great ad-
vances in the past decade, in part owing to rela-
tively broad consensus on target representations,
and in part reflecting the successful execution of a
series of shared tasks at the annual Conference for
Natural Language Learning (CoNLL; Buchholz &
Marsi, 2006; Nivre et al., 2007; inter alios). From
this very active research area accurate and efficient
syntactic parsers have developed for a wide range
of natural languages. However, the predominant
data structure in dependency parsing to date are
trees, in the formal sense that every node in the de-
pendency graph is reachable from a distinguished
root node by exactly one directed path.
This work is licenced under a Creative Commons At-
tribution 4.0 International License. Page numbers and the
proceedings footer are added by the organizers: http://
creativecommons.org/licenses/by/4.0/.
Unfortunately, tree-oriented parsers are ill-suited
for producing meaning representations, i.e. mov-
ing from the analysis of grammatical structure to
sentence semantics. Even if syntactic parsing ar-
guably can be limited to tree structures, this is not
the case in semantic analysis, where a node will
often be the argument of multiple predicates (i.e.
have more than one incoming arc), and it will often
be desirable to leave nodes corresponding to se-
mantically vacuous word classes unattached (with
no incoming arcs).
Thus, Task 8 at SemEval 2014, Broad-Coverage
Semantic Dependency Parsing (SDP 2014),
1
seeks
to stimulate the dependency parsing community
to move towards more general graph processing,
to thus enable a more direct analysis of Who did
What to Whom? For English, there exist several
independent annotations of sentence meaning over
the venerable Wall Street Journal (WSJ) text of the
Penn Treebank (PTB; Marcus et al., 1993). These
resources constitute parallel semantic annotations
over the same common text, but to date they have
not been related to each other and, in fact, have
hardly been applied for training and testing of data-
driven parsers. In this task, we have used three
different such target representations for bi-lexical
semantic dependencies, as demonstrated in Figure 1
below for the WSJ sentence:
(1) A similar technique is almost impossible to apply to
other crops, such as cotton, soybeans, and rice.
Semantically, technique arguably is dependent on
the determiner (the quantificational locus), the mod-
ifier similar, and the predicate apply. Conversely,
the predicative copula, infinitival to, and the vac-
1
See http://alt.qcri.org/semeval2014/
task8/ for further technical details, information on how to
obtain the data, and official results.
63
A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice .
A1 A2
(a) Partial semantic dependencies in PropBank and NomBank.
A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice.
top
ARG2 ARG3 ARG1
ARG2mwe _and_cARG1ARG1
BV
ARG1 implicit_conjARG1
(b) DELPH-IN Minimal Recursion Semantics?derived bi-lexical dependencies (DM).
A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice
top
ARG1
ARG2
ARG1
ARG2
ARG2
ARG1
ARG1 ARG1 ARG1ARG1
ARG1
ARG2
ARG1
ARG2
ARG1
ARG2
ARG1 ARG1 ARG1 ARG2
(c) Enju Predicate?Argument Structures (PAS).
A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice .
RSTR
PAT
EXT
PAT
ACT
RSTR
ADDR
ADDR
ADDR
ADDR
APPS.m
APPS.m
CONJ.m
CONJ.m CONJ.m
top
(d) Parts of the tectogrammatical layer of the Prague Czech-English Dependency Treebank (PCEDT).
Figure 1: Sample semantic dependency graphs for Example (1).
uous preposition marking the deep object of ap-
ply can be argued to not have a semantic contri-
bution of their own. Besides calling for node re-
entrancies and partial connectivity, semantic depen-
dency graphs may also exhibit higher degrees of
non-projectivity than is typical of syntactic depen-
dency trees.
In addition to its relation to syntactic dependency
parsing, the task also has some overlap with Se-
mantic Role Labeling (SRL; Gildea & Jurafsky,
2002). In much previous work, however, target
representations typically draw on resources like
PropBank and NomBank (Palmer et al., 2005; Mey-
ers et al., 2004), which are limited to argument
identification and labeling for verbal and nominal
predicates. A plethora of semantic phenomena?
for example negation and other scopal embedding,
comparatives, possessives, various types of modi-
fication, and even conjunction?typically remain
unanalyzed in SRL. Thus, its target representations
are partial to a degree that can prohibit seman-
tic downstream processing, for example inference-
based techniques. In contrast, we require parsers
to identify all semantic dependencies, i.e. compute
a representation that integrates all content words in
one structure. Another difference to common inter-
pretations of SRL is that the SDP 2014 task defini-
tion does not encompass predicate disambiguation,
a design decision in part owed to our goal to focus
on parsing-oriented, i.e. structural, analysis, and in
part to lacking consensus on sense inventories for
all content words.
Finally, a third closely related area of much cur-
rent interest is often dubbed ?semantic parsing?,
which Kate and Wong (2010) define as ?the task of
mapping natural language sentences into complete
formal meaning representations which a computer
can execute for some domain-specific application.?
In contrast to most work in this tradition, our SDP
target representations aim to be task- and domain-
independent, though at least part of this general-
ity comes at the expense of ?completeness? in the
above sense; i.e. there are aspects of sentence mean-
ing that arguably remain implicit.
2 Target Representations
We use three distinct target representations for se-
mantic dependencies. As is evident in our run-
ning example (Figure 1), showing what are called
the DM, PAS, and PCEDT semantic dependencies,
there are contentful differences among these anno-
tations, and there is of course not one obvious (or
even objective) truth. In the following paragraphs,
64
we provide some background on the ?pedigree? and
linguistic characterization of these representations.
DM: DELPH-IN MRS-Derived Bi-Lexical De-
pendencies These semantic dependency graphs
originate in a manual re-annotation of Sections 00?
21 of the WSJ Corpus with syntactico-semantic
analyses derived from the LinGO English Re-
source Grammar (ERG; Flickinger, 2000). Among
other layers of linguistic annotation, this resource?
dubbed DeepBank by Flickinger et al. (2012)?
includes underspecified logical-form meaning rep-
resentations in the framework of Minimal Recur-
sion Semantics (MRS; Copestake et al., 2005).
Our DM target representations are derived through
a two-step ?lossy? conversion of MRSs, first to
variable-free Elementary Dependency Structures
(EDS; Oepen & L?nning, 2006), then to ?pure?
bi-lexical form?projecting some construction se-
mantics onto word-to-word dependencies (Ivanova
et al., 2012). In preparing our gold-standard
DM graphs from DeepBank, the same conversion
pipeline was used as in the system submission of
Miyao et al. (2014). For this target representa-
tion, top nodes designate the highest-scoping (non-
quantifier) predicate in the graph, e.g. the (scopal)
degree adverb almost in Figure 1.
2
PAS: Enju Predicate-Argument Structures
The Enju parsing system is an HPSG-based parser
for English.
3
The grammar and the disambigua-
tion model of this parser are derived from the Enju
HPSG treebank, which is automatically converted
from the phrase structure and predicate?argument
structure annotations of the PTB. The PAS data
set is extracted from the WSJ portion of the Enju
HPSG treebank. While the Enju treebank is an-
notated with full HPSG-style structures, only its
predicate?argument structures are converted into
the SDP data format for use in this task. Top
nodes in this representation denote semantic heads.
Again, the system description of Miyao et al. (2014)
provides more technical detail on the conversion.
PCEDT: Prague Tectogrammatical Bi-Lexical
Dependencies The Prague Czech-English De-
pendency Treebank (PCEDT; Haji
?
c et al., 2012)
4
is a set of parallel dependency trees over the WSJ
2
Note, however, that non-scopal adverbs act as mere in-
tersective modifiers, e.g. loudly is a predicate in DM, but the
main verb provides the top node in structures like Abrams
sang loudly.
3
See http://kmcs.nii.ac.jp/enju/.
4
See http://ufal.mff.cuni.cz/pcedt2.0/.
id form lemma pos top pred arg1 arg2
#20200002
1 Ms. Ms. NNP ? + _ _
2 Haag Haag NNP ? ? compound ARG1
3 plays play VBZ + + _ _
4 Elianti Elianti NNP ? ? _ ARG2
5 . . . ? ? _ _
Table 1: Tabular SDP data format (showing DM).
texts from the PTB, and their Czech translations.
Similarly to other treebanks in the Prague family,
there are two layers of syntactic annotation: an-
alytical (a-trees) and tectogrammatical (t-trees).
PCEDT bi-lexical dependencies in this task have
been extracted from the t-trees. The specifics of
the PCEDT representations are best observed in the
procedure that converts the original PCEDT data to
the SDP data format; see Miyao et al. (2014). Top
nodes are derived from t-tree roots; i.e. they mostly
correspond to main verbs. In case of coordinate
clauses, there are multiple top nodes per sentence.
3 Graph Representation
The SDP target representations can be character-
ized as labeled, directed graphs. Formally, a se-
mantic dependency graph for a sentence x =
x
1
, . . . , x
n
is a structure G = (V,E, `
V
, `
E
) where
V = {1, . . . , n} is a set of nodes (which are in
one-to-one correspondence with the tokens of the
sentence); E ? V ? V is a set of edges; and `
V
and `
E
are mappings that assign labels (from some
finite alphabet) to nodes and edges, respectively.
More specifically for this task, the label `
V
(i) of a
node i is a tuple consisting of four components: its
word form, lemma, part of speech, and a Boolean
flag indicating whether the corresponding token
represents a top predicate for the specific sentence.
The label `
E
(i? j) of an edge i? j is a seman-
tic relation that holds between i and j. The exact
definition of what constitutes a top node and what
semantic relations are available differs among our
three target representations, but note that top nodes
can have incoming edges.
All data provided for the task uses a column-
based file format (dubbed the SDP data format)
similar to the one of the 2009 CoNLL Shared Task
(Haji
?
c et al., 2009). As in that task, we assume gold-
standard sentence and token segmentation. For
ease of reference, each sentence is prefixed by a
line with just a unique identifier, using the scheme
2SSDDIII, with a constant leading 2, two-digit sec-
tion code, two-digit document code (within each
65
section), and three-digit item number (within each
document). For example, identifier 20200002 de-
notes the second sentence in the first file of PTB
Section 02, the classic Ms. Haag plays Elianti. The
annotation of this sentence is shown in Table 1.
With one exception, our fields (i.e. columns in
the tab-separated matrix) are a subset of the CoNLL
2009 inventory: (1) id, (2) form, (3) lemma, and
(4) pos characterize the current token, with token
identifiers starting from 1 within each sentence. Be-
sides the lemma and part-of-speech information, in
the closed track of our task, there is no explicit
analysis of syntax. Across the three target represen-
tations in the task, fields (1) and (2) are aligned and
uniform, i.e. all representations annotate exactly
the same text. On the other hand, fields (3) and (4)
are representation-specific, i.e. there are different
conventions for lemmatization, and part-of-speech
assignments can vary (but all representations use
the same PTB inventory of PoS tags).
The bi-lexical semantic dependency graph over
tokens is represented by two or more columns start-
ing with the obligatory, binary-valued fields (5)
top and (6) pred. A positive value in the top
column indicates that the node corresponding to
this token is a top node (see Section 2 below). The
pred column is a simplification of the correspond-
ing field in earlier tasks, indicating whether or not
this token represents a predicate, i.e. a node with
outgoing dependency edges. With these minor dif-
ferences to the CoNLL tradition, our file format can
represent general, directed graphs, with designated
top nodes. For example, there can be singleton
nodes not connected to other parts of the graph,
and in principle there can be multiple tops, or a
non-predicate top node.
To designate predicate?argument relations, there
are as many additional columns as there are pred-
icates in the graph (i.e. tokens marked + in the
pred column); these additional columns are called
(7) arg1, (8) arg2, etc. These colums contain
argument roles relative to the i-th predicate, i.e. a
non-empty value in column arg1 indicates that
the current token is an argument of the (linearly)
first predicate in the sentence. In this format, graph
reentrancies will lead to a token receiving argument
roles for multiple predicates (i.e. non-empty arg
i
values in the same row). All tokens of the same sen-
tence must always have all argument columns filled
in, even on non-predicate words; in other words,
all lines making up one block of tokens will have
the same number n of fields, but n can differ across
DM PAS PCEDT
(1) # labels 51 42 68
(2) % singletons 22.62 4.49 35.79
(3) # edge density 0.96 1.02 0.99
(4) %
g
trees 2.35 1.30 56.58
(5) %
g
projective 3.05 1.71 53.29
(6) %
g
fragmented 6.71 0.23 0.56
(7) %
n
reentrancies 27.35 29.40 9.27
(8) %
g
topless 0.28 0.02 0.00
(9) # top nodes 0.9972 0.9998 1.1237
(10) %
n
non-top roots 44.71 55.92 4.36
Table 2: Contrastive high-level graph statistics.
sentences, depending on the count of graph nodes.
4 Data Sets
All three target representations are annotations of
the same text, Sections 00?21 of the WSJ Cor-
pus. For this task, we have synchronized these
resources at the sentence and tokenization levels
and excluded from the SDP 2014 training and test-
ing data any sentences for which (a) one or more of
the treebanks lacked a gold-standard analysis; (b) a
one-to-one alignment of tokens could not be estab-
lished across all three representations; or (c) at least
one of the graphs was cyclic. Of the 43,746 sen-
tences in these 22 first sections of WSJ text, Deep-
Bank lacks analyses for close to 15%, and the Enju
Treebank has gaps for a little more than four per-
cent. Some 500 sentences show tokenization mis-
matches, most owing to DeepBank correcting PTB
idiosyncrasies like ?G.m.b, H.?, ?S.p, A.?, and
?U.S., .?, and introducing a few new ones (Fares
et al., 2013). Finally, 232 of the graphs obtained
through the above conversions were cyclic. In total,
we were left with 34,004 sentences (or 745,543
tokens) as training data (Sections 00?20), and 1348
testing sentences (29,808 tokens), from Section 21.
Quantitative Comparison As a first attempt at
contrasting our three target representations, Table 2
shows some high-level statistics of the graphs com-
prising the training data.
5
In terms of distinctions
5
These statistics are obtained using the ?official? SDP
toolkit. We refer to nodes that have neither incoming nor
outgoing edges and are not marked as top nodes as singletons;
these nodes are ignored in subsequent statistics, e.g. when
determining the proportion of edges per node (3) or the per-
centages of rooted trees (4) and fragmented graphs (6). The
notation ?%
n
? denotes (non-singleton) node percentages, and
?%
g
? percentages over all graphs. We consider a root node any
(non-singleton) node that has no incoming edges; reentrant
nodes have at least two incoming edges. Following Sagae and
Tsujii (2008), we consider a graph projective when there are
no crossing edges (in a left-to-right rendering of nodes) and no
roots are ?covered?, i.e. for any root j there is no edge i? k
66
Directed Undirected
DM PAS PCEDT DM PAS PCEDT
DM ? .6425 .2612 ? .6719 .5675
PAS .6688 ? .2963 .6993 ? .5490
PCEDT .2636 .2963 ? .5743 .5630 ?
Table 3: Pairwise F
1
similarities, including punctu-
ation (upper right diagonals) or not (lower left).
drawn in dependency labels (1), there are clear dif-
ferences between the representations, with PCEDT
appearing linguistically most fine-grained, and PAS
showing the smallest label inventory. Unattached
singleton nodes (2) in our setup correspond to
tokens analyzed as semantically vacuous, which
(as seen in Figure 1) include most punctuation
marks in PCEDT and DM, but not PAS. Further-
more, PCEDT (unlike the other two) analyzes some
high-frequency determiners as semantically vacu-
ous. Conversely, PAS on average has more edges
per (non-singleton) nodes than the other two (3),
which likely reflects its approach to the analysis of
functional words (see below).
Judging from both the percentage of actual trees
(4), the proportions of projective graphs (5), and the
proportions of reentrant nodes (7), PCEDT is much
more ?tree-oriented? than the other two, which at
least in part reflects its approach to the analysis
of modifiers and determiners (again, see below).
We view the small percentages of graphs without
at least one top node (8) and of graphs with at
least two non-singleton components that are not
interconnected (6) as tentative indicators of general
well-formedness. Intuitively, there should always
be a ?top? predicate, and the whole graph should
?hang together?. Only DM exhibits non-trivial (if
small) degrees of topless and fragmented graphs,
and these may indicate imperfections in the Deep-
Bank annotations or room for improvement in the
conversion from full MRSs to bi-lexical dependen-
cies, but possibly also exceptions to our intuitions
about semantic dependency graphs.
Finally, in Table 3 we seek to quantify pairwise
structural similarity between the three representa-
tions in terms of unlabeled dependency F
1
(dubbed
UF in Section 5 below). We provide four variants
of this metric, (a) taking into account the direc-
tionality of edges or not and (b) including edges
involving punctuation marks or not. On this view,
DM and PAS are structurally much closer to each
other than either of the two is to PCEDT, even more
such that i < j < k.
so when discarding punctuation. While relaxing
the comparison to ignore edge directionality also
increases similarity scores for this pair, the effect
is much more pronounced when comparing either
to PCEDT. This suggests that directionality of se-
mantic dependencies is a major source of diversion
between DM and PAS on the one hand, and PCEDT
on the other hand.
Linguistic Comparison Among other aspects,
Ivanova et al. (2012) categorize a range of syntac-
tic and semantic dependency annotation schemes
according to the role that functional elements take.
In Figure 1 and the discussion of Table 2 above, we
already observed that PAS differs from the other
representations in integrating into the graph aux-
iliaries, the infinitival marker, the case-marking
preposition introducing the argument of apply (to),
and most punctuation marks;
6
while these (and
other functional elements, e.g. complementizers)
are analyzed as semantically vacuous in DM and
PCEDT, they function as predicates in PAS, though
do not always serve as ?local? top nodes (i.e. the se-
mantic head of the corresponding sub-graph): For
example, the infinitival marker in Figure 1 takes the
verb as its argument, but the ?upstairs? predicate
impossible links directly to the verb, rather than to
the infinitival marker as an intermediate.
At the same time, DM and PAS pattern alike
in their approach to modifiers, e.g. attributive ad-
jectives, adverbs, and prepositional phrases. Un-
like in PCEDT (or common syntactic dependency
schemes), these are analyzed as semantic predi-
cates and, thus, contribute to higher degrees of
node reentrancy and non-top (structural) roots.
Roughly the same holds for determiners, but here
our PCEDT projection of Prague tectogrammatical
trees onto bi-lexical dependencies leaves ?vanilla?
articles (like a and the) as singleton nodes.
The analysis of coordination is distinct in the
three representations, as also evident in Figure 1.
By design, DM opts for what is often called
the Mel?
?
cukian analysis of coordinate structures
(Mel?
?
cuk, 1988), with a chain of dependencies
rooted at the first conjunct (which is thus consid-
ered the head, ?standing in? for the structure at
large); in the DM approach, coordinating conjunc-
tions are not integrated with the graph but rather
contribute different types of dependencies. In PAS,
the final coordinating conjunction is the head of the
6
In all formats, punctuation marks like dashes, colons, and
sometimes commas can be contentful, i.e. at times occur as
both predicates, arguments, and top nodes.
67
employee stock investment plans
compound compound compound
employee stock investment plans
ARG1
ARG1
ARG1
employee stock investment plans
ACT
PAT REG
Figure 2: Analysis of nominal compounding in DM, PAS, and PCEDT, respectively .
structure and each coordinating conjunction (or in-
tervening punctuation mark that acts like one) is a
two-place predicate, taking left and right conjuncts
as its arguments. Conversely, in PCEDT the last
coordinating conjunction takes all conjuncts as its
arguments (in case there is no overt conjunction, a
punctuation mark is used instead); additional con-
junctions or punctuation marks are not connected
to the graph.
7
A linguistic difference between our representa-
tions that highlights variable granularities of anal-
ysis and, relatedly, diverging views on the scope
of the problem can be observed in Figure 2. Much
noun phrase?internal structure is not made explicit
in the PTB, and the Enju Treebank from which
our PAS representation derives predates the brack-
eting work of Vadas and Curran (2007). In the
four-way nominal compounding example of Fig-
ure 2, thus, PAS arrives at a strictly left-branching
tree, and there is no attempt at interpreting seman-
tic roles among the members of the compound ei-
ther; PCEDT, on the other hand, annotates both the
actual compound-internal bracketing and the as-
signment of roles, e.g. making stock the PAT(ient)
of investment. In this spirit, the PCEDT annota-
tions could be directly paraphrased along the lines
of plans by employees for investment in stocks. In
a middle position between the other two, DM dis-
ambiguates the bracketing but, by design, merely
assigns an underspecified, construction-specific de-
pendency type; its compound dependency, then,
is to be interpreted as the most general type of de-
pendency that can hold between the elements of
this construction (i.e. to a first approximation either
an argument role or a relation parallel to a prepo-
sition, as in the above paraphrase). The DM and
PCEDT annotations of this specific example hap-
pen to diverge in their bracketing decisions, where
the DM analysis corresponds to [...] investments
in stock for employees, i.e. grouping the concept
7
As detailed by Miyao et al. (2014), individual con-
juncts can be (and usually are) arguments of other predicates,
whereas the topmost conjunction only has incoming edges in
nested coordinate structures. Similarly, a ?shared? modifier of
the coordinate structure as a whole would take as its argument
the local top node of the coordination in DM or PAS (i.e. the
first conjunct or final conjunction, respectively), whereas it
would depend as an argument on all conjuncts in PCEDT.
employee stock (in contrast to ?common stock?).
Without context and expert knowledge, these de-
cisions are hard to call, and indeed there has been
much previous work seeking to identify and anno-
tate the relations that hold between members of a
nominal compound (see Nakov, 2013, for a recent
overview). To what degree the bracketing and role
disambiguation in this example are determined by
the linguistic signal (rather than by context and
world knowledge, say) can be debated, and thus the
observed differences among our representations in
this example relate to the classic contrast between
?sentence? (or ?conventional?) meaning, on the one
hand, and ?speaker? (or ?occasion?) meaning, on
the other hand (Quine, 1960; Grice, 1968). In
turn, we acknowledge different plausible points of
view about which level of semantic representation
should be the target representation for data-driven
parsing (i.e. structural analysis guided by the gram-
matical system), and which refinements like the
above could be construed as part of a subsequent
task of interpretation.
5 Task Setup
Training data for the task, providing all columns in
the file format sketched in Section 3 above, together
with a first version of the SDP toolkit?including
graph input, basic statistics, and scoring?were
released to candidate participants in early Decem-
ber 2013. In mid-January, a minor update to the
training data and optional syntactic ?companion?
analyses (see below) were provided, and in early
February the description and evaluation of a sim-
ple baseline system (using tree approximations and
the parser of Bohnet, 2010). Towards the end of
March, an input-only version of the test data was
released, with just columns (1) to (4) pre-filled; par-
ticipants then had one week to run their systems on
these inputs, fill in columns (5), (6), and upwards,
and submit their results (from up to two different
runs) for scoring. Upon completion of the testing
phase, we have shared the gold-standard test data,
official scores, and system results for all submis-
sions with participants and are currently preparing
all data for general release through the Linguistic
Data Consortium.
68
DM PAS PCEDT
LF LP LR LF LM LP LR LF LM LP LR LF LM
Peking 85.91 90.27 88.54 89.40 26.71 93.44 90.69 92.04 38.13 78.75 73.96 76.28 11.05
Priberam 85.24 88.82 87.35 88.08 22.40 91.95 89.92 90.93 32.64 78.80 74.70 76.70 09.42
Copenhagen-
80.77 84.78 84.04 84.41 20.33 87.69 88.37 88.03 10.16 71.15 68.65 69.88 08.01
Malm?
Potsdam 77.34 79.36 79.34 79.35 07.57 88.15 81.60 84.75 06.53 69.68 66.25 67.92 05.19
Alpage 76.76 79.42 77.24 78.32 09.72 85.65 82.71 84.16 17.95 70.53 65.28 67.81 06.82
Link?ping 72.20 78.54 78.05 78.29 06.08 76.16 75.55 75.85 01.19 60.66 64.35 62.45 04.01
DM PAS PCEDT
LF LP LR LF LM LP LR LF LM LP LR LF LM
Priberam 86.27 90.23 88.11 89.16 26.85 92.56 90.97 91.76 37.83 80.14 75.79 77.90 10.68
CMU 82.42 84.46 83.48 83.97 08.75 90.78 88.51 89.63 26.04 76.81 70.72 73.64 07.12
Turku 80.49 80.94 82.14 81.53 08.23 87.33 87.76 87.54 17.21 72.42 72.37 72.40 06.82
Potsdam 78.60 81.32 80.91 81.11 09.05 89.41 82.61 85.88 07.49 70.35 67.33 68.80 05.42
Alpage 78.54 83.46 79.55 81.46 10.76 87.23 82.82 84.97 15.43 70.98 67.51 69.20 06.60
In-House 75.89 92.58 92.34 92.46 48.07 92.09 92.02 92.06 43.84 40.89 45.67 43.15 00.30
Table 4: Results of the closed (top) and open tracks (bottom). For each system, the second column (LF)
indicates the averaged LF score across all target representations), which was used to rank the systems.
Evaluation Systems participating in the task
were evaluated based on the accuracy with which
they can produce semantic dependency graphs for
previously unseen text, measured relative to the
gold-standard testing data. The key measures for
this evaluation were labeled and unlabeled preci-
sion and recall with respect to predicted dependen-
cies (predicate?role?argument triples) and labeled
and unlabeled exact match with respect to complete
graphs. In both contexts, identification of the top
node(s) of a graph was considered as the identifi-
cation of additional, ?virtual? dependencies from
an artificial root node (at position 0). Below we
abbreviate these metrics as (a) labeled precision,
recall, and F
1
: LP, LR, LF; (b) unlabeled precision,
recall, and F
1
: UP, UR, UF; and (c) labeled and
unlabeled exact match: LM, UM.
The ?official? ranking of participating systems, in
both the closed and the open tracks, is determined
based on the arithmetic mean of the labeled depen-
dency F
1
scores (i.e. the geometric mean of labeled
precision and labeled recall) on the three target rep-
resentations (DM, PAS, and PCEDT). Thus, to be
considered for the final ranking, a system had to
submit semantic dependencies for all three target
representations.
Closed vs. Open Tracks The task was sub-
divided into a closed track and an open track, where
systems in the closed track could only be trained
on the gold-standard semantic dependencies dis-
tributed for the task. Systems in the open track, on
the other hand, could use additional resources, such
as a syntactic parser, for example?provided that
they make sure to not use any tools or resources
that encompass knowledge of the gold-standard
syntactic or semantic analyses of the SDP 2014
test data, i.e. were directly or indirectly trained or
otherwise derived from WSJ Section 21.
This restriction implies that typical off-the-shelf
syntactic parsers had to be re-trained, as many data-
driven parsers for English include this section of
the PTB in their default training data. To simplify
participation in the open track, the organizers pre-
pared ready-to-use ?companion? syntactic analyses,
sentence- and token-aligned to the SDP data, in
two formats, viz. PTB-style phrase structure trees
obtained from the parser of Petrov et al. (2006) and
Stanford Basic syntactic dependencies (de Marn-
effe et al., 2006) produced by the parser of Bohnet
and Nivre (2012).
6 Submissions and Results
From 36 teams who had registered for the task,
test runs were submitted for nine systems. Each
team submitted one or two test runs per track. In
total, there were ten runs submitted to the closed
track and nine runs to the open track. Three teams
submitted to both the closed and the open track.
The main results are summarized and ranked in
Table 4. The ranking is based on the average LF
score across all three target representations, which
is given in the LF column. In cases where a team
submitted two runs to a track, only the highest-
ranked score is included in the table.
69
Team Track Approach Resources
Link?ping C extension of Eisner?s algorithm for DAGs, edge-factored
structured perceptron
?
Potsdam C & O graph-to-tree transformation, Mate companion
Priberam C & O model with second-order features, decoding with dual decom-
position, MIRA
companion
Turku O cascade of SVM classifiers (dependency recognition, label
classification, top recognition)
companion,
syntactic n-grams,
word2vec
Alpage C & O transition-based parsing for DAGs, logistic regression, struc-
tured perceptron
companion,
Brown clusters
Peking C transition-based parsing for DAGs, graph-to-tree transforma-
tion, parser ensemble
?
CMU O edge classification by logistic regression, edge-factored struc-
tured SVM
companion
Copenhagen-Malm? C graph-to-tree transformation, Mate ?
In-House O existing parsers developed by the organizers grammars
Table 5: Overview of submitted systems, high-level approaches, and additional resources used (if any).
In the closed track, the average LF scores across
target representations range from 85.91 to 72.20.
Comparing the results for different target represen-
tations, the average LF scores across systems are
85.96 for PAS, 82.97 for DM, and 70.17 for PCEDT.
The scores for labeled exact match show a much
larger variation across both target representations
and systems.
8
In the open track, we see very similar trends.
The average LF scores across target representations
range from 86.27 to 75.89 and the corresponding
scores across systems are 88.64 for PAS, 84.95
for DM, and 67.52 for PCEDT. While these scores
are consistently higher than in the closed track,
the differences are small. In fact, for each of the
three teams that submitted to both tracks (Alpage,
Potsdam, and Priberam) improvements due to the
use of additional resources in the open track do not
exceed two points LF.
7 Overview of Approaches
Table 5 shows a summary of the systems that sub-
mitted final results. Most of the systems took
a strategy to use some algorithm to process (re-
stricted types of) graph structures, and apply ma-
chine learning like structured perceptrons. The
methods for processing graph structures are clas-
sified into three types. One is to transform graphs
into trees in the preprocessing stage, and apply con-
ventional dependency parsing systems (e.g. Mate;
Bohnet, 2010) to the converted trees. Some sys-
tems simply output the result of dependency pars-
ing (which means they inherently lose some depen-
8
Please see the task web page at the address indicated
above for full labeled and unlabeled scores.
dencies), while the others apply post-processing
to recover non-tree structures. The second strat-
egy is to use a parsing algorithm that can directly
generate graph structures (in the spirit of Sagae &
Tsujii, 2008; Titov et al., 2009). In many cases
such algorithms generate restricted types of graph
structures, but these restrictions appear feasible for
our target representations. The last approach is
more machine learning?oriented; they apply classi-
fiers or scoring methods (e.g. edge-factored scores),
and find the highest-scoring structures by some de-
coding method.
It is difficult to tell which approach is the best;
actually, the top three systems in the closed and
open tracks selected very different approaches. A
possible conclusion is that exploiting existing sys-
tems or techniques for dependency parsing was
successful; for example, Peking built an ensemble
of existing transition-based and graph-based depen-
dency parsers, and Priberam extended an existing
dependency parser. As we indicated in the task de-
scription, a novel feature of this task is that we have
to compute graph structures, and cannot assume
well-known properties like projectivity and lack of
reentrancies. However, many of the participants
found that our representations are mostly tree-like,
and this fact motivated them to apply methods that
have been well studied in the field of syntactic de-
pendency parsing.
Finally, we observe that three teams participated
in both the closed and open tracks, and all of them
reported that adding external resources improved
accuracy by a little more than one point. Systems
with (only) open submissions extensively use syn-
tactic features (e.g. dependency paths) from exter-
nal resources, and they are shown effective even
70
with simple machine learning models. Pre-existing,
tree-oriented dependency parsers are relatively ef-
fective, especially when combined with graph-to-
tree transformation. Comparing across our three
target representations, system scores show a ten-
dency PAS> DM> PCEDT, which can be taken as
a tentative indicator of relative levels of ?parsabil-
ity?. As suggested in Section 4, this variation most
likely correlates at least in part with diverging de-
sign decisions, e.g. the inclusion of relatively local
and deterministic dependencies involving function
words in PAS, or the decision to annotate contex-
tually determined speaker meaning (rather than
?mere? sentence meaning) in at least some construc-
tions in PCEDT.
8 Conclusions and Outlook
We have described the motivation, design, and out-
comes of the SDP 2014 task on semantic depen-
dency parsing, i.e. retrieving bi-lexical predicate?
argument relations between all content words
within an English sentence. We have converted to
a common format three existing annotations (DM,
PAS, and PCEDT) over the same text and have put
this to use for the first time in training and testing
data-driven semantic dependency parsers. Building
on strong community interest already to date and
our belief that graph-oriented dependency parsing
will further gain importance in the years to come,
we are preparing a similar (slightly modified) task
for SemEval 2015. Candidate modifications and
extensions will include cross-domain testing and
evaluation at the level of ?complete? predications
(in contrast to more lenient per-dependency F
1
used
this year). As optional new sub-tasks, we plan on
offering cross-linguistic variation and predicate (i.e.
semantic frame) disambiguation for at least some of
the target representations. To further probe the role
of syntax in the recovery of semantic dependency
relations, we will make available to participants
a wider selection of syntactic analyses, as well as
add a third (idealized) ?gold? track, where syntactic
dependencies are provided directly from available
syntactic annotations of the underlying treebanks.
Acknowledgements
We are grateful to ?eljko Agi
?
c and Bernd Bohnet
for consultation and assistance in preparing our
baseline and companion parses, to the Linguistic
Data Consortium (LDC) for support in distributing
the SDP data to participants, as well as to Emily M.
Bender and two anonymous reviewers for feedback
on this manuscript. Data preparation was supported
through access to the ABEL high-performance com-
puting facilities at the University of Oslo, and we
acknowledge the Scientific Computing staff at UiO,
the Norwegian Metacenter for Computational Sci-
ence, and the Norwegian tax payers. Part of this
work has been supported by the infrastructural fund-
ing by the Ministry of Education, Youth and Sports
of the Czech Republic (CEP ID LM2010013).
References
Bohnet, B. (2010). Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (p. 89 ? 97). Beijing, China.
Bohnet, B., & Nivre, J. (2012). A transition-based
system for joint part-of-speech tagging and labeled
non-projective dependency parsing. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Conference on
Natural Language Learning (p. 1455 ? 1465). Jeju
Island, Korea.
Buchholz, S., & Marsi, E. (2006). CoNLL-X shared
task on multilingual dependency parsing. In Pro-
ceedings of the 10th Conference on Natural Lan-
guage Learning (p. 149 ? 164). New York, NY,
USA.
Copestake, A., Flickinger, D., Pollard, C., & Sag, I. A.
(2005). Minimal Recursion Semantics. An introduc-
tion. Research on Language and Computation, 3(4),
281 ? 332.
de Marneffe, M.-C., MacCartney, B., & Manning, C. D.
(2006). Generating typed dependency parses from
phrase structure parses. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (p. 449 ? 454). Genoa, Italy.
Fares, M., Oepen, S., & Zhang, Y. (2013). Machine
learning for high-quality tokenization. Replicating
variable tokenization schemes. In Computational lin-
guistics and intelligent text processing (p. 231 ? 244).
Springer.
Flickinger, D. (2000). On building a more efficient
grammar by exploiting types. Natural Language En-
gineering, 6 (1), 15 ? 28.
Flickinger, D., Zhang, Y., & Kordoni, V. (2012). Deep-
Bank. A dynamically annotated treebank of the Wall
Street Journal. In Proceedings of the 11th Interna-
tional Workshop on Treebanks and Linguistic Theo-
ries (p. 85 ? 96). Lisbon, Portugal: Edi??es Colibri.
Gildea, D., & Jurafsky, D. (2002). Automatic labeling
of semantic roles. Computational Linguistics, 28,
71
245 ? 288.
Grice, H. P. (1968). Utterer?s meaning, sentence-
meaning, and word-meaning. Foundations of Lan-
guage, 4(3), 225 ? 242.
Haji?c, J., Ciaramita, M., Johansson, R., Kawahara, D.,
Mart?, M. A., M?rquez, L., . . . Zhang, Y. (2009).
The CoNLL-2009 Shared Task. syntactic and seman-
tic dependencies in multiple languages. In Proceed-
ings of the 13th Conference on Natural Language
Learning (p. 1 ? 18). Boulder, CO, USA.
Haji?c, J., Haji?cov?, E., Panevov?, J., Sgall, P., Bojar,
O., Cinkov?, S., . . . ?abokrtsk?, Z. (2012). An-
nouncing Prague Czech-English Dependency Tree-
bank 2.0. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(p. 3153 ? 3160). Istanbul, Turkey.
Ivanova, A., Oepen, S., ?vrelid, L., & Flickinger, D.
(2012). Who did what to whom? A contrastive study
of syntacto-semantic dependencies. In Proceedings
of the Sixth Linguistic Annotation Workshop (p. 2 ?
11). Jeju, Republic of Korea.
Kate, R. J., & Wong, Y. W. (2010). Semantic pars-
ing. The task, the state of the art and the future. In
Tutorial abstracts of the 20th Meeting of the Associ-
ation for Computational Linguistics (p. 6). Uppsala,
Sweden.
Marcus, M., Santorini, B., & Marcinkiewicz, M. A.
(1993). Building a large annotated corpora of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19, 313 ? 330.
Mel?
?
cuk, I. (1988). Dependency syntax. Theory and
practice. Albany, NY, USA: SUNY Press.
Meyers, A., Reeves, R., Macleod, C., Szekely, R.,
Zielinska, V., Young, B., & Grishman, R. (2004).
Annotating noun argument structure for NomBank.
In Proceedings of the 4th International Conference
on Language Resources and Evaluation (p. 803 ?
806). Lisbon, Portugal.
Miyao, Y., Oepen, S., & Zeman, D. (2014). In-house:
An ensemble of pre-existing off-the-shelf parsers. In
Proceedings of the 8th International Workshop on
Semantic Evaluation. Dublin, Ireland.
Nakov, P. (2013). On the interpretation of noun com-
pounds: Syntax, semantics, and entailment. Natural
Language Engineering, 19(3), 291 ? 330.
Nivre, J., Hall, J., K?bler, S., McDonald, R., Nilsson,
J., Riedel, S., & Yuret, D. (2007). The CoNLL 2007
shared task on dependency parsing. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Conference on
Natural Language Learning (p. 915 ? 932). Prague,
Czech Republic.
Oepen, S., & L?nning, J. T. (2006). Discriminant-
based MRS banking. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (p. 1250 ? 1255). Genoa, Italy.
Palmer, M., Gildea, D., & Kingsbury, P. (2005). The
Proposition Bank. A corpus annotated with semantic
roles. Computational Linguistics, 31(1), 71 ? 106.
Petrov, S., Barrett, L., Thibaux, R., & Klein, D. (2006).
Learning accurate, compact, and interpretable tree
annotation. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th Meeting of the Association for Computational
Linguistics (p. 433 ? 440). Sydney, Australia.
Quine, W. V. O. (1960). Word and object. Cambridge,
MA, USA: MIT press.
Sagae, K., & Tsujii, J. (2008). Shift-reduce depen-
dency DAG parsing. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics (p. 753 ? 760). Manchester, UK.
Titov, I., Henderson, J., Merlo, P., & Musillo, G.
(2009). Online graph planarisation for synchronous
parsing of semantic and syntactic dependencies. In
Proceedings of the 21st International Joint Confer-
ence on Artifical Intelligence (p. 1562 ? 1567).
Vadas, D., & Curran, J. (2007). Adding Noun Phrase
Structure to the Penn Treebank. In Proceedings of
the 45th Meeting of the Association for Computa-
tional Linguistics (p. 240 ? 247). Prague, Czech Re-
public.
72
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 395?399,
Dublin, Ireland, August 23-24, 2014.
Link?ping: Cubic-Time Graph Parsing with a Simple Scoring Scheme
Marco Kuhlmann
Dept. of Computer and Information Science
Link?ping University, Sweden
marco.kuhlmann@liu.se
Abstract
We turn the Eisner algorithm for parsing
to projective dependency trees into a cubic-
time algorithm for parsing to a restricted
class of directed graphs. To extend the algo-
rithm into a data-driven parser, we combine
it with an edge-factored feature model and
online learning. We report and discuss re-
sults on the SemEval-2014 Task 8 data sets
(Oepen et al., 2014).
1 Introduction
This paper describes the system that we submit-
ted to the closed track of the SemEval-2014 Task
on Broad-Coverage Semantic Dependency Parsing
(Oepen et al., 2014).
1
However, the main contribu-
tion of the paper is not the system as such (which
had the lowest score among all systems submitted
to the task), but the general approach for which it
is a proof of concept.
Graphs support natural representations of lin-
guistic structure. For this reason, algorithms that
can learn, process and transform graphs are of cen-
tral importance to language technology. Yet, most
of the algorithms that are used in natural language
processing today focus on the restricted case of
trees, and do so for a reason: Computation on gen-
eral graphs is hard or even intractable, and efficient
processing is possible only for restricted classes (cf.
Courcelle and Engelfriet (2012)). The task then
is to identify classes of graphs that are both ex-
pressive enough to cover the linguistic data, and
restricted enough to facilitate efficient processing.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
1
https://github.com/liu-nlp/gamma
This paper shows that there are graphs that sat-
isfy both of these desiderata. Our system is based
on a new algorithm for parsing to a restricted class
of directed graphs (Section 2). This class is re-
stricted in so far as our algorithm runs in cubic
time with respect to the length of the sentence; it
thus has the same asymptotic complexity as parsing
with context-free phrase structure grammars. The
class of graphs defined by our algorithm is also
expressive, in so far that it covers more than 98%
of the SemEval data.
To demonstrate that our parsing algorithm can be
turned into a practical system, we combine it with
two techniques taken straight from the literature on
data-driven syntactic dependency parsing:
 an edge-factored scoring model, as it has been
used as the core of practical parsers since the
seminal work of McDonald et al. (2005), and
 online learning using the structured percep-
tron, in the style of Collins (2002).
State-of-the-art parsers use considerably more ad-
vanced (and computationally more demanding)
techniques, and therefore our system cannot be
expected to deliver competitive results. (Its results
on the SemEval data are reported in Section 4.)
Instead, the main point of our contribution to the
SemEval Task is to provide evidence that research
on classes of graphs that balance linguistic cover-
age and parsing efficiency holds a lot of potential.
2 Parsing Algorithm
We start the description of our system with the
description of our cubic-time parsing algorithm.
The remaining components of our system will be
described in Section 3.
395
Items:
i j i j i j i j
1  i  j  n Axioms:
i i i i
Goal:
1 n
Rules:
i j   1 j k
i k
attach-r
i j j k
i k
complete-r
Figure 1: The Eisner algorithm for building the packed forest of all projective dependency trees with n
nodes. Only the rightward versions of attach and complete are shown here.
2.1 The Eisner Algorithm
We recall the algorithm for projective dependency
parsing by Eisner and Satta (1999). The declara-
tive specification of this algorithm in terms of a
deduction system (Shieber et al., 1995) is given in
Figure 1. The algorithm uses four types of items,
, , , and , and two types of inference rules
called attach and complete. These rules can be
interpreted as operations on graphs: An attach
rule concatenates two graphs and adds one of two
possible edges?from the left endpoint of the first
graph to the right endpoint of the second graph, or
vice versa. Similarly, a complete rule fuses two
graphs by unifying the right endpoint of the first
with the left endpoint of the second. The algorithm
by Eisner and Satta (1999) produces a compact
representation of the set of all dependency graphs
over the input sentence that can be built using these
operations. This is exactly the set of projective
dependency trees for the input sentence.
2.2 The Graph-Parsing Algorithm
To parse to dependency graphs rather than trees,
we modify the Eisner algorithm as follows:
 We give up the distinction between and .
This distinction is essential for ensuring that
the parser builds a tree. Since our goal is to
parse to graphs, we do not need it.
 We allow attach to add one, zero, or several
edges. This modification makes it possible to
parse graphs with reentrancies (several incom-
ing edges) and isolated nodes.
To implement the first modification, we introduce
a new type of items, , that subsumes and .
To implement the second modification, we
parametrize the attach rule by a set ! that spec-
ifies the edges that are added during the concate-
nation. We refer to the left and right endpoints of
a graph as its ports and number the ports of the
antecedents of the attach rule left-to-right from 1
to 4. A set ! then takes the form
! 
 
f1; 2g  f3; 4g

[
 
f3; 4g  f1; 2g

:
The rule attach! adds an edge u ! v if and
only if u and v are nodes corresponding to ports
s and t , respectively, and .s; t/ 2 !. For example,
the attach rule in Figure 1 is specified by the
set ! D f.1; 4/g: it adds one edge, from the left
endpoint of the graph corresponding to the first
antecedent to the right endpoint of the other graph.
The complete parsing algorithm is specified in
Figure 2, where the rule conc (for concatenation)
corresponds to the two conflated attach rules and
fuse corresponds to the two complete rules. In-
specting the specification, we find that the algo-
rithm runs in time O.mn3/ where n is the number
of nodes and m is the number of concatenation
rules. Note that, because each concatenation rule
is determined by a set ! as defined above, each
parser in our framework can use at most 28 D 256
different conc rules.2
3 Data-Driven Parsing
We now extend our parsing algorithm into a simple
parser for data-driven parsing. We cast parsing
as an optimization problem over a parametrized
scoring function: Given a sentence x we compute
Oy D arg max
y2Y.x/
s.x; y/ (1)
where Y.x/ is the set of candidate graphs for x and
the scoring function is decomposed as s.x; y/ D
  f .x; y/. The function f returns a high-dimen-
sional feature vector that describes characteristic
properties of the sentence?graph pair .x; y/, and
the vector  assigns to each feature a weight.
2
This is because a set ! specifies up to 2  2C 2  2 D 8
different concatenation operations.
396
Items:
i j i j i j
1  i  j  n Axioms:
i i i i
Goal:
1 n
Rules:
i j   1 j k
i k
conc!
i j j k
i k
fuse-l
i j j k
i k
fuse-r
Figure 2: The parsing algorithm used in this paper. The concatenation rules (conc) are parametrized with
respect to an edge specification ! (see Section 2.2).
3.1 Candidate Graphs
Our set of candidate graphs is the set of all graphs
that can be built using the operations of our pars-
ing algorithm. The size of this set and hence the
maximal coverage of our parser is determined by
the set of conc rules: The more different concate-
nation operations we use, the more graphs we can
build. At the same time, increasing the number of
operations also increases the runtime of our parser.
This means that we need to find a good trade-off
between coverage and parsing efficiency.
To obtain upper bounds on the coverage of our
parser we compute, for each graph G in the Sem-
Eval test data, a graph
QG that maximizes the set
of edges that it has in common with G. This can
be done using a Viterbi-style variant of our parsing
algorithm that scores an item by the number of
edges that it has in common withG. The results are
reported in Table 1. As we can see, our approach
has the potential to achieve more than 98% labelled
recall (LR) on all three representation types used
in the task. This figure is obtained for the full set
of concatenation operations. For our submission
we chose to optimize for parsing speed and used a
parser with a reduced set of only three operations:
!1 D f.1; 4/g ; !2 D f.4; 1/g ; !3 D fg :
These are the two operations that correspond to
the attach rules of the algorithm by Eisner and
Satta (!1, !2), together with the operation that
concatenates two graphs without adding any edges
at all (!3). The latter is required to produce graphs
DM PAS PCEDT
full 98.25 / 75.74 98.13 / 69.81 98.19 / 83.23
reduced 95.70 / 52.15 93.06 / 23.66 93.51 / 54.75
Table 1: Upper bounds for recall (LR/LM) on the
test data for two different sets of operations.
where a node has no incoming edges. As can be
seen in Table 1, the upper bounds for the reduced
set of operations are still surprisingly high when
measured in terms of LR: 95.70% for DM, 93.06%
for PAS, and 93.51% for PCEDT. However, there
is a significant loss when coverage is measured in
terms of labelled exact match (LM).
3.2 Scoring Function
We use the same features as in the first-order model
implemented in the MSTParser system for syntac-
tic dependency parsing (McDonald et al., 2005).
3
Under this model, the feature vector for a depen-
dency graph is the sum of the feature vectors of
its edges, which take into account atomic features
such as the word forms and part-of-speech tags of
the tokens connected by the edge, the length of the
edge, the edge label, as well as combinations of
those atomic features. To set the feature weights
we use averaged perceptron training in the style of
Collins (2002).
3.3 Top-Node Tagger
The final component in our system is a simple tag-
ger that is used to annotate the output of our parser
with information about top nodes (as defined in the
task?s data format). It is based on Matthew Honni-
bal?s part-of-speech tagger
4
and uses features based
on the word form and part-of-speech of the node
to be tagged, as well as the labels of the edges in-
cident to that node; these features were selected
based on tagging accuracy with the recommended
development train/dev-split. The tagger is a se-
quence model without global constraints; in partic-
ular, it does not enforce unique top nodes. Tagging
accuracy on the final test set was 98.50% for DM,
99.21% for PAS, and 99.94% for PCEDT.
3
http://sourceforge.net/projects/mstparser/
4
http://honnibal.wordpress.com/
397
DM PAS PCEDT
LP LR LF LP LR LF LP LR LF
Baseline 83.20% 40.73% 54.68% 88.34% 35.74% 50.89% 74.82% 62.08% 67.84%
Link?ping 78.54% 78.05% 78.29% 76.16% 75.55% 75.85% 60.66% 64.35% 62.45%
Task average 84.21% 81.29% 82.69% 87.95% 83.57% 85.65% 72.17% 68.44% 70.21%
Peking 90.27% 88.54% 89.40% 93.44% 90.69% 92.04% 78.75% 73.96% 76.28%
Table 2: Labelled precision (LP), labelled recall (LR), and labelled F1 (LF) scores of our own system
(Link?ping) and three points of comparison on the SemEval-2014 Task 8 test data: baseline, task average,
and the best-performing system from Peking University (Du et al., 2014).
4 Experiments
We report experimental results on the SemEval data
sets (closed track). We trained one parser for each
representation (DM, PAS, PCEDT). Averaged per-
ceptron training can be parametrized by the number
N of iterations over the training data; to determine
the value of this parameter, for each representation
type and each 1  N  10 we trained a develop-
ment system using the recommended development
train/dev-split and selected that value of N which
gave the highest accuracy on the held-out data. The
selected values and the number of (binary) features
in the resulting systems are reported in Table 3.
Training took around 8 minutes per iteration on an
iMac computer (Late 2013, 3,4 GHz Intel Core i5)
with a 6 GB Java heap size.
4.1 Results
Table 2 reports the labelled precision (LP) and la-
belled recall (LR) of our system on the final test
data. Compared to the tree-based baseline, our
system has substantially lower precision (between
4.66 and 14.16 points) but substantially higher re-
call (between 2.27 and 39.81 points). Compared to
the top-scoring system, our system is way behind
in terms of both scores (11.11?16.19 points). The
scores of our system are also substantially below
the task average, which resulted in it being ranked
last of all six systems participating in the closed
track. Given these results, we have refrained from
doing a detailed error analysis. It may be interest-
ing to note, however, that our system is the only
one in the task for which labelled F1 is higher on
the DM data than on the PAS data.
DM PAS PCEDT
# iterations 4 1 9
# features 7.3M 8.7M 8.1M
Table 3: Characteristics of the trained models.
4.2 Discussion
The comparatively low scores of our system do not
come unexpected. Our parser uses a very simple
scoring model and learning method, whereas even
the baseline relies on a state-of-the-art syntactic de-
pendency parser (Bohnet, 2010). Also, we did not
do any feature engineering (on the parser), but just
used the feature extraction procedure of MSTParser.
Regarding both of these points, the potential for
improving the system is apparent. Finally, our post-
hoc prediction of top nodes is extremely simplistic.
It would have been much more desirable to inte-
grate this prediction into the parser, for example
by adding virtual incoming dependencies to all top
nodes. However, preliminary experiments showed
that this particular strategy had a severely negative
impact on coverage.
5 Conclusion
We have presented a new algorithm for parsing to
a restricted class of digraphs and shown how to
extend this algorithm into a system for data-driven
dependency parsing. Our main goal was to show
that it is possible to develop algorithms for direct
parsing to directed graphs that are both efficient
and achieve good coverage on practical data sets:
Our algorithm runs in cubic time in the length of
the sentence, and has more than 98% coverage on
each of the three data sets.
Our future work will address both theoretical and
practical issues. On the theoretical side, we feel
that it is important to obtain a better understanding
of the specific graph-structural properties that char-
acterise the linguistic data. Our parser provides an
operational definition of a class of graphs (those
graphs that can be built by the parser); it would be
more satisfying to obtain a declarative characteri-
sation that does not depend on a specific algorithm.
Such a characterisation would be interesting even
for a restricted set of operations.
398
On the practical side, we would like to extend
our approach into a more competitive system for
semantic dependency parsing. In particular, we
would like to use a more powerful scoring function
(incorporating second- and third-order features)
and a more predicative learning method (such as
max-margin training).
Acknowledgements
We thank the two anonymous reviewers of this
paper for their detailed and constructive comments.
References
Bernd Bohnet. 2010. Very high accuracy and fast de-
pendency parsing is not a contradiction. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics (COLING), pages 89?97, Bei-
jing, China.
Michael Collins. 2002. Discriminative training meth-
ods for Hidden Markov Models: Theory and ex-
periments with perceptron algorithms. In Proceed-
ings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 1?8,
Philadelphia, USA.
Bruno Courcelle and Joost Engelfriet. 2012. Graph
Structure andMonadic Second-Order Logic, volume
138 of Encyclopedia of Mathematics and its Applica-
tions. Cambridge University Press.
Yantao Du, Fan Zhang, Weiwei Sun, and Xiaojun Wan.
2014. Peking: Profiling syntactic tree parsing tech-
niques for semantic graph parsing. In Proceedings
of the Eighth International Workshop on Semantic
Evaluation (SemEval 2014), Dublin, Republic of Ire-
land.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and Head
Automaton Grammars. In Proceedings of the 37th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 457?464, College
Park, MD, USA.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 91?98, Ann Arbor, USA.
Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,
Daniel Zeman, Dan Flickinger, Jan Haji?c, Angelina
Ivanova, and Yi Zhang. 2014. SemEval 2014 Task
8: Broad-coverage semantic dependency parsing. In
Proceedings of the Eighth International Workshop
on Semantic Evaluation (SemEval 2014), Dublin,
Republic of Ireland.
Stuart M. Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive
parsing. Journal of Logic Programming, 24(1?2):3?
36.
399
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 146?182,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Overview of the SPMRL 2013 Shared Task:
Cross-Framework Evaluation of Parsing Morphologically Rich Languages?
Djam? Seddaha, Reut Tsarfatyb, Sandra K?blerc,
Marie Canditod, Jinho D. Choie, Rich?rd Farkasf , Jennifer Fosterg, Iakes Goenagah,
Koldo Gojenolai, Yoav Goldbergj , Spence Greenk, Nizar Habashl, Marco Kuhlmannm,
Wolfgang Maiern, Joakim Nivreo, Adam Przepi?rkowskip, Ryan Rothq, Wolfgang Seekerr,
Yannick Versleys, Veronika Vinczet, Marcin Wolin?skiu,
Alina Wr?blewskav, Eric Villemonte de la Cl?rgeriew
aU. Paris-Sorbonne/INRIA, bWeizman Institute, cIndiana U., dU. Paris-Diderot/INRIA, eIPsoft Inc., f,tU. of Szeged,
gDublin City U., h,iU. of the Basque Country, jBar Ilan U., kStanford U., l,qColumbia U., m,oUppsala U., nD?sseldorf U.,
p,u,vPolish Academy of Sciences, rStuttgart U., sHeidelberg U., wINRIA
Abstract
This paper reports on the first shared task on
statistical parsing of morphologically rich lan-
guages (MRLs). The task features data sets
from nine languages, each available both in
constituency and dependency annotation. We
report on the preparation of the data sets, on
the proposed parsing scenarios, and on the eval-
uation metrics for parsing MRLs given dif-
ferent representation types. We present and
analyze parsing results obtained by the task
participants, and then provide an analysis and
comparison of the parsers across languages and
frameworks, reported for gold input as well as
more realistic parsing scenarios.
1 Introduction
Syntactic parsing consists of automatically assigning
to a natural language sentence a representation of
its grammatical structure. Data-driven approaches
to this problem, both for constituency-based and
dependency-based parsing, have seen a surge of inter-
est in the last two decades. These data-driven parsing
approaches obtain state-of-the-art results on the de
facto standard Wall Street Journal data set (Marcus et
al., 1993) of English (Charniak, 2000; Collins, 2003;
Charniak and Johnson, 2005; McDonald et al, 2005;
McClosky et al, 2006; Petrov et al, 2006; Nivre et
al., 2007b; Carreras et al, 2008; Finkel et al, 2008;
?Contact authors: djame.seddah@paris-sorbonne.fr,
reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu
Huang, 2008; Huang et al, 2010; Zhang and Nivre,
2011; Bohnet and Nivre, 2012; Shindo et al, 2012),
and provide a foundation on which many tasks oper-
ating on semantic structure (e.g., recognizing textual
entailments) or even discourse structure (coreference,
summarization) crucially depend.
While progress on parsing English ? the main
language of focus for the ACL community ? has in-
spired some advances on other languages, it has not,
by itself, yielded high-quality parsing for other lan-
guages and domains. This holds in particular for mor-
phologically rich languages (MRLs), where impor-
tant information concerning the predicate-argument
structure of sentences is expressed through word for-
mation, rather than constituent-order patterns as is the
case in English and other configurational languages.
MRLs express information concerning the grammati-
cal function of a word and its grammatical relation to
other words at the word level, via phenomena such
as inflectional affixes, pronominal clitics, and so on
(Tsarfaty et al, 2012c).
The non-rigid tree structures and morphological
ambiguity of input words contribute to the challenges
of parsing MRLs. In addition, insufficient language
resources were shown to also contribute to parsing
difficulty (Tsarfaty et al, 2010; Tsarfaty et al, 2012c,
and references therein). These challenges have ini-
tially been addressed by native-speaking experts us-
ing strong in-domain knowledge of the linguistic
phenomena and annotation idiosyncrasies to improve
the accuracy and efficiency of parsing models. More
146
recently, advances in PCFG-LA parsing (Petrov et al,
2006) and language-agnostic data-driven dependency
parsing (McDonald et al, 2005; Nivre et al, 2007b)
have made it possible to reach high accuracy with
classical feature engineering techniques in addition
to, or instead of, language-specific knowledge. With
these recent advances, the time has come for estab-
lishing the state of the art, and assessing strengths
and weaknesses of parsers across different MRLs.
This paper reports on the first shared task on sta-
tistical parsing of morphologically rich languages
(the SPMRL Shared Task), organized in collabora-
tion with the 4th SPMRL meeting and co-located
with the conference on Empirical Methods in Natural
Language Processing (EMNLP). In defining and exe-
cuting this shared task, we pursue several goals. First,
we wish to provide standard training and test sets for
MRLs in different representation types and parsing
scenarios, so that researchers can exploit them for
testing existing parsers across different MRLs. Sec-
ond, we wish to standardize the evaluation protocol
and metrics on morphologically ambiguous input,
an under-studied challenge, which is also present in
English when parsing speech data or web-based non-
standard texts. Finally, we aim to raise the awareness
of the community to the challenges of parsing MRLs
and to provide a set of strong baseline results for
further improvement.
The task features data from nine, typologically di-
verse, languages. Unlike previous shared tasks on
parsing, we include data in both dependency-based
and constituency-based formats, and in addition to
the full data setup (complete training data), we pro-
vide a small setup (a training subset of 5,000 sen-
tences). We provide three parsing scenarios: one in
which gold segmentation, POS tags, and morphologi-
cal features are provided, one in which segmentation,
POS tags, and features are automatically predicted
by an external resource, and one in which we provide
a lattice of multiple possible morphological analyses
and allow for joint disambiguation of the morpholog-
ical analysis and syntactic structure. These scenarios
allow us to obtain the performance upper bound of
the systems in lab settings using gold input, as well
as the expected level of performance in realistic pars-
ing scenarios ? where the parser follows a morpho-
logical analyzer and is a part of a full-fledged NLP
pipeline.
The remainder of this paper is organized as follows.
We first survey previous work on parsing MRLs (?2)
and provide a detailed description of the present task,
parsing scenarios, and evaluation metrics (?3). We
then describe the data sets for the nine languages
(?4), present the different systems (?5), and empiri-
cal results (?6). Then, we compare the systems along
different axes (?7) in order to analyze their strengths
and weaknesses. Finally, we summarize and con-
clude with challenges to address in future shared
tasks (?8).
2 Background
2.1 A Brief History of the SPMRL Field
Statistical parsing saw initial success upon the avail-
ability of the Penn Treebank (PTB, Marcus et al,
1994). With that large set of syntactically annotated
sentences at their disposal, researchers could apply
advanced statistical modeling and machine learning
techniques in order to obtain high quality structure
prediction. The first statistical parsing models were
generative and based on treebank grammars (Char-
niak, 1997; Johnson, 1998; Klein and Manning, 2003;
Collins, 2003; Petrov et al, 2006; McClosky et al,
2006), leading to high phrase-structure accuracy.
Encouraged by the success of phrase-structure
parsers for English, treebank grammars for additional
languages have been developed, starting with Czech
(Hajic? et al, 2000) then with treebanks of Chinese
(Levy and Manning, 2003), Arabic (Maamouri et
al., 2004b), German (K?bler et al, 2006), French
(Abeill? et al, 2003), Hebrew (Sima?an et al, 2001),
Italian (Corazza et al, 2004), Spanish (Moreno et al,
2000), and more. It quickly became apparent that
applying the phrase-based treebank grammar tech-
niques is sensitive to language and annotation prop-
erties, and that these models are not easily portable
across languages and schemes. An exception to that
is the approach by Petrov (2009), who trained latent-
annotation treebank grammars and reported good
accuracy on a range of languages.
The CoNLL shared tasks on dependency parsing
(Buchholz and Marsi, 2006; Nivre et al, 2007a) high-
lighted the usefulness of an alternative linguistic for-
malism for the development of competitive parsing
models. Dependency relations are marked between
input tokens directly, and allow the annotation of
147
non-projective dependencies that are parseable effi-
ciently. Dependency syntax was applied to the de-
scription of different types of languages (Tesni?re,
1959; Mel?c?uk, 2001), which raised the hope that in
these settings, parsing MRLs will further improve.
However, the 2007 shared task organizers (Nivre
et al, 2007a) concluded that: "[Performance] classes
are more easily definable via language characteris-
tics than via characteristics of the data sets. The
split goes across training set size, original data for-
mat [...], sentence length, percentage of unknown
words, number of dependency labels, and ratio of
(C)POSTAGS and dependency labels. The class
with the highest top scores contains languages with
a rather impoverished morphology." The problems
with parsing MRLs have thus not been solved by de-
pendency parsing, but rather, the challenge has been
magnified.
The first event to focus on the particular challenges
of parsing MRLs was a dedicated panel discussion
co-located with IWPT 2009.1 Work presented on
Hebrew, Arabic, French, and German made it clear
that researchers working on non-English parsing face
the same overarching challenges: poor lexical cover-
age (due to high level of inflection), poor syntactic
coverage (due to more flexible word ordering), and,
more generally, issues of data sparseness (due to
the lack of large-scale resources). Additionally, new
questions emerged as to the evaluation of parsers in
such languages ? are the word-based metrics used
for English well-equipped to capture performance
across frameworks, or performance in the face of
morphological complexity? This event provoked ac-
tive discussions and led to the establishment of a
series of SPMRL events for the discussion of shared
challenges and cross-fertilization among researchers
working on parsing MRLs.
The body of work on MRLs that was accumulated
through the SPMRL workshops2 and hosting ACL
venues contains new results for Arabic (Attia et al,
2010; Marton et al, 2013a), Basque (Bengoetxea
and Gojenola, 2010), Croatian (Agic et al, 2013),
French (Seddah et al, 2010; Candito and Seddah,
2010; Sigogne et al, 2011), German (Rehbein, 2011),
Hebrew (Tsarfaty and Sima?an, 2010; Goldberg and
1http://alpage.inria.fr/iwpt09/panel.en.
html
2See http://www.spmrl.org/ and related workshops.
Elhadad, 2010a), Hindi (Ambati et al, 2010), Ko-
rean (Chung et al, 2010; Choi and Palmer, 2011) and
Spanish (Le Roux et al, 2012), Tamil (Green et al,
2012), amongst others. The awareness of the model-
ing challenges gave rise to new lines of work on top-
ics such as joint morpho-syntactic processing (Gold-
berg and Tsarfaty, 2008), Relational-Realizational
Parsing (Tsarfaty, 2010), EasyFirst Parsing (Gold-
berg, 2011), PLCFRS parsing (Kallmeyer and Maier,
2013), the use of factored lexica (Green et al, 2013),
the use of bilingual data (Fraser et al, 2013), and
more developments that are currently under way.
With new models and data, and with lingering in-
terest in parsing non-standard English data, questions
begin to emerge, such as: What is the realistic per-
formance of parsing MRLs using today?s methods?
How do the different models compare with one an-
other? How do different representation types deal
with parsing one particular language? Does the suc-
cess of a parsing model on a language correlate with
its representation type and learning method? How to
parse effectively in the face of resource scarcity? The
first step to answering all of these questions is pro-
viding standard sets of comparable size, streamlined
parsing scenarios, and evaluation metrics, which are
our main goals in this SPMRL shared task.
2.2 Where We Are At: The Need for
Cross-Framework, Realistic, Evaluation
Procedures
The present task serves as the first attempt to stan-
dardize the data sets, parsing scenarios, and evalu-
ation metrics for MRL parsing, for the purpose of
gaining insights into parsers? performance across lan-
guages. Ours is not the first cross-linguistic task on
statistical parsing. As mentioned earlier, two previ-
ous CoNLL shared tasks focused on cross-linguistic
dependency parsing and covered thirteen different
languages (Buchholz and Marsi, 2006; Nivre et al,
2007a). However, the settings of these tasks, e.g.,
in terms of data set sizes or parsing scenarios, made
it difficult to draw conclusions about strengths and
weaknesses of different systems on parsing MRLs.
A key aspect to consider is the relation between
input tokens and tree terminals. In the standard sta-
tistical parsing setup, every input token is assumed
to be a terminal node in the syntactic parse tree (after
deterministic tokenization of punctuation). In MRLs,
148
morphological processes may have conjoined several
words into a single token. Such tokens need to be seg-
mented and their analyses need to be disambiguated
in order to identify the nodes in the parse tree. In
previous shared tasks on statistical parsing, morpho-
logical information was assumed to be known in ad-
vance in order to make the setup comparable to that
of parsing English. In realistic scenarios, however,
morphological analyses are initially unknown and are
potentially highly ambiguous, so external resources
are used to predict them. Incorrect morphological
disambiguation sets a strict ceiling on the expected
performance of parsers in real-world scenarios. Re-
sults reported for MRLs using gold morphological
information are then, at best, optimistic.
One reason for adopting this less-than-realistic
evaluation scenario in previous tasks has been the
lack of sound metrics for the more realistic scenario.
Standard evaluation metrics assume that the number
of terminals in the parse hypothesis equals the num-
ber of terminals in the gold tree. When the predicted
morphological segmentation leads to a different num-
ber of terminals in the gold and parse trees, standard
metrics such as ParsEval (Black et al, 1991) or At-
tachment Scores (Buchholz and Marsi, 2006) fail
to produce a score. In this task, we use TedEval
(Tsarfaty et al, 2012b), a metric recently suggested
for joint morpho-syntactic evaluation, in which nor-
malized tree-edit distance (Bille, 2005) on morpho-
syntactic trees allows us to quantify the success on
the joint task in realistic parsing scenarios.
Finally, the previous tasks focused on dependency
parsing. When providing both constituency-based
and dependency-based tracks, it is interesting to com-
pare results across these frameworks so as to better
understand the differences in performance between
parsers of different types. We are now faced with
an additional question: how can we compare pars-
ing results across different frameworks? Adopting
standard metrics will not suffice as we would be com-
paring apples and oranges. In contrast, TedEval is
defined for both phrase structures and dependency
structures through the use of an intermediate repre-
sentation called function trees (Tsarfaty et al, 2011;
Tsarfaty et al, 2012a). Using TedEval thus allows us
to explore both dependency and constituency parsing
frameworks and meaningfully compare the perfor-
mance of parsers of different types.
3 Defining the Shared-Task
3.1 Input and Output
We define a parser as a structure prediction function
that maps sequences of space-delimited input tokens
(henceforth, tokens) in a language to a set of parse
trees that capture valid morpho-syntactic structures
in that language. In the case of constituency parsing,
the output structures are phrase-structure trees. In de-
pendency parsing, the output consists of dependency
trees. We use the term tree terminals to refer to the
leaves of a phrase-structure tree in the former case
and to the nodes of a dependency tree in the latter.
We assume that input sentences are represented
as sequences of tokens. In general, there may be a
many-to-many relation between input tokens and tree
terminals. Tokens may be identical to the terminals,
as is often the case in English. A token may be
mapped to multiple terminals assigned their own POS
tags (consider, e.g., the token ?isn?t?), as is the case
in some MRLs. Several tokens may be grouped into
a single (virtual) node, as is the case with multiword
expressions (MWEs) (consider ?pomme de terre? for
?potatoe?). This task covers all these cases.
In the standard setup, all tokens are tree terminals.
Here, the task of a parser is to predict a syntactic
analysis in which the tree terminals coincide with the
tokens. Disambiguating the morphological analyses
that are required for parsing corresponds to selecting
the correct POS tag and possibly a set of morpho-
logical features for each terminal. For the languages
Basque, French, German, Hungarian, Korean, Polish,
and Swedish, we assume this standard setup.
In the morphologically complex setup, every token
may be composed of multiple terminals. In this case,
the task of the parser is to predict the sequence of tree
terminals, their POS tags, and a correct tree associ-
ated with this sequence of terminals. Disambiguating
the morphological analysis therefore requires split-
ting the tokens into segments that define the terminals.
For the Semitic languages Arabic and Hebrew, we
assume this morphologically complex setup.
In the multiword expression (MWEs) setup, pro-
vided here for French only, groupings of terminals
are identified as MWEs (non-terminal nodes in con-
stituency trees, marked heads in dependency trees).
Here, the parser is required to predict how terminals
are grouped into MWEs on top of predicting the tree.
149
3.2 Data Sets
The task features nine languages from six language
families, from Germanic languages (Swedish and
German) and Romance (French) to Slavic (Polish),
Koreanic (Korean), Semitic (Arabic, Hebrew), Uralic
(Hungarian), and the language isolate Basque.
These languages cover a wide range of morpho-
logical richness, with Arabic, Basque, and Hebrew
exhibiting a high degree of inflectional and deriva-
tional morphology. The Germanic languages, Ger-
man and Swedish, have greater degrees of phrasal
ordering freedom than English. While French is not
standardly classified as an MRL, it shares MRLs char-
acteristics which pose challenges for parsing, such as
a richer inflectional system than English.
For each contributing language, we provide two
sets of annotated sentences: one annotated with la-
beled phrase-structure trees, and one annotated with
labeled dependency trees. The sentences in the two
representations are aligned at token and POS levels.
Both representations reflect the predicate-argument
structure of the same sentence, but this information
is expressed using different formal terms and thus
results in different tree structures.
Since some of our native data sets are larger than
others, we provide the training set in two sizes: Full
containing all sentences in the standard training set
of the language, and 5k containing the number of
sentences that is equivalent in size to our smallest
training set (5k sentences). For all languages, the data
has been split into sentences, and the sentences are
parsed and evaluated independently of one another.
3.3 Parsing Scenarios
In the shared task, we consider three parsing scenar-
ios, depending on how much of the morphological
information is provided. The scenarios are listed
below, in increasing order of difficulty.
? Gold: In this scenario, the parser is provided
with unambiguous gold morphological segmen-
tation, POS tags, and morphological features for
each input token.
? Predicted: In this scenario, the parser is pro-
vided with disambiguated morphological seg-
mentation. However, the POS tags and mor-
phological features for each input segment are
unknown.
Scenario Segmentation PoS+Feat. Tree
Gold X X ?
Predicted X 1-best ?
Raw (1-best) 1-best 1-best ?
Raw (all) ? ? ?
Table 1: A summary of the parsing and evaluation sce-
narios. X depicts gold information, ? depicts unknown
information, to be predicted by the system.
? Raw: In this scenario, the parser is provided
with morphologically ambiguous input. The
morphological segmentation, POS tags, and
morphological features for each input token are
unknown.
The Predicted and Raw scenarios require predict-
ing morphological analyses. This may be done using
a language-specific morphological analyzer, or it may
be done jointly with parsing. We provide inputs that
support these different scenarios:
? Predicted: Gold treebank segmentation is given
to the parser. The POS tags assignment and mor-
phological features are automatically predicted
by the parser or by an external resource.
? Raw (1-best): The 1st-best segmentation and
POS tags assignment is predicted by an external
resource and given to the parser.
? Raw (all): All possible segmentations and POS
tags are specified by an external resource. The
parser selects jointly a segmentation and a tree.
An overview of all shown in table 1. For languages
in which terminals equal tokens, only Gold and Pre-
dicted scenarios are considered. For Semitic lan-
guages we further provide input for both Raw (1-
best) and Raw (all) scenarios. 3
3.4 Evaluation Metrics
This task features nine languages, two different repre-
sentation types and three different evaluation scenar-
ios. In order to evaluate the quality of the predicted
structures in the different tracks, we use a combina-
tion of evaluation metrics that allow us to compare
the systems along different axes.
3The raw Arabic lattices were made available later than the
other data. They are now included in the shared task release.
150
In this section, we formally define the different
evaluation metrics and discuss how they support sys-
tem comparison. Throughout this paper, we will be
referring to different evaluation dimensions:
? Cross-Parser Evaluation in Gold/Predicted
Scenarios. Here, we evaluate the results of dif-
ferent parsers on a single data set in the Gold
or Predicted setting. We use standard evalu-
ation metrics for the different types of anal-
yses, that is, ParsEval (Black et al, 1991)
on phrase-structure trees, and Labeled At-
tachment Scores (LAS) (Buchholz and Marsi,
2006) for dependency trees. Since ParsEval is
known to be sensitive to the size and depth of
trees (Rehbein and van Genabith, 2007b), we
also provide the Leaf-Ancestor metric (Samp-
son and Babarczy, 2003), which is less sensitive
to the depth of the phrase-structure hierarchy. In
both scenarios we also provide metrics to evalu-
ate the prediction of MultiWord Expressions.
? Cross-Parser Evaluation in Raw Scenarios.
Here, we evaluate the results of different parsers
on a single data set in scenarios where morpho-
logical segmentation is not known in advance.
When a hypothesized segmentation is not iden-
tical to the gold segmentation, standard evalua-
tion metrics such as ParsEval and Attachment
Scores break down. Therefore, we use TedEval
(Tsarfaty et al, 2012b), which jointly assesses
the quality of the morphological and syntactic
analysis in morphologically-complex scenarios.
? Cross-Framework Evaluation. Here, we com-
pare the results obtained by a dependency parser
and a constituency parser on the same set of sen-
tences. In order to avoid comparing apples and
oranges, we use the unlabeled TedEval metric,
which converts all representation types inter-
nally into the same kind of structures, called
function trees. Here we use TedEval?s cross-
framework protocol (Tsarfaty et al, 2012a),
which accomodates annotation idiosyncrasies.
? Cross-Language Evaluation. Here, we com-
pare parsers for the same representation type
across different languages. Conducting a com-
plete and faithful evaluation across languages
would require a harmonized universal annota-
tion scheme (possibly along the lines of (de
Marneffe and Manning, 2008; McDonald et al,
2013; Tsarfaty, 2013)) or task based evaluation.
As an approximation we use unlabeled TedEval.
Since it is unlabeled, it is not sensitive to label
set size. Since it internally uses function-trees,
it is less sensitive to annotation idiosyncrasies
(e.g., head choice) (Tsarfaty et al, 2011).
The former two dimensions are evaluated on the full
sets. The latter two are evaluated on smaller, compa-
rable, test sets. For completeness, we provide below
the formal definitions and essential modifications of
the evaluation software that we used.
3.4.1 Evaluation Metrics for Phrase Structures
ParsEval The ParsEval metrics (Black et al, 1991)
are evaluation metrics for phrase-structure trees. De-
spite various shortcomings, they are the de-facto stan-
dard for system comparison on phrase-structure pars-
ing, used in many campaigns and shared tasks (e.g.,
(K?bler, 2008; Petrov and McDonald, 2012)). As-
sume that G and H are phrase-structure gold and
hypothesized trees respectively, each of which is rep-
resented by a set of tuples (i, A, j) where A is a
labeled constituent spanning from i to j. Assume
that g is the same as G except that it discards the
root, preterminal, and terminal nodes, likewise for h
and H . The ParsEval scores define the accuracy of
the hypothesis in terms of the normalized size of the
intersection of the constituent sets.
Precision(g, h) = |g?h||h|
Recall(g, h) = |g?h||g|
F1(g, h) = 2?P?RP+R
We evaluate accuracy on phrase-labels ignoring any
further decoration, as it is in standard practices.
Evalb, the standard software that implements Par-
sEval,4 takes a parameter file and ignores the labels
specified therein. As usual, we ignore root and POS
labels. Contrary to the standard practice, we do take
punctuation into account. Note that, as opposed to the
official version, we used the SANCL?2012 version5
modified to actually penalize non-parsed trees.
4http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Evalb
5Modified by Petrov and McDonald (2012) to be less sensi-
tive to punctuation errors.
151
Leaf-Ancestor The Leaf-Ancestor metric (Samp-
son and Babarczy, 2003) measures the similarity be-
tween the path from each terminal node to the root
node in the output tree and the corresponding path
in the gold tree. The path consists of a sequence of
node labels between the terminal node and the root
node, and the similarity of two paths is calculated
by using the Levenshtein distance. This distance is
normalized by path length, and the score of the tree
is an aggregated score of the values for all terminals
in the tree (xt is the leaf-ancestor path of t in tree x).
LA(h, g) =
?
t?yield(g) Lv(ht,gt)/(len(ht)+len(gt))
|yield(g)|
This metric was shown to be less sensitive to dif-
ferences between annotation schemes in (K?bler et
al., 2008), and was shown by Rehbein and van Gen-
abith (2007a) to evaluate trees more faithfully than
ParsEval in the face of certain annotation decisions.
We used the implementation of Wagner (2012).6
3.4.2 Evaluation Metrics for Dependency
Structures
Attachment Scores Labeled and Unlabeled At-
tachment scores have been proposed as evaluation
metrics for dependency parsing in the CoNLL shared
tasks (Buchholz and Marsi, 2006; Nivre et al, 2007a)
and have since assumed the role of standard metrics
in multiple shared tasks and independent studies. As-
sume that g, h are gold and hypothesized dependency
trees respectively, each of which is represented by
a set of arcs (i, A, j) where A is a labeled arc from
terminal i to terminal j. Recall that in the gold and
predicted settings, |g| = |h| (because the number of
terminals determines the number of arcs and hence it
is fixed). So Labeled Attachment Score equals preci-
sion and recall, and it is calculated as a normalized
size of the intersection between the sets of gold and
parsed arcs.7
Precision(g, h) = |g?h||g|
Recall(g, h) = |g?h||h|
LAS(g, h) = |g?h||g| =
|g?h|
|h|
6The original version is available at
http://www.grsampson.net/Resources.
html, ours at http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Leaf.
7http://ilk.uvt.nl/conll/software.html.
3.4.3 Evaluation Metrics for Morpho-Syntactic
Structures
TedEval The TedEval metrics and protocols have
been developed by Tsarfaty et al (2011), Tsarfaty
et al (2012a) and Tsarfaty et al (2012b) for coping
with non-trivial evaluation scenarios, e.g., comparing
parsing results across different frameworks, across
representation theories, and across different morpho-
logical segmentation hypotheses.8 Contrary to the
previous metrics, which view accuracy as a normal-
ized intersection over sets, TedEval computes the ac-
curacy of a parse tree based on the tree-edit distance
between complete trees. Assume a finite set of (pos-
sibly parameterized) edit operations A = {a1....an},
and a cost function c : A ? 1. An edit script is the
cost of a sequence of edit operations, and the edit dis-
tance of g, h is the minimal cost edit script that turns
g into h (and vice versa). The normalized distance
subtracted from 1 provides the level of accuracy on
the task. Formally, the TedEval score on g, h is de-
fined as follows, where ted is the tree-edit distance,
and the |x| (size in nodes) discards terminals and root
nodes.
TedEval(g, h) = 1?
ted(g, h)
|g|+ |h|
In the gold scenario, we are not allowed to manipu-
late terminal nodes, only non-terminals. In the raw
scenarios, we can add and delete both terminals and
non-terminals so as to match both the morphological
and syntactic hypotheses.
3.4.4 Evaluation Metrics for
Multiword-Expression Identification
As pointed out in section 3.1, the French data set is
provided with tree structures encoding both syntactic
information and groupings of terminals into MWEs.
A given MWE is defined as a continuous sequence of
terminals, plus a POS tag. In the constituency trees,
the POS tag of the MWE is an internal node of the
tree, dominating the sequence of pre-terminals, each
dominating a terminal. In the dependency trees, there
is no specific node for the MWE as such (the nodes
are the terminals). So, the first token of a MWE is
taken as the head of the other tokens of the same
MWE, with the same label (see section 4.4).
8http://www.tsarfaty.com/unipar/
download.html.
152
To evaluate performance on MWEs, we use the
following metrics.
? R_MWE, P_MWE, and F_MWE are recall, pre-
cision, and F-score over full MWEs, in which
a predicted MWE counts as correct if it has the
correct span (same group as in the gold data).
? R_MWE +POS, R_MWE +POS, and F_MWE
+POS are defined in the same fashion, except
that a predicted MWE counts as correct if it has
both correct span and correct POS tag.
? R_COMP, R_COMP, and F_COMP are recall,
precision and F-score over non-head compo-
nents of MWEs: a non-head component of MWE
counts as correct if it is attached to the head of
the MWE, with the specific label that indicates
that it is part of an MWE.
4 The SPMRL 2013 Data Sets
4.1 The Treebanks
We provide data from nine different languages anno-
tated with two representation types: phrase-structure
trees and dependency trees.9 Statistics about size,
average length, label set size, and other character-
istics of the treebanks and schemes are provided in
Table 2. Phrase structures are provided in an ex-
tended bracketed style, that is, Penn Treebank brack-
eted style where every labeled node may be extended
with morphological features expressed. Dependency
structures are provided in the CoNLL-X format.10
For any given language, the dependency and con-
stituency treebanks are aligned at the token and ter-
minal levels and share the same POS tagset and mor-
phological features. That is, any form in the CoNLL
format is a terminal of the respective bracketed tree.
Any CPOS label in the CoNLL format is the pre-
terminal dominating the terminal in the bracketed
tree. The FEATS in the CoNLL format are repre-
sented as dash-features decorated on the respective
pre-terminal node in the bracketed tree. See Fig-
ure 1(a)?1(b) for an illustration of this alignment.
9Additionally, we provided the data in TigerXML format
(Brants et al, 2002) for phrase structure trees containing cross-
ing branches. This allows the use of more powerful parsing
formalisms. Unfortunately, we received no submissions for this
data, hence we discard them in the rest of this overview.
10See http://ilk.uvt.nl/conll/.
For ambiguous morphological analyses, we pro-
vide the mapping of tokens to different segmentation
possibilities through lattice files. See Figure 1(c) for
an illustration, where lattice indices mark the start
and end positions of terminals.
For each of the treebanks, we provide a three-way
dev/train/set split and another train set containing the
first 5k sentences of train (5k). This section provides
the details of the original treebanks and their anno-
tations, our data-set preparation, including prepro-
cessing and data splits, cross-framework alignment,
and the prediction of morphological information in
non-gold scenarios.
4.2 The Arabic Treebanks
Arabic is a morphologically complex language which
has rich inflectional and derivational morphology. It
exhibits a high degree of morphological ambiguity
due to the absence of the diacritics and inconsistent
spelling of letters, such as Alif and Ya. As a conse-
quence, the Buckwalter Standard Arabic Morpholog-
ical Analyzer (Buckwalter, 2004; Graff et al, 2009)
produces an average of 12 analyses per word.
Data Sets The Arabic data set contains two tree-
banks derived from the LDC Penn Arabic Treebanks
(PATB) (Maamouri et al, 2004b):11 the Columbia
Arabic Treebank (CATiB) (Habash and Roth, 2009),
a dependency treebank, and the Stanford version
of the PATB (Green and Manning, 2010), a phrase-
structure treebank. We preprocessed the treebanks
to obtain strict token matching between the treebanks
and the morphological analyses. This required non-
trivial synchronization at the tree token level between
the PATB treebank, the CATiB treebank and the mor-
phologically predicted data, using the PATB source
tokens and CATiB feature word form as a dual syn-
chronized pivot.
The Columbia Arabic Treebank The Columbia
Arabic Treebank (CATiB) uses a dependency repre-
sentation that is based on traditional Arabic grammar
and that emphasizes syntactic case relations (Habash
and Roth, 2009; Habash et al, 2007). The CATiB
treebank uses the word tokenization of the PATB
11The LDC kindly provided their latest version of the Arabic
Treebanks. In particular, we used PATB 1 v4.1 (Maamouri et al,
2005), PATB 2 v3.1 (Maamouri et al, 2004a) and PATB 3 v3.3.
(Maamouri et al, 2009)
153
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
train:
#Sents 15,762 7,577 14,759 40,472 8,146 23,010 6,578
#Tokens 589,220 96,368 443,113 719,532 170,141 351,184 68,424
Lex. Size 36,906 25,136 27,470 77,222 40,782 11,1540 22,911
Avg. Length 37.38 12.71 30.02 17.77 20.88 15.26 10.40
Ratio #NT/#Tokens 0.19 0.82 0.34 0.60 0.59 0.60 0.94
Ratio #NT/#Sents 7.40 10.50 10.33 10.70 12.38 9.27 9.84
#Non Terminals 22 12 32 25 16 8 34
#POS tags 35 25 29 54 16 1,975 29
#total NTs 116,769 79,588 152,463 433,215 100,885 213,370 64,792
Dep. Label Set Size 9 31 25 43 417 22 27
train5k:
#Sents 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000
#Tokens 224,907 61,905 150,984 87,841 128,046 109,987 68,336 52,123 76,357
Lex. Size 19,433 18,405 15,480 17,421 15,975 29,009 29,715 18,632 14,110
Avg. Length 44.98 12.38 30.19 17.56 25.60 21.99 13.66 10.42 15.27
Ratio #NT/#Tokens 0.15 0.83 0.34 0.60 0.42 0.57 0.68 0.94 0.58
Ratio #NT/#Sents 7.18 10.33 10.32 10.58 10.97 12.57 9.29 9.87 8.96
#Non Terminals 22 12 29 23 60 16 8 34 8
#POS Tags 35 25 29 51 50 16 972 29 25
#total NTs 35,909 5,1691 51,627 52,945 54,856 62,889 46,484 49,381 44,845
Dep. Label Set Size 9 31 25 42 43 349 20 27 61
dev:
#Sents 1,985 948 1,235 5,000 500 1,051 2,066 821 494
#Tokens 73,932 13,851 38,820 76,704 11,301 29,989 30,480 8,600 9,341
Lex. Size 12,342 5,551 6,695 15,852 3,175 10,673 15,826 4,467 2,690
Avg. Length 37.24 14.61 31.43 15.34 22.60 28.53 14.75 10.47 18.90
Ratio #NT/#Tokens 0.19 0.74 0.33 0.63 0.47 047 0.63 0.94 0.48
Ratio #NT/#Sents 7.28 10.92 10.48 9.71 10.67 13.66 9.33 9.90 9.10
#Non Terminals 21 11 27 24 55 16 8 31 8
#POS Tags 32 23 29 50 47 16 760 29 24
#total NTs 14,452 10,356 12,951 48,560 5,338 14,366 19,283 8,132 4,496
Dep. Label Set Size 9 31 25 41 42 210 22 26 59
test:
#Sents 1959 946 2541 5000 716 1009 2287 822 666
#Tokens 73878 11457 75216 92004 16998 19908 33766 8545 10690
Lex. Size 12254 4685 10048 20149 4305 7856 16475 4336 3112
Avg. Length 37.71 12.11 29.60 18.40 23.74 19.73 14.76 10.39 16.05
Ratio #NT/#Tokens 0.19 0.83 0.34 0.60 0.47 0.62 0.61 0.95 0.57
Ratio #NT/#Sents 7.45 10.08 10.09 11.07 11.17 12.26 9.02 9.94 9.18
#Non Terminals 22 12 30 23 54 15 8 31 8
#POS Tags 33 22 30 52 46 16 809 27 25
#total NTs 14,610 9,537 25,657 55,398 8,001 12,377 20,640 8,175 6,118
Dep. Label Set Size 9 31 26 42 41 183 22 27 56
Table 2: Overview of participating languages and treebank properties. ?Sents? = number of sentences, ?Tokens? =
number of raw surface forms. ?Lex. size? and ?Avg. Length? are computed in terms of tagged terminals. ?NT? = non-
terminals in constituency treebanks, ?Dep Labels? = dependency labels on the arcs of dependency treebanks. ? A more
comprehensive table is available at http://www.spmrl.org/spmrl2013-sharedtask.html/#Prop.
154
(a) Constituency Tree
% % every line is a single tree in a bracketed Penn Treebank format
(ROOT (S (NP ( NNP-#pers=3|num=sing# John))(VP ( VB-#pers=3|num=sing# likes)(NP ( NNP-#pers=3|num=sing# Mary)))))
(b) Dependency Tree
%% every line describes a terminal: terminal-id form lemma CPOS FPOS FEATS Head Rel PHead PRel
1 John John NNP NNP pers=3|num=sing 2 sbj _ _
2 likes like VB VB pers=3|num=sing 0 root _ _
3 Mary Mary NNP NNP pers=3|num=sing 2 obj _ _
Input Lattice
0 1 2 3 4 5 6
1:AIF/NN
1:AIF/VB
1:AIF/NNT
2:LA/RB
3:NISH/VB
3:NISH/NN
4:L/PREP
4:LHSTIR/VB
4:HSTIR/VB
5:ZAT/PRP
%% every line describes a terminal: start-id end-id form lemma CPOS FPOS FEATS token-id
0 1 AIF AIF NN NN _ 1
0 1 AIF AIF NNT NNT _ 1
0 1 AIF AIF VB VB _ 1
1 2 LA LA RB RB _ 2
2 3 NISH NISH VB VB _ 3
2 3 NISH NISH NN NN _ 3
3 5 LHSTIR HSTIR VB VB _ 4
3 4 L L PREP PREP _ 4
4 5 HSTIR HSTIR VB VB _ 4
5 6 ZAT ZAT PRP PRP _ 5
Figure 1: File formats. Trees (a) and (b) are aligned constituency and dependency trees for a mockup English example.
Boxed labels are shared across the treebanks. Figure (c) shows an ambiguous lattice. The red part represents the yield
of the gold tree. For brevity, we use empty feature columns, but of course lattice arcs may carry any morphological
features, in the FEATS CoNLL format.
and employs a reduced POS tagset consisting of six
tags only: NOM (non-proper nominals including
nouns, pronouns, adjectives and adverbs), PROP
(proper nouns), VRB (active-voice verbs), VRB-
PASS (passive-voice verbs), PRT (particles such as
prepositions or conjunctions) and PNX (punctuation).
(This stands in extreme contrast with the Buckwalter
Arabic tagset (PATB official tagset) which is almost
500 tags.) To obtain these dependency trees, we used
the constituent-to-dependency tool (Habash and Roth,
2009). Additional CATiB trees were annotated di-
rectly, but we only use the portions that are converted
from phrase-structure representation, to ensure that
the constituent and dependency yields can be aligned.
The Stanford Arabic Phrase Structure Treebank
In order to stay compatible with the state of the art,
we provide the constituency data set with most of the
pre-processing steps of Green and Manning (2010),
as they were shown to improve baseline performance
on the PATB parsing considerably.12
To convert the original PATB to preprocessed
phrase-structure trees ? la Stanford, we first discard
all trees dominated by X, which indicates errors and
non-linguistic text. At the phrasal level, we collapse
unary chains with identical categories like NP? NP.
We finally remove all traces, but, unlike Green and
Manning (2010), we keep all function tags.
In the original Stanford instance, the pre-terminal
morphological analyses were mapped to the short-
ened Bies tag set provided with the treebank (where
Determiner markers, ?DT?, were added to definite
noun and adjectives, resulting in 32 POS tags). Here
we use the Kulick tagset (Kulick et al, 2006) for
12Both the corpus split and pre-processing code are available
with the Stanford parser at http://nlp.stanford.edu/
projects/arabic.shtml.
155
pre-terminal categories in the phrase-structure trees,
where the Bies tag set is included as a morphological
feature (stanpos) in our PATB instance.
Adapting the Data to the Shared Task We con-
verted the CATiB representation to the CoNLL rep-
resentation and added a ?split-from-previous? and
?split-from-next? markers as in LDC?s tree-terminal
fields.
A major difference between the CATiB treebank
and the Stanford treebank lies in the way they han-
dle paragraph annotations. The original PATB con-
tains sequences of annotated trees that belong to a
same discourse unit (e.g., paragraph). While the
CATiB conversion tool considers each sequence a
single parsing unit, the Stanford pre-processor treats
each such tree structure rooted at S, NP or Frag as
a tree spanning a single sentence. To be compati-
ble with the predicted morphology data which was
bootstrapped and trained on the CATiB interpretation,
we deterministically modified the original PATB by
adding pseudo XP root nodes, so that the Stanford
pre-proprecessor will generate the same tree yields
as the CATiB treebank.
Another important aspect of preprocessing (often-
delegated as a technicality in the Arabic parsing lit-
erature) is the normalization of token forms. Most
Arabic parsing work used transliterated text based on
the schemes proposed by Buckwalter (2002). The
transliteration schemes exhibit some small differ-
ences, but enough to increase the out-of-vocabulary
rate by a significant margin (on top of strictly un-
known morphemes). This phenomenon is evident in
the morphological analysis lattices (in the predicted
dev set there is a 6% OOV rate without normalization,
and half a point reduction after normalization is ap-
plied, see (Habash et al, 2009b; Green and Manning,
2010)). This rate is much lower for gold tokenized
predicted data (with an OOV rate of only 3.66%,
similar to French for example). In our data set, all
tokens are minimally normalized: no diacritics, no
normalization.13
Data Splits For the Arabic treebanks, we use the
data split recommended by the Columbia Arabic and
Dialect Modeling (CADiM) group (Diab et al, 2013).
13Except for the minimal normalization present in MADA?s
back-end tools. This script was provided to the participants.
The data of the LDC first three annotated Arabic Tree-
banks (ATB1, ATB2 and ATB3) were divided into
roughly a 10/80/10% dev/train/test split by word vol-
ume. When dividing the corpora, document bound-
aries were maintained. The train5k files are simply
the first 5,000 sentences of the training files.
POS Tagsets Given the richness of Arabic mor-
phology, there are multiple POS tag sets and tokeniza-
tion schemes that have been used by researchers, (see,
e.g., Marton et al (2013a)). In the shared task, we fol-
low the standard PATB tokenization which splits off
several categories of orthographic clitics, but not the
definite article Al+. On top of that, we consider three
different POS tag sets with different degrees of gran-
ularity: the Buckwalter tag set (Buckwalter, 2004),
the Kulick Reduced Tag set (Kulick et al, 2006), and
the CATiB tag set (Habash et al, 2009a), considering
that granularity of the morphological analyses may
affect syntactic processing. For more information see
Habash (2010).
Predicted Morphology To prepare input for the
Raw scenarios (?3.3), we used the MADA+TOKAN
system (Habash et al, 2009b). MADA is a system
for morphological analysis and disambiguation of
Arabic. It can predict the 1-best tokenization, POS
tags, lemmas and diacritization in one fell swoop.
The MADA output was also used to generate the
lattice files for the Raw-all scenario.
To generate input for the gold token / predicted
tag input scenario, we used Morfette (Chrupa?a et al,
2008), a joint lemmatization and POS tagging model
based on an averaged perceptron. We generated two
tagging models, one trained with the Buckwalter tag
set, and the other with the Kulick tag set. Both were
mapped back to the CATiB POS tag set such that all
predicted tags are contained in the feature field.14
4.3 The Basque Treebank
Basque is an agglutinative language with a high ca-
pacity to generate inflected wordforms, with free
constituent order of sentence elements with respect
to the main verb. Contrary to many other treebanks,
the Basque treebank was originally annotated with
dependency trees, which were later on converted to
constituency trees.
14A conversion script from the rich Buckwalter tagset to
CoNLL-like features was provided to the participants.
156
The Basque Dependency Treebank (BDT) is a
dependency treebank in its original design, due to
syntactic characteristics of Basque such as its free
word order. Before the syntactic annotation, mor-
phological analysis was performed, using the Basque
morphological analyzer of Aduriz et al (2000). In
Basque each lemma can generate thousands of word-
forms ? differing in morphological properties such
as case, number, tense, or different types of subordi-
nation for verbs. If only POS category ambiguity is
resolved, the analyses remain highly ambiguous.
For the main POS category, there is an average of
1.55 interpretations per wordform, which rises to 2.65
for the full morpho-syntactic information, resulting
in an overall 64% of ambiguous wordforms. The
correct analysis was then manually chosen.
The syntactic trees were manually assigned. Each
word contains its lemma, main POS category, POS
subcategory, morphological features, and the la-
beled dependency relation. Each form indicates mor-
phosyntactic features such as case, number and type
of subordination, which are relevant for parsing.
The first version of the Basque Dependency Tree-
bank, consisting of 3,700 sentences (Aduriz et al,
2003), was used in the CoNLL 2007 Shared Task on
Dependency Parsing (Nivre et al, 2007a). The cur-
rent shared task uses the second version of the BDT,
which is the result of an extension and redesign of the
original requirements, containing 11,225 sentences
(150,000 tokens).
The Basque Constituency Treebank (BCT) was
created as part of the CESS-ECE project, where the
main aim was to obtain syntactically annotated con-
stituency treebanks for Catalan, Spanish and Basque
using a common set of syntactic categories. BCT
was semi-automatically derived from the dependency
version (Aldezabal et al, 2008). The conversion pro-
duced complete constituency trees for 80% of the
sentences. The main bottlenecks have been sentence
connectors and non-projective dependencies which
could not be straightforwardly converted into projec-
tive tree structures, requiring a mechanism similar to
traces in the Penn English Treebank.
Adapting the Data to the Shared Task As the
BCT did not contain all of the original non-projective
dependency trees, we selected the set of 8,000 match-
ing sentences in both treebanks for the shared task.15
This implies that around 2k trees could not be gen-
erated and therefore were discarded. Furthermore,
the BCT annotation scheme does not contain attach-
ment for most of the punctuation marks, so those
were inserted into the BCT using a simple lower-left
attachment heuristic. The same goes for some con-
nectors that could not be aligned in the first phase.
Predicted Morphology In order to obtain pre-
dicted tags for the non-gold scenarios, we used the
following pipeline. First, morphological analysis as
described above was performed, followed by a dis-
ambiguation step. At that point, it is hard to obtain a
single interpretation for each wordform, as determin-
ing the correct interpretation for each wordform may
require knowledge of long-distance elements on top
of the free constituency order of the main phrasal el-
ements in Basque. The disambiguation is performed
by the module by Ezeiza et al (1998), which uses
a combination of knowledge-based disambiguation,
by means of Constraint Grammar (Karlsson et al,
1995; Aduriz et al, 1997), and a posterior statistical
disambiguation module, using an HMM.16
For the shared task data, we chose a setting that
disambiguates most word forms, and retains ? 97%
of the correct interpretations, leaving an ambiguity
level of 1.3 interpretations. For the remaining cases
of ambiguity, we chose the first interpretation, which
corresponds to the most frequent option. This leaves
open the investigation of more complex approaches
for selecting the most appropriate reading.17
4.4 The French Treebank
French is not a morphologically rich language per se,
though its inflectional system is richer than that of
English, and it also exhibits a limited amount of word
order variation occurring at different syntactic levels
including the word level (e.g. pre- or post-nominal
15We generated a 80/10/10 split, ? train/dev/test ? The first 5k
sentences of the train set were used as a basis for the train5k.
16Note that the statistical module can be parametrized accord-
ing to the level of disambiguation to trade off precision and
recall. For example, disambiguation based on the main cate-
gories (abstracting over morpho-syntactic features) maintains
most of the correct interpretations but still gives an output with
several interpretations per wordform.
17This is not an easy task. The ambiguity left is the hardest to
solve given that the knowledge-based and statistical disambigua-
tion processes have not been able to pick out a single reading.
157
adjective, pre- or post-verbal adverbs) and the phrase
level (e.g. possible alternations between post verbal
NPs and PPs). It also has a high degree of multi-
word expressions, that are often ambiguous with a
literal reading as a sequence of simple words. The
syntactic and MWE analysis shows the same kind of
interaction (though to a lesser extent) as morphologi-
cal and syntactic interaction in Semitic languages ?
MWEs help parsing, and syntactic information may
be required to disambiguate MWE identification.
The Data Set The French data sets were gener-
ated from the French Treebank (Abeill? et al, 2003),
which consists of sentences from the newspaper Le
Monde, manually annotated with phrase structures
and morphological information. Part of the treebank
trees are also annotated with grammatical function
tags for dependents of verbs. In the SPMRL shared
task release, we used only this part, consisting of
18,535 sentences,18 split into 14,759 sentences for
training, 1,235 sentences for development, and 2,541
sentences for the final evaluation.19
Adapting the Data to the Shared Task The con-
stituency trees are provided in an extended PTB
bracketed format, with morphological features at the
pre-terminal level only. They contain slight, auto-
matically performed, modifications with respect to
the original trees of the French treebank. The syntag-
matic projection of prepositions and complementiz-
ers was normalized, in order to have prepositions and
complementizers as heads in the dependency trees
(Candito et al, 2010).
The dependency representations are projective de-
pendency trees, obtained through automatic conver-
sion from the constituency trees. The conversion pro-
cedure is an enhanced version of the one described
by Candito et al (2010).
Both the constituency and the dependency repre-
sentations make use of coarse- and fine-grained POS
tags (CPOS and FPOS respectively). The CPOS are
the categories from the original treebank. The FPOS
18The process of functional annotation is still ongoing, the
objective of the FTB providers being to have all the 20000 sen-
tences annotated with functional tags.
19The first 9,981 training sentences correspond to the canoni-
cal 2007 training set. The development set is the same and the
last 1235 sentences of the test set are those of the canonical test
set.
are merged using the CPOS and specific morphologi-
cal information such as verbal mood, proper/common
noun distinction (Crabb? and Candito, 2008).
Multi-Word Expressions The main difference
with respect to previous releases of the bracketed
or dependency versions of the French treebank
lies in the representation of multi-word expressions
(MWEs). The MWEs appear in an extended format:
each MWE bears an FPOS20 and consists of a se-
quence of terminals (hereafter the ?components? of
the MWE), each having their proper CPOS, FPOS,
lemma and morphological features. Note though that
in the original treebank the only gold information
provided for a MWE component is its CPOS. Since
leaving this information blank for MWE components
would have provided a strong cue for MWE recog-
nition, we made sure to provide the same kind of
information for every terminal, whether MWE com-
ponent or not, by providing predicted morphological
features, lemma, and FPOS for MWE components
(even in the ?gold? section of the data set). This infor-
mation was predicted by the Morfette tool (Chrupa?a
et al, 2008), adapted to French (Seddah et al, 2010).
In the constituency trees, each MWE corresponds
to an internal node whose label is the MWE?s FPOS
suffixed by a +, and which dominates the component
pre-terminal nodes.
In the dependency trees, there is no ?node? for a
MWE as a whole, but one node (a terminal in the
CoNLL format) per MWE component. The first com-
ponent of a MWE is taken as the head of the MWE.
All subsequent components of the MWE depend on
the first one, with the special label dep_cpd. Further-
more, the first MWE component bears a feature mwe-
head equal to the FPOS of the MWE. For instance,
the MWE la veille (the day before) is an adverb, con-
taining a determiner component and a common noun
component. Its bracketed representation is (ADV+
(DET la) (NC veille)), and in the dependency repre-
sentation, the noun veille depends on the determiner
la, which bears the feature mwehead=ADV+.
Predicted Morphology For the predicted mor-
phology scenario, we provide data in which the
mwehead has been removed and with predicted
20In the current data, we did not carry along the lemma and
morphological features pertaining to the MWE itself, though this
information is present in the original trees.
158
FPOS, CPOS, lemma, and morphological features,
obtained by training Morfette on the whole train set.
4.5 The German Treebank
German is a fusional language with moderately free
word order, in which verbal elements are fixed in
place and non-verbal elements can be ordered freely
as long as they fulfill the ordering requirements of
the clause (H?hle, 1986).
The Data Set The German constituency data set
is based on the TiGer treebank release 2.2.21 The
original annotation scheme represents discontinuous
constituents such that all arguments of a predicate
are always grouped under a single node regardless of
whether there is intervening material between them
or not (Brants et al, 2002). Furthermore, punctua-
tion and several other elements, such as parentheses,
are not attached to the tree. In order to make the
constituency treebank usable for PCFG parsing, we
adapted this treebank as described shortly.
The conversion of TiGer into dependencies is a
variant of the one by Seeker and Kuhn (2012), which
does not contain empty nodes. It is based on the same
TiGer release as the one used for the constituency
data. Punctuation was attached as high as possible,
without creating any new non-projective edges.
Adapting the Data to the Shared Task For
the constituency version, punctuation and other
unattached elements were first attached to the tree.
As attachment target, we used roughly the respec-
tive least common ancestor node of the right and
left terminal neighbor of the unattached element (see
Maier et al (2012) for details), and subsequently, the
crossing branches were resolved.
This was done in three steps. In the first step, the
head daughters of all nodes were marked using a
simple heuristic. In case there was a daughter with
the edge label HD, this daughter was marked, i.e.,
existing head markings were honored. Otherwise, if
existing, the rightmost daughter with edge label NK
(noun kernel) was marked. Otherwise, as default, the
leftmost daughter was marked. In a second step, for
each continuous part of a discontinuous constituent,
a separate node was introduced. This corresponds
21This version is available from http://www.ims.
uni-stuttgart.de/forschung/ressourcen/
korpora/tiger.html
to the "raising" algorithm described by Boyd (2007).
In a third steps, all those newly introduced nodes
that did not cover the head daughter of the original
discontinuous node were deleted. For the second
and the third step, we used the same script as for the
Swedish constituency data.
Predicted Morphology For the predicted scenario,
a single sequence of POS tags and morphologi-
cal features has been assigned using the MATE
toolchain via a model trained on the train set via cross-
validation on the training set. The MATE toolchain
was used to provide predicted annotation for lem-
mas, POS tags, morphology, and syntax. In order to
achieve the best results for each annotation level, a
10-fold jackknifing was performed to provide realis-
tic features for the higher annotation levels. The pre-
dicted annotation of the 5k training set were copied
from the full data set.22
4.6 The Hebrew Treebank
Modern Hebrew is a Semitic language, characterized
by inflectional and derivational (templatic) morphol-
ogy and relatively free word order. The function
words for from/to/like/and/when/that/the are prefixed
to the next token, causing severe segmentation ambi-
guity for many tokens. In addition, Hebrew orthogra-
phy does not indicate vowels in modern texts, leading
to a very high level of word-form ambiguity.
The Data Set Both the constituency and the de-
pendency data sets are derived from the Hebrew
Treebank V2 (Sima?an et al, 2001; Guthmann et
al., 2009). The treebank is based on just over 6000
sentences from the daily newspaper ?Ha?aretz?, man-
ually annotated with morphological information and
phrase-structure trees and extended with head infor-
mation as described in Tsarfaty (2010, ch. 5). The
unlabeled dependency version was produced by con-
version from the constituency treebank as described
in Goldberg (2011). Both the constituency and depen-
dency trees were annotated with a set grammatical
function labels conforming to Unified Stanford De-
pendencies by Tsarfaty (2013).
22We also provided a predicted-all scenario, in which we
provided morphological analysis lattices with POS and mor-
phological information derived from the analyses of the SMOR
derivational morphology (Schmid et al, 2004). These lattices
were not used by any of the participants.
159
Adapting the Data to the Shared Task While
based on the same trees, the dependency and con-
stituency treebanks differ in their POS tag sets, as
well as in some of the morphological segmentation
decisions. The main effort towards the shared task
was unifying the two resources such that the two tree-
banks share the same lexical yields, and the same
pre-terminal labels. To this end, we took the layering
approach of Goldberg et al (2009), and included two
levels of POS tags in the constituency trees. The
lower level is lexical, conforming to the lexical re-
source used to build the lattices, and is shared by
the two treebanks. The higher level is syntactic, and
follows the tag set and annotation decisions of the
original constituency treebank.23 In addition, we uni-
fied the representation of morphological features, and
fixed inconsistencies and mistakes in the treebanks.
Data Split The Hebrew treebank is one of the
smallest in our language set, and hence it is provided
in only the small (5k) setting. For the sake of com-
parability with the 5k set of the other treebanks, we
created a comparable size of dev/test sets containing
the first and last 500 sentences respectively, where
the rest serve as the 5k training.24
Predicted Morphology The lattices encoding the
morphological ambiguity for the Raw (all) scenario
were produced by looking up the possible analyses
of each input token in the wide-coverage morpholog-
ical analyzer (lexicon) of the Knowledge Center for
Processing Hebrew (Itai and Wintner, 2008; MILA,
2008), with a simple heuristic for dealing with un-
known tokens. A small lattice encoding the possible
analyses of each token was produced separately, and
these token-lattices were concatenated to produce the
sentence lattice. The lattice for a given sentence may
not include the gold analysis in cases of incomplete
lexicon coverage.
The morphologically disambiguated input files for
the Raw (1-best) scenario were produced by run-
ning the raw text through the morphological disam-
23Note that this additional layer in the constituency treebank
adds a relatively easy set of nodes to the trees, thus ?inflating?
the evaluation scores compared to previously reported results.
To compensate, a stricter protocol than is used in this task would
strip one of the two POS layers prior to evaluation.
24This split is slightly different than the split in previous stud-
ies.
biguator (tagger) described in Adler and Elhadad
(2006; Goldberg et al (2008),Adler (2007). The
disambiguator is based on the same lexicon that is
used to produce the lattice files, but utilizes an extra
module for dealing with unknown tokens Adler et al
(2008). The core of the disambiguator is an HMM
tagger trained on about 70M unannotated tokens us-
ing EM, and being supervised by the lexicon.
As in the case of Arabic, we also provided data
for the Predicted (gold token / predicted morphol-
ogy) scenario. We used the same sequence labeler,
Morfette (Chrupa?a et al, 2008), trained on the con-
catenation of POS and morphological gold features,
leading to a model with respectable accuracy.25
4.7 The Hungarian Treebank
Hungarian is an agglutinative language, thus a lemma
can have hundreds of word forms due to derivational
or inflectional affixation (nominal declination and
verbal conjugation). Grammatical information is typ-
ically indicated by suffixes: case suffixes mark the
syntactic relationship between the head and its argu-
ments (subject, object, dative, etc.) whereas verbs
are inflected for tense, mood, person, number, and
the definiteness of the object. Hungarian is also char-
acterized by vowel harmony.26 In addition, there are
several other linguistic phenomena such as causa-
tion and modality that are syntactically expressed in
English but encoded morphologically in Hungarian.
The Data Set The Hungarian data set used in
the shared task is based on the Szeged Treebank,
the largest morpho-syntactic and syntactic corpus
manually annotated for Hungarian. This treebank
is based on newspaper texts and is available in
both constituent-based (Csendes et al, 2005) and
dependency-based (Vincze et al, 2010) versions.
Around 10k sentences of news domain texts were
made available to the shared task.27 Each word is
manually assigned all its possible morpho-syntactic
25POS+morphology prediction accuracy is 91.95% overall
(59.54% for unseen tokens). POS only prediction accuracy is
93.20% overall (71.38% for unseen tokens).
26When vowel harmony applies, most suffixes exist in two
versions ? one with a front vowel and another one with a back
vowel ? and it is the vowels within the stem that determine which
form of the suffix is selected.
27The original treebank contains 82,000 sentences, 1.2 million
words and 250,000 punctuation marks from six domains.
160
tags and lemmas and the appropriate one is selected
according to the context. Sentences were manu-
ally assigned a constituency-based syntactic struc-
ture, which includes information on phrase structure,
grammatical functions (such as subject, object, etc.),
and subcategorization information (i.e., a given NP
is subcategorized by a verb or an infinitive). The
constituency trees were later automatically converted
into dependency structures, and all sentences were
then manually corrected. Note that there exist some
differences in the grammatical functions applied to
the constituency and dependency versions of the tree-
bank, since some morpho-syntactic information was
coded both as a morphological feature and as dec-
oration on top of the grammatical function in the
constituency trees.
Adapting the Data to the Shared Task Origi-
nally, the Szeged Dependency Treebank contained
virtual nodes for elided material (ELL) and phonolog-
ically covert copulas (VAN). In the current version,
they have been deleted, their daughters have been
attached to the parent of the virtual node, and have
been given complex labels, e.g. COORD-VAN-SUBJ,
where VAN is the type of the virtual node deleted,
COORD is the label of the virtual node and SUBJ is
the label of the daughter itself. When the virtual node
was originally the root of the sentence, its daughter
with a predicative (PRED) label has been selected as
the new root of the sentence (with the label ROOT-
VAN-PRED) and all the other daughters of the deleted
virtual node have been attached to it.
Predicted Morphology In order to provide the
same POS tag set for the constituent and dependency
treebanks, we used the dependency POS tagset for
both treebank instances. Both versions of the tree-
bank are available with gold standard and automatic
morphological annotation. The automatic POS tag-
ging was carried out by a 10-fold cross-validation
on the shared task data set by magyarlanc, a natu-
ral language toolkit for processing Hungarian texts
(segmentation, morphological analysis, POS tagging,
and dependency parsing). The annotation provides
POS tags and deep morphological features for each
input token (Zsibrita et al, 2013).28
28The full data sets of both the constituency and de-
pendency versions of the Szeged Treebank are available at
4.8 The Korean Treebank
The Treebank The Korean corpus is generated by
collecting constituent trees from the KAIST Tree-
bank (Choi et al, 1994), then converting the con-
stituent trees to dependency trees using head-finding
rules and heuristics. The KAIST Treebank consists
of about 31K manually annotated constituent trees
from 97 different sources (e.g., newspapers, novels,
textbooks). After filtering out trees containing an-
notation errors, a total of 27,363 trees with 350,090
tokens are collected.
The constituent trees in the KAIST Treebank29 also
come with manually inspected morphological analy-
sis based on ?eojeol?. An eojeol contains root-forms
of word tokens agglutinated with grammatical affixes
(e.g., case particles, ending markers). An eojeol can
consist of more than one word token; for instance, a
compound noun ?bus stop? is often represented as
one eojeol in Korean, ????????????, which can be
broken into two word tokens,???? (bus) and????????
(stop). Each eojeol in the KAIST Treebank is sepa-
rated by white spaces regardless of punctuation. Fol-
lowing the Penn Korean Treebank guidelines (Han
et al, 2002), punctuation is separated as individual
tokens, and parenthetical notations surrounded by
round brackets are grouped into individual phrases
with a function tag (PRN in our corpus).
All dependency trees are automatically converted
from the constituent trees. Unlike English, which
requires complicated head-finding rules to find the
head of each phrase (Choi and Palmer, 2012), Ko-
rean is a head final language such that the rightmost
constituent in each phrase becomes the head of that
phrase. Moreover, the rightmost conjunct becomes
the head of all other conjuncts and conjunctions in
a coordination phrase, which aligns well with our
head-final strategy.
The constituent trees in the KAIST Treebank do
not consist of function tags indicating syntactic or
semantic roles, which makes it difficult to generate
dependency labels. However, it is possible to gener-
ate meaningful labels by using the rich morphology
in Korean. For instance, case particles give good
the following website: www.inf.u-szeged.hu/rgai/
SzegedTreebank, and magyarlanc is downloadable from:
www.inf.u-szeged.hu/rgai/magyarlanc.
29See Lee et al (1997) for more details about the bracketing
guidelines of the KAIST Treebank.
161
indications of what syntactic roles eojeols with such
particles should take. Given this information, 21
dependency labels were generated according to the
annotation scheme proposed by Choi (2013).
Adapting the Data to the Shared Task All details
concerning the adaptation of the KAIST treebank
to the shared task specifications are found in Choi
(2013). Importantly, the rich KAIST treebank tag set
of 1975 POS tag types has been converted to a list of
CoNLL-like feature-attribute values refining coarse
grained POS categories.
Predicted Morphology Two sets of automatic
morphological analyses are provided for this task.
One is generated by the HanNanum morphological
analyzer.30 The HanNanum morphological ana-
lyzer gives the same morphemes and POS tags as the
KAIST Treebank. The other is generated by the Se-
jong morphological analyzer.31 The Sejong morpho-
logical analyzer gives a different set of morphemes
and POS tags as described in Choi and Palmer (2011).
4.9 The Polish Treebank
The Data Set Sk?adnica is a constituency treebank
of Polish (Wolin?ski et al, 2011; S?widzin?ski and
Wolin?ski, 2010). The trees were generated with
a non-probabilistic DCG parser S?wigra and then
disambiguated and validated manually. The ana-
lyzed texts come from the one-million-token sub-
corpus of the National Corpus of Polish (NKJP,
(Przepi?rkowski et al, 2012)) manually annotated
with morpho-syntactic tags.
The dependency version of Sk?adnica is a re-
sult of an automatic conversion of manually disam-
biguated constituent trees into dependency structures
(Wr?blewska, 2012). The conversion was an entirely
automatic process. Conversion rules were based
on morpho-syntactic information, phrasal categories,
and types of phrase-structure rules encoded within
constituent trees. It was possible to extract dependen-
cies because the constituent trees contain information
about the head of the majority of constituents. For
other constituents, heuristics were defined in order to
select their heads.
30http://kldp.net/projects/hannanum
31http://www.sejong.or.kr
The version of Sk?adnica used in the shared task
comprises parse trees for 8,227 sentences.32
Predicted Morphology For the shared task Pre-
dicted scenario, an automatic morphological an-
notation was generated by the PANTERA tagger
(Acedan?ski, 2010).
4.10 The Swedish Treebank
Swedish is moderately rich in inflections, including
a case system. Word order obeys the verb second
constraint in main clauses but is SVO in subordinate
clauses. Main clause order is freer than in English
but not as free as in some other Germanic languages,
such as German. Also, subject agreement with re-
spect to person and number has been dropped in
modern Swedish.
The Data Set The Swedish data sets are taken
from the Talbanken section of the Swedish Treebank
(Nivre and Megyesi, 2007). Talbanken is a syntacti-
cally annotated corpus developed in the 1970s, orig-
inally annotated according to the MAMBA scheme
(Teleman, 1974) with a syntactic layer consisting
of flat phrase structure and grammatical functions.
The syntactic annotation was later automatically con-
verted to full phrase structure with grammatical func-
tions and from that to dependency structure, as de-
scribed by Nivre et al (2006).
Both the phrase structure and the dependency
version use the functional labels from the original
MAMBA scheme, which provides a fine-grained clas-
sification of syntactic functions with 65 different la-
bels, while the phrase structure annotation (which
had to be inferred automatically) uses a coarse set
of only 8 labels. For the release of the Swedish tree-
bank, the POS level was re-annotated to conform to
the current de facto standard for Swedish, which is
the Stockholm-Ume? tagset (Ejerhed et al, 1992)
with 25 base tags and 25 morpho-syntactic features,
which together produce over 150 complex tags.
For the shared task, we used version 1.2 of the
treebank, where a number of conversion errors in
the dependency version have been corrected. The
phrase structure version was enriched by propagating
morpho-syntactic features from preterminals (POS
32Sk?adnica is available from http://zil.ipipan.waw.
pl/Sklicense.
162
tags) to higher non-terminal nodes using a standard
head percolation table, and a version without crossing
branches was derived using the lifting strategy (Boyd,
2007).
Adapting the Data to the Shared Task Explicit
attribute names were added to the feature field and the
split was changed to match the shared task minimal
training set size.
Predicted Morphology POS tags and morpho-
syntactic features were produced using the Hun-
PoS tagger (Hal?csy et al, 2007) trained on the
Stockholm-Ume? Corpus (Ejerhed and K?llgren,
1997).
5 Overview of the Participating Systems
With 7 teams participating, more than 14 systems for
French and 10 for Arabic and German, this shared
task is on par with the latest large-scale parsing evalu-
ation campaign SANCL 2012 (Petrov and McDonald,
2012). The present shared task was extremely de-
manding on our participants. From 30 individuals or
teams who registered and obtained the data sets, we
present results for the seven teams that accomplished
successful executions on these data in the relevant
scenarios in the given the time frame.
5.1 Dependency Track
Seven teams participated in the dependency track.
Two participating systems are based on MaltParser:
MALTOPTIMIZER (Ballesteros, 2013) and AI:KU
(Cirik and S?ensoy, 2013). MALTOPTIMIZER uses
a variant of MaltOptimizer (Ballesteros and Nivre,
2012) to explore features relevant for the processing
of morphological information. AI:KU uses a combi-
nation of MaltParser and the original MaltOptimizer.
Their system development has focused on the inte-
gration of an unsupervised word clustering method
using contextual and morphological properties of the
words, to help combat sparseness.
Similarly to MaltParser ALPAGE:DYALOG
(De La Clergerie, 2013) also uses a shift-reduce
transition-based parser but its training and decoding
algorithms are based on beam search. This parser is
implemented on top of the tabular logic programming
system DyALog. To the best of our knowledge, this
is the first dependency parser capable of handling
word lattice input.
Three participating teams use the MATE parser
(Bohnet, 2010) in their systems: the BASQUETEAM
(Goenaga et al, 2013), IGM:ALPAGE (Constant et
al., 2013) and IMS:SZEGED:CIS (Bj?rkelund et al,
2013). The BASQUETEAM uses the MATE parser in
combination with MaltParser (Nivre et al, 2007b).
The system combines the parser outputs via Malt-
Blender (Hall et al, 2007). IGM:ALPAGE also uses
MATE and MaltParser, once in a pipeline architec-
ture and once in a joint model. The models are com-
bined via a re-parsing strategy based on (Sagae and
Lavie, 2006). This system mainly focuses on MWEs
in French and uses a CRF tagger in combination
with several large-scale dictionaries to handle MWEs,
which then serve as input for the two parsers.
The IMS:SZEGED:CIS team participated in both
tracks, with an ensemble system. For the depen-
dency track, the ensemble includes the MATE parser
(Bohnet, 2010), a best-first variant of the easy-first
parser by Goldberg and Elhadad (2010b), and turbo
parser (Martins et al, 2010), in combination with
a ranker that has the particularity of using features
from the constituent parsed trees. CADIM (Marton et
al., 2013b) uses their variant of the easy-first parser
combined with a feature-rich ensemble of lexical and
syntactic resources.
Four of the participating teams use exter-
nal resources in addition to the parser. The
IMS:SZEGED:CIS team uses external morpholog-
ical analyzers. CADIM uses SAMA (Graff et al,
2009) for Arabic morphology. ALPAGE:DYALOG
and IGM:ALPAGE use external lexicons for French.
IGM:ALPAGE additionally uses Morfette (Chrupa?a
et al, 2008) for morphological analysis and POS
tagging. Finally, as already mentioned, AI:KU clus-
ters words and POS tags in an unsupervised fashion
exploiting additional, un-annotated data.
5.2 Constituency Track
A single team participated in the constituency parsing
task, the IMS:SZEGED:CIS team (Bj?rkelund et al,
2013). Their phrase-structure parsing system uses a
combination of 8 PCFG-LA parsers, trained using a
product-of-grammars procedure (Petrov, 2010). The
50-best parses of this combination are then reranked
by a model based on the reranker by Charniak and
163
Johnson (2005).33
5.3 Baselines
We additionally provide the results of two baseline
systems for the nine languages, one for constituency
parsing and one for dependency parsing.
For the dependency track, our baseline system is
MaltParser in its default configuration (the arc-eager
algorithm and liblinear for training). Results marked
as BASE:MALT in the next two sections report the
results of this baseline system in different scenarios.
The constituency parsing baseline is based on the
most recent version of the PCFG-LA model of Petrov
et al (2006), used with its default settings and five
split/merge cycles, for all languages.34 We use this
parser in two configurations: a ?1-best? configura-
tion where all POS tags are provided to the parser
(predicted or gold, depending on the scenario), and
another configuration in which the parser performs
its own POS tagging. These baselines are referred to
as BASE:BKY+POS and BASE:BKY+RAW respec-
tively in the following results sections. Note that
even when BASE:BKY+POS is given gold POS tags,
the Berkeley parser sometimes fails to reach a perfect
POS accuracy. In cases when the parser cannot find a
parse with the provided POS, it falls back on its own
POS tagging for all tokens.
6 Results
The high number of submitted system variants and
evaluation scenarios in the task resulted in a large
number of evaluation scores. In the following evalu-
ation, we focus on the best run for each participant,
and we aim to provide key points on the different
dimensions of analysis resulting from our evaluation
protocol. We invite our interested readers to browse
the comprehensive representation of our results on
the official shared-task results webpages.35
33Note that a slight but necessary change in the configuration
of one of our metrics, which occurred after the system submis-
sion deadline, resulted in the IMS:SZEGED:CIS team to submit
suboptimal systems for 4 languages. Their final scores are ac-
tually slightly higher and can be found in (Bj?rkelund et al,
2013).
34For Semitic languages, we used the lattice based PCFG-LA
extension by Goldberg (2011).
35http://www.spmrl.org/
spmrl2013-sharedtask-results.html.
6.1 Gold Scenarios
This section presents the parsing results in gold sce-
narios, where the systems are evaluated on gold seg-
mented and tagged input. This means that the se-
quence of terminals, POS tags, and morphological
features are provided based on the treebank anno-
tations. This scenario was used in most previous
shared tasks on data-driven parsing (Buchholz and
Marsi, 2006; Nivre et al, 2007a; K?bler, 2008). Note
that this scenario was not mandatory. We thank our
participants for providing their results nonetheless.
We start by reviewing dependency-based parsing
results, both on the trees and on multi-word expres-
sion, and continue with the different metrics for
constituency-based parsing.
6.1.1 Dependency Parsing
Full Training Set The results for the gold parsing
scenario of dependency parsing are shown in the top
block of table 3.
Among the six systems, IMS:SZEGED:CIS
reaches the highest LAS scores, not only on aver-
age, but for every single language. This shows that
their approach of combining parsers with (re)ranking
provides robust parsing results across languages with
different morphological characteristics. The second
best system is ALPAGE:DYALOG, the third best sys-
tem is MALTOPTIMIZER. The fact that AI:KU is
ranked below the Malt baseline is due to their sub-
mission of results for 6 out of the 9 languages. Simi-
larly, CADIM only submitted results for Arabic and
ranked in the third place for this language, after the
two IMS:SZEGED:CIS runs. IGM:ALPAGE and
BASQUETEAM did not submit results for this setting.
Comparing LAS results across languages is prob-
lematic due to the differences between languages,
treebank size and annotation schemes (see section 3),
so the following discussion is necessarily tentative. If
we consider results across languages, we see that the
lowest results (around 83% for the best performing
system) are reached for Hebrew and Swedish, the
languages with the smallest data sets. The next low-
est result, around 86%, is reached for Basque. Other
languages reach similar LAS scores, around 88-92%.
German, with the largest training set, reaches the
highest LAS, 91.83%.
Interstingly, all systems have high LAS scores
on the Korean Treebank given a training set size
164
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 89.83 86.68 90.29 91.83 83.87 88.06 89.59 89.58 83.97 88.19
ALPAGE:DYALOG 85.87 80.39 87.69 88.25 80.70 79.60 88.23 86.00 79.80 84.06
MALTOPTIMIZER 87.03 82.07 85.71 86.96 80.03 83.14 89.39 80.49 77.67 83.61
BASE:MALT 82.28 69.19 79.86 79.98 76.61 72.34 88.43 77.70 75.73 78.01
AI:KU 86.39 86.98 79.42 83.67 85.16 78.87 55.61
CADIM 85.56 9.51
2) gold setting / 5k training set
IMS:SZEGED:CIS 87.35 85.69 88.73 87.70 83.87 87.21 83.38 89.16 83.97 86.34
ALPAGE:DYALOG 83.25 79.11 85.66 83.88 80.70 78.42 81.91 85.67 79.80 82.04
MALTOPTIMIZER 85.30 81.40 84.93 83.59 80.03 82.37 83.74 79.79 77.67 82.09
BASE:MALT 80.36 67.13 78.16 76.64 76.61 71.27 81.93 76.64 75.73 76.05
AI:KU 84.98 83.47 79.42 82.84 84.37 78.87 54.88
CADIM 82.67 9.19
3) predicted setting / full training set
IMS:SZEGED:CIS 86.21 85.14 85.24 89.65 80.89 86.13 86.62 87.07 82.13 85.45
ALPAGE:DYALOG 81.20 77.55 82.06 84.80 73.63 75.58 81.02 82.56 77.54 79.55
MALTOPTIMIZER 81.90 78.58 79.00 82.75 73.01 79.63 82.65 79.89 75.82 79.25
BASE:MALT 80.36 70.11 77.98 77.81 69.97 70.15 82.06 75.63 73.21 75.25
AI:KU 72.57 82.32 69.01 78.92 81.86 76.35 51.23
BASQUETEAM 84.25 84.51 88.66 84.97 80.88 47.03
IGM:ALPAGE 85.86 9.54
CADIM 83.20 9.24
4) predicted setting / 5k training set
IMS:SZEGED:CIS 83.66 83.84 83.45 85.08 80.89 85.24 80.80 86.69 82.13 83.53
MALTOPTIMIZER 79.64 77.59 77.56 79.22 73.01 79.00 75.90 79.50 75.82 77.47
ALPAGE:DYALOG 78.65 76.06 80.11 73.07 73.63 74.48 73.79 82.04 77.54 76.60
BASE:MALT 78.48 68.12 76.54 74.81 69.97 69.08 74.87 75.29 73.21 73.37
AI:KU 71.23 79.16 69.01 78.04 81.30 76.35 50.57
BASQUETEAM 83.19 82.65 84.70 84.01 80.88 46.16
IGM:ALPAGE 83.60 9.29
CADIM 80.51 8.95
Table 3: Dependency parsing: LAS scores for full and 5k training sets and for gold and predicted input. Results in bold
show the best results per language and setting.
of approximately 23,000 sentences, which is a little
over half of the German treebank. For German, on
the other hand, only the IMS:SZEGED:CIS system
reaches higher LAS scores than for Korean. This
final observation indicates that more than treebank
size is important for comparing system performance
across treebanks. This is the reason for introducing
the reduced set scenario, in which we can see how the
participating system perform on a common ground,
albeit small.
5k Training Set The results for the gold setting
on the 5k train set are shown in the second block
of Table 3. Compared with the full training, we
see that there is a drop of around 2 points in this
setting. Some parser/language pairs are more sensi-
tive to data sparseness than others. CADIM, for in-
stance, exhibit a larger drop than MALTOPTIMIZER
on Arabic, and MALTOPTIMIZER shows a smaller
drop than IMS:SZEGED:CIS on French. On average,
among all systems that covered all languages, MALT-
OPTIMIZER has the smallest drop when moving to
5k training, possibly since the automatic feature opti-
mization may differ for different data set sizes.
Since all languages have the same number of sen-
tences in the train set, these results can give us limited
insight into the parsing complexity of the different
treebanks. Here, French, Arabic, Polish, and Korean
reach the highest LAS scores while Swedish reaches
165
Team F_MWE F_COMP F_MWE+POS
1) gold setting / full training set
AI:KU 99.39 99.53 99.34
IMS:SZEGED:CIS 99.26 99.39 99.21
MALTOPTIMIZER 98.95 98.99 0
ALPAGE:DYALOG 98.32 98.81 0
BASE:MALT 68.7 72.55 68.7
2) predicted setting / full training set
IGM:ALPAGE 80.81 81.18 77.37
IMS:SZEGED:CIS 79.45 80.79 70.48
ALPAGE:DYALOG 77.91 79.25 0
BASQUE-TEAM 77.19 79.81 0
MALTOPTIMIZER 70.29 74.25 0
BASE:MALT 67.49 71.01 0
AI:KU 0 0 0
3) predicted setting / 5k training set
IGM:ALPAGE 77.66 78.68 74.04
IMS:SZEGED:CIS 77.28 78.92 70.42
ALPAGE:DYALOG 75.17 76.82 0
BASQUETEAM 73.07 76.58 0
MALTOPTIMIZER 65.76 70.42 0
BASE:MALT 62.05 66.8 0
AI:KU 0 0 0
Table 4: Dependency Parsing: MWE results
the lowest one. Treebank variance depends not only
on the language but also on annotation decisions,
such as label set (Swedish, interestingly, has a rela-
tively rich one). A more careful comparison would
then take into account the correlation of data size,
label set size and parsing accuracy. We investigate
these correlations further in section 7.1.
6.1.2 Multiword Expressions
MWE results on the gold setting are found at
the top of Table 4. All systems, with the excep-
tion of BASE:MALT, perform exceedingly well in
identifying the spans and non-head components of
MWEs given gold morphology.36 These almost per-
fect scores are the consequence of the presence of
two gold MWE features, namely MWEHEAD and
PRED=Y, which respectively indicate the node span
of the whole MWE and its dependents, which do not
have a gold feature field. The interesting scenario is,
of course, the predicted one, where these features are
not provided to the parser, as in any realistic applica-
tion.
36Note that for the labeled measure F_MWE+POS, both
MALTOPTIMIZER and ALPAGE:DYALOG have an F-score of
zero, since they do not attempt to predict the MWE label at all.
6.1.3 Constituency Parsing
In this part, we provide accuracy results for phrase-
structure trees in terms of ParsEval F-scores. Since
ParsEval is sensitive to the non-terminals-per-word
ratio in the data set (Rehbein and van Genabith,
2007a; Rehbein and van Genabith, 2007b), and given
the fact that this ratio varies greatly within our data
set (as shown in Table 2), it must be kept in mind that
ParsEval should only be used for comparing parsing
performance over treebank instances sharing the ex-
act same properties in term of annotation schemes,
sentence length and so on. When comparing F-Scores
across different treebanks and languages, it can only
provide a rough estimate of the relative difficulty or
ease of parsing these kinds of data.
Full Training Set The F-score results for the gold
scenario are provided in the first block of Table 5.
Among the two baselines, BASE:BKY+POS fares
better than BASE:BKY+RAW since the latter selects
its own POS tags and thus cannot benefit from the
gold information. The IMS:SZEGED:CIS system
clearly outperforms both baselines, with Hebrew as
an outlier.37
As in the dependency case, the results are not
strictly comparable across languages, yet we can
draw some insights from them. We see consider-
able differences between the languages, with Basque,
Hebrew, and Hungarian reaching F-scores in the low
90s for the IMS:SZEGED:CIS system, Korean and
Polish reaching above-average F-scores, and Ara-
bic, French, German, and Swedish reaching F-scores
below the average, but still in the low 80s. The per-
formance is, again, not correlated with data set sizes.
Parsing Hebrew, with one of the smallest training
sets, obtains higher accuracy many other languages,
including Swedish, which has the same training set
size as Hebrew. It may well be that gold morphologi-
cal information is more useful for combatting sparse-
ness in languages with richer morphology (though
Arabic here would be an outlier for this conjecture),
or it may be that certain treebanks and schemes are
inherently harder to parser than others, as we investi-
gate in section 7.
For German, the language with the largest training
37It might be that the easy layer of syntactic tags benefits from
the gold POS tags provided. See section 4 for further discussion
of this layer.
166
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 82.20 90.04 83.98 82.07 91.64 92.60 86.50 88.57 85.09 86.97
BASE:BKY+POS 80.76 76.24 81.76 80.34 92.20 87.64 82.95 88.13 82.89 83.66
BASE:BKY+RAW 79.14 69.78 80.38 78.99 87.32 81.44 73.28 79.51 78.94 78.75
2) gold setting / 5k training set
IMS:SZEGED:CIS 79.47 88.45 82.25 74.78 91.64 91.87 80.10 88.18 85.09 84.65
BASE:BKY+POS 77.54 74.06 78.07 71.37 92.20 86.74 72.85 87.91 82.89 80.40
BASE:BKY+RAW 75.22 67.16 75.91 68.94 87.32 79.34 60.40 78.30 78.94 74.61
3) predicted setting / full training set
IMS:SZEGED:CIS 81.32 87.86 81.83 81.27 89.46 91.85 84.27 87.55 83.99 85.49
BASE:BKY+POS 78.66 74.74 79.76 78.28 85.42 85.22 78.56 86.75 80.64 80.89
BASE:BKY+RAW 79.19 70.50 80.38 78.30 86.96 81.62 71.42 79.23 79.18 78.53
4) predicted setting / 5k training set
IMS:SZEGED:CIS 78.85 86.65 79.83 73.61 89.46 90.53 78.47 87.46 83.99 83.21
BASE:BKY+POS 74.84 72.35 76.19 69.40 85.42 83.82 67.97 87.17 80.64 77.53
BASE:BKY+RAW 74.57 66.75 75.76 68.68 86.96 79.35 58.49 78.38 79.18 74.24
Table 5: Constituent Parsing: ParsEval F-scores for full and 5k training sets and for gold and predicted input. Results in
bold show the best results per language and setting.
set and the highest scores in dependency parsing,
the F-scores are at the lower end. These low scores,
which are obtained despite the larger treebank and
only moderately free word-order, are surprising. This
may be due to case syncretism; gold morphological
information exhibits its own ambiguity and thus may
not be fully utilized.
5k Training Set Parsing results on smaller com-
parable test sets are presented in the second block
of Table 5. On average, IMS:SZEGED:CIS is less
sensitive than BASE:BKY+POS to the reduced size.
Systems are not equally sensitive to reduced training
sets, and the gaps range from 0.4% to 3%, with Ger-
man and Korean as outliers (Korean suffering a 6.4%
drop in F-score and German 7.3%). These languages
have the largest treebanks in the full setting, so it is
not surprising that they suffer the most. But this in
itself does not fully explain the cross-treebank trends.
Since ParsEval scores are known to be sensitive to
the label set sizes and the depth of trees, we provide
LeafAncestor scores in the following section.
6.1.4 Leaf-Ancestor Results
The variation across results in the previous subsec-
tion may have been due to differences across annota-
tion schemes. One way to neutralize this difference
(to some extent) is to use a different metric. We
evaluated the constituency parsing results using the
Leaf-Ancestor (LA) metric, which is less sensitive
to the number of nodes in a tree (Rehbein and van
Genabith, 2007b; K?bler et al, 2008). As shown in
Table 6, these results are on a different (higher) scale
than ParsEval, and the average gap between the full
and 5k setting is lower.
Full Training Set The LA results in gold setting
for full training sets are shown in the first block of Ta-
ble 6. The trends are similar to the ParsEval F-scores.
German and Arabic present the lowest LA scores
(in contrast to the corresponding F-scores, Arabic is
a full point below German for IMS:SZEGED:CIS).
Basque and Hungarian have the highest LA scores.
Hebrew, which had a higher F-score than Basque,
has a lower LA than Basque and is closer to French.
Korean also ranks worse in the LA analysis. The
choice of evaluation metrics thus clearly impacts sys-
tem rankings ? F-scores rank some languages suspi-
ciously high (e.g., Hebrew) due to deeper trees, and
another metric may alleviate that.
5k Training Set The results for the leaf-ancestor
(LA) scores in the gold setting for the 5k training set
are shown in the second block of Table 6. Across
167
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 88.61 94.90 92.51 89.63 92.84 95.01 91.30 94.52 91.46 92.31
BASE:BKY+POS 87.85 91.55 91.74 88.47 92.69 92.52 90.82 92.81 90.76 91.02
BASE:BKY+RAW 87.05 89.71 91.22 87.77 91.29 90.62 87.11 90.58 88.97 89.37
2) gold setting / 5k training set
IMS:SZEGED:CIS 86.68 94.21 91.56 85.74 92.84 94.79 88.87 94.17 91.46 91.15
BASE:BKY+POS 86.26 90.72 89.71 84.11 92.69 92.11 86.75 92.91 90.76 89.56
BASE:BKY+RAW 84.97 88.68 88.74 83.08 91.29 89.94 81.82 90.31 88.97 87.53
3) predicted setting / full training set
IMS:SZEGED:CIS 88.45 94.50 91.79 89.32 91.95 94.90 90.13 94.11 91.05 91.80
BASE:BKY+POS 86.60 90.90 90.96 87.46 89.66 91.72 89.10 92.56 89.51 89.83
BASE:BKY+RAW 86.97 89.91 91.11 87.46 90.77 90.50 86.68 90.48 89.16 89.23
4) predicted setting / 5k training set
IMS:SZEGED:CIS 86.69 93.85 90.76 85.20 91.95 94.05 87.99 93.99 91.05 90.61
BASE:BKY+POS 84.76 89.83 89.18 83.05 89.66 91.24 84.87 92.74 89.51 88.32
BASE:BKY+RAW 84.63 88.50 89.00 82.69 90.77 89.93 81.50 90.08 89.16 87.36
Table 6: Constituent Parsing: Leaf-Ancestor scores for full and 5k training sets and for gold and predicted input.
parsers, IMS:SZEGED:CIS again has a smaller drop
than BASE:BKY+POS on the reduced size. German
suffers the most from the reduction of the training
set, with a loss of approximately 4 points. Korean,
however, which was also severely affected in terms
of F-scores, only loses 1.17 points in the LA score.
On average, the LA seem to reflect a smaller drop
when reducing the training set ? this underscores
again the impact of the choice of metrics on system
evaluation.
6.2 Predicted Scenarios
Gold scenarios are relatively easy since syntactically
relevant morphological information is disambiguated
in advance and is provided as input. Predicted scenar-
ios are more difficult: POS tags and morphological
features have to be automatically predicted, by the
parser or by external resources.
6.2.1 Dependency Parsing
Eight participating teams submitted dependency
results for this scenario. Two teams submitted for a
single language. Four teams covered all languages.
Full Training Set The results for the predicted
scenario in full settings are shown in the third
block of Table 3. Across the board, the re-
sults are considerably lower than the gold sce-
nario. Again, IMS:SZEGED:CIS is the best per-
forming system, followed by ALPAGE:DYALOG and
MALTOPTIMIZER. The only language for which
IMS:SZEGED:CIS is outperformed is French, for
which IGM:ALPAGE reaches higher results (85.86%
vs. 85.24%). This is due to the specialized treatment
of French MWEs in the IGM:ALPAGE system, which
is thereby shown to be beneficial for parsing in the
predicted setting.
If we compare the results for the predicted set-
ting and the gold one, given the full training set,
the IMS:SZEGED:CIS system shows small differ-
ences between 1.5 and 2 percent. The only ex-
ception is French, for which the LAS drops from
90.29% to 85.24% in the predicted setting. The
other systems show somewhat larger differences than
IMS:SZEGED:CIS, with the highest drops for Ara-
bic and Korean. The AI:KU system shows a similar
problem as IMS:SZEGED:CIS for French.
5k Training Set When we consider the predicted
setting for the 5k training set, in the last block of
Table 3, we see the same trends as comparing with
the full training set or when comparing to the gold
setting. Systems suffer from not having gold stan-
dard data, and they suffer from the small training set.
Interestingly, the loss between the different training
set sizes in the predicted setting is larger than in the
168
gold setting, but only marginally so, with a differ-
ence < 0.5. In other words, the predicted setting
adds a challenge to parsing, but it only minimally
compounds data sparsity.
6.2.2 Multiword Expressions Evaluation
In the predicted setting, shown in the second
block of table 4 for the full training set and in the
third block of the same table for the 5k training set,
we see that only two systems, IGM:ALPAGE and
IMS:SZEGED:CIS can predict the MWE label when
it is not present in the training set. IGM:ALPAGE?s
approach of using a separate classifier in combination
with external dictionaries is very successful, reach-
ing an F_MWE+POS score of 77.37. This is com-
pared to the score of 70.48 by IMS:SZEGED:CIS,
which predicts this node label as a side effect of
their constituent feature enriched dependency model
(Bj?rkelund et al, 2013). AI:KU has a zero score
for all predicted settings, which results from an erro-
neous training on the gold data rather than the pre-
dicted data.38
6.2.3 Constituency Parsing
Full Training Set The results for the predicted set-
ting with the full training set are shown in the third
block of table 5. A comparison with the gold setting
shows that all systems have a lower performance in
the predicted scenario, and the differences are in the
range of 0.88 for Arabic and 2.54 for Basque. It is
interesting to see that the losses are generally smaller
than in the dependency framework: on average, the
loss across languages is 2.74 for dependencies and
1.48 for constituents. A possible explanation can be
found in the two-dimensional structure of the con-
stituent trees, where only a subset of all nodes is
affected by the quality of morphology and POS tags.
The exception to this trend is Basque, for which the
loss in constituents is a full point higher than for de-
pendencies. Another possible explanation is that all
of our constituent parsers select their own POS tags
in one way or another. Most dependency parsers ac-
cept predicted tags from an external resource, which
puts an upper-bound on their potential performance.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the bottom
38Unofficial updated results are to to be found in (Cirik and
S?ensoy, 2013)
block of table 5. They show the same trends as the
dependency ones: The results are slightly lower than
the results obtained in gold setting and the ones uti-
lizing the full training set.
6.2.4 Leaf Ancestor Metrics
Full Training Set The results for the predicted sce-
nario with a full training set are shown in the third
block of table 6. In the LA evaluation, the loss
in moving from gold morphology are considerably
smaller than in F-scores. For most languages, the
loss is less than 0.5 points. Exceptions are French
with a loss of 0.72, Hebrew with 0.89, and Korean
with 1.17. Basque, which had the highest loss in
F-scores, only shows a minor loss of 0.4 points. Also,
the average loss of 0.41 points is much smaller than
the one in the ParsEval score, 1.48.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the last
block of table 6. These results, though considerably
lower (around 3 points), exhibit the exact same trends
as observed in the gold setting.
6.3 Realistic Raw Scenarios
The previous scenarios assume that input surface to-
kens are identical to tree terminals. For languages
such as Arabic and Hebrew, this is not always the
case. In this scenario, we evaluate the capacity of a
system to predict both morphological segmentation
and syntactic parse trees given raw, unsegmented
input tokens. This may be done via a pipeline as-
suming a 1-st best morphological analysis, or jointly
with parsing, assuming an ambiguous morpholog-
ical analysis lattice as input. In this task, both of
these scenarios are possible (see section 3). Thus,
this section presents a realistic evaluation of the par-
ticipating systems, using TedEval, which takes into
account complete morpho-syntactic parses.
Tables 7 and 8 present labeled and unlabeled
TedEval results for both constituency and depen-
dency parsers, calculated only for sentence of length
<= 70.39 We firstly observe that labeled TedEval
scores are considerably lower than unlabeled Ted-
Eval scores, as expected, since unlabeled scores eval-
uate only structural differences. In the labeled setup,
39TedEval builds on algorithms for calculating edit distance
on complete trees (Bille, 2005). In these algorithms, longer
sentences take considerably longer to evaluate.
169
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 83.34 1.63 82.54 0.67 56.47 0.67 69.51 69.51
IMS:SZEGED:CIS 89.12 8.37 87.82 5.56 86.08 8.27 86.95 86.95
CADIM 87.81 6.63 86.43 4.21 - - 43.22 86.43
MALTOPTIMIZER 86.74 5.39 85.63 3.03 83.05 5.33 84.34 84.34
ALPAGE:DYALOG 86.60 5.34 85.71 3.54 82.96 6.17 41.48 82.96
ALPAGE:DYALOG (RAW) - - - - 82.82 4.35 41.41 82.82
AI:KU - - - - 78.57 3.37 39.29 78.57
Table 7: Realistic Scenario: Tedeval Labeled Accuracy and Exact Match for the Raw scenario.
The upper part refers to constituency results, the lower part refers to dependency results
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 92.06 9.49 91.29 7.13 89.30 13.60 90.30 90.30
IMS:SZEGED:CIS 91.74 9.83 90.85 7.30 89.47 16.97 90.16 90.16
ALPAGE:DYALOG 89.99 7.98 89.46 5.67 88.33 12.20 88.90 88.90
MALTOPTIMIZER 90.09 7.08 89.47 5.56 87.99 11.64 88.73 88.73
CADIM 90.75 8.48 89.89 5.67 - - 44.95 89.89
ALPAGE:DYALOG (RAW) - - - - 87.61 10.24 43.81 87.61
AI:KU - - - - 86.70 8.98 43.35 86.70
Table 8: Realistic Scenario: Tedeval Unlabeled Accuracy and Exact Match for the Raw scenario.
Top upper part refers to constituency results, the lower part refers to dependency results.
the IMS:SZEGED:CIS dependency parser are the
best for both languages and data set sizes. Table 8
shows that their unlabeled constituency results reach
a higher accuracy than the next best system, their
own dependency results. However, a quick look at
the exact match metric reveals lower scores than for
its dependency counterparts.
For the dependency-based joint scenarios, there
is obviously an upper bound on parser performance
given inaccurate segmentation. The transition-based
systems, ALPAGE:DYALOG & MALTOPTIMIZER,
perform comparably on Arabic and Hebrew, with
ALPAGE:DYALOG being slightly better on both lan-
guages. Note that ALPAGE:DYALOG reaches close
results on the 1-best and the lattice-based input set-
tings, with a slight advantage for the former. This is
partly due to the insufficient coverage of the lexical
resource we use: many lattices do not contain the
gold path, so the joint prediction can only as be high
as the lattice predicted path allows.
7 Towards In-Depth Cross-Treebank
Evaluation
Section 6 reported evaluation scores across systems
for different scenarios. However, as noted, these re-
sults are not comparable across languages, represen-
tation types and parsing scenarios due to differences
in the data size, label set size, length of sentences and
also differences in evaluation metrics.
Our following discussion in the first part of this
section highlights the kind of impact that data set
properties have on the standard metrics (label set size
on LAS, non-terminal nodes per sentence on F-score).
Then, in the second part of this section we use the
TedEval cross-experiment protocols for comparative
evaluation that is less sensitive to representation types
and annotation idiosyncrasies.
7.1 Parsing Across Languages and Treebanks
To quantify the impact of treebank characteristics on
parsing parsing accuracy we looked at correlations
of treebank properties with parsing results. The most
highly correlated combinations we have found are
shown in Figures 2, 3, and 4 for the dependency track
and the constituency track (F-score and LeafAnces-
170
21/09/13 03:00SPMRL charts
Page 3 sur 3http://pauillac.inria.fr/~seddah/updated_official.spmrl_results.html
Correlation between label set size, treebank size, and mean LAS
FrP
FrP
GeP
GeP
HuP
HuP
SwP
ArP
ArP
ArG
ArG
BaP
BaP
FrG
FrG
GeG
GeG
HeP
HeG
HuG
HuG
PoP
PoP
PoG
PoG
SwG
BaG
BaG
KoP
KoP
KoG
KoG
10 50 100 500 1 000
72
74
76
78
80
82
84
86
88
90
treebank size / #labels
L
A
S
 
(
%
)
Figure 2: The correlation between treebank size, label set size, and LAS scores. x: treebank size / #labels ; y: LAS (%)
01/10/13 00:43SPMRL charts: all sent.
Page 1 sur 5file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-S?/SPMRL_FINAL/RESULTS/OFFICIAL/official_ptb-all.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, all sent.)
(13/10/01 00:34:34
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean F1
Arabic
Basque
French
German
Hebrew
Hungarian
Korean
Polish
Swedish
ArP
ArG
BaP
BaG
FrP
FrG
GeP
GeG
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
8 9 10
72
74
76
78
80
82
84
86
88
90
92
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 3: The correlation between the non terminals per sentence ratio and F-scores. x: #non terminal/ #sentence ; y:
F1 (%)
171
tor) respectively.
Figure 2 presents the LAS against the average num-
ber of tokens relative to the number of labels. The
numbers are averaged per language over all partici-
pating systems, and the size of the ?bubbles? is pro-
portional to the number of participants for a given
language setting. We provide ?bubbles? for all lan-
guages in the predicted (-P) and gold (-G) setting,
for both training set sizes. The lower dot in terms
of parsing scores always corresponds to the reduced
training set size.
Figure 2 shows a clear correlation between data-
set complexity and parsing accuracy. The simpler
the data set is (where ?simple" here translates into
large data size with a small set of labels), the higher
the results of the participating systems. The bubbles
reflects a diagonal that indicates correlation between
these dimensions. Beyond that, we see two interest-
ing points off of the diagonal. The Korean treebank
(pink) in the gold setting and full training set can be
parsed with a high LAS relative to its size and label
set. It is also clear that the Hebrew treebank (purple)
in the predicted version is the most difficult one to
parse, relative to our expectation about its complexity.
Since the Hebrew gold scenario is a lot closer to the
diagonal again, it may be that this outlier is due to the
coverage and quality of the predicted morphology.
Figure 340 shows the correlation of data complex-
ity in terms of the average number of non-terminals
per sentence, and parsing accuracy (ParsEval F-
score). Parsing accuracy is again averaged over all
participating systems for a given language. In this
figure, we see a diagonal similar to the one in figure 2,
where Arabic (dark blue) has high complexity of the
data (here interpreted as flat trees, low number of
non terminals per sentence) and low F-scores accord-
ingly. Korean (pink), Swedish (burgundy), Polish
(light green), and Hungarian (light blue) follow, and
then Hebrew (purple) is a positive outlier, possibly
due to an additional layer of ?easy" syntactic POS
nodes which increases tree size and inflates F-scores.
French (orange), Basque (red), and German (dark
green) are negative outliers, falling off the diago-
nal. German has the lowest F-score with respect to
40This figure was created from the IMS:SZEGED:CIS
(Const.) and our own PCFG-LA baseline in POS Tagged mode
(BASE:BKY+POS) so as to avoid the noise introduced by the
parser?s own tagging step (BASE:BKY+RAW).
what would be expected for the non-terminals per
sentence ratio, which is in contrast to the LAS fig-
ure where German occurs among the less complex
data set to parse. A possible explanation may be
the crossing branches in the original treebank which
were re-attached. This creates flat and variable edges
which might be hard predict accurately.
Figure 441 presents the correlation between parsing
accuracy in terms the LeafAncestor metrics (macro
averaged) and treebank complexity in terms of the
average number of non-terminals per sentence. As
in the correlation figures, the parsing accuracy is
averaged over the participanting systems for any lan-
guage. The LeafAncestor accuracy is calculated over
phrase structure trees, and we see a similar diago-
nal to the one in Figure 3 showing that flatter tree-
banks are harder (that is, are correlated with lower
averaged scores) But, its slope is less steep than for
the F-score, which confirms the observation that the
LeafAncestor metric is less sensitive than F-score to
the non-terminals-per-sentence ratio.
Similarly to Figure 3, German is a negative outlier,
which means that this treebank is harder to parse ? it
obtains lower scores on average than we would ex-
pect. As for Hebrew, it is much closer to the diagonal.
As it turns out, the "easy" POS layer that inflates the
scores does not affect the LA ratings as much.
7.2 Evaluation Across Scenarios, Languages
and Treebanks
In this section we analyze the results in cross-
scenario, cross-annotation, and cross-framework set-
tings using the evaluation protocols discussed in
(Tsarfaty et al, 2012b; Tsarfaty et al, 2011; Tsarfaty
et al, 2012a).
As a starting point, we select comparable sections
of the parsed data, based on system runs trained on
the small train set (train5k). For those, we selected
subsets containing the first 5,000 tree terminals (re-
specting sentence boundaries) of the test set. We only
used TedEval on sentences up to 70 terminals long,
and projectivized non-projective sentences in all sets.
We use the TedEval metrics to calculate scores on
both constituency and dependency structures in all
languages and all scenarios. Since the metric de-
fines one scale for all of these different cases, we can
41This figure was created under the same condition as the
F-score correlation in figure (Figure 3).
172
04/10/13 23:05SPMRL charts:
Page 1 sur 6file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-SHAREDTASK/SPMRL_FINAL/RESULTS/TESTLEAF.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, )
(13/10/04 23:05:31
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean Leaf Accuracy
ArP
ArG
BaP
BaG
FrP
FrG
GeP
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
GeG
8 9 10
74
76
78
80
82
84
86
88
90
92
94
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 4: The correlation between the non terminals per sentence ratio and Leaf Accuracy (macro) scores. x: #non
terminal/ #sentence ; y: Acc.(%)
compare the performance across annotation schemes,
assuming that those subsets are representative of their
original source.42
Ideally, we would be using labeled TedEval scores,
as the labeled parsing task is more difficult, and la-
beled parses are far more informative than unlabeled
ones. However, most constituency-based parsers do
not provide function labels as part of the output, to
be compared with the dependency arcs. Furthermore,
as mentioned earlier, we observed a huge difference
between label set sizes for the dependency runs. Con-
sequently, labeled scores will not be as informative
across treebanks and representation types. We will
therefore only use labels across scenarios for the
same language and representation type.
42We choose this sample scheme for replicability. We first
tried sampling sentences, aiming at the same average sentence
length (20), but that seemed to create artificially difficult test sets
for languages as Polish and overly simplistic ones for French or
Arabic.
7.2.1 Cross-Scenario Evaluation: raw vs. gold
One novel aspect of this shared task is the evalu-
ation on non-gold segmentation in addition to gold
morphology. One drawback is that the scenarios are
currently not using the same metrics ? the metrics
generally applied for gold and predicted scenrios can-
not apply for raw. To assess how well state of the art
parsers perform in raw scenarios compared to gold
scenarios, we present here TedEval results comparing
raw and gold systems using the evaluation protocol
of Tsarfaty et al (2012b).
Table 9 presents the labeled and unlabeled results
for Arabic and Hebrew (in Full and 5k training set-
tings), and Table 10 presents unlabeled TedEval re-
sults (for all languages) in the gold settings. The
unlabeled TedEval results for the raw settings are
substantially lower then TedEval results on the gold
settings for both languages.
When comparing the unlabeled TedEval results for
Arabic and Hebrew on the participating systems, we
see a loss of 3-4 points between Table 9 (raw) and Ta-
ble 10 (gold). In particular we see that for the best per-
173
forming systems on Arabic (IMS:SZEGED:CIS for
both constituency and dependency), the gap between
gold and realistic scenarios is 3.4 and 4.3 points,
for the constituency and the dependency parser re-
spectively. These results are on a par with results
by Tsarfaty et al (2012b), who showed for different
settings, constituency and dependency based, that
raw scenarios are considerably more difficult to parse
than gold ones on the standard split of the Modern
Hebrew treebank.
For Hebrew, the performance gap between unla-
beled TedEval in raw (Table 9) and gold (Table 10)
is even more salient, with around 7 and 8 points of
difference between the scenarios. We can only specu-
late that such a difference may be due to the difficulty
of resolving Hebrew morpho-syntactic ambiguities
without sufficient syntactic information. Since He-
brew and Arabic now have standardized morpholog-
ically and syntactically analyzed data sets available
through this task, it will be possible to investigate
further how cross-linguistic differences in morpho-
logical ambiguity affect full-parsing accuracy in raw
scenarios.
This section compared the raw and gold parsing
results only on unlabeled TedEval metrics. Accord-
ing to what we have seen so far is expected that
for labeled TedEval metrics using the same protocol,
the gap between gold and raw scenario will be even
greater.
7.2.2 Cross-Framework Evaluation:
Dependency vs. Constituency
In this section, our focus is on comparing parsing
results across constituency and dependency parsers
based on the protocol of Tsarfaty et al (2012a) We
have only one submission from IMS:SZEGED:CIS
in the constituency track, and. from the same group,
a submission on the dependency track. We only com-
pare the IMS:SZEGED:CIS results on constituency
and dependency parsing with the two baselines we
provided. The results of the cross-framework evalua-
tion protocol are shown in Table 11.
The results comparing the two variants of the
IMS:SZEGED:CIS systems show that they are very
close for all languages, with differences ranging from
0.03 for German to 0.8 for Polish in the gold setting.
It has often been argued that dependency parsers
perform better than a constituency parser, but we
notice that when using a cross framework protocol,
such as TedEval, and assuming that our test set sam-
ple is representative, the difference between the in-
terpretation of both representation?s performance is
alleviated. Of course, here the metric is unlabeled, so
it simply tells us that both kind of parsing models are
equally able to provide similar tree structures. Said
differently, the gaps in the quality of predicting the
same underlying structure across representations for
MRLs is not as large as is sometimes assumed.
For most languages, the baseline constituency
parser performs better than the dependency base-
line one, with Basque and Korean as an exception,
and at the same time, the dependency version of
IMS:SZEGED:CIS performs slightly better than their
constituent parser for most languages, with the excep-
tion of Hebrew and Hungarian. It goes to show that,
as far as these present MRL results go, there is no
clear preference for a dependency over a constituency
parsing representation, just preferences among par-
ticular models.
More generally, we can say that even if the linguis-
tic coverage of one theory is shown to be better than
another one, it does not necessarily mean that the
statistical version of the formal theory will perform
better for structure prediction. System performance
is more tightly related to the efficacy of the learning
and search algorithms, and feature engineering on
top of the selected formalism.
7.2.3 Cross-Language Evaluation: All
Languages
We conclude with an overall outlook of the Ted-
Eval scores across all languages. The results on the
gold scenario, for the small training set and the 5k
test set are presented in Table 10. We concentrate
on gold scenarios (to avoid the variation in cover-
age of external morphological analyzers) and choose
unlabeled metrics as they are not sensitive to label
set sizes. We emphasize in bold, for each parsing
system (row in the table), the top two languages that
most accurately parsed by it (boldface) and the two
languages it performed the worse on (italics).
We see that the European languages German
and Hungarian are parsed most accurately in the
constituency-based setup, with Polish and Swedish
having an advantage in dependency parsing. Across
all systems, Korean is the hardest to parse, with Ara-
174
Arabic Hebrew AVG1 SOFT AVG Arabic Hebrew AVG2 SOFT AVG2
1) Constituency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS (Bky) 83.59 56.43 70.01 70.01 92.18 88.02 90.1 90.1
2) Dependency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS 88.61 84.74 86.68 86.68 91.41 88.58 90 90
ALPAGE:DYALOG 87.20 81.65 40.83 81.65 90.74 87.44 89.09 89.09
CADIM 87.99 - 44 87.99 91.22 - 45.61 91.22
MALTOPTIMIZER 86.62 81.74 43.31 86.62 90.26 87.00 45.13 90.26
ALPAGE:DYALOG (RAW) - 82.82 41.41 82.82 - 87.43 43.72 87.43
AI:KU - 77.8 38.9 77.8 - 85.87 42.94 85.87
Table 9: Labeled and Unlabeled TedEval Results for raw Scenarios, Trained on 5k sentences and tested on 5k terminals.
The upper part refers to constituency parsing and the lower part refers to dependency parsing.
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) Constituency Evaluation
IMS:SZEGED:CIS (Bky) 95.35 96.91 95.98 97.12 96.22 97.92 92.91 97.19 96.65
BASE:BKY+POS 95.11 94.69 95.08 97.01 95.85 97.08 90.55 96.99 96.38
BASE:BKY+RAW 94.58 94.32 94.72 96.74 95.64 96.15 87.08 95.93 95.90
2) Dependency Evaluation
IMS:SZEGED:CIS 95.76 97.63 96.59 96.88 96.29 97.56 94.62 98.01 97.22
ALPAGE:DYALOG 93.76 95.72 95.75 96.4 95.34 95.63 94.56 96.80 96.55
BASE:MALT 94.16 95.08 94.21 94.55 94.98 95.25 94.27 95.83 95.33
AI:KU - - 95.46 96.34 95.07 96.53 - 96.88 95.87
MALTOPTIMIZER 94.91 96.82 95.23 96.32 95.46 96.30 94.69 96.06 95.90
CADIM 94.66 - - - - - - - -
Table 10: Cross-Language Evaluation: Unlabeled TedEval Results in gold input scenario, On a 5k-sentences set set and
a 5k-terminals test set. The upper part refers to constituency parsing and the lower part refers to dependency parsing.
For each system we mark the two top scoring languages in bold and the two lowest scoring languages in italics.
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) gold setting
IMS:SZEGED:CIS (Bky) 95.82 97.30 96.15 97.43 96.37 98.25 94.07 97.22 96.89
IMS:SZEGED:CIS 95.87 98.06 96.61 97.46 96.31 97.93 94.62 98.04 97.24
BASE:BKY+POS 95.61 95.25 95.48 97.31 96.03 97.53 92.15 96.97 96.66
BASE:MALT 94.26 95.76 94.23 95.53 95.00 96.09 94.27 95.90 95.35
2) predicted setting
IMS:SZEGED:CIS (Bky) 95.74 97.07 96.21 97.31 96.10 98.03 94.05 96.92 96.90
IMS:SZEGED:CIS 95.18 97.67 96.15 97.09 96.22 97.63 94.43 97.50 97.02
BASE:BKY+POS 95.03 95.35 97.12 95.36 97.20 91.34 96.92 96.25
BASE:MALT 95.49 93.84 95.39 94.41 95.72 93.74 96.04 95.09
Table 11: Cross Framework Evaluation: Unlabeled TedEval on generalized gold trees in gold scenario, trained on 5k
sentences and tested on 5k terminals.
bic, Hebrew and to some extent French following. It
appears that on a typological scale, Semitic and Asian
languages are still harder to parse than a range of Eu-
ropean languages in terms of structural difficulty and
complex morpho-syntactic interaction. That said,
note that we cannot tell why certain treebanks appear
more challenging to parse then others, and it is still
unclear whether the difficulty is inherent on the lan-
guage, in the currently available models, or because
of the annotation scheme and treebank consistency.43
43The latter was shown to be an important factor orthogonal
to the morphologically-rich nature of the treebank?s language
175
8 Conclusion
This paper presents an overview of the first shared
task on parsing morphologically rich languages. The
task features nine languages, exhibiting different lin-
guistic phenomena and varied morphological com-
plexity. The shared task saw submissions from seven
teams, and results produced by more than 14 different
systems. The parsing results were obtained in dif-
ferent input scenarios (gold, predicted, and raw) and
evaluated using different protocols (cross-framework,
cross-scenario, and cross-language). In particular,
this is the first time an evaluation campaign reports
on the execution of parsers in realistic, morphologi-
cally ambiguous, setting.
The best performing systems were mostly ensem-
ble systems combining multiple parser outputs from
different frameworks or training runs, or integrat-
ing a state-of-the-art morphological analyzer on top
of a carefully designed feature set. This is con-
sistent with previous shared tasks such as ConLL
2007 or SANCL?2012. However, dealing with am-
biguous morphology is still difficult for all systems,
and a promising approach, as demonstrated by AL-
PAGE:DYALOG, is to deal with parsing and morphol-
ogy jointly by allowing lattice input to the parser. A
promising generalization of this approach would be
the full integration of all levels of analysis that are
mutually informative into a joint model.
The information to be gathered from the results of
this shared task is vast, and we only scratched the
surface with our preliminary analyses. We uncov-
ered and documented insights of strategies that make
parsing systems successful: parser combination is
empirically proven to reach a robust performance
across languages, though language-specific strategies
are still a sound avenue for obtaining high quality
parsers for that individual language. The integration
of morphological analysis into the parsing needs to
be investigated thoroughly, and new approaches that
are morphologically aware need to be developed.
Our cross-parser, cross-scenario, and cross-
framework evaluation protocols have shown that, as
expected, more data is better, and that performance
on gold morphological input is significantly higher
than that in more realistic scenarios. We have shown
that gold morphological information is more help-
(Schluter and van Genabith, 2007)
ful to some languages and parsers than others, and
that it may also interact with successful identification
of multiword expressions. We have shown that dif-
ferences between dependency and constituency are
smaller than previously assumed and that properties
of the learning model and granularity of the output
labels are more influential. Finally, we observed
that languages which are typologically farthest from
English, such as Semitic and Asian languages, are
still amongst the hardest to parse, regardless of the
parsing method used.
Our cross-treebank, in-depth analysis is still pre-
liminary, owing to the limited time between the end
of the shared task and the deadline for publication
of this overview. but we nonetheless feel that our
findings may benefit researchers who aim to develop
parsers for diverse treebanks.44
A shared task is an inspection of the state of the
art, but it may also accelerate research in an area
by providing a stable data basis as well as a set of
strong baselines. The results produced in this task
give a rich picture of the issues associated with pars-
ing MRLs and initial cues towards their resolution.
This set of results needs to be further analyzed to be
fully understood, which will in turn contribute to new
insights. We hope that this shared task will provide
inspiration for the design and evaluation of future
parsing systems for these languages.
Acknowledgments
We heartily thank Miguel Ballesteros and Corentin
Ribeire for running the dependency and constituency
baselines. We warmly thank the Linguistic Data Con-
sortium: Ilya Ahtaridis, Ann Bies, Denise DiPersio,
Seth Kulick and Mohamed Maamouri for releasing
the Arabic Penn Treebank for this shared task and
for their support all along the process. We thank
Alon Itai and MILA, the knowledge center for pro-
cessing Hebrew, for kindly making the Hebrew tree-
bank and morphological analyzer available for us,
Anne Abeill? for allowing us to use the French tree-
bank, and Key-Sun Choi for the Kaist Korean Tree-
bank. We thank Grzegorz Chrupa?a for providing
the morphological analyzer Morfette, and Joachim
44The data set will be made available as soon as possible under
the license distribution of the shared-task, with the exception
of the Arabic data, which will continue to be distributed by the
LDC.
176
Wagner for his LeafAncestor implementation. We
finally thank ?zlem ?etinog?lu, Yuval Marton, Benoit
Crabb? and Benoit Sagot who have been nothing but
supportive during all that time.
At the end of this shared task (though watch out
for further updates and analyses), what remains to be
mentioned is our deep gratitude to all people involved,
either data providers or participants. Without all of
you, this shared task would not have been possible.
References
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel.
2003. Building a treebank for French. In Anne Abeill?,
editor, Treebanks. Kluwer, Dordrecht.
Szymon Acedan?ski. 2010. A Morphosyntactic Brill Tag-
ger for Inflectional Languages. In Advances in Natural
Language Processing, volume 6233 of Lecture Notes
in Computer Science, pages 3?14. Springer-Verlag.
Meni Adler and Michael Elhadad. 2006. An unsupervised
morpheme-based HMM for Hebrew morphological dis-
ambiguation. In Proceedings COLING-ACL, pages
665?672, Sydney, Australia.
Meni Adler, Yoav Goldberg, David Gabay, and Michael
Elhadad. 2008. Unsupervised lexicon-based resolution
of unknown words for full morphological analysis. In
Proceedings of ACL-08: HLT, pages 728?736, Colum-
bus, OH.
Meni Adler. 2007. Hebrew Morphological Disambigua-
tion: An Unsupervised Stochastic Word-based Ap-
proach. Ph.D. thesis, Ben-Gurion University of the
Negev.
Itziar Aduriz, Jos? Mar?a Arriola, Xabier Artola, A D?az
de Ilarraza, et al 1997. Morphosyntactic disambigua-
tion for Basque based on the constraint grammar for-
malism. In Proceedings of RANLP, Tzigov Chark, Bul-
garia.
Itziar Aduriz, Eneko Agirre, Izaskun Aldezabal, I?aki
Alegria, Xabier Arregi, Jose Maria Arriola, Xabier Ar-
tola, Koldo Gojenola, Aitor Maritxalar, Kepa Sarasola,
et al 2000. A word-grammar based morphological
analyzer for agglutinative languages. In Proceedings
of COLING, pages 1?7, Saarbr?cken, Germany.
Itziar Aduriz, Maria Jesus Aranzabe, Jose Maria Arriola,
Aitziber Atutxa, A Diaz de Ilarraza, Aitzpea Garmen-
dia, and Maite Oronoz. 2003. Construction of a
Basque dependency treebank. In Proceedings of the
2nd Workshop on Treebanks and Linguistic Theories
(TLT), pages 201?204, V?xj?, Sweden.
Zeljko Agic, Danijela Merkler, and Dasa Berovic. 2013.
Parsing Croatian and Serbian by using Croatian depen-
dency treebanks. In Proceedings of the Fourth Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Seattle, WA.
I. Aldezabal, M.J. Aranzabe, A. Diaz de Ilarraza, and
K. Fern?ndez. 2008. From dependencies to con-
stituents in the reference corpus for the processing of
Basque. In Procesamiento del Lenguaje Natural, no
41 (2008), pages 147?154. XXIV edici?n del Congreso
Anual de la Sociedad Espa?ola para el Procesamiento
del Lenguaje Natural (SEPLN).
Bharat Ram Ambati, Samar Husain, Joakim Nivre, and
Rajeev Sangal. 2010. On the role of morphosyntactic
features in Hindi dependency parsing. In Proceedings
of the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010), Los
Angeles, CA.
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statistical
latent-variable parsing models for Arabic, English and
French. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL), Los Angeles, CA.
Miguel Ballesteros and Joakim Nivre. 2012. MaltOpti-
mizer: An optimization tool for MaltParser. In Pro-
ceedings of EACL, pages 58?62, Avignon, France.
Miguel Ballesteros. 2013. Effective morphological fea-
ture selection with MaltOptimizer at the SPMRL 2013
shared task. In Proceedings of the Fourth Workshop on
Statistical Parsing of Morphologically-Rich Languages,
pages 53?60, Seattle, WA.
Kepa Bengoetxea and Koldo Gojenola. 2010. Appli-
cation of different techniques to dependency parsing
of Basque. In Proceedings of the NAACL/HLT Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL 2010), Los Angeles, CA.
Philip Bille. 2005. A survey on tree edit distance and re-
lated problems. Theoretical Computer Science, 337(1?
3):217?239, 6.
Anders Bj?rkelund, Ozlem Cetinoglu, Rich?rd Farkas,
Thomas Mueller, and Wolfgang Seeker. 2013.
(Re)ranking meets morphosyntax: State-of-the-art re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing
of Morphologically-Rich Languages, pages 134?144,
Seattle, WA.
Ezra Black, Steven Abney, Dan Flickinger, Claudia
Gdaniec, Ralph Grishman, Philip Harrison, Donald
Hindle, Robert Ingria, Frederick Jelinek, Judith Kla-
vans, Mark Liberman, Mitchell Marcus, Salim Roukos,
Beatrice Santorini, and Tomek Strzalkowski. 1991. A
procedure for quantitatively comparing the syntactic
coverage of English grammars. In Proceedings of the
DARPA Speech and Natural Language Workshop 1991,
pages 306?311, Pacific Grove, CA.
177
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Proceed-
ings of the EMNLP-CoNLL, pages 1455?1465, Jeju,
Korea.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of COL-
ING, pages 89?97, Beijing, China.
Adriane Boyd. 2007. Discontinuity revisited: An im-
proved conversion to context-free representations. In
Proceedings of the Linguistic Annotation Workshop,
Prague, Czech Republic.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER treebank.
In Proceedings of the First Workshop on Treebanks
and Linguistic Theories (TLT), pages 24?41, Sozopol,
Bulgaria.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164, New York, NY.
Tim Buckwalter. 2002. Arabic morphological analyzer
version 1.0. Linguistic Data Consortium.
Tim Buckwalter. 2004. Arabic morphological analyzer
version 2.0. Linguistic Data Consortium.
Marie Candito and Djam? Seddah. 2010. Parsing word
clusters. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Marie Candito, Benoit Crabb?, and Pascal Denis. 2010.
Statistical French dependency parsing: Treebank con-
version and first results. In Proceedings of LREC, Val-
letta, Malta.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for ef-
ficient, feature-rich parsing. In Proceedings of CoNLL,
pages 9?16, Manchester, UK.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of ACL, pages 173?180, Barcelona,
Spain.
Eugene Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In AAAI/IAAI, pages
598?603.
Eugene Charniak. 2000. A maximum entropy inspired
parser. In Proceedings of NAACL, pages 132?139, Seat-
tle, WA.
Jinho D. Choi and Martha Palmer. 2011. Statistical de-
pendency parsing in Korean: From corpus generation
to automatic parsing. In Proceedings of Second Work-
shop on Statistical Parsing of Morphologically Rich
Languages, pages 1?11, Dublin, Ireland.
Jinho D. Choi and Martha Palmer. 2012. Guidelines
for the Clear Style Constituent to Dependency Conver-
sion. Technical Report 01-12, University of Colorado
at Boulder.
Key-sun Choi, Young S. Han, Young G. Han, and Oh W.
Kwon. 1994. KAIST Tree Bank Project for Korean:
Present and Future Development. In In Proceedings
of the International Workshop on Sharable Natural
Language Resources, pages 7?14, Nara, Japan.
Jinho D. Choi. 2013. Preparing Korean data for the
shared task on parsing morphologically rich languages.
arXiv:1309.1649.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-
abith. 2008. Learning morphology with Morfette. In
Proceedings of LREC, Marrakech, Morocco.
Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of Korean parsing. In
Proceedings of the NAACL/HLT Workshop on Sta-
tistical Parsing of Morphologically Rich Languages
(SPMRL 2010), Los Angeles, CA.
Volkan Cirik and H?sn? S?ensoy. 2013. The AI-KU
system at the SPMRL 2013 shared task: Unsuper-
vised features for dependency parsing. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 68?75, Seat-
tle, WA.
Michael Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguistics,
29(4):589?637.
Matthieu Constant, Marie Candito, and Djam? Seddah.
2013. The LIGM-Alpage architecture for the SPMRL
2013 shared task: Multiword expression analysis and
dependency parsing. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 46?52, Seattle, WA.
Anna Corazza, Alberto Lavelli, Giogio Satta, and Roberto
Zanoli. 2004. Analyzing an Italian treebank with
state-of-the-art statistical parsers. In Proceedings of
the Third Workshop on Treebanks and Linguistic Theo-
ries (TLT), T?bingen, Germany.
Benoit Crabb? and Marie Candito. 2008. Exp?riences
d?analyse syntaxique statistique du fran?ais. In Actes
de la 15?me Conf?rence sur le Traitement Automatique
des Langues Naturelles (TALN?08), pages 45?54, Avi-
gnon, France.
D?ra Csendes, J?nos Csirik, Tibor Gyim?thy, and Andr?s
Kocsor. 2005. The Szeged treebank. In Proceedings of
the 8th International Conference on Text, Speech and
Dialogue (TSD), Lecture Notes in Computer Science,
pages 123?132, Berlin / Heidelberg. Springer.
Eric De La Clergerie. 2013. Exploring beam-based
shift-reduce dependency parsing with DyALog: Re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
178
Morphologically-Rich Languages, pages 81?89, Seat-
tle, WA.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Proceedings of the workshop on Cross-
Framework and Cross-Domain Parser Evaluation.
Mona Diab, Nizar Habash, Owen Rambow, and Ryan
Roth. 2013. LDC Arabic treebanks and associated cor-
pora: Data divisions manual. Technical Report CCLS-
13-02, Center for Computational Learning Systems,
Columbia University.
Eva Ejerhed and Gunnel K?llgren. 1997. Stockholm
Ume? Corpus. Version 1.0. Department of Linguis-
tics, Ume? University and Department of Linguistics,
Stockholm University.
Eva Ejerhed, Gunnel K?llgren, Ola Wennstedt, and Mag-
nus ?str?m. 1992. The linguistic annotation system
of the Stockholm?Ume? Corpus project. Technical
Report 33, University of Ume?: Department of Linguis-
tics.
Nerea Ezeiza, I?aki Alegria, Jos? Mar?a Arriola, Rub?n
Urizar, and Itziar Aduriz. 1998. Combining stochastic
and rule-based methods for disambiguation in aggluti-
native languages. In Proceedings of COLING, pages
380?384, Montr?al, Canada.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL, pages
959?967, Columbus, OH.
Alexander Fraser, Helmut Schmid, Rich?rd Farkas, Ren-
jing Wang, and Hinrich Sch?tze. 2013. Knowledge
sources for constituent parsing of German, a morpho-
logically rich and less-configurational language. Com-
putational Linguistics, 39(1):57?85.
Iakes Goenaga, Koldo Gojenola, and Nerea Ezeiza. 2013.
Exploiting the contribution of morphological informa-
tion to parsing: the BASQUE TEAM system in the
SPRML?2013 shared task. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 61?67, Seattle, WA.
Yoav Goldberg and Michael Elhadad. 2010a. Easy-first
dependency parsing of Modern Hebrew. In Proceed-
ings of the NAACL/HLT Workshop on Statistical Pars-
ing of Morphologically Rich Languages (SPMRL 2010),
Los Angeles, CA.
Yoav Goldberg and Michael Elhadad. 2010b. An ef-
ficient algorithm for easy-first non-directional depen-
dency parsing. In Proceedings of HLT: NAACL, pages
742?750, Los Angeles, CA.
Yoav Goldberg and Reut Tsarfaty. 2008. A single frame-
work for joint morphological segmentation and syntac-
tic parsing. In Proceedings of ACL, Columbus, OH.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proc. of ACL, Columbus, OH.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and EM-HMM-based lexical probabilities. In
Proceedings of EALC, pages 327?335, Athens, Greece.
Yoav Goldberg. 2011. Automatic syntactic processing of
Modern Hebrew. Ph.D. thesis, Ben Gurion University
of the Negev.
David Graff, Mohamed Maamouri, Basma Bouziri, Son-
dos Krouna, Seth Kulick, and Tim Buckwalter. 2009.
Standard Arabic Morphological Analyzer (SAMA) ver-
sion 3.1. Linguistic Data Consortium LDC2009E73.
Spence Green and Christopher D. Manning. 2010. Better
Arabic parsing: Baselines, evaluations, and analysis.
In Proceedings of COLING, pages 394?402, Beijing,
China.
Nathan Green, Loganathan Ramasamy, and Zden?k
?abokrtsk?. 2012. Using an SVM ensemble system for
improved Tamil dependency parsing. In Proceedings
of the ACL 2012 Joint Workshop on Statistical Pars-
ing and Semantic Processing of Morphologically Rich
Languages, pages 72?77, Jeju, Korea.
Spence Green, Marie-Catherine de Marneffe, and Christo-
pher D. Manning. 2013. Parsing models for identify-
ing multiword expressions. Computational Linguistics,
39(1):195?227.
Noemie Guthmann, Yuval Krymolowski, Adi Milea, and
Yoad Winter. 2009. Automatic annotation of morpho-
syntactic dependencies in a Modern Hebrew Treebank.
In Proceedings of the Eighth International Workshop on
Treebanks and Linguistic Theories (TLT), Groningen,
The Netherlands.
Nizar Habash and Ryan Roth. 2009. CATiB: The
Columbia Arabic Treebank. In Proceedings of ACL-
IJCNLP, pages 221?224, Suntec, Singapore.
Nizar Habash, Ryan Gabbard, Owen Rambow, Seth
Kulick, and Mitch Marcus. 2007. Determining case in
Arabic: Learning complex linguistic behavior requires
complex linguistic features. In Proceedings of EMNLP-
CoNLL, pages 1084?1092, Prague, Czech Republic.
Nizar Habash, Reem Faraj, and Ryan Roth. 2009a. Syn-
tactic Annotation in the Columbia Arabic Treebank. In
Proceedings of MEDAR International Conference on
Arabic Language Resources and Tools, Cairo, Egypt.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009b.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS tag-
ging, stemming and lemmatization. In Proceedings of
the Second International Conference on Arabic Lan-
guage Resources and Tools. Cairo, Egypt.
179
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publishers.
Jan Hajic?, Alena B?hmov?, Eva Hajic?ov?, and Barbora
Vidov?-Hladk?. 2000. The Prague Dependency Tree-
bank: A three-level annotation scenario. In Anne
Abeill?, editor, Treebanks: Building and Using Parsed
Corpora. Kluwer Academic Publishers.
P?ter Hal?csy, Andr?s Kornai, and Csaba Oravecz. 2007.
HunPos ? an open source trigram tagger. In Proceed-
ings of ACL, pages 209?212, Prague, Czech Republic.
Johan Hall, Jens Nilsson, Joakim Nivre, G?ls?en Eryig?it,
Be?ta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? A study in multilingual
parser optimization. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
933?939, Prague, Czech Republic.
Chung-hye Han, Na-Rae Han, Eon-Suk Ko, Martha
Palmer, and Heejong Yi. 2002. Penn Korean Treebank:
Development and evaluation. In Proceedings of the
16th Pacific Asia Conference on Language, Information
and Computation, Jeju, Korea.
Tilman H?hle. 1986. Der Begriff "Mittelfeld", Anmerkun-
gen ?ber die Theorie der topologischen Felder. In Ak-
ten des Siebten Internationalen Germanistenkongresses
1985, pages 329?340, G?ttingen, Germany.
Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010.
Self-training with products of latent variable grammars.
In Proceedings of EMNLP, pages 12?22, Cambridge,
MA.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of ACL,
pages 586?594, Columbus, OH.
Alon Itai and Shuly Wintner. 2008. Language resources
for Hebrew. Language Resources and Evaluation,
42(1):75?98, March.
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24(4):613?
632.
Laura Kallmeyer and Wolfgang Maier. 2013. Data-driven
parsing using probabilistic linear context-free rewriting
systems. Computational Linguistics, 39(1).
Fred Karlsson, Atro Voutilainen, Juha Heikkilae, and Arto
Anttila. 1995. Constraint Grammar: a language-
independent system for parsing unrestricted text. Wal-
ter de Gruyter.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430, Sapporo, Japan.
Sandra K?bler, Erhard W. Hinrichs, and Wolfgang Maier.
2006. Is it really that difficult to parse German? In Pro-
ceedings of EMNLP, pages 111?119, Sydney, Australia,
July.
Sandra K?bler, Wolfgang Maier, Ines Rehbein, and Yan-
nick Versley. 2008. How to compare treebanks. In
Proceedings of LREC, pages 2322?2329, Marrakech,
Morocco.
Sandra K?bler. 2008. The PaGe 2008 shared task on
parsing German. In Proceedings of the Workshop on
Parsing German, pages 55?63, Columbus, OH.
Seth Kulick, Ryan Gabbard, and Mitch Marcus. 2006.
Parsing the Arabic Treebank: Analysis and Improve-
ments. In Proceedings of the Treebanks and Linguistic
Theories Conference, pages 31?42, Prague, Czech Re-
public.
Joseph Le Roux, Benoit Sagot, and Djam? Seddah. 2012.
Statistical parsing of Spanish and data driven lemmati-
zation. In Proceedings of the Joint Workshop on Statis-
tical Parsing and Semantic Processing of Morphologi-
cally Rich Languages, pages 55?61, Jeju, Korea.
Kong Joo Lee, Byung-Gyu Chang, and Gil Chang Kim.
1997. Bracketing Guidelines for Korean Syntactic Tree
Tagged Corpus. Technical Report CS/TR-97-112, De-
partment of Computer Science, KAIST.
Roger Levy and Christopher D. Manning. 2003. Is it
harder to parse Chinese, or the Chinese treebank? In
Proceedings of ACL, Sapporo, Japan.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2004a. Arabic Treebank: Part 2 v 2.0.
LDC catalog number LDC2004T02.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004b. The Penn Arabic Treebank:
Building a large-scale annotated Arabic corpus. In
NEMLAR Conference on Arabic Language Resources
and Tools, pages 102?109, Cairo, Egypt.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2005. Arabic Treebank: Part 1 v 3.0. LDC
catalog number LDC2005T02.
Mohamed Maamouri, Ann Bies, Seth Kulick, Fatma Gad-
deche, Wigdan Mekki, Sondos Krouna, and Basma
Bouziri. 2009. The Penn Arabic Treebank part 3 ver-
sion 3.1. Linguistic Data Consortium LDC2008E22.
Wolfgang Maier, Miriam Kaeshammer, and Laura
Kallmeyer. 2012. Data-driven PLCFRS parsing re-
visited: Restricting the fan-out to two. In Proceedings
of the Eleventh International Conference on Tree Ad-
joining Grammars and Related Formalisms (TAG+11),
Paris, France.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn TreeBank. Computational
Linguistics, 19(2):313?330.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference. In
Proceedings of EMNLP, pages 34?44, Cambridge, MA.
180
Yuval Marton, Nizar Habash, and Owen Rambow. 2013a.
Dependency parsing of Modern Standard Arabic with
lexical and inflectional features. Computational Lin-
guistics, 39(1):161?194.
Yuval Marton, Nizar Habash, Owen Rambow, and Sarah
Alkhulani. 2013b. SPMRL?13 shared task system:
The CADIM Arabic dependency parser. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 76?80, Seat-
tle, WA.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of HLT:NAACL, pages 152?159, New York, NY.
Ryan T. McDonald, Koby Crammer, and Fernando C. N.
Pereira. 2005. Online large-margin training of depen-
dency parsers. In Proceedings of ACL, pages 91?98,
Ann Arbor, MI.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar
Tackstrom, Claudia Bedini, Nuria Bertomeu Castello,
and Jungmee Lee. 2013. Universal dependency anno-
tation for multilingual parsing. In Proceedings of ACL,
Sofia, Bulgaria.
Igor Mel?c?uk. 2001. Communicative Organization in Nat-
ural Language: The Semantic-Communicative Struc-
ture of Sentences. J. Benjamins.
Knowledge Center for Processing Hebrew
MILA. 2008. Hebrew morphological analyzer.
http://mila.cs.technion.ac.il.
Antonio Moreno, Ralph Grishman, Susana Lopez, Fer-
nando Sanchez, and Satoshi Sekine. 2000. A treebank
of Spanish and its application to parsing. In Proceed-
ings of LREC, Athens, Greece.
Joakim Nivre and Be?ta Megyesi. 2007. Bootstrapping a
Swedish treeebank using cross-corpus harmonization
and annotation projection. In Proceedings of the 6th
International Workshop on Treebanks and Linguistic
Theories, pages 97?102, Bergen, Norway.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Tal-
banken05: A Swedish treebank with phrase structure
and dependency annotation. In Proceedings of LREC,
pages 1392?1395, Genoa, Italy.
Joakim Nivre, Johan Hall, Sandra K?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007a. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task Ses-
sion of EMNLP-CoNLL 2007, pages 915?932, Prague,
Czech Republic.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
G?ls?en Eryig?it, Sandra K?bler, Svetoslav Marinov,
and Erwin Marsi. 2007b. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Slav Petrov and Ryan McDonald. 2012. Overview of the
2012 Shared Task on Parsing the Web. In Proceedings
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL), a NAACL-HLT 2012
workshop, Montreal, Canada.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of COLING-
ACL, Sydney, Australia.
Slav Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Bekeley, Berkeley, CA.
Slav Petrov. 2010. Products of random latent variable
grammars. In Proceedings of HLT: NAACL, pages 19?
27, Los Angeles, CA.
Adam Przepi?rkowski, Miros?aw Ban?ko, Rafa? L. G?rski,
and Barbara Lewandowska-Tomaszczyk, editors. 2012.
Narodowy Korpus Jkezyka Polskiego. Wydawnictwo
Naukowe PWN, Warsaw.
Ines Rehbein and Josef van Genabith. 2007a. Eval-
uating Evaluation Measures. In Proceedings of the
16th Nordic Conference of Computational Linguistics
NODALIDA-2007, Tartu, Estonia.
Ines Rehbein and Josef van Genabith. 2007b. Treebank
annotation schemes and parser evaluation for German.
In Proceedings of EMNLP-CoNLL, Prague, Czech Re-
public.
Ines Rehbein. 2011. Data point selection for self-training.
In Proceedings of the Second Workshop on Statistical
Parsing of Morphologically Rich Languages, pages 62?
67, Dublin, Ireland.
Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In Proceedings of HLT-NAACL, pages
129?132, New York, NY.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(04):365?380.
Natalie Schluter and Josef van Genabith. 2007. Prepar-
ing, restructuring, and augmenting a French Treebank:
Lexicalised parsers or coherent treebanks? In Proc. of
PACLING 07, Melbourne, Australia.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004.
SMOR: A German computational morphology covering
derivation, composition and inflection. In Proceedings
of LREC, Lisbon, Portugal.
Djam? Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
Lemmatization and statistical lexicalized parsing of
morphologically-rich languages. In Proceedings of the
First Workshop on Statistical Parsing of Morphologi-
cally Rich Languages (SPMRL), Los Angeles, CA.
Wolfgang Seeker and Jonas Kuhn. 2012. Making el-
lipses explicit in dependency conversion for a German
181
treebank. In Proceedings of LREC, pages 3132?3139,
Istanbul, Turkey.
Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined tree
substitution grammars for syntactic parsing. In Pro-
ceedings of ACL, pages 440?448, Jeju, Korea.
Anthony Sigogne, Matthieu Constant, and Eric Laporte.
2011. French parsing enhanced with a word clustering
method based on a syntactic lexicon. In Proceedings
of the Second Workshop on Statistical Parsing of Mor-
phologically Rich Languages, pages 22?27, Dublin,
Ireland.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altmann,
and Noa Nativ. 2001. Building a tree-bank of Modern
Hebrew text. Traitement Automatique des Langues,
42:347?380.
Marek S?widzin?ski and Marcin Wolin?ski. 2010. Towards
a bank of constituent parse trees for Polish. In Pro-
ceedings of Text, Speech and Dialogue, pages 197?204,
Brno, Czech Republic.
Ulf Teleman. 1974. Manual f?r grammatisk beskrivning
av talad och skriven svenska. Studentlitteratur.
Lucien Tesni?re. 1959. ?l?ments De Syntaxe Structurale.
Klincksieck, Paris.
Reut Tsarfaty and Khalil Sima?an. 2010. Modeling mor-
phosyntactic agreement in constituency-based parsing
of Modern Hebrew. In Proceedings of the First Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Djame Seddah, Yoav Goldberg, Sandra
K?bler, Marie Candito, Jennifer Foster, Yannick Vers-
ley, Ines Rehbein, and Lamia Tounsi. 2010. Statistical
parsing for morphologically rich language (SPMRL):
What, how and whither. In Proceedings of the First
workshop on Statistical Parsing of Morphologically
Rich Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2011. Evaluating dependency parsing: Robust and
heuristics-free cross-framework evaluation. In Pro-
ceedings of EMNLP, Edinburgh, UK.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012a. Cross-framework evaluation for statistical pars-
ing. In Proceeding of EACL, Avignon, France.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012b. Joint evaluation for segmentation and parsing.
In Proceedings of ACL, Jeju, Korea.
Reut Tsarfaty, Djam? Seddah, Sandra K?bler, and Joakim
Nivre. 2012c. Parsing morphologically rich languages:
Introduction to the special issue. Computational Lin-
guistics, 39(1):15?22.
Reut Tsarfaty. 2010. Relational-Realizational Parsing.
Ph.D. thesis, University of Amsterdam.
Reut Tsarfaty. 2013. A unified morpho-syntactic scheme
of Stanford dependencies. In Proceedings of ACL,
Sofia, Bulgaria.
Veronika Vincze, D?ra Szauter, Attila Alm?si, Gy?rgy
M?ra, Zolt?n Alexin, and J?nos Csirik. 2010. Hungar-
ian Dependency Treebank. In Proceedings of LREC,
Valletta, Malta.
Joachim Wagner. 2012. Detecting Grammatical Errors
with Treebank-Induced Probabilistic Parsers. Ph.D.
thesis, Dublin City University.
Marcin Wolin?ski, Katarzyna G?owin?ska, and Marek
S?widzin?ski. 2011. A preliminary version of
Sk?adnica?a treebank of Polish. In Proceedings of
the 5th Language & Technology Conference, pages
299?303, Poznan?, Poland.
Alina Wr?blewska. 2012. Polish Dependency Bank. Lin-
guistic Issues in Language Technology, 7(1):1?15.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL:HLT, pages 188?193, Portland,
OR.
J?nos Zsibrita, Veronika Vincze, and Rich?rd Farkas.
2013. magyarlanc: A toolkit for morphological and
dependency parsing of Hungarian. In Proceedings of
RANLP, pages 763?771, Hissar, Bulgaria.
182
