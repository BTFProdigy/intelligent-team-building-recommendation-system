Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 189?192, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Semantic Role Lableing System using Maximum Entropy Classier ?
Ting Liu, Wanxiang Che, Sheng Li, Yuxuan Hu and Huaijun Liu
Information Retrieval Lab
School of Computer Science and Technology
Harbin Institute of Technology
China, 150001
{tliu, car, ls, yxhu, hjliu}@ir.hit.edu.cn
Abstract
A maximum entropy classifier is used in
our semantic role labeling system, which
takes syntactic constituents as the labeling
units. The maximum entropy classifier is
trained to identify and classify the predi-
cates? semantic arguments together. Only
the constituents with the largest probabil-
ity among embedding ones are kept. Af-
ter predicting all arguments which have
matching constituents in full parsing trees,
a simple rule-based post-processing is ap-
plied to correct the arguments which have
no matching constituents in these trees.
Some useful features and their combina-
tions are evaluated.
1 Introduction
The semantic role labeling (SRL) is to assign syn-
tactic constituents with semantic roles (arguments)
of predicates (most frequently verbs) in sentences.
A semantic role is the relationship that a syntactic
constituent has with a predicate. Typical semantic
arguments include Agent, Patient, Instrument, etc.
and also adjunctive arguments indicating Locative,
Temporal, Manner, Cause, etc. It can be used in
lots of natural language processing application sys-
tems in which some kind of semantic interpretation
is needed, such as question and answering, informa-
tion extraction, machine translation, paraphrasing,
and so on.
?This research was supported by National Natural Science
Foundation of China via grant 60435020
Last year, CoNLL-2004 hold a semantic role la-
beling shared task (Carreras and Ma`rquez, 2004)
to test the participant systems? performance based
on shallow syntactic parser results. In 2005, SRL
shared task is continued (Carreras and Ma`rquez,
2005), because it is a complex task and now it is
far from desired performance.
In our SRL system, we select maximum en-
tropy (Berger et al, 1996) as a classifier to im-
plement the semantic role labeling system. Dif-
ferent from the best classifier reported in litera-
tures (Pradhan et al, 2005) ? support vector ma-
chines (SVMs) (Vapnik, 1995), it is much eas-
ier for maximum entropy classifier to handle the
multi-class classification problem without additional
post-processing steps. The classifier is much faster
than training SVMs classifiers. In addition, max-
imum entropy classifier can be tuned to minimize
over-fitting by adjusting gaussian prior. Xue and
Palmer (2004; 2005) and Kwon et al (2004) have
applied the maximum entropy classifier to semantic
role labeling task successfully.
In the following sections, we will describe our
system and report our results on development and
test sets.
2 System Description
2.1 Constituent-by-Constituent
We use syntactic constituent as the unit of labeling.
However, it is impossible for each argument to find
its matching constituent in all auto parsing trees. Ac-
cording to statistics, about 10% arguments have no
matching constituents in the training set of 245,353
189
constituents. The top five arguments with no match-
ing constituents are shown in Table 1. Here, Char-
niak parser got 10.08% no matching arguments and
Collins parser got 11.89%.
Table 1: The top five arguments with no matching
constituents.
Args Cha parser Col parser Both
AM-MOD 9179 9205 9153
A1 5496 7273 3822
AM-NEG 3200 3217 3185
AM-DIS 1451 1482 1404
A0 1416 2811 925
Therefore, we can see that Charniak parser got a
better result than Collins parser in the task of SRL.
So we use the full analysis results created by Char-
niak parser as our classifier?s inputs. Assume that
we could label all AM-MOD and AM-NEG arguments
correctly with simple post processing rules, the up-
per bound of performance could achieve about 95%
recall.
At the same time, we can see that for some ar-
guments, both parsers got lots of no matchings such
as AM-MOD, AM-NEG, and so on. After analyzing
the training data, we can recognize that the perfor-
mance of these arguments can improve a lot after
using some simple post processing rules only, how-
ever other arguments? no matching are caused pri-
marily by parsing errors. The comparison between
using and not using post processing rules is shown
in Section 3.2.
Because of the high speed and no affection in the
number of classes with efficiency of maximum en-
tropy classifier, we just use one stage to label all ar-
guments of predicates. It means that the ?NULL?
tag of constituents is regarded as a class like ?ArgN?
and ?ArgM?.
2.2 Features
The following features, which we refer to as the
basic features modified lightly from Pradhan et
al. (2005), are provided in the shared task data for
each constituent.
? Predicate lemma
? Path: The syntactic path through the parse tree from the
parse constituent to the predicate.
? Phrase type
? Position: The position of the constituent with respect to
its predicate. It has two values, ?before? and ?after?,
for the predicate. For the situation of ?cover?, we use
a heuristic rule to ignore all of them because there is no
chance for them to become an argument of the predicate.
? Voice: Whether the predicate is realized as an active or
passive construction. We use a simple rule to recognize
passive voiced predicates which are labeled with part of
speech ? VBN and sequences with AUX.
? Head word stem: The stemming result of the con-
stituent?s syntactic head. A rule based stemming algo-
rithm (Porter, 1980) is used. Collins Ph.D thesis (Collins,
1999)[Appendix. A] describs some rules to identify the
head word of a constituent. Especially for prepositional
phrase (PP) constituent, the normal head words are not
very discriminative. So we use the last noun in the PP
replacing the traditional head word.
? Sub-categorization
We also use the following additional features.
? Predicate POS
? Predicate suffix: The suffix of the predicate. Here, we
use the last 3 characters as the feature.
? Named entity: The named entity?s type in the constituent
if it ends with a named entity. There are four types: LOC,
ORG, PER and MISC.
? Path length: The length of the path between a constituent
and its predicate.
? Partial path: The part of the path from the constituent
to the lowest common ancestor of the predicate and the
constituent.
? Clause layer: The number of clauses on the path between
a constituent and its predicate.
? Head word POS
? Last word stem: The stemming result of the last word of
the constituent.
? Last word POS
We also use some combinations of the above fea-
tures to build some combinational features. Lots of
combinational features which were supposed to con-
tribute the SRL task of added one by one. At the
same time, we removed ones which made the per-
formance decrease in practical experiments. At last,
we keep the following combinations:
? Position + Voice
? Path length + Clause layer
? Predicate + Path
? Path + Position + Voice
? Path + Position + Voice + Predicate
? Head word stem + Predicate
? Head word stem + Predicate + Path
? Head word stem + Phrase
? Clause layer + Position + Predicate
All of the features and their combinations are used
without feature filtering strategy.
190
2.3 Classifier
Le Zhang?s Maximum Entropy Modeling Toolkit 1,
and the L-BFGS parameter estimation algorithm
with gaussian prior smoothing (Chen and Rosenfeld,
1999) are used as the maximum entropy classifier.
We set gaussian prior to be 2 and use 1,000 itera-
tions in the toolkit to get an optimal result through
some comparative experiments.
2.4 No Embedding
The system described above might label two con-
stituents even if one embeds in another, which is not
allowed by the SRL rule. So we keep only one ar-
gument when more arguments embedding happens.
Because it is easy for maximum entropy classifier to
output each prediction?s probability, we can label the
constituent which has the largest probability among
the embedding ones.
2.5 Post Processing Stage
After labeling the arguments which are matched
with constituents exactly, we have to handle the ar-
guments, such as AM-MOD, AM-NEG and AM-DIS,
which have few matching with the constituents de-
scribed in Section 2.1. So a post processing is given
by using some simply rules:
? Tag target verb and successive particles as V.
? Tag ?not? and ?n?t? in target verb chunk as AM-NEG.
? Tag modal verbs in target verb chunk, such as words with
POS of ?MD?, ?going to?, and so on, as AM-MOD.
? Tag the words with POS of ?CC? and ?RB? at the start of
a clause which include the target verb as AM-DIS.
3 Experiments
3.1 Data and Evaluation Metrics
The data provided for the shared task is a part of
PropBank corpus. It consists of the sections from
the Wall Street Journal part of Penn Treebank. Sec-
tions 02-21 are training sets, and Section 24 is devel-
opment set. The results are evaluated for precision,
recall and F?=1 numbers using the srl-eval.pl script
provided by the shared task organizers.
3.2 Post Processing
After using post processing rules, the final F?=1 is
improved from 71.02% to 75.27%.
1http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html
3.3 Performance Curve
Because the training corpus is substantially en-
larged, this allows us to test the scalability of
learning-based SRL systems to large data set and
compute learning curves to see how many data are
necessary to train. We divide the training set, 20
sections Penn Treebank into 5 parts with 4 sections
in each part. There are about 8,000 sentences in each
part. Figure 1 shows the change of performance as
a function of training set size. When all of training
data are used, we get the best system performance as
described in Section 3.4.
Figure 1: Our SRL system performance curve (of
F?=1) effecting of the training set size.
We can see that as the training set becomes larger
and larger, so does the performance of SRL system.
However, the rate of increase slackens. So we can
say that at present state, the larger training data has
favorable effect on the improvement of SRL system
performance.
3.4 Best System Results
In all the experiments, all of the features and their
combinations described above are used in our sys-
tem. Table 2 presents our best system performance
on the development and test sets.
From the test results, we can see that our system
gets much worse performance on Brown corpus than
WSJ corpus. The reason is easy to be understood
for the dropping of automatic syntactic parser per-
formance on new corpus but WSJ corpus.
The training time on PIV 2.4G CPU and 1G Mem
machine is about 20 hours on all 20 sections, 39,832-
191
Precision Recall F?=1
Development 79.65% 71.34% 75.27
Test WSJ 80.48% 72.79% 76.44
Test Brown 71.13% 59.99% 65.09
Test WSJ+Brown 79.30% 71.08% 74.97
Test WSJ Precision Recall F?=1
Overall 80.48% 72.79% 76.44
A0 88.14% 83.61% 85.81
A1 79.62% 72.88% 76.10
A2 73.67% 65.05% 69.09
A3 76.03% 53.18% 62.59
A4 78.02% 69.61% 73.58
A5 100.00% 40.00% 57.14
AM-ADV 59.85% 48.02% 53.29
AM-CAU 68.18% 41.10% 51.28
AM-DIR 56.60% 35.29% 43.48
AM-DIS 76.32% 72.50% 74.36
AM-EXT 83.33% 46.88% 60.00
AM-LOC 65.31% 52.89% 58.45
AM-MNR 58.28% 51.16% 54.49
AM-MOD 98.52% 96.37% 97.43
AM-NEG 97.79% 96.09% 96.93
AM-PNC 43.68% 33.04% 37.62
AM-PRD 50.00% 20.00% 28.57
AM-REC 0.00% 0.00% 0.00
AM-TMP 78.38% 66.70% 72.07
R-A0 81.70% 85.71% 83.66
R-A1 77.62% 71.15% 74.25
R-A2 60.00% 37.50% 46.15
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 100.00% 25.00% 40.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 83.33% 47.62% 60.61
R-AM-MNR 66.67% 33.33% 44.44
R-AM-TMP 77.27% 65.38% 70.83
V 98.71% 98.71% 98.71
Table 2: Overall results (top) and detailed results on
the WSJ test (bottom).
sentences training set with 1,000 iterations and more
than 1.5 million samples and 2 million features.
The predicting time is about 160 seconds on 1,346-
sentences development set.
4 Conclusions
We have described a maximum entropy classifier
is our semantic role labeling system, which takes
syntactic constituents as the labeling units. The
fast training speed of the maximum entropy clas-
sifier allows us just use one stage of arguments
identification and classification to build the system.
Some useful features and their combinations are
evaluated. Only the constituents with the largest
probability among embedding ones are kept. Af-
ter predicting all arguments which have matching
constituents in full parsing trees, a simple rule-
based post-processing is applied to correct the ar-
guments which have no matching constituents. The
constituent-based method depends much on the syn-
tactic parsing performance. The comparison be-
tween WSJ and Brown test sets results fully demon-
strates the point of view.
References
Adam L. Berger, Stephen A. Della Pietra, and Vincent J.
Della Pietra. 1996. A maximum entropy approach to
natural language processing. Computational Linguis-
tics, 22(1):39?71.
Xavier Carreras and Llu??s Ma`rquez. 2004. Introduction
to the conll-2004 shared task: Semantic role labeling.
In Proceedings of CoNLL-2004, pages 89?97, Boston,
MA, USA.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction
to the CoNLL-2005 Shared Task: Semantic Role La-
beling. In Proceedings of CoNLL-2005.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian
prior for smoothing maximum entropy models. Tech-
nical Report CMU-CS-99-108.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Pennsyl-
vania University.
Namhee Kwon, Michael Fleischman, and Eduard Hovy.
2004. Framenet-based semantic parsing using maxi-
mum entropy models. In Proc. Coling 2004.
Martin Porter. 1980. An algorithm for suffix stripping.
Program, 14(3).
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne
Ward, James H. Martin, and Daniel Jurafsky. 2005.
Support vector learning for semantic argument classi-
fication. Machine Learning Journal.
Vladamir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag, Berlin.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proc. EMNLP
2004.
Nianwen Xue and Martha Palmer. 2005. Automatic se-
mantic role labeling for chinese verbs. In Proc. IJCAI
2005.
192
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 165?168,
Prague, June 2007. c?2007 Association for Computational Linguistics
HIT-IR-WSD: A WSD System for English Lexical Sample Task 
Yuhang Guo, Wanxiang Che, Yuxuan Hu, Wei Zhang and Ting Liu 
Information Retrieval Lab 
Harbin Institute of technology 
Harbin, China, 150001 
{yhguo,wxche}@ir.hit.edu.cn 
 
 
Abstract 
HIT-IR-WSD is a word sense disambigua-
tion (WSD) system developed for English 
lexical sample task (Task 11) of Semeval 
2007 by Information Retrieval Lab, Harbin 
Institute of Technology. The system is 
based on a supervised method using an 
SVM classifier. Multi-resources including 
words in the surrounding context, the part-
of-speech of neighboring words, colloca-
tions and syntactic relations are used. The 
final micro-avg raw score achieves 81.9% 
on the test set, the best one among partici-
pating runs. 
1 Introduction 
Lexical sample task is a kind of WSD evaluation 
task providing training and test data in which a 
small pre-selected set of target words is chosen and 
the target words are marked up. In the training data 
the target words? senses are given, but in the test 
data are not and need to be predicted by task par-
ticipants. 
HIT-IR-WSD regards the lexical sample task 
as a classification problem, and devotes to extract 
effective features from the instances. We didn?t use 
any additional training data besides the official 
ones the task organizers provided. Section 2 gives 
the architecture of this system. As the task pro-
vides correct word sense for each instance, a su-
pervised learning approach is used. In this system, 
we choose Support Vector Machine (SVM) as 
classifier. SVM is introduced in section 3. Know-
ledge sources are presented in section 4. The last 
section discusses the experimental results and 
present the main conclusion of the work performed. 
2 The Architecture of the System 
HIT-IR-WSD system consists of 2 parts: feature 
extraction and classification. Figure 1 portrays the 
architecture of the system. 
 
Figure?1:?The?architecture?of?HIT?IR?WSD?
165
Features are extracted from original instances 
and are made into digitized features to feed the 
SVM classifier. The classifier gets the features of 
training data to make a model of the target word. 
Then it uses the model to predict the sense of target 
word in the test data. 
3 Learning Algorithm 
SVM is an effective learning algorithm to WSD 
(Lee and Ng, 2002). The SVM tries to find a 
hyperplane with the largest margin separating the 
training samples into two classes. The instances in 
the same side of the hyperplane have the same 
class label. A test instance?s feature decides the 
position where the sample is in the feature space 
and which side of the hyperplane it is. In this way, 
it leads to get a prediction. SVM could be extended 
to tackle multi-classes problems by using one-
against-one or one-against-rest strategy. 
In the WSD problem, input of SVM is the fea-
ture vector of the instance. Features that appear in 
all the training samples are arranged as a vector 
space. Every instance is mapped to a feature vector. 
If the feature of a certain dimension exists in a 
sample, assign this dimension 1 to this sample, else 
assign it 0. For example, assume the feature vector 
space is <x1, x2, x3, x4, x5, x6, x7>; the instance is 
?x2 x6 x5 x7?. The feature vector of this sample 
should be <0, 1, 0, 0, 1, 1, 1>.  
The implementation of SVM here is libsvm 1 
(Chang and Lin, 2001) for multi-classes. 
4 Knowledge Sources 
We used 4 kinds of features of the target word and 
its context as shown in Table 1. 
Part of the original text of an example is ?? 
This is the <head>age</head> of new media , the 
era of ??. 
Name Extraction Tools Example 
Surrounding 
words 
WordNet 
(morph)2 
?, this, be, age, new, 
medium, ,, era, ? 
Part-of-
speech SVMTool
3 
DT_0, VBZ_0, DT_0, 
NN_t, IN_1, JJ_1, 
NNS_1 
                                                 
1?http://www.csie.ntu.edu.tw/~cjlin/libsvm/?
2?http://wordnet.princeton.edu/man/morph.3WN.html?
3?http://www.lsi.upc.es/~nlp/SVMTool/?
Collocation  
this_0, be_0, the_0, 
age_t, of_1, new_1, 
medium_1, ,_1, the_1 
Syntactic 
relation MaltParser
4 
SYN_HEAD_is 
SYN_HEADPOS_VBZ 
SYN_RELATION_PRD 
SYN_HEADRIGHT 
Table?1:?Features?the?system?extracted?
The next 4 subsections elaborate these features. 
4.1 Words in the Surrounding Context 
We take the neighboring words in the context of 
the target word as a kind of features ignoring their 
exact position information, which is called bag-of-
words approach. 
Mostly, a certain sense of a word is tend to ap-
pear in a certain kind of context, so the context 
words could contain some helpful information to 
disambiguate the sense of the target word. 
Because there would be too many context words 
to be added into the feature vector space, data 
sparseness problem is inevitable. We need to re-
duce the sparseness as possible as we can. A sim-
ple way is to use the words? morphological root 
forms. In addition, we filter the tokens which con-
tain no alphabet character (including punctuation 
symbols) and stop words. The stop words are 
tested separately, and only the effective ones 
would be added into the stop words list. All re-
maining words in the instance are gathered, con-
verted to lower case and replaced by their morpho-
logical root forms. The implementation for getting 
the morphological root forms is WordNet (morph). 
4.2 Part-of-Speechs of Neighboring Words 
As mentioned above, the data sparseness is a se-
rious problem in WSD. Besides changing tokens to 
their morphological root forms, part-of-speech is a 
good choice too. The size of POS tag set is much 
smaller than the size of surrounding words set. 
And the neighboring words? part-of-speeches also 
contain useful information for WSD. In this part, 
we use a POS tagger (Gim?nez and M?rquez, 2004) 
to assign POS tags to those tokens.  
We get the left and right 3 words? POS tags to-
gether with their position information in the target 
words? sentence.  
For example, the word age is to be disambi-
guated in the sentence of ?? This is the 
                                                 
4?http://w3.msi.vxu.se/~nivre/research/MaltParser.html?
166
<head>age</head> of new media , the era of ??. 
The features then will be added to the feature vec-
tor are ?DT_0, VBZ_0, DT_0, NN_t, IN_1, JJ_1, 
NNS_1?, in which _0/_1 stands for the word with 
current POS tag is in the left/right side of the target 
word. The POS tag set in use here is Penn Tree-
bank Tagset5. 
4.3 Collocations 
Different from bag-of-words, collocation feature 
contains the position information of the target 
words? neighboring words. To make this feature in 
the same form with the bag-of-words, we appended 
a symbol to each of the neighboring words? mor-
phological root forms to mark whether this word is 
in the left or in the right of the target word. Like 
POS feature, collocation was extracted in the sen-
tence where the target word belongs to. The win-
dow size of this feature is 5 to the left and 5 to the 
right of the target word, which is attained by em-
pirical value. In this part, punctuation symbol and 
stop words are not removed. 
Take the same instance last subsection has men-
tioned as example. The features we extracted are 
?this_0, be_0, the_0, age_t, of_1, new_1, me-
dium_1?. Like POS, _0/_1 stands for the word is 
in the left/right side of the target word. Then the 
features were added to the feature vector space. 
4.4 Syntactic Relations 
Many effective context words are not in a short 
distance to the target word, but we shouldn?t en-
large the window size too much in case of includ-
ing too many noises. A solution to this problem is 
to use the syntactic relations of the target word and 
its parent head word. 
We use Nivre et al, (2006)?s dependency parser. 
In this part, we get 4 features from every instance: 
head word of the target word, the head word?s POS, 
the head word?s dependency relation with the tar-
get word and the relative position of the head word 
to the target word. 
Still take the same instance which has been 
mentioned in the las subsection as example. The 
features we extracted are ?SYN_HEAD_is, 
SYN_HEADPOS_VBZ, SYN_RELATION_PRD, 
SYN_HEADRIGHT?, in which SYN_HEAD_is 
stands for is is the head word of age; 
SYN_HEADPOS_VBZ stands for the POS of the 
                                                 
5?http://www.lsi.upc.es/~nlp/SVMTool/PennTreebank.html?
head word is is VBZ; SYN_RELATION_PRD 
stands for the relationship between the head word 
is and target word age is PRD; and 
SYN_HEADRIGHT stands for the target word age 
is in the right side of the head word is. 
5 Data Set and Results 
This English lexical sample task: Semeval 2007 
task 116 provides two tracks of the data set for par-
ticipants. The first one is from LDC and the second 
from web. 
We took part in this evaluation in the second 
track. The corpus is from web. In this track the task 
organizers provide a training data and test data set 
for 20 nouns and 20 adjectives. 
In order to develop our system, we divided the 
training data into 2 parts: training and development 
sets. The size of the training set is about 2 times of 
the development set. The development set contains 
1,781 instances. 
4 kinds of features were merged into 15 combi-
nations. Here we use a vector (V) to express which 
features are used. The four dimensions stand for 
syntactic relations, POS, surrounding words and 
collocations, respectively. For example, 1010 
means that the syntactic relations feature and the 
surrounding words feature are used. 
V Precision V Precision
0001 78.6% 1001 78.2% 
0010 80.3% 1010 81.9% 
0011 82.0% 1011 82.8% 
0100 70.4% 1100 73.3% 
0101 79.0% 1101 79.1% 
0110 82.1% 1110 82.5% 
0111 82.9% 1111 82.9% 
1000 72.6%   
Table?2:?Results?of?Combinations?of?Features?
From Table 2, we can conclude that the sur-
rounding words feature is the most useful kind of 
features. It obtains much better performance than 
other kinds of features individually. In other words, 
without it, the performance drops a lot. Among 
these features, syntactic relations feature is the 
most unstable one (the improvement with it is un-
stable), partly because the performance of the de-
pendency parser is not good enough. As the ones 
with the vector 0111 and 1111 get the best perfor-
                                                 
6http://nlp.cs.swarthmore.edu/semeval/tasks/task11/descript
ion.shtml?
167
mance, we chose all of these kinds of features for 
our final system. 
A trade-off parameter C in SVM is tuned, and 
the result is shown in Figure 2. We have also tried 
4 types of kernels of the SVM classifier (parame-
ters are set by default). The experimental results 
show that the linear kernel is the most effective as 
Table 3 shows. 
 
Figure?2:?Accuracy?with?different?C?parameters?
Kernel 
Function 
Type 
Linear Poly-nomial RBF
Sig-
moid
Accuracy 82.9% 68.3% 68.3% 68.3%
Table?3:?Accuracy?with?different?kernel?function?
types?
Another experiment (as shown in Figure 3) also 
validate that the linear kernel is the most suitable 
one. We tried using polynomial function. Unlike 
the parameters set by default above (g=1/k, d=3), 
here we set its Gama parameter as 1 (g=1) but oth-
er parameters excepting degree parameter are still 
set by default. The performance gets better when 
the degree parameter is tuned towards 1. That 
means the closer the kernel function to linear func-
tion the better the system performs. 
 
Figure?3:?Accuracy?with?different?degree? in?po?
lynomial?function?
In order to get the relation between the system 
performance and the size of training data, we made 
several groups of training-test data set from the 
training data the organizers provided. Each of them 
has the same test data but different size of training 
data which are 2, 3, 4 and 5 times of the test data 
respectively. Figure 4 shows the performance 
curve with the training data size. Indicated in Fig-
ure 4, the accuracy increases as the size of training 
data enlarge, from which we can infer that we 
could raise the performance by using more training 
data potentially. 
 
Figure?4:?Accuracy?s?trend?with?the?training?da?
ta?size?
Feature extraction is the most time-consuming 
part of the system, especially POS tagging and 
parsing which take 2 hours approximately on the 
training and test data. The classification part (using 
libsvm) takes no more than 5 minutes on the train-
ing and test data. We did our experiment on a PC 
with 2.0GHz CPU and 960 MB system memory. 
Our official result of HIT-IR-WSD is: micro-
avg raw score 81.9% on the test set, the top one 
among the participating runs. 
Acknowledgement 
We gratefully acknowledge the support for this 
study provided by the National Natural Science 
Foundation of China (NSFC) via grant 60435020, 
60575042, 60575042 and 60675034. 
References 
Lee, Y. K., and Ng, H. T. 2002. An empirical evaluation 
of knowledge sources and learning algorithms for 
word sense disambiguation. In Proceedings of 
EMNLP02, 41?48. 
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a 
library for support vector machines. 
Jes?s Gim?nez and Llu?s M?rquez. 2004. SVMTool: A 
general POS tagger generator based on Support Vec-
tor Machines. Proceedings of the 4th International 
Conference on Language Resources and Evaluation 
(LREC'04). Lisbon, Portugal. 
Nivre, J., Hall, J., Nilsson, J., Eryigit, G. and Marinov, S. 
2006. Labeled Pseudo-Projective Dependency Pars-
ing with Support Vector Machines. In Proceedings of 
the Tenth Conference on Computational Natural 
Language Learning (CoNLL). 
168
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 238?242
Manchester, August 2008
A Cascaded Syntactic and Semantic Dependency Parsing System
Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang Li, Bing Qin, Ting Liu, Sheng Li
Information Retrieval Lab
School of Computer Science and Technology
Harbin Institute of Technology, China, 150001
{car, lzh, yxhu, yqli, qinb, tliu, ls}@ir.hit.edu.cn
Abstract
We describe our CoNLL 2008 Shared Task
system in this paper. The system includes
two cascaded components: a syntactic and
a semantic dependency parsers. A first-
order projective MSTParser is used as our
syntactic dependency parser. In order to
overcome the shortcoming of the MST-
Parser, that it cannot model more global in-
formation, we add a relabeling stage after
the parsing to distinguish some confusable
labels, such as ADV, TMP, and LOC. Be-
sides adding a predicate identification and
a classification stages, our semantic de-
pendency parsing simplifies the traditional
four stages semantic role labeling into two:
a maximum entropy based argument clas-
sification and an ILP-based post inference.
Finally, we gain the overall labeled macro
F1 = 82.66, which ranked the second posi-
tion in the closed challenge.
1 System Architecture
Our CoNLL 2008 Shared Task (Surdeanu et al,
2008) participating system includes two cascaded
components: a syntactic and a semantic depen-
dency parsers. They are described in Section 2
and 3 respectively. Their experimental results are
shown in Section 4. Section 5 gives our conclusion
and future work.
2 Syntactic Dependency Parsing
MSTParser (McDonald, 2006) is selected as our
basic syntactic dependency parser. It views the
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
syntactic dependency parsing as a problem of
finding maximum spanning trees (MST) in di-
rected graphs. MSTParser provides the state-of-
the-art performance for both projective and non-
projective tree banks.
2.1 Features
The score of each labeled arc is computed through
the Eq. (1) in MSTParser.
score(h, c, l) = w ? f(h, c, l) (1)
where node h represents the head node of the arc,
while node c is the dependent node (or child node).
l denotes the label of the arc.
There are three major differences between our
feature set and McDonald (2006)?s:
1) We use the lemma as a generalization feature
of a word, while McDonald (2006) use the word?s
prefix.
2) We add two new features: ?bet-pos-h-same-
num? and ?bet-pos-c-same-num?. They represent
the number of nodes which locate between node h
and node c and whose POS tags are the same with
h and c respectively.
3) We use more back-off features than McDon-
ald (2006) by completely enumerating all of the
possible combinatorial features.
2.2 Relabeling
By observing the current results of MSTParser on
the development data, we find that the performance
of some labels are far below average, such as ADV,
TMP, LOC. We think the main reason lies in that
MSTParser only uses local features restricted to a
single arc (as shown in Eq. (1)) and fails to use
more global information. Consider two sentences:
?I read books in the room.? and ?I read books in
the afternoon.?. It is hard to correctly label the arc
238
Deprel Total Mislabeled as
NMOD 8,922 NAME [0.4], DEP [0.4], LOC [0.1],
AMOD [0.1]
OBJ 1,728 TMP [0.5], ADV [0.4], OPRD[0.3]
ADV 1,256 TMP [2.9], LOC [2.3], MNR [1.8],
DIR [1.5]
NAME 1,138 NMOD [2.2]
VC 953 PRD [0.9]
DEP 772 NMOD [4.0]
TMP 755 ADV [9.9], LOC [6.5]
LOC 556 ADV [12.6], NMOD [7.9], TMP [5.9]
AMOD 536 ADV [2.2]
PRD 509 VC [4.7]
APPO 444 NMOD [2.5]
OPRD 373 OBJ [4.6]
DIR 119 ADV [18.5]
MNR 109 ADV [28.4]
Table 1: Error Analysis of Each Label
between ?read? and ?in? unless we know the object
of ?in?.
We count the errors of each label, and show the
top ones in Table 1. ?Total? refers to the total num-
ber of the corresponding label in the development
data. The column of ?Mislabeled as? lists the la-
bels that an arc may be mislabeled as. The number
in brackets shows the percentage of mislabeling.
As shown in the table, some labels are often con-
fusable with each other, such as ADV, LOC and
TMP.
2.3 Relabeling using Maximum Entropy
Classifier
We constructed two confusable label set which
have a higher mutual mislabeling proportion:
(NMOD, LOC, ADV, TMP, MNR, DIR) and (OBJ,
OPRD). A maximum entropy classifier is used to
relabel them.
Features are shown in Table 2. The first column
lists local features, which contains information of
the head node h and the dependent node c of an arc.
?+ dir dist? means that conjoining existing features
with arc direction and distance composes new fea-
tures. The second column lists features using the
information of node c?s children. ?word c c? rep-
resents form or lemma of one child of the node
c. ?dir c? and ?dist c? represents the direction and
distance of the arc which links node c to its child.
The back-off technique is also used on these fea-
tures.
Local features (+ dir dist) Global features (+ dir c dist c)
word h word c word h word c word c c
Table 2: Relabeling Feature Set (+ dir dist)
3 Semantic Dependency Parsing
3.1 Architecture
The whole procedure is divided into four separate
stages: Predicate Identification, Predicate Classifi-
cation, Semantic Role Classification, and Post In-
ference.
During the Predicate Identification stage we ex-
amine each word in a sentence to discover target
predicates, including both noun predicates (from
NomBank) and verb predicates (from PropBank).
In the Predicate Classification stage, each predi-
cate is assigned a certain sense number. For each
predicate, the probabilities of a word in the sen-
tence to be each semantic role are predicted in the
Semantic Role Classification stage. Maximum en-
tropy model is selected as our classifiers in these
stages. Finally an ILP (Integer Linear Program-
ming) based method is adopted for post infer-
ence (Punyakanok et al, 2004).
3.2 Predicate Identification
The predicate identification is treated as a binary
classification problem. Each word in a sentence is
predicted to be a predicate or not to be. A set of
features are extracted for each word, and an opti-
mized subset of them are adopted in our final sys-
tem. The following is a full list of the features:
DEPREL (a1): Type of relation to the parent.
WORD (a21), POS (a22), LEMMA (a23),
HEAD (a31), HEAD POS (a32), HEAD LEMMA
(a33): The forms, POS tags and lemmas of a word
and it?s headword (parent) .
FIRST WORD (a41), FIRST POS (a42),
FIRST LEMMA (a43), LAST WORD (a51),
LAST POS (a52), LAST LEMMA (a53): A
corresponding ?constituent? for a word consists
of all descendants of it. The forms, POS tags and
lemmas of both the first and the last words in the
constituent are extracted.
POS PAT (a6): A ?POS pattern? is produced for
the corresponding constituent as follows: a POS
bag is produced with the POS tags of the words
in the constituent except for the first and the last
ones, duplicated tags removed and the original or-
der ignored. Then we have the POS PAT feature
239
by combining the POS tag of the first word, the
bag and the POS tag of the last word.
CHD POS (a71), CHD POS NDUP (a72),
CHD REL (a73), CHD REL NDUP (a74): The
POS tags of the child words are joined to-
gether to form feature CHD POS. With adja-
cently duplicated tags reduced to one, feature
CHD POS NDUP is produced. Similarly we can
get CHD REL and CHD REL NDUP too, with
the relation types substituted for the POS tags.
SIB REL (a81), SIB REL NDUP (a82),
SIB POS (a83), SIB POS NDUP (a84): Sibling
words (including the target word itself) and the
corresponding dependency relations (or POS tags)
are considered as well. Four features are formed
similarly to those of child words.
VERB VOICE (a9): Verbs are examined for
voices: if the headword lemma is either ?be? or
?get?, or else the relation type is ?APPO?, then the
verb is considered passive, otherwise active.
Also we used some ?combined? features which
are combinations of single features. The final op-
timized feature set is (a1, a21, a22, a31, a32, a41,
a42, a51, a52, a6, a72, a73, a74, a81, a82, a83,
a1+a21, a21+a31, a21+a6, a21+a74, a73+a81,
a81+a83).
3.3 Predicate Classification
After predicate identification is done, the resulting
predicates are processed for sense classification. A
classifier is trained for each predicate that has mul-
tiple senses on the training data (There are totally
962 multi-sense predicates on the training corpus,
taking up 14% of all) In additional to those fea-
tures described in the predicate identification sec-
tion, some new ones relating to the predicate word
are introduced:
BAG OF WORD (b11), BAG OF WORD O
(b12): All words in a sentence joined, namely
?Bag of Words?. And an ?ordered? version is in-
troduced where each word is prefixed with a letter
?L?, ?R? or ?T? indicating it?s to the left or right of
the predicate or is the predicate itself.
BAG OF POS O (b21), BAG OF POS N
(b22): The POS tags prefixed with ?L?, ?R? or
?T? indicating the word position joined together,
namely ?Bag of POS (Ordered)?. With the
prefixed letter changed to a number indicating
the distance to the predicate (negative for being
left to the predicate and positive for right), an-
other feature is formed, namely ?Bag of POS
(Numbered)?.
WIND5 BIGRAM (b3): 5 closest words from
both left and right plus the predicate itself, in total
11 words form a ?window?, within which bigrams
are enumerated.
The final optimized feature set for the task of
predicate classification is (a1, a21, a23, a71, a72,
a73, a74, a81, a82, a83, a84, a9, b11, b12, b22, b3,
a71+a9).
3.4 Semantic Role Classification
In our system, the identification and classifica-
tion of semantic roles are achieved in a single
stage (Liu et al, 2005) through one single classi-
fier (actually two, one for noun predicates, and the
other for verb predicates). Each word in a sentence
is given probabilities to be each semantic role (in-
cluding none of the these roles) for a predicate.
Features introduced in addition to those of the pre-
vious subsections are the following:
POS PATH (c11), REL PATH (c12): The ?POS
Path? feature consists of POS tags of the words
along the path from a word to the predicate. Other
than ?Up? and ?Down?, the ?Left? and ?Right? di-
rection of the path is added. Similarly, the ?Re-
lation Path? feature consists of the relation types
along the same path.
UP PATH (c21), UP REL PATH (c22): ?Up-
stream paths? are parts of the above paths that stop
at the common ancestor of a word and the predi-
cate.
PATH LEN (c3): Length of the paths
POSITION (c4): The relative position of a word
to the predicate: Left or Right.
PRED FAMILYSHIP (c5): ?Familyship rela-
tion? between a word and the predicate, being one
of ?self?, ?child?, ?descendant?, ?parent?, ?ances-
tor?, ?sibling?, and ?not-relative?.
PRED SENSE (c6): The lemma plus sense
number of the predicate
As for the task of semantic role classification,
the features of the predicate word in addition to
those of the word under consideration can also
be used; we mark features of the predicate with
an extra ?p?. For example, the head word of
the current word is represented as a31, and the
head word of the predicate is represented as pa31.
So, with no doubt for the representation, our fi-
nal optimized feature set for the task of seman-
tic role classification is (a1, a23, a33, a43, a53,
a6, c11, c12, c21, c3, c4, c6, pa23, pa71, pa73,
240
pa83, a1+a23+a33, a21+c5, a23+c12, a33+c12,
a33+c22, a6+a33, a73+c5, c11+c12, pa71+pa73).
3.5 ILP-based Post Inference
The final semantic role labeling result is gener-
ated through an ILP (Integer Linear Programming)
based post inference method. An ILP problem is
formulated with respect to the probability given by
the above stage. The final labeling is formed at the
same time when the problem is solved.
Let W be the set of words in the sentence, and
R be the set of semantic role labels. A virtual label
?NULL? is also added to R, representing ?none of
the roles is assigned?.
For each word w ? W and semantic role label
r ? R we create a binary variable v
wr
? (0, 1),
whose value indicates whether or not the word w
is labeled as label r. p
wr
denotes the possibil-
ity of word w to be labeled as role r. Obviously,
when objective function f =
?
w,r
log(p
wr
? v
wr
)
is maximized, we can read the optimal labeling for
a predicate from the assignments to the variables
v
wr
. There are three constrains used in our system:
C1: Each relation should be and only be la-
beled with one label (including the virtual label
?NULL?), i.e.:
?
r
v
wr
= 1
C2: Roles with a small probability should never
be labeled (except for the virtual role ?NULL?).
The threshold we use in our system is 0.3, which
is optimized from the development data. i.e.:
v
wr
= 0, if p
wr
< 0.3 and r 6= ?NULL?
C3: Statistics shows that the most roles (ex-
cept for the virtual role ?NULL?) usually appear
only once for a predicate, except for some rare ex-
ception. So we impose a no-duplicated-roles con-
straint with an exception list, which is constructed
according to the times of semantic roles? duplica-
tion for each single predicate (different senses of a
predicate are considered different) and the ratio of
duplication to non-duplication.
?
r
v
wr
? 1,
if < p, r > /? {< p, r > |p ? P, r ? R;
d
pr
c
pr
?d
pr
> 0.3 ? d
pr
> 10}
(2)
where P is the set of predicates; c
pr
denotes the
count of words in the training corpus, which are
Predicate Type Predicate Label
Noun president.01 A3
Verb match.01 A1
Verb tie.01 A1
Verb link.01 A1
Verb rate.01 A0
Verb rate.01 A2
Verb attach.01 A1
Verb connect.01 A1
Verb fit.01 A1
Noun trader.01 SU
Table 3: No-duplicated-roles constraint exception
list (obtained by Eq. (2))
labeled as r ? R for predicate p ? P ; while d
pr
denotes something similar to c
pr
, but what taken
into account are only those words labeled with r,
and there are more than one roles within the sen-
tence for the same predicate. Table 3 lists the com-
plete exception set, which has a size of only 10.
4 Experiments
The original MSTParser
1
is implemented in Java.
We were confronted with memory shortage when
trying to train a model with the entire CoNLL 2008
training data with 4GB memory. Therefore, we
rewrote it with C++ which can manage the mem-
ory more exactly. Since the time was limited, we
only rewrote the projective part without consider-
ing second-order parsing technique.
Our maximum entropy classifier is implemented
with Maximum Entropy Modeling Toolkit
2
. The
classifier parameters: gaussian prior and iterations,
are tuned with the development data for different
stages respectively.
lp solve 5.5
3
is chosen as our ILP problem
solver during the post inference stage.
The training time of the syntactic and the se-
mantic parsers are 22 and 5 hours respectively, on
all training data, with 2.0GHz Xeon CPU and 4G
memory. While the prediction can be done within
10 and 5 minutes on the development data.
4.1 Syntactic Dependency Parsing
The experiments on development data show that
relabeling process is helpful, which improves the
1
http://sourceforge.net/projects/mstparser
2
http://homepages.inf.ed.ac.uk/s0450736/maxent
toolkit.html
3
http://sourceforge.net/projects/lpsolve
241
Precision (%) Recall (%) F1
Pred Identification 91.61 91.36 91.48
Pred Classification 86.61 86.37 86.49
Table 4: The performance of predicate identifica-
tion and classification
Precision (%) Recall (%) F1
Simple 81.02 76.00 78.43
ILP-based 82.53 75.26 78.73
Table 5: Comparison between different post infer-
ence strategies
LAS performance from 85.41% to 85.94%. The fi-
nal syntactic dependency parsing performances on
the WSJ and the Brown test data are 87.51% and
80.73% respectively.
4.2 Semantic Dependency Parsing
The semantic dependency parsing component is
based on the last syntactic dependency parsing
component. All stages of the system are trained
with the closed training corpus, while predicted
against the output of the syntactic parsing.
Performance for predicate identification and
classification is given in Table 4, wherein the clas-
sification is done on top of the identification.
Semantic role classification and the post infer-
ence are done on top of the result of predicate iden-
tification and classification. The final performance
is presented in Table 5. A simple post inference
strategy is given for comparison, where the most
possible label (including the virtual label ?NULL?)
is select except for those duplicated non-virtual la-
bels with lower probabilities (lower than 0.5). Our
ILP-based method produces a gain of 0.30 with re-
spect to the F1 score.
The final semantic dependency parsing perfor-
mance on the development and the test (WSJ and
Brown) data are shown in Table 6.
Precision (%) Recall (%) F1
Development 82.53 75.26 78.73
Test (WSJ) 82.67 77.50 80.00
Test (Brown) 64.38 68.50 66.37
Table 6: Semantic dependency parsing perfor-
mances
4.3 Overall Performance
The overall macro scores of our syntactic and se-
mantic dependency parsing system are 82.38%,
83.78% and 73.57% on the development and two
test (WSJ and Brown) data respectively, which is
ranked the second position in the closed challenge.
5 Conclusion and Future Work
We present our CoNLL 2008 Shared Task system
which is composed of two cascaded components:
a syntactic and a semantic dependency parsers,
which are built with some state-of-the-art methods.
Through a fine tuning features and parameters, the
final system achieves promising results. In order
to improve the performance further, we will study
how to make use of more resources and tools (open
challenge) and how to do joint learning between
syntactic and semantic parsing.
Acknowledgments
The authors would like to thank the reviewers for
their helpful comments. This work was supported
by National Natural Science Foundation of China
(NSFC) via grant 60675034, 60575042, and the
?863? National High-Tech Research and Develop-
ment of China via grant 2006AA01Z145.
References
Liu, Ting, Wanxiang Che, Sheng Li, Yuxuan Hu, and
Huaijun Liu. 2005. Semantic role labeling system
using maximum entropy classifier. In Proceedings
of CoNLL-2005, June.
McDonald, Ryan. 2006. Discriminative Learning and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Punyakanok, Vasin, Dan Roth, Wen-tau Yih, and Dav
Zimak. 2004. Semantic role labeling via integer
linear programming inference. In Proceedings of
Coling-2004, pages 1346?1352.
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008).
242
