University of Colorado Dialog Systems for 
Travel and Navigation 
B. Pellom, W. Ward, J. Hansen, R. Cole, K. Hacioglu, J. Zhang, X. Yu, S. Pradhan 
Center for Spoken Language Research, University of Colorado 
Boulder, Colorado 80303, USA 
{pellom, whw, jhlh, cole, hacioglu, zjp, xiu, spradhan}@cslr.colorado.edu 
 
ABSTRACT 
This paper presents recent improvements in the development of 
the University of Colorado ?CU Communicator? and ?CU-
Move? spoken dialog systems. First, we describe the CU 
Communicator system that integrates speech recognition, 
synthesis and natural language understanding technologies using 
the DARPA Hub Architecture. Users are able to converse with an 
automated travel agent over the phone to retrieve up-to-date 
travel information such as flight schedules, pricing, along with 
hotel and rental car availability.  The CU Communicator has 
been under development since April of 1999 and represents our 
test-bed system for developing robust human-computer 
interactions where reusability and dialogue system portability 
serve as two main goals of our work.  Next, we describe our more 
recent work on the CU Move dialog system for in-vehicle route 
planning and guidance.  This work is in joint collaboration with 
HRL and is sponsored as part of the DARPA Communicator 
program.  Specifically, we will provide an overview of the task, 
describe the data collection environment for in-vehicle systems 
development, and describe our initial dialog system constructed 
for route planning. 
1. CU COMMUNICATOR 
1.1 Overview  
The Travel Planning Task 
The CU Communicator system [1,2] is a Hub compliant 
implementation of the DARPA Communicator task [3].  The 
system combines continuous speech recognition, natural 
language understanding and flexible dialogue control to enable 
natural conversational interaction by telephone callers to access 
information from the Internet pertaining to airline flights, hotels 
and rental cars.  Specifically, users can describe a desired airline 
flight itinerary to the Communicator and use natural dialog to 
negotiate a flight plan.  Users can also inquire about hotel 
availability and pricing as well as obtain rental car reservation 
information.   
System Overview 
The dialog system is composed of a Hub and several servers as 
shown in Fig. 1.  The Hub is used as a centralized message router 
through which servers can communicate with one another [4].  
Frames containing keys and values are emitted by each server, 
routed by the hub, and received by a secondary server based on 
rules defined in a ?Hub script?.   
 
 
 
 
 
 
 
 
 
Figure 1.  Block diagram of the functional components that 
comprise the CU Communicator system1. 
1.2 Audio Server 
The audio server is responsible for answering the incoming call, 
playing prompts and recording user input.  Currently, our system 
uses the MIT/MITRE audio server that was provided to DARPA 
Communicator program participants.  The telephony hardware 
consists of an external serial modem device that connects to the 
microphone input and speaker output terminals on the host 
computer.  The record process is pipelined to the speech 
recognition server and the play process is pipelined the text-to-
speech server.  This audio server does not support barge-in. 
Recently we have developed a new audio server that supports 
barge-in using the Dialogic hardware platform.  The new audio 
server implements a Fast Normalized Least-Mean-Square (LMS) 
algorithm for software-based echo cancellation.  During 
operation, the echo from the system speech is actively cancelled 
from the recorded audio to allow the user to cut through while 
                                                          
1
 This work was supported by DARPA through SPAWAR under 
Grant No. N66001-002-8906.  The ?CU Move? system is 
supported in part through a joint collaboration with HRL 
Laboratories. 
 
 
 
Language
Generator
Language
enerator
Hub
Speech 
Recognizer
Speech 
Recognizer
Speech 
Synthesizer
Speech 
Synthesizer
Semantic
Parser
Se antic
Parser
Dialogue
Manager
Dialogue
anager
Data Base / 
Backend
Data Base / 
Backend
Confidence
Server
Confidence
Server
Audio ServerAudio Server
www
the system is speaking.  The new audio server operates in the 
Linux environment and is currently being field-tested at CSLR.  
Because the server implements software-based echo cancellation, 
it can work on virtually any low-cost Dialogic hardware 
platform.  This server will be made available to the research 
community as a resource in the near future. 
1.3 Speech Recognizer 
We are currently using the Carnegie Mellon University Sphinx-II 
system [5] in our speech recognition server. This is a semi-
continuous Hidden Markov Model recognizer with a class 
trigram language model. The recognition server receives the 
input vectors from the audio server. The recognition server 
produces a word lattice from which a single best hypothesis is 
picked and sent to the hub for processing by the dialog manager. 
Acoustic Modeling 
During dialog interaction with the user, the audio server sends 
the acoustic samples to three Sphinx-II speech recognizers.  
While the language model is the same for each decoder, the 
acoustic models consist of (i) speaker independent analog 
telephone, (ii) female adapted analog telephone, and (iii) cellular 
telephone adapted acoustic model sets.   Each decoder outputs a 
word string hypothesis along with a word-sequence probability 
for the best path.  An intermediate server is used to examine each 
hypothesis and pass the most likely word string onto the natural 
language understanding module.   
Language Modeling 
The Communicator system is designed for end users to get up-to-
date worldwide air travel, hotel and rental car information via the 
telephone. In the task there are word lists for countries, cities, 
states, airlines, etc.  To train a robust language model, names are 
clustered into different classes. An utterance with class tagging is 
shown in Fig.2.  In this example, city, hour_number, and am_pm 
are class names. 
Figure 2.  Examples of class-based and grammar-based 
language modeling  
Each commonly used word takes one class. The probability of 
word Wi given class Ci is estimated from training corpora. After 
the corpora are correctly tagged, a back-off class-based trigram 
language model can be computed from the tagged corpora.  We 
use the CMU-Cambridge Statistical Language Modeling Toolkit 
to compute our language models. 
More recently, we have developed a dialog context dependent 
language model (LM) combining stochastic context free 
grammars (SCFGs) and n-grams [6,7].  Based on a spoken 
language production model in which a user picks a set of 
concepts with respective values and constructs word sequences 
using phrase generators associated with each concept in 
accordance with the dialog context, this LM computes the 
probability of a word, P(W), as 
 
         P(W) = P(W/C) P(C/S)          (1) 
 
where W is the sequence of words, C is the sequence of concepts 
and S is the dialog context. Here, the assumptions are (i) S is 
given, (ii) W is independent of S but C, and (iii) W and C 
associations are unambiguous. This formulation can be 
considered as a general extension of the standard class word 
based statistical language model as seen in Fig. 2. 
 
The first term in (1) is modeled by SCFGs, one for each concept. 
The concepts are classes of phrases with the same meaning. Each 
SCFG is compiled into a stochastic recursive transition network 
(STRN). Our grammar is a semantic grammar since the 
nonterminals correspond to semantic concepts instead of 
syntactic constituents. The set of task specific concepts is 
augmented with a single word, multiple word and a small number 
of broad but unambigious part of speech (POS) classes to 
account for the phrases that are not covered by the grammar. 
These classes are considered as "filler" concepts within a unified 
framework. The second term in (1) is modeled as a pool of 
concept n-gram LMs. That is, we have a separate LM for each 
dialog context. At the moment, the dialog context is selected as 
the last question prompted by the system, as it is very simple and 
yet strongly predictive and constraining. SCFG and n-gram 
probabilities are learned by simple counting and smoothing. Our 
semantic grammars have a low degree of ambiguity and therefore 
do not require computationally intensive stochastic training and 
parsing techniques. 
 
Experimental results with N-best list rescoring were found 
promising (5-6% relative improvement in WER).  In addition, we 
have shown that a dynamic combining of our new LM and the 
standard class word n-gram (the LM currently in use in our 
system) should result in further improvements. At the present, we 
are interfacing the grammar LM to the speech recognizer using a 
word graph. 
1.4 Confidence Server 
Our prior work on confidence assessment has considered 
detection and rejection of word-level speech recognition errors 
and out-of-domain phrases using language model features [8].  
More recently [9], we have considered detection and rejection of 
misrecognized units at the concept level.  Because concepts are 
used to update the state of the dialog system, we believe that 
concept level confidence is vitally important to ensuring a 
graceful human-computer interaction.  Our current work on 
concept error detection has considered language model features 
(e.g., LM back-off behavior, language model score) as well as 
acoustic features from the speech recognizer (e.g., normalized 
acoustic score, lattice density, phone perplexity).  Confidence 
Original Utterance 
I want to go from Boston to Portland around nine a_m 
Class-Tagged Utterance 
I want to go from [city:Boston] to [city:Portland] 
around [hour_number:nine] [am_pm:a_m] 
Concept-Tagged Utterance 
[I_want: I want to go] [depart_loc: from Boston] 
[arrive_loc: to Portland] [time:around nine a_m] 
features are combined to compute word-level, concept-level, and 
utterance-level confidence scores.  
1.5 Language Understanding 
We use a modified version of the Phoenix [10] parser to map the 
speech recognizer output onto a sequence of semantic frames. A 
Phoenix frame is a named set of slots, where the slots represent 
related pieces of information. Each slot has an associated 
context-free semantic grammar that specifies word string patterns 
that can fill the slot. The grammars are compiled into Recursive 
Transition Networks, which are matched against the recognizer 
output to fill slots. Each filled slot contains a semantic parse tree 
with the slot name as root.  
Phoenix has been modified to also produce an extracted 
representation of the parse that maps directly onto the task 
concept structures. For example, the utterance  
?I want to go from Boston to Denver Tuesday morning?  
would produce the extracted parse: 
Flight_Constraint: Depart_Location.City.Boston 
Flight_Constraint: Arrive_Location.City.Denver 
Flight Constraints:[Date_Time].[Date].[Day_Name].tuesday 
                             [Time_Range].[Period_Of_Day].morning 
1.6 Dialog Management 
The Dialogue Manager controls the system?s interaction with the 
user and the application server. It is responsible for deciding 
what action the system will take at each step in the interaction. 
The Dialogue Manager has several functions. It resolves 
ambiguities in the current interpretation; Estimates confidence in 
the extracted information; Clarifies the interpretation with the 
user if required; Integrates new input with the dialogue context; 
Builds database queries (SQL); Sends information to NL 
generation for presentation to user; and prompts the user for 
missing information. 
We have developed a flexible, event driven dialogue manager in 
which the current context of the system is used to decide what to 
do next. The system does not use a dialogue network or a 
dialogue script, rather a general engine operates on the semantic 
representations and the current context to control the interaction 
flow.  The Dialogue Manager receives the extracted parse. It then 
integrates the parse into the current context. Context consists of a 
set of frames and a set of global variables. As new extracted 
information arrives, it is put into the context frames and 
sometimes used to set global variables. The system provides a 
general-purpose library of routines for manipulating frames. 
This ?event driven? architecture functions similar to a production 
system. An incoming parse causes a set of actions to fire which 
modify the current context. After the parse has been integrated 
into the current context, the DM examines the context to decide 
what action to take next. The DM attempts the following actions 
in the order listed: 
? Clarify if necessary  
? Sign off if all done  
? Retrieve data and present to user  
? Prompt user for required information  
The rules for deciding what to prompt for next are very 
straightforward. The frame in focus is set to be the frame 
produced in response to the user, or to the last system prompt.  
? If there are unfilled required slots in the focus frame, then 
prompt for the highest priority unfilled slot in the frame. 
? If there are no unfilled required slots in the focus frame, 
then prompt for the highest priority missing piece of 
information in the context.  
Our mechanism does not have separate ?user initiative? and 
?system initiative? modes. If the system has enough information 
to act on, then it does it. If it needs information, then it asks for 
it. The system does not require that the user respond to the 
prompt. The user can respond with anything and the system will 
parse the utterance and set the focus to the resulting frame. This 
allows the user to drive the dialog, but doesn?t require it. The 
system prompts are organized locally, at the frame level. The 
dialog manager or user puts a frame in focus, and the system tries 
to fill it. This representation is easy to author, there is no separate 
dialog control specification required. It is also robust in that it 
has a simple control that has no state to lose track of. 
An additional benefit of Dialog Manager mechanism is that it is 
very largely declarative. Most of the work done by a developer 
will be the creation of frames, forms and grammars. The system 
developer creates a task file that specifies the system ontology 
and templates for communicating about nodes in the hierarchy. 
The templates are filled in from the values in the frames to 
generate output in the desired language. This is the way we 
currently generate SQL queries and user prompts. An example 
task frame specification is: 
Frame:Air 
 [Depart_Loc]+ 
    Prompt: "where are you departing from" 
    [City_Name]* 
 Confirm: "You are departing from $([City_Name]).  
    Is that correct?" 
 Sql: "dep_$[leg_num] in (select airport_code from 
 airport_codes where city like '!%' $(and state_province 
like '[Depart_Loc].[State]' ) )" 
    [Airport_Code]* 
 
This example defines a frame with name Air and slot 
[Depart_Loc]. The child nodes of Depart_Loc are are 
[City_Name] and [Airport_Code]. The ?+? after [Depart_Loc] 
indicates that it is a mandatory field. The Prompt string is the 
template for prompting for the node information. The ?*? after 
[City_Name] and [Airport_Code] indicate that if either of them is 
filled, the parent node [Depart_Loc] is filled. The Confirm string 
is a template to prompt the user to confirm the values. The SQL 
string is the template to use the value in an SQL query to the 
database. 
The system will prompt for all mandatory nodes that have 
prompts. Users may specify information in any order, but the 
system will prompt for whatever information is missing until the 
frame is complete.   
1.7 Database & Internet Interface  
The back-end interface consists of an SQL database and domain-
specific Perl scripts for accessing information from the Internet.  
During operation, database requests are transmitted by the Dialog 
Manager to the database server via a formatted frame. 
The back-end consists of a static and dynamic information 
component.  Static tables contain data such as conversions 
between 3-letter airport codes and the city, state, and country of 
the airport (e.g., BOS for Boston Massachusetts).  There are over 
8000 airports in our database, 200 hotel chains, and 50 car rental 
companies.  The dynamic information content consists of 
database tables for car, hotel, and airline flights.   
When a database request is received, the Dialog Manager?s SQL 
command is used to select records in local memory.  If no 
records are found to match, the back-end can submit an HTTP-
based request for the information via the Internet.  Records 
returned from the Internet are then inserted as rows into the local 
SQL database and the SQL statement is once again applied.   
1.8 Language Generation 
The language generation module uses templates to generate text 
based on dialog speech acts.  Example dialog acts include 
?prompt? for prompting the user for needed information, 
?summarize? for summarization of flights, hotels, and rental cars, 
and ?clarify? for clarifying information such as departure and 
arrival cities that share the same name. 
1.9 Text-to-Speech Synthesis 
For audio output, we have developed a domain-dependent 
concatenative speech synthesizer.  Our concatenative synthesizer 
can adjoin units ranging from phonemes, to words, to phrases 
and sentences.   For domain modeling, we use a voice talent to 
record entire task-dependent utterances  (e.g., ?What are your 
travel plans??) as well as short phrases with carefully determined 
break points (e.g., ?United flight?, ?ten?, ?thirty two?, ?departs 
Anchorage at?).    Each utterance is orthographically transcribed 
and phonetically aligned using a HMM-based recognizer.   Our 
research efforts for data collection are currently focused on 
methods for reducing the audible distortion at segment 
boundaries, optimization schemes for prompt generation, as well 
as tools for rapidly correcting boundary misalignments.  In 
general, we find that some degree of hand-correction is always 
required in order to reduce distortions at concatenation points. 
During synthesis, the text is automatically divided into individual 
sentences that are then synthesized and pipelined to the audio 
server.  A text-to-phoneme conversion is applied using a 
phonetic dictionary.  Words that do not appear in the phonetic 
dictionary are automatically pronounced using a multi-layer 
perceptron based pronunciation module.  Here, a 5-letter context 
is extracted from the word to be pronounced.  The letter input is 
fed through the MLP and a phonetic symbol (or possibly epsilon) 
is output by the network.  By sliding the context window, we can 
extract the phonetic pronunciation of the word.   The MLP is 
trained using letter-context and symbol output pairs from a large 
phonetic dictionary. 
The selection of units to concatenate is determined using a hybrid 
search algorithm that operates at the word or phoneme level.  
During synthesis, sections of word-level text that have been 
recorded are automatically concatenated.  Unrecorded words or 
word sequences are synthesized using a Viterbi beam search 
across all available phonetic units.  The cost function includes 
information regarding phonetic context, pitch, duration, and 
signal amplitude.  Audio segments making up the best-path are 
then concatenated to generate the final sentence waveform.   
2. DATA COLLECTION & EVALUATION 
2.1 Data Collection Efforts 
Local Collection Effort 
The Center for Spoken Language Research maintains a dialup 
Communicator system for data collection1. Users wishing to use 
the dialogue system can register at our web site [1] and receive a 
PIN code and system telephone number. To date, our system has 
fielded over 1750 calls totaling over 25,000 utterances from 
nearly 400 registered users.  
NIST Multi-Site Data Collection 
During the months of June and July of 2000, The National 
Institute of Standards (NIST) conducted a multi-site data 
collection effort for the nine DARPA Communicator 
participants.  Participating sites included: AT&T, IBM, BBN, 
SRI, CMU, Colorado, MIT, Lucent, and MITRE.  In this data 
collection, a pool of potential users was selected from various 
parts of the United States by a market research firm.  The 
selected subjects were native speakers of American English who 
were possible frequent travelers.  Users were asked to perform 
nine tasks.  The first seven tasks consisted of fixed scenarios for 
one-way and round-trip flights both within and outside of the 
United States. The final two tasks consisted of users making 
open-ended business or vacation.   
2.2 System Evaluation 
Task Completion 
A total of 72 calls from NIST participants were received by the 
CU Communicator system.  Of these, 44 callers were female and 
28 were male.  Each scenario was inspected by hand and 
compared against the scenario provided by NIST to the subject. 
For the two open-ended tasks, judgment was made based on what 
the user asked for with that of the data provided to the user. In 
total, 53/72 (73.6%) of the tasks were completed successfully.   
A detailed error analysis can be found in [11]. 
Word Error Rate Analysis 
A total of 1327 utterances were recorded from the 72 NIST calls.  
Of these, 1264 contained user speech.  At the time of the June 
2000 NIST evaluation, the CU Communicator system did not 
implement voice-based barge-in.  We noticed that one source of 
error was due to users who spoke before the recording process 
was started.  Even though a tone was presented to the user to 
signify the time to speak, 6.9% of the utterances contained 
instances in which the user spoke before the tone.  Since all users 
were exposed to several other Communicator systems that 
                                                          
2
 The system can be accessed toll-free at 1-866-735-5189 
employed voice barge-in, there may be some effect from 
exposure to those systems. Table 3 summarizes the word error 
rates for the system utilizing the June 2000 NIST data as the test 
set.  Overall, the system had a word error rate (WER) of 26.0% 
when parallel gender-dependent decoders were utilized. Since 
June of 2000, we have collected an additional 15,000 task-
dependent utterances.  With the extra data, we were able to 
remove our dependence on the CMU Communicator training 
data [12].  When the language model was reestimated and 
language model weights reoptimized using only CU 
Communicator data, the WER dropped from 26.0% to 22.5%.  
This amounts to a 13.5% relative reduction in WER. 
Table 1: CU Communicator Word Error Rates for (A) 
Speaker Independent acoustic models and June 2000 
language model, (B) Gender-dependent parallel recognizers 
with June 2000 Language Model, and (C) Language Model 
retrained in December 2000. 
June 2000 NIST Evaluation Data, 1264 
utterances, 72 speakers 
Word Error 
Rate 
(A) Speaker Indep. HMMs (LM#1) 29.8% 
(B) Gender Dependent HMMs (LM#1) 26.0% 
(C) Gender Dependent HMMs (LM#2)  22.5% 
 
Core Metrics 
Sites in the DARPA Communicator program agreed to log a 
common set of metrics for their systems. The proposed set of 
metrics was: Task Completion, Time to Completion, Turns to 
Completion, User Words/Turn, System Words/Turn, User 
Concepts/Turn, Concept Efficiency, State of Itinerary, Error 
Messages, Help Messages, Response Latency, User Words to 
Completion, System Words to Completion, User Repeats, System 
Repeats/Reprompts, Word Error, Mean Length of System 
Utterance, and Mean Length of System Turn. 
Table 2: Dialogue system evaluation metrics 
Item Min Mean Max 
Time to Completion (secs) 120.9 260.3 537.2 
Total Turns to Completion 23 37.6 61 
Response Latency (secs) 1.5 1.9 2.4 
User Words to Task End 19 39.4 105 
System Words to End 173 331.9 914 
Number of Reprompts 0 2.4 15 
 
Table 2 summarizes results obtained from metrics derived 
automatically from the logged timing markers for the calls in 
which the user completed the task assigned to them.  The average 
time to task completion is 260.  During this period there are an 
average of 19 user turns and 19 computer turns (37.6 average 
total turns).  The average response latency was 1.86 seconds.  
The response latency also includes the time required to access the 
data live from the Internet travel information provider. 
3. CU MOVE 
3.1 Task Overview 
The ?CU Move? system represents our work towards achieving 
graceful human-computer interaction in automobile 
environments.  Initially, we have considered the task of vehicle 
route planning and navigation.  As our work progresses, we will 
expand our dialog system to new tasks such as information 
retrieval and summarization and multimedia access. 
The problem of voice dialog within vehicle environments offers 
some important speech research challenges. Speech recognition 
in car environments is in general fragile, with word-error-rates 
(WER) ranging from 30-65% depending on driving conditions. 
These changing environmental conditions include speaker 
changes (task stress, emotion, Lombard effect, etc.) as well as the 
acoustic environment (road/wind noise from windows, air 
conditioning, engine noise, exterior traffic, etc.).   
In developing the CU-Move system [13,14], there are a number 
of research challenges that must be overcome to achieve reliable 
and natural voice interaction within the car environment. Since 
the speaker is performing a task (driving the vehicle), the driver 
will experience a measured level of user task stress and therefore 
this should be included in the speaker-modeling phase. Previous 
studies have clearly shown that the effects of speaker stress and 
Lombard effect can cause speech recognition systems to fail 
rapidly. In addition, microphone type and placement for in-
vehicle speech collection can impact the level of acoustic 
background noise and speech recognition performance.    
3.2 Signal Processing  
Our research for robust recognition in automobile environments 
is concentrated on development of an intelligent microphone 
array.  Here, we employ a Gaussian Mixture Model (GMM) 
based environmental classification scheme to characterize the 
noise conditions in the automobile.  By integrating an 
environmental classification system into the microphone array 
design, decisions can be made as to how best to utilize a noise-
adaptive frequency-partitioned iterative enhancement algorithm 
[15,16] or model-based adaptation algorithms [17,18] during 
decoding to optimize speech recognition accuracy on the beam-
formed signal. 
3.3 Data Collection 
A five-channel microphone array was constructed using Knowles 
microphones and a multi-channel data recorder housing built 
(Fostex) for in-vehicle data collection. An additional reference 
microphone is situated behind the driver?s seat.  Fig. 3 shows the 
constructed microphone array and data recorder housing. 
      
Figure 3: Microphone array and reference microphone (left), 
Fostex multi-channel data recorder (right). 
As part of the CU-Move system formulation, a two phase data 
collection plan has been initiated. Phase I focuses on collecting 
acoustic noise and probe speech from a variety of cars and 
driving conditions. Phase II focuses on a extensive speaker 
collection across multiple U.S. sites. A total of eight vehicles 
have been selected for acoustic noise analysis. These include the 
following: a compact car, minivan, cargo van, sport utility 
vehicle (SUV), compact and full size trucks, sports car, full size 
luxury car.  A fixed 10 mile route through Boulder, CO was used 
for Phase I data collection. The route consisted of city (25 & 
45mph) and highway driving (45 & 65mph). The route included 
stop-and-go traffic, and prescribed locations where 
driver/passenger windows, turn signals, wiper blades, air 
conditioning were operated. Each data collection run per car 
lasted approximately 35-45 minutes.  A detailed acoustic analysis 
of Phase I data can be found in [13]. Our plan is to begin Phase 
II speech/dialogue data collection during spring 2001, which will 
include (i) phonetically balanced utterances, (ii) task-specific 
vocabularies, (iii) natural extemporaneous speech, and (iv) 
human-to-human and Wizard-of-Oz (WOZ) interaction with CU-
Communicator and CU-Move dialog systems. 
3.4 Prototype Dialog System 
Finally, we have developed a prototype dialog system for data 
collection in the car environment.  The dialog system is based on 
the MIT Galaxy-II Hub architecture with base system 
components derived from the CU Communicator system [1].  
Users interacting with the dialog system can enter their origin 
and destination address by voice. Currently, 1107 street names 
for Boulder, CO area are modeled.  The system can resolve street 
addresses by business name via interaction with an Internet 
telephone book.  This allows users to ask more natural route 
queries (e.g., ?I need an auto repair shop?, or ?I need to get to the 
Boulder Marriott?).  The dialog system automatically retrieves 
the driving instructions from the Internet using an online WWW 
route direction provider.  Once downloaded, the driving 
directions are queried locally from an SQL database.  During 
interaction, users mark their location on the route by providing 
spoken odometer readings.  Odometer readings are needed since 
GPS information has not yet been integrated into the prototype 
dialog system. Given the odometer reading of the vehicle as an 
estimate of position, route information such as turn descriptions, 
distances, and summaries can be queried during travel (e.g., 
"What's my next turn", "How far is it", etc.).  
The prototype system uses the CMU Sphinx-II speech recognizer 
with cellular telephone acoustic models along with the Phoenix 
Parser [10] for semantic parsing.  The dialog manager is mixed-
initiative and event driven.  For route guidance, the natural 
language generator formats the driving instructions before 
presentation to the user by the text-to-speech server.   For 
example, the direction,  "Park Ave W. becomes 22nd St." is 
reformatted to, "Park Avenue West becomes Twenty Second  
Street".  Here, knowledge of the task-domain can be used to 
significantly improve the quality of the output text.   For speech 
synthesis, we have developed a Hub-compliant server that 
interfaces to the AT&T NextGen speech synthesizer.   
3.5 Future Work 
We have developed a Hub compliant server that interfaces a 
Garmin GPS-III global positioning device to a mobile computer 
via a serial port link.  The GPS server reports vehicle velocity in 
the X,Y,Z directions as well as real-time updates of  vehicle 
position in latitude and longitude.  HRL Laboratories has 
developed a route server that interfaces to a major navigation 
content provider.  The HRL route server can take GPS 
coordinates as inputs and can describe route maneuvers in terms 
of GPS coordinates.  In the near-term, we will interface our GPS 
server to the HRL route server in order to provide real-time 
updating of vehicle position.  This will eliminate the need for 
periodic location update by the user and also will allow for more 
interesting dialogs to be established (e.g., the computer might 
proactively tell the user about upcoming points of interest, etc.). 
 
4. REFERENCES 
[1] http://communicator.colorado.edu 
[2] W. Ward, B. Pellom, "The CU Communicator System," IEEE 
Workshop on Automatic Speech Recognition and Understanding, 
Keystone Colorado, December, 1999. 
[3] http://fofoca.mitre.org 
[4] Seneff, S., Hurley, E., Lau, R., Pao, C., Schmid, P., Zue,  V., 
?Galaxy-II: A Reference Architecture for Conversational System 
Development,? Proc. ICSLP, Sydney Australia, Vol. 3, pp. 931-
934, 1998. 
[5] Ravishankar, M.K., ?Efficient Algorithms for Speech 
Recognition?. Unpublished Dissertation CMU-CS-96-138, 
Carnegie Mellon University, 1996 
[6] K. Hacioglu, W. Ward, "Dialog-Context Dependent Language 
Modeling Using N-Grams and Stochastic Context-Free Grammars", 
Proc. IEEE ICASSP, Salt Lake City, May 2001. 
[7] K. Hacioglu, W. Ward, "Combining Language Models : Oracle 
Approach", Proc. Human Language Technology Conference, San 
Diego, March 2001. 
[8] R. San-Segundo, B. Pellom, W. Ward, J. M. Pardo, "Confidence 
Measures for Dialogue Management in the CU Communicator 
System," Proc. IEEE ICASSP, Istanbul Turkey, June 2000. 
[9] R. San-Segundo, B. Pellom, K. Hacioglu, W. Ward, J.M. Pardo, 
"Confidence Measures for Dialogue Systems," Proc. IEEE ICASSP, 
Salt Lake City, May 2001.  
[10] Ward, W., ?Extracting Information From Spontaneous Speech?, 
Proc. ICSLP, September 1994. 
[11] B. Pellom, W. Ward, S. Pradhan, "The CU Communicator: An 
Architecture for Dialogue Systems", Proc. ICSLP, Beijing China, 
November 2000. 
[12] Eskenazi,  M., Rudnicky, A., Gregory, K., Constantinides, P.,  
Brennan, R., Bennett, K., Allen, J., ?Data Collection and 
Processing in the Carnegie Mellon Communicator,?   Proc. 
Eurospeech-99, Budapest, Hungary. 
[13] J.H.L. Hansen, J. Plucienkowski, S. Gallant, B.L. Pellom, W. Ward, 
"CU-Move: Robust Speech Processing for In-Vehicle Speech 
Systems," Proc. ICSLP, vol. 1, pp. 524-527, Beijing, China, Oct. 
2000. 
[14] http://cumove.colorado.edu/ 
[15] J.H.L. Hansen, M.A. Clements, ?Constrained Iterative Speech 
Enhancement with Application to Speech Recognition,? IEEE 
Trans. Signal Proc., 39(4):795-805, 1991. 
[16] B. Pellom, J.H.L. Hansen, ?An Improved Constrained Iterative 
Speech Enhancement Algorithm for Colored Noise Environments," 
IEEE Trans. Speech & Audio Proc., 6(6):573-79, 1998. 
[17] R. Sarikaya, J.H.L. Hansen, "Improved Jacobian Adaptation for 
Fast Acoustic Model Adaptation in Noisy Speech Recognition," 
Proc. ICSLP, vol. 3, pp. 702-705, Beijing, China, Oct. 2000. 
[18] R. Sarikaya, J.H.L. Hansen, "PCA-PMC: A novel use of a priori 
knowledge for fast model combination," Proc. ICASSP, vol. II, pp. 
1113-1116, Istanbul, Turkey, June 2000. 
Advances in Children?s Speech Recognition  
within an Interactive Literacy Tutor 
 
Andreas Hagen, Bryan Pellom, Sarel Van Vuuren, and Ronald Cole 
Center for Spoken Language Research 
University of Colorado at Boulder 
http://cslr.colorado.edu 
 
 
 
 
Abstract1 
In this paper we present recent advances in 
acoustic and language modeling that improve 
recognition performance when children read 
out loud within digital books. First we extend 
previous work by incorporating cross-
utterance word history information and dy-
namic n-gram language modeling. By addi-
tionally incorporating Vocal Tract Length 
Normalization (VTLN), Speaker-Adaptive 
Training (SAT) and iterative unsupervised 
structural maximum a posteriori linear regres-
sion (SMAPLR) adaptation we demonstrate a 
54% reduction in word error rate.  Next, we 
show how data from children?s read-aloud 
sessions can be utilized to improve accuracy 
in a spontaneous story summarization task.  
An error reduction of 15% over previous pub-
lished results is shown.  Finally we describe a 
novel real-time implementation of our re-
search system that incorporates time-adaptive 
acoustic and language modeling. 
1 Introduction 
Pioneering research by MIT and CMU as well as more 
recent work by the IBM Watch-me-Read Project have 
demonstrated that speech recognition can play an effec-
tive role in systems designed to improve children?s 
reading abilities (Mostow et al, 1994; Zue et al, 1996). 
In CMU?s Project LISTEN, for example, the tutor oper-
ates by prompting children to read individual sentences 
out loud.  The tutor listens to the child using speech 
recognition and extracts features that can be used to 
detect oral reading miscues (Mostow et al, 2002; Tam 
et al 2003).   Upon detecting reading miscues, the tutor 
provides appropriate feedback to the child.  Recent re-
                                                        
1
 This work was supported in part by grants from the National Science 
Foundation's Information Technology Research (ITR) Program and 
the Interagency Educational Research Initiative (IERI) under grants 
NSF/ITR: REC-0115419, NSF/IERI: EIA-0121201, NSF/ITR: IIS-
0086107, NSF/IERI: 1R01HD-44276.01, NSF: INT-0206207; and the 
Coleman Institute for Cognitive Disabilities. The views expressed in 
this paper do not necessarily represent the views of the NSF. 
sults show that such automated reading tutors can im-
prove student achievement (Mostow et al 2003). Pro-
viding real time feedback by highlighting words as the 
are read out loud is the basis of at least one commercial 
product today (http://www.soliloquy.com).  
Cole et al (2003) and Wise et al (in press) describe 
a new scientifically-based literacy program, Founda-
tions to Fluency, in which a virtual tutor?a lifelike 3D 
computer model?interacts with children in multimodal 
learning tasks to teach them to read. A key component 
of this program is the Interactive Book, which combines 
real-time speech recognition, facial animation, and natu-
ral language understanding capabilities to teach children 
to read and comprehend text.  Interactive Books are 
designed to improve student achievement by helping 
students to learn to read fluently, to acquire new knowl-
edge through deep understanding of what they read, to 
make connections to other knowledge, and to express 
their ideas concisely through spoken or written summa-
ries. Transcribed spoken summaries can be graded 
automatically to provide feedback to the student about 
their comprehension.  
During reading out loud activities in Interactive 
Books, the goal is to design a computer interface and 
speech recognizer that combine to teach the student to 
read fluently and naturally.  Here, speech recognition is 
used to track a child?s position within the text during 
read-aloud sessions in addition to providing timing and 
confidence information which can be used for reading 
assessment. The speech recognizer must follow the stu-
dents verbal behaviors accurately and quickly, so the 
cursor (or highlighted word) appears at the right place 
and right time when the student is reading fluently, and 
pauses when the student hesitates to sound out a word. 
The recognizer must also score mispronounced words 
accurately so that the student can revisit these words 
and receive feedback about their pronunciation after 
completing a paragraph or page (since highlighting hy-
pothesized mispronounced words when reading out loud 
may disrupt fluent reading behavior).   
In this paper we focus on the problem of speech rec-
ognition to track and provide feedback during reading 
out loud and to transcribe spoken summaries of text. 
Specifically, we describe several new methods for in-
corporating language modeling knowledge into the read 
aloud task.  In addition, through use of speaker adapta-
tion, we also demonstrate the potential for significant 
gains in recognition accuracy.  Finally, we leverage 
improvements in speech recognition for read aloud 
tracking to improve performance for spoken story sum-
marization.  Work reported here extends previous work 
in several important ways: by integrating the research 
advances into a real time system, and by including time-
adaptive language modeling and time-adaptive acoustic 
modeling of the child?s voice into the system. 
The paper is organized as follows. Sect. 2 describes 
our baseline speech recognition system and reading 
tracking method. Sect. 3 presents our rationale for using 
word-error-rate as a measure of performance.  Sect. 4 
describes the read aloud and story summarization cor-
pora used in this work. Sect. 5 describes and evaluates 
proposed improvements in a read aloud speech recogni-
tion task. Sect. 6 describes how these improvements 
translate to improved recognition of story summaries 
produced by a child. Sect. 7 details our real-time system 
implementation. 
2 Baseline System 
For this work we use the SONIC speech recognition 
system (Pellom, 2001; Pellom and Hacioglu, 2003).  
The recognizer implements an efficient time-
synchronous, beam-pruned Viterbi token-passing search 
through a static re-entrant lexical prefix tree while 
utilizing continuous density mixture Gaussian HMMs.  
For children?s speech, the recognizer has been trained 
on 46 hours of data from children in grades K through 9 
extracted from the CU Read and Prompted speech 
corpus (Hagen et al, 2003) and the OGI Kids? speech 
corpus (Shobaki et al, 2000).  Further, the baseline 
system utilizes PMVDR cepstral coefficients (Yapanel 
and Hansen, 2003) for improved noise robustness. 
During read-aloud operation, the speech recognizer 
models the story text using statistical n-gram language 
models.  This approach gives the recognizer flexibility 
to insert/delete/substitute words based on acoustics and 
to provide accurate confidence information from the 
word-lattice.  The recognizer receives packets of audio 
and automatically detects voice activity.  When the 
child speaks, the partial hypotheses are sent to a reading 
tracking module.  The reading tracking module deter-
mines the current reading location by aligning each par-
tial hypothesis with the book text using a Dynamic 
Programming search.  In order to allow for skipping of 
words or even skipping to a different place within the 
text, the search finds words that when strung together 
minimize a weighted cost function of adjacent word-
proximity and distance from the reader's last active 
reading location. The Dynamic Programming search 
additionally incorporates constraints to account for 
boundary effects at the ends of each partial phrase. 
3 Evaluation Methodology 
There are many different ways in which speech recogni-
tion can be used to serve children. In computer-based 
literacy tutors, speech recognition can be used to meas-
ure children's ability to read fluently and pronounce 
words while reading out loud, to engage in spoken dia-
logues with an animated agent to assess and train com-
prehension, or to transcribe spoken summaries of stories 
that can be graded automatically.  Because of the variety 
of ways of using speech recognition systems, it is criti-
cally important to establish common metrics that are 
used by the research community so that progress can be 
measured both within and across systems. 
For this reason, we argue that word error rate calcu-
lations using the widely accepted NIST scoring software 
provides the most widely accepted, easy to use and 
highly valid metric.  In this scoring procedure, word 
error rate is computed strictly by comparing the speech 
recognizer output against a known human transcription 
(or the text in a book).  Of course, authors are free to 
define and report other measures, such as detection/false 
alarm curves for useful events such as reading miscues.  
However, such analyses should always supplement re-
ports of word error rates using a single standardized 
measure. Adopting this strategy enables fair and bal-
anced comparisons within and across systems for any 
speech data given a known word-level transcription. 
4 Experimental Data 
For all experiments in this paper we use speech data and 
associated transcriptions from 106 children (grade 3: 17 
speakers, grade 4: 28 speakers, and grade 5: 61 speak-
ers) who were asked to read one of ten stories and to 
provide a spoken story summary.  The 16 kHz audio 
data contains an average of 1054 words (min 532 
words; max 1926 words) with an average of 413 unique 
words per story.  The resulting summaries spoken by 
children contain an average of 168 words. 
5 Improved Read-Aloud Recognition 
Baseline: Our baseline read-aloud system utilizes a 
trigram language model constructed from a normalized 
version of the story text. Text normalization consists 
primarily of punctuation removal and determination of 
sentence-like units.  For example,  
 
It was the first day of summer vacation.  Sue and Billy were 
eating breakfast.  ?What can we do today?? Billy asked. 
 
is normalized as: 
 
<s> IT WAS THE FIRST DAY OF SUMMERVACATION</s> 
<s> SUE AND BILLY WERE EATING BREAKFAST</s> 
<s> WHAT CAN WE DO TODAY </s> 
<s> BILLY ASKED </s> 
 
The resulting text is used to estimate a back-off trigram 
language model. We stress that only the story text is 
used to construct the language model. Details on the 
story texts are provided in Hagen et al (2003). Note that 
the sentence markers (<s> and </s>) are used to repre-
sent positions of expected speaker pause.  This baseline 
system is shown in Table 1(A) to produce a 17.4% word 
error rate. 
Improved Sentence Context Modeling: It is impor-
tant in the context of this research to note that children 
do not pause between each estimated sentence bound-
ary.  Instead, many children read fluently across phrases 
and sentences, where more experienced readers would 
pause. For this reason, we improved upon our baseline 
system by estimating language model parameters using 
a combined text material that is generated both with and 
without the contextual sentence markers (<s> and </s>).  
Results of this modification are shown in Table 1(B) 
and show a reduction in error from 17.4% to 13.5%. 
Improved Word History Modeling:  Most speech 
recognition systems operate on the utterance as a pri-
mary unit of recognition.  Word history information 
typically is not maintained across segmented utterances.  
However, in our text example, the words ?do today? 
should provide useful information to the recognizer that 
?Billy asked? may follow.  We therefore modify the 
recognizer to incorporate knowledge of previous utter-
ance word history. During token-passing search, the 
initial word-history tokens are modified to account for 
the fact that the incoming sentence may be either the 
beginning of a new sentence or a direct extension of the 
previous utterance?s word-end history.  Incorporating 
this constraint lowers the word error rate from 13.5% to 
12.7% as shown in Table 1(C). 
Dynamic n-gram Language Modeling:  During story 
reading we can anticipate words that are likely to be 
spoken next based upon the words in the text that are 
currently being read aloud.  To account for this knowl-
edge, we estimate a series of position-sensitive n-gram 
language models by partitioning the story into overlap-
ping regions containing at most 150 words (i.e., each 
region is centered on 50 words of text with 50 words 
before and 50 words after).  For each partition, we con-
struct an n-gram language model by using the entire 
normalized story text in addition to a 10x weighting of 
text within the partition.  Each position-sensitive lan-
guage model therefore contains the entire story vocabu-
lary.  We also compute a general language model 
estimated solely from the entire story text (similar to 
Table 1(C)).   At run-time, the recognizer implements a 
word-history buffer containing the most recent 15 rec-
ognized words.  After decoding each utterance, the 
probability of the text within the word history buffer is 
computed using each of the position-sensitive language 
models.  The language model with the highest probabil-
ity is selected for the first-pass decoding of the subse-
quent utterance.  This modification decreases the word 
error rate from 12.7% to 10.7% (Table 1(D)). 
Vocal Tract Normalization and Acoustic Adaptation:  
We further extend on our baseline system by incorporat-
ing the Vocal Tract Length Normalization (VTLN) 
method described in Welling et al (1999).  Based on 
results shown in Table 1(E), we see that VTLN provides 
only a marginal gain (0.1% absolute).  Our final set of 
acoustic models for the read aloud task are both VTLN 
normalized and estimated using Speaker Adaptive 
Training (SAT).  The SAT models are determined by 
estimating a single linear feature space transform for 
each training speaker (Gales, 1997).  The means and 
variances of the VTLN/SAT models are then iteratively 
adapted using the SMAPLR algorithm (Siohan, 2002) to 
yield a final recognition error rate of 8.0% absolute (Ta-
ble 1(G)).  By combining all of these techniques, we 
achieved a 54% reduction in word error rate relative to 
the baseline system.    
 
Word Error Rate (%) Experimental Configuration MFCC PMVDR 
(A) Baseline: single n-gram 
language model 17.7% 17.4% 
(B) (A) + Begin/End Sentence 
Context Modeling 14.0% 13.5% 
(C) (B) + between utterance 
word history modeling 13.0% 12.7% 
(D) (C) + dynamic  
n-gram language model 11.0% 10.7% 
(E) (D) + VTLN 10.9% 10.6% 
(F) (E) + VTLN/SAT + 
SMAPLR (iteration 1) 8.2% 8.2% 
(G) (E) + VTLN/SAT + 
SMAPLR (iteration 2) 8.0% 8.0% 
Table 1: Recognition of children?s read out-loud data. 
6 Improved Story Summary Recognition 
One of the unique and powerful features of our interac-
tive books is the notion of assessing and training com-
prehension by providing feedback to the student about a 
typed summary of text that the student has just read 
(Cole et al, 2003). Verbal input is especially important 
for younger children who often can not type well. Util-
izing summaries from the children?s speech corpus, 
Hagen et al (2003) showed that an error rate of 42.6% 
could be achieved.  The previous work, however, did 
not consider utilizing the read story material to provide 
improved initial acoustic models for the summarization 
task.  In Table 2 we demonstrate several findings using 
a language model trained on story text and example 
summaries produced by children (leaving out data from 
the child under test).  Without any adaptation the error 
rate is 47.1%.  However, utilizing adapted models from 
the read stories (see Table 1(G)) provides an initial per-
formance gain of nearly 10% absolute.  Further use 
SMAPLR adaptation reduces the error rate to 36.1%. 
 Word Error Rate (%) Experimental Configuration MFCC PMVDR 
(A) Baseline / no adaptation 47.0% 47.1% 
(B) Read-aloud adapted models 
(VTLN/SAT) 37.2% 38.0% 
(C) (B) + SMAPLR  
adaptation iteration #1 36.0% 36.6% 
(D) (C) + SMAPLR 
adaptation iteration #2 35.1% 36.1% 
Table 2:  Recognition of spontaneous story summaries 
7 Practical Real-Time Implementation 
The research systems described in Sect. 5 and 6 do not 
operate in real-time since multiple adaptation passes 
over the data are required.  To address this issue, we 
have implemented a real-time system that operates on 
small pipelined audio segments (250ms on average).  
When evaluated on the read-aloud task (Sect. 5), the 
initial baseline system achieves an error rate of 19.5%.  
This system has a real-time factor of 0.56 on a 2.4 GHz 
Intel Pentium 4 PC with 512MB of RAM. When inte-
grated, the proposed methods show the error rate can be 
reduced from 19.5% to 12.7% (compare with 10.7% 
error research system in Table 1(D)).  The revised sys-
tem which incorporates dynamic language modeling 
operates 35% faster than the single language model 
method while also reducing the variance in real-time 
factor for each processed chunk of audio.  Further gains 
are possible by incorporating adaptation in an incre-
mental manner.  For example, in Table 3(C) a real-time 
system that incorporates incremental unsupervised 
maximum likelihood linear regression (MLLR) adapta-
tion of the Gaussian means is shown.  This final real-
time system simultaneously adapts both language and 
acoustic model parameters during system use. The sys-
tem is now being refined for deployment in classrooms 
within the CLT project. We were able to further im-
prove the system after the submission deadline. The 
current WER on the story read aloud task improved to 
7.6%; while a WER of 32.2% was achieved on the 
summary recognition task. The improvements are due to 
the inclusion of a breath model and the additional use of 
audio data from 103 second graders for more accurate 
acoustic modeling. 
 
PMVDR Front-End System Description WER (%) RTF 
(A) Baseline: single LM 19.5% 0.56  (  2=0.11) 
(B) Proposed System 12.7% 0.36 (  2=0.06) 
(C) (B) + Incremental 
MLLR adaptation 11.5% 
0.80 
(  2=0.33) 
Table 3:  Evaluation of real-time read out-loud system. 
References 
 
V. Zue, S. Seneff, J. Polifroni, H. Meng, J. Glass 
(1996). ?Multilingual Human-Computer Interactions: 
From Information Acess to Language Learning,? 
ICSLP-96, Philadelphia, PA 
J. Mostow, S. Roth, A. G. Hauptmann, and M. Kane 
(1994). "A Prototype Reading Coach that Listens", 
AAAI-94, Seattle, WA, pp. 785-792.  
Y-C. Tam, J. Mostow, J. Beck, and S. Banerjee (2003). 
?Training a Confidence Measure for a Reading Tutor 
that Listens?. Eurospeech, Geneva, Switzerland, 
3161-3164. 
J. Mostow, J. Beck, S. Winter, S. Wang, and B. Tobin 
(2002). ?Predicting oral reading miscues? ICSLP-02, 
Denver, Colorado. 
J. Mostow, G. Aist, P. Burkhead, A. Corbett, A. Cuneo, 
S. Eitelman, C. Huang, B. Junker, M. B. Sklar, and 
B. Tobin (2003). ?Evaluation of an automated Read-
ing Tutor that listens:  Comparison to human tutoring 
and classroom instruction?. Journal of Educational 
Computing Research, 29(1), 61-117 
R. Cole, S. van Vuuren, B. Pellom, K. Hacioglu, J. Ma, 
J. Movellan, S. Schwartz, D. Wade-Stein, W. Ward, 
J. Yan (2003). ?Perceptive Animated Interfaces: First 
Steps Toward a New Paradigm for Human Computer 
Interaction,? Proceedings of the IEEE, Vol. 91, No. 
9, pp. 1391-1405 
A. Hagen, B. Pellom, and R. Cole (2003). "Children?s 
Speech Recognition with Application to Interactive 
Books and Tutors", ASRU-2003, St. Thomas, USA 
B. Pellom (2001). "SONIC: The University of Colorado 
Continuous Speech Recognizer", Technical Report 
TR-CSLR-2001-01, University of Colorado. 
B. Pellom and K. Hacioglu (2003). "Recent Improve-
ments in the CU Sonic ASR System for Noisy 
Speech: The SPINE Task", ICASSP-2003, Hong 
Kong, China. 
U. Yapanel, J. H.L. Hansen (2003). "A New Perspective    
on Feature Extraction for Robust In-vehicle Speech  
Recognition" Eurospeech, Geneva, Switzerland. 
K. Shobaki, J.-P. Hosom, and R. Cole (2000). "The OGI 
Kids' Speech Corpus and Recognizers", Proc. 
ICSLP-2000, Beijing, China. 
L. Welling, S. Kanthak, and H. Ney. (1999) "Improved 
Methods for Vocal Tract Length Normalization", 
ICASSP, Phoenix, Arizona. 
M. Gales (1997). Maximum Likelihood Linear Trans-
formations for HMM-Based Speech Recognition", 
Tech. Report, CUED/F-INFENG/TR291, Cambridge 
University. 
O. Siohan, T. Myrvoll, and C.-H. Lee (2002) "Structural 
Maximum a Posteriori Linear Regression for Fast 
HMM Adaptation", Computer, Speech and Lan-
guage, 16, pp. 5-24.  
