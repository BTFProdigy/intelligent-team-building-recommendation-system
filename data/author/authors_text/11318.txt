Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 80?88,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Retrieval of Reading Materials for Vocabulary and Reading Practice
Michael Heilman, Le Zhao, Juan Pino and Maxine Eskenazi
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{mheilman,lezhao,jmpino,max}@cs.cmu.edu
Abstract
Finding appropriate, authentic reading mate-
rials is a challenge for language instructors.
The Web is a vast resource of texts, but most
pages are not suitable for reading practice, and
commercial search engines are not well suited
to finding texts that satisfy pedagogical con-
straints such as reading level, length, text qual-
ity, and presence of target vocabulary. We
present a system that uses various language
technologies to facilitate the retrieval and pre-
sentation of authentic reading materials gath-
ered from the Web. It is currently deployed in
two English as a Second Language courses at
the University of Pittsburgh.
1 Introduction
Reading practice is an important component of first
and second language learning, especially with re-
gards to vocabulary learning (Hafiz and Tudor,
1989). Appropriating suitable reading material for
the needs of a particular curriculum or particular stu-
dent, however, is a challenging process. Manually
authoring or editing readings is time-consuming and
raises issues of authenticity, which are particularly
significant in second language learning (Peacock,
1997). On the other hand, the Web is a vast resource
of authentic reading material, but commercial search
engines which are designed for a wide variety of in-
formation needs may not effectively facilitate the re-
trieval of appropriate readings for language learners.
In order to demonstrate the problem of finding ap-
propriate reading materials, here is a typical exam-
ple of an information need from a teacher of an En-
glish as a Second Language (ESL) course focused
on reading skills. This example was encountered
during the development of the system. It should
be noted that while we describe the system in the
context of ESL, we claim that the approach is gen-
eral enough to be applied to first language reading
practice and to languages other than English. To
fit within his existing curriculum, the ESL teacher
wanted to find texts on the specific topic of ?interna-
tional travel.? He sought texts that contained at least
a few words from the list of target vocabulary that
his student were learning that week. In addition, he
needed the texts to be within a particular range of
reading difficulty, fifth to eighth grade in an Ameri-
can school, and shorter than a thousand words.
Sending the query ?international travel? to a pop-
ular search engine did not produce a useful list of re-
sults1. The first result was a travel warning from the
Department of State2, which was at a high reading
level (grade 10 according to the approach described
by (Heilman et al, 2008)) and not likely to be of
interest to ESL students because of legal and techni-
cal details. Most of the subsequent results were for
commercial web sites and travel agencies. A query
for a subset of the target vocabulary words for the
course also produced poor results. Since the search
engine used strict boolean retrieval methods, the top
results for the query ?deduce deviate hierarchy im-
plicit undertake? were all long lists of ESL vocabu-
lary words3.
We describe a search system, called REAP
Search, that is tailored to the needs of language
1www.google.com, March 5, 2008
2http://travel.state.gov/travel/cis pa tw/cis pa tw 1168.html
3e.g., www.espindle.org/university word list uwl.html
80
teachers and learners. The system facilitates the re-
trieval of texts satisfying particular pedagogical con-
straints such as reading level and text length, and al-
lows the user to constrain results so that they con-
tain at least some, but not necessarily all, of the
words from a user-specified target vocabulary list.
It also filters out inappropriate material as well as
pages that do not contain significant amounts of text
in well-formed sentences. The system provides sup-
port for learners including an interface for reading
texts, easy access to dictionary definitions, and vo-
cabulary exercises for practice and review.
The educational application employs multiple
language technologies to achieve its various goals.
Information retrieval and web search technologies
provide the core components. Automated text clas-
sifiers organize potential readings by general topic
area and reading difficulty. We are also developing
an approach to measuring reading difficulty that uses
a parser to extract grammatical structures. Part of
Speech (POS) tagging is used to filter web pages to
maintain text quality.
2 Path of a Reading
In the REAP Search system, reading materials take a
path from the Web to students through various inter-
mediate steps as depicted in Figure 1. First, a crawl-
ing program issues queries to large-scale commer-
cial search engines to retrieve candidate documents.
These documents are annotated, filtered, and stored
in a digital library, or corpus. This digital library cre-
ation process is done offline. A customized search
interface facilitates the retrieval of useful reading
materials by teachers, who have particular curricu-
lar goals and constraints as part of their information
needs. The teachers organize their selected readings
through a curriculum manager. The reading inter-
face for students accesses the curriculum manager?s
database and provides the texts along with support
in the form of dictionary definitions and practice ex-
ercises.
3 Creating a Digital Library of Readings
The foundation of the system is a digital library of
potential reading material. The customized search
component does not search the Web directly, but
rather accesses this filtered and annotated database
of Web pages. The current library consists of ap-
proximately five million documents. Construction
of the digital library begins with a set of target vo-
cabulary words that might be covered by a course or
set of courses (typically 100-1,500 words), and a set
of constraints on text characteristics. The constraints
can be divided into three sets: those that can be ex-
pressed in a search engine query (e.g., target words,
number of target words per text, date, Web domain),
those that can be applied using just information in
the Web search result list (e.g., document size), and
those that require local annotation and filtering (e.g.,
reading level, text quality, profanity).
The system obtains candidate documents by
query-based crawling, as opposed to following
chains of links. The query-based document crawl-
ing approach is designed to download documents
for particular target words. Queries are submitted
to a commercial Web search engine4, result links are
downloaded, and then the corresponding documents
are downloaded. A commercial web search engine
is used to avoid the cost of maintaining a massive,
overly general web corpus.
Queries consist of combinations of multiple tar-
get words. The system generates 30 queries for each
target word (30 is a manageable and sufficient num-
ber in practice). These are spread across 2-, 3-,
and 4-word combinations with other target words.
Queries to search engines can often specify a date
range. We employ ranges to find more recent mate-
rial, which students prefer. The tasks of submitting
queries, downloading the result pages, and extract-
ing document links are distributed among a dozen
or so clients running on desktop machines, to run as
background tasks. The clients periodically upload
their results to a server, and request a new batch of
queries.
Once the server has a list of candidate pages, it
downloads them and applies various filters. The fi-
nal yield of texts is typically approximately one per-
cent of the originally downloaded results. Many web
pages are too long, contain too little well-formed
text, or are far above the appropriate reading level
for language learners. After downloading docu-
ments, the system annotates them as described in
the next section. It then stores the pages in a full-
4www.altavista.com
81
Figure 1: Path of Reading Materials from the Web to a Student.
text search engine called Indri, which is part of
the Lemur Toolkit5. This index provides a consis-
tent and efficient interface to the documents. Using
Lemur and the Indri Query Language allows for the
retrieval of annotated documents according to user-
specified constraints.
4 Annotations and Filters
Annotators automatically tag the documents in the
corpus to enable the filtering and retrieval of read-
ing material that matches user-specified pedagogical
constraints. Annotations include reading difficulty,
general topic area, text quality, and text length. Text
length is simply the number of word tokens appear-
ing in the document.
4.1 Reading Level
The system employs a language modeling ap-
proach developed by Collins-Thompson and Callan
(Collins-Thompson and Callan, 2005) that creates a
model of the lexicon for each grade level and pre-
dicts reading level, or readability, of given docu-
ments according to those models. The readabil-
ity predictor is a specialized Naive Bayes classi-
fier with lexical unigram features. For web docu-
ments in particular, Collins-Thompson and Callan
report that this language modeling-based prediction
has a stronger correlation with human-assigned lev-
els than other commonly used readability measures.
This automatic readability measure allows the sys-
tem to satisfy user-specified constraints on reading
difficulty.
We are also experimenting with using syntac-
tic features to predict reading difficulty. Heilman,
Collins-Thompson, and Eskenazi (Heilman et al,
2008) describe an approach that combines predic-
tions based on lexical and grammatical features. The
5www.lemurproject.org
grammatical features are frequencies of occurrence
of grammatical constructions, which are computed
from automatic parses of input texts. Using multiple
measures of reading difficulty that focus on different
aspects of language may allow users more freedom
to find texts that match their needs. For example,
a teacher may want to find grammatically simpler
texts for use in a lesson focused on introducing dif-
ficult vocabulary.
4.2 General Topic Area
A set of binary topic classifiers automatically clas-
sifies each potential reading by its general topic, as
described by Heilman, Juffs, and Eskenazi (2007).
This component allows users to search for readings
on their general interests without specifying a par-
ticular query (e.g., ?international travel?) that might
unnecessarily constrain the results to a very narrow
topic.
A Linear Support Vector Machine text classifier
(Joachims, 1999) was trained on Web pages from
the Open Directory Project (ODP)6. These pages ef-
fectively have human-assigned topic labels because
they are organized into a multi-level hierarchy of
topics. The following general topics were manually
selected from categories in the ODP: Movies and
Theater; Music; Visual Arts; Computers and Tech-
nology; Business; Math, Physics and Chemistry; Bi-
ology and Environment; Social Sciences; Health and
Medicine; Fitness and Nutrition; Religion; Politics;
Law and Crime; History; American Sports; and Out-
door Recreation.
Web pages from the ODP were used as gold-
standard labels in the training data for the classi-
fiers. SVM-Light (Joachims, 1999) was used as an
implementation of the Support Vector Machines. In
preliminary tests, the linear kernel produced slightly
6dmoz.org
82
better performance than a radial basis function ker-
nel. The values of the decision functions of the clas-
sifiers for each topic are used to annotate readings
with their likely topics.
The binary classifiers for each topic category were
evaluated according to the F1 measure, the harmonic
mean of precision and recall, using leave-one-out
cross-validation. Values for the F1 statistic range
from .68 to .86, with a mean value of .76 across
topics. For comparison, random guessing would be
expected to correctly choose the gold-standard label
only ten percent of the time. During an error analy-
sis, we observed that many of the erroneous classifi-
cations were, in fact, plausible for a human to make
as well. Many readings span multiple topics. For
example, a document on a hospital merger might be
classified as ?Health and Medicine? when the cor-
rect label is ?Business.? In the evaluation, the gold
standard included only the single topic specified by
the ODP. The final system, however, assigns multi-
ple topic labels when appropriate.
4.3 Text Quality
A major challenge of using Web documents for ed-
ucational applications is that many web pages con-
tain little or no text in well-formed sentences and
paragraphs. We refer to this problem as ?Text Qual-
ity.? Many pages consist of lists of links, navigation
menus, multimedia, tables of numerical data, etc. A
special annotation tool filters out such pages so that
they do not clutter up search results and make it dif-
ficult for users to find suitable reading materials.
The text quality filter estimates the proportion of
the word tokens in a page that are contained in well-
formed sentences. To do this it parses the Document
Object Model structure of the web page, and orga-
nizes it into text units delineated by the markup tags
in the document. Each new paragraph, table ele-
ment, span, or divider markup tag corresponds to the
beginning of a new text unit. The system then runs
a POS tagger7 over each text unit. We have found
that a simple check for whether the text unit con-
tains both a noun and a verb can effectively distin-
guish between content text units and those text units
that are just part of links, menus, etc. The proportion
7The OpenNLP toolkit?s tagger was used
(opennlp.sourceforge.net).
of the total tokens that are part of content text units
serves as a useful measure of text quality. We have
found that a threshold of about 85% content text is
appropriate, since most web pages contain at least
some non-content text in links, menus, etc. This ap-
proach to content extraction is related to previous
work on increasing the accessibility of web pages
(Gupta et al, 2003).
5 Constructing Queries
Users search for readings in the annotated corpus
through a simple interface that appears similar to,
but extends the functionality of, the interfaces for
commercial web search engines. Figure 2 shows
a screenshot of the interface. Users have the op-
tion to specify ad hoc queries in a text field. They
can also use drop down menus to specify optional
minimum and/or maximum reading levels and text
lengths. Another optional drop-down menu allows
users to constrain the general topic area of results. A
separate screen allows users to specify a list of tar-
get vocabulary words, some but not all of which are
required to appear in the search results. For ease of
use, the target word list is stored for an entire session
(i.e., until the web browser application is closed)
rather than specified with each query. After the user
submits a query, the system displays multiple results
per screen with titles and snippets.
5.1 Ranked versus Boolean Retrieval
In a standard boolean retrieval model, with AND as
the default operator, the results list consists of doc-
uments that contain all query terms. In conjunc-
tion with relevance ranking techniques, commercial
search engines typically use this model, a great ad-
vantage of which is speed. Boolean retrieval can en-
counter problems when queries have many terms be-
cause every one of the terms must appear in a doc-
ument for it to be selected. In such cases, few or
no satisfactory results may be retrieved. This issue
is relevant because a teacher might want to search
for texts that contain some, but not necessarily all,
of a list of target vocabulary words. For example,
a teacher might have a list of ten words, and any
text with five of those words would be useful to give
as vocabulary and reading practice. In such cases,
ranked retrieval models are more appropriate be-
83
Figure 2: Screenshot of Search Interface for Finding Appropriate Readings.
cause they do not require that all of the query terms
appear. Instead, these models prefer multiple occur-
rences of different word types as opposed to multiple
occurrences of the same word tokens, allowing them
to rank documents with more distinct query terms
higher than those with distinct query terms. Docu-
ments that contain only some of the query terms are
thus assigned nonzero weights, allowing the user to
find useful texts that contain only some of the target
vocabulary. The REAP search system uses the Indri
Query Language?s ?combine? and ?weight? opera-
tors to implement a ranked retrieval model for target
vocabulary. For more information on text retrieval
models, see (Manning et al, 2008).
5.2 Example Query
Figure 3 shows an example of a structured query
produced by the system from a teacher?s original
query and constraints. This example was slightly
altered from its original form for clarity of presen-
tation. The first line with the filrej operator filters
and rejects any documents that contain any of a long
list of words considered to be profanity, which are
omitted in the illustration for brevity and posterity.
The filreq operator in line 2 requires that all of the
constraints on reading level, text length and quality
in lines 2-4 are met. The weight operator at the start
of line 5 balances between the ad hoc query terms in
line 5 and the user-specific target vocabulary terms
in lines 6-8. The uw10 operator on line 5 tells the
system to prefer texts where the query terms appear
together in an unordered window of size 10. Such
proximity operators cause search engines to prefer
documents in which query terms appear near each
other. The implicit assumption is that the terms in
queries such as ?coal miners safety? are more likely
to appear in the same sentence or paragraph in rele-
vant documents than irrelevant ones, even if they do
not appear consecutively. Importantly, query terms
are separated from target words because there are
usually a much greater number of target words, and
thus combining the two sets would often result in
the query terms being ignored. The higher weight
assigned to the set of target words ensures they are
not ignored.
6 Learner and Teacher Support
In addition to search facilities, the system provides
extensive support for students to read and learn from
texts as well as support for teachers to track stu-
dents? progress. All interfaces are web-based for
easy access and portability. Teachers use the search
system to find readings, which are stored in a cur-
riculum manager that allows them to organize their
selected texts. The manager interface allows teach-
ers to perform tasks such as specifying the order
of presentation of their selected readings, choosing
target words to be highlighted in the texts to focus
learner attention, and specifying time limits for each
text.
The list of available readings are shown to stu-
dents when they log in during class time or for
homework. Students select a text to read and move
on to the reading interface, which is illustrated in
Figure 4. The chosen web page is displayed in its
original format except that the original hyperlinks
and pop-ups are disabled. Target words that were
84
Figure 3: Example Structured Query. The line numbers on the left are for reference only.
chosen by the teacher are highlighted and linked to
definitions. Students may also click on any other
unknown words to access definitions. The dictio-
nary definitions are provided from the Cambridge
Advanced Learner?s Dictionary8, which is authored
specifically for ESL learners. All dictionary access
is logged, and teachers can easily see which words
students look up.
The system also provides vocabulary exercises af-
ter each reading for additional practice and review
of target words. Currently, students complete cloze,
or fill-in-the-blank, exercises for each target word in
the readings. Other types of exercises are certainly
possible. For extra review, students also complete
exercises for target words from previous readings.
Students receive immediate feedback on the prac-
tice and review exercises. Currently, sets of the ex-
ercises are manually authored for each target word
and stored in a database, but we are exploring auto-
mated question generation techniques (Brown et al,
2005; Liu et al, 2005). At runtime, the system se-
lects practice and review exercises from this reposi-
tory.
7 Related Work
A number of recent projects have taken similar ap-
proaches to providing authentic texts for language
learners. WERTi (Amaral et al, 2006) is an in-
telligent automatic workbook that uses texts from
the Web to increase knowledge of English gram-
matical forms and functions. READ-X (Miltsakaki
and Troutt, 2007) is a tool for finding texts at spec-
ified reading levels. SourceFinder (Sheehan et al,
2007) is an authoring tool for finding suitable texts
for standardized test items on verbal reasoning and
8dictionary.cambridge.org
reading comprehension.
The REAP Tutor (Brown and Eskenazi, 2004;
Heilman et al, 2006) for ESL vocabulary takes a
slightly different approach. Rather than teachers
choosing texts as in the REAP Search system, the
REAP Tutor itself selects individualized practice
readings from a digital library. The readings contain
target vocabulary words that a given student needs
to learn based on a student model. While the in-
dividualized REAP Tutor has the potential to better
match the needs of each student since each student
can work with different texts, a drawback of its ap-
proach is that instructors may have difficulty coor-
dinating group discussion about readings and inte-
grating the Tutor into their curriculum. In the REAP
Search system, however, teachers can find texts that
match the needs and interests of the class as a whole.
While some degree of individualization is lost, the
advantages of better coordinated support from teach-
ers and classroom integration are gained.
8 Pilot Study
8.1 Description
Two teachers and over fifty students in two ESL
courses at the University of Pittsburgh used the sys-
tem as part of a pilot study in the Spring of 2008.
The courses focus on developing the reading skills
of high-intermediate ESL learners. The target vo-
cabulary words covered in the courses come from
the Academic Word List (Coxhead, 2000), a list
of broad-coverage, general purpose English words
that frequently appear in academic writing. Students
used the system once per week in a fifty-minute class
for eight weeks. For approximately half of a ses-
sion, students read the teacher-selected readings and
worked through individualized practice exercises.
85
Figure 4: Screenshot of Student Interface Displaying a Reading and Dictionary Definition.
For the other half of each session, the teacher pro-
vided direct instruction on and facilitated discussion
about the texts and target words, making connec-
tions to the rest of the curriculum when possible.
For each session, the teachers found three to five
readings. Students read through at least two of the
readings, which were discussed in class. The extra
readings allowed faster readers to progress at their
own pace if they complete the first two. Teachers
learned to use the system in a training session that
lasted about 30 minutes.
8.2 Usage Analysis
To better understand the two teachers? interactions
with the search system, we analyzed query log data
from a four week period. In total, the teachers used
the system to select 23 readings for their students.
In the process, they issued 47 unique queries to the
system. Thus, on average they issued 2.04 queries
per chosen text. Ideally, a user would only have to
issue a single query to find useful texts, but from
the teachers? comments it appears that the system?s
usability is sufficiently good in general. Most of
the time, they specified 20 target words, only some
of which appeared in their selected readings. The
teachers included ad hoc queries only some of the
time. These were informational in nature and ad-
dressed a variety of topics. Example queries in-
clude the following: ?surviving winter?, ?coal min-
ers safety?, ?gender roles?, and ?unidentified flying
objects?. The teachers chose these topics because
they matched up with topics discussed in other parts
of their courses? curricula. In other cases, it was
more important for them to search for texts with tar-
get vocabulary rather than those on specific topics,
so they only specified target words and pedagogical
constraints.
8.3 Post-test and Survey Results
At the end of the semester, students took an exit sur-
vey followed by a post-test consisting of cloze vo-
cabulary questions for the target words they prac-
ticed with the system. In previous semesters, the
REAP Tutor has been used in one of the two courses
that were part of the pilot study. For comparison
with those results, we focus our analysis on the sub-
set of data for the 20 students in that course. The
exit survey results, shown in 5, indicate that stu-
dents felt it was easy-to-use and should be used in
future classes. These survey results are actually very
similar to previous results from a Spring 2006 study
with the REAP Tutor (Heilman et al, 2006). How-
ever, responses to the prompt ?My teacher helped
me to learn by discussing the readings after I read
86
Figure 5: The results from the pilot study exit survey, which used a Likert response format from 1-5 with 1=Strongly
Disagree, 3=Neither Agree nor Disagree, and 5=Strongly Agree. Error bars indicate standard deviations.
them? suggest that the tight integration of an edu-
cational system with other classroom activities, in-
cluding teacher-led discussions, can be beneficial.
Learning of target words was directly measured
by the post-test. On average, students answered
89% of cloze exercises correctly, compared to less
than 50% in previous studies with the REAP Tutor.
A direct comparison to those studies is challenging
since the system in this study provided instruction
on words that students were also studying as part of
their regular coursework, whereas systems in previ-
ous studies did not.
9 Discussion and Future Work
We have described a system that enables teachers
to find appropriate, authentic texts from the Web
for vocabulary and reading practice. A variety of
language technologies ranging from text retrieval to
POS tagging perform essential functions in the sys-
tem. The system has been used in two courses by
over fifty ESL students.
A number of questions remain. Can language
learners effectively and efficiently use such a system
to search for reading materials directly, rather than
reading what a teacher selects? Students could use
the system, but a more polished user interface and
further progress on filtering out readings of low text
quality is necessary. Is such an approach adaptable
to other languages, especially less commonly taught
languages for which there are fewer available Web
pages? Certainly there are sufficient resources avail-
able on the Web in commonly taught languages such
as French or Japanese, but extending to other lan-
guages with fewer resources might be significantly
more challenging. How effective would such a tool
be in a first language classroom? Such an approach
should be suitable for use in first language class-
rooms, especially by teachers who need to find sup-
plemental materials for struggling readers. Are there
enough high-quality, low-reading level texts for very
young readers? From observations made while de-
veloping REAP, the proportion of Web pages below
fourth grade reading level is small. Finding appro-
priate materials for beginning readers is a challenge
that the REAP developers are actively addressing.
Issues of speed and scale are also important to
consider. Complex queries such as the one shown
in Figure 3 are not as efficient as boolean queries.
The current system takes a few seconds to return re-
sults from its database of several million readings.
Scaling up to a much larger digital library may re-
quire sophisticated distributed processing of queries
across multiple disks or multiple servers. However,
we maintain that this is an effective approach for
providing texts within a particular grade level range
or known target word list.
Acknowledgments
This research was supported in part by the Insti-
tute of Education Sciences, U.S. Department of Ed-
ucation, through Grant R305B040063 to Carnegie
Mellon University; Dept. of Education grant
R305G03123; the Pittsburgh Science of Learning
Center which is funded by the National Science
Foundation, award number SBE-0354420; and a Na-
tional Science Foundation Graduate Research Fel-
lowship awarded to the first author. Any opin-
ions, findings, conclusions, or recommendations ex-
pressed in this material are the authors, and do not
necessarily reflect those of the sponsors.
References
Luiz Amaral, Vanessa Metcalf and Detmar Meurers.
87
2006. Language Awareness through Re-use of NLP
Technology. Pre-conference Workshop on NLP in
CALL ? Computational and Linguistic Challenges.
CALICO 2006.
Jon Brown and Maxine Eskenazi. 2004. Retrieval of
authentic documents for reader-specific lexical prac-
tice. Proceedings of InSTIL/ICALL Symposium 2004.
Venice, Italy.
Jon Brown, Gwen Frishkoff, and Maxine Eskenazi.
2005. Automatic question generation for vocabulary
assessment. Proceedings of HLT/EMNLP 2005. Van-
couver, B.C.
Kevyn Collins-Thompson and Jamie Callan. 2005.
Predicting reading difficulty with statistical language
models. Journal of the American Society for Informa-
tion Science and Technology, 56(13). pp. 1448-1462.
Averil Coxhead. 2000. A New Academic Word List.
TESOL Quarterly, 34(2). pp. 213-238.
S. Gupta, G. Kaiser, D. Neistadt, and P. Grimm. 2003.
DOM-based content extraction of HTML documents.
ACM Press, New York.
F. M. Hafiz and Ian Tudor. 1989. Extensive reading
and the development of language skills. ELT Journal
43(1):4-13. Oxford University Press.
Michael Heilman, Kevyn Collins-Thompson, Maxine Es-
kenazi. 2008. An Analysis of Statistical Models and
Features for Reading Difficulty Prediction. The 3rd
Workshop on Innovative Use of NLP for Building Edu-
cational Applications. Association for Computational
Linguistics.
Michael Heilman, Alan Juffs, Maxine Eskenazi. 2007.
Choosing Reading Passages for Vocabulary Learning
by Topic to Increase Intrinsic Motivation. Proceedings
of the 13th International Conferenced on Artificial In-
telligence in Education. Marina del Rey, CA.
Michael Heilman, Kevyn Collins-Thompson, Jamie
Callan, and Maxine Eskenazi. 2006. Classroom suc-
cess of an Intelligent Tutoring System for lexical prac-
tice and reading comprehension. Proceedings of the
Ninth International Conference on Spoken Language
Processing. Pittsburgh, PA.
Thorsten Joachims. 1999. Making large-Scale SVM
Learning Practical. Advances in Kernel Methods -
Support Vector Learning, B. Schlkopf and C. Burges
and A. Smola (ed.) MIT-Press.
Chao-Lin Liu, Chun-Hung Wang, Zhao-Ming Gao, and
Shang-Ming Huang. 2005. Applications of Lexical
Information for Algorithmically Composing Multiple-
Choice Cloze Items Proceedings of the Second
Workshop on Building Educational Applications Us-
ing NLP. Association for Computational Linguistics.
Christopher D. Manning, Prabhakar Raghavan and Hin-
rich Schtze. 2008. Introduction to Information Re-
trieval. Cambridge University Press. Draft available
at http://www-csli.stanford.edu/?hinrich/information-
retrieval-book.html.
Eleni Miltsakaki and Audrey Troutt. 2007. Read-X: Au-
tomatic Evaluation of Reading Difficulty of Web Text.
Proceedings of E-Learn 2007, sponsored by the Asso-
ciation for the Advancement of Computing in Educa-
tion. Quebec, Canada.
Matthew Peacock. 1997. The effect of authentic mate-
rials on the motivation of EFL learners. ELT Journal
51(2):144-156. Oxford University Press.
Kathleen M. Sheehan, Irene Kostin, Yoko Futagi. 2007.
SourceFinder: A Construct-Driven Approach for Lo-
cating Appropriately Targeted Reading Comprehen-
sion Source Texts. Proceedings of the SLaTE Work-
shop on Speech and Language Technology in Educa-
tion. Carnegie Mellon University and International
Speech Communication Association (ISCA).
88
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1291?1300,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Exploiting Syntactic and Distributional Information
for Spelling Correction with Web-Scale N-gram Models
Wei Xuc,?Joel Tetreaulta Martin Chodorowb Ralph Grishmanc Le Zhaod
aEducational Testing Service, Princeton, NJ, USA
jtetreault@ets.org
bHunter College of CUNY, New York, NY, USA
martin.chodorow@hunter.cuny.edu
cNew York University, NY, USA
{xuwei,grishman}@cs.nyu.edu
dCarnegie Mellon University, Pittsburgh, PA, USA
lezhao@cs.cmu.edu
Abstract
We propose a novel way of incorporating de-
pendency parse and word co-occurrence in-
formation into a state-of-the-art web-scale n-
gram model for spelling correction. The syn-
tactic and distributional information provides
extra evidence in addition to that provided by a
web-scale n-gram corpus and especially helps
with data sparsity problems. Experimental
results show that introducing syntactic fea-
tures into n-gram based models significantly
reduces errors by up to 12.4% over the current
state-of-the-art. The word co-occurrence in-
formation shows potential but only improves
overall accuracy slightly.
1 Introduction
The function of context-sensitive text correction is
to identify word-choice errors in text (Bergsma et
al., 2009). It can be viewed as a lexical disambigua-
tion task (Lapata and Keller, 2005), where a system
selects from a predefined confusion word set, such
as {affect, effect} or {complement, compliment},
and provides the most appropriate word choice given
the context. Typically, one determines if a word has
been used correctly based on lexical, syntactic and
semantic information from the context of the word.
One of the top performing models of spelling cor-
rection (Bergsma et al, 2010) is based on web-scale
n-gram counts, which reflect both syntax and mean-
ing. However, even with a large-scale n-gram cor-
pus, data sparsity can hurt performance in two ways.
?This work was done when the first author was an intern
for Educational Testing Service.
First, n-gram based methods require exact word and
order matches. If there is a low frequency word in
the context, such as a person?s name, there will be
little, if any, evidence in the n-gram data to sup-
port the usage. Second, if the target confusable word
is rare, there will not be enough n-gram support or
training data to render a confident decision. Because
of the data sparsity problem, language modeling is
not always sufficient to capture the meaning of the
sentence and the correct usage of the word.
Take a sentence from The New York Times
(NYT) for example: ??This fellow?s won a war,? the
dean of the capital?s press corps, David Broder, an-
nounced on ?Meet the Press? after complimenting
the president on the ?great sense of authority and
command? he exhibited in a flight suit.? Unfortu-
nately, neither the phrase ?complementing the pres-
ident? nor ?complimenting the president? exists in
the web-scale Google N-gram corpus (Brants and
Franz, 2006). The n-gram models decide solely
based on the frequency of the bi-grams ?after com-
ple(i)menting? and ?comple(i)menting the?, which
are common usages for both words. The real ques-
tion is whether we are more likely to ?compliment?
or ?complement? a person, the ?president?. Several
clues could help us answer that question. A de-
pendency parser can identify the word ?president?
as the subject of ?compliment? or ?complement?
which also may be the case in some of the train-
ing data. Lexical co-occurrence (Edmonds, 1997)
and semantic word relatedness measurements, such
as Random Indexing (Sahlgren, 2006), could pro-
vide evidence that ?compliment? is more likely to
co-occur with ?president? than ?complement?. Fur-
1291
thermore, some important clues can be quite distant
from the target word, e.g. outside the 9-word context
window Bergsma et al (2010) and Carlson (2007)
used. Consider another sentence in the NYT corpus,
?GM says the addition of OnStar, which includes a
system that automatically notifies an OnStar opera-
tor if the vehicle is involved in a collision, comple-
ments the Vue?s top five-star safety rating for the
driver and front passenger in both front- and side-
impact crash tests.? The dependency parser finds the
object of ?complement? is ?rating?, which is outside
the 9-word window.
We propose enhancing state-of-the-art web-scale
n-gram models for spelling correction with syntac-
tic structures and distributional information. For our
work, we build on a baseline system that combines
n-gram and lexical features (Bergsma et al, 2010).
Specifically, this paper makes the following contri-
butions:
1. We show that the baseline system can be
improved by augmenting it with dependency
parse features.
2. We show that the impact of parse features can
be further augmented when combined with dis-
tributional information, specifically word co-
occurrence information.
In the following section, we describe related
work and how our approach differs from these ap-
proaches. In Sections 3 and 4, we discuss our meth-
ods for using parse features and word co-occurrence
information. In Section 5, we present experimental
results and analysis.
2 Related Work
A variety of approaches have been proposed for
context-sensitive spelling correction ranging from
semantic methods to machine learning classifiers to
large-scale n-gram models.
Some semantics-based systems have been devel-
oped based on an intuitive assumption that the in-
tended word is more likely to be semantically coher-
ent with the context than is a spelling error. Jones
and Martin (1997) made use of the semantic simi-
larity produced by Latent Semantic Analysis. Bu-
danitsky and Hirst (2001) investigated the effective-
ness of predicting words based on different semantic
similarity/distance measures in WordNet. Both sys-
tems report performance that is lower than systems
developed more recently.
A variety of machine-learning methods have been
proposed in spelling correction and preposition and
article error correction fields, such as Bayesian clas-
sifiers (Golding, 1995; Golding and Roth, 1996),
Winnow-based learning (Golding and Roth, 1999),
decision lists (Golding, 1995), transformation-based
learning (Mangu and Brill, 1997), augmented mix-
ture models (Cucerzan and Yarowsky, 2002) and
maximum entropy classifiers (Izumi et al, 2003;
Han et al, 2006; Chodorow et al, 2007; Tetreault
and Chodorow, 2008; Felice and Pulman, 2008).
Despite their differences, these approaches mainly
use contextual features to capture the lexical, seman-
tic and/or syntactic environment of the target word.
The use of distributional similarity measures for
spelling correction has been previously explored in
(Mohammad and Hist, 2006). In our work, distribu-
tional similarity is not the primary contribution but
we show the impact it can have when used in con-
junction with a large scale n-gram model and with
parse features, which allows the system to select
words outside the local window for distributional
similarity. In the prior work, the words for distri-
butional similarity are constrained to the local win-
dow, and positional information of the words is not
encoded.
Recent work (Carlson and Fette, 2007; Gamon
et al, 2008; Bergsma et al, 2009) has demon-
strated that large-scale language modeling is ex-
tremely helpful for contextual spelling correction
and other lexical disambiguation tasks. These sys-
tems make the word choice depending on how fre-
quently each candidate word has been seen in the
given context in web-scale data. As n-gram data has
become more readily available, such as the Google
N-gram Corpus, the likelihood of a word being used
in a certain context can be better estimated.
Bergsma et al (2009; 2010) presented a series
of simple but powerful models which relied heavily
on web-scale n-gram counts. From the Google Web
N-gram Corpus, they retrieve counts of n-grams of
different sizes (2-5) and positions that span the tar-
get word w0 within a window of 9 words. For
example, for the following sentence: ?The system
tried to decide {among, between} the two confus-
1292
able words.?, the method would extract the five 5-
gram patterns, shown below in Figure 2, where w0
can be either word in the confusion set {among, be-
tween} in this particular example. Similarly, there
are four 4-grams, three 3-grams, and two 2-grams,
in total, 14 n-grams for each of the words in the con-
fusion set.
system tried to decide w0
tried to decide w0 the
to decide w0 the two
decide w0 the two confusable
w0 the two confusable words
We briefly describe three of Bergsma et al?s
(2009; 2010) best systems below, which are reported
to achieve state-of-the-art accuracy (NG = n-gram;
LEX = lexical).
1. sumLM: For each candidate word, (Bergsma
et al, 2009) sum the log-counts of all 14 pat-
terns filled with the candidate, and choose the
candidate with the highest total.
2. NG: Bergsma et al (2009) exploit each can-
didate?s 14 log-counts of n-gram patterns as
features in a Support Vector Machine (SVM)
model.
3. NG+LEX: Bergsma et al (2010) augment the
NG model with lexical features (described in
detail in Section 3.1).
Bergsma et al (2009; 2010) restricted their exper-
iments to only five confusion sets where the reported
performance in (Golding and Roth, 1999) was below
90%: {among, between}, {amount, number}, {cite,
sight, site}, {peace, piece} and {raise, rise}. They
reported that the SVM model with NG features out-
performed its unsupervised version, sumLM. How-
ever, the limited confusion word sets they evaluated
may not comprehensively represent the word usage
errors that writers typically make. In this paper, we
test nine additional commonly confused word pairs
to expand the scope of the evaluation. These words
were selected based on their lower frequencies com-
pared to the five pairs in the above work (as shown
later in Table 2).
3 Enhanced N-gram Models with Parse
Features
To our knowledge, only (Elmi and Evans, 1998)
have used parsing for spell correction. They focus
on using a parser as a filter to discriminate between
possible real-world corrections where the part-of-
speech differs. In our work, we show that parse fea-
tures are effective when used directly in the classifi-
cation mode (as opposed to as a final filter) to select
the best correction regardless of whether or not the
part-of-speech of the choices differ.
Statistical parsers have also seen limited use in
the sister tasks of preposition and article error detec-
tion (Hermet et al, 2008; Lee and Knutsson, 2008;
Felice and Pulman, 2009; Tetreault et al, 2010)
and verb sense disambiguation (Dligach and Palmer,
2008). In those instances where parsers have been
used, they have mainly provided shallow analyses
or relations involving specific target words, such as
a preposition or verb. Unlike preposition errors,
spelling errors can occur in any word.
In this paper, we propose a novel way to incor-
porate the parse into spelling correction, applying
the parser to sentences filled by each candidate word
equivalently and extracting salient features. This
overcomes two problem in the existing methods: 1)
the parse trees of the same sentence filled by differ-
ent confusion words can be different. However, in
the test phase, we do not know which word should
be put in the sentences to create parse features for
test examples. Previous studies (Tetreault et al,
2010) failed to discuss this issue. 2) Some existing
work (Whitelaw et al, 2009; Rozovskaya and Roth,
2010) in the text correction field introduced artificial
errors into training data to adapt the system to bet-
ter handle ill-formed text. But this method will en-
counter serious data sparsity problems when facing
rare words.
3.1 Baseline System
We chose one of the leading spelling correction sys-
tems, (Bergsma et al, 2010), as our primary base-
line. As noted earlier, it is an SVM-based system
combining web-scale n-gram counts (NG) and con-
textual words (LEX) as features. To simplify the ex-
planation, throughout the paper, we will only con-
sider the situation with two confusion words. The
1293
problem with more than two words in pre-defined
confusion sets can be solved similarly by using a
one-vs.-all strategy. As we mentioned in Section 2,
NG features include log-counts of 3-to-5-gram pat-
terns for each candidate word with the given context.
LEX features can be broken down into three sub-
categories: 1) bag-of-words (words at all positions
in a 9-word window around the target word), 2) in-
dicators for the words preceding or following the tar-
get word, and 3) indicators for all n-grams and their
positions. For the sentence ?The system tried to de-
cide {among, between} the two confusable words.?,
examples of bag-of-word features would be ?tried?,
?two?, etc., the two positional bigrams would be
?decide? and ?the?, and examples of the n-gram fea-
tures would be right-trigram = ?among the two? and
left-4-gram = ?tried to decide between?.
3.2 Parse Features
The benefit of introducing dependency parse fea-
tures is that 1) parse features capture contextual in-
formation in a larger context window; 2) parse fea-
tures specify which words in the context are salient
to the usage of the target word while purely lexi-
cally based approaches treat all words in the context
equally. We use the Stanford dependency parser (de
Marneffe et al, 2006) to extract six relevant feature
classes.
Parse Features (PAR):
1. relation names (target word as head)
2. complement of the target word
3. combination of 1 and 2
4. relation names (target word as complement)
5. head of the target word
6. combination of 4 and 5
Each of these six classes of PAR features can
contain zero to many values, since the target word
can be involved in none to multiple grammatical
relations and features of different filler words are
merged together. The PAR features, like the LEX
features, are binary. In Table 1, we present the parse
features for an example sentence. The parse fea-
tures here are listed as string values, but are later
converted into binary numbers in the vectors for the
SVM model.
4 Distributional Word Co-occurrence
Though lexical and parse features are complemen-
tary to n-gram models, they are learned from a nor-
mal training corpus and may not have enough cov-
erage due to data sparsity. Take a sentence from the
NYT for example: ?An economist, he began his ca-
reer as a professor ? he is still called ?the professor,?
by friends as a compliment and by foes as an insult ?
and taught at Harvard and Stanford .? If the most in-
dicative word ?friends? does not appear or does not
appear enough times in the local context or depen-
dencies with ?compliment? as compared to ?com-
plement? in the training corpus, then the classifier
may be unable to make the correct selection.
It is impractical and computationally costly to en-
large the training corpus without limit to include
all possible language phenomena. A good compro-
mise is to use word co-occurrence information from
web-scale data. The other option is to make use of
high-order word co-occurrence, which is included in
many semantic word relatedness measures, such as
Latent Semantic Analysis (LSA) (Landauer et al,
1998; Deerwester et al, 1990) or Random Indexing,
both of which can be estimated from a moderate-size
corpus.
Our intuition is to choose the confusion word
which is most relevant to a given context. We define
the salient words in context as a set M=m1, m2, m3,
..., and the relevance between two words as a func-
tion Relevance(w1, w2), which can either be calcu-
lated fromword co-occurrence or Random Indexing.
The score of each candidate word c in the confusion
set given a context with meaningful words M is cal-
culated by the following formula:
Score(c) =
?
m?M
Relevance(c,m)
In this paper, we experiment with first-order word
co-occurrence and Random Indexing as relevance
measures. And we define salient contextual words
as heads or complements in the dependency rela-
tions with the target word. In this way, we use the
parse information to constrain the two distribution
models. Thus the word co-occurrence information
1294
Feature Name PAR Features (compliment) PAR Features (complement)
1. Head Relation Name ccomp appos
2. Head of Relation says collisions
3. Head Combination ccomp says appos collisions
4. Comp Relation Name nsubj dep
5. Comp of Relation addition rating
6. Comp Combination nsub addition dep rating
Table 1: Parse Feature Example for the sentence: ?GM says the addition of OnStar, which includes a system that
automatically notifies an OnStar operator if the vehicle is involved in a collision, complements the Vue?s top five-star
safety rating for the driver and front passenger in both front- and side-impact crash tests.?
considerably overlaps with some values of the PAR
features, but provides extra evidence from web-scale
data rather than a limited amount of training data.
4.1 First-order Word Co-occurrence
The relevance based on first-order word co-
occurrence is calculated from the Google Web 5-
gram Corpus in a fashion similar to how we dealt
with n-gram counts in the previous section. Given
two words, w1 and w2, we consider all 8 possible
patterns that appear in a local context (5-word win-
dow), where we use wildcard (*) to indicate any to-
ken:
w1 w2
w1 * w2
w1 * * w2
w1 * * * w2
w2 w1
w2 * w1
w2 * * w1
w2 * * * w1
The relevance is then calculated by summing the
logarithm of each of the 8 different counts. Finally,
we compare the score of each candidate word and
output the one with higher score.
4.2 Random Indexing
The relevance scores based on Random Indexing
are provided by a tool FRanI (Higgins, 2004) and
a model trained on the Touchstone Applied Science
Associates (TASA) corpus which contains 750k sen-
tences and covers diverse topics (from a diversity of
textbooks up to the college level). Take the sentence
at the beginning of this section for example, where
only the words ?a? and ?friends? are related to the
target word (either ?complement? or ?compliment?)
by either relevance measure. The relevance based
on Random Indexing for (complement, friends) is
0.08, (compliment, friends) is 0.19 and both (com-
pliment, a) and (complement, a) are 0 because ?a?
is in the stop word list. Meanwhile, the relevance
based on first order word co-occurrence for (com-
pliment, friends) is 7.39, (complement, friends) is
5.38, (compliment, a) is 13.25, and (complement, a)
is 13.42. The system with either kind of relevance
outputs ?compliment?.
4.3 System Combination
Since the numeric measurement of word co-
occurrence is not as specific as the PAR features and
less trustworthy, adding word co-occurrence infor-
mation as features into the classifier along with n-
gram counts, lexical and parse features will hurt the
overall performance. It is more practical to combine
the two approaches in the following fashion:
1. When the SVM classifier (using NG, LEX and
PAR features) has high confidence (over a cer-
tain threshold) in the output label, output that
label;
2. Otherwise, output the results of the word
relatedness/co-occurrence-based system.
5 Evaluation
We evaluate the effectiveness of syntactic and dis-
tributional information on spelling correction. The
performance of the system is measured by accu-
racy: the percentage of sentences in the test data
for which the system chooses the correct word. We
compare our results against two baselines: 1) MA-
JOR chooses the most frequent candidate from the
1295
confusion set in the training corpus, and 2) Bergsma
et al?s (2010) best systems, NG+LEX. We include
inflectional variants (?-ing?, ?-ed?, ?-s?, ?-ly?) of
confusion words in the evaluation, such as comple-
menting, complimenting in addition to complement,
compliment, because this better corresponds to the
range of errors that may be encountered in actual
use and thus increases the scope of the system as a
real world application. Also following Bergsma et
al. (2010), we use a linear SVM, more exactly, the
L2-regularized L2-loss dual SVM in LIBLINEAR
(Fan et al, 2008). Unlike Bergsma et al, who used
development data to optimize parameters, we always
use default parameters, since training data is limited
for many of the words we are dealing with.
5.1 Data
Following Bergsma et al (2009; 2010), the test
examples are extracted from The New York Times
(NYT) portion of Gigaword1, but constrained to a
9-month publication time frame from October 2005
to July 2006. Unlike Bergsma et al who use the
same source as training data for the lexical features,
our training data (for both lexical and parse features)
comes from larger and more diverse news sources.
We use the very large database from Sekine?s n-gram
search engine (Sekine, 2008) as training data, which
consists of 1.9B words of newspaper text spanning
89 years from NYT, BBC, WSJ, Xinhua, etc.
We evaluate our systems on 5 confusion sets from
Bergsma et al (2009; 2010) and 9 commonly con-
fused word pairs with moderate frequency in daily
usage (randomly selected from those listed in En-
glish educational resources2). Shown in Table 2,
these 9 sets of words appear much less frequently
than the words selected by Bergsma et al, even
given the fact that we are using a considerably large
training corpus.
For each confusable word pair, sentences that
contain either of the words are extracted to form
training and test data. The word that appears in the
original sentences of the news article is treated as
the gold standard. For frequently occurring confu-
sion word sets used by Bergsma et al, we extract
up to 10k examples for testing, and up to 100k ex-
1Available from the LDC as LDC2003T05
2Such as an English learning blog post at
http://elisaenglish.pixnet.net/blog/post/1335194
Word Confusion Set # in Training Corpus
adverse / averse 13.5k / 1.8k
advice / advise 62.k / 12.9k
allusion / illusion 1.0k / 5.4k
complement / compliment 6.8k / 3.1k
confidant / confident 2.4k / 63.6k
desert / dessert 24.7k / 3.7k
discreet / discrete 0.7k / 2.4k
elicit / illicit 1.9k / 10.0k
stationary / stationery 2.5k/2.3k
wander / wonder 3.3k / 39.5k
Table 2: Training Data Sizes for Common ESL Confused
Words
amples for training. For the 9 less frequent confu-
sion word sets, we extract all the unique examples
for training and testing from the above sources. The
spelling correction system is evaluated by measur-
ing its accuracy in comparison to the gold standard
in test data. The error rate is the complement of ac-
curacy.
Following Carlson et al (2007) and Bergsma
et al (2009; 2010), we obtain the n-gram counts
from the GoogleWeb 1T 5-gram Corpus (Brants and
Franz, 2006).
5.2 Experimental Results
We present the results for each set separately be-
cause each set may behave very differently, depend-
ing upon its frequency, part-of-speech, number of
senses and other differences between the words in
each confusion set. The overall accuracy across con-
fusion sets is also presented to show the effective-
ness of different approaches. The results are tested
for statistical significance using McNemar?s test of
correlated proportions. The performance differences
are marked as significant when p < 0.05.
5.2.1 Effectiveness of Parse Features
We exploit the n-gram counts (NG), lexical fea-
tures (LEX) of Bergsma et al (2010) and our own
parse features (PAR) in linear SVM models.
The first comparison is between the supervised
learning systems with LEX and LEX+PAR. As
shown in Table 3, by exploiting our unique parse
features, for the total 14 confusion sets, the accuracy
increases on 12 sets and decreases on 2 sets. Over-
all, the spelling correction accuracy improves an ab-
1296
solute 1.35% for our 9 confusion sets and 0.60% for
Bergsma et al?s 5 confusion sets.
The second comparison is to see how parse fea-
tures interact with n-gram count features in a su-
pervised classifier. The best system from (Bergsma
et al, 2010) is listed in the table as ?NG+LEX?.
As shown in Table 3, the parse features proved to
be beneficial when augmenting this baseline, except
for the decrease in accuracy on adverse, averse by
only 2 cases out of 368, and among, between by
2 cases out of 10227. For all other confusion sets,
parse features decrease the error rate by as much as
2.74% (absolute) and as much as 38.5% (relative).
Improvements are statistically significant on all con-
fusion sets together, although for each separate set,
improvements are significant on only 5 sets, in part
due to an insufficient number of test cases.
The reason that parse features are occasionally not
helpful is because they sometimes include an un-
common word in dependencies, which happens to
appear once with the wrong word but not with the
correct word in the training data; or they sometimes
include too common words, which bias the classifier
in favor of the more frequent word in the confusion
set. We also noticed that lexical features are not al-
ways helpful when added to n-gram count features,
even for in-domain applications (i.e., with training
data and test data coming from the same domain or
corpus), as marked by underlines. However, lexical
and parse features together show more significant
and constant improvement over n-gram count-based
models, as marked by ?.
Of the six systems, every system that uses parse
features gets the example correct in Section 1, ?com-
plementing the president?; LEX by itself also gets
the example correct, but NG and NG+LEX fail.
In summary, our system NG+LEX+PAR outper-
forms the state-of-the-art system NG+LEX. It re-
duces the error rate by 12.4% across our 9 confusion
sets and by 8.4% across Bergsma et al?s 5 confusion
sets. Both improvements are significant (p < 0.05)
by the McNemar test. In addition, while NG+LEX
is not always better than NG, NG+LEX+PAR is con-
sistently better than NG.
5.2.2 Impact of Word Co-occurrence
The LIBLINEAR tool does not provide probabil-
ity estimates for SVM models but Logistic Regres-
sion can. In this set of experiments, we train a Logis-
tic Regression model with NG+LEX+PAR features
and empirically set the confidence threshold at 0.6,
as described in Section 4, based on the performance
on two word pairs. In the combined system, when
the Logistic Regression model estimates a probabil-
ity higher than the threshold we output its results,
otherwise we output the result of the system based
on word co-occurrence.
Surprisingly, although Random Indexing takes
into account more information than first-order word
co-occurrence, it lowered overall performance sub-
stantially. Thus in Table 4, we only present results
of using first-order word co-occurrence rather than
Random Indexing. For all 12 confusion sets, distri-
butional word co-occurrence information improves
9 sets and hurts 5 sets. Overall, it reduces the er-
ror rate slightly by 0.2% for our 9 sets and 1.5% for
Bergsma et al?s sets.
We believe there are two reasons why Ran-
dom Indexing fared worse than first-order word
co-occurrence: 1) Random Indexing considers co-
occurrence on a document level, while our first-
order word co-occurrence is limited to a 5-word win-
dow context. The latter is more suitable to context-
sensitive spelling correction. 2) The model for Ran-
dom Indexing is trained on a relatively small size
corpus compared to the web-scale data we used to
get n-gram count features for the classifier and thus
is not able to introduce much new evidence besides
the information carried by NG+LEX+PAR features.
Reason 2) also suggests why first-order co-
occurrence helps on some occasions while not on
other occasions. Its impact is limited because the
word co-occurrence information overlaps with some
of the PAR feature values as mentioned earlier. It
improves some cases because it provides some new
evidence from web-scale data to the system based on
NG+LEX+PAR features. It introduces new errors
because it simply favors the word that co-occurred
more often regardless of other factors. Its impact is
also limited because it is only considered when clas-
sifiers with NG+LEX+PAR features are not confi-
dent.
1297
CONFUSION SET # TEST MAJOR LEX LEX+PAR NG NG+LEX NG+LEX+PAR (&)
9 commonly cited ESL confusion pairs
adverse / averse 368 85.87 97.01 96.74 91.03 97.55 97.01 (+22.2%) ?
allusion / illusion 535 76.64 91.22 91.40 91.40 92.52 93.08 (-7.5%) ?
complement / compliment 860 51.51 83.84 85.12 88.49 88.37 89.53 (-10.0%)
confidant / confident 2416 94.41 97.97 98.30 98.51 99.05 99.09 (-4.3%) ?
desert / dessert 2357 70.81 90.71 91.56 87.31 93.68 94.57 (-14.1%) ?*
discreet / discrete 219 79.45 84.48 85.84 85.84 90.41 91.32 (-9.5%) ?
elicit / illicit 563 53.46 82.77 95.56 97.51 97.51 98.22 (-28.6%)
stationary / stationery 182 62.64 87.36 92.31* 93.96 92.86 95.60 (-38.5%)
wander / wonder 6506 86.37 96.42 97.42* 97.56 98.23 98.48 (-13.9%) ?*
Total 13972 81.08 93.94 95.29* 94.82 96.56 96.99 (-12.4%) ?*
5 Original Bergsma pairs
# among / between 10227 57.46 91.89 91.86 88.34 93.60 93.58 (+3.1%) ?
# amount / number 7398 76.44 92.34 93.16* 93.03 93.42 94.08 (-10.1%) ?*
# cite / site 10185 95.71 99.42 99.53 99.16 99.52 99.63 (-22.4%)?
# peace / piece 7330 56.81 95.01 97.01* 95.55 96.74 97.46 (-22.2%)? *
# raise / rise 9464 55.98 96.12 96.64* 94.45 96.68 97.05 (-11.5%) ?
Total 44604 68.92 95.09 95.69* 94.07 96.09 96.42 (-8.4%) ?
Table 3: Spelling correction precision (%), impact of adding parse features
SVM trained on 1G words of news text, tested on 9-months of NYT data.
*: Improvement of (NG+)LEX+PAR vs. (NG+)LEX is statistically significant.
?: Improvement of NG+LEX+PAR vs. NG is statistically significant.
&: Relative increase or decrease of error rate compared to ?NG+LEX?
#: As in Bergsma et al (2009; 2010) no morphological variants of the words are used in evaluation
CONFUSION SET # TEST MAJOR CLASSIFIER COMBINED SYSTEM (&)
9 commonly cited ESL confusion pairs
adverse / averse 368 85.87 97.55 96.74 (+33.3%)
allusion / illusion 535 76.64 92.34 92.34 (- 0.0%)
complement / compliment 860 51.51 89.88 90.81 (-9.2%)
confidant / confident 2416 94.41 99.13 99.05 (+9.5%)
desert / dessert 2357 70.81 93.98 94.23 (-3.7%)
discreet / discrete 219 79.45 90.41 91.78 (-14.3%)
elicit / illicit 563 53.46 98.40 98.76 (-22.2%)
stationary / stationery 182 62.64 93.41 93.96 (-9.1%)
wander / wonder 6506 86.37 98.49 98.36 (+9.2%)
5 Original Bergsma pairs
# among / between 10227 57.46 92.73 92.73 (-0.1%)
# amount / number 7398 76.44 93.44 93.76 (-4.74%)
# cite / site 10185 95.71 99.49 99.47 (+3.8%)
# peace / piece 7330 56.81 96.19 96.38 (-5.0%)
# raise / rise 9464 55.98 96.66 96.59 (+2.2%)
Table 4: Spelling correction accuracy (%), impact of combining word co-occurrence
CLASSIFIER: Logistic Regression trained on 1G words of news text, tested on 9-months NYT data.
COMBINED SYSTEM: CLASSIFER plus system based on first-order word co-occurrence.
&: Relative increase or decrease in error rate compared to CLASSIFIER
#: As in Bergsma et al (2009; 2010), no morphological variants of the words are used in evaluation
1298
6 Conclusions
We propose a novel approach that uses parse
features and lexical features together to improve
the performance of web-scale n-gram models for
spelling correction. This method is especially adap-
tive when less training data are available, which is
the case for confusable words that are not very fre-
quently used. We also investigate the effectiveness
of incorporating web-scale word co-occurrence and
corpus-based semantic word relatedness (Random
Indexing).
For future work, we will investigate using seman-
tic information (e.g. WordNet) to extend n-gram
models. It will be interesting to see if the usage of
the word ?compliment? in ?complimenting the pres-
ident? can be estimated by considering similar us-
ages in the corpus, such as ?complimenting the stu-
dent? or by creating an n-gram database of synset
patterns. We will investigate extending, to other ap-
plications, this general methodology combining dis-
tributional, semantic and syntactic information with
language models.
Acknowledgments
We wish to thank Michael Flor of Educational
Testing Service for his TrendStream tool, which
provides fast access and easy manipulation of the
Google N-gram Corpus. We also thank Derrick Hig-
gins of Educational Testing Service for his Random
Indexing support. We also thank Satoshi Sekine of
New York University, Matthew Snover of City Uni-
versity of New York, and Jing Jiang of Singapore
Management University for their advice.
References
Shane Bergsma, Dekang Lin, and Randy Goebel. 2009.
Web-scale n-gram models for lexical disambiguation.
In IJCAI.
Shane Bergsma, Emily Pitler, and Dekang Lin. 2010.
Creating robust supervised classifiers via web-scale n-
gram data. In ACL.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
version 1. Available at http://www.ldc.upenn.edu.
Alexander Budanitsky and Graeme Hirst. 2001. Seman-
tic distance in wordnet: An experimental, application-
oriented evaluation of five measures. In ACL Work-
shop on WordNet and Other Lexical Resources.
Andrew Carlson and Ian Fette. 2007. Memory-based
context sensitive spelling correction at web scale. In
ICMLA.
Martin Chodorow, Joel Tetreault, and Na-Rae Han. 2007.
Detection of grammatical errors involving preposi-
tions. In Proceedings of the Fourth ACL-SIGSEM
Workshop on Prepositions, pages 25?30.
Silviu Cucerzan and David Yarowsky. 2002. Aug-
mented mixture models for lexical disambigua-tion. In
EMNLP.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC, Genoa, Italy.
Scott Deerwester, Susan Dumais, George Furmas,
Thomas Landauer, and Richar Harshman. 1990. In-
dexing by latent semantic analysis. The American So-
ciety for Information Science.
Dmitriy Dligach and Martha Palmer. 2008. Novel se-
mantic features for verb sense disambiguation. In
ACL.
Philip Edmonds. 1997. Choosing the word most typical
in context using a lexical co-occurrence network. In
EACL.
Mohammed Ali Elmi and Martha Evans. 1998. Spelling
correction using context. In COLING.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. Liblinear: A library
for large linear classification. Machine Learning Re-
search, 9(1871-1874).
Rachele De Felice and Stephen G. Pulman. 2008.
A classifier-based approach to preposition and deter-
miner error correction in L2 English. In Proceedings
of COLING, Manchester, UK.
Rachele De Felice and Stephen G. Pulman. 2009. Auto-
matic detection of preposition errors in learner writing.
CALICO Journal, 26(3).
Michael Gamon, Jianfeng Gao, Chris Brockett, Alex
Klementiev, William B. Dolan, Dmitriy Belenko, and
Lucy Vanderwende. 2008. Using contextual speller
techniques and language modeling for ESL error cor-
rection. In Proceedings of the International Joint Con-
ference on Natural Language Processing (IJCNLP),
pages 449?456, Hyderabad, India.
Andrew Golding and Dan Roth. 1996. Applying Win-
now to context-sensitive spelling correction. In Pro-
ceedings of the International Conference on Machine
Learning (ICML), pages 182?190.
Andrew Golding and Dan Roth. 1999. A winnow-based
approach to context-sensitive spelling correction. Ma-
chine Learning, 34(1-3):107?130.
Andrew Golding. 1995. A Bayesian hybrid method for
context sensitive spelling correction. In Proceedings
1299
of the Third Workshop on Very Large Corpora (WVLC-
3), pages 39?53.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineering,
12(2):115?129.
Matthieu Hermet, Alain De?silets, and Stan Szpakowicz.
2008. Using the web as a linguistic resource to au-
tomatically correct lexico-syntactic errors. In Pro-
ceedings of the Sixth International Conference on Lan-
guage Resources and Evaluation (LREC), pages 390?
396, Marrekech, Morocco.
Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thepchai
Supnithi, and Hitoshi Isahara. 2003. Automatic er-
ror detection in the Japanese learners? English spoken
data. In Companion Volume to the Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 145?148.
Michael Jones and James Martin. 1997. Contextual
spelling correction using latent semantic analysis. In
ANLC.
Thomas Landauer, Darrell Laham, and Peter Foltz. 1998.
Learning human-like knowledge by singular value de-
composition: A progress report. Advances in Neural
Information Processing Systems, 10:45?51.
Mirella Lapata and Frank Keller. 2005. Web-based mod-
els for natural language processing. ACM Transac-
tions on Speech and Language Processing, 21:1?31.
John Lee and Ola Knutsson. 2008. The role of pp attach-
ment in preposition generation. In CICLING.
Lidia Mangu and Eric Brill. 1997. Automatic rule acqui-
sition for spelling correction. In ICML.
Saif Mohammad and Graeme Hist. 2006. Distributional
measures of concept distance: A task-oriented evalua-
tion. In EMNLP.
Alla Rozovskaya and Dan Roth. 2010. Training
paradigms for correcting errors in grammar and usage.
In ACL.
Magnus Sahlgren. 2006. The Word-Space Model: us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis.
Joel Tetreault and Martin Chodorow. 2008. The ups and
downs of prepostion error detection in esl writing. In
COLING.
Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010. Using parse features for preposition selection
and error detection. In ACL.
Casey Whitelaw, Ben Hutchinson, Grace Y. Chung, and
Gerard Ellis. 2009. Using the web for language inde-
pendent spellchecking and autocorrection. In ACL.
1300
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 665?670,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Filling Knowledge Base Gaps for Distant Supervision
of Relation Extraction
Wei Xu+ Raphael Hoffmann? Le Zhao#,* Ralph Grishman+
+New York University, New York, NY, USA
{xuwei, grishman}@cs.nyu.edu
?University of Washington, Seattle, WA, USA
raphaelh@cs.washington.edu
#Google Inc., Mountain View, CA, USA
lezhao@google.com
Abstract
Distant supervision has attracted recent in-
terest for training information extraction
systems because it does not require any
human annotation but rather employs ex-
isting knowledge bases to heuristically la-
bel a training corpus. However, previous
work has failed to address the problem
of false negative training examples misla-
beled due to the incompleteness of knowl-
edge bases. To tackle this problem, we
propose a simple yet novel framework that
combines a passage retrieval model using
coarse features into a state-of-the-art rela-
tion extractor using multi-instance learn-
ing with fine features. We adapt the in-
formation retrieval technique of pseudo-
relevance feedback to expand knowledge
bases, assuming entity pairs in top-ranked
passages are more likely to express a rela-
tion. Our proposed technique significantly
improves the quality of distantly super-
vised relation extraction, boosting recall
from 47.7% to 61.2% with a consistently
high level of precision of around 93% in
the experiments.
1 Introduction
A recent approach for training information ex-
traction systems is distant supervision, which ex-
ploits existing knowledge bases instead of anno-
tated texts as the source of supervision (Craven
and Kumlien, 1999; Mintz et al, 2009; Nguyen
and Moschitti, 2011). To combat the noisy train-
ing data produced by heuristic labeling in distant
supervision, researchers (Bunescu and Mooney,
2007; Riedel et al, 2010; Hoffmann et al, 2011;
Surdeanu et al, 2012) exploited multi-instance
*This work was done while Le Zhao was at Carnegie
Mellon University.
learning models. Only a few studies have directly
examined the influence of the quality of the train-
ing data and attempted to enhance it (Sun et al,
2011; Wang et al, 2011; Takamatsu et al, 2012).
However, their methods are handicapped by the
built-in assumption that a sentence does not ex-
press a relation unless it mentions two entities
which participate in the relation in the knowledge
base, leading to false negatives.
aligned 
mentions 
true 
 mentions 5.5% 2.7% 1.7% false  negatives false  
positives 
Figure 1: Noisy training data in distant supervi-
sion
In reality, knowledge bases are often incom-
plete, giving rise to numerous false negatives in
the training data. We sampled 1834 sentences that
contain two entities in the New York Times 2006
corpus and manually evaluated whether they ex-
press any of a set of 50 common Freebase1 rela-
tions. As shown in Figure 1, of the 133 (7.3%)
sentences that truly express one of these relations,
only 32 (1.7%) are covered by Freebase, leaving
101 (5.5%) false negatives. Even for one of the
most complete relations in Freebase, Employee-of
(with more than 100,000 entity pairs), 6 out of 27
sentences with the pattern ?PERSON executive of
ORGANIZATION? contain a fact that is not in-
cluded in Freebase and are thus mislabeled as neg-
ative. These mislabelings dilute the discriminative
capability of useful features and confuse the mod-
els. In this paper, we will show how reducing this
source of noise can significantly improve the per-
formance of distant supervision. In fact, our sys-
tem corrects the relation labels of the above 6 sen-
tences before training the relation extractor.
1http://www.freebase.com
665
 
D ocuments  Knowledge 
Base  
Relation 
Extractor  
Passage  
Retriever  
? 
? 
? 
Pseudo - relevant 
Relation Instances  
? 
? 
? 
Figure 2: Overall system architecture: The system
(1) matches relation instances to sentences and (2)
learns a passage retrieval model to (3) provide rel-
evance feedback on sentences; Relevant sentences
(4) yield new relation instances which are added
to the knowledge base; Finally, instances are again
(5) matched to sentences to (6) create training data
for relation extraction.
Encouraged by the recent success of simple
methods for coreference resolution (Raghunathan
et al, 2010) and inspired by pseudo-relevance
feedback (Xu and Croft, 1996; Lavrenko and
Croft, 2001; Matveeva et al, 2006; Cao et al,
2008) in the field of information retrieval, which
expands or reformulates query terms based on
the highest ranked documents of an initial query,
we propose to increase the quality and quantity
of training data generated by distant supervision
for information extraction task using pseudo feed-
back. As shown in Figure 2, we expand an orig-
inal knowledge base with possibly missing rela-
tion instances with information from the highest
ranked sentences returned by a passage retrieval
model (Xu et al, 2011) trained on the same data.
We use coarse features for our passage retrieval
model to aggressively expand the knowledge base
for maximum recall; at the same time, we exploit
a multi-instance learning model with fine features
for relation extraction to handle the newly intro-
duced false positives and maintain high precision.
Similar to iterative bootstrapping tech-
niques (Yangarber, 2001), this mechanism uses
the outputs of the first trained model to expand
training data for the second model, but unlike
bootstrapping it does not require iteration and
avoids the problem of semantic drift. We further
note that iterative bootstrapping over a single
distant supervision system is difficult, because
state-of-the-art systems (Surdeanu et al, 2012;
Hoffmann et al, 2011; Riedel et al, 2010; Mintz
et al, 2009), detect only few false negatives in the
training data due to their high-precision low-recall
features, which were originally proposed by Mintz
et al (2009). We present a reliable and novel way
to address these issues and achieve significant
improvement over the MULTIR system (Hoff-
mann et al, 2011), increasing recall from 47.7%
to 61.2% at comparable precision. The key to this
success is the combination of two different views
as in co-training (Blum and Mitchell, 1998):
an information extraction technique with fine
features for high precision and an information
retrieval technique with coarse features for high
recall. Our work is developed in parallel with
Min et al (2013), who take a very different
approach by adding additional latent variables to
a multi-instance multi-label model (Surdeanu et
al., 2012) to solve this same problem.
2 System Details
In this section, we first introduce some formal no-
tations then describe in detail each component of
the proposed system in Figure 2.
2.1 Definitions
A relation instance is an expression r(e1, e2)
where r is a binary relation, and e1 and e2 are
two entities having such a relation, for example
CEO-of(Tim Cook, Apple). The knowledge-based
distant supervised learning problem takes as input
(1) ?, a training corpus, (2) E, a set of entities
mentioned in that corpus, (3) R, a set of relation
names, and (4) ?, a set of ground facts of relations
in R. To generate our training data, we further as-
sume (5) T , a set of entity types, as well as type
signature r(E1, E2) for relations.
We define the positive data set POS(r) to be
the set of sentences in which any related pair
of entities of relation r (according to the knowl-
edge base) is mentioned. The negative data set
RAW (r) is the rest of the training data, which
contain two entities of the required types in the
knowledge base, e.g. one person and one or-
ganization for the CEO-of relation in Freebase.
Another negative data set with more conservative
sense NEG(r) is defined as the set of sentences
which contain the primary entity e1 (e.g. person
in any CEO-of relation in the knowledge base) and
any secondary entity e2 of required type (e.g. or-
ganization for the CEO-of relation) but the relation
does not hold for this pair of entities in the knowl-
edge base.
666
2.2 Distantly Supervised Passage Retrieval
We extend the learning-to-rank techniques (Liu,
2011) to distant supervision setting (Xu et al,
2011) to create a robust passage retrieval system.
While relation extraction systems exploit rich and
complex features that are necessary to extract the
exact relation (Mintz et al, 2009; Riedel et al,
2010; Hoffmann et al, 2011), passage retrieval
components use coarse features in order to provide
different and complementary feedback to informa-
tion extraction models.
We exploit two types of lexical features: Bag-
Of-Words and Word-Position. The two types of
simple binary features are shown in the following
example:
Sentence: Apple founder Steve Jobs died.
Target (Primary) entity: Steve Jobs
Bag-Of-Word features: ?apple? ?founder? ?died? ?.?
Word-Position features: ?apple:-2? ?founder:-1?
?died:+1? ?.:+2?
For each relation r, we assume each sentence
has a binary relevance label to form distantly su-
pervised training data: sentences in POS(r) are
relevant and sentences in NEG(r) are irrelevant.
As a pointwise learning-to-rank approach (Nallap-
ati, 2004), the probabilities of relevance estimated
by SVMs (Platt and others, 1999) are used for
ranking all the sentences in the original training
corpus for each relation respectively. We use Lib-
SVM 2 (Chang and Lin, 2011) in our implementa-
tion.
2.3 Psuedo-relevance Relation Feedback
In the field of information retrieval, pseudo-
relevance feedback assumes that the top-ranked
documents from an initial retrieval are likely rel-
evant, and extracts relevant terms to expand the
original query (Xu and Croft, 1996; Lavrenko and
Croft, 2001; Cao et al, 2008). Analogously, our
assumption is that entity pairs that appear in more
relevant and more sentences are more likely to
express the relation, and can be used to expand
knowledge base and reduce false negative noise in
the training data for information extraction. We
identify the most likely relevant entity pairs as fol-
lows:
2http://www.csie.ntu.edu.tw/?cjlin/
libsvm
initialize ?? ?? ?
for each relation type r ? R do
learn a passage (sentence) retrieval model L(r)
using coarse features and POS(r)?NEG(r)
as training data
score the sentences in the RAW (r) by L(r)
score the entity pairs according to the scores
of sentences they are involved in
select the top ranked pairs of entities, then add
the relation r to their label in ??
end for
We select the entity pairs whose average score
of the sentences they are involved in is greater
than p, where p is a parameter tuned on develop-
ment data.3 The relation extraction model is then
trained using (?, E,R,??) with a more complete
database than the original knowledge base ?.
2.4 Distantly Supervised Relation Extraction
We use a state-of-the-art open-source system,
MULTIR (Hoffmann et al, 2011), as the rela-
tion extraction component. MULTIR is based
on multi-instance learning, which assumes that
at least one sentence of those matching a given
entity-pair contains the relation of interest (Riedel
et al, 2010) in the given knowledge base to tol-
erate false positive noise in the training data and
superior than previous models (Riedel et al, 2010;
Mintz et al, 2009) by allowing overlapping rela-
tions. MULTIR uses features which are based on
Mintz et al (2009) and consist of conjunctions of
named entity tags, syntactic dependency paths be-
tween arguments, and lexical information.
3 Experiments
For evaluating extraction accuracy, we follow the
experimental setup of Hoffmann et al (2011), and
use their implementation of MULTIR4 with 50
training iterations as our baseline. Our complete
system, which we call IRMIE, combines our pas-
sage retrieval component with MULTIR. We use
the same datasets as in Hoffmann et al (2011) and
Riedel et al (2010), which include 3-years of New
York Times articles aligned with Freebase. The
sentential extraction evaluation is performed on
a small amount of manually annotated sentences,
sampled from the union of matched sentences and
3We found p = 0.5 to work well in practice.
4http://homes.cs.washington.edu/
?raphaelh/mr/
667
Test Data Set Original Test Set Corrected Test Set
P? R? F? ?F? P? R? F? ?F?
MULTIR 80.0 44.6 62.3 92.7 47.7 70.2
IRMIE 84.6 56.1 70.3 +8.0 92.6 61.2 76.9 +6.7
MULTIRLEX 91.8 43.0 67.4 79.6 57.0 68.3
IRMIELEX 89.2 52.5 70.9 +3.5 78.0 69.2 73.6 +5.3
Table 1: Overall sentential extraction performance evaluated on the original test set of Hoffmann et
al. (2011) and our corrected test set: Our proposed relevance feedback technique yields a substantial
increase in recall.
system predictions. We define Se as the sentences
where some system extracted a relation and SF
as the sentences that match the arguments of a
fact in ?. The sentential precision and recall is
computed on a randomly sampled set of sentences
from Se?SF , in which each sentence is manually
labeled whether it expresses any relation in R.
Figure 3 shows the precision/recall curves for
MULTIR with and without pseudo-relevance feed-
back computed on the test dataset of 1000 sen-
tence used by Hoffmann et al (2011). With the
pseudo-relevance feedback from passage retrieval,
IRMIE achieves significantly higher recall at a
consistently high level of precision. At the highest
recall point, IRMIE reaches 78.5% precision and
59.2% recall, for an F1 score of 68.9%.
Because the two types of lexical features used in
our passage retrieval models are not used in MUL-
TIR, we created another baseline MULTIRLEX
by adding these features into MULTIR in order
to rule out the improvement from additional infor-
mation. Note that the sentences are sampled from
the union of Freebase matches and sentences from
which some systems in Hoffmann et al (2011) ex-
tracted a relation. It underestimates the improve-
ments of the newly developed systems in this pa-
per. We therefore also created a new test set of
1000 sentences by sampling from the union of
Freebase matches and sentences where MULTIR-
LEX or IRMIELEX extracted a relation. Table 1
shows the overall precision and recall computed
against these two test datasets, with and without
adding lexical features into multi-instance learn-
ing models. The performance improvement by us-
ing pseudo-feedback is significant (p < 0.05) in
McNemar?s test for both datasets.
4 Conclusion and Perspectives
This paper proposes a novel approach to address
an overlooked problem in distant supervision: the
knowledge base is often incomplete causing nu-
Recall
Precision
0.0 0.1 0.2 0.3 0.4 0.5 0.6
0.5
0.6
0.7
0.8
0.9
1.0
IRMIE
MULTIR
Figure 3: Sentential extraction: precision/recall
curves using exact same training and test data,
features and system settings as in Hoffmann et
al. (2011).
merous false negatives in the training data. It
greatly improves a state-of-the-art multi-instance
learning model by correcting the most likely false
negatives in the training data based on the ranking
of a passage retrieval model.
In the future, we would like to more tightly inte-
grate a coarser featured estimator of sentential rel-
evance and a finer featured relation extractor, such
that a single joint-model can be learned.
Acknowledgments
Supported in part by NSF grant IIS-1018317,
the Air Force Research Laboratory (AFRL)
under prime contract number FA8750-09-C-
0181 and the Intelligence Advanced Research
Projects Activity (IARPA) via Department of In-
terior National Business Center contract number
D11PC20154. The U.S. Government is authorized
to reproduce and distribute reprints for Govern-
mental purposes notwithstanding any copyright
annotation thereon. Disclaimer: The views and
conclusions contained herein are those of the au-
thors and should not be interpreted as necessarily
representing the official policies or endorsements,
either expressed or implied, of AFRL, IARPA,
DoI/NBC, or the U.S. Government.
668
References
Avrim Blum and Tom M. Mitchell. 1998. Combin-
ing labeled and unlabeled sata with co-training. In
Proceedings of the 11th Annual Conference on Com-
putational Learning Theory (COLT), pages 92?100.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using
minimal supervision. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics (ACL).
Guihong Cao, Jian-Yun Nie, Jianfeng Gao, and
Stephen Robertson. 2008. Selecting good expan-
sion terms for pseudo-relevance feedback. In Pro-
ceedigns of the 31st Annual International ACM SI-
GIR Conference on Research and Development in
Information Retrieval (SIGIR), pages 243?250.
Chih-Chung Chang and Chih-Jen Lin. 2011. Lib-
svm: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2(3):27.
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting informa-
tion from text sources. In Proceedings of the Sev-
enth International Conference on Intelligent Systems
for Molecular Biology (ISMB), pages 77?86.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke S. Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 541?550.
Victor Lavrenko and W. Bruce Croft. 2001.
Relevance-based language models. In Proceedings
of the 24th Annual International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval (SIGIR), pages 120?127.
Tie-Yan Liu. 2011. Learning to Rank for Information
Retrieval. Springer-Verlag Berlin Heidelberg.
Irina Matveeva, Chris Burges, Timo Burkard, Andy
Laucius, and Leon Wong. 2006. High accuracy re-
trieval with multiple nested ranker. In Proceedings
of the 29th Annual International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval (SIGIR), pages 437?444.
Bonan Min, Ralph Grishman, Li Wan, Chang Wang,
and David Gondek. 2013. Distant supervision for
relation extraction with an incomplete knowledge
base. In Proceedings of the Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL 2013).
Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedigns of the
47th Annual Meeting of the Association for Compu-
tational Linguistics and the 4th International Joint
Conference on Natural Language Processing (ACL),
pages 1003?1011.
Ramesh Nallapati. 2004. Discriminative models for
information retrieval. In Proceedigns of the 27th An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval
(SIGIR), pages 64?71.
Truc Vien T Nguyen and Alessandro Moschitti. 2011.
End-to-end relation extraction using distant super-
vision from external semantic repositories. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 277?282.
John Platt et al 1999. Probabilistic outputs for sup-
port vector machines and comparisons to regular-
ized likelihood methods. Advances in Large Margin
Classifiers, 10(3):61?74.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 492?501.
Association for Computational Linguistics.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedigns of the European
Conference on Machine Learning and Principles
and Practice of Knowledge Discovery in Databases
(ECML/PKDD), pages 148?163.
Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min.
2011. New york university 2011 system for kbp slot
filling. In Text Analysis Conference 2011 Workshop.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
455?465.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervi-
sion for relation extraction. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 721?729.
Chang Wang, James Fan, Aditya Kalyanpur, and David
Gondek. 2011. Relation extraction with relation
topics. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1426?1436.
Jinxi Xu and W. Bruce Croft. 1996. Query expansion
using local and global document analysis. In Hans-
Peter Frei, Donna Harman, Peter Scha?uble, and Ross
Wilkinson, editors, Proceedings of the 19th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR),
pages 4?11. ACM.
669
Wei Xu, Ralph Grishman, and Le Zhao. 2011. Passage
retrieval for information extraction using distant su-
pervision. In Proceedings of the International Joint
Conference on Natural Language Processing (IJC-
NLP), pages 1046?1054.
Roman Yangarber. 2001. Scenario customization for
information extraction. Ph.D. thesis, Department of
Computer Science, Graduate School of Arts and Sci-
ence, New York University.
670
