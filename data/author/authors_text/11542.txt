Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 137?145,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Without a ?doubt??
Unsupervised discovery of downward-entailing operators
Cristian Danescu-Niculescu-Mizil, Lillian Lee, and Richard Ducott
Department of Computer Science
Cornell University
Ithaca, NY 14853-7501
cristian@cs.cornell.edu, llee@cs.cornell.edu, rad47@cornell.edu
Abstract
An important part of textual inference is mak-
ing deductions involving monotonicity, that
is, determining whether a given assertion en-
tails restrictions or relaxations of that asser-
tion. For instance, the statement ?We know the
epidemic spread quickly? does not entail ?We
know the epidemic spread quickly via fleas?,
but ?We doubt the epidemic spread quickly?
entails ?We doubt the epidemic spread quickly
via fleas?. Here, we present the first algorithm
for the challenging lexical-semantics prob-
lem of learning linguistic constructions that,
like ?doubt?, are downward entailing (DE).
Our algorithm is unsupervised, resource-lean,
and effective, accurately recovering many DE
operators that are missing from the hand-
constructed lists that textual-inference sys-
tems currently use.
1 Introduction
Making inferences based on natural-language state-
ments is a crucial part of true natural-language un-
derstanding, and thus has many important applica-
tions. As the field of NLP has matured, there has
been a resurgence of interest in creating systems ca-
pable of making such inferences, as evidenced by
the activity surrounding the ongoing sequence of
?Recognizing Textual Entailment? (RTE) competi-
tions (Dagan, Glickman, and Magnini, 2006; Bar-
Haim, Dagan, Dolan, Ferro, Giampiccolo, Magnini,
and Szpektor, 2006; Giampiccolo, Magnini, Dagan,
and Dolan, 2007) and the AQUAINT knowledge-
based evaluation project (Crouch, Saur??, and Fowler,
2005).
The following two examples help illustrate the
particular type of inference that is the focus of this
paper.
1. ?We know the epidemic spread quickly?
2. ?We doubt the epidemic spread quickly?
A relaxation of ?spread quickly? is ?spread?; a re-
striction of it is ?spread quickly via fleas?. From
statement 1, we can infer the relaxed version, ?We
know the epidemic spread?, whereas the restricted
version, ?We know the epidemic spread quickly via
fleas?, does not follow. But the reverse holds for
statement 2: it entails the restricted version ?We
doubt the epidemic spread quickly via fleas?, but not
the relaxed version. The reason is that ?doubt? is a
downward-entailing operator;1 in other words, it al-
lows one to, in a sense, ?reason from sets to subsets?
(van der Wouden, 1997, pg. 90).
Downward-entailing operators are not restricted
to assertions about belief or to verbs. For example,
the preposition ?without? is also downward entail-
ing: from ?The applicants came without payment or
waivers? we can infer that all the applicants came
without payment. (Contrast this with ?with?, which,
like ?know?, is upward entailing.) In fact, there are
many downward-entailing operators, encompassing
many syntactic types; these include explicit nega-
tions like ?no? and ?never?, but also many other
terms, such as ?refuse (to)?, ?preventing?, ?nothing?,
?rarely?, and ?too [adjective] to?.
1Synonyms for ?downward entailing? include downward-
monotonic and monotone decreasing. Related concepts include
anti-additivity, veridicality, and one-way implicatives.
137
As the prevalence of these operators indicates and
as van der Wouden (1997, pg. 92) states, downward
entailment ?plays an extremely important role in
natural language? (van Benthem, 1986; Hoeksema,
1986; Sa?nchez Valencia, 1991; Dowty, 1994; Mac-
Cartney and Manning, 2007). Yet to date, only a few
systems attempt to handle the phenomenon in a gen-
eral way, i.e., to consider more than simple direct
negation (Nairn, Condoravdi, and Karttunen, 2006;
MacCartney and Manning, 2008; Christodoulopou-
los, 2008; Bar-Haim, Berant, Dagan, Greental,
Mirkin, Shnarch, and Szpektor, 2008). These sys-
tems rely on lists of items annotated with respect to
their behavior in ?polar? (positive or negative) envi-
ronments. The lists contain a relatively small num-
ber of downward-entailing operators, at least in part
because they were constructed mainly by manual
inspection of verb lists (although a few non-verbs
are sometimes also included). We therefore propose
to automatically learn downward-entailing opera-
tors2 ? henceforth DE operators for short ? from
data; deriving more comprehensive lists of DE op-
erators in this manner promises to substantially en-
hance the ability of textual-inference systems to han-
dle monotonicity-related phenomena.
Summary of our approach There are a num-
ber of significant challenges to applying a learning-
based approach. First, to our knowledge there do
not exist DE-operator-annotated corpora, and more-
over, relevant types of semantic information are ?not
available in or deducible from any public lexical
database? (Nairn et al, 2006). Also, it seems there
is no simple test one can apply to all possible candi-
dates; van der Wouden (1997, pg. 110) remarks, ?As
a rule of thumb, assume that everything that feels
negative, and everything that [satisfies a condition
described below], is monotone decreasing. This rule
of thumb will be shown to be wrong as it stands; but
2We include superlatives (?tallest?), comparatives (?taller?),
and conditionals (?if?) in this category because they have non-
default (i.e., non-upward entailing) properties ? for instance,
?he is the tallest father? does not entail ?he is the tallest man?.
Thus, they also require special treatment when considering en-
tailment relations. In fact, there have been some attempts
to unify these various types of non-upward entailing opera-
tors (von Fintel, 1999). We use the term downward entailing
(narrowly-defined) (DE(ND)) when we wish to specifically ex-
clude superlatives, comparatives, and conditionals.
it sort of works, like any rule of thumb.?
Our first insight into how to overcome these chal-
lenges is to leverage a finding from the linguistics lit-
erature, Ladusaw?s (1980) hypothesis, which can be
treated as a cue regarding the distribution of DE op-
erators: it asserts that a certain class of lexical con-
structions known as negative polarity items (NPIs)
can only appear in the scope of DE operators. Note
that this hypothesis suggests that one can develop
an unsupervised algorithm based simply on check-
ing for co-occurrence with known NPIs.
But there are significant problems with apply-
ing this idea in practice, including: (a) there is no
agreed-upon list of NPIs; (b) terms can be ambigu-
ous with respect to NPI-hood; and (c) many non-DE
operators tend to co-occur with NPIs as well. To
cope with these issues, we develop a novel unsuper-
vised distillation algorithm that helps filter out the
noise introduced by these problems. This algorithm
is very effective: it is accurate and derives many DE
operators that do not appear on pre-existing lists.
Contributions Our project draws a connection be-
tween the creation of textual entailment systems and
linguistic inquiry regarding DE operators and NPIs,
and thus relates to both language-engineering and
linguistic concerns.
To our knowledge, this work represents the first
attempt to aid in the process of discovering DE oper-
ators, a task whose importance we have highlighted
above. At the very least, our method can be used
to provide high-quality raw materials to help human
annotators create more extensive DE operator lists.
In fact, while previous manual-classification efforts
have mainly focused on verbs, we retrieve DE oper-
ators across multiple parts of speech. Also, although
we discover many items (including verbs) that are
not on pre-existing manually-constructed lists, the
items we find occur frequently ? they are not some-
how peculiar or rare.
Our algorithm is surprisingly accurate given that it
is quite resource- and knowledge-lean. Specifically,
it relies only on Ladusaw?s hypothesis as initial in-
spiration, a relatively short and arguably noisy list
of NPIs, and a large unannotated corpus. It does
not use other linguistic information ? for exam-
ple, we do not use parse information, even though
c-command relations have been asserted to play a
138
key role in the licensing of NPIs (van der Wouden,
1997).
2 Method
We mentioned in the introduction some significant
challenges to developing a machine-learning ap-
proach to discovering DE operators. The key insight
we apply to surmount these challenges is that in the
linguistics literature, it has been hypothesized that
there is a strong connection between DE operators
and negative polarity items (NPIs), which are terms
that tend to occur in ?negative environments?. An
example NPI is ?anymore?: one can say ?We don?t
have those anymore? but not ?We have those any-
more?.
Specifically, we propose to take advantage of the
seminal hypothesis of Ladusaw (1980, influenced by
Fauconnier (1975), inter alia):
(Ladusaw) NPIs only appear within the
scope of downward-entailing operators.
This hypothesis has been actively discussed, up-
dated, and contested by multiple parties (Linebarger,
1987; von Fintel, 1999; Giannakidou, 2002, inter
alia). It is not our intent to comment (directly) on its
overall validity. Rather, we simply view it as a very
useful starting point for developing computational
tools to find DE operators? indeed, even detractors
of the theory have called it ?impressively algorith-
mic? (Linebarger, 1987, pg. 361).
First, a word about scope. For Ladusaw?s hypoth-
esis, scope should arguably be defined in terms of c-
command, immediate scope, and so on (von Fintel,
1999, pg. 100). But for simplicity and to make our
approach as resource-lean as possible, we simply as-
sume that potential DE operators occur to the left of
NPIs,3 except that we ignore text to the left of any
preceding commas or semi-colons as a way to en-
force a degree of locality. For example, in both ?By
the way, we don?t have plants anymoreNPI because
they died? and ?we don?t have plants anymoreNPI?,
we look for DE operators within the sequence of
words ?we don?t have plants?. We refer to such se-
quences in which we seek DE operators as NPI con-
texts.
3There are a few exceptions, such as with the NPI ?for the
life of me? (Hoeksema, 1993).
Now, Ladusaw?s hypothesis suggests that we can
find DE operators by looking for words that tend to
occur more often in NPI contexts than they occur
overall. We formulate this as follows:
Assumption: For any DE operator d,
FbyNPIpdq ? F pdq.
Here, FbyNPIpdq is the number of occurrences of d
in NPI contexts4 divided by the number of words
in NPI contexts, and F pxq refers to the number of
occurrences of x relative to the number of words in
the corpus.
An additional consideration is that we would like
to focus on the discovery of novel or non-obvious
DE operators. Therefore, for a given candidate DE
operator c, we compute pFbyNPIpcq: the value of
FbyNPIpcq that results if we discard all NPI con-
texts containing a DE operator on a list of 10 well-
known instances, namely, ?not?, ?n?t?, ?no?, ?none?,
?neither?, ?nor?, ?few?, ?each?, ?every?, and ?without?.
(This list is based on the list of DE operators used by
the RTE system presented in MacCartney and Man-
ning (2008).) This yields the following scoring func-
tion:
Spcq : pFbyNPIpcqF pcq . (1)
Distillation There are certain terms that are not
DE operators, but nonetheless co-occur with NPIs as
a side-effect of co-occurring with true DE operators
themselves. For instance, the proper noun ?Milken?
(referring to Michael Milken, the so-called ?junk-
bond king?) occurs relatively frequently with the DE
operator ?denies?, and ?vigorously? occurs frequently
with DE operators like ?deny? and ?oppose?. We re-
fer to terms like ?milken? and ?vigorously? as ?pig-
gybackers?, and address the piggybackers problem
by leveraging the following intuition: in general, we
do not expect to have two DE operators in the same
NPI context.5 One way to implement this would be
to re-score the candidates in a winner-takes-all fash-
ion: for each NPI context, reward only the candidate
4Even if d occurs multiple times in a single NPI context we
only count it once; this way we ?dampen the signal? of func-
tion words that can potentially occur multiple times in a single
sentence.
5One reason is that if two DE operators are composed, they
ordinarily create a positive context, which would not license
NPIs (although this is not always the case (Dowty, 1994)).
139
with the highest score S. However, such a method
is too aggressive because it would force us to pick
a single candidate even when there are several with
relatively close scores? and we know our score S is
imperfect. Instead, we propose the following ?soft?
mechanism. Each sentence distributes a ?budget? of
total score 1 among the candidates it contains ac-
cording to the relative scores of those candidates;
this works out to yield the following new distilled
scoring function
Sdpcq 
?
NPIcontexts p
Spcq
nppq
Npcq , (2)
where nppq  ?cP p Spcq is an NPI-context normal-
izing factor and Npcq is the number of NPI con-
texts containing the candidate c. This way, plausi-
ble candidates that have high S scores relative to the
other candidates in the sentence receive enhanced Sd
scores. To put it another way: apparently plausible
candidates that often appear in sentences with mul-
tiple good candidates (i.e., piggybackers) receive a
low distilled score, despite a high initial score.
Our general claim is that the higher the distilled
score of a candidate, the better its chances of being
a DE operator.
Choice of NPIs Our proposed method requires ac-
cess to a set of NPIs. However, there does not ap-
pear to be universal agreement on such a set. Lichte
and Soehn (2007) mention some doubts regarding
approximately 200 (!) of the items on a roughly 350-
item list of German NPIs (Ku?rschner, 1983). For
English, the ?moderately complete?6 Lawler (2005)
list contains two to three dozen items; however,
there is also a list of English NPIs that is several
times longer (von Bergen and von Bergen, 1993,
written in German), and Hoeksema (1997) asserts
that English should have hundreds of NPIs, similarly
to French and Dutch.
We choose to focus on the items on these lists
that seem most likely to be effective cues for our
task. Specifically, we select a subset of the Lawler
NPIs, focusing mostly on those that do not have
a relatively frequent non-NPI sense. An example
discard is ?much?, whose NPI-hood depends on
6www-personal.umich.edu/jlawler/aue/
npi.html
what it modifies and perhaps on whether there
are degree adverbs pre-modifying it (Hoeksema,
1997). There are some ambiguous NPIs that we
do retain due to their frequency. For example,
?any? occurs both in a non-NPI ?free choice?
variant, as in ?any idiot can do that?, and in an
NPI version. Although it is ambiguous with re-
spect to NPI-hood, ?any? is also a very valuable
cue due to its frequency.7 Here is our NPI list:
any in weeks/ages/years budge yet
at all drink a drop red cent ever
give a damn last/be/take long but what bother to
do a thing arrive/leave until give a shit lift a finger
bat an eye would care/mind eat a bite to speak of
3 Experiments
Our main set of evaluations focuses on the precision
of our method at discovering new DE operators. We
then briefly discuss evaluation of other dimensions.
3.1 Setup
We applied our method to the entirety of the BLLIP
(Brown Laboratory for Linguistic Information Pro-
cessing) 1987?89 WSJ Corpus Release 1, available
from the LDC (LDC2000T43). The 1,796,379 sen-
tences in the corpus comprise 53,064 NPI contexts;
after discarding the ones containing the 10 well-
known DE operators, 30,889 NPI contexts were left.
To avoid sparse data problems, we did not consider
candidates with very low frequency in the corpus
(?150 occurrences) or in the NPI contexts (?10 oc-
currences).
Methodology for eliciting judgments The obvi-
ous way to evaluate the precision of our algorithm is
to have human annotators judge each output item as
to whether it is a DE operator or not. However, there
are some methodological issues that arise.
First, if the judges know that every term they are
rating comes from our system and that we are hoping
that the algorithm extracts DE operators, they may
be biased towards calling every item ?DE? regard-
less of whether it actually is. We deal with this prob-
lem by introducing distractors ? items that are not
produced by our algorithm, but are similar enough
to not be easily identifiable as ?fakes?. Specifically,
7It is by far the most frequent NPI, appearing in 36,554 of
the sentences in the BLLIP corpus (see Section 3).
140
for each possible part of speech of each of our sys-
tem?s outputs c that exists in WordNet, we choose a
distractor that is either in a ?sibling? synset (a hy-
ponym of c?s hypernym) or an antonym. Thus, the
distractors are highly related to the candidates. Note
that they may in fact also be DE operators.
The judges were made aware of the presence of
a substantial number of distractors (about 70 for the
set of top 150 outputs). This design choice did seem
to help ensure that the judges carefully evaluated
each item.
The second issue is that, as mentioned in the in-
troduction, there does not seem to be a uniform test
that judges can apply to all items to ascertain their
DE-ness; but we do not want the judges to impro-
vise excessively, since that can introduce undesir-
able randomness into their decisions. We therefore
encouraged the judges to try to construct sentences
wherein the arguments for candidate DE operators
were drawn from a set of phrases and restricted
replacements we specified (example: ?singing? vs
?singing loudly?). However, improvisation was still
required in a number of cases; for example, the can-
didate ?act?, as either a noun or a verb, cannot take
?singing? as an argument.
The labels that the judges could apply were
?DE(ND)? (downward entailing (narrowly-
defined)), ?superlative?, ?comparative?, ?condi-
tional?, ?hard to tell?, and ?not-DE? (= none of the
above). We chose this fine-grained sub-division
because the second through fourth categories are
all known to co-occur with NPIs. There is some
debate in the linguistics literature as to whether
they can be considered to be downward entailing,
narrowly construed, or not (von Fintel, 1999,
inter alia), but nonetheless, such operators call for
special reasoning quite distinct from that required
when dealing with upward entailing operators ?
hence, we consider it a success when our algorithm
identifies them.
Since monotonicity phenomena can be rather sub-
tle, the judges engaged in a collaborative process.
Judge A (the second author) annotated all items, but
worked in batches of around 10 items. At the end of
each batch, Judge B (the first author) reviewed Judge
A?s decisions, and the two consulted to resolve dis-
agreements as far as possible.
One final remark regarding the annotation: some
decisions still seem uncertain, since various factors
such as context, Gricean maxims, what should be
presupposed8 and so on come into play. However,
we take comfort in a comment by Eugene Charniak
(personal communication) to the effect that if a word
causes a native speaker to pause, that word is inter-
esting enough to be included. And indeed, it seems
reasonable that if a native speaker thinks there might
be a sense in which a word can be considered down-
ward entailing, then our system should flag it as a
word that an RTE system should at least perhaps
pass to a different subsystem for further analysis.
3.2 Precision Results
We now examine the 150 items that were most
highly ranked by our system, which were sub-
sequently annotated as just described. (For
full system output that includes the unannotated
items, see http://www.cs.cornell.edu/
cristian. We would welcome external anno-
tation help.) As shown in Figure 1a, which depicts
precision at k for various values of k, our system
performs very well. In fact, 100% of the first 60 out-
puts are DE, broadly construed. It is also interesting
to note the increasing presence of instances that the
judges found hard to categorize as we move further
down the ranking.
Of our 73 distractors, 46% were judged to be
members of one of our goal categories. The fact that
this percentage is substantially lower than our algo-
rithm?s precision at both 73 and 150 (the largest k we
considered) confirms that our judges were not mak-
ing random decisions. (We expect the percentage
of DE operators among the distractors to be much
higher than 0 because they were chosen to be simi-
lar to our system?s outputs, and so can be expected
to also be DE operators some fraction of the time.)
Table 1 shows the lemmas of just the DE(ND) op-
erators that our algorithm placed in its top 150 out-
puts.9 Most of these lemmas are new discoveries, in
the sense of not appearing in Ladusaw?s (1980) (im-
plicit) enumeration of DE operators. Moreover, the
8For example, ?X doubts the epidemic spread quickly? might
be said to entail ?X would doubt the epidemic spreads via fleas,
presupposing that X thinks about the flea issue?.
9By listing lemmas, we omit variants of the same word, such
as ?doubting? and ?doubted?, to enhance readability. We omit
superlatives, comparatives, and conditionals for brevity.
141
10 20 30 40 50 60 70 80 90 100 110 120 130 140 1500
10
20
30
40
50
60
70
80
90
100
k
Prec
ision
 at k
 
 DE(ND)S/C/CHard
10 20 30 40 50 60 70 80 90 100 110 120 130 140 1500
10
20
30
40
50
60
70
80
90
100
k
Prec
ision
 at k
 
 DE(ND)S/C/CHard
(a) (b)
Figure 1: (a) Precision at k for k divisible by 10 up to k  150. The bar divisions are, from the x-axis up,
DE(ND) (blue, the largest); Superlatives/Conditionals/Comparatives (green, 2nd largest); and Hard (red, sometimes
non-existent). For example, all of the first 10 outputs were judged to be either downward entailing (narrowly-defined)
(8 of 10, or 80%) or in one of the related categories (20%). (b) Precision at k when the distillation step is omitted.
not-DE Hard
almost firmly one-day approve
ambitious fined signal cautioned
considers liable remove dismissed
detect notify vowed fend
Table 3: Examples of words judged to be either not in
one of our monotonicity categories of interest (not-DE)
or hard to evaluate (Hard).
lists of DE(ND) operators that are used by textual-
entailment systems are significantly smaller than
that depicted in Table 1; for example, MacCartney
and Manning (2008) use only about a dozen (per-
sonal communication).
Table 3 shows examples of the words in our sys-
tem?s top 150 outputs that are either clear mistakes
or hard to evaluate. Some of these are due to id-
iosyncrasies of newswire text. For instance, we of-
ten see phrases like ?biggest one-day drop in ...?,
where ?one-day? piggybacks on superlatives, and
?vowed? piggybacks on the DE operator ?veto?, as
in the phrase ?vowed to veto?.
Effect of distillation In order to evaluate the im-
portance of the distillation process, we study how
the results change when distillation is omitted (thus
using as score function S from Equation 1 rather
than Sd). When comparing the results (summarized
in Figure 1b) with those of the complete system
(Figure 1a) we observe that the distillation indeed
has the desired effect: the number of highly ranked
words that are annotated as not-DE decreases after
distillation. This results in an increase of the preci-
sion at k ranging from 5% to 10% (depending on k),
as can be observed by comparing the height of the
composite bars in the two figures.10
Importantly, this improvement does indeed seem
to stem at least in part from the distillation process
handling the piggybacking problem. To give just a
few examples: ?vigorously? is pushed down from
rank 48 (undistilled scoring) to rank 126 (distilled
scoring), ?one-day? from 25th to 65th, ?vowed? from
45th to 75th, and ?Milken? from 121st to 350th.
3.3 Other Results
It is natural to ask whether the (expected) decrease
in precision at k is due to the algorithm assigning
relatively low scores to DE operators, so that they
do not appear in the top 150, or due to there be-
ing no more more true DE operators to rank. We
cannot directly evaluate our method?s recall because
no comprehensive list of DE operators exists. How-
ever, to get a rough impression, we can check how
our system ranks the items in the largest list we are
aware of, namely, the Ladusaw (implicit) list men-
tioned above. Of the 31 DE operator lemmas on this
list (not including the 10 well-known DE operators),
only 7 of those frequent enough to be considered by
our algorithm are not in its top 150 outputs, and only
10The words annotated ?hard? do not affect this increase in
precision.
142
absence of
absent from
anxious about 
to avoid (L)
to bar
barely
to block
cannot (L)
compensate for 
to decline
to defer
to deny (L)
to deter
to discourage
to dismiss
to doubt (L)
to eliminate
essential for 
to exclude
to fail (L)
hardly (L)
to lack
innocent of 
to minimize 
never (L)
nobody
nothing
to oppose
to postpone 
to preclude
premature to
to prevent
to prohibit
rarely (L)
to refrain from
to refuse (L)
regardless 
to reject
reluctant to (L)
to resist
to rule out
skeptical 
to suspend
to thwart
unable to
unaware of
unclear on
unlike
unlikely (L)
unwilling to
to veto
wary of
warned that (L)
whenever
withstand
Table 1: The 55 lemmas for the 90 downward entailing (narrowly-defined) operators among our algorithm?s top 150
outputs. (L) marks instances from Ladusaw?s list.  marks some of the more interesting cases. We have added
function words (e.g., ?to?, ?for?) to indicate parts of speech or subcategorization; our algorithm does not discover
multi-word phrases.
Original ? Restriction
Dan is unlikely to sing. ??
??{
Dan is unlikely to sing loudly.
Olivia compensates for eating by exercising. ??
??{
Olivia compensates for eating late by exercising.
Ursula refused to sing or dance. ??
??{
Ursula refused to sing.
Bob would postpone singing. ??
??{
Bob would postpone singing loudly.
Talent is essential for singing. ??
??{
Talent is essential for singing a ballad.
She will finish regardless of threats. ??
??{
She will finish regardless of threats to my career.
Table 2: Example demonstrations that the underlined expressions (selected from Table 1) are DE operators: the
sentences on the left entail those on the right. We also have provided??{ indicators because the reader might find it
helpful to reason in the opposite direction and see that these expressions are not upward entailing.
5 are not in the top 300. Remember that we only an-
notated the top 150 outputs; so, there may be many
other DE operators between positions 150 and 300.
Another way of evaluating our method would be
to assess the effect of our newly discovered DE op-
erators on downstream RTE system performance.
There are two factors to take into account. First, the
DE operators we discovered are quite prevalent in
naturally occurring text11 : the 90 DE(ND) operators
appearing in our algorithm?s top 150 outputs occur
in 111,456 sentences in the BLLIP corpus (i.e., in
6% of its sentences). Second, as previously men-
tioned, systems do already account for monotonic-
ity to some extent ? but they are limited by the fact
that their DE operator lexicons are restricted mostly
to well-known instances; to take a concrete example
with a publicly available RTE system: Nutcracker
(Bos and Markert, 2006) correctly infers that ?We
did not know the disease spread? entails ?We did not
know the disease spread quickly? but it fails to in-
11However, RTE competitions do not happen to currently
stress inferences involving monotonicity. The reasons why are
beyond the scope of this paper.
fer that ?We doubt the disease spread? entails ?We
doubt the disease spread quickly?. So, systems can
use monotonicity information but currently do not
have enough of it; our method can provide themwith
this information, enabling them to handle a greater
fraction of the large number of naturally occurring
instances of this phenomenon than ever before.
4 Related work not already discussed
Magnini (2008), in describing modular approaches
to textual entailment, hints that NPIs may be used
within a negation-detection sub-component.
There is a substantial body of work in the linguis-
tics literature regarding the definition and nature of
polarity items (Polarity Items Bibliography). How-
ever, very little of this work is computational. There
has been passing speculation that one might want
to learn polarity-inverting verbs (Christodoulopou-
los, 2008, pg. 47). There have also been a few
projects on the discovery of NPIs, which is the con-
verse of the problem we consider. Hoeksema (1997)
discusses some of the difficulties with corpus-based
determination of NPIs, including ?rampant? poly-
143
semy and the problem of ?how to determine inde-
pendently which predicates should count as nega-
tive?? a problemwhich our work addresses. Lichte
and Soehn (Lichte, 2005; Lichte and Soehn, 2007)
consider finding German NPIs using a method con-
ceptually similar in some respects to our own, al-
though again, their objective is the reverse of ours.
Their discovery statistic for single-word NPIs is the
ratio of within-licenser-clause occurrences to total
occurrences, where, to enhance precision, the list of
licensers was filtered down to a set of fairly unam-
biguous, easily-identified items. They do not con-
sider distillation, which we found to be an impor-
tant component of our DE-operator-detection algo-
rithm. Their evaluation scheme, unlike ours, did not
employ a bias-compensation mechanism. They did
employ a collocation-detection technique to extend
their list to multi-word NPIs, but our independent
experiments with a similar technique (not reported
here) did not yield good results.
5 Conclusions and future work
To our knowledge, this work represents the first at-
tempt to discover downward entailing operators. We
introduced a unsupervised algorithm that is moti-
vated by research in linguistics but employs simple
distributional statistics in a novel fashion. Our algo-
rithm is highly accurate and discovers many reason-
able DE operators that are missing from pre-existing
manually-built lists.
Since the algorithm is resource-lean ? requiring
no parser or tagger but only a list of NPIs? it can be
immediately applied to languages where such lists
exist, such as German and Romanian (Trawin?ski and
Soehn, 2008). On the other hand, although the re-
sults are already quite good for English, it would
be interesting to see what improvements could be
gained by using more sophisticated syntactic infor-
mation.
For languages where NPI lists are not extensive,
one could envision applying an iterative co-learning
approach: use the newly-derived DE operators to in-
fer new NPIs, and then discover even more new DE
operators given the new NPI list. (For English, our
initial attempts at bootstrapping from our initial NPI
list on the BLLIP corpus did not lead to substantially
improved results.)
In practice, subcategorization is an important fea-
ture to capture. In Table 1, we indicate which sub-
categorizations are DE. An interesting extension of
our work would be to try to automatically distin-
guish particular DE subcategorizations that are lex-
ically apparent, e.g., ?innocent? (not DE) vs. ?inno-
cent of? (as in ?innocent of burglary?, DE).
Our project provides a connection (among many)
between the creation of textual entailment systems
(the domain of language engineers) and the char-
acterization of DE operators (the subject of study
and debate among linguists). The prospect that our
method might potentially eventually be refined in
such a way so as to shed at least a little light on lin-
guistic questions is a very appealing one, although
we cannot be certain that any progress will be made
on that front.
Acknowledgments We thank Roy Bar-Haim, Cleo Con-
doravdi, and Bill MacCartney for sharing their systems? lists
and information about their work with us; Mats Rooth for
helpful conversations; Alex Niculescu-Mizil for technical as-
sistance; and Eugene Charniak for reassuring remarks. We also
thank Marisa Ferrara Boston, Claire Cardie, Zhong Chen, Yejin
Choi, Effi Georgala, Myle Ott, Stephen Purpura, and Ainur
Yessenalina at Cornell University, the UT-Austin NLP group,
Roy Bar-Haim, Bill MacCartney, and the anonymous review-
ers for for their comments on this paper. This paper is based
upon work supported in part by DHS grant N0014-07-1-0152,
National Science Foundation grant No. BCS-0537606, a Ya-
hoo! Research Alliance gift, a CU Provost?s Award for Distin-
guished Scholarship, and a CU Institute for the Social Sciences
Faculty Fellowship. Any opinions, findings, and conclusions or
recommendations expressed are those of the authors and do not
necessarily reflect the views or official policies, either expressed
or implied, of any sponsoring institutions, the U.S. government,
or any other entity.
References
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
The second PASCAL Recognising Textual Entailment
challenge. In Proceedings of the Second PASCAL
Challenges Workshop on Recognising Textual Entail-
ment, 2006.
Roy Bar-Haim, Jonathan Berant, Ido Dagan, Iddo Green-
tal, Shachar Mirkin, Eyal Shnarch, and Idan Szpektor.
Efficient semantic deduction and approximate match-
ing over compact parse forests. In Proceedings of TAC,
2008.
Johan Bos and Katja Markert. Recognising textual en-
tailment with robust logical inference. In Quin?onero
Candela, Dagan, Magnini, and d?Alche? Buc (2006),
pages 404?426.
Christos Christodoulopoulos. Creating a natural logic in-
ference system with combinatory categorial grammar.
Master?s thesis, University of Edinburgh, 2008.
144
Dick Crouch, Roser Saur??, and Abraham Fowler.
AQUAINT pilot knowledge-based evalua-
tion: Annotation guidelines. http://www2.
parc.com/istl/groups/nltt/papers/
aquaint kb pilot evaluation guide.pdf,
2005.
Ido Dagan, Oren Glickman, and Bernardo Magnini. The
PASCAL Recognising Textual Entailment challenge.
In Quin?onero Candela et al (2006), pages 177?190.
David Dowty. The role of negative polarity and con-
cord marking in natural language reasoning. In Mandy
Harvey and Lynn Santelmann, editors, Proceedings of
SALT IV, pages 114?144, Ithaca, New York, 1994.
Cornell University.
Gilles Fauconnier. Polarity and the scale principle. In
Proceedings of the Chicago Linguistic Society (CLS),
pages 188?199, 1975. Reprinted in Javier Gutierrez-
Rexach (ed.), Semantics: Critical Concepts in Linguis-
tics, 2003.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and
Bill Dolan. The third PASCAL Recognizing Textual
Entailment challenge. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing, pages 1?9, 2007. URL http://www.
aclweb.org/anthology/W/W07/W07-1401.
Anastasia Giannakidou. Licensing and sensitivity in po-
larity items: from downward entailment to nonveridi-
cality. In Proceedings of the Chicago Linguistic Soci-
ety (CLS), 2002.
Jack Hoeksema. Monotonicity phenomena in natural lan-
guage. Linguistic Analysis, 16:25?40, 1986.
Jack Hoeksema. As (of) yet. Appears in Language and
Cognition 3, the 1992 yearbook of the research group
for theoretical and experimental linguistics of the Uni-
versity of Groningen, 1993. http://www.let.
rug.nl/hoeksema/asofyet.pdf.
Jack Hoeksema. Corpus study of negative polar-
ity items. IV-V Jornades de corpus linguistics
1996-1997, 1997. http://odur.let.rug.nl/
hoeksema/docs/barcelona.html.
Wilfried Ku?rschner. Studien zur Negation im Deutschen.
Narr, 1983.
William A. Ladusaw. Polarity Sensitivity as Inherent
Scope Relations. Garland Press, New York, 1980.
Ph.D. thesis date 1979.
John Lawler. Negation and NPIs. http://www.
umich.edu/jlawler/NPIs.pdf, 2005. Ver-
sion of 10/29/2005.
Timm Lichte. Corpus-based acquisition of complex neg-
ative polarity items. In ESSLLI Student Session, 2005.
Timm Lichte and Jan-Philipp Soehn. The retrieval and
classification of Negative Polarity Items using statisti-
cal profiles. In Sam Featherston and Wolfgang Sterne-
feld, editors, Roots: Linguistics in Search of its Ev-
idential Base, pages 249?266. Mouton de Gruyter,
2007.
Marcia Linebarger. Negative polarity and grammatical
representation. Linguistics and philosophy, 10:325?
387, 1987.
Bill MacCartney and Christopher D. Manning. Natural
logic for textual inference. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing, pages 193?200, 2007.
Bill MacCartney and Christopher D. Manning. Mod-
eling semantic containment and exclusion in natu-
ral language inference. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics (Coling 2008), pages 521?528, Manchester, UK,
August 2008. Coling 2008 Organizing Committee.
URL http://www.aclweb.org/anthology/
C08-1066.
Bernardo Magnini. Slides for a presentation entitled ?Se-
mantic Knowledge for Textual Entailment?. Sympo-
sium on Semantic Knowledge Discovery, Organiza-
tion and Use, New York University, November 14 and
15, 2008.
Rowan Nairn, Cleo Condoravdi, and Lauri Karttunen.
Computing relative polarity for textual inference. In
Proceedings of Inference in Computational Semantics
(ICoS), 2006.
Polarity Items Bibliography. The polarity items
bibliography. http://www.sfb441.
uni-tuebingen.de/a5/pib/XML2HTML/
list.html, 2008. Maintenance guaranteed only
through December 2008.
Joaquin Quin?onero Candela, Ido Dagan, Bernardo
Magnini, and Florence d?Alche? Buc, editors. Ma-
chine Learning Challenges, Evaluating Predictive Un-
certainty, Visual Object Classification and Recogniz-
ing Textual Entailment, First PASCAL Machine Learn-
ing Challenges Workshop, MLCW 2005, Southamp-
ton, UK, April 11-13, 2005, Revised Selected Papers,
volume 3944 of Lecture Notes in Computer Science
(LNCS), 2006. Springer.
V??ctor Sa?nchez Valencia. Studies on natural logic and
categorial grammar. PhD thesis, University of Ams-
terdam, 1991.
Beata Trawin?ski and Jan-Philipp Soehn. A Multilingual
Database of Polarity Items. In Proceedings of LREC
2008, May 28?30, Marrakech, Morocco, 2008.
Johan van Benthem. Essays in Logical Semantics. Reidel,
Dordrecht, 1986.
Ton van der Wouden. Negative contexts: Collocation,
polarity and multiple negation. Routledge, 1997.
Anke von Bergen and Karl von Bergen. Negative
Polarita?t im Englischen. Gunter Narr, 1993. List
extracted and compiled by Manfred Sailer, 2008,
http://www.sfs.uni-tuebingen.de/fr/
esslli/08/byday/english-npi.pdf.
Kai von Fintel. NPI licensing, Strawson entailment, and
context dependency. Journal of Semantics, 16:97?148,
1999.
145
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2008?2018,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Brighter than Gold: Figurative Language in User Generated Comparisons
Vlad Niculae and Cristian Danescu-Niculescu-Mizil
MPI-SWS
Cornell University
vniculae@mpi-sws.org, cristian@mpi-sws.org
Abstract
Comparisons are common linguistic de-
vices used to indicate the likeness of two
things. Often, this likeness is not meant
in the literal sense?for example, ?I slept
like a log? does not imply that logs ac-
tually sleep. In this paper we propose a
computational study of figurative compar-
isons, or similes. Our starting point is a
new large dataset of comparisons extracted
from product reviews and annotated for
figurativeness. We use this dataset to char-
acterize figurative language in naturally
occurring comparisons and reveal linguis-
tic patterns indicative of this phenomenon.
We operationalize these insights and ap-
ply them to a new task with high relevance
to text understanding: distinguishing be-
tween figurative and literal comparisons.
Finally, we apply this framework to ex-
plore the social context in which figurative
language is produced, showing that simi-
les are more likely to accompany opinions
showing extreme sentiment, and that they
are uncommon in reviews deemed helpful.
1 Introduction
In argument similes are like songs in love; they
describe much, but prove nothing.
? Franz Kafka
Comparisons are fundamental linguistic devices
that express the likeness of two things?be it en-
tities, concepts or ideas. Given that their work-
ing principle is to emphasize the relation between
the shared properties of two arguments (Bredin,
1998), comparisons can synthesize important se-
mantic knowledge.
Often, comparisons are not meant to be under-
stood literally. Figurative comparisons are an im-
portant figure of speech called simile. Consider the
following two examples paraphrased from Ama-
zon product reviews:
(1) Sterling is much cheaper than gold.
(2) Her voice makes this song shine brighter than gold.
In (1) the comparison draws on the relation be-
tween the price property shared by the two metals,
sterling and gold. While (2) also draws on a com-
mon property (brightness), the polysemantic use
(vocal timbre vs. light reflection) makes the com-
parison figurative.
Importantly, there is no general rule separating
literal from figurative comparisons. More gen-
erally, the distinction between figurative and lit-
eral language is blurred and subjective (Hanks,
2006). Multiple criteria for delimiting the two
have been proposed in the linguistic and philo-
sophical literature?for a comprehensive review,
see Shutova (2010)?but they are not without ex-
ceptions, and are often hard to operationalize in a
computational framework. When considering the
specific case of comparisons, such criteria cannot
be directly applied.
Recently, the simile has received increasing at-
tention from linguists and lexicographers (Moon,
2008; Moon, 2011; Hanks, 2013) as it became
clearer that similes need to be treated separately
from metaphors since they operate on funda-
mentally different principles (Bethlehem, 1996).
Metaphors are linguistically simple structures hid-
ing a complex mapping between two domains,
through which many properties are transferred.
For example the conceptual metaphor of life as
a journey can be instantiated in many particular
ways: being at a fork in the road, reaching the end
of the line (Lakoff and Johnson, 1980). In contrast,
the semantic context of similes tends to be very
shallow, transferring a single property (Hanks,
2013). Their more explicit syntactic structure al-
lows, in exchange, for more lexical creativity. As
Hanks (2013) puts it, similes ?tend to license all
2008
sorts of logical mayhem.? Moreover, the over-
lap between the expressive range of similes and
metaphors is now known to be only partial: there
are similes that cannot be rephrased as metaphors,
and the other way around (Israel et al., 2004). This
suggests that figurativeness in similes should be
modeled differently than in metaphors. To further
underline the necessity of a computational model
for similes, we give the first estimate of their fre-
quency in the wild: over 30% of comparisons are
figurative.
1
We also confirm that a state of the art
metaphor detection system performs poorly when
applied directly to the task of detecting similes.
In this work we propose a computational study
of figurative language in comparisons. To this end,
we build the first large collection of naturally oc-
curring comparisons with figurativeness annota-
tion, which we make publicly available. Using
this resource we explore the linguistic patterns that
characterize similes, and group them in two con-
ceptually distinctive classes. The first class con-
tains cues that are agnostic of the context in which
the comparison appears (domain-agnostic cues).
For example, we find that the higher the seman-
tic similarity between the two arguments, the less
likely it is for the comparison to be figurative?in
the examples above, sterling is semantically very
similar to gold, both being metals, but song and
gold are semantically dissimilar. The second type
of cues are domain-specific, drawing on the in-
tuition that the domain in which a comparison is
used is a factor in determining its figurativeness.
We find, for instance, that the less specific a com-
parison is to the domain in which it appears, the
more likely it is to be used in a figurative sense
(e.g., in example (2), gold is very unexpected in
the musical domain).
We successfully exploit these insights in a new
prediction task relevant to text understanding: dis-
criminating figurative comparisons from literal
ones. Encouraged by the high accuracy of our
system?which is within 10% of that obtained by
human annotators?we automatically extend the
figurativeness labels to 80,000 comparisons occur-
ring in product reviews. This enables us to conduct
a fine-grained analysis of how comparison usage
interacts with their social context, opening up a
research direction with applications in sentiment
analysis and opinion mining. In particular we find
1
This estimate is based on the set of noun-noun compar-
isons with non-identical arguments collected for this study
from Amazon.com product reviews.
that figurative comparisons are more likely to ac-
company reviews showing extreme sentiment, and
that they are uncommon in opinions deemed as be-
ing helpful. To the best of our knowledge, this is
the first time figurative language is tied to the so-
cial context in which it appears.
To summarize, the main contributions of this
work are as follows:
? it introduces the first large dataset of compar-
isons with figurativeness annotations (Sec-
tion 3);
? it unveils new linguistic patterns characteriz-
ing figurative comparisons (Section 4);
? it introduces the task of distinguishing figura-
tive from literal comparisons (Section 5);
? it establishes the relation between figurative
language and the social context in which it
appears (Section 6).
2 Further Related Work
Corpus studies on figurative language in compar-
isons are scarce, and none directly address the
distinction between figurative and literal compar-
isons. Roncero et al. (2006) observed, by search-
ing the web for several stereotypical comparisons
(e.g., education is like a stairway), that similes
are more likely to be accompanied by explana-
tions than equivalent metaphors (e.g., education
is a stairway). Related to figurativeness is irony,
which Veale (2012a) finds to often be lexically
marked. By using a similar insight to filter out
ironic comparisons, and by assuming that the rest
are literal, Veale and Hao (2008) learn stereotyp-
ical knowledge about the world from frequently
compared terms. A similar process has been ap-
plied to both English and Chinese by Li et al.
(2012), thereby encouraging the idea that the trope
behaves similarly in different languages. A related
system is the Jigsaw Bard (Veale and Hao, 2011),
a thesaurus driven by figurative conventional sim-
iles extracted from the Google N-grams. This sys-
tem aims to build and generate canned expressions
by using items frequently associated with the sim-
ile pattern above. An extension of the principles of
the Jigsaw Bard is found in Thesaurus Rex (Veale
and Li, 2013), a data-driven partition of words into
ad-hoc categories. Thesaurus Rex is constructed
using simple comparison and hypernym patterns
2009
and is able to provide weighted lists of categories
for given words.
In text understanding systems, literal compar-
isons are used to detect analogies between related
geographical places (Lofi et al., 2014). Tandon et
al. (2014) use relative comparative patterns (e.g.,
X is heavier than Y) to enrich a common-sense
knowledge base. Jindal and Liu (2006) extract
graded comparisons from various sources, with
the objective of mining consumer opinion about
products. They note that identifying objective vs.
subjective comparisons?related to literality?is
an important future direction. Given that many
comparisons are figurative, a system that discrim-
inates literal from figurative comparisons is essen-
tial for such text understanding and information
retrieval systems.
The vast majority of previous work on figu-
rative language focused on metaphor detection.
Tsvetkov et al. (2014a) propose a cross-lingual
system based on word-level conceptual features
and they evaluate it on Subject-Verb-Object triples
and Adjective-Noun pairs. Their features include
and extend the idea of abstractness used by Turney
et al. (2011) for Adjective-Noun metaphors. Hovy
et al. (2013) contribute an unrestricted metaphor
corpus and propose a method based on tree ker-
nels. Bridging the gap between metaphor identifi-
cation and interpretation, Shutova and Sun (2013)
proposed an unsupervised system to learn source-
target domain mappings. The system fits concep-
tual metaphor theory (Lakoff and Johnson, 1980)
well, at the cost of not being able to tackle figu-
rative language in general, and similes in particu-
lar, as similes do not map entire domains to one
another. Since similes operate on fundamentally
different principles than metaphors, our work pro-
poses a computational approach tailored specifi-
cally for comparisons.
3 Background and Data
3.1 Structure of a comparison
Unlike metaphors, which are generally unre-
stricted, comparisons are more structured but also
more lexically and semantically varied. This en-
ables a more structured computational representa-
tion of which we take advantage. The constituents
of a comparison according to Hanks (2012) are:
? the TOPIC, sometimes called tenor: it is usu-
ally a noun phrase and acts as logical subject;
? the VEHICLE: it is the object of the compari-
son and is also usually a noun phrase;
? the shared PROPERTY or ground: it expresses
what the two entities have in common?it can
be explicit but is often implicit, left for the
reader to infer;
? the EVENT (eventuality or state): usually a
verb, it sets the frame for the observation of
the common property;
? the COMPARATOR: commonly a preposition
(like) or part of an adjectival phrase (better
than), it is the trigger word or phrase that
marks the presence of a comparison.
The literal example (1) would be segmented as:
[Sterling /TOPIC] [is /EVENT] much [cheaper
/PROPERTY] [than /COMPARATOR] [gold /VE-
HICLE]
3.2 Annotation
People resort to comparisons often when mak-
ing descriptions, as they are a powerful way of
expressing properties by example. For this rea-
son we collect a dataset of user-generated compar-
isons in Amazon product reviews (McAuley and
Leskovec, 2013), where users have to be descrip-
tive and precise, but also to express personal opin-
ion. We supplement the data with a smaller set of
comparisons from WaCky and WaCkypedia (Ba-
roni et al., 2009) to cover more genres. In pre-
liminary work, we experimented with dependency
parse tree patterns for extracting comparisons and
labeling their parts (Niculae, 2013). We use the
same approach, but with an improved set of pat-
terns, to extract comparisons with the COMPARA-
TORS like, as and than.
2
We keep only the matches
where the TOPIC and the VEHICLE are nouns, and
the PROPERTY, if present, is an adjective, which
is the typical case. Also, the head words of the
constituents are constrained to occur in the distri-
butional resources used (Baroni and Lenci, 2010;
Faruqui and Dyer, 2014).
3
2
We process the review corpus with part-of-speech tag-
ging using the IRC model for TweetNLP (Owoputi et al.,
2013; Forsyth and Martell, 2007) and dependency parsing
using the TurboParser standard model (Martins et al., 2010).
3
Due to the strong tendency of comparisons with the same
TOPIC and VEHICLE to be trivially literal in the WaCky
examples, we filtered out such examples from the Amazon
product reviews. We also filtered proper nouns using a capi-
talization heuristic.
2010
We proceed to validate and annotate for figu-
rativeness a random sample of the comparisons
extracted using the automated process described
above. The annotation is performed using crowd-
sourcing on the Amazon Mechanical Turk plat-
form, in two steps. First, the annotators are asked
to determine whether a displayed sentence is in-
deed a comparison between the highlighted words
(TOPIC and VEHICLE). Sentences qualified by
two out of three annotators as comparisons are
used in the second round, where the task is to
rate how metaphorical a comparison is. We use
a scale of 1 to 4 following Turney et al. (2011),
and then binarize to consider scores of 1?2 as lit-
eral and 3?4 as figurative. Finally, in this work we
only consider comparisons where all three annota-
tors agree on this binary notion of figurativeness.
For both tasks, we provide guidelines mostly in
the form of examples and intuition, motivated on
one hand by the annotators not having specialized
knowledge, and on the other hand by the observa-
tion that the literal-figurative distinction is subjec-
tive. All annotators have the master worker qual-
ification, reside in the U.S. and completed a lin-
guistic background questionnaire that verifies their
experience with English. In both tasks, control
sentences with confidently known labels are used
to filter low quality answers; in addition, we test
annotators with a simple paraphrasing task shown
to be effective for eliciting and verifying linguis-
tic attention (Munro et al., 2010). Both tasks
seem relatively difficult for humans, with inter-
annotator agreement given by Fleiss? k of 0.48
for the comparison identification task and of 0.54
for the figurativeness annotation after binarization.
This is comparable to 0.57 reported by Hovy et al.
(2013) for general metaphor labeling. We show
some statistics about the collected data in Table 1.
Overall, this is a costly process: out of 2400 auto-
matically extracted comparison candidates, about
60% were deemed by the annotators to be actual
comparisons and only 12% end up being selected
confidently enough as figurative comparisons.
Our dataset of human-filtered comparisons,
with the scores given by the three annotators,
is made publicly available to encourage further
work.
4
This also includes about 400 comparisons
where the annotators do not agree perfectly on bi-
nary figurativeness. Such cases can be interest-
ing to other analyses, even if we don?t consider
4
http://vene.ro/figurative-comparisons/
Domain fig. lit. % fig.
Books 177 313 36%
Music 45 68 40%
Electronics 23 105 18%
Jewelery 9 126 7%
WaCky 19 79 19%
Total 273 609 31%
Table 1: Figurativeness annotation results. Only
comparisons where all three annotators agree are
considered.
them in our experiments. It is worth noting that
the existing corpora annotated for metaphor can-
not be directly used to study comparisons. For ex-
ample, in TroFi (Birke and Sarkar, 2006), a cor-
pus of 6436 sentences annotated for figurative-
ness, we only find 42 noun-noun comparisons with
sentence-level (thus noisy) figurativeness labels.
4 Linguistic Insights
We now proceed to exploring the linguistic pat-
terns that discriminate figurative from literal com-
parisons. We consider two broad classes of cues,
which we discuss next.
4.1 Domain-specific cues
Figurative language is often used for striking ef-
fects, and comparisons are used to describe new
things in terms of something given (Hanks, 2013).
Since the norms that define what is surprising and
what is well-known vary across domains, we ex-
pect that such contextual information should play
an important role in figurative language detection.
This is a previously unexplored dimension of figu-
rative language, and Amazon product reviews of-
fer a convenient testbed for this intuition since cat-
egory information is provided.
Specificity To estimate whether a compari-
son can be considered striking in a particular
domain?whether it references images or ideas
that are unexpected in its context?we employ a
simple measure of word specificity with respect to
a domain: the ratio of the word frequency within
the domain and the word frequency in all domains
being considered.
5
It should be noted that speci-
ficity is not purely a function of the word, but
5
We measure specificity for the VEHICLE, PROPERTY
and EVENT.
2011
(a) VEHICLE specificity. (b) TOPIC-VEHICLE similarity. (c) Imageability of the PROPERTY.
Figure 1: Distribution of some of the features we use, across literal and figurative comparisons in the test
set. The profile of the plot is a kernel density estimation of the distribution, and the markers indicate the
median and the first and third quartiles.
of the word and the context in which it appears.
A comparison in the music domain that involves
melodies is not surprising:
But the title song really feels like a pretty bland
vocal melody [...]
But the same word can play a very different role
in another context, for example, book reviews:
Her books are like sweet melodies that flow
through your head.
Indeed, the word melody has a specificity of 96%
in the music domain and only of 3% in the books
domain.
An analysis on the labeled data confirms that
literal comparisons do indeed tend to have more
domain-specific VEHICLES (Mann-Whitney U
test, p < 0.01) than figurative ones. Further-
more, the distribution of specificity across both
types of comparisons, as shown in Figure 1a, has
the appearance of a mixture model of general and
specific words. Figurative comparison VEHICLES
largely exhibit only the general component of the
mixture.
6
Domain label An analysis of the annotation re-
sults reveals that the percentage of comparisons
that are figurative differs widely across domains,
as indicated in the last column in Table 1. This
suggests that simply knowing the domain of a
text can serve to adjust some prior expectation
about figurative language presence and therefore
improve detection. We test this hypothesis using
6
The mass around 0.25 in Figure 1a is largely explained
by generic words such as thing, others, nothing, average and
barely specific words like veil, reputation, dream, garbage.
a Z-test comparing all Amazon categories. With
the exception of books and music reviews, that
have similar ratios, all other pairs of categories
show significantly different proportions of figura-
tive comparisons (p < 0.01).
4.2 Domain-agnostic cues
Linguistic studies of figurative language suggest
that there is a fundamental generic notion of fig-
urativeness. We attempt to capture this notion in
the context of comparisons using syntactic and se-
mantic information.
Topic-Vehicle similarity The default role of lit-
eral comparisons is to assert similarity of things.
Therefore, we expect that a high semantic simi-
larity between the TOPIC and the VEHICLE of a
comparison is a sign of literal usage, as we pre-
viously hypothesized in preliminary work (Nicu-
lae, 2013). To test this hypothesis, we compute
TOPIC-VEHICLE similarity using Distributional
Memory (Baroni and Lenci, 2010), a freely avail-
able distributional semantics resource that cap-
tures word relationships through grammatical role
co-occurrence.
By applying this measure to our data, we find
that there is indeed an important difference be-
tween the distributions of TOPIC-VEHICLE simi-
larity in figurative and literal comparisons (shown
in Figure 1b); the means of the two distribu-
tions are significantly different (Mann-Whitney
p < 0.01).
Metaphor-inspired features We also seek to
understand to what extent insights provided by
computational work on metaphor detection can be
2012
more concrete less concrete
more imageable cinnamon, kiss devil, happiness
less imageable casque, pugilist aspect, however
Table 2: Examples of words with high and low
concreteness and imageability scores from the
MRC Psycholinguistic Database.
applied in the context of comparisons. To that end
we consider features shown to provide state of the
art performance in the task of metaphor detection
(Tsvetkov et al., 2014a): abstractness, imageabil-
ity and supersenses.
Abstractness and imageability features are de-
rived from the MRC Psycholinguistic Database
(Coltheart, 1981), a dictionary based on manually
annotated datasets of psycholinguistic norms. Im-
ageability is the property of a word to arouse a
mental image, be it in the form of a mental pic-
ture, sound or any other sense. Concreteness is
defined as ?any word that refers to objects, materi-
als or persons,? while abstractness, at the other end
of the spectrum, is represented by words that can-
not be usually experienced by the senses (Paivio
et al., 1968). Table 2 shows a few examples of
words with high and low concreteness and image-
ability scores. Supersenses are a very coarse form
of meaning representation. Tsvetkov et al. (2014a)
used WordNet (Miller, 1995) semantic classes
for nouns and verbs, for example noun.body,
noun.animal, verb.consumption, or verb.motion.
For adjectives, Tsvetkov et al. (2014b) developed
and made available a novel classification in the
same spirit.
7
We compute abstractness, image-
ability and supersenses for the TOPIC, VEHICLE,
EVENT, and PROPERTY.
8
We concatenate these
features with the raw vector representations of the
constituents, following Tsvetkov et al. (2014a).
We find that such features relate to figurative
comparisons in a meaningful way. For example,
out of all comparisons with explicit properties, fig-
urative comparisons tend to have properties that
7
Following Tsvetkov et al. (2014a) we train a classifier to
predict these features from a vector space representation of a
word. We use the same cross-lingually optimized represen-
tation from Faruqui and Dyer (2014) and a simpler classifier,
a logistic regression, which we find to perform as well as the
random forests used in Tsvetkov et al. (2014a). We treat su-
persense prediction as a multi-label problem and apply a one-
versus-all transformation, effectively learning a linear classi-
fier for each supersense.
8
If the PROPERTY is implicit, all corresponding features
are set to zero. An extra binary feature indicates whether the
PROPERTY is explicit or implicit.
are more imageable (Mann-Whitney p < 0.01), as
illustrated by Figure 1c. This is in agreement with
Hanks (2005), who observed that similes are char-
acterized by their appeal to sensory imagination.
Definiteness We introduce another simple but
effective syntactic cue that relates to concreteness:
the presence of a definite article versus an indefi-
nite one (or none at all). We search for the indefi-
nite articles a and an and the definite article the in
each component of a comparison.
We find that similes tend to have indefinite arti-
cles in the VEHICLE more often and definite arti-
cles less often (Mann-Whitney p < 0.01). In par-
ticular, 59% of comparisons where the VEHICLE
has a indefinite article are figurative, as opposed
to 13% of the comparisons where VEHICLE has a
definite article.
5 Prediction Task
We now turn to the task of predicting whether a
comparison is figurative or literal. Not only does
this task allow us to assess and compare the effi-
ciency of the linguistic cues we discussed, but it is
also highly relevant in the context of natural lan-
guage understanding systems.
We conduct a logistic regression analysis, and
compare the efficiency of the features derived
from our analysis to a bag of words baseline.
In addition to the features inspired by the pre-
viously described linguistic insights, we also try
to computationally capture the lexical usage pat-
terns of comparisons using a version of bag of
words adapted to the comparison structure. In this
slotted bag of words system, features correspond
to occurrence of words within constituents (e.g.,
bright ? PROPERTY).
We perform a stratified split of our compari-
son dataset into equal train and test sets (each set
containing 408 comparisons, out of which 134 are
figurative),
9
and use a 5-fold stratified cross vali-
dation over the training set to choose the optimal
value for the logistic regression regularization pa-
rameter and the type of regularization (?
1
or ?
2
) for
each feature set.
10
9
The entire analysis described in Section 4 is only con-
ducted on the training set. Also, in order to ensure that we are
assessing the performance of the classifier on unseen com-
parisons, we discard from our dataset all those with the same
TOPIC and VEHICLE pair.
10
We use the logistic regression implementation
of liblinear (Fan et al., 2008) wrapped by the
scikit-learn library (Pedregosa et al., 2011).
2013
Model # features Acc. P R F
1
AUC
Bag of words 1970 0.79 0.63 0.84 0.72 0.87
Slotted bag of words 1840 0.80 0.64 0.90 0.75 0.89
Domain-agnostic cues 357 0.81 0.70 0.74 0.72 0.90
only metaphor inspired 345 0.75 0.60 0.72 0.65 0.84
Domain-specific cues 8 0.69 0.51 0.81 0.63 0.76
All linguistic insight cues 365 0.86 0.76 0.83 0.79 0.92
Full 2202 0.88 0.80 0.84 0.82 0.94
Human - 0.96 0.92 0.96 0.94 -
Table 3: Classification performance on the test set for the different sets of features we considered; human
performance is shown for reference.
Classifier performance The performance on
the classification task is summarized in Table 3.
We note that the bag of words baseline is remark-
ably strong, because of common idiomatic simi-
les that can be captured through keywords. Our
full system (which relies on our linguistically in-
spired cues discussed in Section 4 in addition to
slotted bag of words) significantly outperforms the
bag of words baseline and the slotted bag of words
system in terms of accuracy, F
1
score and AUC
(p < 0.05),
11
suggesting that linguistic insights
complement idiomatic simile matching. Impor-
tantly, a system using only our linguistic insight
cues also significantly improves over the baseline
in terms of accuracy and AUC and it is not signif-
icantly different from the full system in terms of
performance, in spite of having about an order of
magnitude fewer features. It is also worth noting
that the domain-specific cues play an important
role in bringing the performance to this level by
capturing a different aspect of what it means for a
comparison to be figurative.
The features used by the state of the art
metaphor detection system of Tsvetkov et al.
(2014a), adapted to the comparison structure, per-
form poorly by themselves and do not improve
significantly over the baseline. This is consis-
tent with the theoretical motivation that figura-
tiveness in comparisons requires special compu-
tational treatment, as discussed in Section 1. Fur-
thermore, the linguistic insight features not only
significantly outperform the metaphor inspired
features (p < 0.05), but are also better at exploit-
ing larger amounts of data, as shown in Figure 2.
11
All statistical significance results in this paragraph are
obtained from 5000 bootstrap samples.
Figure 2: Learning curves. Each point is obtained
by fitting a model on 10 random subsets of the
training set. Error bars show 95% confidence in-
tervals.
Comparison to human performance To gauge
how well humans would perform at the classifica-
tion task on the actual test data, we perform an-
other Amazon Mechanical Turk evaluation on 140
examples from the test set. For the evaluation,
we use majority voting between the three anno-
tators,
12
and compare to the agreed labels in the
dataset. Estimated human accuracy is 96%, plac-
ing our full systemwithin 10% of human accuracy.
Feature analysis The predictive analysis we
perform allows us to investigate to what extent the
features inspired by our linguistic insights have
discriminative power, and whether they actually
cover different aspects of figurativeness.
12
Majority voting helps account for the noise inherent to
crowdsourced annotation, which is less accurate than profes-
sional annotation. Taking the less optimistic invididual turker
answers, human performance is on the same level as our full
system.
2014
Feature Coef. Example where the feature is positively activated
TOPIC-VEHICLE similarity ?11.3 the older man was wiser and stronger than the boy
VEHICLE specificity ?5.8 the cord is more durable than the adapter [Electronics]
VEHICLE imageability 4.9 the explanations are as clear as mud
VEHICLE communication supersense ?4.6 the book reads like six short articles
VEHICLE indefiniteness 4.0 his fame drew foreigners to him like a magnet
life ? VEHICLE 7.1 the hero is truly larger than life: godlike, yet flawed
picture ? VEHICLE ?6.0 the necklace looks just like the picture
other ? VEHICLE ?5.9 this one is just as nice as the other
others ? VEHICLE ?5.5 some songs are more memorable than others
crap ? VEHICLE 4.7 the headphones sounded like crap
Table 4: Top 5 linguistic insight features (top) and slotted bag of words features (bottom) in the full model
and their logistic regression coefficients. A positive coefficient means the feature indicates figurativeness.
Table 4 shows the best linguistic insight and
slotted bag of words features selected by the full
model. The strongest feature by far is the seman-
tic similarity between the TOPIC and the VEHI-
CLE. By itself, this feature gets 70% accuracy and
61% F
1
score.
The rest of the top features involve mostly the
VEHICLE. This suggests that the VEHICLE is the
most informative element of a comparison when it
comes to figurativeness. Features involving other
constituents also get selected, but with slightly
lower weights, not making it to the top.
VEHICLE specificity is one of the strongest fea-
tures, with positive values indicating literal com-
parisons. This confirms our intuition that domain
information is important to discriminate figurative
from literal language.
Of the adapted metaphor features, the noun
communication supersense and the imageability
of the VEHICLE make it to the top. Nouns with
low communication rating occurring in the train-
ing set include puddles, arrangements, carbohy-
drates while nouns with high communication rat-
ing include languages and subjects.
Presence of an indefinite article in the VEHICLE
is a strong indicator of figurativeness. By them-
selves, the definiteness and indefiniteness features
perform quite well, attaining 78% accuracy and
67% F
1
score.
The salient bag of words features correspond to
specific types of comparisons. The words other
and others in the VEHICLE indicate comparisons
between the same kind of arguments, for exam-
ple some songs are more memorable than others,
and these are likely to be literal. The word pic-
ture is specific to the review setting, as products
are accompanied by photos, and for certain kinds
of products, the resemblance of the product with
the image is an important factor for potential buy-
ers.
13
The bag of words systems are furthermore
able to learn idiomatic comparisons by identify-
ing common figurative VEHICLES such as life and
crap, corresponding to fixed expressions such as
larger than life.
Error analysis Many of the errors made by our
full system involve indirect semantic mechanisms
such as metonymy. For example, the false pos-
itive the typeface was larger than most books
really means larger than the typefaces found in
most books, but without the implicit expansion the
meaning can appear figurative. A similar kind of
ellipsis makes the example a lot [of songs] are
even better than sugar be wrongly classified as
literal. Another source of error is polysemy. Ex-
amples like the rejuvelac formula is about 10 times
better than yogurt are misclassified because of the
multiple meanings of the word formula, one being
closely related to yogurt and food, but the more
common ones being general and abstract, suggest-
ing figurativeness.
6 Social Correlates
The advantage of studying comparisons situated
in a social context is that we can understand how
their usage interacts with internal and external hu-
man factors. An internal factor is the sentiment of
13
This feature is highly correlated with the domain: it ap-
pears 25 times in the training set, 24 of which in the jewelery
domain and once in book reviews.
2015
(a) Figurative comparisons are more likely to be found in re-
views with strongly polarized sentiment.
(b) Helpful reviews are less likely to contain figurative compar-
isons.
Figure 3: Interaction between figurative language and social context aspects. Error bars show 95%
confidence intervals. The dashed horizontal line marks the average proportion of figurative comparisons.
In Figure 3b the average proportion is different because we only consider reviews rated by at least 10
readers.
the user towards the reviewed product, indicated
by the star rating of the review. An external factor
present in the data is how helpful the review is per-
ceived by other users. In this section we analyze
how these factors interact with figurative language
in comparisons.
To gain insight about fine grained interactions
with human factors at larger scale, we use our clas-
sifier to find over 80,000 figurative and literal com-
parisons from the same four categories. The trends
we reveal also hold significantly on the manually
annotated data.
Sentiment While it was previously noted that
similes often transmit strong affect (Hanks, 2005;
Veale, 2012a; Veale, 2012b), the connection be-
tween figurativeness and sentiment was never em-
pirically validated. The setting of product reviews
is convenient for investigating this issue, since
the star ratings associated with the reviews can
be used as sentiment labels. We find that com-
parisons are indeed significantly more likely to
be figurative when the users express strong opin-
ions, i.e., in one-star or five-star reviews (Mann-
Whitney p < 0.02 on the manually annotated
data). Figure 3a shows how the proportion of fig-
urative comparisons varies with the polarity of the
review.
Helpfulness It is also interesting to understand
to what extent figurative language relates to the
external perception of the content in which it ap-
pears. We find that comparisons in helpful re-
views
14
are less likely to be figurative. Figure 3b
shows a near-constant high ratio of figurative com-
parisons among unhelpful and average reviews; as
helpfulness increases, figurative comparisons be-
come less frequent. We further validate that this
effect is not a confound of the distribution of help-
fulness ratings across reviews of different polarity
by controlling for the star rating: given a fixed star
rating, the proportion of figurative comparisons is
still lower in helpful (helpfulness over 50%) than
in unhelpful (helpfulness under 50%) reviews; this
difference is significant (Mann-Whitney p < 0.01)
for all classes of ratings except one-star. The
size of the manually annotated data does not al-
low for star rating stratification, but the overall dif-
ference is statistically significant (Mann-Whitney
p < 0.01). This result encourages further exper-
imentation to determine whether there is a causal
link between the use of figurative language in user
generated content and its external perception.
7 Conclusions and Future Work
This work proposes a computational study of fig-
urative language in comparisons. Starting from
a new dataset of naturally occurring comparisons
with figurativeness annotation (which we make
publicly available) we explore linguistic patterns
that are indicative of similes. We show that these
14
In order to have reliable helpfulness scores, we only con-
sider reviews that have been rated by at least by ten readers.
2016
insights can be successfully operationalized in a
new prediction task: distinguishing literal from
figurative comparisons. Our system reaches ac-
curacy that is within 10% of human performance,
and is outperforming a state of the art metaphor
detection system, thus confirming the need for
a computational approach tailored specifically to
comparisons. While we take a data-driven ap-
proach, our annotated dataset can be useful for
more theoretical studies of the kinds of compar-
isons and similes people use.
We discover that domain knowledge is an im-
portant factor in identifying similes. This suggests
that future work on automatic detection of figura-
tive language should consider contextual parame-
ters such as the topic and community where the
content appears.
Furthermore, we are the first to tie figurative
language to the social context in which it is pro-
duced and show its relation to internal and exter-
nal human factors such as opinion sentiment and
helpfulness. Future investigation into the causal
effects of these interactions could lead to a better
understanding of the role of figurative language in
persuasion and rhetorics.
In our work, we consider common noun TOP-
ICS and VEHICLES and adjectival PROPERTIES.
This is the most typical case, but supporting other
parts of speech?such as proper nouns, pronouns,
and adverbs?can make a difference in many ap-
plications. Capturing compositional interaction
between the parts of the comparison could lead to
more flexible models that give less weight to the
VEHICLE.
This study is also the first to estimate how
prevalent similes are in the wild, and reports that
about one third of the comparisons we consider are
figurative. This is suggestive of the need to build
systems that can properly process figurative com-
parisons in order to correctly harness the semantic
information encapsulated in comparisons.
Acknowledgements
We would like to thank Yulia Tsvetkov for con-
structive discussion about figurative language and
about her and her co-authors? work. We are grate-
ful for the suggestions of Patrick Hanks, Con-
stantin Or?asan, Sylviane Cardey, Izabella Thomas,
Ekaterina Shutova, Tony Veale, and Niket Tan-
don. We extend our gratitude to Julian McAuley
for preparing and sharing the Amazon review
dataset. We are thankful to the anonymous review-
ers, whose comments were like a breath of fresh
air. We acknowledge the help of the Amazon Me-
chanical Turk annotators and of the MPI-SWS stu-
dents involved in pilot experiments.
Vlad Niculae was supported in part by the
European Commission, Education & Training,
Erasmus Mundus: EMMC 2008-0083, Erasmus
Mundus Masters in NLP & HLT.
References
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional memory: A general framework for
corpus-based semantics. Computational Linguis-
tics, 36(4):673?721.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky wide web:
A collection of very large linguistically processed
web-crawled corpora. Language Resources and
Evaluation, 43(3):209?226.
Louise Shabat Bethlehem. 1996. Simile and figurative
language. Poetics Today, 17(2):203?240.
Julia Birke and Anoop Sarkar. 2006. A clustering ap-
proach for nearly unsupervised recognition of non-
literal language. In Proceedings of EACL.
Hugh Bredin. 1998. Comparisons and similes. Lin-
gua, 105(1):67?78.
Max Coltheart. 1981. The MRC psycholinguistic
database. The Quarterly Journal of Experimental
Psychology, 33(4):497?505.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Manaal Faruqui and Chris Dyer. 2014. Improving
vector space word representations using multilingual
correlation. In Proceedings of EACL.
Eric N Forsyth and Craig H Martell. 2007. Lexical and
discourse analysis of online chat dialog. In Proceed-
ings of ICSC.
Patrick Hanks. 2005. Similes and sets: The English
preposition ?like?. In R. Blatn?a and V. Petkevic, ed-
itors, Languages and Linguistics: Festschrift for Fr.
Cermak. Charles University, Prague.
Patrick Hanks. 2006. Metaphoricity is grad-
able. Trends in Linguistic Studies and Monographs,
171:17.
Patrick Hanks. 2012. The roles and structure of com-
parisons, similes, and metaphors in natural language
(an analogical system). Presented at the Stockholm
Metaphor Festival.
2017
Patrick Hanks. 2013. Lexical Analysis: Norms and
Exploitations. MIT Press.
Dirk Hovy, Shashank Srivastava, Sujay Kumar Jauhar,
Mrinmaya Sachan, Kartik Goyal, Huiying Li, Whit-
ney Sanders, and Eduard Hovy. 2013. Identifying
metaphorical word use with tree kernels. In Pro-
ceedings of the NAACL Workshop on Metaphors for
NLP.
M. Israel, J.R. Harding, and V. Tobin. 2004. On simile.
Language, Culture, and Mind. CSLI Publications.
Nitin Jindal and Bing Liu. 2006. Identifying compara-
tive sentences in text documents. In Proceedings of
SIGIR.
George Lakoff and Mark Johnson. 1980. Metaphors
we live by. University of Chicago Press.
Bin Li, Jiajun Chen, and Yingjie Zhang. 2012. Web
based collection and comparison of cognitive prop-
erties in English and Chinese. In Proceedings of the
Joint Workshop on Automatic Knowledge Base Con-
struction and Web-scale Knowledge Extraction.
Christoph Lofi, Christian Nieke, and Nigel Collier.
2014. Discriminating rhetorical analogies in social
media. In Proceedings of EACL.
Andr?e FT Martins, Noah A Smith, Eric P Xing, Pe-
dro MQ Aguiar, and M?ario AT Figueiredo. 2010.
Turbo parsers: Dependency parsing by approximate
variational inference. In Proceedings of EMNLP.
Julian McAuley and Jure Leskovec. 2013. Hidden fac-
tors and hidden topics: Understanding rating dimen-
sions with review text. In Proceedings of RecSys.
George A Miller. 1995. WordNet: a lexical
database for English. Communications of the ACM,
38(11):39?41.
Rosamund Moon. 2008. Conventionalized as-similes
in English: A problem case. International Journal
of Corpus Linguistics, 13(1):3?37.
Rosamund Moon. 2011. Simile and dissimilarity.
Journal of Literary Semantics, 40(2):133?157.
Robert Munro, Steven Bethard, Victor Kuperman,
Vicky Tzuyin Lai, Robin Melnick, Christopher
Potts, Tyler Schnoebelen, and Harry Tily. 2010.
Crowdsourcing and language studies: The new gen-
eration of linguistic data. In Proceedings of the
NAACL Workshop on Creating Speech and Lan-
guage Data with Amazon?s Mechanical Turk.
Vlad Niculae. 2013. Comparison pattern matching and
creative simile recognition. In Proceedings of the
Joint Symposium on Semantic Processing.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL-HLT.
Allan Paivio, John C Yuille, and Stephen A Madigan.
1968. Concreteness, imagery, and meaningfulness
values for 925 nouns. Journal of Experimental Psy-
chology, 76(1p2):1.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, et al. 2011. Scikit-learn:
Machine learning in Python. Journal of Machine
Learning Research, 12:2825?2830.
Carlos Roncero, John M Kennedy, and Ron Smyth.
2006. Similes on the internet have explanations.
Psychonomic Bulletin & Review, 13(1):74?77.
Ekaterina Shutova and Lin Sun. 2013. Unsupervised
metaphor identification using hierarchical graph fac-
torization clustering. In Proceedings of NAACL-
HLT.
Ekaterina Shutova. 2010. Models of metaphor in NLP.
In Proceedings of ACL.
Niket Tandon, Gerard de Melo, and Gerhard Weikum.
2014. Smarter than you think: Acquiring compara-
tive commonsense knowledge from the web. In Pro-
ceedings of AAAI.
Yulia Tsvetkov, Leonid Boytsov, Anatole Gershman,
Eric Nyberg, and Chris Dyer. 2014a. Metaphor de-
tection with cross-lingual model transfer. In Pro-
ceedings of ACL.
Yulia Tsvetkov, Nathan Schneider, Dirk Hovy, Archna
Bhatia, Manaal Faruqui, and Chris Dyer. 2014b.
Augmenting English adjective senses with super-
senses. In Proceedings of LREC.
Peter D Turney, Yair Neuman, Dan Assaf, and Yohai
Cohen. 2011. Literal and metaphorical sense iden-
tification through concrete and abstract context. In
Proceedings of EMNLP.
Tony Veale and Yanfen Hao. 2008. A context-sensitive
framework for lexical ontologies. Knowledge Engi-
neering Review, 23(1):101?115.
Tony Veale and Yanfen Hao. 2011. Exploiting ready-
mades in linguistic creativity: A system demonstra-
tion of the Jigsaw Bard. In Proceedings of ACL (Sys-
tem Demonstrations).
Tony Veale and Guofu Li. 2013. Creating similarity:
Lateral thinking for vertical similarity judgments. In
Proceedings of ACL.
Tony Veale. 2012a. A computational exploration of
creative similes. Metaphor in Use: Context, Cul-
ture, and Communication, 38:329.
Tony Veale. 2012b. A context-sensitive, multi-faceted
model of lexico-conceptual affect. In Proceedings
of ACL.
2018
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 365?368,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
For the sake of simplicity:
Unsupervised extraction of lexical simplifications from Wikipedia
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-Mizil and Lillian Lee
my89@cornell.edu, bopang@yahoo-inc.com, cristian@cs.cornell.edu, llee@cs.cornell.edu
Abstract
We report on work in progress on extract-
ing lexical simplifications (e.g., ?collaborate?
? ?work together?), focusing on utilizing
edit histories in Simple English Wikipedia for
this task. We consider two main approaches:
(1) deriving simplification probabilities via an
edit model that accounts for a mixture of dif-
ferent operations, and (2) using metadata to
focus on edits that are more likely to be sim-
plification operations. We find our methods
to outperform a reasonable baseline and yield
many high-quality lexical simplifications not
included in an independently-created manu-
ally prepared list.
1 Introduction
Nothing is more simple than greatness; indeed, to be
simple is to be great. ?Emerson, Literary Ethics
Style is an important aspect of information pre-
sentation; indeed, different contexts call for differ-
ent styles. Here, we consider an important dimen-
sion of style, namely, simplicity. Systems that can
rewrite text into simpler versions promise to make
information available to a broader audience, such as
non-native speakers, children, laypeople, and so on.
One major effort to produce such text is the
Simple English Wikipedia (henceforth SimpleEW)1,
a sort of spin-off of the well-known English
Wikipedia (henceforth ComplexEW) where hu-
man editors enforce simplicity of language through
rewriting. The crux of our proposal is to learn lexical
simplifications from SimpleEW edit histories, thus
leveraging the efforts of the 18K pseudonymous in-
dividuals who work on SimpleEW. Importantly, not
all the changes on SimpleEW are simplifications; we
thus also make use of ComplexEW edits to filter out
non-simplifications.
Related work and related problems Previous
work usually involves general syntactic-level trans-
1http://simple.wikipedia.org
formation rules [1, 9, 10].2 In contrast, we explore
data-driven methods to learn lexical simplifications
(e.g., ?collaborate? ? ?work together?), which are
highly specific to the lexical items involved and thus
cannot be captured by a few general rules.
Simplification is strongly related to but distinct
from paraphrasing and machine translation (MT).
While it can be considered a directional form of
the former, it differs in spirit because simplification
must trade off meaning preservation (central to para-
phrasing) against complexity reduction (not a con-
sideration in paraphrasing). Simplification can also
be considered to be a form of MT in which the two
?languages? in question are highly related. How-
ever, note that ComplexEW and SimpleEW do not
together constitute a clean parallel corpus, but rather
an extremely noisy comparable corpus. For ex-
ample, Complex/Simple same-topic document pairs
are often written completely independently of each
other, and even when it is possible to get good
sentence alignments between them, the sentence
pairs may reflect operations other than simplifica-
tion, such as corrections, additions, or edit spam.
Our work joins others in using Wikipedia revi-
sions to learn interesting types of directional lexical
relations, e.g, ?eggcorns?3 [7] and entailments [8].
2 Method
As mentioned above, a key idea in our work is to
utilize SimpleEW edits. The primary difficulty in
working with these modifications is that they include
not only simplifications but also edits that serve
other functions, such as spam removal or correction
of grammar or factual content (?fixes?). We describe
two main approaches to this problem: a probabilis-
tic model that captures this mixture of different edit
operations (?2.1), and the use of metadata to filter
out undesirable revisions (?2.2).
2One exception [5] changes verb tense and replaces pro-
nouns. Other lexical-level work focuses on medical text [4, 2],
or uses frequency-filtered WordNet synonyms [3].
3A type of lexical corruption, e.g., ?acorn???eggcorn?.
365
2.1 Edit model
We say that the kth article in a Wikipedia corre-
sponds to (among other things) a title or topic (e.g.,
?Cat?) and a sequence ~dk of article versions caused
by successive edits. For a given lexical item or
phrase A, we write A ? ~dk if there is any version
in ~dk that contains A. From each ~dk we extract a
collection ek = (ek,1, ek,2, . . . , ek,nk) of lexical edit
instances, repeats allowed, where ek,i = A ? a
means that phrase A in one version was changed to
a in the next, A 6= a; e.g., ?stands for? ? ?is the
same as?. (We defer detailed description of how we
extract lexical edit instances from data to ?3.1.) We
denote the collection of ~dk in ComplexEW and Sim-
pleEW as C and S, respectively.
There are at least four possible edit operations: fix
(o1), simplify (o2), no-op (o3), or spam (o4). How-
ever, for this initial work we assume P (o4) = 0.4
Let P (oi | A) be the probability that oi is applied
to A, and P (a | A, oi) be the probability of A ? a
given that the operation is oi. The key quantities of
interest are P (o2 | A) in S, which is the probability
thatA should be simplified, and P (a | A, o2), which
yields proper simplifications of A. We start with an
equation that models the probability that a phrase A
is edited into a:
P (a | A) =
?
oi??
P (oi | A)P (a | A, oi), (1)
where ? is the set of edit operations. This involves
the desired parameters, which we solve for by esti-
mating the others from data, as described next.
Estimation Note that P (a | A, o3) = 0 if A 6= a.
Thus, if we have estimates for o1-related probabili-
ties, we can derive o2-related probabilities via Equa-
tion 1. To begin with, we make the working as-
sumption that occurrences of simplification in Com-
plexEW are negligible in comparison to fixes. Since
we are also currently ignoring edit spam, we thus
assume that only o1 edits occur in ComplexEW.5
Let fC(A) be the fraction of ~dk in C
containing A in which A is modified:
fC(A) =
|{~dk?C|?a,i such that ek,i=A?a}|
|{~dk?C|A?~dk}|
.
4Spam/vandalism detection is a direction for future work.
5This assumption also provides useful constraints to EM,
which we plan to apply in the future, by reducing the number of
parameter settings yielding the same likelihood.
We similarly define fS(A) on ~dk in S. Note that we
count topics (version sequences), not individual ver-
sions: if A appears at some point and is not edited
until 50 revisions later, we should not conclude
that A is unlikely to be rewritten; for example, the
intervening revisions could all be minor additions,
or part of an edit war.
If we assume that the probability of any particular
fix operation being applied in SimpleEW is propor-
tional to that in ComplexEW? e.g., the SimpleEW
fix rate might be dampened because already-edited
ComplexEW articles are copied over ? we have6
P? (o1 | A) = ?fC(A)
where 0 ? ? ? 1. Note that in SimpleEW,
P (o1 ? o2 | A) = P (o1 | A) + P (o2 | A),
where P (o1 ? o2 | A) is the probability that A is
changed to a different word in SimpleEW, which we
estimate as P? (o1 ? o2 | A) = fS(A). We then set
P?(o2 | A) = max (0, fS(A)? ?fC(A)).
Next, under our working assumption, we estimate
the probability of A being changed to a as a fix
by the proportion of ComplexEW edit instances that
rewrite A to a:
P? (a | A, o1) =
|{(k, i) pairs | ek,i = A? a ? ~dk ? C}|
?
a? |{(k, i) pairs | ek,i = A? a
? ? ~dk ? C}|
.
A natural estimate for the conditional probability
of A being rewritten to a under any operation type
is based on observations of A ? a in SimpleEW,
since that is the corpus wherein both operations are
assumed to occur:
P? (a | A) =
|{(k, i) pairs | ek,i = A? a ? ~dk ? S}|
?
a? |{(k, i) pairs | ek,i = A? a
? ? ~dk ? S}|
.
Thus, from (1) we get that for A 6= a:
P?(a | A,o2) =
P?(a | A)? P?(o1 | A)P?(a | A,o1)
P?(o2 | A)
.
2.2 Metadata-based methods
Wiki editors have the option of associating a com-
ment with each revision, and such comments some-
times indicate the intent of the revision. We there-
fore sought to use comments to identify ?trusted?
6Throughout, ?hats? denote estimates.
366
revisions wherein the extracted lexical edit instances
(see ?3.1) would be likely to be simplifications.
Let ~rk = (r1k, . . . , r
i
k, . . .) be the sequence of revi-
sions for the kth article in SimpleEW, where rik is the
set of lexical edit instances (A ? a) extracted from
the ith modification of the document. Let cik be the
comment that accompanies rik, and conversely, let
R(Set) = {rik|c
i
k ? Set}.
We start with a seed set of trusted comments,
Seed. To initialize it, we manually inspected a small
sample of the 700K+ SimpleEW revisions that bear
comments, and found that comments containing a
word matching the regular expression *simpl* (e.g,
?simplify?) seem promising. We thus set Seed :=
{ ? simpl?} (abusing notation).
The SIMPL method Given a set of trusted revi-
sions TRev (in our case TRev = R(Seed)), we
score each A ? a ? TRev by the point-wise mu-
tual information (PMI) between A and a.7 We write
RANK(TRev) to denote the PMI-based ranking of
A? a ? TRev, and use SIMPL to denote our most
basic ranking method, RANK(R(Seed)).
Two ideas for bootstrapping We also considered
bootstrapping as a way to be able to utilize revisions
whose comments are not in the initial Seed set.
Our first idea was to iteratively expand the set
of trusted comments to include those that most of-
ten accompany already highly ranked simplifica-
tions. Unfortunately, our initial implementations in-
volved many parameters (upper and lower comment-
frequency thresholds, number of highly ranked sim-
plifications to consider, number of comments to add
per iteration), making it relatively difficult to tune;
we thus omit its results.
Our second idea was to iteratively expand the
set of trusted revisions, adding those that contain
already highly ranked simplifications. While our
initial implementation had fewer parameters than
the method sketched above, it tended to terminate
quickly, so that not many new simplifications were
found; so, again, we do not report results here.
An important direction for future work is to differ-
entially weight the edit instances within a revision,
as opposed to placing equal trust in all of them; this
7PMI seemed to outperform raw frequency and conditional
probability.
could prevent our bootstrapping methods from giv-
ing common fixes (e.g., ?a?? ?the?) high scores.
3 Evaluation8
3.1 Data
We obtained the revision histories of both Sim-
pleEW (November 2009 snapshot) and ComplexEW
(January 2008 snapshot). In total, ?1.5M revisions
for 81733 SimpleEW articles were processed (only
30% involved textual changes). For ComplexEW,
we processed ?16M revisions for 19407 articles.
Extracting lexical edit instances. For each ar-
ticle, we aligned sentences in each pair of adja-
cent versions using tf-idf scores in a way simi-
lar to Nelken and Shieber [6] (this produced sat-
isfying results because revisions tended to repre-
sent small changes). From the aligned sentence
pairs, we obtained the aforementioned lexical edit
instances A ? a. Since the focus of our study
was not word alignment, we used a simple method
that identified the longest differing segments (based
on word boundaries) between each sentence, except
that to prevent the extraction of entire (highly non-
matching) sentences, we filtered out A ? a pairs if
either A or a contained more than five words.
3.2 Comparison points
Baselines RANDOM returns lexical edit instances
drawn uniformly at random from among those ex-
tracted from SimpleEW. FREQUENT returns the
most frequent lexical edit instances extracted from
SimpleEW.
Dictionary of simplifications The SimpleEW ed-
itor ?Spencerk? (Spencer Kelly) has assembled a list
of simple words and simplifications using a combi-
nation of dictionaries and manual effort9. He pro-
vides a list of 17,900 simple words ? words that do
not need further simplification ? and a list of 2000
transformation pairs. We did not use Spencerk?s set
as the gold standard because many transformations
we found to be reasonable were not on his list. In-
stead, we measured our agreement with the list of
transformations he assembled (SPLIST).
8Results at http://www.cs.cornell.edu/home/llee/data/simple
9http://www.spencerwaterbed.com/soft/simple/about.html
367
3.3 Preliminary results
The top 100 pairs from each system (edit model10
and SIMPL and the two baselines) plus 100 ran-
domly selected pairs from SPLIST were mixed and
all presented in random order to three native English
speakers and three non-native English speakers (all
non-authors). Each pair was presented in random
orientation (i.e., either as A ? a or as a ? A),
and the labels included ?simpler?, ?more complex?,
?equal?, ?unrelated?, and ??? (?hard to judge?). The
first two labels correspond to simplifications for the
orientations A ? a and a ? A, respectively. Col-
lapsing the 5 labels into ?simplification?, ?not a sim-
plification?, and ??? yields reasonable agreement
among the 3 native speakers (? = 0.69; 75.3% of the
time all three agreed on the same label). While we
postulated that non-native speakers11 might be more
sensitive to what was simpler, we note that they dis-
agreed more than the native speakers (? = 0.49) and
reported having to consult a dictionary. The native-
speaker majority label was used in our evaluations.
Here are the results; ?-x-y? means that x and y are
the number of instances discarded from the precision
calculation for having no majority label or majority
label ???, respectively:
Method Prec@100 # of pairs
SPLIST 86% (-0-0) 2000
Edit model 77% (-0-1) 1079
SIMPL 66% (-0-0) 2970
FREQUENT 17% (-1-7) -
RANDOM 17% (-1-4) -
Both baselines yielded very low precisions ?
clearly not all (frequent) edits in SimpleEW were
simplifications. Furthermore, the edit model yielded
higher precision than SIMPL for the top 100 pairs.
(Note that we only examined one simplification per
A for those A where P? (o2 | A) was well-defined;
thus ?# of pairs? does not directly reflect the full
potential recall that either method can achieve.)
Both, however, produced many high-quality pairs
(62% and 71% of the correct pairs) not included in
SPLIST. We also found the pairs produced by these
two systems to be complementary to each other. We
10We only considered those A such that freq(A ? ?) >
1 ? freq(A) > 100 on both SimpleEW and ComplexEW. The
final top 100 A ? a pairs were those with As with the highest
P (o2 | A). We set ? = 1.
11Native languages: Russian; Russian; Russian and Kazakh.
believe that these two approaches provide a good
starting point for further explorations.
Finally, some examples of simplifications found
by our methods: ?stands for? ? ?is the same
as?, ?indigenous? ? ?native?, ?permitted? ? ?al-
lowed?, ?concealed? ? ?hidden?, ?collapsed? ?
?fell down?, ?annually?? ?every year?.
3.4 Future work
Further evaluation could include comparison with
machine-translation and paraphrasing algorithms. It
would be interesting to use our proposed estimates
as initialization for EM-style iterative re-estimation.
Another idea would be to estimate simplification pri-
ors based on a model of inherent lexical complexity;
some possible starting points are number of sylla-
bles (which is used in various readability formulae)
or word length.
Acknowledgments We first wish to thank Ainur Yessenalina
for initial investigations and helpful comments. We are
also thankful to R. Barzilay, T. Bruce, C. Callison-Burch, J.
Cantwell, M. Dredze, C. Napoles, E. Gabrilovich, & the review-
ers for helpful comments; W. Arms and L. Walle for access to
the Cornell Hadoop cluster; J. Cantwell for access to computa-
tional resources; R. Hwa & A. Owens for annotation software;
M. Ulinski for preliminary explorations; J. Cantwell, M. Ott, J.
Silverstein, J. Yatskar, Y. Yatskar, & A. Yessenalina for annota-
tions. Supported by NSF grant IIS-0910664.
References
[1] R. Chandrasekar, B. Srinivas. Automatic induction of rules
for text simplification. Knowledge-Based Systems, 1997.
[2] L. Dele?ger, P. Zweigenbaum. Extracting lay paraphrases
of specialized expressions from monolingual comparable
medical corpora. Workshop on Building and Using Com-
parable Corpora, 2009.
[3] S. Devlin, J. Tait. The use of a psycholinguistic database in
the simplification of text for aphasic readers. In Linguistic
Databases, 1998.
[4] N. Elhadad, K. Sutaria. Mining a lexicon of technical terms
and lay equivalents. Workshop on BioNLP, 2007.
[5] B. Beigman Klebanov, K. Knight, D. Marcu. Text simplifi-
cation for information-seeking applications. OTM Confer-
ences, 2004.
[6] R. Nelken, S. M. Shieber. Towards robust context-sensitive
sentence alignment for monolingual corpora. EACL, 2006.
[7] R. Nelken, E. Yamangil. Mining Wikipedia?s article re-
vision history for training computational linguistics algo-
rithms. WikiAI, 2008.
[8] E. Shnarch, L. Barak, I. Dagan. Extracting lexical reference
rules from Wikipedia. ACL, 2009.
[9] A. Siddharthan, A. Nenkova, K. McKeown. Syntactic
simplification for improving content selection in multi-
document summarization. COLING, 2004.
[10] D. Vickrey, D. Koller. Sentence simplification for seman-
tic role labeling/ ACL, 2008.
368
Proceedings of the ACL 2010 Conference Short Papers, pages 247?252,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Don?t ?have a clue??
Unsupervised co-learning of downward-entailing operators
Cristian Danescu-Niculescu-Mizil and Lillian Lee
Department of Computer Science, Cornell University
cristian@cs.cornell.edu, llee@cs.cornell.edu
Abstract
Researchers in textual entailment have
begun to consider inferences involving
downward-entailing operators, an inter-
esting and important class of lexical items
that change the way inferences are made.
Recent work proposed a method for learn-
ing English downward-entailing operators
that requires access to a high-quality col-
lection of negative polarity items (NPIs).
However, English is one of the very few
languages for which such a list exists. We
propose the first approach that can be ap-
plied to the many languages for which
there is no pre-existing high-precision
database of NPIs. As a case study, we
apply our method to Romanian and show
that our method yields good results. Also,
we perform a cross-linguistic analysis that
suggests interesting connections to some
findings in linguistic typology.
1 Introduction
Cristi: ?Nicio? ... is that adjective you?ve mentioned.
Anca: A negative pronominal adjective.
Cristi: You mean there are people who analyze that
kind of thing?
Anca: The Romanian Academy.
Cristi: They?re crazy.
?From the movie Police, adjective
Downward-entailing operators are an interest-
ing and varied class of lexical items that change
the default way of dealing with certain types of
inferences. They thus play an important role in
understanding natural language [6, 18?20, etc.].
We explain what downward entailing means by
first demonstrating the ?default? behavior, which
is upward entailing. The word ?observed? is an
example upward-entailing operator: the statement
(i) ?Witnesses observed opium use.?
implies
(ii) ?Witnesses observed narcotic use.?
but not vice versa (we write i ? ( 6?) ii). That
is, the truth value is preserved if we replace the
argument of an upward-entailing operator by a su-
perset (a more general version); in our case, the set
?opium use? was replaced by the superset ?narcotic
use?.
Downward-entailing (DE) (also known as
downward monotonic or monotone decreasing)
operators violate this default inference rule: with
DE operators, reasoning instead goes from ?sets to
subsets?. An example is the word ?bans?:
?The law bans opium use?
6? (?)
?The law bans narcotic use?.
Although DE behavior represents an exception to
the default, DE operators are as a class rather com-
mon. They are also quite diverse in sense and
even part of speech. Some are simple negations,
such as ?not?, but some other English DE opera-
tors are ?without?, ?reluctant to?, ?to doubt?, and
?to allow?.1 This variety makes them hard to ex-
tract automatically.
Because DE operators violate the default ?sets
to supersets? inference, identifying them can po-
tentially improve performance in many NLP tasks.
Perhaps the most obvious such tasks are those in-
volving textual entailment, such as question an-
swering, information extraction, summarization,
and the evaluation of machine translation [4]. Re-
searchers are in fact beginning to build textual-
entailment systems that can handle inferences in-
volving downward-entailing operators other than
simple negations, although these systems almost
all rely on small handcrafted lists of DE operators
[1?3, 15, 16].2 Other application areas are natural-
language generation and human-computer interac-
tion, since downward-entailing inferences induce
1Some examples showing different constructions for ana-
lyzing these operators: ?The defendant does not own a blue
car? 6? (?) ?The defendant does not own a car?; ?They are
reluctant to tango? 6? (?) ?They are reluctant to dance?;
?Police doubt Smith threatened Jones? 6? (?) ?Police doubt
Smith threatened Jones or Brown?; ?You are allowed to use
Mastercard? 6? (?) ?You are allowed to use any credit card?.
2The exception [2] employs the list automatically derived
by Danescu-Niculescu-Mizil, Lee, and Ducott [5], described
later.
247
greater cognitive load than inferences in the oppo-
site direction [8].
Most NLP systems for the applications men-
tioned above have only been deployed for a small
subset of languages. A key factor is the lack
of relevant resources for other languages. While
one approach would be to separately develop a
method to acquire such resources for each lan-
guage individually, we instead aim to ameliorate
the resource-scarcity problem in the case of DE
operators wholesale: we propose a single unsuper-
vised method that can extract DE operators in any
language for which raw text corpora exist.
Overview of our work Our approach takes the
English-centric work of Danescu-Niculescu-Mizil
et al [5] ? DLD09 for short ? as a starting point,
as they present the first and, until now, only al-
gorithm for automatically extracting DE operators
from data. However, our work departs signifi-
cantly from DLD09 in the following key respect.
DLD09 critically depends on access to a high-
quality, carefully curated collection of negative
polarity items (NPIs) ? lexical items such as
?any?, ?ever?, or the idiom ?have a clue? that tend
to occur only in negative environments (see ?2
for more details). DLD09 use NPIs as signals of
the occurrence of downward-entailing operators.
However, almost every language other than En-
glish lacks a high-quality accessible NPI list.
To circumvent this problem, we introduce a
knowledge-lean co-learning approach. Our al-
gorithm is initialized with a very small seed set
of NPIs (which we describe how to generate), and
then iterates between (a) discovering a set of DE
operators using a collection of pseudo-NPIs ? a
concept we introduce ? and (b) using the newly-
acquired DE operators to detect new pseudo-NPIs.
Why this isn?t obvious Although the algorith-
mic idea sketched above seems quite simple, it is
important to note that prior experiments in that
direction have not proved fruitful. Preliminary
work on learning (German) NPIs using a small
list of simple known DE operators did not yield
strong results [14]. Hoeksema [10] discusses why
NPIs might be hard to learn from data.3 We cir-
cumvent this problem because we are not inter-
ested in learning NPIs per se; rather, for our pur-
3In fact, humans can have trouble agreeing on NPI-hood;
for instance, Lichte and Soehn [14] mention doubts about
over half of Ku?rschner [12]?s 344 manually collected German
NPIs.
poses, pseudo-NPIs suffice. Also, our prelim-
inary work determined that one of the most fa-
mous co-learning algorithms, hubs and authorities
or HITS [11], is poorly suited to our problem.4
Contributions To begin with, we apply our al-
gorithm to produce the first large list of DE opera-
tors for a language other than English. In our case
study on Romanian (?4), we achieve quite high
precisions at k (for example, iteration achieves a
precision at 30 of 87%).
Auxiliary experiments explore the effects of us-
ing a large but noisy NPI list, should one be avail-
able for the language in question. Intriguingly, we
find that co-learning new pseudo-NPIs provides
better results.
Finally (?5), we engage in some cross-linguistic
analysis based on the results of applying our al-
gorithm to English. We find that there are some
suggestive connections with findings in linguistic
typology.
Appendix available A more complete account
of our work and its implications can be found in a
version of this paper containing appendices, avail-
able at www.cs.cornell.edu/?cristian/acl2010/.
2 DLD09: successes and challenges
In this section, we briefly summarize those aspects
of the DLD09 method that are important to under-
standing how our new co-learning method works.
DE operators and NPIs Acquiring DE opera-
tors is challenging because of the complete lack of
annotated data. DLD09?s insight was to make use
of negative polarity items (NPIs), which are words
or phrases that tend to occur only in negative con-
texts. The reason they did so is that Ladusaw?s hy-
pothesis [7, 13] asserts that NPIs only occur within
the scope of DE operators. Figure 1 depicts exam-
ples involving the English NPIs ?any?5 and ?have
a clue? (in the idiomatic sense) that illustrate this
relationship. Some other English NPIs are ?ever?,
?yet? and ?give a damn?.
Thus, NPIs can be treated as clues that a DE
operator might be present (although DE operators
may also occur without NPIs).
4We explored three different edge-weighting schemes
based on co-occurrence frequencies and seed-set member-
ship, but the results were extremely poor; HITS invariably
retrieved very frequent words.
5The free-choice sense of ?any?, as in ?I can skim any pa-
per in five minutes?, is a known exception.
248
NPIs
DE operators any3 have a clue, idiomatic sense
not or n?t X We do n?t have any apples X We do n?t have a clue
doubt XI doubt they have any apples X I doubt they have a clue
no DE operator ? They have any apples ? They have a clue
Figure 1: Examples consistent with Ladusaw?s hypothesis that NPIs can only occur within the scope of
DE operators. A X denotes an acceptable sentence; a ? denotes an unacceptable sentence.
DLD09 algorithm Potential DE operators are
collected by extracting those words that appear in
an NPI?s context at least once.6 Then, the potential
DE operators x are ranked by
f(x) :=
fraction of NPI contexts that contain x
relative frequency of x in the corpus
,
which compares x?s probability of occurrence
conditioned on the appearance of an NPI with its
probability of occurrence overall.7
The method just outlined requires access to a
list of NPIs. DLD09?s system used a subset of
John Lawler?s carefully curated and ?moderately
complete? list of English NPIs.8 The resultant
rankings of candidate English DE operators were
judged to be of high quality.
The challenge in porting to other languages:
cluelessness Can the unsupervised approach of
DLD09 be successfully applied to languages other
than English? Unfortunately, for most other lan-
guages, it does not seem that large, high-quality
NPI lists are available.
One might wonder whether one can circumvent
the NPI-acquisition problem by simply translating
a known English NPI list into the target language.
However, NPI-hood need not be preserved under
translation [17]. Thus, for most languages, we
lack the critical clues that DLD09 depends on.
3 Getting a clue
In this section, we develop an iterative co-
learning algorithm that can extract DE operators
in the many languages where a high-quality NPI
6DLD09 policies: (a) ?NPI context? was defined as the
part of the sentence to the left of the NPI up to the first
comma, semi-colon or beginning of sentence; (b) to encour-
age the discovery of new DE operators, those sentences con-
taining one of a list of 10 well-known DE operators were dis-
carded. For Romanian, we treated only negations (?nu? and
?n-?) and questions as well-known environments.
7DLD09 used an additional distilled score, but we found
that the distilled score performed worse on Romanian.
8http://www-personal.umich.edu/?jlawler/aue/npi.html
database is not available, using Romanian as a
case study.
3.1 Data and evaluation paradigm
We used Rada Mihalcea?s corpus of?1.45 million
sentences of raw Romanian newswire articles.
Note that we cannot evaluate impact on textual
inference because, to our knowledge, no publicly
available textual-entailment system or evaluation
data for Romanian exists. We therefore examine
the system outputs directly to determine whether
the top-ranked items are actually DE operators or
not. Our evaluation metric is precision at k of a
given system?s ranked list of candidate DE oper-
ators; it is not possible to evaluate recall since no
list of Romanian DE operators exists (a problem
that is precisely the motivation for this paper).
To evaluate the results, two native Romanian
speakers labeled the system outputs as being
?DE?, ?not DE? or ?Hard (to decide)?. The la-
beling protocol, which was somewhat complex
to prevent bias, is described in the externally-
available appendices (?7.1). The complete system
output and annotations are publicly available at:
http://www.cs.cornell.edu/?cristian/acl2010/.
3.2 Generating a seed set
Even though, as discussed above, the translation
of an NPI need not be an NPI, a preliminary re-
view of the literature indicates that in many lan-
guages, there is some NPI that can be translated
as ?any? or related forms like ?anybody?. Thus,
with a small amount of effort, one can form a min-
imal NPI seed set for the DLD09 method by us-
ing an appropriate target-language translation of
?any?. For Romanian, we used ?vreo? and ?vreun?,
which are the feminine and masculine translations
of English ?any?.
3.3 DLD09 using the Romanian seed set
We first check whether DLD09 with the two-
item seed set described in ?3.2 performs well on
Romanian. In fact, the results are fairly poor:
249
0 5 9 10 150
5
10
15
20
25
30
35
40
k=10
k=20
k=30
k=40
k=50
k=80
Iteration
Num
ber 
of D
E?o
pera
tors
10 20 30 40 50 60 70 800
10
20
30
40
50
60
70
80
90
100
k
Pre
cisi
on a
t k (i
n %)
 
 DEHard
Figure 2: Left: Number of DE operators in the top k results returned by the co-learning method at each iteration.
Items labeled ?Hard? are not included. Iteration 0 corresponds to DLD09 applied to {?vreo?, ?vreun?}. Curves for
k = 60 and 70 omitted for clarity. Right: Precisions at k for the results of the 9th iteration. The bar divisions are:
DE (blue/darkest/largest) and Hard (red/lighter, sometimes non-existent).
for example, the precision at 30 is below 50%.
(See blue/dark bars in figure 3 in the externally-
available appendices for detailed results.)
This relatively unsatisfactory performance may
be a consequence of the very small size of the NPI
list employed, and may therefore indicate that it
would be fruitful to investigate automatically ex-
tending our list of clues.
3.4 Main idea: a co-learning approach
Our main insight is that not only can NPIs be used
as clues for finding DE operators, as shown by
DLD09, but conversely, DE operators (if known)
can potentially be used to discover new NPI-like
clues, which we refer to as pseudo-NPIs (or pNPIs
for short). By ?NPI-like? we mean, ?serve as pos-
sible indicators of the presence of DE operators,
regardless of whether they are actually restricted
to negative contexts, as true NPIs are?. For exam-
ple, in English newswire, the words ?allegation? or
?rumor? tend to occur mainly in DE contexts, like
? denied ? or ? dismissed ?, even though they are
clearly not true NPIs (the sentence ?I heard a ru-
mor? is fine). Given this insight, we approach the
problem using an iterative co-learning paradigm
that integrates the search for new DE operators
with a search for new pNPIs.
First, we describe an algorithm that is the ?re-
verse? of DLD09 (henceforth rDLD), in that it re-
trieves and ranks pNPIs assuming a given list of
DE operators. Potential pNPIs are collected by ex-
tracting those words that appear in a DE context
(defined here, to avoid the problems of parsing or
scope determination, as the part of the sentence to
the right of a DE operator, up to the first comma,
semi-colon or end of sentence); these candidates x
are then ranked by
fr(x) :=
fraction of DE contexts that contain x
relative frequency of x in the corpus
.
Then, our co-learning algorithm consists of the
iteration of the following two steps:
? (DE learning) Apply DLD09 using a set N
of pseudo-NPIs to retrieve a list of candidate
DE operators ranked by f (defined in Section
2). Let D be the top n candidates in this list.
? (pNPI learning) Apply rDLD using the set D
to retrieve a list of pNPIs ranked by fr; ex-
tend N with the top nr pNPIs in this list. In-
crement n.
Here, N is initialized with the NPI seed set. At
each iteration, we consider the output of the al-
gorithm to be the ranked list of DE operators re-
trieved in the DE-learning step. In our experi-
ments, we initialized n to 10 and set nr to 1.
4 Romanian results
Our results show that there is indeed favorable
synergy between DE-operator and pNPI retrieval.
Figure 2 plots the number of correctly retrieved
DE operators in the top k outputs at each iteration.
The point at iteration 0 corresponds to a datapoint
already discussed above, namely, DLD09 applied
to the two ?any?-translation NPIs. Clearly, we see
general substantial improvement over DLD09, al-
though the increases level off in later iterations.
250
(Determining how to choose the optimal number
of iterations is a subject for future research.)
Additional experiments, described in the
externally-available appendices (?7.2), suggest
that pNPIs can even be more effective clues than
a noisy list of NPIs. (Thus, a larger seed set
does not necessarily mean better performance.)
pNPIs also have the advantage of being derivable
automatically, and might be worth investigating
from a linguistic perspective in their own right.
5 Cross-linguistic analysis
Applying our algorithm to English: connec-
tions to linguistic typology So far, we have
made no assumptions about the language on which
our algorithm is applied. A valid question is, does
the quality of the results vary with choice of appli-
cation language? In particular, what happens if we
run our algorithm on English?
Note that in some sense, this is a perverse ques-
tion: the motivation behind our algorithm is the
non-existence of a high-quality list of NPIs for
the language in question, and English is essen-
tially the only case that does not fit this descrip-
tion. On the other hand, the fact that DLD09 ap-
plied their method for extraction of DE operators
to English necessitates some form of comparison,
for the sake of experimental completeness.
We thus ran our algorithm on the English
BLLIP newswire corpus with seed set {?any?} .
We observe that, surprisingly, the iterative addi-
tion of pNPIs has very little effect: the precisions
at k are good at the beginning and stay about the
same across iterations (for details see figure 5 in
in the externally-available appendices). Thus, on
English, co-learning does not hurt performance,
which is good news; but unlike in Romanian, it
does not lead to improvements.
Why is English ?any? seemingly so ?powerful?,
in contrast to Romanian, where iterating beyond
the initial ?any? translations leads to better re-
sults? Interestingly, findings from linguistic typol-
ogy may shed some light on this issue. Haspel-
math [9] compares the functions of indefinite pro-
nouns in 40 languages. He shows that English is
one of the minority of languages (11 out of 40)9 in
which there exists an indefinite pronoun series that
occurs in all (Haspelmath?s) classes of DE con-
texts, and thus can constitute a sufficient seed on
9English, Ancash Quechua, Basque, Catalan, French,
Hindi/Urdu, Irish, Portuguese, Swahili, Swedish, Turkish.
its own. In the other languages (including Roma-
nian),10 no indirect pronoun can serve as a suffi-
cient seed. So, we expect our method to be vi-
able for all languages; while the iterative discov-
ery of pNPIs is not necessary (although neither is
it harmful) for the subset of languages for which a
sufficient seed exists, such as English, it is essen-
tial for the languages for which, like Romanian,
?any?-equivalents do not suffice.
Using translation Another interesting question
is whether directly translating DE operators from
English is an alternative to our method. First, we
emphasize that there exists no complete list of En-
glish DE operators (the largest available collec-
tion is the one extracted by DLD09). Second, we
do not know whether DE operators in one lan-
guage translate into DE operators in another lan-
guage. Even if that were the case, and we some-
how had access to ideal translations of DLD09?s
list, there would still be considerable value in us-
ing our method: 14 (39%) of our top 36 highest-
ranked Romanian DE operators for iteration 9 do
not, according to the Romanian-speaking author,
have English equivalents appearing on DLD09?s
90-item list. Some examples are: ?abt?inut? (ab-
stained), ?criticat? (criticized) and ?react?ionat? (re-
acted). Therefore, a significant fraction of the
DE operators derived by our co-learning algorithm
would have been missed by the translation alterna-
tive even under ideal conditions.
6 Conclusions
We have introduced the first method for discov-
ering downward-entailing operators that is univer-
sally applicable. Previous work on automatically
detecting DE operators assumed the existence of
a high-quality collection of NPIs, which renders it
inapplicable in most languages, where such a re-
source does not exist. We overcome this limita-
tion by employing a novel co-learning approach,
and demonstrate its effectiveness on Romanian.
Also, we introduce the concept of pseudo-NPIs.
Auxiliary experiments described in the externally-
available appendices show that pNPIs are actually
more effective seeds than a noisy ?true? NPI list.
Finally, we noted some cross-linguistic differ-
ences in performance, and found an interesting
connection between these differences and Haspel-
math?s [9] characterization of cross-linguistic vari-
ation in the occurrence of indefinite pronouns.
10Examples: Chinese, German, Italian, Polish, Serbian.
251
Acknowledgments We thank Tudor Marian for
serving as an annotator, Rada Mihalcea for ac-
cess to the Romanian newswire corpus, and Claire
Cardie, Yejin Choi, Effi Georgala, Mark Liber-
man, Myle Ott, Joa?o Paula Muchado, Stephen Pur-
pura, Mark Yatskar, Ainur Yessenalina, and the
anonymous reviewers for their helpful comments.
Supported by NSF grant IIS-0910664.
References
[1] Roy Bar-Haim, Jonathan Berant, Ido Da-
gan, Iddo Greental, Shachar Mirkin, Eyal
Shnarch, and Idan Szpektor. Efficient seman-
tic deduction and approximate matching over
compact parse forests. In Proceedings of the
Text Analysis Conference (TAC), 2008.
[2] Eric Breck. A simple system for detecting
non-entailment. In Proceedings of the Text
Analysis Conference (TAC), 2009.
[3] Christos Christodoulopoulos. Creating a nat-
ural logic inference system with combinatory
categorial grammar. Master?s thesis, Univer-
sity of Edinburgh, 2008.
[4] Ido Dagan, Oren Glickman, and Bernardo
Magnini. The PASCAL Recognising Textual
Entailment challenge. In Machine Learn-
ing Challenges, Evaluating Predictive Un-
certainty, Visual Object Classification and
Recognizing Textual Entailment, First PAS-
CAL Machine Learning Challenges Work-
shop, pages 177?190. Springer, 2006.
[5] Cristian Danescu-Niculescu-Mizil, Lillian
Lee, and Richard Ducott. Without a ?doubt??
Unsupervised discovery of downward-
entailing operators. In Proceedings of
NAACL HLT, 2009.
[6] David Dowty. The role of negative polar-
ity and concord marking in natural language
reasoning. In Mandy Harvey and Lynn San-
telmann, editors, Proceedings of SALT IV,
pages 114?144, 1994.
[7] Gilles Fauconnier. Polarity and the scale
principle. In Proceedings of the Chicago Lin-
guistic Society (CLS), pages 188?199, 1975.
Reprinted in Javier Gutierrez-Rexach (ed.),
Semantics: Critical Concepts in Linguistics,
2003.
[8] Bart Geurts and Frans van der Slik. Mono-
tonicity and processing load. Journal of Se-
mantics, 22(1):97?117, 2005.
[9] Martin Haspelmath. Indefinite Pronouns.
Oxford University Press, 2001.
[10] Jack Hoeksema. Corpus study of negative
polarity items. IV-V Jornades de corpus lin-
guistics 1996-1997, 1997. http://odur.let.rug.
nl/?hoeksema/docs/barcelona.html.
[11] Jon Kleinberg. Authoritative sources in a hy-
perlinked environment. In Proceedings of
the 9th ACM-SIAM Symposium on Discrete
Algorithms (SODA), pages 668?677, 1998.
Extended version in Journal of the ACM,
46:604?632, 1999.
[12] Wilfried Ku?rschner. Studien zur Negation im
Deutschen. Narr, 1983.
[13] William A. Ladusaw. Polarity Sensitivity as
Inherent Scope Relations. Garland Press,
New York, 1980. Ph.D. thesis date 1979.
[14] Timm Lichte and Jan-Philipp Soehn. The re-
trieval and classification of Negative Polar-
ity Items using statistical profiles. In Sam
Featherston and Wolfgang Sternefeld, edi-
tors, Roots: Linguistics in Search of its Ev-
idential Base, pages 249?266. Mouton de
Gruyter, 2007.
[15] Bill MacCartney and Christopher D. Man-
ning. Modeling semantic containment and
exclusion in natural language inference. In
Proceedings of COLING, pages 521?528,
2008.
[16] Rowan Nairn, Cleo Condoravdi, and Lauri
Karttunen. Computing relative polarity for
textual inference. In Proceedings of In-
ference in Computational Semantics (ICoS),
2006.
[17] Frank Richter, Janina Rado?, and Manfred
Sailer. Negative polarity items: Corpus
linguistics, semantics, and psycholinguis-
tics: Day 2: Corpus linguistics. Tutorial
slides: http://www.sfs.uni-tuebingen.de/?fr/
esslli/08/byday/day2/day2-part1.pdf, 2008.
[18] V??ctor Sa?nchez Valencia. Studies on natural
logic and categorial grammar. PhD thesis,
University of Amsterdam, 1991.
[19] Johan van Benthem. Essays in Logical Se-
mantics. Reidel, Dordrecht, 1986.
[20] Ton van der Wouden. Negative contexts:
Collocation, polarity and multiple negation.
Routledge, 1997.
252
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 892?901,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
You Had Me at Hello: How Phrasing Affects Memorability
Cristian Danescu-Niculescu-Mizil Justin Cheng Jon Kleinberg Lillian Lee
Department of Computer Science
Cornell University
cristian@cs.cornell.edu, jc882@cornell.edu, kleinber@cs.cornell.edu, llee@cs.cornell.edu
Abstract
Understanding the ways in which information
achieves widespread public awareness is a re-
search question of significant interest. We
consider whether, and how, the way in which
the information is phrased ? the choice of
words and sentence structure ? can affect this
process. To this end, we develop an analy-
sis framework and build a corpus of movie
quotes, annotated with memorability informa-
tion, in which we are able to control for both
the speaker and the setting of the quotes. We
find that there are significant differences be-
tween memorable and non-memorable quotes
in several key dimensions, even after control-
ling for situational and contextual factors. One
is lexical distinctiveness: in aggregate, memo-
rable quotes use less common word choices,
but at the same time are built upon a scaf-
folding of common syntactic patterns. An-
other is that memorable quotes tend to be more
general in ways that make them easy to ap-
ply in new contexts ? that is, more portable.
We also show how the concept of ?memorable
language? can be extended across domains.
1 Hello. My name is Inigo Montoya.
Understanding what items will be retained in the
public consciousness, and why, is a question of fun-
damental interest in many domains, including mar-
keting, politics, entertainment, and social media; as
we all know, many items barely register, whereas
others catch on and take hold in many people?s
minds.
An active line of recent computational work has
employed a variety of perspectives on this question.
Building on a foundation in the sociology of diffu-
sion [27, 31], researchers have explored the ways in
which network structure affects the way information
spreads, with domains of interest including blogs
[1, 11], email [37], on-line commerce [22], and so-
cial media [2, 28, 33, 38]. There has also been recent
research addressing temporal aspects of how differ-
ent media sources convey information [23, 30, 39]
and ways in which people react differently to infor-
mation on different topics [28, 36].
Beyond all these factors, however, one?s everyday
experience with these domains suggests that the way
in which a piece of information is expressed ? the
choice of words, the way it is phrased ? might also
have a fundamental effect on the extent to which it
takes hold in people?s minds. Concepts that attain
wide reach are often carried in messages such as
political slogans, marketing phrases, or aphorisms
whose language seems intuitively to be memorable,
?catchy,? or otherwise compelling.
Our first challenge in exploring this hypothesis is
to develop a notion of ?successful? language that is
precise enough to allow for quantitative evaluation.
We also face the challenge of devising an evaluation
setting that separates the phrasing of a message from
the conditions in which it was delivered ? highly-
cited quotes tend to have been delivered under com-
pelling circumstances or fit an existing cultural, po-
litical, or social narrative, and potentially what ap-
peals to us about the quote is really just its invoca-
tion of these extra-linguistic contexts. Is the form
of the language adding an effect beyond or indepen-
dent of these (obviously very crucial) factors? To
investigate the question, one needs a way of control-
892
ling ? as much as possible ? for the role that the
surrounding context of the language plays.
The present work (i): Evaluating language-based
memorability Defining what makes an utterance
memorable is subtle, and scholars in several do-
mains have written about this question. There is
a rough consensus that an appropriate definition
involves elements of both recognition ? people
should be able to retain the quote and recognize it
when they hear it invoked ? and production ? peo-
ple should be motivated to refer to it in relevant sit-
uations [15]. One suggested reason for why some
memes succeed is their ability to provoke emotions
[16]. Alternatively, memorable quotes can be good
for expressing the feelings, mood, or situation of an
individual, a group, or a culture (the zeitgeist): ?Cer-
tain quotes exquisitely capture the mood or feeling
we wish to communicate to someone. We hear them
... and store them away for future use? [10].
None of these observations, however, serve as
definitions, and indeed, we believe it desirable to
not pre-commit to an abstract definition, but rather
to adopt an operational formulation based on exter-
nal human judgments. In designing our study, we
focus on a domain in which (i) there is rich use of
language, some of which has achieved deep cultural
penetration; (ii) there already exist a large number of
external human judgments ? perhaps implicit, but
in a form we can extract; and (iii) we can control for
the setting in which the text was used.
Specifically, we use the complete scripts of
roughly 1000 movies, representing diverse genres,
eras, and levels of popularity, and consider which
lines are the most ?memorable?. To acquire memo-
rability labels, for each sentence in each script, we
determine whether it has been listed as a ?memo-
rable quote? by users of the widely-known IMDb
(the Internet Movie Database), and also estimate the
number of times it appears on the Web. Both of these
serve as memorability metrics for our purposes.
When we evaluate properties of memorable
quotes, we compare them with quotes that are not as-
sessed as memorable, but were spoken by the same
character, at approximately the same point in the
same movie. This enables us to control in a fairly
fine-grained way for the confounding effects of con-
text discussed above: we can observe differences
that persist even after taking into account both the
speaker and the setting.
In a pilot validation study, we find that human
subjects are effective at recognizing the more IMDb-
memorable of two quotes, even for movies they have
not seen. This motivates a search for features in-
trinsic to the text of quotes that signal memorabil-
ity. In fact, comments provided by the human sub-
jects as part of the task suggested two basic forms
that such textual signals could take: subjects felt that
(i) memorable quotes often involve a distinctive turn
of phrase; and (ii) memorable quotes tend to invoke
general themes that aren?t tied to the specific setting
they came from, and hence can be more easily in-
voked for future (out of context) uses. We test both
of these principles in our analysis of the data.
The present work (ii): What distinguishes mem-
orable quotes Under the controlled-comparison
setting sketched above, we find that memorable
quotes exhibit significant differences from non-
memorable quotes in several fundamental respects,
and these differences in the data reinforce the two
main principles from the human pilot study. First,
we show a concrete sense in which memorable
quotes are indeed distinctive: with respect to lexi-
cal language models trained on the newswire por-
tions of the Brown corpus [21], memorable quotes
have significantly lower likelihood than their non-
memorable counterparts. Interestingly, this distinc-
tiveness takes place at the level of words, but not
at the level of other syntactic features: the part-of-
speech composition of memorable quotes is in fact
more likely with respect to newswire. Thus, we can
think of memorable quotes as consisting, in an ag-
gregate sense, of unusual word choices built on a
scaffolding of common part-of-speech patterns.
We also identify a number of ways in which mem-
orable quotes convey greater generality. In their pat-
terns of verb tenses, personal pronouns, and deter-
miners, memorable quotes are structured so as to be
more ?free-standing,? containing fewer markers that
indicate references to nearby text.
Memorable quotes differ in other interesting as-
pects as well, such as sound distributions.
Our analysis of memorable movie quotes suggests
a framework by which the memorability of text in
a range of different domains could be investigated.
893
We provide evidence that such cross-domain prop-
erties may hold, guided by one of our motivating
applications in marketing. In particular, we analyze
a corpus of advertising slogans, and we show that
these slogans have significantly greater likelihood
at both the word level and the part-of-speech level
with respect to a language model trained on mem-
orable movie quotes, compared to a corresponding
language model trained on non-memorable movie
quotes. This suggests that some of the principles un-
derlying memorable text have the potential to apply
across different areas.
Roadmap ?2 lays the empirical foundations of our
work: the design and creation of our movie-quotes
dataset, which we make publicly available (?2.1), a
pilot study with human subjects validating IMDb-
based memorability labels (?2.2), and further study
of incorporating search-engine counts (?2.3). ?3 de-
tails our analysis and prediction experiments, using
both movie-quotes data and, as an exploration of
cross-domain applicability, slogans data. ?4 surveys
related work across a variety of fields. ?5 briefly
summarizes and indicates some future directions.
2 I?m ready for my close-up.
2.1 Data
To study the properties of memorable movie quotes,
we need a source of movie lines and a designation
of memorability. Following [8], we constructed a
corpus consisting of all lines from roughly 1000
movies, varying in genre, era, and popularity; for
each movie, we then extracted the list of quotes from
IMDb?s Memorable Quotes page corresponding to
the movie.1
A memorable quote in IMDb can appear either as
an individual sentence spoken by one character, or
as a multi-sentence line, or as a block of dialogue in-
volving multiple characters. In the latter two cases,
it can be hard to determine which particular portion
is viewed as memorable (some involve a build-up to
a punch line; others involve the follow-through after
a well-phrased opening sentence), and so we focus
in our comparisons on those memorable quotes that
1This extraction involved some edit-distance-based align-
ment, since the exact form of the line in the script can exhibit
minor differences from the version typed into IMDb.
1 2 3 4 5 6 7 8 9 10Decile0
100
200
300
400
500
600
700
800
Num
ber o
f mem
orabl
e quo
tes
Figure 1: Location of memorable quotes in each decile
of movie scripts (the first 10th, the second 10th, etc.),
summed over all movies. The same qualitative results
hold if we discard each movie?s very first and last line,
which might have privileged status.
appear as a single sentence rather than a multi-line
block.2
We now formulate a task that we can use to eval-
uate the features of memorable quotes. Recall that
our goal is to identify effects based in the language
of the quotes themselves, beyond any factors arising
from the speaker or context. Thus, for each (single-
sentence) memorable quote M , we identify a non-
memorable quote that is as similar as possible to M
in all characteristics but the choice of words. This
means we want it to be spoken by the same charac-
ter in the same movie. It also means that we want
it to have the same length: controlling for length is
important because we expect that on average, shorter
quotes will be easier to remember than long quotes,
and that wouldn?t be an interesting textual effect to
report. Moreover, we also want to control for the
fact that a quote?s position in a movie can affect
memorability: certain scenes produce more mem-
orable dialogue, and as Figure 1 demonstrates, in
aggregate memorable quotes also occur dispropor-
tionately near the beginnings and especially the ends
of movies. In summary, then, for each M , we pick a
contrasting (single-sentence) quote N from the same
movie that is as close in the script as possible to M
(either before or after it), subject to the conditions
that (i) M and N are uttered by the same speaker,
(ii) M and N have the same number of words, and
(iii) N does not occur in the IMDb list of memorable
2We also ran experiments relaxing the single-sentence as-
sumption, which allows for stricter scene control and a larger
dataset but complicates comparisons involving syntax. The
non-syntax results were in line with those reported here.
894
Movie First Quote Second Quote
Jackie Brown Half a million dollars will always be missed. I know the type, trust me on this.
Star Trek: Nemesis I think it?s time to try some unsafe velocities. No cold feet, or any other parts of our
anatomy.
Ordinary People A little advice about feelings kiddo; don?t ex-
pect it always to tickle.
I mean there?s someone besides your
mother you?ve got to forgive.
Table 1: Three example pairs of movie quotes. Each pair satisfies our criteria: the two component quotes are spoken
close together in the movie by the same character, have the same length, and one is labeled memorable by the IMDb
while the other is not. (Contractions such as ?it?s? count as two words.)
quotes for the movie (either as a single line or as part
of a larger block).
Given such pairs, we formulate a pairwise com-
parison task: given M and N , determine which is
the memorable quote. Psychological research on
subjective evaluation [35], as well as initial experi-
ments using ourselves as subjects, indicated that this
pairwise set-up easier to work with than simply pre-
senting a single sentence and asking whether it is
memorable or not; the latter requires agreement on
an ?absolute? criterion for memorability that is very
hard to impose consistently, whereas the former sim-
ply requires a judgment that one quote is more mem-
orable than another.
Our main dataset, available at http://www.cs.
cornell.edu/?cristian/memorability.html,3 thus con-
sists of approximately 2200 such (M,N) pairs, sep-
arated by a median of 5 same-character lines in the
script. The reader can get a sense for the nature of
the data from the three examples in Table 1.
We now discuss two further aspects to the formu-
lation of the experiment: a preliminary pilot study
involving human subjects, and the incorporation of
search engine counts into the data.
2.2 Pilot study: Human performance
As a preliminary consideration, we did a small pilot
study to see if humans can distinguish memorable
from non-memorable quotes, assuming our IMDB-
induced labels as gold standard. Six subjects, all na-
tive speakers of English and none an author of this
paper, were presented with 11 or 12 pairs of mem-
orable vs. non-memorable quotes; again, we con-
trolled for extra-textual effects by ensuring that in
each pair the two quotes come from the same movie,
are by the same character, have the same length, and
3Also available there: other examples and factoids.
subject number of matches with
IMDb-induced annotation
A 11/11 = 100%
B 11/12 = 92%
C 9/11 = 82%
D 8/11 = 73%
E 7/11 = 64%
F 7/12 = 58%
macro avg ? 78%
Table 2: Human pilot study: number of matches to
IMDb-induced annotation, ordered by decreasing match
percentage. For the null hypothesis of random guessing,
these results are statistically significant, p < 2?6 ? .016.
appear as nearly as possible in the same scene.4 The
order of quotes within pairs was randomized. Im-
portantly, because we wanted to understand whether
the language of the quotes by itself contains signals
about memorability, we chose quotes from movies
that the subjects said they had not seen. (This means
that each subject saw a different set of quotes.)
Moreover, the subjects were requested not to consult
any external sources of information.5 The reader is
welcome to try a demo version of the task at http:
//www.cs.cornell.edu/?cristian/memorability.html.
Table 2 shows that all the subjects performed
(sometimes much) better than chance, and against
the null hypothesis that all subjects are guessing ran-
domly, the results are statistically significant, p <
2?6 ? .016. These preliminary findings provide ev-
idence for the validity of our task: despite the appar-
ent difficulty of the job, even humans who haven?t
seen the movie in question can recover our IMDb-
4In this pilot study, we allowed multi-sentence quotes.
5We did not use crowd-sourcing because we saw no way to
ensure that this condition would be obeyed by arbitrary subjects.
We do note, though, that after our research was completed and
as of Apr. 26, 2012, ? 11,300 people completed the online test:
average accuracy: 72%, mode number correct: 9/12.
895
induced labels with some reliability.6
2.3 Incorporating search engine counts
Thus far we have discussed a dataset in which mem-
orability is determined through an explicit label-
ing drawn from the IMDb. Given the ?produc-
tion? aspect of memorability discussed in ?1, we
should also expect that memorable quotes will tend
to appear more extensively on Web pages than non-
memorable quotes; note that incorporating this in-
sight makes it possible to use the (implicit) judg-
ments of a much larger number of people than are
represented by the IMDb database. It therefore
makes sense to try using search-engine result counts
as a second indication of memorability.
We experimented with several ways of construct-
ing memorability information from search-engine
counts, but this proved challenging. Searching for
a quote as a stand-alone phrase runs into the prob-
lem that a number of quotes are also sentences that
people use without the movie in mind, and so high
counts for such quotes do not testify to the phrase?s
status as a memorable quote from the movie. On
the other hand, searching for the quote in a Boolean
conjunction with the movie?s title discards most of
these uses, but also eliminates a large fraction of
the appearances on the Web that we want to find:
precisely because memorable quotes tend to have
widespread cultural usage, people generally don?t
feel the need to include the movie?s title when in-
voking them. Finally, since we are dealing with
roughly 1000 movies, the result counts vary over an
enormous range, from recent blockbusters to movies
with relatively small fan bases.
In the end, we found that it was more effective to
use the result counts in conjunction with the IMDb
labels, so that the counts played the role of an ad-
ditional filter rather than a free-standing numerical
value. Thus, for each pair (M,N) produced using
the IMDb methodology above, we searched for each
of M and N as quoted expressions in a Boolean con-
junction with the title of the movie. We then kept
only those pairs for which M (i) produced more than
five results in our (quoted, conjoined) search, and (ii)
produced at least twice as many results as the cor-
6The average accuracy being below 100% reinforces that
context is very important, too.
responding search for N . We created a version of
this filtered dataset using each of Google and Bing,
and all the main findings were consistent with the
results on the IMDb-only dataset. Thus, in what fol-
lows, we will focus on the main IMDb-only dataset,
discussing the relationship to the dataset filtered by
search engine counts where relevant (in which case
we will refer to the +Google dataset).
3 Never send a human to do a machine?s job.
We now discuss experiments that investigate the hy-
potheses discussed in ?1. In particular, we devise
methods that can assess the distinctiveness and gen-
erality hypotheses and test whether there exists a no-
tion of ?memorable language? that operates across
domains. In addition, we evaluate and compare the
predictive power of these hypotheses.
3.1 Distinctiveness
One of the hypotheses we examine is whether the
use of language in memorable quotes is to some ex-
tent unusual. In order to quantify the level of dis-
tinctiveness of a quote, we take a language-model
approach: we model ?common language? using
the newswire sections of the Brown corpus [21]7,
and evaluate how distinctive a quote is by evaluat-
ing its likelihood with respect to this model ? the
lower the likelihood, the more distinctive. In or-
der to assess different levels of lexical and syntactic
distinctiveness, we employ a total of six Laplace-
smoothed8 language models: 1-gram, 2-gram, and
3-gram word LMs and 1-gram, 2-gram and 3-gram
part-of-speech9 LMs.
We find strong evidence that from a lexical per-
spective, memorable quotes are more distinctive
than their non-memorable counterparts. As indi-
cated in Table 3, for each of our lexical ?common
language? models, in about 60% of the quote pairs,
the memorable quote is more distinctive.
Interestingly, the reverse is true when it comes to
7Results were qualitatively similar if we used the fiction por-
tions. The age of the Brown corpus makes it less likely to con-
tain modern movie quotes.
8We employ Laplace (additive) smoothing with a smoothing
parameter of 0.2. The language models? vocabulary was that of
the entire training corpus.
9Throughout we obtain part-of-speech tags by using the
NLTK maximum entropy tagger with default parameters.
896
?common language?
model
IMDb-only +Google
lexical
1-gram 61.13%??? 59.21%???
2-gram 59.22%??? 57.03%???
3-gram 59.81%??? 58.32%???
syntactic
1-gram 43.60%??? 44.77%???
2-gram 48.31% 47.84%
3-gram 50.91% 50.92%
Table 3: Distinctiveness: percentage of quote pairs
in which the the memorable quote is more distinctive
than the non-memorable one according to the respec-
tive ?common language? model. Significance accord-
ing to a two-tailed sign test is indicated using *-notation
(???=?p<.001?).
syntax: memorable quotes appear to follow the syn-
tactic patterns of ?common language? as closely as
or more closely than non-memorable quotes. To-
gether, these results suggest that memorable quotes
consist of unusual word sequences built on common
syntactic scaffolding.
3.2 Generality
Another of our hypotheses is that memorable quotes
are easier to use outside the specific context in which
they were uttered ? that is, more ?portable? ? and
therefore exhibit fewer terms that refer to those set-
tings. We use the following syntactic properties as
proxies for the generality of a quote:
? Fewer 3rd-person pronouns, since these com-
monly refer to a person or object that was intro-
duced earlier in the discourse. Utterances that
employ fewer such pronouns are easier to adapt
to new contexts, and so will be considered more
general.
? More indefinite articles like a and an, since
they are more likely to refer to general concepts
than definite articles. Quotes with more indefi-
nite articles will be considered more general.
? Fewer past tense verbs and more present
tense verbs, since the former are more likely
to refer to specific previous events. Therefore
utterances that employ fewer past tense verbs
(and more present tense verbs) will be consid-
ered more general.
Table 4 gives the results for each of these four
metrics ? in each case, we show the percentage of
Generality metric IMDb-only +Google
fewer 3rd pers. pronouns 64.37%??? 62.93%???
more indef. article 57.21%??? 58.23%???
less past tense 57.91%??? 59.74%???
more present tense 54.60%??? 55.86%???
Table 4: Generality: percentage of quote pairs in which
the memorable quote is more general than the non-
memorable ones according to the respective metric. Pairs
where the metric does not distinguish between the quotes
are not considered.
quote pairs for which the memorable quote scores
better on the generality metric.
Note that because the issue of generality is a com-
plex one for which there is no straightforward single
metric, our approach here is based on several prox-
ies for generality, considered independently; yet, as
the results show, all of these point in a consistent
direction. It is an interesting open question to de-
velop richer ways of assessing whether a quote has
greater generality, in the sense that people intuitively
attribute to memorable quotes.
3.3 ?Memorable? language beyond movies
One of the motivating questions in our analysis
is whether there are general principles underlying
?memorable language.? The results thus far suggest
potential families of such principles. A further ques-
tion in this direction is whether the notion of mem-
orability can be extended across different domains,
and for this we collected (and distribute on our web-
site) 431 phrases that were explicitly designed to
be memorable: advertising slogans (e.g., ?Quality
never goes out of style.?). The focus on slogans is
also in keeping with one of the initial motivations
in studying memorability, namely, marketing appli-
cations ? in other words, assessing whether a pro-
posed slogan has features that are consistent with
memorable text.
The fact that it?s not clear how to construct a col-
lection of ?non-memorable? counterparts to slogans
appears to pose a technical challenge. However, we
can still use a language-modeling approach to as-
sess whether the textual properties of the slogans are
closer to the memorable movie quotes (as one would
conjecture) or to the non-memorable movie quotes.
Specifically, we train one language model on memo-
rable quotes and another on non-memorable quotes
897
(Non)memorable
language models
Slogans Newswire
lexical
1-gram 56.15%?? 33.77%???
2-gram 51.51% 25.15%???
3-gram 52.44% 28.89%???
syntactic
1-gram 73.09%??? 68.27%???
2-gram 64.04%??? 50.21%
3-gram 62.88%??? 55.09%???
Table 5: Cross-domain concept of ?memorable? lan-
guage: percentage of slogans that have higher likelihood
under the memorable language model than under the non-
memorable one (for each of the six language models con-
sidered). Rightmost column: for reference, the percent-
age of newswire sentences that have higher likelihood un-
der the memorable language model than under the non-
memorable one.
Generality metric slogans mem. n-mem.
% 3rd pers. pronouns 2.14% 2.16% 3.41%
% indefinite articles 2.68% 2.63% 2.06%
% past tense 14.60% 21.13% 26.69%
Table 6: Slogans are most general when compared to
memorable and non-memorable quotes. (%s of 3rd pers.
pronouns and indefinite articles are relative to all tokens,
%s of past tense are relative to all past and present verbs.)
and compare how likely each slogan is to be pro-
duced according to these two models. As shown in
the middle column of Table 5, we find that slogans
are better predicted both lexically and syntactically
by the former model. This result thus offers evi-
dence for a concept of ?memorable language? that
can be applied beyond a single domain.
We also note that the higher likelihood of slogans
under a ?memorable language? model is not simply
occurring for the trivial reason that this model pre-
dicts all other large bodies of text better. In partic-
ular, the newswire section of the Brown corpus is
predicted better at the lexical level by the language
model trained on non-memorable quotes.
Finally, Table 6 shows that slogans employ gen-
eral language, in the sense that for each of our
generality metrics, we see a slogans/memorable-
quotes/non-memorable quotes spectrum.
3.4 Prediction task
We now show how the principles discussed above
can provide features for a basic prediction task, cor-
responding to the task in our human pilot study:
given a pair of quotes, identify the memorable one.
Our first formulation of the prediction task uses
a standard bag-of-words model10. If there were
no information in the textual content of a quote
to determine whether it were memorable, then an
SVM employing bag-of-words features should per-
form no better than chance. Instead, though, it ob-
tains 59.67% (10-fold cross-validation) accuracy, as
shown in Table 7. We then develop models using
features based on the measures formulated earlier
in this section: generality measures (the four listed
in Table 4); distinctiveness measures (likelihood ac-
cording to 1, 2, and 3-gram ?common language?
models at the lexical and part-of-speech level for
each quote in the pair, their differences, and pair-
wise comparisons between them); and similarity-
to-slogans measures (likelihood according to 1, 2,
and 3-gram slogan-language models at the lexical
and part-of-speech level for each quote in the pair,
their differences, and pairwise comparisons between
them).
Even a relatively small number of distinctive-
ness features, on their own, improve significantly
over the much larger bag-of-words model. When
we include additional features based on generality
and language-model features measuring similarity to
slogans, the performance improves further (last line
of Table 7).
Thus, the main conclusion from these prediction
tasks is that abstracting notions such as distinctive-
ness and generality can produce relatively stream-
lined models that outperform much heavier-weight
bag-of-words models, and can suggest steps toward
approaching the performance of human judges who
? very much unlike our system ? have the full cul-
tural context in which movies occur at their disposal.
3.5 Other characteristics
We also made some auxiliary observations that may
be of interest. Specifically, we find differences in let-
ter and sound distribution (e.g., memorable quotes
? after curse-word removal ? use significantly
more ?front sounds? (labials or front vowels such
as represented by the letter i) and significantly fewer
?back sounds? such as the one represented by u),11
10We discarded terms appearing fewer than 10 times.
11These findings may relate to marketing research on sound
symbolism [7, 19, 40].
898
Feature set # feats Accuracy
bag of words 962 59.67%
distinctiveness 24 62.05%?
generality 4 56.70%
slogan sim. 24 58.30%
all three types together 52 64.27%??
Table 7: Prediction: SVM 10-fold cross validation results
using the respective feature sets. Random baseline accu-
racy is 50%. Accuracies statistically significantly greater
than bag-of-words according to a two-tailed t-test are in-
dicated with *(p<.05) and **(p<.01).
word complexity (e.g., memorable quotes use words
with significantly more syllables) and phrase com-
plexity (e.g., memorable quotes use fewer coordi-
nating conjunctions). The latter two are in line with
our distinctiveness hypothesis.
4 A long time ago, in a galaxy far, far away
How an item?s linguistic form affects the reaction it
generates has been studied in several contexts, in-
cluding evaluations of product reviews [9], political
speeches [12], on-line posts [13], scientific papers
[14], and retweeting of Twitter posts [36]. We use
a different set of features, abstracting the notions of
distinctiveness and generality, in order to focus on
these higher-level aspects of phrasing rather than on
particular lower-level features.
Related to our interest in distinctiveness, work in
advertising research has studied the effect of syntac-
tic complexity on recognition and recall of slogans
[5, 6, 24]. There may also be connections to Von
Restorff?s isolation effect Hunt [17], which asserts
that when all but one item in a list are similar in some
way, memory for the different item is enhanced.
Related to our interest in generality, Knapp et al
[20] surveyed subjects regarding memorable mes-
sages or pieces of advice they had received, finding
that the ability to be applied to multiple concrete sit-
uations was an important factor.
Memorability, although distinct from ?memoriz-
ability?, relates to short- and long-term recall. Thorn
and Page [34] survey sub-lexical, lexical, and se-
mantic attributes affecting short-term memorability
of lexical items. Studies of verbatim recall have also
considered the task of distinguishing an exact quote
from close paraphrases [3]. Investigations of long-
term recall have included studies of culturally signif-
icant passages of text [29] and findings regarding the
effect of rhetorical devices of alliterative [4], ?rhyth-
mic, poetic, and thematic constraints? [18, 26].
Finally, there are complex connections between
humor and memory [32], which may lead to interac-
tions with computational humor recognition [25].
5 I think this is the beginning of a
beautiful friendship.
Motivated by the broad question of what kinds of in-
formation achieve widespread public awareness, we
studied the the effect of phrasing on a quote?s mem-
orability. A challenge is that quotes differ not only
in how they are worded, but also in who said them
and under what circumstances; to deal with this dif-
ficulty, we constructed a controlled corpus of movie
quotes in which lines deemed memorable are paired
with non-memorable lines spoken by the same char-
acter at approximately the same point in the same
movie. After controlling for context and situation,
memorable quotes were still found to exhibit, on av-
erage (there will always be individual exceptions),
significant differences from non-memorable quotes
in several important respects, including measures
capturing distinctiveness and generality. Our ex-
periments with slogans show how the principles we
identify can extend to a different domain.
Future work may lead to applications in market-
ing, advertising and education [4]. Moreover, the
subtle nature of memorability, and its connection to
research in psychology, suggests a range of further
research directions. We believe that the framework
developed here can serve as the basis for further
computational studies of the process by which infor-
mation takes hold in the public consciousness, and
the role that language effects play in this process.
My mother thanks you. My father thanks you.
My sister thanks you. And I thank you: Re-
becca Hwa, Evie Kleinberg, Diana Minculescu, Alex
Niculescu-Mizil, Jennifer Smith, Benjamin Zimmer, and
the anonymous reviewers for helpful discussions and
comments; our annotators Steven An, Lars Backstrom,
Eric Baumer, Jeff Chadwick, Evie Kleinberg, and Myle
Ott; and the makers of Cepacol, Robitussin, and Sudafed,
whose products got us through the submission deadline.
This paper is based upon work supported in part by NSF
grants IIS-0910664, IIS-1016099, Google, and Yahoo!
899
References
[1] Eytan Adar, Li Zhang, Lada A. Adamic, and
Rajan M. Lukose. Implicit structure and the
dynamics of blogspace. In Workshop on the
Weblogging Ecosystem, 2004.
[2] Lars Backstrom, Dan Huttenlocher, Jon Klein-
berg, and Xiangyang Lan. Group formation
in large social networks: Membership, growth,
and evolution. In Proceedings of KDD, 2006.
[3] Elizabeth Bates, Walter Kintsch, Charles R.
Fletcher, and Vittoria Giuliani. The role of
pronominalization and ellipsis in texts: Some
memory experiments. Journal of Experimental
Psychology: Human Learning and Memory, 6
(6):676?691, 1980.
[4] Frank Boers and Seth Lindstromberg. Find-
ing ways to make phrase-learning feasible: The
mnemonic effect of alliteration. System, 33(2):
225?238, 2005.
[5] Samuel D. Bradley and Robert Meeds.
Surface-structure transformations and advertis-
ing slogans: The case for moderate syntactic
complexity. Psychology and Marketing, 19:
595?619, 2002.
[6] Robert Chamblee, Robert Gilmore, Gloria
Thomas, and Gary Soldow. When copy com-
plexity can help ad readership. Journal of Ad-
vertising Research, 33(3):23?23, 1993.
[7] John Colapinto. Famous names. The New
Yorker, pages 38?43, 2011.
[8] Cristian Danescu-Niculescu-Mizil and Lillian
Lee. Chameleons in imagined conversations:
A new approach to understanding coordination
of linguistic style in dialogs. In Proceedings
of the Workshop on Cognitive Modeling and
Computational Linguistics, 2011.
[9] Cristian Danescu-Niculescu-Mizil, Gueorgi
Kossinets, Jon Kleinberg, and Lillian Lee.
How opinions are received by online commu-
nities: A case study on Amazon.com helpful-
ness votes. In Proceedings of WWW, pages
141?150, 2009.
[10] Stuart Fischoff, Esmeralda Cardenas, Angela
Hernandez, Korey Wyatt, Jared Young, and
Rachel Gordon. Popular movie quotes: Re-
flections of a people and a culture. In Annual
Convention of the American Psychological As-
sociation, 2000.
[11] Daniel Gruhl, R. Guha, David Liben-Nowell,
and Andrew Tomkins. Information diffusion
through blogspace. Proceedings of WWW,
pages 491?501, 2004.
[12] Marco Guerini, Carlo Strapparava, and
Oliviero Stock. Trusting politicians? words
(for persuasive NLP). In Proceedings of
CICLing, pages 263?274, 2008.
[13] Marco Guerini, Carlo Strapparava, and Go?zde
O?zbal. Exploring text virality in social net-
works. In Proceedings of ICWSM (poster),
2011.
[14] Marco Guerini, Alberto Pepe, and Bruno
Lepri. Do linguistic style and readability of
scientific abstracts affect their virality? In Pro-
ceedings of ICWSM, 2012.
[15] Richard Jackson Harris, Abigail J. Werth,
Kyle E. Bures, and Chelsea M. Bartel. Social
movie quoting: What, why, and how? Ciencias
Psicologicas, 2(1):35?45, 2008.
[16] Chip Heath, Chris Bell, and Emily Steinberg.
Emotional selection in memes: The case of
urban legends. Journal of Personality, 81(6):
1028?1041, 2001.
[17] R. Reed Hunt. The subtlety of distinctiveness:
What von Restorff really did. Psychonomic
Bulletin & Review, 2(1):105?112, 1995.
[18] Ira E. Hyman Jr. and David C. Rubin. Mem-
orabeatlia: A naturalistic study of long-term
memory. Memory & Cognition, 18(2):205?
214, 1990.
[19] Richard R. Klink. Creating brand names with
meaning: The use of sound symbolism. Mar-
keting Letters, 11(1):5?20, 2000.
[20] Mark L. Knapp, Cynthia Stohl, and Kath-
leen K. Reardon. ?Memorable? mes-
sages. Journal of Communication, 31(4):27?
41, 1981.
[21] Henry Kuc?era and W. Nelson Francis. Compu-
tational analysis of present-day American En-
glish. Dartmouth Publishing Group, 1967.
900
[22] Jure Leskovec, Lada Adamic, and Bernardo
Huberman. The dynamics of viral market-
ing. ACM Transactions on the Web, 1(1), May
2007.
[23] Jure Leskovec, Lars Backstrom, and Jon Klein-
berg. Meme-tracking and the dynamics of the
news cycle. In Proceedings of KDD, pages
497?506, 2009.
[24] Tina M. Lowrey. The relation between
script complexity and commercial memorabil-
ity. Journal of Advertising, 35(3):7?15, 2006.
[25] Rada Mihalcea and Carlo Strapparava. Learn-
ing to laugh (automatically): Computational
models for humor recognition. Computational
Intelligence, 22(2):126?142, 2006.
[26] Milman Parry and Adam Parry. The making of
Homeric verse: The collected papers of Mil-
man Parry. Clarendon Press, Oxford, 1971.
[27] Everett Rogers. Diffusion of Innovations. Free
Press, fourth edition, 1995.
[28] Daniel M. Romero, Brendan Meeder, and Jon
Kleinberg. Differences in the mechanics of
information diffusion across topics: Idioms,
political hashtags, and complex contagion on
Twitter. Proceedings of WWW, pages 695?704,
2011.
[29] David C. Rubin. Very long-term memory for
prose and verse. Journal of Verbal Learning
and Verbal Behavior, 16(5):611?621, 1977.
[30] Nathan Schneider, Rebecca Hwa, Philip Gi-
anfortoni, Dipanjan Das, Michael Heilman,
Alan W. Black, Frederick L. Crabbe, and
Noah A. Smith. Visualizing topical quotations
over time to understand news discourse. Tech-
nical Report CMU-LTI-01-103, CMU, 2010.
[31] David Strang and Sarah Soule. Diffusion in or-
ganizations and social movements: From hy-
brid corn to poison pills. Annual Review of So-
ciology, 24:265?290, 1998.
[32] Hannah Summerfelt, Louis Lippman, and
Ira E. Hyman Jr. The effect of humor on mem-
ory: Constrained by the pun. The Journal of
General Psychology, 137(4), 2010.
[33] Eric Sun, Itamar Rosenn, Cameron Marlow,
and Thomas M. Lento. Gesundheit! Model-
ing contagion through Facebook News Feed. In
Proceedings of ICWSM, 2009.
[34] Annabel Thorn and Mike Page. Interactions
Between Short-Term and Long-Term Memory
in the Verbal Domain. Psychology Press, 2009.
[35] Louis L. Thurstone. A law of comparative
judgment. Psychological Review, 34(4):273?
286, 1927.
[36] Oren Tsur and Ari Rappoport. What?s in
a Hashtag? Content based prediction of the
spread of ideas in microblogging communities.
In Proceedings of WSDM, 2012.
[37] Fang Wu, Bernardo A. Huberman, Lada A.
Adamic, and Joshua R. Tyler. Information flow
in social groups. Physica A: Statistical and
Theoretical Physics, 337(1-2):327?335, 2004.
[38] Shaomei Wu, Jake M. Hofman, Winter A. Ma-
son, and Duncan J. Watts. Who says what to
whom on Twitter. In Proceedings of WWW,
2011.
[39] Jaewon Yang and Jure Leskovec. Patterns of
temporal variation in online media. In Pro-
ceedings of WSDM, 2011.
[40] Eric Yorkston and Geeta Menon. A sound idea:
Phonetic effects of brand names on consumer
judgments. Journal of Consumer Research, 31
(1):43?51, 2004.
901
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 250?259,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A computational approach to politeness with application to social factors
Cristian Danescu-Niculescu-Mizil??, Moritz Sudhof?, Dan Jurafsky?,
Jure Leskovec?, and Christopher Potts?
?Computer Science Department, ?Linguistics Department
??Stanford University, ?Max Planck Institute SWS
cristiand|jure@cs.stanford.edu, sudhof|jurafsky|cgpotts@stanford.edu
Abstract
We propose a computational framework
for identifying linguistic aspects of polite-
ness. Our starting point is a new corpus
of requests annotated for politeness, which
we use to evaluate aspects of politeness
theory and to uncover new interactions
between politeness markers and context.
These findings guide our construction of
a classifier with domain-independent lexi-
cal and syntactic features operationalizing
key components of politeness theory, such
as indirection, deference, impersonaliza-
tion and modality. Our classifier achieves
close to human performance and is effec-
tive across domains. We use our frame-
work to study the relationship between po-
liteness and social power, showing that po-
lite Wikipedia editors are more likely to
achieve high status through elections, but,
once elevated, they become less polite. We
see a similar negative correlation between
politeness and power on Stack Exchange,
where users at the top of the reputation
scale are less polite than those at the bot-
tom. Finally, we apply our classifier to
a preliminary analysis of politeness vari-
ation by gender and community.
1 Introduction
Politeness is a central force in communication, ar-
guably as basic as the pressure to be truthful, in-
formative, relevant, and clear (Grice, 1975; Leech,
1983; Brown and Levinson, 1978). Natural lan-
guages provide numerous and diverse means for
encoding politeness and, in conversation, we con-
stantly make choices about where and how to use
these devices. Kaplan (1999) observes that ?peo-
ple desire to be paid respect? and identifies hon-
orifics and other politeness markers, like please,
as ?the coin of that payment?. In turn, polite-
ness markers are intimately related to the power
dynamics of social interactions and are often a
decisive factor in whether those interactions go
well or poorly (Gyasi Obeng, 1997; Chilton, 1990;
Andersson and Pearson, 1999; Rogers and Lee-
Wong, 2003; Holmes and Stubbe, 2005).
The present paper develops a computational
framework for identifying and characterizing po-
liteness marking in requests. We focus on re-
quests because they involve the speaker imposing
on the addressee, making them ideal for exploring
the social value of politeness strategies (Clark and
Schunk, 1980; Francik and Clark, 1985). Requests
also stimulate extensive use of what Brown and
Levinson (1987) call negative politeness: speaker
strategies for minimizing (or appearing to mini-
mize) the imposition on the addressee, for exam-
ple, by being indirect (Would you mind) or apolo-
gizing for the imposition (I?m terribly sorry, but)
(Lakoff, 1973; Lakoff, 1977; Brown and Levin-
son, 1978).
Our investigation is guided by a new corpus
of requests annotated for politeness. The data
come from two large online communities in which
members frequently make requests of other mem-
bers: Wikipedia, where the requests involve edit-
ing and other administrative functions, and Stack
Exchange, where the requests center around a di-
verse range of topics (e.g., programming, garden-
ing, cycling). The corpus confirms the broad out-
lines of linguistic theories of politeness pioneered
by Brown and Levinson (1987), but it also reveals
new interactions between politeness markings and
the morphosyntactic context. For example, the po-
liteness of please depends on its syntactic position
and the politeness markers it co-occurs with.
Using this corpus, we construct a polite-
ness classifier with a wide range of domain-
independent lexical, sentiment, and dependency
features operationalizing key components of po-
250
liteness theory, including not only the negative
politeness markers mentioned above but also el-
ements of positive politeness (gratitude, positive
and optimistic sentiment, solidarity, and inclusive-
ness). The classifier achieves near human-level ac-
curacy across domains, which highlights the con-
sistent nature of politeness strategies and paves the
way to using the classifier to study new data.
Politeness theory predicts a negative correlation
between politeness and the power of the requester,
where power is broadly construed to include so-
cial status, authority, and autonomy (Brown and
Levinson, 1987). The greater the speaker?s power
relative to her addressee, the less polite her re-
quests are expected to be: there is no need for her
to incur the expense of paying respect, and failing
to make such payments can invoke, and hence re-
inforce, her power. We support this prediction by
applying our politeness framework to Wikipedia
and Stack Exchange, both of which provide in-
dependent measures of social status. We show
that polite Wikipedia editors are more likely to
achieve high status through elections; however,
once elected, they become less polite. Similarly,
on Stack Exchange, we find that users at the top of
the reputation scale are less polite than those at the
bottom.
Finally, we briefly address the question of how
politeness norms vary across communities and so-
cial groups. Our findings confirm established re-
sults about the relationship between politeness and
gender, and they identify substantial variation in
politeness across different programming language
subcommunities on Stack Exchange.
2 Politeness data
Requests involve an imposition on the addressee,
making them a natural domain for studying the
inter-connections between linguistic aspects of po-
liteness and social variables.
Requests in online communities We base our
analysis on two online communities where re-
quests have an important role: the Wikipedia
community of editors and the Stack Exchange
question-answer community.1 On Wikipedia, to
coordinate on the creation and maintenance of
the collaborative encyclopedia, editors can in-
teract with each other on user talk-pages;2 re-
1http://stackexchange.com/about
2http://en.wikipedia.org/wiki/
Wikipedia:User_pages
quests posted on a user talk-page, although pub-
lic, are generally directed to the owner of the talk-
page. On Stack Exchange, users often comment
on existing posts requesting further information or
proposing edits; these requests are generally di-
rected to the authors of the original posts.
Both communities are not only rich in user-
to-user requests, but these requests are also part
of consequential conversations, not empty social
banter; they solicit specific information or con-
crete actions, and they expect a response.
Politeness annotation Computational studies of
politeness, or indeed any aspect of linguistic prag-
matics, demand richly labeled data. We there-
fore label a large portion of our request data
(over 10,000 utterances) using Amazon Mechan-
ical Turk (AMT), creating the largest corpus with
politeness annotations (see Table 1 for details).3
We choose to annotate requests containing ex-
actly two sentences, where the second sentence
is the actual request (and ends with a question
mark). This provides enough context to the an-
notators while also controlling for length effects.
Each annotator was instructed to read a batch of
13 requests and consider them as originating from
a co-worker by email. For each request, the anno-
tator had to indicate how polite she perceived the
request to be by using a slider with values rang-
ing from ?very impolite? to ?very polite?.4 Each
request was labeled by five different annotators.
We vetted annotators by restricting their resi-
dence to be in the U.S. and by conducting a lin-
guistic background questionnaire. We also gave
them a paraphrasing task shown to be effective
for verifying and eliciting linguistic attentiveness
(Munro et al, 2010), and we monitored the an-
notation job and manually filtered out annotators
who submitted uniform or seemingly random an-
notations.
Because politeness is highly subjective and an-
notators may have inconsistent scales, we ap-
plied the standard z-score normalization to each
worker?s scores. Finally, we define the politeness
score (henceforth politeness) of a request as the
average of the five scores assigned by the annota-
tors. The distribution of resulting request scores
(shown in Figure 1) has an average of 0 and stan-
3Publicly available at http://www.mpi-sws.org/
?cristian/Politeness.html4We used non-categorical ratings for finer granularity and
to help account for annotators? different perception scales.
251
domain #requests #annotated #annotators
Wiki 35,661 4,353 219
SE 373,519 6,604 212
Table 1: Summary of the request data and its po-
liteness annotations.
Figure 1: Distribution of politeness scores. Posi-
tive scores indicate requests perceived as polite.
dard deviation of 0.7 for both domains; positive
values correspond to polite requests (i.e., requests
with normalized annotations towards the ?very po-
lite? extreme) and negative values to impolite re-
quests. A summary of all our request data is shown
in Table 1.
Inter-annotator agreement To evaluate the re-
liability of the annotations we measure the inter-
annotator agreement by computing, for each batch
of 13 documents that were annotated by the same
set of 5 users, the mean pairwise correlation of the
respective scores. For reference, we compute the
same quantities after randomizing the scores by
sampling from the observed distribution of polite-
ness scores. As shown in Figure 2, the labels are
coherent and significantly different from the ran-
domized procedure (p < 0.0001 according to a
Wilcoxon signed rank test).5
Binary perception Although we did not im-
pose a discrete categorization of politeness, we
acknowledge an implicit binary perception of the
phenomenon: whenever an annotator moved a
slider in one direction or the other, she made a
binary politeness judgment. However, the bound-
5The commonly used Cohen/Fleiss Kappa agreement
measures are not suitable for this type of annotation, in which
labels are continuous rather than categorical.
Figure 2: Inter-annotator pairwise correlation,
compared to the same measure after randomizing
the scores.
Quartile: 1st 2nd 3rd 4th
Wiki 62% 8% 3% 51%
SE 37% 4% 6% 46%
Table 2: The percentage of requests for which all
five annotators agree on binary politeness. The
4th quartile contains the requests with the top 25%
politeness scores in the data. (For reference, ran-
domized scoring yields agreement percentages of
<20% for all quartiles.)
ary between somewhat polite and somewhat im-
polite requests can be blurry. To test this intuition,
we break the set of annotated requests into four
groups, each corresponding to a politeness score
quartile. For each quartile, we compute the per-
centage of requests for which all five annotators
made the same binary politeness judgment. As
shown in Table 2, full agreement is much more
common in the 1st (bottom) and 4th (top) quar-
tiles than in the middle quartiles. This suggests
that the politeness scores assigned to requests that
are only somewhat polite or somewhat impolite
are less reliable and less tied to an intuitive notion
of binary politeness. This discrepancy motivates
our choice of classes in the prediction experiments
(Section 4) and our use of the top politeness quar-
tile (the 25% most polite requests) as a reference
in our subsequent discussion.
3 Politeness strategies
As we mentioned earlier, requests impose on the
addressee, potentially placing her in social peril if
she is unwilling or unable to comply. Requests
therefore naturally give rise to the negative po-
252
liteness strategies of Brown and Levinson (1987),
which are attempts to mitigate these social threats.
These strategies are prominent in Table 3, which
describes the core politeness markers we analyzed
in our corpus of Wikipedia requests. We do not
include the Stack Exchange data in this analysis,
reserving it as a ?test community? for our predic-
tion task (Section 4).
Requests exhibiting politeness markers are au-
tomatically extracted using regular expression
matching on the dependency parse obtained by the
Stanford Dependency Parser (de Marneffe et al,
2006), together with specialized lexicons. For ex-
ample, for the hedges marker (Table 3, line 19),
we match all requests containing a nominal subject
dependency edge pointing out from a hedge verb
from the hedge list created by Hyland (2005). For
each politeness strategy, Table 3 shows the aver-
age politeness score of the respective requests (as
described in Section 2; positive numbers indicate
polite requests), and their top politeness quartile
membership (i.e., what percentage fall within the
top quartile of politeness scores). As discussed at
the end of Section 2, the top politeness quartile
gives a more robust and more intuitive measure of
politeness. For reference, a random sample of re-
quests will have a 0 politeness score and a 25% top
quartile membership; in both cases, larger num-
bers indicate higher politeness.
Gratitude and deference (lines 1?2) are ways
for the speaker to incur a social cost, helping to
balance out the burden the request places on the
addressee. Adopting Kaplan (1999)?s metaphor,
these are the coin of the realm when it comes to
paying the addressee respect. Thus, they are indi-
cators of positive politeness.
Terms from the sentiment lexicon (Liu et al,
2005) are also tools for positive politeness, either
by emphasizing a positive relationship with the ad-
dressee (line 4), or being impolite by using nega-
tive sentiment that damages this positive relation-
ship (line 5). Greetings (line 3) are another way to
build a positive relationship with the addressee.
The remainder of the cues in Table 3 are neg-
ative politeness strategies, serving the purpose of
minimizing, at least in appearance, the imposition
on the addressee. Apologizing (line 6) deflects the
social threat of the request by attuning to the impo-
sition itself. Being indirect (line 9) is another way
to minimize social threat. This strategy allows the
speaker to avoid words and phrases convention-
ally associated with requests. First-person plural
forms like we and our (line 15) are also ways of
being indirect, as they create the sense that the
burden of the request is shared between speaker
and addressee (We really should . . . ). Though in-
directness is not invariably interpreted as polite-
ness marking (Blum-Kulka, 2003), it is nonethe-
less a reliable marker of it, as our scores indicate.
What?s more, direct variants (imperatives, state-
ments about the addressee?s obligations) are less
polite (lines 10?11).
Indirect strategies also combine with hedges
(line 19) conveying that the addressee is unlikely
to accept the burden (Would you by any chance
. . . ?, Would it be at all possible . . . ?). These too
serve to provide the addressee with a face-saving
way to deny the request. We even see subtle effects
of modality at work here: the irrealis, counterfac-
tual forms would and could are more polite than
their ability (dispositional) or future-oriented vari-
ants can and will; compare lines 12 and 13. This
parallels the contrast between factuality markers
(impolite; line 20) and hedging (polite; line 19).
Many of these features are correlated with each
other, in keeping with the insight of Brown and
Levinson (1987) that politeness markers are of-
ten combined to create a cumulative effect of in-
creased politeness. Our corpora also highlight in-
teractions that are unexpected (or at least unac-
counted for) on existing theories of politeness. For
example, sentence-medial please is polite (line 7),
presumably because of its freedom to combine
with other negative politeness strategies (Could
you please . . . ). In contrast, sentence-initial please
is impolite (line 8), because it typically signals a
more direct strategy (Please do this), which can
make the politeness marker itself seem insincere.
We see similar interactions between pronominal
forms and syntactic structure: sentence-initial you
is impolite (You need to . . . ), whereas sentence-
medial you is often part of the indirect strategies
we discussed above (Would/Could you . . . ).
4 Predicting politeness
We now show how our linguistic analysis can be
used in a machine learning model for automati-
cally classifying requests according to politeness.
A classifier can help verify the predictive power,
robustness, and domain-independent generality of
the linguistic strategies of Section 3. Also, by pro-
viding automatic politeness judgments for large
253
Strategy Politeness In top quartile Example
1. Gratitude 0.87*** 78%*** I really appreciate that you?ve done them.
2. Deference 0.78*** 70%*** Nice work so far on your rewrite.
3. Greeting 0.43*** 45%*** Hey, I just tried to . . .
4. Positive lexicon 0.12*** 32%*** Wow! / This is a great way to deal. . .
5. Negative lexicon -0.13*** 22%** If you?re going to accuse me . . .
6. Apologizing 0.36*** 53%*** Sorry to bother you . . .
7. Please 0.49*** 57%*** Could you please say more. . .
8. Please start ?0.30* 22% Please do not remove warnings . . .
9. Indirect (btw) 0.63*** 58%** By the way, where did you find . . .
10. Direct question ?0.27*** 15%*** What is your native language?
11. Direct start ?0.43*** 9%*** So can you retrieve it or not?
12. Counterfactual modal 0.47*** 52%*** Could/Would you . . .
13. Indicative modal 0.09 27% Can/Will you . . .
14. 1st person start 0.12*** 29%** I have just put the article . . .
15. 1st person pl. 0.08* 27% Could we find a less complex name . . .
16. 1st person 0.08*** 28%*** It is my view that ...
17. 2nd person 0.05*** 30%*** But what?s the good source you have in mind?
18. 2nd person start ?0.30*** 17%** You?ve reverted yourself . . .
19. Hedges 0.14*** 28% I suggest we start with . . .
20. Factuality ?0.38*** 13%*** In fact you did link, . . .
Table 3: Positive (1-5) and negative (6?20) politeness strategies and their relation to human perception of
politeness. For each strategy we show the average (human annotated) politeness scores for the requests
exhibiting that strategy (compare with 0 for a random sample of requests; a positive number indicates
the strategy is perceived as being polite), as well as the percentage of requests exhibiting the respective
strategy that fall in the top quartile of politeness scores (compare with 25% for a random sample of
requests). Throughout the paper: for politeness scores, statistical significance is calculated by comparing
the set of requests exhibiting the strategy with the rest using a Mann-Whitney-Wilcoxon U test; for top
quartile membership a binomial test is used.
amounts of new data on a scale unfeasible for hu-
man annotation, it can also enable a detailed anal-
ysis of the relation between politeness and social
factors (Section 5).
Task setup To evaluate the robustness and
domain-independence of the analysis from Sec-
tion 3, we run our prediction experiments on two
very different domains. We treat Wikipedia as a
?development domain? since we used it for de-
veloping and identifying features and for training
our models. Stack Exchange is our ?test domain?
since it was not used for identifying features. We
take the model (features and weights) trained on
Wikipedia and use them to classify requests from
Stack Exchange.
We consider two classes of requests: polite
and impolite, defined as the top and, respectively,
bottom quartile of requests when sorted by their
politeness score (based on the binary notion of
politeness discussed in Section 2). The classes
are therefore balanced, with each class consisting
of 1,089 requests for the Wikipedia domain and
1,651 requests for the Stack Exchange domain.
We compare two classifiers ? a bag of words
classifier (BOW) and a linguistically informed
classifier (Ling.) ? and use human labelers as a
reference point. The BOW classifier is an SVM
using a unigram feature representation.6 We con-
sider this to be a strong baseline for this new
6Unigrams appearing less than 10 times are excluded.
254
classification task, especially considering the large
amount of training data available. The linguisti-
cally informed classifier (Ling.) is an SVM using
the linguistic features listed in Table 3 in addition
to the unigram features. Finally, to obtain a ref-
erence point for the prediction task we also collect
three new politeness annotations for each of the re-
quests in our dataset using the same methodology
described in Section 2. We then calculate human
performance on the task (Human) as the percent-
age of requests for which the average score from
the additional annotations matches the binary po-
liteness class of the original annotations (e.g., a
positive score corresponds to the polite class).
Classification results We evaluate the classi-
fiers both in an in-domain setting, with a standard
leave-one-out cross validation procedure, and in a
cross-domain setting, where we train on one do-
main and test on the other (Table 4). For both our
development and our test domains, and in both the
in-domain and cross-domain settings, the linguis-
tically informed features give 3-4% absolute im-
provement over the bag of words model. While
the in-domain results are within 3% of human per-
formance, the greater room for improvement in the
cross-domain setting motivates further research on
linguistic cues of politeness.
The experiments in this section confirm that
our theory-inspired features are indeed effective in
practice, and generalize well to new domains. In
the next section we exploit this insight to automat-
ically annotate a much larger set of requests (about
400,000) with politeness labels, enabling us to re-
late politeness to several social variables and out-
comes. For new requests, we use class probabil-
ity estimates obtained by fitting a logistic regres-
sion model to the output of the SVM (Witten and
Frank, 2005) as predicted politeness scores (with
values between 0 and 1; henceforth politeness, by
abuse of language).
5 Relation to social factors
We now apply our framework to studying the rela-
tionship between politeness and social variables,
focussing on social power dynamics. Encour-
aged by the close-to-human performance of our
in-domain classifiers, we use them to assign po-
liteness labels to our full dataset and then compare
these labels to independent measures of power and
status in our data. The results closely match those
obtained with human-labeled data alone, thereby
In-domain Cross-domain
Train Wiki SE Wiki SE
Test Wiki SE SE Wiki
BOW 79.84% 74.47% 64.23% 72.17%
Ling. 83.79% 78.19% 67.53% 75.43%
Human 86.72% 80.89% 80.89% 86.72%
Table 4: Accuracies of our two classifiers for
Wikipedia (Wiki) and Stack Exchange (SE), for
in-domain and cross-domain settings. Human per-
formance is included as a reference point. The ran-
dom baseline performance is 50%.
supporting the use of computational methods to
pursue questions about social variables.
5.1 Relation to social outcome
Earlier, we characterized politeness markings as
currency used to pay respect. Such language is
therefore costly in a social sense, and, relatedly,
tends to incur costs in terms of communicative ef-
ficiency (Van Rooy, 2003). Are these costs worth
paying? We now address this question by studying
politeness in the context of the electoral system of
the Wikipedia community of editors.
Among Wikipedia editors, status is a salient so-
cial variable (Anderson et al, 2012). Administra-
tors (admins) are editors who have been granted
certain rights, including the ability to block other
editors and to protect or delete articles.7 Ad-
mins have a higher status than common editors
(non-admins), and this distinction seems to be
widely acknowledged by the community (Burke
and Kraut, 2008b; Leskovec et al, 2010; Danescu-
Niculescu-Mizil et al, 2012). Aspiring editors
become admins through public elections,8 so we
know when the status change from non-admin to
admins occurred and can study users? language
use in relation to that time.
To see whether politeness correlates with even-
tual high status, we compare, in Table 5, the po-
liteness levels of requests made by users who will
eventually succeed in becoming administrators
(Eventual status: Admins) with requests made by
users who are not admins (Non-admins).9 We ob-
serve that admins-to-be are significantly more po-
7http://en.wikipedia.org/wiki/
Wikipedia:Administrators
8http://en.wikipedia.org/wiki/
Wikipedia:Requests_for_adminship
9We consider only requests made up to one month before
the election, to avoid confusion with pre-election behavior.
255
Eventual status Politeness Top quart.
Admins 0.46** 30%***
Non-admins 0.39*** 25%
Failed 0.37** 22%
Table 5: Politeness and status. Editors who
will eventually become admins are more polite
than non-admins (p<0.001 according to a Mann-
Whitney-Wilcoxon U test) and than editors who
will eventually fail to become admins (p<0.001).
Out of their requests, 30% are rated in the top po-
liteness quartile (significantly more than the 25%
of a random sample; p<0.001 according to a bi-
nomial test). This analysis was conducted on 31k
requests (1.4k for Admins, 28.9k for Non-admins,
652 for Failed).
lite than non-admins. One might wonder whether
this merely reflects the fact that not all users aspire
to become admins, and those that do are more po-
lite. To address this, we also consider users who
ran for adminship but did not earn community ap-
proval (Eventual status: Failed). These users are
also significantly less polite than their successful
counterparts, indicating that politeness indeed cor-
relates with a positive social outcome here.
5.2 Politeness and power
We expect a rise in status to correlate with a de-
cline in politeness (as predicted by politeness the-
ory, and discussed in Section 1). The previous sec-
tion does not test this hypothesis, since all editors
compared in Table 5 had the same (non-admin)
status when writing the requests. However, our
data does provide three ways of testing this hy-
pothesis.
First, after the adminship elections, successful
editors get a boost in power by receiving admin
privileges. Figure 3 shows that this boost is mir-
rored by a significant decrease in politeness (blue,
diamond markers). Losing an election has the op-
posite effect on politeness (red, circle markers),
perhaps as a consequence of reinforced low status.
Second, Stack Exchange allows us to test more
situational power effects.10 On the site, users re-
quest, from the community, information they are
lacking. This informational asymmetry between
the question-asker and his audience puts him at
10We restrict all experiments in this section to the largest
subcommunity of Stack Exchange, namely Stack Overflow.
Before election Election After election
0.41
0.37
0.39
0.46
Pred
icte
d po
liten
ess 
sco
res
Successful candidatesFailed candidates
Figure 3: Successful and failed candidates be-
fore and after elections. Editors that will even-
tually succeed (diamond marker) are significantly
more polite than those that will fail (circle mark-
ers). Following the elections, successful editors
become less polite while unsuccessful editors be-
come more polite.
a social disadvantage. We therefore expect the
question-asker to be more polite than the people
who respond. Table 6 shows that this expectation
is born out: comments posted to a thread by the
original question-asker are more polite than those
posted by other users.
Role Politeness Top quart.
Question-asker 0.65*** 32%***
Answer-givers 0.52*** 20%***
Table 6: Politeness and dependence. Requests
made in comments posted by the question-asker
are significantly more polite than the other re-
quests. Analysis conducted on 181k requests
(106k for question-askers, 75k for answer-givers).
Third, Stack Exchange allows us to examine
power in the form of authority, through the com-
munity?s reputation system. Again, we see a neg-
ative correlation between politeness and power,
even after controlling for the role of the user mak-
ing the requests (i.e., Question-asker or Answer-
giver). Table 7 summarizes the results.11
Human validation The above analyses are
based on predicted politeness from our classifier.
This allows us to use the entire request data cor-
11Since our data does not contain time stamps for reputa-
tion scores, we only consider requests that were issued in the
six months prior to the available snapshot.
256
Reputation level Politeness Top quart.
Low reputation 0.68*** 27%***
Middle reputation 0.66*** 25%
High reputation 0.64*** 23%***
Table 7: Politeness and Stack Exchange reputation
(texts by question-askers only). High-reputation
users are less polite. Analysis conducted on 25k
requests (4.5k low, 12.5k middle, 8.4k high).
pus to test our hypotheses and to apply precise
controls to our experiments (such as restricting
our analysis to question-askers in the reputation
experiment). In order to validate this methodol-
ogy, we turned again to human annotation: we
collected additional politeness annotation for the
types of requests involved in the newly designed
experiments. When we re-ran our experiments on
human-labeled data alone we obtained the same
qualitative results, with statistical significance al-
ways lower than 0.01.12
Prediction-based interactions The human val-
idation of classifier-based results suggests that
our prediction framework can be used to explore
differences in politeness levels across factors of
interest, such as communities, geographical re-
gions and gender, even where gathering suffi-
cient human-annotated data is infeasible. We
mention just a few such preliminary results here:
(i) Wikipedians from the U.S. Midwest are most
polite (when compared to other census-defined
regions), (ii) female Wikipedians are generally
more polite (consistent with prior studies in which
women are more polite in a variety of domains;
(Herring, 1994)), and (iii) programming language
communities on Stack Exchange vary significantly
by politeness (Table 8; full disclosure: our analy-
ses were conducted in Python).
6 Related work
Politeness has been a central concern of modern
pragmatic theory since its inception (Grice, 1975;
Lakoff, 1973; Lakoff, 1977; Leech, 1983; Brown
and Levinson, 1978), because it is a source of
pragmatic enrichment, social meaning, and cul-
tural variation (Harada, 1976; Matsumoto, 1988;
12However, due to the limited size of the human-labeled
data, we could not control for the role of the user in the Stack
Exchange reputation experiment.
PL name Politeness Top quartile
Python 0.47*** 23%
Perl 0.49 24%
PHP 0.51 24%
Javascript 0.53** 26%**
Ruby 0.59*** 28%*
Table 8: Politeness of requests from different lan-
guage communities on Stack Exchange.
Ide, 1989; Blum-Kulka and Kasper, 1990; Blum-
Kulka, 2003; Watts, 2003; Byon, 2006). The start-
ing point for most research is the theory of Brown
and Levinson (1987). Aspects of this theory
have been explored from game-theoretic perspec-
tives (Van Rooy, 2003) and implemented in lan-
guage generation systems for interactive narratives
(Walker et al, 1997), cooking instructions, (Gupta
et al, 2007), translation (Faruqui and Pado, 2012),
spoken dialog (Wang et al, 2012), and subjectivity
analysis (Abdul-Mageed and Diab, 2012), among
others.
In recent years, politeness has been studied in
online settings. Researchers have identified vari-
ation in politeness marking across different con-
texts and media types (Herring, 1994; Brennan
and Ohaeri, 1999; Duthler, 2006) and between
different social groups (Burke and Kraut, 2008a).
The present paper pursues similar goals using or-
ders of magnitude more data, which facilitates a
fuller survey of different politeness strategies.
Politeness marking is one aspect of the broader
issue of how language relates to power and status,
which has been studied in the context of workplace
discourse (Bramsen et al, ; Diehl et al, 2007;
Peterson et al, 2011; Prabhakaran et al, 2012;
Gilbert, 2012; McCallum et al, 2007) and so-
cial networking (Scholand et al, 2010). However,
this research focusses on domain-specific textual
cues, whereas the present work seeks to lever-
age domain-independent politeness cues, build-
ing on the literature on how politeness affects
worksplace social dynamics and power structures
(Gyasi Obeng, 1997; Chilton, 1990; Andersson
and Pearson, 1999; Rogers and Lee-Wong, 2003;
Holmes and Stubbe, 2005). Burke and Kraut
(2008b) study the question of how and why spe-
cific individuals rise to administrative positions
on Wikipedia, and Danescu-Niculescu-Mizil et al
(2012) show that power differences on Wikipedia
257
are revealed through aspects of linguistic accom-
modation. The present paper complements this
work by revealing the role of politeness in social
outcomes and power relations.
7 Conclusion
We construct and release a large collection of
politeness-annotated requests and use it to evalu-
ate key aspects of politeness theory. We build a
politeness classifier that achieves near-human per-
formance and use it to explore the relation between
politeness and social factors such as power, status,
gender, and community membership. We hope the
publicly available collection of annotated requests
enables further study of politeness and its relation
to social factors, as this paper has only begun to
explore this area.
Acknowledgments
We thank Jean Wu for running the AMT an-
notation task, and all the participating turkers.
We thank Diana Minculescu and the anonymous
reviewers for their helpful comments. This
work was supported in part by NSF IIS-1016909,
CNS-1010921, IIS-1149837, IIS-1159679, ARO
MURI, DARPA SMISC, Okawa Foundation, Do-
como, Boeing, Allyes, Volkswagen, Intel, Alfred
P. Sloan Fellowship, the Microsoft Faculty Fel-
lowship, the Gordon and Dailey Pattee Faculty
Fellowship, and the Center for Advanced Study in
the Behavioral Sciences at Stanford.
References
Muhammad Abdul-Mageed and Mona Diab. 2012.
AWATIF: A multi-genre corpus for Modern Stan-
dard Arabic subjectivity and sentiment analysis. In
Proceedings of LREC, pages 3907?3914.
Ashton Anderson, Daniel Huttenlocher, Jon Kleinberg,
and Jure Leskovec. 2012. Effects of user similarity
in social media. In Proceedings of WSDM, pages
703?712.
Lynne M. Andersson and Christine M. Pearson. 1999.
Tit for tat? the spiraling effect of incivility in the
workplace. The Academy of Management Review,
24(3):452?471.
Shoshana Blum-Kulka and Gabriele Kasper. 1990.
Special issue on politeness. Journal of Pragmatics,
144(2).
Shoshana Blum-Kulka. 2003. Indirectness and po-
liteness in requests: Same or different? Journal of
Pragmatics, 11(2):131?146.
Philip Bramsen, Martha Escobar-Molana, Ami Patel,
and Rafael Alonso. Extracting social power rela-
tionships from natural language. In Proceedings of
ACL, pages 773?782.
Susan E Brennan and Justina O Ohaeri. 1999. Why
do electronic conversations seem less polite? the
costs and benefits of hedging. SIGSOFT Softw. Eng.
Notes, 24(2):227?235.
Penelope Brown and Stephen C. Levinson. 1978.
Universals in language use: Politeness phenomena.
In Esther N. Goody, editor, Questions and Polite-
ness: Strategies in Social Interaction, pages 56?311,
Cambridge. Cambridge University Press.
Penelope Brown and Stephen C Levinson. 1987. Po-
liteness: some universals in language usage. Cam-
bridge University Press.
Moira Burke and Robert Kraut. 2008a. Mind your
Ps and Qs: the impact of politeness and rudeness
in online communities. In Proceedings of CSCW,
pages 281?284.
Moira Burke and Robert Kraut. 2008b. Taking up the
mop: identifying future wikipedia administrators. In
CHI ?08 extended abstracts on Human factors in
computing systems, pages 3441?3446.
Andrew Sangpil Byon. 2006. The role of linguistic in-
directness and honorifics in achieving linguistic po-
liteness in Korean requests. Journal of Politeness
Research, 2(2):247?276.
Paul Chilton. 1990. Politeness, politics, and diplo-
macy. Discourse and Society, 1(2):201?224.
Herbert H. Clark and Dale H. Schunk. 1980. Polite
responses to polite requests. Cognition, 8(1):111?
143.
Cristian Danescu-Niculescu-Mizil, Lillian Lee,
Bo Pang, and Jon Kleinberg. 2012. Echoes of
power: Language effects and power differences in
social interaction. In Proceedings of WWW, pages
699?708.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, pages 449?454.
Christopher P. Diehl, Galileo Namata, and Lise Getoor.
2007. Relationship identification for social network
discovery. In Proceedings of the AAAI Workshop on
Enhanced Messaging, pages 546?552.
Kirk W Duthler. 2006. The Politeness of Requests
Made Via Email and Voicemail: Support for the Hy-
perpersonal Model. Journal of Computer-Mediated
Communication, 11(2):500?521.
Manaal Faruqui and Sebastian Pado. 2012. Towards a
model of formal and informal address in english. In
Proceedings of EACL, pages 623?633.
258
Elen P. Francik and Herbert H. Clark. 1985. How to
make requests that overcome obstacles to compli-
ance. Journal of Memory and Language, 24:560?
568.
Eric Gilbert. 2012. Phrases that signal workplace hier-
archy. In Proceedings of CSCW, pages 1037?1046.
H. Paul Grice. 1975. Logic and conversation. In Pe-
ter Cole and Jerry Morgan, editors, Syntax and Se-
mantics, volume 3: Speech Acts, pages 43?58. Aca-
demic Press, New York.
S Gupta, M Walker, and D Romano. 2007. How rude
are you?: Evaluating politeness and affect in inter-
action. Affective Computing and Intelligent Interac-
tion, pages 203?217.
Samuel Gyasi Obeng. 1997. Language and politics:
Indirectness in political discourse. Discourse and
Society, 8(1):49?83.
S. I. Harada. 1976. Honorifics. In Masayoshi
Shibatani, editor, Syntax and Semantics, volume
5: Japanese Generative Grammar, pages 499?561.
Academic Press, New York.
Susan Herring. 1994. Politeness in computer cul-
ture: Why women thank and men flame. In Cul-
tural performances: Proceedings of the third Berke-
ley women and language conference, volume 278,
page 94.
Janet Holmes and Maria Stubbe. 2005. Power and Po-
liteness in the Workplace: A Sociolinguistic Analysis
of Talk at Work. Longman, London.
Ken Hyland. 2005. Metadiscourse: Exploring Interac-
tion in Writing. Continuum, London and New York.
Sachiko Ide. 1989. Formal forms and discernment:
Two neglected aspects of universals of linguistic po-
liteness. Multilingua, 8(2?3):223?248.
David Kaplan. 1999. What is meaning? Explorations
in the theory of Meaning as Use. Brief version ?
draft 1. Ms., UCLA.
Robin Lakoff. 1973. The logic of politeness; or, mid-
ing your P?s and Q?s. In Proceedings of the 9th
Meeting of the Chicago Linguistic Society, pages
292?305.
Robin Lakoff. 1977. What you can do with words:
Politeness, pragmatics and performatives. In Pro-
ceedings of the Texas Conference on Performatives,
Presuppositions and Implicatures, pages 79?106.
Geoffrey N. Leech. 1983. Principles of Pragmatics.
Longman, London and New York.
Jure Leskovec, Daniel Huttenlocher, and Jon Klein-
berg. 2010. Governance in Social Media: A case
study of the Wikipedia promotion process. In Pro-
ceedings of ICWSM, pages 98?105.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion Observer: analyzing and comparing opin-
ions on the Web. In Proceedings of WWW, pages
342?351.
Yoshiko Matsumoto. 1988. Reexamination of the uni-
versality of face: Politeness phenomena in Japanese.
Journal of Pragmatics, 12(4):403?426.
Andrew McCallum, Xuerui Wang, and Andr?es
Corrada-Emmanuel. 2007. Topic and role discovery
in social networks with experiments on Enron and
academic email. Journal of Artificial Intelligence
Research, 30(1):249?272.
Robert Munro, Steven Bethard, Victor Kuperman,
Vicky Tzuyin Lai, Robin Melnick, Christopher
Potts, Tyler Schnoebelen, and Harry Tily. 2010.
Crowdsourcing and language studies: the new gen-
eration of linguistic data. In Proceedings of the
NAACL HLT 2010 Workshop on Creating Speech
and Language Data with Amazon?s Mechanical
Turk, pages 122?130.
Kelly Peterson, Matt Hohensee, and Fei Xia. 2011.
Email formality in the workplace: A case study on
the enron corpus. In Proceedings of the ACL Work-
shop on Language in Social Media, pages 86?95.
Vinodkumar Prabhakaran, Owen Rambow, and Mona
Diab. 2012. Predicting Overt Display of Power in
Written Dialogs. In Proceedings of NAACL-HLT,
pages 518?522.
Priscilla S. Rogers and Song Mei Lee-Wong. 2003.
Reconceptualizing politeness to accommodate dy-
namic tensions in subordinate-to-superior reporting.
Journal of Business and Technical Communication,
17(4):379?412.
Andrew J. Scholand, Yla R. Tausczik, and James W.
Pennebaker. 2010. Social language network analy-
sis. In Proceedings of CSCW, pages 23?26.
Robert Van Rooy. 2003. Being polite is a handicap:
Towards a game theoretical analysis of polite lin-
guistic behavior. In Proceedings of TARK, pages
45?58.
Marilyn A Walker, Janet E Cahn, and Stephen J Whit-
taker. 1997. Improvising linguistic style: social and
affective bases for agent personality. In Proceedings
of AGENTS, pages 96?105.
William Yang Wang, Samantha Finkelstein, Amy
Ogan, Alan W. Black, and Justine Cassell. 2012.
?love ya, jerkface?: Using sparse log-linear mod-
els to build positive and impolite relationships with
teens. In Proceedings of SIGDIAL, pages 20?29.
Richard J. Watts. 2003. Politeness. Cambridge Uni-
versity Press, Cambridge.
Ian H Witten and Eibe Frank. 2005. Data Mining:
Practical machine learning tools and techniques.
Morgan Kaufmann.
259
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1650?1659,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Linguistic Models for Analyzing and Detecting Biased Language
Marta Recasens
Stanford University
recasens@google.com
Cristian Danescu-Niculescu-Mizil
Stanford University
Max Planck Institute SWS
cristiand@cs.stanford.edu
Dan Jurafsky
Stanford University
jurafsky@stanford.edu
Abstract
Unbiased language is a requirement for
reference sources like encyclopedias and
scientific texts. Bias is, nonetheless, ubiq-
uitous, making it crucial to understand its
nature and linguistic realization and hence
detect bias automatically. To this end we
analyze real instances of human edits de-
signed to remove bias from Wikipedia ar-
ticles. The analysis uncovers two classes
of bias: framing bias, such as praising or
perspective-specific words, which we link
to the literature on subjectivity; and episte-
mological bias, related to whether propo-
sitions that are presupposed or entailed in
the text are uncontroversially accepted as
true. We identify common linguistic cues
for these classes, including factive verbs,
implicatives, hedges, and subjective inten-
sifiers. These insights help us develop fea-
tures for a model to solve a new prediction
task of practical importance: given a bi-
ased sentence, identify the bias-inducing
word. Our linguistically-informed model
performs almost as well as humans tested
on the same task.
1 Introduction
Writers and editors of reference works such as
encyclopedias, textbooks, and scientific articles
strive to keep their language unbiased. For ex-
ample, Wikipedia advocates a policy called neu-
tral point of view (NPOV), according to which
articles should represent ?fairly, proportionately,
and as far as possible without bias, all signifi-
cant views that have been published by reliable
sources? (Wikipedia, 2013b). Wikipedia?s style
guide asks editors to use nonjudgmental language,
to indicate the relative prominence of opposing
points of view, to avoid presenting uncontroversial
facts as mere opinion, and, conversely, to avoid
stating opinions or contested assertions as facts.
Understanding the linguistic realization of bias
is important for linguistic theory; automatically
detecting these biases is equally significant for
computational linguistics. We propose to ad-
dress both by using a powerful resource: edits in
Wikipedia that are specifically designed to remove
bias. Since Wikipedia maintains a complete revi-
sion history, the edits associated with NPOV tags
allow us to compare the text in its biased (before)
and unbiased (after) form, helping us better under-
stand the linguistic realization of bias. Our work
thus shares the intuition of prior NLP work apply-
ing Wikipedia?s revision history (Nelken and Ya-
mangil, 2008; Yatskar et al, 2010; Max and Wis-
niewski, 2010; Zanzotto and Pennacchiotti, 2010).
The analysis of Wikipedia?s edits provides valu-
able linguistic insights into the nature of biased
language. We find two major classes of bias-driven
edits. The first, framing bias, is realized by sub-
jective words or phrases linked with a particular
point of view. In (1), the term McMansion, unlike
homes, appeals to a negative attitude toward large
and pretentious houses. The second class, episte-
mological bias, is related to linguistic features that
subtly (often via presupposition) focus on the be-
lievability of a proposition. In (2), the assertive
stated removes the bias introduced by claimed,
which casts doubt on Kuypers? statement.
(1) a. Usually, smaller cottage-style houses have been de-
molished to make way for these McMansions.
b. Usually, smaller cottage-style houses have been de-
molished to make way for these homes.
(2) a. Kuypers claimed that the mainstream press in Amer-
ica tends to favor liberal viewpoints.
b. Kuypers stated that the mainstream press in America
tends to favor liberal viewpoints.
Bias is linked to the lexical and grammatical cues
identified by the literature on subjectivity (Wiebe
et al, 2004; Lin et al, 2011), sentiment (Liu et
al., 2005; Turney, 2002), and especially stance
1650
or ?arguing subjectivity? (Lin et al, 2006; So-
masundaran and Wiebe, 2010; Yano et al, 2010;
Park et al, 2011; Conrad et al, 2012). For ex-
ample, like stance, framing bias is realized when
the writer of a text takes a particular position on
a controversial topic and uses its metaphors and
vocabulary. But unlike the product reviews or de-
bate articles that overtly use subjective language,
editors in Wikipedia are actively trying to avoid
bias, and hence biases may appear more subtly,
in the form of covert framing language, or pre-
suppositions and entailments that may not play as
important a role in other genres. Our linguistic
analysis identifies common classes of these subtle
bias cues, including factive verbs, implicatives and
other entailments, hedges, and subjective intensi-
fiers.
Using these cues could help automatically de-
tect and correct instances of bias, by first finding
biased phrases, then identifying the word that in-
troduces the bias, and finally rewording to elim-
inate the bias. In this paper we propose a so-
lution for the second of these tasks, identifying
the bias-inducing word in a biased phrase. Since,
as we show below, this task is quite challenging
for humans, our system has the potential to be
very useful in improving the neutrality of refer-
ence works like Wikipedia. Tested on a subset of
non-neutral sentences from Wikipedia, our model
achieves 34% accuracy?and up to 59% if the
top three guesses are considered?on this difficult
task, outperforming four baselines and nearing hu-
mans tested on the same data.
2 Analyzing a Dataset of Biased
Language
We begin with an empirical analysis based on
Wikipedia?s bias-driven edits. This section de-
scribes the data, and summarizes our linguistic
analysis.1
2.1 The NPOV Corpus from Wikipedia
Given Wikipedia?s strict enforcement of an NPOV
policy, we decided to build the NPOV corpus,
containing Wikipedia edits that are specifically de-
signed to remove bias. Editors are encouraged to
identify and rewrite biased passages to achieve a
more neutral tone, and they can use several NPOV
1The data and bias lexicon we developed are available at
http://www.mpi-sws.org/?cristian/Biased_
language.html
Data Articles Revisions Words Edits Sents
Train 5997 2238K 11G 13807 1843
Dev 653 210K 0.9G 1261 163
Test 814 260K 1G 1751 230
Total 7464 2708K 13G 16819 2235
Table 1: Statistics of the NPOV corpus, extracted
from Wikipedia. (Edits refers to bias-driven ed-
its, i.e., with an NPOV comment. Sents refers to
sentences with a one-word bias-driven edit.)
tags to mark biased content.2 Articles tagged this
way fall into Wikipedia?s category of NPOV dis-
putes.
We constructed the NPOV corpus by retrieving
all articles that were or had been in the NPOV-
dispute category3 together with their full revision
history. We used Stanford?s CoreNLP tools4 to to-
kenize and split the text into sentences. Table 1
shows the statistics of this corpus, which we split
into training (train), development (dev), and test.
Following Wikipedia?s terminology, we call each
version of a Wikipedia article a revision, and so an
article can be viewed as a set of (chronologically
ordered) revisions.
2.2 Extracting Edits Meant to Remove Bias
Given all the revisions of a page, we extracted the
changes between pairs of revisions with the word-
mode diff function from the Diff Match and Patch
library.5 We refer to these changes between revi-
sions as edits, e.g., McMansion > large home. An
edit consists of two strings: the old string that is
being replaced (i.e., the before form), and the new
modified string (i.e., the after form).
Our assumption was that among the edits hap-
pening in NPOV disputes, we would have a high
density of edits intended to remove bias, which we
call bias-driven edits, like (1) and (2) from Sec-
tion 1. But many other edits occur even in NPOV
disputes, including edits to fix spelling or gram-
matical errors, simplify the language, make the
meaning more precise, or even vandalism (Max
2{{POV}}, {{POV-check}}, {{POV-section}}, etc.
Adding these tags displays a template such as ?The neutrality
of this article is disputed. Relevant discussion may be found
on the talk page. Please do not remove this message until the
dispute is resolved.?
3http://en.wikipedia.org/wiki/
Category:All_NPOV_disputes
4http://nlp.stanford.edu/software/
corenlp.shtml
5http://code.google.com/p/google-diff-
match-patch
1651
and Wisniewski, 2010). Therefore, in order to ex-
tract a high-precision set of bias-driven edits, we
took advantage of the comments that editors can
associate with a revision?typically short and brief
sentences describing the reason behind the revi-
sion. We considered as bias-driven edits those that
appeared in a revision whose comment mentioned
(N)POV, e.g., Attempts at presenting some claims
in more NPOV way; or merging in a passage
from the researchers article after basic NPOV-
ing. We only kept edits whose before and af-
ter forms contained five or fewer words, and dis-
carded those that only added a hyperlink or that
involved a minimal change (character-based Lev-
enshtein distance < 4). The final number of bias-
driven edits for each of the data sets is shown in
the ?Edits? column of Table 1.
2.3 Linguistic Analysis
Style guides talk about biased language in a pre-
scriptive manner, listing a few words that should
be avoided because they are flattering, vague, or
endorse a particular point of view (Wikipedia,
2013a). Our focus is on analyzing actual bi-
ased text and bias-driven edits extracted from
Wikipedia.
As we suggested above, this analysis uncovered
two major classes of bias: epistemological bias
and framing bias. Table 2 shows the distribution
(from a sample of 100 edits) of the different types
and subtypes of bias presented in this section.
(A) Epistemological bias involves propositions
that are either commonly agreed to be true or com-
monly agreed to be false and that are subtly pre-
supposed, entailed, asserted or hedged in the text.
1. Factive verbs (Kiparsky and Kiparsky, 1970)
presuppose the truth of their complement
clause. In (3-a) and (4-a), realize and re-
veal presuppose the truth of ?the oppression
of black people...? and ?the Meditation tech-
nique produces...?, whereas (3-b) and (4-b)
present the two propositions as somebody?s
stand or an experimental result.
(3) a. He realized that the oppression of black peo-
ple was more of a result of economic exploita-
tion than anything innately racist.
b. His stand was that the oppression of black
people was more of a result of economic ex-
ploitation than anything innately racist.
(4) a. The first research revealed that the Meditation
technique produces a unique state fact.
b. The first research indicated that the Medita-
tion technique produces a unique state fact.
Bias Subtype %
A. Epistemological bias 43
- Factive verbs 3
- Entailments 25
- Assertives 11
- Hedges 4
B. Framing bias 57
- Intensifiers 19
- One-sided terms 38
Table 2: Proportion of the different bias types.
2. Entailments are directional relations that
hold whenever the truth of one word or
phrase follows from another, e.g., murder en-
tails kill because there cannot be murdering
without killing (5). However, murder en-
tails killing in an unlawful, premeditated way.
This class includes implicative verbs (Kart-
tunen, 1971), which imply the truth or un-
truth of their complement, depending on the
polarity of the main predicate. In (6-a), co-
erced into accepting entails accepting in an
unwilling way.
(5) a. After he murdered three policemen, the
colony proclaimed Kelly a wanted outlaw.
b. After he killed three policemen, the colony
proclaimed Kelly a wanted outlaw.
(6) a. A computer engineer who was coerced into
accepting a plea bargain.
b. A computer engineer who accepted a plea bar-
gain.
3. Assertive verbs (Hooper, 1975) are those
whose complement clauses assert a proposi-
tion. The truth of the proposition is not pre-
supposed, but its level of certainty depends
on the asserting verb. Whereas verbs of say-
ing like say and state are usually neutral,
point out and claim cast doubt on the cer-
tainty of the proposition.
(7) a. The ?no Boeing? theory is a controversial is-
sue, even among conspiracists, many of whom
have pointed out that it is disproved by ...
b. The ?no Boeing? theory is a controversial is-
sue, even among conspiracists, many of whom
have said that it is disproved by...
(8) a. Cooper says that slavery was worse in South
America and the US than Canada, but clearly
states that it was a horrible and cruel practice.
b. Cooper says that slavery was worse in South
America and the US than Canada, but points
out that it was a horrible and cruel practice.
1652
4. Hedges are used to reduce one?s commit-
ment to the truth of a proposition, thus
avoiding any bold predictions (9-b) or state-
ments (10-a).6
(9) a. Eliminating the profit motive will decrease the
rate of medical innovation.
b. Eliminating the profit motive may have a
lower rate of medical innovation.
(10) a. The lower cost of living in more rural areas
means a possibly higher standard of living.
b. The lower cost of living in more rural areas
means a higher standard of living.
Epistemological bias is bidirectional, that is,
bias can occur because doubt is cast on a propo-
sition commonly assumed to be true, or because
a presupposition or implication is made about a
proposition commonly assumed to be false. For
example, in (7) and (8) above, point out is replaced
in the former case, but inserted in the second case.
If the truth of the proposition is uncontroversially
accepted by the community (i.e., reliable sources,
etc.), then the use of a factive is unbiased. In con-
trast, if only a specific viewpoint agrees with its
truth, then using a factive is biased.
(B) Framing bias is usually more explicit than
epistemological bias because it occurs when sub-
jective or one-sided words are used, revealing the
author?s stance in a particular debate (Entman,
2007).
1. Subjective intensifiers are adjectives or ad-
verbs that add (subjective) force to the mean-
ing of a phrase or proposition.
(11) a. Schnabel himself did the fantastic reproduc-
tions of Basquiat?s work.
b. Schnabel himself did the accurate reproduc-
tions of Basquiat?s work.
(12) a. Shwekey?s albums are arranged by many tal-
ented arrangers.
b. Shwekey?s albums are arranged by many dif-
ferent arrangers.
2. One-sided terms reflect only one of the sides
of a contentious issue. They often belong
to controversial subjects (e.g., religion, ter-
rorism, etc.) where the same event can be
seen from two or more opposing perspec-
tives, like the Israeli-Palestinian conflict (Lin
et al, 2006).
6See Choi et al (2012) for an exploration of the interface
between hedging and framing.
(13) a. Israeli forces liberated the eastern half of
Jerusalem.
b. Israeli forces captured the eastern half of
Jerusalem.
(14) a. Concerned Women for America?s major ar-
eas of political activity have consisted of op-
position to gay causes, pro-life law...
b. Concerned Women for America?s major ar-
eas of political activity have consisted of op-
position to gay causes, anti-abortion law...
(15) a. Colombian terrorist groups.
b. Colombian paramilitary groups.
Framing bias has been studied within the liter-
ature on stance recognition and arguing subjectiv-
ity. Because this literature has focused on iden-
tifying which side an article takes on a two-sided
debate such as the Israeli-Palestinian conflict (Lin
et al, 2006), most studies cast the problem as a
two-way classification of documents or sentences
into for/positive vs. against/negative (Anand et
al., 2011; Conrad et al, 2012; Somasundaran and
Wiebe, 2010), or into one of two opposing views
(Yano et al, 2010; Park et al, 2011). The fea-
tures used by these models include subjectivity
and sentiment lexicons, counts of unigrams and
bigrams, distributional similarity, discourse rela-
tionships, and so on.
The datasets used by these studies come from
genres that overtly take a specific stance (e.g.,
debates, editorials, blog posts). In contrast,
Wikipedia editors are asked not to advocate a par-
ticular point of view, but to provide a balanced ac-
count of the different available perspectives. For
this reason, overtly biased opinion statements such
as ?I believe that...? are not common in Wikipedia.
The features used by the subjectivity literature
help us detect framing bias, but we also need fea-
tures that capture epistemological bias expressed
through presuppositions and entailments.
3 Automatically Identifying Biased
Language
We now show how the bias cues identified in Sec-
tion 2.3 can help solve a new task. Given a biased
sentence (e.g., a sentence that a Wikipedia editor
has tagged as violating the NPOV policy), our goal
in this new task is to identify the word that intro-
duces bias. This is part of a potential three-step
process for detecting and correcting biased lan-
guage: (1) finding biased phrases, (2) identifying
the word that introduces the bias, (3) rewording to
eliminate the bias. As we will see below, it can be
1653
hard even for humans to track down the sources of
bias, because biases in reference works are often
subtle and implicit. An automatic bias detector
that can highlight the bias-inducing word(s) and
draw the editors? attention to words that need to
be modified could thus be important for improving
reference works like Wikipedia or even in news re-
porting.
We selected the subset of sentences that had a
single NPOV edit involving one (original) word.
(Although the before form consists of only one
word, the after form can be either one or more
words or the null string (i.e., deletion edits); we do
not use the after string in this identification task).
The number of sentences in the train, dev and test
sets is shown in the last column of Table 1.
We trained a logistic regression model on a
feature vector for every word that appears in the
NPOV sentences from the training set, with the
bias-inducing words as the positive class, and all
the other words as the negative class. The features
are described in the next section.
At test time, the model is given a set of sen-
tences and, for each of them, it ranks the words ac-
cording to their probability to be biased, and out-
puts the highest ranked word (TOP1 model), the
two highest ranked words (TOP2 model), or the
three highest ranked words (TOP3 model).
3.1 Features
The types of features used in the logistic regres-
sion model are listed in Table 3, together with
their value space. The total number of features is
36,787. The ones targeting framing bias draw on
previous work on sentiment and subjectivity de-
tection (Wiebe et al, 2004; Liu et al, 2005). Fea-
tures to capture epistemological bias are based on
the bias cues identified in Section 2.3.
A major split separates the features that de-
scribe the word under analysis (e.g., lemma, POS,
whether it is a hedge, etc.) from those that de-
scribe its surrounding context (e.g., the POS of the
word to the left, whether there is a hedge in the
context, etc.). We define context as a 5-gram win-
dow, i.e., two words to the left of the word un-
der analysis, and two to the right. Taking con-
text into account is important given that biases can
be context-dependent, especially epistemological
bias since it depends on the truth of a proposition.
To define some of the features like POS and gram-
matical relation, we used the Stanford?s CoreNLP
tagger and dependency parser (de Marneffe et al,
2006).
Features 9?10 use the list of hedges from Hy-
land (2005), features 11?14 use the factives and
assertives from Hooper (1975), features 15?16
use the implicatives from Karttunen (1971), fea-
tures 19?20 use the entailments from Berant et
al. (2012), features 21?25 employ the subjectiv-
ity lexicon from Riloff and Wiebe (2003), and fea-
tures 26?29 use the sentiment lexicon?positive
and negative words?from Liu et al (2005). If the
word (or a word in the context) is in the lexicon,
then the feature is true, otherwise it is false.
We also included a ?bias lexicon? (feature 31)
that we built based on our NPOV corpus from
Wikipedia. We used the training set to extract the
lemmas of words that were the before form of at
least two NPOV edits, and that occurred in at least
two different articles. Of the 654 words included
in this lexicon, 433 were unique to this lexicon
(i.e., recorded in neither Riloff and Wiebe?s (2003)
subjectivity lexicon nor Liu et al?s (2005) senti-
ment lexicon) and represented many one-sided or
controversial terms, e.g., abortion, same-sex, exe-
cute.
Finally, we also included a ?collaborative fea-
ture? that, based on the previous revisions of the
edit?s article, computes the ratio between the num-
ber of times that the word was NPOV-edited and
its frequency of occurrence. This feature was de-
signed to capture framing bias specific to an article
or topic.
3.2 Baselines
Previous work on subjectivity and stance recog-
nition has been evaluated on the task of classify-
ing documents as opinionated vs. factual, for vs.
against, positive vs. negative. Given that the task
of identifying the bias-inducing word of a sentence
is novel, there were no previous results to compare
directly against. We ran the following five base-
lines.
1. Random guessing. Naively returns a random
word from every sentence.
2. Role baseline. Selects the word with the
syntactic role that has the highest probabil-
ity to be biased, as computed on the train-
ing set. This is the parse tree root (proba-
bility p = .126 to be biased), followed by
verbal arguments (p = .085), and the subject
(p = .084).
1654
ID Feature Value Description
1* Word <string> Word w under analysis.
2 Lemma <string> Lemma of w.
3* POS {NNP, JJ, ...} POS of w.
4* POS ? 1 {NNP, JJ, ...} POS of one word before w.
5 POS ? 2 {NNP, JJ, ...} POS of two words before w.
6* POS + 1 {NNP, JJ, ...} POS of one word after w.
7 POS + 2 {NNP, JJ, ...} POS of two words after w.
8 Position in sentence {start, mid, end} Position of w in the sentence (split into three parts).
9 Hedge {true, false} w is in Hyland?s (2005) list of hedges (e.g., apparently).
10* Hedge in context {true, false} One/two words) around w is a hedge (Hyland, 2005).
11* Factive verb {true, false} w is in Hooper?s (1975) list of factives (e.g., realize).
12* Factive verb in context {true, false} One/two word(s) around w is a factive (Hooper, 1975).
13* Assertive verb {true, false} w is in Hooper?s (1975) list of assertives (e.g., claim).
14* Assertive verb in context {true, false} One/two word(s) around w is an assertive (Hooper, 1975).
15 Implicative verb {true, false} w is in Karttunen?s (1971) list of implicatives (e.g., manage).
16* Implicative verb in context {true, false} One/two word(s) around w is an implicative (Karttunen, 1971).
17* Report verb {true, false} w is a report verb (e.g., add).
18 Report verb in context {true, false} One/two word(s) around w is a report verb.
19* Entailment {true, false} w is in Berant et al?s (2012) list of entailments (e.g., kill).
20* Entailment in context {true, false} One/two word(s) around w is an entailment (Berant et al, 2012).
21* Strong subjective {true, false} w is in Riloff and Wiebe?s (2003) list of strong subjectives (e.g.,
absolute).
22 Strong subjective in context {true, false} One/two word(s) around w is a strong subjective (Riloff and
Wiebe, 2003).
23* Weak subjective {true, false} w is in Riloff and Wiebe?s (2003) list of weak subjectives (e.g.,
noisy).
24* Weak subjective in context {true, false} One/two word(s) around w is a weak subjective (Riloff and
Wiebe, 2003).
25 Polarity {+, ?, both, ...} The polarity of w according to Riloff and Wiebe (2003), e.g.,
praising is positive.
26* Positive word {true, false} w is in Liu et al?s (2005) list of positive words (e.g., excel).
27* Positive word in context {true, false} One/two word(s) around w is positive (Liu et al, 2005).
28* Negative word {true, false} w is in Liu et al?s (2005) list of negative words (e.g., terrible).
29* Negative word in context {true, false} One/two word(s) around w is negative (Liu et al, 2005).
30* Grammatical relation {root, subj, ...} Whether w is the subject, object, root, etc. of its sentence.
31 Bias lexicon {true, false} w has been observed in NPOV edits (e.g., nationalist).
32* Collaborative feature <numeric> Number of times that w was NPOV-edited in the article?s prior
history / frequency of w.
Table 3: Features used by the bias detector. The star (*) shows the most contributing features.
3. Sentiment baseline. Logistic regression
model that only uses the features based on
Liu et al?s (2005) lexicons of positive and
negative words (i.e., features 26?29).
4. Subjectivity baseline. Logistic regression
model that only uses the features based on
Riloff and Wiebe?s (2003) lexicon of subjec-
tive words (i.e., features 21?25).
5. Wikipedia baseline. Selects as biased the
words that appear in Wikipedia?s list of words
to avoid (Wikipedia, 2013a).
These baselines assessed the difficulty of the
task, as well as the extent to which traditional
sentiment-analysis and subjectivity features would
suffice to detect biased language.
3.3 Results and Discussion
To measure performance, we used accuracy de-
fined as:
#sentences with the correctly predicted biased word
#sentences
The results are shown in Table 4. As explained
earlier, we evaluated all the models by outputting
as biased either the highest ranked word or the
two or three highest ranked words. These corre-
spond to the TOP1, TOP2 and TOP3 columns, re-
spectively. The TOP3 score increases to 59%. A
tool that highlights up to three words to be revised
would simplify the editors? job and decrease sig-
nificantly the time required to revise.
Our model outperforms all five baselines by a
large margin, showing the importance of consid-
ering a wide range of features. Wikipedia?s list
of words to avoid falls very short on recall. Fea-
1655
System TOP1 TOP2 TOP3
Baseline 1: Random 2.18 7.83 9.13
Baseline 2: Role 15.65 20.43 25.65
Baseline 3: Sentiment 14.78 22.61 27.83
Baseline 4: Subjectivity 16.52 25.22 33.91
Baseline 5: Wikipedia 10.00 10.00 10.00
Our system 34.35 46.52 58.70
Humans (AMT) 37.39 50.00 59.13
Table 4: Accuracy (%) of the bias detector on the
test set.
tures that contribute the most to the model?s per-
formance (in a feature ablation study on the dev
set) are highlighted with a star (*) in Table 3. In
addition to showing the importance of linguistic
cues for different classes of bias, the ablation study
highlights the role of contextual features. The bias
lexicon does not seem to help much, suggesting
that it is overfit to the training data.
An error analysis shows that our system makes
acceptable errors in that words wrongly predicted
as bias-inducing may well introduce bias in a dif-
ferent context. In (16), the system picked eschew,
whereas orthodox would have been the correct
choice according to the gold edit. Note that both
the sentiment and the subjectivity lexicons list es-
chew as a negative word. The bias type that poses
the greatest challenge to the system are terms that
are one-sided or loaded in a particular topic, such
as orthodox in this example.
(16) a. Some Christians eschew orthodox theology; such
as the Unitarians, Socinian, [...]
b. Some Christians eschew mainstream trinitarian
theology; such as the Unitarians, Socinian, [...]
The last row in Table 4 lists the performance
of humans on the same task, presented in the next
section.
4 Human Perception of Biased Language
Is it difficult for humans to find the word in a
sentence that induces bias, given the subtle, of-
ten implicit biases in Wikipedia. We used Ama-
zon Mechanical Turk7 (AMT) to elicit annotations
from humans for the same 230 sentences from the
test set that we used to evaluate the bias detector
in Section 3.3. The goal of this annotation was
twofold: to compare the performance of our bias
detector against a human baseline, and to assess
the difficulty of this task for humans. While AMT
labelers are not trained Wikipedia editors, under-
7http://www.mturk.com
standing how difficult these cases are for untrained
labelers is an important baseline.
4.1 Task
Our HIT (Human Intelligence Task) was called
?Find the biased word!?. We kept the task descrip-
tion succinct. Turkers were shown Wikipedia?s
definition of a ?biased statement? and two exam-
ple sentences that illustrated the two types of bias,
framing and epistemological. In each HIT, annota-
tors saw 10 sentences, one after another, and each
one followed by a text box entitled ?Word intro-
ducing bias.? For each sentence, they were asked
to type in the text box the word that caused the
statement to be biased. They were only allowed to
enter a single word.
Before the 10 sentences, turkers were asked to
list the languages they spoke as well as their pri-
mary language in primary school. This was En-
glish in all the cases. In addition, we included a
probe question in the form of a paraphrasing task:
annotators were given a sentence and two para-
phrases (a correct and a bad one) to choose from.
The goal of this probe question was to discard
annotators who were not paying attention or did
not have a sufficient command of English. This
simple test was shown to be effective in verifying
and eliciting linguistic attentiveness (Munro et al,
2010). This was especially important in our case
as we were interested in using the human annota-
tions as an oracle. At the end of the task, partici-
pants were given the option to provide additional
feedback.
We split the 230 sentences into 23 sets of 10
sentences, and asked for 10 annotations of each
set. Each approved HIT was rewarded with $0.30.
4.2 Results and Discussion
On average, it took turkers about four minutes to
complete each HIT. The feedback that we got from
some of them confirmed our hypothesis that find-
ing the bias source is difficult: ?Some of the ?bi-
ases? seemed very slight if existent at all,? ?This
was a lot harder than I thought it would be... Inter-
esting though!?.
We postprocessed the answers ignoring case,
punctuation signs, and spelling errors. To ensure
an answer quality as high as possible, we only
kept those turkers who answered attentively by ap-
plying two filters: we only accepted answers that
matched a valid word from the sentence, and we
discarded answers from participants who did not
1656
2 3 4 5 6 7 8 9 10
Number of times the top word was selected
Nu
mb
er o
f se
nte
nce
s
0
10
20
30
40
50
Figure 1: Distribution of the number of turkers
who selected the top word (i.e., the word selected
by the majority of turkers).
pass the paraphrasing task?there were six such
cases. These filters provided us with confidence in
the turkers? answers as a fair standard of compari-
son.
Overall, humans correctly identified the biased
word 30% of the time. For each sentence, we
ranked the words according to the number of turk-
ers (out of 10) who selected them and, like we
did for the automated system, we assessed per-
formance when considering only the top word
(TOP1), the top 2 words (TOP2), and the top 3
words (TOP3). The last row of Table 4 reports the
results. Only 37.39% of the majority answers co-
incided with the gold label, slightly higher than
our system?s accuracy. The fact that the human
answers are very close to the results of our system
reflects the difficulty of the task. Biases in refer-
ence works can be very subtle and go unnoticed
by humans; automated systems could thus be ex-
tremely helpful.
As a measure of inter-rater reliability, we com-
puted pairwise agreement. The turkers agreed
40.73% of the time, compared to the 5.1% chance
agreement that would be achieved if raters had
randomly selected a word for each sentence. Fig-
ure 1 plots the number of times the top word of
each sentence was selected. The bulk of the sen-
tences only obtained between four and six answers
for the same word.
There is a good amount of overlap (?34%) be-
tween the correct answers predicted by our system
and those from humans. Much like the automated
system, humans also have the hardest time identi-
fying words that are one-sided or controversial to
a specific topic. They also picked eschew for (16)
instead of orthodox. Compared to the system, they
do better in detecting bias-inducing intensifiers,
and about the same with epistemological bias.
5 Related Work
The work in this paper builds upon prior work on
subjectivity detection (Wiebe et al, 2004; Lin et
al., 2011; Conrad et al, 2012) and stance recogni-
tion (Yano et al, 2010; Somasundaran and Wiebe,
2010; Park et al, 2011), but applied to the genre
of reference works such as Wikipedia. Unlike the
blogs, online debates and opinion pieces which
have been the major focus of previous work, bias
in reference works is undesirable. As a result,
the expression of bias is more implicit, making it
harder to detect by both computers and humans.
Of the two classes of bias that we uncover, fram-
ing bias is indeed strongly linked to subjectiv-
ity, but epistemological bias is not. In this re-
spect, our research is comparable to Greene and
Resnik?s (2009) work on identifying implicit sen-
timent or perspective in journalistic texts, based on
semantico-syntactic choices.
Given that the data that we use is not supposed
to be opinionated, our task consists in detecting
(implicit) bias instead of classifying into side A
or B documents about a controversial topic like
ObamaCare (Conrad et al, 2012) or the Israeli-
Palestinian conflict (Lin et al, 2006; Greene and
Resnik, 2009). Our model detects whether all
the relevant perspectives are fairly represented by
identifying statements that are one-sided. To this
end, the features based on subjectivity and senti-
ment lexicons turn out to be helpful, and incor-
porating more features for stance detection is an
important direction for future work.
Other aspects of Wikipedia structure have been
used for other NLP applications. The Wikipedia
revision history has been used for spelling correc-
tion, text summarization (Nelken and Yamangil,
2008), lexical simplification (Yatskar et al, 2010),
paraphrasing (Max and Wisniewski, 2010), and
textual entailment (Zanzotto and Pennacchiotti,
2010). Ganter and Strube (2009) have used
Wikipedia?s weasel-word tags to train a hedge de-
tector. Callahan and Herring (2011) have exam-
ined cultural bias based on Wikipedia?s NPOV
policy.
1657
6 Conclusions
Our study of bias in Wikipedia has implications
for linguistic theory and computational linguis-
tics. We show that bias in reference works falls
broadly into two classes, framing and epistemo-
logical. The cues to framing bias are more ex-
plicit and are linked to the literature on subjec-
tivity; cues to epistemological bias are subtle and
implicit, linked to presuppositions and entailments
in the text. Epistemological bias has not received
much attention since it does not play a major role
in overtly opinionated texts, the focus of much re-
search on stance recognition. However, our logis-
tic regression model reveals that epistemological
and other features can usefully augment the tradi-
tional sentiment and subjectivity features for ad-
dressing the difficult task of identifying the bias-
inducing word in a biased sentence.
Identifying the bias-inducing word is a chal-
lenging task even for humans. Our linguistically-
informed model performs nearly as well as hu-
mans tested on the same task. Given the sub-
tlety of some of these biases, an automated sys-
tem that highlights one or more potentially biased
words would provide a helpful tool for editors of
reference works and news reports, not only mak-
ing them aware of unnoticed biases but also sav-
ing them hours of time. Future work could in-
vestigate the incorporation of syntactic features or
further features from the stance detection litera-
ture. Features from the literature on veridicality
(de Marneffe et al, 2012) could be informative of
the writer?s commitment to the truth of the events
described, and document-level features could help
assess the extent to which the article provides a
balanced account of all the facts and points of
view.
Finally, the NPOV data and the bias lexicon that
we release as part of this research could prove use-
ful in other bias related tasks.
Acknowledgments
We greatly appreciate the support of Jean Wu and
Christopher Potts in running our task on Ama-
zon Mechanical Turk, and all the Amazon Turkers
who participated. We benefited from comments
by Valentin Spitkovsky on a previous draft and
from the helpful suggestions of the anonymous re-
viewers. The first author was supported by a Beat-
riu de Pino?s postdoctoral scholarship (2010 BP-A
00149) from Generalitat de Catalunya. The sec-
ond author was supported by NSF IIS-1016909.
The last author was supported by the Center for
Advanced Study in the Behavioral Sciences at
Stanford.
References
Pranav Anand, Marilyn Walker, Rob Abbott, Jean
E. Fox Tree, Robeson Bowmani, and Michael Mi-
nor. 2011. Cats rule and dogs drool!: Classifying
stance in online debate. In Proceedings of ACL-
HLT 2011 Workshop on Computational Approaches
to Subjectivity and Sentiment Analysis, pages 1?9.
Jonathan Berant, Ido Dagan, Meni Adler, and Jacob
Goldberger. 2012. Efficient tree-based approxima-
tion for entailment graph learning. In Proceedings
of ACL 2012, pages 117?125.
Ewa Callahan and Susan C. Herring. 2011. Cul-
tural bias in Wikipedia articles about famous per-
sons. Journal of the American Society for Informa-
tion Science and Technology, 62(10):1899?1915.
Eunsol Choi, Chenhao Tan, Lillian Lee, Cristian
Danescu-Niculescu-Mizil, and Jennifer Spindel.
2012. Hedge detection as a lens on framing in the
GMO debates: a position paper. In Proceedings
of the ACL-2012 Workshop on Extra-Propositional
Aspects of Meaning in Computational Linguistics,
pages 70?79.
Alexander Conrad, Janyce Wiebe, and Rebecca Hwa.
2012. Recognizing arguing subjectivity and argu-
ment tags. In Proceedings of ACL-2012 Workshop
on Extra-Propositional Aspects of Meaning in Com-
putational Linguistics, pages 80?88.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC 2006.
Marie-Catherine de Marneffe, Christopher D. Man-
ning, and Christopher Potts. 2012. Did it hap-
pen? The pragmatic complexity of veridicality as-
sessment. Computational Linguistics, 38(2):301?
333.
Robert M. Entman. 2007. Framing bias: Media in the
distribution of power. Journal of Communication,
57(1):163?173.
Viola Ganter and Michael Strube. 2009. Finding
hedges by chasing weasels: Hedge detection using
Wikipedia tags and shallow linguistic features. In
Proceedings of ACL-IJCNLP 2009, pages 173?176.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment.
In Proceedings of NAACL-HLT 2009, pages 503?
511.
1658
Joan B. Hooper. 1975. On assertive predicates. In
J. Kimball, editor, Syntax and Semantics, volume 4,
pages 91?124. Academic Press, New York.
Ken Hyland. 2005. Metadiscourse: Exploring Interac-
tion in Writing. Continuum, London and New York.
Lauri Karttunen. 1971. Implicative verbs. Language,
47(2):340?358.
Paul Kiparsky and Carol Kiparsky. 1970. Fact. In
M. Bierwisch and K. E. Heidolph, editors, Progress
in Linguistics, pages 143?173. Mouton, The Hague.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you
on? Identifying perspectives at the document and
sentence levels. In Proceedings of CoNLL 2006,
pages 109?116.
Chenghua Lin, Yulan He, and Richard Everson.
2011. Sentence subjectivity detection with
weakly-supervised learning. In Proceedings of
AFNLP 2011, pages 1153?1161.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion Observer: analyzing and comparing opin-
ions on the Web. In Proceedings of WWW 2005,
pages 342?351.
Aure?lien Max and Guillaume Wisniewski. 2010. Min-
ing naturally-occurring corrections and paraphrases
from Wikipedia?s revision history. In Proceedings
of LREC 2010, pages 3143?3148.
Robert Munro, Steven Bethard, Victor Kuperman,
Vicky Tzuyin Lai, Robin Melnick, Christopher
Potts, Tyler Schnoebelen, and Harry Tily. 2010.
Crowdsourcing and language studies: the new gen-
eration of linguistic data. In Proceedings of the
NAACL-HLT 2010 Workshop on Creating Speech
and Language Data With Amazons Mechanical
Turk, pages 122?130.
Rani Nelken and Elif Yamangil. 2008. Mining
Wikipedias article revision history for training Com-
putational Linguistics algorithms. In Proceedings of
the 1st AAAI Workshop on Wikipedia and Artificial
Intelligence.
Souneil Park, KyungSoon Lee, and Junehwa Song.
2011. Contrasting opposing views of news articles
on contentious issues. In Proceedings of ACL 2011,
pages 340?349.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of EMNLP 2003, pages 105?112.
Swapna Somasundaran and Janyce Wiebe. 2010. Rec-
ognizing stances in ideological on-line debates. In
Proceedings of the NAACL-HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, pages 116?124.
Peter D. Turney. 2002. Thumbs up or thumbs down?
Semantic orientation applied to unsupervised clas-
sification of reviews. In Proceedings of ACL 2002,
pages 417?424.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce,
Matthew Bell, and Melanie Martin. 2004. Learn-
ing subjective language. Computational Linguistics,
30(3):277?308.
Wikipedia. 2013a. Wikipedia: Manual of style / Words
to watch. http://en.wikipedia.org/
wiki/Wikipedia:Words_to_avoid. [Re-
trieved February 5, 2013].
Wikipedia. 2013b. Wikipedia: Neutral point of view.
http:http://en.wikipedia.org/wiki/
Wikipedia:Neutral_point_of_view.
[Retrieved February 5, 2013].
Tae Yano, Philip Resnik, and Noah A. Smith. 2010.
Shedding (a thousand points of) light on biased lan-
guage. In Proceedings of the NAACL-HLT 2010
Workshop on Creating Speech and Language Data
With Amazons Mechanical Turk, pages 152?158.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of sim-
plicity: Unsupervised extraction of lexical simplifi-
cations from Wikipedia. In Proceedings of NAACL-
HLT 2010, pages 365?368.
Fabio M. Zanzotto and Marco Pennacchiotti. 2010.
Expanding textual entailment corpora from
Wikipedia using co-training. In Proceedings of
the 2nd Coling Workshop on The People?s Web
Meets NLP: Collaboratively Constructed Semantic
Resources, pages 28?36.
1659
Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 76?87,
Portland, Oregon, June 2011. c?2011 Association for Computational Linguistics
Chameleons in imagined conversations: A new approach to understanding
coordination of linguistic style in dialogs
Cristian Danescu-Niculescu-Mizil and Lillian Lee
Department of Computer Science, Cornell University
cristian@cs.cornell.edu, llee@cs.cornell.edu
Abstract
Conversational participants tend to immedi-
ately and unconsciously adapt to each other?s
language styles: a speaker will even adjust the
number of articles and other function words in
their next utterance in response to the number
in their partner?s immediately preceding utter-
ance. This striking level of coordination is
thought to have arisen as a way to achieve so-
cial goals, such as gaining approval or empha-
sizing difference in status. But has the adap-
tation mechanism become so deeply embed-
ded in the language-generation process as to
become a reflex? We argue that fictional di-
alogs offer a way to study this question, since
authors create the conversations but don?t re-
ceive the social benefits (rather, the imagined
characters do). Indeed, we find significant co-
ordination across many families of function
words in our large movie-script corpus. We
also report suggestive preliminary findings on
the effects of gender and other features; e.g.,
surprisingly, for articles, on average, charac-
ters adapt more to females than to males.
1 Introduction
?...it is dangerous to base any sociolinguistic argu-
mentation on the evidence of language in fictional
texts only? (Bleichenbacher (2008), crediting Mare?
(2000))
The chameleon effect is the ?nonconscious
mimicry of the postures, mannerisms, facial expres-
sions, and other behaviors of one?s interaction part-
ners? (Chartrand and Bargh, 1999).1 For exam-
ple, if one conversational participant crosses their
1The term is a reference to the movie Zelig, wherein a ?hu-
arms, their partner often unconsciously crosses their
arms as well. The effect occurs for language, too,
ranging from matching of acoustic features such as
accent, speech rate, and pitch (Giles et al, 1991;
Chartrand and van Baaren, 2009) to lexico-syntactic
priming across adjacent or nearby utterances (Bock,
1986; Pickering and Garrod, 2004; Ward and Lit-
man, 2007; Reitter et al, 2011).
Our work focuses on adjacent-utterance coordina-
tion with respect to classes of function words. To ex-
emplify the phenomenon, we discuss two short con-
versations.
? First example: The following exchange from the
movie ?The Getaway? (1972) demonstrates quanti-
fier coordination.
Doc: At least you were outside.
Carol: It doesn?t make much difference where you are [...]
Note that ?Carol? used a quantifier, one that is differ-
ent than the one ?Doc? employed. Also, notice that
?Carol? could just as well have replied in a way that
doesn?t include a quantifier, for example, ?It doesn?t
really matter where you are...?.
? Second example: Levelt and Kelter (1982) report
an experiment involving preposition coordination.
Shopkeepers who were called and asked ? At what
time does your shop close?? were significantly more
likely to say ? At five o?clock? than ?five o?clock?.2
man chameleon? uncontrollably takes on the characteristics of
those around him. The term is meant to contrast with ?aping?,
a word connoting intentional imitation.
Related terms include adaptation, alignment, entrainment,
priming, and Du Bois? dialogic syntax.
2This is an example of lexical matching manifested as part
of syntactic coordination.
76
Coordination of function-word class has been pre-
viously documented in several settings (Niederhof-
fer and Pennebaker, 2002; Taylor and Thomas,
2008; Ireland et al, 2011; Gonzales et al, 2010),
the largest-scale study being on Twitter (Danescu-
Niculescu-Mizil et al, 2011).
Problem setting People don?t consciously track
function words (Levelt and Kelter, 1982; Segalowitz
and Lane, 2004; Petten and Kutas, 1991) ? it?s not
easy to answer the question, ?how many preposi-
tions were there in the sentence I just said??. There-
fore, it is quite striking that humans nonetheless in-
stantly adapt to each other?s function-word rates. In-
deed, there is active debate regarding what mecha-
nisms cause nonconscious coordination (Ireland et
al., 2011; Branigan et al, 2010).
One line of thought is that convergence represents
a social strategy3 whose aim is to gain the other?s so-
cial approval (Giles, 2008; Street and Giles, 1982)
or enhance the other?s comprehension (Clark, 1996;
Bortfeld and Brennan, 1997).4 This hypothesis is
supported by studies showing that coordination is af-
fected by a number of social factors, including rel-
ative social status (Natale, 1975; Gregory and Web-
ster, 1996; Thakerar et al, 1982) and gender role
(Bilous and Krauss, 1988; Namy et al, 2002; Ire-
land and Pennebaker, 2010).
But an important question is whether the adap-
tation mechanism has become so deeply embed-
ded in the language-generation process as to have
transformed into a reflex not requiring any social
triggering.5 Indeed, it has been argued that un-
conscious mimicry is partly innate (Chartrand and
Bargh, 1999), perhaps due to evolutionary pressure
to foster relationships (Lakin et al, 2003).
To answer this question, we take a radical ap-
proach: we consider a setting in which the per-
sons generating the coordinating dialog are different
from those engaged in the dialog (and standing to
reap the social benefits) ? imagined conversations,
specifically, scripted movie dialogs.
3In fact, social signaling may also be the evolutionary cause
of chameleons? color-changing ability (Stuart-Fox et al, 2008).
4For the purpose of our discussion, we are conflating social-
approval and audience-design hypotheses under the category of
social strategy.
5This hypothesis relates to characterizations of alignment as
an unmediated mechanism (Pickering and Garrod, 2004).
Life is beautiful, but cinema is paradise A pri-
ori, it is not clear that movie conversations would ex-
hibit convergence. Dialogs between movie charac-
ters are not truthful representations of real-life con-
versations. They often are ?too carefully polished,
too rhythmically balanced, too self-consciously art-
ful? (Kozloff, 2000), due to practical and artis-
tic constraints and scriptwriting practice (McKee,
1999). For example, mundane phenomena such as
stuttering and word repetitions are generally nonex-
istent on the big screen. Moreover, writers have
many goals to accomplish, including the need to ad-
vance the plot, reveal character, make jokes as funny
as possible, and so on, all incurring a cognitive load.
So, the question arises: do scripted movie di-
alogs, in spite of this quasi-artificiality and the
aforementioned generation/engagement gap, exhibit
the real-life phenomenon of stylistic convergence?
When imagining dialogs, do scriptwriters (noncon-
sciously6) adjust the respondent?s replies to echo the
initiator?s use of articles, prepositions, and other ap-
parently minor aspects of lexical choice? According
to our results, this is indeed the case, which has fas-
cinating implications.
First, this provides evidence that coordination, as-
sumed to be driven by social motivations, has be-
come so deeply embedded into our ideas of what
conversations ?sound like? that the phenomenon oc-
curs even when the person generating the dialog is
not the recipient of the social benefits.7
Second, movies can be seen as a controlled en-
vironment in which preconceptions about the rela-
tion between communication patterns and the social
features of the participants can be studied. This
gives us the opportunity to understand how people
(scriptwriters) nonconsciously expect convergence
to relate to factors such as gender, status and rela-
tion type. Are female characters thought to accom-
modate more to male characters than vice-versa?
Furthermore, movie scripts constitute a corpus
that is especially convenient because meta-features
6The phenomenon of real-life language convergence is not
widely known among screenplay authors (Beth F. Milles, pro-
fessor of acting and directing, personal communication).
7Although some writers may perhaps imagine themselves
"in the shoes" of the recipients, recall that authors generally
don?t include in their scripts the repetitions and ungrammati-
calities of "real-life" speech.
77
like gender can be more or less readily obtained.
Contributions We check for convergence in a
corpus of roughly 250,000 conversational exchanges
from movie scripts (available at http://www.
cs.cornell.edu/~cristian/movies).
Specifically, we examine the set of nine families of
stylistic features previously utilized by Ireland et
al. (2011), and find a statistically significant con-
vergence effect for all these families. We thereby
provide evidence that language coordination is so
implanted within our conception of conversational
behavior that, even if such coordination is socially
motivated, it is exhibited even when the person
generating the language in question is not receiving
any of the presumed social advantages.
We also study the effects of gender, narrative im-
portance, and hostility. Intriguingly, we find that
these factors indeed ?affect? movie characters? lin-
guistic behavior; since the characters aren?t real,
and control of stylistic lexical choice is largely non-
conscious, the effects of these factors can only be
springing from patterns existing in the scriptwriters?
minds.
Our findings, by enhancing our understanding of
linguistic adaptation effects in stylistic word choice
and its relation to various socially relevant factors,
may in the future aid in practical applications. Such
an understanding would give us insight into how
and what kinds of language coordination yield more
satisfying interactions ? convergence has been al-
ready shown to enhance communication in organiza-
tional contexts (Bourhis, 1991), psychotherapy (Fer-
rara, 1991), care of the mentally disabled (Hamilton,
1991), and police-community interactions (Giles et
al., 2007). Moreover, a deeper understanding can aid
human-computer interaction by informing the con-
struction of natural-language generation systems,
since people are often more satisfied with encoun-
ters exhibiting appropriate linguistic convergence
(Bradac et al, 1988; van Baaren et al, 2003), even
when the other conversational participant is known
to be a computer (Nass and Lee, 2000; Branigan et
al., 2010).
2 Related work not already mentioned
Linguistic style and human characteristics Us-
ing stylistic (i.e., non-topical) elements like arti-
cles and prepositions to characterize the utterer in
some way has a long history, including in author-
ship attribution (Mosteller and Wallace, 1984; Juola,
2008), personality-type classification (Argamon et
al., 2005; Oberlander and Gill, 2006; Mairesse et al,
2007), gender categorization (Koppel et al, 2002;
Mukherjee and Liu, 2010; Herring and Paolillo,
2006), identification of interactional style (Jurafsky
et al, 2009; Ranganath et al, 2009), and recognizing
deceptive language (Hancock et al, 2008; Mihalcea
and Strapparava, 2009).
Imagined conversations There has been work in
the NLP community applying computational tech-
niques to fiction, scripts, and other types of text
containing imagined conversations. For example,
one recent project identifies conversational networks
in novels, with the goal of evaluating various liter-
ary theories (Elson et al, 2010; Elson and McKe-
own, 2010). Movie scripts were used as word-sense-
disambiguation evaluation data as part of an effort
to generate computer animation from the scripts (Ye
and Baldwin, 2006). Sonderegger (2010) employed
a corpus of English poetry to study the relation-
ship between pronunciation and network structure.
Rayson et al (2001) computed part-of-speech fre-
quencies for imaginative writing in the British Na-
tional Corpus, finding a typology gradient progress-
ing from conversation to imaginative writing (e.g.,
novels) to task-oriented speech to informative writ-
ing. The data analyzed by Oberlander and Gill
(2006) consisted of emails that participants were in-
structed to write by imagining that they were going
to update a good friend on their current goings-on.
3 Movie dialogs corpus
To address the questions raised in the introduc-
tion, we created a large set of imagined conver-
sations, starting from movie scripts crawled from
various sites.8 Metadata for conversation analy-
sis and duplicate-script detection involved mostly-
automatic matching of movie scripts with the IMDB
movie database; clean-up resulted in 617 unique ti-
tles tagged with genre, release year, cast lists, and
8The source of these scripts and more detail about the corpus
are given in the README associated with the Cornell movie-
dialogs corpus, available at http://www.cs.cornell.
edu/~cristian/movies .
78
IMDB information. We then extracted 220,579
conversational exchanges between pairs of charac-
ters engaging in at least 5 exchanges, and auto-
matically matched these characters to IMDB to re-
trieve gender (as indicated by the designations ?ac-
tor? or ?actress?) and/or billing-position information
when possible (?9000 characters, ?3000 gender-
identified and ?3000 billing-positioned). The latter
feature serves as a proxy for narrative importance:
the higher up in the credits, the more important the
character tends to be in the film.
To the best of our knowledge, this is the largest
dataset of (metadata-rich) imaginary conversations
to date.
4 Measuring linguistic style
For consistency with prior work, we employed the
nine LIWC-derived categories (Pennebaker et al,
2007) deemed by Ireland et al (2011) to be pro-
cessed by humans in a generally non-conscious fash-
ion. The nine categories are: articles, auxiliary
verbs, conjunctions, high-frequency adverbs, im-
personal pronouns, negations, personal pronouns,
prepositions, and quantifiers (451 lexemes total).
It is important to note that language coordination
is multimodal: it does not necessarily occur simulta-
neously for all features (Ferrara, 1991), and speakers
may converge on some features but diverge on others
(Thakerar et al, 1982); for example, females have
been found to converge on pause frequency with
male conversational partners but diverge on laugh-
ter (Bilous and Krauss, 1988).
5 Measuring convergence
Niederhoffer and Pennebaker (2002) use the correla-
tion coefficient to measure accommodation with re-
spect to linguistic style features. While correlation
at first seems reasonable, it has some problematic as-
pects in our setting (we discuss these problems later)
that motivate us to employ an alternative measure.
We instead use a convergence measure introduced
in Danescu-Niculescu-Mizil et al (2011) that quan-
tifies how much a given feature family t serves as an
immediate trigger or stimulus, meaning that one per-
son?s utterance exhibiting such a feature triggers the
appearance of that feature in the respondent?s imme-
diate reply.
For example, we might be studying whether one
person A?s inclusion of articles in an utterance trig-
gers the usage of articles in respondent B?s reply.
Note that this differs from asking whether B uses ar-
ticles more often when talking to A than when talk-
ing to other people (it is not so surprising that peo-
ple speak differently to different audiences). This
also differs from asking whether B eventually starts
matching A?s behavior in later utterances within the
same conversation. We specifically want to know
whether each utterance by A triggers an immediate
change in B?s behavior, as such instantaneous adap-
tation is what we consider the most striking aspect
of convergence, although immediate and long-term
coordination are clearly related.
We now describe the statistic we employ to mea-
sure the extent to which person B accommodates to
A. Consider an arbitrary conversational exchange
started by A, and let a denote A?s initiating utterance
and b??a denote B?s reply to a.9 Note that we use
lowercase to emphasize when we are talking about
individual utterances rather than all the utterances of
the particular person, and that thus, the arrow in b??a
indicates that we mean the reply to the specific sin-
gle utterance a. Let at be the indicator variable for a
exhibiting t, and similarly for bt??a. Then, we define
the convergence ConvA,B(t) of B to A as:
P (bt??a = 1|a
t = 1)? P (bt??a = 1). (1)
Note that this quantity can be negative (indicating
divergence). The overall degree Conv(t) to which t
serves as a trigger is then defined as the expectation
of ConvA,B(t) over all initiator-respondent pairs:
Conv(t)
def
= Epairs(A,B)(ConvA,B(t)). (2)
Comparison with correlation: the importance
of asymmetry10 Why do we employ ConvA,B ,
Equation (1), instead of the well-known correlation
coefficient? One reason is that correlation fails to
9We use ?initiating? and ?reply? loosely: in our terminology,
the conversation ?A: ?Hi.? B: ?Eaten?? A: ?Nope.?? has two
exchanges, one initiated by A?s ?Hi?, the other by B?s ?Eaten??.
10Other asymmetric measures based on conditional prob-
ability of occurrence have been proposed for adaptation
within monologues (Church, 2000) and between conversations
(Stenchikova and Stent, 2007). Since our focus is different, we
control for different factors.
79
capture an important asymmetry. The case where
at = 1 but bt??a = 0 represents a true failure to ac-
commodate; but the case where at = 0 but bt??a = 1
should not, at least not to the same degree. For ex-
ample, a may be very short (e.g., ?What??) and thus
not contain an article, but we don?t assume that this
completely disallows B from using articles in their
reply. In other words, we are interested in whether
the presence of t acts as a trigger, not in whether
b??a exhibits t if and only if a does, the latter being
what correlation detects.11
It bears mentioning that since at and bt??a are
binary, a simple calculation shows that the covari-
ance12 cov(at, bt??a) = ConvA,B(t) ? P (a
t = 1).
But, the two terms on the right hand side are
not independent: raising P (at = 1) could cause
ConvA,B(t) to decrease by affecting the first term
in its definition, P (bt??a = 1|a
t = 1) (see eq. 1).
6 Experimental results
6.1 Convergence exists in fictional dialogs
For each ordered pair of characters (A,B) and for
each feature family t, we estimate equation (1) in a
straightforward manner: the fraction of B?s replies
to t-manifesting A utterances that themselves ex-
hibit t, minus the fraction of all replies of B to A
that exhibit t.13 Fig. 1 compares the average values
of these two fractions (as a way of putting conver-
gence values into context), showing positive differ-
ences for all of the considered families of features
(statistically significant, paired t-test p < 0.001); this
demonstrates that movie characters do indeed con-
verge to each other?s linguistic style on all consid-
ered trigger families.14
11One could also speculate that it is easier for B to (uncon-
sciously) pick up on the presence of t than on its absence.
12The covariance of two random variables is their correlation
times the product of their standard deviations.
13For each t, we discarded pairs of characters where some
relevant count is < 10, e.g., where B had fewer than 10 replies
manifesting the trigger.
14We obtained the same qualitative results when measuring
convergence via the correlation coefficient, doing so for the sake
of comparability with prior work (Niederhoffer and Pennebaker,
2002; Taylor and Thomas, 2008).
Ne
ga
tio
n
In
de
f. 
pr
on
.
Qu
an
tif
ie
r
Au
x.
 v
er
b
Ad
ve
rb
Pe
rs
. p
ro
n.
Co
nj
un
ct
io
n
Ar
tic
le
Pr
ep
os
iti
on
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
Figure 1: Implicit depiction of convergence for each trig-
ger family t, illustrated as the difference between the
means of P (bt??a = 1|a
t = 1) (right/light-blue bars) and
P (bt??a = 1) (left/dark-blue bars). (This implicit repre-
sentation allows one to see the magnitude of the two com-
ponents making up our definition of convergence.) The
trigger families are ordered by decreasing convergence.
All differences are statistically significant (paired t-test).
In all figures in this paper, error bars represent standard
error, estimated via bootstrap resampling (Koehn, 2004).
(Here, the error bars, in red, are very tight.)
Movies vs. Twitter One can ask how our results
on movie dialogs correspond to those for real-life
conversations. To study this, we utilize the results
of Danescu-Niculescu-Mizil et al (2011) on a large-
scale collection of Twitter exchanges as data on
real conversational exchanges. Figure 2 depicts the
comparison, revealing two interesting effects. First,
Twitter users coordinate more than movie characters
on all the trigger families we considered, which does
show that the convergence effect is stronger in actual
interchanges. On the other hand, from the perspec-
tive of potentially using imagined dialogs as prox-
ies for real ones, it is intriguing to see that there is
generally a correspondence between how much con-
vergence occurs in real dialogs for a given feature
family and how much convergence occurs for that
feature in imagined dialogs, although conjunctions
and articles show a bit less convergence in fictional
80
Ne
ga
tio
n
Ind
ef.
 pr
on
.
Qu
an
tifi
er
Co
nju
nc
tio
n
Ar
tic
le
Au
x. 
ve
rb
Ad
ve
rb
Pe
rs.
 pr
on
.
Pre
po
sit
ion
0.00
0.01
0.02
0.03
0.04
0.05
Co
nv
er
ge
nc
e
Twitter
Movies
Figure 2: Convergence in Twitter conversations (left bars)
vs. convergence in movie dialogs (right bars; corre-
sponds to the difference between the two respective bars
in Fig. 1) for each trigger family. The trigger families are
ordered by decreasing convergence in Twitter.
Ar
tic
le
Au
x. 
ve
rb
Ad
ve
rb
Co
nju
nc
tio
n
Ind
ef.
 pr
on
.
Ne
ga
tio
n
Pe
rs.
 pr
on
.
Pre
po
sit
ion
Qu
an
tifi
er0.01
0.00
0.01
0.02
0.03
0.04
0.05
Co
nv
er
ge
nc
e
Adjacent
Non-adjacent
Figure 3: Immediate vs. within-conversation effects
(for conversations with at least 5 utterances). Sup-
pose that we have a conversation a1 b2 a3 b4 a5 . . .. The
lefthand/dark-green bars show the usual convergence
measure, which involves the utterance pair a1 and b2. The
righthand/mustard-green bars show convergence based
on pairs like a1 and b4 ? utterances in the same con-
versation, but not adjacent. We see that there is a much
stronger triggering effect for immediately adjacent utter-
ances.
exchanges than this pattern would suggest.
6.2 Potential alternative explanations
Immediate vs. within-conversation effects An
additional natural question is, how much are these
accommodation effects due to an immediate trigger-
ing effect, as opposed to simply being a by-product
of utterances occurring within the same conversa-
tion? For instance, could the results be due just to
the topic of the conversation?
To answer this question requires measuring ?con-
vergence? between utterances that are not adjacent,
but are still in the same conversation. To this end,
we first restricted attention to those conversations
in which there were at least five utterances, so that
they would have the structure a1 b2 a3 b4 a5.... We
then measure convergence not between adjacent ut-
terances, like a1 and b2, but where we skip an utter-
ance, such as the pair a1, b4 or b2, a5. This helps
control for topic effects, since b4 and a1 are still
close and thus fairly likely to be on the same sub-
ject.15
Figure 3 shows that the level of convergence al-
ways falls off after the skipped utterance, sometimes
dramatically so, thus demonstrating that the level
of immediate adaptation effects we see cannot be
solely explained by the topic of conversation or other
conversation-level effects. These results accord with
the findings of Levelt and Kelter (1982), where in-
terposing ?interfering? questions lowered the chance
of a question?s preposition being echoed by the re-
spondent, and Reitter et al (2006), where the effects
of structural priming were shown to decay quickly
with the distance between the priming trigger and
the priming target.
Towards the same end, we also performed ran-
domization experiments in which we shuffled the or-
der of each participant?s utterances in each conversa-
tion, while maintaining alternation between speak-
ers. We again observed drop-offs in this randomized
condition in comparison to immediate convergence,
the main focus of this paper.
Self-coordination Could our results be explained
entirely by the script author converging to their own
self, given that self-alignment has been documented
15It is true that they might be on different topics, but in fact
even b2 might be on a different subject from a1.
81
(Pickering and Garrod, 2004; Reitter et al, 2006)?
If that were the case, then the characters that the au-
thor is writing about should converge to themselves
no more than they converge to different characters.
But we ran experiments showing that this is not the
case, thus invalidating this alternative hypothesis. In
fact, characters converge to themselves much more
than they converge to other characters.
6.3 Convergence and imagined relation
We now analyze how convergence patterns vary with
the type of relationship between the (imagined) par-
ticipants. Note that, given the multimodal charac-
ter of convergence, treating each trigger family sep-
arately is the most appropriate way to proceed, since
in past work, for the same experimental factor (e.g.,
gender), different features converge differently (re-
fer back to ?4). For clarity of exposition, we discuss
in detail only the results for the Articles feature fam-
ily; but the results for all trigger families are sum-
marized in Fig. 7, discussed later.
Imagined gender Fig. 4(a) shows how conver-
gence on article usage depends on the gender of the
initiator and respondent. Females are more influ-
ential than males: movie characters of either gen-
der accommodate more to female characters than to
male characters (compare the Female initiator bar
with the Male initiator bar, statistically significant,
independent t-test, p < 0.05). Also, female char-
acters seem to accommodate slightly more to other
characters than male characters do (though not sta-
tistically significantly so in our data).
We also compare the amount of convergence be-
tween all the possible types of gendered initiator-
respondent pairs involved (Fig. 4(b)). One can ob-
serve, for example, that male characters adapt less in
same-gender situations (Male-Male conversations)
than in mixed-gender situations (Female initiator-
Male respondent), while the opposite is true for fe-
male characters (Female-Female vs. Male-Female).
Interpreting these results lies beyond the scope
of this paper. We note that these results could be
a correlate of many factors, such as the roles that
male and female characters are typically assigned in
movie scripts.16
16A comparison to previously reported results on real-life
gender effects is not straightforward, since they pertain to differ-
Narrative importance Does the relative impor-
tance bestowed by the scriptwriter to the characters
affect the amount of linguistic coordination he or she
(nonconsciously) embeds in their dialogs? Fig. 5
shows that, on average, the lead character converges
to the second-billed character more than vice-versa
(compare left bar in 1st resp. group with left bar in
2nd resp. group).
One possible confounding factor is that there is
significant gender imbalance in the data (82% of all
lead characters are males, versus only 51% of the
secondary characters). Could the observed differ-
ence be a direct consequence of the relation between
gender and convergence discussed above? The an-
swer is no: the same qualitative observation holds if
we restrict our analysis to same-gender pairs (com-
pare the righthand bars in each group in Fig. 517).
It would be interesting to see whether these re-
sults could be brought to bear on previous results
regarding the relationship between social status and
convergence, but such interpretation lies beyond the
scope of this paper, since the connection between
billing order and social status is not straightforward.
Quarreling The level of contention in conversa-
tions has also been shown to be related to the amount
of convergence (Giles, 2008; Niederhoffer and Pen-
nebaker, 2002; Taylor and Thomas, 2008). To test
whether this tendency holds in the case of imagined
conversations, as a small pilot study, we manually
classified the conversations between 24 main pairs
of characters from romantic comedies18 as: quarrel-
ing, some quarreling and no quarreling. Although
the experiment was too small in scale to provide
statistical significance, the results (Fig. 6) suggest
that indeed the level of convergence is affected by
ent features; Ireland and Pennebaker (2010) show that females
match their linguistic style more than males, where style match-
ing is averaged over the same 9 trigger families we employ (they
do not report gender effect for each family separately).
17Figure 5 also shows that our convergence measure does
achieve negative values in practice, indicating divergence. Di-
vergence is a rather common phenomenon which deserves at-
tention in future work; see Danescu-Niculescu-Mizil et al
(2011) for an account.
18We chose the romantic comedy genre since it is often char-
acterized by some level of contention between the two people
in the main couple.
82
All
F i
nit
.
M 
ini
t.
F r
es
p.
M 
re
sp
.0.00
0.01
0.02
0.03
0.04
Co
nv
er
ge
nc
e
article
(a)
    
   A
ll
M-
M F-M M-
F F-F
0.00
0.01
0.02
0.03
0.04
Co
nv
er
ge
nc
e
article
(b)
Figure 4: Relation between Article convergence and imagined gender. (a) compares cases when the initiator and
respondent are Male or Female; (b) compares types of gendered initiator-respondent relations: Male-Male, Female-
Male, Male-Female, Female-Female. For comparison, the All bars represents the general Article convergence (illus-
trated in Fig. 1 as the difference between the two respective bars).
Ind
ef. 
pro
n.
Art
icle
Co
nju
nct
ion
Pre
pos
itio
n
Ad
ver
b
Per
s. p
ron
.
Au
x. v
erb
Ne
gat
ion
Qu
ant
ifie
r0.010
0.005
0.000
0.005
0.010
0.015
(a) F resp. minus M resp.
Art
icle
Pre
pos
itio
n
Ind
ef. 
pro
n.
Co
nju
nct
ion
Per
s. p
ron
.
Au
x. v
erb
Ne
gat
ion
Ad
ver
b
Qu
ant
ifie
r0.010
0.005
0.000
0.005
0.010
0.015
(b) F init. minus M init.
Art
icle
Ne
gat
ion
Qu
ant
ifie
r
Ad
ver
b
Per
s. p
ron
.
Ind
ef. 
pro
n.
Pre
pos
itio
n
Au
x. v
erb
Co
nju
nct
ion
0.010
0.005
0.000
0.005
0.010
0.015
(c) 1st resp. minus 2nd resp.
Per
s. p
ron
.
Art
icle
Ind
ef.
 pr
on.
Ne
gat
ion
Co
nju
nct
ion
Qu
ant
ifie
r
Au
x. v
erb
Pre
pos
itio
n
Ad
ver
b0.06
0.02
0.02
0.06
(d) Quarrel minus No quarrel
Figure 7: Summary of the relation between convergence and imagined gender (a and b), billing order (c), and quarrel-
ing (d). The bars represent the difference between the convergence observed in the respective cases; e.g., the Article
(red) bar in (a) represents the difference between the F resp. and the M resp. bars in Fig. 4(a). In each plot, the
trigger families are sorted according to the respective difference, but the color assigned to each family is consistent
across plots. The scale of (d) differs from the others.
the presence of controversy: quarreling exhibited
considerably more convergence for articles than the
other categories (the same holds for personal and in-
definite pronouns; see Fig. 7). Interestingly, the
reverse is true for adverbs; there, we observe di-
vergence for contentious conversations and conver-
gence for non-contentious conversations (detailed
plot omitted due to space constraints). This corre-
sponds to Niederhoffer and Pennebaker?s (2002) ob-
servations made on real conversations in their study
of the Watergate transcripts: when the relationship
between the two deteriorated, Richard Nixon con-
verged more to John Dean on articles, but diverged
on other features.19
Results for the other features Our results above
suggest some intriguing interplay between conver-
gence and gender, status, and level of hostility in
imagined dialogs, which may shed light on how
people (scriptwriters) nonconsciously expect con-
19Adverbs were not included in their study.
83
All
1s
t r
es
p.
2n
d r
es
p.0.02
0.01
0.00
0.01
0.02
0.03
Co
nv
er
ge
nc
e
article
Figure 5: Comparison of the convergence of first-billed
(lead) characters to second-billed characters (left bar in
1st resp. group) to that of second-billed characters to
leads (left bar in 2nd resp. group); righthand bars (dark
green) in each group show results for Male-Male pairs
only.
vergence to relate to such factors. (Interpreting these
sometimes apparently counterintuitive findings is
beyond the scope of this paper, but represents a fas-
cinating direction for future work.) Fig. 7 shows
how the nature of these relations depends on the trig-
ger family considered. The variation among families
is in line with the previous empirical results on the
multimodality of convergence in real conversations,
as discussed in ?4.
7 Summary and future work
We provide some insight into the causal mecha-
nism behind convergence, a topic that has gener-
ated substantial scrutiny and debate for over 40 years
(Ireland et al, 2011; Branigan et al, 2010). Our
work, along with Elson and McKeown (2010), ad-
vocates for the value of fictional sources in the study
of linguistic and social phenomena. To stimulate
such studies, we render our metadata-rich corpus of
movie dialog public.
In ?1, we described some practical applications
of a better understanding of the chameleon effect in
language; it boils down to improving communica-
tion both between humans and between humans and
computers. Also, our results on contention could
Ro
m.
 co
m.
No
 qu
arr
el
Sm
. q
ua
rre
l
Qu
arr
el0.01
0.00
0.01
0.02
0.03
0.04
0.05
0.06
Co
nv
erg
en
ce
article
Figure 6: Relation between contention and convergence.
The third bar combines quarreling and some quarreling
to ameliorate data sparsity. For comparison, Rom. com.
shows convergence calculated on all the conversations of
the 24 romantic-comedy pairs considered in this experi-
ment.
be used to further automatic controversy detection
(Mishne and Glance, 2006; G?mez et al, 2008).
Moreover, if we succeeded in linking our results
on narrative importance to relative social status, we
might further the development of systems that can
infer social relationships in online social networks
when conversational data is present but other, more
explicit cues are absent (Wyatt et al, 2008; Bram-
sen et al, 2011). Such systems could be valuable to
the rapidly expanding field of analyzing social net-
works.
Acknowledgments Many thanks for their help are
due to Claire Cardie, Catalin Draghici, Susan Du-
mais, Shimon Edelman, Michael Gamon, Jon Klein-
berg, Magdalena Naroz?niak, Alex Niculescu-Mizil,
Myle Ott, Bo Pang, Morgan Sonderegger, plus NLP
seminar participants Eric Baumer, Bin Lu, Chenhao
Tan, Lu Wang, Bishan Yang, Ainur Yessenalina, and
Marseille, and the anonymous reviewers (who went
far beyond the call of duty!). Supported by NSF IIS-
0910664, the Yahoo! FREP program, and a Yahoo!
Key Scientific Challenges award.
84
References
Shlomo Argamon, Sushant Dhawle, and Moshe Koppel.
2005. Lexical predictors of personality type. Proceed-
ings of the 2005 Joint Annual Meeting of the Interface
and the Classification Society of North America.
Frances Bilous and Robert Krauss. 1988. Dominance
and accommodation in the conversational behavior of
same- and mixed-gender dyads. Language and Com-
munication, 8:183?194.
Lukas Bleichenbacher. 2008. Multilingualism in the
movies: Hollywood characters and their language
choices. francke verlag, Jan.
J. Kathryn Bock. 1986. Syntactic persistence in lan-
guage production. Cognitive Psychology, 18(3):355
? 387.
Heather Bortfeld and Susan E. Brennan. 1997. Use and
acquisition of idiomatic expressions in referring by na-
tive and non-native speakers. Discourse Processes,
23(2):119?147.
Richard Y. Bourhis. 1991. Organizational communi-
cation and accommodation: Toward some conceptual
and empirical links. In Howard Giles, Justine Coup-
land, and Nikolas Coupland, editors, Contexts of Ac-
commodation. Cambridge University Press.
James J. Bradac, Anthony Mulac, and Ann House. 1988.
Lexical diversity and magnitude of convergent ver-
sus divergent style shifting: Perceptual and evaluative
consequences. Language and Communication, 8:213?
228, Nov.
Philip Bramsen, Martha Escobar-Molana, Ami Patel, and
Rafael Alonso. 2011. Extracting social power rela-
tionships from natural language. In Proceedings of
ACL HLT.
Holly P. Branigan, Martin J. Pickering, Jamie Pearson,
and Janet F. McLean. 2010. Linguistic alignment be-
tween people and computers. Journal of Pragmatics,
42(9):2355?2368.
Tanya L. Chartrand and John A. Bargh. 1999. The
chameleon effect: The perception-behavior link and
social interaction. J. Personality and Social Psychol-
ogy, 76(6):893?910.
Tanya L. Chartrand and Rick van Baaren. 2009. Chap-
ter 5: Human mimicry. In Mark P. Zanna, editor, Ad-
vances in Experimental Social Psychology, volume 41,
pp. 219?274. Academic Press.
Kenneth W. Church. 2000. Empirical estimates of adap-
tation: the chance of two noriegas is closer to p/2 than
p2. In Proceedings of COLING, pp. 180?186.
Herbert H. Clark. 1996. Using language. Cambridge
University Press, second edition.
Cristian Danescu-Niculescu-Mizil, Michael Gamon, and
Susan Dumais. 2011. Mark my words! Linguistic
style accommodation in social media. In Proceedings
of WWW.
David Elson and Kathleen McKeown. 2010. Automatic
attribution of quoted speech in literary narrative. In
Proceedings of AAAI.
David Elson, Nicholas Dames, and Kathleen McKeown.
2010. Extracting social networks from literary fiction.
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pp. 138?147.
Kathleen Ferrara. 1991. Accommodation in therapy. In
Accommodation theory: Communication, context, and
consequences. Cambridge University Press.
Howard Giles, Justine Coupland, and Nikolas Coup-
land. 1991. Accommodation theory: Communica-
tion, context, and consequences. In Accommodation
theory: Communication, context, and consequences.
Cambridge University Press.
Howard Giles, Michael Willemyns, Cynthia Gallois, and
Michelle Anderson. 2007. Accommodating a new
frontier: The context of law enforcement. In Klaus
Fiedler, editor, Social Communication, Frontiers of
Social Psychology, chapter 5, pp. 129?162.
Howard Giles. 2008. Communication accommodation
theory. In Engaging theories in interpersonal commu-
nication: Multiple perspectives. Sage Publications.
Vicen? G?mez, Andreas Kaltenbrunner, and Vicente
L?pez. 2008. Statistical analysis of the social network
and discussion threads in Slashdot. In Proceedings of
WWW, pp. 645?654.
Amy L. Gonzales, Jeffrey T. Hancock, and James W. Pen-
nebaker. 2010. Language style matching as a predic-
tor of social dynamics in small groups. Communica-
tion Research, 37(1):3?19, Feb.
Stanford W. Gregory and Stephen Webster. 1996. A
nonverbal signal in voices of interview partners effec-
tively predicts communication accommodation and so-
cial status perceptions. J. Personality and Social Psy-
chology, 70(6):1231?1240.
Heidi Hamilton. 1991. Accommodation and mental
disability. In Accommodation theory: Communica-
tion, context, and consequences. Cambridge Univer-
sity Press.
Jeffrey T. Hancock, Lauren E. Curry, Saurabh Goorha,
and Michael Woodworth. 2008. On lying and be-
ing lied to: A linguistic analysis of deception in
computer-mediated communication. Discourse Pro-
cesses, 45(1):1?23.
Susan C. Herring and John C. Paolillo. 2006. Gender
and genre variation in weblogs. Journal of Sociolin-
guistics, Jan.
Walter Hill. 1972. The Getaway. Directed by Sam Peck-
inpah.
85
Molly E. Ireland and James W. Pennebaker. 2010. Lan-
guage style matching in writing: Synchrony in essays,
correspondence, and poetry. J. Personality and Social
Psychology, 99(3):549?571.
Molly E. Ireland, Richard B. Slatcher, Paul W. Eastwick,
Lauren E. Scissors, Eli J. Finkel, and James W. Pen-
nebaker. 2011. Language style matching predicts re-
lationship initiation and stability. Psychological Sci-
ence, 22:39?44.
Patrick Juola. 2008. Authorship Attribution. Now Pub-
lishers.
Dan Jurafsky, Rajesh Ranganath, and Dan McFarland.
2009. Extracting social meaning: Identifying inter-
actional style in spoken conversation. In Proceedings
of the NAACL, pp. 638?646.
Philipp Koehn. 2004. Statistical significance tests
for machine translation evaluation. Proceedings of
EMNLP, pp. 388?395.
Moshe Koppel, Shlomo Argamon, and Anat Shimoni.
2002. Automatically categorizing written texts by au-
thor gender. Literary and Linguistic Computing.
Sarah Kozloff. 2000. Overhearing Film Dialogue. Uni-
versity of California Press.
Jessica L. Lakin, Valerie E. Jefferis, Clara Michelle
Cheng, and Tanya L. Chartrand. 2003. The chameleon
effect as social glue: Evidence for the evolutionary sig-
nificance of nonconscious mimicry. Journal of Non-
verbal Behavior, 27:145?162.
Willem J.M. Levelt and Stephanie Kelter. 1982. Surface
form and memory in question answering. Cognitive
Psychology, 14(1):78?106.
Fran?ois Mairesse, Marilyn A. Walker, Matthias R. Mehl,
and Roger K. Moore. 2007. Using linguistic cues for
the automatic recognition of personality in conversa-
tion and text. JAIR, pp. 457?500.
Petr Mare?. 2000. Fikce, konvence a realita: K v?ce-
jazyc?nosti v ume?leck?ch textech [fiction, convention,
and reality: On multilingualism in literary texts].
Slovo a slovesnost, 61(1):47?53.
Robert McKee. 1999. Story: Substance, Structure, Style,
and the Principles of Screenwriting. Methuen.
Rada Mihalcea and Carlo Strapparava. 2009. The lie
detector: Explorations in the automatic recognition
of deceptive language. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pp. 309?312.
Gilad Mishne and Natalie Glance. 2006. Leave a reply:
An analysis of weblog comments. Third annual work-
shop on the Weblogging ecosystem.
Frederick Mosteller and David L. Wallace. 1984. Ap-
plied Bayesian and Classical Inference: The Case of
the Federalist Papers. Springer-Verlag.
Arjun Mukherjee and Bing Liu. 2010. Improving gender
classification of blog authors. EMNLP.
Randall Munroe. 2010. http://xkcd.com/813/.
Laura L. Namy, Lynne C. Nygaard, and Denise Sauerteig.
2002. Gender differences in vocal accommodation. J.
Language and Social Psychology, 21(4):422?432.
Clifford Nass and Kwan Min Lee. 2000. Does computer-
generated speech manifest personality? An experi-
mental test of similarity-attraction. In Proceedings of
CHI, pp. 329?336.
Michael Natale. 1975. Convergence of mean vocal in-
tensity in dyadic communication as a function of so-
cial desirability. J. Personality and Social Psychol-
ogy, 32(5):790?804.
Kate G. Niederhoffer and James W. Pennebaker. 2002.
Linguistic style matching in social interaction. J. Lan-
guage and Social Psychology.
Jon Oberlander and Alastair J. Gill. 2006. Language
with character: A stratified corpus comparison of in-
dividual differences in e-mail communication. Dis-
course Processes, 42(3):239?270.
James W. Pennebaker, Roger J. Booth, and Martha E.
Francis. 2007. Linguistic inquiry and word
count (LIWC): A computerized text analysis program.
http://www.liwc.net/.
Cyma Van Petten and Marta Kutas. 1991. Influences of
semantic and syntactic context on open- and closed-
class words. Memory and Cognition, 19(1):95?112.
Martin Pickering and Simon Garrod. 2004. Toward a
mechanistic psychology of dialogue. Behavioral and
Brain Sciences, 27(02):169?190.
Rajesh Ranganath, Dan Jurafsky, and Dan McFarland.
2009. It?s not you, it?s me: Detecting flirting and
its misperception in speed-dates. In Proceedings of
EMNLP, pp. 334?342.
Paul Rayson, Andrew Wilson, and Geoffrey Leech.
2001. Grammatical word class variation within the
British National Corpus Sampler. Language and Com-
puters, 36:295?306(12).
David Reitter, Johanna D. Moore, and Frank Keller.
2006. Priming of syntactic rules in task-oriented di-
alogue and spontaneous conversation. In Proceedings
of the Conference of the Cognitive Science Society.
David Reitter, Frank Keller, and Johanna D. Moore.
2011. A Computational Cognitive Model of Syntac-
tic Priming. Cognitive Science.
Sidney J. Segalowitz and Korri Lane. 2004. Perceptual
fluency and lexical access for function versus content
words. Behavioral and Brain Sciences, 27(02):307?
308.
Morgan Sonderegger. 2010. Applications of graph the-
ory to an English rhyming corpus. Computer Speech
& Language, In Press, Corrected Proof.
Svetlana Stenchikova and Amanda Stent. 2007. Mea-
suring adaptation between dialogs. In Proc. of the 8th
SIGdial Workshop on Discourse and Dialogue.
86
Richard L. Street and Howard Giles. 1982. Speech ac-
commodation theory. In Social cognition and commu-
nication. Sage Publications.
Devi Stuart-Fox and Adnan Moussalli. 2008. Selection
for social signalling drives the evolution of chameleon
colour change. PLoS Biol, 6(1):e25, 01.
Paul J. Taylor and Sally Thomas. 2008. Linguistic style
matching and negotiation outcome. Negotiation and
Conflict Management Research, 1(3):263?281.
Jitendra N. Thakerar, Howard Giles, and Jenny Cheshire.
1982. Psychological and linguistic parameters of
speech accommodation theory. In C. Fraser and K.R.
Scherer, editors, Advances in the Social Psychology of
Language. Cambridge.
Rick B. van Baaren, Rob W. Holland, Bregje Steenaert,
and Ad van Knippenberg. 2003. Mimicry for money:
Behavioral consequences of imitation. Journal of Ex-
perimental Social Psychology, 39(4):393?398.
Arthur Ward and Diane Litman. 2007. Dialog conver-
gence and learning. In Artificial Intelligence in Edu-
cation (AIED), pp. 262?269.
Danny Wyatt, Jeff Bilmes, Tanzeem Choudhury, and
James A. Kitts. 2008. Towards the automated so-
cial analysis of situated speech data. In Proceedings
of Ubicomp, pp. 168?171.
Patrick Ye and Timothy Baldwin. 2006. Verb sense
disambiguation using selectional preferences extracted
with a state-of-the-art semantic role labeler. In Pro-
ceedings of the Australasian Language Technology
Workshop 2006, pp. 139?148.
87
Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012),
pages 70?79, Jeju, Republic of Korea, 13 July 2012. c?2012 Association for Computational Linguistics
Hedge Detection as a Lens on Framing in the GMO Debates:
A Position Paper
Eunsol Choi?, Chenhao Tan?, Lillian Lee?, Cristian Danescu-Niculescu-Mizil? and Jennifer Spindel?
?Department of Computer Science, ?Department of Plant Breeding and Genetics
Cornell University
ec472@cornell.edu, chenhao|llee|cristian@cs.cornell.edu, jes462@cornell.edu
Abstract
Understanding the ways in which participants
in public discussions frame their arguments is
important in understanding how public opin-
ion is formed. In this paper, we adopt the po-
sition that it is time for more computationally-
oriented research on problems involving fram-
ing. In the interests of furthering that goal,
we propose the following specific, interesting
and, we believe, relatively accessible ques-
tion: In the controversy regarding the use
of genetically-modified organisms (GMOs) in
agriculture, do pro- and anti-GMO articles dif-
fer in whether they choose to adopt a more
?scientific? tone?
Prior work on the rhetoric and sociology of
science suggests that hedging may distin-
guish popular-science text from text written
by professional scientists for their colleagues.
We propose a detailed approach to studying
whether hedge detection can be used to un-
derstanding scientific framing in the GMO de-
bates, and provide corpora to facilitate this
study. Some of our preliminary analyses sug-
gest that hedges occur less frequently in scien-
tific discourse than in popular text, a finding
that contradicts prior assertions in the litera-
ture. We hope that our initial work and data
will encourage others to pursue this promising
line of inquiry.
1 Introduction
1.1 Framing, ?scientific discourse?, and GMOs
in the media
The issue of framing (Goffman, 1974; Scheufele,
1999; Benford and Snow, 2000) is of great im-
portance in understanding how public opinion is
formed. In their Annual Review of Political Science
survey, Chong and Druckman (2007) describe fram-
ing effects as occurring ?when (often small) changes
in the presentation of an issue or an event produce
(sometimes large) changes of opinion? (p. 104);
as an example, they cite a study wherein respon-
dents answered differently, when asked whether a
hate group should be allowed to hold a rally, depend-
ing on whether the question was phrased as one of
?free speech? or one of ?risk of violence?.
The genesis of our work is in a framing question
motivated by a relatively current political issue. In
media coverage of transgenic crops and the use of
genetically modified organisms (GMOs) in food, do
pro-GMO vs. anti-GMO articles differ not just with
respect to word choice, but in adopting a more ?sci-
entific? discourse, meaning the inclusion of more
uncertainty and fewer emotionally-laden words? We
view this as an interesting question from a text anal-
ysis perspective (with potential applications and im-
plications that lie outside the scope of this article).
1.2 Hedging as a sign of scientific discourse
To obtain a computationally manageable character-
ization of ?scientific discourse?, we turned to stud-
ies of the culture and language of science, a body
of work spanning fields ranging from sociology to
applied linguistics to rhetoric and communication
(Gilbert and Mulkay, 1984; Latour, 1987; Latour
and Woolgar, 1979; Halliday and Martin, 1993; Baz-
erman, 1988; Fahnestock, 2004; Gross, 1990).
One characteristic that has drawn quite a bit of
attention in such studies is hedging (Myers, 1989;
70
Hyland, 1998; Lewin, 1998; Salager-Meyer, 2011).1
Hyland (1998, pg. 1) defines hedging as the ex-
pression of ?tentativeness and possibility? in com-
munication, or, to put it another way, language cor-
responding to ?the writer withholding full commit-
ment to statements? (pg. 3). He supplies many
real-life examples from scientific research articles,
including the following:
1. ?It seems that this group plays a critical role in
orienting the carboxyl function? (emphasis Hy-
land?s)
2. ?...implies that phytochrome A is also not nec-
essary for normal photomorphogenesis, at least
under these irradiation conditions? (emphasis
Hyland?s)
3. ?We wish to suggest a structure for the salt of
deoxyribose nucleic acid (D.N.A.)? (emphasis
added)2
Several scholars have asserted the centrality of hedg-
ing in scientific and academic discourse, which cor-
responds nicely to the notion of ?more uncertainty?
mentioned above. Hyland (1998, p. 6) writes, ?De-
spite a widely held belief that professional scientific
writing is a series of impersonal statements of fact
which add up to the truth, hedges are abundant in
science and play a critical role in academic writing?.
Indeed, Myers (1989, p. 13) claims that in scien-
tific research articles, ?The hedging of claims is so
common that a sentence that looks like a claim but
has no hedging is probably not a statement of new
knowledge?.3
Not only is understanding hedges important to un-
derstanding the rhetoric and sociology of science,
but hedge detection and analysis ? in the sense of
identifying uncertain or uncertainly-sourced infor-
mation (Farkas et al, 2010) ? has important appli-
cations to information extraction, broadly construed,
and has thus become an active sub-area of natural-
language processing. For example, the CoNLL 2010
1In linguistics, hedging has been studied since the 1970s
(Lakoff, 1973).
2This example originates from Watson and Crick?s land-
mark 1953 paper. Although the sentence is overtly tentative,
did Watson and Crick truly intend to be polite and modest in
their claims? See Varttala (2001) for a review of arguments re-
garding this question.
3Note the inclusion of the hedge ?probably?.
Shared Task was devoted to this problem (Farkas
et al, 2010).
Putting these two lines of research together, we
see before us what appears to be an interesting in-
terdisciplinary and, at least in principle, straightfor-
ward research program: relying on the aforemen-
tioned rhetoric analyses to presume that hedging is
a key characteristic of scientific discourse, build a
hedge-detection system to computationally ascertain
which proponents in the GMO debate tend to use
more hedges and thus, by presumption, tend to adopt
a more ?scientific? frame.4
1.3 Contributions
Our overarching goal in this paper is to convince
more researchers in NLP and computational linguis-
tics to work on problems involving framing. We
try to do so by proposing a specific problem that
may be relatively accessible. Despite the apparent
difficulty in addressing such questions, we believe
that progress can be made by drawing on observa-
tions drawn from previous literature across many
fields, and integrating such work with movements
in the computational community toward considera-
tion of extra-propositional and pragmatic concerns.
We have thus intentionally tried to ?cover a lot of
ground?, as one referee put it, in the introductory
material just discussed.
Since framing problems are indeed difficult, we
elected to narrow our scope in the hope of making
some partial progress. Our technical goal here, at
this workshop, where hedge detection is one of the
most relevant topics to the broad questions we have
raised, is not to learn to classify texts as being pro-
vs. anti-GMO, or as being scientific or not, per se.5
Our focus is on whether hedging specifically, con-
sidered as a single feature, is correlated with these
different document classes, because of the previous
research attention that has been devoted to hedging
in particular and because of hedging being one of the
topics of this workshop. The point of this paper is
4However, this presumption that more hedges characterize a
more scientific discourse has been contested. See section 2 for
discussion and section 4.2 for our empirical investigation.
5Several other groups have addressed the problem of try-
ing to identify different sides or perspectives (Lin et al, 2006;
Hardisty et al, 2010; Beigman Klebanov et al, 2010; Ahmed
and Xing, 2010).
71
thus not to compare the efficacy of hedging features
with other types, such as bag-of-words features. Of
course, to do so is an important and interesting di-
rection for future work.
In the end, we were not able to achieve satisfac-
tory results even with respect to our narrowed goal.
However, we believe that other researchers may be
able to follow the plan of attack we outline below,
and perhaps use the data we are releasing, in order
to achieve our goal. We would welcome hearing the
results of other people?s efforts.
2 How should we test whether hedging
distinguishes scientific text?
One very important point that we have not yet ad-
dressed is: While the literature agrees on the impor-
tance of hedging in scientific text, the relative de-
gree of hedging in scientific vs. non-scientific text is
a matter of debate.
On the one side, we have assertions like those of
Fahnestock (1986), who shows in a clever, albeit
small-scale, study involving parallel texts that when
scientific observations pass into popular accounts,
changes include ?removing hedges ... thus con-
ferring greater certainty on the reported facts? (pg.
275). Similarly, Juanillo, Jr. (2001) refers to a shift
from a forensic style to a ?celebratory? style when
scientific research becomes publicized, and credits
Brown (1998) with noting that ?celebratory scien-
tific discourses tend to pay less attention to caveats,
contradictory evidence, and qualifications that are
highlighted in forensic or empiricist discourses. By
downplaying scientific uncertainty, it [sic] alludes to
greater certainty of scientific results for public con-
sumption? (Juanillo, Jr., 2001, p. 42).
However, others have contested claims that the
popularization process involves simplification, dis-
tortion, hype, and dumbing down, as Myers (2003)
colorfully puts it; he provides a critique of the rel-
evant literature. Varttala (1999) ran a corpus anal-
ysis in which hedging was found not just in pro-
fessional medical articles, but was also ?typical of
popular scientific articles dealing with similar top-
ics? (p. 195). Moreover, significant variation in use
of hedging has been found across disciplines and au-
thors? native language; see Salager-Meyer (2011) or
Varttala (2001) for a review.
To the best of our knowledge, there have been no
large-scale empirical studies validating the hypoth-
esis that hedges appear more or less frequently in
scientific discourse.
Proposed procedure Given the above, our first
step must be to determine whether hedges are more
or less prominent in ?professional scientific? (hence-
forth ?prof-science??) vs. ?public science? (hence-
forth ?pop-science?) discussions of GMOs. Of
course, for a large-scale study, finding hedges re-
quires developing and training an effective hedge de-
tection algorithm.
If the first step shows that hedges can indeed be
used to effectively distinguish prof-science vs. pop-
science discourse on GMOs, then the second step is
to examine whether the use of hedging in pro-GMO
articles follows our inferred ?scientific? occurrence
patterns to a greater extent than the hedging in anti-
GMO articles.
However, as our hedge classifier trained on the
CoNLL dataset did not perform reliably on the dif-
ferent domain of prof-science vs. pop-science dis-
cussions of GMOs, we focus the main content of this
paper on the first step. We describe data collection
for the second step in the appendix.
3 Data
To accomplish the first step of our proposed pro-
cedure outlined above, we first constructed a prof-
science/pop-science corpus by pulling text from
Web of Science for prof-science examples and from
LexisNexis for pop-science examples, as described
in Section 3.1. Our corpus will be posted online
at https://confluence.cornell.edu/display/llresearch/
HedgingFramingGMOs.
As noted above, computing the degree of hedg-
ing in the aforementioned corpus requires access to
a hedge-detection algorithm. We took a supervised
approach, taking advantage of the availability of the
CoNLL 2010 hedge-detection training and evalua-
tion corpora, described in Section 3.2
3.1 Prof-science/pop-science data: LEXIS and
WOS
As mentioned previously, a corpus of prof-science
and pop-science articles is required to ascertain
whether hedges are more prevalent in one or the
72
Dataset Doc type # docs # sentences Avg sentence length Flesch reading ease
Prof-science/pop-science corpus
WOS abstracts 648 5596 22.35 23.39
LEXIS (short) articles 928 36795 24.92 45.78
Hedge-detection corpora
Bio (train) abstracts, articles 1273, 9 14541 (18% uncertain) 29.97 20.77
Bio (eval) articles 15 5003 (16% uncertain) 31.30 30.49
Wiki (train) paragraphs 2186 11111 (22% uncertain) 23.07 35.23
Wiki (eval) paragraphs 2346 9634 (23% uncertain) 20.82 31.71
Table 1: Basic descriptive statistics for the main corpora we worked with. We created the first two. Higher Flesch
scores indicate text that is easier to read.
other of these two writing styles. Since our ultimate
goal is to look at discourse related to GMOs, we re-
strict our attention to documents on this topic.
Thomson Reuter?s Web of Science (WOS), a
database of scientific journal and conference arti-
cles, was used as a source of prof-science samples.
We chose to collect abstracts, rather than full scien-
tific articles, because intuition suggests that the lan-
guage in abstracts is more high-level than that in the
bodies of papers, and thus more similar to the lan-
guage one would see in a public debate on GMOs.
To select for on-topic abstracts, we used the phrase
?transgenic foods? as a search keyword and dis-
carded results containing any of a hand-selected list
of off-topic filtering terms (e.g., ?mice? or ?rats?).
We then made use of domain expertise to manually
remove off-topic texts. The process yielded 648 doc-
uments for a total of 5596 sentences.
Our source of pop-science articles was Lexis-
Nexis (LEXIS). On-topic documents were collected
from US newspapers using the search keywords ?ge-
netically modified foods? or ?transgenic crops? and
then imposing the additional requirement that at
least two terms on a hand-selected list7 be present
in each document. After the removal of duplicates
and texts containing more than 2000 words to delete
excessively long articles, our final pop-science sub-
corpus was composed of 928 documents.
7The term list: GMO, GM, GE, genetically modified, ge-
netic modification, modified, modification, genetic engineer-
ing, engineered, bioengineered, franken, transgenic, spliced,
G.M.O., tweaked, manipulated, engineering, pharming, aqua-
culture.
3.2 CoNLL hedge-detection training data 8
As described in Farkas et al (2010), the motivation
behind the CoNLL 2010 shared task is that ?distin-
guishing factual and uncertain information in texts is
of essential importance in information extraction?.
As ?uncertainty detection is extremely important for
biomedical information extraction?, one component
of the dataset is biological abstracts and full arti-
cles from the BioScope corpus (Bio). Meanwhile,
the chief editors of Wikipedia have drawn the at-
tention of the public to specific markers of uncer-
tainty known as weasel words9: they are words or
phrases ?aimed at creating an impression that some-
thing specific and meaningful has been said?, when,
in fact, ?only a vague or ambiguous claim, or even
a refutation, has been communicated?. An example
is ?It has been claimed that ...?: the claimant has not
been identified, so the source of the claim cannot be
verified. Thus, another part of the dataset is a set
of Wikipedia articles (Wiki) annotated with weasel-
word information. We view the combined Bio+Wiki
corpus (henceforth the CoNLL dataset) as valuable
for developing hedge detectors, and we attempt to
study whether classifiers trained on this data can be
generalized to other datasets.
3.3 Comparison
Table 1 gives the basic statistics on the main datasets
we worked with. Though WOS and LEXIS differ in
the total number of sentences, the average sentence
length is similar. The average sentence length in Bio
is longer than that in Wiki. The articles in WOS
are markedly more difficult to read than the articles
8http://www.inf.u-szeged.hu/rgai/conll2010st/
9http://en.wikipedia.org/wiki/Weasel word
73
in LEXIS according to Flesch reading ease (Kincaid
et al, 1975).
4 Hedging to distinguish scientific text:
Initial annotation
As noted in Section 1, it is not a priori clear whether
hedging distinguishes scientific text or that more
hedges correspond to a more ?scientific? discourse.
To get an initial feeling for how frequently hedges
occur in WOS and LEXIS, we hand-annotated a
sample of sentences from each. In Section 4.1, we
explain the annotation policy of the CoNLL 2010
Shared Task and our own annotation method for
WOS and LEXIS. After that, we move forward in
Section 4.2 to compare the percentage of uncertain
sentences in prof-science vs. pop-science text on
this small hand-labeled sample, and gain some ev-
idence that there is indeed a difference in hedge oc-
currence rates, although, perhaps surprisingly, there
seem to be more hedges in the pop-science texts.
As a side benefit, we subsequently use the
hand-labeled sample we produce to investigate the
accuracy of an automatic hedge detector in the
WOS+LEXIS domain; more on this in Section 5.
4.1 Uncertainty annotation
CoNLL 2010 Shared Task annotation policy As
described in Farkas et al (2010, pg. 4), the data an-
notation polices for the CoNLL 2010 Shared Task
were that ?sentences containing at least one cue
were considered as uncertain, while sentences with
no cues were considered as factual?, where a cue
is a linguistic marker that in context indicates un-
certainty. A straightforward example of a sentence
marked ?uncertain? in the Shared Task is ?Mild blad-
der wall thickening raises the question of cystitis.?
The annotated cues are not necessarily general, par-
ticularly in Wiki, where some of the marked cues
are as specific as ?some of schumann?s best choral
writing?, ?people of the jewish tradition?, or ?certain
leisure or cultural activities?.
Note that ?uncertainty? in the Shared Task def-
inition also encompassed phrasing that ?creates an
impression that something important has been said,
but what is really communicated is vague, mislead-
ing, evasive or ambiguous ... [offering] an opinion
without any backup or source?. An example of such
Dataset % of uncertain sentences
WOS (estimated from 75-sentence sample) 20
LEXIS (estimated from 78-sentence sample) 28
Bio 17
Wiki 23
Table 2: Percentages of uncertain sentences.
a sentence, drawn from Wikipedia and marked ?un-
certain? in the Shared Task, is ?Some people claim
that this results in a better taste than that of other diet
colas (most of which are sweetened with aspartame
alone).?; Farkas et al (2010) write, ?The ... sentence
does not specify the source of the information, it is
just the vague term ?some people? that refers to the
holder of this opinion?.
Our annotation policy We hand-annotated 200
randomly-sampled sentences, half from WOS and
half from LEXIS10, to gauge the frequency with
which hedges occur in each corpus. Two annota-
tors each followed the rules of the CoNLL 2010
Shared Task to label sentences as certain, uncertain,
or not a proper sentence.11 The annotators agreed on
153 proper sentences of the 200 sentences (75 from
WOS and 78 from LEXIS). Cohen?s Kappa (Fleiss,
1981) was 0.67 on the annotation, which means that
the consistency between the two annotators was fair
or good. However, there were some interesting cases
where the two annotators could not agree. For ex-
ample, in the sentence ?Cassava is the staple food of
tropical Africa and its production, averaged over 24
countries, has increased more than threefold from
1980 to 2005 ... ?, one of the annotators believed
that ?more than? made the sentence uncertain. These
borderline cases indicate that the definition of hedg-
ing should be carefully delineated in future studies.
4.2 Percentages of uncertain sentences
To validate the hypothesis that prof-science articles
contain more hedges, we computed the percentage
10We took steps to attempt to hide from the annotators any
explicit clues as to the source of individual sentences: the sub-
set of authors who did the annotation were not those that col-
lected the data, and the annotators were presented the sentences
in random order.
11The last label was added because of a few errors in scraping
the data.
74
of uncertain sentences in our labeled data. As shown
in Table 2, we observed a trend contradicting ear-
lier studies. Uncertain sentences were more frequent
in LEXIS than in WOS, though the difference was
not statistically significant12 (perhaps not surprising
given the small sample size). The same trend was
seen in the CoNLL dataset: there, too, the percent-
age of uncertain sentences was significantly smaller
in Bio (prof-science articles) than in Wiki. In order
to make a stronger argument about prof-science vs
pop-science, however, more annotation on the WOS
and LEXIS datasets is needed.
5 Experiments
As stated in Section 1, our proposal requires devel-
oping an effective hedge detection algorithm. Our
approach for the preliminary work described in this
paper is to re-implement Georgescul?s (2010) algo-
rithm; the experimental results on the Bio+Wiki do-
main, given in Section 5.1, are encouraging. Then
we use this method to attempt to validate (at a larger
scale than in our manual pilot annotation) whether
hedges can be used to distinguish between prof-
science and pop-science discourse on GMOs. Un-
fortunately, our results, given in Section 5.2, are
inconclusive, since our trained model could not
achieve satisfactory automatic hedge-detection ac-
curacy on the WOS+LEXIS domain.
5.1 Method
We adopted the method of Georgescul (2010): Sup-
port Vector Machine classification based on a Gaus-
sian Radial Basis kernel function (Vapnik, 1998; Fan
et al, 2005), employing n-grams from annotated cue
phrases as features, as described in more detail be-
low. This method achieved the top performance in
the CoNLL 2010 Wikipedia hedge-detection task
(Farkas et al, 2010), and SVMs have been proven
effective for many different applications. We used
the LIBSVM toolkit in our experiments13.
As described in Section 3.2, there are two separate
datasets in the CoNLL dataset. We experimented on
them separately (Bio, Wiki). Also, to make our clas-
sifier more generalizable to different datasets, we
12Throughout, ?statistical significance? refers to the student
t-test with p < .05.
13http://www.csie.ntu.edu.tw/?cjlin/libsvm/
also trained models based on the two datasets com-
bined (Bio+Wiki). As for features, we took advan-
tage of the observation in Georgescul (2010) that the
bag-of-words model does not work well for this task.
We used different sets of features based on hedge
cue words that have been annotated as part of the
CoNLL dataset distribution14. The basic feature set
was the frequency of each hedge cue word from the
training corpus after removing stop words and punc-
tuation and transforming words to lowercase. Then,
we extracted unigrams, bigrams and trigrams from
each hedge cue phrase. Table 3 shows the number
of features in different settings. Notice that there are
many more features in Wiki. As mentioned above,
in Wiki, some cues are as specific as ?some of schu-
mann?s best choral writing?, ?people of the jewish
tradition?, or ? certain leisure or cultural activities?.
Taking n-grams from such specific cues can cause
some sentences to be classified incorrectly.
Feature source #features
Bio 220
Bio (cues + bigram + trigram) 340
Wiki 3740
Wiki (cues + bigram + trigram) 10603
Table 3: Number of features.
Best cross-validation performance
Dataset (C, ?) P R F
Bio (40, 2?3) 84.0 92.0 87.8
Wiki (30, 2?6) 64.0 76.3 69.6
Bio+Wiki (10, 2?4) 66.7 78.3 72.0
Table 4: Best 5-fold cross-validation performance for Bio
and/or Wiki after parameter tuning. As a reminder, we
repeat that our intended final test set is the WOS+LEXIS
corpus, which is disjoint from Bio+Wiki.
We adopted several techniques from Georgescul
(2010) to optimize performance through cross vali-
dation. Specifically, we tried different combinations
of feature sets (the cue phrases themselves, cues +
14For the Bio model, we used cues extracted from Bio. Like-
wise, the Wiki model used cues from Wiki, and the Bio+Wiki
model used cues from Bio+Wiki.
75
Evaluation set Model P R F
WOS+LEXIS Bio 54 68 60
WOS+LEXIS Wiki 38 54 45
WOS+LEXIS Bio+Wiki 21 93 34
Sub-corpus performance of the model based on Bio
WOS Bio 58 73 65
LEXIS Bio 52 64 57
Table 5: The upper part shows the performance on WOS
and LEXIS based on models trained on the CoNLL
dataset. The lower part gives the sub-corpus results for
Bio, which provided the best performance on the full
WOS+LEXIS corpus.
unigram, cues + bigram, cues + trigram, cues + uni-
gram + bigram + trigram, cues + bigram + trigram).
We tuned the width of the RBF kernel (?) and the
regularization parameter (C) via grid search over the
following range of values: {2?9, 2?8, 2?7, . . . , 24}
for ?, {1, 10, 20, 30, . . . , 150} for C. We also tried
different weighting strategies for negative and pos-
itive classes (i.e., either proportional to the number
of positive instances, or uniform). We performed 5-
fold cross validation for each possible combination
of experimental settings on the three datasets (Bio,
Wiki, Bio+Wiki).
Table 4 shows the best performance on all three
datasets and the corresponding parameters. In the
three datasets, cue+bigram+trigram provided the
best performance, and the weighted model con-
sistently produced superior results to the uniform
model. The F1 measure for Bio was 87.8, which
was satisfactory, while the F1 results for Wiki were
69.6, which were the worst of all the datasets.
This resonates with our observation that the task on
Wikipedia is more subtly defined and thus requires
a more sophisticated approach than counting the oc-
currences of bigrams and trigrams.
5.2 Results on WOS+LEXIS
Next, we evaluated whether our best classifier
trained on the CoNLL dataset can be generalized to
other datasets, in particular, the WOS and LEXIS
corpus. Performance was measured on the 153 sen-
tences on which our annotators agreed, a dataset
that was introduced in Section 4.1. Table 5 shows
how the best models trained on Bio, Wiki, and
Evaluation set (C, ?) P R F
WOS + LEXIS (50, 2?9) 68 62 65
WOS (50, 2?9) 85 73 79
LEXIS (50, 2?9) 57 54 56
Table 6: Best performance after parameter tuning
based on the 153 labeled WOS+LEXIS sentences; this
gives some idea of the upper-bound potential of our
Georgescul-based method. The training set is Bio, which
gave the best performance in Table 5.
Bio+Wiki, respectively, performed on the 153 la-
beled sentences. First, we can see that the perfor-
mance degraded significantly compared to the per-
formance for in-domain cross validation. Second, of
the three different models, Bio showed the best per-
formance. Bio+Wiki gave the worst performance,
which hints that combining two datasets and cue
words may not be a promising strategy: although
Bio+Wiki shows very good recall, this can be at-
tributed to its larger feature set, which contains all
available cues and perhaps as a result has a very high
false-positive rate. We further investigated and com-
pared performance on LEXIS and WOS for the best
model (Bio). Not surprisingly, our classifier works
better in WOS than in LEXIS.
It is clear that there exist domain differences be-
tween the CoNLL dataset and WOS+LEXIS. To bet-
ter understand the poor cross-domain performance
of the classifier, we tuned another model based on
the performance on the 153 labeled sentences us-
ing Bio as training data. As we can see in Table
6, the performance on WOS improved significantly,
while the performance on LEXIS decreased. This
is probably caused by the fact that WOS is a col-
lection of scientific paper abstracts, which is more
similar to the training corpus than LEXIS, which is
a collection of news media articles15. Also, LEXIS
articles are hard to classify even with the tuned
model, which challenges the effectiveness of a cue-
words frequency approach beyond professional sci-
entific texts. Indeed, the simplicity of our reim-
plementation of Georgescul?s algorithm seems to
cause longer sentences to be classified as uncer-
tain, because cue phrases (or n-grams extracted from
15The Wiki model performed better on LEXIS than on WOS.
Though the performance was not good, this result further rein-
forces the possibility of a domain-dependence problem.
76
cue phrases) are more likely to appear in lengthier
sentences. Analysis of the best performing model
shows that the false-positive sentences are signifi-
cantly longer than the false-negative ones.16
Dataset Model % classified uncertain
WOS Bio 16
LEXIS Bio 19
WOS Tuned 15
LEXIS Tuned 14
Table 7: For completeness, we report here the percentage
of uncertain sentences in WOS and LEXIS according to
our trained classifiers, although we regard these results as
unreliable since those classifiers have low accuracy. Bio
refers to the best model trained on Bio only in Section 5.1,
while Tuned refers to the model in Table 6 that is tuned
based on the 153 labeled sentences in WOS+LEXIS.
While the cross-domain results were not reliable,
we produced preliminary results on whether there
exist fewer hedges in scientific text. We can see that
the relative difference in certain/uncertain ratios pre-
dicted by the two different models (Bio, Tuned) are
different in Table 7. In the tuned model, the differ-
ence between LEXIS and WOS in terms of the per-
centage of uncertain sentences was not statistically
significant, while in the Bio model, their difference
was statistically significant. Since the performance
of our hedge classifier on the 153 hand-annotated
WOS+LEXIS sentences was not reliable, though,
we must abstain from making conclusive statements
here.
6 Conclusion and future work
In this position paper, we advocated that researchers
apply hedge detection not only to the classic moti-
vation of information-extraction problems, but also
to questions of how public opinion forms. We pro-
posed a particular problem in how participants in de-
bates frame their arguments. Specifically, we asked
whether pro-GMO and anti-GMO articles differ in
adopting a more ?scientific? discourse. Inspired by
earlier studies in social sciences relating hedging to
texts aimed at professional scientists, we proposed
16Average length of true positive sentences : 28.6, false pos-
itive sentences 35.09, false negative sentences: 22.0.
addressing the question with automatic hedge de-
tection as a first step. To develop a hedge clas-
sifier, we took advantage of the CoNLL dataset
and a small annotated WOS and LEXIS dataset.
Our preliminary results show there may exist a gap
which indicates that hedging may, in fact, distin-
guish prof-science and pop-science documents. In
fact, this computational analysis suggests the possi-
bility that hedges occur less frequently in scientific
prose, which contradicts several prior assertions in
the literature.
To confirm the argument that pop-science tends
to use more hedging than prof-science, we need
a hedge classifier that performs more reliably in
the WOS and LEXIS dataset than ours does. An
interesting research direction would be to develop
transfer-learning techniques to generalize hedge
classifiers for different datasets, or to develop a gen-
eral hedge classifier relatively robust to domain dif-
ferences. In either case, more annotated data on
WOS and LEXIS is needed for better evaluation or
training.
Another strategy would be to bypass the first step,
in which we determine whether hedges are more
or less prominent in scientific discourse, and pro-
ceed directly to labeling and hedge-detection in pro-
GMO and anti-GMO texts. However, this will not
answer the question of whether advocates in debates
other than on GMO-related topics employ a more
scientific discourse. Nonetheless, to aid those who
wish to pursue this alternate strategy, we have col-
lected two sets of opinionated articles on GMO (pro-
and anti-); see appendix for more details.
Acknowledgments We thank Daniel Hopkins and
Bonnie Webber for reference suggestions, and the
anonymous reviewers for helpful and thoughtful
comments. This paper is based upon work sup-
ported in part by US NSF grants IIS-0910664 and
IIS-1016099, a US NSF graduate fellowship to JS,
Google, and Yahoo!
References
Amr Ahmed and Eric P Xing. Staying informed: su-
pervised and semi-supervised multi-view topical
analysis of ideological perspective. In EMNLP,
pages 1140?1150, 2010.
Charles Bazerman. Shaping Written Knowledge:
77
The Genre and Activity of the Experimental Ar-
ticle in Science. University of Wisconsin Press,
Madison, Wis., 1988.
Beata Beigman Klebanov, Eyal Beigman, and
Daniel Diermeier. Vocabulary choice as an indi-
cator of perspective. In ACL Short Papers, pages
253?257, Stroudsburg, PA, USA, 2010. Associa-
tion for Computational Linguistics.
Robert D. Benford and David A. Snow. Framing
processes and social movements: An overview
and assessment. Annual Review of Sociology, 26:
611?639, 2000.
Richard Harvey Brown. Toward a democratic sci-
ence: Scientific narration and civic communica-
tion. Yale University Press, New Haven, 1998.
Dennis Chong and James N. Druckman. Framing
theory. Annual Review of Political Science, 10:
103?126, 2007.
Jeanne Fahnestock. Accommodating Science. Writ-
ten Communication, 3(3):275?296, 1986.
Jeanne Fahnestock. Preserving the figure: Consis-
tency in the presentation of scientific arguments.
Written Communication, 21(1):6?31, 2004.
Rong-En Fan, Pai-Hsuen Chen, and Chih-Jen Lin.
Working set selection using second order in-
formation for training support vector machines.
JMLR, 6:1889?1918, December 2005. ISSN
1532-4435.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra,
Ja?nos Csirik, and Gyo?rgy Szarvas. The CoNLL-
2010 shared task: Learning to detect hedges and
their scope in natural language text. In CoNLL?
Shared Task, pages 1?12, 2010.
Joseph L. Fleiss. Statistical Methods for Rates
and Proportions. Wiley series in probability and
mathematical statistics. John Wiley & Sons, New
York, second edition, 1981.
Maria Georgescul. A hedgehop over a max-margin
framework using hedge cues. In CONLL?
Shared-Task, pages 26?31, 2010.
G. Nigel Gilbert and Michael Joseph Mulkay. Open-
ing Pandora?s box: A sociological analysis of sci-
entists? discourse. CUP Archive, 1984.
Erving Goffman. Frame analysis: An essay on the
organization of experience. Harvard University
Press, 1974.
Alan G. Gross. The rhetoric of science. Harvard
University Press, Cambridge, Mass., 1990.
Michael Alexander Kirkwood Halliday and
James R. Martin. Writing science: Literacy and
discursive power. Psychology Press, London
[u.a.], 1993.
Eric A Hardisty, Jordan Boyd-Graber, and Philip
Resnik. Modeling perspective using adaptor
grammars. In EMNLP, pages 284?292, 2010.
Ken Hyland. Hedging in scientific research articles.
John Benjamins Pub. Co., Amsterdam; Philadel-
phia, 1998.
Napoleon K. Juanillo, Jr. Frames for Public Dis-
course on Biotechnology. In Genetically Modified
Food and the Consumer: Proceedings of the 13th
meeting of the National Agricultural Biotechnol-
ogy Council, pages 39?50, 2001.
J. Peter Kincaid, Robert P. Fishburne, Richard L.
Rogers, and Brad S. Chissom. Derivation of new
readability formulas for navy enlisted personnel.
Technical report, National Technical Information
Service, Springfield, Virginia, February 1975.
George Lakoff. Hedges: A study in meaning cri-
teria and the logic of fuzzy concepts. Journal of
Philosophical Logic, 2(4):458?508, 1973.
Bruno Latour. Science in action: How to follow sci-
entists and engineers through society. Harvard
University Press, Cambridge, Mass., 1987.
Bruno Latour and Steve Woolgar. Laboratory life:
The social construction of scientific facts. Sage
Publications, Beverly Hills, 1979.
Beverly A. Lewin. Hedging: Form and function
in scientific research texts. In Genre Studies in
English for Academic Purposes, volume 9, pages
89?108. Universitat Jaume I, 1998.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. Which side are you on?
identifying perspectives at the document and sen-
tence levels. In CoNLL, 2006.
Greg Myers. The pragmatics of politeness in sci-
entific articles. Applied Linguistics, 10(1):1?35,
1989.
78
Greg Myers. Discourse studies of scientific popular-
ization: Questioning the boundaries. Discourse
Studies, 5(2):265?279, 2003.
Franc?oise Salager-Meyer. Scientific discourse and
contrastive linguistics: hedging. European Sci-
ence Editing, 37(2):35?37, 2011.
Dietram A. Scheufele. Framing as a theory of media
effects. Journal of Communication, 49(1):103?
122, 1999.
Vladimir N. Vapnik. Statistical Learning Theory.
Wiley-Interscience, 1998.
Teppo Varttala. Remarks on the communicative
functions of hedging in popular scientific and spe-
cialist research articles on medicine. English for
Specific Purposes, 18(2):177?200, 1999.
Teppo Varttala. Hedging in scientifically oriented
discourse: Exploring variation according to dis-
cipline and intended audience. PhD thesis, Uni-
versity of Tampere, 2001.
7 Appendix: pro- vs. anti-GMO dataset
Here, we describe the pro- vs. anti-GMO dataset we
collected, in the hopes that this dataset may prove
helpful in future research regarding the GMO de-
bates, even though we did not use the corpus in the
project described in this paper.
The second step of our overall procedure out-
lined in the introduction ? that step being to ex-
amine whether the use of hedging in pro-GMO arti-
cles corresponds with our inferred ?scientific? oc-
currence patterns more than that in anti-GMO ar-
ticles ? requires a collection of opinionated arti-
cles on GMOs. Our first attempt to use news me-
dia articles (LEXIS) was unsatisfying, as we found
many articles attempt to maintain a neutral position.
This led us to collect documents from more strongly
opinionated organizational websites such as Green-
peace (anti-GMO), Non GMO Project (anti-GMO),
or Why Biotechnology (pro-GMO). Articles were
collected from 20 pro-GMO and 20 anti-GMO or-
ganizational web sites.
After the initial collection of data, near-duplicates
and irrelevant articles were filtered through cluster-
ing, keyword searches and distance between word
vectors at the document level. We have collected
762 ?anti? documents and 671 ?pro? documents.
We reduced this to a 404 ?pro? and 404 ?con?
set as follows. Each retained ?document? con-
sists of only the first 200 words after excluding the
first 50 words of documents containing over 280
words. This was done to avoid irrelevant sections
such as Educators have permission to reprint arti-
cles for classroom use; other users, please contact
editor@actionbioscience.org for reprint permission.
See reprint policy.
The data will be posted online at
https://confluence.cornell.edu/display/llresearch/
HedgingFramingGMOs.
79
