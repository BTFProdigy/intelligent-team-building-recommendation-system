Proceedings of the 12th Conference of the European Chapter of the ACL, pages 558?566,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Evaluating the Inferential Utility of Lexical-Semantic Resources
Shachar Mirkin, Ido Dagan, Eyal Shnarch
Computer Science Department, Bar-Ilan University
Ramat-Gan 52900, Israel
{mirkins,dagan,shey}@cs.biu.ac.il
Abstract
Lexical-semantic resources are used ex-
tensively for applied semantic inference,
yet a clear quantitative picture of their
current utility and limitations is largely
missing. We propose system- and
application-independent evaluation and
analysis methodologies for resources? per-
formance, and systematically apply them
to seven prominent resources. Our find-
ings identify the currently limited recall of
available resources, and indicate the po-
tential to improve performance by exam-
ining non-standard relation types and by
distilling the output of distributional meth-
ods. Further, our results stress the need
to include auxiliary information regarding
the lexical and logical contexts in which
a lexical inference is valid, as well as its
prior validity likelihood.
1 Introduction
Lexical information plays a major role in seman-
tic inference, as the meaning of one term is of-
ten inferred form another. Lexical-semantic re-
sources, which provide the needed knowledge for
lexical inference, are commonly utilized by ap-
plied inference systems (Giampiccolo et al, 2007)
and applications such as Information Retrieval and
Question Answering (Shah and Croft, 2004; Pasca
and Harabagiu, 2001). Beyond WordNet (Fell-
baum, 1998), a wide range of resources has been
developed and utilized, including extensions to
WordNet (Moldovan and Rus, 2001; Snow et al,
2006) and resources based on automatic distri-
butional similarity methods (Lin, 1998; Pantel
and Lin, 2002). Recently, Wikipedia is emerg-
ing as a source for extracting semantic relation-
ships (Suchanek et al, 2007; Kazama and Tori-
sawa, 2007).
As of today, only a partial comparative picture
is available regarding the actual utility and limi-
tations of available resources for lexical-semantic
inference. Works that do provide quantitative
information regarding resources utility have fo-
cused on few particular resources (Kouylekov and
Magnini, 2006; Roth and Sammons, 2007) and
evaluated their impact on a specific system. Most
often, works which utilized lexical resources do
not provide information about their isolated con-
tribution; rather, they only report overall per-
formance for systems in which lexical resources
serve as components.
Our paper provides a step towards clarify-
ing this picture. We propose a system- and
application-independent evaluation methodology
that isolates resources? performance, and sys-
tematically apply it to seven prominent lexical-
semantic resources. The evaluation and analysis
methodology is specified within the Textual En-
tailment framework, which has become popular in
recent years for modeling practical semantic infer-
ence in a generic manner (Dagan and Glickman,
2004). To that end, we assume certain definitions
that extend the textual entailment paradigm to the
lexical level.
The findings of our work provide useful insights
and suggested directions for two research com-
munities: developers of applied inference systems
and researchers addressing lexical acquisition and
resource construction. Beyond the quantitative
mapping of resources? performance, our analysis
points at issues concerning their effective utiliza-
tion and major characteristics. Even more impor-
tantly, the results highlight current gaps in exist-
ing resources and point at directions towards fill-
ing them. We show that the coverage of most
resources is quite limited, where a substantial
part of recall is attributable to semantic relations
that are typically not available to inference sys-
tems. Notably, distributional acquisition methods
558
are shown to provide many useful relationships
which are missing from other resources, but these
are embedded amongst many irrelevant ones. Ad-
ditionally, the results highlight the need to rep-
resent and inference over various aspects of con-
textual information, which affect the applicability
of lexical inferences. We suggest that these gaps
should be addressed by future research.
2 Sub-sentential Textual Entailment
Textual entailment captures the relation between a
text t and a textual statement (termed hypothesis)
h, such that a person reading t would infer that h
is most likely correct (Dagan et al, 2005).
The entailment relation has been defined insofar
in terms of truth values, assuming that h is a com-
plete sentence (proposition). However, there are
major aspects of inference that apply to the sub-
sentential level. First, in certain applications, the
target hypotheses are often sub-sentential. For ex-
ample, search queries in IR, which play the hy-
pothesis role from an entailment perspective, typ-
ically consist of a single term, like drug legaliza-
tion. Such sub-sentential hypotheses are not re-
garded naturally in terms of truth values and there-
fore do not fit well within the scope of the textual
entailment definition. Second, many entailment
models apply a compositional process, through
which they try to infer each sub-part of the hy-
pothesis from some parts of the text (Giampiccolo
et al, 2007).
Although inferences over sub-sentential ele-
ments are being applied in practice, so far there
are no standard definitions for entailment at sub-
sentential levels. To that end, and as a prerequisite
of our evaluation methodology and our analysis,
we first establish two relevant definitions for sub-
sentential entailment relations: (a) entailment of a
sub-sentential hypothesis by a text, and (b) entail-
ment of one lexical element by another.
2.1 Entailment of Sub-sentential Hypotheses
We first seek a definition that would capture the
entailment relationship between a text and a sub-
sentential hypothesis. A similar goal was ad-
dressed in (Glickman et al, 2006), who defined
the notion of lexical reference to model the fact
that in order to entail a hypothesis, the text has
to entail each non-compositional lexical element
within it. We suggest that a slight adaptation of
their definition is suitable to capture the notion of
entailment for any sub-sentential hypotheses, in-
cluding compositional ones:
Definition 1 A sub-sentential hypothesis h is en-
tailed by a text t if there is an explicit or implied
reference in t to a possible meaning of h.
For example, the sentence ?crude steel output
is likely to fall in 2000? entails the sub-sentential
hypotheses production, steel production and steel
output decrease.
Glickman et al, achieving good inter-annotator
agreement, empirically found that almost all non-
compositional terms in an entailed sentential hy-
pothesis are indeed referenced in the entailing text.
This finding suggests that the above definition is
consistent with the original definition of textual
entailment for sentential hypotheses and can thus
model compositional entailment inferences.
We use this definition in our annotation method-
ology described in Section 3.
2.2 Entailment between Lexical Elements
In the majority of cases, the reference to an
?atomic? (non-compositional) lexical element e in
h stems from a particular lexical element e? in t,
as in the example above where the word output
implies the meaning of production.
To identify this relationship, an entailment sys-
tem needs a knowledge resource that would spec-
ify that the meaning of e? implies the meaning of
e, at least in some contexts. We thus suggest the
following definition to capture this relationship be-
tween e? and e:
Definition 2 A lexical element e? entails another
lexical element e, denoted e??e, if there exist
some natural (non-anecdotal) texts containing e?
which entail e, such that the reference to the mean-
ing of e can be implied solely from the meaning of
e? in the text.
(Entailment of e by a text follows Definition 1).
We refer to this relation in this paper as lexical
entailment1, and call e? ? e a lexical entailment
rule. e? is referred to as the rule?s left hand side
(LHS) and e as its right hand side (RHS).
Currently there are no knowledge resources de-
signed specifically for lexical entailment model-
ing. Hence, the types of relationships they cap-
ture do not fully coincide with entailment infer-
ence needs. Thus, the definition suggests a spec-
ification for the rules that should be provided by
1Section 6 discusses other definitions of lexical entailment
559
a lexical entailment resource, following an oper-
ative rationale: a rule e? ? e should be included
in an entailment knowledge resource if it would be
needed, as part of a compositional process, to infer
the meaning of e from some natural texts. Based
on this definition, we perform an analysis of the re-
lationships included in lexical-semantic resources,
as described in Section 5.
A rule need not apply in all contexts, as long
as it is appropriate for some texts. Two contex-
tual aspects affect rule applicability. First is the
?lexical context? specifying the meanings of the
text?s words. A rules is applicable in a certain con-
text only when the intended sense of its LHS term
matches the sense of that term in the text. For ex-
ample, the application of the rule lay ? produce is
valid only in contexts where the producer is poul-
try and the products are eggs. This is a well known
issue observed, for instance, by Voorhees (1994).
A second contextual factor requiring validation
is the ?logical context?. The logical context de-
termines the monotonicity of the LHS and is in-
duced by logical operators such as negation and
(explicit or implicit) quantifiers. For example, the
rule mammal ? whale may not be valid in most
cases, but is applicable in universally quantified
texts like ?mammals are warm-blooded?. This is-
sue has been rarely addressed in applied inference
systems (de Marneffe et al, 2006). The above
mentioned rules both comply with Definition 2
and should therefore be included in a lexical en-
tailment resource.
3 Evaluating Entailment Resources
Our evaluation goal is to assess the utility of
lexical-semantic resources as sources for entail-
ment rules. An inference system applies a rule by
inferring the rule?s RHS from texts that match its
LHS. Thus, the utility of a resource depends on the
performance of its rule applications rather than on
the proportion of correct rules it contains. A rule,
whether correct or incorrect, has insignificant ef-
fect on the resource?s utility if it rarely matches
texts in real application settings. Additionally,
correct rules might produce incorrect applications
when applied in inappropriate contexts. There-
fore, we use an instance-based evaluation method-
ology, which simulates rule applications by col-
lecting texts that contain rules? LHS and manually
assessing the correctness of their applications.
Systems typically handle lexical context either
implicitly or explicitly. Implicit context valida-
tion occurs when the different terms of a compos-
ite hypothesis disambiguate each other. For exam-
ple, the rule waterside ? bank is unlikely to be
applied when trying to infer the hypothesis bank
loans, since texts that match waterside are unlikely
to contain also the meaning of loan. Explicit meth-
ods, such as word-sense disambiguation or sense
matching, validate each rule application according
to the broader context in the text. Few systems
also address logical context validation by handling
quantifiers and negation. As we aim for a system-
independent comparison of resources, and explicit
approaches are not standardized yet within infer-
ence systems, our evaluation uses only implicit
context validation.
3.1 Evaluation Methodology
Figure 1: Evaluation methodology flow chart
The input for our evaluation methodology is a
lexical-semantic resource R, which contains lex-
ical entailment rules. We evaluate R?s utility by
testing how useful it is for inferring a sample of
test hypotheses H from a corpus. Each hypothesis
in H contains more than one lexical element in or-
der to provide implicit context validation for rule
applications, e.g. h: water pollution.
We next describe the steps of our evaluation
methodology, as illustrated in Figure 1. We refer
to the examples in the figure when needed:
1) Fetch rules: For each h ? H and each
lexical element e ? h (e.g. water), we fetch all
rules e? ? e in R that might be applied to entail e
(e.g. lake ? water).
2) Generate intermediate hypotheses h?:
For each rule r: e? ? e, we generate an intermedi-
ate hypothesis h? by replacing e in h with e? (e.g.
560
h?1: lake pollution). From a text t entailing h
?, h
can be further entailed by the single application of
r. We thus simulate the process by which an en-
tailment system would infer h from t using r.
3) Retrieve matching texts: For each h? we
retrieve from a corpus all texts that contain the
lemmatized words of h? (not necessarily as a sin-
gle phrase). These texts may entail h?. We dis-
card texts that also match h since entailing h from
them might not require the application of any rule
from the evaluated resource. In our example, the
retrieved texts contain lake and pollution but do
not contain water.
4) Annotation: A sample of the retrieved texts
is presented to human annotators. The annotators
are asked to answer the following two questions
for each text, simulating the typical inference pro-
cess of an entailment system:
a) Does t entail h?? If t does not entail h?
then the text would not provide a useful example
for the application of r. For instance, t1 (in Fig-
ure 1) does not entail h?1 and thus we cannot de-
duce h from it by applying the rule r. Such texts
are discarded from further evaluation.
b) Does t entail h? If t is annotated as en-
tailing h?, an entailment system would then infer
h from h? by applying r. If h is not entailed from
t even though h? is, the rule application is consid-
ered invalid. For instance, t2 does not entail h even
though it entails h?2. Indeed, the application of r2:
*soil ? water 2, from which h?2 was constructed,
yields incorrect inference. If the answer is ?yes?,
as in the case of t3, the application of r for t is
considered valid.
The above process yields a sample of annotated
rule applications for each test hypothesis, from
which we can measure resources performance, as
described in Section 5.
4 Experimental Setting
4.1 Dataset and Annotation
Current available state-of-the-art lexical-semantic
resources mainly deal with nouns. Therefore, we
used nominal hypotheses for our experiment3.
We chose TREC 1-8 (excluding 4) as our test
corpus and randomly sampled 25 ad-hoc queries
of two-word compounds as our hypotheses. We
did not use longer hypotheses to ensure that
2The asterisk marks an incorrect rule.
3We suggest that the definitions and methodologies can be
applied for other parts of speech as well.
enough texts containing the intermediate hypothe-
ses are found in the corpus. For annotation sim-
plicity, we retrieved single sentences as our texts.
For each rule applied for an hypothesis h, we
sampled 10 sentences from the sentences retrieved
for that rule. As a baseline, we also sampled 10
sentences for each original hypothesis h in which
both words of h are found. In total, 1550 unique
sentences were sampled and annotated by two an-
notators.
To assess the validity of our evaluation method-
ology, the annotators first judged a sample of 220
sentences. The Kappa scores for inter-annotator
agreement were 0.74 and 0.64 for judging h? and
h, respectively. These figures correspond to sub-
stantial agreement (Landis and Koch, 1997) and
are comparable with related semantic annotations
(Szpektor et al, 2007; Bhagat et al, 2007).
4.2 Lexical-Semantic Resources
We evaluated the following resources:
WordNet (WNd): There is no clear agreement
regarding which set of WordNet relations is use-
ful for entailment inference. We therefore took a
conservative approach using only synonymy and
hyponymy rules, which typically comply with the
lexical entailment relation and are commonly used
by textual entailment systems, e.g. (Herrera et al,
2005; Bos and Markert, 2006). Given a term e,
we created a rule e? ? e for each e? amongst the
synonyms or direct hyponyms for all senses of e
in WordNet 3.0.
Snow (Snow30k): Snow et al (2006) pre-
sented a probabilistic model for taxonomy induc-
tion which considers as features paths in parse
trees between related taxonomy nodes. They show
that the best performing taxonomy was the one
adding 30,000 hyponyms to WordNet. We created
an entailment rule for each new hyponym added to
WordNet by their algorithm4.
LCC?s extended WordNet (XWN?): In
(Moldovan and Rus, 2001) WordNet glosses were
transformed into logical form axioms. From this
representation we created a rule e? ? e for each e?
in the gloss which was tagged as referring to the
same entity as e.
CBC: A knowledgebase of labeled clusters gen-
erated by the statistical clustering and labeling al-
gorithms in (Pantel and Lin, 2002; Pantel and
4Available at http://ai.stanford.edu/? rion/swn
561
Ravichandran, 2004)5. Given a cluster label e, an
entailment rule e? ? e is created for each member
e? of the cluster.
Lin Dependency Similarity (Lin-dep): A
distributional word similarity resource based on
syntactic-dependency features (Lin, 1998). Given
a term e and its list of similar terms, we construct
for each e? in the list the rule e? ? e. This resource
was previously used in textual entailment engines,
e.g. (Roth and Sammons, 2007).
Lin Proximity Similarity (Lin-prox): A
knowledgebase of terms with their cooccurrence-
based distributionally similar terms. Rules are cre-
ated from this resource as from the previous one6.
Wikipedia first sentence (WikiFS): Kazama
and Torisawa (2007) used Wikipedia as an exter-
nal knowledge to improve Named Entity Recog-
nition. Using the first step of their algorithm, we
extracted from the first sentence of each page a
noun that appears in a is-a pattern referring to the
title. For each such pair we constructed a rule title
? noun (e.g. Michelle Pfeiffer ? actress).
The above resources represent various meth-
ods for detecting semantic relatedness between
words: Manually and semi-automatically con-
structed (WNd and XWN?, respectively), automat-
ically constructed based on a lexical-syntactic pat-
tern (WikiFS), distributional methods (Lin-dep and
Lin-prox) and combinations of pattern-based and
distributional methods (CBC and Snow30k).
5 Results and Analysis
The results and analysis described in this section
reveal new aspects concerning the utility of re-
sources for lexical entailment, and experimentally
quantify several intuitively-accepted notions re-
garding these resources and the lexical entailment
relation. Overall, our findings highlight where ef-
forts in developing future resources and inference
systems should be invested.
5.1 Resources Performance
Each resource was evaluated using two measures -
Precision and Recall-share, macro averaged over
all hypotheses. The results achieved for each re-
source are summarized in Table 1.
5Kindly provided to us by Patrick Pantel.
6Lin?s resources were downloaded from:
http://www.cs.ualberta.ca/? lindek/demos.htm
Resource Precision (%) Recall-share (%)
Snow30k 56 8
WNd 55 24
XWN? 51 9
WikiFS 45 7
CBC 33 9
Lin-dep 28 45
Lin-prox 24 36
Table 1: Lexical resources performance
5.1.1 Precision
The Precision of a resource R is the percentage of
valid rule applications for the resource. It is esti-
mated by the percentage of texts entailing h from
those that entail h?: countR(entailing h=yes)countR(entailing h?=yes) .
Not surprisingly, resources such as WNd, XWN?
or WikiFS achieved relatively high precision
scores, due to their accurate construction meth-
ods. In contrast, Lin?s distributional resources are
not designed to include lexical entailment relation-
ships. They provide pairs of contextually simi-
lar words, of which many have non-entailing rela-
tionships, such as co-hyponyms7 (e.g. *doctor ?
journalist) or topically-related words, such as *ra-
diotherapy ? outpatient. Hence their relatively
low precision.
One visible outcome is the large gap between
the perceived high accuracy of resources con-
structed by accurate methods, most notably WNd,
and their performance in practice. This finding
emphasizes the need for instance-based evalua-
tions, which capture the ?real? contribution of a
resource. To better understand the reasons for
this gap we further assessed the three factors
that contribute to incorrect applications: incorrect
rules, lexical context and logical context (see Sec-
tion 2.2). This analysis is presented in Table 2.
From Table 2 we see that the gap for accurate
resources is mainly caused by applications of cor-
rect rules in inappropriate contexts. More inter-
estingly, the information in the table allows us to
asses the lexical ?context-sensitivity? of resources.
When considering only the COR-LEX rules to re-
calculate resources precision, we find that Lin-dep
achieves precision of 71% ( 15%15%+6% ), while WN
d
yields only 56% ( 55%55%+44% ). This result indicates
that correct Lin-dep rules are less sensitive to lexi-
cal context, meaning that their prior likelihoods to
7a.k.a. sister terms or coordinate terms
562
(%)
Invalid Rule Applications Valid Rule Applications
INCOR COR-LOG COR-LEX Total INCOR COR-LOG COR-LEX Total (P)
WNd 1 0 44 45 0 0 55 55
WikiFS 13 0 42 55 3 0 42 45
XWN? 19 0 30 49 0 0 51 51
Snow30k 23 0 21 44 0 0 56 56
CBC 51 12 4 67 14 0 19 33
Lin-prox 59 4 13 76 8 3 13 24
Lin-dep 61 5 6 72 9 4 15 28
Table 2: The distribution of invalid and valid rule applications by rule types: incorrect rules (INCOR), correct rules requiring
?logical context? validation (COR-LOG), and correct rules requiring ?lexical context? matching (COR-LEX). The numbers of each
resource?s valid applications add up to the resource?s precision.
be correct are higher. This is explained by the fact
that Lin-dep?s rules are calculated across multiple
contexts and therefore capture the more frequent
usages of words. WordNet, on the other hand, in-
cludes many anecdotal rules whose application is
rare, and thus is very sensitive to context. Simi-
larly, WikiFS turns out to be very context-sensitive.
This resource contains many rules for polysemous
proper nouns that are scarce in their proper noun
sense, e.g. Captive ? computer game. Snow30k,
when applied with the same calculation, reaches
73%, which explains how it achieved a compara-
ble result to WNd, even though it contains many
incorrect rules in comparison to WNd.
5.1.2 Recall
Absolute recall cannot be measured since the total
number of texts in the corpus that entail each hy-
pothesis is unknown. Instead, we measure recall-
share, the contribution of each resource to recall
relative to matching only the words of the origi-
nal hypothesis without any rules. We denote by
yield(h) the number of texts that match h directly
and are annotated as entailing h. This figure is es-
timated by the number of sampled texts annotated
as entailing h multiplied by the sampling propor-
tion. In the same fashion, for each resource R,
we estimate the number of texts entailing h ob-
tained through entailment rules of the resource R,
denoted yieldR(h). Recall-share of R for h is the
proportion of the yield obtained by the resource?s
rules relative to the overall yield with and without
the rules from R: yieldR(h)yield(h)+yieldR(h) .
From Table 1 we see that along with their rela-
tively low precision, Lin?s resources? recall greatly
surpasses that of any other resource, including
WordNet8. The rest of the resources are even infe-
8A preliminary experiment we conducted showed that re-
rior to WNd in that respect, indicating their limited
utility for inference systems.
As expected, synonyms and hyponyms in Word-
Net contributed a noticeable portion to recall in all
resources. Additional correct rules correspond to
hyponyms and synonyms missing from WordNet,
many of them proper names and some slang ex-
pressions. These rules were mainly provided by
WikiFS and Snow30k, significantly supplementing
WordNet, whose HasInstance relation is quite par-
tial. However, there are other interesting types of
entailment relations contributing to recall. These
are discussed in Sections 5.2 and 5.3. Examples
for various rule types are found in Table 3.
5.1.3 Valid Applications of Incorrect Rules
We observed that many entailing sentences were
retrieved by inherently incorrect rules in the distri-
butional resources. Analysis of these rules reveals
they were matched in entailing texts when the LHS
has noticeable statistical correlation with another
term in the text that does entail the RHS. For ex-
ample, for the hypothesis wildlife extinction, the
rule *species ? extinction yielded valid applica-
tions in contexts about threatened or endangered
species. Has the resource included a rule between
the entailing term in the text and the RHS, the
entailing text would have been matched without
needing the incorrect rule.
These correlations accounted for nearly a third
of Lin resources? recall. Nonetheless, in princi-
ple, we suggest that such rules, which do not con-
form with Definition 2, should not be included in a
lexical entailment resource, since they also cause
invalid rule applications, while the entailing texts
they retrieve will hopefully be matched by addi-
call does not dramatically improve when using the entire hy-
ponymy subtree from WordNet.
563
Type Correct Rules
HYPO Shevardnadze ? official Snow30k
ANT efficacy ? ineffectiveness Lin-dep
HOLO government ? official Lin-prox
HYPER arms ? gun Lin-prox
? childbirth ? motherhood Lin-dep
? mortgage ? bank Lin-prox
? Captive ? computer WikiFS
? negligence ? failure CBC
? beatification ? pope XWN?
Type Incorrect Rules
CO-HYP alcohol ? cigarette CBC
? radiotherapy ? outpatient Lin-dep
? teen-ager ? gun Snow30k
? basic ? paper WikiFS
? species ? extinction Lin-prox
Table 3: Examples of lexical resources rules by types.
HYPO: hyponymy, HYPER: hypernymy (class entailment of
its members), HOLO: holonymy, ANT: antonymy, CO-HYP: co-
hyponymy. The non-categorized relations do not correspond
to any WordNet relation.
tional correct rules in a more comprehensive re-
source.
5.2 Non-standard Entailment Relations
An important finding of our analysis is that some
less standard entailment relationships have a con-
siderable impact on recall (see Table 3). These
rules, which comply with Definition 2 but do
not conform to any WordNet relation type, were
mainly contributed by Lin?s distributional re-
sources and to a smaller degree are also included
in XWN?. In Lin-dep, for example, they accounted
for approximately a third of the recall.
Among the finer grained relations we identi-
fied in this set are topical entailment (e.g. IBM
as the company entailing the topic computers),
consequential relationships (pregnancy?mother-
hood) and an entailment of inherent arguments by
a predicate, or of essential participants by a sce-
nario description, e.g. beatification ? pope. A
comprehensive typology of these relationships re-
quires further investigation, as well as the identi-
fication and development of additional resources
from which they can be extracted.
As opposed to hyponymy and synonymy rules,
these rules are typically non-substitutable, i.e. the
RHS of the rule is unlikely to have the exact same
role in the text as the LHS. Many inference sys-
tems perform rule-based transformations, substi-
tuting the LHS by the RHS. This finding suggests
that different methods may be required to utilize
such rules for inference.
5.3 Logical Context
WordNet relations other than synonyms and hy-
ponyms, including antonyms, holonyms and hy-
pernyms (see Table 3), contributed a noticeable
share of valid rule applications for some resources.
Following common practice, these relations are
missing by construction from the other resources.
As shown in Table 2 (COR-LOG columns), such
relations accounted for a seventh of Lin-dep?s
valid rule applications, as much as was the con-
tribution of hyponyms and synonyms to this re-
source?s recall. Yet, using these rules resulted with
more erroneous applications than correct ones. As
discussed in Section 2.2, the rules induced by
these relations do conform with our lexical entail-
ment definition. However, a valid application of
these rules requires certain logical conditions to
occur, which is not the common case. We thus
suggest that such rules are included in lexical en-
tailment resources, as long as they are marked
properly by their types, allowing inference sys-
tems to utilize them only when appropriate mech-
anisms for handling logical context are in place.
5.4 Rules Priors
In Section 5.1.1 we observed that some resources
are highly sensitive to context. Hence, when con-
sidering the validity of a rule?s application, two
factors should be regarded: the actual context in
which the rule is to be applied, as well as the rule?s
prior likelihood to be valid in an arbitrary con-
text. Somewhat indicative, yet mostly indirect, in-
formation about rules? priors is contained in some
resources. This includes sense ranks in WordNet,
SemCor statistics (Miller et al, 1993), and similar-
ity scores and rankings in Lin?s resources. Infer-
ence systems often incorporated this information,
typically as top-k or threshold-based filters (Pan-
tel and Lin, 2003; Roth and Sammons, 2007). By
empirically assessing the effect of several such fil-
ters in our setting, we found that this type of data
is indeed informative in the sense that precision
increases as the threshold rises. Yet, no specific
filters were found to improve results in terms of
F1 score (where recall is measured relatively to
the yield of the unfiltered resource) due to a sig-
nificant drop in relative recall. For example, Lin-
564
prox loses more than 40% of its recall when only
the top-50 rules for each hypothesis are exploited,
and using only the first sense of WNd costs the re-
source over 60% of its recall. We thus suggest a
better strategy might be to combine the prior in-
formation with context matching scores in order
to obtain overall likelihood scores for rule appli-
cations, as in (Szpektor et al, 2008). Furthermore,
resources should include explicit information re-
garding the prior likelihoods of of their rules.
5.5 Operative Conclusions
Our findings highlight the currently limited re-
call of available resources for lexical inference.
The higher recall of Lin?s resources indicates
that many more entailment relationships can be
acquired, particularly when considering distribu-
tional evidence. Yet, available distributional ac-
quisition methods are not geared for lexical entail-
ment. This suggests the need to develop acqui-
sition methods for dedicated and more extensive
knowledge resources that would subsume the cor-
rect rules found by current distributional methods.
Furthermore, substantially better recall may be ob-
tained by acquiring non-standard lexical entail-
ment relationships, as discussed in Section 5.2, for
which a comprehensive typology is still needed.
At the same time, transformation-based inference
systems would need to handle these kinds of rules,
which are usually non-substitutable. Our results
also quantify and stress earlier findings regarding
the severe degradation in precision when rules are
applied in inappropriate contexts. This highlights
the need for resources to provide explicit informa-
tion about the suitable lexical and logical contexts
in which an entailment rule is applicable. In par-
allel, methods should be developed to utilize such
contextual information within inference systems.
Additional auxiliary information needed in lexical
resources is the prior likelihood for a given rule to
be correct in an arbitrary context.
6 Related Work
Several prior works defined lexical entailment.
WordNet?s lexical entailment is a relationship be-
tween verbs only, defined for propositions (Fell-
baum, 1998). Geffet and Dagan (2004) defined
substitutable lexical entailment as a relation be-
tween substitutable terms. We find this definition
too restrictive as non-substitutable rules may also
be useful for entailment inference. Examples are
breastfeeding ? baby and hospital ? medical.
Hence, Definition 2 is more broadly applicable for
defining the desired contents of lexical entailment
resources. We empirically observed that the rules
satisfying their definition are a proper subset of
the rules covered by our definition. Dagan and
Glickman (2004) referred to entailment at the sub-
sentential level by assigning truth values to sub-
propositional text fragments through their existen-
tial meaning. We find this criterion too permissive.
For instance, the existence of country implies the
existence of its flag. Yet, the meaning of flag is
typically not implied by country.
Previous works assessing rule application via
human annotation include (Pantel et al, 2007;
Szpektor et al, 2007), which evaluate acquisition
methods for lexical-syntactic rules. They posed an
additional question to the annotators asking them
to filter out invalid contexts. In our methodology
implicit context matching for the full hypothesis
was applied instead. Other related instance-based
evaluations (Giuliano and Gliozzo, 2007; Connor
and Roth, 2007) performed lexical substitutions,
but did not handle the non-substitutable cases.
7 Conclusions
This paper provides several methodological and
empirical contributions. We presented a novel
evaluation methodology for the utility of lexical-
semantic resources for semantic inference. To that
end we proposed definitions for entailment at sub-
sentential levels, addressing a gap in the textual
entailment framework. Our evaluation and analy-
sis provide a first quantitative comparative assess-
ment of the isolated utility of a range of prominent
potential resources for entailment rules. We have
shown various factors affecting rule applicability
and resources performance, while providing oper-
ative suggestions to address them in future infer-
ence systems and resources.
Acknowledgments
The authors would like to thank Naomi Frankel
and Iddo Greental for their excellent annotation
work, as well as Roy Bar-Haim and Idan Szpektor
for helpful discussion and advice. This work was
partially supported by the Negev Consortium of
the Israeli Ministry of Industry, Trade and Labor,
the PASCAL-2 Network of Excellence of the Eu-
ropean Community FP7-ICT-2007-1-216886 and
the Israel Science Foundation grant 1095/05.
565
References
Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007.
LEDIR: An unsupervised algorithm for learning di-
rectionality of inference rules. In Proceedings of
EMNLP-CoNLL.
J. Bos and K. Markert. 2006. When logical infer-
ence helps determining textual entailment (and when
it doesn?t). In Proceedings of the Second PASCAL
RTE Challenge.
Michael Connor and Dan Roth. 2007. Context sensi-
tive paraphrasing with a global unsupervised classi-
fier. In Proceedings of ECML.
Ido Dagan and Oren Glickman. 2004. Probabilistic
textual entailment: Generic applied modeling of lan-
guage variability. In PASCAL Workshop on Learn-
ing Methods for Text Understanding and Mining.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment
challenge. In Joaquin Quinonero Candela, Ido Da-
gan, Bernardo Magnini, and Florence d?Alche? Buc,
editors, MLCW, Lecture Notes in Computer Science.
Marie-Catherine de Marneffe, Bill MacCartney, Trond
Grenager, Daniel Cer, Anna Rafferty, and Christo-
pher D. Manning. 2006. Learning to distinguish
valid textual entailments. In Proceedings of the Sec-
ond PASCAL RTE Challenge.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and
Communication). The MIT Press.
Maayan Geffet and Ido Dagan. 2004. Feature vector
quality and distributional similarity. In Proceedings
of COLING.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third pascal recogniz-
ing textual entailment challenge. In Proceedings of
ACL-WTEP Workshop.
Claudio Giuliano and Alfio Gliozzo. 2007. Instance
based lexical entailment for ontology population. In
Proceedings of EMNLP-CoNLL.
Oren Glickman, Eyal Shnarch, and Ido Dagan. 2006.
Lexical reference: a semantic matching subtask. In
Proceedings of EMNLP.
Jesu?s Herrera, Anselmo Pen?as, and Felisa Verdejo.
2005. Textual entailment recognition based on de-
pendency analysis and wordnet. In Proceedings of
the First PASCAL RTE Challenge.
Jun?ichi Kazama and Kentaro Torisawa. 2007. Ex-
ploiting Wikipedia as external knowledge for named
entity recognition. In Proceedings of EMNLP-
CoNLL.
Milen Kouylekov and Bernardo Magnini. 2006. Build-
ing a large-scale repository of textual entailment
rules. In Proceedings of LREC.
J. R. Landis and G. G. Koch. 1997. The measurements
of observer agreement for categorical data. In Bio-
metrics, pages 33:159?174.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
Proceedings of HLT.
Dan Moldovan and Vasile Rus. 2001. Logic form
transformation of wordnet and its applicability to
question answering. In Proceedings of ACL.
Patrick Pantel and Dekang Lin. 2002. Discovering
word senses from text. In Proceedings of ACM
SIGKDD.
Patrick Pantel and Dekang Lin. 2003. Automatically
discovering word senses. In Proceedings of NAACL.
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically labeling semantic classes. In Proceedings
of HLT-NAACL.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning inferential selectional preferences. In Pro-
ceedings of HLT.
Marius Pasca and Sanda M. Harabagiu. 2001. The in-
formative role of wordnet in open-domain question
answering. In Proceedings of NAACL Workshop on
WordNet and Other Lexical Resources.
Dan Roth and Mark Sammons. 2007. Semantic and
logical inference model for textual entailment. In
Proceedings of ACL-WTEP Workshop.
Chirag Shah and Bruce W. Croft. 2004. Evaluating
high accuracy retrieval techniques. In Proceedings
of SIGIR.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of COLING-ACL.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A core of semantic knowl-
edge - unifying wordnet and wikipedia. In Proceed-
ings of WWW.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acqui-
sition. In Proceedings of ACL.
Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob
Goldberger. 2008. Contextual preferences. In Pro-
ceedings of ACL.
Ellen M. Voorhees. 1994. Query expansion using
lexical-semantic relations. In Proceedings of SIGIR.
566
Proceedings of NAACL HLT 2009: Short Papers, pages 33?36,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Text Categorization from Category Name via Lexical Reference
Libby Barak
Department of Computer Science
University of Toronto
Toronto, Canada M5S 1A4
libbyb@cs.toronto.edu
Ido Dagan and Eyal Shnarch
Department of Computer Science
Bar-Ilan University
Ramat-Gan 52900, Israel
{dagan, shey}@cs.biu.ac.il
Abstract
Requiring only category names as user input
is a highly attractive, yet hardly explored, set-
ting for text categorization. Earlier bootstrap-
ping results relied on similarity in LSA space,
which captures rather coarse contextual sim-
ilarity. We suggest improving this scheme by
identifying concrete references to the category
name?s meaning, obtaining a special variant of
lexical expansion.
1 Introduction
Topical Text Categorization (TC), the task of clas-
sifying documents by pre-defined topics, is most
commonly addressed as a supervised learning task.
However, the supervised setting requires a substan-
tial amount of manually labeled documents, which
is often impractical in real-life settings.
Keyword-based TC methods (see Section 2) aim
at a more practical setting. Each category is rep-
resented by a list of characteristic keywords, which
should capture the category meaning. Classifica-
tion is then based on measuring similarity between
the category keywords and the classified documents,
typically followed by a bootstrapping step. The
manual effort is thus reduced to providing a key-
word list per category, which was partly automated
in some works through clustering.
The keyword-based approach still requires non-
negligible manual work in creating a representative
keyword list per category. (Gliozzo et al, 2005)
succeeded eliminating this requirement by using the
category name alone as the initial keyword, yet ob-
taining superior performance within the keyword-
based approach. This was achieved by measur-
ing similarity between category names and docu-
ments in Latent Semantic space (LSA), which im-
plicitly captures contextual similarities for the cate-
gory name through unsupervised dimensionality re-
duction. Requiring only category names as user in-
put seems very attractive, particularly when labeled
training data is too costly while modest performance
(relative to supervised methods) is still useful.
The goal of our research is to further improve the
scheme of text categorization from category name,
which was hardly explored in prior work. When an-
alyzing the behavior of the LSA representation of
(Gliozzo et al, 2005) we noticed that it captures
two types of similarities between the category name
and document terms. One type regards words which
refer specifically to the category name?s meaning,
such as pitcher for the category Baseball. How-
ever, typical context words for the category which do
not necessarily imply its specific meaning, like sta-
dium, also come up as similar to baseball in LSA
space. This limits the method?s precision, due to
false-positive classifications of contextually-related
documents that do not discuss the specific category
topic (such as other sports documents wrongly clas-
sified to Baseball). This behavior is quite typical
for query expansion methods, which expand a query
with contextually correlated terms.
We propose a novel scheme that models sepa-
rately these two types of similarity. For one, it
identifies words that are likely to refer specifically
to the category name?s meaning (Glickman et al,
2006), based on certain relations in WordNet and
33
Wikipedia. In tandem, we assess the general contex-
tual fit of the category topic using an LSA model,
to overcome lexical ambiguity and passing refer-
ences. The evaluations show that tracing lexical
references indeed increases classification precision,
which in turn improves the eventual classifier ob-
tained through bootstrapping.
2 Background: Keyword-based Text
Categorization
The majority of keyword-based TC methods fit the
general bootstrapping scheme outlined in Figure 1,
which is cast in terms of a vector-space model. The
simplest version for step 1 is manual generation of
the keyword lists (McCallum and Nigam, 1999).
(Ko and Seo, 2004; Liu et al, 2004) partly auto-
mated this step, using clustering to generate candi-
date keywords. These methods employed a standard
term-space representation in step 2.
As described in Section 1, the keyword list in
(Gliozzo et al, 2005) consisted of the category name
alone. This was accompanied by representing the
category names and documents (step 2) in LSA
space, obtained through cooccurrence-based dimen-
sionality reduction. In this space, words that tend
to cooccur together, or occur in similar contexts, are
represented by similar vectors. Thus, vector similar-
ity in LSA space (in step 3) captures implicitly the
similarity between the category name and contextu-
ally related words within the classified documents.
Step 3 yields an initial similarity-based classifi-
cation that assigns a single (most similar) category
to each document, with Sim(c, d) typically being
the cosine between the corresponding vectors. This
classification is used, in the subsequent bootstrap-
ping step, to train a standard supervised classifier
(either single- or multi-class), yielding the eventual
classifier for the category set.
3 Integrating Reference and Context
Our goal is to augment the coarse contextual simi-
larity measurement in earlier work with the identifi-
cation of concrete references to the category name?s
meaning. We were mostly inspired by (Glickman et
al., 2006), which coined the term lexical reference
to denote concrete references in text to the specific
meaning of a given term. They further showed that
Input: set of categories and unlabeled documents
Output: a classifier
1. Acquire a keyword list per category
2. Represent each category c and document d
as vectors in a common space
3. For each document d
CatSim(d) = argmaxc(Sim(c, d))
4. Train a supervised classifier on step (3) output
Figure 1: Keyword-based categorization scheme
Category name WordNet Wikipedia
Cryptography decipher digital signature
Medicine cardiology biofeedback, homeopathy
Macintosh Apple Mac, Mac
Motorcycle bike, cycle Honda XR600
Table 1: Referring terms from WordNet and Wikipedia
an entailing text (in the textual entailment setting)
typically includes a concrete reference to each term
in the entailed statement. Analogously, we assume
that a relevant document for a category typically in-
cludes concrete terms that refer specifically to the
category name?s meaning.
We thus extend the scheme in Figure 1 by cre-
ating two vectors per category (in steps 1 and 2): a
reference vector ~cref in term space, consisting of re-
ferring terms for the category name; and a context
vector ~ccon, representing the category name in LSA
space, as in (Gliozzo et al, 2005). Step 3 then com-
putes a combined similarity score for categories and
documents based on the two vectors.
3.1 References to category names
Referring terms are collected from WordNet and
Wikipedia, by utilizing relations that are likely to
correspond to lexical reference. Table 1 illustrates
that WordNet provides mostly referring terms of
general terminology while Wikipedia provides more
specific terms. While these resources were used pre-
viously for text categorization, it was mostly for en-
hancing document representation in supervised set-
tings, e.g. (Rodr??guez et al, 2000).
WordNet. Referring terms were found in Word-
Net starting from relevant senses of the category
name and transitively following relation types that
correspond to lexical reference. To that end, we
34
specified for each category name those senses which
fit the category?s meaning, such as the outer space
sense for the category Space.1
A category name sense is first expanded by its
synonyms and derivations, all of which are then ex-
panded by their hyponyms. When a term has no
hyponyms it is expanded by its meronyms instead,
since we observed that in such cases they often spec-
ify unique components that imply the holonym?s
meaning, such as Egypt for Middle East. However,
when a term is not a leaf in the hyponymy hierarchy
then its meronyms often refer to generic sub-parts,
such as door for car. Finally, the hyponyms and
meronyms are expanded by their derivations. As
a common heuristic, we considered only the most
frequent senses (top 4) of referring terms, avoiding
low-ranked (rare) senses which are likely to intro-
duce noise.
Wikipedia. We utilized a subset of a lexical ref-
erence resource extracted from Wikipedia (anony-
mous reference). For each category name we ex-
tracted referring terms of two types, capturing hy-
ponyms and synonyms. Terms of the first type are
Wikipedia page titles for which the first definition
sentence includes a syntactic ?is-a? pattern whose
complement is the category name, such as Chevrolet
for the category Autos. Terms of the second type
are extracted from Wikipedia?s redirect links, which
capture synonyms such as x11 for Windows-X.
The reference vector ~cref for a category consists
of the category name and all its referring terms,
equally weighted. The corresponding similarity
function is Simref (c, d) = cos(~cref , ~dterm)), where
~dterm is the document vector in term space.
3.2 Incorporating context similarity
Our key motivation is to utilize Simref as the ba-
sis for classification in step 3 (Figure 1). However,
this may yield false positive classifications in two
cases: (a) inappropriate sense of an ambiguous re-
ferring term, e.g., the narcotic sense of drug should
not yield classification to Medicine; (b) a passing
reference, e.g., an analogy to cars in a software doc-
ument, should not yield classification to Autos.
1We assume that it is reasonable to specify relevant senses
as part of the typically manual process of defining the set of
categories and their names. Otherwise, when expanding names
through all their senses F1-score dropped by about 2%.
In both these cases the overall context in the docu-
ment is expected to be atypical for the triggered cat-
egory. We therefore measure the contextual similar-
ity between a category c and a document d utilizing
LSA space, replicating the method in (Gliozzo et
al., 2005): ~ccon and ~dLSA are taken as the LSA vec-
tors of the category name and the document, respec-
tively, yielding Simcon(c, d) = cos(~ccon, ~dLSA)).2
The overall similarity score of step 3 is de-
fined as Sim(c, d) = Simref (c, d) ? Simcon(c, d).
This formula fulfils the requirement of finding at
least one referring term in the document; otherwise
Simref (c, d) would be zero. Simcon(c, d) is com-
puted in the reduced LSA space and is thus prac-
tically non-zero, and would downgrade Sim(c, d)
when there is low contextual similarity between the
category name and the document. Documents for
which Sim(c, d) = 0 for all categories are omitted.
4 Results and Conclusions
We tested our method on the two corpora used in
(Gliozzo et al, 2005): 20-NewsGroups, classified
by a single-class scheme (single category per doc-
ument), and Reuters-10 3, of a multi-class scheme.
As in their work, non-standard category names were
adjusted, such as Foreign exchange for Money-fx.
4.1 Initial classification
Table 2 presents the results of the initial classifica-
tion (step 3). The first 4 lines refer to classification
based on Simref alone. As a baseline, including
only the category name in the reference vector (Cat-
Name) yields particularly low recall. Expansion by
WordNet is notably more powerful than by the auto-
matically extracted Wikipedia resource; still, the lat-
ter consistently provides a small marginal improve-
ment when using both resources (Reference), indi-
cating their complementary nature.
As we hypothesized, the Reference model
achieves much better precision than the Context
model from (Gliozzo et al, 2005) alone (Simcon).
For 20-NewsGroups the recall of Reference is lim-
ited, due to partial coverage of our current expansion
2The original method includes a Gaussian Mixture re-
scaling step for Simcon, which wasn?t found helpful when
combined with Simref (as specified next).
310 most frequent categories in Reuters-21578
35
Reuters-10 20 Newsgroups
Method R P F1 R P F1
CatName 0.22 0.67 0.33 0.19 0.55 0.28
WordNet 0.67 0.78 0.72 0.29 0.56 0.38
Wikipedia 0.24 0.68 0.35 0.22 0.57 0.31
Reference 0.69 0.80 0.74 0.31 0.57 0.40
Context 0.59 0.64 0.61 0.46 0.46 0.46
Combined 0.71 0.82 0.76 0.32 0.58 0.41
Table 2: Initial categorization results (step 3)
Method Feature Reuters-10 20 NGSet R P F1 F1
Reference TF-IDF 0.91 0.50 0.65 0.51LSA 0.89 0.67 0.76 0.56
Context TF-IDF 0.84 0.48 0.61 0.48LSA 0.73 0.56 0.63 0.44
Combined TF-IDF 0.92 0.50 0.65 0.52LSA 0.89 0.71 0.79 0.56
Table 3: Final bootstrapping results (step 4)
resources, yielding a lower F1. Yet, its higher pre-
cision pays off for the bootstrapping step (Section
4.2). Finally, when the two models are Combined a
small precision improvement is observed.
4.2 Final bootstrapping results
The output of step 3 was fed as standard training
for a binary SVM classifier for each category (step
4). We used the default setting for SVM-light, apart
from the j parameter which was set to the number of
categories in each data set, as suggested by (Morik
et al, 1999). For Reuters-10, classification was
determined independently by the classifier of each
category, allowing multiple classes per document.
For 20-NewsGroups, the category which yielded the
highest classification score was chosen (one-versus-
all), fitting the single-class setting. We experimented
with two document representations for the super-
vised step: either as vectors in tf-idf weighted term
space or as vectors in LSA space.
Table 3 shows the final classification results.4
First, we observe that for the noisy bootstrapping
training data LSA document representation is usu-
ally preferred. Most importantly, our Reference and
Combined models clearly improve over the earlier
4Notice that P=R=F1 when all documents are classified to
a single class, as in step 4 for 20-NewsGroups, while in step 3
some documents are not classified, yielding distinct P/R/F1.
Context. Combining reference and context yields
some improvement for Reuters-10, but not for 20-
NewsGroups. We noticed though that the actual ac-
curacy of our method on 20-NewsGroups is notably
higher than measured relative to the gold standard,
due to its single-class scheme: in many cases, a doc-
ument should truly belong to more than one cate-
gory while that chosen by our algorithm was counted
as false positive. Future research is proposed to in-
crease the method?s recall via broader coverage lexi-
cal reference resources, and to improve its precision
through better context models than LSA, which was
found rather noisy for quite a few categories.
To conclude, the results support our main contri-
bution ? the benefit of identifying referring terms for
the category name over using noisier context mod-
els alone. Overall, our work highlights the potential
of text categorization from category names when la-
beled training sets are not available, and indicates
important directions for further research.
Acknowledgments
The authors would like to thank Carlo Strapparava
and Alfio Gliozzo for valuable discussions. This
work was partially supported by the NEGEV project
(www.negev-initiative.org).
References
O. Glickman, E. Shnarch, and I. Dagan. 2006. Lexical
reference: a semantic matching subtask. In EMNLP.
A. Gliozzo, C. Strapparava, and I. Dagan. 2005. Inves-
tigating unsupervised learning for text categorization
bootstrapping. In Proc. of HLT/EMNLP.
Y. Ko and J. Seo. 2004. Learning with unlabeled data
for text categorization using bootstrapping and feature
projection techniques. In Proc. of ACL.
B. Liu, X. Li, W. S. Lee, and P. S. Yu. 2004. Text classi-
fication by labeling words. In Proc. of AAAI.
A. McCallum and K. Nigam. 1999. Text classification
by bootstrapping with keywords, EM and shrinkage.
In ACL Workshop for Unsupervised Learning in NLP.
K. Morik, P. Brockhausen, and T. Joachims. 1999. Com-
bining statistical learning with a knowledge-based ap-
proach - a case study in intensive care monitoring. In
Proc. of the 16th Int?l Conf. on Machine Learning.
M. d. B. Rodr??guez, J. M. Go?mez-Hidalgo, and B. D??az-
Agudo, 2000. Using WordNet to complement training
information in text categorization, volume 189 of Cur-
rent Issues in Linguistic Theory, pages 353?364.
36
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 456?463,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Instance-based Evaluation of Entailment Rule Acquisition
Idan Szpektor, Eyal Shnarch, Ido Dagan
Dept. of Computer Science
Bar Ilan University
Ramat Gan, Israel
{szpekti,shey,dagan}@cs.biu.ac.il
Abstract
Obtaining large volumes of inference knowl-
edge, such as entailment rules, has become
a major factor in achieving robust seman-
tic processing. While there has been sub-
stantial research on learning algorithms for
such knowledge, their evaluation method-
ology has been problematic, hindering fur-
ther research. We propose a novel evalua-
tion methodology for entailment rules which
explicitly addresses their semantic proper-
ties and yields satisfactory human agreement
levels. The methodology is used to compare
two state of the art learning algorithms, ex-
posing critical issues for future progress.
1 Introduction
In many NLP applications, such as Question An-
swering (QA) and Information Extraction (IE), it is
crucial to recognize that a particular target mean-
ing can be inferred from different text variants. For
example, a QA system needs to identify that ?As-
pirin lowers the risk of heart attacks? can be inferred
from ?Aspirin prevents heart attacks? in order to an-
swer the question ?What lowers the risk of heart at-
tacks??. This type of reasoning has been recognized
as a core semantic inference task by the generic tex-
tual entailment framework (Dagan et al, 2006).
A major obstacle for further progress in seman-
tic inference is the lack of broad-scale knowledge-
bases for semantic variability patterns (Bar-Haim et
al., 2006). One prominent type of inference knowl-
edge representation is inference rules such as para-
phrases and entailment rules. We define an entail-
ment rule to be a directional relation between two
templates, text patterns with variables, e.g. ?X pre-
vent Y ? X lower the risk of Y ?. The left-hand-
side template is assumed to entail the right-hand-
side template in certain contexts, under the same
variable instantiation. Paraphrases can be viewed
as bidirectional entailment rules. Such rules capture
basic inferences and are used as building blocks for
more complex entailment inference. For example,
given the above rule, the answer ?Aspirin? can be
identified in the example above.
The need for large-scale inference knowledge-
bases triggered extensive research on automatic ac-
quisition of paraphrase and entailment rules. Yet the
current precision of acquisition algorithms is typ-
ically still mediocre, as illustrated in Table 1 for
DIRT (Lin and Pantel, 2001) and TEASE (Szpek-
tor et al, 2004), two prominent acquisition algo-
rithms whose outputs are publicly available. The
current performance level only stresses the obvious
need for satisfactory evaluation methodologies that
would drive future research.
The prominent approach in the literature for eval-
uating rules, termed here the rule-based approach, is
to present the rules to human judges asking whether
each rule is correct or not. However, it is difficult to
explicitly define when a learned rule should be con-
sidered correct under this methodology, and this was
mainly left undefined in previous works. As the cri-
terion for evaluating a rule is not well defined, using
this approach often caused low agreement between
human judges. Indeed, the standards for evaluation
in this field are lower than other fields: many papers
456
don?t report on human agreement at all and those
that do report rather low agreement levels. Yet it
is crucial to reliably assess rule correctness in or-
der to measure and compare the performance of dif-
ferent algorithms in a replicable manner. Lacking a
good evaluation methodology has become a barrier
for further advances in the field.
In order to provide a well-defined evaluation
methodology we first explicitly specify when entail-
ment rules should be considered correct, following
the spirit of their usage in applications. We then
propose a new instance-based evaluation approach.
Under this scheme, judges are not presented only
with the rule but rather with a sample of sentences
that match its left hand side. The judges then assess
whether the rule holds under each specific example.
A rule is considered correct only if the percentage of
examples assessed as correct is sufficiently high.
We have experimented with a sample of input
verbs for both DIRT and TEASE. Our results show
significant improvement in human agreement over
the rule-based approach. It is also the first compar-
ison between such two state-of-the-art algorithms,
which showed that they are comparable in precision
but largely complementary in their coverage.
Additionally, the evaluation showed that both al-
gorithms learn mostly one-directional rules rather
than (symmetric) paraphrases. While most NLP ap-
plications need directional inference, previous ac-
quisition works typically expected that the learned
rules would be paraphrases. Under such an expec-
tation, unidirectional rules were assessed as incor-
rect, underestimating the true potential of these algo-
rithms. In addition, we observed that many learned
rules are context sensitive, stressing the need to learn
contextual constraints for rule applications.
2 Background: Entailment Rules and their
Evaluation
2.1 Entailment Rules
An entailment rule ?L ? R? is a directional rela-
tion between two templates, L and R. For exam-
ple, ?X acquire Y ? X own Y ? or ?X beat Y ?
X play against Y ?. Templates correspond to text
fragments with variables, and are typically either lin-
ear phrases or parse sub-trees.
The goal of entailment rules is to help applica-
Input Correct Incorrect
(?) X modify Y X adopt Y
X change Y (?) X amend Y X create Y
(DIRT) (?) X revise Y X stick to Y
(?) X alter Y X maintain Y
X change Y (?) X affect Y X follow Y
(TEASE) (?) X extend Y X use Y
Table 1: Examples of templates suggested by DIRT
and TEASE as having an entailment relation, in
some direction, with the input template ?X change
Y ?. The entailment direction arrows were judged
manually and added for readability.
tions infer one text variant from another. A rule can
be applied to a given text only when L can be in-
ferred from it, with appropriate variable instantia-
tion. Then, using the rule, the application deduces
that R can also be inferred from the text under the
same variable instantiation. For example, the rule
?X lose to Y ?Y beat X? can be used to infer ?Liv-
erpool beat Chelsea? from ?Chelsea lost to Liver-
pool in the semifinals?.
Entailment rules should typically be applied only
in specific contexts, which we term relevant con-
texts. For example, the rule ?X acquire Y ?
X buy Y ? can be used in the context of ?buying?
events. However, it shouldn?t be applied for ?Stu-
dents acquired a new language?. In the same man-
ner, the rule ?X acquire Y ?X learn Y ? should be
applied only when Y corresponds to some sort of
knowledge, as in the latter example.
Some existing entailment acquisition algorithms
can add contextual constraints to the learned rules
(Sekine, 2005), but most don?t. However, NLP ap-
plications usually implicitly incorporate some con-
textual constraints when applying a rule. For ex-
ample, when answering the question ?Which com-
panies did IBM buy?? a QA system would apply
the rule ?X acquire Y ?X buy Y ? correctly, since
the phrase ?IBM acquire X? is likely to be found
mostly in relevant economic contexts. We thus ex-
pect that an evaluation methodology should consider
context relevance for entailment rules. For example,
we would like both ?X acquire Y ?X buy Y ? and
?X acquire Y ?X learn Y ? to be assessed as cor-
rect (the second rule should not be deemed incorrect
457
just because it is not applicable in frequent economic
contexts).
Finally, we highlight that the common notion of
?paraphrase rules? can be viewed as a special case
of entailment rules: a paraphrase ?L? R? holds if
both templates entail each other. Following the tex-
tual entailment formulation, we observe that many
applied inference settings require only directional
entailment, and a requirement for symmetric para-
phrase is usually unnecessary. For example, in or-
der to answer the question ?Who owns Overture??
it suffices to use a directional entailment rule whose
right hand side is ?X own Y ?, such as ?X acquire
Y ?X own Y ?, which is clearly not a paraphrase.
2.2 Evaluation of Acquisition Algorithms
Many methods for automatic acquisition of rules
have been suggested in recent years, ranging from
distributional similarity to finding shared contexts
(Lin and Pantel, 2001; Ravichandran and Hovy,
2002; Shinyama et al, 2002; Barzilay and Lee,
2003; Szpektor et al, 2004; Sekine, 2005). How-
ever, there is still no common accepted framework
for their evaluation. Furthermore, all these methods
learn rules as pairs of templates {L,R} in a sym-
metric manner, without addressing rule directional-
ity. Accordingly, previous works (except (Szpektor
et al, 2004)) evaluated the learned rules under the
paraphrase criterion, which underestimates the prac-
tical utility of the learned rules (see Section 2.1).
One approach which was used for evaluating au-
tomatically acquired rules is to measure their contri-
bution to the performance of specific systems, such
as QA (Ravichandran and Hovy, 2002) or IE (Sudo
et al, 2003; Romano et al, 2006). While measuring
the impact of learned rules on applications is highly
important, it cannot serve as the primary approach
for evaluating acquisition algorithms for several rea-
sons. First, developers of acquisition algorithms of-
ten do not have access to the different applications
that will later use the learned rules as generic mod-
ules. Second, the learned rules may affect individual
systems differently, thus making observations that
are based on different systems incomparable. Third,
within a complex system it is difficult to assess the
exact quality of entailment rules independently of
effects of other system components.
Thus, as in many other NLP learning settings,
a direct evaluation is needed. Indeed, the promi-
nent approach for evaluating the quality of rule ac-
quisition algorithms is by human judgment of the
learned rules (Lin and Pantel, 2001; Shinyama et
al., 2002; Barzilay and Lee, 2003; Pang et al, 2003;
Szpektor et al, 2004; Sekine, 2005). In this evalua-
tion scheme, termed here the rule-based approach, a
sample of the learned rules is presented to the judges
who evaluate whether each rule is correct or not. The
criterion for correctness is not explicitly described in
most previous works. By the common view of con-
text relevance for rules (see Section 2.1), a rule was
considered correct if the judge could think of rea-
sonable contexts under which it holds.
We have replicated the rule-based methodology
but did not manage to reach a 0.6 Kappa agree-
ment level between pairs of judges. This approach
turns out to be problematic because the rule correct-
ness criterion is not sufficiently well defined and is
hard to apply. While some rules might obviously
be judged as correct or incorrect (see Table 1), judg-
ment is often more difficult due to context relevance.
One judge might come up with a certain context
that, to her opinion, justifies the rule, while another
judge might not imagine that context or think that
it doesn?t sufficiently support rule correctness. For
example, in our experiments one of the judges did
not identify the valid ?religious holidays? context
for the correct rule ?X observe Y ?X celebrate Y ?.
Indeed, only few earlier works reported inter-judge
agreement level, and those that did reported rather
low Kappa values, such as 0.54 (Barzilay and Lee,
2003) and 0.55 - 0.63 (Szpektor et al, 2004).
To conclude, the prominent rule-based methodol-
ogy for entailment rule evaluation is not sufficiently
well defined. It results in low inter-judge agreement
which prevents reliable and consistent assessments
of different algorithms.
3 Instance-based Evaluation Methodology
As discussed in Section 2.1, an evaluation methodol-
ogy for entailment rules should reflect the expected
validity of their application within NLP systems.
Following that line, an entailment rule ?L ? R?
should be regarded as correct if in all (or at least
most) relevant contexts in which the instantiated
template L is inferred from the given text, the instan-
458
Rule Sentence Judgment
1 X seek Y ?X disclose Y If he is arrested, he can immediately seek bail. Left not entailed
2 X clarify Y ?X prepare Y He didn?t clarify his position on the subject. Left not entailed
3 X hit Y ?X approach Y Other earthquakes have hit Lebanon since ?82. Irrelevant context
4 X lose Y ?X surrender Y Bread has recently lost its subsidy. Irrelevant context
5 X regulate Y ?X reform Y The SRA regulates the sale of sugar. No entailment
6 X resign Y ?X share Y Lopez resigned his post at VW last week. No entailment
7 X set Y ?X allow Y The committee set the following refunds. Entailment holds
8 X stress Y ?X state Y Ben Yahia also stressed the need for action. Entailment holds
Table 2: Rule evaluation examples and their judgment.
tiated template R is also inferred from the text. This
reasoning corresponds to the common definition of
entailment in semantics, which specifies that a text
L entails another text R if R is true in every circum-
stance (possible world) in which L is true (Chierchia
and McConnell-Ginet, 2000).
It follows that in order to assess if a rule is cor-
rect we should judge whether R is typically en-
tailed from those sentences that entail L (within rel-
evant contexts for the rule). We thus present a new
evaluation scheme for entailment rules, termed the
instance-based approach. At the heart of this ap-
proach, human judges are presented not only with
a rule but rather with a sample of examples of the
rule?s usage. Instead of thinking up valid contexts
for the rule the judges need to assess the rule?s va-
lidity under the given context in each example. The
essence of our proposal is a (apparently non-trivial)
protocol of a sequence of questions, which deter-
mines rule validity in a given sentence.
We shall next describe how we collect a sample of
examples for evaluation and the evaluation process.
3.1 Sampling Examples
Given a rule ?L?R?, our goal is to generate evalua-
tion examples by finding a sample of sentences from
which L is entailed. We do that by automatically re-
trieving, from a given corpus, sentences that match
L and are thus likely to entail it, as explained below.
For each example sentence, we automatically ex-
tract the arguments that instantiate L and generate
two phrases, termed left phrase and right phrase,
which are constructed by instantiating the left tem-
plate L and the right template R with the extracted
arguments. For example, the left and right phrases
generated for example 1 in Table 2 are ?he seek bail?
and ?he disclose bail?, respectively.
Finding sentences that match L can be performed
at different levels. In this paper we match lexical-
syntactic templates by finding a sub-tree of the sen-
tence parse that is identical to the template structure.
Of course, this matching method is not perfect and
will sometimes retrieve sentences that do not entail
the left phrase for various reasons, such as incorrect
sentence analysis or semantic aspects like negation,
modality and conditionals. See examples 1-2 in Ta-
ble 2 for sentences that syntactically match L but
do not entail the instantiated left phrase. Since we
should assess R?s entailment only from sentences
that entail L, such sentences should be ignored by
the evaluation process.
3.2 Judgment Questions
For each example generated for a rule, the judges are
presented with the given sentence and the left and
right phrases. They primarily answer two questions
that assess whether entailment holds in this example,
following the semantics of entailment rule applica-
tion as discussed above:
Qle: Is the left phrase entailed from the sentence?
A positive/negative answer corresponds to a
?Left entailed/not entailed? judgment.
Qre: Is the right phrase entailed from the sentence?
A positive/negative answer corresponds to an
?Entailment holds/No entailment? judgment.
The first question identifies sentences that do not en-
tail the left phrase, and thus should be ignored when
evaluating the rule?s correctness. While inappropri-
ate matches of the rule left-hand-side may happen
459
and harm an overall system precision, such errors
should be accounted for a system?s rule matching
module rather than for the rules? precision. The sec-
ond question assesses whether the rule application is
valid or not for the current example. See examples
5-8 in Table 2 for cases where entailment does or
doesn?t hold.
Thus, the judges focus only on the given sentence
in each example, so the task is actually to evaluate
whether textual entailment holds between the sen-
tence (text) and each of the left and right phrases
(hypotheses). Following past experience in textual
entailment evaluation (Dagan et al, 2006) we expect
a reasonable agreement level between judges.
As discussed in Section 2.1, we may want to ig-
nore examples whose context is irrelevant for the
rule. To optionally capture this distinction, the
judges are asked another question:
Qrc: Is the right phrase a likely phrase in English?
A positive/negative answer corresponds to a
?Relevant/Irrelevant context? evaluation.
If the right phrase is not likely in English then the
given context is probably irrelevant for the rule, be-
cause it seems inherently incorrect to infer an im-
plausible phrase. Examples 3-4 in Table 2 demon-
strate cases of irrelevant contexts, which we may
choose to ignore when assessing rule correctness.
3.3 Evaluation Process
For each example, the judges are presented with the
three questions above in the following order: (1) Qle
(2) Qrc (3) Qre. If the answer to a certain question
is negative then we do not need to present the next
questions to the judge: if the left phrase is not en-
tailed then we ignore the sentence altogether; and if
the context is irrelevant then the right phrase cannot
be entailed from the sentence and so the answer to
Qre is already known as negative.
The above entailment judgments assume that we
can actually ask whether the left or right phrases
are correct given the sentence, that is, we assume
that a truth value can be assigned to both phrases.
This is the case when the left and right templates
correspond, as expected, to semantic relations. Yet
sometimes learned templates are (erroneously) not
relational, e.g. ?X , Y , IBM? (representing a list).
We therefore let the judges initially mark rules that
include such templates as non-relational, in which
case their examples are not evaluated at all.
3.4 Rule Precision
We compute the precision of a rule by the percent-
age of examples for which entailment holds out
of all ?relevant? examples. We can calculate the
precision in two ways, as defined below, depending
on whether we ignore irrelevant contexts or not
(obtaining lower precision if we don?t). When
systems answer an information need, such as a
query or question, irrelevant contexts are sometimes
not encountered thanks to additional context which
is present in the given input (see Section 2.1). Thus,
the following two measures can be viewed as upper
and lower bounds for the expected precision of the
rule applications in actual systems:
upper bound precision: #Entailment holds#Relevant context
lower bound precision: #Entailment holds#Left entailed
where # denotes the number of examples with
the corresponding judgment.
Finally, we consider a rule to be correct only if
its precision is at least 80%, which seems sensible
for typical applied settings. This yields two alterna-
tive sets of correct rules, corresponding to the upper
bound and lower bound precision measures. Even
though judges may disagree on specific examples for
a rule, their judgments may still agree overall on the
rule?s correctness. We therefore expect the agree-
ment level on rule correctness to be higher than the
agreement on individual examples.
4 Experimental Settings
We applied the instance-based methodology to eval-
uate two state-of-the-art unsupervised acquisition al-
gorithms, DIRT (Lin and Pantel, 2001) and TEASE
(Szpektor et al, 2004), whose output is publicly
available. DIRT identifies semantically related tem-
plates in a local corpus using distributional sim-
ilarity over the templates? variable instantiations.
TEASE acquires entailment relations from the Web
for a given input template I by identifying charac-
teristic variable instantiations shared by I and other
templates.
460
For the experiment we used the published DIRT
and TEASE knowledge-bases1. For every given in-
put template I , each knowledge-base provides a list
of learned output templates {Oj}nI1 , where nI is the
number of output templates learned for I . Each out-
put template is suggested as holding an entailment
relation with the input template I , but the algorithms
do not specify the entailment direction(s). Thus,
each pair {I,Oj} induces two candidate directional
entailment rules: ?I?Oj? and ?Oj?I?.
4.1 Test Set Construction
The test set construction consists of three sampling
steps: selecting a set of input templates for the two
algorithms, selecting a sample of output rules to be
evaluated, and selecting a sample of sentences to be
judged for each rule.
First, we randomly selected 30 transitive verbs
out of the 1000 most frequent verbs in the Reuters
RCV1 corpus2. For each verb we manually
constructed a lexical-syntactic input template by
adding subject and object variables. For exam-
ple, for the verb ?seek? we constructed the template
?X subj??? seek obj??? Y ?.
Next, for each input template I we considered
the learned templates {Oj}nI1 from each knowledge-
base. Since DIRT has a long tail of templates with
a low score and very low precision, DIRT templates
whose score is below a threshold of 0.1 were filtered
out3. We then sampled 10% of the templates in each
output list, limiting the sample size to be between
5-20 templates for each list (thus balancing between
sufficient evaluation data and judgment load). For
each sampled template O we evaluated both direc-
tional rules, ?I?O? and ?O?I?. In total, we sam-
pled 380 templates, inducing 760 directional rules
out of which 754 rules were unique.
Last, we randomly extracted a sample of example
sentences for each rule ?L?R? by utilizing a search
engine over the first CD of Reuters RCV1. First, we
retrieved all sentences containing all lexical terms
within L. The retrieved sentences were parsed using
the Minipar dependency parser (Lin, 1998), keep-
ing only sentences that syntactically match L (as
1Available at http://aclweb.org/aclwiki/index.php?title=Te-
xtual Entailment Resource Pool
2http://about.reuters.com/researchandstandards/corpus/
3Following advice by Patrick Pantel, DIRT?s co-author.
explained in Section 3.1). A sample of 15 match-
ing sentences was randomly selected, or all match-
ing sentences if less than 15 were found. Finally,
an example for judgment was generated from each
sampled sentence and its left and right phrases (see
Section 3.1). We did not find sentences for 108
rules, and thus we ended up with 646 unique rules
that could be evaluated (with 8945 examples to be
judged).
4.2 Evaluating the Test-Set
Two human judges evaluated the examples. We
randomly split the examples between the judges.
100 rules (1287 examples) were cross annotated for
agreement measurement. The judges followed the
procedure in Section 3.3 and the correctness of each
rule was assessed based on both its upper and lower
bound precision values (Section 3.4).
5 Methodology Evaluation Results
We assessed the instance-based methodology by
measuring the agreement level between judges. The
judges agreed on 75% of the 1287 shared exam-
ples, corresponding to a reasonable Kappa value of
0.64. A similar kappa value of 0.65 was obtained
for the examples that were judged as either entail-
ment holds/no entailment by both judges. Yet, our
evaluation target is to assess rules, and the Kappa
values for the final correctness judgments of the
shared rules were 0.74 and 0.68 for the lower and
upper bound evaluations. These Kappa scores are
regarded as ?substantial agreement? and are substan-
tially higher than published agreement scores and
those we managed to obtain using the standard rule-
based approach. As expected, the agreement on
rules is higher than on examples, since judges may
disagree on a certain example but their judgements
would still yield the same rule assessment.
Table 3 illustrates some disagreements that were
still exhibited within the instance-based evaluation.
The primary reason for disagreements was the dif-
ficulty to decide whether a context is relevant for
a rule or not, resulting in some confusion between
?Irrelevant context? and ?No entailment?. This may
explain the lower agreement for the upper bound
precision, for which examples judged as ?Irrelevant
context? are ignored, while for the lower bound both
461
Rule Sentence Judge 1 Judge 2
X sign Y ?X set Y Iraq and Turkey sign agreement
to increase trade cooperation
Entailment holds Irrelevant context
X worsen Y ?X slow Y News of the strike worsened the
situation
Irrelevant context No entailment
X get Y ?X want Y He will get his parade on Tuesday Entailment holds No entailment
Table 3: Examples for disagreement between the two judges.
judgments are conflated and represent no entailment.
Our findings suggest that better ways for distin-
guishing relevant contexts may be sought in future
research for further refinement of the instance-based
evaluation methodology.
About 43% of all examples were judged as ?Left
not entailed?. The relatively low matching precision
(57%) made us collect more examples than needed,
since ?Left not entailed? examples are ignored. Bet-
ter matching capabilities will allow collecting and
judging fewer examples, thus improving the effi-
ciency of the evaluation process.
6 DIRT and TEASE Evaluation Results
DIRT TEASE
P Y P Y
Rules:
Upper Bound 30.5% 33.5 28.4% 40.3
Lower Bound 18.6% 20.4 17% 24.1
Templates:
Upper Bound 44% 22.6 38% 26.9
Lower Bound 27.3% 14.1 23.6% 16.8
Table 4: Average Precision (P) and Yield (Y) at the
rule and template levels.
We evaluated the quality of the entailment rules
produced by each algorithm using two scores: (1)
micro average Precision, the percentage of correct
rules out of all learned rules, and (2) average Yield,
the average number of correct rules learned for each
input template I , as extrapolated based on the sam-
ple4. Since DIRT and TEASE do not identify rule
directionality, we also measured these scores at the
4Since the rules are matched against the full corpus (as in IR
evaluations), it is difficult to evaluate their true recall.
template level, where an output template O is con-
sidered correct if at least one of the rules ?I?O? or
?O? I? is correct. The results are presented in Ta-
ble 4. The major finding is that the overall quality of
DIRT and TEASE is very similar. Under the specific
DIRT cutoff threshold chosen, DIRT exhibits some-
what higher Precision while TEASE has somewhat
higher Yield (recall that there is no particular natural
cutoff point for DIRT?s output).
Since applications typically apply rules in a spe-
cific direction, the Precision for rules reflects their
expected performance better than the Precision for
templates. Obviously, future improvement in pre-
cision is needed for rule learning algorithms. Mean-
while, manual filtering of the learned rules can prove
effective within limited domains, where our evalua-
tion approach can be utilized for reliable filtering as
well. The substantial yield obtained by these algo-
rithms suggest that they are indeed likely to be valu-
able for recall increase in semantic applications.
In addition, we found that only about 15% of the
correct templates were learned by both algorithms,
which implies that the two algorithms largely com-
plement each other in terms of coverage. One ex-
planation may be that DIRT is focused on the do-
main of the local corpus used (news articles for the
published DIRT knowledge-base), whereas TEASE
learns from the Web, extracting rules from multiple
domains. Since Precision is comparable it may be
best to use both algorithms in tandem.
We also measured whether O is a paraphrase of
I , i.e. whether both ?I ?O? and ?O? I? are cor-
rect. Only 20-25% of all correct templates were as-
sessed as paraphrases. This stresses the significance
of evaluating directional rules rather than only para-
phrases. Furthermore, it shows that in order to im-
prove precision, acquisition algorithms must iden-
tify rule directionality.
462
About 28% of all ?Left entailed? examples were
evaluated as ?Irrelevant context?, yielding the large
difference in precision between the upper and lower
precision bounds. This result shows that in order
to get closer to the upper bound precision, learning
algorithms and applications need to identify the rel-
evant contexts in which a rule should be applied.
Last, we note that the instance-based quality as-
sessment corresponds to the corpus from which the
example sentences were taken. It is therefore best to
evaluate the rules using a corpus of the same domain
from which they were learned, or the target applica-
tion domain for which the rules will be applied.
7 Conclusions
Accurate learning of inference knowledge, such as
entailment rules, has become critical for further
progress of applied semantic systems. However,
evaluation of such knowledge has been problematic,
hindering further developments. The instance-based
evaluation approach proposed in this paper obtained
acceptable agreement levels, which are substantially
higher than those obtained for the common rule-
based approach.
We also conducted the first comparison between
two state-of-the-art acquisition algorithms, DIRT
and TEASE, using the new methodology. We found
that their quality is comparable but they effectively
complement each other in terms of rule coverage.
Also, we found that most learned rules are not para-
phrases but rather one-directional entailment rules,
and that many of the rules are context sensitive.
These findings suggest interesting directions for fu-
ture research, in particular learning rule direction-
ality and relevant contexts, issues that were hardly
explored till now. Such developments can be then
evaluated by the instance-based methodology, which
was designed to capture these two important aspects
of entailment rules.
Acknowledgements
The authors would like to thank Ephi Sachs and
Iddo Greental for their evaluation. This work was
partially supported by ISF grant 1095/05, the IST
Programme of the European Community under the
PASCAL Network of Excellence IST-2002-506778,
and the ITC-irst/University of Haifa collaboration.
References
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The second pascal recognising textual entail-
ment challenge. In Second PASCAL Challenge Work-
shop for Recognizing Textual Entailment.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of NAACL-HLT.
Gennaro Chierchia and Sally McConnell-Ginet. 2000.
Meaning and Grammar (2nd ed.): an introduction to
semantics. MIT Press, Cambridge, MA.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment chal-
lenge. Lecture Notes in Computer Science, 3944:177?
190.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on Evalu-
ation of Parsing Systems at LREC.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of HLT-NAACL.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of ACL.
Lorenza Romano, Milen Kouylekov, Idan Szpektor, Ido
Dagan, and Alberto Lavelli. 2006. Investigating a
generic paraphrase-based approach for relation extrac-
tion. In Proceedings of EACL.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of IWP.
Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and
Ralph Grishman. 2002. Automatic paraphrase acqui-
sition from news articles. In Proceedings of HLT.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representation
model for automatic IE pattern acquisition. In Pro-
ceedings of ACL.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition of
entailment relations. In Proceedings of EMNLP.
463
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 450?458,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Extracting Lexical Reference Rules from Wikipedia
Eyal Shnarch
Computer Science Department
Bar-Ilan University
Ramat-Gan 52900, Israel
shey@cs.biu.ac.il
Libby Barak
Dept. of Computer Science
University of Toronto
Toronto, Canada M5S 1A4
libbyb@cs.toronto.edu
Ido Dagan
Computer Science Department
Bar-Ilan University
Ramat-Gan 52900, Israel
dagan@cs.biu.ac.il
Abstract
This paper describes the extraction from
Wikipedia of lexical reference rules, iden-
tifying references to term meanings trig-
gered by other terms. We present extrac-
tion methods geared to cover the broad
range of the lexical reference relation and
analyze them extensively. Most extrac-
tion methods yield high precision levels,
and our rule-base is shown to perform bet-
ter than other automatically constructed
baselines in a couple of lexical expan-
sion and matching tasks. Our rule-base
yields comparable performance to Word-
Net while providing largely complemen-
tary information.
1 Introduction
A most common need in applied semantic infer-
ence is to infer the meaning of a target term from
other terms in a text. For example, a Question An-
swering system may infer the answer to a ques-
tion regarding luxury cars from a text mentioning
Bentley, which provides a concrete reference to the
sought meaning.
Aiming to capture such lexical inferences we
followed (Glickman et al, 2006), which coined
the term lexical reference (LR) to denote refer-
ences in text to the specific meaning of a target
term. They further analyzed the dataset of the First
Recognizing Textual Entailment Challenge (Da-
gan et al, 2006), which includes examples drawn
from seven different application scenarios. It was
found that an entailing text indeed includes a con-
crete reference to practically every term in the en-
tailed (inferred) sentence.
The lexical reference relation between two
terms may be viewed as a lexical inference rule,
denoted LHS? RHS. Such rule indicates that the
left-hand-side term would generate a reference, in
some texts, to a possible meaning of the right hand
side term, as the Bentley? luxury car example.
In the above example the LHS is a hyponym of
the RHS. Indeed, the commonly used hyponymy,
synonymy and some cases of the meronymy rela-
tions are special cases of lexical reference. How-
ever, lexical reference is a broader relation. For
instance, the LR rule physician ? medicine may
be useful to infer the topic medicine in a text cate-
gorization setting, while an information extraction
system may utilize the rule Margaret Thatcher
? United Kingdom to infer a UK announcement
from the text ?Margaret Thatcher announced?.
To perform such inferences, systems need large
scale knowledge bases of LR rules. A prominent
available resource is WordNet (Fellbaum, 1998),
from which classical relations such as synonyms,
hyponyms and some cases of meronyms may be
used as LR rules. An extension to WordNet was
presented by (Snow et al, 2006). Yet, available
resources do not cover the full scope of lexical ref-
erence.
This paper presents the extraction of a large-
scale rule base from Wikipedia designed to cover
a wide scope of the lexical reference relation. As
a starting point we examine the potential of defi-
nition sentences as a source for LR rules (Ide and
Jean, 1993; Chodorow et al, 1985; Moldovan and
Rus, 2001). When writing a concept definition,
one aims to formulate a concise text that includes
the most characteristic aspects of the defined con-
cept. Therefore, a definition is a promising source
for LR relations between the defined concept and
the definition terms.
In addition, we extract LR rules from Wikipedia
redirect and hyperlink relations. As a guide-
line, we focused on developing simple extrac-
tion methods that may be applicable for other
Web knowledge resources, rather than focusing
on Wikipedia-specific attributes. Overall, our rule
base contains about 8 million candidate lexical ref-
450
erence rules. 1
Extensive analysis estimated that 66% of our
rules are correct, while different portions of the
rule base provide varying recall-precision trade-
offs. Following further error analysis we intro-
duce rule filtering which improves inference per-
formance. The rule base utility was evaluated
within two lexical expansion applications, yield-
ing better results than other automatically con-
structed baselines and comparable results to Word-
Net. A combination with WordNet achieved the
best performance, indicating the significant mar-
ginal contribution of our rule base.
2 Background
Many works on machine readable dictionaries uti-
lized definitions to identify semantic relations be-
tween words (Ide and Jean, 1993). Chodorow et
al. (1985) observed that the head of the defining
phrase is a genus term that describes the defined
concept and suggested simple heuristics to find it.
Other methods use a specialized parser or a set of
regular expressions tuned to a particular dictionary
(Wilks et al, 1996).
Some works utilized Wikipedia to build an on-
tology. Ponzetto and Strube (2007) identified
the subsumption (IS-A) relation from Wikipedia?s
category tags, while in Yago (Suchanek et al,
2007) these tags, redirect links and WordNet were
used to identify instances of 14 predefined spe-
cific semantic relations. These methods depend
on Wikipedia?s category system. The lexical refer-
ence relation we address subsumes most relations
found in these works, while our extractions are not
limited to a fixed set of predefined relations.
Several works examined Wikipedia texts, rather
than just its structured features. Kazama and Tori-
sawa (2007) explores the first sentence of an ar-
ticle and identifies the first noun phrase following
the verb be as a label for the article title. We repro-
duce this part of their work as one of our baselines.
Toral and Mun?oz (2007) uses all nouns in the first
sentence. Gabrilovich and Markovitch (2007) uti-
lized Wikipedia-based concepts as the basis for a
high-dimensional meaning representation space.
Hearst (1992) utilized a list of patterns indica-
tive for the hyponym relation in general texts.
Snow et al (2006) use syntactic path patterns as
features for supervised hyponymy and synonymy
1For download see Textual Entailment Resource Pool at
the ACL-wiki (http://aclweb.org/aclwiki)
classifiers, whose training examples are derived
automatically from WordNet. They use these clas-
sifiers to suggest extensions to the WordNet hierar-
chy, the largest one consisting of 400K new links.
Their automatically created resource is regarded in
our paper as a primary baseline for comparison.
Many works addressed the more general notion
of lexical associations, or association rules (e.g.
(Ruge, 1992; Rapp, 2002)). For example, The
Beatles, Abbey Road and Sgt. Pepper would all
be considered lexically associated. However this
is a rather loose notion, which only indicates that
terms are semantically ?related? and are likely to
co-occur with each other. On the other hand, lex-
ical reference is a special case of lexical associa-
tion, which specifies concretely that a reference to
the meaning of one term may be inferred from the
other. For example, Abbey Road provides a con-
crete reference to The Beatles, enabling to infer a
sentence like ?I listened to The Beatles? from ?I
listened to Abbey Road?, while it does not refer
specifically to Sgt. Pepper.
3 Extracting Rules from Wikipedia
Our goal is to utilize the broad knowledge of
Wikipedia to extract a knowledge base of lexical
reference rules. Each Wikipedia article provides
a definition for the concept denoted by the title
of the article. As the most concise definition we
take the first sentence of each article, following
(Kazama and Torisawa, 2007). Our preliminary
evaluations showed that taking the entire first para-
graph as the definition rarely introduces new valid
rules while harming extraction precision signifi-
cantly.
Since a concept definition usually employs
more general terms than the defined concept (Ide
and Jean, 1993), the concept title is more likely
to refer to terms in its definition rather than vice
versa. Therefore the title is taken as the LHS of
the constructed rule while the extracted definition
term is taken as its RHS. As Wikipedia?s titles are
mostly noun phrases, the terms we extract as RHSs
are the nouns and noun phrases in the definition.
The remainder of this section describes our meth-
ods for extracting rules from the definition sen-
tence and from additional Wikipedia information.
Be-Comp Following the general idea in
(Kazama and Torisawa, 2007), we identify the IS-
A pattern in the definition sentence by extract-
ing nominal complements of the verb ?be?, taking
451
No. Extraction Rule
James Eugene ?Jim? Carrey is a Canadian-American actor
and comedian
1 Be-Comp Jim Carrey? Canadian-American actor
2 Be-Comp Jim Carrey? actor
3 Be-Comp Jim Carrey? comedian
Abbey Road is an album released by The Beatles
4 All-N Abbey Road? The Beatles
5 Parenthesis Graph? mathematics
6 Parenthesis Graph? data structure
7 Redirect CPU? Central processing unit
8 Redirect Receptors IgG? Antibody
9 Redirect Hypertension? Elevated blood-pressure
10 Link pet? Domesticated Animal
11 Link Gestaltist? Gestalt psychology
Table 1: Examples of rule extraction methods
them as the RHS of a rule whose LHS is the article
title. While Kazama and Torisawa used a chun-
ker, we parsed the definition sentence using Mini-
par (Lin, 1998b). Our initial experiments showed
that parse-based extraction is more accurate than
chunk-based extraction. It also enables us extract-
ing additional rules by splitting conjoined noun
phrases and by taking both the head noun and the
complete base noun phrase as the RHS for sepa-
rate rules (examples 1?3 in Table 1).
All-N The Be-Comp extraction method yields
mostly hypernym relations, which do not exploit
the full range of lexical references within the con-
cept definition. Therefore, we further create rules
for all head nouns and base noun phrases within
the definition (example 4). An unsupervised reli-
ability score for rules extracted by this method is
investigated in Section 4.3.
Title Parenthesis A common convention in
Wikipedia to disambiguate ambiguous titles is
adding a descriptive term in parenthesis at the end
of the title, as in The Siren (Musical), The Siren
(sculpture) and Siren (amphibian). From such ti-
tles we extract rules in which the descriptive term
inside the parenthesis is the RHS and the rest of
the title is the LHS (examples 5?6).
Redirect As any dictionary and encyclopedia,
Wikipedia contains Redirect links that direct dif-
ferent search queries to the same article, which has
a canonical title. For instance, there are 86 differ-
ent queries that redirect the user to United States
(e.g. U.S.A., America, Yankee land). Redirect
links are hand coded, specifying that both terms
refer to the same concept. We therefore generate a
bidirectional entailment rule for each redirect link
(examples 7?9).
Link Wikipedia texts contain hyper links to ar-
ticles. For each link we generate a rule whose LHS
is the linking text and RHS is the title of the linked
article (examples 10?11). In this case we gener-
ate a directional rule since links do not necessarily
connect semantically equivalent entities.
We note that the last three extraction methods
should not be considered as Wikipedia specific,
since many Web-like knowledge bases contain
redirects, hyper-links and disambiguation means.
Wikipedia has additional structural features such
as category tags, structured summary tablets for
specific semantic classes, and articles containing
lists which were exploited in prior work as re-
viewed in Section 2.
As shown next, the different extraction meth-
ods yield different precision levels. This may al-
low an application to utilize only a portion of the
rule base whose precision is above a desired level,
and thus choose between several possible recall-
precision tradeoffs.
4 Extraction Methods Analysis
We applied our rule extraction methods over a
version of Wikipedia available in a database con-
structed by (Zesch et al, 2007)2. The extraction
yielded about 8 million rules altogether, with over
2.4 million distinct RHSs and 2.8 million distinct
LHSs. As expected, the extracted rules involve
mostly named entities and specific concepts, typi-
cally covered in encyclopedias.
4.1 Judging Rule Correctness
Following the spirit of the fine-grained human
evaluation in (Snow et al, 2006), we randomly
sampled 800 rules from our rule-base and pre-
sented them to an annotator who judged them for
correctness, according to the lexical reference no-
tion specified above. In cases which were too dif-
ficult to judge the annotator was allowed to ab-
stain, which happened for 20 rules. 66% of the re-
maining rules were annotated as correct. 200 rules
from the sample were judged by another annotator
for agreement measurement. The resulting Kappa
score was 0.7 (substantial agreement (Landis and
2English version from February 2007, containing 1.6 mil-
lion articles. www.ukp.tu-darmstadt.de/software/JWPL
452
Extraction Per Method Accumulated
Method P Est. #Rules P %obtained
Redirect 0.87 1,851,384 0.87 31
Be-Comp 0.78 1,618,913 0.82 60
Parenthesis 0.71 94,155 0.82 60
Link 0.7 485,528 0.80 68
All-N 0.49 1,580,574 0.66 100
Table 2: Manual analysis: precision and estimated number
of correct rules per extraction method, and precision and %
of correct rules obtained of rule-sets accumulated by method.
Koch, 1997)), either when considering all the ab-
stained rules as correct or as incorrect.
The middle columns of Table 2 present, for each
extraction method, the obtained percentage of cor-
rect rules (precision) and their estimated absolute
number. This number is estimated by multiplying
the number of annotated correct rules for the ex-
traction method by the sampling proportion. In to-
tal, we estimate that our resource contains 5.6 mil-
lion correct rules. For comparison, Snow?s pub-
lished extension to WordNet3, which covers simi-
lar types of terms but is restricted to synonyms and
hyponyms, includes 400,000 relations.
The right part of Table 2 shows the perfor-
mance figures for accumulated rule bases, created
by adding the extraction methods one at a time in
order of their precision. % obtained is the per-
centage of correct rules in each rule base out of
the total number of correct rules extracted jointly
by all methods (the union set).
We can see that excluding the All-N method
all extraction methods reach quite high precision
levels of 0.7-0.87, with accumulated precision of
0.84. By selecting only a subset of the extrac-
tion methods, according to their precision, one can
choose different recall-precision tradeoff points
that suit application preferences.
The less accurate All-N method may be used
when high recall is important, accounting for 32%
of the correct rules. An examination of the paths
in All-N reveals, beyond standard hyponymy and
synonymy, various semantic relations that satisfy
lexical reference, such as Location, Occupation
and Creation, as illustrated in Table 3. Typical re-
lations covered by Redirect and Link rules include
3http://ai.stanford.edu/?rion/swn/
4As a non-comparable reference, Snow?s fine-grained
evaluation showed a precision of 0.84 on 10K rules and 0.68
on 20K rules; however, they were interested only in the hy-
ponym relation while we evaluate our rules according to the
broader LR relation.
synonyms (NY State Trooper ? New York State
Police), morphological derivations (irritate ? ir-
ritation), different spellings or naming (Pytagoras
? Pythagoras) and acronyms (AIS? Alarm Indi-
cation Signal).
4.2 Error Analysis
We sampled 100 rules which were annotated as in-
correct and examined the causes of errors. Figure
1 shows the distribution of error types.
Wrong NP part - The most common error
(35% of the errors) is taking an inappropriate part
of a noun phrase (NP) as the rule right hand side
(RHS). As described in Section 3, we create two
rules from each extracted NP, by taking both the
head noun and the complete base NP as RHSs.
While both rules are usually correct, there are
cases in which the left hand side (LHS) refers to
the NP as a whole but not to part of it. For ex-
ample, Margaret Thatcher refers to United King-
dom but not to Kingdom. In Section 5 we suggest
a filtering method which addresses some of these
errors. Future research may exploit methods for
detecting multi-words expressions.
All-N pa
ttern er
rors
13%Tra
nsparen
t head 11%
Wrong N
P part 35%
Technic
al error
s
10%
Dates a
nd Plac
es
5% Link err
ors 5% Redirec
t errors 5%
Related
 
but not Referrin
g 16%
Figure 1: Error analysis: type of incorrect rules
Related but not Referring - Although all terms
in a definition are highly related to the defined con-
cept, not all are referred by it. For example the
origin of a person (*The Beatles? Liverpool5) or
family ties such as ?daughter of? or ?sire of?.
All-N errors - Some of the articles start with a
long sentence which may include information that
is not directly referred by the title of the article.
For instance, consider *Interstate 80 ? Califor-
nia from ?Interstate 80 runs from California to
New Jersey?. In Section 4.3 we further analyze
this type of error and point at a possible direction
for addressing it.
Transparent head - This is the phenomenon in
which the syntactic head of a noun phrase does
5The asterisk denotes an incorrect rule
453
Relation Rule Path Pattern
Location Lovek? Cambodia Lovek city in Cambodia
Occupation Thomas H. Cormen? computer science Thomas H. Cormen professor of computer science
Creation Genocidal Healer? James White Genocidal Healer novel by James White
Origin Willem van Aelst? Dutch Willem van Aelst Dutch artist
Alias Dean Moriarty? Benjamin Linus Dean Moriarty is an alias of Benjamin Linus on Lost.
Spelling Egushawa? Agushaway Egushawa, also spelled Agushaway...
Table 3: All-N rules exemplifying various types of LR relations
not bear its primary meaning, while it has a mod-
ifier which serves as the semantic head (Fillmore
et al, 2002; Grishman et al, 1986). Since parsers
identify the syntactic head, we extract an incorrect
rule in such cases. For instance, deriving *Prince
William ? member instead of Prince William ?
British Royal Family from ?Prince William is a
member of the British Royal Family?. Even though
we implemented the common solution of using a
list of typical transparent heads, this solution is
partial since there is no closed set of such phrases.
Technical errors - Technical extraction errors
were mainly due to erroneous identification of the
title in the definition sentence or mishandling non-
English texts.
Dates and Places - Dates and places where a
certain person was born at, lived in or worked at
often appear in definitions but do not comply to
the lexical reference notion (*Galileo Galilei ?
15 February 1564).
Link errors - These are usually the result of
wrong assignment of the reference direction. Such
errors mostly occur when a general term, e.g. rev-
olution, links to a more specific albeit typical con-
cept, e.g. French Revolution.
Redirect errors - These may occur in some
cases in which the extracted rule is not bidirec-
tional. E.g. *Anti-globalization ? Movement of
Movements is wrong but the opposite entailment
direction is correct, as Movement of Movements is
a popular term in Italy for Anti-globalization.
4.3 Scoring All-N Rules
We observed that the likelihood of nouns men-
tioned in a definition to be referred by the con-
cept title depends greatly on the syntactic path
connecting them (which was exploited also in
(Snow et al, 2006)). For instance, the path pro-
duced by Minipar for example 4 in Table 1 is title
subj
??album vrel??released by?subj?? bypcomp?n?? noun.
In order to estimate the likelihood that a syn-
tactic path indicates lexical reference we collected
from Wikipedia all paths connecting a title to a
noun phrase in the definition sentence. We note
that since there is no available resource which cov-
ers the full breadth of lexical reference we could
not obtain sufficiently broad supervised training
data for learning which paths correspond to cor-
rect references. This is in contrast to (Snow et al,
2005) which focused only on hyponymy and syn-
onymy relations and could therefore extract posi-
tive and negative examples from WordNet.
We therefore propose the following unsuper-
vised reference likelihood score for a syntactic
path p within a definition, based on two counts:
the number of times p connects an article title with
a noun in its definition, denoted by Ct(p), and the
total number of p?s occurrences in Wikipedia de-
finitions, C(p). The score of a path is then de-
fined as Ct(p)C(p) . The rational for this score is that
C(p)? Ct(p) corresponds to the number of times
in which the path connects two nouns within the
definition, none of which is the title. These in-
stances are likely to be non-referring, since a con-
cise definition typically does not contain terms that
can be inferred from each other. Thus our score
may be seen as an approximation for the probabil-
ity that the two nouns connected by an arbitrary
occurrence of the path would satisfy the reference
relation. For instance, the path of example 4 ob-
tained a score of 0.98.
We used this score to sort the set of rules ex-
tracted by the All-N method and split the sorted list
into 3 thirds: top, middle and bottom. As shown in
Table 4, this obtained reasonably high precision
for the top third of these rules, relative to the other
two thirds. This precision difference indicates that
our unsupervised path score provides useful infor-
mation about rule reliability.
It is worth noting that in our sample 57% of All-
N errors, 62% of Related but not Referring incor-
rect rules and all incorrect rules of type Dates and
454
Extraction Per Method Accumulated
Method P Est. #Rules P %obtained
All-Ntop 0.60 684,238 0.76 83
All-Nmiddle 0.46 380,572 0.72 90
All-Nbottom 0.41 515,764 0.66 100
Table 4: Splitting All-N extraction method into 3 sub-types.
These three rows replace the last row of Table 2
Places were extracted by the All-Nbottom method
and thus may be identified as less reliable. How-
ever, this split was not observed to improve per-
formance in the application oriented evaluations
of Section 6. Further research is thus needed to
fully exploit the potential of the syntactic path as
an indicator for rule correctness.
5 Filtering Rules
Following our error analysis, future research is
needed for addressing each specific type of error.
However, during the analysis we observed that all
types of erroneous rules tend to relate terms that
are rather unlikely to co-occur together. We there-
fore suggest, as an optional filter, to recognize
such rules by their co-occurrence statistics using
the common Dice coefficient:
2 ? C(LHS,RHS)
C(LHS) + C(RHS)
where C(x) is the number of articles in Wikipedia
in which all words of x appear.
In order to partially overcome the Wrong NP
part error, identified in Section 4.2 to be the most
common error, we adjust the Dice equation for
rules whose RHS is also part of a larger noun
phrase (NP):
2 ? (C(LHS,RHS)? C(LHS,NPRHS))
C(LHS) + C(RHS)
where NPRHS is the complete NP whose part
is the RHS. This adjustment counts only co-
occurrences in which the LHS appears with the
RHS alone and not with the larger NP. This sub-
stantially reduces the Dice score for those cases in
which the LHS co-occurs mainly with the full NP.
Given the Dice score rules whose score does not
exceed a threshold may be filtered. For example,
the incorrect rule *aerial tramway? car was fil-
tered, where the correct RHS for this LHS is the
complete NP cable car. Another filtered rule is
magic? cryptography which is correct only for a
very idiosyncratic meaning.6
We also examined another filtering score, the
cosine similarity between the vectors representing
the two rule sides in LSA (Latent Semantic Analy-
sis) space (Deerwester et al, 1990). However, as
the results with this filter resemble those for Dice
we present results only for the simpler Dice filter.
6 Application Oriented Evaluations
Our primary application oriented evaluation is
within an unsupervised lexical expansion scenario
applied to a text categorization data set (Section
6.1). Additionally, we evaluate the utility of our
rule base as a lexical resource for recognizing tex-
tual entailment (Section 6.2).
6.1 Unsupervised Text Categorization
Our categorization setting resembles typical query
expansion in information retrieval (IR), where the
category name is considered as the query. The ad-
vantage of using a text categorization test set is
that it includes exhaustive annotation for all doc-
uments. Typical IR datasets, on the other hand,
are partially annotated through a pooling proce-
dure. Thus, some of our valid lexical expansions
might retrieve non-annotated documents that were
missed by the previously pooled systems.
6.1.1 Experimental Setting
Our categorization experiment follows a typical
keywords-based text categorization scheme (Mc-
Callum and Nigam, 1999; Liu et al, 2004). Tak-
ing a lexical reference perspective, we assume that
the characteristic expansion terms for a category
should refer to the term (or terms) denoting the
category name. Accordingly, we construct the cat-
egory?s feature vector by taking first the category
name itself, and then expanding it with all left-
hand sides of lexical reference rules whose right-
hand side is the category name. For example, the
category ?Cars? is expanded by rules such as Fer-
rari F50? car. During classification cosine sim-
ilarity is measured between the feature vector of
the classified document and the expanded vectors
of all categories. The document is assigned to
the category which yields the highest similarity
score, following a single-class classification ap-
proach (Liu et al, 2004).
6Magic was the United States codename for intelligence
derived from cryptanalysis during World War II.
455
Rule Base R P F1
Baselines:
No Expansion 0.19 0.54 0.28
WikiBL 0.19 0.53 0.28
Snow400K 0.19 0.54 0.28
Lin 0.25 0.39 0.30
WordNet 0.30 0.47 0.37
Extraction Methods from Wikipedia:
Redirect + Be-Comp 0.22 0.55 0.31
All rules 0.31 0.38 0.34
All rules + Dice filter 0.31 0.49 0.38
Union:
WordNet + WikiAll rules+Dice 0.35 0.47 0.40
Table 5: Results of different rule bases for 20 newsgroups
category name expansion
It should be noted that keyword-based text
categorization systems employ various additional
steps, such as bootstrapping, which generalize to
multi-class settings and further improve perfor-
mance. Our basic implementation suffices to eval-
uate comparatively the direct impact of different
expansion resources on the initial classification.
For evaluation we used the test set of the
?bydate? version of the 20-News Groups collec-
tion,7 which contains 18,846 documents parti-
tioned (nearly) evenly over the 20 categories8.
6.1.2 Baselines Results
We compare the quality of our rule base expan-
sions to 5 baselines (Table 5). The first avoids any
expansion, classifying documents based on cosine
similarity with category names only. As expected,
it yields relatively high precision but low recall,
indicating the need for lexical expansion.
The second baseline is our implementation of
the relevant part of the Wikipedia extraction in
(Kazama and Torisawa, 2007), taking the first
noun after a be verb in the definition sentence, de-
noted as WikiBL. This baseline does not improve
performance at all over no expansion.
The next two baselines employ state-of-the-art
lexical resources. One uses Snow?s extension to
WordNet which was mentioned earlier. This re-
source did not yield a noticeable improvement, ei-
7www.ai.mit.edu/people/jrennie/20Newsgroups.
8The keywords used as category names are: athe-
ism; graphic; microsoft windows; ibm,pc,hardware;
mac,hardware; x11,x-windows; sale; car; motorcycle;
baseball; hockey; cryptography; electronics; medicine; outer
space; christian(noun & adj); gun; mideast,middle east;
politics; religion
ther over the No Expansion baseline or over Word-
Net when joined with its expansions. The sec-
ond uses Lin dependency similarity, a syntactic-
dependency based distributional word similarity
resource described in (Lin, 1998a)9. We used var-
ious thresholds on the length of the expansion list
derived from this resource. The best result, re-
ported here, provides only a minor F1 improve-
ment over No Expansion, with modest recall in-
crease and significant precision drop, as can be ex-
pected from such distributional method.
The last baseline uses WordNet for expansion.
First we expand all the senses of each category
name by their derivations and synonyms. Each ob-
tained term is then expanded by its hyponyms, or
by its meronyms if it has no hyponyms. Finally,
the results are further expanded by their deriva-
tions and synonyms.10 WordNet expansions im-
prove substantially both Recall and F1 relative to
No Expansion, while decreasing precision.
6.1.3 Wikipedia Results
We then used for expansion different subsets
of our rule base, producing alternative recall-
precision tradeoffs. Table 5 presents the most in-
teresting results. Using any subset of the rules
yields better performance than any of the other
automatically constructed baselines (Lin, Snow
and WikiBL). Utilizing the most precise extrac-
tion methods of Redirect and Be-Comp yields the
highest precision, comparable to No Expansion,
but just a small recall increase. Using the entire
rule base yields the highest recall, while filtering
rules by the Dice coefficient (with 0.1 threshold)
substantially increases precision without harming
recall. With this configuration our automatically-
constructed resource achieves comparable perfor-
mance to the manually built WordNet.
Finally, since a dictionary and an encyclopedia
are complementary in nature, we applied the union
of WordNet and the filtered Wikipedia expansions.
This configuration yields the best results: it main-
tains WordNet?s precision and adds nearly 50% to
the recall increase of WordNet over No Expansion,
indicating the substantial marginal contribution of
Wikipedia. Furthermore, with the fast growth of
Wikipedia the recall of our resource is expected to
increase while maintaining its precision.
9Downloaded from www.cs.ualberta.ca/lindek/demos.htm
10We also tried expanding by the entire hyponym hierarchy
and considering only the first sense of each synset, but the
method described above achieved the best performance.
456
Category Name Expanding Terms
Politics opposition, coalition, whip(a)
Cryptography adversary, cryptosystem, key
Mac PowerBook, Radius(b), Grab(c)
Religion heaven, creation, belief, missionary
Medicine doctor, physician, treatment, clinical
Computer Graphics radiosity(d), rendering, siggraph(e)
Table 6: Some Wikipedia rules not in WordNet, which con-
tributed to text categorization. (a) a legislator who enforce
leadership desire (b) a hardware firm specializing in Macin-
tosh equipment (c) a Macintosh screen capture software (d)
an illumination algorithm (e) a computer graphics conference
Configuration Accuracy Accuracy Drop
WordNet + Wikipedia 60.0 % -
Without WordNet 57.7 % 2.3 %
Without Wikipedia 58.9 % 1.1 %
Table 7: RTE accuracy results for ablation tests.
Table 6 illustrates few examples of useful rules
that were found in Wikipedia but not in WordNet.
We conjecture that in other application settings
the rules extracted from Wikipedia might show
even greater marginal contribution, particularly in
specialized domains not covered well by Word-
Net. Another advantage of a resource based on
Wikipedia is that it is available in many more lan-
guages than WordNet.
6.2 Recognizing Textual Entailment (RTE)
As a second application-oriented evaluation we
measured the contributions of our (filtered)
Wikipedia resource and WordNet to RTE infer-
ence (Giampiccolo et al, 2007). To that end, we
incorporated both resources within a typical basic
RTE system architecture (Bar-Haim et al, 2008).
This system determines whether a text entails an-
other sentence based on various matching crite-
ria that detect syntactic, logical and lexical cor-
respondences (or mismatches). Most relevant for
our evaluation, lexical matches are detected when
a Wikipedia rule?s LHS appears in the text and
its RHS in the hypothesis, or similarly when pairs
of WordNet synonyms, hyponyms-hypernyms and
derivations appear across the text and hypothesis.
The system?s weights were trained on the devel-
opment set of RTE-3 and tested on RTE-4 (which
included this year only a test set).
To measure the marginal contribution of the two
resources we performed ablation tests, comparing
the accuracy of the full system to that achieved
when removing either resource. Table 7 presents
the results, which are similar in nature to those ob-
tained for text categorization. Wikipedia obtained
a marginal contribution of 1.1%, about half of the
analogous contribution of WordNet?s manually-
constructed information. We note that for current
RTE technology it is very typical to gain just a
few percents in accuracy thanks to external knowl-
edge resources, while individual resources usually
contribute around 0.5?2% (Iftene and Balahur-
Dobrescu, 2007; Dinu and Wang, 2009). Some
Wikipedia rules not in WordNet which contributed
to RTE inference are Jurassic Park ? Michael
Crichton, GCC? Gulf Cooperation Council.
7 Conclusions and Future Work
We presented construction of a large-scale re-
source of lexical reference rules, as useful in ap-
plied lexical inference. Extensive rule-level analy-
sis showed that different recall-precision tradeoffs
can be obtained by utilizing different extraction
methods. It also identified major reasons for er-
rors, pointing at potential future improvements.
We further suggested a filtering method which sig-
nificantly improved performance.
Even though the resource was constructed by
quite simple extraction methods, it was proven to
be beneficial within two different application set-
ting. While being an automatically built resource,
extracted from a knowledge-base created for hu-
man consumption, it showed comparable perfor-
mance to WordNet, which was manually created
for computational purposes. Most importantly, it
also provides complementary knowledge to Word-
Net, with unique lexical reference rules.
Future research is needed to improve resource?s
precision, especially for the All-N method. As
a first step, we investigated a novel unsupervised
score for rules extracted from definition sentences.
We also intend to consider the rule base as a di-
rected graph and exploit the graph structure for
further rule extraction and validation.
Acknowledgments
The authors would like to thank Idan Szpektor
for valuable advices. This work was partially
supported by the NEGEV project (www.negev-
initiative.org), the PASCAL-2 Network of Excel-
lence of the European Community FP7-ICT-2007-
1-216886 and by the Israel Science Foundation
grant 1112/08.
457
References
Roy Bar-Haim, Jonathan Berant, Ido Dagan, Iddo
Greental, Shachar Mirkin, Eyal Shnarch, and Idan
Szpektor. 2008. Efficient semantic deduction and
approximate matching over compact parse forests.
In Proceedings of TAC.
Martin S. Chodorow, Roy J. Byrd, and George E. Hei-
dorn. 1985. Extracting semantic hierarchies from a
large on-line dictionary. In Proceedings of ACL.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Lecture Notes in Computer Science,
volume 3944, pages 177?190.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science, 41:391?
407.
Georgiana Dinu and Rui Wang. 2009. Inference rules
for recognizing textual entailment. In Proceedings
of the IWCS.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and
Communication). The MIT Press.
Charles J. Fillmore, Collin F. Baker, and Hiroaki Sato.
2002. Seeing arguments through transparent struc-
tures. In Proceedings of LREC.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In Proceedings of
IJCAI.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third pascal recogniz-
ing textual entailment challenge. In Proceedings of
ACL-WTEP Workshop.
Oren Glickman, Eyal Shnarch, and Ido Dagan. 2006.
Lexical reference: a semantic matching subtask. In
Proceedings of EMNLP.
Ralph Grishman, Lynette Hirschman, and Ngo Thanh
Nhan. 1986. Discovery procedures for sublanguage
selectional patterns: Initial experiments. Computa-
tional Linguistics, 12(3):205?215.
Marti Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
COLING.
Nancy Ide and Ve?ronis Jean. 1993. Extracting
knowledge bases from machine-readable dictionar-
ies: Have we wasted our time? In Proceedings of
KB & KS Workshop.
Adrian Iftene and Alexandra Balahur-Dobrescu. 2007.
Hypothesis transformation and semantic variability
rules used in recognizing textual entailment. In Pro-
ceedings of the ACL-PASCAL Workshop on Textual
Entailment and Paraphrasing.
Jun?ichi Kazama and Kentaro Torisawa. 2007. Ex-
ploiting Wikipedia as external knowledge for named
entity recognition. In Proceedings of EMNLP-
CoNLL.
J. Richard Landis and Gary G. Koch. 1997. The
measurements of observer agreement for categorical
data. In Biometrics, pages 33:159?174.
Dekang Lin. 1998a. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL.
Dekang Lin. 1998b. Dependency-based evaluation of
MINIPAR. In Proceedings of the Workshop on Eval-
uation of Parsing Systems at LREC.
Bing Liu, Xiaoli Li, Wee Sun Lee, and Philip S. Yu.
2004. Text classification by labeling words. In Pro-
ceedings of AAAI.
Andrew McCallum and Kamal Nigam. 1999. Text
classification by bootstrapping with keywords, EM
and shrinkage. In Proceedings of ACL Workshop for
unsupervised Learning in NLP.
Dan Moldovan and Vasile Rus. 2001. Logic form
transformation of wordnet and its applicability to
question answering. In Proceedings of ACL.
Simone P. Ponzetto and Michael Strube. 2007. De-
riving a large scale taxonomy from wikipedia. In
Proceedings of AAAI.
Reinhard Rapp. 2002. The computation of word asso-
ciations: comparing syntagmatic and paradigmatic
approaches. In Proceedings of COLING.
Gerda Ruge. 1992. Experiment on linguistically-based
term associations. Information Processing & Man-
agement, 28(3):317?332.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In NIPS.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of COLING-ACL.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A core of semantic knowl-
edge - unifying wordnet and wikipedia. In Proceed-
ings of WWW.
Antonio Toral and Rafael Mun?oz. 2007. A proposal
to automatically build and maintain gazetteers for
named entity recognition by using wikipedia. In
Proceedings of NAACL/HLT.
Yorick A. Wilks, Brian M. Slator, and Louise M.
Guthrie. 1996. Electric words: dictionaries, com-
puters, and meanings. MIT Press, Cambridge, MA,
USA.
Torsten Zesch, Iryna Gurevych, and Max Mu?hlha?user.
2007. Analyzing and accessing wikipedia as a lex-
ical semantic resource. In Data Structures for Lin-
guistic Resources and Applications, pages 197?205.
458
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 172?179,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Lexical Reference: a Semantic Matching Subtask
Oren Glickman and Eyal Shnarch and Ido Dagan
Computer Science Department
Bar Ilan University
Ramat Gan, Israel
{glikmao, dagan}@cs.biu.ac.il
Abstract
Semantic lexical matching is a prominent
subtask within text understanding applica-
tions. Yet, it is rarely evaluated in a di-
rect manner. This paper proposes a def-
inition for lexical reference which cap-
tures the common goals of lexical match-
ing. Based on this definition we created
and analyzed a test dataset that was uti-
lized to directly evaluate, compare and im-
prove lexical matching models. We sug-
gest that such decomposition of the global
semantic matching task is critical in order
to fully understand and improve individual
components.
1 Introduction
A fundamental task for text understanding ap-
plications is to identify semantically equivalent
pieces of text. For example, Question Answer-
ing (QA) systems need to match corresponding
parts in the question and in the answer passage,
even though such parts may be expressed in dif-
ferent terms. Summarization systems need to rec-
ognize (redundant) semantically matching parts
in multiple sentences that are phrased differently.
Other applications, such as information extraction
and retrieval, face pretty much the same seman-
tic matching task. The degree of semantic match-
ing found is typically factored into systems? scor-
ing and ranking mechanisms. The recently pro-
posed framework of textual entailment (Dagan et
al., 2006) attempts to formulate the generic seman-
tic matching problem in an application indepen-
dent manner.
The most commonly implemented semantic
matching component addresses the lexical level.
At this level the goal is to identify whether the
meaning of a lexical item of one text is expressed
also within the other text. Typically, lexical match-
ing models measure the degree of literal lexical
overlap, augmented with lexical substitution cri-
teria based on resources such as Wordnet or the
output of statistical similarity methods (see Sec-
tion 2). Many systems apply semantic matching
only at the lexical level, which is used to approx-
imate the overall degree of semantic matching be-
tween texts. Other systems incorporate lexical
matching as a component within more complex
models that examine matching at higher syntactic
and semantic levels.
While lexical matching models are so promi-
nent within semantic systems they are rarely eval-
uated in a direct manner. Typically, improve-
ments to a lexical matching model are evaluated by
their marginal contribution to overall system per-
formance. Yet, such global and indirect evaluation
does not indicate the absolute performance of the
model relative to the sheer lexical matching task
for which it was designed. Furthermore, the indi-
rect application-dependent evaluation mode does
not facilitate improving lexical matching models
in an application dependent manner, and does not
allow proper comparison of such models which
were developed (and evaluated) by different re-
searchers within different systems.
This paper proposes a generic definition for the
lexical matching task, which we term lexical ref-
erence. This definition is application indepen-
dent and enables annotating test datasets that eval-
uate directly lexical matching models. Conse-
quently, we created a dataset annotated for lexical
reference, using a sample of sentence pairs (text-
hypothesis) from the 1st Recognising Textual En-
tailment dataset. Further analysis identified sev-
172
eral sub-types of lexical reference, pointing at the
many interesting cases where lexical reference is
derived from a complete context rather than from
a particular matching lexical item.
Next, we used the lexical reference dataset to
evaluate and compare several state-of-the-art ap-
proaches for lexical matching. Having a direct
evaluation task enabled us to capture the actual
performance level of these models, to reveal their
relative strengths and weaknesses, and even to
construct a simple combination of two models that
outperforms all the original ones. Overall, we sug-
gest that it is essential to decompose global se-
mantic matching and textual entailment tasks into
proper subtasks, like lexical reference. Such de-
composition is needed in order to fully understand
the behavior of individual system components and
to guide their future improvements.
2 Background
2.1 Term Matching
Thesaurus-based term expansion is a commonly
used technique for enhancing the recall of NLP
systems and coping with lexical variability. Ex-
pansion consists of altering a given text (usu-
ally a query) by adding terms of similar meaning.
WordNet is commonly used as a source of related
words for expansion. For example, many QA sys-
tems perform expansion in the retrieval phase us-
ing query related words based on WordNet?s lexi-
cal relations such as synonymy or hyponymy (e.g
(Harabagiu et al, 2000; Hovy et al, 2001)). Lex-
ical similarity measures (e.g. (Lin, 1998)) have
also been suggested to measure semantic similar-
ity. They are based on the distributional hypothe-
sis, suggesting that words that occur within similar
contexts are semantically similar.
2.2 Textual Entailment
The Recognising Textual Entailment (RTE-1) chal-
lenge (Dagan et al, 2006) is an attempt to promote
an abstract generic task that captures major seman-
tic inference needs across applications. The task
requires to recognize, given two text fragments,
whether the meaning of one text can be inferred
(entailed) from another text. Different techniques
and heuristics were applied on the RTE-1 dataset
to specifically model textual entailment. Interest-
ingly, a number of works (e.g. (Bos and Mark-
ert, 2005; Corley and Mihalcea, 2005; Jijkoun and
de Rijke, 2005; Glickman et al, 2006)) applied or
utilized lexical based word overlap measures. Var-
ious word-to-word similarity measures where ap-
plied, including distributional similarity (such as
(Lin, 1998)), web-based co-occurrence statistics
and WordNet based similarity measures (such as
(Leacock et al, 1998)).
2.3 Paraphrase Acquisition
A substantial body of work has been dedicated to
learning patterns of semantic equivalency between
different language expressions, typically consid-
ered as paraphrases. Recently, several works ad-
dressed the task of acquiring paraphrases (semi-)
automatically from corpora. Most attempts were
based on identifying corresponding sentences in
parallel or ?comparable? corpora, where each cor-
pus is known to include texts that largely corre-
spond to texts in another corpus (e.g. (Barzilay
and McKeown, 2001)). Distributional Similarity
was also used to identify paraphrase patterns from
a single corpus rather than from a comparable
set of corpora (Lin and Pantel, 2001). Similarly,
(Glickman and Dagan, 2004) developed statistical
methods that match verb paraphrases within a reg-
ular corpus.
3 The Lexical Reference Dataset
3.1 Motivation and Definition
One of the major observations of the 1st Recog-
nizing Textual Entailment (RTE-1) challenge re-
ferred to the rich structure of entailment modeling
systems and the need to evaluate and optimize in-
dividual components within them. When building
such a compound system it is valuable to test each
component directly during its development, rather
than indirectly evaluating the component?s perfor-
mance via the behavior of the entire system. If
given tools to evaluate each component indepen-
dently researchers can target and perfect the per-
formance of the subcomponents without the need
of building and evaluating the entire end-to-end
system.
A common subtask, addressed by practically all
participating systems in RTE-1, was to recognize
whether each lexical meaning in the hypothesis is
referenced by some meaning in the corresponding
text. We suggest that this common goal can be
captured through the following definition:
Definition 1 A word w is lexically referenced by
a text t if there is an explicit or implied reference
173
from a set of words in t to a possible meaning of
w.
Lexical reference may be viewed as a natural ex-
tension of textual entailment for sub-sentential hy-
potheses such as words. In this work we fo-
cus on words meanings, however this work can
be directly generalized to word compounds and
phrases. A concrete version of detailed annotation
guidelines for lexical reference is presented in the
next section.1 Lexical Reference is, in some sense,
a more general notion than paraphrases. If the text
includes a paraphrase for w then naturally it does
refer to w?s meaning. However, a text need not
include a paraphrase for the concrete meaning of
the referenced word w, but only an implied refer-
ence. Accordingly, the referring part might be a
large segment of the text, which captures informa-
tion different than w?s meaning, but still implies a
reference to w as part of the text?s meaning.
It is typically a necessary, but not sufficient,
condition for textual entailment that the lexical
concepts in a hypothesis h are referred in a given
text t. For example, in order to infer from a text
the hypothesis ?a dog bit a man,? it is a neces-
sary that the concepts of dog, bite and man must
be referenced by the text, either directly or in an
implied manner. However, for proper entailment
it is further needed that the right relations would
hold between these concepts2. Therefore lexical
entailment should typically be a component within
a more complex entailment modeling (or semantic
matching) system.
3.2 Dataset Creation and Annotation Process
We created a lexical reference dataset derived
from the RTE-1 development set by randomly
choosing 400 out of the 567 text-hypothesis exam-
ples. We then created sentence-word examples for
all content words in the hypotheses which do not
appear in the corresponding sentence and are not
a morphological derivation of a word in it (since a
simple morphologic module could easily identify
these cases). This resulted in a total of 708 lexi-
cal reference examples. Two annotators annotated
these examples as described in the next section.
1These terms should not be confused with the use of lex-
ical entailment in WordNet, which is used to describe an en-
tailment relationship between verb lexical types, nor with the
related notion of reference in classical linguistics, generally
describing the relation between nouns or pronouns and ob-
jects that are named by them (Frege, 1892)
2or quoting the known journalism saying ? ?Dog bites
man? isn?t news, but ?Man bites dog? is.
Taking the same approach as of the RTE-1 dataset
creation (Dagan et al, 2006), we limited our ex-
periments to the resulting 580 examples that the
two annotators agreed upon3.
3.2.1 Annotation guidelines
We asked two annotators to annotate the
sentence-word examples according to the follow-
ing guidelines. Given a sentence and a target word
the annotators were asked to decide whether the
target word is referred by the sentence (true) or
not (false). Annotators were guided to mark the
pair as true in the following cases:
Word: if there is a word in the sentence which,
in the context of the sentence, implies a meaning
of the target word (e.g. a synonym or hyponym),
or which implies a reference to the target word?s
meaning (e.g. blind?see, sight). See examples 1-
2 in Table 1 where the word that implies the refer-
ence is emphasized in the text. Note that in exam-
ple 2 murder is not a synonym of died nor does it
share the same meaning of died; however it is clear
from its presence in the sentence that it refers to a
death. Also note that in example 8 although home
is a possible synonym for house, in the context of
the text it does not appear in that meaning and the
example should be annotated as false.
Phrase: if there is a multi-word independent ex-
pression in the sentence that implies the target (im-
plication in the same sense that a Word does). See
examples 3-4 in Table 1.
Context: if there is a clear reference to the mean-
ing of the target word by the overall meaning of
some part(s) of the sentence (possibly all the sen-
tence), though it is not referenced by any single
word or phrase. The reference is derived from the
complete context of the relevant sentence part. See
examples 5-7 in Table 1.
If there is no reference from the sentence to
the target word the annotators were instructed to
choose false. In example 9 in Table 1 the target
word ?HIV-positive? should be considered as one
word that cannot be broken down from its unit and
although both the general term ?HIV status? and
the more specific term ?HIV negative? are referred
to, the target word cannot be understood or derived
from the text. In example 10 although the year
1945 may refer to a specific war, there is no ?war?
either specifically or generally understood by the
text.
3dataset avaiable at http://ir-srv.cs.biu.ac.
il:64080/emnlp06_dataset.zip
174
ID TEXT TARGET VALUE
1 Oracle had fought to keep the forms from being released. document word
2 The court found two men guilty of murdering Shapour Bakhtiar. died word
3 The new information prompted them to call off the search. cancelled phrase
4 Milan, home of the famed La Scala opera house,. . . located phrase
5 Successful plaintiffs recovered punitive damages in Texas discrimination cases 53 legal context
6 Recreational marijuana smokers are no more likely to develop oral cancer than nonusers. risk context
7 A bus ticket cost nowadays 5.2 NIS whereas last year it cost 4.9. increase context
8 Pakistani officials announced that two South African men in their custody had confessed to
planning attacks at popular tourist spots in their home country.
house false
9 For women who are HIV negative or who do not know their HIV status, breastfeeding should
be promoted for six months.
HIV-positive false
10 On Feb. 1, 1945, the Polish government made Warsaw its capital, and an office for urban
reconstruction was set up.
war false
Table 1: Lexical Reference Annotation Examples
3.2.2 Annotation results
Wemeasured the agreement on the lexical refer-
ence binary task (in which Word, Phrase and Con-
text are conflated to true). The resulting kappa
statistic of 0.63 is regarded as substantial agree-
ment (Landis and Koch, 1997). The resulting
dataset is not balanced in terms of true and false
examples and a straw-baseline for accuracy is
0.61, representing a system which predicts all ex-
amples as true.
3.3 Dataset Analysis
In a similar manner to (Bar-Haim et al, 2005; Van-
derwende et al, 2005) we investigated the rela-
tionship between lexical reference and textual en-
tailment. We checked the performance of a textual
entailment system which relies solely on an ideal
lexical reference component which makes no mis-
takes and asserts that a hypothesis is entailed from
a text if and only if all content words in the hypoth-
esis are referred in the text. Based on the lexical
reference dataset annotations, such an ?ideal? sys-
tem would obtain an accuracy of 74% on the cor-
responding subset of the textual entailment task.
The corresponding precision is 68% and a recall
of 82%. This is significantly higher than the re-
sults of the best performing systems that partici-
pated in the challenge on the RTE-1 test set. This
suggests that lexical reference is a valuable sub-
task for entailment. Interestingly, a similar entail-
ment system based on a lexical reference compo-
nent which doesn?t account for the contextual lex-
ical reference (i.e. all Context annotations are re-
garded as false) would achieve an accuracy of only
63% with 41% precision and a recall of 63%. This
suggests that lexical reference in general and con-
textual entailment in particular, play an important
(though not sufficient) role in entailment recogni-
tion.
Further, we wanted to investigate the validity
of the assumption that for entailment relationship
to hold all content words in the hypothesis must
be referred by the text. We examined the exam-
ples in our dataset which were derived from text-
hypothesis pairs that were annotated as true (en-
tailing) in the RTE dataset. Out of 257 such exam-
ples only 34 were annotated as false by both anno-
tators. Table 2 lists a few such examples in which
entailment at whole holds, however, there exists a
word in the hypothesis (highlighted in the table)
which is not lexically referenced by the text. In
many cases, the target word was part of a non com-
positional compound in the hypothesis, and there-
fore should not be expected to be referenced by
the text (see examples 1-2). This finding indicates
that the basic assumption is a reasonable approxi-
mation for entailment. We could not have revealed
this fact without the dataset for the subtask of lex-
ical reference.
4 Lexical Reference Models
The lexical reference dataset facilitates qualita-
tive and quantitative comparison of various lexical
models. This section describes four state-of-the-
art models that can be applied to the lexical refer-
ence task. The performance of these models was
tested and analyzed, as described in the next sec-
tion, using the lexical reference dataset. All mod-
els assign a [0, 1] score to a given pair of text t
and target word u which can be interpreted as the
confidence that u is lexically referenced in t.
175
ID TEXT HYPOTHESIS ENTAIL-
MENT
REFER-
ENCE
1 Iran is said to give up al Qaeda members. Iran hands over al Qaeda members. true false
2 It would help the economy by putting people
back to work and more money in the hands of
consumers.
More money in the hands of consumers
means more money can be spent to get the
economy going.
true false
3 The Securities and Exchange Commission?s
new rule to beef up the independence of mutual
fund boards represents an industry defeat.
The SEC?s new rule will give boards inde-
pendence.
true false
4 Texas Data Recovery is also successful at re-
trieving lost data from notebooks and laptops,
regardless of age, make or model.
In the event of a disaster you could use Texas
Data Recovery and you will have the capabil-
ity to restore lost data.
true false
Table 2: examples demonstrating cases when lexical entailment does not correlate with entailment. Tar-
get word is shown in bold.
4.1 WordNet
Following the common practice in NLP applica-
tions (see Section 2.1) we evaluated the perfor-
mance of a straight-forward utilization of Word-
Net?s lexical information. Our wordnet model first
lemmatizes the text and target word. It then as-
signs a score of 1 if the text contains a synonym,
hyponym or derived form of the target word and a
score of 0 otherwise.
4.2 Similarity
As a second measure we used the distributional
similarity measure of (Lin, 1998). For a text t and
a word u we assign the max similarity score as fol-
lows:
similarity(t, u) = max
v?t
sim(u, v) (1)
where sim(u, v) is the similarity score for u and
v4.
4.3 Alignment model
(Glickman et al, 2006) was among the top scor-
ing systems on the RTE-1 challenge and supplies a
probabilistically motivated lexical measure based
on word co-occurrence statistics. It is defined for
a text t and a word u as follows:
align(t, u) = max
v?t
P(u|v) (2)
where P(u|v) is simply the co-occurrence proba-
bility ? the probability that a sentence containing v
also contains u. The co-occurrence statistics were
collected from the Reuters Corpus Volume 1.
4the scores were obtained from the following online re-
source: http://www.cs.ualberta.ca/?lindek/
downloads.htm
4.4 Baysean model
(Glickman et al, 2005) provide a contextual mea-
sure which takes into account the whole context
of the text rather than from a single word in the
text as do the previous models. This model is
the only model which addresses contextual refer-
ence rather than just word-to-word matching. The
model is based on a Na??ve Bayes text classification
approach in which corpus sentences serve as doc-
uments and the class is the reference of the target
word u. Sentences containing the word u are used
as positive examples while all other sentences are
considered as negative examples. It is defined for
a text t and a word u as follows:
bayes(t, u) =
P(u)
?
v?t P(v|u)
n(v,t)
P(?u)
?
v?t P(v|?u)
n(v,t)+P(u)
?
v?t P(v|u)
n(v,t)
(3)
where n(w, t) is the number of times word w ap-
pears in t, P(u) is the probability that a sentence
contains the word u andP(v|?u) is the probability
that a sentence NOT containing u contains v. In
order to reduce data size and to account for zero
probabilities we applied smoothing and informa-
tion gain based feature selection on the data prior
to running the model. The co-occurrence prob-
abilities were collected from sentences from the
Reuters corpus in a similar manner to the align-
ment model.
4.5 Combined Model
The WordNet and Bayesian models are derived
from quite different motivations. One would ex-
pect the WordNet model to be better in identify-
ing the word-to-word explicit reference examples
while the Bayesian model is expected to model the
contextualy implied references. For this reason we
tried to combine forces by evaluating a na??ve linear
176
interpolation of the two models (by simply averag-
ing the score of the two models). This model have
not been previously suggested and to the best of
our knowledge this type of combination is novel.
5 Empirical Evaluation and Analysis
5.1 Results
In order to evaluate the scores produced by the
various models as a potential component in an en-
tailment system we compared the recall-precision
graphs. In addition we compared the average pre-
cision which is a single number measure equiv-
alent to the area under an uninterpolated recall-
precision curve and is commonly used to evaluate
a systems ranking ability (Voorhees and Harman,
1999). On our dataset an average precision greater
than 0.65 is better than chance at the 0.05 level
and an average precision greater than 0.66 is sig-
nificant at the 0.01 level.
Figure 1 compares the average precision and
recall-precision results for the various models. As
can be seen, the combined wordnet+bayes model
performs best. In terms of average precision,
the similarity and wordnet models are comparable
and are slightly better than bayes. The alignment
model, however, is not significantly better than
random guessing. The recall-precision figure indi-
cates that the baysian model succeeds to rank quite
well both within the the positively scored wordnet
examples and within the negatively scored word-
net examples and thus resulting in improved av-
erage precision of the combined model. A better
understanding of the systems? performance is evi-
dent from the following analysis.
5.2 Analysis
Table 3 lists a few examples from the lexical refer-
ence dataset alng with their gold-standard anno-
tation and the Bayesian model score. Manual in-
spection of the data shows that the Bayesian model
commonly assigns a low score to correct examples
which have an entailing trigger word or phrase in
the sentence but yet the context of the sentence as a
whole is not typical for the target hypothesized en-
tailed word. For example, in example 5 the entail-
ing phrase ?set in place? and in example 6 the en-
tailing word ?founder? do appear in the text how-
ever the contexts of the sentences are not typical
news domain contexts of issued or founded. An in-
teresting future work would be to change the gen-
erative story and model to account for such cases.
The WordNet model identified a matching word
in the text for 99 out of the 580 examples. This
corresponds to a somewhat low recall of 25% and
a quite high precision of 90%. Table 4 lists typical
mistakes of the wordnet model. Examples 1-3 are
false positive examples in which there is a word
in the text (emphasized in the table) which is a
synonym or hyponym of the target word for some
sense in WordNet, however in the context of the
text it is not of such a sense. Examples 4-6 show
false negative examples, in which the annotators
identified a trigger word in the text (emphasized
in the table) but yet it or no other word in the text
is a synonym or hyponym of the target word.
5.3 Subcategory analysis
word phrase context false
word 178 16 59 32
phrase 4 12 9 4
context 15 5 56 25
false 24 5 38 226
Table 5: inter-annotator confusion matrix for the
auxiliary annotation.
As seen above, the combined model outper-
forms the others since it identifies both word-
to-word lexical reference as well as context-to-
word lexical reference. These are quite different
cases. We asked the annotators to state the sub-
category when they annotated an example as true
(as described in the annotation guidelines in Sec-
tion 3.2.1). The Word subcategory corresponds
to a word-to-word match and Phrase and Context
subcategories correspond to more than one word
to word match. As can be expected, the agreement
on such a task resulted in a lower Kappa of 0.5
which corresponds to moderate agreement (Landis
and Koch, 1997). the confusion matrix between
the two annotators is presented in Table 5. This de-
composition enables the evaluation of the strength
and weakness of different lexical reference mod-
ules, free from the context of the bigger entailment
system.
We used the subcategories dataset to test the
performances of the different models. Table 6
lists for each subcategory the recall of correctly
identified examples for each model?s 25% recall
level. The table shows that the wordnet and simi-
larity models? strength is in identifying examples
where lexical reference is triggered by a dominant
word in the sentence. The bayes model, however,
177
Figure 1: comparison of average precision (left) and recall-precision (right) results for the various models
id text token annotation score
1 QNX Software Systems Ltd., a leading provider of real-time software and ser-
vices to the embedded computing market, is pleased to announce the appoint-
ment of Mr. Sachin Lawande to the position of vice president, engineering ser-
vices.
named PHRASE 0.98
2 NIH?s FY05 budget request of $28.8 billion includes $2 billion for the National
Institute of General Medical Sciences, a 3.4-percent increase, and $1.1 billion
for the National Center for Research Resources, and a 7.2-percent decrease from
FY04 levels.
reduced WORD 0.91
3 Pakistani officials announced that two South African men in their custody had
confessed to planning attacks at popular tourist spots in their home country.
security CONTEXT 0.80
4 With $549 million in cash as of June 30, Google can easily afford to make
amends.
shares FALSE 0.03
5 In the year 538, Cyrus set in place a policy which demanded the return of the
various gods to their proper places.
issued PHRASE 7e-4
6 The black Muslim activist said that he had relieved Muhammad of his duties
?until he demonstrates that he is willing to conform to the manner of representing
Allah and the honorable Elijah Muhammad (founder of the Nation of Islam)?.
founded WORD 3e-6
Table 3: A sample from the lexical reference dataset alng with the Bayesian model?s score
id text token annotation
1 Kerry hit Bush hard on his conduct on the war in Iraq shot FALSE
2 Pakistani officials announced that two South African men in their custody had confessed to
planning attacks at popular tourist spots in their home country
forces FALSE
3 It would help the economy by putting people back to work and more money in the hands of
consumers
get FALSE
4 Eating lots of foods that are a good source of fiber may keep your blood glucose from rising
too fast after you eat
sugar WORD
5 Hippos do come into conflict with people quite often human WORD
6 Weinstock painstakingly reviewed dozens of studies for evidence of any link between sun-
screen use and either an increase or decrease in melanoma
cancer WORD
Table 4: A few erroneous examples of WordNet model
is better at identifying phrase and context exam-
ples. The combined WordNet and Bayesian mod-
els? strength can be explained by the quite dif-
ferent behaviors of the two models - the Word-
Net model seems to be better in identifying the
word-to-word explicit reference examples while
the Bayesian model is better in modeling the con-
textual implied references.
6 Conclusions
This paper proposed an explicit task definition for
lexical reference. This task captures directly the
goal of common lexical matching models, which
typically operate within more complex systems
178
method word disagreement phrase/context
wordnet 38% 9% 17%
similarity 39% 7% 17%
bayes 22% 21% 37%
Table 6: Breakdown of recall of correctly identi-
fied example types at an overall system?s recall of
25%. Disagreement refers to examples for which
the annotators did not agree on the subcategory an-
notation (word vs. phrase/context).
that address more complex tasks. This defini-
tion enabled us to create an annotated dataset for
the lexical reference task, which provided insights
into interesting sub-classes that require different
types of modeling. The dataset enabled us to
make a direct evaluation and comparison of lexical
matching models, reveal insightful differences be-
tween them, and create a simple improved model
combination. In the long run, we believe that
the availability of such datasets will facilitate im-
proved models that consider the various sub-cases
of lexical reference, as well as applying supervised
learning to optimize model combination and per-
formance.
References
[Bar-Haim et al2005] Roy Bar-Haim, Idan Szpecktor,
and Oren Glickman. 2005. Definition and analysis
of intermediate entailment levels. In Proceedings
of the ACL Workshop on Empirical Modeling of Se-
mantic Equivalence and Entailment, pages 55?60,
Ann Arbor, Michigan, June. Association for Com-
putational Linguistics.
[Barzilay and McKeown2001] Regina Barzilay and
Kathleen McKeown. 2001. Extracting paraphrases
from a parallel corpus. In ACL, pages 50?57.
[Bos and Markert2005] Johan Bos and Katja Markert.
2005. Recognising textual entailment with logical
inference techniques. In EMNLP.
[Corley and Mihalcea2005] Courtney Corley and Rada
Mihalcea. 2005. Measuring the semantic similarity
of texts. In Proceedings of the ACL Workshop on
Empirical Modeling of Semantic Equivalence and
Entailment, pages 13?18.
[Dagan et al2006] Ido Dagan, Oren Glickman, and
Bernardo Magnini, editors. 2006. The PASCAL
Recognising Textual Entailment Challenge, volume
3944. Lecture Notes in Computer Science.
[Frege1892] Gottlob Frege. 1892. On sense and
reference. Reprinted in P. Geach and M. Black,
eds., Translations from the Philosophical Writings
of Gottlob Frege. 1960.
[Glickman and Dagan2004] Oren Glickman and Ido
Dagan, 2004. Recent Advances in Natural Lan-
guage Processing III, chapter Acquiring lexical
paraphrases from a single corpus, pages 81?90.
John Benjamins.
[Glickman et al2005] Oren Glickman, Ido Dagan, and
Moshe Koppel. 2005. A probabilistic classification
approach for lexical textual entailment. In AAAI,
pages 1050?1055.
[Glickman et al2006] Oren Glickman, Ido Dagan, and
Moshe Koppel. 2006. A lexical alignment model
for probabilistic textual entailment, volume 3944.
In Lecture Notes in Computer Science, pages 287 ?
298. Springer.
[Harabagiu et al2000] Sanda M. Harabagiu, Dan I.
Moldovan, Marius Pasca, Rada Mihalcea, Mihai
Surdeanu, Razvan C. Bunescu, Roxana Girju, Vasile
Rus, and Paul Morarescu. 2000. Falcon: Boosting
knowledge for answer engines. In TREC.
[Hovy et al2001] Eduard H. Hovy, Ulf Hermjakob, and
Chin-Yew Lin. 2001. The use of external knowl-
edge of factoid QA. In Text REtrieval Conference.
[Jijkoun and de Rijke2005] Valentin Jijkoun and
Maarten de Rijke. 2005. Recognizing textual
entailment using lexical similarity. Proceedings of
the PASCAL Challenges Workshop on Recognising
Textual Entailment (and forthcoming LNAI book
chapter).
[Landis and Koch1997] J. R. Landis and G. G. Koch.
1997. The measurements of observer agreement for
categorical data. Biometrics, 33:159?174.
[Leacock et al1998] Claudia Leacock, George A.
Miller, and Martin Chodorow. 1998. Using
corpus statistics and wordnet relations for sense
identification. Comput. Linguist., 24(1):147?165.
[Lin and Pantel2001] Dekang Lin and Patrik Pantel.
2001. Discovery of inference rules for question an-
swering. Natural Language Engineering, 4(7):343?
360.
[Lin1998] Dekang Lin. 1998. Automatic retrieval and
clustering of similar words. In Proceedings of the
17th international conference on Computational lin-
guistics, pages 768?774, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
[Vanderwende et al2005] Lucy Vanderwende, Deborah
Coughlin, and Bill Dolan. 2005. What syntax
can contribute in entailment task. Proceedings of
the PASCAL Challenges Workshop on Recognising
Textual Entailment.
[Voorhees and Harman1999] Ellen M. Voorhees and
Donna Harman. 1999. Overview of the seventh text
retrieval conference. In Proceedings of the Seventh
Text REtrieval Conference (TREC-7). NIST Special
Publication.
179
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 770?778,
Beijing, August 2010
Recognising Entailment within Discourse
Shachar Mirkin?, Jonathan Berant?, Ido Dagan?, Eyal Shnarch?
? Computer Science Department, Bar-Ilan University
? The Blavatnik School of Computer Science, Tel-Aviv University
Abstract
Texts are commonly interpreted based on
the entire discourse in which they are sit-
uated. Discourse processing has been
shown useful for inference-based applica-
tion; yet, most systems for textual entail-
ment ? a generic paradigm for applied in-
ference ? have only addressed discourse
considerations via off-the-shelf corefer-
ence resolvers. In this paper we explore
various discourse aspects in entailment in-
ference, suggest initial solutions for them
and investigate their impact on entailment
performance. Our experiments suggest
that discourse provides useful informa-
tion, which significantly improves entail-
ment inference, and should be better ad-
dressed by future entailment systems.
1 Introduction
This paper investigates the problem of recognising
textual entailment within discourse. Textual En-
tailment (TE) is a generic framework for applied
semantic inference (Dagan et al, 2009). Under
TE, the relationship between a text (T) and a tex-
tual assertion (hypothesis, H) is defined such that
T entails H if humans reading T would infer that
H is most likely true (Dagan et al, 2006).
TE has been successfully applied to a variety of
natural language processing applications, includ-
ing information extraction (Romano et al, 2006)
and question answering (Harabagiu and Hickl,
2006). Yet, most entailment systems have thus
far paid little attention to discourse aspects of in-
ference. In part, this is the result of the unavail-
ability of adept tools for handling the kind of dis-
course processing required for inference. In addi-
tion in the main TE benchmarks, the Recognising
Textual Entailment (RTE) challenges, discourse
played little role. This state of affairs has started
to change with the recent introduction of the RTE
Pilot ?Search? task (Bentivogli et al, 2009b), in
which assessed texts are situated within complete
documents. In this setting, texts need to be inter-
preted based on their entire discourse (Bentivogli
et al, 2009a), hence attending to discourse issues
becomes essential. Consider the following exam-
ple from the task?s dataset:
(T) The seven men on board were said to have
as little as 24 hours of air.
For the interpretation of T, e.g. the identity and
whereabouts of the seven men, one must consider
T?s discourse. The preceding sentence T?, for in-
stance, provides useful information to that aim:
(T?) The Russian navy worked desperately to
save a small military submarine.
This example demonstrates a common situation in
texts, and is also applicable to the RTE Search
task?s setting. Still, little was done by the task?s
participants to consider discourse, and sentences
were mostly processed independently.
Analyzing the Search task?s development set,
we identified several key discourse aspects that af-
fect entailment in a discourse-dependent setting.
First, we observed that the coverage of available
coreference resolution tools is considerably lim-
ited. To partly address this problem, we extend the
set of coreference relations to phrase pairs with
a certain degree of lexical overlap, as long as no
semantic incompatibility is found between them.
Second, many bridging relations (Clark, 1975) are
realized in the form of ?global information? per-
ceived as known for entire documents. As bridg-
ing falls completely out of the scope of available
resolvers, we address this phenomenon by iden-
tifying and weighting prominent document terms
and allowing their incorporation in inference even
770
when they are not explicitly mentioned in a sen-
tence. Finally, we observed a coherence-related
discourse phenomenon, namely inter-relations be-
tween entailing sentences in the discourse, such
as the tendency of entailing sentences to be ad-
jacent to one another. To that end, we apply a
two-phase classification scheme, where a second-
phase meta-classifier is applied, extracting dis-
course and document-level features based on the
classification of each sentence on its own.
Our results show that, even when simple so-
lutions are employed, the reliance on discourse-
based information is helpful and achieves a sig-
nificant improvement of results. We analyze the
contribution of each component and suggest some
future work to better attend to discourse in entail-
ment systems. To our knowledge, this is the most
extensive effort thus far to empirically explore the
effect of discourse on entailment systems.
2 Background
Discourse plays a key role in text understanding
applications such as question answering or infor-
mation extraction. Yet, such applications typically
only handle a narrow aspect of discourse, address-
ing coreference by term substitution (Dali et al,
2009; Li et al, 2009). The limited coverage and
scope of existing tools for coreference resolution
and the unavailability of tools for addressing other
discourse aspects also contribute to this situation.
For instance, VP anaphora and bridging relations
are usually not handled at all by such resolvers. A
similar situation is seen in the TE research field.
The prominent benchmark for entailment sys-
tems evaluation is the series of RTE challenges.
The main task in these challenges has tradition-
ally been to determine, given a text-hypothesis
pair (T,H), whether T entails H. Discourse played
no role in the first two RTE challenges as
T?s were constructed of short simplified texts.
In RTE-3 (Giampiccolo et al, 2007), where
some paragraph-long texts were included, inter-
sentential relations became relevant for correct in-
ference. Yet the texts in the task were manually
modified to ensure they are self-contained. Con-
sequently, little effort was invested by the chal-
lenges? participants to address discourse issues
beyond the standard substitution of coreferring
nominal phrases, using publicly available tools
such as JavaRap (Qiu et al, 2004) or OpenNLP1,
e.g. (Bar-Haim et al, 2008).
A major step in the RTE challenges towards a
more practical setting of text processing applica-
tions occurred with the introduction of the Search
task in the Fifth RTE challenge (RTE-5). In this
task entailing sentences are situated within doc-
uments and depend on other sentences for their
correct interpretation. Thus, discourse becomes
a substantial factor impacting inference. Surpris-
ingly, discourse hardly received any treatment in
this task beyond the standard use of coreference
resolution (Castillo, 2009; Litkowski, 2009), and
an attempt to address globally-known information
by removing from H words that appear in docu-
ment headlines (Clark and Harrison, 2009).
3 The RTE Search Task
The RTE-5 Search task was derived from the
TAC Summarization task2. The dataset consists
of several corpora, each comprised of news arti-
cles concerning a specific topic, such as the im-
pact of global warming on the Arctic or the Lon-
don terrorist attacks in 2005. Hypotheses were
manually generated based on Summary Content
Units (Nenkova et al, 2007), clause-long state-
ments taken from manual summaries of the cor-
pora. Texts are unmodified sentences in the arti-
cles. Given a topic and a hypothesis, entailment
systems are required to identify all sentences in
the topic?s corpus that entail the hypothesis.
Each sentence-hypothesis pair in both the de-
velopment and test sets was annotated, judging
whether the sentence entails the hypothesis. Out
of 20,104 annotations in the development set, only
810 were judged as positive. This small ratio (4%)
of positive examples, in comparison to 50% in tra-
ditional RTE tasks, better corresponds to the natu-
ral distribution of entailing texts in a corpus, thus
better simulates practical settings.
The task may seem as a variant of information
retrieval (IR), as it requires finding specific texts
in a corpus. Yet, it is fundamentally different from
IR for two reasons. First, the target output is a set
1http://opennlp.sourceforge.net
2http://www.nist.gov/tac/2009/Summarization/
771
of sentences, each evaluated independently, rather
than a set of documents. Second, the decision cri-
terion is entailment rather than relevance.
Despite the above, apparently, IR techniques
provided hard-to-beat baselines for the RTE
Search task (MacKinlay and Baldwin, 2009), out-
performing every other system that relied on in-
ference without IR-based pre-filtering. At the cur-
rent state of performance of entailment systems, it
seems that lexical coverage largely overshadows
any other approach in this task. Still, most (6 out
of 8) participants in the challenge applied their en-
tailment systems to the entire dataset without a
prior retrieval of candidate sentences. F1 scores
for such systems vary between 10% and 33%, in
comparison to over 40% of the IR-based methods.
4 The Baseline RTE System
In this work we used BIUTEE, Bar-Ilan Univer-
sity Textual Entailment Engine (Bar-Haim et al,
2008; Bar-Haim et al, 2009), a state of the art
RTE system, as a baseline and as a basis for our
discourse-based enhancements. This section de-
scribes this system?s architecture; the methods by
which it was augmented to address discourse are
presented in Section 5.
To determine entailment, BIUTEE performs the
following main steps:
Preprocessing First, all documents are parsed
and processed with standard tools for named en-
tity recognition (Finkel et al, 2005) and corefer-
ence resolution. For the latter purpose, we use
OpenNLP and enable the substitution of corefer-
ring terms. This is the only way by which BIUTEE
addresses discourse, representing the state of the
art in entailment systems.
Entailment-based transformations Given a
T-H pair (both represented as dependency
parse trees), the system applies a sequence of
knowledge-based entailment transformations over
T, generating a set of texts which are entailed by
it. The goal is to obtain consequent texts which
are more similar to H. Based on preliminary re-
sults on the development set, in our experiments
(Section 6) we use WordNet (Fellbaum, 1998) as
the system?s only knowledge resource, using its
synonymy, hyponymy and derivation relations.
Classification A supervised classifier, trained
on the development set, is applied to determine
entailment of each pair based on a set of syntactic
and lexical syntactic features assessing the degree
by which T and its consequents cover H.
5 Addressing Discourse
In the following subsections we describe the
prominent discourse phenomena that affect infer-
ence, which we have identified in an analysis of
the development set and addressed in our imple-
mentation. As mentioned, these phenomena are
poorly addressed by available reference resolvers
or fall completely out of their scope.
5.1 Augmented coreference set
A large number of coreference relations are com-
prised of terms which share lexical elements, (e.g.
?airliners?s first flight? and ?Airbus A380?s first
flight?). Although common in coreference rela-
tions, standard resolvers miss many of these cases.
For the purpose of identifying additional corefer-
ring terms, we consider two noun phrases in the
same document as coreferring if: (i) their heads
are identical and (ii) no semantic incompatibil-
ity is found between their modifiers. The types
of incompatibility we handle are: (a) mismatch-
ing numbers, (b) antonymy and (c) co-hyponymy
(coordinate terms), as specified by WordNet. For
example, two nodes of the noun distance would
be considered incompatible if one is modified by
short and the second by its antonym long. Simi-
larly, two modifier co-hyponyms of distance, such
as walking and running would also result such
an incompatibility. Adding more incompatibility
types (e.g. first vs. second flight) may further im-
prove the precision of this method.
5.2 Global information
Key terms or prominent pieces of information that
appear in the document, typically at the title or the
first few sentences, are many times perceived as
?globally? known throughout the document. For
example, the geographic location of the document
theme, mentioned at the beginning of the docu-
ment, is assumed to be known from that point on,
and will often not be mentioned explicitly in fur-
ther sentences. This is a bridging phenomenon
772
that is typically not addressed by available dis-
course processing tools. To compensate for that,
we identify key terms for each document based
on tf-idf scores and consider them as global in-
formation for that document. For example, global
terms for the topic discussing the ice melting in
the Arctic, typically contain a location such as
Arctic or Antarctica and terms referring to ice, like
permafrost or iceshelf.
We use a variant of tf-idf, where term frequency
is computed as follows: tf(ti,j) = ni,j+~?> ? ~fi,j .
Here, ni,j is the frequency of term i in document j
(ti,j), which is incremented by additional positive
weights (~?) for a set of features ( ~fi,j) of the term.
Based on our analysis, we defined the following
features, which correlated mostly with global in-
formation: (i) does the term appear in the title?
(ii) is it a proper name? (iii) is it a location? The
weights for these features are set empirically.
The document?s top-n global terms are added
to each of its sentences. As a result, a global term
that occurs in the hypothesis is matched in each
sentence of the document, regardless of whether
the term explicitly appears in the sentence.
Considering the previous sentence Another
method for addressing missing coreference and
bridging relations is based on the assumption that
adjacent sentences often refer to the same entities
and events. Thus, when extracting classification
features for a given sentence, in addition to the
features extracted from the parse tree of the sen-
tence itself, we extract the same set of features
from the current and previous sentences together.
Recall the example presented in Section 1. T is
annotated as entailing the hypothesis ?The AS-28
mini-submarine was trapped underwater?, but the
word submarine, e.g., appears only in its preced-
ing sentence T?. Thus, considering both sentences
together when classifying T increases its coverage
of the hypothesis. Indeed, a bridging reference re-
lates on board in T with submarine in T?, justify-
ing our assumption in this case.
5.3 Document-level classification
Beyond discourse references addressed above,
further information concerning discourse and doc-
ument structure is available in the Search setting
and may contribute to entailment classification.
We observed that entailing sentences tend to come
in bulks. This reflects a common coherence as-
pect, where the discussion of a specific topic is
typically continuous rather than scattered across
the entire document. This locality phenomenon
may be useful for entailment classification since
knowing that a sentence entails the hypothesis in-
creases the probability that adjacent sentences en-
tail the hypothesis as well.
To capture this phenomenon, we use a two-
phase meta-classification scheme, in which a
meta-classifier utilizes entailment classifications
of the first classification phase to extract meta-
features and determine the final classification de-
cision. This scheme also provides a convenient
way to combine scores from multiple classifiers
used in the first classification phase. We refer
to these as base-classifiers. This scheme and the
meta-features we used are detailed hereunder.
Let us write (s, h) for a sentence-hypothesis
pair. We denote the set of pairs in the development
(training) set asD and in the test set as T . We split
D into two halves, D1 and D2. We make use of n
base-classifiers, C1, . . . , Cn, among which C? is
a designated classifier with additional roles in the
process, as described below. Classifiers may dif-
fer, for example, in their classification algorithm.
An additional meta-classifier is denoted CM . The
classification scheme is shown as Algorithm 1.
Algorithm 1 Meta-classification
Training
1: Extract features for every (s, h) in D
2: Train C1, . . . , Cn on D1
3: Classify D2, using C1, . . . , Cn
4: Extract meta-features for D2 using the
classification of C1, . . . , Cn
5: Train CM on D2
Classification
6: Extract features for every (s, h) in T
7: Classify T using C1, . . . , Cn
8: Extract meta-features for T
9: Classify T using CM
At Step 1, features are extracted for every (s, h)
pair in the training set, as in the baseline system.
773
In Steps 2 and 3 we split the training set into two
halves (taking half of each topic), train n different
classifiers on the first half and then classify the
second half using each of the n classifiers. Given
the classification scores of the n base-classifiers
to the (s, h) pairs in the second half of the train-
ing set, D2, we add in Step 4 the meta-features
described in Section 5.3.1.
After adding the meta-features, we train
(Step 5) a meta-classifier on this new set of fea-
tures. Test sentences then go through the same
process: features are extracted for them and they
are classified by the already trained n classifiers
(Steps 6 and 7), meta-features are extracted in
Step 8, and a final classification decision is made
by the meta-classifier in Step 9.
A retrieval step may precede the actual en-
tailment classification, allowing the processing of
fewer and potentially ?better? candidates.
5.3.1 Meta-features
The following features are extracted in our
meta-classification scheme:
Classification scores The classification score of
each of the n base-classifiers.
Title entailment In many texts, and in news ar-
ticles in particular, the title and the first few sen-
tences often represent the entire document?s con-
tent. Thus, knowing whether these sentences en-
tail the hypothesis may be an indicator to the gen-
eral potential of the document to include entailing
sentences. Two binary features are added accord-
ing to the classification of C? indicating whether
the title entails the hypothesis and whether the first
sentence entails it.
Second-closest entailment Considering the lo-
cality phenomenon described above, we add a fea-
ture assigning higher scores to sentences in the
vicinity of an entailment environment. This fea-
ture is computed as the distance to the second-
closest entailing sentence in the document (count-
ing the sentence itself as well), according to the
classification ofC?. Formally, let i be the index of
the current sentence and J be the set of indices of
entailing sentences in the document according to
C?. For each j ? J we compute di,j = |i?j|, and
choose the second smallest di,j as di. The idea is
Ent?
# 1110987654321
NO NOYESYESYESYESNONONONONO
d
2nd cl
oses
t
8887 or 96 or 8777777
2 3111123456
d
Close
st
8887 or 96 or 8766666
1 2111112345
Figure 1: Comparison of the closest and second-closest
schemes when applied to a bulk of entailing sentences (in
white) situated within a non-entailing environment (in gray).
Unlike the closest one, the second-closest scheme assigns
larger distance values to non-entailing sentences located on
the ?edge? of the bulk (5 and 10) than to entailing ones.
that if entailing sentences indeed always come in
bulks, then di = 1 for all entailing sentences, but
di > 1 for all non-entailing ones. Figure 1 illus-
trates such a case, comparing the second-closest
distance with the distance to the closest entailing
sentence. In the closest scheme we do not count
the sentence as closest to itself since it would dis-
regard the environment of the sentence altogether,
eliminating the desired effect. We scale the dis-
tance and add the feature score: ? log(di).
Smoothed entailment This feature addressed
the locality phenomenon by smoothing the
classification score of sentence i with the scores
of adjacent sentences, weighted by their distance
from the current sentence i. Let s(i) be the
score assigned by C? to sentence i. We add the
Smoothed Entailment feature score:
SE(i) =
?
w(b|w|?s(i+w))?
w(b|w|)
where 0 < b < 1 is the decay parameter and w is
an integer bounded between?N and N , denoting
the distance from sentence i.
1st sentence entailing title Bensley and Hickl
(2008) showed that the first sentence in a news ar-
ticle typically entails the article?s title. We there-
fore assume that in each document, s1 ? s0,
where s1 and s0 are the document?s first sentence
and title respectively. Hence, under entailment
transitivity, if s0 ? h then s1 ? h. The cor-
responding binary feature states whether the sen-
tence being classified is the document?s first sen-
tence and the title entails h according to C?.
774
P (%) R (%) F1 (%)
BIU-BL 14.53 55.25 23.00
BIU-DISC 20.82 57.25 30.53
BIU-BL3 14.86 59.00 23.74
BIU-DISCno?loc 22.35 57.12 32.13All-yes baseline 4.6 100.0 8.9
Table 1: Micro-average results.
Note that the above locality-based features rely
on high accuracy of the base classifier C?. Oth-
erwise, it will provide misleading information to
the features computation. We analyze the effect
of this accuracy in Section 6.
6 Results and Analysis
Using the RTE-5 Search data, we compare
BIUTEE in its baseline configuration (cf. Sec-
tion 4), denoted BIU-BL, with its discourse-aware
enhancement (BIU-DISC) which uses all the com-
ponents described in Section 5. To alleviate the
strong IR effect described in Section 3, both sys-
tems are applied to the complete datasets (both
training and test), without candidates pre-filtering.
BIU-DISC uses three base-classifiers (n = 3):
SVMperf (Joachims, 2006), and Na??ve Bayes and
Logistic Regression from the WEKA package
(Witten and Frank, 2005). The first among these
is set as our designated classifier C?, which is
used for the computation of the document-level
features. SVMperf is also used for the meta-
classifier. For the smoothed entailment score (cf.
Section 5.3), we used b = 0.9 and N = 3. Global
information is added by enriching each sentence
with the highest-ranking term in the document, ac-
cording to tf-idf scores (cf. Section 5.2), where
document frequencies were computed based on
about half a million documents from the TIP-
STER corpus (Harman, 1992). The set of weights
~? equals {2, 1, 4} for title terms, proper names and
locations, respectively. All parameters were tuned
based on a 10-fold cross-validation on the devel-
opment set, optimizing the micro-averaged F1.
The results are presented in Table 1. As can be
seen in the table, BIU-DISC outperforms BIU-BL in
every measure, showing the impact of addressing
discourse in this setting. To rule out the option that
the improvement is simply due to the fact that we
use three classifiers for BIU-DISC and a single one
P (%) R (%) F1 (%)
By Topic
BIU-BL 16.54 55.62 25.50
BIU-DISC 22.69 57.96 32.62
All-yes baseline 4.85 100.00 9.25
By Hypothesis
BIU-BL 22.87 59.62 33.06
BIU-DISC 27.81 61.97 38.39
All-yes baseline 4.96 100.00 9.46
Table 2: Macro-average results.
for BIU-BL, we show (BIU-BL3) the results when
the baseline system is applied in the same meta-
classification configuration as BIU-DISC, with the
same three classifiers. Apparently, without the
discourse information this configuration?s contri-
bution is limited.
As mentioned in Section 5.3, the benefit from
the locality features rely directly on the perfor-
mance of the base classifiers. Hence, considering
the low precision scores obtained here, we applied
BIU-DISC to the data in the meta-classification
scheme, but with locality features removed. The
results, shown as BIU-DISCno?loc in the Table, in-
dicate that indeed performance increases without
these features. The last line of the table shows the
results obtained by a na??ve baseline where all test-
set pairs are considered entailing.
For completeness, Table 2 shows the macro-
averaged results, when averaged over the topics or
over the hypotheses. Although we tuned our sys-
tem to maximize micro-averaged F1, these figures
comply with the ones shown in Table 1.
Analysis of locality As discussed in Section 5,
determining whether a sentence entails a hypothe-
sis should take into account whether adjacent sen-
tences also entail the hypothesis. In the above ex-
periment we were unable to show the contribution
of our system?s component that attempts to cap-
ture this information; on the contrary, the results
show it had a negative impact on performance.
Still, we claim that this information can be use-
ful when used within a more accurate system. We
try to validate this conjecture by understanding
how performance of the locality features varies as
the systems becomes more accurate. We do so via
the following simulation.
When classifying a certain sentence, the classi-
775
2025303540 0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
p
F1
Figure 2: F1 performance of BIU-DISC as a function of
the accuracy in classifying adjacent sentences.
fications of its adjacent sentences are given by an
oracle classifier that provides the correct answer
with probability p. The system is applied using
two locality features: the 1st sentence entailing
title feature and a close variant of the smoothed
entailment feature, which calculates the weighted
average of adjacent sentences, but disregards the
score of the currently evaluated sentence.3 Thus
we supply information about adjacent sentences
and test whether overall performance increases
with the accuracy of this information.
We performed this experiment for p in a range
of [0.5-1.0]. Figure 2 shows the results of this sim-
ulation, based on the average F1 of five runs for
each p. Since performance, from a certain point,
increases with the accuracy of the oracle classi-
fier, we can conclude that indeed precise infor-
mation about adjacent sentences improves perfor-
mance on the current sentence, and that locality is
a true phenomenon in the data. We note, however,
that performance improves only when accuracy is
very high, suggesting the currently limited prac-
tical potential of this information, at least in the
way locality was represented in this work.
Ablation tests Table 3 presents the results of the
ablation tests performed to evaluate the contribu-
tion of each component. Based on the result re-
ported in Table 1 and the above discussion, the
tests were performed relative to BIU-DISCno?loc,
the optimal configuration. As seen in the table,
the removal of each component causes a drop
in results. For global information we see a mi-
3The second-closest entailment feature was not used as it
considers the oracle?s decision for the current sentence, while
we wish to use only information about adjacent sentences.
Component removed F1 (%) ?F1 (%)
Previous sent. features 28.55 3.58
Augmented coref. 26.73 5.40
Global information 31.76 0.37
Table 3: Results of ablation tests relative to
BIU-DISCno?loc. The columns specify the compo-nent removed, the micro-averaged F1 score achieved without
it, and the marginal contribution of the component.
nor difference, which is not surprising considering
the conservative approach we took, using a sin-
gle global term for each sentence. Possibly, this
information is also included in the other compo-
nents, thus proving no marginal contribution rel-
ative to them. Under the conditions of an over-
whelming majority of negative examples, this is
a risky method to use, and should be considered
when the ratio of positive examples is higher. For
future work, we intend to use this information via
classification features (e.g. the coverage obtained
with and without global information), rather than
the crude addition of the term to the sentence.
Analysis of augmented coreferences We an-
alyzed the performance of the component for
augmenting coreference relations relative to the
OpenNLP resolver. Recall that our component
works on top of the resolver?s output and can add
or remove coreference relations. As a complete
annotation of coreference chains in the dataset is
unavailable, we performed the following evalua-
tion. Recall is computed based on the number
of identified pairs from a sample of 100 intra-
document coreference and bridging relations from
the annotated dataset described in (Mirkin et al,
2010). Precision is computed based on 50 pairs
sampled from the output of each method, equally
distributed over topics. The results, shown in Ta-
ble 4, indicate the much higher recall obtained
by our component at some cost in precision. Al-
though rather simple, the ablation test of this com-
ponent shows its usefulness. Still, both methods
achieve low absolute recall, suggesting the need
for more robust tools for this task.
P (%) R (%) F1 (%)
OpenNLP 74 16 26.3
Augmented coref. 60 28 38.2
Table 4: Performance of coreference methods.
776
05101520253035404550 0
10
20
30
40
50
60
70
80
90
100
k
F1
BIU-B
L
BIU-D
ISC
Lucen
e
Figure 3: F1 performance as a function of the number of
retrieved candidates.
Candidate retrieval setting As mentioned in
Section 3, best performance of RTE systems in the
task was obtained when applying a first step of IR-
based candidate filtering. We therefore compare
the performance of BIU-DISC with that of BIU-BL
under this setting as well.4 For candidate retrieval
we used Lucene, a state of the art search engine5,
in a range of top-k retrieved candidates. The re-
sults are shown in Figure 3. For reference, the fig-
ure also shows the performance along this range
of Lucene as-is, when no further inference is ap-
plied to the retrieved candidates.
While BIU-DISC does not outperform BIU-BL at
every point, the area under the curve is clearly
larger for BIU-DISC. The figure also indicates that
BIU-DISC is far more robust, maintaining a stable
F1 and enabling a stable tradeoff between recall
and precision along the whole range (recall ranges
between 42% and 55% for k ? [15 ? 100], with
corresponding precision range of 51% to 33%).
Finally, Table 5 shows the results of the best
systems as determined in our first experiment.
We performed a single experiment to compare
BIU-DISCno?loc and BIU-BL3 under a candidate re-
trieval setting, using k = 20, where both systems
highly perform. We compare these results to the
highest score obtained by Lucene, as well as to the
two best submissions to the RTE-5 Search task6.
BIU-DISCno?loc outperforms all other methods and
its result is significantly better than BIU-BL3 with
p < 0.01 according to McNemar?s test.
4This time, for global information, the document?s three
highest ranking terms were added to each sentence.
5http://lucene.apache.org
6The best one is an earlier version of this work (Mirkin et
al., 2009); the second is MacKinlay and Baldwin?s (2009).
P (%) R (%) F1 (%)
BIU-DISCno?loc 50.77 45.12 47.78
BIU-BL3 51.68 40.38 45.33
Lucene, top-15 35.93 52.50 42.66
RTE-5 best 40.98 51.38 45.59
RTE-5 second-best 42.94 38.00 40.32
Table 5: Performance of best configurations.
7 Conclusions
While it is generally assumed that discourse inter-
acts with semantic entailment inference, the con-
crete impacts of discourse on such inference have
been hardly explored. This paper presented a first
empirical investigation of discourse processing
aspects related to entailment. We argue that avail-
able discourse processing tools should be substan-
tially improved towards this end, both in terms of
the phenomena they address today, namely nom-
inal coreference, and with respect to the cover-
ing of additional phenomena, such as bridging
anaphora. Our experiments show that even rather
simple methods for addressing discourse can have
a substantial positive impact on the performance
of entailment inference. Concerning the local-
ity phenomenon stemming from discourse coher-
ence, we learned that it does carry potentially use-
ful information, which might become beneficial
in the future when better-performing entailment
systems become available. Until then, integrating
this information with entailment confidence may
be useful. Overall, we suggest that entailment sys-
tems should extensively incorporate discourse in-
formation, while developing sound algorithms for
addressing various discourse phenomena, includ-
ing the ones described in this paper.
Acknowledgements
The authors are thankful to Asher Stern and Ilya
Kogan for their help in implementing and evalu-
ating the augmented coreference component, and
to Roy Bar-Haim for useful advice concerning
this paper. This work was partially supported
by the Israel Science Foundation grant 1112/08
and the PASCAL-2 Network of Excellence of the
European Community FP7-ICT-2007-1-216886.
Jonathan Berant is grateful to the Azrieli Foun-
dation for the award of an Azrieli Fellowship.
777
References
Bar-Haim, Roy, Jonathan Berant, Ido Dagan, Iddo
Greental, Shachar Mirkin, Eyal Shnarch, and Idan
Szpektor. 2008. Efficient semantic deduction and
approximate matching over compact parse forests.
In Proc. of Text Analysis Conference (TAC).
Bar-Haim, Roy, Jonathan Berant, and Ido Dagan.
2009. A compact forest for scalable inference
over entailment and paraphrase rules. In Proc. of
EMNLP.
Bensley, Jeremy and Andrew Hickl. 2008. Unsuper-
vised resource creation for textual inference appli-
cations. In Proc. of LREC.
Bentivogli, Luisa, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, Medea Lo Leggio, and Bernardo
Magnini. 2009a. Considering discourse refer-
ences in textual entailment annotation. In Proc. of
the 5th International Conference on Generative Ap-
proaches to the Lexicon (GL2009).
Bentivogli, Luisa, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009b. The
fifth PASCAL recognizing textual entailment chal-
lenge. In Proc. of TAC.
Castillo, Julio J. 2009. Sagan in TAC2009: Using
support vector machines in recognizing textual en-
tailment and TE search pilot task. In Proc. of TAC.
Clark, Peter and Phil Harrison. 2009. An inference-
based approach to recognizing entailment. In Proc.
of TAC.
Clark, Herbert H. 1975. Bridging. In Schank, R. C.
and B. L. Nash-Webber, editors, Theoretical issues
in natural language processing, pages 169?174. As-
sociation of Computing Machinery.
Dagan, Ido, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In Machine Learning Challenges, vol-
ume 3944 of Lecture Notes in Computer Science,
pages 177?190. Springer.
Dagan, Ido, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2009. Recognizing textual entailment: Ratio-
nal, evaluation and approaches. Natural Language
Engineering, pages 15(4):1?17.
Dali, Lorand, Delia Rusu, Blaz Fortuna, Dunja
Mladenic, and Marko Grobelnik. 2009. Question
answering based on semantic graphs. In Proc. of the
Workshop on Semantic Search (SemSearch 2009).
Fellbaum, Christiane, editor. 1998. WordNet: An
Electronic Lexical Database (Language, Speech,
and Communication). The MIT Press.
Finkel, Jenny Rose, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proc. of ACL.
Giampiccolo, Danilo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third PASCAL recog-
nizing textual entailment challenge. In Proc. of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing.
Harabagiu, Sanda and Andrew Hickl. 2006. Methods
for using textual entailment in open-domain ques-
tion answering. In Proc. of ACL.
Harman, Donna. 1992. The DARPA TIPSTER
project. SIGIR Forum, 26(2):26?28.
Joachims, Thorsten. 2006. Training linear SVMs in
linear time. In Proc. of the ACM Conference on
Knowledge Discovery and Data Mining (KDD).
Li, Fangtao, Yang Tang, Minlie Huang, and Xiaoyan
Zhu. 2009. Answering opinion questions with ran-
dom walks on graphs. In Proc. of ACL-IJCNLP.
Litkowski, Ken. 2009. Overlap analysis in textual en-
tailment recognition. In Proc. of TAC.
MacKinlay, Andrew and Timothy Baldwin. 2009. A
baseline approach to the RTE5 search pilot. In Proc.
of TAC.
Mirkin, Shachar, Roy Bar-Haim, Jonathan Berant, Ido
Dagan Eyal Shnarch, Asher Stern, and Idan Szpek-
tor. 2009. Addressing discourse and document
structure in the RTE search task. In Proc. of TAC.
Mirkin, Shachar, Ido Dagan, and Sebastian Pado?.
2010. Assessing the role of discourse references in
entailment inference. In Proc. of ACL.
Nenkova, Ani, Rebecca Passonneau, and Kathleen
Mckeown. 2007. The pyramid method: incorpo-
rating human content selection variation in summa-
rization evaluation. In ACM Transactions on Speech
and Language Processing.
Qiu, Long, Min-Yen Kan, and Tat-Seng Chua. 2004.
A public reference implementation of the RAP
anaphora resolution algorithm. In Proc. of LREC.
Romano, Lorenza, Milen Kouylekov, Idan Szpektor,
and Ido Dagan. 2006. Investigating a generic
paraphrase-based approach for relation extraction.
In Proc. of EACL.
Witten, Ian H. and Eibe Frank. 2005. Data Mining:
Practical machine learning tools and techniques,
2nd Edition. Morgan Kaufmann, San Francisco.
778
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 558?563,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Probabilistic Modeling Framework for Lexical Entailment
Eyal Shnarch
Computer Science Department
Bar-Ilan University
Ramat-Gan, Israel
shey@cs.biu.ac.il
Jacob Goldberger
School of Engineering
Bar-Ilan University
Ramat-Gan, Israel
goldbej@eng.biu.ac.il
Ido Dagan
Computer Science Department
Bar-Ilan University
Ramat-Gan, Israel
dagan@cs.biu.ac.il
Abstract
Recognizing entailment at the lexical level is
an important and commonly-addressed com-
ponent in textual inference. Yet, this task has
been mostly approached by simplified heuris-
tic methods. This paper proposes an initial
probabilistic modeling framework for lexical
entailment, with suitable EM-based parame-
ter estimation. Our model considers promi-
nent entailment factors, including differences
in lexical-resources reliability and the impacts
of transitivity and multiple evidence. Evalu-
ations show that the proposed model outper-
forms most prior systems while pointing at re-
quired future improvements.
1 Introduction and Background
Textual Entailment was proposed as a generic
paradigm for applied semantic inference (Dagan et
al., 2006). This task requires deciding whether a tex-
tual statement (termed the hypothesis-H) can be in-
ferred (entailed) from another text (termed the text-
T ). Since it was first introduced, the six rounds
of the Recognizing Textual Entailment (RTE) chal-
lenges1, currently organized under NIST, have be-
come a standard benchmark for entailment systems.
These systems tackle their complex task at vari-
ous levels of inference, including logical represen-
tation (Tatu and Moldovan, 2007; MacCartney and
Manning, 2007), semantic analysis (Burchardt et al,
2007) and syntactic parsing (Bar-Haim et al, 2008;
Wang et al, 2009). Inference at these levels usually
1http://www.nist.gov/tac/2010/RTE/index.html
requires substantial processing and resources (e.g.
parsing) aiming at high performance.
Nevertheless, simple entailment methods, per-
forming at the lexical level, provide strong baselines
which most systems did not outperform (Mirkin
et al, 2009; Majumdar and Bhattacharyya, 2010).
Within complex systems, lexical entailment model-
ing is an important component. Finally, there are
cases in which a full system cannot be used (e.g.
lacking a parser for a targeted language) and one
must resort to the simpler lexical approach.
While lexical entailment methods are widely
used, most of them apply ad hoc heuristics which do
not rely on a principled underlying framework. Typ-
ically, such methods quantify the degree of lexical
coverage of the hypothesis terms by the text?s terms.
Coverage is determined either by a direct match of
identical terms in T and H or by utilizing lexi-
cal semantic resources, such as WordNet (Fellbaum,
1998), that capture lexical entailment relations (de-
noted here as entailment rules). Common heuristics
for quantifying the degree of coverage are setting a
threshold on the percentage coverage of H?s terms
(Majumdar and Bhattacharyya, 2010), counting ab-
solute number of uncovered terms (Clark and Har-
rison, 2010), or applying an Information Retrieval-
style vector space similarity score (MacKinlay and
Baldwin, 2009). Other works (Corley and Mihal-
cea, 2005; Zanzotto and Moschitti, 2006) have ap-
plied a heuristic formula to estimate the similarity
between text fragments based on a similarity func-
tion between their terms.
These heuristics do not capture several important
aspects of entailment, such as varying reliability of
558
entailment resources and the impact of rule chaining
and multiple evidence on entailment likelihood. An
additional observation from these and other systems
is that their performance improves only moderately
when utilizing lexical resources2.
We believe that the textual entailment field would
benefit from more principled models for various en-
tailment phenomena. Inspired by the earlier steps
in the evolution of Statistical Machine Translation
methods (such as the initial IBM models (Brown et
al., 1993)), we formulate a concrete generative prob-
abilistic modeling framework that captures the basic
aspects of lexical entailment. Parameter estimation
is addressed by an EM-based approach, which en-
ables estimating the hidden lexical-level entailment
parameters from entailment annotations which are
available only at the sentence-level.
While heuristic methods are limited in their abil-
ity to wisely integrate indications for entailment,
probabilistic methods have the advantage of be-
ing extendable and enabling the utilization of well-
founded probabilistic methods such as the EM algo-
rithm.
We compared the performance of several model
variations to previously published results on RTE
data sets, as well as to our own implementation
of typical lexical baselines. Results show that
both the probabilistic model and our percentage-
coverage baseline perform favorably relative to prior
art. These results support the viability of the proba-
bilistic framework while pointing at certain model-
ing aspects that need to be improved.
2 Probabilistic Model
Under the lexical entailment scope, our modeling
goal is obtaining a probabilistic score for the like-
lihood that all H?s terms are entailed by T. To that
end, we model prominent aspects of lexical entail-
ment, which were mostly neglected by previous lex-
ical methods: (1) distinguishing different reliabil-
ity levels of lexical resources; (2) allowing transi-
tive chains of rule applications and considering their
length when estimating their validity; and (3) con-
sidering multiple entailments when entailing a term.
2See ablation tests reports in http://aclweb.org/aclwiki/ in-
dex.php?title=RTE Knowledge Resources#Ablation Tests
cha
in
t 1
t?
Re
so
urc
e 2
t n
h 1
h i
h m
t j
Tex
t:
Hyp
oth
es
is:
.
 
.
 
.
Re
so
urc
e 1
.
 
.
 
.
.
 
.
 
.MA
TC
H
Re
so
urc
e 1
.
 
.
 
.
Re
so
urc
e 3
Figure 1: The generative process of entailing terms of a hy-
pothesis from a text. Edges represent entailment rules. There
are 3 evidences for the entailment of hi: a rule from Resource1,
another one from Resource3 both suggesting that tj entails it,
and a chain from t1 through an intermediate term t?.
2.1 Model Description
For T to entail H it is usually a necessary, but not
sufficient, that every term h ? H would be en-
tailed by at least one term t ? T (Glickman et al,
2006). Figure 1 describes the process of entailing
hypothesis terms. The trivial case is when identical
terms, possibly at the stem or lemma level, appear
in T and H (a direct match as tn and hm in Fig-
ure 1). Alternatively, we can establish entailment
based on knowledge of entailing lexical-semantic
relations, such as synonyms, hypernyms and mor-
phological derivations, available in lexical resources
(e.g the rule inference? reasoning from WordNet).
We denote by R(r) the resource which provided the
rule r.
Since entailment is a transitive relation, rules may
compose transitive chains that connect a term t ? T
to a term h ? H through intermediate terms. For
instance, from the rules infer? inference and infer-
ence ? reasoning we can deduce the rule infer ?
reasoning (were inference is the intermediate term
as t? in Figure 1).
Multiple chains may connect t to h (as for tj and
hi in Figure 1) or connect several terms in T to h
(as t1 and tj are indicating the entailment of hi in
Figure 1), thus providing multiple evidence for h?s
entailment. It is reasonable to expect that if a term t
indeed entails a term h, it is likely to find evidences
for this relation in several resources.
Taking a probabilistic perspective, we assume a
559
parameter ?R for each resource R, denoting its re-
liability, i.e. the prior probability that applying a
rule from R corresponds to a valid entailment in-
stance. Direct matches are considered as a special
?resource?, called MATCH, for which ?MATCH is ex-
pected to be close to 1.
We now present our probabilistic model. For a
text term t ? T to entail a hypothesis term h by a
chain c, denoted by t
c
?? h, the application of every
r ? c must be valid. Note that a rule r in a chain c
connects two terms (its left-hand-side and its right-
hand-side, denoted lhs ? rhs). The lhs of the first
rule in c is t ? T and the rhs of the last rule in it is
h ? H . We denote the event of a valid rule applica-
tion by lhs
r
?? rhs. Since a-priori a rule r is valid
with probability ?R(r), and assuming independence
of all r ? c, we obtain Eq. 1 to specify the prob-
ability of the event t
c
?? h. Next, let C(h) denote
the set of chains which suggest the entailment of h.
The probability that T does not entail h at all (by
any chain), specified in Eq. 2, is the probability that
all these chains are not valid. Finally, the probabil-
ity that T entails all of H , assuming independence
of H?s terms, is the probability that every h ? H is
entailed, as given in Eq. 3. Notice that there could
be a term h which is not covered by any available
rule chain. Under this formulation, we assume that
each such h is covered by a single rule coming from
a special ?resource? called UNCOVERED (expecting
?UNCOVERED to be relatively small).
p(t
c
?? h) =
?
r?c
p(lhs
r
?? rhs) =
?
r?c
?R(r)(1)
p(T 9 h) =
?
c?C(h)
[1? p(t
c
?? h)] (2)
p(T ? H) =
?
h?H
p(T ? h) (3)
As can be seen, our model indeed distinguishes
varying resource reliability, decreases entailment
probability as rule chains grow and increases it when
entailment of a term is supported by multiple chains.
The above treatment of uncovered terms in H ,
as captured in Eq. 3, assumes that their entailment
probability is independent of the rest of the hypoth-
esis. However, when the number of covered hypoth-
esis terms increases the probability that the remain-
ing terms are actually entailed by T increases too
(even though we do not have supporting knowledge
for their entailment). Thus, an alternative model is
to group all uncovered terms together and estimate
the overall probability of their joint entailment as a
function of the lexical coverage of the hypothesis.
We denote Hc as the subset of H?s terms which are
covered by some rule chain and Huc as the remain-
ing uncovered part. Eq. 3a then provides a refined
entailment model for H , in which the second term
specifies the probability that Huc is entailed given
that Hc is validly entailed and the corresponding
lengths:
p(T?H) = [
?
h?Hc
p(T?h)]?p(T?Huc | |Hc|,|H|)
(3a)
2.2 Parameter Estimation
The difficulty in estimating the ?R values is that
these are term-level parameters while the RTE-
training entailment annotation is given for the
sentence-level. Therefore, we use EM-based esti-
mation for the hidden parameters (Dempster et al,
1977). In the E step we use the current ?R values
to compute all whcr(T,H) values for each training
pair. whcr(T,H) stands for the posterior probability
that application of the rule r in the chain c for h ? H
is valid, given that either T entails H or not accord-
ing to the training annotation (see Eq. 4). Remember
that a rule r provides an entailment relation between
its left-hand-side (lhs) and its right-hand-side (rhs).
Therefore Eq. 4 uses the notation lhs
r
?? rhs to des-
ignate the application of the rule r (similar to Eq. 1).
E :
whcr(T,H) =
?
????????
????????
p(lhs
r
?? rhs|T ? H) =
p(T?H|lhs
r??rhs)p(lhs r??rhs)
p(T?H)
if T ? H
p(lhs
r
?? rhs|T 9 H) =
p(T9H|lhs
r??rhs)p(lhs r??rhs)
p(T9H)
if T 9 H
(4)
After applying Bayes? rule we get a fraction with
Eq. 3 in its denominator and ?R(r) as the second term
of the numerator. The first numerator term is defined
as in Eq. 3 except that for the corresponding rule ap-
plication we substitute ?R(r) by 1 (per the condition-
ing event). The probabilistic model defined by Eq.
1-3 is a loop-free directed acyclic graphical model
560
(aka a Bayesian network). Hence the E-step proba-
bilities can be efficiently calculated using the belief
propagation algorithm (Pearl, 1988).
The M step uses Eq. 5 to update the parameter set.
For each resourceR we average thewhcr(T,H) val-
ues for all its rule applications in the training, whose
total number is denoted nR.
M : ?R =
1
nR
?
T,H
?
h?H
?
c?C(h)
?
r?c|R(r)=R
whcr(T,H)
(5)
For Eq. 3a we need to estimate also p(T?Huc |
|Hc|,|H|). This is done directly via maximum likeli-
hood estimation over the training set, by calculating
the proportion of entailing examples within the set
of all examples of a given hypothesis length (|H|)
and a given number of covered terms (|Hc|). As
|Hc| we take the number of identical terms in T and
H (exact match) since in almost all cases terms in
H which have an exact match in T are indeed en-
tailed. We also tried initializing the EM algorithm
with these direct estimations but did not obtain per-
formance improvements.
3 Evaluations and Results
The 5th Recognizing Textual Entailment challenge
(RTE-5) introduced a new search task (Bentivogli
et al, 2009) which became the main task in RTE-
6 (Bentivogli et al, 2010). In this task participants
should find all sentences that entail a given hypothe-
sis in a given document cluster. This task?s data sets
reflect a natural distribution of entailments in a cor-
pus and demonstrate a more realistic scenario than
the previous RTE challenges.
In our system, sentences are tokenized and
stripped of stop words and terms are lemmatized and
tagged for part-of-speech. As lexical resources we
use WordNet (WN) (Fellbaum, 1998), taking as en-
tailment rules synonyms, derivations, hyponyms and
meronyms of the first senses of T and H terms, and
the CatVar (Categorial Variation) database (Habash
and Dorr, 2003). We allow rule chains of length up
to 4 in WordNet (WN4).
We compare our model to two types of baselines:
(1) RTE published results: the average of the best
runs of all systems, the best and second best per-
forming lexical systems and the best full system of
each challenge; (2) our implementation of lexical
coverage model, tuning the percentage-of-coverage
threshold for entailment on the training set. This
model uses the same configuration as our probabilis-
tic model. We also implemented an Information Re-
trieval style baseline3 (both with and without lex-
ical expansions), but given its poorer performance
we omit its results here.
Table 1 presents the results. We can see that
both our implemented models (probabilistic and
coverage) outperform all RTE lexical baselines on
both data sets, apart from (Majumdar and Bhat-
tacharyya, 2010) which incorporates additional lex-
ical resources, a named entity recognizer and a
co-reference system. On RTE-5, the probabilis-
tic model is comparable in performance to the best
full system, while the coverage model achieves con-
siderably better results. We notice that our imple-
mented models successfully utilize resources to in-
crease performance, as opposed to typical smaller
or less consistent improvements in prior works (see
Section 1).
Model
F1%
RTE-5 RTE-6
R
T
E
avg. of all systems 30.5 33.8
2nd best lexical system 40.31 44.02
best lexical system 44.43 47.64
best full system 45.63 48.05
co
ve
ra
ge
no resource 39.5 44.8
+ WN 45.8 45.1
+ CatVar 47.2 45.5
+ WN + CatVar 48.5 44.7
+ WN4 46.3 43.1
pr
ob
ab
il
is
ti
c no resource 41.8 42.1
+ WN 45.0 45.3
+ CatVar 42.0 45.9
+ WN + CatVar 42.8 45.5
+ WN4 45.8 42.6
Table 1: Evaluation results on RTE-5 and RTE-6. RTE systems
are: (1)(MacKinlay and Baldwin, 2009), (2)(Clark and Harri-
son, 2010), (3)(Mirkin et al, 2009)(2 submitted runs), (4)(Ma-
jumdar and Bhattacharyya, 2010) and (5)(Jia et al, 2010).
While the probabilistic and coverage models are
comparable on RTE-6 (with non-significant advan-
tage for the former), on RTE-5 the latter performs
3Utilizing Lucene search engine (http://lucene.apache.org)
561
better, suggesting that the probabilistic model needs
to be further improved. In particular, WN4 performs
better than the single-step WN only on RTE-5, sug-
gesting the need to improve the modeling of chain-
ing. The fluctuations over the data sets and impacts
of resources suggest the need for further investiga-
tion over additional data sets and resources. As for
the coverage model, under our configuration it poses
a bigger challenge for RTE systems than perviously
reported baselines. It is thus proposed as an easy to
implement baseline for future entailment research.
4 Conclusions and Future Work
This paper presented, for the first time, a principled
and relatively rich probabilistic model for lexical en-
tailment, amenable for estimation of hidden lexical-
level parameters from standard sentence-level an-
notations. The positive results of the probabilistic
model compared to prior art and its ability to exploit
lexical resources indicate its future potential. Yet,
further investigation is needed. For example, analyz-
ing current model?s limitations, we observed that the
multiplicative nature of eqs. 1 and 3 (reflecting inde-
pendence assumptions) is too restrictive, resembling
a logical AND. Accordingly we plan to explore re-
laxing this strict conjunctive behavior through mod-
els such as noisy-AND (Pearl, 1988). We also in-
tend to explore the contribution of our model, and
particularly its estimated parameter values, within a
complex system that integrates multiple levels of in-
ference.
Acknowledgments
This work was partially supported by the NEGEV
Consortium of the Israeli Ministry of Industry,
Trade and Labor (www.negev-initiative.org), the
PASCAL-2 Network of Excellence of the European
Community FP7-ICT-2007-1-216886, the FIRB-
Israel research project N. RBIN045PXH and by the
Israel Science Foundation grant 1112/08.
References
Roy Bar-Haim, Jonathan Berant, Ido Dagan, Iddo Green-
tal, Shachar Mirkin, Eyal Shnarch, and Idan Szpektor.
2008. Efficient semantic deduction and approximate
matching over compact parse forests. In Proceedings
of Text Analysis Conference (TAC).
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The fifth
PASCAL recognizing textual entailment challenge. In
Proceedings of Text Analysis Conference (TAC).
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. The sixth
PASCAL recognizing textual entailment challenge. In
Proceedings of Text Analysis Conference (TAC).
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2):263?311,
June.
Aljoscha Burchardt, Nils Reiter, Stefan Thater, and
Anette Frank. 2007. A semantic approach to textual
entailment: System evaluation and task analysis. In
Proceedings of the ACL-PASCAL Workshop on Textual
Entailment and Paraphrasing.
Peter Clark and Phil Harrison. 2010. BLUE-Lite: a
knowledge-based lexical entailment system for RTE6.
In Proceedings of Text Analysis Conference (TAC).
Courtney Corley and Rada Mihalcea. 2005. Measur-
ing the semantic similarity of texts. In Proceedings of
the ACLWorkshop on Empirical Modeling of Semantic
Equivalence and Entailment.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In Lecture Notes in Computer Science, vol-
ume 3944, pages 177?190.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the royal statistical society, se-
ries [B], 39(1):1?38.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and Com-
munication). The MIT Press.
Oren Glickman, Eyal Shnarch, and Ido Dagan. 2006.
Lexical reference: a semantic matching subtask. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 172?179. As-
sociation for Computational Linguistics.
Nizar Habash and Bonnie Dorr. 2003. A categorial vari-
ation database for english. In Proceedings of the North
American Association for Computational Linguistics.
Houping Jia, Xiaojiang Huang, Tengfei Ma, Xiaojun
Wan, and Jianguo Xiao. 2010. PKUTM participa-
tion at TAC 2010 RTE and summarization track. In
Proceedings of Text Analysis Conference (TAC).
Bill MacCartney and Christopher D. Manning. 2007.
Natural logic for textual inference. In Proceedings
of the ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing.
562
Andrew MacKinlay and Timothy Baldwin. 2009. A
baseline approach to the RTE5 search pilot. In Pro-
ceedings of Text Analysis Conference (TAC).
Debarghya Majumdar and Pushpak Bhattacharyya.
2010. Lexical based text entailment system for main
task of RTE6. In Proceedings of Text Analysis Confer-
ence (TAC).
Shachar Mirkin, Roy Bar-Haim, Jonathan Berant, Ido
Dagan, Eyal Shnarch, Asher Stern, and Idan Szpektor.
2009. Addressing discourse and document structure in
the RTE search task. In Proceedings of Text Analysis
Conference (TAC).
Judea Pearl. 1988. Probabilistic reasoning in intelli-
gent systems: networks of plausible inference. Morgan
Kaufmann.
Marta Tatu and Dan Moldovan. 2007. COGEX at RTE
3. In Proceedings of the ACL-PASCAL Workshop on
Textual Entailment and Paraphrasing.
Rui Wang, Yi Zhang, and Guenter Neumann. 2009. A
joint syntactic-semantic representation for recognizing
textual relatedness. In Proceedings of Text Analysis
Conference (TAC).
Fabio Massimo Zanzotto and Alessandro Moschitti.
2006. Automatic learning of textual entailments with
cross-pair similarities. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics.
563
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 97?102,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
PLIS: a Probabilistic Lexical Inference System
Eyal Shnarch1, Erel Segal-haLevi1, Jacob Goldberger2, Ido Dagan1
1Computer Science Department, Bar-Ilan University, Israel
2Faculty of Engineering, Bar-Ilan University, Israel
{shey,erelsgl,dagan}@cs.biu.ac.il
goldbej@eng.biu.ac.il
Abstract
This paper presents PLIS, an open source
Probabilistic Lexical Inference System
which combines two functionalities: (i)
a tool for integrating lexical inference
knowledge from diverse resources, and (ii)
a framework for scoring textual inferences
based on the integrated knowledge. We
provide PLIS with two probabilistic im-
plementation of this framework. PLIS is
available for download and developers of
text processing applications can use it as
an off-the-shelf component for injecting
lexical knowledge into their applications.
PLIS is easily configurable, components
can be extended or replaced with user gen-
erated ones to enable system customiza-
tion and further research. PLIS includes
an online interactive viewer, which is a
powerful tool for investigating lexical in-
ference processes.
1 Introduction and background
Semantic Inference is the process by which ma-
chines perform reasoning over natural language
texts. A semantic inference system is expected to
be able to infer the meaning of one text from the
meaning of another, identify parts of texts which
convey a target meaning, and manipulate text units
in order to deduce new meanings.
Semantic inference is needed for many Natural
Language Processing (NLP) applications. For in-
stance, a Question Answering (QA) system may
encounter the following question and candidate
answer (Example 1):
Q: which explorer discovered the New World?
A: Christopher Columbus revealed America.
As there are no overlapping words between the
two sentences, to identify that A holds an answer
for Q, background world knowledge is needed
to link Christopher Columbus with explorer and
America with New World. Linguistic knowledge
is also needed to identify that reveal and discover
refer to the same concept.
Knowledge is needed in order to bridge the gap
between text fragments, which may be dissimilar
on their surface form but share a common mean-
ing. For the purpose of semantic inference, such
knowledge can be derived from various resources
(e.g. WordNet (Fellbaum, 1998) and others, de-
tailed in Section 2.1) in a form which we denote as
inference links (often called inference/entailment
rules), each is an ordered pair of elements in which
the first implies the meaning of the second. For in-
stance, the link ship?vessel can be derived from
the hypernym relation of WordNet.
Other applications can benefit from utilizing in-
ference links to identify similarity between lan-
guage expressions. In Information Retrieval, the
user?s information need may be expressed in rele-
vant documents differently than it is expressed in
the query. Summarization systems should identify
text snippets which convey the same meaning.
Our work addresses a generic, application in-
dependent, setting of lexical inference. We there-
fore adopt the terminology of Textual Entailment
(Dagan et al, 2006), a generic paradigm for ap-
plied semantic inference which captures inference
needs of many NLP applications in a common un-
derlying task: given two textual fragments, termed
hypothesis (H) and text (T ), the task is to recog-
nize whether T implies the meaning of H , denoted
T?H. For instance, in a QA application, H rep-
resents the question, and T a candidate answer. In
this setting, T is likely to hold an answer for the
question if it entails the question.
It is challenging to properly extract the needed
inference knowledge from available resources,
and to effectively utilize it within the inference
process. The integration of resources, each has its
own format, is technically complex and the quality
97
? 
Lexical Inference 
Lexical Integrator 
? ? ? ?  
WordNet 
Wikipedia 
VerbOcean 
Text 
Hypothesis ?1 ?2 ?3 ?4 
?1 ?3 
?(? ? ?3) 
?2 
?? 
Lexical 
Resources 
?(?3 ? ?2) 
Figure 1: PLIS schema - a text-hypothesis pair is processed
by the Lexical Integrator which uses a set of lexical resources
to extract inference chains which connect the two. The Lexi-
cal Inference component provides probability estimations for
the validity of each level of the process.
of the resulting inference links is often unknown in
advance and varies considerably. For coping with
this challenge we developed PLIS, a Probabilis-
tic Lexical Inference System1. PLIS, illustrated in
Fig 1, has two main modules: the Lexical Integra-
tor (Section 2) accepts a set of lexical resources
and a text-hypothesis pair, and finds all the lex-
ical inference relations between any pair of text
term ti and hypothesis term hj , based on the avail-
able lexical relations found in the resources (and
their combination). The Lexical Inference module
(Section 3) provides validity scores for these rela-
tions. These term-level scores are used to estimate
the sentence-level likelihood that the meaning of
the hypothesis can be inferred from the text, thus
making PLIS a complete lexical inference system.
Lexical inference systems do not look into the
structure of texts but rather consider them as bag
of terms (words or multi-word expressions). These
systems are easy to implement, fast to run, practi-
cal across different genres and languages, while
maintaining a competitive level of performance.
PLIS can be used as a stand-alone efficient in-
ference system or as the lexical component of any
NLP application. PLIS is a flexible system, al-
lowing users to choose the set of knowledge re-
sources as well as the model by which inference
1The complete software package is available at http://
www.cs.biu.ac.il/nlp/downloads/PLIS.html and an online in-
teractive viewer is available for examination at http://irsrv2.
cs.biu.ac.il/nlp-net/PLIS.html.
is done. PLIS can be easily extended with new
knowledge resources and new inference models. It
comes with a set of ready-to-use plug-ins for many
common lexical resources (Section 2.1) as well
as two implementation of the scoring framework.
These implementations, described in (Shnarch et
al., 2011; Shnarch et al, 2012), provide probabil-
ity estimations for inference. PLIS has an inter-
active online viewer (Section 4) which provides a
visualization of the entire inference process, and is
very helpful for analysing lexical inference mod-
els and lexical resources usability.
2 Lexical integrator
The input for the lexical integrator is a set of lex-
ical resources and a pair of text T and hypothe-
sis H . The lexical integrator extracts lexical in-
ference links from the various lexical resources to
connect each text term ti?T with each hypothesis
term hj ?H2. A lexical inference link indicates a
semantic relation between two terms. It could be
a directional relation (Columbus?navigator) or a
bidirectional one (car?? automobile).
Since knowledge resources vary in their rep-
resentation methods, the lexical integrator wraps
each lexical resource in a common plug-in inter-
face which encapsulates resource?s inner repre-
sentation method and exposes its knowledge as a
list of inference links. The implemented plug-ins
that come with PLIS are described in Section 2.1.
Adding a new lexical resource and integrating it
with the others only demands the implementation
of the plug-in interface.
As the knowledge needed to connect a pair of
terms, ti and hj , may be scattered across few re-
sources, the lexical integrator combines inference
links into lexical inference chains to deduce new
pieces of knowledge, such as Columbus resource1???????
navigator resource2??????? explorer. Therefore, the only
assumption the lexical integrator makes, regarding
its input lexical resources, is that the inferential
lexical relations they provide are transitive.
The lexical integrator generates lexical infer-
ence chains by expanding the text and hypothesis
terms with inference links. These links lead to new
terms (e.g. navigator in the above chain example
and t? in Fig 1) which can be further expanded,
as all inference links are transitive. A transitivity
2Where i and j run from 1 to the length of the text and
hypothesis respectively.
98
limit is set by the user to determine the maximal
length for inference chains.
The lexical integrator uses a graph-based rep-
resentation for the inference chains, as illustrates
in Fig 1. A node holds the lemma, part-of-speech
and sense of a single term. The sense is the ordi-
nal number of WordNet sense. Whenever we do
not know the sense of a term we implement the
most frequent sense heuristic.3 An edge represents
an inference link and is labeled with the semantic
relation of this link (e.g. cytokine?protein is la-
beled with the WordNet relation hypernym).
2.1 Available plug-ins for lexical resources
We have implemented plug-ins for the follow-
ing resources: the English lexicon WordNet
(Fellbaum, 1998)(based on either JWI, JWNL
or extJWNL java APIs4), CatVar (Habash and
Dorr, 2003), a categorial variations database,
Wikipedia-based resource (Shnarch et al, 2009),
which applies several extraction methods to de-
rive inference links from the text and structure
of Wikipedia, VerbOcean (Chklovski and Pantel,
2004), a knowledge base of fine-grained semantic
relations between verbs, Lin?s distributional simi-
larity thesaurus (Lin, 1998), and DIRECT (Kotler-
man et al, 2010), a directional distributional simi-
larity thesaurus geared for lexical inference.
To summarize, the lexical integrator finds all
possible inference chains (of a predefined length),
resulting from any combination of inference links
extracted from lexical resources, which link any
t, h pair of a given text-hypothesis. Developers
can use this tool to save the hassle of interfac-
ing with the different lexical knowledge resources,
and spare the labor of combining their knowledge
via inference chains.
The lexical inference model, described next,
provides a mean to decide whether a given hypoth-
esis is inferred from a given text, based on weigh-
ing the lexical inference chains extracted by the
lexical integrator.
3 Lexical inference
There are many ways to implement an infer-
ence model which identifies inference relations
between texts. A simple model may consider the
3This disambiguation policy was better than considering
all senses of an ambiguous term in preliminary experiments.
However, it is a matter of changing a variable in the configu-
ration of PLIS to switch between these two policies.
4http://wordnet.princeton.edu/wordnet/related-projects/
number of hypothesis terms for which inference
chains, originated from text terms, were found. In
PLIS, the inference model is a plug-in, similar to
the lexical knowledge resources, and can be easily
replaced to change the inference logic.
We provide PLIS with two implemented base-
line lexical inference models which are mathemat-
ically based. These are two Probabilistic Lexical
Models (PLMs), HN-PLM and M-PLM which are
described in (Shnarch et al, 2011; Shnarch et al,
2012) respectively.
A PLM provides probability estimations for the
three parts of the inference process (as shown in
Fig 1): the validity probability of each inference
chain (i.e. the probability for a valid inference re-
lation between its endpoint terms) P (ti ? hj), the
probability of each hypothesis term to be inferred
by the entire text P (T ? hj) (term-level proba-
bility), and the probability of the entire hypothesis
to be inferred by the text P (T ? H) (sentence-
level probability).
HN-PLM describes a generative process by
which the hypothesis is generated from the text.
Its parameters are the reliability level of each of
the resources it utilizes (that is, the prior proba-
bility that applying an arbitrary inference link de-
rived from each resource corresponds to a valid in-
ference). For learning these parameters HN-PLM
applies a schema of the EM algorithm (Demp-
ster et al, 1977). Its performance on the recog-
nizing textual entailment task, RTE (Bentivogli et
al., 2009; Bentivogli et al, 2010), are in line with
the state of the art inference systems, including
complex systems which perform syntactic analy-
sis. This model is improved by M-PLM, which de-
duces sentence-level probability from term-level
probabilities by a Markovian process. PLIS with
this model was used for a passage retrieval for a
question answering task (Wang et al, 2007), and
outperformed state of the art inference systems.
Both PLMs model the following prominent as-
pects of the lexical inference phenomenon: (i)
considering the different reliability levels of the
input knowledge resources, (ii) reducing inference
chain probability as its length increases, and (iii)
increasing term-level probability as we have more
inference chains which suggest that the hypothesis
term is inferred by the text. Both PLMs only need
sentence-level annotations from which they derive
term-level inference probabilities.
To summarize, the lexical inference module
99
?(? ? ?) 
?(??? ??) 
?(? ? ??) 
configuration 
1 
2 
3 
4 
Figure 2: PLIS interactive viewer with Example 1 demonstrates knowledge integration of multiple inference chains and
resource combination (additional explanations, which are not part of the demo, are provided in orange).
provides the setting for interfacing with the lexi-
cal integrator. Additionally, the module provides
the framework for probabilistic inference models
which estimate term-level probabilities and inte-
grate them into a sentence-level inference deci-
sion, while implementing prominent aspects of
lexical inference. The user can choose to apply
another inference logic, not necessarily probabilis-
tic, by plugging a different lexical inference model
into the provided inference infrastructure.
4 The PLIS interactive system
PLIS comes with an online interactive viewer5 in
which the user sets the parameters of PLIS, inserts
a text-hypothesis pair and gets a visualization of
the entire inference process. This is a powerful
tool for investigating knowledge integration and
lexical inference models.
Fig 2 presents a screenshot of the processing of
Example 1. On the right side, the user configures
the system by selecting knowledge resources, ad-
justing their configuration, setting the transitivity
limit, and choosing the lexical inference model to
be applied by PLIS.
After inserting a text and a hypothesis to the
appropriate text boxes, the user clicks on the in-
fer button and PLIS generates all lexical inference
chains, of length up to the transitivity limit, that
connect text terms with hypothesis terms, as avail-
able from the combination of the selected input re-
5http://irsrv2.cs.biu.ac.il/nlp-net/PLIS.html
sources. Each inference chain is presented in a line
between the text and hypothesis.
PLIS also displays the probability estimations
for all inference levels; the probability of each
chain is presented at the end of its line. For each
hypothesis term, term-level probability, which
weighs all inference chains found for it, is given
below the dashed line. The overall sentence-level
probability integrates the probabilities of all hy-
pothesis terms and is displayed in the box at the
bottom right corner.
Next, we detail the inference process of Exam-
ple 1, as presented in Fig 2. In this QA example,
the probability of the candidate answer (set as the
text) to be relevant for the given question (the hy-
pothesis) is estimated. When utilizing only two
knowledge resources (WordNet and Wikipedia),
PLIS is able to recognize that explorer is inferred
by Christopher Columbus and that New World is
inferred by America. Each one of these pairs has
two independent inference chains, numbered 1?4,
as evidence for its inference relation.
Both inference chains 1 and 3 include a single
inference link, each derived from a different rela-
tion of the Wikipedia-based resource. The infer-
ence model assigns a higher probability for chain
1 since the BeComp relation is much more reliable
than the Link relation. This comparison illustrates
the ability of the inference model to learn how to
differ knowledge resources by their reliability.
Comparing the probability assigned by the in-
100
ference model for inference chain 2 with the prob-
abilities assigned for chains 1 and 3, reveals the
sophisticated way by which the inference model
integrates lexical knowledge. Inference chain 2
is longer than chain 1, therefore its probability is
lower. However, the inference model assigns chain
2 a higher probability than chain 3, even though
the latter is shorter, since the model is sensitive
enough to consider the difference in reliability lev-
els between the two highly reliable hypernym re-
lations (from WordNet) of chain 2 and the less re-
liable Link relation (from Wikipedia) of chain 3.
Another aspect of knowledge integration is ex-
emplified in Fig 2 by the three circled probabili-
ties. The inference model takes into consideration
the multiple pieces of evidence for the inference
of New World (inference chains 3 and 4, whose
probabilities are circled). This results in a term-
level probability estimation for New World (the
third circled probability) which is higher than the
probabilities of each chain separately.
The third term of the hypothesis, discover, re-
mains uncovered by the text as no inference chain
was found for it. Therefore, the sentence-level
inference probability is very low, 37%. In order
to identify that the hypothesis is indeed inferred
from the text, the inference model should be pro-
vided with indications for the inference of dis-
cover. To that end, the user may increase the tran-
sitivity limit in hope that longer inference chains
provide the needed information. In addition, the
user can examine other knowledge resources in
search for the missing inference link. In this ex-
ample, it is enough to add VerbOcean to the in-
put of PLIS to expose two inference chains which
connect reveal with discover by combining an in-
ference link from WordNet and another one from
VerbOcean. With this additional information, the
sentence-level probability increases to 76%. This
is a typical scenario of utilizing PLIS, either via
the interactive system or via the software, for ana-
lyzing the usability of the different knowledge re-
sources and their combination.
A feature of the interactive system, which is
useful for lexical resources analysis, is that each
term in a chain is clickable and links to another
screen which presents all the terms that are in-
ferred from it and those from which it is inferred.
Additionally, the interactive system communi-
cates with a server which runs PLIS, in a full-
duplex WebSocket connection6. This mode of op-
eration is publicly available and provides a method
for utilizing PLIS, without having to install it or
the lexical resources it uses.
Finally, since PLIS is a lexical system it can
easily be adjusted to other languages. One only
needs to replace the basic lexical text processing
tools and plug in knowledge resources in the tar-
get language. If PLIS is provided with bilingual
resources,7 it can operate also as a cross-lingual
inference system (Negri et al, 2012). For instance,
the text in Fig 3 is given in English, while the hy-
pothesis is written in Spanish (given as a list of
lemma:part-of-speech). The left side of the figure
depicts a cross-lingual inference process in which
the only lexical knowledge resource used is a man-
ually built English-Spanish dictionary. As can be
seen, two Spanish terms, jugador and casa remain
uncovered since the dictionary alone cannot con-
nect them to any of the English terms in the text.
As illustrated in the right side of Fig 3,
PLIS enables the combination of the bilingual
dictionary with monolingual resources to pro-
duce cross-lingual inference chains, such as foot-
baller hypernym???????player manual??????jugador. Such in-
ference chains have the capability to overcome
monolingual language variability (the first link
in this chain) as well as to provide cross-lingual
translation (the second link).
5 Conclusions
To utilize PLIS one should gather lexical re-
sources, obtain sentence-level annotations and
train the inference model. Annotations are avail-
able in common data sets for task such as QA,
Information Retrieval (queries are hypotheses and
snippets are texts) and Student Response Analysis
(reference answers are the hypotheses that should
be inferred by the student answers).
For developers of NLP applications, PLIS of-
fers a ready-to-use lexical knowledge integrator
which can interface with many common lexical
knowledge resources and constructs lexical in-
ference chains which combine the knowledge in
them. A developer who wants to overcome lex-
ical language variability, or to incorporate back-
ground knowledge, can utilize PLIS to inject lex-
6We used the socket.io implementation.
7A bilingual resource holds inference links which connect
terms in different languages (e.g. an English-Spanish dictio-
nary can provide the inference link explorer?explorador).
101
Figure 3: PLIS as a cross-lingual inference system. Left: the process with a single manual bilingual resource. Right: PLIS
composes cross-lingual inference chains to increase hypothesis coverage and increase sentence-level inference probability.
ical knowledge into any text understanding appli-
cation. PLIS can be used as a lightweight infer-
ence system or as the lexical component of larger,
more complex inference systems.
Additionally, PLIS provides scores for infer-
ence chains and determines the way to combine
them in order to recognize sentence-level infer-
ence. PLIS comes with two probabilistic lexical
inference models which achieved competitive per-
formance levels in the tasks of recognizing textual
entailment and passage retrieval for QA.
All aspects of PLIS are configurable. The user
can easily switch between the built-in lexical re-
sources, inference models and even languages, or
extend the system with additional lexical resources
and new inference models.
Acknowledgments
The authors thank Eden Erez for his help with
the interactive viewer and Miquel Espla` Gomis
for the bilingual dictionaries. This work was par-
tially supported by the European Community?s
7th Framework Programme (FP7/2007-2013) un-
der grant agreement no. 287923 (EXCITEMENT)
and the Israel Science Foundation grant 880/12.
References
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The
fifth PASCAL recognizing textual entailment chal-
lenge. In Proc. of TAC.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. The sixth
PASCAL recognizing textual entailment challenge.
In Proc. of TAC.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the web for fine-grained semantic verb
relations. In Proc. of EMNLP.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In Lecture Notes in Computer Science,
volume 3944, pages 177?190.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the royal statistical soci-
ety, series [B], 39(1):1?38.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Massachusetts.
Nizar Habash and Bonnie Dorr. 2003. A categorial
variation database for English. In Proc. of NAACL.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359?389.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proc. of COLOING-ACL.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2012.
Semeval-2012 task 8: Cross-lingual textual entail-
ment for content synchronization. In Proc. of Se-
mEval.
Eyal Shnarch, Libby Barak, and Ido Dagan. 2009. Ex-
tracting lexical reference rules from Wikipedia. In
Proc. of ACL.
Eyal Shnarch, Jacob Goldberger, and Ido Dagan. 2011.
Towards a probabilistic model for lexical entailment.
In Proc. of the TextInfer Workshop.
Eyal Shnarch, Ido Dagan, and Jacob Goldberger. 2012.
A probabilistic lexical model for ranking textual in-
ferences. In Proc. of *SEM.
Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? A quasi-
synchronous grammar for QA. In Proc. of EMNLP.
102
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 237?245,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
A Probabilistic Lexical Model for Ranking Textual Inferences
Eyal Shnarch and Ido Dagan
Computer Science Department
Bar-Ilan University
Ramat-Gan 52900, Israel
{shey,dagan}@cs.biu.ac.il
Jacob Goldberger
Faculty of Engineering
Bar-Ilan University
Ramat-Gan 52900, Israel
goldbej@eng.biu.ac.il
Abstract
Identifying textual inferences, where the
meaning of one text follows from another, is
a general underlying task within many natu-
ral language applications. Commonly, it is ap-
proached either by generative syntactic-based
methods or by ?lightweight? heuristic lexical
models. We suggest a model which is confined
to simple lexical information, but is formu-
lated as a principled generative probabilistic
model. We focus our attention on the task of
ranking textual inferences and show substan-
tially improved results on a recently investi-
gated question answering data set.
1 Introduction
The task of identifying texts which share semantic
content arises as a general need in many natural lan-
guage processing applications. For instance, a para-
phrasing application has to recognize texts which
convey roughly the same content, and a summariza-
tion application needs to single out texts which con-
tain the content stated by other texts. We refer to this
general task as textual inference similar to prior use
of this term (Raina et al, 2005; Schoenmackers et
al., 2008; Haghighi et al, 2005).
In many textual inference scenarios the setting re-
quires a classification decision of whether the infer-
ence relation holds or not. But in other scenarios
ranking according to inference likelihood would be
the natural task. In this work we focus on ranking
textual inferences; given a sentence and a corpus,
the task is to rank the corpus passages by their plau-
sibility to imply as much of the sentence meaning as
possible. Most naturally, this is the case in question
answering (QA), where systems search for passages
that cover the semantic components of the question.
A recent line of research was dedicated to this task
(Wang et al, 2007; Heilman and Smith, 2010; Wang
and Manning, 2010).
A related scenario is the task of Recognizing Tex-
tual Entailment (RTE) within a corpus (Bentivogli
et al, 2010)1. In this task, inference systems should
identify, for a given hypothesis, the sentences which
entail it in a given corpus. Even though RTE was
presented as a classification task, it has an appeal-
ing potential as a ranking task as well. For instance,
one may want to find texts that validate a claim such
as cellular radiation is dangerous for children, or to
learn more about it from a newswire corpus. To that
end, one should look for additional mentions of this
claim such as extensive usage of cell phones may be
harmful for youngsters. This can be done by rank-
ing the corpus passages by their likelihood to entail
the claim, where the top ranked passages are likely
to contain additional relevant information.
Two main approaches have been used to address
textual inference (for either ranking or classifica-
tion). One is based on transformations over syntac-
tic parse trees (Echihabi and Marcu, 2003; Heilman
and Smith, 2010). Some works in this line describe
a probabilistic generative process in which the parse
tree of the question is generated from the passage
(Wang et al, 2007; Wang and Manning, 2010).
In the second approach, lexical models have been
employed for textual inference (MacKinlay and
Baldwin, 2009; Clark and Harrison, 2010). Typi-
1http://www.nist.gov/tac/2010/RTE/index.html
237
cally, lexical models consider a text fragment as a
bag of terms and split the inference decision into
two steps. The first is a term-level estimation of the
inference likelihood for each term independently,
based on direct lexical match and on lexical knowl-
edge resources. Some commonly used resources are
WordNet (Fellbaum, 1998), distributional-similarity
thesauri (Lin, 1998), and web knowledge resources
such as (Suchanek et al, 2007). The second step
is making a final sentence-level decision based on
these estimations for the component terms. Lex-
ical models have the advantage of being fast and
easy to utilize (e.g. no dependency on parsing tools)
while being highly competitive with top performing
systems, e.g. the system of Majumdar and Bhat-
tacharyya (2010).
In this work, we investigate how well such lexi-
cal models can perform in textual inference ranking
scenarios. However, while lexical models usually
apply heuristic methods, we would like to pursue a
principled learning-based generative framework, in
analogy to the approaches for syntactic-based infer-
ence. An attractive work in this spirit is presented in
(Shnarch et al, 2011a), that propose a model which
is both lexical and probabilistic. Later, Shnarch et
al. (2011b) improved this model and reported re-
sults that outperformed previous lexical models and
were on par with state-of-the-art RTE models.
Whereas their term-level model provides means
to integrate lexical knowledge in a probabilistic
manner, their sentence-level model depends to a
great extent on heuristic normalizations which were
introduced to incorporate prominent aspects of the
sentence-level decision. This deviates their model
from a pure probabilistic methodology.
Our work aims at amending this deficiency and
proposes a new probabilistic sentence-level model
based on a Markovian process. In that model, all
parameters are estimated by an EM algorithm. We
evaluate this model on the tasks of ranking passages
for QA and ranking textual entailments within a cor-
pus, and show that eliminating the need for heuris-
tic normalizations greatly improves state-of-the-art
performance. The full implementation of our model
is available for download2 and can be used as an
easy-to-install and highly competitive inference en-
2http://www.cs.biu.ac.il/?nlp/downloads/probLexModel.html
gine that operates only on lexical knowledge, or as a
lexical component integrated within a more complex
inference system.
2 Background
Wang et al (2007) provided an annotated data set,
based on the Text REtrieval Conference (TREC) QA
tracks3, specifically for the task of ranking candidate
answer passages. We adopt their experimental setup
and next review the line of syntactic-based works
which reported results on this data set.
2.1 Syntactic generative models
Wang et al (2007) propose a quasi-synchronous
grammar formulation which specifies the generation
of the question parse tree, loosely conditioned on the
parse tree of the candidate answer passage. Their
model showed improvement over previous syntac-
tic models for QA: Punyakanok et al (2004), who
computed similarity between question-answer pairs
with a generalized tree-edit distance, and Cui et al
(2005), who developed an information measure for
sentence similarity based on dependency paths of
aligned words. Wang et al (2007) reproduced these
methods and extended them to utilize WordNet.
More recently, Heilman and Smith (2010) im-
proved Wang et al (2007) results with a classifica-
tion based approach. Feature for the classifier were
extracted from a greedy algorithm which searches
for tree-edit sequences which transform the parse
tree of the candidate answer into the one of the ques-
tion. Unlike other works reviewed here, this one
does not utilize lexical knowledge resources.
Similarly, Wang and Manning (2010) present an
extended tree-edit operations set and search for edit
sequences to generate the question from the answer
candidate. Their CRF-based classifier models these
sequences as latent variables.
An important merit of these methods is that they
offer principled, often probabilistic, generative mod-
els for the task of ranking candidate answers. Their
drawback is the need for syntactic analysis which
makes them slower to run, dependent on parsing per-
formance, which is often mediocre in many text gen-
res, and inadequate for languages which lack proper
parsing tools.
3http://trec.nist.gov/data/qamain.html
238
2.2 Lexical models
Lexical models, on the other hand, are faster, eas-
ier to implement and are more practical for vari-
ous genres and languages. Such models derive from
knowledge resources lexical inference rules which
indicate that the meaning of a lexical term can be
inferred from the meaning of another term (e.g.
youngsters? children and harmful? dangerous).
They are common in the Recognizing Textual En-
tailment (RTE) systems and we present some rep-
resentative methods for that task. We adopt textual
entailment terminology and henceforth use Hypoth-
esis (denoted H) for the inferred text fragment and
Text (denoted T ) for the text from which it is being
inferred4.
Majumdar and Bhattacharyya (2010) utilized a
simple union of lexical rules derived from vari-
ous lexical resources for the term-level step. They
derived their sentence-level decision based on the
number of matched hypothesis terms. The results
of this simple model were only slightly worse than
the best results of the RTE-6 challenge which were
achieved by a syntactic-based system (Jia et al,
2010). Clark and Harrison (2010), on the other hand,
considered the number of mismatched terms in es-
tablishing their sentence-level decision. MacKinlay
and Baldwin (2009) represented text and hypothe-
sis as word vectors augmented with lexical knowl-
edge. For sentence-level similarity they used a vari-
ant of the cosine similarity score. Common to most
of these lexical models is the application of heuris-
tic methods in both the term and the sentence level
steps.
Targeted to replace heuristic methods with princi-
pled ones, Shnarch et al (2011a) present a model
which aims at combining the advantages of a proba-
bilistic generative model with the simplicity of lex-
ical methods. In some analogy to generative parse-
tree based models, they propose a generative process
for the creation of the hypothesis from the text.
At the term-level, their model combines knowl-
edge from various input resources and has the ad-
vantages of considering the effect of transitive rule
application (e.g. mobile phone? cell phone? cel-
lular) as well as the integration of multiple pieces
4In the task of passage ranking for QA, the hypothesis is the
question and the text is the candidate passage.
of evidence for the inference of a term (e.g. both
the appearance of harmful and risky in T provide
evidence for the inference of dangerous in H). We
denote this term-level Probabilistic Lexical Model
as PLMTL, and have reproduced it in our work as
presented in Section 4.1. For the sentence-level de-
cision they describe an AND gate mechanism, i.e.
deducing a positive inference decision for H as a
whole only if all its terms were inferred from T .
In an extension to that work, Shnarch et al
(2011b) modified PLMTL to improve the sentence-
level step. They pointed out some prominent aspects
for the sentence-level decision. First, they suggest
that a hypothesis as a whole can be inferred from
the text even if some of its terms are not inferred.
To model this, they introduced a noisy-AND mech-
anism (Pearl, 1988). Additionally, they emphasized
the effect of hypothesis length and the dependency
between terms on the sentence-level decision. How-
ever, they did not fully achieve their target of pre-
senting a fully coherent probabilistic model, as their
model included heuristic normalization formulae.
On the contrary, the model we present is the first
along this line to be fully specified in terms of a
generative setting and formulated in pure probabilis-
tic terms. We introduce a Markovian-style proba-
bilistic model for the sentence-level decision. This
model receives as input term-level probabilistic es-
timates, which may be provided by any term-level
model. In our implementation we embed PLMTL as
the term-level model and present a complete coher-
ent Markovian-based Probabilistic Lexical Model,
which we term M-PLM.
3 Markovian sentence-level model
The goal of a sentence-level model is to integrate
term-level inputs into an inference decision for the
hypothesis as a whole. For a hypothesis H =
h1, . . . , hn and a text T , term-level models first esti-
mate independently for each term ht its probability
to be inferred from T . Let xt be a binary random
variable representing the event that ht is indeed in-
ferred from T (i.e., xt = 1 if ht is inferred and 0
otherwise).
Given these term-level probabilities, a sentence-
level model is employed to estimate the probability
that H as a whole is inferred from T . This step is
239
term
-leve
l
sente
nce-l
evel
Text: Hypo
:
t 1
t m
?
h 1
h 2
h n
?
x 1
x 2
x n
?
y 1
y 2
y n
?
Figure 1: A probabilistic lexical model: the upper part is the
term-level input to the sentence-level Markovian process, de-
picted in the lower part. xi is a binary variable representing the
inference of hi and yj is a variable for the accumulative infer-
ence decision for the first j terms of Hypo. The final sentence-
level decision is given by yn.
the focus of our work. We assume that the term-
level probabilities are given as input. Section 4.1
describes PLMTL, as a concrete method for deriving
these probabilities.
Our sentence-level model is based on a Marko-
vian process and is described in Section 3.1. In par-
ticular, it takes into account, in probabilistic terms,
the prominent factors in lexical entailment, men-
tioned in Section 2. An efficient inference algorithm
for our model is given in Section 3.2 and EM-based
learning is specified in Section 3.3.
3.1 Markovian sentence-level decision
The motivation for proposing a Markovian process
for the sentence-level is to establish an intermedi-
ate model, lying between two extremes: assuming
full independence between hypothesis terms versus
assuming that every term is dependent on all other
terms. The former alternative is too weak, while
the latter alternative is computationally hard and
not very informative, and thus hard to capture in
a model. Our model specifies a Markovian depen-
dence structure, which limits the dependence scope
to adjacent terms, as follows.
We define a binary variable yt to be the accumu-
lated sentence-level inference decision up to ht. In
other words, yt=1 if the subset {h1, . . . , ht} of H?s
terms is inferred as a whole from T .
Note that this means that yt can be 1 even if some
terms amongst h1, . . . , ht are not inferred. As yn is
the decision for the complete hypothesis, our model
addresses this way the prominent aspect that the hy-
pothesis as a whole may be inferred even if some of
its terms are not inferred. The reason for allowing
this is that such un-inferred terms may be inferred
from the global context of T , or alternatively, are ac-
tually inferred from T but the knowledge resources
in use do not contain the proper lexical rule to make
such inference.
Figure 1 describes both steps of a full lexical in-
ference model. Its lower part depicts our Markovian
process. In the proposed model the inference deci-
sion at each position t is a combination of xt, the
variable for the event of ht being inferred, and yt?1,
the accumulated decision at the previous position.
Therefore, the transition parameters of M-PLM can
be modeled as:
qij(k)=P (yt=k|yt?1 = i, xt=j) ?k, i, j?{0, 1}
where y1=x1. For instance, q01(1) is the probability
that yt=1, given that yt?1 =0 and xt=1.
Applying the Markovian process on the entire
hypothesis we get yn, which represents the final
sentence-level decision, where a soft decision is ob-
tained by computing the probability of yn=1:
P (yn=1) =
?
x1, ..., xn
y2, ..., yn?1, yn=1
P (x1)
n?
t=2
P (xt)P (yt|yt?1, xt)
The summation is done over all possible binary
values of the term-level variables x1, ..., xn and the
accumulated sentence-level variables y2, ..., yn?1
where yn=1. Note that for clarity, in this formula xt
and yt denote the binary values at the corresponding
variable positions. A tractable form for computing
P (yn=1) is presented in Section 3.2.
Overall, the prominent factors in lexical entail-
ment, raised by prior works, are incorporated within
the core structure of this probabilistic model, with-
out the need to resort to heuristic normalizations.
Reducing the negative affect of hypothesis length on
the entailment probability is achieved by having yt,
at each position, being directly dependent only on xt
and yt?1 as opposed to being affected by all hypoth-
esis terms. The second factor, modeling the depen-
dency between hypothesis terms, is addressed by the
240
indirect dependency of yn on all preceding hypothe-
sis terms. This dependency arises from the recursive
nature of the Markovian model, as can be seen in the
next section.
Our proposed Markovian process presents a linear
dependency between terms which, to some extent,
poses an anomaly with respect to the structure of the
entailment phenomenon. Yet, as we do want to limit
the dependence structure, following the natural or-
der of the sentence words seems the most reasonable
choice, as common in many other types of sequential
models. We also tried randomizing the word order
which, on average, did not improve performance.
3.2 Inference
The accumulated sentence-level inference can be
efficiently computed using a typical forward algo-
rithm. We denote the probability of xt=j, j?{0, 1}
by ht(j) = P (xt = j). The forward step is given in
Eq. (1) and its initialization is defined in Eq. (2).
?t(k) = P (yt=k)=
?
i,j?{0,1}
?t?1(i)ht(j)qij(k) (1)
?1(k) = P (x1 =k) (2)
where k?{0, 1} and t = 2, ..., n.
?t(k) is the probability that the accumulated de-
cision at position t is k. It is calculated by sum-
ming over the probabilities of all four combinations
of ?t?1(i) and ht(j), multiplied by the correspond-
ing transition probability, qij(k).
The soft sentence-level decision can be efficiently
calculated by:
P (yn=a) = ?n(a) a?{0, 1} (3)
3.3 Learning
Typically, natural language applications work at the
sentence-level. The training data for such applica-
tions is, therefore, available as annotations at the
sentence-level. Term-level alignments between pas-
sage terms and question terms are rarely available.
Hence, we learn our term-level parameters from
available sentence-level annotations, using the gen-
erative process described above to bridge the gap be-
tween these two levels.
For learning we use the typical backwards algo-
rithm which is described by Eq. (4) and Eq. (5),
where ?t(a|i) is the probability that the full hypoth-
esis inference value is a given that yt= i.
?n(a|i) = P (yn=a|yn= i) = 1{a=i} (4)
?t(a|i) = P (yn=a|yt= i) =
=
?
j,k?{0,1}
ht+1(j)qij(k)?t+1(a|k) (5)
where t = n?1, .., 1, a ? {0, 1} and 1{condition} is
the indicator function which returns 1 if condition
holds and 0 otherwise.
To estimate qij(k), the parameters of the Marko-
vian process, we employ the EM algorithm:
E-step: For each (T,H) pair in the training
data set, annotated with a ? {0, 1} as its sentence-
level inference value, we evaluate the expected
probability of every transition given the annotation
value a:
wtijk(T,H) = P (yt?1 = i, xt=j, yt=k|yn=a)
=
?t?1(i)ht(j)qij(k)?t(a|k)
P (yn=a)
(6)
?i, j, k?{0, 1} and t = 2, ..., |H|.
M-step: Given the values of wtijk(T,H) we
can estimate each qij(1), i, j?{0, 1}, by taking the
proportion of transitions in which yt?1 = i, xt = j
and yt = 1, out of the total transitions in which
yt?1 = i and xt=j:
qij(1)?
?
(T,H)
?|H|
t=2wtij1(T,H)
?
(T,H)
?|H|
t=2
?
k?{0,1}wtijk(T,H)
(7)
qij(0) = 1?qij(1)
4 Complete model implementation
We next describe the end-to-end probabilistic lexical
inference model we used in our evaluations. We im-
plemented PLMTL as our term-level model to pro-
vide us with ht(j), the term-level probabilities. We
chose this model since it is fully lexical, has the ad-
vantages of lexical knowledge integration described
in Section 2 and achieved top results on RTE data
sets. Next, we summarize PLMTL, and in Appendix
A we show how to adjust the learning schema to fit
into our sentence-level model.
241
4.1 PLMTL
Shnarch et al (2011a) provide a term-level model
which integrates lexical rules from various knowl-
edge resources. As described below it also consid-
ers transitive chains of rule applications as well as
the impact of parallel chains which provide multiple
evidence that h?H is inferred from T .
Their model assumes a parameter ?R for each
knowledge resource R in use. ?R specifies the re-
source?s reliability, i.e. the prior probability that ap-
plying a rule from R to an arbitrary text-hypothesis
pair would yield a valid inference.
Next, transitive chains may connect a text term to
a hypothesis term via intermediate term(s). For in-
stance, starting from the text term T-Mobile, a chain
that utilizes the lexical rules T-Mobile? telecom
and telecom? cell phone enables the inference of
the term cell phone from T . They compute, for each
step in a chain, the probability that this step is valid
based on the ?R values. Denoting the resource which
provided a rule r by R(r), Eq. (8) specifies that the
validity probability of the inference step correspond-
ing to the application of the rule r within the chain c
pointing at ht (as represented by xtcr) is ?R(r).
Next, for a chain c pointing at ht (represented by
xtc) to be valid, all its rule steps should be valid for
this pair. Eq. (9) estimates this probability by the
joint probability that the applications of all rules r?
c are valid, assuming independence of rules.
Several chains may connect terms in T to ht, thus
providing multiple pieces of evidence that ht is in-
ferred from T . For instance, both youngsters and
kids in T may indicate the inference of children in
H . For a term ht to be inferred from the entire sen-
tence T it is enough that at least one of the chains
from T to ht is valid. This is the complement event
of ht not being inferred from T which happens when
all chains which suggest the inference of ht, denoted
by C(ht), are invalid. Eq. (10) specifies this proba-
bility (again assuming independence of chains).
P (xtcr = 1) = ?R(r) (8)
P (xtc = 1) =
?
r?c
P (xtcr = 1) (9)
ht(1) = P (xt = 1) = 1?P (xt = 0) (10)
= 1?
?
c?C(ht)
P (xtc = 0)
With respect to the contributions of our work, we
note that previous works resorted to applying some
heuristic amendments on these equations to achieve
valuable results. In contrast, our work is the first
to present a purely generative model. This achieve-
ment shows that it is possible to shift from ad-hoc
heuristic methods, which are common practice, to
more solid mathematically-based methods.
Finally, for ranking text passages from a corpus
for a given hypothesis (question in the QA scenario),
our Markovian sentence-level model takes as its in-
put the outcome of Eq. (10) for each ht ? H . For
PLMTL we need to estimate the model parameters,
that is the various ?R values. In our Markovian
model this is done by the scheme detailed in Ap-
pendix A. Given these term-level probabilities, our
model computes for each hypothesis its probabil-
ity to be inferred from each of the corpus passages,
namely P (yn = 1) in Eq (3). Passages are then
ranked according to this probability.
5 Evaluations and Results
To evaluate the performance of M-PLM for ranking
textual inferences we focused on the task of ranking
candidate answer passages for question answering
(QA) as presented in Section 5.1. Additionally, we
demonstrate the added value of our sentence-level
model in another ranking experiment based on RTE
data sets, described in Section 5.2.
5.1 Answer ranking for question answering
Data set We adopted the experimental setup of
Wang et al (2007) who also provided an annotated
data set for answer passage ranking in QA5.
In their data set an instance is a pair of a factoid
question and a candidate answer passage (a single
sentence in this data set). It was constructed from the
data of the QA tracks at TREC 8?13. The question-
candidate pairs were manually judged and a pair was
annotated as positive if the candidate passage indi-
cates the correct answer for the question. The train-
ing and test sets roughly contain 5700 and 1500 pairs
correspondingly.
5The data set was kindly provided to us by
Mengqiu Wang and is available for download at
http://www.cs.stanford.edu/?mengqiu/data/qg-emnlp07-
data.tgz.
242
Method PLMTL utilizes WordNet and the Catvar
(Categorial Variation) derivations database (Habash
and Dorr, 2003) as generic and publicly available
lexical knowledge resources, when question and
answer terms are restricted to the first WordNet
sense. In order to be consistent with (Shnarch et al,
2011b), the best performing model of prior work,
we restricted our model to utilize only these two re-
sources which they used. However, additional lexi-
cal resources can be provided as input to our model
(e.g. a distributional similarity-base thesaurus).
We report Mean Average Precision (MAP) and
Mean Reciprocal Rank (MRR), the standard mea-
sures for ranked lists. In the cases of tie we took
a conservative approach and ranked positive anno-
tated instances below the negative instances scored
with the same probability. Hence, the reported fig-
ures are lower-bounds for any tie-breaking method
that could have been applied.
Results We compared our model to all 5 mod-
els evaluated for this data set, described in Sec-
tion 2, and to our own implementation of (Shnarch
et al, 2011b). We term this model Heuristically-
Normalized Probabilistic Lexical Model, HN-PLM,
since it modifies PLMTL by introducing heuristic
normalization formulae. As explained earlier, both
M-PLM and HN-PLM embed PLMTL in their im-
plementation but they differ in their sentence-level
model. In our implementation of both models,
PLMTL applies chains of transitive rule applications
whose maximal length is 3.
As seen in Table 1, M-PLM outperforms all prior
models by a large margin. A comparison of M-PLM
and HN-PLM reveals the major positive effect of
choosing the Markovian process for the sentence-
level decision. By avoiding heuristically-normalized
formulae and having all our parameters being part of
the Markovian model, we managed to increases both
MAP and MRR by nearly 2.5%6.
Ablation Test As an additional examination of
the impact of the Markovian process components,
we evaluated the contribution of having 4 transition
parameters. The AND-logic applied by (Shnarch et
6The difference is not significant according to the Wilcoxon
test, however we note that given the data set size it is hard to get
a significant difference and that both Heilman and Smith (2010)
and Wang and Manning (2010) improvements over the results
of Wang et al (2007) were not statistically significant.
System MAP MRR
Punyakanok et al 41.89 49.39
Cui et al 43.50 55.69
Wang & Manning 59.51 69.51
Wang et al 60.29 68.52
Heilman & Smith 60.91 69.17
Shnarch et al HN-PLM 61.89 70.24
M-PLM 64.38 72.69
Table 1: Results (in %) for the task of answer ranking for
question answering (sorted by MAP).
al., 2011a) to their sentence-level decision roughly
corresponds to 2 of the Markovian parameters. A
binary AND outputs 1 if both its inputs are 1. This
corresponds to q11(1) which is indeed estimate to be
near 1. In any other case an AND gate outputs 0.
This corresponds to q00(1) which was estimated to
be near zero.
The two parameters q01 and q10 are novel to the
Markovian process and do not have counterparts in
(Shnarch et al, 2011a). These parameters are the
cases in which the sentence-level decision accumu-
lated so far and the term-level decision do not agree.
Introducing these 2 parameters enables our model to
provide a positive decision for the hypothesis as a
whole (or for a part of it) even if some of its terms
were not inferred. We performed an ablation test on
each of these two parameters by forcing the value of
the ablated parameter to be zero. The notable perfor-
mance drop presented in Table 2 indicates the crucial
contribution of these parameters to our model.
Ablated parameter ? MAP ? MRR
q01(1) = 0 -2.61 -4.91
q10(1) = 0 -2.12 -2.86
Table 2: Ablation test for the novel parameters of the Marko-
vian process. Results (in %) indicate performance drop when
forcing a parameter to be zero.
5.2 RTE evaluations
To assess the added value of our model on an addi-
tional ranking evaluation, we utilize the search task
data sets of the recent Recognizing Textual Entail-
ment (RTE) benchmarks (Bentivogli et al, 2009;
Bentivogli et al, 2010), which were originally con-
243
structed for the task of entailment classification. In
that task a hypothesis is given with a corpus and the
goal is to identify which sentences of the corpus en-
tail the hypothesis. This setting naturally lends itself
to a ranking scenario, in which the desired output is
a list of the corpus sentences ranked by their proba-
bility to entail the given hypothesis.
To that end, we employed the same method-
ology as described in the previous section. Ta-
ble 3 presents the improvement of our model over
HN-PLM, whose classification performance was re-
ported to be on par with best-performing systems on
these data sets7. As can be seen, the improvement
is substantial for both measures on both data sets.
These results further assess the contribution of our
Markovian sentence-level model.
RTE-5 RTE-6
MAP MRR MAP MRR
HN-PLM 58.0 82.9 54.0 71.9
M-PLM 61.6 84.8 60.0 79.2
? +3.6 +1.9 +6.0 +7.3
Table 3: Improvements of our sentence-level model over
HN-PLM. Results (in %) are shown for the last RTE and
for the search task in RTE-5.
6 Discussion
This paper investigated probabilistic lexical mod-
els for ranking textual inferences focusing on pas-
sage ranking for QA. We showed that our coher-
ent probabilistic model, whose sentence-level model
is based on a Markovian process, considerably im-
proves five prior syntactic-based models as well as
a heuristically-normalized lexical model. Therefore,
it raises the baseline for future methods.
In future work we would like to further explore
a broader range of related probabilistic models. Es-
pecially, as our Markovian process is dependent on
term order, it would be interesting to investigate
models which are not order dependent.
Initial experiments on the classification task show
that M-PLM performs well above the average sys-
tem but below HN-PLM, since it does not normalize
7RTE data sets were only used for the classification task
so far, therefore there are no state-of-the-art results to compare
with, when utilizing them for the ranking task.
the estimated probability well across hypothesis. We
therefore suggest a future work on better classifica-
tion models.
Finally, we view this work as joining a line of re-
search which develops principled probabilistic mod-
els for the task of textual inference and demonstrates
their superiority over heuristic methods.
A Appendix: Adaptation of PLMTL
learning
M-PLM embeds PLMTL as its term-level model.
PLMTL introduces ?R values as additional parame-
ters for the complete model. We show how we mod-
ify (Shnarch et al, 2011a) E-step formula to fit our
Markovian modeling, described in Section 3.1. The
M-step formula remains exactly the same.
Eq. (11) estimates the a-posteriori validity prob-
ability of a single application of the rule r in the
transitive chain c pointing at ht, given that the an-
notation of the pair is a.
wtcr(T,H) = P (xtcr = 1|yn = a) =
(11)?
i,j,k?{0,1} ?t?1(i)P (xt=j|xtcr =1)?R(r)qij(k)?t(a|k)
P (yn = a)
where t=2 . . . n and P (xt =j|xtcr =1) is the prob-
ability that the inference value of xt is j, given that
the application of r provides a valid inference step.
As appeared in (Shnarch et al, 2011b) this probabil-
ity can be evaluated as follows:
P (xt=1|xtcr =1)=1?
P (xt = 0)
P (xtc = 0)
(
1?
P (xtc = 1)
?R(r)
)
For t = 1 there is no accumulated sentence-level
decision at the previous position (i.e. no ?t?1) there-
fore Eq. (11) becomes:
w1cr(T,H) =
?
j?{0,1}P (x1 =j|x1cr =1)?R(r)?1(a|j)
P (yn = a)
Acknowledgments
This work was partially supported by the Israel
Science Foundation grant 1112/08, the PASCAL-
2 Network of Excellence of the European Com-
munity FP7-ICT-2007-1-216886, and the Euro-
pean Community?s Seventh Framework Programme
(FP7/2007-2013) under grant agreement no. 287923
(EXCITEMENT).
244
References
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The fifth
PASCAL recognizing textual entailment challenge. In
Proceedings of the Text Analysis Conference.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. The sixth
PASCAL recognizing textual entailment challenge. In
Proceedings of the Text Analysis Conference.
Peter Clark and Phil Harrison. 2010. BLUE-Lite: a
knowledge-based lexical entailment system for RTE6.
In Proceedings of the Text Analysis Conference.
Hang Cui, Renxu Sun, Keya Li, Min yen Kan, and Tat
seng Chua. 2005. Question answering passage re-
trieval using dependency relations. In Proceedings of
SIGIR.
Abdessamad Echihabi and Daniel Marcu. 2003. A
noisy-channel approach to question answering. In
Proceedings of ACL.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and Com-
munication). The MIT Press.
Nizar Habash and Bonnie Dorr. 2003. A categorial vari-
ation database for english. In Proceedings of the Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics.
Aria Haghighi, Andrew Ng, and Christopher Manning.
2005. Robust textual inference via graph matching. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing.
Michael Heilman and Noah A. Smith. 2010. Tree
edit models for recognizing textual entailments, para-
phrases, and answers to questions. In Proceedings of
the Conference of the North American Chapter of the
Association for Computational Linguistics.
Houping Jia, Xiaojiang Huang, Tengfei Ma, Xiaojun
Wan, and Jianguo Xiao. 2010. PKUTM participation
at the Text Analysis Conference 2010 RTE and sum-
marization track. In Proceedings of the Text Analysis
Conference.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING.
Andrew MacKinlay and Timothy Baldwin. 2009. A
baseline approach to the RTE5 search pilot. In Pro-
ceedings of the Text Analysis Conference.
Debarghya Majumdar and Pushpak Bhattacharyya.
2010. Lexical based text entailment system for main
task of RTE6. In Proceedings of the Text Analysis
Conference.
Judea Pearl. 1988. Probabilistic reasoning in intelli-
gent systems: networks of plausible inference. Morgan
Kaufmann.
Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2004.
Mapping dependencies trees: An application to ques-
tion answering. In Proceedings of the International
Symposium on Artificial Intelligence and Mathemat-
ics.
Rajat Raina, Andrew Y. Ng, and Christopher D. Man-
ning. 2005. Robust textual inference via learning and
abductive reasoning. In Proceedings of AAAI.
Stefan Schoenmackers, Oren Etzioni, and Daniel Weld.
2008. Scaling textual inference to the web. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Eyal Shnarch, Jacob Goldberger, and Ido Dagan. 2011a.
A probabilistic modeling framework for lexical entail-
ment. In Proceedings of ACL.
Eyal Shnarch, Jacob Goldberger, and Ido Dagan. 2011b.
Towards a probabilistic model for lexical entailment.
In Proceedings of the TextInfer Workshop on Textual
Entailment.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A Core of Semantic Knowl-
edge. In Proceedings of WWW.
Mengqiu Wang and Christopher Manning. 2010. Proba-
bilistic tree-edit models with structured latent variables
for textual entailment and question answering. In Pro-
ceedings of Coling.
Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? a quasi-
synchronous grammar for QA. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing.
245
Proceedings of the TextInfer 2011 Workshop on Textual Entailment, EMNLP 2011, pages 10?19,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Towards a Probabilistic Model for Lexical Entailment
Eyal Shnarch
Computer Science Department
Bar-Ilan University
Ramat-Gan, Israel
shey@cs.biu.ac.il
Jacob Goldberger
School of Engineering
Bar-Ilan University
Ramat-Gan, Israel
goldbej@eng.biu.ac.il
Ido Dagan
Computer Science Department
Bar-Ilan University
Ramat-Gan, Israel
dagan@cs.biu.ac.il
Abstract
While modeling entailment at the lexical-level
is a prominent task, addressed by most textual
entailment systems, it has been approached
mostly by heuristic methods, neglecting some
of its important aspects. We present a prob-
abilistic approach for this task which cov-
ers aspects such as differentiating various re-
sources by their reliability levels, considering
the length of the entailed sentence, the num-
ber of its covered terms and the existence of
multiple evidence for the entailment of a term.
The impact of our model components is vali-
dated by evaluations, which also show that its
performance is in line with the best published
entailment systems.
1 Introduction
Textual Entailment was proposed as a generic
paradigm for applied semantic inference (Dagan et
al., 2006). Given two textual fragments, termed hy-
pothesis (H) and text (T ), the text is said to textually
entail the hypothesis (T?H) if a person reading the
text can infer the meaning of the hypothesis. Since it
was first introduced, the six rounds of the Recogniz-
ing Textual Entailment (RTE) challenges1 have be-
come a standard benchmark for entailment systems.
Entailment systems apply various techniques to
tackle this task, including logical inference (Tatu
and Moldovan, 2007; MacCartney and Manning,
2007), semantic analysis (Burchardt et al, 2007)
and syntactic parsing (Bar-Haim et al, 2008; Wang
1http://www.nist.gov/tac/
et al, 2009). Inference at these levels usually re-
quires substantial processing and resources, aim-
ing at high performance. Nevertheless, simple lex-
ical level entailment systems pose strong baselines
which most complex entailment systems did not out-
perform (Mirkin et al, 2009a; Majumdar and Bhat-
tacharyya, 2010). Additionally, within a complex
system, lexical entailment modeling is one of the
most effective component. Finally, the simpler lex-
ical approach can be used in cases where complex
systems cannot be used, e.g. when there is no parser
for a targeted language.
For these reasons lexical entailment systems are
widely used. They derive sentence-level entailment
decision base on lexical-level entailment evidence.
Typically, this is done by quantifying the degree of
lexical coverage of the hypothesis terms by the text
terms (where a term may be multi-word). A hy-
pothesis term is covered by a text term if either they
are identical (possibly at the stem or lemma level)
or there is a lexical entailment rule suggesting the
entailment of the former by the latter. Such rules
are derived from lexical semantic resources, such
as WordNet (Fellbaum, 1998), which capture lexi-
cal entailment relations.
Common heuristics for quantifying the degree of
coverage are setting a threshold on the percentage
of coverage of H?s terms (Majumdar and Bhat-
tacharyya, 2010), counting the absolute number of
uncovered terms (Clark and Harrison, 2010), or ap-
plying an Information Retrieval-style vector space
similarity score (MacKinlay and Baldwin, 2009).
Other works (Corley and Mihalcea, 2005; Zanzotto
and Moschitti, 2006) have applied heuristic formu-
10
las to estimate the similarity between text fragments
based on a similarity function between their terms.
The above mentioned methods do not capture sev-
eral important aspects of entailment. Such aspects
include the varying reliability levels of entailment
resources and the impact of rule chaining and multi-
ple evidence on entailment likelihood. An additional
observation from these and other systems is that
their performance improves only moderately when
utilizing lexical-semantic resources2.
We believe that the textual entailment field would
benefit from more principled models for various en-
tailment phenomena. In this work we formulate a
concrete generative probabilistic modeling frame-
work that captures the basic aspects of lexical entail-
ment. A first step in this direction was proposed in
Shnarch et al (2011) (a short paper), where we pre-
sented a base model with a somewhat complicated
and difficult to estimate extension to handle cover-
age. This paper extends that work to a more mature
model with new extensions.
We first consider the ?logical? structure of lexical
entailment reasoning and then interpret it in proba-
bilistic terms. Over this base model we suggest sev-
eral extensions whose significance is then assessed
by our evaluations. Learning the parameters of a
lexical model poses a challenge since there are no
lexical-level entailment annotations. We do, how-
ever, have sentence-level annotations available for
the RTE data sets. To bridge this gap, we formu-
late an instance of the EM algorithm (Dempster et
al., 1977) to estimate hidden lexical-level entailment
parameters from sentence-level annotations.
Overall, we suggest that the main contribution of
this paper is in presenting a probabilistic model for
lexical entailment. Such a model can better integrate
entailment indicators and has the advantage of being
able to utilize well-founded probabilistic methods
such as the EM algorithm. Our model?s performance
is in line with the best entailment systems, while
opening up directions for future improvements.
2 Background
We next review several entailment systems, mostly
those that work at the lexical level and in particular
2See ablation tests reports in http://aclweb.org/aclwiki/ in-
dex.php?title=RTE Knowledge Resources#Ablation Tests
those with which we compare our results on the RTE
data sets.
The 5th Recognizing Textual Entailment chal-
lenge (RTE-5) introduced a new pilot task (Ben-
tivogli et al, 2009) which became the main task in
RTE-6 (Bentivogli et al, 2010). In this task the goal
is to find all sentences that entail each hypothesis in
a given document cluster. This task?s data sets re-
flect a natural distribution of entailments in a corpus
and demonstrate a more realistic scenario than the
earlier RTE challenges.
As reviewed in the following paragraphs there are
several characteristic in common to most entailment
systems: (1) lexical resources have a minimal im-
pact on their performance, (2) they heuristically uti-
lize lexical resources, and (3) there is no principled
method for making the final entailment decision.
The best performing system of RTE-5 was pre-
sented by Mirkin et. al (2009a). It applies super-
vised classifiers over a parse tree representations to
identify entailment. They reported that utilizing lex-
ical resources only slightly improved their perfor-
mance.
MacKinlay and Baldwin (2009) presented the
best lexical-level system at RTE-5. They use a vec-
tor space method to measure the lexical overlap be-
tween the text and the hypothesis. Since usually
texts of RTE are longer than their corresponding hy-
potheses, the standard cosine similarity score came
out lower than expected. To overcome this prob-
lem they suggested a simple ad-hoc variant of the
cosine similarity score which removed from the text
all terms which did not appear in the correspond-
ing hypothesis. While this heuristic improved per-
formance considerably, they reported a decrease in
performance when utilizing synonym and derivation
relations from WordNet.
On the RTE-6 data set, the syntactic-based sys-
tem of Jia et. al (2010) achieved the best results,
only slightly higher than the lexical-level system
of (Majumdar and Bhattacharyya, 2010). The lat-
ter utilized several resources for matching hypoth-
esis terms with text terms: WordNet, VerbOcean
(Chklovski and Pantel, 2004), utilizing two of its
relations, as well as an acronym database, num-
ber matching module, co-reference resolution and
named entity recognition tools. Their final entail-
ment decision was based on a threshold over the
11
number of matched hypothesis terms. They found
out that hypotheses of different length require dif-
ferent thresholds.
While the above systems measure the number of
hypothesis terms matched by the text, Clark and
Harrison (2010) based their entailment decision on
the number of mismatched hypothesis terms. They
utilized both WordNet and the DIRT paraphrase
database (Lin and Pantel, 2001). With WordNet,
they used one set of relations to identify the concept
of a term while another set of relations was used to
identify entailment between concepts. Their results
were inconclusive about the overall effect of DIRT
while WordNet produced a net benefit in most con-
figurations. They have noticed that setting a global
threshold for the entailment decision, decreased per-
formance for some topics of the RTE-6 data set.
Therefore, they tuned a varying threshold for each
topic based on an idiosyncracy of the data, by which
the total number of entailments per topic is approxi-
mately a constant.
Glickman et al (2005) presented a simple model
that recasted the lexical entailment task as a variant
of text classification and estimated entailment prob-
abilities solely from co-occurrence statistics. Their
model did not utilize any lexical resources.
In contrary to these systems, our model shows
improvement when utilizing high quality resources
such as WordNet and the CatVar (Categorial Varia-
tion) database (Habash and Dorr, 2003). As Majum-
dar and Bhattacharyya (2010), our model considers
the impact of hypothesis length, however it does not
require the tuning of a unique threshold for each
length. Finally, most of the above systems do not
differentiate between the various lexical resources
they use, even though it is known that resources re-
liability vary considerably (Mirkin et al, 2009b).
Our probabilistic model, on the other hand, learns
a unique reliability parameter for each resource it
utilizes. As mentioned above, this work extends the
base model in (Shnarch et al, 2011), which is de-
scribed in the next section.
3 A Probabilistic Model
We aim at obtaining a probabilistic score for the like-
lihood that the hypothesis terms are entailed by the
terms of the text. There are several prominent as-
cro
wd 
 
sur
rou
nd  
Jag
uar
Text Hyp
othe
sis
h jh j
h nh n
t 1t 1
t it i
t mt m
t'
Res
ourc
e 1
chain
yy
OR
OR
50 p
eop
le s
urr
ou
nd c
ar
OR
OR
so
cial gro
up
Res
ourc
e 1 O
R
Res
ourc
e 1
Res
ourc
e 3
Res
ourc
e 2 h 1h 1
Res
ourc
e 1
MA
TCH
MA
TCH
Res
ourc
e 3
Res
ourc
e 2
UNC
OVE
RED
Figure 1: Left: the base model of entailing a hypothesis from
a text; Right: a concrete example for it (stop-words removed).
Edges in the upper part of the diagram represent entailment
rules. Rules compose chains through AND gates (omitted for
visual clarity). Chains are gathered by OR gates to entail terms,
and the final entailment decision y is the result of their AND
gate.
pects of entailment, mostly neglected by previous
lexical methods, which our model aims to capture:
(1) the reliability variability of different lexical re-
sources; (2) the effect of the length of transitive rule
application chain on the likelihood of its validity;
and (3) addressing cases of multiple entailment evi-
dence when entailing a term.
3.1 The Base Model
Our base model follows the one presented in
(Shnarch et al, 2011), which is described here in
detail to make the current paper self contained.
3.1.1 Entailment generation process
We first specify the process by which a decision
of lexical entailment between T andH using knowl-
edge resources should be determined, as illustrated
in Figure 1 (a general description on the left and
a concrete example on the right). There are two
ways by which a term h ? H is entailed by a term
t ? T . A direct MATCH is the case in which t and
h are identical terms (possibly at the stem or lemma
level). Alternatively, lexical entailment can be es-
tablished based on knowledge of entailing lexical-
12
semantic relations, such as synonyms, hypernyms
and morphological derivations, available in lexical
resources. These relations provide lexical entail-
ment rules, e.g. Jaguar ? car. We denote the re-
source which provided the rule r by R(r).
It should be noticed at this point that such rules
specify a lexical entailment relation that might hold
for some (T,H) pairs but not necessarily for all
pairs, e.g. the rule Jaguar ? car does not hold
in the wildlife context. Thus, the application of an
available rule to infer lexical entailment in a given
(T,H) pair might be either valid or invalid. We note
here the difference between covering a term and en-
tailing it. A term is covered when the available re-
sources suggest its entailment. However, since a rule
application may be invalid for the particular (T,H)
context, a term is entailed only if there is a valid rule
application from T to it.
Entailment is a transitive relation, therefore rules
may compose transitive chains that connect t to h
via intermediate term(s) t? (e.g. crowd ? social
group ? people). For a chain to be valid for the
current (T,H) pair, all its composing rule applica-
tions should be valid for this pair. This corresponds
to a logical AND gate (omitted in Figure 1 for visual
clarity) which takes as input the validity values (1/0)
of the individual rule applications.
Next, multiple chains may connect t to h (as for
ti and hj in Figure 1) or connect several terms in
T to h (as t1 and ti are indicating the entailment of
hj in Figure 1), thus providing multiple evidence for
h?s entailment. For a term h to be entailed by T it
is enough that at least one of the chains from T to
h would be valid. This condition is realized in the
model by an OR gate. Finally, for T to lexically en-
tail H it is usually assumed that every h?H should
be entailed by T (Glickman et al, 2006). Therefore,
the final decision follows an AND gate combining
the entailment decisions for all hypothesis terms.
Thus, the 1-bit outcome of this gate y corresponds
to the sentence-level entailment status.
3.1.2 Probabilistic Setting
When assessing entailment for (T,H) pair, we do
not know for sure which rule applications are valid.
Taking a probabilistic perspective, we assume a pa-
rameter ?R for each resourceR, denoting its reliabil-
ity, i.e. the prior probability that applying a rule from
R for an arbitrary (T,H) pair corresponds to valid
entailment3. Under this perspective, direct MATCHs
are considered as rules coming from a special ?re-
source?, for which ?MATCH is expected to be close to
1. Additionally, there could be a term h which is not
covered by any of the resources at hand, whose cov-
erage is inevitably incomplete. We assume that each
such h is covered by a single rule coming from a
dummy resource called UNCOVERED, while expect-
ing ?UNCOVERED to be relatively small. Based on the
?R values we can now estimate, for each entailment
inference step in Figure 1, the probability that this
step is valid (the corresponding bit is 1).
Equations (1) - (3) correspond to the three steps in
calculating the probability for entailing a hypothesis.
p(t
c
?? h) =
?
r?c
p(L
r
?? R) =
?
r?c
?R(r) (1)
p(T?h) =1?p(T9h)=1?
?
c?C(h)
[1?p(t
c
?? h)] (2)
p(T?H) =
?
h?H
p(T?h) (3)
First, Eq. (1) specifies the probability of a partic-
ular chain c, connecting a text term t to a hypothesis
term h, to correspond to a valid entailment between
t and h. This event is denoted by t
c
??h and its prob-
ability is the joint probability that the applications
of all rules r ? c are valid. Note that every rule r
in a chain c connects two terms, its left-hand-side L
and its right-hand-side R. The left-hand-side of the
first rule in c is t? T and the right-hand-side of the
last rule in it is h ? H . Let us denote the event of
a valid rule application by L
r
??R. Since a-priori a
rule r is valid with probability ?R(r), and assuming
independence of all r?c, we obtain Eq. (1).
Next, Eq. (2) utilizes Eq. (1) to specify the prob-
ability that T entails h (at least by one chain). Let
C(h) denote the set of chains which suggest the en-
tailment of h. The requested probability is equal to
1 minus the probability of the complement event,
that is, T does not entail h by any chain. The lat-
ter probability is the product of probabilities that all
3Modeling a conditional probability for the validity of r,
which considers contextual aspects of r?s validity in the current
(T,H) context, is beyond the scope of this paper (see discus-
sion in Section 6)
13
chains c?C(h) are not valid (again assuming inde-
pendence of chains).
Finally, Eq. (3) gives the probability that T entails
all of H (T ? H), assuming independence of H?s
terms. This is the probability that every h ? H is
entailed by T , as specified by Eq. (2).
Altogether, these formulas fall out of the standard
probabilistic estimate for the output of AND and OR
gates when assuming independence amongst their
input bits.
As can be seen, the base model distinguishes
varying resource reliabilities, as captured by ?R, de-
creases entailment probability as rule chain grows,
having more elements in the product of Eq. (1), and
increases it when entailment of a term is supported
by multiple chains with more inputs to the OR gate.
Next we describe two extensions for this base model
which address additional important phenomena of
lexical entailment.
3.2 Relaxing the AND Gate
Based on term-level decisions for the entailment of
each h ? H , the model has to produce a sentence-
level decision of T ? H . In the model described so
far, for T to entailH it must entail all its terms. This
demand is realized by the AND gate at the bottom of
Figure 1. In practice, this demand is too strict, and
we would like to leave some option for entailing H
even if not every h?H is entailed. Thus, it is desired
to relax this strict demand enforced by the AND gate
in the model.
OR
AND
b1
OR
xn
bn
x1
Noisy-AND
y
Figure 2: A noisy-AND gate
The Noisy-AND model (Pearl, 1988), depicted in
Figure 2, is a soft probabilistic version of the AND
gate, which is often used to describe the interaction
between causes and their common effect. In this
variation, each one of the binary inputs b1, ..., bn of
the AND gate is first joined with a ?noise? bit xi by
an OR gate. Each ?noise? bit is 1 with probability p,
which is the parameter of the gate. The output bit y
is defined as:
y = (b1 ? x1) ? (b2 ? x2) ? ? ? ? ? (bn ? xn)
and the conditional probability for it to be 1 is:
p(y = 1|b1, ..., bn, n) =
n?
i=1
p(1?bi) = p(n?
?
i bi)
If all the binary input values are 1, the output is de-
terministically 1. Otherwise, the probability that the
output is 1 is proportional to the number of ones in
the input, where the distribution depends on the pa-
rameter p. In case p = 0 the model reduces to the
regular AND.
In our model we replace the final strict AND with
a noisy-AND, thus increasing the probability of T to
entail H , to account for the fact that sometimes H
might be entailed from T even though some h ?H
is not directly entailed.
The input size n for the noisy-AND is the length
of the hypotheses and therefore it varies from H to
H . Had we used the same model parameter p for all
lengths, the probability to output 1 would have de-
pended solely on the number of 0 bits in the input
without considering the number of ones. For exam-
ple, the probability to entail a hypothesis with 10
terms given that 8 of them are entailed by T (and 2
are not) is p2. The same probability is obtained for a
hypothesis of length 3 with a single entailed term.
We, however, expect the former to have a higher
probability since a larger portion of its terms is en-
tailed by T .
There are many ways to incorporate the length of
a hypothesis into the noisy-AND model in order to
normalize its parameter. The approach we take is
defining a separate parameter pn for each hypothesis
length n such that pn = ?
1
n
NA, where ?NA becomes
the underlying parameter value of the noisy-AND,
i.e.
p(y = 1|b1, ..., bn, n) = p
(n?
?
bi)
n = ?
n?
?
bi
n
NA
This way, if non of the hypothesis terms is entailed,
the probability for its entailment is ?NA, indepen-
dent of its length:
p(y = 1|0, 0, ..., 0, n) = pnn = ?NA
14
As can be seen from Figure 1, replacing the final
AND gate by a noisy-AND gate is equivalent to
adding an additional chain to the OR gate of each
hypothesis term. Therefore we update Eq. (2) to:
p(T ? h) =1? p(T 9 h)
=1? [(1? ?
1
n
NA) ?
?
c?C(h)
[1? p(t
c
?? h)]]
(2?)
In the length-normalized noisy-AND model the
value of the parameter p becomes higher for longer
hypotheses. This increases the probability to entail
such hypotheses, compensating for the lower proba-
bility to strictly entail all of their terms.
3.3 Considering Coverage Level
The second extension of the base model follows our
observation that the prior validity likelihood for a
rule application, increases as more of H?s terms are
covered by the available resources. In other words,
if we have a hypothesis H1 with k covered terms
and a hypothesis H2 in which only j < k terms are
covered, then an arbitrary rule application for H1 is
more likely to be valid than an arbitrary rule appli-
cation for H2.
We chose to model this phenomenon by normal-
izing the reliability ?R of each resource according
to the number of covered terms in H . The normal-
ization is done in a similar manner to the length-
normalized noisy-AND described above, obtaining
a modified version of Eq. (1):
p(t
c
?? h) =
?
r?c
?
1
#covered
R(r) (1
?)
As a results, the larger the number of covered terms
is, the larger ?R values our model uses and, in total,
the entailment probability increases.
To sum up, we have presented the base model,
providing a probabilistic estimate for the entailment
status in our generation process specified in 3.1.
Two extensions were then suggested: one that re-
laxes the strict AND gate and normalizes this re-
laxation by the length of the hypothesis; the second
extension adjusts the validity of rule applications as
a function of the number of the hypothesis covered
terms. Overall, our full model combines both exten-
sions over the base probabilistic model.
4 Parameter Estimation
The difficulty in estimating the ?R values from train-
ing data arises because these are term-level param-
eters while the RTE-training entailment annotation
is given for the sentence-level, each (T,H) pair in
the training is annotated as either entailing or not.
Therefore, we use an instance of the EM algorithm
(Dempster et al, 1977) to estimate these hidden pa-
rameters.
4.1 E-Step
In the E-step, for each application of a rule r in a
chain c for h?H in a training pair (T,H), we com-
pute whcr(T,H), the posterior probability that the
rule application was valid given the training annota-
tion:
whcr(T,H) =
?
?
?
p(L
r
??R|T?H) if T?H
p(L
r
??R|T9H) if T9H
(4)
where the two cases refer to whether the training pair
is annotated as entailing or non-entailing. For sim-
plicity, we write whcr when the (T,H) context is
clear.
The E-step can be efficiently computed using
dynamic programming as follows; For each train-
ing pair (T,H) we first compute the probability
p(T ? H) and keep all the intermediate computa-
tions (Eq. (1)- (3)). Then, the two cases of Eq. (4),
elaborated next, can be computed from these expres-
sions. For computing Eq. (4) in the case that T?H
we have:
p(L
r
?? R|T?H) = p(L
r
?? R|T ? h) =
p(T?h|L
r
?? R)p(L
r
??R)
p(T?h)
The first equality holds since when T entails H ev-
ery h ? H is entailed by it. Then we apply Bayes?
rule. We have already computed the denominator
(Eq. (2)), p(L
r
?? R) ? ?R(r) and it can be shown
4
that:
p(T?h|L
r
??R) = 1?
p(T9h)
1? p(t
c
??h)
? (1?
p(t
c
??h)
?R(r)
)
(5)
4The first and second denominators reduce elements from
the products in Eq. 2 and Eq. 1 correspondingly
15
where c is the chain which contains the rule r.
For computing Eq. (4), in the second case, that
T9H , we have:
p(L
r
??R|T9H) =
p(T9H|L r??R)p(L r??R)
p(T9H)
In analogy to Eq. (5) it can be shown that
p(T9H|L r??R) = 1?
p(T?H)
p(T?h)
?p(T?h|L
r
??R)
(6)
while the expression for p(T?h|L
r
??R) appears in
Eq. (5).
This efficient computation scheme is an instance
of the belief-propagation algorithm (Pearl, 1988) ap-
plied to the entailment process, which is a loop-free
directed graph (Bayesian network).
4.2 M-Step
In the M-step we need to maximize the EM auxiliary
function Q(?) where ? is the set of all resources re-
liability values. Applying the derivation of the aux-
iliary function to our model (first without the exten-
sions) we obtain:
Q(?) =
?
T,H
?
h?H
?
c?C(h)
?
r?c
(whcr log ?R(r) +
(1? whcr) log(1? ?R(r)))
We next denote by nR the total number of applica-
tions of rules from resource R in the training data.
We can maximize Q(?) for each R separately to ob-
tain the M-step parameter-updating formula:
?R =
1
nR
?
T,H
?
h?H
?
c?C(h)
?
r?c|R(r)=R
whcr (7)
The updated parameter value averages the posterior
probability that rules from resource R have been
validly applied, across all its utilizations in the train-
ing data.
4.3 EM for the Extended Model
In case we normalize the noisy-AND parameter by
the hypothesis length, for each length we use a dif-
ferent parameter value for the noisy-AND and we
cannot simply merge the information from all the
training pairs (T,H). To find the optimal param-
eter value for ?NA, we need to maximize the fol-
lowing expression (the derivation of the auxiliary
function to the hypothesis-length-normalized noisy-
AND ?resource?):
Q(?NA) =
?
T,H
?
h?H
(whNA log(?
1
n
NA) +
(1? whNA) log(1? ?
1
n
NA)) (8)
where n is the length of H , ?NA is the parameter
value of the noisy-AND model andwhNA is the pos-
terior probability that the noisy-AND was used to
validly entail the term h5, i.e.
whNA(T,H) =
?
??
??
p(T
NA
???h|T?H) if T?H
p(T
NA
???h|T9H) if T9H
The two cases of the above equation are similar to
Eq. (4) and can be efficiently computed in analogy
to Eq. (5) and Eq. (6).
There is no close-form expression for the param-
eter value ?NA that maximizes expression (8). Since
?NA?[0, 1] is a scalar parameter, we can find ?NA
value that maximizes Q(?NA) using an exhaustive
grid search on the interval [0, 1], in each iteration of
the M-step. Alternatively, for an iterative procedure
to maximize expression (8), see Appendix A.
In the same manner we address the normalization
of the reliability ?R of each resourcesR by the num-
ber of H?s covered terms. Expression (8) becomes:
Q(?R) =
?
T,H
?
h?H
?
c?C(h)
?
r?c|R(r)=R
(whcr log(?
cov
R ) + (1? whcr) log(1? ?
cov
R ))
were 1cov is the number of H terms which are cov-
ered. We can find the ?R that maximizes this equa-
tion in one of the methods described above.
5 Evaluation and Results
For our evaluation we use the RTE-5 pilot task and
the RTE-6 main task data sets described in Sec-
tion 2. In our system, sentences are tokenized and
stripped of stop words and terms are tagged for part-
of-speech and lemmatized. We utilized two lexical
resources, WordNet (Fellbaum, 1998) and CatVar
5In contrary to Eq. 4, here there is no specific t ? T that
entails h, therefore we write T
NA
???h
16
(Habash and Dorr, 2003). From WordNet we took as
entailment rules synonyms, derivations, hyponyms
and meronyms of the first senses of T and H terms.
CatVar is a database of clusters of uninflected words
(lexemes) and their categorial (i.e. part-of-speech)
variants (e.g. announce (verb), announcer and an-
nouncement(noun) and announced (adjective)). We
deduce an entailment relation between any two lex-
emes in the same cluster. Model?s parameters were
estimated from the development set, taken as train-
ing. Based on these parameters, the entailment prob-
ability was estimated for each pair (T,H) in the test
set, and the classification threshold was tuned by
classification over the development set.
We next present our evaluation results. First we
investigate the impact of utilizing lexical resources
and of chaining rules. In section 5.2 we evaluate the
contribution of each extension of the base model and
in Section 5.3 we compare our performance to that
of state-of-the-art entailment systems.
5.1 Resources and Rule-Chaining Impact
As mentioned in Section 2, in the RTE data sets it
is hard to show more than a moderate improvement
when utilizing lexical resources. Our analysis as-
cribes this fact to the relatively small amount of rule
applications in both data sets. For instance, in RTE-
6 there are 10 times more direct matches of identi-
cal terms than WordNet and CatVar rule applications
combined, while in RTE-5 this ratio is 6. As a result
the impact of rule applications can be easily shad-
owed by the large amount of direct matches.
Table 1 presents the performance of our (full)
model when utilizing no resources at all, WordNet,
CatVar and both, with chains of a single step. We
also considered rule chains of length up to 4 and
present here the results of 2 chaining steps with
WordNet-2 and (WordNet+CatVar)-2.
Overall, despite the low level of rule applications,
we see that incorporating lexical resources in our
model significantly6 and quite consistently improves
performance over using no resources at all. Natu-
rally, the optimal combination of resources may vary
somewhat across the data sets.
In RTE-6 WordNet-2 significantly improved per-
6All significant results in this section are according to Mc-
Nemar?s test with p < 0.01 unless stated otherwise
formance over the single-stepped WordNet. How-
ever, mostly chaining did not help, suggesting the
need for future work to improve chain modeling in
our framework.
Model
F1%
RTE-5 RTE-6
no resources 41.6 44.9
WordNet 45.8 44.6
WordNet-2 45.7 45.5
CatVar 46.9 45.6
WordNet + CatVar 48.3 45.6
(WordNet + CatVar)-2 47.1 44.0
Table 1: Evaluation of the impact of resources and chaining.
5.2 Model Components impact
We next assess the impact of each of our proposed
extensions to the base probabilistic model. To that
end, we incorporate WordNet+CatVar (our best con-
figuration above) as resources for the base model
(Section 3.1) and compare it with the noisy-AND
extension (Eq. (2?)), the covered-norm extension
which normalizes the resource reliability parame-
ter by the number of covered terms (Eq. (1?)) and
the full model which combines both extensions. Ta-
ble 2 presents the results: both noisy-AND and
covered-norm extensions significantly increase F1
over the base model (by 4.5-8.4 points). This scale
of improvement was observed with all resources and
chain-length combinations. In both data sets, the
combination of noisy-AND and covered-norm ex-
tensions in the full model significantly outperforms
each of them separately7, showing their complemen-
tary nature. We also observed that applying noisy-
AND without the hypothesis length normalization
hardly improved performance over the base model,
emphasising the importance of considering hypothe-
sis length. Overall, we can see that both base model
extensions improve performance.
Table 3 illustrates a set of maximum likelihood
parameters that yielded our best results (full model).
The parameter value indicates the learnt reliability
of the corresponding resource.
7With the following exception: in RTE-5 the full model is
better than the noisy-AND extension with significance of only
p = 0.06
17
Model
F1%
RTE-5 RTE-6
base model 36.2 38.5
noisy-AND 44.6 43.1
covered-norm 42.8 44.7
full model 48.3 45.6
Table 2: Impact of model components.
?MATCH ?WORDNET ?CATVAR ?UNCOVERED ?NA
0.80 0.70 0.65 0.17 0.05
Table 3: A parameter set of the full model which maximizes
the likelihood of the training set.
5.3 Comparison to Prior Art
Finally, in Table 4, we put these results in the con-
text of the best published results on the RTE task.
We compare our model to the average of the best
runs of all systems, the best and second best per-
forming lexical systems and the best full system of
each challenge. For both data sets our model is situ-
ated high above the average system. For the RTE-6
data set, our model?s performance is third best with
Majumdar and Bhattacharyya (2010) being the only
lexical-level system which outperforms it. However,
their system utilized additional processing that we
did not, such as named entity recognition and co-
reference resolution8. On the RTE-5 data set our
model outperforms any other published result.
Model
F1%
RTE-5 RTE-6
full model 48.3 45.6
avg. of all systems 30.5 33.8
2nd best lexical system 40.3a 44.0b
best lexical system 44.4c 47.6d
best full system 45.6c 48.0e
Table 4: Comparison to RTE-5 and RTE-6 best entailment
systems: (a)(MacKinlay and Baldwin, 2009), (b)(Clark and
Harrison, 2010), (c)(Mirkin et al, 2009a)(2 submitted runs),
(d)(Majumdar and Bhattacharyya, 2010) and (e)(Jia et al,
2010).
8We note that the submitted run which outperformed our re-
sult utilized a threshold which was a manual modification of the
threshold obtained systematically in another run. The latter run
achieved F1 of 42.4% which is below our result.
We conclude that our probabilistic model demon-
strates quality results which are also consistent,
without applying heuristic methods of the kinds re-
viewed in Section 2
6 Conclusions and Future Work
We presented, a probabilistic model for lexical en-
tailment whose innovations are in (1) considering
each lexical resource separately by associating an
individual reliability value for it, (2) considering the
existence of multiple evidence for term entailment
and its impact on entailment assessment, (3) setting
forth a probabilistic method to relax the strict de-
mand that all hypothesis terms must be entailed, and
(4) taking account of the number of covered terms in
modeling entailment reliability.
We addressed the impact of the various compo-
nents of our model and showed that its performance
is in line with the best state-of-the-art inference sys-
tems. Future work is still needed to reflect the im-
pact of transitivity. We consider replacing the AND
gate on the rules of a chain by a noisy-AND, to relax
its strict demand that all its input rules must be valid.
Additionally, we would like to integrate Contextual
Preferences (Szpektor et al, 2008) and other works
on Selectional Preference (Erk and Pado, 2010) to
verify the validity of the application of a rule in a
specific (T,H) context. We also intend to explore
the contribution of our model within a complex sys-
tem that integrates multiple levels of inference as
well as its contribution for other applications, such
as Passage Retrieval.
References
Roy Bar-Haim, Jonathan Berant, Ido Dagan, Iddo Green-
tal, Shachar Mirkin, Eyal Shnarch, and Idan Szpektor.
2008. Efficient semantic deduction and approximate
matching over compact parse forests. In Proc. of TAC.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The fifth
PASCAL recognizing textual entailment challenge. In
Proc. of TAC.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. The sixth
PASCAL recognizing textual entailment challenge. In
Proc. of TAC.
Aljoscha Burchardt, Nils Reiter, Stefan Thater, and
Anette Frank. 2007. A semantic approach to textual
18
entailment: System evaluation and task analysis. In
Proc. of the ACL-PASCAL Workshop on Textual En-
tailment and Paraphrasing.
Timothy Chklovski and Patrick Pantel. 2004. Verbo-
cean: Mining the web for fine-grained semantic verb
relations. In Proc. of EMNLP.
Peter Clark and Phil Harrison. 2010. BLUE-Lite: a
knowledge-based lexical entailment system for RTE6.
In Proc. of TAC.
Courtney Corley and Rada Mihalcea. 2005. Measuring
the semantic similarity of texts. In Proc. of the ACL
Workshop on Empirical Modeling of Semantic Equiv-
alence and Entailment.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In Lecture Notes in Computer Science, vol-
ume 3944, pages 177?190.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the royal statistical society, se-
ries [B], 39(1):1?38.
Katrin Erk and Sebastian Pado. 2010. Exemplar-based
models for word meaning in context. In Proc. of the
ACL.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and Com-
munication). The MIT Press.
Oren Glickman, Ido Dagan, and Moshe Koppel. 2005. A
probabilistic classification approach for lexical textual
entailment. In Proc. of AAAI.
Oren Glickman, Eyal Shnarch, and Ido Dagan. 2006.
Lexical reference: a semantic matching subtask. In
Proceedings of the EMNLP.
Nizar Habash and Bonnie Dorr. 2003. A categorial vari-
ation database for english. In Proc. of NAACL.
Houping Jia, Xiaojiang Huang, Tengfei Ma, Xiaojun
Wan, and Jianguo Xiao. 2010. PKUTM participation
at TAC 2010 RTE and summarization track. In Proc.
of TAC.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7:343?360.
Bill MacCartney and Christopher D. Manning. 2007.
Natural logic for textual inference. In Proc. of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing.
Andrew MacKinlay and Timothy Baldwin. 2009. A
baseline approach to the RTE5 search pilot. In Proc.
of TAC.
Debarghya Majumdar and Pushpak Bhattacharyya.
2010. Lexical based text entailment system for main
task of RTE6. In Proc. of TAC.
Shachar Mirkin, Roy Bar-Haim, Jonathan Berant, Ido
Dagan, Eyal Shnarch, Asher Stern, and Idan Szpektor.
2009a. Addressing discourse and document structure
in the RTE search task. In Proc. of TAC.
Shachar Mirkin, Ido Dagan, and Eyal Shnarch. 2009b.
Evaluating the inferential utility of lexical-semantic re-
sources. In Proc. of EACL.
Judea Pearl. 1988. Probabilistic reasoning in intelli-
gent systems: networks of plausible inference. Morgan
Kaufmann.
Eyal Shnarch, Jacob Goldberger, and Ido Dagan. 2011.
A probabilistic modeling framework for lexical entail-
ment. In Proc. of ACL, pages 558?563.
Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob
Goldberger. 2008. Contextual preferences. In Proc.
of ACL-08: HLT.
Marta Tatu and Dan Moldovan. 2007. COGEX at RTE
3. In Proc. of the ACL-PASCAL Workshop on Textual
Entailment and Paraphrasing.
Rui Wang, Yi Zhang, and Guenter Neumann. 2009. A
joint syntactic-semantic representation for recognizing
textual relatedness. In Proc. of TAC.
Fabio Massimo Zanzotto and Alessandro Moschitti.
2006. Automatic learning of textual entailments with
cross-pair similarities. In Proc. of ACL.
A Appendix: An Iterative Procedure to
Maximize Q(?NA)
There is no close-form expression for the parameter
value ?NA that maximizes expression (8) from Sec-
tion 4.3. Instead we can apply the following iterative
procedure. The derivative of Q(?NA) is:
dQ(?NA)
d?NA
=
?
(
l?whNA
?NA
?
(1?whNA)l??
(l?1)
NA
1? ?lNA
)
where 1l is the hypothesis length and the summation
is over all terms h in the training set. Setting this
derivative to zero yields an equation which the opti-
mal value satisfies:
?NA =
?
l?whNA
? (1?whNA)l??
(l?1)
NA
1??lNA
(9)
Eq. (9) can be utilized as a heuristic iterative proce-
dure to find the optimal value of ?NA:
?NA ?
?
l?whNA
? (1?whNA)l??
(l?1)
NA
1??lNA
19
