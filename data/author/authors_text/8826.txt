SCANMail: Audio Navigation in the Voicemail Domain
Michiel Bacchiani Julia Hirschberg Aaron Rosenberg Steve Whittaker
Donald Hindle Phil Isenhour Mark Jones Litza Stark
Gary Zamchick
fmichiel,julia,aer,stevewg@research.att.com, dhindle@answerlogic.com, isenhour@vt.edu,
jones@research.att.com, litza@udel.edu, zamchick@attlabs.att.com
AT&T Labs ? Research
180 Park Avenue
Florham Park, NJ 07932-0971, USA
ABSTRACT
This paper describes SCANMail, a system that allows users to
browse and search their voicemail messages by content through
a GUI. Content based navigation is realized by use of automatic
speech recognition, information retrieval, information extraction
and human computer interaction technology. In addition to the
browsing and querying functionalities, acoustics-based caller ID
technology is used to proposes caller names from existing caller
acoustic models trained from user feedback. The GUI browser also
provides a note-taking capability. Comparing SCANMail to a regu-
lar voicemail interface in a user study, SCANMail performed better
both in terms of objective (time to and quality of solutions) as well
as subjective objectives.
1. INTRODUCTION
Increasing amounts of public, corporate, and private audio present
a major challenge to speech, information retrieval, and human-
computer interaction research: how can we help people to take ad-
vantage of these resources when current techniques for navigating
them fall far short of text-based search methods? In this paper,
we describe SCANMail, a system that employs automatic speech
recognition (ASR), information retrieval (IR), information extrac-
tion (IE), and human computer interaction (HCI) technology to per-
mit users to browse and search their voicemail messages by content
through a GUI interface. A CallerId server also proposes caller
names from existing caller acoustic models and is trained from
user feedback. An Email server sends the original message plus
its ASR transcription to a mailing address specified in the user?s
profile. The SCANMail GUI also provides note-taking capabilities
as well as browsing and querying features. Access to messages and
information about them is presented to the user via a Java applet
running under Netscape. Figure 1 shows the SCANMail GUI.
.
2. SYSTEM DESCRIPTION
In SCANMail, messages are first retrieved from a voicemail server,
then processed by the ASR server that provides a transcription. The
message audio and/or transcription are then passed to the IE, IR,
Email, and CallerId servers. The acoustic and language model of
the recognizer, and the IE and IR servers are trained on 60 hours of
a 100 hour voicemail corpus, transcribed and hand labeled for tele-
phone numbers, caller names, times, dates, greetings and closings.
The corpus includes approximately 10,000 messages from approx-
imately 2500 speakers. About 90% of the messages were recorded
from regular handsets, the rest from cellular and speaker-phones.
The corpus is approximately gender balanced and approximately
12% of the messages were from non-native speakers. The mean
duration of the messages was 36.4 seconds; the median was 30.0
seconds.
2.1 Automatic Speech Recognition
The baseline ASR system is a decision-tree based state-clustered
triphone system with 8k tied states. The emission probabilities of
the states are modeled by 12 component Gaussian mixture distribu-
tions. The system uses a 14k vocabulary, automatically generated
by the AT&T Labs NextGen Text To Speech system. The language
model is a Katz-style backoff trigram trained on 700k words from
the transcriptions of the 60 hour training set. The word-error rate
of this system on a 40 hour test set is 34.9%.
Since the messages come from a highly variable source both in
terms of speaker as well as channel characteristics, transcription ac-
curacy is significantly improved by application of various normal-
ization techniques, developed for Switchboard evaluations [9]. The
ASR server uses Vocal Tract Length Normalization (VTLN) [5],
Constrained Modelspace Adaptation (CMA) [3], Maximum Like-
lihood Linear Regression (MLLR) [6] and Semi-Tied Covariances
(STC) [4] to obtain progressively more accurate acoustic models
and uses these in a rescoring framework. In contrast to Switch-
board, voicemail messages are generally too short too allow direct
application of the normalization techniques. A novel message clus-
tering algorithm based on MLLR likelihood [1] is used to guarantee
sufficient data for normalization. The final transcripts, obtained af-
ter 6 recognition passes, have a word error rate of 28.7% ? a 6.2%
accuracy improvement. Gender dependency provides 1.6% of this
gain. VTLN then additively improves accuracy with 1.0% when
applied only on the test data and an additional 0.3% when sub-
sequently applied with a VTLN trained model. The use of STC
further improves accuracy with 1.2%. Finally CMA and MLLR
provide additive gains of 1.5% and 0.6% respectively. The ASR
Figure 1: The SCANMail User Interface
server, running on a 667 MHz 21264 Alpha processor, produces
the final transcripts in approximately 20 times real-time.
2.2 Information Retrieval
Messages transcripts are indexed by the IR server using the SMART
IR [8, 2] engine. SMART is based on the vector space model
of information retrieval. It generates weighted term (word) vec-
tors for the automatic transcriptions of the messages. SMART pre-
processes the automatic transcriptions of each new message by to-
kenizing the text into words, removing common words that appear
on its stop-list, and performing stemming on the remaining words
to derive a set of terms, against which later user queries can be
compared. When the IR server is used to execute a user query, the
query terms are also converted into weighted term vectors. Vector
inner-product similarity computation is then used to rank messages
in decreasing order of their similarity to the user query.
2.3 Information Extraction
Key information is extracted from the ASR transcription by the
IE server, which currently extracts any phone numbers identified
in the message. Currently, this is done by recognizing digit strings
and scoring them based on the sequence length. An improved ex-
traction algorithm, trained on our hand-labeled voicemail corpus,
employs a digit string recognizer combined with a trigram language
model, to recognize strings in their lexical contexts, e.g. <word>
<digit string> <word>.
2.4 Caller Identification
The CallerID server proposes caller names by matching mes-
sages against existing caller models; this module is trained from
user feedback. The caller identification capability is based on text
independent speaker recognition techniques applied to the processed
speech in the voicemail messages. A user may elect to label a mes-
sage he/she has reviewed with a caller name for the purpose of
creating a speaker model for that caller. When the cumulative du-
ration of such user-labeled messages is sufficient, a caller model
is constructed. Subsequent messages will be processed and scored
against this caller model and models for other callers the user may
have designated. If the best matching model score for an incom-
ing message exceeds a decision threshold, a caller name hypothesis
is sent to the GUI client; if there is no PBX-supplied identifica-
tion (i.e. caller name supplied from the owner of the extension for
calls internal to the PBX), the CallerId hypothesis is presented in
the message header, for either accepting or editing by the user; if
there is a PBX identification, the CallerId hypothesis appears as the
first item in a user ?contact menu?, together with all previously id?d
callers for that user. To optimize the use of the available speech
data, and to speed model-building, caller models are shared among
users. Details and a performance evaluation of the CallerId process
are described in [7].
2.5 Graphical User Interface
In the SCANMail GUI, users see message headers (callerid, time
and date, length in seconds, first line of any attached note, and
presence of extracted phone numbers) as well as a thumbnail and
the ASR transcription of the current message. Any note attached
to the current message is also displayed. A search panel permits
users to search the contents of their mailbox by inputting any text
query. Results are presented in a new search window, with key-
words color-coded in the query, transcript, and thumbnail.
2.6 User Studies
User studies compared SCANMail with a standard over-the-phone
voicemail access. Eight subjects performed a series of fact-finding,
relevance ranking, and summarization tasks on artificial mailboxes
of twenty messages each, using either SCANMail or phone access.
SCANMail showed advantages for fact-finding and relevance rank-
ing tasks in quality of solution normalized by time to solution, for
fact-finding in time to solution and in overall user preference. Nor-
malized performance scores are higher when subjects employ IR
searches that are successful (i.e. the queries they choose contain
words correctly recognized by the recognizer) and for subjects who
listen to less audio and rely more upon the transcripts. However, we
also found that SCANMail?s search capability can be misleading,
causing subjects to assume that they have found all relevant doc-
uments when in fact some are NOT retrieved, and that when sub-
jects rely upon the accuracy of the ASR transcript, they can miss
crucial but unrecognized information. A trial of 10 friendly users
is currently underway, with modifications to access functionality
suggested by our subject users. A larger trial of the system is be-
ing prepared, for more extensive testing of user behavior with their
own mailboxes over time.
Acknowledgements
The authors would like to thank Andrej Ljolje, S. Parthasarathy,
Fernando Pereira, and Amit Singhal for their help in developing
this application.
3. REFERENCES
[1] M. Bacchiani. Using maximum likelihood linear regression
for segment clustering and speaker identification. In
Proceedings of the Sixth International Conference on Spoken
Language Processing, volume 4, pages 536?539, Beijing,
2000.
[2] C. Buckley. Implementation of the SMART information
retrieval system. Technical Report TR85-686, Department of
Computer Science, Cornell University, Ithaca, NY 14853,
May 1985.
[3] M. J. F. Gales. Maximum likelihood linear transformations for
hmm-based speech recognition. Computer Speech and
Language, pages 75?90, 1998.
[4] M. J. F. Gales. Semi-tied covariance matrices for hidden
markov models. IEEE Transactions on Acoustics, Speech, and
Signal Processing, 7(3), 1999.
[5] T. Kamm, G. Andreou, and J. Cohen. Vocal tract
normalization in speech recognition: Compensating for
systematic speaker variability. In Proceedings of the 15th
Annual Speech Research Symposium, pages 161?167, Johns
Hopkins University, Baltimore, MD, 1995.
[6] C. J. Legetter and P. C. Woodland. Maximum likelihood linear
regression for speaker adaptation of continuous density hidden
markov models. Computer Speech and Language, pages
171?185, 1995.
[7] A. Rosenberg, S. Parthasarathy, J. Hirschberg, and
S. Whittaker. Foldering voicemail messages by caller using
text independent speaker recognition. In Proceedings of the
Sixth International Conference on Spoken Language
Processing, Beijing, 2000.
[8] G. Salton, editor. The SMART Retrieval System?Experiments
in Automatic Document Retrieval. Prentice Hall Inc.,
Englewood Cliffs, NJ, 1971.
[9] Proceedings of the Speech Transcription Workshop,
University of Maryland, May 2000.
MATCH: An Architecture for Multimodal Dialogue Systems
Michael Johnston, Srinivas Bangalore, Gunaranjan Vasireddy, Amanda Stent
Patrick Ehlen, Marilyn Walker, Steve Whittaker, Preetam Maloor
AT&T Labs - Research, 180 Park Ave, Florham Park, NJ 07932, USA
johnston,srini,guna,ehlen,walker,stevew,pmaloor@research.att.com
Now at SUNY Stonybrook, stent@cs.sunysb.edu
Abstract
Mobile interfaces need to allow the user
and system to adapt their choice of com-
munication modes according to user pref-
erences, the task at hand, and the physi-
cal and social environment. We describe a
multimodal application architecture which
combines finite-state multimodal language
processing, a speech-act based multimodal
dialogue manager, dynamic multimodal
output generation, and user-tailored text
planning to enable rapid prototyping of
multimodal interfaces with flexible input
and adaptive output. Our testbed appli-
cation MATCH (Multimodal Access To
City Help) provides a mobile multimodal
speech-pen interface to restaurant and sub-
way information for New York City.
1 Multimodal Mobile Information Access
In urban environments tourists and residents alike
need access to a complex and constantly changing
body of information regarding restaurants, theatre
schedules, transportation topology and timetables.
This information is most valuable if it can be de-
livered effectively while mobile, since places close
and plans change. Mobile information access devices
(PDAs, tablet PCs, next-generation phones) offer
limited screen real estate and no keyboard or mouse,
making complex graphical interfaces cumbersome.
Multimodal interfaces can address this problem by
enabling speech and pen input and output combining
speech and graphics (See (Andre?, 2002) for a detailed
overview of previous work on multimodal input and
output). Since mobile devices are used in different
physical and social environments, for different tasks,
by different users, they need to be both flexible in in-
put and adaptive in output. Users need to be able to
provide input in whichever mode or combination of
modes is most appropriate, and system output should
be dynamically tailored so that it is maximally effec-
tive given the situation and the user?s preferences.
We present our testbed multimodal application
MATCH (Multimodal Access To City Help) and the
general purpose multimodal architecture underlying
it, that: is designed for highly mobile applications;
enables flexible multimodal input; and provides flex-
ible user-tailored multimodal output.
Figure 1: MATCH running on Fujitsu PDA
Highly mobile MATCH is a working city guide
and navigation system that currently enables mobile
users to access restaurant and subway information for
New York City (NYC). MATCH runs standalone on
a Fujitsu pen computer (Figure 1), and can also run
in client-server mode across a wireless network.
Flexible multimodal input Users interact with a
graphical interface displaying restaurant listings and
a dynamic map showing locations and street infor-
mation. They are free to provide input using speech,
by drawing on the display with a stylus, or by us-
ing synchronous multimodal combinations of the two
modes. For example, a user might ask to see cheap
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 376-383.
                         Proceedings of the 40th Annual Meeting of the Association for
Italian restaurants in Chelsea by saying show cheap
italian restaurants in chelsea, by circling an area on
the map and saying show cheap italian restaurants
in this neighborhood; or, in a noisy or public envi-
ronment, by circling an area and writing cheap and
italian (Figure 2). The system will then zoom to the
appropriate map location and show the locations of
restaurants on the map. Users can ask for information
about restaurants, such as phone numbers, addresses,
and reviews. For example, a user might circle three
restaurants as in Figure 3 and say phone numbers for
these three restaurants (or write phone). Users can
also manipulate the map interface directly. For exam-
ple, a user might say show upper west side or circle
an area and write zoom.
Figure 2: Unimodal pen command
Flexible multimodal output MATCH provides
flexible, synchronized multimodal generation and
can take initiative to engage in information-seeking
subdialogues. If a user circles the three restaurants in
Figure 3 and writes phone, the system responds with
a graphical callout on the display, synchronized with
a text-to-speech (TTS) prompt of the phone number,
for each restaurant in turn (Figure 4).
Figure 3: Two area gestures
Figure 4: Phone query callouts
The system also provides subway directions. If the
user says How do I get to this place? and circles one
of the restaurants displayed on the map, the system
will ask Where do you want to go from? The user
can then respond with speech (e.g., 25th Street and
3rd Avenue), with pen by writing (e.g., 25th St & 3rd
Ave), or multimodally ( e.g, from here with a circle
gesture indicating location). The system then calcu-
lates the optimal subway route and dynamically gen-
erates a multimodal presentation of instructions. It
starts by zooming in on the first station and then grad-
ually zooms out, graphically presenting each stage of
the route along with a series of synchronized TTS
prompts. Figure 5 shows the final display of a sub-
way route heading downtown on the 6 train and trans-
ferring to the L train Brooklyn bound.
Figure 5: Multimodal subway route
User-tailored generation MATCH can also pro-
vide a user-tailored summary, comparison, or rec-
ommendation for an arbitrary set of restaurants, us-
ing a quantitative model of user preferences (Walker
et al, 2002). The system will only discuss restau-
rants that rank highly according to the user?s dining
preferences, and will only describe attributes of those
restaurants the user considers important. This per-
mits concise, targeted system responses. For exam-
ple, the user could say compare these restaurants and
circle a large set of restaurants (Figure 6). If the user
considers inexpensiveness and food quality to be the
most important attributes of a restaurant, the system
response might be:
Compare-A: Among the selected restaurants, the following
offer exceptional overall value. Uguale?s price is 33 dollars. It
has excellent food quality and good decor. Da Andrea?s price is
28 dollars. It has very good food quality and good decor. John?s
Pizzeria?s price is 20 dollars. It has very good food quality and
mediocre decor.
Figure 6: Comparing a large set of restaurants
2 Multimodal Application Architecture
The multimodal architecture supporting MATCH
consists of a series of agents which communicate
through a facilitator MCUBE (Figure 7).
Figure 7: Multimodal Architecture
MCUBE is a Java-based facilitator which enables
agents to pass messages either to single agents or
groups of agents. It serves a similar function to sys-
tems such as OAA (Martin et al, 1999), the use of
KQML for messaging in Allen et al(2000), and the
Communicator hub (Seneff et al, 1998). Agents may
reside either on the client device or elsewhere on the
network and can be implemented in multiple differ-
ent languages. MCUBE messages are encoded in
XML, providing a general mechanism for message
parsing and facilitating logging.
Multimodal User Interface Users interact with
the system through the Multimodal UI, which is
browser-based and runs in Internet Explorer. This
greatly facilitates rapid prototyping, authoring, and
reuse of the system for different applications since
anything that can appear on a webpage (dynamic
HTML, ActiveX controls, etc.) can be used in
the visual component of a multimodal user inter-
face. A TCP/IP control enables communication with
MCUBE.
MATCH uses a control that provides a dynamic
pan-able, zoomable map display. The control has ink
handling capability. This enables both pen-based in-
teraction (on the map) and normal GUI interaction
(on the rest of the page) without requiring the user to
overtly switch ?modes?. When the user draws on the
map their ink is captured and any objects potentially
selected, such as currently displayed restaurants, are
identified. The electronic ink is broken into a lat-
tice of strokes and sent to the gesture recognition
and handwriting recognition components which en-
rich this stroke lattice with possible classifications of
strokes and stroke combinations. The UI then trans-
lates this stroke lattice into an ink meaning lattice
representing all of the possible interpretations of the
user?s ink and sends it to MMFST.
In order to provide spoken input the user must tap
a click-to-speak button on the Multimodal UI. We
found that in an application such as MATCH which
provides extensive unimodal pen-based interaction, it
is preferable to use click-to-speak rather than pen-
to-speak or open-mike. With pen-to-speak, spurious
speech results received in noisy environments can
disrupt unimodal pen commands.
The Multimodal UI also provides graphical output
capabilities and performs synchronization of multi-
modal output. For example, it synchronizes the dis-
play actions and TTS prompts in the answer to the
route query mentioned in Section 1.
Speech Recognition MATCH uses AT&T?s Wat-
son speech recognition engine. A speech manager
running on the device gathers audio and communi-
cates with a recognition server running either on the
device or on the network. The recognition server pro-
vides word lattice output which is passed to MMFST.
Gesture and handwriting recognition Gesture
and handwriting recognition agents provide possible
classifications of electronic ink for the UI. Recogni-
tions are performed both on individual strokes and
combinations of strokes in the input ink lattice. The
handwriting recognizer supports a vocabulary of 285
words, including attributes of restaurants (e.g. ?chi-
nese?,?cheap?) and zones and points of interest (e.g.
?soho?,?empire?,?state?,?building?). The gesture rec-
ognizer recognizes a set of 10 basic gestures, includ-
ing lines, arrows, areas, points, and question marks.
It uses a variant of Rubine?s classic template-based
gesture recognition algorithm (Rubine, 1991) trained
on a corpus of sample gestures. In addition to classi-
fying gestures the gesture recognition agent also ex-
tracts features such as the base and head of arrows.
Combinations of this basic set of gestures and hand-
written words provide a rich visual vocabulary for
multimodal and pen-based commands.
Gestures are represented in the ink meaning lat-
tice as symbol complexes of the following form: G
FORM MEANING (NUMBER TYPE) SEM. FORM
indicates the physical form of the gesture and has val-
ues such as area, point, line, arrow. MEANING indi-
cates the meaning of that form; for example an area
can be either a loc(ation) or a sel(ection). NUMBER
and TYPE indicate the number of entities in a selec-
tion (1,2,3, many) and their type (rest(aurant), the-
atre). SEM is a place holder for the specific content
of the gesture, such as the points that make up an area
or the identifiers of objects in a selection.
When multiple selection gestures are present
an aggregation technique (Johnston and Bangalore,
2001) is employed to overcome the problems with
deictic plurals and numerals described in John-
ston (2000). Aggregation augments the ink meaning
lattice with aggregate gestures that result from com-
bining adjacent selection gestures. This allows a de-
ictic expression like these three restaurants to com-
bine with two area gestures, one which selects one
restaurant and the other two, as long as their sum is
three. For example, if the user makes two area ges-
tures, one around a single restaurant and the other
around two restaurants (Figure 3), the resulting ink
meaning lattice will be as in Figure 8. The first ges-
ture (node numbers 0-7) is either a reference to a
location (loc.) (0-3,7) or a reference to a restaurant
(sel.) (0-2,4-7). The second (nodes 7-13,16) is either
a reference to a location (7-10,16) or to a set of two
restaurants (7-9,11-13,16). The aggregation process
applies to the two adjacent selections and adds a se-
lection of three restaurants (0-2,4,14-16). If the user
says show chinese restaurants in this neighborhood
and this neighborhood, the path containing the two
locations (0-3,7-10,16) will be taken when this lat-
tice is combined with speech in MMFST. If the user
says tell me about this place and these places, then
the path with the adjacent selections is taken (0-2,4-
9,11-13,16). If the speech is tell me about these or
phone numbers for these three restaurants then the
aggregate path (0-2,4,14-16) will be chosen.
Multimodal Integrator (MMFST) MMFST re-
ceives the speech lattice (from the Speech Manager)
and the ink meaning lattice (from the UI) and builds
a multimodal meaning lattice which captures the po-
tential joint interpretations of the speech and ink in-
puts. MMFST is able to provide rapid response times
by making unimodal timeouts conditional on activity
in the other input mode. MMFST is notified when the
user has hit the click-to-speak button, when a speech
result arrives, and whether or not the user is inking on
the display. When a speech lattice arrives, if inking
is in progress MMFST waits for the ink meaning lat-
tice, otherwise it applies a short timeout (1 sec.) and
treats the speech as unimodal. When an ink meaning
lattice arrives, if the user has tapped click-to-speak
MMFST waits for the speech lattice to arrive, other-
wise it applies a short timeout (1 sec.) and treats the
ink as unimodal.
MMFST uses the finite-state approach to multi-
modal integration and understanding proposed by
Johnston and Bangalore (2000). Possibilities for
multimodal integration and understanding are cap-
tured in a three tape device in which the first tape
represents the speech stream (words), the second the
ink stream (gesture symbols) and the third their com-
bined meaning (meaning symbols). In essence, this
device takes the speech and ink meaning lattices as
inputs, consumes them using the first two tapes, and
writes out a multimodal meaning lattice using the
third tape. The three tape finite-state device is sim-
ulated using two transducers: G:W which is used to
align speech and ink and G W:M which takes a com-
posite alphabet of speech and gesture symbols as in-
put and outputs meaning. The ink meaning lattice
G and speech lattice W are composed with G:W and
the result is factored into an FSA G W which is com-
posed with G W:M to derive the meaning lattice M.
In order to capture multimodal integration using
finite-state methods, it is necessary to abstract over
specific aspects of gestural content (Johnston and
Bangalore, 2000). For example, all possible se-
quences of coordinates that could occur in an area
gesture cannot be encoded in the finite-state device.
We employ the approach proposed in (Johnston and
Bangalore, 2001) in which the ink meaning lattice is
converted to a transducer I:G, where G are gesture
symbols (including SEM) and I contains both gesture
symbols and the specific contents. I and G differ only
in cases where the gesture symbol on G is SEM, in
which case the corresponding I symbol is the specific
interpretation. After multimodal integration a pro-
jection G:M is taken from the result G W:M machine
and composed with the original I:G in order to rein-
corporate the specific contents that were left out of
the finite-state process (I:G o G:M = I:M).
The multimodal finite-state transducers used at
runtime are compiled from a declarative multimodal
context-free grammar which captures the structure
Figure 8: Ink Meaning Lattice
and interpretation of multimodal and unimodal com-
mands, approximated where necessary using stan-
dard approximation techniques (Nederhof, 1997).
This grammar captures not just multimodal integra-
tion patterns but also the parsing of speech and ges-
ture, and the assignment of meaning. In Figure 9 we
present a small simplified fragment capable of han-
dling MATCH commands such as phone numbers for
these three restaurants. A multimodal CFG differs
from a normal CFG in that the terminals are triples:
W:G:M, where W is the speech stream (words), G
the ink stream (gesture symbols) and M the meaning
stream (meaning symbols). An XML representation
for meaning is used to facilate parsing and logging
by other system components. The meaning tape sym-
bols concatenate to form coherent XML expressions.
The epsilon symbol (eps) indicates that a stream is
empty in a given terminal.
When the user says phone numbers for these
three restaurants and circles two groups of restau-
rants (Figure 3). The gesture lattice (Figure 8) is
turned into a transducer I:G with the same sym-
bol on each side except for the SEM arcs which are
split. For example, path 15-16 SEM([id1,id2,id3])
becomes [id1,id2,id3]:SEM. After G and the speech
W are integrated using G:W and G W:M. The G path
in the result is used to re-establish the connection
between SEM symbols and their specific contents
in I:G (I:G o G:M = I:M). The meaning read off
I:M is<cmd><phone><restaurant> [id1,id2,id3]
</restaurant> </phone> </cmd>. This is passed
to the multimodal dialog manager (MDM) and from
there to the Multimodal UI resulting in a display like
Figure 4 with coordinated TTS output. Since the
speech input is a lattice and there is also potential
for ambiguity in the multimodal grammar, the output
from MMFST to MDM is an N-best list of potential
multimodal interpretations.
Multimodal Dialog Manager (MDM) The MDM
is based on previous work on speech-act based mod-
els of dialog (Stent et al, 1999; Rich and Sidner,
1998). It uses a Java-based toolkit for writing dialog
managers that is similar in philosophy to TrindiKit
(Larsson et al, 1999). It includes several rule-based
S ! eps:eps:<cmd> CMD eps:eps:</cmd>
CMD ! phone:eps:<phone> numbers:eps:eps
for:eps:eps DEICTICNP
eps:eps:</phone>
DEICTICNP ! DDETPL eps:area:eps eps:selection:eps
NUM RESTPL eps:eps:<restaurant>
eps:SEM:SEM eps:eps:</restaurant>
DDETPL ! these:G:eps
RESTPL ! restaurants:restaurant:eps
NUM ! three:3:eps
Figure 9: Multimodal grammar fragment
processes that operate on a shared state. The state
includes system and user intentions and beliefs, a di-
alog history and focus space, and information about
the speaker, the domain and the available modalities.
The processes include interpretation, update, selec-
tion and generation processes.
The interpretation process takes as input an N-best
list of possible multimodal interpretations for a user
input from MMFST. It rescores them according to a
set of rules that encode the most likely next speech
act given the current dialogue context, and picks the
most likely interpretation from the result. The update
process updates the dialogue context according to the
system?s interpretation of user input. It augments the
dialogue history, focus space, models of user and sys-
tem beliefs, and model of user intentions. It also al-
ters the list of current modalities to reflect those most
recently used by the user.
The selection process determines the system?s next
move(s). In the case of a command, request or ques-
tion, it first checks that the input is fully specified
(using the domain ontology, which contains informa-
tion about required and optional roles for different
types of actions); if it is not, then the system?s next
move is to take the initiative and start an information-
gathering subdialogue. If the input is fully specified,
the system?s next move is to perform the command or
answer the question; to do this, MDM communicates
with the UI. Since MDM is aware of the current set
of preferred modalities, it can provide feedback and
responses tailored to the user?s modality preferences.
The generation process performs template-based
generation for simple responses and updates the sys-
tem?s model of the user?s intentions after generation.
The text planner is used for more complex genera-
tion, such as the generation of comparisons.
In the route query example in Section 1, MDM first
receives a route query in which only the destination
is specified How do I get to this place? In the se-
lection phase it consults the domain model and de-
termines that a source is also required for a route.
It adds a request to query the user for the source to
the system?s next moves. This move is selected and
the generation process selects a prompt and sends it
to the TTS component. The system asks Where do
you want to go from? If the user says or writes 25th
Street and 3rd Avenue then MMFST will assign this
input two possible interpretations. Either this is a re-
quest to zoom the display to the specified location or
it is an assertion of a location. Since the MDM dia-
logue state indicates that it is waiting for an answer
of the type location, MDM reranks the assertion as
the most likely interpretation. A generalized overlay
process (Alexandersson and Becker, 2001) is used to
take the content of the assertion (a location) and add
it into the partial route request. The result is deter-
mined to be complete. The UI resolves the location
to map coordinates and passes on a route request to
the SUBWAY component.
We found this traditional speech-act based dia-
logue manager worked well for our multimodal inter-
face. Critical in this was our use of a common seman-
tic representation across spoken, gestured, and multi-
modal commands. The majority of the dialogue rules
operate in a mode-independent fashion, giving users
flexibility in the mode they choose to advance the di-
alogue. On the other hand, mode sensitivity is also
important since user modality choice can be used to
determine system mode choice for confirmation and
other responses.
Subway Route Constraint Solver (SUBWAY)
This component has access to an exhaustive database
of the NYC subway system. When it receives a route
request with the desired source and destination points
from the Multimodal UI, it explores the search space
of possible routes to identify the optimal one, using a
cost function based on the number of transfers, over-
all number of stops, and the walking distance from
the station at each end. It builds a list of actions re-
quired to reach the destination and passes them to the
multimodal generator.
Multimodal Generator and Text-to-speech The
multimodal generator processes action lists from
SUBWAY and other components and assigns appro-
priate prompts for each action using a template-based
generator. The result is a ?score? of prompts and ac-
tions which is passed to the Multimodal UI. The Mul-
timodal UI plays this ?score? by coordinating changes
in the interface with the corresponding TTS prompts.
AT&T?s Natural Voices TTS engine is used to pro-
vide the spoken output. When the UI receives a mul-
timodal score, it builds a stack of graphical actions
such as zooming the display to a particular location
or putting up a graphical callout. It then sends the
prompts to be rendered by the TTS server. As each
prompt is synthesized the TTS server sends progress
notifications to the Multimodal UI, which pops the
next graphical action off the stack and executes it.
Text Planner and User Model The text plan-
ner receives instructions from MDM for execution
of ?compare?, ?summarize?, and ?recommend? com-
mands. It employs a user model based on multi-
attribute decision theory (Carenini and Moore, 2001).
For example, in order to make a comparison between
the set of restaurants shown in Figure 6, the text
planner first ranks the restaurants within the set ac-
cording to the predicted ranking of the user model.
Then, after selecting a small set of the highest ranked
restaurants, it utilizes the user model to decide which
restaurant attributes are important to mention. The
resulting text plan is converted to text and sent to TTS
(Walker et al, 2002). A user model for someone who
cares most highly about cost and secondly about food
quality and decor leads to a system response such as
that in Compare-A above. A user model for someone
whose selections are driven by food quality and food
type first, and cost only second, results in a system
response such as that shown in Compare-B.
Compare-B: Among the selected restaurants, the following of-
fer exceptional overall value. Babbo?s price is 60 dollars. It has
superb food quality. Il Mulino?s price is 65 dollars. It has superb
food quality. Uguale?s price is 33 dollars. It has excellent food.
Note that the restaurants selected for the user who
is not concerned about cost includes two rather more
expensive restaurants that are not selected by the text
planner for the cost-oriented user.
Multimodal Logger User studies, multimodal data
collection, and debugging were accomplished by in-
strumenting MATCH agents to send details of user
inputs, system processes, and system outputs to a log-
ger agent that maintains an XML log designed for
multimodal interactions. Our critical objective was
to collect data continually throughout system devel-
opment, and to be able to do so in mobile settings.
While this rendered the common practice of video-
taping user interactions impractical, we still required
high fidelity records of each multimodal interaction.
To address this problem, MATCH logs the state of
the UI and the user?s ink, along with detailed data
from other components. These components can in
turn dynamically replay the user?s speech and ink as
they were originally received, and show how the sys-
tem responded. The browser- and component-based
architecture of the Multimodal UI facilitated its reuse
in a Log Viewer that reads multimodal log files, re-
plays interactions between the user and system, and
allows analysis and annotation of the data. MATCH?s
logging system is similar in function to STAMP (Ovi-
att and Clow, 1998), but does not require multimodal
interactions to be videotaped and allows rapid re-
configuration for different annotation tasks since it
is browser-based. The ability of the system to log
data standalone is important, since it enables testing
and collection of multimodal data in realistic mobile
environments without relying on external equipment.
3 Experimental Evaluation
Our multimodal logging infrastructure enabled
MATCH to undergo continual user trials and evalu-
ation throughout development. Repeated evaluations
with small numbers of test users both in the lab and
in mobile settings (Figure 10) have guided the design
and iterative development of the system.
Figure 10: Testing MATCH in NYC
This iterative development approach highlighted
several important problems early on. For example,
while it was originally thought that users would for-
mulate queries and navigation commands primarily
by specifying the names of New York neighborhoods,
as in show italian restaurants in chelsea, early field
test studies in the city revealed that the need for
neighborhood names in the grammar was minimal
compared to the need for cross-streets and points of
interest; hence, cross-streets and a sizable list of land-
marks were added. Other early tests revealed the
need for easily accessible ?cancel? and ?undo? fea-
tures that allow users to make quick corrections. We
also discovered that speech recognition performance
was initially hindered by placement of the ?click-to-
speak? button and the recognition feedback box on
the bottom-right side of the device, leading many
users to speak ?to? this area, rather than toward the
microphone on the upper left side. This placement
also led left-handed users to block the microphone
with their arms when they spoke. Moving the but-
ton and the feedback box to the top-left of the device
resolved both of these problems.
After initial open-ended piloting trials, more struc-
tured user tests were conducted, for which we devel-
oped a set of six scenarios ordered by increasing level
of difficulty. These required the test user to solve
problems using the system. These scenarios were left
as open-ended as possible to elicit natural responses.
Sample scenario:You have plans to meet your aunt for dinner
later this evening at a Thai restaurant on the Upper West Side
near her apartment on 95th St. and Broadway. Unfortunately,
you forgot what time you?re supposed to meet her, and you can?t
reach her by phone. Use MATCH to find the restaurant and write
down the restaurant?s telephone number so you can check on the
reservation time.
Test users received a brief tutorial that was inten-
tionally vague and broad in scope so the users might
overestimate the system?s capabilities and approach
problems in new ways. Figure 11 summarizes re-
sults from our last scenario-based data collection for
a fixed version of the system. There were five sub-
jects (2 male, 3 female) none of whom had been in-
volved in system development. All of these five tests
were conducted indoors in offices.
exchanges 338 asr word accuracy 59.6%
speech only 171 51% asr sent. accuracy 36.1%
multimodal 93 28% handwritten sent. acc. 64%
pen only 66 19% task completion rate 85%
GUI actions 8 2% average time/scenario 6.25m
Figure 11: MATCH study
There were an average of 12.75 multimodal ex-
changes (pairs of user input and system response) per
scenario. The overall time per scenario varied from
1.5 to to 15 minutes. The longer completion times
resulted from poor ASR performance for some of the
users. Although ASR accuracy was low, overall task
completion was high, suggesting that the multimodal
aspects of the system helped users to complete tasks.
Unimodal pen commands were recognized more suc-
cessfully than spoken commands; however, only 19%
of commands were pen only. In ongoing work, we
are exploring strategies to increase users? adoption of
more robust pen-based and multimodal input.
MATCH has a very fast system response time.
Benchmarking a set of speech, pen, and multimodal
commands, the average response time is approxi-
mately 3 seconds (time from end of user input to sys-
tem response). We are currently completing a larger
scale scenario-based evaluation and an independent
evaluation of the functionality of the text planner.
In addition to MATCH, the same multimodal ar-
chitecture has been used for two other applications:
a multimodal interface to corporate directory infor-
mation and messaging and a medical application to
assist emergency room doctors. The medical proto-
type is the most recent and demonstrates the utility of
the architecture for rapid prototyping. System devel-
opment took under two days for two people.
4 Conclusion
The MATCH architecture enables rapid develop-
ment of mobile multimodal applications. Combin-
ing finite-state multimodal integration with a speech-
act based dialogue manager enables users to interact
flexibly using speech, pen, or synchronized combina-
tions of the two depending on their preferences, task,
and physical and social environment. The system
responds by generating coordinated multimodal pre-
sentations adapted to the multimodal dialog context
and user preferences. Features of the system such
as the browser-based UI and general purpose finite-
state architecture for multimodal integration facili-
tate rapid prototyping and reuse of the technology for
different applications. The lattice-based finite-state
approach to multimodal understanding enables both
multimodal integration and dialogue context to com-
pensate for recognition errors. The multimodal log-
ging infrastructure has enabled an iterative process
of pro-active evaluation and data collection through-
out system development. Since we can replay multi-
modal interactions without video we have been able
to log and annotate subjects both in the lab and in
NYC throughout the development process and use
their input to drive system development.
Acknowledgements
Thanks to AT&T Labs and DARPA (contract MDA972-99-3-
0003) for financial support. We would also like to thank Noemie
Elhadad, Candace Kamm, Elliot Pinson, Mazin Rahim, Owen
Rambow, and Nika Smith.
References
J. Alexandersson and T. Becker. 2001. Overlay as the ba-
sic operation for discourse processing in a multimodal
dialogue system. In 2nd IJCAI Workshop on Knowl-
edge and Reasoning in Practical Dialogue Systems.
J. Allen, D. Byron, M. Dzikovska, G. Ferguson,
L. Galescu, and A. Stent. 2000. An architecture for
a generic dialogue shell. JNLE, 6(3).
E. Andre?. 2002. Natural language in multime-
dia/multimodal systems. In Ruslan Mitkov, editor,
Handbook of Computational Linguistics. OUP.
G. Carenini and J. D. Moore. 2001. An empirical study of
the influence of user tailoring on evaluative argument
effectiveness. In IJCAI, pages 1307?1314.
M. Johnston and S. Bangalore. 2000. Finite-state mul-
timodal parsing and understanding. In Proceedings of
COLING 2000, Saarbru?cken, Germany.
M. Johnston and S. Bangalore. 2001. Finite-state meth-
ods for multimodal parsing and integration. In ESSLLI
Workshop on Finite-state Methods, Helsinki, Finland.
M. Johnston. 2000. Deixis and conjunction in mul-
timodal systems. In Proceedings of COLING 2000,
Saarbru?cken, Germany.
S. Larsson, P. Bohlin, J. Bos, and D. Traum. 1999.
TrindiKit manual. Technical report, TRINDI Deliver-
able D2.2.
D. Martin, A. Cheyer, and D. Moran. 1999. The Open
Agent Architecture: A framework for building dis-
tributed software systems. Applied Artificial Intelli-
gence, 13(1?2):91?128.
M-J. Nederhof. 1997. Regular approximations of CFLs:
A grammatical view. In Proceedings of the Interna-
tional Workshop on Parsing Technology, Boston.
S. L. Oviatt and J. Clow. 1998. An automated tool for
analysis of multimodal system performance. In Pro-
ceedings of ICSLP.
C. Rich and C. Sidner. 1998. COLLAGEN: A collabora-
tion manager for software interface agents. User Mod-
eling and User-Adapted Interaction, 8(3?4):315?350.
D. Rubine. 1991. Specifying gestures by example. Com-
puter graphics, 25(4):329?337.
S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, and
V. Zue. 1998. Galaxy-II: A reference architecture for
conversational system development. In ICSLP-98.
A. Stent, J. Dowding, J. Gawron, E. Bratt, and R. Moore.
1999. The CommandTalk spoken dialogue system. In
Proceedings of ACL?99.
M. A. Walker, S. J. Whittaker, P. Maloor, J. D. Moore,
M. Johnston, and G. Vasireddy. 2002. Speech-Plans:
Generating evaluative responses in spoken dialogue. In
In Proceedings of INLG-02.
Evaluation Metrics for Generation 
Sr in ivas  Banga lore  and  Owen Rambow and Steve  Whi t taker  
AT&T Labs  - Research  
180 Park  Ave,  PO Box 971 
F lo rham Park ,  N J  07932-0971, USA 
{srini,r~mbow, stevew}@reSearch, art tom 
Abst rac t  
Certain generation applications may profit from the 
use of stochastic methods. In developing stochastic 
methods, it is crucial to be able to quickly assess 
the relative merits of different approaches or mod- 
els. In this paper, we present several types of in- 
trinsic (system internal) metrics which we have used 
for baseline quantitative assessment. This quanti- 
tative assessment should then be augmented to a 
fuller evaluation that examines qualitative aspects. 
To this end, we describe an experiment that tests 
correlation between the quantitative metrics and hu- 
man qualitative judgment. The experiment confirms 
that intrinsic metrics cannot replace human evalu- 
ation, but some correlate significantly with human 
judgments of quality and understandability and can 
be used for evaluation during development. 
1 In t roduct ion  
For many applications in natural language genera- 
tion (NLG), the range of linguistic expressions that 
must be generated is quite restricted, and a gram- 
mar for a surface realization component can be fully 
specified by hand. Moreover, iLL inany cases it is 
very important not to deviate from very specific out- 
put in generation (e.g., maritime weather reports), 
in which case hand-crafted grammars give excellent 
control. In these cases, evaluations of the generator 
that rely on human judgments (Lester and Porter, 
I997) or on human annotation of the test corpora 
(Kukich, 1983) are quite sufficient . . . .  
However. in other NLG applications the variety of 
the output is much larger, and the demands on 
the quality of the output are solnewhat less strin- 
gent. A typical example is NLG in the context of 
(interlingua- or transfer-based) inachine translation. 
Another reason for relaxing the quality of the out- 
put may be that not enough time is available to de- 
velop a full gramnlar for a new target, language in 
NLG. ILL all these cases, stochastic methods provide 
an alternative to hand-crafted approaches to NLG. 
1 
To our knowledge, the first to use stochastic tech- 
niques in an NLG realization module were Langkilde 
and Knight (1998a) and (~998b) (see also (Langk- 
ilde, 2000)). As is the case for stochastic approaches 
in natural anguage understanding, the research and 
development itself requires an effective intrinsic met- 
ric in order to be able to evaluate progress. 
In this paper, we discuss several evaluation metrics 
that we are using during the development of FERGUS 
(Flexible Empiricist/Rationalist Generation Using 
Syntax). FERCUS, a realization module, follows 
Knight and Langkilde's eminal work in using an 
n-gram language model, but we augment it with a 
tree-based stochastic model and a lexicalized syntac- 
tic grammar. The metrics are useful to us as rela- 
tive quantitative assessments of different models we 
experiment with; however, we do not pretend that 
these metrics in themselves have any validity. In- 
stead, we follow work done in dialog systems (Walker 
et al, 1997) and attempt o find metrics which on 
tim one hand can be computed easily but on the 
other hand correlate with empirically verified human 
judgments in qualitative categories such as readabil- 
ity. 
The structure of the paper is as follows. In Section 2, 
we briefly describe the architecture of FEacUS, and 
some of the modules. In Section 3 we present four 
metrics and some results obtained with these met- 
rics. In Section 4 we discuss the for experimental 
validation of the metrics using human judgments, 
and present a new metric based on the results of 
these experiments. In Section 5 we discuss some of 
the 'many problematic issues related to  the use  Of 
metrics and our metrics in particular, and discuss 
on-going work. 
2 Sys tem Overv iew 
FERGUS is composed of three mQdules: .the Tree 
Chooser, tile Unraveler, and the Linear Precedence 
(LP) Chooser (Figure 1). Tile input to the system is 
a dependency tree as shown in Figure 2. t Note that 
the nodes are unordered and are labeled only with 
lexemes, not with any sort of syntactic annotations. 2 
The Tree Chooser uses a stochastic tree model to 
choose syntactic properties (expressed as trees in a 
Tree Adjoining Grammar) for the nodes in the in- 
put structure. This step can be seen as analogous to 
"supertagging" -(Bangalore-und doshh 1:999);. except 
that now supertags (i.e., names of trees which en- 
code the syntactic properties of a lexical head) must 
be found for words in a tree rather than for words 
in a linear sequence. The Tree Chooser makes the 
siinplifying assumptions that the choice of a tree for 
a node depends only on its daughter nodes, thus al- 
lowing for a top-down algorithm. The Tree Chooser 
draws on a tree model, which is a analysis in terms 
of syntactic dependency for 1,000,000 words of the 
Wall Street Journal (WSJ). 3 
The supertagged tree which is output from the Tree 
Chooser still does not fully determine the surface 
string, because there typically are different ways to 
attach a daughter node to her mother (for example, 
an adverb can be placed in different positions with 
respect o its verbal head). The Unraveler therefore 
uses the XTAG grammar of English (XTAG-Group, 
1999) to produce a lattice of all possible lineariza- 
tions that are compatible with the supertagged tree. 
Specifically, the daughter nodes are ordered with re- 
spect to the head at each level of the derivation tree. 
In cases where the XTAG grammar allows a daugh- 
ter node to be attached at more than one place in 
the mother supertag (as is the case in our exam- 
ple for was and for; generaUy, such underspecifica- 
tion occurs with adjuncts and with arguments if their 
syntactic role is not specified), a disjunction of all 
these positions is assigned to the daughter node. A 
bottom-up algorithm then constructs a lattice that 
encodes the strings represented by each level of the 
derivation tree. The lattice at the root of the deriva- 
tion tree is the result of the Unraveler. 
Finally. the LP Chooser chooses the most likely 
traversal of this lattice, given a linear language 
1The sentence generated by this tree is a predicativenoun 
construction. The XTAG grammar analyzes these as being 
headed by the noun,rather-than by.the copula, and we fol- 
low the XTAG analysis. However, it would of course also be 
possible to use a graminar that allows for the copula-headed 
analysis. 
21n the system that we used in the experiments described 
in Section 3. all words (including function words) need to be 
present in the input representation, fully inflected. Further- 
more, there is no indication of syntactic role at all. This is of 
course unrealistic f~r applications see ,Section 5 for further 
renlarks. 
:3This wa~s constructed from the Penn Tree Bank using 
some heuristics, sirice the. l)enn Tree Bank does not contain 
full head-dependerit information; as a result of the tlse of 
heuristics, the Tree Model is tint fully correct. 
2 
I 
TAG Derivation Tree 
without Supertags 
i 
One single semi-specif ied~ 
TAG Deri~tion Trees 
Word Lattice 
i 
\[ cPc.oo=, \] 
l 
String 
Figure 1: Architecture of FERGUS 
estimate 
there was no cost for 
I 
phase 
the second 
Figure 2: Input to FERGUS 
model (n-gram). The lattice output from the Un- 
raveler encodes all possible word sequences permit- 
ted by the supertagged ependency structure. \Ve 
rank these word sequences in the order of their likeN- 
hood by composing the lattice with a finite-state ma- 
chine representing a trigram language model. This 
model has been constructed from the 1.000,0000 
words WSJ training corpus. We pick the best path 
through the lattice resulting from the composition 
using the Viterbi algorithm, and this top ranking 
word sequence is the output of the LP Chooser and 
the generator. 
When we tally the results we obtain the score shown 
in the first column of Table 1. 
Note that if there are insertions and deletions, the 
number of operations may be larger than the number 
of tokens involved for either one of the two strings. 
As a result, the simple string accuracy metric may 
3 Base l ine -Qua_nt i tmt ive ,Met r i cs  ...,:-~.--..~,-:..,be.:..~eg~i~ee (t:hoagk:it, As, nevel:-greater._than 1, of 
We have used four different baseline quantitative 
metrics for evaluating our generator. The first two 
metrics are based entirely on the surface string. The 
next two metrics are based on a syntactic represen- 
tation of the sentence. 
3.1 S t r ing -Based  Met r i cs  
We employ two metrics that measure the accuracy 
of a generated string. The first metric, s imple ac- 
curacy,  is the same string distance metric used for 
measuring speech recognition accuracy. This met- 
ric has also been used to measure accuracy of MT 
systems (Alshawi et al, 1998). It is based on string 
edit distance between the output of the generation 
system and the reference corpus string. Simple ac- 
curacy is the number of insertion (I), deletion (D) 
and substitutions (S) errors between the reference 
strings in the test corpus and the strings produced by 
the generation model. An alignment algorithm us- 
ing substitution, insertion and deletion of tokens as 
operations attempts to match the generated string 
with the reference string. Each of these operations 
is assigned a cost value such that a substitution op- 
eration is cheaper than the combined cost of a dele- 
tion and an insertion operation. The alignment al- 
gorithm attempts to find the set of operations that 
minimizes the cost of aligning the generated string 
to tile reference string. Tile metric is summarized 
in Equation (1). R is the number of tokens in the 
target string. 
course). 
The simple string accuracy metric penalizes a mis- 
placed token twice, as a deletion from its expected 
position and insertion at a different position. This is 
particularly worrisome in our case, since in our eval- 
uation scenario the generated sentence is a permuta- 
tion of the tokens in the reference string. We there- 
fore use a second metric, Generat ion  Str ing Ac- 
curacy,  shown in Equation (3), which treats dele- 
tion of a token at one location in the string and the 
insertion of the same token at another location in 
the string as one single movement error (M). This 
is in addition to the remaining insertions (I ') and 
deletions (D'). 
(3) Generat ion  St r ing  Accuracy  = 
( 1 -- M~-/~.P-~--~-~) 
In our example sentence (2), we see that the inser- 
tion and deletion of no can be collapsed into one 
move. However, the wrong positions of cost and of 
phase are not analyzed as two moves, since one takes 
the place of the other, and these two tokens still re- 
sult in one deletion, one substitution, and one inser- 
tion. 5 Thus, the generation string accuracy depe- 
nalizes simple moves, but still treats complex moves 
(involving more than one token) harshly. Overall, 
the scores for the two metrics introduced so far are 
shown in the first two columns of Table 1. 
3.2 Tree-Based Metr ics  
(1) Simple Str ing Accuracy  = (1 I+*)+s I? ) \Vhile tile string-b~u~ed metrics are very easy to ap- 
ply, they have the disadvantage that they do not 
reflect the intuition that all token moves are not Consider tile fifth)wing example. The target sentence 
is on top, tile generated sentence below. Tile third equally "bad". Consider the subphrase stimate for 
line represents the operation needed to. transfor m .. phase the second of the sentence in (2). \Vhile this is 
one sentence into another: a period is used t.o indi- bad; i t  seems better:tiara rt alternative such as es- 
cate that no operation is needed. 4 
(2) There was no cost estimate for tile 
There was estimate for l)hase tile 
d (1 i 
second phase 
second no cost  
i s 
? I Note that the metric is symmetric, 
timate phase for tile second. Tile difference between 
the two strings is that the first scrambled string, but 
not tile second,  can be read off fl'om tile dependency  
tree for the sentence (as shown ill Figure 2) with- 
out violation of projectivity, i.e., without (roughly 
STiffs shows the importance of the alignment algorithm in 
the definition of Ihese two metrics: had it. not, aligned phase 
and cost as a substitution (but each with an empty position 
in the other~string-:instead),, then ~khe simple string accuracy 
would have 6 errors instead of 5, but the generation string 
accuracy would have 3 errors instead of ,1, 
speaking) creating discontinuous constituents. It 
has long been observed (though informally) that the 
dependency trees of a vast majority of sentences in 
the languages of the world are projective (see e.g. 
(Mel'euk, 1988)), so that a violation of projectivity 
is presumably a more severe rror than a word order 
variation that does not violate projectivity. 
We designed thet ree-based ' -acet t rucymetr i cs  in 
order to account for this effect. Instead of compar- 
ing two strings directly, we relate the two strings 
to a dependency tree of the reference string. For 
each treelet (i.e., non-leaf node with all of its daugh- 
ters) of the reference dependency tree, we construct 
strings of the head and its dependents in the order 
they appear in the reference string, and in the order 
they appear in the result string. We then calculate 
the number of substitutions, deletions, and inser- 
tions as for the simple string accuracy, and the num- 
ber of substitutions, moves, and remaining deletions 
and insertions as for the generation string metrics, 
for all treelets that form the dependency tree. We 
sum these scores, and then use the values obtained 
in the formulas given above for the two string-based 
metrics, yielding the S imple  Tree Accuracy  and 
Generat ion  Tree Accuracy .  The scores for our 
example sentence are shown in the last two columns 
of Table 1. 
3.3 Eva luat ion  Resu l ts  
The simple accuracy, generation accuracy, simple 
tree accuracy and generation tree accuracy for the 
two experiments are tabulated in Table 2. The test 
corpus is a randomly chosen subset of 100 sentences 
from the Section 20 of WSJ. The dependency struc- 
tures for the test sentences were obtained automat- 
ically from converting the Penn TreeBank phrase 
structure trees, in the same way as was done to 
Create the training corpus. The average length of 
the test sentences i 16.7 words with a longest sen- 
tence being 24 words in length. As can be seen, the 
supertag-based model improves over the baseline LR 
model on all four baseline quantitative metrics. 
4 Qua l i ta t ive  Eva luat ion  o f  the  
Quant i ta t ive  Met r i cs  
4.1 The  Exper iments  
We have presented four metrics which we can com- 
pute automatically. In order to determine whether 
the metrics correlate with independent notions un- 
derstandability or quality, we have performed eval- 
uation experiments with human subjects. 
In the web-based experiment, we ask human sub- 
jects to read a short paragraph from the WSJ. We 
present hree or five variants of the last sentence of 
this paragraph on the same page, and ask the sub- 
ject to judge them along two dimensions: 
Here we summarize two experiments that we have 
performed that use different tree nmdels. (For a 
more detailed comparisons of different tree models, 
see (Bangalore and Rainbow, 2000).) 
o For the baseline experiment, we impose a ran- 
dom tree structure for each sentence of the cor- 
pus and build a Tree Model whose parameters 
consist of whether a lexeme ld precedes or fol- 
lows her mother lexeme \[ .... We call this the 
Baseline Left-Right (LR) Model. This model 
generates There was est imate for  phase the sec- 
ond no cost .  for our example input. 
o In the second experiment we use the-system 
as described in Section 2. We employ the 
supertag-based tree model whose parameters 
consist of whether a lexeme ld with supertag 
sd is a dependent of lexeme 1,,, with supertag 
s,,,. Furthermore we use the information pro- 
vided by the XTAG grammar to order the de- 
pendents. This model generates There was no 
cost est imate for" the second phase . for our ex- 
ample input, .which is indeed.the sentence found 
in the WS.I. 
o Unders tandab i l i ty :  How easy is this sentence 
to understand? Options range from "Extremely 
easy" (= 7) to "Just barely possible" (=4) to 
"Impossible" (=1). (Intermediate numeric val- 
ues can also be chosen but have no description 
associated with them.) 
o Qual i ty:  How well-written is this sentence? 
Options range from "Extremely well-written'" 
(= 7) to "Pretty bad" (=4) to "Horrible (=1). 
(Again. intermediate numeric values can also t)e 
chosen, but have no description associated with 
them.) 
The 3-5 variants of each of 6 base sentences are con- 
strutted by us (most of the variants lraxre not actu- 
ally been generated by FERGUS) to sample multiple 
values of each intrinsic metric as well as to contrast 
differences between the intrinsic measures. Thus for 
one sentence "tumble", two of the five variants have 
approximately identical values for each of the met- 
rics but with the absolute values being high (0.9) 
and medium (0.7) respectively. For two other sen- 
\[,('II('(}S ~ve have contrasting intrinsic values for tree 
trod string based measures. For .the final sentence 
we have contrasts between the string measures with 
Metric Simple Generation Simple Generation 
String Accuracy String Accuracy Tree Accuracy Tree Accuracy 
Total number of tokens 9 9 9 9 
Unchanged 
Substitutions 
Insertions 
Deletions 
Moves 
6 
1 
2 
2 
0 
6 
0 
3 
3 
O..  
6 
0 
0 
0 
.3 
Total number of problems 5 4 " 6 3 
Score 0.44 0.56 0.33 0.67 
Table 1: Scores for the sample sentence according to the four metrics 
Tree Model Simple Generation Simple Generation 
String Accuracy String Accuracy Tree Accuracy Tree Accuracy i
Baseline LR Model 0.41 0.56 0.41 0.63 
. . . . .  i 
Supertag-based Model 0.58 0.72 0.65 0.76 I 
Table 2: Performance results 
tree measures being approximately equal. Ten sub- 
jects who were researchers from AT&T carried out 
the experiment. Each subject made a total of 24 
judgments. 
Given the variance between subjects we first nor- 
malized the data. We subtracted the mean score 
for each subject from each observed score and then 
divided this by standard eviation of the scores for 
that subject. As expected our data showed strong 
correlations between ormalized understanding and 
quality judgments for each sentence variant (r(22) = 
0.94, p < 0.0001). 
Our main hypothesis i that the two tree-based met- 
rics correlate better with both understandability and 
quality than the string-based metrics. This was con- 
firmed. Correlations of the two string metrics with 
normalized understanding for each sentence variant 
were not significant (r(22) = 0.08 and rl.2.21 = 0.23, for 
simple accuracy and generation accuracy: for both 
p > 0.05). In contrast both of the tree metrics were 
significant (r(2.2) = 0.51 and r(22) = 0.48: for tree 
accuracy and generation tree accuracy, for both p 
< 0.05). Similar results were achieved--for thegor- 
realized quality metric: (r(.2.21 = 0.16 and r(221 = 
0,33: for simple accuracy and generation accuracy, 
for both p > 0.05), (r(ee) = 0.45 and r(.2.2) = 0.42, 
for tree accuracy and generation tree accuracy, for 
both p < 0.05). 
A second aim of ()Lit" qualitative valuation was to 
lest various models of the relationship between in- 
trinsic variables and qualitative user judgments. \Ve 
proposed a mmlber-of'models:in which various conL- 
from the two tree models 
binations of intrinsic metrics were used to predict 
user judgments of understanding and quality. .We 
conducted a series of linear regressions with nor- 
malized judgments of understanding and quality as 
the dependent measures and as independent mea- 
sures different combinations of one of our four met- 
rics with sentence length, and with the "problem" 
variables that we used to define the string metrics 
(S, I, D, M, I ' ,  D' - see Section 3 for definitions). 
One sentence variant was excluded from the data set, 
on the grounds that the severely "mangled" sentence 
happened to turn out well-formed and with nearly 
the same nleaning as the target sentence. The re- 
sults are shown in Table 3. 
We first tested models using one of our metrics as a 
single intrinsic factor to explain the dependent vari- 
able. We then added the "problem" variables. 6 and 
could boost tile explanatory power while maintain- 
ing significance. In Table 3, we show only some con> 
binations, which show that tile best results were ob- 
tained by combining the simple tree accuracy with 
the number of Substitutions (S) and the sentence 
length. As we can see, the number of substitutions 
..... has an.important effecVon explanatory.power,, while 
that of sentence length is much more modest (but 
more important for quality than for understanding). 
Furthermore, the number of substitutions has more 
explanatory power than the number of moves (and 
in fact. than any of the other "problem" variables). 
The two regressions for understanding and writing 
show very sinlilar results. Normalized understand- 
6None of tile "problem" variables have much explanatory 
power on their own (nor (lid they achieve significance). 
Model User Metric Explanatory Power Statistical Significance 
(R 2) (p value) 
Simple String Accuracy Understanding 0.02 0.571 
Simple String Accuracy Quality 0.00 0.953 
Generation String Accuracy 
Generation String Accuracy 
S imple  T ree  Accuracy . . . . . . .  . - ~,  
Simple Tree Accuracy 
Generation Tree Accuracy 
Generation Tree Accuracy 
Simple Tree Accuracy + S 
Simple Tree Accuracy + S 
Simple Tree Accuracy + M 
Simple Tree Accuracy + M 
Simple Tree Accuracy + Length 
Simple Tree Accuracy + Length 
Simple Tree Accuracy + S + Length 
Simple Tree Accuracy + S + Length 
Understanding 
Quality 
::Unders~aatdiag 
Quality 
Understanding 
Quality 
0.02 
0.05 
: . ,  . . . .  0.36  
0.34 
0.35 
0.35 
0.584 
0.327 
. . ? . . . . . . . . . .  ".0;003.. - . :  
0.003 
0.003 
0.003 
Understanding 0.48 0.001 
Quality 0.47 0.002 
Understanding 0.38 0.008 
Quality 0.34 0.015 
Understanding 0.40 0.006 
Quality 0.42 0.006 
0.51 
0.53 
Understanding 
Quality 
0.003 
0.002 
Table 3: Testing different models of user judgments (S is number of substitutions, M number of moved 
elements) 
ing was best modeled as: 
Normalized understanding = 1.4728*sim- 
ple tree accuracy - 0.1015*substitutions- 
0.0228 * length - 0.2127. 
This model was significant: F(3,1 .9  ) = 6.62, p < 0.005. 
Tile model is plotted in Figure 3. with the data point 
representing the removed outlier at the top of the 
diagram. 
This model is also intuitively plausible. The simple 
tree metric was designed to measure the quality of a 
sentence and it has a positive coefficient. A substitu- 
tion represents a case in the string metrics in which 
not only a word is in the wrong place, but the word 
that should have been in that place is somewhere 
else, Therefore, substitutions, more than moves or 
insertions or deletions, represent grave cases of word 
order anomalies. Thus, it is plausible to penalize 
them separately. (,Note that tile simple tree accuracy 
is bounded by 1, while the number of substitutions i
l/ounded by the length of the sentence. In practice, 
in our sentences S ranges between 0 and 10 with 
a mean of 1,583.) Finally, it is also plausible that 
longer sentem:es are more difficult to understand, so 
that length has a (small) negative coefficient. 
We now turn to model for quality, 
Normalized quality = 1.2134*simple tree 
accuracy- 0.0839*substitutions - 0.0280 * 
length - 0.0689. 
This model was also significant: F(3A9) = 7.23, p < 
0.005. The model is plotted in Figure 4, with the 
data point representing the removed outlier at the 
top of the diagram. The quality model is plausible 
for the same reasons that the understanding model 
is. 
L2 
PP 
j ,1"  
.i / 
. / /  
/ 
/ /  
? . , j -  
-05 O0 05 
I a728"SLmo~eTteeMel~ - 0 I015"S - 0 0228"lerN~h 
Figure 3: Regression for Understanding 
6 
o 
Du~h~ 
- (0  -O5 0.0 05  I 0 
1 4728*S,mpleT~eeMetr ? - 0 I015"S - 0 0228"len~l~h 
Figure 4: Regression for Quality (Well-Formedness) 
4.2 Two New Metr i cs  
A further goal of these experiments was to obtain 
one or two metrics which can be automatically com- 
puted, and which have been shown to significantly 
correlate with relevant human judgments? We use as 
a starting point the two linear models for normalized 
understanding and quality given above, but we make 
two changes. First, we observe that while it is plau- 
sible to model human judgments by penalizing long 
sentences, this seems unmotivated in an accuracy 
metric: we do not want to give a perfectly generated 
longer sentence a lower score than a perfectly gener- 
ated shorter sentence. We therefore use models that 
just use the simple tree accuracy and the number 
of substitutions as independent variables? Second, 
we note that once we have done so, a perfect sen- 
tence gets a score of 0.8689 (for understandability) 
or 0.6639 (for quality). We therefore divide by this 
score to assure that a perfect sentence gets a score 
of 1. (As for the previously introduced metrics, the 
scores may be less than 0.) 
\Ve obtain the following new metrics: 
(4) Unders tandab i l i ty  ? Accuracy  = 
(1.3147*simple tree accuracy 0.1039*sub- 
stitutions - 0.4458) / 0.8689 - 
(5) Qua l i ty  Accuracy  = (1.0192*simple tree ac- 
curacy-  0.0869*substitutions - 0.3553) / 0.6639 
\ \e  reevahtated our system and the baseline model 
using the new metrics, in order to veri(v whether 
the nloro motivated metrics we have developed still 
show that FER(;I:S improves l)erforniance over the 
baseline. This is indeed the  case: the resuhs are 
Slllnm.arized ill Tabh'-t.  
Tree Model Understandability Quality 
Accuracy Accuracy 
Baseline -0.08 -0.12 
Supertag-based 0.44 0.42 
. Table 4: Performance results from the .two tree mod- 
..... els:using the:new metrics . . . . . . .  
5 D iscuss ion  
We have devised the baseline quantitative metrics 
presented in this paper for internal use during re- 
search and development, in order to evaluate dif- 
ferent versions of FERGUS. However, the question 
also arises whether they can be used to compare two 
completely different realization modules. In either 
case, there are two main issues facing the proposed 
corpus-based quantitative valuation: does it gener- 
alize and is it fair? 
The problem in generalization is this: can we use 
this method to evaluate anything other than ver- 
sions of FERGUS which generate sentences from the 
WSJ? We claim that we can indeed use the quan- 
titative evaluation procedure to evaluate most real- 
ization modules generating sentences from any cor- 
pus of unannotated English text. The fact that the 
tree-based metrics require dependency parses of the 
corpus is not a major impediment. Using exist- 
ing syntactic parsers plus ad-hoc postprocessors as 
needed, one can create the input representations to
the generator as well as the syntactic dependency 
trees needed for the tree-based metrics. The fact 
that the parsers introduce errors should not affect 
the way the scores are used, namely as relative scores 
(they have no real value absolutely). Which realiza- 
tion modules can be evaluated? First, it is clear 
that our approach can only evaluate single-sentence 
realization modules which may perform some sen- 
tence planning tasks, but cruciaUy not including sen- 
tence scoping/aggregation. Second, this approach 
:only works for generators whose input representa- 
tion is fairly "syntactic". For example, it may be 
difficult to evaluate in this manner a generator that 
-uses semanzic roles in-its inpntrepresent~ion,  since 
we currently cannot map large corpora of syntac- 
tic parses onto such semantic representations, and 
therefore cannot create the input representation for 
the evaluation. 
The second question is that of fairness of the evalu- 
ation. FE\[,tGt.'S as described in this paper is of lim- 
ited use. since it only chooses word order (and, to a 
certain extent, syntactic structure). Other realiza- 
tion and sentence planning tin{ks-which are needed 
for most applications and which may profit from a 
stochastic model include lexical choice, introduction 
of function words and punctuation, and generation 
of morphology. (See (Langkilde and Knight, 1998a) 
for a relevant discussion. FERGUS currently can per- 
form punctuation and function word insertion, and 
morphology and lexical choice are under develop- 
ment.) The question arises whether our metrics will 
. fairly measure the:quality,~of,a, more comp!ete real~ .... 
ization module (with some sentence planning). Once 
the range of choices that the generation component 
makes expands, one quickly runs into the problem 
that, while the gold standard may be a good way of 
communicating the input structure, there are usu- 
ally other good ways of doing so as well (using other 
words, other syntactic constructions, and so on). 
Our metrics will penalize such variation. However, 
in using stochastic methods one is of course precisely 
interested in learning from a corpus, so that the fact 
that there may be other ways of expressing an input 
is less relevant: the whole point of the stochastic ap- 
proach is precisely to express the input in a manner 
that resembles as much as possible the realizations 
found in the corpus (given its genre, register, id- 
iosyncratic hoices, and so on). Assuming the test 
corpus is representative of the training corpus, we 
can then use our metrics to measure deviance from 
the corpus, whether it be merely in word order or in 
terms of more complex tasks such as lexical choice 
as well. Thus, as long as the goal of the realizer 
is to enmlate as closely as possible a given corpus 
(rather than provide a maximal range of paraphras- 
tic capability), then our approach can be used for 
evaluation, r 
As in the case of machine translation, evaluation in 
generation is a complex issue. (For a discussion, see 
(Mellish and Dale, 1998).) Presumably, the qual- 
ity of most generation systems can only be assessed 
at a system level in a task-oriented setting (rather 
than by taking quantitative measures or by asking 
humans for quality assessments). Such evaluations 
are costly, and they cannot be the basis of work in 
stochastic generation, for which evaluation is a fre- 
quent step in research and development. An advan- 
tage of our approach is that our quantitative metrics 
allow us to evaluate without human intervention, au- 
tomatically and objectively (objectively with respect 
to the defined metric,-that is).- Independently, the 
use of the metrics has been validated using human 
subjects (as discussed in Section 4): once this has 
happened, the researcher can have increased confi- 
dence that choices nlade in research and develop- 
ment based on the quantitative metrics will in fact 
7We could also assume a set of acceptable paraphrases for 
each sentence in the test corpus. Our metrics are run on all 
paraphrases, and the best score chosen. However. for many 
applications it will not be emsy to construct such paraphrase 
sets, be it by hand or automatically. 
8 
correlate with relevant subjective qualitative mea- 
sures. 
References  
Hiyan Alshawi, Srinivas Bangalore, and Shona Dou- 
glas. 1998. Automatic acquisition of hierarchical 
~traalsduatian.:models :for ~machine. tr:anslation, tn 
Proceedings of the 36th Annual Meeting Association 
for Computational Linguistics, Montreal, Canada. 
Srinivas Bangalore and Aravind Joshi. 1999. Su- 
pertagging: An approach to almost parsing. Com- 
putational Linguistics, 25(2). 
Srinivas Bangalore and Owen Rambow. 2000. Ex- 
ploiting a probabilistic hierarchical model for gem 
eration. In Proceedings of the 18th International 
Conference on Computational Linguistics (COLING 
2000), Saarbriicken, Germany. 
Karen Kukich. 1983. Knowledge-Based Report Gen- 
eration: A Knowledge Engineering Approach to Nat- 
ural Language Report Generation. Ph.D. thesis, Uni- 
versity of Pittsuburgh. 
Irene Langkilde and Kevin Knight. 1998a. Gener- 
ation that exploits corpus-based statistical knowl- 
edge. In 36th Meeting of the Association for Com- 
putational Linguistics and 17th International Con- 
ference on Computational Linguistics (COLING- 
ACL'98), pages 704-710, Montreal, Canada. 
Irene Langkilde and Kevin Knight. 1998b. The 
practical value of n-grams in generation. In Proceed- 
ings of the Ninth International Natural Language 
Generation Workshop (INLG'98), Niagara-on-the- 
Lake, Ontario. 
Irene Langkilde. 2000. Forest-based statistical sen- 
tence generation. In 6th Applied Natural Language 
Processing Conference (ANLP'2000), pages 170- 
177, Seattle, WA. 
James C. Lester and Bruce W. Porter. 1997. De- 
veloping and empirically evaluating robust explana- 
tion generators: The KNIGHT experiments. Compu- 
tational Linguistics. 23(1):65-102. 
Igor A. Mel'~uk. 19S8. Dependency Syntax: Theory 
and Practice. State University of New ~%rk Press. 
New York. 
Chris Mellish and Robert Dale. 1998. Evahlation in 
the context of natural language generation. Corn= 
puter Speech and Language, 12:349-373. 
M. A. Walker, D. Litman, C. A. Kamm. and 
A. Abella. 1997. PARADISE: A general framework 
for evahlating spoken dialogue agents. In Proceed- 
ings of the 35th Annual Meeting of the Association 
of Computational Linguistics, A CL/EA CL 97. pages 
271-280. 
The XTAG-Group. 1999. A lexicalized Tree Adjoin- 
ing Gralnmar for English. Technical report, Insti- 
- tu;te for 1Research in Cognitive Science, University of 
Pennsylvania. 
