Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 458?467, Prague, June 2007. c?2007 Association for Computational Linguistics
Automatic Identification of Important Segments and Expressions for Mining
of Business-Oriented Conversations at Contact Centers
Hironori Takeuchi?, L Venkata Subramaniam?, Tetsuya Nasukawa?, and Shourya Roy?
?IBM Research, Tokyo Research Laboratory ?IBM Research, India Research Laboratory
Shimotsuruma 1623-14, Yamato-shi Institutional Area 4, Block-C, Vasant Kunj
Kanagawa 2428502 Japan New Delhi 110070 India
{hironori, nasukawa}@jp.ibm.com {lvsubram, rshourya}@in.ibm.com
Abstract
Textual records of business-oriented conver-
sations between customers and agents need
to be analyzed properly to acquire useful
business insights that improve productivity.
For such an analysis, it is critical to iden-
tify appropriate textual segments and ex-
pressions to focus on, especially when the
textual data consists of complete transcripts,
which are often lengthy and redundant. In
this paper, we propose a method to iden-
tify important segments from the conversa-
tions by looking for changes in the accuracy
of a categorizer designed to separate differ-
ent business outcomes. We extract effective
expressions from the important segments to
define various viewpoints. In text mining a
viewpoint defines the important associations
between key entities and it is crucial that the
correct viewpoints are identified. We show
the effectiveness of the method by using real
datasets from a car rental service center.
1 Introduction
?Contact center? is a general term for customer ser-
vice centers, help desks, and information phone
lines. Many companies operate contact centers to
sell their products, handle customer issues, and ad-
dress product-related and services-related issues. In
contact centers, analysts try to get insights for im-
proving business processes from stored customer
contact data. Gigabytes of customer contact records
are produced every day in the form of audio record-
ings of speech, transcripts, call summaries, email,
etc. Though analysis by experts results in insights
that are very deep and useful, such analysis usually
covers only a very small (1-2%) fraction of the total
call volume and yet requires significant workload.
The demands for extracting trends and knowledge
from the whole text data collection by using text
mining technology, therefore, are increasing rapidly.
In order to acquire valuable knowledge through
text mining, it is generally critical to identify im-
portant expressions to be monitored and compared
within the textual data. For example, given a large
collection of contact records at the contact center
of a manufacturer, the analysis of expressions for
products and expressions for problems often leads to
business value by identifying specific problems in a
specific product. If 30% of the contact records with
expressions for a specific product such as ?ABC?
contain expressions about a specific trouble such
as ?cracked?, while the expressions about the same
trouble appear in only 5% of the contact records for
similar products, then it should be a clue that the
product ?ABC? may actually have a crack-related
problem. An effective way to facilitate this type
of analysis is to register important expressions in a
lexicon such as ?ABC? and ?cracked? as associated
respectively with their categories such as ?product?
and ?problem? so that the behavior of terms in the
same category can be compared easily. It is actu-
ally one of the most important steps of text mining
to identify such relevant expressions and their cate-
gories that can potentially lead to some valuable in-
sights. A failure in this step often leads to a failure
in the text mining. Also, it has been considered an
artistic task that requires highly experienced consul-
458
tants to define such categories, which are often de-
scribed as the viewpoint for doing the analysis, and
their corresponding expressions through trial and er-
ror.
In this paper, we propose a method to identify im-
portant segments of textual data for analysis from
full transcripts of conversations. Compared to the
written summary of a conversation, a transcription
of an entire conversation tends to be quite lengthy
and contains various forms of redundancy. Many
of the terms appearing in the conversation are not
relevant for specific analysis. For example, the
terms for greeting such as ?Hello? and ?Welcome
to (Company A)? are unlikely to be associated with
specific business results such as purchased-or-not
and satisfied-or-not, especially because the conver-
sation is transcribed without preserving the nonver-
bal moods such as tone of voice, emotion etc. Thus
it is crucial to identify key segments and notable
expressions within conversations for analysis to ac-
quire valuable insights.
We exploit the fact that business conversations
follow set patterns such as an opening followed by a
request and the confirmation of details followed by
a closing, etc. By taking advantage of this feature of
business conversations, we have developed a method
to identify key segments and the notable expressions
within conversations that tend to discriminate be-
tween the business results. Such key segments, the
trigger segments, and the notable expressions asso-
ciated with certain business results lead us to easily
understand appropriate viewpoints for analysis.
Application of our method for analyzing nearly
one thousand conversations from a rental car reser-
vation office enabled us to acquire novel insights for
improving agent productivity and resulted in an ac-
tual increase in revenues.
Organization of the Paper: We start by describ-
ing the properties of the conversation data used in
this paper. Section 3 describes the method for iden-
tifying useful viewpoints and expressions that meet
the specified purpose. Section 4 provides the results
using conversational data. After the discussion in
Section 5, we conclude the paper in Section 6.
2 Business-Oriented Conversation Data
We consider business-oriented conversation data
collected at contact centers handling inbound tele-
phone sales and reservations. Such business oriented
conversations have the following properties.
? Each conversation is a one-to-one interaction
between a customer and an agent.
? For many contact center processes the conver-
sation flow is well defined in advance.
? There are a fixed number of outcomes and each
conversation has one of these outcomes.
For example, in car rentals, the following conversa-
tion flow is pre-defined for the agent. In practice
most calls to a car rental center follow this call flow.
? Opening - contains greeting, brand name, name
of agent
? Pick-up and return details - agent asks location,
dates and times of pick up and return, etc.
? Offering car and rate - agent offers a car spec-
ifying rate and mentions applicable special of-
fers.
? Personal details - agent asks for customer?s in-
formation such as name, address, etc.
? Confirm specifications - agent recaps reserva-
tion information such as name, location, etc.
? Mandatory enquiries - agent verifies clean driv-
ing record, valid license, etc.
? Closing - agent gives confirmation number and
thanks the customer for calling.
In these conversations the participants speak in turns
and the segments can be clearly identified. Figure 1
shows part of a transcribed call.
Each call has a specific outcome. For example,
each car rental transaction has one of two call types,
reservation or unbooked, as an outcome.
Because the call process is pre-defined, the con-
versations look similar in spite of having different
results. In such a situation, finding the differences in
the conversations that have effects on the outcomes
459
is very important, but it is very expensive and dif-
ficult to find such unknown differences by human
analysis. We show that it is possible to define proper
viewpoints and corresponding expressions leading
to insights on how to change the outcomes of the
calls.
AGENT: Welcome to CarCompanyA. My name is Albert. How may I
help you?
.........
AGENT: Allright may i know the location you want to pick the
car from.
CUSTOMER: Aah ok I need it from SFO.
AGENT: For what date and time.
.........
AGENT: Wonderful so let me see ok mam so we have a 12 or 15
passenger van avilable on this location on those dates and
for that your estimated total for those three dates just
300.58$ this is with Taxes with surcharges and with free
unlimited free milleage.
.........
AGENT : alright mam let me recap the dates you want to pick
it up from SFO on 3rd August and drop it off on august 6th in
LA alright
CUSTOMER : and one more questions Is it just in states or
could you travel out of states
.........
AGENT : The confirmation number for your booking is 221 384.
CUSTOMER : ok ok Thank you
Agent : Thank you for calling CarCompanyA and you have a
great day good bye
Figure 1: Transcript of a car rental dialog (partial)
3 Trigger Segment Detection and Effective
Expression Extraction
In this section, we describe a method for automat-
ically identifying valuable segments and concepts
from the data for the user-specified difference anal-
ysis. First, we present a model to represent the con-
versational data. After that we introduce a method
to detect the segments where the useful concepts for
the analysis appear. Finally, we select useful expres-
sions in each detected trigger segment.
3.1 Data Model
Each conversational data record in the collection D
is defined as di. Each di can be seen as a sequence
of conversational turns in the conversational data,
and then di can be divided as
di = d1i + d2i + ? ? ?+ dMii , (1)
where dki is the k-th turn in di and Mi is the total
number of turns in di. The + operator in the above
equation can be seen as an equivalent of the string
concatenation operator. We define d?ji as the por-
tion of di from the beginning to turn j. Using the
same notation, d?ji = d1i + d2i + ? ? ? + dji . The
collection of d?mki constitutes the Chronologically
Cumulative Data up to turn mk (Dk). Dk is repre-
sented as
Dk = (d?mk1 ,d?mk2 , . . . ,d?mkn ). (2)
Figure 2 shows an image of the data model. We set
some mk and prepare the chronologically cumula-
tive data set as shown in Figure 3. We represent bi-
nary mutually exclusive business outcomes such as
success and failure resulting from the conversations
as ?A? and ?not A?.
di= di1+?+diMi
Number of turns0 1 2 3 Mi
di1 di2 di3 diMimk
di~mk= i1+?+dimk
Figure 2: Conversation data model
m5 turnm1 m2 m3 m40 1 2 5 10 15
Ddidi~m5
D5di~m4
D4di~m3
D3
D2
D1
di~m2
di~m1
m1=1, m2=2, m3=5, m4=10, m5=15
Figure 3: Chronologically cumulative conversa-
tional data
3.2 Trigger Segment Detection
Trigger segments can be viewed as portions of the
data which have important features which distin-
guish data of class ?A? from data of class ?not A?.
460
To detect such segments, we divide each chrono-
logically cumulative data set Dk into two data sets,
training data Dtrainingk and test data Dtestk . Start-
ing from D1, for each Dk we trained a classifier
using Dtrainingk and evaluated it on Dtestk . Using
accuracy, the fraction of correctly classified docu-
ments, as a metric of performance (Yang and Liu,
1999), we denote the evaluation result of the cat-
egorization as acc(categorizer(Dk)) for each Dk
and plot it along with its turn. Figure 4 shows the
effect of gradually increasing the training data for
the classification. The distribution of expressions
m1 m2 m3 m4 m5
acc(categorizer(Di))
trigger trigger
D1
D2 D3
D4
D5 D all
turn
Figure 4: Plot of acc(categorizer(Dk))
in a business-oriented conversation will change al-
most synchronously because the call flow is pre-
defined. Therefore acc(categorizer(Dk)) will in-
crease if features that contribute to the categorization
appear in Dk. In contrast, acc(categorizer(Dk))
will decrease if no features that contribute to
the categorization are in Dk. Therefore, from
the transitions of acc(categorizer(Dk)), we can
identify the segments with increases as triggers
where the features that have an effect on the out-
come appear. We denote a trigger segment as
seg(start position, end position). Because the to-
tal numbers of turns can be different, we do not
detect the last section as a trigger. In Figure 4,
seg(m1,m2) and seg(m4,m5) are triggers. It is
important to note that using the cumulative data is
key to the detection of trigger segments. Using non-
cumulative segment data would give us the catego-
rization accuracy for the features within that seg-
ment but would not tell us whether the features of
this segment are improving the accuracy or decreas-
ing it. It is this gradient information between seg-
ments that is key to identifying trigger segments.
Many approaches have been proposed for docu-
ment classification (Yang and Liu, 1999). In this
research, however, we are not interested in the clas-
sification accuracy itself but in the increase and de-
crease of the accuracy within particular segments.
For example, the greeting, or the particular method
of payment may not affect the outcome, but the
mention of a specific feature of the product may
have an effect on the outcome. Therefore in our
research we are interested in identifying the partic-
ular portion of the call where this product feature
is mentioned, along with its mention, which has an
effect on the outcome of the call. In our experi-
ments we used the SVM (Support Vector Machine)
classifier (Joachims, 1998), but almost any classifier
should work because our approach does not depend
on the classification method.
3.3 Effective Expression Extraction
In this section, we describe our method to extract
effective expressions from the detected trigger seg-
ments.
The effective expressions in Dk are those which
are representative in the selected documents and
appear for the first time in the trigger segments
seg(mi,mj). Numerous methods to select features
exist (Hisamitsu and Niwa, 2002) (Yang and Ped-
ersen, 1997). We use the ?2 statistic for each ex-
pression in Dk as a representative metric. For the
two-by-two contingency table of a expression w and
a class ?A? shown in Table 1, the ?2 statistic is cal-
culated as
Table 1: Contingency table for calculating the ?2
statistic
# of documents # of documents
including w not including w
A n11 n12
not-A n21 n22
?2 = N(n11n22 ? n12n21)
2
(n11 + n12)(n11 + n21)(n12 + n22)(n21 + n22) (3)
where N is the number of documents. This statis-
tic can be compared to the ?2 distribution with one
degree of freedom to judge representativeness.
We also want to extract the expressions that have
not had an effect on the outcome before Dk. To de-
tect the new expressions in Dk, we define the metric
461
new(w) = w(Dk)max(w(Dk?1), 1)/
mk
mk?1
?sign(w(DAk )? w(DnotAk )), (4)
where w(Dk) is the frequency of expression w in
the chronologically cumulative data Dk, max(a, b)
selects the larger value in the arguments, mk is the
number of turns in Dk, w(DAk ) is the frequency of
w in Dk with the outcome of the corresponding data
being ?A?, and sign(?) is the signum function. When
w in class ?A? appears in Dk much more frequently
than Dk?1 compared with the ratio of their turns,
this metric will be more than 1. We detect signifi-
cant expressions by considering the combined score
?2(w) ? new(w). Using this combined score, we
can filter out the representative expressions that have
already appeared before Dk and distinguish signifi-
cant expressions that first appear in Dk for each class
?A? and ?not A?.
3.4 Appropriate Viewpoint Selection
In a text mining system, to get an association that
leads to a useful insight, we have to define appro-
priate viewpoints. Viewpoints refer to objects in re-
lation to other objects. In analysis using a conven-
tional text mining system (Nasukawa and Nagano,
2001), the viewpoints are selected based on expres-
sions in user dictionaries prepared by domain ex-
perts. We have identified important segments of the
conversations by seeing changes in the accuracy of a
categorizer designed to segregate different business
outcomes. We have also been able to extract effec-
tive expressions from these trigger segments to de-
fine various viewpoints. Hence, viewpoint selection
is now based on the trigger segments and effective
expressions identified automatically based on speci-
fied business outcomes. In the next section we apply
our technique to a real life dataset and show that we
can successfully select useful viewpoints.
4 Experiments and Results
4.1 Experiment Data and System
We collected 914 recorded calls from the car rental
help desk and manually transcribed them. Figure 1
shows part of a call that has been transcribed.
There are three types of calls:
1. Reservation Calls: Calls which got converted.
Here, ?converted? means the customer made a
reservation for a car. Reserved cars can get
picked-up or not picked-up, so some reserved
cars do not eventually get picked-up by cus-
tomers (no shows and cancellations).
2. Unbooked Calls: Calls which did not get con-
verted.
3. Service Calls: Customers changing or enquir-
ing about a previous booking.
The distribution of the calls is given in Table 2.
Table 2: Distribution of calls
Unbooked Calls 461
Reservation Calls (Picked-Up) 72
Reservation Calls (Not Picked-Up) 65
Service Calls 326
Total Calls 914
The reservation calls are most important in this
context, so we focus on those 137 calls. In the reser-
vation calls, there are two types of outcomes, car
picked-up and car not picked-up. All reservation
calls look similar in spite of having different out-
comes (in terms of pick up). The reservation hap-
pens during the call but the pick up happens at a
later date. If we can find differences in the conver-
sation that affect the outcome, it is expected that we
could improve the agent productivity. Reservation
calls follow the pre-defined reservation call flow that
we mentioned in Section 2 and it is very difficult
to find differences between them manually. In this
experiment, by using the proposed method, we try
to extract trigger segments and expressions to find
viewpoints that affect the outcome of the reservation
calls.
For the analysis, we constructed a text mining sys-
tem for the difference analysis ?picked-up? vs. ?not
picked-up?. The experimental system consists of
two parts, an information extraction part and a text
mining part. In the information extraction part we
define dictionaries and templates to identify useful
expressions. In the text mining part we define appro-
priate viewpoints based on the identified expressions
to get useful associations leading to useful insights.
462
4.2 Results of Trigger Segment Detection and
Effective Expression Extraction
Based on the pre-defined conversation flow de-
scribed in Section 2, we set m1=1, m2=2,
m3=5, m4=10, m5=15, and m6=20 and prepared
D1, . . . , D6 and D. The features of di consist of
nouns, compound nouns, specified noun phrases
(e.g. adjective+noun), and verbs. For each Dk
we calculated acc(categorizer(Dk)) for the classes
?picked-up? and ?not picked-up.? In this process, we
use a SVM-based document categorizer (Joachims,
2002). Of the 137 calls, we used 100 calls for
training the categorizer and 37 calls for trigger
segment detection. Figure 5 shows the results of
acc(categorizer(Dk)) for picked-up. The accuracy
of classification using the data of entire conversa-
tions (acc(categorizer(D)) is 67.6% but we are try-
ing to detect important segments by considering not
the accuracy values themselves but the gradients be-
tween segments. From these results, seg(1, 2) and
0
10
20
30
40
50
60
70
80
0 5 10 15 20 25 30 35 40 45
Turn (m_j)
A
c
c
u
r
a
c
y
 
[
%
]
D1
D2
D3
D4
D5
D
D6
Figure 5: Result of acc(categorizer(Dk))
seg(10, 15) are detected as trigger segments. We
now know that these segments are highly correlated
to the outcome of the call.
For each detected trigger segment, we extract ef-
fective expressions in each class using the metric de-
scribed in Section 3.3. Table 3 shows some expres-
sions with high values for the metric for each trigger.
In this table, ?just NUMERIC dollars? is a canonical
expression and an expression such as ?just 160 dol-
lars? is mapped to this canonical expression in the
information extraction process. From this result, in
seg(1, 2), ?make?, ?reservation? are correlated with
?pick up? and ?rate? and ?check? are correlated with
Table 3: Selected expressions in trigger segments
Trigger Selected expressions
pick up not picked up
seg(1, 2) make, return, tomorrow, rate, check, see
day, airport, look, want, week
assist, reservation, tonight
seg(10, 15) number, corporate program, go, impala
contract, card, have,
tax surcharge,
just NUMERIC dollars,
discount, customer club,
good rate, economy
?not-picked up?. By looking at some documents
containing these expressions, we found customer in-
tention phrases such as ?would like to make a reser-
vation?, ?want to check a rate?, etc. Therefore, it
can be induced that the way a customer starts the
call may have an impact on the outcome. From ex-
pressions in seg(10, 15), it can be said that discount-
related phrases and mentions of the good rates by the
agent can have an effect on the outcome.
We can directly apply the conventional methods
for representative feature selection to D. The fol-
lowing expressions were selected as the top 20 ex-
pressions from whole conversational data by using
the ?2 metric defined in (3).
corporate program, contract, counter, September,
mile, rate, economy, last name,
valid driving license,BRAND NAME, driving,
telephone, midsize, tonight, use, credit, moment,
airline, afternoon
From these results, we see that looking at the call as
a whole does not point us to the fact that discount-
related phrases, or the first customers-utterance, af-
fect the outcome. Detecting trigger segments and
extracting important expressions from each trigger
segment are key to identifying subtle differences be-
tween very similar looking calls that have entirely
opposite outcomes.
4.3 Results of Text Mining Analysis using
Selected Viewpoints and Expressions
From the detected segments and expressions we de-
termined that the customer?s first utterance along
with discount phrases and value selling phrases af-
fected the call outcomes. Under these hypotheses,
we prepared the following semantic categories.
463
? Customer intention at start of call: From the
customer?s first utterance, we extract the fol-
lowing intentions based on the patterns.
? strong start: would like to make a booking,
need to pick up a car, . . .
? weak start: would like to check the rates,
want to know the rate for vans, . . .
Under our hypotheses, the customer with a
strong start has the intention of booking a car
and we classify such a customer as a book-
ing customer. The customer with a weak start
usually just wants to know the rates and is clas-
sified as a rates customer.
? discount-related phrases: discount, corporate
program, motor club, buying club . . . are reg-
istered into the domain dictionary as discount-
related phrases.
? value selling phrases: we extract phrases men-
tioning good rates and good vehicles by match-
ing patterns related to such utterances.
? mentions of good rates: good rate, won-
derful price, save money, just need to pay
this low amount, . . .
? mentions of good vehicles: good car, fan-
tastic car, latest model, . . .
Using these three categories, we tried to find insights
to improve agent productivity.
Table 4 shows the result of two-dimensional as-
sociation analysis for 137 reservation calls. This ta-
ble shows the association between customer types
based on customer intention at the start of a call
and pick up information. From these results, 67%
Table 4: Association between customer types and
pick up information
Customer types extracted from texts Pick up information
based on customer intent at start of call pick up not-picked up
booking customer (w/ strong start) (70) 47 23
rates customer (w/ weak start) (37) 13 24
(47 out of 70) of the booking customers picked up
the reserved car and only 35% (13 out of 37) of the
rates customers picked it up. This supports our hy-
pothesis and means that pick up is predictable from
the customer?s first or second utterance.
It was found that cars booked by rates customers
tend to be ?not picked up,? so if we can find any
actions by agents that convert such customers into
?pick up,? then the revenue will improve. In the
booking customer case, to keep the ?pick up? high,
we need to determine specific agent actions that con-
cretize the customer?s intent.
Table 5 shows how mentioning discount-related
phrases affects the pick up ratios for rates customers
and booking customers. From this table, it can
Table 5: Association between mention of discount
phrases and pick up information
Rates customer Pick up information
Mention of discount phrases by agents pick up not-picked up
yes (21) 10 11
no (16) 3 13
Booking customer Pick up information
Mention of discount phrases by agents pick up not picked up
yes (40) 30 10
no (30) 17 13
be seen that mentioning discount phrases affects
the final status of both types of customers. In the
rates customer case, the probability that the booked
car will be picked up, P (pick-up) is improved to
0.476 by mentioning discount phrases. This means
customers are attracted by offering discounts and
this changes their intention from ?just checking rate?
to ?make a reservation here?. We found similar
trends for the association between mention of value
selling phrases and pick up information.
4.4 Improving Agent Productivity
From the results of the text mining analysis experi-
ment, we derived the following actionable insights:
? There are two types of customers in reservation
calls.
? Booking customer (with strong start)
tends to pick up the reserved car.
? Rates customer (with weak start) tends
not to pick up the reserved car.
? In the rates customer case, ?pick up? is im-
proved by mentioning discount phrases.
By implementing the actionable insights derived
from the analysis in an actual car rental process, we
verified improvements in pick up. We divided the
83 agents in the car rental reservation center into
two groups. One of them, consisting of 22 agents,
was trained based on the insights from the text min-
ing analysis. The remaining 61 agents were not
told about these findings. By comparing these two
464
groups over a period of one month we hoped to see
how the actionable insights contributed to improv-
ing agent performance. As the evaluation metric, we
used the pick up ratio - that is the ratio of the number
of ?pick-ups? to the number of reservations.
Following the training the pick up ratio of the
trained agents increased by 4.75%. The average
pick up ratio for the remaining agents increased by
2.08%. Before training the ratios of both groups
were comparable. The seasonal trends in this indus-
try mean that depending on the month the bookings
and pickups may go up or down. We believe this
is why the average pick up ratio for the remaining
agents also increased. Considering this, it can be es-
timated that by implementing the actionable insights
the pick up ratio for the pilot group was improved by
about 2.67%. We confirmed that this difference is
meaningful because the p-value of the t-test statistic
is 0.0675 and this probability is close to the stan-
dard t-test (?=0.05). Seeing this, the contact center
trained all of its agents based on the insights from
the text mining analysis.
5 Discussion
There has been a lot of work on specific tools for
analyzing the conversational data collected at con-
tact centers. These include call type classification
for the purpose of categorizing calls (Tang et al,
2003) (Zweig et al, 2006), call routing (Kuo and
Lee, 2003) (Haffner et al, 2003), obtaining call log
summaries (Douglas et al, 2005), agent assisting
and monitoring (Mishne et al, 2005), and building
of domain models (Roy and Subramaniam, 2006).
Filtering problematic dialogs automatically from an
automatic speech recognizer has also been studied
(Hastie et al, 2002) (Walker et al, 2002). In con-
trast to these technologies, in this paper we con-
sider the task of trying to find insights from a col-
lection of complete conversations. In (Nasukawa
and Nagano, 2001), such an analysis was attempted
for agent-entered call summaries of customer con-
tacts by extracting phrases based on domain-expert-
specified viewpoints. In our work we have shown
that even for conversational data, which is more
complex, we could identify proper viewpoints and
prepare expressions for each viewpoint. Call sum-
maries by agents tend to mask the customers? inten-
tion at the start of the call. We get more valuable
insights from the text mining analysis of conversa-
tional data. For such an analysis of conversational
data, our proposed method has an important role.
With our method, we find the important segments
in the data for doing analyses. Also our analyses are
closely linked to the desired outcomes.
In trigger detection, we created a chronologically
cumulative data set based on turns. We can also
use the segment information such as the ?opening?
and ?enquiries? described in Section 2. We prepared
data with segment information manually assigned,
made the chronologically cumulative data and ap-
plied our trigger detection method. Figure 6 shows
the results of acc(categorizer(Dk)). The trend in
40
45
50
55
60
call start -->
opening
call start -->
details
call start -->
offering
call start -->
personal
details
call start -->
confirmation,
mandatory
questions,
closing
Conversation flow
A
c
c
u
r
a
c
y
 
[
%
]
Figure 6: Result of acc(categorizer(Dk)) using
segment information
Figure 6 is similar to that in Figure 5. From this
result, it is observed that ?opening? and ?offering?
segments are trigger segments. Usually, segmenta-
tion is not done in advance and to assign such infor-
mation automatically we need data with labeled seg-
mentation information. The results show that even
in the absence of labeled data our trigger detection
method identifies the trigger segments. In the exper-
iments in Section 4, we set turns for each chrono-
logically cumulative data by taking into account the
pre-defined call flow.
In Figure 5 we observe that the accuracy of the
categorizer is decreasing even when using increas-
ing parts of the call. Even the accuracy using the
complete call is less than using only the first turn.
This indicates that the first turn is very informative,
but it also indicates that the features are not being
used judiciously. In a conventional classification
task, the number of features are sometimes restricted
465
when constructing a categorizer. It is known that se-
lecting only significant features improves the clas-
sification accuracy (Yang and Pedersen, 1997). We
used Information Gain for selecting features from
the document collection. This method selects the
most discriminative features between two classes.
As expected the classification accuracy improved
significantly as we reduced the total number of fea-
tures from over 2,000 to the range of 100 to 300.
Figure 7 shows the changes in accuracy. In the pro-
40
45
50
55
60
65
70
75
80
85
90
0 5 10 15 20 25 30 35 40 45
Turn (m_j)
A
c
c
u
r
a
c
y
 
[
%
}
100
200
300
D1
D2
D3
D4 D5 D
D6
Figure 7: Result of acc(categorizer(Dk)) with top
100 to 300 features selected using information gain
posed method, we detect trigger segments using the
increases and decreases of the classification accu-
racy. By selecting features, the noisy features are not
added in the segments. Decreasing portions, there-
fore are not observed. In this situation, as a trigger
segment, we can detect the portion where the gra-
dient of the accuracy curve increases. Also using
feature selection, we find that the classification ac-
curacy is highest when using the entire document,
which is expected. However, we notice that the trig-
ger segments obtained with and without feature se-
lection are almost the same.
In the experiment, we use manually transcribed
data. As future work we would like to use the noisy
output of an automatic speech recognition system to
obtain viewpoints and expressions.
6 Conclusion
In this paper, we have proposed methods for iden-
tifying appropriate segments and expressions auto-
matically from the data for user specified difference
analysis. We detected the trigger segments using the
property that a business-oriented conversation fol-
lows a pre-defined flow. After that, we identified
the appropriate expressions from each trigger seg-
ment. It was found that in a long business-priented
conversation there are important segments affecting
the outcomes that can not been easily detected by
just looking through the conversation, but such seg-
ments can be detected by monitoring the changes
of the categorization accuracy. For the trigger seg-
ment detection, we do not use semantic segment in-
formation but only the positional segment informa-
tion based on the conversational turns. Because our
method does not rely on the semantic information in
the data, therefore our method can be seen as robust.
Through experiments with real conversational data,
using identified segments and expressions we were
able to define appropriate viewpoints and concepts
leading to insights for improving the car rental busi-
ness process.
Acknowledgment
The authors would like to thank Sreeram Balakr-
ishnan, Raghuram Krishnapuram, Hideo Watanabe,
and Koichi Takeda at IBM Research for their sup-
port. The authors also appreciate the efforts of Jatin
Joy Giri at IBM India in providing domain knowl-
edge about the car rental process and thank him for
help in constructing the dictionaries.
References
S. Douglas, D. Agarwal, T. Alonso, R. M. Bell,
M. Gilbert, D. F. Swayne, and C. Volinsky. 2005.
Mining customer care dialogs for ?daily news?.
IEEE Transaction on Speech and Audio Processing,
13(5):652?660.
P. Haffner, G. Tur, and J. H. Wright. 2003. Optimizing
svms for complex call classification. In Proceedings of
IEEE International Conference on Acoustics, Speech,
and Signal Processing (ICASSP), pages 632?635.
H. W. Hastie, R. Prasad, and M. A. Walker. 2002. What?s
the trouble: Automatically identifying problematic di-
alogues in darpa communicator dialogue systems. In
Proceedings of the 40th Annual Meeting of the ACL,
pages 384?391.
T. Hisamitsu and Y. Niwa. 2002. A measure of term rep-
resentativeness based on the number of co-occurring
sailent words. In Proceedings of the 19th International
Conference on Computational Linguistics (COLING),
pages 1?7.
466
T. Joachims. 1998. Text categorization with support vec-
tor machines: Learning with many relevant features.
In Proceedings of the 10th European Conference on
Machine Learning (ECML), pages 137?142.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In Proceedings of the ACM Con-
ference on Knowledge Discovery and Data Mining
(KDD), pages 133?142.
H.-K J. Kuo and C.-H. Lee. 2003. Discriminative train-
ing of natural language call routers. IEEE Transaction
on Speech and Audio Processing, 11(1):24?35.
G. Mishne, D. Carmel, R. Hoory, A. Roytman, and
A. Soffer. 2005. Automatic analysis of call-center
conversations. In Proceedings of ACM Conference
on Information and Knowledge Management (CIKM),
pages 453?459.
T. Nasukawa and T. Nagano. 2001. Text analysis and
knowledge mining system. IBM Systems Journal,
pages 967?984.
S. Roy and L. V. Subramaniam. 2006. Automatic
generation of domain models for call centers from
noisy transcriptions. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the ACL (COLING/ACL),
pages 737?744.
M. Tang, B. Pellom, and K. Hacioglu. 2003. Call-
type classification and unsupervised training for the
call center domain. In Proceesings of IEEE Workshop
on Automatic Speech Recognition and Understanding,
pages 204?208.
M. A. Walker, I. Langkilde-Geary, H. W. Hastie,
J. Wright, and A. Gorin. 2002. Automatically train-
ing a problematic dialogue predictor for a spoken di-
alogue system. Journal of Artificial Intelligence Re-
search, 16:393?319.
Y. Yang and X. Liu. 1999. A re-examination of text cate-
gorization methods. In Proceedings of the 22th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages 42?
49.
Y. Yang and J. O. Pedersen. 1997. A comparative study
on feature selection in text categorization. In Proceed-
ings of the 14th International Conference on Machine
Learning (ICML), pages 412?420.
G. Zweig, O. Shiohan, G. Saon, B. Ramabhadran,
D. Povey, L. Mangu, and B. Kingsbury. 2006. Au-
tomatic analysis of call-center conversations. In Pro-
ceedings of IEEE Internatinal Conference of Acous-
tics, Speech and Signal Processing (ICASSP), pages
589?592.
467
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 737?744,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatic Generation of Domain Models for Call Centers from Noisy
Transcriptions
Shourya Roy and L Venkata Subramaniam
IBM Research
India Research Lab
IIT Delhi, Block-1
New Delhi 110016
India
rshourya,lvsubram@in.ibm.com
Abstract
Call centers handle customer queries from various
domains such as computer sales and support, mo-
bile phones, car rental, etc. Each such domain
generally has a domain model which is essential
to handle customer complaints. These models
contain common problem categories, typical cus-
tomer issues and their solutions, greeting styles.
Currently these models are manually created over
time. Towards this, we propose an unsupervised
technique to generate domain models automati-
cally from call transcriptions. We use a state of
the art Automatic Speech Recognition system to
transcribe the calls between agents and customers,
which still results in high word error rates (40%)
and show that even from these noisy transcrip-
tions of calls we can automatically build a domain
model. The domain model is comprised of pri-
marily a topic taxonomy where every node is char-
acterized by topic(s), typical Questions-Answers
(Q&As), typical actions and call statistics. We
show how such a domain model can be used for
topic identification of unseen calls. We also pro-
pose applications for aiding agents while handling
calls and for agent monitoring based on the do-
main model.
1 Introduction
Call center is a general term for help desks, infor-
mation lines and customer service centers. Many
companies today operate call centers to handle
customer issues. It includes dialog-based (both
voice and online chat) and email support a user
receives from a professional agent. Call centers
have become a central focus of most companies as
they allow them to be in direct contact with their
customers to solve product-related and services-
related issues and also for grievance redress. A
typical call center agent handles over a hundred
calls in a day. Gigabytes of data is produced ev-
ery day in the form of speech audio, speech tran-
scripts, email, etc. This data is valuable for doing
analysis at many levels, e.g., to obtain statistics
about the type of problems and issues associated
with different products and services. This data can
also be used to evaluate agents and train them to
improve their performance.
Today?s call centers handle a wide variety of do-
mains such as computer sales and support, mobile
phones and apparels. To analyze the calls in any
domain, analysts need to identify the key issues
in the domain. Further, there may be variations
within a domain, say mobile phones, based on the
service providers. The analysts generate a domain
model through inspection of the call records (au-
dio, transcripts and emails). Such a model can in-
clude a listing of the call categories, types of prob-
lems solved in each category, listing of the cus-
tomer issues, typical questions-answers, appropri-
ate call opening and closing styles, etc. In essence,
these models provide a structured view of the do-
main. Manually building such models for vari-
ous domains may become prohibitively resource
intensive. Another important point to note is that
these models are dynamic in nature and change
over time. As a new version of a mobile phone
is introduced, software is launched in a country, a
sudden attack of a virus, the model may need to be
refined. Hence, an automated way of creating and
maintaining such a model is important.
In this paper, we have tried to formalize the es-
sential aspects of a domain model. It comprises
of primarily a topic taxonomy where every node
is characterized by topic(s), typical Questions-
737
Answers (Q&As), typical actions and call statis-
tics. To build the model, we first automatically
transcribe the calls. Current automatic speech
recognition technology for telephone calls have
moderate to high word error rates (Padmanabhan
et al, 2002). We applied various feature engi-
neering techniques to combat the noise introduced
by the speech recognition system and applied text
clustering techniques to group topically similar
calls together. Using clustering at different gran-
ularity and identifying the relationship between
groups at different granularity we generate a tax-
onomy of call types. This taxonomy is augmented
with various meta information related to each node
as mentioned above. Such a model can be used
for identification of topics of unseen calls. To-
wards this, we envision an aiding tool for agents
to increase agent effectiveness and an administra-
tive tool for agent appraisal and training.
Organization of the paper: We start by de-
scribing related work in relevant areas. Section 3
talks about the call center dataset and the speech
recognition system used. The following section
contains the definition and describes an unsuper-
vised mechanism for building a topical model
from automatically transcribed calls. Section 5
demonstrates the usability of such a topical model
and proposes possible applications. Section 6 con-
cludes the paper.
2 Background and Related Work
In this work, we are trying to bridge the gap be-
tween a few seemingly unrelated research areas
viz. (1) Automatic Speech Recognition(ASR), (2)
Text Clustering and Automatic Taxonomy Gener-
ation (ATG) and (3) Call Center Analytics. We
present some relevant work done in each of these
areas.
Automatic Speech Recognition(ASR): Auto-
matic transcription of telephonic conversations is
proven to be more difficult than the transcription
of read speech. According to (Padmanabhan et
al., 2002), word-error rates are in the range of 7-
8% for read speech whereas for telephonic speech
it is more than 30%. This degradation is due
to the spontaneity of speech as well as the tele-
phone channel. Most speech recognition systems
perform well when trained for a particular accent
(Lawson et al, 2003). However, with call cen-
ters now being located in different parts of the
world, the requirement of handling different ac-
cents by the same speech recognition system fur-
ther increases word error rates.
Automatic Taxonomy Generation (ATG): In re-
cent years there has been some work relating to
mining domain specific documents to build an on-
tology. Mostly these systems rely on parsing (both
shallow and deep) to extract relationships between
key concepts within the domain. The ontology is
constructed from this by linking the extracted con-
cepts and relations (Jiang and Tan, 2005). How-
ever, the documents contain well formed sentences
which allow for parsers to be used.
Call Center Analytics: A lot of work on auto-
matic call type classification for the purpose of
categorizing calls (Tang et al, 2003), call rout-
ing (Kuo and Lee, 2003; Haffner et al, 2003), ob-
taining call log summaries (Douglas et al, 2005),
agent assisting and monitoring (Mishne et al,
2005) has appeared in the past. In some cases, they
have modeled these as text classification problems
where topic labels are manually obtained (Tang et
al., 2003) and used to put the calls into different
buckets. Extraction of key phrases, which can be
used as features, from the noisy transcribed calls
is an important issue. For manually transcribed
calls, which do not have any noise, in (Mishne et
al., 2005) a phrase level significance estimate is
obtained by combining word level estimates that
were computed by comparing the frequency of a
word in a domain-specific corpus to its frequency
in an open-domain corpus. In (Wright et al, 1997)
phrase level significance was obtained for noisy
transcribed data where the phrases are clustered
and combined into finite state machines. Other
approaches use n-gram features with stop word re-
moval and minimum support (Kuo and Lee, 2003;
Douglas et al, 2005). In (Bechet et al, 2004) call
center dialogs have been clustered to learn about
dialog traces that are similar.
Our Contribution: In the call center scenario, the
authors are not aware of any work that deals with
automatically generating a taxonomy from tran-
scribed calls. In this paper, we have tried to for-
malize the essential aspects of a domain model.
We show an unsupervised method for building a
domain model from noisy unlabeled data, which is
available in abundance. This hierarchical domain
model contains summarized topic specific details
for topics of different granularity. We show how
such a model can be used for topic identification
of unseen calls. We propose two applications for
738
aiding agents while handling calls and for agent
monitoring based on the domain model.
3 Issues with Call Center Data
We obtained telephonic conversation data col-
lected from the internal IT help desk of a com-
pany. The calls correspond to users making spe-
cific queries regarding problems with computer
software such as Lotus Notes, Net Client, MS Of-
fice, MS Windows, etc. Under these broad cate-
gories users faced specific problems e.g. in Lotus
Notes users had problems with their passwords,
mail archiving, replication, installation, etc. It is
possible that many of the sub problem categories
are similar, e.g. password issues can occur with
Lotus Notes, Net Client and MS Windows.
We obtained automatic transcriptions of the di-
alogs using an Automatic Speech Recognition
(ASR) system. The transcription server, used for
transcribing the call center data, is an IBM re-
search prototype. The speech recognition system
was trained on 300 hours of data comprising of
help desk calls sampled at 6KHz. The transcrip-
tion output comprises information about the rec-
ognized words along with their durations, i.e., be-
ginning and ending times of the words. Further,
speaker turns are marked, so the agent and cus-
tomer portions of speech are demarcated without
exactly naming which part is the agent and which
the customer. It should be noted that the call cen-
ter agents and the customers were of different na-
tionalities having varied accents and this further
made the job of the speech recognizer hard. The
resultant transcriptions have a word error rate of
about 40%. This high error rate implies that many
wrong deletions of actual words and wrong inser-
tion of dictionary words have taken place. Also
often speaker turns are not correctly identified and
voice portions of both speakers are assigned to a
single speaker. Apart from speech recognition er-
rors there are other issues related to spontaneous
speech recognition in the transcriptions. There are
no punctuation marks, silence periods are marked
but it is not possible to find sentence boundaries
based on these. There are repeats, false starts, a
lot of pause filling words such as um and uh, etc.
Portion of a transcribed call is shown in figure 1.
Generally, at these noise levels such data is hard
to interpret by a human. We used over 2000 calls
that have been automatically transcribed for our
analysis. The average duration of a call is about 9
SPEAKER 1: windows thanks for calling and you can
learn yes i don?t mind it so then i went to
SPEAKER 2: well and ok bring the machine front
end loaded with a standard um and that?s um it?s
a desktop machine and i did that everything was
working wonderfully um I went ahead connected
into my my network um so i i changed my network
settings to um to my home network so i i can you
know it?s showing me for my workroom um and then
it is said it had to reboot in order for changes
to take effect so i rebooted and now it?s asking
me for a password which i never i never said
anything up
SPEAKER 1: ok just press the escape key i can
doesn?t do anything can you pull up so that i mean
Figure 1: Partial transcript of a help desk dialog
minutes. For 125 of these calls, call topics were
manually assigned.
4 Generation of Domain Model
Fig 2 shows the steps for generating a domain
model in the call center scenario. This section ex-
plains different modules shown in the figure.
4.1 Description of Model
We propose the Domain Model to be comprised
of primarily a topic taxonomy where every node
is characterized by topic(s), typical Questions-
Answers (Q&As), typical actions and call statis-
tics. Generating such a taxonomy manually from
scratch requires significant effort. Further, the
changing nature of customer problems requires
frequent changes to the taxonomy. In the next sub-
section, we show that meaningful taxonomies can
be built without any manual supervision from a
collection of noisy call transcriptions.
4.2 Taxonomy Generation
As mentioned in section 3, automatically tran-
scribed data is noisy and requires a good amount
of feature engineering before applying any text
analytics technique. Each transcription is passed
through a Feature Engineering Component to per-
form noise removal. We performed a sequence of
cleansing operations to remove stopwords such as
the, of, seven, dot, january, hello. We also remove
pause filling words such as um, uh, huh . The re-
maining words in every transcription are passed
through a stemmer (using Porter?s stemming algo-
739
Stopword
Removal
N-gram
Extraction
Datab
ase,
 
archiv
e, 
replic
ate
Ca
n 
you
 
ac
ce
ss
 
yah
oo
? 
Is m
ode
m 
on
?
Call
 statistics
Feature Engineering
ASR
Clusterer TaxonomyBuilder
Model
Builder
Component
Clusters of different 
granularity
Voice help-desk data
1
2
3 4
5
Figure 2: 5 Steps to automatically build domain model from a collection of telephonic conversation
recordings
rithm 1) to extract the root form of every word e.g.
call from called. We extract all n-grams which
occur more frequently than a threshold and do not
contain any stopword. We observed that using
all n-grams without thresholding deteriorates the
quality of the generated taxonomy. a t & t, lotus
notes, and expense reimbursement are some exam-
ples of extracted n-grams.
The Clusterer generates individual levels of
the taxonomy by using text clustering. We used
CLUTO package 2 for doing text clustering. We
experimented with all the available clustering
functions in CLUTO but no one clustering al-
gorithm consistently outperformed others. Also,
there was not much difference between various
algorithms based on the available goodness met-
rics. Hence, we used the default repeated bisec-
tion technique with cosine function as the similar-
ity metric. We ran this algorithm on a collection
of 2000 transcriptions multiple times. First we
generate 5 clusters from the 2000 transcriptions.
Next we generate 10 clusters from the same set
of transcriptions and so on. At the finest level we
split them into 100 clusters. To generate the topic
1http://www.tartarus.org/?martin/PorterStemmer
2http://glaros.dtc.umn.edu/gkhome/views/cluto
taxonomy, these sets containing 5 to 100 clusters
are passed through the Taxonomy Builder compo-
nent. This component (1) removes clusters con-
taining less than n documents (2) introduces di-
rected edges from cluster v1 to v2 if v1 and v2
share at least one document between them, and
where v2 is one level finer than v1. Now v1 and v2
become nodes in adjacent layers in the taxonomy.
Here we found the taxonomy to be a tree but in
general it can be a DAG. Now onwards, each node
in the taxonomy will be referred to as a topic.
This kind of top-down approach was preferred
over a bottom-up approach because it not only
gives the linkage between clusters of various gran-
ularity but also gives the most descriptive and dis-
criminative set of features associated with each
node. CLUTO defines descriptive (and discrimi-
native) features as the set of features which con-
tribute the most to the average similarity (dissim-
ilarity) between documents belonging to the same
cluster (different clusters). In general, there is a
large overlap between descriptive and discrimina-
tive features. These features, topic features, are
later used for generating topic specific informa-
tion. Figure 3 shows a part of the taxonomy ob-
tained from the IT help desk dataset. The labels
740
atandt
connect lotusnot
click client
connect
wireless
network
default
properti
net
netclient
localarea
areaconnect
router
cabl
databas
server folder
copi archiv
replic
mail
slash
folder
file
archiv
databas
servercopi
localcopi
Figure 3: A part of the automatically generated
ontology along with descriptive features.
shown in Figure 3 are the most descriptive and dis-
criminative features of a node given the labels of
its ancestors.
4.3 Topic Specific Information
The Model Builder component in Figure 2 creates
an augmented taxonomy with topic specific infor-
mation extracted from noisy transcriptions. Topic
specific information includes phrases that describe
typical actions, typical Q&As and call statistics
(for each topic in the taxonomy).
Typical Actions: Actions correspond to typical is-
sues raised by the customer, problems and strate-
gies for solving them. We observed that action re-
lated phrases are mostly found around topic fea-
tures. Hence, we start by searching and collect-
ing all the phrases containing topic words from
the documents belonging to the topic. We define
a 10-word window around the topic features and
harvest all phrases from the documents. The set
of collected phrases are then searched for n-grams
with support above a preset threshold. For exam-
ple, both the 10-grams note in click button to set
up for all stops and to action settings and click the
button to set up increase the support count of the
5-gram click button to set up.
The search for the n-grams proceeds based on
a threshold on a distance function that counts the
insertions necessary to match the two phrases. For
example can you is closer to can < ... > you than
to can < ... >< ... > you. Longer n-grams are
allowed a higher distance threshold than shorter n-
grams. After this stage we extracted all the phrases
that frequently occur within the cluster.
In the second step, phrase tiling and ordering,
we prune and merge the extracted phrases and or-
der them. Tiling constructs longer n-grams from
sequences of overlapping shorter n-grams. We
noted that the phrases have more meaning if they
are ordered by their appearance. For example, if
go to the program menu typically appears before
select options from program menu then it is more
thank you for calling this is
problem with our serial number software
Q: may i have your serial number
Q: how may i help you today
A: i?m having trouble with my at&t network
............
............
click on advance log in properties
i want you to right click
create a connection across an existing internet
connection
in d. n. s. use default network
............
............
Q: would you like to have your ticket
A: ticket number is two
thank you for calling and have a great day
thank you for calling bye bye
anything else i can help you with
have a great day you too
Figure 4: Topic specific information
useful to present them in the order of their appear-
ance. We establish this order based on the average
turn number where a phrase occurs.
Typical Questions-Answers: To understand a
customer?s issue the agent needs to ask the right
set of questions. Asking the right questions is the
key to effective call handling. We search for all the
questions within a topic by defining question tem-
plates. The question templates basically look for
all phrases beginning with how, what, can I, can
you, were there, etc. This set comprised of 127
such templates for questions. All 10-word phrases
conforming to the question templates are collected
and phrase harvesting, tiling and ordering is done
on them as described above. For the answers we
search for phrases in the vicinity immediately fol-
lowing the question.
Figure 4 shows a part of the topic specific in-
formation that has been generated for the default
properti node in Fig 3. There are 123 documents
in this node. We have selected phrases that occur
at least 5 times in these 123 documents. We have
captured the general opening and closing styles
used by the agents in addition to typical actions
and Q&As for the topic. In this node the docu-
ments pertain to queries on setting up a new A T &
T network connection. Most of the topic specific
issues that have been captured relate to the agent
741
leading the customer through the steps for setting
up the connection. In the absence of tagged dataset
we could not quantify our observation. However,
when we compared the automatically generated
topic specific information to the extracted infor-
mation from the hand labeled calls, we noted that
almost all the issues have been captured. In fact
there are some issues in the automatically gener-
ated set that are missing from the hand labeled set.
The following observations can be made from the
topic specific information that has been generated:
? The phrases that have been captured turn out
to be quite well formed. Even though the
ASR system introduces a lot of noise, the re-
sulting phrases when collected over the clus-
ters are clean.
? Some phrases appear in multiple forms thank
you for calling how can i help you, how may
i help you today, thanks for calling can i
be of help today. While tiling is able to
merge matching phrases, semantically simi-
lar phrases are not merged.
? The list of topic specific phrases, as already
noted, matched and at times was more ex-
haustive than similar hand generated sets.
Call Statistics: We compute various aggregate
statistics for each node in the topic taxonomy as
part of the model viz. (1) average call duration(in
seconds), (2) average transcription length(number
of words) (3) average number of speaker turns and
(4) number of calls. We observed that call dura-
tions and number of speaker turns varies signifi-
cantly from one topic to another. Figure 5 shows
average call duration and corresponding average
transcription lengths for a few interesting topics. It
can be seen that in topic cluster-1, which is about
expense reimbursement and related stuff, most of
the queries can be answered quickly in standard
ways. However, some connection related issues
(topic cluster-5) require more information from
customers and are generally longer in duration. In-
terestingly, topic cluster-2 and topic cluster-4 have
similar average call durations but quite different
average transcription lengths. On investigation we
found that cluster-4 is primarily about printer re-
lated queries where the customer many a times is
not ready with details like printer name, ip address
of the printer, resulting in long hold time whereas
for cluster-2, which is about online courses, users
 0
 100
 200
 300
 400
 500
 600
 700
 800
 900
54321
 0
 200
 400
 600
 800
 1000
 1200
 1400
 1600
Ca
ll
 D
ur
at
io
n(
se
cs
)
Tr
an
sc
ri
pt
io
n 
Le
ng
th
(n
o.
 o
f 
wo
rd
s)
Topic Cluster
Figure 5: Call duration and transcription length for
some topic clusters
generally have details like course name, etc. ready
with them and are interactive in nature.
We build a hierarchical index of type
{topic?information} based on this automat-
ically generated model for each topic in the topic
taxonomy. An entry of this index contains topic
specific information viz. (1) typical Q&As, (2)
typical actions, and (3) call statistics. As we
go down this hierarchical index the information
associated with each topic becomes more and
more specific. In (Mishne et al, 2005) a manually
developed collection of issues and their solutions
is indexed so that they can be matched to the
call topic. In our work the indexed collection is
automatically obtained from the call transcrip-
tions. Also, our index is more useful because of
its hierarchical nature where information can be
obtained for topics of various granularity unlike
(Mishne et al, 2005) where there is no concept of
topics at all.
5 Application of Domain Model
Information retrieval from spoken dialog data is an
important requirement for call centers. Call cen-
ters constantly endeavor to improve the call han-
dling efficiency and identify key problem areas.
The described model provides a comprehensive
and structured view of the domain that can be used
to do both. It encodes three levels of information
about the domain:
? General: The taxonomy along with the la-
bels gives a general view of the domain. The
general information can be used to monitor
trends on how the number of calls in differ-
ent categories change over time e.g. daily,
weekly, monthly.
742
? Topic level: This includes a listing of the spe-
cific issues related to the topic, typical cus-
tomer questions and problems, usual strate-
gies for solving the problems, average call
durations, etc. It can be used to identify pri-
mary issues, problems and solutions pertain-
ing to any category.
? Dialog level: This includes information on
how agents typically open and close calls, ask
questions and guide customers, average num-
ber of speaker turns, etc. The dialog level
information can be used to monitor whether
agents are using courteous language in their
calls, whether they ask pertinent questions,
etc.
The {topic?information} index requires iden-
tification of the topic for each call to make use
of information available in the model. Below we
show examples of the use of the model for topic
identification.
5.1 Topic Identification
Many of the customer complaints can be catego-
rized into coarse as well as fine topic categories
by listening to only the initial part of the call. Ex-
ploiting this observation we do fast topic identi-
fication using a simple technique based on distri-
bution of topic specific descriptive and discrimi-
native features (Sec 4.2) within the initial portion
of the call. Figure 6 shows variation in prediction
accuracy using this technique as a function of the
fraction of a call observed for 5, 10 and 25 clus-
ters verified over the 125 hand-labeled transcrip-
tions. It can be seen, at coarse level, nearly 70%
prediction accuracy can be achieved by listening
to the initial 30% of the call and more than 80% of
the calls can be correctly categorized by listening
only to the first half of the call. Also calls related
to some categories can be quickly detected com-
pared to some other clusters as shown in Figure 7.
5.2 Aiding and Administrative Tool
Using the techniques presented in this paper so far
it is possible to put together many applications for
a call center. In this section we give some exam-
ple applications and describe ways in which they
can be implemented. Based on the hierarchical
model described in Section 4 and topic identifica-
tion mentioned in the last sub-section we describe
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
100908070605040302010
Pr
ed
ic
ti
on
 a
cc
ur
ac
y(
%)
Fraction of call observed(%)
?5-Clusters?
?10-Clusters?
?25-Clusters?
Figure 6: Variation in prediction accuracy with
fraction of call observed for 5, 10 and 25 clusters
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
10987654321
Pr
ed
ic
ti
on
 a
cc
ur
ac
y(
%)
Cluster ID
25% observed
50% observed
75% observed
100% observed
Figure 7: Cluster wise variation in prediction ac-
curacy for 10 clusters
(1) a tool capable of aiding agents for efficient
handling of calls to improve customer satisfaction
as well as to reduce call handling time, (2) an ad-
ministrative tool for agent appraisal and training.
Agent aiding is done based on the automati-
cally generated domain model. The hierarchical
nature of the model helps to provide generic to
specific information to the agent as the call pro-
gresses. During call handling the agent can be
provided the automatically generated taxonomy
and the agent can get relevant information asso-
ciated with different nodes by say clicking on the
nodes. For example, once the agent identifies a
call to be about {lotusnot} in Fig 3 then he can
see the generic Lotus Notes related Q&As and ac-
tions. By interacting further with the customer the
agent identifies it to be of {copi archiv replic}
topic and typical Q&As and actions change ac-
cordingly. Finally, the agent narrows down to the
topic as {servercopi localcopi} and suggest solu-
tion for replication problem in Lotus Notes.
The concept of administrative tool is primar-
ily driven by Dialog and Topic level information.
We envision this post-processing tool to be used
743
for comparing completed individual calls with cor-
responding topics based on the distribution of
Q&As, actions and call statistics. Based on the
topic level information we can check whether the
agent identified the issues and offered the known
solutions on a given topic. We can use the dialog
level information to check whether the agent used
courteous opening and closing sentences. Calls
that deviate from the topic specific distributions,
can be identified in this way and agents handling
these calls can be offered further training on the
subject matter, courtesy, etc. This kind of post-
processing tool can also help us to catch abnor-
mally long calls, agents with high average call
handle time, etc.
6 Discussion and Future Work
We have shown that it is possible to retrieve use-
ful information from noisy transcriptions of call
center voice conversations. We have shown that
the extracted information can be put in the form of
a model that succinctly captures the domain and
provides a comprehensive view of it. We briefly
showed through experiments that this model is an
accurate description of the domain. We have also
suggested useful scenarios where the model can be
used to aid and improve call center performance.
A call center handles several hundred-thousand
calls per year in various domains. It is very diffi-
cult to monitor the performance based on manual
processing of the calls. The framework presented
in this paper, allows a large part of this work
to be automated. A domain specific model that
is automatically learnt and updated based on the
voice conversations allows the call center to iden-
tify problem areas quickly and allocate resources
more effectively.
In future we would like to semantically clus-
ter the topic specific information so that redundant
topics are eliminated from the list. We can use Au-
tomatic Taxonomy Generation(ATG) algorithms
for document summarization (Kummamuru et al,
2004) to build topic taxonomies. We would also
like to link our model to technical manuals, cata-
logs, etc. already available on the different topics
in the given domain.
Acknowledgements: We thank our colleagues
Raghuram Krishnapuram and Sreeram Balakrish-
nan for helpful discussions. We also thank Olivier
Siohan from the IBM T. J. Watson Research Cen-
ter for providing us with call transcriptions.
References
F. Bechet, G. Riccardi and D. Hakkani-Tur 2004. Min-
ing Spoken Dialogue Corpora for System Evaluation
and Modeling. Conference on Empirical Methods
in Natural Language Processing (EMNLP). July,
Barcelona, Spain.
S. Douglas, D. Agarwal, T. Alonso, R. M. Bell, M.
Gilbert, D. F. Swayne and C. Volinsky. 2005. Min-
ing Customer Care Dialogs for ?Daily News?. IEEE
Trans. on Speech and Audio Processing, 13(5):652?
660.
P. Haffner, G. Tur and J. H. Wright 2003. Optimiz-
ing SVMs for Complex Call Classification. IEEE
International Conference on Acoustics, Speech, and
Signal Processing. April 6-10, Hong Kong.
X. Jiang and A.-H. Tan. 2005. Mining Ontolog-
ical Knowledge from Domain-Specific Text Doc-
uments. IEEE International Conference on Data
Mining, November 26-30, New Orleans, Louisiana,
USA.
K. Kummamuru, R. Lotlikar, S. Roy, K. Singal and R.
Krishnapuram. 2004. A hierarchical monothetic
document clustering algorithm for summarization
and browsing search results. International Confer-
ence on World Wide Web. New York, NY, USA.
H.-K J. Kuo and C.-H. Lee. 2003. Discriminative
Training of Natural Language Call Routers. IEEE
Trans. on Speech and Audio Processing, 11(1):24?
35.
A. D. Lawson, D. M. Harris, J. J. Grieco. 2003. Ef-
fect of Foreign Accent on Speech Recognition in
the NATO N-4 Corpus. Eurospeech. September 1-
4, Geneva, Switzerland.
G. Mishne, D. Carmel, R. Hoory, A. Roytman and A.
Soffer. 2005. Automatic Analysis of Call-center
Conversations. Conference on Information and
Knowledge Management. October 31-November 5,
Bremen, Germany.
M. Padmanabhan, G. Saon, J. Huang, B. Kingsbury
and L. Mangu.. 2002. Automatic Speech Recog-
nition Performance on a Voicemail Transcription
Task. IEEE Trans. on Speech and Audio Process-
ing, 10(7):433?442.
M. Tang, B. Pellom and K. Hacioglu. 2003. Call-
type Classification and Unsupervised Training for
the Call Center Domain. Automatic Speech Recog-
nition and Understanding Workshop. November 30-
December 4, St. Thomas, U S Virgin Islands.
J. Wright, A. Gorin and G. Riccardi. 1997. Auto-
matic Acquisition of Salient Grammar Fragments
for Call-type Classification. Eurospeech. Septem-
ber, Rhodes, Greece.
744
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 175?180,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
TransDoop: A Map-Reduce based Crowdsourced Translation for
Complex Domains
Anoop Kunchukuttan?, Rajen Chatterjee?, Shourya Roy?, Abhijit Mishra?,
Pushpak Bhattacharyya?
? Department of Computer Science and Engineering, IIT Bombay,
{anoopk,abhijitmishra,pb}@cse.iitb.ac.in, rajen.k.chatterjee@gmail.com
? Xerox India Research Centre,
Shourya.Roy@xerox.com
Abstract
Large amount of parallel corpora is re-
quired for building Statistical Machine
Translation (SMT) systems. We describe
the TransDoop system for gathering trans-
lations to create parallel corpora from on-
line crowd workforce who have familiar-
ity with multiple languages but are not
expert translators. Our system uses a
Map-Reduce-like approach to translation
crowdsourcing where sentence translation
is decomposed into the following smaller
tasks: (a) translation of constituent phrases
of the sentence; (b) validation of qual-
ity of the phrase translations; and (c)
composition of complete sentence trans-
lations from phrase translations. Trans-
Doop incorporates quality control mech-
anisms and easy-to-use worker user in-
terfaces designed to address issues with
translation crowdsourcing. We have eval-
uated the crowd?s output using the ME-
TEOR metric. For a complex domain like
judicial proceedings, the higher scores ob-
tained by the map-reduce based approach
compared to complete sentence translation
establishes the efficacy of our work.
1 Introduction
Crowdsourcing is no longer a new term in the do-
main of Computational Linguistics and Machine
Translation research (Callison-Burch and Dredze,
2010; Snow et al, 2008; Callison-Burch, 2009).
Crowdsourcing - basically where task outsourcing
is delegated to a largely unknown Internet audi-
ence - is emerging as a new paradigm of human
in the loop approaches for developing sophisti-
cated techniques for understanding and generat-
ing natural language content. Amazon Mechanical
Turk(AMT) and CrowdFlower 1 are representative
general purpose crowdsourcing platforms where
as Lingotek and Gengo2 are companies targeted
at localization and translation of content typically
leveraging freelancers.
Our interest is towards developing a crowd-
sourcing based system to enable general, non-
expert crowd-workers generate natural language
content equivalent in quality to that of expert lin-
guists. Realization of the potential of attaining
great scalability and cost-benefit of crowdsourcing
for natural language tasks is limited by the abil-
ity of novice multi-lingual workers generate high
quality translations. We have specific interest in
Indian languages due to the large linguistic diver-
sity as well as the scarcity of linguistic resources in
these languages when compared to European lan-
guages. Crowdsourcing is a promising approach
as many Indian languages are spoken by hundreds
of Millions of people (approximately, Hindi-Urdu
by 500M, Bangla by 200M, Punjabi by over 100M
3) coupled with the fact that representation of In-
dian workers in online crowdsourcing platforms is
very high (close to 40% in Amazon Mechanical
Turk (AMT)).
However, this is a non-trivial task owing to lack
of expertise of novice crowd workers in transla-
tion of content. It is well understood that famil-
iarity with multiple languages might not be good
enough for people to generate high quality transla-
tions. This is compounded by lack of sincerity and
in certain cases, dishonest intention of earning re-
wards disproportionate to the effort and time spent
for online tasks. Common techniques for quality
control like gold data based validation and worker
reputation are not effective for a subjective task
1http://www.mturk.com,http://www.
crowdflower.com
2http://www.lingotek.com,http:///www.
gengo.com
3http://en.wikipedia.org/wiki/List_of_
languages_by_total_number_of_speakers
175
like translation which does not have any task spe-
cific measurements. Having expert linguists man-
ually validate crowd generated content defies the
purpose of deploying crowdsourcing on a large
scale.
In this work, we propose a technique, based
on the Divide-and-Conquer principle. The tech-
nique can be considered similar to a Map-Reduce
task run on crowd processors, where the transla-
tion task is split into simpler tasks distributed to
the crowd (the map stage) and the results are later
combined in a reduce stage to generate complete
translations. The attempt is to make translation
tasks easy and intuitive for novice crowd-workers
by providing translations aids to help them gen-
erate high quality of translations. Our contribu-
tion in this work is a end-to-end, crowdsourcing-
platform-independent, translation crowdsourcing
system that completely automates the translation
crowdsourcing task by (i) managing the transla-
tion pipeline through software components and the
crowd; (ii) performing quality control on work-
ers? output; and (iii) interfacing with crowdsourc-
ing service providers. The multi-stage, Map-
reduce approach simplifies the translation task for
crowd workers, while novel design of user inter-
face makes the task convenient for the worker and
discourages spamming. The system thus offers the
potential to generate high quality parallel corpora
on a large scale.
We discuss related work in Section 2 and the
multi-staged approach which is central to our sys-
tem in Section 3. Section 4 describes the sys-
tem architecture and workflow, while Section 5
presents important aspects of the user interfaces
in the system. We present our preliminary exper-
iments and observations in Section 6. Section 7
concludes the paper, pointing to future directions.
2 Related Work
Lately, crowdsourcing has been explored as a
source for generating data for NLP tasks (Snow
et al, 2008; Callison-Burch and Dredze, 2010).
Specifically, it has been explored as a channel for
collecting different resources for SMT - evalua-
tions of MT output (Callison-Burch, 2009), word
alignments in parallel sentences (Gao et al, 2010)
and post-edited versions of MT output (Aikawa et
al., 2012). Ambati and Vogel (2010), Kunchukut-
tan et al (2012) have shown the feasibility of
crowdsourcing for collecting parallel corpora and
pointed out that quality assurance is a major issue
for successful translation crowdsourcing.
The most popular methods for quality control
of crowdsourced tasks are based on sampling and
redundancy. For translation crowdsourcing, Am-
bati et al (2010) use inter-translator agreement for
selection of a good translation from multiple, re-
dundant worker translations. Zaidan and Callison-
Burch (2011) score translations using a feature
based model comprising sentence level, worker
level and crowd ranking based features. However,
automatic evaluation of translation quality is diffi-
cult, such automatic methods being either inaccu-
rate or expensive. Post et al (2012) have collected
Indic language corpora data utilizing the crowd for
collecting translations as well as validations. The
quality of the validations is ensured using gold-
standard sentence translations. Our approach to
quality control is similar to Post et al (2012), but
we work at the level of phrases.
While most crowdsourcing activities for data
gathering has been concerned with collecting sim-
ple annotations like relevance judgments, there has
been work to explore the use of crowdsourcing
for more complex tasks, of which translation is
a good example. Little et al (2010) propose that
many complex tasks can be modeled either as iter-
ative workflows (where workers iteratively build
on each other?s works) or as parallel workflows
(where workers solve the tasks in parallel, with the
best result voted upon later). Kittur et al (2011)
suggest a map-and-reduce approach to solve com-
plex problems, where a problem is decomposed
into smaller problems, which are solved in the map
stage and the results are combined in the reduce
stage. Our method can be seen as an instance
of the map-reduce approach applied to translation
crowdsourcing, with two map stages (phrase trans-
lation and translation validation) and one reduce
stage (sentence combination).
3 Multi-Stage Crowdsourcing Pipeline
Our system is based on a multi-stage pipeline,
whose central idea is to simplify the translation
task into smaller tasks. The high level block di-
agram of the system is shown in Figure 1. Source
language documents are sentencified using stan-
dard NLP tokenizers and sentence splitters. Ex-
tracted sentences are then split into phrases us-
ing a standard chunker and rule-based merging
of small chunks. This step creates small phrases
176
Figure 1: Multistage crowdsourced translation
from complex sentences which can be easily and
independently translated. This leads to a crowd-
sourcing pipeline, with three stages of tasks for the
crowd: Phrase Translation (PT), Phrase Transla-
tion Validation (PV), Sentence Composition (SC).
A group of crowd workers translate source lan-
guage phrases, the translations are validated by a
different group of workers and finally a third group
of workers put the phrase translation together to
create target language sentences. The validation
is done by workers by providing ratings on a k-
point scale. This kind of divide and conquer ap-
proach helps to tackle the complexity of crowd-
sourcing translations since: (1) the tasks are sim-
pler for workers; (2) uniformity of smaller tasks
brings about efficiency as in any industrial assem-
bly line; (3) pricing can be controlled for each
stage depending on the complexity; and (4) quality
control can be performed better for smaller tasks.
4 System Architecture
Figure 2 shows the architecture of TransDoop,
which implements the 3-stage pipeline. The major
design considerations were: (i) translation crowd-
sourcing pipeline should be independent of spe-
cific crowdsourcing platforms; (ii) support multi-
ple crowdsourcing platforms; (iii) customize job
parameters like pricing, quality control method
and task design; and (iv) support multiple lan-
guages and domains.
The core component in the system is the
Crowdsourcing Engine. The engine manages the
execution of the crowdsourcing pipeline, lifecycle
of jobs and quality control of submitted tasks. The
Engine exposes its capabilities through the Re-
quester API, which can be used by clients for
setting up, customizing and monitoring transla-
tion crowdsourcing jobs and controlling their exe-
cution. These capabilities are made available to
requesters via the Requester Portal. In order
to make the crowdsourcing engine independent
of any specific crowdsourcing platform, platform
specific Connectors are developed. The Crowd-
sourcing system makes the tasks to be crowd-
sourced available through the Connector API.
The connectors are responsible for polling the en-
gine for tasks to be crowdsourced, pushing the
tasks to crowdsourcing platforms, hosting worker
interfaces for the tasks and pushing the results
back to the engine after they have been completed
by workers on the crowdsourcing platform. Cur-
rently the system supports the AMT crowdsourc-
ing platform.
Figure 3 depicts the lifecycle of a translation
crowdsourcing job. The requester initiates a trans-
lation job for a document (a set of sentences). The
Crowdsourcing Engine schedules the job for exe-
cution. It first splits each sentence into phrases.
For the job, PT tasks are created and made avail-
able through the Connector API. The connector
for the specified platform periodically polls the
Crowdsourcing Engine via the Connector API.
Once the connector has new PT tasks for crowd-
sourcing, it interacts with the crowdsourcing plat-
form to request crowdsourcing services. The con-
nector monitors the progress of the tasks and on
completion provides the results and execution sta-
tus to the Crowdsourcing Engine. Once all the PT
tasks for the job are completed, the crowdsourcing
Engine initiates the PV task to obtain validations
for the translations. The Quality Control system
kicks in when all the PV tasks for the job have
been completed.
The quality control (QC) relies on a combina-
tion of sampling and redundancy. Each PV task
has a few gold-standard phrase translation pairs,
which is used to ensure that the validators are hon-
estly doing their tasks. The judgments from the
177
Figure 2: Architecture of TransDoop
Figure 3: Lifecycle of a Translation Job
good validators are used to determine the quality
of the phrase translation, based on majority voting,
average rating, etc. using multiple judgments col-
lected for each phrase translation. If any phrase
validations or translations are incorrect, then the
corresponding phrases/translations are again sent
to the PT/PV stage as the case may be. This will
continue until all phrase translations in the job are
correctly translated or a pre-configured number of
iterations are done.
Once phrase translations are obtained for all
phrases in a sentence, the Crowdsourcing Engine
creates SC tasks, where the workers are asked
to compose a single correct, coherent translation
from the phrase translation obtained in the previ-
ous stages.
5 User Interfaces
5.1 Worker User Interfaces
This section describes the worker user interfaces
for each stage in the pipeline. These are man-
aged by the Connector and have been designed to
make the task convenient for the worker and pre-
vent spam submissions. In the rest of the section,
we describe the salient features of the PT and SC
UI?s. PV UI is similar to k-scale voting tasks com-
monly found in crowdsourcing platforms.
? Translation UI: Figure 4a shows the trans-
lation UI for the PT stage. The user in-
terface discourages spamming by: (a) dis-
playing source text as images; and (b) alert-
ing workers if they don?t provide a transla-
tion or spend very little time on a task. The
UI also provides transliteration support for
non-Latin scripts (especially helpful for Indic
scripts). A Vocabulary Support, which shows
translation suggestions for word sequences
appearing in the source phrase, is also avail-
able. Suggested translations can be copied to
the input area with ease and speed.
? Sentence Translation Composition UI: The
sentence translation composition UI (shown
in Figure 4b) facilitates composition of sen-
tence translations from phrase translations.
First, the worker can drag and rearrange the
translated phrases into the right order, fol-
lowed by reordering of individual words.
This is important because many Indian lan-
guages have different constituent order ( S-O-
V) with respect to English (S-V-O). Finally,
the synthesized language sentence can be
post-edited to correct spelling, case marking,
inflectional errors, etc. The system also cap-
tures the reordering performed by the worker,
an important byproduct, which can be used
for training reordering models for SMT.
5.2 Requester UI
The system provides a Requester Portal through
which the requester can create, control and mon-
itor jobs and retrieve results. The portal allows
the requester to customize the job during creation
by configuring various parameters: (a) domain
and language pair (b) entire sentence vs multi-
stage translation (c) price for task at each stage
(d) task design (number of tasks in a task group,
etc.) (e) translation redundancy (f) validation qual-
ity parameters. Translation redundancy refers to
the number of translations requested for a source
phrase. Validation redundancy refers to the num-
ber of validations collected for each phrase trans-
lation pair and the redundancy based acceptance
criteria for phrase translations (majority, consen-
sus, threshold, etc.)
178
(a) Phrase Translation UI (b) Sentence Composition UI
Figure 4: Worker User Interfaces
6 Experiments and Observations
Using TransDoop, we conducted a set of small-
scale, preliminary translation experiments. We ob-
tained translations for English-Hindi and English-
Marathi language pairs for the Judicial and
Tourism domains. For each experiment, 15 sen-
tences were given as input to the pipeline. For
evaluation, we chose METEOR, a well-known
translation evaluation metric (Banerjee and Lavie,
2005). We compared the results obtained from the
crowdsourcing system with a expert human trans-
lation and the output of Google Translate. We also
compared two expert translations using METEOR
to establish a skyline for the translation accuracy.
Table 1 summarizes the results of our experiments.
The translations with Quality Control and mul-
tistage pipeline are better than Google translations
and translations obtained from the crowd without
any quality control, as evaluated by METEOR.
Multi-stage translation yields better than complete
sentence translation. Moreover, the translation
quality is comparable to that of expert human
translation. This behavior is observed across the
two language pairs and domains. This can be seen
in some examples of crowdsourced translations
obtained through the system which are shown in
Table 2.
Incorrect splitting of sentences can cause diffi-
culties in translation for the worker. For instance,
discontinuous phrases will not be available to the
worker as a single translation unit. In the English
interrogative sentence, the noun phrase splits the
verb phrase, therefore the auxiliary and main verb
could be in different translation units. e.g.
Why did you buy the book?
In addition, the phrase structures of the source
and target languages may not map, making trans-
lation difficult. For instance, the vaala modifier in
Hindi translates to a clause in English. It does not
contain any tense information, therefore the tense
of the English clause cannot be determined by the
worker. e.g.
Lucknow vaalaa ladkaa
could translate to any one of:
the boy who lives/lived/is living in Lucknow
We rely on the worker in sentence composition
stage to correct mistakes due to these inadequacies
and compose a good translation. In addition, the
worker in the PT stage could be provided with the
sentence context for translation. However, there
is a tradeoff between the cognitive load of context
processing versus uncertainty in translation. More
elaborately, to what extent can the cognitive load
be reduced before uncertainty of translation sets
in? Similarly, how much of context can be shown
before the cognitive load becomes pressing?
7 Conclusions
In this system demonstration, we present Trans-
Doop as a translation crowdsourcing system which
has the potential to harness the strength of the
crowd to collect high quality human translations
on a large scale. It simplifies the tedious trans-
lation tasks by decomposing them into several
?easy-to-solve? subtasks while ensuring quality.
Our evaluation on small scale data shows that
the multistage approach performs better than com-
plete sentence translation. We would like to exten-
sively use this platform for large scale experiments
on more language pairs and complex domains like
Health, Parliamentary Proceedings, Technical and
Scientific literature etc. to establish the utility of
179
Language Pair Domain Google No QC Translation with QC Reference
Translate single stage multi stage Human
en-mr Tourism 0.227? 0.30 0.368 0.372 0.48
en-hi Tourism 0.292 0.363 0.387 0.422 0.51
en-hi Judicial 0.252 0.30 0.388 0.436 0.49
Table 1: Experimental Results: Comparison of METEOR scores for different techniques, language pairs and domains
?Translated by an internal Moses-based SMT system
Accordingly the penalty imposed by AO is not justified and the same is cancelled.
isk an  sAr e aO ?ArA lgAy gy d\X uEcta nhF\ h{ aOr ek hF r? kr EdyA h{
Accordingly A O by imposed penalty justified not is and one also cancel did
tadAn  sAr e ao ?ArA lgAyA gyA d\X jAy) nhF\ h{ aOr us r? kr EdyA h{
Accordingly A O by imposed penalty justified not is and that cancel did
(a) English-Hindi Judicial Translation
A crowd of devotees engulf Haridwar during the time of daily prayer in the evening
fAm m\ d{Enk ?ATnA k smy k dOrAn B?o\ ko apnF cpV m\ l hEr?Ar kF BFX
evening in daily prayer of time during devotees its engulf in take Haridwar of crowd
??Al  ao\ kF BFX fAm m\ d{Enk ?ATnA k smy hEr?Ar ko apnF cpV m\ ltaF h{
devotees of crowd evening in daily prayer of time haridwar its engulf in take
(b) English-Hindi Tourism Translation
Table 2: Examples of translation from Google and three
staged pipeline for source sentence (2nd, 3rd and 1st rows
of each table respectively). Domains and languages are indi-
cated above.
the method for collection of parallel corpora on a
large scale.
References
Takako Aikawa, Kentaro Yamamoto, and Hitoshi Isa-
hara. 2012. The impact of crowdsourcing post-
editing with the collaborative translation frame-
work. In Advances in Natural Language Processing.
Springer Berlin Heidelberg.
Vamshi Ambati and Stephan Vogel. 2010. Can crowds
build parallel corpora for machine translation sys-
tems? In Proceedings of the NAACL HLT 2010
Workshop on Creating Speech and Language Data
with Amazon?s Mechanical Turk.
Vamshi Ambati, Stephan Vogel, and Jaime Carbonell.
2010. Active learning and crowd-sourcing for ma-
chine translation. Language Resources and Evalua-
tion LREC.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
correlation with human judgments. In Proceed-
ings of the ACL Workshop on Intrinsic and Extrin-
sic Evaluation Measures for Machine Translation
and/or Summarization.
Chris Callison-Burch and Mark Dredze. 2010. Cre-
ating speech and language data with amazon?s me-
chanical turk. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk.
Chris Callison-Burch. 2009. Fast, cheap, and cre-
ative: evaluating translation quality using amazon?s
mechanical turk. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing.
Qin Gao, Nguyen Bach, and Stephan Vogel. 2010. A
semi-supervised word alignment algorithm with par-
tial manual alignments. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR.
Aniket Kittur, Boris Smus, Susheel Khamkar, and
Robert E Kraut. 2011. Crowdforge: Crowdsourc-
ing complex work. In Proceedings of the 24th an-
nual ACM symposium on User interface software
and technology.
Anoop Kunchukuttan, Shourya Roy, Pratik Patel,
Kushal Ladha, Somya Gupta, Mitesh Khapra, and
Pushpak Bhattacharyya. 2012. Experiences in re-
source generation for machine translation through
crowdsourcing. Language Resources and Evalua-
tion LREC.
Greg Little, Lydia B Chilton, Max Goldman, and
Robert C Miller. 2010. Exploring iterative and par-
allel human computation processes. In Proceedings
of the ACM SIGKDD workshop on human computa-
tion.
Matt Post, Chris Callison-Burch, and Miles Osborne.
2012. Constructing parallel corpora for six indian
languages via crowdsourcing. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good?: evaluating non-expert annotations for natural
language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing.
Omar Zaidan and Chris Callison-Burch. 2011. Crowd-
sourcing translation: Professional quality from non-
professionals. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk.
180
