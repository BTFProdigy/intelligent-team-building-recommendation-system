Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1533?1541,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Phrase Dependency Parsing for Opinion Mining
Yuanbin Wu, Qi Zhang, Xuanjing Huang, Lide Wu
Fudan University
School of Computer Science
{ybwu,qi zhang,xjhuang,ldwu}@fudan.edu.cn
Abstract
In this paper, we present a novel approach
for mining opinions from product reviews,
where it converts opinion mining task to
identify product features, expressions of
opinions and relations between them. By
taking advantage of the observation that a
lot of product features are phrases, a con-
cept of phrase dependency parsing is in-
troduced, which extends traditional depen-
dency parsing to phrase level. This con-
cept is then implemented for extracting re-
lations between product features and ex-
pressions of opinions. Experimental eval-
uations show that the mining task can ben-
efit from phrase dependency parsing.
1 Introduction
As millions of users contribute rich information
to the Internet everyday, an enormous number of
product reviews are freely written in blog pages,
Web forums and other consumer-generated medi-
ums (CGMs). This vast richness of content be-
comes increasingly important information source
for collecting and tracking customer opinions. Re-
trieving this information and analyzing this con-
tent are impossible tasks if they were to be manu-
ally done. However, advances in machine learning
and natural language processing present us with
a unique opportunity to automate the decoding of
consumers? opinions from online reviews.
Previous works on mining opinions can be di-
vided into two directions: sentiment classification
and sentiment related information extraction. The
former is a task of identifying positive and neg-
ative sentiments from a text which can be a pas-
sage, a sentence, a phrase and even a word (So-
masundaran et al, 2008; Pang et al, 2002; Dave
et al, 2003; Kim and Hovy, 2004; Takamura et
al., 2005). The latter focuses on extracting the el-
ements composing a sentiment text. The elements
include source of opinions who expresses an opin-
ion (Choi et al, 2005); target of opinions which
is a receptor of an opinion (Popescu and Etzioni,
2005); opinion expression which delivers an opin-
ion (Wilson et al, 2005b). Some researchers refer
this information extraction task as opinion extrac-
tion or opinion mining. Comparing with the for-
mer one, opinion mining usually produces richer
information.
In this paper, we define an opinion unit as a
triple consisting of a product feature, an expres-
sion of opinion, and an emotional attitude(positive
or negative). We use this definition as the basis for
our opinion mining task. Since a product review
may refer more than one product feature and ex-
press different opinions on each of them, the rela-
tion extraction is an important subtask of opinion
mining. Consider the following sentences:
1. I highly [recommend]
(1)
the Canon SD500
(1)
to
anybody looking for a compact camera that can take
[good]
(2)
pictures
(2)
.
2. This camera takes [amazing]
(3)
image qualities
(3)
and its size
(4)
[cannot be beat]
(4)
.
The phrases underlined are the product features,
marked with square brackets are opinion expres-
sions. Product features and opinion expressions
with identical superscript compose a relation. For
the first sentence, an opinion relation exists be-
tween ?the Canon SD500? and ?recommend?, but
not between ?picture? and ?recommend?. The ex-
ample shows that more than one relation may ap-
pear in a sentence, and the correct relations are not
simple Cartesian product of opinion expressions
and product features.
Simple inspection of the data reveals that prod-
uct features usually contain more than one word,
such as ?LCD screen?, ?image color?, ?Canon
PowerShot SD500?, and so on. An incomplete
product feature will confuse the successive anal-
ysis. For example, in passage ?Image color is dis-
1533
appointed?, the negative sentiment becomes ob-
scure if only ?image? or ?color? is picked out.
Since a product feature could not be represented
by a single word, dependency parsing might not be
the best approach here unfortunately, which pro-
vides dependency relations only between words.
Previous works on relation extraction usually use
the head word to represent the whole phrase and
extract features from the word level dependency
tree. This solution is problematic because the in-
formation provided by the phrase itself can not be
used by this kind of methods. And, experimental
results show that relation extraction task can ben-
efit from dependencies within a phrase.
To solve this issue, we introduce the concept
of phrase dependency parsing and propose an ap-
proach to construct it. Phrase dependency pars-
ing segments an input sentence into ?phrases? and
links segments with directed arcs. The parsing
focuses on the ?phrases? and the relations be-
tween them, rather than on the single words inside
each phrase. Because phrase dependency parsing
naturally divides the dependencies into local and
global, a novel tree kernel method has also been
proposed.
The remaining parts of this paper are organized
as follows: In Section 2 we discuss our phrase de-
pendency parsing and our approach. In Section 3,
experiments are given to show the improvements.
In Section 4, we present related work and Section
5 concludes the paper.
2 The Approach
Fig. 1 gives the architecture overview for our ap-
proach, which performs the opinion mining task
in three main steps: (1) constructing phrase de-
pendency tree from results of chunking and de-
pendency parsing; (2) extracting candidate prod-
uct features and candidate opinion expressions; (3)
extracting relations between product features and
opinion expressions.
2.1 Phrase Dependency Parsing
2.1.1 Overview of Dependency Grammar
Dependency grammar is a kind of syntactic the-
ories presented by Lucien Tesni`ere(1959). In de-
pendency grammar, structure is determined by the
relation between a head and its dependents. In
general, the dependent is a modifier or comple-
ment; the head plays a more important role in de-
termining the behaviors of the pair. Therefore, cri-
Phrase Dependency Parsing  
Review Crawler 
Review Database
 Chunking DependencyParsing
  
CandidateProduct FeaturesIdentification
CandidateOpinion ExpressionsExtraction
Relation ExtractionOpinionDatabase
Phrase Dependency Tree
Figure 1: The architecture of our approach.
teria of how to establish dependency relations and
how to distinguish the head and dependent in such
relations is central problem for dependency gram-
mar. Fig. 2(a) shows the dependency represen-
tation of an example sentence. The root of the
sentence is ?enjoyed?. There are seven pairs of
dependency relationships, depicted by seven arcs
from heads to dependents.
2.1.2 Phrase Dependency Parsing
Currently, the mainstream of dependency parsing
is conducted on lexical elements: relations are
built between single words. A major informa-
tion loss of this word level dependency tree com-
pared with constituent tree is that it doesn?t ex-
plicitly provide local structures and syntactic cat-
egories (i.e. NP, VP labels) of phrases (Xia and
Palmer, 2001). On the other hand, dependency
tree provides connections between distant words,
which are useful in extracting long distance rela-
tions. Therefore, compromising between the two,
we extend the dependency tree node with phrases.
That implies a noun phrase ?Cannon SD500 Pow-
erShot? can be a dependent that modifies a verb
phrase head ?really enjoy using? with relation type
?dobj?. The feasibility behind is that a phrase is a
syntactic unit regardless of the length or syntac-
tic category (Santorini and Kroch, 2007), and it is
acceptable to substitute a single word by a phrase
with same syntactic category in a sentence.
Formally, we define the dependency parsing
with phrase nodes as phrase dependency parsing.
A dependency relationship which is an asymmet-
ric binary relationship holds between two phrases.
One is called head, which is the central phrase in
the relation. The other phrase is called dependent,
which modifies the head. A label representing the
1534
enjoyed
We nsubj reallyadvmod using
partmod
SD500
thedet Canon PowerShotnn nn
dobj
enjoyed
nsubj really using
partmod
We 
VP
NP SD500
the
det
Canon PowerShot
nn nn
NP
advmod
dobj
(a)
(c)(b)
  NP SEGMENT:      [We] VP SEGMENT:      [really]      [enjoyed ]      [using] NP SEGMENT:      [the]      [Canon]      [PowerShot]      [SD500]
Figure 2: Example of Phrase Dependency Parsing.
relation type is assigned to each dependency rela-
tionship, such as subj (subject), obj (object), and
so on. Fig.2(c) shows an example of phrase de-
pendency parsing result.
By comparing the phrase dependency tree and
the word level dependency tree in Fig.2, the for-
mer delivers a more succinct tree structure. Local
words in same phrase are compacted into a sin-
gle node. These words provide local syntactic and
semantic effects which enrich the phrase they be-
long to. But they should have limited influences on
the global tree topology, especially in applications
which emphasis the whole tree structures, such as
tree kernels. Pruning away local dependency re-
lations by additional phrase structure information,
phrase dependency parsing accelerates following
processing of opinion relation extraction .
To construct phrase dependency tree, we pro-
pose a method which combines results from an
existing shallow parser and a lexical dependency
parser. A phrase dependency tree is defined as
T = (V ,E ), where V is the set of phrases,
E is the dependency relations among the phrases
in V representing by direct edges. To reserve
the word level dependencies inside a phrase, we
define a nested structure for a phrase T
i
in V :
T
i
= (V
i
, E
i
). V
i
= {v
1
, v
2
, ? ? ? , v
m
} is the inter-
nal words, E
i
is the internal dependency relations.
We conduct the phrase dependency parsing in
this way: traverses word level dependency tree
in preorder (visits root node first, then traverses
the children recursively). When visits a node R,
searches in its children and finds the node set D
which are in the same phrase with R according
Algorithm 1 Pseudo-Code for constructing the
phrase dependency tree
INPUT:
T
?
= (V
?
, E
?
) a word level dependency tree
P = phrases
OUTPUT:
phrase dependency tree T = (V , E ) where
V = {T
1
(V
1
, E
1
), T
2
(V
2
, E
2
), ? ? ? , T
n
(V
n
, E
n
)}
Initialize:
V ? {({v
?
}, {})|v
?
? V
?
}
E ? {(T
i
, T
j
)|(v
?
i
, v
?
j
) ? E
?
, v
?
i
? V
i
, v
?
j
? V
j
}
R = (V
r
, E
r
) root of T
PhraseDPTree(R, P )
1: Find p
i
? P where word[R] ? p
i
2: for each S = (V
s
, E
s
), (R,S) ? E do
3: if word[S] ? p
i
then
4: V
r
? V
r
? v
s
; v
s
? V
s
5: E
r
? E
r
? (v
r
, root[S]); v
r
? V
r
6: V ? V ? S
7: E ? E + (R, l); ?(S, l) ? E
8: E ? E ? (R,S)
9: end if
10: end for
11: for each (R,S) ? E do
12: PhraseDPTree(S,P )
13: end for
14: return (V , E )
to the shallow parsing result. Compacts D and R
into a single node. Then traverses all the remain-
ing children in the same way. The algorithm is
shown in Alg. 1.
The output of the algorithm is still a tree, for we
only cut edges which are compacted into a phrase,
the connectivity is keeped. Note that there will be
inevitable disagrees between shallow parser and
lexical dependency parser, the algorithm implies
that we simply follow the result of the latter one:
the phrases from shallow parser will not appear in
the final result if they cannot be found in the pro-
cedure.
Consider the following example:
?We really enjoyed using the Canon PowerShot SD500.?
Fig.2 shows the procedure of phrase depen-
dency parsing. Fig.2(a) is the result of the lex-
ical dependency parser. Shallow parsers result
is shown in Fig.2(b). Chunk phrases ?NP(We)?,
?VP(really enjoyed using)? and ?NP(the Canon
PowerShot SD500)? are nodes in the output phrase
dependency tree. When visiting node ?enjoyed? in
Fig.2(a), the shallow parser tells that ?really? and
?using? which are children of ?enjoy? are in the
same phrase with their parent, then the three nodes
are packed. The final phrase dependency parsing
tree is shown in the Fig. 2(c).
1535
2.2 Candidate Product Features and Opinion
Expressions Extraction
In this work, we define that product features
are products, product parts, properties of prod-
ucts, properties of parts, company names and re-
lated objects. For example,in consumer elec-
tronic domain, ?Canon PowerShot?, ?image qual-
ity?,?camera?, ?laptop? are all product features.
From analyzing the labeled corpus, we observe
that more than 98% of product features are in a
single phrase, which is either noun phrase (NP) or
verb phrase (VP). Based on it, all NPs and VPs
are selected as candidate product features. While
prepositional phrases (PPs) and adjectival phrases
(ADJPs) are excluded. Although it can cover
nearly all the true product features, the precision
is relatively low. The large amount of noise can-
didates may confuse the relation extraction clas-
sifier. To shrink the size of candidate set, we in-
troduce language model by an intuition that the
more likely a phrase to be a product feature, the
more closely it related to the product review. In
practice, for a certain domain of product reviews,
a language model is build on easily acquired unla-
beled data. Each candidate NP or VP chunk in the
output of shallow parser is scored by the model,
and cut off if its score is less than a threshold.
Opinion expressions are spans of text that ex-
press a comment or attitude of the opinion holder,
which are usually evaluative or subjective phrases.
We also analyze the labeled corpus for opinion ex-
pressions and observe that many opinion expres-
sions are used in multiple domains, which is iden-
tical with the conclusion presented by Kobayashi
et al (2007). They collected 5,550 opinion ex-
pressions from various sources . The coverage of
the dictionary is high in multiple domains. Moti-
vated by those observations, we use a dictionary
which contains 8221 opinion expressions to select
candidates (Wilson et al, 2005b). An assump-
tion we use to filter candidate opinion expressions
is that opinion expressions tend to appear closely
with product features, which is also used to extract
product features by Hu and Liu (2004). In our ex-
periments, the tree distance between product fea-
ture and opinion expression in a relation should be
less than 5 in the phrase dependency parsing tree.
2.3 Relation Extraction
This section describes our method on extracting
relations between opinion expressions and product
features using phrase dependency tree. Manually
built patterns were used in previous works which
have an obvious drawback that those patterns can
hardly cover all possible situations. By taking ad-
vantage of the kernel methods which can search a
feature space much larger than that could be repre-
sented by a feature extraction-based approach, we
define a new tree kernel over phrase dependency
trees and incorporate this kernel within an SVM to
extract relations between opinion expressions and
product features.
The potential relation set consists of the all
combinations between candidate product features
and candidate opinion expressions in a sentence.
Given a phrase dependency parsing tree, we
choose the subtree rooted at the lowest common
parent(LCP) of opinion expression and product
feature to represent the relation.
Dependency tree kernels has been proposed by
(Culotta and Sorensen, 2004). Their kernel is de-
fined on lexical dependency tree by the convolu-
tion of similarities between all possible subtrees.
However, if the convolution containing too many
irrelevant subtrees, over-fitting may occur and de-
creases the performance of the classifier. In phrase
dependency tree, local words in a same phrase are
compacted, therefore it provides a way to treat ?lo-
cal dependencies? and ?global dependencies? dif-
ferently (Fig. 3). As a consequence, these two
kinds of dependencies will not disturb each other
in measuring similarity. Later experiments prove
the validity of this statement.
B
A C
D
E
B
A
C
D E
Phrase Local dependencies
Global dependencies
Figure 3: Example of ?local dependencies? and
?global dependencies?.
We generalize the definition by (Culotta and
Sorensen, 2004) to fit the phrase dependency tree.
Use the symbols in Section 2.1.2, T
i
and T
j
are
two trees with root R
i
and R
j
, K(T
i
,T
j
) is the
kernel function for them. Firstly, each tree node
T
k
? T
i
is augmented with a set of features F ,
and an instance of F for T
k
is F
k
= {f
k
}. A
match function m(T
i
, T
j
) is defined on comparing
a subset of nodes? features M ? F . And in the
same way, a similarity function s(T
i
, T
j
) are de-
1536
fined on S ? F
m(T
i
, T
j
) =
{
1 if f
i
m
= f
j
m
?f
m
? M
0 otherwise
(1)
and
s(T
i
, T
j
) =
?
f
s
?S
C(f
i
s
, f
j
s
) (2)
where
C(f
i
s
, f
j
s
) =
{
1 if f
i
s
= f
j
s
0 otherwise
(3)
For the given phrase dependency parsing trees,
the kernel function K(T
i
,T
j
) is defined as fol-
low:
K(T
i
,T
j
) =
?
?
?
?
?
0 if m(R
i
, R
j
) = 0
s(R
i
, R
j
) +K
in
(R
i
, R
j
)
+K
c
(R
i
.C, R
j
.C) otherwise
(4)
where K
in
(R
i
, R
j
) is a kernel function over
R
i
= (V
i
r
, E
i
r
) and R
j
= (V
j
r
, E
j
r
)?s internal
phrase structures,
K
in
(R
i
, R
j
) = K(R
i
, R
j
) (5)
K
c
is the kernel function over R
i
and R
j
?s chil-
dren. Denote a is a continuous subsequence of in-
dices a, a+1, ? ? ? a+ l(a) for R
i
?s children where
l(a) is its length, a
s
is the s-th element in a. And
likewise b for R
j
.
K
c
(R
i
.C, R
j
.C) =
?
a,b,l(a)=l(b)
?
l(a)
K(R
i
.[a], R
j
.[b])
?
?
s=1..l(a)
m(R
i
.[a
s
], R
j
.[b
s
])
(6)
where the constant 0 < ? < 1 normalizes the ef-
fects of children subsequences? length.
Compared with the definitions in (Culotta and
Sorensen, 2004), we add term K
in
to handle the
internal nodes of a pharse, and make this exten-
sion still satisfy the kernel function requirements
(composition of kernels is still a kernel (Joachims
et al, 2001)). The consideration is that the local
words should have limited effects on whole tree
structures. So the kernel is defined on external
children (K
c
) and internal nodes (K
in
) separately,
Table 1: Statistics for the annotated corpus
Category # Products # Sentences
Cell Phone 2 1100
Diaper 1 375
Digital Camera 4 1470
DVD Player 1 740
MP3 Player 3 3258
as the result, the local words are not involved in
subsequences of external children for K
c
. After
the kernel computing through training instances,
support vector machine (SVM) is used for classi-
fication.
3 Experiments and Results
In this section, we describe the annotated corpus
and experiment configurations including baseline
methods and our results on in-domain and cross-
domain.
3.1 Corpus
We conducted experiments with labeled corpus
which are selected from Hu and Liu (2004), Jin-
dal and Liu (2008) have built. Their documents
are collected from Amazon.com and CNet.com,
where products have a large number of reviews.
They also manually labeled product features and
polarity orientations. Our corpus is selected
from them, which contains customer reviews of
11 products belong to 5 categories(Diaper, Cell
Phone, Digital Camera, DVD Player, and MP3
Player). Table 1 gives the detail statistics.
Since we need to evaluate not only the prod-
uct features but also the opinion expressions and
relations between them, we asked two annotators
to annotate them independently. The annotators
started from identifying product features. Then for
each product feature, they annotated the opinion
expression which has relation with it. Finally, one
annotator A
1
extracted 3595 relations, while the
other annotator A
2
extracted 3745 relations, and
3217 cases of them matched. In order to measure
the annotation quality, we use the following metric
to measure the inter-annotator agreement, which is
also used by Wiebe et al (2005).
agr(a||b) =
|A matches B|
|A|
1537
Table 2: Results for extracting product features
and opinion expressions
P R F
Product Feature 42.8% 85.5% 57.0%
Opinion Expression 52.5% 75.2% 61.8%
Table 3: Features used in SVM-1: o denotes an
opinion expression and t a product feature
1) Positions of o/t in sentence(start, end, other);
2) The distance between o and t (1, 2, 3, 4, other);
3) Whether o and t have direct dependency relation;
4) Whether o precedes t;
5) POS-Tags of o/t.
where agr(a||b) represents the inter-annotator
agreement between annotator a and b, A and B
are the sets of anchors annotated by annotators a
and b. agr(A
1
||A
2
) was 85.9% and agr(A
2
||A
1
)
was 89.5%. It indicates that the reliability of our
annotated corpus is satisfactory.
3.2 Preprocessing Results
Results of extracting product features and opin-
ion expressions are shown in Table 2. We use
precision, recall and F-measure to evaluate perfor-
mances. The candidate product features are ex-
tracted by the method described in Section 2.2,
whose result is in the first row. 6760 of 24414
candidate product features remained after the fil-
tering, which means we cut 72% of irrelevant can-
didates with a cost of 14.5%(1-85.5%) loss in true
answers. Similar to the product feature extraction,
the precision of extracting opinion expression is
relatively low, while the recall is 75.2%. Since
both product features and opinion expressions ex-
tractions are preprocessing steps, recall is more
important.
3.3 Relation Extraction Experiments
3.3.1 Experiments Settings
In order to compare with state-of-the-art results,
we also evaluated the following methods.
1. Adjacent method extracts relations between a
product feature and its nearest opinion expression,
which is also used in (Hu and Liu, 2004).
2. SVM-1. To compare with tree kernel based
Table 4: Features used in SVM-PTree
Features for match function
1) The syntactic category of the tree node
(e.g. NP, VP, PP, ADJP).
2) Whether it is an opinion expression node
3) Whether it is a product future node.
Features for similarity function
1) The syntactic category of the tree node
(e.g. NP, VP, PP, ADJP).
2) POS-Tag of the head word of node?s internal
phrases.
3) The type of phrase dependency edge linking
to node?s parent.
4) Feature 2) for the node?s parent
5) Feature 3) for the node?s parent
approaches, we evaluated an SVM
1
result with a
set of manually selected features(Table 3), which
are also used in (Kobayashi et al, 2007).
3. SVM-2 is designed to compare the effective-
ness of cross-domain performances. The features
used are simple bag of words and POS-Tags be-
tween opinion expressions and product features.
4. SVM-WTree uses head words of opinion ex-
pressions and product features in the word-level
dependency tree, as the previous works in infor-
mation extraction. Then conducts tree kernel pro-
posed by Culotta and Sorensen (2004).
5. SVM-PTree denotes the results of our tree-
kernel based SVM, which is described in the Sec-
tion 2.3. Stanford parser (Klein and Manning,
2002) and Sundance (Riloff and Phillips, 2004)
are used as lexical dependency parser and shallow
parser. The features in match function and simi-
larity function are shown in Table 4.
6. OERight is the result of SVM-PTree with
correct opinion expressions.
7. PFRight is the result of SVM-PTree with
correct product features.
Table 5 shows the performances of different
relation extraction methods with in-domain data.
For each domain, we conducted 5-fold cross val-
idation. Table 6 shows the performances of the
extraction methods on cross-domain data. We use
the digital camera and cell phone domain as train-
ing set. The other domains are used as testing set.
1
libsvm 2.88 is used in our experiments
1538
Table 5: Results of different methods
Cell Phone MP3 Player Digital Camera DVD Player Diaper
Methods P R F P R F P R F P R F P R F
Adjacent 40.3% 60.5% 48.4% 26.5% 59.3% 36.7% 32.7% 59.1% 42.1% 31.8% 68.4% 43.4% 23.4% 78.8% 36.1%
SVM-1 69.5% 42.3% 52.6% 60.7% 30.6% 40.7% 61.4% 32.4% 42.4% 56.0% 27.6% 37.0% 29.3% 14.1% 19.0%
SVM-2 60.7% 19.7% 29.7% 63.6% 23.8% 34.6% 66.9% 23.3% 34.6% 66.7% 13.2% 22.0% 79.2% 22.4% 34.9%
SVM-WTree 52.6% 52.7% 52.6% 46.4% 43.8% 45.1% 49.1% 46.0% 47.5% 35.9% 32.0% 33.8% 36.6% 31.7% 34.0%
SVM-PTree 55.6% 57.2% 56.4% 51.7% 50.7% 51.2% 54.0% 49.9% 51.9% 37.1% 35.4% 36.2% 37.3% 30.5% 33.6%
OERight 66.7% 69.5% 68.1% 65.6% 65.9% 65.7% 64.3% 61.0% 62.6% 59.9% 63.9% 61.8% 55.8% 58.5% 57.1%
PFRight 62.8% 62.1% 62.4% 61.3% 56.8% 59.0% 59.7% 56.2% 57.9% 46.9% 46.6% 46.7% 58.5% 51.3% 53.4%
Table 6: Results for total performance with cross domain training data
Diaper DVD Player MP3 Player
Methods P R F P R F P R F
Adjacent 23.4% 78.8% 36.1% 31.8% 68.4% 43.4% 26.5% 59.3% 36.7%
SVM-1 22.4% 30.6% 25.9% 52.8% 30.9% 39.0% 55.9% 36.8% 44.4%
SVM-2 71.9% 15.1% 25.0% 51.2% 13.2% 21.0% 63.1% 22.0% 32.6%
SVM-WTree 38.7% 52.4% 44.5% 30.7% 59.2% 40.4% 38.1% 47.2% 42.2%
SVM-PTree 37.3% 53.7% 44.0% 59.2% 48.3% 46.3% 43.0% 48.9% 45.8%
3.3.2 Results Discussion
Table 5 presents different methods? results in five
domains. We observe that the three learning based
methods(SVM-1, SVM-WTree, SVM-PTree) per-
form better than the Adjacent baseline in the first
three domains. However, in other domains, di-
rectly adjacent method is better than the learning
based methods. The main difference between the
first three domains and the last two domains is the
size of data(Table 1). It implies that the simple Ad-
jacent method is also competent when the training
set is small.
A further inspection into the result of first 3
domains, we can also conclude that: 1) Tree
kernels(SVM-WTree and SVM-PTree) are better
than Adjacent, SVM-1 and SVM-2 in all domains.
It proofs that the dependency tree is important
in the opinion relation extraction. The reason
for that is a connection between an opinion and
its target can be discovered with various syntac-
tic structures. 2) The kernel defined on phrase
dependency tree (SVM-PTree) outperforms ker-
nel defined on word level dependency tree(SVM-
WTree) by 4.8% in average. We believe the main
reason is that phrase dependency tree provides a
more succinct tree structure, and the separative
treatment of local dependencies and global depen-
dencies in kernel computation can indeed improve
the performance of relation extraction.
To analysis the results of preprocessing steps?
influences on the following relation extraction,
we provide 2 additional experiments which the
product features and opinion expressions are all
correctly extracted respectively: OERight and
PFRight. These two results show that given an
exactly extraction of opinion expression and prod-
uct feature, the results of opinion relation extrac-
tion will be much better. Further, opinion expres-
sions are more influential which naturally means
the opinion expressions are crucial in opinion re-
lation extraction.
For evaluations on cross domain, the Adjacent
method doesn?t need training data, its results are
the same as the in-domain experiments. Note
in Table 3 and Table 4, we don?t use domain
related features in SVM-1, SVM-WTree, SVM-
PTree, but SVM-2?s features are domain depen-
dent. Since the cross-domain training set is larger
than the original one in Diaper and DVD domain,
the models are trained more sufficiently. The fi-
nal results on cross-domain are even better than
in-domain experiments on SVM-1, SVM-WTree,
and SVM-PTree with percentage of 4.6%, 8.6%,
10.3% in average. And the cross-domain train-
ing set is smaller than in-domain in MP3, but
it also achieve competitive performance with the
1539
in-domain. On the other hand, SVM-2?s result
decreased compared with the in-domain experi-
ments because the test domain changed. At the
same time, SVM-PTree outperforms other meth-
ods which is similar in in-domain experiments.
4 Related Work
Opinion mining has recently received consider-
able attention. Amount of works have been
done on sentimental classification in different lev-
els (Zhang et al, 2009; Somasundaran et al, 2008;
Pang et al, 2002; Dave et al, 2003; Kim and
Hovy, 2004; Takamura et al, 2005). While we
focus on extracting product features, opinion ex-
pressions and mining relations in this paper.
Kobayashi et al (2007) presented their work on
extracting opinion units including: opinion holder,
subject, aspect and evaluation. Subject and aspect
belong to product features, while evaluation is the
opinion expression in our work. They converted
the task to two kinds of relation extraction tasks
and proposed a machine learning-based method
which combines contextual clues and statistical
clues. Their experimental results showed that the
model using contextual clues improved the perfor-
mance. However since the contextual information
in a domain is specific, the model got by their ap-
proach can not easily converted to other domains.
Choi et al (2006) used an integer linear pro-
gramming approach to jointly extract entities and
relations in the context of opinion oriented infor-
mation extraction. They identified expressions of
opinions, sources of opinions and the linking re-
lation that exists between them. The sources of
opinions denote to the person or entity that holds
the opinion.
Another area related to our work is opinion
expressions identification (Wilson et al, 2005a;
Breck et al, 2007). They worked on identify-
ing the words and phrases that express opinions
in text. According to Wiebe et al (2005), there are
two types of opinion expressions, direct subjective
expressions and expressive subjective elements.
5 Conclusions
In this paper, we described our work on min-
ing opinions from unstructured documents. We
focused on extracting relations between product
features and opinion expressions. The novelties
of our work included: 1) we defined the phrase
dependency parsing and proposed an approach
to construct the phrase dependency trees; 2) we
proposed a new tree kernel function to model
the phrase dependency trees. Experimental re-
sults show that our approach improved the perfor-
mances of the mining task.
6 Acknowledgement
This work was (partially) funded by Chinese
NSF 60673038, Doctoral Fund of Ministry of
Education of China 200802460066, and Shang-
hai Science and Technology Development Funds
08511500302. The authors would like to thank the
reviewers for their useful comments.
References
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Iden-
tifying expressions of opinion in context. In Pro-
ceedings of IJCAI-2007.
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying sources of opinions
with conditional random fields and extraction pat-
terns. In Proceedings of HLT/EMNLP.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recog-
nition. In Proceedings EMNLP.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In In Proceed-
ings of ACL 2004.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: opinion extraction
and semantic classification of product reviews. In
Proceedings of WWW 2003.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the ACM
SIGKDD 2004.
Nitin Jindal and Bing Liu. 2008. Opinion spam and
analysis. In Proceedings of WSDM ?08.
Thorsten Joachims, Nello Cristianini, and John Shawe-
Taylor. 2001. Composite kernels for hypertext cate-
gorisation. In Proceedings of ICML ?01.
Soo-Min Kim and Eduard Hovy. 2004. Determining
the sentiment of opinions. In Proceedings of Coling
2004. COLING.
Dan Klein and Christopher D. Manning. 2002. Fast
exact inference with a factored model for natural
language parsing. In In Advances in Neural Infor-
mation Processing Systems.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of
relations in opinion mining. In Proceedings of
EMNLP-CoNLL 2007.
1540
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proc. of EMNLP
2002.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of HLT/EMNLP.
E. Riloff and W. Phillips. 2004. An introduction to
the sundance and autoslog systems. In University of
Utah School of Computing Technical Report UUCS-
04-015.
Beatrice Santorini and Anthony Kroch. 2007.
The syntax of natural language: An on-
line introduction using the Trees program.
http://www.ling.upenn.edu/ beatrice/syntax-
textbook.
Swapna Somasundaran, Janyce Wiebe, and Josef Rup-
penhofer. 2008. Discourse level opinion interpreta-
tion. In Proceedings of COLING 2008.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words us-
ing spin model. In Proceedings of ACL?05.
L. Tesni`ere. 1959. El?ements de syntaxe structurale.
Editions Klincksieck.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2/3).
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patward-
han. 2005a. Opinionfinder: A system for subjectiv-
ity analysis. In Demonstration Description in Con-
ference on Empirical Methods in Natural Language
Processing.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of HLT-
EMNLP.
Fei Xia and Martha Palmer. 2001. Converting depen-
dency structures to phrase structures. In HLT ?01:
Proceedings of the first international conference on
Human language technology research.
Qi Zhang, Yuanbin Wu, Tao Li, Mitsunori Ogihara,
Joseph Johnson, and Xuanjing Huang. 2009. Min-
ing product reviews based on shallow dependency
parsing. In Proceedings of SIGIR 2009.
1541
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1332?1341,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Structural Opinion Mining for Graph-based Sentiment Representation
Yuanbin Wu, Qi Zhang, Xuanjing Huang, Lide Wu
Fudan University
School of Computer Science
{ybwu,qz,xjhuang,ldwu}@fudan.edu.cn
Abstract
Based on analysis of on-line review corpus
we observe that most sentences have compli-
cated opinion structures and they cannot be
well represented by existing methods, such as
frame-based and feature-based ones. In this
work, we propose a novel graph-based rep-
resentation for sentence level sentiment. An
integer linear programming-based structural
learning method is then introduced to produce
the graph representations of input sentences.
Experimental evaluations on a manually la-
beled Chinese corpus demonstrate the effec-
tiveness of the proposed approach.
1 Introduction
Sentiment analysis has received much attention in
recent years. A number of automatic methods have
been proposed to identify and extract opinions, emo-
tions, and sentiments from text. Previous researches
on sentiment analysis tackled the problem on vari-
ous levels of granularity including document, sen-
tence, phrase and word (Pang et al, 2002; Riloff et
al., 2003; Dave et al, 2003; Takamura et al, 2005;
Kim and Hovy, 2006; Somasundaran et al, 2008;
Dasgupta and Ng, 2009; Hassan and Radev, 2010).
They mainly focused on two directions: sentiment
classification which detects the overall polarity of a
text; sentiment related information extraction which
tries to answer the questions like ?who expresses
what opinion on which target?.
Most of the current studies on the second direc-
tion assume that an opinion can be structured as a
frame which is composed of a fixed number of slots.
Typical slots include opinion holder, opinion expres-
sion, and evaluation target. Under this representa-
tion, they defined the task as a slots filling prob-
lem for each of the opinions. Named entity recog-
nition and relation extraction techniques are usually
applied in this task (Hu and Liu, 2004; Kobayashi
et al, 2007; Wu et al, 2009).
However, through data analysis, we observe that
60.5% of sentences in our corpus do not follow the
assumption used by them. A lot of important infor-
mation about an opinion may be lost using those rep-
resentation methods. Consider the following exam-
ples, which are extracted from real online reviews:
Example 1: The interior is a bit noisy on the free-
way1.
Example 2: Takes good pictures during the day-
time. Very poor picture quality at night2.
Based on the definition of opinion unit proposed
by Hu and Liu (2004), from the first example, the
information we can get is the author?s negative opin-
ion about ?interior? using an opinion expression
?noisy?. However, the important restriction ?on the
freeway?, which narrows the scope of the opinion,
is ignored. In fact, the tuple (?noisy?,?on the free-
way?) cannot correctly express the original opinion:
it is negative but under certain condition. The sec-
ond example is similar. If the conditions ?during the
daytime? and ?at night? are dropped, the extracted
elements cannot correctly represent user?s opinions.
Example 3: The camera is actually quite good for
outdoors because of the software.
Besides that, an opinion expression may induce
other opinions which are not expressed directly. In
example 3, the opinion expression is ?good? whose
1http://reviews.carreview.com/blog/2010-ford-focus-
review-the-compact-car-that-can/
2http://www.dooyoo.co.uk/digital-camera/sony-cyber-shot-
dsc-s500/1151680/
1332
target is ?camera?. But the ?software? which trig-
gers the opinion expression ?good? is also endowed
with a positive opinion. In practice, this induced
opinion on ?software? is actually more informative
than its direct counterpart. Mining those opinions
may help to form a complete sentiment analysis re-
sult.
Example 4: The image quality is in the middle of
its class, but it can still be a reasonable choice for
students.
Furthermore, the relations among individual opin-
ions also provide additional information which is
lost when they are considered separately. Example
4 is such a case that the whole positive comment of
camera is expressed by a transition from a negative
opinion to a positive one.
In order to address those issues, this paper de-
scribes a novel sentiment representation and analysis
method. Our main contributions are as follows:
1. We investigate the use of graphs for repre-
senting sentence level sentiment. The ver-
tices are evaluation target, opinion expression,
modifiers of opinion. The Edges represent
relations among them. The semantic rela-
tions among individual opinions are also in-
cluded. Through the graph, various informa-
tion on opinion expressions which is ignored
by current representation methods can be well
handled. And the proposed representation is
language-independent.
2. We propose a supervised structural learning
method which takes a sentence as input and the
proposed sentiment representation for it as out-
put. The inference algorithm is based on in-
teger linear programming which helps to con-
cisely and uniformly handle various properties
of our sentiment representation. By setting ap-
propriate prior substructure constraints of the
graph, the whole algorithm achieves reasonable
performances.
The remaining part of this paper is organized as
follows: In Section 2 we discuss the proposed rep-
resentation method. Section 3 describes the com-
putational model used to construct it. Experimental
results in test collections and analysis are shown in
Section 4. In Section 5, we present the related work
and Section 6 concludes the paper.
2 Graph-based Sentiment Representation
In this work, we propose using directed graph to
represent sentiments. In the graph, vertices are
text spans in the sentences which are opinion ex-
pressions, evaluation targets, conditional clauses etc.
Two types of edges are included in the graph: (1)
relations among opinion expressions and their mod-
ifiers; (2) relations among opinion expressions. The
edges of the first type exist within individual opin-
ions. The second type of the edges captures the re-
lations among individual opinions. The following
sections detail the definition.
2.1 Individual Opinion Representation
Let r be an opinion expression in a sentence, the rep-
resentation unit for r is a set of relations {(r, dk)}.
For each relation (r, dk), dk is a modifier which is a
span of text specifying the change of r?s meaning.
The relations between modifier and opinion ex-
pression can be the type of any kind. In this work,
we mainly consider two basic types:
? opinion restriction. (r, dk) is called an opin-
ion restriction if dk narrows r?s scope, adds a
condition, or places limitations on r?s original
meaning.
? opinion expansion. (r, dk) is an opinion expan-
sion if r?s scope expands to dk, r induces an-
other opinion on dk, or the opinion on dk is im-
plicitly expressed by r.
Mining the opinion restrictions can help to get ac-
curate meaning of an opinion, and the opinion ex-
pansions are useful to cover more indirect opinions.
As with previous sentiment representations, we ac-
tually consider the third type of modifier which dk is
the evaluation target of r.
Figure 1 shows a concrete example. In this ex-
ample, there are three opinion expressions: ?good?,
?sharp?, ?slightly soft?. The modifiers of ?good?
are ?indoors? and ?Focus accuracy?, where relation
(?good?,?indoors?) is an opinion restriction because
?indoors? is the condition under which ?Focus ac-
curacy? is good. On the other hand, the relation
1333
(?sharp?, ?little 3x optical zooms?) is an opinion ex-
pansion because the ?sharp? opinion on ?shot? im-
plies a positive opinion on ?little 3x optical zooms?.
It is worth to remark that: 1) a modifier dk can re-
late to more than one opinion expression. For exam-
ple, multiple opinion expressions may share a same
condition; 2) dk itself can employ a set of relations,
although the case appears occasionally. The follow-
ing is an example:
Example 5: The camera wisely get rid of many
redundant buttons.
In the example, ?redundant buttons? is the eval-
uation target of opinion expression ?wisely get rid
of?, but itself is a relation between ?redundant?
and ?buttons?. Such nested semantic structure is
described by a path: ?wisely get rid of? target?????
[?redundant? target??????buttons?]nested target.
2.2 Relations between Individual Opinion
Representation
Assume ?ri? are opinion expressions ordered by
their positions in sentence, and each of them has
been represented by relations {(ri, dik)} individu-
ally (the nested relations for dik have also been de-
termined). Then we define two relations on adja-
cent pair ri, ri+1: coordination when the polarities
of ri and ri+1 are consistent, and transition when
they are opposite. Those relations among ri form a
set B called opinion thread. In Figure 1, the opin-
ion thread is: {(?good?, ?sharp?), (?sharp?, ?slightly
soft?)}.
The whole sentiment representation for a sentence
can be organized by a direct graphG = (V,E). Ver-
tex set V includes all opinion expressions and mod-
ifiers. Edge set E collects both relations of each
individual opinion and relations in opinion thread.
The edges are labeled with relation types in label set
L={?restriction?, ?expansion?, ?target?, ?coordina-
tion?, ?transition?} 3.
Compared with previous works, the advantages of
using G as sentiment representation are: 1) for in-
dividual opinions, the modifiers will collect more
information than using opinion expression alone.
3We don?t define any ?label? on vertices: if two span of text
satisfy a relation in L, they are chosen to be vertices and an
edge with proper label will appear inE. In other words, vertices
are identified by checking whether there exist relations among
them.
Focus accuracy was good indoors, and although the 
little 3x optical zooms produced sharp shots, the 
edges were slightly soft on the Canon. 
Focus 
accuracy 
edges 
slightly soft shots 
sharp 
little 3x optical 
zooms 
indoors 
good 
Expansion 
Target 
Coordinate 
Transition 
Target 
Target 
Restriction 
r
1 
r
2 
r
3 
d
11 
d
12 
d
21 
d
22 
d
31 
Figure 1: Sentiment representation for an example sen-
tence
Thus G is a relatively complete and accurate rep-
resentation; 2) the opinion thread can help to catch
global sentiment information, for example the gen-
eral polarity of a sentence, which is dropped when
the opinions are separately represented.
3 System Description
To produce the representation graph G for a sen-
tence, we need to extract candidate vertices and
build the relations among them to get a graph struc-
ture. For the first task, the experimental results in
Section 4 demonstrate that the standard sequential
labeling method with simple features can achieve
reasonable performance. In this section, we focus
on the second task, and assume the vertices in the
graph have already been correctly collected in the
following formulation of algorithm.
3.1 Preliminaries
In order to construct graph G, we use a structural
learning method. The framework is from the first or-
der discriminative dependency parsing model (Mc-
donald and Pereira, 2005). A sentence is denoted by
s; x are text spans which will be vertices of graph;
xi is the ith vertex in x ordered by their positions in
s. For a set of vertices x, y is the graph of its sen-
timent representation, and e = (xi, xj) ? y is the
direct edge from xi to xj in y. In addition, x0 is a
1334
virtual root node without inedge. G = {(xn,yn)}Nn
is training set.
Following the edge based factorization, the score
of a graph is the sum of its edges? scores,
score(x,y) =
?
(xi,xj)?y
score(xi, xj)
=
?
(xi,xj)?y
?T f(xi, xj), (1)
f(xi, xj) is a high dimensional feature vector of the
edge (xi, xj). The components of f are either 0 or 1.
For example the k-th component could be
fk(xi, xj) =
?
?
?
1 if xi.POS = JJ and xj .POS = NN
and label of (xi, xj)is restriction
0 otherwise
.
Then the score of an edge is the linear combination
of f ?s components, and the coefficients are in vector
?.
Algorithm 1 shows the parameter learning pro-
cess. It aims to get parameter ? which will assign
the correct graph y with the highest score among all
possible graphs of x (denoted by Y).
Algorithm 1 Online structural learning
Training Set:G = {(xn, yn)}Nn
1: ?0 = 0, r = 0, T =maximum iteration
2: for t = 0 to T do
3: for n = 0 to N do
4: y? = argmaxy?Y score(xn, y) B Inference
5: if y? 6= yn then
6: update ?t to ?t+1 B PA
7: r = r + ?t+1
8: end if
9: end for
10: end for
11: return ? = r/(N ? T )
3.2 Inference
Like other structural learning tasks, the ?argmax?
operation in the algorithm (also called inference)
y? = argmax
y?Y
score(x,y)
= argmax
y?Y
?
(xi,xj)?y
?T f(xi, xj) (2)
is hard because all possible values of y form a huge
search space. In our case, Y is all possible directed
acyclic graphs of the given vertex set, which num-
ber is exponential. Directly solving the problem of
finding maximum weighted acyclic graph is equiva-
lent to finding maximum feedback arc set, which is a
NP-hard problem (Karp, 1972). We will use integer
linear programming (ILP) as the framework for this
inference problem.
3.2.1 Graph Properties
We first show some properties of graph G either
from the definition of relations or corpus statistics.
Property 1. The graph is connected and without
directed cycle. From individual opinion represen-
tation, each subgraph of G which takes an opinion
expression as root is connected and acyclic. Thus
the connectedness is guaranteed for opinion expres-
sions are connected in opinion thread; the acyclic is
guaranteed by the fact that if a modifier is shared by
different opinion expressions, the inedges from them
always keep (directed) acyclic.
Property 2. Each vertex can have one outedge
labeled with coordination or transition at most. The
opinion thread B is a directed path in graph.
Property 3. The graph is sparse. The average
in-degree of a vertex is 1.03 in our corpus, thus the
graph is almost a rooted tree. In other words, the
cases that a modifier connects to more than one opin-
ion expression rarely occur comparing with those
vertices which have a single parent. An explaination
for this sparseness is that opinions in online reviews
always concentrate in local context and have local
semantic connections.
3.2.2 ILP Formulation
Based on the property 3, we divide the inference
algorithm into two steps: i) constructing G?s span-
ning tree (arborescence) with property 1 and 2; ii)
finding additional non-tree edges as a post process-
ing task. The first step is close to the works on ILP
formulations of dependency parsing (Riedel and
Clarke, 2006; Martins et al, 2009). In the second
step, we use a heuristic method which greedily adds
non-tree edges. A similar approximation method
is also used in (Mcdonald and Pereira, 2006) for
acyclic dependency graphs.
Step 1. Find MST. Following the multicommodity
1335
flow formulation of maximum spanning tree (MST)
problem in (Magnanti and Wolsey, 1994), the ILP
for MST is:
max.
?
i,j
yij ? score(xi, xj) (3)
s.t.
?
i,j
yij = |V | ? 1 (4)
?
i
fuij ?
?
k
fujk = ?uj ,1 ? u, j ? |V | (5)
?
k
fu0k = 1, 1 ? u ? |V | (6)
fuij ? yij , 1 ? u, j ? |V |,
0 ? i ? |V | (7)
fuij ? 0, 1 ? u, j ? |V |,
0 ? i ? |V | (8)
yij ? { 0, 1}, 0 ? i, j ? |V |. (9)
In this formulation, yij is an edge indicator vari-
able that (xi, xj) is a spanning tree edge when yij =
1, (xi, xj) is a non-tree edge when yij = 0. Then
output y is represented by the set {yij , 0 ? i, j ?
|V |} 4. Eq(4) ensures that there will be exactly
|V | ? 1 edges are chosen. Thus if the edges cor-
responding to those non zero yij is a connected sub-
graph, y is a well-formed spanning tree. Objective
function just says the optimal solution of yij have
the maximum weight.
The connectedness is guaranteed if for every ver-
tex, there is exactly one path from root to it. It is for-
mulated by using |V | ? 1 flows {fu, 1 ? u ? |V |}.
fu starts from virtual root x0 towards vertex xu.
Each flow fu = {fuij , 0 ? i, j ? |V |}. fuij indi-
cates whether flow fu is through edge (xi, xj). so
it should be 0 if edge (xi, xj) does not exist (by
(7)). The Kronecker?s delta ?uj in (5) guarantees fu
is only assumed by vertex xu, so fu is a well-formed
path from root to xu. (6) ensures there is only one
flow (path) from root to xu. Thus the subgraph is
connected. The following are our constraints:
c1: Constraint on edges in opinion thread (10)-
(11).
From the definition of opinion thread, we impose
a constraint on every vertex?s outedges in opinion
thread, which are labeled with ?coordination? or
4For simplicity, we overload symbol y from the graph of the
sentiment represetation to the MST of it.
?transition?. Let Iob be a characteristic function on
edges: Iob((j, k)) = 1 when edge (xj , xk) is labeled
with ?coordination? or ?transition?, otherwise 0. We
denote q variables for vertices:
qj =
?
k
yjk ? Iob((j, k)), 0 ? j ? |V |. (10)
Then following linear inequalities bound the number
of outedges in opinion thread (? 1) on each vertex:
qj ? 1, 0 ? j ? |V |. (11)
c2: Constraint on target edge (12).
We also bound the number of evaluation targets
for a vertex in a similar way. Let It be characteris-
tic function on edges identifing whether it is labeled
with ?target?,
?
k
yjk ? It((j, k)) ? Ct, 0 ? j ? |V |. (12)
The parameter Ct can be adjusted according to the
style of document. In online reviews, authors tend
to use simple and short comments on individual tar-
gets, so Ct could be set small.
c3: Constraint on opinion thread (13)-(18).
From graph property 2, the opinion thread should
be a directed path. It implies the number of con-
nected components whose edges are ?coordination?
or ?transition? should be less than 1. Two set of ad-
ditional variables are needed: {cj , 0 ? j ? |V |} and
{hj , 0 ? j ? |V |}, where
cj =
{
1 if an opinion thread starts at xj
0 otherwise ,
and
hj =
?
i
yij ? Iob((i, j)). (13)
Then cj = ?hj ? qj , which can be linearized by
cj? qj ? hj , (14)
cj? 1 ? hj , (15)
cj? qj , (16)
cj? 0. (17)
If the sum of cj is no more than 1, the opinion thread
of graph is a directed path.
?
j
cj ? 1. (18)
1336
1 
2 
3 
4 
5 
6 
7 
(a) 
(b) 
1 
2 
3 
4 
5 
6 7 
(c) 
1 
2 3 
4 
5 
6 
7 
Figure 2: The effects of c1 and c3. Assume solid lines
are edges labeled with ?coordination? and ?transition?,
dot lines are edges labeled with other types. (a) is an
arbitrary tree. (b) is a tree with c1 constraints. (c) is a
tree with c1 and c3. It shows c1 are not sufficient for
graph property 2: the edges in opinion thread may not be
connected.
Figure 2 illustrates the effects of c1 and c3.
Equations (10)-(18), together with basic multi-
commodity flow model build up the inference algo-
rithm. The entire ILP formulation involves O(|V |3)
variables and O(|V |2) constraints. Generally, ILP
falls into NPC, but as an important result, in the mul-
ticommodity flow formulation of maximum span-
ning tree problem, the integer constraints (9) on yij
can be dropped. So the problem reduces to a linear
programming which is polynomial solvable (Mag-
nanti and Wolsey, 1994). Unfortunately, with our
additional constraints the LP relaxation is not valid.
Step 2. Adding non-tree edges. We examine the
case that a modifier attaches to different opinion ex-
pressions. That often occurs as the result of the
sharing of modifiers among adjacent opinion expres-
sions. We add those edges in the following heuristic
way: If a vertex ri in opinion thread does not have
any modifier, we search the modifiers of its adjacent
vertices ri+1, ri?1 in the opinion thread, and add
edge (ri, d?) where
d? = argmax
d?S
score(ri, d),
and S are the modifiers of ri?1 and ri+1.
3.3 Training
We use online passive aggressive algorithm (PA)
with Hamming cost of two graphs in training (Cram-
mer et al, 2006).
Unigram Feature Template
xi.text w0.text w1.text
w0.POS w1.POS
wk?1.text wk.text
Inside wk?1.POS wk.POS
Features xi.hasDigital
xi.isSingleWord
xi.hasSentimentWord
xi.hasParallelPhrase
w?1.text w?2.text
w?1.POS w?2.POS
wk+1.text wk+2.text
Outside wk+1.POS wk+2.POS
Features c?1.text c?2.text
c?1.POS c?2.POS
cl+1.text cl+2.text
cl+1.POS cl+2.POS
Other Features
distance between parent and child
dependency parsing relations
Table 1: Feature set
3.4 Feature Construction
For each vertex xi in graph, we use 2 sets of fea-
tures: inside features which are extracted inside the
text span of xi; outside features which are outside
the text span of xi. A vertex xi is described both in
word sequence (w0, w1, ? ? ? , wk) and character se-
quence (c0, c1, ? ? ? , cl), for the sentences are in Chi-
nese.
? ? ? , w?1, w0, w1, w2, ? ? ? , wk?1, wk? ?? ?
xi
, wk+1 ? ? ?
? ? ? , c?1, c0, c1, c2, ? ? ? , cl?1, cl? ?? ?
xi
, cl+1 ? ? ?
For an edge (xi, xj), the high dimensional feature
vector f(xi, xj) is generated by using unigram fea-
tures in Table 1 on xi and xj respectively. The dis-
tance between parent and child in sentence is also
attached in features. In order to involve syntactic
information, whether there is certain type of depen-
dency relation between xi and xj is also used as a
feature.
1337
4 Experiments
4.1 Corpus
We constructed a Chinese online review corpus from
Pcpop.com, Zol.com.cn, and It168.com, which have
a large number of reviews about digital camera. The
corpus contains 138 documents and 1735 sentences.
Since some sentences do not contain any opinion,
1390 subjective sentences were finally chosen and
manually labeled.
Two annotators labeled the corpus independently.
The annotators started from locating opinion expres-
sions, and for each of them, they annotated other
modifiers related to it. In order to keep the relia-
bility of annotations, another annotator was asked
to check the corpus and determine the conflicts. Fi-
nally, we extracted 6103 elements, which are con-
nected by 6284 relations.
Relation Number
Target 2479
Coordinate 1173
Transition 154
Restriction 693
Expansion 386
Table 2: Statistics of relation types
Table 2 shows the number of various relation
types appearing in the labeled corpus. We observe
60.5% of sentences and 32.1% of opinion expres-
sions contain other modifiers besides ?target?. Thus
only mining the relations between opinion expres-
sions and evaluation target is actually at risk of inac-
curate and incomplete results.
4.2 Experiments Configurations
In all the experiments below, we take 90% of the cor-
pus as training set, 10% as test set and run 10 folder
cross validation. In feature construction, we use
an external Chinese sentiment lexicon which con-
tains 4566 positive opinion words and 4370 nega-
tive opinion words. For Chinese word segment, we
use ctbparser 5. Stanford parser (Klein and Man-
ning, 2003) is used for dependency parsing. In the
settings of PA, the maximum iteration number is
5http://code.google.com/p/ctbparser/
set to 2, which is chosen by maximizing the test-
ing performances, aggressiveness parameter C is set
to 0.00001. For parameters in inference algorithm,
Ct = 2, the solver of ILP is lpsolve6.
We evaluate the system from the following as-
pects: 1) whether the structural information helps
to mining opinion relations. 2) How the proposed
inference algorithm performs with different con-
straints. 3) How the various features affect the sys-
tem. Except for the last one, the feature set used for
different experiments are the same (?In+Out+Dep?
in Table 5). The criteria for evaluation are simi-
lar to the unlabeled attachment score in parser eval-
uations, but due to the equation |E| = |V | ? 1
is not valid if G is not a tree, we evaluate pre-
cision P = #true edges in result graph#edges in result graph , recall
R = #true edges in result graph#edges in true graph , and F-score
F = 2P ?RP+R .
4.3 Results
1. The effects of structural information. An alter-
native method to extract relations is directly using
a classifier to judge whether there is a relation be-
tween any two elements. Those kinds of methods
were used in previous opinion mining works (Wu
et al, 2009; Kobayashi et al, 2007). To show the
entire structural information is important for min-
ing relations, we use SVM for binary classification
on candidate pairs. The data point representing a
pair (xi, xj) is the same as the high dimensional fea-
ture vectors f(xi, xj). The setting of our algorithm
?MST+c1+c2+c3? is the basic MSTwith all the con-
straints. The results are shown in the Table 3.
P R F
SVM 64.9 24.0 35.0
MST+c1+c2+c3-m 61.5 74.0 67.2
MST+c1+c2+c3 73.1 71.0 72.1
Table 3: Binary classifier and structural learning
From the results, the performance of SVM (espe-
cially recall) is relatively poor. A possible reason
is that the huge imbalance of positive and negative
training samples (only ?(n) positive pairs among
all n2 pairs). And the absence of global structural
6http://sourceforge.net/projects/lpsolve/
1338
knowledge makes binary classifier unable to use
the information provided by classification results of
other pairs.
In order to examine whether the complicated sen-
timent representation would disturb the classifier in
finding relations between opinion expressions and
its target, we evaluate the system by discarding the
modifiers of opinion restriction and expansion from
the corpus. The result is shown in the second row of
Table 3. We observe that ?MST+c1+c2+c3? is still
better which means at least on overall performance
the additional modifiers do not harm.
2. The effect of constraints on inference algo-
rithm. In the inference algorithm, we utilized the
properties of graph G and adapted the basic multi-
commodity flow ILP to our specific task. To evaluate
how the constraints affect the system, we decompose
the algorithm and combine them in different ways.
P R F
MST 69.3 67.3 68.3
MST+c1 70.0 68.0 69.0
MST+c2 69.8 67.8 68.8
MST+c1+c2 70.6 68.6 69.6
MST+c1+c3 72.4 70.4 71.4
MST+c1+c2+c3 73.1 71.0 72.1
MST+c1+c2+c3+g 72.5 72.3 72.4
Table 4: Results on inference methods. ?MST? is the ba-
sic multicommodity flow formulation of maximum span-
ning tree; c1, c2, c3 are groups of constraint from Section
3.2.2; ?g? is our heuristic method for additional non span-
ning tree edges.
From Table 4, we observe that with any additional
constraints the inference algorithm outperforms the
basic maximum spanning tree method. It implies al-
though we did not use high order model (e.g. involv-
ing grandparent and sibling features), prior struc-
tural constraints can also help to get a better out-
put graph. By comparing with different constraint
combinations, the constraints on opinion thread (c1,
c3) are more effective than constraints on evaluation
targets (c2). It is because opinion expressions are
more important in the entire sentiment representa-
tion. The main structure of a graph is clear once the
relations between opinion expressions are correctly
determined.
3. The effects of various features. We evaluate the
performances of different feature configurations in
Table 5. From the results, the outside feature set is
more effective than inside feature set, even if it does
not use any external resource. A possible reason is
that the content of a vertex can be very complicated
(a vertex even can be a clause), but the features sur-
rounding the vertex are relatively simple and easy
to identify (for example, a single preposition can
identify a complex condition). The dependency fea-
ture has limited effect, due to that lots of online re-
view sentences are ungrammatical and parsing re-
sults are unreliable. And the complexity of vertices
also messes the dependency feature.
P R F
In-s 66.3 66.3 66.3
In 66.7 66.4 66.6
Out 67.8 67.4 67.6
In+Out 72.0 70.5 71.0
In+Out+Dep 72.5 72.3 72.4
Table 5: Results with different features. ?In? repre-
sents the result of inside feature set; ?In-s? is ?In? with-
out the external opinion lexicon feature; ?Out? uses the
outside feature set; ?In+Out? uses both ?In? and ?Out?,
?In+Out+Dep? adds the dependency feature. The infer-
ence algorithm is ?MST+c1+c2+c3+g? in Table 4.
We analyze the errors in test results. A main
source of errors is the confusion of classifier be-
tween ?target? relations and ?coordination?, ?tran-
sition? relations. The reason may be that for a mod-
ification on opinion expression (r, dk), we allow
dk recursively has its own modifiers (Example 5).
Thus an opinion expression can be a modifier which
brings difficulties to classifier.
4. Extraction of vertices. Finally we conduct an
experiment on vertex extraction using standard se-
quential labeling method. The tag set is simply {B,
I, O} which are signs of begin, inside, outside of a
vertex. The underlying model is conditional random
field 7. Feature templates involved are in Table 6.
We only use basic features in the experiment. 10
folder cross validation results are in table 7. We sus-
pect that the performances (especially recall) could
be improved if some external resources(i.e. ontol-
ogy, domain related lexicon, etc.) are involved.
7We use CRF++ toolkit, http://crfpp.sourceforge.net/
1339
Unigram Template
ci.char character
ci.isDigit digit
ci.isAlpha english letter
ci.isPunc punctuation
ci.inDict in a sentiment word
ci.BWord start of a word
ci.EWord end of a word
Table 6: Features for vertex extraction. The sequential
labeling is conducted on character level (ci). The senti-
ment lexicon used in ci.inDict is the same as Table1. We
also use bigram feature templates on ci.char, ci.isAlpha,
ci.inDict with respect to ci?1 and ci+1.
P R F
E+Unigram 56.8 45.1 50.3
E+Unigram+Bigram 57.3 47.9 52.1
O+Unigram 71.9 57.2 63.7
O+Unigram+Bigram 72.3 60.2 65.6
Table 7: Results on vertices extraction with 10 folder
cross validation. We use two criterion: 1) the vertex is
correct if it is exactly same as ground truth(?E?), 2) the
vertex is correct if it overlaps with ground truth(?O?).
5 Related Work
Opinion mining has recently received considerable
attentions. Large amount of work has been done on
sentimental classification in different levels and sen-
timent related information extraction. Researches on
different types of sentences such as comparative sen-
tences (Jindal and Liu, 2006) and conditional sen-
tences (Narayanan et al, 2009) have also been pro-
posed.
Kobayashi et al (2007) presented their work on
extracting opinion units including: opinion holder,
subject, aspect and evaluation. They used slots
to represent evaluations, converted the task to two
kinds of relation extraction tasks and proposed a ma-
chine learning-based method which used both con-
textual and statistical clues.
Jindal and Liu (2006) studied the problem of iden-
tifying comparative sentences. They analyzed dif-
ferent types of comparative sentences and proposed
learning approaches to identify them.
Sentiment analysis of conditional sentences were
studied by Narayanan et al (2009). They aimed
to determine whether opinions expressed on dif-
ferent topics in a conditional sentence are posi-
tive, negative or neutral. They analyzed the con-
ditional sentences in both linguistic and computi-
tional perspectives and used learning method to do
it. They followed the feature-based sentiment anal-
ysis model (Hu and Liu, 2004), which also use flat
frames to represent evaluations.
Integer linear programming was used in many
NLP tasks (Denis and Baldridge, 2007), for its
power in both expressing and approximating various
inference problems, especially in parsing (Riedel
and Clarke, 2006; Martins et al, 2009). Martins
etc. (2009) also applied ILP with flow formulation
for maximum spanning tree, besides, they also han-
dled dependency parse trees involving high order
features(sibling, grandparent), and with projective
constraint.
6 Conclusions
This paper introduces a representation method for
opinions in online reviews. Inspections on corpus
show that the information ignored in previous sen-
timent representation can cause incorrect or incom-
plete mining results. We consider opinion restric-
tion, opinion expansions, relations between opin-
ion expressions, and represent them with a directed
graph. Structural learning method is used to produce
the graph for a sentence. An inference algorithm is
proposed based on the properties of the graph. Ex-
perimental evaluations with a manually labeled cor-
pus are given to show the importance of structural
information and effectiveness of proposed inference
algorithm.
7 Acknowledgement
The author wishes to thank the anonymous review-
ers for their helpful comments. This work was
partially funded by 973 Program (2010CB327906),
National Natural Science Foundation of China
(61003092, 61073069),863 Program of China
(2009AA01A346), Shanghai Science and Tech-
nology Development Funds(10dz1500104), Doc-
toral Fund of Ministry of Education of China
(200802460066), Shanghai Leading Academic Dis-
cipline Project (B114), and Key Projects in
the National Science & Technology Pillar Pro-
1340
gram(2009BAH40B04).
References
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
Sajib Dasgupta and Vincent Ng. 2009. Mine the easy,
classify the hard: A semi-supervised approach to auto-
matic sentiment classification. In Proceedings of ACL-
IJCNLP.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: opinion extraction
and semantic classification of product reviews. In Pro-
ceedings of WWW.
Pascal Denis and Jason Baldridge. 2007. Joint determi-
nation of anaphoricity and coreference resolution us-
ing integer programming. In Proceedings of NAACL-
HLT.
Ahmed Hassan and Dragomir R. Radev. 2010. Identify-
ing text polarity using random walks. In Proceedings
of ACL, pages 395?403, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of SIGKDD.
Nitin Jindal and Bing Liu. 2006. Identifying comparative
sentences in text documents. In Proceedings of SIGIR.
R. Karp. 1972. Reducibility among combinatorial prob-
lems. In R. Miller and J. Thatcher, editors, Complex-
ity of Computer Computations, pages 85?103. Plenum
Press.
Soo-Min Kim and Eduard Hovy. 2006. Automatic iden-
tification of pro and con reasons in online reviews. In
Proceedings of the COLING-ACL.
Dan Klein and Christopher D.Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In In Advances in Neural Information Pro-
cessing Systems 15 (NIPS, pages 3?10. MIT Press.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of rela-
tions in opinion mining. In Proceedings of EMNLP-
CoNLL.
Thomas L. Magnanti and Laurence A. Wolsey. 1994.
Optimal trees.
Andre Martins, Noah Smith, and Eric Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proceedings of ACL-IJCNLP.
R. Mcdonald and F. Pereira. 2005. Identifying gene
and protein mentions in text using conditional random
fields. BMC Bioinformatics.
Ryan Mcdonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proc. of EACL, pages 81?88.
Ramanathan Narayanan, Bing Liu, and Alok Choudhary.
2009. Sentiment analysis of conditional sentences. In
Proceedings of EMNLP.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. In Proc. of EMNLP 2002.
Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective depen-
dency parsing. In Proceedings of EMNLP.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Proceedings of the seventh confer-
ence on Natural language learning at HLT-NAACL.
Swapna Somasundaran, Janyce Wiebe, and Josef Rup-
penhofer. 2008. Discourse level opinion interpreta-
tion. In Proceedings of COLING.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In Proceedings of ACL.
Yuanbin Wu, Qi Zhang, Xuangjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion mining.
In Proceedings of EMNLP.
1341
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1456?1465,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Grammatical Error Correction Using Integer Linear Programming
Yuanbin Wu
Department of Computer Science
National University of Singapore
13 Computing Drive
Singapore 117417
wuyb@comp.nus.edu.sg
Hwee Tou Ng
Department of Computer Science
National University of Singapore
13 Computing Drive
Singapore 117417
nght@comp.nus.edu.sg
Abstract
We propose a joint inference algorithm for
grammatical error correction. Different
from most previous work where different
error types are corrected independently,
our proposed inference process considers
all possible errors in a unied framework.
We use integer linear programming (ILP)
to model the inference process, which can
easily incorporate both the power of exist-
ing error classiers and prior knowledge
on grammatical error correction. Exper-
imental results on the Helping Our Own
shared task show that our method is com-
petitive with state-of-the-art systems.
1 Introduction
Grammatical error correction is an important task
of natural language processing (NLP). It has many
potential applications and may help millions of
people who learn English as a second language
(ESL). As a research eld, it faces the challenge of
processing ungrammatical language, which is dif-
ferent from other NLP tasks. The task has received
much attention in recent years, and was the focus
of two shared tasks on grammatical error correc-
tion in 2011 and 2012 (Dale and Kilgarriff, 2011;
Dale et al, 2012).
To detect and correct grammatical errors, two
different approaches are typically used ? knowl-
edge engineering or machine learning. The rst
relies on handcrafting a set of rules. For exam-
ple, the superlative adjective best is preceded by
the article the. In contrast, the machine learn-
ing approach formulates the task as a classication
problem based on learning from training data. For
example, an article classier takes a noun phrase
(NP) as input and predicts its article using class
labels a/an, the, or ? (no article).
Both approaches have their advantages and dis-
advantages. One can readily handcraft a set of
rules to incorporate various prior knowledge from
grammar books and dictionaries, but rules often
have exceptions and it is difcult to build rules
for all grammatical errors. On the other hand, the
machine learning approach can learn from texts
written by ESL learners where grammatical errors
have been annotated. However, training data may
be noisy and classiers may need prior knowledge
to guide their predictions.
Another consideration in grammatical error cor-
rection is how to deal with multiple errors in an
input sentence. Most previous work deals with
errors individually: different classiers (or rules)
are developed for different types of errors (article
classier, preposition classier, etc). Classiers
are then deployed independently. An example is
a pipeline system, where each classier takes the
output of the previous classier as its input and
proposes corrections of one error type.
One problem of this pipeline approach is that
the relations between errors are ignored. For ex-
ample, assume that an input sentence contains a
cats. An article classier may propose to delete
a, while a noun number classier may propose
to change cats to cat. A pipeline approach will
choose one of the two corrections based purely
on which error classier is applied rst. Another
problem is that when applying a classier, the sur-
rounding words in the context are assumed to be
correct, which is not true if grammatical errors ap-
pear close to each other in a sentence.
In this paper, we formulate grammatical er-
ror correction as a task suited for joint inference.
Given an input sentence, different types of errors
are jointly corrected as follows. For every possi-
ble error correction, we assign a score which mea-
sures how grammatical the resulting sentence is if
the correction is accepted. We then choose a set
of corrections which will result in a corrected sen-
tence that is judged to be the most grammatical.
The inference problem is solved by integer lin-
1456
ear programming (ILP). Variables of ILP are indi-
cators of possible grammatical error corrections,
the objective function aims to select the best set of
corrections, and the constraints help to enforce a
valid and grammatical output. Furthermore, ILP
not only provides a method to solve the inference
problem, but also allows for a natural integration
of grammatical constraints into a machine learn-
ing approach. We will show that ILP fully utilizes
individual error classiers, while prior knowledge
on grammatical error correction can be easily ex-
pressed using linear constraints. We evaluate our
proposed ILP approach on the test data from the
Helping Our Own (HOO) 2011 shared task (Dale
and Kilgarriff, 2011). Experimental results show
that the ILP formulation is competitive with state-
of-the-art grammatical error correction systems.
The remainder of this paper is organized as fol-
lows. Section 2 gives the related work. Section
3 introduces a basic ILP formulation. Sections
4 and 5 improve the basic ILP formulation with
more constraints and second order variables, re-
spectively. Section 6 presents the experimental re-
sults. Section 7 concludes the paper.
2 Related Work
The knowledge engineering approach has been
used in early grammatical error correction systems
(Murata and Nagao, 1993; Bond et al, 1995; Bond
and Ikehara, 1996; Heine, 1998). However, as
noted by (Han et al, 2006), rules usually have ex-
ceptions, and it is hard to utilize corpus statistics
in handcrafted rules. As such, the machine learn-
ing approach has become the dominant approach
in grammatical error correction.
Previous work in the machine learning approach
typically formulates the task as a classication
problem. Article and preposition errors are the two
main research topics (Knight and Chander, 1994;
Han et al, 2006; Tetreault and Chodorow, 2008;
Dahlmeier and Ng, 2011). Features used in classi-
cation include surrounding words, part-of-speech
tags, language model scores (Gamon, 2010), and
parse tree structures (Tetreault et al, 2010). Learn-
ing algorithms used include maximum entropy
(Han et al, 2006; Tetreault and Chodorow, 2008),
averaged perceptron, na?ve Bayes (Rozovskaya
and Roth, 2011), etc. Besides article and prepo-
sition errors, verb form errors also attract some
attention recently (Liu et al, 2010; Tajiri et al,
2012).
Several research efforts have started to deal with
correcting different errors in an integrated manner
(Gamon, 2011; Park and Levy, 2011; Dahlmeier
and Ng, 2012a). Gamon (2011) uses a high-order
sequential labeling model to detect various errors.
Park and Levy (2011) models grammatical error
correction using a noisy channel model, where a
predened generative model produces correct sen-
tences and errors are added through a noise model.
The work of (Dahlmeier and Ng, 2012a) is proba-
bly the closest to our current work. It uses a beam-
search decoder, which iteratively corrects an in-
put sentence to arrive at the best corrected output.
The difference between their work and our ILP
approach is that the beam-search decoder returns
an approximate solution to the original inference
problem, while ILP returns an exact solution to an
approximate inference problem.
Integer linear programming has been success-
fully applied to many NLP tasks, such as depen-
dency parsing (Riedel and Clarke, 2006; Martins
et al, 2009), semantic role labeling (Punyakanok
et al, 2005), and event extraction (Riedel and Mc-
Callum, 2011).
3 Inference with First Order Variables
The inference problem for grammatical error cor-
rection can be stated as follows: ?Given an input
sentence, choose a set of corrections which results
in the best output sentence.? In this paper, this
problem will be expressed and solved by integer
linear programming (ILP).
To express an NLP task in the framework of ILP
requires the following steps:
1. Encode the output space of the NLP task us-
ing integer variables;
2. Express the inference objective as a linear
objective function; and
3. Introduce problem-specic constraints to re-
ne the feasible output space.
In the following sections, we follow the above
formulation. For the grammatical error correc-
tion task, the variables in ILP are indicators of the
corrections that a word needs, the objective func-
tion measures how grammatical the whole sen-
tence is if some corrections are accepted, and the
constraints guarantee that the corrections do not
conict with each other.
1457
3.1 First Order Variables
Given an input sentence, the main question that a
grammatical error correction system needs to an-
swer is: What corrections at which positions? For
example, is it reasonable to change the word cats
to cat in the sentence A cats sat on the mat? Given
the corrections at various positions in a sentence,
the system can readily come up with the corrected
sentence. Thus, a natural way to encode the output
space of grammatical error correction requires in-
formation about sentence position, error type (e.g.,
noun number error), and correction (e.g., cat).
Suppose s is an input sentence, and |s| is its
length (i.e., the number of words in s). Dene rst
order variables:
Zkl,p ? {0, 1}, (1)
where
p? {1, 2, . . . , |s|} is a position in a sentence,
l? L is an error type,
k? {1, 2, . . . , C(l)} is a correction of type l.
L: the set of error types,
C(l): the number of corrections for error type l.
If Zkl,p = 1, the word at position p should be cor-
rected to k that is of error type l. Otherwise, the
word at position p is not applicable for this correc-
tion. Deletion of a word is represented as k = ?.
For example, Za
Art,1 = 1 means that the article
(Art) at position 1 of the sentence should be a. If
Za
Art,1 = 0, then the article should not be a. Ta-
ble 1 contains the error types handled in this work,
their possible corrections and applicable positions
in a sentence.
3.2 The Objective Function
The objective of the inference problem is to nd
the best output sentence. However, there are expo-
nentially many different combinations of correc-
tions, and it is not possible to consider all com-
binations. Therefore, instead of solving the orig-
inal inference problem, we will solve an approx-
imate inference problem by introducing the fol-
lowing decomposable assumption: Measuring the
output quality of multiple corrections can be de-
composed into measuring the quality of the indi-
vidual corrections.
Let s? be the resulting sentence if the correction
Zkl,p is accepted for s, or for simplicity denoting
it as s
Zkl,p??? s?. Let wl,p,k ? R, measure how
grammatical s? is. Dene the objective function as
max
?
l,p,k
wl,p,kZkl,p.
This linear objective function aims to select a set
of Zkl,p, such that the sum of their weights is the
largest among all possible candidate corrections,
which in turn gives the most grammatical sentence
under the decomposable assumption.
Although the decomposable assumption is a
strong assumption, it performs well in practice,
and one can relax the assumption by using higher
order variables (see Section 5).
For an individual correction Zkl,p, we measure
the quality of s? based on three factors:
1. The language model score h(s?,LM) of s?
based on a large web corpus;
2. The condence scores f(s?, t) of classiers,
where t ? E andE is the set of classiers. For ex-
ample, an article classier trained on well-written
documents will score every article in s?, and mea-
sure the quality of s? from the perspective of an
article ?expert?.
3. The disagreement scores g(s?, t) of classi-
ers, where t ? E. A disagreement score mea-
sures how ungrammatical s? is from the perspec-
tive of a classier. Take the article classier as an
example. For each article instance in s?, the clas-
sier computes the difference between the maxi-
mum condence score among all possible choices
of articles, and the condence score of the ob-
served article. This difference represents the dis-
agreement on the observed article by the article
classier or ?expert?. Dene the maximum differ-
ence over all article instances in s? to be the article
classier disagreement score of s?. In general, this
score is large if the sentence s? is more ungram-
matical.
The weight wl,p,k is a combination of these
scores:
wl,p,k = ?LMh(s?,LM) +
?
t?E
?tf(s?, t)
+
?
t?E
?tg(s?, t), (2)
where ?
LM
, ?t, and ?t are the coefcients.
3.3 Constraints
An observation on the objective function is that
it is possible, for example, to set Za
Art,p = 1 and
1458
Type l Correction k C(l) Applicable Variables
article a, the, ? 3 article or NP Za
Art,p, Z theArt,p,Z?Art,p
preposition on, at, in, . . . |confusion set| preposition Zon
Prep,p, ZatPrep,p, Z inPrep,p, . . .
noun number singular, plural 2 noun Zsingular
Noun,p , Z
plural
Noun,p
punctuation punctuation symbols |candidates| determined by rules Zoriginal
Punct,p , Zcand1Punct,p, Zcand2Punct,p,. . .
spelling correctly spelled |candidates| determined by a Zoriginal
Spell,p , Zcand1Spell,p, Zcand2Spell,p,. . .
words spell checker
Table 1: Error types and corrections. The Applicable column indicates which parts of a sentence are
applicable to an error type. In the rst row, ? means deleting an article.
Z the
Art,p = 1, which means there are two corrections
a and the for the same sentence position p, but ob-
viously only one article is allowed.
A simple constraint to avoid these conicts is
?
k
Zkl,p = 1, ? applicable l, p
It reads as follows: for each error type l, only one
output k is allowed at any applicable position p
(note that Zkl,p is a Boolean variable).
Putting the variables, objective function, and
constraints together, the ILP problem with respect
to rst order variables is as follows:
max
?
l,p,k
wl,p,kZkl,p (3)
s.t.
?
k
Zkl,p = 1, ? applicable l, p (4)
Zkl,p ? {0, 1} (5)
The ILP problem is solved using lp solve
1
, an
integer linear programming solver based on the re-
vised simplex method and the branch-and-bound
method for integers.
3.4 An Illustrating Example
To illustrate the ILP formulation, consider an ex-
ample input sentence s:
A cats sat on the mat . (6)
First, the constraint (4) at position 1 is:
Za
Art,1 + Z theArt,1 + Z?Art,1 = 1,
which means only one article in {a, the, ?} is se-
lected.
1
http://lpsolve.sourceforge.net/
Next, to compute wl,p,k, we collect language
model score and condence scores from the arti-
cle (ART), preposition (PREP), and noun number
(NOUN) classier, i.e., E = {ART,PREP,NOUN}.
The weight for Zsingular
Noun,2 is:
w
Noun,2,singular = ?LMh(s?,LM)+
?
ART
f(s?,ART) + ?
PREP
f(s?,PREP) + ?
NOUN
f(s?,NOUN)+
?
ART
g(s?,ART) + ?
PREP
g(s?,PREP) + ?
NOUN
g(s?,NOUN).
where s
Zsingular
Noun,2????? s? = A cat sat on the mat .
The condence score f(s?, t) of classier t is
the average of the condence scores of t on the
applicable instances in s?.
For example, there are two article instances in
s?, located at position 1 and 5 respectively, hence,
f(s?,ART)= 12
 
f(s?[1], 1,ART) + f(s?[5], 5,ART)

= 12
 
f(a, 1,ART) + f(the, 5,ART)

.
Here, the symbol ft(s?[p], p,ART) refers to the
condence score of the article classier at position
p, and s?[p] is the word at position p of s?.
Similarly, the disagreement score g(s?,ART) of
the article classier is
g(s?,ART) = max(g1, g2)
g1= argmax
k
f(k, 1,ART)? f(a, 1,ART)
g2= argmax
k
f(k, 5,ART)? f(the, 5,ART)
Putting them together, the weight forZsingular
Noun,2 is:
w
Noun,2,singular = ?LMh(s?,LM)
+ ?ART2
 
f(a, 1,ART) + f(the, 5,ART)

+ ?
PREP
f(on, 4,PREP)
+ ?NOUN2
 
f(cat, 2,NOUN) + f(mat, 6,NOUN)

+ ?
ART
g(s?,ART)
+ ?
PREP
g(s?,PREP)
+ ?
NOUN
g(s?,NOUN)
1459
Input A cats sat on the mat
Corrections The, ? cat at, in a, ? mats
Za
Art,1 Z
singular
Noun,2 ZonPrep,4 ZaArt,5 Z
singular
Noun,6
Variables Z the
Art,1 Z
plural
Noun,2 ZatPrep,4 Z theArt,5 Z
plural
Noun,6
Z?
Art,1 Z inPrep,4 Z?Art,5
Table 2: The possible corrections on example (6).
3.5 Complexity
The time complexity of ILP is determined by
the number of variables and constraints. Assume
that for each sentence position, at most K classi-
ers are applicable
2
. The number of variables is
O(K|s|C(l?)), where l? = argmaxl?LC(l). The
number of constraints is O(K|s|).
4 Constraints for Prior Knowledge
4.1 Modication Count Constraints
In practice, we usually have some rough gauge
of the quality of an input sentence. If an input
sentence is mostly grammatical, the system is ex-
pected to make few corrections. This require-
ment can be easily satised by adding modica-
tion count constraints.
In this work, we constrain the number of modi-
cations according to error types. For the error type
l, a parameter Nl controls the number of modi-
cations allowed for type l. For example, the mod-
ication count constraint for article corrections is
?
p,k
Zk
Art,p ? NArt, where k 6= s[p]. (7)
The condition ensures that the correction k is dif-
ferent from the original word in the input sentence.
Hence, the summation only counts real modica-
tions. There are similar constraints for preposi-
tion, noun number, and spelling corrections:
?
p,k
Zk
Prep,p? NPrep, where k 6= s[p], (8)
?
p,k
Zk
Noun,p? NNoun, where k 6= s[p], (9)
?
p,k
Zk
Spell,p? NSpell, where k 6= s[p]. (10)
2
In most cases, K = 1. An example of K > 1 is a noun
that requires changing the word form (between singular and
plural) and inserting an article, for whichK = 2.
4.2 Article-Noun Agreement Constraints
An advantage of the ILP formulation is that it
is relatively easy to incorporate prior linguistic
knowledge. We now take article-noun agreement
as an example to illustrate how to encode such
prior knowledge using linear constraints.
A noun in plural form cannot have a (or an)
as its article. That two Boolean variables Z1 and
Z2 are mutually exclusive can be handled using a
simple inequality Z1 + Z2 ? 1. Thus, the fol-
lowing inequality correctly enforces article-noun
agreement:
Za
Art,p1 + Z
plural
Noun,p2 ? 1, (11)
where the article at p1 modies the noun at p2.
4.3 Dependency Relation Constraints
Another set of constraints involves dependency
relations, including subject-verb relation and
determiner-noun relation. Specically, for a noun
n at position p, we check the word w related to n
via a child-parent or parent-child relation. Ifw be-
longs to a set of verbs or determiners (are, were,
these, all) that takes a plural noun, then the noun
n is required to be in plural form by adding the
following constraint:
Zplural
Noun,p = 1. (12)
Similarly, if a noun n at position p is required to
be in singular form due to subject-verb relation
or determiner-noun relation, we add the following
constraint:
Zsingular
Noun,p = 1. (13)
5 Inference with Second Order Variables
5.1 Motivation and Denition
To relax the decomposable assumption in Section
3.2, instead of treating each correction separately,
one can combine multiple corrections into a single
correction by introducing higher order variables.
1460
Consider the sentence A cat sat on the mat.
When measuring the gain due to Zplural
Noun,2 = 1
(change cat to cats), the weight w
Noun,2,plural is
likely to be small since A cats will get a low lan-
guage model score, a low article classier con-
dence score, and a low noun number classier
condence score. Similarly, the weight w
Art,1,? of
Z?
Art,1 (delete article A) is also likely to be small
because of the missing article. Thus, if one con-
siders the two corrections separately, they are both
unlikely to appear in the nal corrected output.
However, the correction from A cat sat on the
mat. toCats sat on the mat. should be a reasonable
candidate, especially if the context indicates that
there are many cats (more than one) on the mat.
Due to treating corrections separately, it is difcult
to deal with multiple interacting corrections with
only rst order variables.
In order to include the correction ? Cats, one
can use a new set of variables, second order vari-
ables. To keep symbols clear, let Z = {Zu|Zu =
Zkl,p,?l, p, k} be the set of rst order variables, and
wu = wl,p,k be the weight of Zu = Zkl,p. Dene a
second order variableXu,v:
Xu,v = Zu ? Zv, (14)
where Zu and Zv are rst order variables:
Zu , Zk1l1,p1 , Zv , Zk2l2,p2 . (15)
The denition ofXu,v states that a second order
variable is set to 1 if and only if its two compo-
nent rst order variables are both set to 1. Thus, it
combines two corrections into a single correction.
In the above example, a second order variable is
introduced:
Xu,v = Z?
Art,1 ? Zplural
Noun,2,
s Xu,v???? s? = Cats sat on the mat .
Similar to rst order variables, let wu,v be the
weight of Xu,v. Note that denition (2) only de-
pends on the output sentence s?, and the weight of
the second order variable wu,v can be dened in
the same way:
wu,v = ?LMh(s?,LM) +
?
t?E
?tf(s?, t)
+
?
t?E
?tg(s?, t). (16)
5.2 ILP with Second Order Variables
A set of new constraints is needed to enforce con-
sistency between the rst and second order vari-
ables. These constraints are the linearization of
denition (14) ofXu,v:
Xu,v = Zu ? Zv ?
Xu,v ? Zu
Xu,v ? Zv
Xu,v ? Zu + Zv ? 1
(17)
A new objective function combines the weights
from both rst and second order variables:
max
?
l,p,k
wl,p,kZkl,p +
?
u,v
wu,vXu,v. (18)
In our experiments, due to noisy data, some
weights of second order variables are small, even
if both of its rst order variables have large
weights and satisfy all prior knowledge con-
straints. They will affect ILP proposing good cor-
rections. We nd that the performance will be bet-
ter if we change the weights of second order vari-
ables to w?u,v, where
w?u,v , max{wu,v, wu, wv}. (19)
Putting them together, (20)-(25) is an ILP for-
mulation using second order variables, whereX is
the set of all second order variables which will be
explained in the next subsection.
max
?
l,p,k
wl,p,kZkl,p +
?
u,v
w?u,vXu,v (20)
s.t.
?
k
Zkl,p = 1, ? applicable l, p (21)
Xu,v ? Zu, (22)
Xu,v ? Zv, (23)
Xu,v ? Zu + Zv ? 1,?Xu,v ? X (24)
Xu,v, Zkl,p ? {0, 1} (25)
5.3 Complexity and Variable Selection
Using the notation in section 3.5, the num-
ber of second order variables is O(|Z|2) =
O(K2|s|2C(l?)2) and the number of constraints is
O(K2|s|2C(l?)2). More generally, for variables
with higher order h ? 2, the number of variables
(and constraints) is O(Kh|s|hC(l?)h).
Note that both the number of variables and the
number of constraints increase exponentially with
increasing variable order. In practice, a small
subset of second order variables is sufcient to
1461
Data set Sentences Words Edits
Dev set 939 22,808 1,264
Test set 722 18,790 1,057
Table 3: Overview of the HOO 2011 data sets.
Corrections are called edits in the HOO 2011
shared task.
achieve good performance. For example, noun
number corrections are only coupled with nearby
article corrections, and have no connection with
distant or other types of corrections.
In this work, we only introduce second or-
der variables that combine article corrections and
noun number corrections. Furthermore, we re-
quire that the article and the noun be in the same
noun phrase. The set X of second order variables
in Equation (24) is dened as follows:
X ={Xu,v = Zu ? Zv|l1 = Art, l2 = Noun,
s[p1], s[p2] are in the same noun phrase},
where l1, l2, p1, p2 are taken from Equation (15).
6 Experiments
Our experiments mainly focus on two aspects:
how our ILP approach performs compared to other
grammatical error correction systems; and how
the different constraints and the second order vari-
ables affect the ILP performance.
6.1 Evaluation Corpus and Metric
We follow the evaluation setup in the HOO 2011
shared task on grammatical error correction (Dale
and Kilgarriff, 2011). The development set and
test set in the shared task consist of conference and
workshop papers taken from the Association for
Computational Linguistics (ACL). Table 3 gives
an overview of the data sets.
System performance is measured by precision,
recall, and F measure:
P = # true edits
# system edits
, R = # true edits
# gold edits
, F = 2PRP + R.
(26)
The difculty lies in how to generate the system
edits from the system output. In the HOO 2011
shared task, participants can submit system edits
directly or the corrected plain-text system output.
In the latter case, the ofcial HOO scorer will ex-
tract system edits based on the original (ungram-
matical) input text and the corrected system output
text, using GNU Wdiff
3
.
Consider an input sentence The data is simi-
lar with test set. taken from (Dahlmeier and Ng,
2012a). The gold-standard edits are with? to and
? ? the. That is, the grammatically correct sen-
tence should be The data is similar to the test set.
Suppose the corrected output of a system to be
evaluated is exactly this perfectly corrected sen-
tence The data is similar to the test set. However,
the ofcial HOO scorer using GNUWdiff will au-
tomatically extract only one system edit with? to
the for this system output. Since this single system
edit does not match any of the two gold-standard
edits, the HOO scorer returns an F measure of 0,
even though the system output is perfectly correct.
In order to overcome this problem, the Max-
Match (M2) scorer was proposed in (Dahlmeier
and Ng, 2012b). Given a set of gold-standard ed-
its, the original (ungrammatical) input text, and
the corrected system output text, the M2 scorer
searches for the system edits that have the largest
overlap with the gold-standard edits. For the above
example, the system edits automatically deter-
mined by theM2 scorer are identical to the gold-
standard edits, resulting in an F measure of 1 as we
would expect. We will use the M2 scorer in this
paper to determine the best system edits. Once the
system edits are found, P , R, and F are computed
using the standard denition (26).
6.2 ILP Conguration
6.2.1 Variables
The rst order variables are given in Table 1. If
the indenite article correction a is chosen, then
the nal choice between a and an is decided by a
rule-based post-processing step. For each prepo-
sition error variable Zk
Prep,p, the correction k is re-
stricted to a pre-dened confusion set of prepo-
sitions which depends on the observed preposi-
tion at position p. For example, the confusion
set of on is { at, for, in, of }. The list of prepo-
sitions corrected by our system is about, among,
at, by, for, in, into, of, on, over, to, under, with,
and within. Only selected positions in a sentence
(determined by rules) undergo punctuation correc-
tion. The spelling correction candidates are given
by a spell checker. We used GNU Aspell
4
in our
work.
3
http://www.gnu.org/software/wdiff/
4
http://aspell.net
1462
6.2.2 Weights
As described in Section 3.2, the weight of each
variable is a linear combination of the language
model score, three classier condence scores,
and three classier disagreement scores. We use
the Web 1T 5-gram corpus (Brants and Franz,
2006) to compute the language model score for
a sentence. Each of the three classiers (article,
preposition, and noun number) is trained with the
multi-class condence weighted algorithm (Cram-
mer et al, 2009). The training data consists of all
non-OCR papers in the ACL Anthology
5
, minus
the documents that overlap with the HOO 2011
data set. The features used for the classiers fol-
low those in (Dahlmeier and Ng, 2012a), which
include lexical and part-of-speech n-grams, lexi-
cal head words, web-scale n-gram counts, depen-
dency heads and children, etc. Over 5 million
training examples are extracted from the ACL An-
thology for use as training data for the article and
noun number classiers, and over 1 million train-
ing examples for the preposition classier.
Finally, the language model score, classier
condence scores, and classier disagreement
scores are normalized to take values in [0, 1],
based on the HOO 2011 development data. We use
the following values for the coefcients: ?
LM
= 1
(language model); ?t = 1 (classier condence);
and ?t = ?1 (classier disagreement).
6.2.3 Constraints
In Section 4, three sets of constraints are in-
troduced: modication count (MC), article-noun
agreement (ANA), and dependency relation (DR)
constraints. The values for the modication count
parameters are set as follows: N
Art
= 3, N
Prep
=
2, N
Noun
= 2, and N
Spell
= 1.
6.3 Experimental Results
We compare our ILP approach with two other sys-
tems: the beam search decoder of (Dahlmeier and
Ng, 2012a) which achieves the best published per-
formance to date on the HOO 2011 data set, and
UI Run1 (Rozovskaya et al, 2011) which achieves
the best performance among all participating sys-
tems at the HOO 2011 shared task. The results are
given in Table 4.
The HOO 2011 shared task provides two sets of
gold-standard edits: the original gold-standard ed-
its produced by the annotator, and the ofcial gold-
5
http://aclweb.org/anthology-new/
System Original Ofcial
P R F P R F
UI Run1 40.86 11.21 17.59 54.61 14.57 23.00
Beam search 30.28 19.17 23.48 33.59 20.53 25.48
ILP 20.54 27.93 23.67 21.99 29.04 25.03
Table 4: Comparison of three grammatical error
correction systems.
standard edits which incorporated corrections pro-
posed by the HOO 2011 shared task participants.
All three systems listed in Table 4 use the M2
scorer to extract system edits. The results of the
beam search decoder and UI Run1 are taken from
Table 2 of (Dahlmeier and Ng, 2012a).
Overall, ILP inference outperforms UI Run1 on
both the original and ofcial gold-standard edits,
and the improvements are statistically signicant
at the level of signicance 0.01. The performance
of ILP inference is also competitive with the beam
search decoder. The results indicate that a gram-
matical error correction system benets from cor-
rections made at a whole sentence level, and that
joint correction of multiple error types achieves
state-of-the-art performance.
Table 5 provides the comparison of the beam
search decoder and ILP inference in detail. The
main difference between the two is that, except for
spelling errors, ILP inference gives higher recall
than the beam search decoder, while its precision
is lower. This indicates that ILP inference is more
aggressive in proposing corrections.
Next, we evaluate ILP inference in different
congurations. We only focus on article and noun
number error types. Table 6 shows the perfor-
mance of ILP in different congurations. From
the results, MC and DR constraints improve pre-
cision, indicating that the two constraints can help
to restrict the number of erroneous corrections. In-
cluding second order variables gives the best F
measure, which supports our motivation for intro-
ducing higher order variables.
Adding article-noun agreement constraints
(ANA) slightly decreases performance. By exam-
ining the output, we nd that although the overall
performance worsens slightly, the agreement re-
quirement is satised. For example, for the input
We utilize search engine to . . . , the output without
ANA isWe utilize a search engines to . . . but with
ANA is We utilize the search engines to . . . , while
the only gold edit inserts a.
1463
Original Ofcial
Error type Beam search ILP Beam search ILP
P R F P R F P R F P R F
Spelling 36.84 0.69 1.35 60.00 0.59 1.17 36.84 0.66 1.30 60.00 0.57 1.12
+ Article 19.84 12.59 15.40 18.54 14.75 16.43 22.45 13.72 17.03 20.37 15.61 17.68
+ Preposition 22.62 14.26 17.49 17.61 18.58 18.09 24.84 15.14 18.81 19.24 19.68 19.46
+ Punctuation 24.27 18.09 20.73 20.52 23.50 21.91 27.13 19.58 22.75 22.49 24.98 23.67
+ Noun number 30.28 19.17 23.48 20.54 27.93 23.67 33.59 20.53 25.48 21.99 29.04 25.03
Table 5: Comparison of the beam search decoder and ILP inference. ILP is equipped with all constraints
(MC, ANA, DR) and default parameters. Second order variables related to article and noun number error
types are also used in the last row.
Setting Original Ofcial
P R F P R F
Art+Nn, 1st ord. 17.19 19.37 18.22 18.59 20.44 19.47
+ MC 17.87 18.49 18.17 19.23 19.39 19.31
+ ANA 17.78 18.39 18.08 19.04 19.11 19.07
+ DR 17.95 18.58 18.26 19.23 19.30 19.26
+ 2nd ord. 18.75 18.88 18.81 20.04 19.58 19.81
Table 6: The effects of different constraints and second order variables.
7 Conclusion
In this paper, we model grammatical error correc-
tion as a joint inference problem. The inference
problem is solved using integer linear program-
ming. We provide three sets of constraints to in-
corporate additional linguistic knowledge, and in-
troduce a further extension with second order vari-
ables. Experiments on the HOO 2011 shared task
show that ILP inference achieves state-of-the-art
performance on grammatical error correction.
Acknowledgments
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Ofce.
References
Francis Bond and Satoru Ikehara. 1996. When and
how to disambiguate? countability in machine trans-
lation. In Proceedings of the International Seminar
on Multimodal Interactive Disambiguation.
Francis Bond, Kentaro Ogura, and Tsukasa Kawaoka.
1995. Noun phrase reference in Japanese-to-English
machine translation. In Proceedings of the 6th In-
ternational Conference on Theoretical and Method-
ological Issues in Machine Translation.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram corpus version 1.1. Technical report, Google
Research.
Koby Crammer, Mark Dredze, and Alex Kulesza.
2009. Multi-class condence weighted algorithms.
In Proceedings of EMNLP.
Daniel Dahlmeier and Hwee Tou Ng. 2011. Grammat-
ical error correction with alternating structure opti-
mization. In Proceedings of ACL.
Daniel Dahlmeier and Hwee Tou Ng. 2012a. A beam-
search decoder for grammatical error correction. In
Proceedings of EMNLP.
Daniel Dahlmeier and Hwee Tou Ng. 2012b. Better
evaluation for grammatical error correction. In Pro-
ceedings of NAACL.
Robert Dale and Adam Kilgarriff. 2011. Helping Our
Own: The HOO 2011 pilot shared task. In Proceed-
ings of the 13th EuropeanWorkshop on Natural Lan-
guage Generation.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A report on the preposition and
determiner error correction shared task. In Proceed-
ings of the Seventh Workshop on Innovative Use of
NLP for Building Educational Applications, pages
54?62.
Michael Gamon. 2010. Using mostly native data to
correct errors in learners' writing. In Proceedings of
NAACL.
1464
Michael Gamon. 2011. High-order sequence model-
ing for language learner error detection. In Proceed-
ings of the Sixth Workshop on Innovative Use of NLP
for Building Educational Applications.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineer-
ing, 12(2).
Julia Heine. 1998. Deniteness predictions for
Japanese noun phrases. In Proceedings of ACL-
COLING.
Kevin Knight and Ishwar Chander. 1994. Automated
postediting of documents. In Proceedings of AAAI.
Xiaohua Liu, Bo Han, Kuan Li, Stephan Hyeonjun
Stiller, and Ming Zhou. 2010. SRL-based verb se-
lection for ESL. In Proceedings of EMNLP.
Andre Martins, Noah Smith, and Eric Xing. 2009.
Concise integer linear programming formulations
for dependency parsing. In Proceedings of ACL-
IJCNLP.
Masaki Murata and Makoto Nagao. 1993. Determina-
tion of referential property and number of nouns in
Japanese sentences for machine translation into En-
glish. In Proceedings of the 5th International Con-
ference on Theoretical and Methodological Issues in
Machine Translation.
Y. Albert Park and Roger Levy. 2011. Automated
whole sentence grammar correction using a noisy
channel model. In Proceedings of ACL.
Vasin Punyakanok, Dan Roth, Wen tau Yih, and Dav
Zimak. 2005. Learning and inference over con-
strained output. In Proceedings of IJCAI.
Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective de-
pendency parsing. In Proceedings of EMNLP.
Sebastian Riedel and Andrew McCallum. 2011. Fast
and robust joint models for biomedical event extrac-
tion. In Proceedings of EMNLP.
Alla Rozovskaya and Dan Roth. 2011. Algorithm
selection and model adaptation for ESL correction
tasks. In Proceedings of ACL.
Alla Rozovskaya, Mark Sammons, Joshua Gioja, and
Dan Roth. 2011. University of Illinois system in
HOO text correction shared task. In Proceedings of
the 13th European Workshop on Natural Language
Generation.
Toshikazu Tajiri, Mamoru Komachi, and Yuji Mat-
sumoto. 2012. Tense and aspect error correction for
ESL learners using global context. In Proceedings
of ACL.
Joel R. Tetreault and Martin Chodorow. 2008. The
ups and downs of preposition error detection in ESL
writing. In Proceedings of COLING.
Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010. Using parse features for preposition selection
and error detection. In Proceedings of ACL.
1465
Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 1?12,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
The CoNLL-2013 Shared Task on Grammatical Error Correction
Hwee Tou Ng
Department of Computer Science
National University of Singapore
nght@comp.nus.edu.sg
Siew Mei Wu
Centre for English Language Communication
National University of Singapore
elcwusm@nus.edu.sg
Yuanbin Wu and Christian Hadiwinoto
Department of Computer Science
National University of Singapore
{wuyb,chrhad}@comp.nus.edu.sg
Joel Tetreault
Nuance Communications, Inc.
Joel.Tetreault@nuance.com
Abstract
The CoNLL-2013 shared task was devoted
to grammatical error correction. In this
paper, we give the task definition, present
the data sets, and describe the evaluation
metric and scorer used in the shared task.
We also give an overview of the various
approaches adopted by the participating
teams, and present the evaluation results.
1 Introduction
Grammatical error correction is the shared task
of the Seventeenth Conference on Computational
Natural Language Learning in 2013 (CoNLL-
2013). In this task, given an English essay written
by a learner of English as a second language, the
goal is to detect and correct the grammatical errors
present in the essay, and return the corrected essay.
This task has attracted much recent research in-
terest, with two shared tasks Helping Our Own
(HOO) 2011 and 2012 organized in the past two
years (Dale and Kilgarriff, 2011; Dale et al,
2012). In contrast to previous CoNLL shared tasks
which focused on particular subtasks of natural
language processing, such as named entity recog-
nition, semantic role labeling, dependency pars-
ing, or coreference resolution, grammatical error
correction aims at building a complete end-to-end
application. This task is challenging since for
many error types, current grammatical error cor-
rection systems do not achieve high performance
and much research is still needed. Also, tackling
this task has far-reaching impact, since it is esti-
mated that hundreds of millions of people world-
wide are learning English and they benefit directly
from an automated grammar checker.
The CoNLL-2013 shared task provides a forum
for participating teams to work on the same gram-
matical error correction task, with evaluation on
the same blind test set using the same evaluation
metric and scorer. This overview paper contains a
detailed description of the shared task, and is orga-
nized as follows. Section 2 provides the task def-
inition. Section 3 describes the annotated training
data provided and the blind test data. Section 4 de-
scribes the evaluation metric and the scorer. Sec-
tion 5 lists the participating teams and outlines the
approaches to grammatical error correction used
by the teams. Section 6 presents the results of the
shared task. Section 7 concludes the paper.
2 Task Definition
The goal of the CoNLL-2013 shared task is to
evaluate algorithms and systems for automati-
cally detecting and correcting grammatical errors
present in English essays written by second lan-
guage learners of English. Each participating
team is given training data manually annotated
with corrections of grammatical errors. The test
data consists of new, blind test essays. Prepro-
cessed test essays, which have been sentence-
segmented and tokenized, are also made available
to the participating teams. Each team is to submit
its system output consisting of the automatically
corrected essays, in sentence-segmented and tok-
enized form.
Grammatical errors consist of many different
types, including articles or determiners, preposi-
tions, noun form, verb form, subject-verb agree-
ment, pronouns, word choice, sentence structure,
punctuation, capitalization, etc. Of all the er-
ror types, determiners and prepositions are among
1
the most frequent errors made by learners of En-
glish. Not surprisingly, much published research
on grammatical error correction focuses on arti-
cle and preposition errors (Han et al, 2006; Ga-
mon, 2010; Rozovskaya and Roth, 2010; Tetreault
et al, 2010; Dahlmeier and Ng, 2011b), with rel-
atively less work on correcting word choice errors
(Dahlmeier and Ng, 2011a). Article and preposi-
tion errors were also the only error types featured
in the HOO 2012 shared task. Likewise, although
all error types were included in the HOO 2011
shared task, almost all participating teams dealt
with article and preposition errors only (besides
spelling and punctuation errors).
In the CoNLL-2013 shared task, it was felt
that the community is now ready to deal with
more error types, including noun number, verb
form, and subject-verb agreement, besides arti-
cles/determiners and prepositions. Table 1 shows
examples of the five error types in our shared task.
Since there are five error types in our shared task
compared to two in HOO 2012, there is a greater
chance of encountering multiple, interacting errors
in a sentence in our shared task. This increases the
complexity of our shared task relative to that of
HOO 2012. To illustrate, consider the following
sentence:
Although we have to admit some bad
effect which is brought by the new
technology, still the advantages of the
new technologies cannot be simply dis-
carded.
The noun number error effect needs to be corrected
(effect? effects). This necessitates the correction
of a subject-verb agreement error (is ? are). A
pipeline system in which corrections for subject-
verb agreement errors occur strictly before correc-
tions for noun number errors would not be able
to arrive at a fully corrected sentence for this ex-
ample. The ability to correct multiple, interacting
errors is thus necessary in our shared task. The re-
cent work of (Dahlmeier and Ng, 2012a), for ex-
ample, is designed to deal with multiple, interact-
ing errors.
Note that the essays in the training data and the
test essays naturally contain grammatical errors of
all types, beyond the five error types focused in our
shared task. In the automatically corrected essays
returned by a participating system, only correc-
tions necessary to correct errors of the five types
are made. The other errors are to be left uncor-
rected.
3 Data
This section describes the training and test data
released to each participating team in our shared
task.
3.1 Training Data
The training data provided in our shared task is
the NUCLE corpus, the NUS Corpus of Learner
English (Dahlmeier et al, 2013). As noted by
(Leacock et al, 2010), the lack of a manually an-
notated and corrected corpus of English learner
texts has been an impediment to progress in gram-
matical error correction, since it prevents com-
parative evaluations on a common benchmark test
data set. NUCLE was created precisely to fill this
void. It is a collection of 1,414 essays written
by students at the National University of Singa-
pore (NUS) who are non-native speakers of En-
glish. The essays were written in response to some
prompts, and they cover a wide range of topics,
such as environmental pollution, health care, etc.
The grammatical errors in these essays have been
hand-corrected by professional English instructors
at NUS. For each grammatical error instance, the
start and end character offsets of the erroneous text
span are marked, and the error type and the cor-
rection string are provided. Manual annotation is
carried out using a graphical user interface specif-
ically built for this purpose. The error annotations
are saved as stand-off annotations, in SGML for-
mat.
To illustrate, consider the following sentence at
the start of the first paragraph of an essay:
From past to the present, many impor-
tant innovations have surfaced.
There is an article/determiner error (past ? the
past) in this sentence. The error annotation, also
called correction or edit, in SGML format is
shown in Figure 1. start par (end par) de-
notes the paragraph ID of the start (end) of the er-
roneous text span (paragraph ID starts from 0 by
convention). start off (end off) denotes the
character offset of the start (end) of the erroneous
text span (again, character offset starts from 0 by
convention). The error tag is ArtOrDet, and the
correction string is the past.
2
Error tag Error type Example sentence Correction (edit)
ArtOrDet Article or determiner In late nineteenth century, there
was a severe air crash happening
at Miami international airport.
late ? the late
Prep Preposition Also tracking people is very
dangerous if it has been con-
trolled by bad men in a not good
purpose.
in ? for
Nn Noun number I think such powerful device
shall not be made easily avail-
able.
device ? devices
Vform Verb form However, it is an achievement as
it is an indication that our soci-
ety is progressed well and peo-
ple are living in better condi-
tions.
progressed ? progressing
SVA Subject-verb agreement People still prefers to bear the
risk and allow their pets to have
maximum freedom.
prefers ? prefer
Table 1: The five error types in our shared task.
<MISTAKE start par="0" start off="5" end par="0" end off="9">
<TYPE>ArtOrDet</TYPE>
<CORRECTION>the past</CORRECTION>
</MISTAKE>
Figure 1: An example error annotation.
The NUCLE corpus was first used in
(Dahlmeier and Ng, 2011b), and has been
publicly available for research purposes since
June 20111. All instances of grammatical errors
are annotated in NUCLE, and the errors are
classified into 27 error types (Dahlmeier et al,
2013).
To help participating teams in their prepara-
tion for the shared task, we also performed au-
tomatic preprocessing of the NUCLE corpus and
released the preprocessed form of NUCLE. The
preprocessing operations performed on the NU-
CLE essays include sentence segmentation and
word tokenization using the NLTK toolkit (Bird
et al, 2009), and part-of-speech (POS) tagging,
constituency and dependency tree parsing using
the Stanford parser (Klein and Manning, 2003;
de Marneffe et al, 2006). The error annotations,
which are originally at the character level, are
then mapped to error annotations at the word to-
ken level. Error annotations at the word token
1http://www.comp.nus.edu.sg/?nlp/corpora.html
level also facilitate scoring, as we will see in Sec-
tion 4, since our scorer operates by matching to-
kens. Note that although we released our own
preprocessed version of NUCLE, the participating
teams were however free to perform their own pre-
processing if they so preferred.
3.1.1 Revised version of NUCLE
NUCLE release version 2.3 was used in the
CoNLL-2013 shared task. In this version, 17 es-
says were removed from the first release of NU-
CLE since these essays were duplicates with mul-
tiple annotations.
In the original NUCLE corpus, there is not an
explicit preposition error type. Instead, prepo-
sition errors are part of the Wcip (wrong collo-
cation/idiom/preposition) and Rloc (local redun-
dancy) error types. The Wcip error type combines
errors concerning collocations, idioms, and prepo-
sitions together into one error type. The Rloc er-
ror type annotates extraneous words which are re-
dundant and should be removed, and they include
redundant articles, determiners, and prepositions.
3
Training data Test data
(NUCLE)
# essays 1,397 50
# sentences 57,151 1,381
# word tokens 1,161,567 29,207
Table 2: Statistics of training and test data.
In our shared task, in order to facilitate the detec-
tion and correction of article/determiner errors and
preposition errors, we performed automatic map-
ping of error types in the original NUCLE cor-
pus. The mapping relies on POS tags, constituent
parse trees, and error annotations at the word token
level. Specifically, we map the error types Wcip
and Rloc to Prep, Wci, ArtOrDet, and Rloc?.
Prepositions in the error type Wcip or Rloc are
mapped to a new error type Prep, and redundant
articles or determiners in the error type Rloc are
mapped to ArtOrDet. The remaining unaffected
Wcip errors are assigned the new error type Wci
and the remaining unaffected Rloc errors are as-
signed the new error type Rloc?. The code that
performs automatic error type mapping was also
provided to the participating teams.
The statistics of the NUCLE corpus (release 2.3
version) are shown in Table 2. The distribution
of errors among the five error types is shown in
Table 3. The newly added noun number error type
in our shared task accounts for the second highest
number of errors among the five error types. The
five error types in our shared task constitute 35%
of all grammatical errors in the training data, and
47% of all errors in the test data. These figures
support our choice of these five error types to be
the focus of our shared task, since they account
for a large percentage of all grammatical errors in
English learner essays.
While the NUCLE corpus is provided in our
shared task, participating teams are free to not use
NUCLE, or to use additional resources and tools
in building their grammatical error correction sys-
tems, as long as these resources and tools are pub-
licly available and not proprietary. For example,
participating teams are free to use the Cambridge
FCE corpus (Yannakoudakis et al, 2011; Nicholls,
2003) (the training data provided in HOO 2012
(Dale et al, 2012)) as additional training data.
Error tag Training % Test %
data data
(NUCLE)
ArtOrDet 6,658 14.8 690 19.9
Prep 2,404 5.3 312 9.0
Nn 3,779 8.4 396 11.4
Vform 1,453 3.2 122 3.5
SVA 1,527 3.4 124 3.6
5 types 15,821 35.1 1,644 47.4
all types 45,106 100.0 3,470 100.0
Table 3: Error type distribution of the training and
test data.
3.2 Test Data
25 NUS students, who are non-native speakers of
English, were recruited to write new essays to be
used as blind test data in the shared task. Each
student wrote two essays in response to the two
prompts shown in Table 4, one essay per prompt.
Essays written using the first prompt are present
in the NUCLE training data, while the second
prompt is a new prompt not used previously. As
a result, 50 test essays were collected. The statis-
tics of the test essays are shown in Table 2.
Error annotation on the test essays was carried
out by a native speaker of English who is a lecturer
at the NUS Centre for English Language Commu-
nication. The distribution of errors in the test es-
says among the five error types is shown in Ta-
ble 3. The test essays were then preprocessed in
the same manner as the NUCLE corpus. The pre-
processed test essays were released to the partici-
pating teams.
Unlike the test data used in HOO 2012 which
was proprietary and not available after the shared
task, the test essays and their error annotations in
the CoNLL-2013 shared task are freely available
after the shared task.
4 Evaluation Metric and Scorer
A grammatical error correction system is evalu-
ated by how well its proposed corrections or edits
match the gold-standard edits. An essay is first
sentence-segmented and tokenized before evalua-
tion is carried out on the essay. To illustrate, con-
sider the following tokenized sentence S written
by an English learner:
There is no a doubt, tracking system
4
ID Prompt
1 Surveillance technology such as RFID (radio-frequency identification) should not be used to
track people (e.g., human implants and RFID tags on people or products). Do you agree? Sup-
port your argument with concrete examples.
2 Population aging is a global phenomenon. Studies have shown that the current average life span
is over 65. Projections of the United Nations indicate that the population aged 60 or over in
developed and developing countries is increasing at 2% to 3% annually. Explain why rising life
expectancies can be considered both a challenge and an achievement.
Table 4: The two prompts used for the test essays.
has brought many benefits in this infor-
mation age .
The set of gold-standard edits of a human annota-
tor is g = {a doubt ? doubt, system ? systems,
has ? have}. Suppose the tokenized output sen-
tence H of a grammatical error correction system
given the above sentence is:
There is no doubt, tracking system has
brought many benefits in this informa-
tion age .
That is, the set of system edits is e = {a doubt
? doubt}. The performance of the grammatical
error correction system is measured by how well
the two sets g and e match, in the form of recall
R, precision P , and F1 measure: R = 1/3, P =
1/1, F1 = 2RP/(R + P ) = 1/2.
More generally, given a set of n sentences,
where gi is the set of gold-standard edits for sen-
tence i, and ei is the set of system edits for sen-
tence i, recall, precision, and F1 are defined as
follows:
R =
?n
i=1 |gi ? ei|?n
i=1 |gi|
(1)
P =
?n
i=1 |gi ? ei|?n
i=1 |ei|
(2)
F1 =
2?R? P
R + P
(3)
where the intersection between gi and ei for sen-
tence i is defined as
gi ? ei = {e ? ei|?g ? gi,match(g, e)} (4)
Evaluation by the HOO scorer (Dale and Kilgar-
riff, 2011) is based on computing recall, precision,
and F1 measure as defined above.
Note that there are multiple ways to specify a
set of gold-standard edits that denote the same cor-
rections. For example, in the above learner-written
sentence S, alternative but equivalent sets of gold-
standard edits are {a ? , system ? systems, has
? have}, {a ? , system has ? systems have},
etc. Given the same learner-written sentence S
and the same system output sentence H shown
above, one would expect a scorer to give the same
R,P, F1 scores regardless of which of the equiv-
alent sets of gold-standard edits is specified by an
annotator.
However, this is not the case with the HOO
scorer. This is because the HOO scorer uses
GNU wdiff2 to extract the differences between
the learner-written sentence S and the system out-
put sentence H to form a set of system edits.
Since in general there are multiple ways to spec-
ify a set of gold-standard edits that denote the
same corrections, the set of system edits com-
puted by the HOO scorer may not match the set of
gold-standard edits specified, leading to erroneous
scores. In the above example, the set of system
edits computed by the HOO scorer for S and H is
{a ? }. Given that the set of gold-standard edits
g is {a doubt ? doubt, system ? systems, has ?
have}, the scores computed by the HOO scorer are
R = P = F1 = 0, which are erroneous.
The MaxMatch (M2) scorer3 (Dahlmeier and
Ng, 2012b) was designed to overcome this limita-
tion of the HOO scorer. The key idea is that the
set of system edits automatically computed and
used in scoring should be the set that maximally
matches the set of gold-standard edits specified by
the annotator. The M2 scorer uses an efficient al-
gorithm to search for such a set of system edits
using an edit lattice. In the above example, given
S, H , and g, the M2 scorer is able to come up
with the best matching set of system edits e = {a
doubt ? doubt}, thus giving the correct scores
R = 1/3, P = 1/1, F1 = 1/2. We use the M2
2http://www.gnu.org/s/wdiff/
3http://www.comp.nus.edu.sg/?nlp/software.html
5
scorer in the CoNLL-2013 shared task.
The original M2 scorer implemented in
(Dahlmeier and Ng, 2012b) assumes that there
is one set of gold-standard edits gi for each
sentence i. However, it is often the case that
multiple alternative corrections are acceptable for
a sentence. As we allow participating teams to
submit alternative sets of gold-standard edits for
a sentence, we also extend the M2 scorer to deal
with multiple alternative sets of gold-standard
edits.
Based on Equations 1 and 2, Equation 3 can be
re-expressed as:
F1 =
2?
?n
i=1 |gi ? ei|?n
i=1 (|gi|+ |ei|)
(5)
To deal with multiple alternative sets of gold-
standard edits gi for a sentence i, the extended
M2 scorer chooses the gi that maximizes the cu-
mulative F1 score for sentences 1, . . . , i. Ties
are broken based on the following criteria: first
choose the gi that maximizes the numerator?n
i=1 |gi ? ei|, then choose the gi that minimizes
the denominator
?n
i=1 (|gi|+ |ei|), finally choose
the gi that appears first in the list of alternatives.
5 Approaches
54 teams registered to participate in the shared
task, out of which 17 teams submitted the output
of their grammatical error correction systems by
the deadline. These teams are listed in Table 5.
Each team is assigned a 3 to 4-letter team ID. In
the remainder of this paper, we will use the as-
signed team ID to refer to a participating team.
Every team submitted a system description paper
(the only exception is the SJT2 team).
Many different approaches are adopted by par-
ticipating teams in the CoNLL-2013 shared task,
and Table 6 summarizes these approaches. A com-
monly used approach in the shared task and in
grammatical error correction research in general
is to build a classifier for each error type. For ex-
ample, the classifier for noun number returns the
classes {singular, plural}, the classifier for article
returns the classes {a/an, the, }, etc. The classi-
fier for an error type may be learned from train-
ing examples encoding the surrounding context of
an error occurrence, or may be specified by deter-
ministic hand-crafted rules, or may be built using
a hybrid approach combining both machine learn-
ing and hand-crafted rules. These approaches are
denoted by M, R, and H respectively in Table 6.
The machine translation approach (denoted by
T in Table 6) to grammatical error correction
treats the task as ?translation? from bad English
to good English. Both phrase-based translation
and syntax-based translation approaches are used
by teams in the CoNLL-2013 shared task. An-
other related approach is the language modeling
approach (denoted by L in Table 6), in which
the probability of a learner sentence is compared
with the probability of a candidate corrected sen-
tence, based on a language model built from a
background corpus. The candidate correction is
chosen if it results in a corrected sentence with a
higher probability. In general, these approaches
are not mutually exclusive. For example, the
work of (Dahlmeier and Ng, 2012a; Yoshimoto et
al., 2013) includes elements of machine learning-
based classification, machine translation, and lan-
guage modeling approaches.
When different approaches are used to tackle
different error types by a system, we break down
the error types into different rows in Table 6, and
specify the approach used for each group of error
types. For instance, the HIT team uses a machine
learning approach to deal with article/determiner,
noun number, and preposition errors, and a rule-
based approach to deal with subject-verb agree-
ment and verb form errors. As such, the entry for
HIT is sub-divided into two rows, to make it clear
which particular error type is handled by which
approach.
Table 6 also shows the linguistic features used
by the participating teams, which include lexical
features (i.e., words, collocations, n-grams), parts-
of-speech (POS), constituency parses, dependency
parses, and semantic features (including semantic
role labels).
While all teams in the shared task use the NU-
CLE corpus, they are also allowed to use addi-
tional external resources (both corpora and tools)
so long as they are publicly available and not pro-
prietary. The external resources used by the teams
are also listed in Table 6.
6 Results
All submitted system output was evaluated using
the M2 scorer, based on the error annotations pro-
vided by our annotator. The recall (R), precision
(P ), and F1 measure of all teams are shown in Ta-
ble 7. The performance of the teams varies greatly,
6
Team ID Affiliation
CAMB University of Cambridge
HIT Harbin Institute of Technology
IITB Indian Institute of Technology, Bombay
KOR Korea University
NARA Nara Institute of Science and Technology
NTHU National Tsing Hua University
SAAR Saarland University
SJT1 Shanghai Jiao Tong University (Team #1)
SJT2 Shanghai Jiao Tong University (Team #2)
STAN Stanford University
STEL Stellenbosch University
SZEG University of Szeged
TILB Tilburg University
TOR University of Toronto
UAB Universitat Auto`noma de Barcelona
UIUC University of Illinois at Urbana-Champaign
UMC University of Macau
Table 5: The list of 17 participating teams.
Rank Team R P F1
1 UIUC 23.49 46.45 31.20
2 NTHU 26.35 23.80 25.01
3 HIT 16.56 35.65 22.61
4 NARA 18.62 27.39 22.17
5 UMC 17.53 28.49 21.70
6 STEL 13.33 27.00 17.85
7 SJT1 10.96 40.18 17.22
8 CAMB 10.10 39.15 16.06
9 IITB 4.99 28.18 8.48
10 STAN 4.69 25.50 7.92
11 TOR 4.81 17.67 7.56
12 KOR 3.71 43.88 6.85
13 TILB 7.24 6.25 6.71
14 SZEG 3.16 5.52 4.02
15 UAB 1.22 12.42 2.22
16 SAAR 1.10 27.69 2.11
17 SJT2 0.24 13.33 0.48
Table 7: Scores (in %) without alternative an-
swers.
from barely half a per cent to 31.20% for the top
team.
The nature of grammatical error correction is
such that multiple, different corrections are of-
ten acceptable. In order to allow the participating
teams to raise their disagreement with the original
gold-standard annotations provided by the anno-
tator, and not understate the performance of the
teams, we allow the teams to submit their pro-
posed alternative answers. This was also the prac-
tice adopted in HOO 2011 and HOO 2012. Specif-
ically, after the teams submitted their system out-
put and the error annotations on the test essays
were released, we allowed the teams to propose al-
ternative answers (gold-standard edits), to be sub-
mitted within four days after the initial error an-
notations were released. The same annotator who
provided the error annotations on the test essays
also judged the alternative answers proposed by
the teams, to ensure consistency. In all, five teams
(NTHU, STEL, TOR, UIUC, UMC) submitted al-
ternative answers.
The same submitted system output was then
evaluated using the extended M2 scorer, with the
original annotations augmented with the alterna-
tive answers. Table 8 shows the recall (R), preci-
sion (P ), and F1 measure of all teams under this
new evaluation setting.
The F1 measure of every team improves when
7
Te
am
E
rr
or
A
pp
ro
ac
h
D
es
cr
ip
ti
on
of
A
pp
ro
ac
h
L
in
gu
is
ti
c
F
ea
tu
re
s
E
xt
er
na
lR
es
ou
rc
es
C
A
M
B
A
N
P
S
V
T
fa
ct
or
ed
ph
ra
se
-b
as
ed
tr
an
sl
at
io
n
m
od
el
w
it
h
IR
S
T
la
ng
ua
ge
m
od
el
le
xi
ca
l,
P
O
S
C
am
br
id
ge
L
ea
rn
er
C
or
pu
s
H
IT
A
N
P
M
m
ax
im
um
en
tr
op
y
w
it
h
co
nfi
de
nc
e
tu
ni
ng
,
an
d
ge
ne
ti
c
al
go
-
ri
th
m
fo
r
fe
at
ur
e
se
le
ct
io
n
le
xi
ca
l,
P
O
S
,c
on
st
it
ue
nc
y
pa
rs
e,
de
pe
nd
en
cy
pa
rs
e,
se
m
an
ti
c
W
or
dN
et
,L
on
gm
an
di
ct
io
na
ry
S
V
R
ru
le
-b
as
ed
P
O
S
,d
ep
en
de
nc
y
pa
rs
e,
se
m
an
ti
c
II
T
B
A
N
M
m
ax
im
um
en
tr
op
y
le
xi
ca
l,
P
O
S
,n
ou
n
pr
op
er
ti
es
W
ik
ti
on
ar
y
S
R
ru
le
-b
as
ed
P
O
S
,c
on
st
it
ue
nc
y
pa
rs
e,
de
pe
nd
en
cy
pa
rs
e
K
O
R
A
N
P
M
m
ax
im
um
en
tr
op
y
le
xi
ca
l,
P
O
S
,h
ea
d-
m
od
ifi
er
,d
ep
en
de
nc
y
pa
rs
e
(n
on
e)
N
A
R
A
A
P
T
ph
ra
se
-b
as
ed
st
at
is
ti
ca
lm
ac
hi
ne
tr
an
sl
at
io
n
le
xi
ca
l
L
an
g-
8
N
M
ad
ap
tiv
e
re
gu
la
ri
za
ti
on
of
w
ei
gh
tv
ec
to
rs
le
xi
ca
l,
le
m
m
a,
co
ns
ti
tu
en
cy
pa
rs
e
G
ig
aw
or
d
S
V
L
tr
ee
le
t(
tr
ee
-b
as
ed
)
la
ng
ua
ge
m
od
el
le
xi
ca
l,
P
O
S
,c
on
st
it
ue
nc
y
pa
rs
e
P
en
n
T
re
eb
an
k,
G
ig
aw
or
d
N
T
H
U
A
N
P
V
L
n-
gr
am
-b
as
ed
an
d
de
pe
nd
en
cy
-b
as
ed
la
ng
ua
ge
m
od
el
le
xi
ca
l,
P
O
S
,c
on
st
it
ue
nc
y
pa
rs
e,
de
pe
nd
en
cy
pa
rs
e
G
oo
gl
e
W
eb
-1
T
S
A
A
R
A
M
m
ul
ti
-c
la
ss
S
V
M
an
d
na
iv
e
B
ay
es
le
xi
ca
l,
P
O
S
,c
on
st
it
ue
nc
y
pa
rs
e
C
M
U
P
ro
no
un
ci
ng
D
ic
ti
on
ar
y
S
R
ru
le
-b
as
ed
P
O
S
,d
ep
en
de
nc
y
pa
rs
e
S
JT
1
A
N
P
S
V
M
m
ax
im
um
en
tr
op
y
(w
it
h
L
M
po
st
-fi
lt
er
in
g)
le
xi
ca
l,
le
m
m
a,
P
O
S
,c
on
st
it
ue
nc
y
pa
rs
e,
de
pe
nd
en
cy
pa
rs
e
E
ur
op
ar
l
S
TA
N
A
N
P
S
V
H
E
ng
li
sh
R
es
ou
rc
e
G
ra
m
m
ar
(E
R
G
),
he
ad
-d
ri
ve
n
ph
ra
se
st
ru
c-
tu
re
,e
xt
en
de
d
w
it
h
ha
nd
-c
od
ed
m
al
-r
ul
es
le
xi
ca
l,
P
O
S
,c
on
st
it
ue
nc
y
pa
rs
e,
se
m
an
ti
c
E
ng
li
sh
R
es
ou
rc
e
G
ra
m
m
ar
S
T
E
L
A
N
P
S
V
T
tr
ee
-t
o-
st
ri
ng
w
it
h
G
H
K
M
tr
an
sd
uc
er
co
ns
ti
tu
en
cy
pa
rs
e
W
ik
ip
ed
ia
,W
or
dN
et
S
Z
E
G
A
N
M
m
ax
im
um
en
tr
op
y
L
F
G
,l
ex
ic
al
,c
on
st
it
ue
nc
y
pa
rs
e,
de
pe
nd
en
cy
pa
rs
e
(n
on
e)
T
IL
B
A
N
P
S
V
M
bi
na
ry
an
d
m
ul
ti
-c
la
ss
IG
T
re
e
le
xi
ca
l,
le
m
m
a,
P
O
S
G
oo
gl
e
W
eb
-1
T,
G
ig
aw
or
d
T
O
R
A
N
P
S
V
T
no
is
y
ch
an
ne
lm
od
el
in
vo
lv
in
g
tr
an
sf
or
m
at
io
n
of
si
ng
le
w
or
ds
le
xi
ca
l,
P
O
S
W
ik
ip
ed
ia
U
A
B
A
N
P
S
V
R
ru
le
-b
as
ed
le
xi
ca
l,
de
pe
nd
en
cy
pa
rs
e
To
p
25
0
un
co
un
ta
bl
e
no
un
s,
F
re
eL
in
g
m
or
ph
ol
og
ic
al
di
ct
io
na
ry
U
IU
C
A
N
P
S
V
M
A
:m
ul
ti
-c
la
ss
av
er
ag
ed
pe
rc
ep
tr
on
;o
th
er
s:
na
iv
e
B
ay
es
le
xi
ca
l,
P
O
S
,s
ha
ll
ow
pa
rs
e
G
oo
gl
e
W
eb
-1
T,
G
ig
aw
or
d
U
M
C
A
N
P
S
V
H
pi
pe
li
ne
:
ru
le
-b
as
ed
fi
lt
er
?
se
m
i-
su
pe
rv
is
ed
m
ul
ti
-c
la
ss
m
ax
im
um
en
tr
op
y
cl
as
si
fi
er
?
L
M
co
nfi
de
nc
e
sc
or
er
le
xi
ca
l,
P
O
S
,d
ep
en
de
nc
y
pa
rs
e
N
ew
s
co
rp
us
,J
M
yS
pe
ll
di
ct
io
na
ry
,G
oo
gl
e
W
eb
-1
T,
P
en
n
T
re
eb
an
k
Ta
bl
e
6:
P
ro
fi
le
of
th
e
pa
rt
ic
ip
at
in
g
te
am
s.
T
he
E
rr
or
co
lu
m
n
sh
ow
s
th
e
er
ro
r
ty
pe
,
w
he
re
ea
ch
le
tt
er
de
no
te
s
th
e
er
ro
r
ty
pe
be
gi
nn
in
g
w
it
h
th
at
in
it
ia
l
le
tt
er
.
T
he
A
pp
ro
ac
h
co
lu
m
n
sh
ow
s
th
e
ap
pr
oa
ch
ad
op
te
d
by
ea
ch
te
am
,s
om
et
im
es
br
ok
en
do
w
n
ac
co
rd
in
g
to
th
e
er
ro
r
ty
pe
:
H
de
no
te
s
a
hy
br
id
cl
as
si
fi
er
ap
pr
oa
ch
,L
de
no
te
s
a
la
ng
ua
ge
m
od
el
in
g-
ba
se
d
ap
pr
oa
ch
,M
de
no
te
s
a
m
ac
hi
ne
le
ar
ni
ng
-b
as
ed
cl
as
si
fi
er
ap
pr
oa
ch
,R
de
no
te
s
a
ru
le
-b
as
ed
cl
as
si
fi
er
(n
on
-m
ac
hi
ne
le
ar
ni
ng
)
ap
pr
oa
ch
,a
nd
T
de
no
te
s
a
m
ac
hi
ne
tr
an
sl
at
io
n
ap
pr
oa
ch
8
evaluated with alternative answers. Not surpris-
ingly, the teams which submitted alternative an-
swers tend to show the greatest improvements in
their F1 measure. Overall, the UIUC team (Ro-
zovskaya et al, 2013) achieves the best F1 mea-
sure, with a clear lead over the other teams in the
shared task, under both evaluation settings (with-
out and with alternative answers).
For future research which uses the test data of
the CoNLL-2013 shared task, we recommend that
evaluation be carried out in the setting that does
not use alternative answers, to ensure a fairer eval-
uation. This is because the scores of the teams
which submitted alternative answers tend to be
higher in a biased way when evaluated with alter-
native answers.
Rank Team R P F1
1 UIUC 31.87 62.19 42.14
2 NTHU 34.62 30.57 32.46
3 UMC 23.66 37.12 28.90
4 NARA 24.05 33.92 28.14
5 HIT 20.29 41.75 27.31
6 STEL 18.91 37.12 25.05
7 CAMB 14.19 52.11 22.30
8 SJT1 13.67 47.77 21.25
9 TOR 8.77 30.67 13.64
10 IITB 6.55 34.93 11.03
11 STAN 5.86 29.93 9.81
12 KOR 4.78 53.24 8.77
13 TILB 9.29 7.60 8.36
14 SZEG 4.07 6.67 5.06
15 UAB 1.81 17.39 3.28
16 SAAR 1.68 40.00 3.23
17 SJT2 0.33 16.67 0.64
Table 8: Scores (in %) with alternative answers.
We are also interested in the analysis of scores
of each of the five error types. To compute the
recall of an error type, we need to know the er-
ror type of each gold-standard edit, which is pro-
vided by the annotator. To compute the precision
of each error type, we need to know the error type
of each system edit, which however is not avail-
able since the submitted system output only con-
tains the corrected sentences with no indication of
the error type of the system edits.
In order to determine the error type of system
edits, we first perform POS tagging on the submit-
ted system output using the Stanford parser (Klein
andManning, 2003). We also make use of the POS
tags assigned in the preprocessed form of the test
essays. We then assign an error type to a system
edit based on the automatically determined POS
tags, as follows:
? ArtOrDet: The system edit involves a change
(insertion, deletion, or substitution) of words
tagged as article/determiner, i.e., DT or PDT.
? Prep: The system edit involves a change of
words tagged as preposition, i.e., IN or TO.
? Nn: The system edit involves a change of
words such that a word in the source string
is a singular noun (tagged as NN or NNP)
and a word in the replacement string is a plu-
ral noun (tagged as NNS or NNPS), or vice
versa. Since a word tagged as JJ (adjective)
can serve as a noun, a system edit that in-
volves a change of POS tags from JJ to one of
{NN, NNP, NNS, NNPS} or vice versa also
qualifies.
? Vform/SVA: The system edit involves a
change of words tagged as one of the verb
POS tags, i.e., VB, VBD, VBG, VBN, VBP,
and VBZ.
The verb form and subject-verb agreement error
types are grouped together into one category, since
it is difficult to automatically distinguish the two in
a reliable way.
The scores when distinguished by error type are
shown in Tables 9 and 10. Based on the F1 mea-
sure of each error type, the noun number error type
gives the highest scores, and preposition errors re-
main the most challenging error type to correct.
7 Conclusions
The CoNLL-2013 shared task saw the participa-
tion of 17 teams worldwide to evaluate their gram-
matical error correction systems on a common test
set, using a common evaluation metric and scorer.
The five error types included in the shared task
account for at least one-third to close to one-half
of all errors in English learners? essays. The best
system in the shared task achieves an F1 score of
42%, when it is scored with multiple acceptable
answers. There is still much room for improve-
ment, both in the accuracy of grammatical error
correction systems, and in the coverage of systems
to deal with a more comprehensive set of error
9
Te
am
A
rt
O
rD
et
P
re
p
N
n
V
fo
rm
/S
V
A
R
P
F
1
R
P
F
1
R
P
F
1
R
P
F
1
C
A
M
B
15
.0
7
38
.6
6
21
.6
9
3.
54
40
.7
4
6.
51
7.
58
55
.5
6
13
.3
3
8.
54
31
.8
2
13
.4
6
H
IT
24
.2
0
42
.8
2
30
.9
3
2.
89
28
.1
2
5.
25
17
.1
7
29
.6
9
21
.7
6
11
.3
8
26
.4
2
15
.9
1
II
T
B
1.
30
21
.4
3
2.
46
(n
ot
do
ne
)
9.
85
28
.6
8
14
.6
6
13
.8
2
30
.0
9
18
.9
4
K
O
R
4.
78
53
.2
3
8.
78
0.
32
4.
76
0.
60
6.
82
49
.0
9
11
.9
7
(n
ot
do
ne
)
N
A
R
A
20
.4
3
34
.0
6
25
.5
4
12
.5
4
29
.1
0
17
.5
3
16
.4
1
48
.8
7
24
.5
7
24
.8
0
14
.8
1
18
.5
4
N
T
H
U
21
.0
1
35
.8
0
26
.4
8
12
.8
6
12
.0
1
12
.4
2
45
.9
6
40
.9
0
43
.2
8
26
.8
3
12
.2
2
16
.7
9
S
A
A
R
0.
72
62
.5
0
1.
43
(n
ot
do
ne
)
(n
ot
do
ne
)
5.
28
23
.2
1
8.
61
S
JT
1
16
.8
1
47
.1
5
24
.7
9
1.
29
12
.5
0
2.
33
13
.6
4
42
.1
9
20
.6
1
2.
44
14
.6
3
4.
18
S
JT
2
0.
00
0.
00
0.
00
0.
00
0.
00
0.
00
1.
01
13
.3
3
1.
88
0.
00
0.
00
0.
00
S
TA
N
3.
91
20
.4
5
6.
57
0.
32
20
.0
0
0.
63
6.
06
29
.6
3
10
.0
6
10
.1
6
32
.0
5
15
.4
3
S
T
E
L
12
.6
1
27
.7
1
17
.3
3
9.
32
25
.6
6
13
.6
8
18
.1
8
46
.7
5
26
.1
8
12
.6
0
17
.6
1
14
.6
9
S
Z
E
G
1.
16
1.
70
1.
38
(n
ot
do
ne
)
11
.1
1
13
.6
2
12
.2
4
(n
ot
do
ne
)
T
IL
B
4.
49
4.
49
4.
49
10
.6
1
5.
07
6.
86
7.
07
21
.2
1
10
.6
1
10
.9
8
9.
57
10
.2
3
T
O
R
8.
55
25
.5
4
12
.8
1
2.
25
5.
38
3.
17
1.
77
31
.8
2
3.
35
2.
44
12
.2
4
4.
07
U
A
B
0.
00
0.
00
0.
00
0.
00
0.
00
0.
00
0.
00
0.
00
0.
00
8.
13
12
.4
2
9.
83
U
IU
C
25
.6
5
47
.8
4
33
.4
0
4.
18
26
.5
3
7.
22
38
.3
8
52
.2
3
44
.2
5
17
.8
9
38
.9
4
24
.5
1
U
M
C
21
.0
1
30
.2
7
24
.8
1
1.
93
35
.2
9
3.
66
23
.2
3
27
.9
6
25
.3
8
18
.2
9
28
.8
5
22
.3
9
Ta
bl
e
9:
S
co
re
s
(i
n
%
)
w
it
ho
ut
al
te
rn
at
iv
e
an
sw
er
s,
di
st
in
gu
is
he
d
by
er
ro
r
ty
pe
.
If
a
te
am
in
di
ca
te
s
th
at
it
s
sy
st
em
do
es
no
th
an
dl
e
a
pa
rt
ic
ul
ar
er
ro
r
ty
pe
,i
ts
en
tr
y
fo
r
th
at
er
ro
r
ty
pe
is
m
ar
ke
d
as
?(
no
td
on
e)
?.
10
Te
am
A
rt
O
rD
et
P
re
p
N
n
V
fo
rm
/S
V
A
R
P
F
1
R
P
F
1
R
P
F
1
R
P
F
1
C
A
M
B
19
.6
2
49
.8
1
28
.1
5
5.
04
50
.0
0
9.
15
9.
50
69
.0
9
16
.7
0
16
.5
2
54
.4
1
25
.3
4
H
IT
27
.4
1
47
.4
4
34
.7
4
4.
58
37
.5
0
8.
16
19
.9
5
35
.3
7
25
.5
1
17
.9
0
38
.3
2
24
.4
0
II
T
B
1.
79
28
.5
7
3.
37
(n
ot
do
ne
)
11
.9
1
35
.2
9
17
.8
1
18
.6
7
36
.8
4
24
.7
8
K
O
R
5.
95
64
.5
2
10
.9
0
1.
53
19
.0
5
2.
83
7.
52
54
.5
5
13
.2
2
(n
ot
do
ne
)
N
A
R
A
25
.7
9
43
.3
4
32
.3
4
17
.6
0
34
.5
6
23
.3
3
19
.1
5
57
.0
4
28
.6
8
34
.6
2
19
.1
0
24
.6
2
N
T
H
U
25
.3
0
42
.5
4
31
.7
3
20
.4
5
16
.1
7
18
.0
6
52
.3
5
50
.8
7
51
.6
0
43
.0
3
19
.2
2
26
.5
7
S
A
A
R
1.
04
87
.5
0
2.
06
(n
ot
do
ne
)
(n
ot
do
ne
)
8.
60
33
.9
3
13
.7
2
S
JT
1
19
.6
5
54
.0
7
28
.8
2
1.
92
15
.6
2
3.
42
16
.4
6
52
.3
4
25
.0
5
4.
05
21
.9
5
6.
84
S
JT
2
0.
00
0.
00
0.
00
0.
00
0.
00
0.
00
1.
26
16
.6
7
2.
35
0.
00
0.
00
0.
00
S
TA
N
4.
91
24
.8
1
8.
20
0.
38
20
.0
0
0.
75
6.
78
33
.3
3
11
.2
7
13
.5
1
37
.9
7
19
.9
3
S
T
E
L
16
.2
3
35
.6
9
22
.3
1
12
.8
3
29
.8
2
17
.9
4
26
.0
5
70
.0
0
37
.9
7
20
.5
2
26
.5
5
23
.1
5
S
Z
E
G
1.
50
2.
13
1.
76
(n
ot
do
ne
)
12
.8
7
15
.9
5
14
.2
5
(n
ot
do
ne
)
T
IL
B
5.
78
5.
64
5.
71
13
.9
1
5.
68
8.
07
8.
25
24
.2
6
12
.3
1
16
.3
6
12
.7
7
14
.3
4
T
O
R
13
.1
0
39
.1
3
19
.6
3
5.
97
12
.3
1
8.
04
3.
52
63
.6
4
6.
67
8.
14
35
.2
9
13
.2
4
U
A
B
0.
00
0.
00
0.
00
0.
00
0.
00
0.
00
0.
00
0.
00
0.
00
12
.6
1
17
.3
9
14
.6
2
U
IU
C
31
.9
9
59
.8
4
41
.6
9
8.
81
46
.9
4
14
.8
4
46
.8
8
70
.0
0
56
.1
5
28
.5
7
60
.7
1
38
.8
6
U
M
C
25
.8
8
36
.7
4
30
.3
7
3.
47
56
.2
5
6.
55
30
.6
1
38
.6
4
34
.1
6
26
.8
1
40
.1
3
32
.1
4
Ta
bl
e
10
:
S
co
re
s
(i
n
%
)
w
it
h
al
te
rn
at
iv
e
an
sw
er
s,
di
st
in
gu
is
he
d
by
er
ro
r
ty
pe
.
If
a
te
am
in
di
ca
te
s
th
at
it
s
sy
st
em
do
es
no
th
an
dl
e
a
pa
rt
ic
ul
ar
er
ro
r
ty
pe
,i
ts
en
tr
y
fo
r
th
at
er
ro
r
ty
pe
is
m
ar
ke
d
as
?(
no
td
on
e)
?.
11
types. The evaluation data sets and scorer used
in our shared task serve as a benchmark for future
research on grammatical error correction4.
Acknowledgments
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
References
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media.
Daniel Dahlmeier and Hwee Tou Ng. 2011a. Cor-
recting semantic collocation errors with L1-induced
paraphrases. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 107?117.
Daniel Dahlmeier and Hwee Tou Ng. 2011b. Gram-
matical error correction with alternating structure
optimization. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics, pages 915?923.
Daniel Dahlmeier and Hwee Tou Ng. 2012a. A beam-
search decoder for grammatical error correction. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
568?578.
Daniel Dahlmeier and Hwee Tou Ng. 2012b. Better
evaluation for grammatical error correction. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
568?572.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a large annotated corpus of learner
English: The NUS Corpus of Learner English. In
Proceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 22?31.
Robert Dale and Adam Kilgarriff. 2011. Helping Our
Own: The HOO 2011 pilot shared task. In Proceed-
ings of the 13th EuropeanWorkshop on Natural Lan-
guage Generation, pages 242?249.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A report on the preposition and
determiner error correction shared task. In Proceed-
ings of the 7th Workshop on the Innovative Use of
NLP for Building Educational Applications, pages
54?62.
4http://www.comp.nus.edu.sg/?nlp/conll13st.html
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the Fifth Conference on Language
Resources and Evaluation, pages 449?454.
Michael Gamon. 2010. Using mostly native data to
correct errors in learners? writing: A meta-classifier
approach. In Proceedings of the Annual Meeting of
the North American Chapter of the Association for
Computational Linguistics, pages 163?171.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineer-
ing, 12(2):115?129.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 423?430.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical
Error Detection for Language Learners. Morgan &
Claypool Publishers.
Diane Nicholls. 2003. The Cambridge Learner Cor-
pus: Error coding and analysis for lexicography and
ELT. In Proceedings of the Corpus Linguistics 2003
Conference, pages 572?581.
Alla Rozovskaya and Dan Roth. 2010. Generating
confusion sets for context-sensitive error correction.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
961?970.
Alla Rozovskaya, Kai-Wei Chang, Mark Sammons,
and Dan Roth. 2013. The University of Illinois
system in the CoNLL-2013 shared task. In Pro-
ceedings of the Seventeenth Conference on Compu-
tational Natural Language Learning: Shared Task.
Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010. Using parse features for preposition selection
and error detection. In Proceedings of the ACL 2010
Conference Short Papers, pages 353?358.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading ESOL texts. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics, pages 180?189.
Ippei Yoshimoto, Tomoya Kose, Kensuke Mitsuzawa,
Keisuke Sakaguchi, Tomoya Mizumoto, Yuta
Hayashibe, Mamoru Komachi, and Yuji Matsumoto.
2013. NAIST at 2013 CoNLL grammatical error
correction shared task. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task.
12
