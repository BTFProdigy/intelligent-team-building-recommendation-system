Proceedings of ACL-08: HLT, pages 710?718,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Using Conditional Random Fields to Extract Contexts and Answers of
Questions from Online Forums
Shilin Ding ? ? Gao Cong? ? Chin-Yew Lin? Xiaoyan Zhu?
?Department of Computer Science and Technology, Tsinghua University, Beijing, China
?Department of Computer Science, Aalborg University, Denmark
?Microsoft Research Asia, Beijing, China
dingsl@gmail.com gaocong@cs.aau.dk
cyl@microsoft.com zxy-dcs@tsinghua.edu.cn
Abstract
Online forum discussions often contain vast
amounts of questions that are the focuses of
discussions. Extracting contexts and answers
together with the questions will yield not only
a coherent forum summary but also a valu-
able QA knowledge base. In this paper, we
propose a general framework based on Con-
ditional Random Fields (CRFs) to detect the
contexts and answers of questions from forum
threads. We improve the basic framework by
Skip-chain CRFs and 2D CRFs to better ac-
commodate the features of forums for better
performance. Experimental results show that
our techniques are very promising.
1 Introduction
Forums are web virtual spaces where people can ask
questions, answer questions and participate in dis-
cussions. The availability of vast amounts of thread
discussions in forums has promoted increasing in-
terests in knowledge acquisition and summarization
for forum threads. Forum thread usually consists
of an initiating post and a number of reply posts.
The initiating post usually contains several ques-
tions and the reply posts usually contain answers to
the questions and perhaps new questions. Forum
participants are not physically co-present, and thus
reply may not happen immediately after questions
are posted. The asynchronous nature and multi-
participants make multiple questions and answers
?This work was done when Shilin Ding was a visiting stu-
dent at the Microsoft Research Asia
?This work was done when Gao Cong worked as a re-
searcher at the Microsoft Research Asia.
<context id=1>S1: Hi I am looking for a pet friendly
hotel in Hong Kong because all of my family is go-
ing there for vacation. S2: my family has 2 sons
and a dog.</context> <question id=1>S3: Is there
any recommended hotel near Sheung Wan or Tsing
Sha Tsui?</question><context id=2,3>S4: We also
plan to go shopping in Causeway Bay.</context>
<question id=2>S5: What?s the traffic situa-
tion around those commercial areas?</question>
<question id=3>S6: Is it necessary to take a
taxi?</question>. S7: Any information would be ap-
preciated.
<answer qid=1>S8: The Comfort Lodge near
Kowloon Park allows pet as I know, and usually fits
well within normal budget. S9: It is also conve-
niently located, nearby the Kowloon railway station
and subway.</answer>
<answer qid=2,3> S10: It?s very crowd in those ar-
eas, so I recommend MTR in Causeway Bay because
it is cheap to take you around </answer>
Figure 1: An example thread with question-context-
answer annotated
interweaved together, which makes it more difficult
to summarize.
In this paper, we address the problem of detecting
the contexts and answers from forum threads for the
questions identified in the same threads. Figure 1
gives an example of a forum thread with questions,
contexts and answers annotated. It contains three
question sentences, S3, S5 and S6. Sentences S1
and S2 are contexts of question 1 (S3). Sentence S4
is the context of questions 2 and 3, but not 1. Sen-
tence S8 is the answer to question 3. (S4-S5-S10) is
one example of question-context-answer triple that
we want to detect in the thread. As shown in the ex-
ample, a forum question usually requires contextual
information to provide background or constraints.
710
Moreover, it sometimes needs contextual informa-
tion to provide explicit link to its answers. For
example, S8 is an answer of question 1, but they
cannot be linked with any common word. Instead,
S8 shares word pet with S1, which is a context of
question 1, and thus S8 could be linked with ques-
tion 1 through S1. We call contextual information
the context of a question in this paper.
A summary of forum threads in the form of
question-context-answer can not only highlight the
main content, but also provide a user-friendly orga-
nization of threads, which will make the access to
forum information easier.
Another motivation of detecting contexts and an-
swers of the questions in forum threads is that it
could be used to enrich the knowledge base of
community-based question and answering (CQA)
services such as Live QnA and Yahoo! Answers,
where context is comparable with the question de-
scription while question corresponds to the question
title. For example, there were about 700,000 ques-
tions in the Yahoo! Answers travel category as of
January 2008. We extracted about 3,000,000 travel
related questions from six online travel forums. One
would expect that a CQA service with large QA data
will attract more users to the service. To enrich the
knowledge base, not only the answers, but also the
contexts are critical; otherwise the answer to a ques-
tion such as How much is the taxi would be useless
without context in the database.
However, it is challenging to detecting contexts
and answers for questions in forum threads. We as-
sume the questions have been identified in a forum
thread using the approach in (Cong et al, 2008).
Although identifying questions in a forum thread is
also nontrivial, it is beyond the focus of this paper.
First, detecting contexts of a question is important
and non-trivial. We found that 74% of questions in
our corpus, which contain 1,064 questions from 579
forum threads about travel, need contexts. However,
relative position information is far from adequate to
solve the problem. For example, in our corpus 63%
of sentences preceding questions are contexts and
they only represent 34% of all correct contexts. To
effectively detect contexts, the dependency between
sentences is important. For example in Figure 1,
both S1 and S2 are contexts of question 1. S1 could
be labeled as context based on word similarity, but it
is not easy to link S2 with the question directly. S1
and S2 are linked by the common word family, and
thus S2 can be linked with question 1 through S1.
The challenge here is how to model and utilize the
dependency for context detection.
Second, it is difficult to link answers with ques-
tions. In forums, multiple questions and answers
can be discussed in parallel and are interweaved to-
gether while the reply relationship between posts is
usually unavailable. To detect answers, we need to
handle two kinds of dependencies. One is the depen-
dency relationship between contexts and answers,
which should be leveraged especially when ques-
tions alone do not provide sufficient information to
find answers; the other is the dependency between
answer candidates (similar to sentence dependency
described above). The challenge is how to model
and utilize these two kinds of dependencies.
In this paper we propose a novel approach for de-
tecting contexts and answers of the questions in fo-
rum threads. To our knowledge this is the first work
on this.We make the following contributions:
First, we employ Linear Conditional Random
Fields (CRFs) to identify contexts and answers,
which can capture the relationships between con-
tiguous sentences.
Second, we also found that context is very im-
portant for answer detection. To capture the depen-
dency between contexts and answers, we introduce
Skip-chain CRF model for answer detection. We
also extend the basic model to 2D CRFs to model
dependency between contiguous questions in a fo-
rum thread for context and answer identification.
Finally, we conducted experiments on forum data.
Experimental results show that 1) Linear CRFs out-
perform SVM and decision tree in both context
and answer detection; 2) Skip-chain CRFs outper-
form Linear CRFs for answer finding, which demon-
strates that context improves answer finding; 3)
2D CRF model improves the performance of Linear
CRFs and the combination of 2D CRFs and Skip-
chain CRFs achieves better performance for context
detection.
The rest of this paper is organized as follows:
The next section discusses related work. Section 3
presents the proposed techniques. We evaluate our
techniques in Section 4. Section 5 concludes this
paper and discusses future work.
711
2 Related Work
There is some research on summarizing discussion
threads and emails. Zhou and Hovy (2005) seg-
mented internet relay chat, clustered segments into
subtopics, and identified responding segments of
the first segment in each sub-topic by assuming
the first segment to be focus. In (Nenkova and
Bagga, 2003; Wan and McKeown, 2004; Rambow
et al, 2004), email summaries were organized by
extracting overview sentences as discussion issues.
Carenini et al(2007) leveraged both quotation re-
lation and clue words for email summarization. In
contrast, given a forum thread, we extract questions,
their contexts, and their answers as summaries.
Shrestha and McKeown (2004)?s work on email
summarization is closer to our work. They used
RIPPER as a classifier to detect interrogative ques-
tions and their answers and used the resulting ques-
tion and answer pairs as summaries. However, it did
not consider contexts of questions and dependency
between answer sentences.
We also note the existing work on extracting
knowledge from discussion threads. Huang et
al.(2007) used SVM to extract input-reply pairs from
forums for chatbot knowledge. Feng et al (2006a)
used cosine similarity to match students? query with
reply posts for discussion-bot. Feng et al (2006b)
identified the most important message in online
classroom discussion board. Our problem is quite
different from the above work.
Detecting context for question in forums is related
to the context detection problem raised in the QA
roadmap paper commissioned by ARDA (Burger et
al., 2006). To our knowledge, none of the previous
work addresses the problem of context detection.
The method of finding follow-up questions (Yang
et al, 2006) from TREC context track could be
adapted for context detection. However, the follow-
up relationship is limited between questions while
context is not. In our other work (Cong et al, 2008),
we proposed a supervised approach for question de-
tection and an unsupervised approach for answer de-
tection without considering context detection.
Extensive research has been done in question-
answering, e.g. (Berger et al, 2000; Jeon et al,
2005; Cui et al, 2005; Harabagiu and Hickl, 2006;
Dang et al, 2007). They mainly focus on con-
structing answer for certain types of question from a
large document collection, and usually apply sophis-
ticated linguistic analysis to both questions and the
documents in the collection. Soricut and Brill (2006)
used statistical translation model to find the appro-
priate answers from their QA pair collections from
FAQ pages for the posted question. In our scenario,
we not only need to find answers for various types
of questions in forum threads but also their contexts.
3 Context and Answer Detection
A question is a linguistic expression used by a ques-
tioner to request information in the form of an an-
swer. The sentence containing request focus is
called question. Context are the sentences contain-
ing constraints or background information to the
question, while answer are that provide solutions. In
this paper, we use sentences as the detection segment
though it is applicable to other kinds of segments.
Given a thread and a set of m detected questions
{Qi}mi=1, our task is to find the contexts and an-
swers for each question. We first discuss using Lin-
ear CRFs for context and answer detection, and then
extend the basic framework to Skip-chain CRFs and
2D CRFs to better model our problem. Finally, we
will briefly introduce CRF models and the features
that we used for CRF model.
3.1 Using Linear CRFs
For ease of presentation, we focus on detecting con-
texts using Linear CRFs. The model could be easily
extended to answer detection.
Context detection. As discussed in Introduction
that context detection cannot be trivially solved by
position information (See Section 4.2 for details),
and dependency between sentences is important for
context detection. Recall that in Figure 1, S2 could
be labeled as context of Q1 if we consider the de-
pendency between S2 and S1, and that between S1
and Q1, while it is difficult to establish connection
between S2 and Q1 without S1. Table 1 shows that
the correlation between the labels of contiguous sen-
tences is significant. In other words, when a sen-
tence Yt?s previous Yt?1 is not a context (Yt?1 6= C)
then it is very likely that Yt (i.e. Yt 6= C) is also not a
context. It is clear that the candidate contexts are not
independent and there are strong dependency rela-
712
Contiguous sentences yt = C yt 6= C
yt?1 = C 901 1,081
yt?1 6= C 1,081 47,190
Table 1: Contingency table(?2 = 9,386,p-value<0.001)
tionships between contiguous sentences in a thread.
Therefore, a desirable model should be able to cap-
ture the dependency.
The context detection can be modeled as a clas-
sification problem. Traditional classification tools,
e.g. SVM, can be employed, where each pair of
question and candidate context will be treated as an
instance. However, they cannot capture the depen-
dency relationship between sentences.
To this end, we proposed a general framework to
detect contexts and answers based on Conditional
Random Fields (Lafferty et al, 2001) (CRFs) which
are able to model the sequential dependencies be-
tween contiguous nodes. A CRF is an undirected
graphical model G of the conditional distribution
P (Y|X). Y are the random variables over the la-
bels of the nodes that are globally conditioned on X,
which are the random variables of the observations.
(See Section 3.4 for more about CRFs)
Linear CRF model has been successfully applied
in NLP and text mining tasks (McCallum and Li,
2003; Sha and Pereira, 2003). However, our prob-
lem cannot be modeled with Linear CRFs in the
same way as other NLP tasks, where one node has a
unique label. In our problem, each node (sentence)
might have multiple labels since one sentence could
be the context of multiple questions in a thread.
Thus, it is difficult to find a solution to tag context
sentences for all questions in a thread in single pass.
Here we assume that questions in a given thread
are independent and are found, and then we can
label a thread with m questions one-by-one in m-
passes. In each pass, one question Qi is selected
as focus and each other sentence in the thread will
be labeled as context C of Qi or not using Linear
CRF model. The graphical representations of Lin-
ear CRFs is shown in Figure2(a). The linear-chain
edges can capture the dependency between two con-
tiguous nodes. The observation sequence x = <x1,
x2,...,xt>, where t is the number of sentences in a
thread, represents predictors (to be described in Sec-
tion 3.5), and the tag sequence y=<y1,...,yt>, where
yi ? {C,P}, determines whether a sentence is plain
text P or context C of question Qi.
Answer detection. Answers usually appear in the
posts after the post containing the question. There
are also strong dependencies between contiguous
answer segments. Thus, position and similarity in-
formation alone are not adequate here. To cope
with the dependency between contiguous answer
segments, Linear CRFs model are employed as in
context detection.
3.2 Leveraging Context for Answer Detection
Using Skip-chain CRFs
We observed in our corpus 74% questions lack con-
straints or background information which are very
useful to link question and answers as discussed in
Introduction. Therefore, contexts should be lever-
aged to detect answers. The Linear CRF model can
capture the dependency between contiguous sen-
tences. However, it cannot capture the long distance
dependency between contexts and answers.
One straightforward method of leveraging context
is to detect contexts and answers in two phases, i.e.
to first identify contexts, and then label answers us-
ing both the context and question information (e.g.
the similarity between context and answer can be
used as features in CRFs). The two-phase proce-
dure, however, still cannot capture the non-local de-
pendency between contexts and answers in a thread.
To model the long distance dependency between
contexts and answers, we will use Skip-chain CRF
model to detect context and answer together. Skip-
chain CRF model is applied for entity extraction
and meeting summarization (Sutton and McCallum,
2006; Galley, 2006). The graphical representation
of a Skip-chain CRF given in Figure2(b) consists
of two types of edges: linear-chain (yt?1 to yt) and
skip-chain edges (yi to yj).
Ideally, the skip-chain edges will establish the
connection between candidate pairs with high prob-
ability of being context and answer of a question.
To introduce skip-chain edges between any pairs of
non-contiguous sentences will be computationally
expensive, and also introduce noise. To make the
cardinality and number of cliques in the graph man-
ageable and also eliminate noisy edges, we would
like to generate edges only for sentence pairs with
high possibility of being context and answer. This is
713
(a) Linear CRFs (b) Skip-chain CRFs (c) 2D CRFs
Figure 2: CRF Models
Skip-Chain yv = A yv 6= A
yu = C 4,105 5,314
yu 6= C 3,744 9,740
Table 2: Contingence table(?2=615.8,p-value < 0.001)
achieved as follows. Given a question Qi in post Pj
of a thread with n posts, its contexts usually occur
within post Pj or before Pj while answers appear in
the posts after Pj . We will establish an edge between
each candidate answer v and one condidate context
in {Pk}jk=1 such that they have the highest possibil-
ity of being a context-answer pair of question Qi:
u = argmax
u?{Pk}jk=1
sim(xu, Qi).sim(xv, {xu, Qi})
here, we use the product of sim(xu, Qi) and
sim(xv, {xu, Qi} to estimate the possibility of be-
ing a context-answer pair for (u, v) , where sim(?, ?)
is the semantic similarity calculated on WordNet as
described in Section 3.5. Table 2 shows that yu and
yv in the skip chain generated by our heuristics in-
fluence each other significantly.
Skip-chain CRFs improve the performance of
answer detection due to the introduced skip-chain
edges that represent the joint probability conditioned
on the question, which is exploited by skip-chain
feature function: f(yu, yv, Qi,x).
3.3 Using 2D CRF Model
Both Linear CRFs and Skip-chain CRFs label the
contexts and answers for each question in separate
passes by assuming that questions in a thread are in-
dependent. Actually the assumption does not hold
in many cases. Let us look at an example. As in Fig-
ure 1, sentence S10 is an answer for both question
Q2 and Q3. S10 could be recognized as the answer
of Q2 due to the shared word areas and Causeway
bay (in Q2?s context, S4), but there is no direct re-
lation between Q3 and S10. To label S10, we need
consider the dependency relation between Q2 and
Q3. In other words, the question-answer relation be-
tween Q3 and S10 can be captured by a joint mod-
eling of the dependency among S10, Q2 and Q3.
The labels of the same sentence for two contigu-
ous questions in a thread would be conditioned on
the dependency relationship between the questions.
Such a dependency cannot be captured by both Lin-
ear CRFs and Skip-chain CRFs.
To capture the dependency between the contigu-
ous questions, we employ 2D CRFs to help context
and answer detection. 2D CRF model is used in
(Zhu et al, 2005) to model the neighborhood de-
pendency in blocks within a web page. As shown
in Figure2(c), 2D CRF models the labeling task for
all questions in a thread. For each thread, there are
m rows in the grid, where the ith row corresponds
to one pass of Linear CRF model (or Skip-chain
model) which labels contexts and answers for ques-
tion Qi. The vertical edges in the figure represent
the joint probability conditioned on the contiguous
questions, which will be exploited by 2D feature
function: f(yi,j , yi+1,j , Qi, Qi+1,x). Thus, the in-
formation generated in single CRF chain could be
propagated over the whole grid. In this way, context
and answer detection for all questions in the thread
could be modeled together.
3.4 Conditional Random Fields (CRFs)
The Linear, Skip-Chain and 2D CRFs can be gen-
eralized as pairwise CRFs, which have two kinds of
cliques in graph G: 1) node yt and 2) edge (yu, yv).
The joint probability is defined as:
p(y|x)= 1Z(x) exp
{?
k,t
?kfk(yt,x)+
?
k,t
?kgk(yu, yv,x)
}
714
where Z(x) is the normalization factor, fk is the
feature on nodes, gk is on edges between u and v,
and ?k and ?k are parameters.
Linear CRFs are based on the first order Markov
assumption that the contiguous nodes are dependent.
The pairwise edges in Skip-chain CRFs represent
the long distance dependency between the skipped
nodes, while the ones in 2D CRFs represent the de-
pendency between the neighboring nodes.
Inference and Parameter Estimation. For Linear
CRFs, dynamic programming is used to compute the
maximum a posteriori (MAP) of y given x. How-
ever, for more complicated graphs with cycles, ex-
act inference needs the junction tree representation
of the original graph and the algorithm is exponen-
tial to the treewidth. For fast inference, loopy Belief
Propagation (Pearl, 1988) is implemented.
Given the training Data D = {x(i),y(i)}ni=1, the
parameter estimation is to determine the parame-
ters based on maximizing the log-likelihood L? =?n
i=1 log p(y(i)|x(i)). In Linear CRF model, dy-
namic programming and L-BFGS (limited memory
Broyden-Fletcher-Goldfarb-Shanno) can be used to
optimize objective function L?, while for compli-
cated CRFs, Loopy BP are used instead to calculate
the marginal probability.
3.5 Features used in CRF models
The main features used in Linear CRF models for
context detection are listed in Table 3.
The similarity feature is to capture the word sim-
ilarity and semantic similarity between candidate
contexts and answers. The word similarity is based
on cosine similarity of TF/IDF weighted vectors.
The semantic similarity between words is computed
based on Wu and Palmer?s measure (Wu and Palmer,
1994) using WordNet (Fellbaum, 1998).1 The simi-
larity between contiguous sentences will be used to
capture the dependency for CRFs. In addition, to
bridge the lexical gaps between question and con-
text, we learned top-3 context terms for each ques-
tion term from 300,000 question-description pairs
obtained from Yahoo! Answers using mutual infor-
mation (Berger et al, 2000) ( question description
in Yahoo! Answers is comparable to contexts in fo-
1The semantic similarity between sentences is calculated as
in (Yang et al, 2006).
Similarity features:
? Cosine similarity with the question
? Similarity with the question using WordNet
? Cosine similarity between contiguous sentences
? Similarity between contiguous sentences using WordNet
? Cosine similarity with the expanded question using the lexical
matching words
Structural features:
? The relative position to current question
? Is its author the same with that of the question?
? Is it in the same paragraph with its previous sentence?
Discourse and lexical features:
? The number of Pronouns in the question
? The presence of fillers, fluency devices (e.g. ?uh?, ?ok?)
? The presence of acknowledgment tokens
? The number of non-stopwords
? Whether the question has a noun or not?
? Whether the question has a verb or not?
Table 3: Features for Linear CRFs. Unless otherwise
mentioned, we refer to features of the sentence whose la-
bel to be predicted
rums), and then use them to expand question and
compute cosine similarity.
The structural features of forums provide strong
clues for contexts. For example, contexts of a ques-
tion usually occur in the post containing the question
or preceding posts.
We extracted the discourse features from a ques-
tion, such as the number of pronouns in the question.
A more useful feature would be to find the entity in
surrounding sentences referred by a pronoun. We
tried GATE (Cunningham et al, 2002) for anaphora
resolution of the pronouns in questions, but the per-
formance became worse with the feature, which is
probably due to the difficulty of anaphora resolution
in forum discourse. We also observed that questions
often need context if the question do not contain a
noun or a verb.
In addition, we use similarity features between
skip-chain sentences for Skip-chain CRFs and simi-
larity features between questions for 2D CRFs.
4 Experiments
4.1 Experimental setup
Corpus. We obtained about 1 million threads
from TripAdvisor forum; we randomly selected 591
threads and removed 22 threads which has more than
40 sentences and 6 questions; the remaining 579 fo-
rum threads form our corpus 2. Each thread in our
2TripAdvisor (http://www.tripadvisor.com/ForumHome) is
one of the most popular travel forums; the list of 579 urls is
715
Model Prec(%) Rec(%) F1(%)
Context Detection
SVM 75.27 68.80 71.32
C4.5 70.16 64.30 67.21
L-CRF 75.75 72.84 74.45
Answer Detection
SVM 73.31 47.35 57.52
C4.5 65.36 46.55 54.37
L-CRF 63.92 58.74 61.22
Table 4: Context and Answer Detection
corpus contains at least two posts and on average
each thread consists of 3.87 posts. Two annotators
were asked to tag questions, their contexts, and an-
swers in each thread. The kappa statistic for identi-
fying question is 0.96, for linking context and ques-
tion given a question is 0.75, and for linking answer
and question given a question is 0.69. We conducted
experiments on both the union and intersection of
the two annotated data. The experimental results on
both data are qualitatively comparable. We only re-
port results on union data due to space limitation.
The union data contains 1,064 questions, 1,458 con-
texts and 3,534 answers.
Metrics. We calculated precision, recall,
and F1-score for all tasks. All the experimental
results are obtained through the average of 5 trials
of 5-fold cross validation.
4.2 Experimental results
Linear CRFs for Context and Answer Detection.
This experiment is to evaluate Linear CRF model
(Section 3.1) for context and answer detection by
comparing with SVM and C4.5(Quinlan, 1993). For
SVM, we use SVMlight(Joachims, 1999). We tried
linear, polynomial and RBF kernels and report the
results on polynomial kernel using default param-
eters since it performs the best in the experiment.
SVM and C4.5 use the same set of features as Lin-
ear CRFs. As shown in Table 4, Linear CRF model
outperforms SVM and C4.5 for both context and an-
swer detection. The main reason for the improve-
ment is that CRF models can capture the sequen-
tial dependency between segments in forums as dis-
cussed in Section 3.1.
given in http://homepages.inf.ed.ac.uk/gcong/acl08/; Removing
the 22 long threads can greatly reduce the training and test time.
position Prec(%) Rec(%) F1(%)
Context Detection
Previous One 63.69 34.29 44.58
Previous All 43.48 76.41 55.42
Anwer Detection
Following One 66.48 19.98 30.72
Following All 31.99 100 48.48
Table 5: Using position information for detection
Context Prec(%) Rec(%) F1(%)
No context 63.92 58.74 61.22
Prev. sentence 61.41 62.50 61.84
Real context 63.54 66.40 64.94
L-CRF+context 65.51 63.13 64.06
Table 6: Contextual Information for Answer Detection.
Prev. sentence uses one previous sentence of the current
question as context. RealContext uses the context anno-
tated by experts. L-CRF+context uses the context found
by Linear CRFs
We next report a baseline of context detection
using previous sentences in the same post with its
question since contexts often occur in the question
post or preceding posts. Similarly, we report a base-
line of answer detecting using following segments of
a question as answers. The results given in Table 5
show that location information is far from adequate
to detect contexts and answers.
The usefulness of contexts. This experiment is to
evaluate the usefulness of contexts in answer de-
tection, by adding the similarity between the con-
text (obtained with different methods) and candi-
date answer as an extra feature for CRFs. Table 6
shows the impact of context on answer detection
using Linear CRFs. Linear CRFs with contextual
information perform better than those without con-
text. L-CRF+context is close to that using real con-
text, while it is better than CRFs using the previous
sentence as context. The results clearly shows that
contextual information greatly improves the perfor-
mance of answer detection.
Improved Models. This experiment is to evaluate
the effectiveness of Skip-Chain CRFs (Section 3.2)
and 2D CRFs (Section 3.3) for our tasks. The results
are given in Table 7 and Table 8.
In context detection, Skip-Chain CRFs have simi-
716
Model Prec(%) Rec(%) F1(%)
L-CRF+Context 75.75 72.84 74.45
Skip-chain 74.18 74.90 74.42
2D 75.92 76.54 76.41
2D+Skip-chain 76.27 78.25 77.34
Table 7: Skip-chain and 2D CRFs for context detection
lar results as Linear CRFs, i.e. the inter-dependency
captured by the skip chains generated using the
heuristics in Section 3.2 does not improve the con-
text detection. The performance of Linear CRFs is
improved in 2D CRFs (by 2%) and 2D+Skip-chain
CRFs (by 3%) since they capture the dependency be-
tween contiguous questions.
In answer detection, as expected, Skip-chain
CRFs outperform L-CRF+context since Skip-chain
CRFs can model the inter-dependency between con-
texts and answers while in L-CRF+context the con-
text can only be reflected by the features on the ob-
servations. We also observed that 2D CRFs improve
the performance of L-CRF+context due to the de-
pendency between contiguous questions. In contrast
with our expectation, the 2D+Skip-chain CRFs does
not improve Skip-chain CRFs in terms of answer de-
tection. The possible reason could be that the struc-
ture of the graph is very complicated and too many
parameters need to be learned on our training data.
Evaluating Features. We also evaluated the con-
tributions of each category of features in Table 3
to context detection. We found that similarity fea-
tures are the most important and structural feature
the next. We also observed the same trend for an-
swer detection. We omit the details here due to space
limitation.
As a summary, 1) our CRF model outperforms
SVM and C4.5 for both context and answer detec-
tions; 2) context is very useful in answer detection;
3) the Skip-chain CRF method is effective in lever-
aging context for answer detection; and 4) 2D CRF
model improves the performance of Linear CRFs for
both context and answer detection.
5 Discussions and Conclusions
We presented a new approach to detecting contexts
and answers for questions in forums with good per-
formance. We next discuss our experience not cov-
ered by the experiments, and future work.
Model Prec(%) Rec(%) F1(%)
L-CRF+context 65.51 63.13 64.06
Skip-chain 67.59 71.06 69.40
2D 65.77 68.17 67.34
2D+Skip-chain 66.90 70.56 68.89
Table 8: Skip-chain and 2D CRFs for answer detection
Since contexts of questions are largely unexplored
in previous work, we analyze the contexts in our
corpus and classify them into three categories: 1)
context contains the main content of question while
question contains no constraint, e.g. ?i will visit NY at
Oct, looking for a cheap hotel but convenient. Any good
suggestion? ?; 2) contexts explain or clarify part of
the question, such as a definite noun phrase, e.g. ?We
are going on the Taste of Paris. Does anyone know if it is
advisable to take a suitcase with us on the tour., where
the first sentence is to describe the tour; and 3) con-
texts provide constraint or background for question
that is syntactically complete, e.g. ?We are inter-
ested in visiting the Great Wall(and flying from London).
Can anyone recommend a tour operator.? In our corpus,
about 26% questions do not need context, 12% ques-
tions need Type 1 context, 32% need Type 2 context
and 30% Type 3. We found that our techniques often
do not perform well on Type 3 questions.
We observed that factoid questions, one of fo-
cuses in the TREC QA community, take less than
10% question in our corpus. It would be interesting
to revisit QA techniques to process forum data.
Other future work includes: 1) to summarize mul-
tiple threads using the triples extracted from indi-
vidual threads. This could be done by clustering
question-context-answer triples; 2) to use the tradi-
tional text summarization techniques to summarize
the multiple answer segments; 3) to integrate the
Question Answering techniques as features of our
framework to further improve answer finding; 4) to
reformulate questions using its context to generate
more user-friendly questions for CQA services; and
5) to evaluate our techniques on more online forums
in various domains.
Acknowledgments
We thank the anonymous reviewers for their detailed
comments, and Ming Zhou and Young-In Song for
their valuable suggestions in preparing the paper.
717
References
A. Berger, R. Caruana, D. Cohn, D. Freitag, and V. Mit-
tal. 2000. Bridging the lexical chasm: statistical ap-
proaches to answer-finding. In Proceedings of SIGIR.
J. Burger, C. Cardie, V. Chaudhri, R. Gaizauskas,
S. Harabagiu, D. Israel, C. Jacquemin, C. Lin,
S. Maiorano, G. Miller, D. Moldovan, B. Ogden,
J. Prager, E. Riloff, A. Singhal, R. Shrihari, T. Strza-
lkowski16, E. Voorhees, and R. Weishedel. 2006. Is-
sues, tasks and program structures to roadmap research
in question and answering (qna). ARAD: Advanced
Research and Development Activity (US).
G. Carenini, R. Ng, and X. Zhou. 2007. Summarizing
email conversations with clue words. In Proceedings
of WWW.
G. Cong, L. Wang, C.Y. Lin, Y.I. Song, and Y. Sun. 2008.
Finding question-answer pairs from online forums. In
Proceedings of SIGIR.
H. Cui, R. Sun, K. Li, M. Kan, and T. Chua. 2005. Ques-
tion answering passage retrieval using dependency re-
lations. In Proceedings of SIGIR.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. Gate: A framework and graphical
development environment for robust nlp tools and ap-
plications. In Proceedings of ACL.
H. Dang, J. Lin, and D. Kelly. 2007. Overview of the
trec 2007 question answering track. In Proceedings of
TREC.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database (Language, Speech, and Communica-
tion). The MIT Press, May.
D. Feng, E. Shaw, J. Kim, and E. Hovy. 2006a. An intel-
ligent discussion-bot for answering student queries in
threaded discussions. In Proceedings of IUI.
D. Feng, E. Shaw, J. Kim, and E. Hovy. 2006b. Learning
to detect conversation focus of threaded discussions.
In Proceedings of HLT-NAACL.
M. Galley. 2006. A skip-chain conditional random field
for ranking meeting utterances by importance. In Pro-
ceedings of EMNLP.
S. Harabagiu and A. Hickl. 2006. Methods for using tex-
tual entailment in open-domain question answering.
In Proceedings of ACL.
J. Huang, M. Zhou, and D. Yang. 2007. Extracting chat-
bot knowledge from online discussion forums. In Pro-
ceedings of IJCAI.
J. Jeon, W. Croft, and J. Lee. 2005. Finding similar
questions in large question and answer archives. In
Proceedings of CIKM.
T. Joachims. 1999. Making large-scale support vector
machine learning practical. MIT Press, Cambridge,
MA, USA.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML.
A. McCallum and W. Li. 2003. Early results for named
entity recognition with conditional random fields, fea-
ture induction and web-enhanced lexicons. In Pro-
ceedings of CoNLL-2003.
A. Nenkova and A. Bagga. 2003. Facilitating email
thread access by extractive summary generation. In
Proceedings of RANLP.
J. Pearl. 1988. Probabilistic reasoning in intelligent sys-
tems: networks of plausible inference. Morgan Kauf-
mann Publishers Inc., San Francisco, CA, USA.
J. Quinlan. 1993. C4.5: programs for machine learn-
ing. Morgan Kaufmann Publishers Inc., San Fran-
cisco, CA, USA.
O. Rambow, L. Shrestha, J. Chen, and C. Lauridsen.
2004. Summarizing email threads. In Proceedings of
HLT-NAACL.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. In HLT-NAACL.
L. Shrestha and K. McKeown. 2004. Detection of
question-answer pairs in email conversations. In Pro-
ceedings of COLING.
R. Soricut and E. Brill. 2006. Automatic question an-
swering using the web: Beyond the Factoid. Informa-
tion Retrieval, 9(2):191?206.
C. Sutton and A. McCallum. 2006. An introduction to
conditional random fields for relational learning. In
Lise Getoor and Ben Taskar, editors, Introduction to
Statistical Relational Learning. MIT Press. To appear.
S. Wan and K. McKeown. 2004. Generating overview
summaries of ongoing email thread discussions. In
Proceedings of COLING.
Z. Wu and M. S. Palmer. 1994. Verb semantics and lexi-
cal selection. In Proceedings of ACL.
F. Yang, J. Feng, and G. Fabbrizio. 2006. A data
driven approach to relevancy recognition for contex-
tual question answering. In Proceedings of the Inter-
active Question Answering Workshop at HLT-NAACL
2006.
L. Zhou and E. Hovy. 2005. Digesting virtual ?geek?
culture: The summarization of technical internet relay
chats. In Proceedings of ACL.
J. Zhu, Z. Nie, J. Wen, B. Zhang, and W. Ma. 2005. 2d
conditional random fields for web information extrac-
tion. In Proceedings of ICML.
718
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 167?176,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Better Binarization for the CKY Parsing
Xinying Song? ? Shilin Ding? ? Chin-Yew Lin?
?MOE-MS Key Laboratory of NLP and Speech, Harbin Institute of Technology, Harbin, China
?Department of Statistics, University of Wisconsin-Madison, Madison, USA
?Microsoft Research Asia, Beijing, China
xysong@mtlab.hit.edu.cn dingsl@gmail.com cyl@microsoft.com
Abstract
We present a study on how grammar binariza-
tion empirically affects the efficiency of the
CKY parsing. We argue that binarizations af-
fect parsing efficiency primarily by affecting
the number of incomplete constituents gener-
ated, and the effectiveness of binarization also
depends on the nature of the input. We pro-
pose a novel binarization method utilizing rich
information learnt from training corpus. Ex-
perimental results not only show that differ-
ent binarizations have great impacts on pars-
ing efficiency, but also confirm that our learnt
binarization outperforms other existing meth-
ods. Furthermore we show that it is feasible to
combine existing parsing speed-up techniques
with our binarization to achieve even better
performance.
1 Introduction
Binarization, which transforms an n-ary grammar
into an equivalent binary grammar, is essential for
achieving an O(n3) time complexity in the context-
free grammar parsing. O(n3) tabular parsing al-
gorithms, such as the CKY algorithm (Kasami,
1965; Younger, 1967), the GHR parser (Graham
et al, 1980), the Earley algorithm (Earley, 1970) and
the chart parsing algorithm (Kay, 1980; Klein and
Manning, 2001) all convert their grammars into bi-
nary branching forms, either explicitly or implicitly
(Charniak et al, 1998).
In fact, the number of all possible binarizations
of a production with n + 1 symbols on its right
?This work was done when Xinying Song and Shilin Ding
were visiting students at Microsoft Research Asia.
hand side is known to be the nth Catalan Number
Cn = 1n+1
(2n
n
). All binarizations lead to the same
parsing accuracy, but maybe different parsing effi-
ciency, i.e. parsing speed. We are interested in in-
vestigating whether and how binarizations will af-
fect the efficiency of the CKY parsing.
Do different binarizations lead to different pars-
ing efficiency? Figure 1 gives an example to help
answer this question. Figure 1(a) illustrates the cor-
rect parse of the phrase ?get the bag and go?. We
assume that NP ? NP CC NP is in the original
grammar. The symbols enclosed in square brackets
in the figure are intermediate symbols.
VP
VP VPVB NP CC VBget DT NN and gothe bag
(a) final parse
VP
[NP CC]VP VPVB NP CC VBget DT NN and gobag
[VP CC] NP?
NP?
the
(b) with left
VP
VP VPVB NP CC VBget DT NN and gobag
[CC VP]
the
(c) with right
Figure 1: Parsing with left and right binarization.
If a left binarized grammar is used, see Fig-
ure 1(b), an extra constituent [NP CC] spanning
?the bag and? will be produced. Because rule
[NP CC] ? NP CC is in the left binarized gram-
mar and there is an NP over ?the bag? and a CC
over the right adjacent ?and?. Having this con-
stituent is unnecessary, because it lacks an NP to
the right to complete the production. However, if a
right binarization is used, as shown in Figure 1(c),
such unnecessary constituent can be avoided.
One observation from this example is that differ-
ent binarizations affect constituent generation, thus
affect parsing efficiency. Another observation is that
167
for rules like X ? Y CC Y , it is more suitable to
binarize them in a right branching way. This can
be seen as a linguistic nature: for ?and?, usually
the right neighbouring word can indicate the correct
parse. A good binarization should reflect such ligu-
istic nature.
In this paper, we aim to study the effect of bina-
rization on the efficiency of the CKY parsing. To our
knowledge, this is the first work on this problem.
We propose the problem to find the optimal bina-
rization in terms of parsing efficiency (Section 3).
We argue that binarizations affect parsing efficiency
primarily by affecting the number of incomplete
constituents generated, and the effectiveness of bi-
narization also depends on the nature of the input
(Section 4). Therefore we propose a novel binariza-
tion method utilizing rich information learnt from
training corpus (Section 5). Experimental results
show that our binarization outperforms other exist-
ing methods (Section 7.2).
Since binarization is usually a preprocessing step
before parsing, we argue that better performance can
be achieved by combining other parsing speed-up
techniques with our binarization (Section 6). We
conduct experiments to confirm this (Section 7.3).
2 Binarization
In this paper we assume that the original gram-
mar, perhaps after preprocessing, contains no ?-
productions or useless symbols. However, we allow
the existence of unary productions, since we adopt
an extended version of the CKY algorithm which
can handle the unary productions. Moreover we do
not distinguish nonterminals and terminals explic-
itly. We treat them as symbols. What we focus on is
the procedure of binarization.
Definition 1. A binarization is a function pi, map-
ping an n-ary grammar G to an equivalent binary
grammar G?. We say that G? is a binarized grammar
of G, denoted as pi(G).
Two grammars are equivalent if they define the
same probability distribution over strings (Charniak
et al, 1998).
We use the most widely used left binarization
(Aho and Ullman, 1972) to show the procedure of
binarization, as illustrated in Table 1, where p and q
are the probabilities of the productions.
Original grammar Left binarized grammar
Y ? ABC : p [AB] ? AB : 1.0
Z ? ABD : q Y ? [AB]C : p
Z ? [AB]D : q
Table 1: Left binarization
In the binarized grammar, symbols of form [AB]
are new (also called intermediate) nonterminals.
Left binarization always selects the left most pair of
symbols and combines them to form an intermedi-
ate nonterminal. This procedure is repeated until all
productions are binary.
In this paper, we assume that all binarizations fol-
low the fashion above, except that the choice of pair
of symbols for combination can be arbitrary. Next
we show three other known binarizations.
Right binarization is almost the same with left
binarization, except that it always selects the right
most pair, instead of left, to combine.
Head binarization always binarizes from the head
outward (Klein and Manning, 2003b). Please refer
to Charniak et al (2006) for more details.
Compact binarization (Schmid, 2004) tries to
minimize the size of the binarized grammar. It leads
to a compact grammar. We therefore call it compact
binarization. It is done via a greedy approach: it al-
ways selects the pair that occurs most on the right
hand sides of rules to combine.
3 The optimal binarization
The optimal binarization should help CKY parsing
to achieve its best efficiency. We formalize the idea
as follows:
Definition 2. The optimal binarization is pi?, for a
given n-ary grammar G and a test corpus C:
pi? = argminpi T (pi(G), C) (1)
where T (pi(G), C) is the running time for CKY to
parse corpus C, using the binarized grammar pi(G).
It is hard to find the optimal binarization directly
from Definition 2. We next give an empirical anal-
ysis of the running time of the CKY algorithm and
simplify the problem by introducing assumptions.
3.1 Analysis of CKY parsing efficiency
It is known that the complexity of the CKY algo-
rithm is O(n3L). The constant L depends on the bi-
168
narized grammar in use. Therefore binarization will
affect L. Our goal is to find a good binarization that
makes parsing more efficient.
It is also known that in the inner most loop of
CKY as shown in Algorithm 1, the for-statement in
Line 1 can be implemented in several different meth-
ods. The choice will affect the efficiency of CKY.
We present here four possible methods:
M1 Enumerate all rules X ? Y Z, and check if Y is in
left span and Z in right span.
M2 For each Y in left span, enumerate all rules X ?
Y Z, and check if Z is in right span.
M3 For each Z in right span, enumerate all rules X ?
Y Z, and check if Y is in left span.
M4 Enumerate each Y in left span and Z in right span1,
check if there are any rules X ? Y Z.
Algorithm 1 The inner most loop of CKY
1: for X ? Y Z, Y in left span and Z in right span
2: Add X to parent span
3.2 Model assumption
We have shown that both binarization and the for-
statement implementation in the inner most loop of
CKY will affect the parsing speed.
About the for-statement implementations, no pre-
vious study has addressed which one is superior.
The actual choice may affect our study on binariza-
tion. If using M1, since it enumerates all rules in
the grammar, the optimal binarization will be the
one with minimal number of rules, i.e. minimal bi-
narized grammar size. However, M1 is usually not
preferred in practice (Goodman, 1997). For other
methods, it is hard to tell which binarization is op-
timal theoretically. In this paper, for simplicity rea-
sons we do not consider the effect of for-statement
implementations on the optimal binarization.
On the other hand, it is well known that reduc-
ing the number of constituents produced in parsing
can greatly improve CKY parsing efficiency. That
is how most thresholding systems (Goodman, 1997;
Tsuruoka and Tsujii, 2004; Charniak et al, 2006)
speed up CKY parsing. Apparently, the number of
1Note that we should skip Y (Z) if it never appears as the
first (second) symbol on the right hand side of any rule.
constituents produced in parsing is not affected by
for-statement implementations.
Therefore we assume that the running time of
CKY is primarily determined by the number of con-
stituents generated in parsing. We simplify the opti-
mal binarization to be:
pi? ? argminpi E(pi(G), C) (2)
where E(pi(G), C) is the number of constituents
generated when CKY parsing C with pi(G).
We next discuss how binarizations affect the num-
ber of constituents generated in parsing, and present
our algorithm for finding a good binarization.
4 How binarizations affect constituents
Throughout this section and the next, we will use an
example to help illustrate the idea. The grammar is:
X ? A B C D
Y ? A B C
C ? C D
Z ? A B C E
W ? F C D E
The input sentence is 0A1B2C3D4E5, where the
subscripts are used to indicate the positions of spans.
For example, [1, 3] stands for BC. The final parse2
is shown in Figure 2. Symbols surrounded by dashed
circles are fictitious, which do not actually exist in
the parse.
F
B:[1,2]A:[0,1] C:[2,3] D:[3,4] E:[4,5]
WY:[0,3] X:[0,4] C:[2,4]
Y:[0,4] Z:[0,5]
Figure 2: Parse of the sentence ABC DE
4.1 Complete and incomplete constituents
In the procedure of CKY parsing, there are two kinds
of constituents generated: complete and incomplete.
Complete constituents (henceforth CCs) are those
composed by the original grammar symbols and
2More precisely, it is more than a parse tree for it contains
all symbols recognized in parsing.
169
spans. For example in Figure 2, X : [0, 4], Y : [0, 3]
and Y :[0, 4] are all CCs.
Incomplete constituents (henceforth ICs) are
those labeled by intermediate symbols. Figure 2
does not show them directly, but we can still read the
possible ones. For example, if the binarized gram-
mar in use contains an intermediate symbol [ABC],
then there will be two related ICs [ABC]:[0, 3] and
[ABC]:[0, 4] (the latter is due to C:[2, 4]) produced
in parsing. ICs represent the intermediate steps to
recognize and complete CCs.
4.2 Impact on complete constituents
Binarizations do not affect whether a CC will be pro-
duced. If there is a CC in the parse, whatever bi-
narization we use, it will be produced. The differ-
ence merely lies on what intermediate ICs are used.
Therefore given a grammar and an input sentence,
no matter what binarization is used, the CKY pars-
ing will generate the same set of CCs.
For example in Figure 2 there is a CC X : [0, 4],
which is associated with rule X ? ABC D. No
matter what binarization we use, this CC will be rec-
ognized eventually. For example if using left bina-
rization, we will get [AB]:[0, 2], [ABC]:[0, 3] and
finally X :[0, 4]; if using right binarization, we will
get [C D]:[2, 4], [BC D]:[1, 4] and again X:[0, 4].
4.3 Impact on incomplete constituents
Binarizations do affect the generation of ICs, be-
cause they generate different intermediate symbols.
We discuss the impact on two aspects:
Shared IC. Some ICs can be used to generate
multiple CCs in parsing. We call them shared. If a
binarization can lead to more shared ICs, then over-
all there will be fewer ICs needed in parsing.
For example, in Figure 2, if we use left binariza-
tion, then [AB]:[0, 2] can be shared to generate both
X :[0, 4] and Y :[0, 3], in which we can save one IC
overall. However, if right binarization is used, there
will be no common ICs to share in the generation
steps of X :[0, 4] and Y :[0, 3], and overall there are
one more IC generated.
Failed IC. For a CC, if it can be recognized even-
tually by applying an original rule of length k, what-
ever binarization to use, we will have to generate the
same number of k ? 2 ICs before we can complete
the CC. However, if the CC cannot be fully recog-
nized but only partially recognized, then the number
of ICs needed will be quite different.
For example, in Figure 2, the rule W ? F C DE
can be only partially recognized over [2, 5], so it can-
not generate the corresponding CC. Right binariza-
tion needs two ICs ([DE]:[3, 5] and [C DE]:[2, 5])
to find that the CC cannot be recognized, while left
binarization needs none.
As mentioned earlier, ICs are auxiliary means to
generate CCs. If an IC cannot help generate any
CCs, it is totally useless and even harmful. We call
such an IC failed, otherwise it is successful. There-
fore, if a binarization can help generate fewer failed
ICs then parsing would be more efficient.
4.4 Binarization and the nature of the input
Now we show that the impact of binarization also
depends on the actual input. When the input
changes, the impact may also change.
For example, in the previous example about the
rule W ? F C DE in Figure 2, we believe that
left binarization is better based on the observation
that there are more snippets of [C DE] in the in-
put which lack for F to the left. If there are more
snippets of [F C D] in the input lacking for E to the
right, then right binarization would be better.
The discussion above confirms such a view: the
effect of binarization depends on the nature of the
input language, and a good binarization should re-
flect this nature. This accords with our intuition. So
we use training corpus to learn a good binarization.
And we verify the effectiveness of the learnt bina-
rization using a test corpus with the same nature.
In summary, binarizations affect the efficiency of
parsing primarily by affecting the number of ICs
generated, where more shared and fewer failed ICs
will help lead to higher efficiency. Meanwhile, the
effectiveness of binarization also depends on the na-
ture of its input language.
5 Towards a good binarization
Based on the analysis in the previous section, we
employ a greedy approach to find a good binariza-
tion. We use training corpus to compute metrics
for every possible intermediate symbol. We use this
information to greedily select the best pair to com-
bine.
170
5.1 Algorithm
Given the original grammar G and training corpus
C, for every sentence in C, we firstly obtain the final
parse (like Figure 2). For every possible intermedi-
ate symbol, i.e. every ngram of the original symbols,
denoted by w, we compute the following two met-
rics:
1. How many ICs labeled by w can be generated
in the final parse, denoted by num(w) (number
of related ICs).
2. How many CCs can be generated via ICs la-
beled by w, denoted by ctr(w) (contribution of
related ICs).
For example in Figure 2, for a possible inter-
mediate symbol [ABC], there are two related ICs
([ABC] : [0, 3] and [ABC] : [0, 4]) in the parse,
so we have num([ABC]) = 2. Meanwhile, four
CCs (Y : [0, 3], X : [0, 4], Y : [0, 4] and Z : [0, 5]) can
be generated from the two related ICs. Therefore
ctr([ABC]) = 4. We list the two metrics for every
ngram in Figure 2 in Table 2. We will discuss how
to compute these two metrics in Section 5.2.
w num ctr w num ctr
[AB] 1 4 [BC E] 1 1
[ABC] 2 4 [C D] 1 2
[ABC D] 1 1 [C DE] 1 0
[ABC E] 1 1 [C E] 1 1
[BC] 2 4 [DE] 1 0
[BC D] 1 1
Table 2: Metrics of every ngram
The two metrics indicate the goodness of a possi-
ble intermediate symbol w: num(w) indicates how
many ICs labeled by w are likely to be generated in
parsing; while ctr(w) represents how much w can
contribute to the generation of CCs. If ctr(w) is
larger, the corresponding ICs are more likely to be
shared. If ctr is zero, those ICs are surely failed.
Therefore the smaller num(w) is and the larger
ctr(w) is, the better w would be.
Combining num and ctr, we define a utility func-
tion for each ngram w in the original grammar:
utility(w) = f(num(w), ctr(w)) (3)
where f is a ranking function, satisfying that f(x, y)
is larger when x is smaller and y is larger. We will
discuss more details about it in Section 5.3.
Using utility as the ranking function, we sort all
pairs of symbols and choose the best to combine.
The formal algorithm is as follows:
S1 For every symbol pair of ?v1, v2? (where v1 and
v2 can be original symbols or intermediate symbols
generated in previous rounds), let w1 and w2 be the
ngrams of original symbols represented by v1 and
v2, respectively. Let w = w1w2 be the ngram rep-
resented by the symbol pair. Compute utility(w).
S2 Select the ngram w with the highest utility(w), let
it be w? (in case of a tie, select the one with a
smaller num). Let the corresponding symbol pair
be ?v?1 , v?2?.
S3 Add a new intermediate symbol v?, and replace all
the occurrences of ?v?1 , v?2? on the right hand sides
of rules with v?.
S4 Add a new rule v? ? v?1v?2 : 1.0.
S5 Repeat S1 ? S4, until there are no rules with more
than two symbols on the right hand side.
5.2 Metrics computing
In this section, we discuss how to compute num and
ctr in details.
Computing ctr is straightforward. First we get
final parses like in Figure 2 for training sentences.
From a final parse, we traverse along every parent
node and enumerate every subsequence of its child
nodes. For example in Figure 2, from the parent
node of X : [0, 4], we can enumerate the follow-
ing: [AB] : [0, 2], [ABC] : [0, 3], [ABC D] : [0, 4],
[BC]:[1, 3], [BC D]:[1, 4], [C D]:[2, 4]. We add 1 to
all the ctr of these ngrams, respectively.
To compute num, we resort to the same idea
of dynamic programming as in CKY. We perform
a normal left binarization except that we add all
ngrams in the original grammar G as intermediate
symbols into the binarized grammar G?. For exam-
ple, for the rule of S ? ABC : p, the constructed
grammar is as follows:
[AB] ? A B : 1.0
S ? [AB] C : p
[BC] ? B C : 1.0
Using the constructed G?, we employ a normal
CKY parsing on the training corpus and compute
171
how many constituents are produced for each ngram.
The result is num. Suppose the length of the train-
ing sentence is n, the original grammar G has N
symbols, and the maximum length of rules is k,
then the complexity of this method can be written
as O(Nkn3).
5.3 Ranking function
We discuss the details of the ranking function f used
to compute the utility of each ngram w. We come
up with two forms for f : linear and log-linear
1. linear: f(x, y) = ??1x+ ?2y
2. log-linear3: f(x, y) = ??1 log(x) + ?2 log(y)
where ?1 and ?2 are non-negative weights subject to
?1 + ?2 = 14.
We will use development set to determine which
form is better and to learn the best weight settings.
6 Combination with other techniques
Binarization usually plays a role of preprocessing in
the procedure of parsing. Grammars are binarized
before they are fed into the stage of parsing. There
are many known works on speeding up the CKY
parsing. So we can expect that if we replace the
part of binarization by a better one while keeping
the subsequent parsing unchanged, the parsing will
be more efficient. We will conduct experiment to
confirm this idea in the next section.
We would like to make more discussions be-
fore we advance to the experiments. The first is
about parsing accuracy in combining binarization
with other parsing speed-up techniques. Binariza-
tion itself does not affect parsing accuracy. When
combined with exact inference algorithms, like the
iterative CKY (Tsuruoka and Tsujii, 2004), the ac-
curacy will be the same. However, if combined with
other inexact pruning techniques like beam-pruning
(Goodman, 1997) or coarse-to-fine parsing (Char-
niak et al, 2006), binarization may interact with
those pruning methods in a complicated way to af-
fect parsing accuracy. This is due to different bina-
rizations generate different sets of intermediate sym-
3For log-linear form, if num(w) = 0 (and consequently
ctr(w) = 0), we set f(num(w), ctr(w)) = 0; if num(w) >
0 but ctr(w) = 0, we set f(num(w), ctr(w)) = ??.
4Since f is used for ranking, the magnitude is not important.
bols. With the same complete constituents, one bi-
narization might derive incomplete constitutes that
could be pruned while another binarization may not.
This would affect the accuracy. We do not address
this interaction on in this paper, but leave it to the
future work. In Section 7.3 we will use the iterative
CKY for testing.
In addition, we believe there exist some speed-up
techniques which are incompatible with our bina-
rization. One such example may be the top-down
left-corner filtering (Graham et al, 1980; Moore,
2000), which seems to be only applicable to the pro-
cess of left binarization. A detailed investigation on
this problem will be left to the future work.
The last issue is how our binarization performs
on a lexicalized parser, like Collins (1997). Our in-
tuition is that we cannot apply our binarization to
Collins (1997). The key fact in lexicalized parsers
is that we cannot explicitly write down all rules
and compute their probabilities precisely, due to the
great number of rules and the severe data sparsity
problem. Therefore in Collins (1997) grammar rules
are already factorized into a set of probabilities.
In order to capture the dependency relationship be-
tween lexcial heads Collins (1997) breaks down the
rules from head outwards, which prevents us from
factorizing them in other ways. Therefore our bina-
rization cannot apply to the lexicalized parser. How-
ever, there are state-of-the-art unlexicalized parsers
(Klein and Manning, 2003b; Petrov et al, 2006), to
which we believe our binarization can be applied.
7 Experiments
We conducted two experiments on Penn Treebank II
corpus (Marcus et al, 1994). The first is to com-
pare the effects of different binarizations on parsing
and the second is to test the feasibility to combine
our work with iterative CKY parsing (Tsuruoka and
Tsujii, 2004) to achieve even better efficiency.
7.1 Experimental setup
Following conventions, we learnt the grammar from
Wall Street Journal (WSJ) section 2 to 21 and mod-
ified it by discarding all functional tags and empty
nodes. The parser obtained this way is a pure un-
lexicalized context-free parser with the raw treebank
grammar. Its accuracy turns out to be 72.46% in
172
terms of F1 measure, quite the same as 72.62% as
stated in Klein and Manning (2003b). We adopt this
parser in our experiment not only because of sim-
plicity but also because we focus on parsing effi-
ciency.
For all sentences with no more than 40 words in
section 22, we use the first 10% as the development
set, and the last 90% as the test set. There are 158
and 1,420 sentences in development set and test set,
respectively. We use the whole 2,416 sentences in
section 23 as the training set.
We use the development set to determine the bet-
ter form of the ranking function f as well as to
tune its weights. Both metrics of num and ctr
are normalized before use. Since there is only one
free variable in ?1 and ?2, we can just enumerate
0 ? ?1 ? 1, and set ?2 = 1 ? ?1. The increasing
step is firstly set to 0.05 for the approximate loca-
tion of the optimal weight, then set to 0.001 to learn
more precisely around the optimal.
We find that the optimal is 5,773,088 (constituents
produced in parsing development set) with ?1 =
0.014 for linear form, while for log-linear form the
optimal is 5,905,292 with ?1 = 0.691. Therefore we
determine that the better form for the ranking func-
tion is linear with ?1 = 0.014 and ?2 = 0.986.
The size of each binarized grammar used in the
experiment is shown in Table 3. ?Original? refers
to the raw treebank grammar. ?Ours? refers to the
learnt binarized grammar by our approach. For the
rest please refer to Section 2.
# of Symbols # of Rules
Original 72 14,971
Right 10,654 25,553
Left 12,944 27,843
Head 11,798 26,697
Compact 3,644 18,543
Ours 8,407 23,306
Table 3: Grammar size of different binarizations
We also tested whether the size of the training set
would have significant effect. We use the first 10%,
20%, ? ? ? , up to 100% of section 23 as the training
set, respectively, and parse the development set. We
find that all sizes examined have a similar impact,
since the numbers of constituents produced are all
around 5,780,000. It means the training corpus does
not have to be very large.
The entire experiments are conducted on a server
with an Intel Xeon 2.33 GHz processor and 8 GB
memory.
7.2 Experiment 1: compare among
binarizations
In this part, we use CKY to parse the entire test set
and evaluate the efficiency of different binarizations.
The for-statement implementation of the inner
most loop of CKY will affect the parsing time
though it won?t affect the number of constituents
produced as discussed in Section 3.2. The best im-
plementations may be different for different bina-
rized grammars. We examine M1?M4, testing their
parsing time on the development set. Results show
that for right binarization the best method is M3,
while for the rest the best is M2. We use the best
method for each binarized grammar when compar-
ing the parsing time in Experiment 1.
Table 4 reports the total number of constituents
and total time required for parsing the entire test set.
It shows that different binarizations have great im-
pacts on the efficiency of CKY. With our binariza-
tion, the number of constituents produced is nearly
20% of that required by right binarization and nearly
25% of that by the widely-used left binarization. As
for the parsing time, CKY with our binarization is
about 2.5 times as fast as with right binarization and
about 1.75 times as fast as with left binarization.
This illustrates that our binarization can significantly
improve the efficiency of the CKY parsing.
Binarization Constituents Time (s)
Right 241,924,229 5,747
Left 193,238,759 3,474
Head 166,425,179 3,837
Compact 94,257,478 2,302
Ours 52,206,466 2,182
Table 4: Performance on test set
Figure 3 reports the detailed number of complete
constituents, successful incomplete constituents and
failed incomplete constituents produced in parsing.
The result proves that our binarization can signifi-
cantly reduce the number of failed incomplete con-
stituents, by a factor of 10 in contrast with left bi-
narization. Meanwhile, the number of successful in-
173
complete constituents is also reduced by a factor of
2 compared to left binarization.
0.0e+00
2.0e+07
4.0e+07
6.0e+07
8.0e+07
1.0e+08
1.2e+08
1.4e+08
1.6e+08
1.8e+08
Right Left Head Compact Ours
# 
of 
Co
ns
titu
en
ts
Binarizations
complete
successful incomplete
failed incomplete
Figure 3: Comparison on various constituents
Another interesting observation is that parsing
with a smaller grammar does not always yield a
higher efficiency. Our binarized grammar is more
than twice the size of compact binarization, but ours
is more efficient. It proves that parsing efficiency is
related to both the size of grammar in use as well as
the number of constituents produced.
In Section 1, we used an example of ?get the
bag and go? to illustrate that for rules like X ?
Y CC Y , right binarization is more suitable. We
also investigated the corresponding linguistic nature
that the word to the right of ?and? is more likely to
indicate the true relationship represented by ?and?.
We argued that a better binarization can reflect such
linguistic nature of the input language. To our sur-
prise, our learnt binarization indeed captures this lin-
guistic insight, by binarizing NP ? NP CC NP
from right to left.
Finally, we would like to acknowledge the limi-
tation of our assumption made in Section 3.2. Ta-
ble 4 shows that the parsing time of CKY is not
always monotonic increasing with the number of
constituents produced. Head binarization produces
fewer constituents than left binarization but con-
sumes more parsing time.
7.3 Experiment 2: combine with iterative CKY
In this part, we test the performance of combining
our binarization with the iterative CKY (Tsuruoka
and Tsujii, 2004) (henceforth T&T) algorithm.
Iterative CKY is a procedure of multiple passes
of normal CKY: in each pass, it uses a threshold to
prune bad constituents; if it cannot find a successful
parse in one pass, it will relax the threshold and start
another; this procedure is repeated until a successful
parse is returned. T&T used left binarization. We
re-implement their experiments and combine itera-
tive CKY with our binarization. Note that iterative
CKY is an exact inference algorithm that guarantees
to return the optimal parse. As discussed in Sec-
tion 6, the parsing accuracy is not changed in this
experiment.
T&T used a held-out set to learn the best step of
threshold decrease. They reported that the best step
was 11 (in log-probability). We found that the best
step was indeed 11 for left binarization; for our bina-
rizaiton, the best step was 17. T&T used M4 as the
for-statement implementation of CKY. In this part,
we follow the same method.
The result is shown in Table 5. We can see that
iterative CKY can achieve better performance by us-
ing a better binarization. We also see that the reduc-
tion by binarization with pruning is less significant
than without pruning. It seems that the pruning itself
in iterative CKY can counteract the reduction effect
of binarization to some extent. Still the best per-
formance is archieved by combining iterative CKY
with a better binarization.
CKY + Binarization Constituents Time (s)
Tsuruoka and Tsujii (2004)
CKY + Left 45,406,084 1,164
Iterative CKY + Left 17,520,427 613
Reimplement
CKY + Left 52,128,941 932
CKY + Ours 14,892,203 571
Iterative CKY + Left 23,267,594 377
Iterative CKY + Ours 10,966,272 314
Table 5: Combining with iterative CKY parsing
8 Related work
Almost all work on parsing starts from a binarized
grammar. Usually binarization plays a role of pre-
processing. Left binarization is widely used (Aho
and Ullman, 1972; Charniak et al, 1998; Tsuruoka
and Tsujii, 2004) while right binarization is rarely
used in the literature. Compact binarization was in-
troduced in Schmid (2004), based on the intuition
that a more compact grammar will help acheive a
highly efficient CKY parser, though from our exper-
iment it is not always true.
174
We define the fashion of binarizations in Sec-
tion 2, where we encode an intermediate symbol us-
ing the ngrams of original symbols (content) it de-
rives. This encoding is known as the Inside-Trie (I-
Trie) in Klein and Manning (2003a), in which they
also mentioned another encoding called Outside-
Trie (O-Trie). O-Trie encodes an intermediate sym-
bol using the its parent and the symbols surrounding
it in the original rule (context). Klein and Manning
(2003a) claimed that O-Trie is superior for calculat-
ing estimates for A* parsing. We plan to investigate
binarization defined by O-Trie in the future.
Both I-Trie and O-Trie are equivalent encodings,
resulting in equivalent grammars, because they both
encode using the complete content or context infor-
mation of an intermediate symbol. If we use part of
the information to encode, for example just parent in
O-Trie case, the encoding will be non-equivalent.
Proper non-equivalent encodings are used to gen-
eralize the grammar and prevent the binarized gram-
mar becoming too specific (Charniak et al, 2006). It
is equipped with head binarization to help improve
parsing accuracy, following the traditional linguistic
insight that phrases are organized around the head
(Collins, 1997; Klein and Manning, 2003b). In con-
trast, we focus our attention on parsing efficiency
not accuracy in this paper.
Binarization also attracts attention in the syntax-
based models for machine translation, where trans-
lation can be modeled as a parsing problem and bi-
narization is essential for efficient parsing (Zhang
et al, 2006; Huang, 2007).
Wang et al (2007) employs binarization to de-
compose syntax trees to acquire more re-usable
translation rules in order to improve translation ac-
curacy. Their binarization is restricted to be a mix-
ture of left and right binarization. This constraint
may decrease the power of binarization when ap-
plied to speeding up parsing in our problem.
9 Conclusions and future work
We have studied the impact of grammar binarization
on parsing efficiency and presented a novel bina-
rization which utilizes rich information learnt from
training corpus. Experiments not only showed that
our learnt binarization outperforms other existing
ones in terms of parsing efficiency, but also demon-
strated the feasibility to combine our binarization
with known parsing speed-up techniques to achieve
even better performance.
An advantage of our approach to finding a good
binarization would be that the training corpus does
not need to be parsed sentences. Only POS tagged
sentences will suffice for training. This will save the
effort to adapt the model to a new domain.
Our approach is based on the assumption that the
efficiency of CKY parsing is primarily determined
by the number of constituents produced. This is a
fairly sound one, but not always true, as shown in
Section 7.2. One future work will be relaxing the
assumption and finding a better appraoch.
Another future work will be to apply our work to
chart parsing. It is known that binarization is also
essential for an O(n3) complexity of chart parsing,
where dotted rules are used to binarize the grammar
implicitly from left. As shown in Charniak et al
(1998), we can binarize explicitly and use intermedi-
ate symbols to replace dotted rules in chart parsing.
Therefore chart parsing can use multiple binariza-
tions. We expect that a better binarization will also
help improve the efficiency of chart parsing.
Acknowledgements
We thank the anonymous reviwers for their pertinent
comments, Yoshimasa Tsuruoka for the detailed ex-
planations on his referred paper, Yunbo Cao, Shu-
jian Huang, Zhenxing Wang , John Blitzer and Liang
Huang for their valuable suggestions in preparing
the paper.
References
Aho, A. V. and Ullman, J. D. (1972). The theory
of parsing, translation, and compiling. Prentice-
Hall, Inc., Upper Saddle River, NJ, USA.
Charniak, E., Goldwater, S., and Johnson, M.
(1998). Edge-based best-first chart parsing. In
Proceedings of the Six Workshop on Very Large
Corpora, pages 127?133.
Charniak, E., Johnson, M., Elsner, M., Austerweil,
J., Ellis, D., Haxton, I., Hill, C., Shrivaths, R.,
Moore, J., Pozar, M., and Vu, T. (2006). Multi-
level coarse-to-fine pcfg parsing. In HLT-NAACL.
Collins, M. (1997). Three generative, lexicalised
models for statistical parsing. In ACL.
175
Earley, J. (1970). An efficient context-free parsing
algorithm. Commun. ACM, 13(2):94?102.
Goodman, J. (1997). Global thresholding and
multiple-pass parsing. In EMNLP.
Graham, S. L., Harrison, M. A., and Ruzzo, W. L.
(1980). An improved context-free recognizer.
ACM Trans. Program. Lang. Syst., 2(3):415?462.
Huang, L. (2007). Binarization, synchronous bina-
rization, and target-side binarization. In Proceed-
ings of SSST, NAACL-HLT 2007 / AMTA Work-
shop on Syntax and Structure in Statistical Trans-
lation, pages 33?40, Rochester, New York. Asso-
ciation for Computational Linguistics.
Kasami, T. (1965). An efficient recognition and
syntax analysis algorithm for context-free lan-
guages. Technical Report AFCRL-65-758, Air
Force Cambridge Research Laboratory, Bedford,
Massachusetts.
Kay, M. (1980). Algorithm schemata and data struc-
tures in syntactic processing. Technical Report
CSL80-12, Xerox PARC, Palo Alto, CA.
Klein, D. and Manning, C. D. (2001). Parsing and
hypergraphs. In IWPT.
Klein, D. and Manning, C. D. (2003a). A* parsing:
Fast exact viterbi parse selection. In HLT-NAACL.
Klein, D. and Manning, C. D. (2003b). Accurate
unlexicalized parsing. In ACL.
Marcus, M. P., Kim, G., Marcinkiewicz, M. A.,
MacIntyre, R., Bies, A., Ferguson, M., Katz, K.,
and Schasberger, B. (1994). The penn treebank:
Annotating predicate argument structure. In HLT-
NAACL.
Moore, R. C. (2000). Improved left-corner chart
parsing for large context-free grammars. In IWPT.
Petrov, S., Barrett, L., Thibaux, R., and Klein, D.
(2006). Learning accurate, compact, and inter-
pretable tree annotation. In ACL.
Schmid, H. (2004). Efficient parsing of highly am-
biguous context-free grammars with bit vectors.
In COLING.
Tsuruoka, Y. and Tsujii, J. (2004). Iterative cky pars-
ing for probabilistic context-free grammars. In
IJCNLP.
Wang, W., Knight, K., and Marcu, D. (2007). Bina-
rizing syntax trees to improve syntax-based ma-
chine translation accuracy. In EMNLP-CoNLL.
Younger, D. H. (1967). Recognition and parsing of
context-free languages in time n3. Information
and Control, 10(2):189?208.
Zhang, H., Huang, L., Gildea, D., and Knight, K.
(2006). Synchronous binarization for machine
translation. In HLT-NAACL.
176
