Combining Multiple Models for Speech Information Retrieval 
Muath Alzghool and Diana Inkpen  
School of Information Technology and Engineering 
University of Ottawa 
{alzghool,diana}@ site.uottawa.ca  
Abstract 
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance 
in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated 
on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many 
recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain 
good results on this data set, except for the case when manual summaries are included. 
 
1. Introduction  
Conversational speech such as recordings of interviews or 
teleconferences is difficult to search through. The 
transcripts produced with Automatic Speech Recognition 
(ASR) systems tend to contain many recognition errors, 
leading to low Information Retrieval (IR) performance 
(Oard et al, 2007). 
Previous research has explored the idea of combining 
the results of different retrieval strategies; the motivation is 
that each technique will retrieve different sets of relevant 
documents; therefore combining the results could produce 
a better result than any of the individual techniques. We 
propose new data fusion techniques for combining the 
results of different IR models. We applied our data fusion 
techniques to the Mallach collection (Oard et al, 2007) 
used in the Cross-Language Speech Retrieval (CLSR) task 
at Cross-Language Evaluation Forum (CLEF) 2007. The 
Mallach collection comprises 8104 ?documents? which are 
manually-determined topically-coherent segments taken 
from 272 interviews with Holocaust survivors, witnesses 
and rescuers, totalling 589 hours of speech. Figure 1 shows 
the document structure in CLSR test collection, two ASR 
transcripts are available for this data, in this work we use 
the ASRTEXT2004A field provided by IBM research with 
a word error rate of 38%. Additionally, metadata fields for 
each document include: two sets of 20 automatically 
assigned keywords determined using two different kNN 
classifiers (AK1 and AK2), a set of a varying number of 
manually-assigned keywords (MK), and a manual 
3-sentence summary written by an expert in the field.  A set 
of 63 training topics and 33 test topics were generated for 
this task. The topics provided with the collection were 
created in English from actual user requests. Topics were 
structured using the standard TREC format of Title, 
Description and Narrative fields. To enable CL-SR 
experiments the topics were translated into Czech, German, 
French, and Spanish by native speakers; Figure 2 and 3 
show two examples for English and its translation in 
French respectively. Relevance judgments were generated 
using a search-guided procedure and standard pooling 
methods. See (Oard et al, 2004) for full details of the 
collection design.  
We present results on the automatic transcripts for 
English queries and translated queries (cross-language) 
for two combination methods; we also present results 
when manual summaries and manual keywords are 
indexed. 
 
<DOC> 
<DOCNO>VHF[IntCode]-[SegId].[SequenceNum]</DOCNO\> 
<INTERVIEWDATA>Interviewee name(s) and 
birthdate</INTERVIEWDATA> 
<NAME>Full name of every person mentioned</NAME> 
<MANUALKEYWORD>Thesaurus keywords assigned to the 
segment</MANUALKEYWORD> 
<SUMMARY>3-sentence segment summary</SUMMARY> 
<ASRTEXT2004A>ASR transcript produced in 
2004</ASRTEXT2004A> 
<ASRTEXT2006A>ASR transcript produced in 
2006</ASRTEXT2006A> 
<AUTOKEYWORD2004A1>Thesaurus keywords from a kNN 
classifier</AUTOKEYWORD2004A1> 
<AUTOKEYWORD2004A2>Thesaurus keywords from a second 
kNN classifier</AUTOKEYWORD2004A2> 
</DOC> 
Figure 1. Document structure in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Child survivors in Sweden  
<desc>Describe survival mechanisms of children born 
in 1930-1933 who spend the war in concentration 
camps or in hiding and who presently live in Sweden. 
 <narr>The relevant material should describe the 
circumstances and inner resources of the surviving 
children. The relevant material also describes how 
the wartime experience affected their post-war 
adult life. </top> 
Figure 2. Example for English topic in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Les enfants survivants en Su?de  
<desc>Descriptions des m?canismes de survie des 
enfants n?s entre 1930 et 1933 qui ont pass? la 
guerre en camps de concentration ou cach?s et qui 
vivent actuellement en Su?de.  
<narr>? 
</top>  
Figure 3. Example for French topic in CL-SR test collection. 
2. System Description  
Our Cross-Language Information Retrieval systems 
were built with off-the-shelf components. For the retrieval 
part, the SMART (Buckley, Salton, &Allan, 1992; Salton 
&Buckley, 1988) IR system and the Terrier (Amati &Van 
Rijsbergen, 2002; Ounis et al, 2005) IR system were 
tested with many different weighting schemes for 
indexing the collection and the queries.  
SMART was originally developed at Cornell 
University in the 1960s. SMART is based on the vector 
space model of information retrieval. We use the standard 
notation: weighting scheme for the documents, followed 
by dot, followed by the weighting scheme for the queries, 
each term-weighting scheme is described as a 
combination of term frequency, collection frequency, and 
length normalization components where the schemes are 
abbreviated according to its components variations (n no 
normalization, c cosine, t idf, l log, etc.) We used nnn.ntn, 
ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn , 
nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntc 
weighting schemes (Buckley, Salton, &Allan, 1992; 
Salton &Buckley, 1988);  lnn.ntn performs very well in 
CLEF-CLSR 2005 and 2006 (Alzghool &Inkpen, 2007; 
Inkpen, Alzghool, &Islam, 2006); lnn.ntn means that lnn 
was used for documents and ntn for queries according to 
the following formulas:  
0.1)ln(nln += tfweight        (1) 
tn
Ntfweight logntn ?=     (2)      
where tf denotes the term frequency of a term t in the 
document or query, N denotes the number of documents 
in the collection, and nt denotes the number of documents 
in which the term t occurs.  
Terrier was originally developed at the University of 
Glasgow. It is based on Divergence from Randomness 
models (DFR) where IR is seen as a probabilistic process 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005). We 
experimented with the In_expC2 (Inverse Expected 
Document Frequency model with Bernoulli after-effect 
and normalization) weighting model, one of Terrier?s 
DFR-based document weighting models.  
Using the In_expC2 model, the relevance score of a 
document d for a query q is given by the formula: 
                  (3) ?
?
=
qt
dtwqtfqdsim ),(.),(
where qtf is the frequency of term t in the query q, and w(t,d) 
is the relevance score of a document d for the query term t, 
given by: 
)
5.0
1log()
)1(
1(),( 2 +
+??+?
+=
e
e
et n
Ntfn
tfnn
Fdtw   (4) 
where 
-F is the term frequency of t in the whole collection. 
-N is the number of document in the whole collection.  
-nt is the document frequency of t. 
-ne is given by ))
1
(1( Fte N
n
Nn
???=  (5) 
- tfne is the normalized within-document frequency of the 
term t in the document d. It is given by the normalization 2 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005): 
)_1(log
l
lavgctftfn ee ?+?=     (6) 
where c is a parameter, tf is the within-document 
frequency of the term t in the document d, l is the 
document length, and avg_l is the average document 
length in the whole collection. 
We estimated the parameter c of the Terrier's 
normalization 2 formula by running some experiments on 
the training data, to get the best values for c depending on 
the topic fields used. We obtained the following values: 
c=0.75 for queries using the Title only, c=1 for queries 
using the Title and Description fields, and c=1 for queries 
using the Title, Description, and Narrative fields. We select 
the c value that has a best MAP score according to the 
training data. 
For translating the queries from French and Spanish 
into English, several free online machine translation tools 
were used. The idea behind using multiple translations is 
that they might provide more variety of words and 
phrases, therefore improving the retrieval performance. 
Seven online MT systems (Inkpen, Alzghool, &Islam, 
2006) were used for translating from Spanish and from 
French into English. We combined the outputs of the MT 
systems by simply concatenating all the translations. All 
seven translations of a title made the title of the translated 
query; the same was done for the description and narrative 
fields.  
We propose two methods for combining IR models. We 
use the sum of normalized weighted similarity scores of 15 
different IR schemes as shown in the following formulas: 
 
 ?
?
?+=
schemsIRi
iMAPr NormSimiWiWFusion )]()([1
34      (7) 
?
?
?=
schemsIRi
iMAPr NormSimiWiWFusion )(*)(2
34      (8)                         
where Wr(i) and WMAP(i) are experimentally determined 
weights based on the recall (the number of relevant 
documents retrieved) and precision (MAP score) values for 
each IR scheme computed on the training data. For 
example, suppose that two retrieval runs r1 and r2 give 0.3 
and 0.2 (respectively) as  MAP scores on training data; we 
normalize these scores by dividing them by the maximum 
MAP value: then WMAP(r1) is 1 and WMAP(r2) is 0.66 (then 
we compute the power 3 of these weights, so that one 
weight stays 1 and the other one decreases; we chose power 
3 for MAP score and power 4 for recall, because the MAP 
is more important than the recall). We hope that when we 
multiply the similarity values with the weights and take the 
summation over all the runs, the performance of the 
combined run will improve. NormSimi is the normalized 
similarity for each IR scheme. We did the normalization by 
dividing the similarity by the maximum similarity in the 
run. The normalization is necessary because different 
weighting schemes will generate different range of 
similarity values, so a normalization method should 
applied to each run.  Our method is differed than the work 
done by Fox and Shaw in (1994), and Lee in ( 1995); they 
combined the results by taking the summation of the 
similarity scores without giving any weight to each run. In 
our work we weight each run according to the precision 
and recall on the training data.  
3. Experimental Results 
We applied the data fusion methods described in section 2 
to 14 runs produced by SMART and one run produced by 
Terrier.  Performance results for each single run and fused 
runs are presented in Table 1, in which % change is given 
with respect to the run providing better effectiveness in 
each combination on the training data. The Manual 
English column represents the results when only the 
manual keywords and the manual summaries were used 
for indexing the documents using English topics, the 
Auto-English column represents the results when 
automatic transcripts are indexed from the documents, for 
English topics. For cross-languages experiments the 
results are represented in the columns Auto-French, and 
Auto-Spanish, when using the combined translations 
produced by the seven online MT tools, from French and 
Spanish into English. Since the result of combined 
translation for each language was better than when using 
individual translations from each MT tool on the training 
data (Inkpen, Alzghool, &Islam, 2006), we used only the 
combined translations in our experiments. 
Data fusion helps to improve the performance (MAP 
score) on the test data. The best improvement using data 
fusion (Fusion1) was on the French cross-language 
experiments with 21.7%, which is statistically significant 
while on monolingual the improvement was only 6.5% 
which is not significant. We computed these 
improvements relative to the results of the best 
single-model run, as measured on the training data. This 
supports our claim that data fusion improves the recall by 
bringing some new documents that were not retrieved by 
all the runs. On the training data, the Fusion2 method 
gives better results than Fusion1 for all cases except on 
Manual English, but on the test data Fusion1 is better than 
Fusion2. In general, the data fusion seems to help, 
because the performance on the test data in not always 
good for weighting schemes that obtain good results on 
the training data, but combining models allows the 
best-performing weighting schemes to be taken into 
consideration. 
The retrieval results for the translations from French 
were very close to the monolingual English results, 
especially on the training data, but on the test data the 
difference was significantly worse. For Spanish, the 
difference was significantly worse on the training data, 
but not on the test data.  
Experiments on manual keywords and manual 
summaries available in the test collection showed high 
improvements, the MAP score jumped from 0.0855 to 
0.2761 on the test data. 
4. Conclusion 
We experimented with two different systems: Terrier 
and SMART, with combining the various weighting 
schemes for indexing the document and query terms. We 
proposed two methods to combine different weighting 
scheme from different systems, based on weighted 
summation of normalized similarity measures; the weight 
for each scheme was based on the relative precision and 
recall on the training data. Data fusion helps to improve 
the retrieval significantly for some experiments 
(Auto-French) and for other not significantly (Manual 
English). Our result on automatic transcripts for English 
queries (the required run for the CLSR task at CLEF 
2007), obtained a MAP score of 0.0855. This result was 
significantly better than the other 4 systems that 
participated in the CLSR task at CLEF 2007(Pecina et al, 
2007). 
In future work we plan to investigate more methods of 
data fusion (to apply a normalization scheme scalable to 
unseen data), removing or correcting some of the speech 
recognition errors in the ASR content words, and to use 
speech lattices for indexing.  
5. References 
 
Alzghool, M. & Inkpen, D. (2007). Experiments for the 
cross language speech retrieval task at CLEF 2006. In 
C. Peters, (Ed.), Evaluation of multilingual and 
multi-modal information retrieval (Vol. 4730/2007, 
pp. 778-785). Springer. 
Amati, G. & Van Rijsbergen, C. J. (2002). Probabilistic 
models of information retrieval based on measuring 
the divergence from randomness (Vol. 20). ACM,  
New York. 
Buckley, C., Salton, G., & Allan, J. (1992). Automatic 
retrieval with locality information using smart. In 
Text retrieval conferenc (TREC-1) (pp. 59-72). 
Inkpen, D., Alzghool, M., & Islam, A. (2006). Using 
various indexing schemes and multiple translations in 
the CL-SR task at CLEF 2005. In C. Peters, (Ed.), 
Accessing multilingual information repositories 
(Vol. 4022/2006, pp. 760-768). Springer,  London. 
Lee, J. H. (1995). Combining multiple evidence from 
different properties of weighting schemes, 
Proceedings of the 18th annual international ACM 
SIGIR conference on Research and development in 
information retrieval. ACM, Seattle, Washington, 
United States. 
Oard, D. W., Soergel, D., Doermann, D., Huang, X., 
Murray, G. C., Wang, J., Ramabhadran, B., Franz, 
M., & Gustman, S. (2004). Building an information 
retrieval test collection for spontaneous 
conversational speech, Proceedings of the 27th 
annual international ACM SIGIR conference on 
Research and development in information retrieval. 
ACM, Sheffield, United Kingdom. 
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., 
Pecina, P., Soergel, D., Huang, X., & Shafran, I. 
(2007). Overview of the CLEF-2006 cross-language 
speech retrieval track. In C. Peters, (Ed.), Evaluation 
of multilingual and multi-modal information 
retrieval (Vol. 4730/2007, pp. 744-758). Springer,  
Heidelberg. 
Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, 
C., & Johnson, D. (2005). Terrier information 
retrieval platform In Advances in information 
retrieval (Vol. 3408/2005, pp. 517-519). Springer,  
Heidelberg. 
Pecina, P., Hoffmannov?a, P., Jones, G. J. F., Zhang, Y., 
& Oard, D. W. (2007). Overview of the CLEF-2007 
cross language speech retrieval track, Working Notes 
of the CLEF- 2007 Evaluation, . CLEF2007, 
Budapest-Hungary. 
Salton, G. & Buckley, C. (1988). Term weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24(5): 513-523. 
Shaw, J. A. & Fox, E. A. (1994). Combination of multiple 
searches. In Third text retrieval conference (trec-3) 
(pp. 105-108). National Institute of Standards and 
Technology Special Publication. 
 
 
Manual English Auto-English Auto-French Auto-Spanish Weighting 
scheme Training Test Training Test Training Test Training Test 
nnc.ntc 0.2546 0.2293 0.0888 0.0819 0.0792 0.055 0.0593 0.0614 
ntc.ntc 0.2592 0.2332 0.0892 0.0794 0.0841 0.0519 0.0663 0.0545 
lnc.ntc 0.2710 0.2363 0.0898 0.0791 0.0858 0.0576 0.0652 0.0604 
ntc.nnc 0.2344 0.2172 0.0858 0.0769 0.0745 0.0466 0.0585 0.062 
anc.ntc 0.2759 0.2343 0.0723 0.0623 0.0664 0.0376 0.0518 0.0398 
ltc.ntc 0.2639 0.2273 0.0794 0.0623 0.0754 0.0449 0.0596 0.0428 
atc.ntc 0.2606 0.2184 0.0592 0.0477 0.0525 0.0287 0.0437 0.0304 
nnn.ntn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ntn.ntn 0.2738 0.2369 0.0933 0.0795 0.0843 0.0507 0.0691 0.0578 
lnn.ntn 0.2858 0.245 0.0969 0.0799 0.0905 0.0566 0.0701 0.0589 
ntn.nnn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ann.ntn 0.2903 0.2441 0.0750 0.0670 0.0743 0.038 0.057 0.0383 
ltn.ntn 0.2870 0.2435 0.0799 0.0655 0.0871 0.0522 0.0701 0.0501 
atn.ntn 0.2843 0.2364 0.0620 0.0546 0.0722 0.0347 0.0586 0.0355 
In_expC2 0.3177 0.2737 0.0885 0.0744 0.0908 0.0487 0.0747 0.0614 
Fusion 1 0.3208 0.2761 0.0969 0.0855 0.0912 0.0622 0.0731 0.0682 
% change 1.0% 0.9% 0.0% 6.5% 0.4% 21.7% -2.2% 10.0% 
Fusion 2 0.3182 0.2741 0.0975 0.0842 0.0942 0.0602 0.0752 0.0619 
% change 0.2% 0.1% 0.6% 5.1% 3.6% 19.1% 0.7% 0.8% 
Table 1. Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In_expC2 model), and the results 
for the two Fusions Methods. In bold are the best scores for the 15 single runs on the training data and the corresponding 
results on the test data.  
 
Weighting 
scheme 
Manual English Auto-English Auto- French Auto- Spanish 
 Train. Test Train. Test Train. Test Train. Test 
nnc. ntc 2371 1827 1726 1306 1687 1122 1562 1178 
ntc.ntc 2402 1857 1675 1278 1589 1074 1466 1155 
lnc.ntc 2402 1840 1649 1301 1628 1111 1532 1196 
ntc.nnc 2354 1810 1709 1287 1662 1121 1564 1182 
anc.ntc 2405 1858 1567 1192 1482 1036 1360 1074 
ltc.ntc 2401 1864 1571 1211 1455 1046 1384 1097 
atc.ntc 2387 1858 1435 1081 1361 945 1255 1011 
nnn.ntn 2370 1823 1740 1321 1748 1158 1643 1190 
ntn.ntn 2432 1863 1709 1314 1627 1093 1502 1174 
lnn.ntn 2414 1846 1681 1325 1652 1130 1546 1194 
ntn.nnn 2370 1823 1740 1321 1748 1158 1643 1190 
ann.ntn 2427 1859 1577 1198 1473 1027 1365 1060 
ltn.ntn 2433 1876 1582 1215 1478 1070 1408 1134 
atn.ntn 2442 1859 1455 1101 1390 975 1297 1037 
In_expC2 2638 1823 1624 1286 1676 1061 1631 1172 
Fusion 1 2645 1832 1745 1334 1759 1147 1645 1219 
% change 0.3% 0.5 % 0.3% 1.0% 0.6% -1.0% 0.1% 2.4% 
Fusion 2 2647 1823 1727 1337 1736 1098 1631 1172 
% change 0.3% 0.0% 0.8% 1.2% -0.7% -5.5% -0.7% -1.5% 
Table 2. Results (number of relevant documents retrieved) for 15 weighting schemes using Terrier and SMART, and the 
results for the Fusions Methods. In bold are the best scores for the 15 single runs on training data and the corresponding 
test data. 
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 479?489,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Building Specialized Bilingual Lexicons Using Large-Scale Background
Knowledge
Dhouha Bouamor1, Adrian Popescu1, Nasredine Semmar1, Pierre Zweigenbaum2
1 CEA, LIST, Vision and Content Engineering Laboratory, 91191
Gif-sur-Yvette CEDEX, France; firstname.lastname@cea.fr
2LIMSI-CNRS, F-91403 Orsay CEDEX, France; pz@limsi.fr
Abstract
Bilingual lexicons are central components of
machine translation and cross-lingual infor-
mation retrieval systems. Their manual con-
struction requires strong expertise in both lan-
guages involved and is a costly process. Sev-
eral automatic methods were proposed as an
alternative but they often rely on resources
available in a limited number of languages
and their performances are still far behind
the quality of manual translations. We intro-
duce a novel approach to the creation of spe-
cific domain bilingual lexicon that relies on
Wikipedia. This massively multilingual en-
cyclopedia makes it possible to create lexi-
cons for a large number of language pairs.
Wikipedia is used to extract domains in each
language, to link domains between languages
and to create generic translation dictionaries.
The approach is tested on four specialized do-
mains and is compared to three state of the art
approaches using two language pairs: French-
English and Romanian-English. The newly in-
troduced method compares favorably to exist-
ing methods in all configurations tested.
1 Introduction
The plethora of textual information shared on the
Web is strongly multilingual and users? information
needs often go well beyond their knowledge of for-
eign languages. In such cases, efficient machine
translation and cross-lingual information retrieval
systems are needed. Machine translation already has
a decades long history and an array of commercial
systems were already deployed, including Google
Translate 1 and Systran 2. However, due to the intrin-
sic difficulty of the task, a number of related prob-
lems remain open, including: the gap between text
semantics and statistically derived translations, the
scarcity of resources in a large majority of languages
and the quality of automatically obtained resources
and translations. While the first challenge is general
and inherent to any automatic approach, the second
and the third can be at least partially addressed by
an appropriate exploitation of multilingual resources
that are increasingly available on the Web.
In this paper we focus on the automatic creation of
domain-specific bilingual lexicons. Such resources
play a vital role in Natural Language Processing
(NLP) applications that involve different languages.
At first, research on lexical extraction has relied on
the use of parallel corpora (Och and Ney, 2003).
The scarcity of such corpora, in particular for spe-
cialized domains and for language pairs not involv-
ing English, pushed researchers to investigate the
use of comparable corpora (Fung, 1998; Chiao and
Zweigenbaum, 2003). These corpora include texts
which are not exact translation of each other but
share common features such as domain, genre, sam-
pling period, etc.
The basic intuition that underlies bilingual lexi-
con creation is the distributional hypothesis (Harris,
1954) which puts that words with similar meanings
occur in similar contexts. In a multilingual formu-
lation, this hypothesis states that the translations of
a word are likely to appear in similar lexical envi-
ronments across languages (Rapp, 1995). The stan-
dard approach to bilingual lexicon extraction builds
1http://translate.google.com/
2http://www.systransoft.com/
479
on the distributional hypothesis and compares con-
text vectors for each word of the source and tar-
get languages. In this approach, the comparison of
context vectors is conditioned by the existence of a
seed bilingual dictionary. A weakness of the method
is that poor results are obtained for language pairs
that are not closely related (Ismail and Manandhar,
2010). Another important problem occurs whenever
the size of the seed dictionary is small due to ignor-
ing many context words. Conversely, when dictio-
naries are detailed, ambiguity becomes an important
drawback.
We introduce a bilingual lexicon extraction ap-
proach that exploits Wikipedia in an innovative
manner in order to tackle some of the problems
mentioned above. Important advantages of using
Wikipedia are:
? The resource is available in hundreds of lan-
guages and it is structured as unambiguous con-
cepts (i.e. articles).
? The languages are explicitly linked through
concept translations proposed by Wikipedia
contributors.
? It covers a large number of domains and is thus
potentially useful in order to mine a wide array
of specialized lexicons.
Mirroring the advantages, there are a number of
challenges associated with the use of Wikipedia:
? The comparability of concept descriptions in
different languages is highly variable.
? The translation graph is partial since, when
considering any language pair, only a part of
the concepts are available in both languages
and explicitly connected.
? Domains are unequally covered in Wikipedia
(Halavais and Lackaff, 2008) and efficient do-
main targeting is needed.
The approach introduced in this paper aims to
draw on Wikipedia?s advantages while appropri-
ately addressing associated challenges. Among
the techniques devised to mine Wikipedia content,
we hypothesize that an adequate adaptation of Ex-
plicit Semantic Analysis (ESA) (Gabrilovich and
Markovitch, 2007) is fitted to our application con-
text. ESA was already successfully tested in differ-
ent NLP tasks, such as word relatedness estimation
or text classification, and we modify it to mine spe-
cialized domains, to characterize these domains and
to link them across languages.
The evaluation of the newly introduced approach
is realized on four diversified specialized domains
(Breast Cancer, Corporate Finance, Wind Energy
and Mobile Technology) and for two pairs of lan-
guages: French-English and Romanian-English.
This choice allows us to study the behavior of dif-
ferent approaches for a pair of languages that are
richly represented and for a pair that includes Roma-
nian, a language that has fewer associated resources
than French and English. Experimental results show
that the newly introduced approach outperforms the
three state of the art methods that were implemented
for comparison.
2 Related Work
In this section, we first give a review of the stan-
dard approach and then introduce methods that build
upon it. Finally, we discuss works that rely on Ex-
plicit Semantic Analysis to solve other NLP tasks.
2.1 Standard Approach (SA)
Most previous approaches that address bilingual lex-
icon extraction from comparable corpora are based
on the standard approach (Fung, 1998; Chiao and
Zweigenbaum, 2002; Laroche and Langlais, 2010).
This approach is composed of three main steps:
1. Building context vectors: Vectors are first
extracted by identifying the words that ap-
pear around the term to be translated Wcand
in a window of n words. Generally, asso-
ciation measures such as the mutual infor-
mation (Morin and Daille, 2006), the log-
likelihood (Morin and Prochasson, 2011) or the
Discounted Odds-Ratio (Laroche and Langlais,
2010) are employed to shape the context vec-
tors.
2. Translation of context vectors: To enable the
comparison of source and target vectors, source
vectors are translated intoto the target language
by using a seed bilingual dictionary. When-
ever several translations of a context word exist,
480
all translation variants are taken into account.
Words not included in the seed dictionary are
simply ignored.
3. Comparison of source and target vectors:
Given Wcand, its automatically translated con-
text vector is compared to the context vectors
of all possible translations from the target lan-
guage. Most often, the cosine similarity is
used to rank translation candidates but alterna-
tive metrics, including the weighted Jaccard in-
dex (Prochasson et al, 2009) and the city-block
distance (Rapp, 1999), were studied.
2.2 Improvements of the Standard Approach
Most of the improvements of the standard approach
are based on the observation that the more repre-
sentative the context vectors of a candidate word
are, the better the bilingual lexicon extraction is. At
first, additional linguistic resources, such as special-
ized dictionaries (Chiao and Zweigenbaum, 2002) or
transliterated words (Prochasson et al, 2009), were
combined with the seed dictionary to translate con-
text vectors.
The ambiguities that appear in the seed bilingual
dictionary were taken into account more recently.
(Morin and Prochasson, 2011) modify the standard
approach by weighting the different translations ac-
cording to their frequency in the target corpus. In
(Bouamor et al, 2013), we proposed a method that
adds a word sense disambiguation process relying
on semantic similarity measurement from WordNet
to the standard approach. Given a context vector in
the source language, the most probable translation of
polysemous words is identified and used for build-
ing the corresponding vector in the target language.
The most probable translation is identified using the
monosemic words that appear in the same lexical en-
vironment.
On specialized French-English comparable cor-
pora, this approach outperforms the one proposed in
(Morin and Prochasson, 2011), which is itself bet-
ter than the standard approach. The main weakness
of (Bouamor et al, 2013) is that the approach relies
on WordNet and its application depends on the ex-
istence of this resource in the target language. Also,
the method is highly dependent on the coverage of
the seed bilingual dictionary.
2.3 Explicit Semantic Analysis
Explicit Semantic Analysis (ESA) (Gabrilovich and
Markovitch, 2007) is a method that maps textual
documents onto a structured semantic space using
classical text indexing schemes such as TF-IDF. Ex-
amples of semantic spaces used include Wikipedia
or the Open Directory Project but, due to superior
performances, Wikipedia is most frequently used.
In the original evaluation, ESA outperformed state
of the art methods in a word relatedness estimation
task.
Subsequently, ESA was successfully exploited in
other NLP tasks and in information retrieval. Radin-
sky and al. (2011) added a temporal dimension to
word vectors and showed that this addition improves
the results of word relatedness estimation. (Hassan
and Mihalcea, 2011) introduced Salient Semantic
Analysis (SSA), a development of ESA that relies
on the detection of salient concepts prior to map-
ping words to concepts. SSA and the original ESA
implementation were tested on several word related-
ness datasets and results were mixed. Improvements
were obtained for text classification when compar-
ing SSA with the authors? in-house representation
of the method. ESA has weak language depen-
dence and was already deployed in multilingual con-
texts. (Sorg and Cimiano, 2012) extended ESA to
other languages and showed that it is useful in cross-
lingual and multilingual retrieval task. Their focus
was on creating a language independent conceptual
space in which documents would be mapped and
then retrieved.
Some open ESA topics related to bilingual lex-
icon creation include: (1) the document represen-
tation which is simply done by summing individ-
ual contributions of words, (2) the adaptation of the
method to specific domains and (3) the coverage of
the underlying resource in different language.
3 ESA for Bilingual Lexicon Extraction
The main objective of our approach is to devise lex-
icon translation methods that are easily applicable
to a large number of language pairs, while preserv-
ing the overall quality of results. A subordinated
objective is to exploit large scale background mul-
tilingual knowledge, such as the encyclopedic con-
tent available in Wikipedia. As we mentioned, ESA
481
Figure 1: Overview of the Explicit Semantic
Analysis enabled bilingual lexicon extraction.
(Gabrilovich and Markovitch, 2007) was exploited
in a number of NLP tasks but not in bilingual lexi-
con extraction.
Figure 1 shows the overall architecture of the lex-
ical extraction process we propose. The process is
completed in the following three steps:
1. Given a word to be translated and its con-
text vector in the source language, we derive
a ranked list of similar Wikipedia concepts (i.e.
articles) using the ESA inverted index.
2. Then, a translation graph is used to retrieve the
corresponding concepts in the target language.
3. Candidate translations are found through a sta-
tistical processing of concept descriptions from
the ESA direct index in the target language.
In this section, we first introduce the elements of
the original formulation of ESA necessary in our ap-
proach. Then, we detail the three steps that com-
pose the main bilingual lexicon extraction method
illustrated in Figure 1. Finally, as a complement to
the main method we introduce a measure for domain
word specificity and present a method for extracting
generic translation lexicons.
3.1 ESA Word and Concept Representation
Given a semantic space structured using a set of M
concepts and including a dictionary of N words,
a mapping between words and concepts can be
expressed as the following matrix:
w(W1, C1) w(W2, C1) ... w(WN , C1)
w(W1, C2) w(W2, C2) ... w(WN , C2)
... ... ...
w(W1, CM ) w(W2, CM ) ... w(WN , CM )
When Wikipedia is exploited concepts are
equated to Wikipedia articles and the texts of the ar-
ticles are processed in order to obtain the weights
that link words and concepts. In (Gabrilovich and
Markovitch, 2007), the weights w that link words
and concepts were obtained through a classical TF-
IDF weighting of Wikipedia articles. A series of
tweaks destined to improve the method?s perfor-
mance were used and disclosed later3. For instance,
administration articles, lists, articles that are too
short or have too few links are discarded. Higher
weight is given to words in the article title and
more longer articles are favored over shorter ones.
We implemented a part of these tweaks and tested
our own version of ESA with the Wikipedia ver-
sion used in the original implementation. The cor-
relation with human judgments of word relatedness
was 0.72 against 0.75 reported by (Gabrilovich and
Markovitch, 2007). The ESA matrix is sparse since
the N size of the dictionary, is usually in the range
of hundreds of thousands and each concept is usu-
ally described by hundreds of distinct words. The
direct ESA index from Figure 1 is obtained by read-
ing the matrix horizontally while the inverted ESA
index is obtained by reading the matrix vertically.
3https://github.com/faraday/
wikiprep-esa/wiki/roadmap
482
Terme Concepts
action e?valuation d?action, communisme, actionnaire activiste, socialisme,
de?velopement durable . . .
de?ficit crise de la dette dans la zone euro, dette publique, re`gle d?or budge?taire,
de?ficit, trouble du de?ficit de l?attention . . .
cisaillement taux de cisaillement, zone de cisaillement, cisaillement, contrainte de cisaille-
ment, viscoanalyseur . . .
turbine ffc turbine potsdam, turbine a` gaz, turbine, urbine hydraulique, coge?ne?ration
. . .
cryptage TEMPEST, chiffrement, liaison 16, Windows Vista, transfert de fichiers . . .
protocole Ad-hoc On-demand Distance Vector, protocole de Kyoto, optimized link state
routing protocol, liaison 16, IPv6 . . .
biopsie biopsie, maladie de Horton, cancer du sein, cancer du poumon, imagerie par
re?sonance magne?tique . . .
palpation cancer du sein, cellulite, examen clinique, appendicite, te?nosynovite . . .
Table 1: The five most similar Wikipedia concepts to the French terms action[share], de?ficit[deficit], ci-
saillement[shear], turbine[turbine], cryptage[encryption], biopsie[biopsie] and palpation[palpation] and
their context vectors.
3.2 Source Language Processing
The objective of the source language processing is
to obtain a ranked list of similar Wikipedia concepts
for each candidate word (Wcand) in a specialized do-
main. To do this, a context vector is first built for
each Wcand from a specialized monolingual corpus.
The association measure between Wcand and context
words is obtained using the Odds-Ratio (defined in
equation 5). Wikipedia concepts in the source lan-
guage Cs that are similar to Wcand and to a part of its
context words are extracted and ranked using equa-
tion 1.
Rank(Cs) = (10 ?max(Odds
Wcand
Wsi
)
?w(Wcand, Cs)) +
n?
i=1
OddsWcandWsi
?w(Wsi , Cs)
(1)
where max(OddsWcandWsi
) is the highest Odds-Ratio
association between Wcand and any of its context
words Wsi ; the factor 10 was empirically set to
give more importance to Wcand over context words;
w(Wcand, Cs) is the weight of the association be-
tween Wcand and Cs from the ESA matrix; n is the
total number of words Wsi in the context vector of
Wcand; Odds
Wcand
Wsi
is the association value between
Wcand and Wsi and w(Wsi , Cs) are the weights of
the associations between each context word Wsi and
Cs from the ESA matrix. The use of contextual in-
formation in equation 1 serves to characterize the
candidate word in the target domain.
In table 1, we present the five most similar
Wikipedia concepts to the French terms action,
de?ficit, cisaillement, turbine, cryptage, biopsie and
palpation and their context vectors. These terms are
part of the four specialized domains we are studying
here. From observing these examples, we note that
despite the difference between the specialized do-
mains and word ambiguity (words action and proto-
cole), our method has the advantage of successfully
representing each word to be translated by relevant
conceptual spaces.
3.3 Translation Graph Construction
To bridge the gap between the source and target lan-
guages, a concept translation graph that enables the
multilingual extension of ESA is used. This con-
cept translation graph is extracted from the explicit
translation links available in Wikipedia articles and
is exploited in order to connect a word?s conceptual
space in the source language with the correspond-
ing conceptual space in the target language. Only a
part of the articles have translations and the size of
483
the conceptual space in the target language is usu-
ally smaller than the space in the source language.
For instance, the French-English translation graph
contains 940,215 pairs of concepts while the French
and English Wikipedias contain approximately 1.4
million articles, respectively 4.25 million articles.
3.4 Target Language Processing
The third step of the approach takes place in the tar-
get language. Using the translation graph, we select
the 100 most similar concept translations (thresh-
old determined empirically after preliminary exper-
iments) from the target language and use their di-
rect ESA representations in order to retrieve poten-
tial translations for the candidate word Wcand from
source language. These candidate translations Wt
are ranked using equation 2.
Rank(Wt) = (
n?
i=1
w(Wt, Cti)
avg(Cti)
)
? log(count(Wt,S)) (2)
with w(Wt, Cti) is the weight of the translation can-
didate WT for concept Cti from the ESA matrix
in the target language; avg(Cti) is the average TF-
IDF score of words that appear in Cti ; S is the set
of similar concepts Cti in the target language and
count(Wt,S) accounts for the number of different
concepts from S in which the candidate translation
WT appears.
The accumulation of weights w(Wt, Cti) fol-
lows the way original ESA text representations
are calculated (Gabrilovich and Markovitch, 2007)
and avg(Cti) is used in order to correct the
bias of the TF-IDF scheme towards short articles.
log(count(Wt,S)) is used to favor words that are
associated with a larger number of concepts. log
weighting was chosen after preliminary experiments
with a wide range of functions.
3.5 Domain Specificity
In previous works, ESA was usually exploited
in generic tasks that did not require any domain
adaptation. Here we process information from
specific domains and we need to measure the
specificity of words in those domains. The domain
extraction is seeded by using Wikipedia concepts
(noted Cseed) that best describes the domain in
the target language. For instance, in English,
the Corporate Finance domain is seeded with
https://en.wikipedia.org/wiki/Corporate finance.
We extract a set of 10 words with the highest
TF-IDF score from this article (noted SW ) and use
them to retrieve a domain ranking of concepts in the
target language Rankdom(Ct) with equation 3.
Rankdom(Ct) = (
n?
i=1
w(Wti , Ct)
? w(Cseed,Wti)) ? count(SW,Ct) (3)
where n is size of the seed list of words (i.e. 10
items), w(Wti , Ct) is the weight of the domain
words in the concept Ct ; w(Cseed,Wti) is the
weight of Wti in Cseed, the seed concept of the do-
main, and count(SW,Ct) is the number of distinct
seed words from SW that appear in Ct.
The first part of equation 3 sums up the contribu-
tions of different words from SW that appear in Ct
while the second part is meant to further reinforce
articles that contain a larger number of domain key-
words from SW .
Domain delimitation is performed by retaining
articles whose Rankdom(Ct) is at least 1% or the
score of the top Rankdom(Ct) score. This threshold
was set up during preliminary experiments. Given
the delimitation obtained with equation 3, we calcu-
late a domain specificity score (specifdom(Wt)) for
each word that occurs in the domain ( equation 4).
specifdom(Wt) estimates how much of a word?s use
in an underlying corpus is related to a target domain.
specifdom(Wt) =
DFdom(Wt)
DFgen(Wt)
(4)
where DFdom and DFgen stand for the domain and
the generic document frequency of the word Wt.
specifdom(Wt) will be used to favor words with
greater domain specificity over more general ones
when several translations are available in a seed
generic translation lexicon. For instance, the French
word action is ambiguous and has English transla-
tions such as action, stock, share etc. In a general
case, the most frequent translation is action whereas
in a corporate finance context, share or stock are
more relevant. The specificity of the three transla-
tions, from highest to lowest, is: share, stock and ac-
tion and is used to rank these potential translations.
484
3.6 Generic Dictionaries
Generic translation dictionaries, already used by ex-
isting bilingual lexicon extraction approaches, can
also be integrated in the newly proposed approach.
The Wikipedia translation graph is transformed into
a translation dictionary by removing the disam-
biguation marks from ambiguous concept titles, as
well as lists, categories and other administration
pages. Moreover, since the approach does not han-
dle multiword units, we retain only translation pairs
that are composed of unigrams in both languages.
When existing, unigram redirections are also added
in each language.
The obtained dictionaries are incomplete since:
(1) Wikipedia focuses on concepts that are most of-
ten nouns, (2) specialized domain terms often do not
have an associated Wikipedia entry and (3) the trans-
lation graph covers only a fraction of the concepts
available in a language. For instance, the result-
ing translation dictionaries have 193,543 entries for
French-English and 136,681 entries for Romanian-
English. They can be used in addition to or instead
of other resources available and are especially useful
when there are only few other resources that link the
pair of languages processed.
4 Evaluation
The performances of our approach are evaluated
against the standard approach and its developments
proposed by (Morin and Prochasson, 2011) and
(Bouamor et al, 2013). In this section, we first
describe the data and resources we used in our ex-
periments. We then present differents parameters
needed in the implementation of the different meth-
ods tested. Finally, we discuss the obtained results.
4.1 Data and Resources
Comparable corpora
We conducted our experiments on four French-
English and Romanian-English specialized compa-
rable corpora: Corporate Finance, Breast Can-
cer, Wind Energy and Mobile Technology. For
the Romanian-English language pair, we used
Wikipedia to collect comparable corpora for all do-
mains since they were not already available. The
Wikipedia corpora are harvested using a category-
based selection. We consider the topic in the source
Domain FR EN
Corporate Finance 402,486 756,840
Breast Cancer 396,524 524,805
Wind Energy 145,019 345,607
Mobile Technology 197,689 144,168
Domain RO EN
Corporate Finance 206,169 524,805
Breast Cancer 22,539 322,507
Wind Energy 121,118 298,165
Mobile Technology 200,670 124,149
Table 2: Number of content words in the
comparable corpora.
language (for instance Cancer Mamar [Breast Can-
cer]) as a query to Wikipedia and extract all its sub-
topics (i.e., sub-categories) to construct a domain-
specific category tree. Then, based on the con-
structed tree, we collect all Wikipedia articles be-
longing to at least one of these categories and use
inter-language links to build the comparable cor-
pora.
Concerning the French-English pair, we followed
the strategy described above to extract the compa-
rable corpora related to the Corporate Finance and
Breast Cancer domains since they were otherwise
unavailable. For the two other domains, we used
the corpora released in the TTC project4. All cor-
pora were normalized through the following linguis-
tic preprocessing steps: tokenization, part-of-speech
tagging, lemmatization, and function word removal.
The resulting corpora5 sizes are presented in Table
2. The size of the domain corpora vary within and
across languages, with the corporate finance domain
being the richest in both languages. In Romanian,
Breast Cancer is particularly small, with approxi-
mately 22,000 tokens included. This variability will
allow us to test if there is a correlation between cor-
pus size and quality of results.
Bilingual dictionary
The seed generic French-English dictionary used
to translate French context vectors consists of an
in-house manually built resource which contains
approximately 120,000 entries. For Romanian-
4http://www.ttc-project.eu/index.php/
releases-publications
5Comparable corpora will be shared publicly
485
Domain FR-EN RO-EN
Corporate Finance 125 69
Breast Cancer 96 38
Wind Energy 89 38
Mobile Technology 142 94
Table 3: Sizes of the evaluation lists.
English, we used the generic dictionary extracted
following the procedure described in Subsection 3.6.
Gold standard
In bilingual terminology extraction from compara-
ble corpora, a reference list is required to evaluate
the performance of the alignment. Such lists are usu-
ally composed of around 100 single terms (Hazem
and Morin, 2012; Chiao and Zweigenbaum, 2002).
Reference lists6 were created for the four specialized
domains and the two pairs of languages. For the
French-English, reference words from the Corpo-
rate Finance domain were extracted from the glos-
sary of bilingual micro-finance terms7. For Breast
Cancer, the list is derived from the MESH and the
UMLS thesauri8. Concerning Wind Energy and Mo-
bile Technology, lists were extracted from special-
ized glossaries found on the Web. The Romanian-
English gold standard was manually created by a na-
tive speaker starting from the French-English lists.
Table 3 displays the sizes of the obtained lists. Ref-
erence terms pairs were retained if each word com-
posing them appeared at least five times in the com-
parable domain corpora.
4.2 Experimental setup
Aside from those already mentioned, three param-
eters need to be set up: (1) the window size that
defines contexts, (2) the association measure that
measures the strength of the association between
words and the (3) similarity measure that ranks can-
didate translations for state of the art methods. Con-
text vectors are defined using a seven-word window
which approximates syntactic dependencies. The
association and the similarity measures (Discounted
Log-Odds ratio (equation 5) and the cosine simi-
6Reference lists will be shared publicly
7http://www.microfinance.lu/en/
8http://www.nlm.nih.gov/
larity) were set following Laroche and Langlais
(2010), a comprehensive study of the influence of
these parameters on the bilingual alignment.
Odds-Ratiodisc = log
(O11 + 12 )(O22 +
1
2 )
(O12 + 12 )(O21 +
1
2 )
(5)
where Oij are the cells of the 2? 2 contingency ma-
trix of a token s co-occurring with the term S within
a given window size.
The F-measure of the Top 20 results (F-
Measure@20), which measures the harmonic mean
of precision and recall, is used as evaluation metric.
Precision is the total number of correct translations
divided by the number of terms for which the system
returned at least one answer. Recall is equal to the
ratio between the number of correct translation and
the total number of words to translate (Wcand).
4.3 Results and discussion
In addition to the basic approach based on ESA
(denoted ESA), we evaluate the performances of
a method so-called DicoSpec in which the transla-
tions are extracted from a generic dictionary and
a method we called ESASpec which combine ESA
and DicoSpec. DICOSpec is based on the generic
dictionary we presented in subsection 3.6 and pro-
ceeds as follows: we extract a list of translations for
each word to be translated from the generic dictio-
nary. The domain specificity introduced in subsec-
tion 3.5 is then used to rank these translations. For
instance, the french term port referring in the Mobile
Technology domain, to the system that allows com-
puters to receive and transmit information is trans-
lated into port and seaport. According to domain
specificity values, the following ranking is obtained:
the English term port obtain the highest specificity
value (0.48). seaport comes next with a specificity
value of 0.01. In ESASpec, the translations set out in
the translations lists proposed by both ESA and the
generic dictionary are weighted according to their
domain specificity values. The main intuition be-
hind this method is that by adding the information
about the domain specificity, we obtain a new rank-
ing of the bilingual extraction results.
The obtained results are displayed in table 4. The
comparison of state of the art method shows that
BA13 performs better than STAPP and MP11 for
French-English and has comparable performances
486
a)
F
R
-E
N
Method
F-Measure@20
Breast Cancer Corporate Finance Wind Eenrgy Mobile Technology
STAPP 0.49 0.17 0.08 0.06
MP11 0.55 0.33 0.24 0.05
BA13 0.61 0.37 0.30 0.24
Dicospec 0.50 0.20 0.36 0.25
ESA 0.74 0.50 0.83 0.72
ESAspec 0.81 0.56 0.86 0.75
b)
R
O
-E
N
Method
F-Measure@20
Breast Cancer Corporate Finance Wind Eenrgy Mobile Technology
STAPP 0.21 0.13 0.08 0.16
MP11 0.21 0.13 0.08 0.16
BA13 0.21 0.14 0.08 0.17
Dicospec 0.44 0.11 0.21 0.16
ESA 0.76 0.17 0.58 0.53
ESAspec 0.78 0.24 0.58 0.55
Table 4: Results of the specialized dictionary creation on four specific domains, two pairs of languages.Three
state of the art methods were used for comparison: STAPP is the standard approach, MP11 is the improve-
ment of the standard approach introduced in (Morin and Prochasson, 2011), BA13 is a recent method that
we developed (Bouamor et al, 2013). Dicospec exploits a generic dictionary, combined with the use of do-
main specificity (see Subsection 3.5). ESA stands for the ESA based approach introduced in this paper (see
Figure 1). ESAspec combines the results of Dicospec and ESA.
for RO-EN. Consequently, we will use BA13 as the
main baseline for discussing the newly introduced
approach. The results presented in Table 4 show
that ESAspec clearly outperforms the three base-
lines for the four domains and the two pairs of lan-
guages tested. When comparing ESAspec to BA13
for French-English, improvements range between
0.19 for Corporate Finance and 0.56 for Wind En-
ergy. For RO-EN, the improvements vary from 0.1
for Corporate Finance to 0.5 for Wind Energy. Also,
except for the Corporate Finance domain in Roma-
nian, the performance variation across domains is
much smaller for ESAspec than for the three state
of the art methods. This shows that ESAspec is more
robust to domain change and thus more generic.
The results obtained with ESA are signifi-
cantly better than those obtained with Dicospec and
ESAspec, their combination, further improves the
results. The main contribution to ESAspec perfor-
mances comes from ESA, a finding that validates
our assumption that the adequate use of a rich multi-
lingual resource such as Wikipedia is appropriate for
specialized lexicon translation. Dicospec is a sim-
ple method that ranks the different meanings of a
candidate word available in a generic dictionary but
its average performances are comparable to those
of BA13 for FR-EN and higher for RO-EN. This
finding advocates for the importance of good qual-
ity generic dictionaries in specialized lexicon trans-
lation approaches. However, it is clear that such
dictionaries are far from being sufficient in order
to cover all possible domains. There is no clear
correlation between domain size and quality of re-
sults. Although richer than the other three domains,
Corporate Finance has the lowest associated per-
formances. This finding is probably explained by
the intrinsic difficulty of each domain. When pass-
ing from FR-EN to RO-EN the average performance
drop is more significant for BA13 than for the ESA
based methods. The result indicates that our ap-
proach is more robust to language change.
5 Conclusion
We have presented a new approach to the creation
of specialized bilingual lexicons, one of the central
487
building blocks of machine translation systems. The
proposed approach directly tackles two of the ma-
jor challenges identified in the Introduction. The
scarcity of resources is addressed by an adequate
exploitation of Wikipedia, a resource that is avail-
able in hundreds of languages. The quality of auto-
matic translations was improved by appropriate do-
main delimitation and linking across languages, as
well as by an adequate statistical processing of con-
cepts similar to a word in a given context.
The main advantages of our approach compared
to state of the art methods come from: the increased
number of languages that can be processed, from
the smaller sensitivity to structured resources and
the appropriate domain delimitation. Experimental
validation is obtained through evaluation with four
different domains and two pairs of languages which
shows consistent performance improvement. For
French-English, two languages that have rich asso-
ciated Wikipedia representations, performances are
very interesting and are starting to approach those of
manual translations for three domains out of four (F-
Measure@20 around 0.8). For Romanian-English, a
pair involving a language with a sparser Wikipedia
representation, the performances of our method drop
compared to French-English . However, they do not
decrease to the same extent as those of the best state
of the art method tested. This finding indicates that
our approach is more general and, given its low lan-
guage dependence, it can be easily extended to a
large number of language pairs.
The results presented here are very encouraging
and we will to pursue work in several directions.
First, we will pursue the integration of our method,
notably through comparable corpora creation using
the data driven domain delimitation technique de-
scribed in Subsection 3.5. Equally important, the
size of the domain can be adapted so as to find
enough context for all the words in domain reference
lists. Second, given a word in a context, we currently
exploit all similar concepts from the target language.
Given that comparability of article versions in the
source and the target language varies, we will eval-
uate algorithms for filtering out concepts from the
target language that have low alignment with their
source language versions. A final line of work is
constituted by the use of distributional properties of
texts in order to automatically rank parts of concept
descriptions (i.e. articles) by their relatedness to the
candidate word. Similar to the second direction, this
process involves finding comparable text blocks but
rather at a paragraph or sentence level than at the
article level.
References
Dhouha Bouamor, Nasredine Semmar, and Pierre
Zweigenbaum. 2013. Context vector disambiguation
for bilingual lexicon extraction. In Proceedings of the
51st Association for Computational Linguistics (ACL-
HLT), Sofia, Bulgaria, August.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in spe-
cialized, comparable corpora. In Proceedings of the
19th international conference on Computational lin-
guistics - Volume 2, COLING ?02, pages 1?5. Associ-
ation for Computational Linguistics.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2003. The
effect of a general lexicon in corpus-based identifi-
cation of french-english medical word translations.
In Proceedings Medical Informatics Europe, volume
95 of Studies in Health Technology and Informatics,
pages 397?402, Amsterdam.
Pascale Fung. 1998. A statistical view on bilingual lexi-
con extraction: From parallel corpora to non-parallel
corpora. In Parallel Text Processing, pages 1?17.
Springer.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using wikipedia-based
explicit semantic analysis. In Proceedings of the
20th international joint conference on Artifical intel-
ligence, IJCAI?07, pages 1606?1611, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Alexander Halavais and Derek Lackaff. 2008. An Anal-
ysis of Topical Coverage of Wikipedia. Journal of
Computer-Mediated Communication, 13(2):429?440.
Z.S. Harris. 1954. Distributional structure. Word.
Samer Hassan and Rada Mihalcea. 2011. Semantic re-
latedness using salient semantic analysis. In AAAI.
Amir Hazem and Emmanuel Morin. 2012. Adaptive dic-
tionary for bilingual lexicon extraction from compara-
ble corpora. In Proceedings, 8th international confer-
ence on Language Resources and Evaluation (LREC),
Istanbul, Turkey, May.
Azniah Ismail and Suresh Manandhar. 2010. Bilin-
gual lexicon extraction from comparable corpora us-
ing in-domain terms. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics:
Posters, COLING ?10, pages 481?489. Association for
Computational Linguistics.
488
Audrey Laroche and Philippe Langlais. 2010. Revisiting
context-based projection methods for term-translation
spotting in comparable corpora. In 23rd Interna-
tional Conference on Computational Linguistics (Col-
ing 2010), pages 617?625, Beijing, China, Aug.
Emmanuel Morin and Be?atrice Daille. 2006. Compara-
bilite? de corpus et fouille terminologique multilingue.
In Traitement Automatique des Langues (TAL).
Emmanuel Morin and Emmanuel Prochasson. 2011.
Bilingual lexicon extraction from comparable corpora
enhanced with parallel corpora. In Proceedings, 4th
Workshop on Building and Using Comparable Cor-
pora (BUCC), page 27?34, Portland, Oregon, USA.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Comput. Linguist., 29(1):19?51, March.
Emmanuel Prochasson, Emmanuel Morin, and Kyo
Kageura. 2009. Anchor points for bilingual lexi-
con extraction from small comparable corpora. In
Proceedings, 12th Conference on Machine Translation
Summit (MT Summit XII), page 284?291, Ottawa, On-
tario, Canada.
Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich,
and Shaul Markovitch. 2011. A word at a time: com-
puting word relatedness using temporal semantic anal-
ysis. In Proceedings of the 20th international confer-
ence on World wide web, WWW ?11, pages 337?346,
New York, NY, USA. ACM.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd annual
meeting on Association for Computational Linguistics,
ACL ?95, pages 320?322. Association for Computa-
tional Linguistics.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated english and german cor-
pora. In Proceedings of the 37th annual meeting of the
Association for Computational Linguistics on Compu-
tational Linguistics, ACL ?99, pages 519?526. Asso-
ciation for Computational Linguistics.
P. Sorg and P. Cimiano. 2012. Exploiting wikipedia for
cross-lingual and multilingual information retrieval.
Data Knowl. Eng., 74:26?45, April.
489
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 162?168, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
LIPN-CORE: Semantic Text Similarity using n-grams, WordNet, Syntactic
Analysis, ESA and Information Retrieval based Features
Davide Buscaldi, Joseph Le Roux,
Jorge J. Garc??a Flores
Laboratoire d?Informatique de Paris Nord,
CNRS, (UMR 7030)
Universite? Paris 13, Sorbonne Paris Cite?,
F-93430, Villetaneuse, France
{buscaldi,joseph.le-roux,jgflores}
@lipn.univ-paris13.fr
Adrian Popescu
CEA, LIST,
Vision & Content
Engineering Laboratory
F-91190 Gif-sur-Yvette, France
adrian.popescu@cea.fr
Abstract
This paper describes the system used by the
LIPN team in the Semantic Textual Similarity
task at *SEM 2013. It uses a support vector re-
gression model, combining different text sim-
ilarity measures that constitute the features.
These measures include simple distances like
Levenshtein edit distance, cosine, Named En-
tities overlap and more complex distances like
Explicit Semantic Analysis, WordNet-based
similarity, IR-based similarity, and a similar-
ity measure based on syntactic dependencies.
1 Introduction
The Semantic Textual Similarity task (STS) at
*SEM 2013 requires systems to grade the degree of
similarity between pairs of sentences. It is closely
related to other well known tasks in NLP such as tex-
tual entailment, question answering or paraphrase
detection. However, as noticed in (Ba?r et al, 2012),
the major difference is that STS systems must give a
graded, as opposed to binary, answer.
One of the most successful systems in *SEM
2012 STS, (Ba?r et al, 2012), managed to grade pairs
of sentences accurately by combining focused mea-
sures, either simple ones based on surface features
(ie n-grams), more elaborate ones based on lexical
semantics, or measures requiring external corpora
such as Explicit Semantic Analysis, into a robust
measure by using a log-linear regression model.
The LIPN-CORE system is built upon this idea of
combining simple measures with a regression model
to obtain a robust and accurate measure of tex-
tual similarity, using the individual measures as fea-
tures for the global system. These measures include
simple distances like Levenshtein edit distance, co-
sine, Named Entities overlap and more complex dis-
tances like Explicit Semantic Analysis, WordNet-
based similarity, IR-based similarity, and a similar-
ity measure based on syntactic dependencies.
The paper is organized as follows. Measures are
presented in Section 2. Then the regression model,
based on Support Vector Machines, is described in
Section 3. Finally we discuss the results of the sys-
tem in Section 4.
2 Text Similarity Measures
2.1 WordNet-based Conceptual Similarity
(Proxigenea)
First of all, sentences p and q are analysed in or-
der to extract all the included WordNet synsets. For
each WordNet synset, we keep noun synsets and put
into the set of synsets associated to the sentence, Cp
and Cq, respectively. If the synsets are in one of the
other POS categories (verb, adjective, adverb) we
look for their derivationally related forms in order
to find a related noun synset: if there is one, we put
this synsets in Cp (or Cq). For instance, the word
?playing? can be associated in WordNet to synset
(v)play#2, which has two derivationally related
forms corresponding to synsets (n)play#5 and
(n)play#6: these are the synsets that are added
to the synset set of the sentence. No disambiguation
process is carried out, so we take all possible mean-
ings into account.
GivenCp andCq as the sets of concepts contained
in sentences p and q, respectively, with |Cp| ? |Cq|,
162
the conceptual similarity between p and q is calcu-
lated as:
ss(p, q) =
?
c1?Cp
max
c2?Cq
s(c1, c2)
|Cp|
(1)
where s(c1, c2) is a conceptual similarity measure.
Concept similarity can be calculated by different
ways. For the participation in the 2013 Seman-
tic Textual Similarity task, we used a variation of
the Wu-Palmer formula (Wu and Palmer, 1994)
named ?ProxiGenea? (from the french Proximite?
Ge?ne?alogique, genealogical proximity), introduced
by (Dudognon et al, 2010), which is inspired by the
analogy between a family tree and the concept hi-
erarchy in WordNet. Among the different formula-
tions proposed by (Dudognon et al, 2010), we chose
the ProxiGenea3 variant, already used in the STS
2012 task by the IRIT team (Buscaldi et al, 2012).
The ProxiGenea3 measure is defined as:
s(c1, c2) =
1
1 + d(c1) + d(c2)? 2 ? d(c0)
(2)
where c0 is the most specific concept that is present
both in the synset path of c1 and c2 (that is, the Least
Common Subsumer or LCS). The function returning
the depth of a concept is noted with d.
2.2 IC-based Similarity
This measure has been proposed by (Mihalcea et
al., 2006) as a corpus-based measure which uses
Resnik?s Information Content (IC) and the Jiang-
Conrath (Jiang and Conrath, 1997) similarity metric:
sjc(c1, c2) =
1
IC(c1) + IC(c2)? 2 ? IC(c0)
(3)
where IC is the information content introduced by
(Resnik, 1995) as IC(c) = ? logP (c).
The similarity between two text segments T1 and
T2 is therefore determined as:
sim(T1, T2) =
1
2
?
?
?
?
w?{T1}
max
w2?{T2}
ws(w,w2) ? idf(w)
?
w?{T1}
idf(w)
+
?
w?{T2}
max
w1?{T1}
ws(w,w1) ? idf(w)
?
w?{T2}
idf(w)
?
?
?(4)
where idf(w) is calculated as the inverse document
frequency of word w, taking into account Google
Web 1T (Brants and Franz, 2006) frequency counts.
The semantic similarity between words is calculated
as:
ws(wi, wj) = max
ci?Wi,cjinWj
sjc(ci, cj). (5)
where Wi and Wj are the sets containing all synsets
in WordNet corresponding to word wi and wj , re-
spectively. The IC values used are those calcu-
lated by Ted Pedersen (Pedersen et al, 2004) on the
British National Corpus1.
2.3 Syntactic Dependencies
We also wanted for our systems to take syntac-
tic similarity into account. As our measures are
lexically grounded, we chose to use dependen-
cies rather than constituents. Previous experiments
showed that converting constituents to dependen-
cies still achieved best results on out-of-domain
texts (Le Roux et al, 2012), so we decided to use
a 2-step architecture to obtain syntactic dependen-
cies. First we parsed pairs of sentences with the
LORG parser2. Second we converted the resulting
parse trees to Stanford dependencies3.
Given the sets of parsed dependenciesDp andDq,
for sentence p and q, a dependency d ? Dx is a
triple (l, h, t) where l is the dependency label (for in-
stance, dobj or prep), h the governor and t the depen-
dant. We define the following similarity measure be-
tween two syntactic dependencies d1 = (l1, h1, t1)
and d2 = (l2, h2, t2):
dsim(d1, d2) = Lev(l1, l2)
?
idfh ? sWN (h1, h2) + idft ? sWN (t1, t2)
2
(6)
where idfh = max(idf(h1), idf(h2)) and idft =
max(idf(t1), idf(t2)) are the inverse document fre-
quencies calculated on Google Web 1T for the gov-
ernors and the dependants (we retain the maximum
for each pair), and sWN is calculated using formula
2, with two differences:
? if the two words to be compared are antonyms,
then the returned score is 0;
1
http://www.d.umn.edu/?tpederse/similarity.html
2
https://github.com/CNGLdlab/LORG-Release
3We used the default built-in converter provided with the
Stanford Parser (2012-11-12 revision).
163
? if one of the words to be compared is not in
WordNet, their similarity is calculated using
the Levenshtein distance.
The similarity score between p and q, is then cal-
culated as:
sSD(p, q) = max
?
?
?
?
di?Dp
max
djinDq
dsim(di, dj)
|Dp|
,
?
di?Dq
max
djinDp
dsim(di, dj)
|Dq|
?
?
?
(7)
2.4 Information Retrieval-based Similarity
Let us consider two texts p and q, an Information Re-
trieval (IR) system S and a document collection D
indexed by S. This measure is based on the assump-
tion that p and q are similar if the documents re-
trieved by S for the two texts, used as input queries,
are ranked similarly.
Let be Lp = {dp1 , . . . , dpK} and Lq =
{dq1 , . . . , dqK}, dxi ? D the sets of the top K docu-
ments retrieved by S for texts p and q, respectively.
Let us define sp(d) and sq(d) the scores assigned by
S to a document d for the query p and q, respectively.
Then, the similarity score is calculated as:
simIR(p, q) = 1?
?
d?Lp?Lq
?
(sp(d)?sq(d))2
max(sp(d),sq(d))
|Lp ? Lq|
(8)
if |Lp ? Lq| 6= ?, 0 otherwise.
For the participation in this task we indexed a
collection composed by the AQUAINT-24 and the
English NTCIR-85 document collections, using the
Lucene6 4.2 search engine with BM25 similarity.
The K value was empirically set to 20 after some
tests on the STS 2012 data.
2.5 ESA
Explicit Semantic Analysis (Gabrilovich and
Markovitch, 2007) represents meaning as a
4
http://www.nist.gov/tac/data/data_desc.html#AQUAINT-2
5
http://metadata.berkeley.edu/NTCIR-GeoTime/
ntcir-8-databases.php
6
http://lucene.apache.org/core
weighted vector of Wikipedia concepts. Weights
are supposed to quantify the strength of the relation
between a word and each Wikipedia concept using
the tf-idf measure. A text is then represented as a
high-dimensional real valued vector space spanning
all along the Wikipedia database. For this particular
task we adapt the research-esa implementation
(Sorg and Cimiano, 2008)7 to our own home-made
weighted vectors corresponding to a Wikipedia
snapshot of February 4th, 2013.
2.6 N-gram based Similarity
This feature is based on the Clustered Keywords Po-
sitional Distance (CKPD) model proposed in (Bus-
caldi et al, 2009) for the passage retrieval task.
The similarity between a text fragment p and an-
other text fragment q is calculated as:
simngrams(p, q) =
?
?x?Q
h(x, P )
1
d(x, xmax)
?n
i=1wi
(9)
Where P is the set of n-grams with the highest
weight in p, where all terms are also contained in q;
Q is the set of all the possible n-grams in q and n
is the total number of terms in the longest passage.
The weights for each term and each n-gram are cal-
culated as:
? wi calculates the weight of the term tI as:
wi = 1?
log(ni)
1 + log(N)
(10)
Where ni is the frequency of term ti in the
Google Web 1T collection, and N is the fre-
quency of the most frequent term in the Google
Web 1T collection.
? the function h(x, P ) measures the weight of
each n-gram and is defined as:
h(x, Pj) =
{ ?j
k=1wk if x ? Pj
0 otherwise
(11)
7
http://code.google.com/p/research-esa/
164
Where wk is the weight of the k-th term (see
Equation 10) and j is the number of terms that
compose the n-gram x;
? 1d(x,xmax) is a distance factor which reduces the
weight of the n-grams that are far from the
heaviest n-gram. The function d(x, xmax) de-
termines numerically the value of the separa-
tion according to the number of words between
a n-gram and the heaviest one:
d(x, xmax) = 1 + k? ln(1 + L) (12)
where k is a factor that determines the impor-
tance of the distance in the similarity calcula-
tion and L is the number of words between a
n-gram and the heaviest one (see Equation 11).
In our experiments, k was set to 0.1, the default
value in the original model.
2.7 Other measures
In addition to the above text similarity measures, we
used also the following common measures:
2.7.1 Cosine
Given p = (wp1 , . . . , wpn) and q =
(wq1 , . . . , wqn) the vectors of tf.idf weights asso-
ciated to sentences p and q, the cosine distance is
calculated as:
simcos(p,q) =
n?
i=1
wpi ? wqi
?
n?
i=1
wpi2 ?
?
n?
i=1
wqi2
(13)
The idf value was calculated on Google Web 1T.
2.7.2 Edit Distance
This similarity measure is calculated using the
Levenshtein distance as:
simED(p, q) = 1?
Lev(p, q)
max(|p|, |q|)
(14)
where Lev(p, q) is the Levenshtein distance be-
tween the two sentences, taking into account the
characters.
2.7.3 Named Entity Overlap
We used the Stanford Named Entity Recognizer
by (Finkel et al, 2005), with the 7 class model
trained for MUC: Time, Location, Organization,
Person, Money, Percent, Date. Then we calculated a
per-class overlap measure (in this way, ?France? as
an Organization does not match ?France? as a Loca-
tion):
ONER(p, q) =
2 ? |Np ?Nq|
|Np|+ |Nq|
(15)
where Np and Nq are the sets of NEs found, respec-
tively, in sentences p and q.
3 Integration of Similarity Measures
The integration has been carried out using the
?-Support Vector Regression model (?-SVR)
(Scho?lkopf et al, 1999) implementation provided
by LIBSVM (Chang and Lin, 2011), with a radial
basis function kernel with the standard parameters
(? = 0.5).
4 Results
In order to evaluate the impact of the different fea-
tures, we carried out an ablation test, removing one
feature at a time and training a new model with the
reduced set of features. In Table 2 we show the re-
sults of the ablation test for each subset of the *SEM
2013 test set; in Table 1 we show the same test on the
whole test set. Note: the results have been calculated
as the Pearson correlation test on the whole test set
and not as an average of the correlation scores cal-
culated over the composing test sets.
Feature Removed Pearson Loss
None 0.597 0
N-grams 0.596 0.10%
WordNet 0.563 3.39%
SyntDeps 0.602 ?0.43%
Edit 0.584 1.31%
Cosine 0.596 0.10%
NE Overlap 0.603 ?0.53%
IC-based 0.598 ?0.10%
IR-Similarity 0.510 8.78%
ESA 0.601 ?0.38%
Table 1: Ablation test for the different features on the
whole 2013 test set.
165
FNWN Headlines OnWN SMT
Feature Pearson Loss Pearson Loss Pearson Loss Pearson Loss
None 0.404 0 0.706 0 0.694 0 0.301 0
N-grams 0.379 2.49% 0.705 0.12% 0.698 ?0.44% 0.289 1.16%
WordNet 0.376 2.80% 0.695 1.09% 0.682 1.17% 0.278 2.28%
SyntDeps 0.403 0.08% 0.699 0.70% 0.679 1.49% 0.284 1.62%
Edit 0.402 0.19% 0.689 1.70% 0.667 2.72% 0.286 1.50%
Cosine 0.393 1.03% 0.683 2.38% 0.676 1.80% 0.303 ?0.24%
NE Overlap 0.410 ?0.61% 0.700 0.67% 0.680 1.37% 0.285 1.58%
IC-based 0.391 1.26% 0.699 0.75% 0.669 2.50% 0.283 1.76%
IR-Similarity 0.426 ?2.21% 0.633 7.33% 0.589 10.46% 0.249 5.19%
ESA 0.391 1.22% 0.691 1.57% 0.702 ?0.81% 0.275 2.54%
Table 2: Ablation test for the different features on the different parts of the 2013 test set.
FNWN Headlines OnWN SMT ALL
N-grams 0.285 0.532 0.459 0.280 0.336
WordNet 0.395 0.606 0.552 0.282 0.477
SyntDeps 0.233 0.409 0.345 0.323 0.295
Edit 0.220 0.536 0.089 0.355 0.230
Cosine 0.306 0.573 0.541 0.244 0.382
NE Overlap 0.000 0.216 0.000 0.013 0.020
IC-based 0.413 0.540 0.642 0.285 0.421
IR-based 0.067 0.598 0.628 0.241 0.541
ESA 0.328 0.546 0.322 0.289 0.390
Table 3: Pearson correlation calculated on individual features.
The ablation test show that the IR-based feature
showed up to be the most effective one, especially
for the headlines subset (as expected), and, quite sur-
prisingly, on the OnWN data. In Table 3 we show
the correlation between each feature and the result
(feature values normalised between 0 and 5): from
this table we can also observe that, on average, IR-
based similarity was better able to capture the se-
mantic similarity between texts. The only exception
was the FNWN test set: the IR-based similarity re-
turned a 0 score 178 times out of 189 (94.1%), indi-
cating that the indexed corpus did not fit the content
of the FNWN sentences. This result shows also the
limits of the IR-based similarity score which needs
a large corpus to achieve enough coverage.
4.1 Shared submission with INAOE-UPV
One of the files submitted by INAOE-UPV,
INAOE-UPV-run3 has been produced using seven
features produced by different teams: INAOE, LIPN
and UMCC-DLSI. We contributed to this joint sub-
mission with the IR-based, WordNet and cosine fea-
tures.
5 Conclusions and Further Work
In this paper we introduced the LIPN-CORE sys-
tem, which combines semantic, syntactic an lexi-
cal measures of text similarity in a linear regression
model. Our system was among the best 15 runs for
the STS task. According to the ablation test, the best
performing feature was the IR-based one, where a
sentence is considered as a query and its meaning
represented as a set of documents indexed by an IR
system. The second and third best-performing mea-
sures were WordNet similarity and Levenshtein?s
edit distance. On the other hand, worst perform-
ing similarity measures were Named Entity Over-
lap, Syntactic Dependencies and ESA. However, a
correlation analysis calculated on the features taken
one-by-one shows that the contribution of a feature
166
on the overall regression result does not correspond
to the actual capability of the measure to represent
the semantic similarity between the two texts. These
results raise the methodological question of how to
combine semantic, syntactic and lexical similarity
measures in order to estimate the impact of the dif-
ferent strategies used on each dataset.
Further work will include richer similarity mea-
sures, like quasi-synchronous grammars (Smith and
Eisner, 2006) and random walks (Ramage et al,
2009). Quasi-synchronous grammars have been
used successfully for paraphrase detection (Das and
Smith, 2009), as they provide a fine-grained model-
ing of the alignment of syntactic structures, in a very
flexible way, enabling partial alignments and the in-
clusion of external features, like Wordnet lexical re-
lations for example. Random walks have been used
effectively for paraphrase recognition and as a fea-
ture for recognizing textual entailment. Finally, we
will continue analyzing the question of how to com-
bine a wide variety of similarity measures in such a
way that they tackle the semantic variations of each
dataset.
Acknowledgments
We would like to thank the Quaero project and the
LabEx EFL8 for their support to this work.
References
[Ba?r et al2012] Daniel Ba?r, Chris Biemann, Iryna
Gurevych, and Torsten Zesch. 2012. Ukp: Computing
semantic textual similarity by combining multiple
content similarity measures. In Proceedings of the
6th International Workshop on Semantic Evaluation,
held in conjunction with the 1st Joint Conference
on Lexical and Computational Semantics, pages
435?440, Montreal, Canada, June.
[Brants and Franz2006] Thorsten Brants and Alex Franz.
2006. Web 1t 5-gram corpus version 1.1.
[Buscaldi et al2009] Davide Buscaldi, Paolo Rosso,
Jose? Manuel Go?mez, and Emilio Sanchis. 2009. An-
swering questions with an n-gram based passage re-
trieval engine. Journal of Intelligent Information Sys-
tems (JIIS), 34(2):113?134.
[Buscaldi et al2012] Davide Buscaldi, Ronan Tournier,
Nathalie Aussenac-Gilles, and Josiane Mothe. 2012.
8http://www.labex-efl.org
Irit: Textual similarity combining conceptual simi-
larity with an n-gram comparison method. In Pro-
ceedings of the 6th International Workshop on Se-
mantic Evaluation (SemEval 2012), Montreal, Que-
bec, Canada.
[Chang and Lin2011] Chih-Chung Chang and Chih-Jen
Lin. 2011. LIBSVM: A library for support vector
machines. ACM Transactions on Intelligent Systems
and Technology, 2:27:1?27:27. Software available
at http://www.csie.ntu.edu.tw/?cjlin/
libsvm.
[Das and Smith2009] Dipanjan Das and Noah A. Smith.
2009. Paraphrase identification as probabilistic quasi-
synchronous recognition. In Proc. of ACL-IJCNLP.
[Dudognon et al2010] Damien Dudognon, Gilles Hubert,
and Bachelin Jhonn Victorino Ralalason. 2010.
Proxige?ne?a : Une mesure de similarite? conceptuelle.
In Proceedings of the Colloque Veille Strate?gique Sci-
entifique et Technologique (VSST 2010).
[Finkel et al2005] Jenny Rose Finkel, Trond Grenager,
and Christopher Manning. 2005. Incorporating non-
local information into information extraction systems
by gibbs sampling. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?05, pages 363?370, Stroudsburg, PA,
USA. Association for Computational Linguistics.
[Gabrilovich and Markovitch2007] Evgeniy Gabrilovich
and Shaul Markovitch. 2007. Computing seman-
tic relatedness using wikipedia-based explicit semantic
analysis. In Proceedings of the 20th international joint
conference on Artifical intelligence, IJCAI?07, pages
1606?1611, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
[Jiang and Conrath1997] J.J. Jiang and D.W. Conrath.
1997. Semantic similarity based on corpus statistics
and lexical taxonomy. In Proc. of the Int?l. Conf. on
Research in Computational Linguistics, pages 19?33.
[Le Roux et al2012] Joseph Le Roux, Jennifer Foster,
Joachim Wagner, Rasul Samad Zadeh Kaljahi, and
Anton Bryl. 2012. DCU-Paris13 Systems for the
SANCL 2012 Shared Task. In The NAACL 2012 First
Workshop on Syntactic Analysis of Non-Canonical
Language (SANCL), pages 1?4, Montre?al, Canada,
June.
[Mihalcea et al2006] Rada Mihalcea, Courtney Corley,
and Carlo Strapparava. 2006. Corpus-based and
knowledge-based measures of text semantic similarity.
In Proceedings of the 21st national conference on Ar-
tificial intelligence - Volume 1, AAAI?06, pages 775?
780. AAAI Press.
[Pedersen et al2004] Ted Pedersen, Siddharth Patward-
han, and Jason Michelizzi. 2004. Wordnet::similarity:
measuring the relatedness of concepts. In Demon-
stration Papers at HLT-NAACL 2004, HLT-NAACL?
167
Demonstrations ?04, pages 38?41, Stroudsburg, PA,
USA. Association for Computational Linguistics.
[Ramage et al2009] Daniel Ramage, Anna N. Rafferty,
and Christopher D. Manning. 2009. Random walks
for text semantic similarity. In Proceedings of the
2009 Workshop on Graph-based Methods for Natural
Language Processing, pages 23?31. The Association
for Computer Linguistics.
[Resnik1995] Philip Resnik. 1995. Using information
content to evaluate semantic similarity in a taxonomy.
In Proceedings of the 14th international joint confer-
ence on Artificial intelligence - Volume 1, IJCAI?95,
pages 448?453, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
[Scho?lkopf et al1999] Bernhard Scho?lkopf, Peter
Bartlett, Alex Smola, and Robert Williamson. 1999.
Shrinking the tube: a new support vector regression
algorithm. In Proceedings of the 1998 conference on
Advances in neural information processing systems II,
pages 330?336, Cambridge, MA, USA. MIT Press.
[Smith and Eisner2006] David A. Smith and Jason Eisner.
2006. Quasi-synchronous grammars: Alignment by
soft projection of syntactic dependencies. In Proceed-
ings of the HLT-NAACL Workshop on Statistical Ma-
chine Translation, pages 23?30, New York, June.
[Sorg and Cimiano2008] Philipp Sorg and Philipp Cimi-
ano. 2008. Cross-lingual Information Retrieval with
Explicit Semantic Analysis. In Working Notes for the
CLEF 2008 Workshop.
[Wu and Palmer1994] Zhibiao Wu and Martha Palmer.
1994. Verbs semantics and lexical selection. In Pro-
ceedings of the 32nd annual meeting on Association
for Computational Linguistics, ACL ?94, pages 133?
138, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
168
