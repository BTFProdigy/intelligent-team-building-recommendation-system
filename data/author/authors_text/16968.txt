First Joint Conference on Lexical and Computational Semantics (*SEM), pages 579?585,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
sranjans : Semantic Textual Similarity using Maximal Weighted
Bipartite Graph Matching
Sumit Bhagwani, Shrutiranjan Satapathy, Harish Karnick
Computer Science and Engineering
IIT Kanpur, Kanpur - 208016, India
{sumitb,sranjans,hk}@cse.iitk.ac.in
Abstract
The paper aims to come up with a sys-
tem that examines the degree of semantic
equivalence between two sentences. At the
core of the paper is the attempt to grade
the similarity of two sentences by find-
ing the maximal weighted bipartite match
between the tokens of the two sentences.
The tokens include single words, or multi-
words in case of Named Entitites, adjec-
tivally and numerically modified words.
Two token similarity measures are used for
the task - WordNet based similarity, and a
statistical word similarity measure which
overcomes the shortcomings of WordNet
based similarity. As part of three systems
created for the task, we explore a simple
bag of words tokenization scheme, a more
careful tokenization scheme which cap-
tures named entities, times, dates, mone-
tary entities etc., and finally try to capture
context around tokens using grammatical
dependencies.
1 Introduction
Semantic Textual Similarity (STS) measures
the degree of semantic equivalence between
texts. The goal of this task is to create a unified
framework for the evaluation of semantic textual
similarity modules and to characterize their im-
pact on NLP applications. The task is part of the
Semantic Evaluation 2012 Workshop (Agirre et
al., 2012).
STS is related to both Textual Entailment and
Paraphrase, but differs in a number of ways and
it is more directly applicable to a number of NLP
tasks. Also, STS is a graded similarity notion -
this graded bidirectional nature of STS is useful
for NLP tasks such as MT evaluation, informa-
tion extraction, question answering, and summa-
rization.
We propose a lexical similarity approach to
grade the similarity of two sentences, where a
maximal weighted bipartite match is found be-
tween the tokens of the two sentences. The ap-
proach is robust enough to apply across different
datasets. The results on the STS test datasets are
encouraging to say the least. The tokens are sin-
gle word tokens in case of the first system, while
in the second system, named and monetary en-
tities, percentages, dates and times are handled
too. A token-token similarity measure is integral
to the approach and we use both a statistical sim-
ilarity measure and a WordNet based word sim-
ilarity measure for the same. In the final run
of the task, apart from capturing the aforemen-
tioned entities, we heuristically extract adjecti-
vally and numerically modified words. Also, the
last run naively attempts to capture the context
around the tokens using grammatical dependen-
cies, which in turn is used to measure context
similarity.
Section 2 discusses the previous work done
in this area. Section 3 describes the datasets,
the baseline system and the evaluation measures
used by the task organizers. Section 4, 5 and 6
introduce the systems developed and discuss the
results of each system. Finally, section 7 con-
579
cludes the work and section 8 offers suggestions
for future work.
2 Related Work
Various systems exist in literature for tex-
tual similarity measurement, be it bag of
words based models or complex semantic sys-
tems. (Achananuparp et al, 2008) enumerates a
few word overlap measures, like Jaccard Similar-
ity Coefficient, IDF Overlap measures, Phrasal
overlap measures etc, that have been used for
sentential similarity.
(Liu et al, 2008) proposed an approach to cal-
culate sentence similarity, which takes into ac-
count both semantic information and word order.
They define semantic similarity of sentence 1 rel-
ative to sentence 2 as the ratio of the sum of the
word similarity weighted by information content
of words in sentence 1 to the overall information
content included in both sentences. The syntactic
similarity is calculated as the correlation coeffi-
cient between word order vectors.
A similar semantic similarity measure, pro-
posed by (Li et al, 2006), uses a semantic-vector
approach to measure sentence similarity. Sen-
tences are transformed into feature vectors hav-
ing individual words from the sentence pair as
a feature set. Term weights are derived from
the maximum semantic similarity score between
words in the feature vector and words in the cor-
responding sentence. To utilize word order in the
similarity calculation, they define a word order
similarity measure as the normalized difference
of word order between the two sentences. They
have empirically proved that a sentence simi-
larity measure performs the best when semantic
measure is weighted more than syntactic measure
(ratio ? 4:1). This follows the conclusion from
a psychological experiment conducted by them
which emphasizes the role of semantic informa-
tion over syntactic information in passage under-
standing.
3 Task Evaluation
3.1 Datasets
The development datasets are drawn from the
following sources :
? MSR Paraphrase : This dataset consists
of pairs of sentences which have been ex-
tracted from news sources on the web.
? MSR Video : This dataset consists of
pairs of sentences where each sentence of a
pair tries to summarize the action in a short
video snippet.
? SMT Europarl : This dataset consists of
pairs sentences drawn from the proceedings
of the European Parliament, where each
sentence of a pair is a translation from a Eu-
ropean language to English.
In addition to the above sources, the test
datasets also contained the following sources :
? SMT News : This dataset consists of ma-
chine translated news conversation sentence
pairs.
? On WN : This dataset consists of pairs
of sentences where the first comes from
Ontonotes(Hovy et al, 2006) and the sec-
ond from a WordNet definition. Hence, the
sentences are rather phrases.
3.2 Baseline
The task organizers have used the following
baseline scoring scheme. Scores are produced
using a simple word overlap baseline system.
The input sentences are tokenised by splitting
at white spaces, and then each sentence is rep-
resented as a vector in the multidimensional to-
ken space. Each dimension has 1 if the token is
present in the sentence, 0 otherwise. Similarity
of vectors is computed using the cosine similar-
ity.
3.3 Evaluation Criteria
The scores obtained by the participating systems
are evaluated against the gold standard of the
datasets using a pearson correlation measure. In
order to evaluate the overall performance of the
systems on all the five datasets, the organizers
use three evaluation measures :
? ALL : This measure takes the union of all
the test datasets, and finds the Pearson cor-
relation of the system scores with the gold
standard of the union.
580
? ALL Normalized : In this measure, a lin-
ear fit is found for the system scores on
each dataset using a least squared error cri-
terion, and then the union of the linearly fit-
ted scores is used to calculate the Pearson
correlation against the gold standard union.
? Weighted Mean : The average of the Pear-
son correlation scores of the systems on the
individual datasets is taken, weighted by the
number of test instances in each dataset.
4 SYSTEM 1
4.1 Tokenization Scheme
Each sentence is tokenized into words, filter-
ing out punctuations and stop-words. The stop-
words are taken from the stop-word list provided
by the NLTK Toolkit (Bird et al, 2009). All
the word tokens are reduced to their lemmatized
form using the Stanford CoreNLP Toolkit (Min-
nen et al, 2001). The tokenization is basic in
nature and doesn?t handle named entities, times,
dates, monetary entities or multi-word expres-
sions. The challenge with handling multi-word
tokens is in calculating multi-word token simi-
larity, which is not supported in a WordNet word-
similarity scheme or a statistical word similarity
measure.
4.2 Maximal Weighted Bipartite Match
A weighted bipartite graph is constructed
where the two sets of vertices are the word-
tokens extracted in the earlier subsection. The
bipartite graph is made complete by assigning an
edge weight to every pair of tokens from the two
sentences. The edge weight is based on a suitable
word similarity measure. We had two resources
at hand - WordNet based word similarity and a
statistical word similarity measure.
4.2.1 WordNet Based Word Similarity
The is-a hierarchy of WordNet is used in cal-
culating the word similarity of two words. Nouns
and verbs have separate is-a hierarchies. We use
the Lin word-sense similarity measure (Lin ,
1998a). Adjectives and adverbs do not have an
is-a hierarchy and hence do not figure in the Lin
similarity measure. To disambiguate the Word-
Net sense of a word in a sentence, a variant of
the Simplified Lesk Algorithm (Kilgarriff and
J. Rosenzweig , 2000) is used. WordNet based
word similarity has the following drawbacks :
? sparse in named entity content : similarity
of named entities with other words becomes
infeasible to calculate.
? doesn?t support cross-POS similarity.
? applicable only to nouns and verbs.
4.2.2 Statistical Word Similarity
We use DISCO (Kolb , 2008) as our statisti-
cal word similarity measure. DISCO is a tool for
retrieving the distributional similarity between
two given words. Pre-computed word spaces are
freely available for a number of languages. We
use the English Wikipedia word space. One pri-
mary reason for using a statistical word similarity
measure is because of the shortcomings of calcu-
lating cross-POS word similarity when using a
knowledge base like WordNet.
DISCO works as follows : a term-(term,
relative position) matrix is constructed with
weights being pointwise mutual information
scores. From this, a surface level word similar-
ity score is obtained by using Lin?s information
theoretic measure (Lin , 1998b) for word vector
similarity. This score is used as matrix weights
to get second order word vectors, which are used
to compute a second order word similarity mea-
sure . This measure tries to emulate an LSA like
similarity giving better performance, and hence
is used for the task.
A point to note here is that the precomputed
word spaces that DISCO uses are case sensitive,
which we think is a drawback. We preserve the
case of proper nouns, while all other words are
converted to lower case, prior to evaluating word
similarity scores.
4.3 Edge Weighting Scheme
Sentences in the MSR video dataset are simpler
and shorter than the remaining datasets, with a
high degree of POS correspondence between the
581
Dataset DISCO WordNet DISCO + WordNet
MSR Video 0.61 0.71 0.73
MSR Paraphrase 0.62 0.43 0.57
SMT Europarl 0.58 0.44 0.54
Figure 1: Edge Weight Scheme Evaluation on Development Datasets
Category NE Normalized NE
DATE 26th November, November 26 XXXX-11-26
PERCENT 13 percent, 13% %13.0
MONEY 56 dollars, $56, 56$ $56.0
TIME 3 pm, 15:00 T15:00
Figure 2: Normalization performed by Stanford CoreNLP
tokens of two sentences, as can be observed in
the following example :
? A man is riding a bicycle. VS A man is rid-
ing a bike.
This allows for the use of a Knowledge-Base
Word Similarity measure like WordNet word
similarity. All the other datasets have length-
ier sentences, resulting in cross-POS correspon-
dence. Additionally, there is an abundance of
named entities in these datasets. The following
examples, which are drawn from the MSR Para-
phrase dataset, highlight these points :
? If convicted of the spying charges, he could
face the death penalty. VS The charges of
espionage and aiding the enemy can carry
the death penalty.
? Microsoft has identified the freely dis-
tributed Linux software as one of the biggest
threats to its sales. VS The company
has publicly identified Linux as one of its
biggest competitive threats.
Keeping this in mind, we use DISCO for edge-
weighting in all the datasets except MSR Video.
For MSR Video, we use the following edge
weighting scheme : for same-POS words, Word-
Net similarity is used, DISCO otherwise. This
choice is justified by the results obtained in fig-
ure 1 on the development datasets.
4.3.1 Scoring
A maximal weighted bipartite match is found
for the bipartite graph constructed, using the
Hungarian Algorithm (Kuhn , 1955) - the
intuition behind this being that every keyword
in a sentence matches injectively to a unique
keyword in the other sentence. The maximal
bipartite score is normalized by the sentences?
length for two reasons - normalization and
punishment for extra detailing in either sentence.
So the final sentence similarity score between
sentences s1 and s2 is:
sim(s1, s2) =
MaximalBipartiteMatchSum(s1,s2)
max(tokens(s1),tokens(s2))
4.4 Results
The results are evaluated on the test datasets
provided for the STS task. Figure 3 compares
the performance of our systems with the top 3
systems for the task. The scores in the figure
are Pearson Correlation scores. Figure 4 shows
the performance and ranks of all our systems. A
total of 89 systems were submitted, including
the baseline. The results are taken from the
Semeval?12 Task 6 webpage1
As can be seen, System 1 suffers slightly
on the MSR Paraphrase and Video datasets,
while doing comparably well on the other three
datasets when compared with the top 3 submis-
sions. Our ALL score suffers because we use
1http://www.cs.york.ac.uk/semeval-
2012/task6/index.php?id=results-update
582
System ALL MSR Para-
phrase
MSR Video SMT Eu-
roparl
OnWN SMT News
Rank 1 0.8239 0.6830 0.8739 0.5280 0.6641 0.4937
Rank 2 0.8138 0.6985 0.8620 0.3612 0.7049 0.4683
Rank 3 0.8133 0.7343 0.8803 0.4771 0.6797 0.3989
System 1 0.6529 0.6124 0.7240 0.5581 0.6703 0.4533
System 2 0.6651 0.6254 0.7538 0.5328 0.6649 0.5036
System 3 0.5045 0.6167 0.7061 0.5666 0.5664 0.3968
Li et al 0.4981 0.6141 0.6084 0.5382 0.6055 0.3760
Baseline 0.3110 0.4334 0.2996 0.4542 0.5864 0.3908
Figure 3: Results of top 3 Systems and Our Systems
System ALL ALL Rank All Nor-
malized
All Nor-
malized
Rank
Weighted
Mean
Weighted
Mean
Rank
System 1 0.6529 30 0.8018 39 0.6249 12
System 2 0.6651 24 0.8128 22 0.6366 8
System 3 0.5045 62 0.7846 52 0.5905 30
Figure 4: Evaluation of our Systems on different criteria
a combination of WordNet and statistical word
similarity measure for the MSR Video dataset,
which affects the Pearson Correlation of all the
datasets combined. The correlation values for
the ALL Normalized criterion are high because
of the linear fitting it performs. We get the best
performance on the Weighted Mean evaluation
criterion.
5 SYSTEM 2
In System 2, in addition to System 1, we cap-
ture named entities, dates and times, percentages
and monetary entities and normalize them. The
tokens resulting from this can be multi-word be-
cause of named entities. This tokenization strat-
egy gives us the best results among all our three
runs. For capturing and normalizing the above
mentioned expressions, we make use of the Stan-
ford NER Toolkit (Finkel et al, 2005). Some
normalized samples are mentioned in figure 2.
When grading the similarity of multi-word
tokens, we use a second level maximal bipartite
match, which is normalized by the smaller of the
two multi-word token lengths. Thus, similarity
between two multi-word tokens t1 and t2 is
defined as:
sim(t1, t2) =
MaximalBipartiteMatchSum(t1,t2)
min(words(t1),words(t2))
This was done to ensure that a complete
named entity in the first sentence matches ex-
actly with a partial named entity (indicating the
same entity as the first) in the second sentence.
For eg. John Doe vs John will be given a score
of 1. Such occurrences are frequent in the task
datasets. For the sentence similarity, the score
defined in System 1 is used, where the token
length of a sentence is the number of multi-word
tokens in it.
5.1 Results
Refer to figures 3 and 4 for results.
This system gives the best results among all
our systems. The credit for this improvement can
be attributed to recognition and normalization of
named entities, dates and times, percentages and
monetary entities, as the datasets provided con-
tain these in fairly large numbers.
583
6 SYSTEM 3
In System 3, in addition to System 2, we heuris-
tically capture compound nouns, adjectivally
and numerically modified words like ?passenger
plane?, ?easy job?, ?10 years? etc. using the POS
based regular expression
[JJ |NN |CD]?NN
POS Tagging is done using the Stanford POS
Tagger Toolkit (Toutanova et al, 2003).
To make matching more context dependent,
rather than just a bag of words approach, we
naively attempt to capture the similarity of the
contexts of two tokens. We define the context
of a word in a sentence as all the words in the
sentence which are grammatically related to
it. The grammatical relations are all the col-
lapsed dependencies produced by the Stanford
Dependency parser (Marneffe et al, 2006). The
context of a multi-word token is defined as the
union of contexts of all the words in it. We
further filter the context by removing stop-words
and punctuations in it. The contexts of two
tokens are then used to obtain context/syntactic
similarity between tokens, which is defined
using the Jaccard Similarity Measure:
Jaccard(C1, C2) =
|C1 ? C2|
|C1 ? C2|
A linear combination of word similarity and
context similarity is taken as an edge weight in
the token-token similarity bipartite graph. Moti-
vated by (Li et al, 2006), we chose a ratio of 4:1
for lexical similarity to context similarity.
As in System 2, for multi-word token simi-
larity, we use a second level maximal bipartite
match, normalized by smaller of the two token
lengths. This helps in matching multi-word to-
kens expressing the same meaning with score
1, for e.g. passenger plane VS Cuban plane,
divided Supreme Court VS Supreme Court etc.
The sentence similarity score is the same as the
one defined in System 2.
6.1 Results
Refer to figures 3 and 4 for results.
This system gives a reduced performance
compared to our other systems. This could be
due to various factors. Capturing adjectivally and
numerically modified words could be done using
grammatical dependencies instead of a heuristic
POS-tag regular expression. Also, token-token
similarity should be handled in a more precise
way than a generic second level maximal bipar-
tite match. A better context capturing method
can further improve the system.
7 Conclusions
Among the three systems proposed for the task,
System 2 performs best on the test datasets, pri-
marily because it identifies named entities as sin-
gle entities, normalizes dates, times, percentages
and monetary figures. The results for System
3 suffer because of naive context capturing. A
better job can be done using syntacto-semantic
structured representations for the sentences. The
performance of our systems are compared with
(Li et al, 2006) on the test datasets in figure
3. This highlights the improvement of maximal
weighted bipartite matching over greedy match-
ing.
8 Future Work
Our objective is to group words together which
share a common meaning. This includes group-
ing adjectival, adverbial, numeric modifiers with
the modified word, group the words of a collo-
quial phrase together, capture multi-word expres-
sions, etc. These word-clusters will form the ver-
tices of the bipartite graph. The other challenge
then is to come up with a suitable cluster-cluster
similarity measure. NLP modules such as Lex-
ical Substitution can help when we are using a
word-word similarity measure at the core.
Acknowledgments
The authors would like to thank the anonymous
reviewers for their valuable comments and sug-
gestions to improve the quality of the paper.
584
References
Dan Klein and Christopher D. Manning. 2003. Ac-
curate Unlexicalized Parsing. Proceedings of the
41st Meeting of the Association for Computational
Linguistics, pp. 423-430.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw and Ralph Weischedel. 2006.
OntoNotes: The 90% Solution. Proceedings of
HLT/NAACL, New York, 2006.
Eneko Agirre, Daniel Cer, Mona Diab and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6:
A Pilot on Semantic Textual Similarity. In Pro-
ceedings of the 6th International Workshop on Se-
mantic Evaluation (SemEval 2012), in conjunc-
tion with the First Joint Conference on Lexical and
Computational Semantics (*SEM 2012).
G. Minnen, J. Carroll and D. Pearce. 2001. Ap-
plied morphological processing of English. Nat-
ural Language Engineering, 7(3). 207-223.
Harold W. Kuhn. 1955. The Hungarian Method for
the assignment problem. Naval Research Logistics
Quarterly, 2:8397, 1955.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local In-
formation into Information Extraction Systems by
Gibbs Sampling. Proceedings of the 43nd Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2005), pp. 363-370
Kilgarriff and J. Rosenzweig. 2000. English SEN-
SEVAL : Report and Results. In Proceedings of
the 2nd International Conference on Language Re-
sources and Evaluation, LREC, Athens, Greece.
Kristina Toutanova, Dan Klein, Christopher Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-
of-Speech Tagging with a Cyclic Dependency Net-
work. In Proceedings of HLT-NAACL 2003, pp.
252-259.
Lin, D. 1998a. An information-theoretic definition
of similarity. In Proceedings of the International
Conference on Machine Learning.
Lin, D. 1998b. Automatic Retrieval and Clustering of
Similar Words.. In Proceedings of COLING-ACL
1998, Montreal.
Marie-Catherine de Marneffe, Bill MacCartney and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses.
In LREC 2006.
Palakorn Achananuparp, Xiaohua Hu and Shen Xi-
ajiong. 2008. The Evaluation of Sentence Simi-
larity Measures. Science And Technology, 5182,
305-316. Springer.
Peter Kolb. 2008. DISCO: A Multilingual Database
of Distributionally Similar Words. In Proceedings
of KONVENS-2008, Berlin.
Steven Bird, Ewan Klein, and Edward Loper. 2009.
Natural Language Processing with Python - An-
alyzing Text with the Natural Language Toolkit.
O?Reilly Media, 2009
Xiao-Ying Liu, Yi-Ming Zhou, Ruo-Shi Zheng.
2008. Measuring Semantic Similarity Within Sen-
tences. Proceedings of the Seventh International
Conference on Machine Learning and Cybernetics,
Kunming.
Yuhua Li, David McLean, Zuhair A. Bandar, James
D. OShea, and Keeley Crockett. 2006. Sentence
Similarity Based on Semantic Nets and Corpus
Statistics. IEEE Transections on Knowledge and
Data Engineering, Vol. 18, No. 8
585
Proceedings of the TextGraphs-8 Workshop, pages 11?19,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Merging Word Senses
Sumit Bhagwani, Shrutiranjan Satapathy, Harish Karnick
Computer Science and Engineering
IIT Kanpur, Kanpur - 208016, India
{sumitb,sranjans,hk}@cse.iitk.ac.in
Abstract
WordNet, a widely used sense inventory for
Word Sense Disambiguation(WSD), is often
too fine-grained for many Natural Language
applications because of its narrow sense dis-
tinctions. We present a semi-supervised ap-
proach to learn similarity between WordNet
synsets using a graph based recursive sim-
ilarity definition. We seed our framework
with sense similarities of all the word-sense
pairs, learnt using supervision on human-
labelled sense clusterings. Finally we discuss
our method to derive coarse sense invento-
ries at arbitrary granularities and show that the
coarse-grained sense inventory obtained sig-
nificantly boosts the disambiguation of nouns
on standard test sets.
1 Introduction
With different applications requiring different levels
of word sense granularity, producing sense clustered
inventories with the requisite level of sense granu-
larity has become important. The subtleties of sense
distinctions captured by WordNet(Miller, 1995) are
helpful for language learners (Snow et al, 2007)
and in machine translation of languages as diverse
as Chinese and English (Ng et al, 2003). On the
other hand, for tasks like Document Categorization
and Information Retrieval (Buitelaar, 2000), it may
be sufficient to know if a given word belongs to a
coarsely defined class of WordNet senses. Using
the fine grained sense inventory of WordNet may be
detrimental to the performance of these applications.
Thus developing a framework which can generate
sense inventories with different granularities can im-
prove the performance of many applications.
To generate a coarse sense inventory, many re-
searchers have focused on generating coarse senses
for each word by merging the fine-grained senses
(Chugur et al, 2002) (Navigli, 2006). This approach
has two problems. First, it requires a stopping crite-
rion for each word ? for example the number of
final classes. The right number of classes for each
word cannot usually be predetermined even if the
application is known. So such systems cannot be
used to derive coarse senses for all the words. Sec-
ond, inconsistent sense clusters are obtained because
coarse senses are independently generated for each
word. This leads to transitive closure errors and sug-
gests that for deriving consistent coarse senses, in-
stead of clustering senses for each word separately
we should cluster synsets.
We propose a framework that derives a coarse
sense inventory by learning a synset similarity met-
ric. We focus on coarsening the noun synsets of
WordNet and show that the obtained coarse-grained
sense inventory greatly improves the noun sense
disambiguation. Our approach closely resembles
(Snow et al, 2007) for supervised learning of synset
similarity. But to learn similarity between synset
pairs which do not share a word we use a variant
of the SimRank framework (Jeh and Widom, 2002)
and avoid giving them zero similarity. Thus the sim-
ilarity learnt is more than a binary decision and is
reflective of a more comprehensive semantic simi-
larity between the synsets. The use of SimRank for
learning synset similarity is inspired by the success
of graph-centrality algorithms in WSD. We do not
modify the WordNet ontology, unlike (Snow et al,
2007), as it may introduce spurious relations and re-
move some manually encoded information.
11
In section 2, we discuss past work in sense clus-
tering. In section 3 and 4, we describe our frame-
work of learning synset similarity using SimRank.
In section 5, we discuss our methodology of produc-
ing coarse senses using the learnt similarity metric.
Section 6 describes the experimental setup and eval-
uates the framework described. Section 7 contains
conclusions and discusses the directions for future
work.
2 Related Work
A wide variety of automatic methods have been pro-
posed for coarsening fine-grained inventories. The
earliest attempt on WordNet include (Mihalcea and
Moldovan, 2001) which merged synsets on seman-
tic principles like sharing a pertainym, antonym or
verb group. We discuss some of the ideas which
are related to our work. Though promising, many of
these techniques are severely limited by the amount
of available manually annotated data.
(Chugur et al, 2002) constructed sense similarity
matrices using translation equivalences in four lan-
guages. With the advent of WordNets being devel-
oped in multiple languages1 as well as multilingual
ontologies like BabelNet (Navigli and Ponzetto,
2012), this seems a promising area.
(McCarthy, 2006) estimated sense similarities us-
ing a combination of word-to-word distributional
similarity combined with the JCN WordNet based
similarity measure (Jiang and Conrath, 1997). They
introduce a more relaxed notion of sense relatedness
which allows the user to control the granularity for
the application in hand.
(Navigli, 2006) produced a fixed set sense clusters
by mapping WordNet word senses to Oxford En-
glish Dictionary(OED) word senses exploiting sim-
ilarities in glosses and semantic relationships in the
sense inventories. It is expected that the different
WordNet senses that are semantically close mapped
to the same sense in the other ontology via an ef-
ficient mapping that is able to capture the semantic
similarity between the concepts in both the ontolo-
1GlobalWordNet lists the WordNets available in the pub-
lic domains: http://www.globalwordnet.org/gwa/wordnet table.
html.
gies. The drawback of this method is the generation
of inconsistent sense clusters.
(Snow et al, 2007) presented a novel supervised
approach in which they train a Support Vector Ma-
chine(SVM) using features derived from WordNet
and other lexical resources, whose predictions serve
as a distance measure between synsets. Assuming
zero similarity between synset pairs with no com-
mon words, they cluster synsets using average link
agglomerative clustering and the synset similarity
model learnt.
3 SimRank
SimRank (Jeh and Widom, 2002) is a graph based
similarity measure applicable in any domain with
object-to-object relationships. It uses the intuition
that ?two objects are similar if they are related to
similar objects?. Since SimRank has a recursive
structure, the base cases play an important role.
Let us denote the SimRank similarity between ob-
jects ? and ? by s(?, ?). It is defined as 1 if ? = ?,
otherwise it is given by:
s(?, ?) =
C
|I(?)||I(?)|
|I(?)|?
i=1
|I(?)|?
j=1
s(Ii(?), Ij(?))
(1)
where C ? (0, 1) is a constant decay factor and
I(v) is the set consisting of in-neighbours of node v,
whose individual members are referred to as Ij(v),
1 ? j ? |I(v)|.
3.1 Solution and its Properties
(Jeh and Widom, 2002) proved that a solution s(?, ?)
to the SimRank equations always exists and is
unique. For a graphG(V,E), the solution is reached
by iteration to a fixed-point. For each iteration k, we
keep |V |2 entries Sk(?, ?), where Sk(?, ?) is the es-
timate of similarity between ? and ? at the kth iter-
ation. We start with S0(?, ?) which is 1 for single-
ton nodes like (x, x), 0 otherwise. We successively
compute Sk+1(?, ?) based on Sk(?, ?) using equa-
tion 1.
Regarding the convergence of the above computa-
tion process, (Lizorkin et al, 2010) proved that the
difference between the SimRank theoretical scores
12
and iterative similarity scores decreases exponen-
tially in the number of iterations and uniformly for
every pair of nodes i.e.
s(?, ?)? Sk(?, ?) ? Ck+1 ??, ? ? V ; k = 0, 1, 2 . . .
(2)
3.2 Personalizing SimRank
In many scenarios we do not have complete informa-
tion about the objects and thus have similarities for
only some pairs of objects. These similarities may
be independently learnt and may not directly con-
form with the underlying graph. In such situations,
we would like to get a more complete and consis-
tent similarity metric between objects while simul-
taneously using the existing information. For this
we propose a personalized framework for SimRank
where we bias the SimRank by changing the initial-
ization. If we know similarities of some pairs, we
fix them in our set of equations and let the rest of the
values be automatically learnt by the system.
Let us call the map of node pairs to their similarity
values as InitStore. It also contains all the single-
ton nodes like (x, x) which have values equal to 1.
For other node pairs, the system of equations is the
same as equation 1. In the personalized framework,
we have no constraints on the initialization as long
as all values initialized are in the range [0, C].
3.3 Learning Synset Similarity using SimRank
The Personalized SimRank framework requires an
underlying graph G(V,E), where V is the set of
objects to be clustered and E is the set of seman-
tic links connecting these objects and an InitStore
containing the similarity values over some pairs
from V ? V learnt or known otherwise. Note that
the values in the InitStore have an upper bound of
C.
For learning synset similarity, V is the set of
synsets to be clustered and E is the set of Word-
Net relations connecting these synsets. We use the
Hypernymy, Hyponymy, Meronymy and Holonymy
relations of WordNet as the semantic links. The
method for seeding the InitStore is described in
section 4 and can be summed up as follows:
? We train the SVMs from synset-merging data
from OntoNotes (Hovy et al, 2006) to pre-
dict the similarity values of all the synset pairs
which share at least one word.
? We estimate the posterior probabilities from the
SVM predictions by approximating the poste-
rior by a sigmoid function, using the method
discussed in (Lin et al, 2003).
? We scale the posterior probabilities obtained to
range between [0, C] by linear scaling, where
C is the SimRank decay parameter.
4 Seeding SimRank with supervision
4.1 Outline
We learn semantic similarity between different
senses of a word using supervision, which allows
us to intelligently combine and weigh the different
features and thus give us an insight into how hu-
mans relate word senses. We obtain pairs of synsets
which human-annotators have labeled as ?merged?
or ?not merged? and describe each pair as a feature
vector. We learn a synset similarity measure by us-
ing an SVM on this extracted dataset, where positive
examples are the pairs which were merged and neg-
ative examples are the ones which were not merged
by the annotators. We then calculate the posterior
probability using the classifier score which is used
as an estimate of the similarity between synsets con-
stituting the pair.
4.2 Gold standard sense clustering dataset
Since our methodology depends upon the availabil-
ity of labelled judgements of synset relatedness, a
dataset with a high Inter-Annotator agreement is re-
quired. We use the manually labelled mappings
from the Omega ontology2 (Philpot et al, 2005)
to the WordNet senses, provided by the OntoNotes
project (Hovy et al, 2006).
The OntoNotes dataset creation involved a rigor-
ous iterative annotation process producing a coarse
sense inventory which guarantees at least 90% Inter-
Tagger agreement on the sense-tagging of the sam-
ple sentences used in the annotation process. Thus
we expect the quality of the final clustering of senses
and the derived labelled judgements to be reasonably
high.
2http://omega.isi.edu/
13
We use OntoNotes Release 3.0 3 for extracting
WordNet sense clusters.4. The dataset consists of
senses for selected words in sense files. The senses
in OntoNotes are mapped to WordNet senses, if a
good mapping between senses exists. The steps in-
volved in extraction are as follows:
1. OntoNotes has mappings to 4 WordNet ver-
sions: 1.7, 2.0, 2.1 and 3.0. We mapped all
the senses5 to WordNet 3.0.
2. Validating clusters on WN3.0:
? We removed the sense files which did not
contain all the senses of the word i.e. the
clustering was not complete.
? We removed the sense files in which the
clusters had a clash i.e. one sense be-
longed to multiple clusters.
3. We removed instances that were present in both
positive and negative examples. This situa-
tion arises because the annotators were work-
ing with word senses and there were inconsis-
tent sense clusters.
Statistics Nouns Verbs
# of Word Sense File Before Processing 2033 2156
# of Word Sense Files After Processing 1680 1951
Distinct Offsets encountered 4930 6296
Positive Examples 1214 6881
Negative Examples 11974 20899
Percentage of Positive examples 9.20 24.76
Table 1: Statistics of Pairwise Classification Dataset ob-
tained from OntoNotes
4.3 Feature Engineering
In this section, we describe the feature space con-
struction. We derive features from the structure of
WordNet and other available lexical resources. Our
features can be broadly categorized into two parts:
derived from WordNet and derived from other cor-
pora. Many of the listed features are motivated by
(Snow et al, 2007) and (Mihalcea and Moldovan,
2001).
3 http://www.ldc.upenn.edu/Catalog/docs/LDC2009T24/
OntoNotes-Release-3.0.pdf
4The OntoNotes groupings will be available through the
LDC at http://www.ldc.upenn.edu
5We dropped WN1.7 as there were very few senses and the
mapping from WN1.7 to WN3.0 was not easily available.
4.3.1 Features derived from WordNet
WordNet based features are further subdivided
into similarity measures and features. Among the
WordNet similarity measures, we used Path Based
Similarity Measures: WUP (Wu and Palmer, 1994),
LCH (Leacock et al, 1998); Information Content
Based Measures: RES (Resnik, 1995), JCN (Jiang
and Conrath, 1997), LIN (Lin, 1998); Gloss Based
Heuristics (variants of Lesk (Lesk, 1986)): Adapted
Lesk (Banerjee and Pedersen, 2002), Adapted Lesk
Tanimoto and Adapted Lesk Tanimoto without hy-
ponyms6
Other synset and sense based features include
number of lemmas common in two synsets, SenseC-
ount: maximum polysemy degree among the lem-
mas shared by the synsets, SenseNum: number of
lemmas having maximum polysemy degree among
the lemmas shared by the synsets, whether two
synsets have the same lexicographer file, number of
common hypernyms, autohyponymy: whether the
two synsets have a hyponym-hypernym relation be-
tween them and merging heuristics by (Mihalcea
and Moldovan, 2001).7
4.3.2 Features derived from External Corpora
? eXtended WordNet Domains Project (Gonza?lez
et al, 2012) provides us the score of a synset
with respect to 169 hierarchically organized
domain-labels(excluding factotum label). We
obtain a representation of a synset in the do-
main label space and use cosine similarity, L1
distance and L2 distance computed over the
weight representations of the synsets as fea-
tures.
? BabelNet (Navigli and Ponzetto, 2012) pro-
vides us with the translation of noun word
senses in 6 languages namely: English, Ger-
man, Spanish, Catalan, Italian and French and
the mapping of noun synsets to DBpedia8 en-
tries. For features we use counts of common
6We call the lesk variants as AdapLesk, AdapLeskTani and
AdapLeskTaniNoHypo.
7We divide mergeSP1 2 into two features: The strict heuris-
tic checks whether all the hypernyms are shared or not whereas
the relaxed heuristic checks if the synsets have at least 1 com-
mon hypernym.
8http://dbpedia.org/About
14
lemmas in all 6 languages and count of com-
mon DBpedia entries.
? SentiWordNet (Baccianella et al, 2010) pro-
vides us with a mapping from a synset to a triad
of three weights. The weights correspond to the
score given to a synset based on its objectivity
and subjectivity(positive and negative). We use
cosine similarity, L1 distance and L2 distance
of the weight representations of the synsets as
features.
? We use the sense clusterings produced by map-
ping WordNet senses to OED senses by the
organizers of the coarse-grained AW task in
SemEval-20079 (Navigli et al, 2007). For each
pair of synsets, we check if there are senses in
the synsets that belong to the same cluster in
the OED mapping.
4.4 Classifier and Training
We train SVMs using the features above on the
synset pairs extracted from OntoNotes, where ev-
ery synset pair is given either a ?merged? or ?not-
merged? label. Because of the skewed class distribu-
tion in the dataset, we randomly generated balanced
datasets (equal number of positive and negative in-
stances) and then divided them in a ratio of 7:3 for
training and testing respectively. We repeated the
process multiple number of times and report the av-
erage.
To train the SVMs we used an implementation by
(Joachims, 1998), whose java access is provided by
JNI-SVMLight 10 library. For all experiments re-
ported, we use the linear kernel with the default pa-
rameters provided by the library. 11
We scale the ranges of all the features to a com-
mon range [-1,1]. The main advantage offered by
scaling is that it prevents domination of attributes
with smaller numeric ranges by those with greater
numeric ranges. It also avoids numerical difficulties
like overflow errors caused by large attribute values.
Note that both training and testing data should be
scaled with the same parameters.
9 http://lcl.uniroma1.it/coarse-grained-aw/
10JNI-SVMLight: http://adrem.ua.ac.be/?tmartin/
11We also tested our system with an RBF kernel but the best
results were obtained with the linear kernel(Bhagwani, 2013)
4.5 Estimating Posterior Probabilities from
SVM Scores
For seeding SimRank, we need an estimate of the
posterior probability Pr(y = +1|x) instead of the
class label. (Platt, 1999) proposed approximating
the posterior by a sigmoid function
Pr(y = +1|x) ? PA,B(f(x)) ?
1
1+exp(Af(x)+B)
We use the method described in (Lin et al, 2003),
as it avoids numerical difficulties faced by (Platt,
1999).
5 Coarsening WordNet
We construct an undirected graph G(V,E) where
the vertex set V contains the synsets of WordNet and
edge set E comprises of edges obtained by thresh-
olding the similarity metric learnt using the person-
alized SimRank model (see section 3.2). On varying
the threshold, we obtain different graphs which dif-
fer in the number of edges. On these graphs, we find
connected components12, which gives us a partition
over synsets. All the senses of a word occurring in
the same component are grouped as a single coarse
sense. We call our approach Connected Components
Clustering(CCC).
For lower thresholds, we obtain denser graphs
and thus fewer connected components. This small
number of components translates into more coarser
senses. Therefore, using this threshold as a param-
eter of the system, we can control the granularity of
the coarse senses produced.
6 Experimental Setup and Evaluation
6.1 Feature Analysis
We analyze the feature space used for SVMs in two
ways. We evaluate Information Gain(IG) and Gain
Ratio(GR) functions over the features and do a fea-
ture ablation study. The former tries to capture the
discrimination ability of the feature on its own and
the latter measures how a feature corroborates with
other features in the feature space.
12a connected component of an undirected graph is a sub-
graph in which any two vertices are connected to each other by
paths, and which is connected to no additional vertices in the
supergraph.
15
We extracted all the features over the complete
OntoNotes dataset without any normalization and
evaluated them using IG and GR functions. We re-
port the top 7 features of both the evaluators in table
213.
Feature GR IG
LCH 0.0129 0.0323
WUP 0.0148 0.0290
JCN 0.0215 0.0209
AdapLesk 0.0169 0.0346
AdapLeskTani 0.0231 0.0360
AdapLeskTaniNoHypo 0.0168 0.0301
mergeSP1 2 strict 0.0420 0.0010
mergeSP1 2 relaxed 0.0471 0.0012
number of Common Hypernyms 0.0883 0.0096
Domain-Cosine Similarity 0.0200 0.0442
OED 0.0326 0.0312
Table 2: Information Gain and Gain Ratio Based Evalua-
tion
We divide our features into 6 broad categories and
report the average F-Score of both the classes ob-
served by removing that category of features from
our feature space. The SVMs are trained with fea-
tures normalized using MinMax Normalization for
this study.
Features Removed FScore Pos FScore Neg
WordNet Similarity Measures 0.6948 0.6784
WordNet Based Features 0.7227 0.7092
BabelNet Features 0.7232 0.7127
Domain Similarity Features 0.6814 0.6619
OED Feature 0.6957 0.7212
SentiWordNet Features 0.7262 0.7192
Without Removing Features 0.7262 0.7192
Table 3: Feature Ablation Study
From tables 2 and 3, we observe that the most sig-
nificant contributors in SVM performance are Word-
Net similarity measures and domain cosine similar-
ity. The former highlights the importance of the on-
tology structure and the gloss definitions in Word-
Net. The latter stresses the fact that approximately
matching the domain of two senses is a strong cue
about whether the two senses are semantically re-
lated enough to be merged.
13Table lists only 11 features as 3 features are common in top
7 features of both the evaluators
Other notable observations are the effectiveness
of the OED feature and the low Information Gain
and Gain Ratio of multilingual features. We
also found that SentiWordNet features were non-
discriminatory as most of the noun synsets were de-
scribed as objective concepts.
6.2 Estimating Posterior Probabilities from
SVM Scores
We learn parameters A and B of the sigmoid that
transforms SVM predictions to posterior probabili-
ties (see section 4.5). Since using the same data set
that was used to train the model we want to calibrate
will introduce unwanted bias we calibrate on an in-
dependently generated random balanced subset from
OntoNotes.
The values of A and B obtained are -1.1655 and
0.0222 respectively. Using these values, the SVM
prediction of value 0 gets mapped to 0.4944.
6.3 Semi-Supervised Similarity Learning
We learn similarity models using the SimRank vari-
ant described in section 3. (Jeh and Widom, 2002)
use C = 0.8 and find that 5-6 iterations are enough.
(Lizorkin et al, 2010) suggest lower values of C or
more number of iterations. We vary the values for C
between 0.6, 0.7 and 0.8 and we run all systems for
10 iterations to avoid convergence issues.
6.4 Coarsening WordNet
We assess the effect of automatic synset clustering
on the English all-words task at Senseval-3 (Snyder
and Palmer, 2004) 14. The task asked WSD systems
to select the apt sense for 2,041 content words in
running texts comprising of 351 sentences. Since
the BabelNet project provided multilingual equiva-
lences for only nouns, we focussed on nouns and
used the 890 noun instances.
We consider the three best performing WSD sys-
tems: GAMBL (Decadt et al, 2004), SenseLearner
(Mihalcea and Faruque, 2004) and Koc University
(Yuret, 2004) - and the best unsupervised system:
IRST-DDD (Strapparava et al, 2004) submitted in
the task. The answer by the system is given full
14This evaluation is similar to the evaluation used by (Nav-
igli, 2006) and (Snow et al, 2007)
16
C System F-Score Threshold CCC Random Improvement
0.6
GAMBL 0.7116 0.36 0.9031 0.8424 0.0607
SenseLearner 0.7104 0.37 0.8824 0.8305 0.0518
KOC University 0.7191 0.37 0.8924 0.8314 0.0610
IRST-DDD 0.6367 0.35 0.8731 0.8013 0.0718
0.7
GAMBL 0.7116 0.52 0.8453 0.7864 0.0589
SenseLearner 0.7104 0.49 0.8541 0.8097 0.0444
KOC University 0.7191 0.52 0.8448 0.7911 0.0538
IRST-DDD 0.6367 0.49 0.7970 0.7402 0.0568
0.8
GAMBL 0.7116 0.59 0.8419 0.7843 0.0577
SenseLearner 0.7104 0.56 0.8439 0.7984 0.0455
KOC University 0.7191 0.59 0.8414 0.7879 0.0535
IRST-DDD 0.6367 0.47 0.8881 0.8324 0.0557
Table 4: Improvement in Senseval-3 WSD performance using Connected Component Clustering Vs Random Cluster-
ing at the same granularity
credit if it belongs to the cluster of the correct an-
swer.
Observe that any clustering will only improve the
WSD performance. Therefore to assess the improve-
ment obtained because of our clustering, we calcu-
late the expected F-Score, the harmonic mean of ex-
pected precision and expected recall, for a random
clustering at the same granularity and study the im-
provement over the random clustering.
Let the word to be disambiguated have N senses,
each mapped to a unique synset. Let the clustering
of these N synsets on a particular granularity give
us k clusters C1, . . . Ck. The expectation that an in-
correctly chosen sense and the actual correct sense
would belong to same cluster is
?k
i=1|Ci|(|Ci|?1)
N(N ? 1)
(3)
We experiment with C = 0.6, 0.7 and 0.8. The
SVM probability boundaries when scaled to [0, C]
for these values are 0.30, 0.35 and 0.40. To find the
threshold giving the best improvement against the
random clustering baseline, we use the search space
[C ? 0.35, C]. The performance of the systems at
these thresholds for different values of C is reported
in table 4.
Commenting theoretically about the impact of C
on the performance is tough as by changing C we
are changing all the |V |2 simultaneous equations to
be solved. Empirically, we observe that across all
systems improvements over the baseline keep de-
creasing as C increases. This might be due to the
slow convergence of SimRank for higher values of
C.
Figure 1 shows that by varying thresholds the im-
provement of the Connected Components Cluster-
ing over the random clustering baseline at the same
granularity first increases and then decreases. This
behaviour is shared by both supervised and unsuper-
vised systems. Similar figures are obtained for other
values of C (0.7 and 0.8), but are omitted because of
lack of space.
Across supervised and unsupervised systems, we
observe higher improvements for unsupervised sys-
tems. This could be because the unsupervised sys-
tem was underperforming compared to the super-
vised systems in the fine grained WSD task setting.
7 Conclusions and Future Work
We presented a model for learning synset similarity
utilizing the taxonomy information and information
learnt from manually obtained sense clustering. The
framework obtained is generic and can be applied to
other parts of speech as well. For coarsening senses,
we used one of the simplest approaches to cluster
senses but the generic nature of the similarity gives
us the flexibility to use other clustering algorithms
17
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 0.25  0.3  0.35  0.4  0.45  0.5  0.55  0.6  0.65
FSco
re
Threshold
Connected Components ClusteringRandom Clustering
(a)
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 0.35  0.4  0.45  0.5  0.55  0.6  0.65  0.7  0.75
FSco
re
Threshold
Connected Components ClusteringRandom Clustering
(b)
Figure 1: Improvement in (a) average performance of best 3 Supervised Systems and (b) performance of best Unuper-
vised System in Senseval-3 using Connected Component Clustering Vs Random Clustering at the same granularity
with C = 0.6
for experimentation. We show that the clustering ob-
tained by partitioning synsets in connected compo-
nents gives us a maximum improvement of 5.78%
on supervised systems and 7.18% on an unsuper-
vised system. This encourages us to study graph
based similarity learning methods further as they al-
low us to employ available wide-coverage knowl-
edge bases.
We use the WordNet relations Hypernymy, Hy-
ponymy, Meronymy and Holonymy without any dif-
ferentiation. If we can grade the weights of the rela-
tions based on their relative importance we can ex-
pect an improvement in the system. These weights
can be obtained by annotator feedback from cogni-
tive experiments or in a task based setting. In ad-
dition to the basic WordNet relations, we can also
enrich our relation set using the Princeton WordNet
Gloss Corpus15, in which all the WordNet glosses
have been sense disambiguated. Any synset occur-
ing in the gloss of a synset is directly related to that
synset via the gloss relation. This relation helps
make the WordNet graph denser and richer by cap-
turing the notion of semantic relatedness, rather than
just the notion of semantic similarity captured by the
basic WordNet relations.
15http://wordnet.princeton.edu/glosstag.shtml
Acknowledgments
The authors would like to thank the anonymous re-
viewers for their valuable comments and sugges-
tions to improve the quality of the paper.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining. In
Proceedings of LREC.
Satanjeev Banerjee and Ted Pedersen. 2002. An adapted
lesk algorithm for word sense disambiguation using
wordnet. In Proceedings of CICLing 2002.
Sumit Bhagwani. 2013. Merging word senses. Master?s
thesis, Indian Institute of Technology Kanpur.
Paul Buitelaar. 2000. Reducing lexical semantic com-
plexity with systematic polysemous classes and un-
derspecification. In NAACL-ANLP 2000 Workshop:
Syntactic and Semantic Complexity in Natural Lan-
guage Processing Systems, pages 14?19. Association
for Computational Linguistics.
Irina Chugur, Julio Gonzalo, and Felisa Verdejo. 2002.
Polysemy and sense proximity in the senseval-2 test
suite. In Proceedings of the ACL 2002 WSD workshop.
Bart Decadt, Ve?ronique Hoste, Walter Daelemans, and
Antal Van den Bosch. 2004. Gambl, genetic algo-
rithm optimization of memory-based wsd. In Proceed-
ings of ACL/SIGLEX Senseval-3.
18
Aitor Gonza?lez, German Rigau, and Mauro Castillo.
2012. A graph-based method to improve wordnet do-
mains. In Proceedings of CICLing 2012.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Proceedings of HLT-NAACL
2006.
Glen Jeh and Jennifer Widom. 2002. Simrank: A mea-
sure of structural-context similarity. In KDD, pages
538?543.
Jay J. Jiang and David W. Conrath. 1997. Semantic sim-
ilarity based on corpus statistics and lexical taxonomy.
In Proceedings of ROCLING?97.
Thorsten Joachims. 1998. Making large-scale support
vector machine learning practical.
Claudia Leacock, George A. Miller, and Martin
Chodorow. 1998. Using corpus statistics and wordnet
relations for sense identification. Comput. Linguist.,
24(1):147?165, March.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In Proceedings of SIG-
DOC 1986.
Hsuan-tien Lin, Chih-Jen Lin, and Ruby C. Weng. 2003.
A note on platt?s probabilistic outputs for support vec-
tor machines.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of ICML 1998.
Dmitry Lizorkin, Pavel Velikhov, Maxim Grinev, and De-
nis Turdakov. 2010. Accuracy estimate and optimiza-
tion techniques for simrank computation. The VLDB
Journal, 19(1):45?66, February.
Diana McCarthy. 2006. Relating wordnet senses for
word sense disambiguation. Making Sense of Sense:
Bringing Psycholinguistics and Computational Lin-
guistics Together, 17.
Rada Mihalcea and Ehsanul Faruque. 2004. Sense-
learner: Minimally supervised word sense disam-
biguation for all words in open text. In Proceedings
of ACL/SIGLEX Senseval-3.
Rada Mihalcea and Dan Moldovan. 2001. Ez.wordnet:
principles for automatic generation of a coarse grained
wordnet. In Proceedings of Flairs 2001, pages 454?
459.
George A Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM, 38(11):39?41.
Roberto Navigli and Simone Paolo Ponzetto. 2012. Ba-
belNet: The automatic construction, evaluation and
application of a wide-coverage multilingual semantic
network. Artificial Intelligence, 193:217?250.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. Semeval-2007 task 07: Coarse-grained
english all-words task. In Proceedings of SemEval-
2007, pages 30?35. Association for Computational
Linguistics, June.
Roberto Navigli. 2006. Meaningful clustering of senses
helps boost word sense disambiguation performance.
In Proceedings of COLING-ACL, pages 105?112.
Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003. Ex-
ploiting parallel texts for word sense disambiguation:
an empirical study. In Proceedings of ACL 2003.
Andrew Philpot, Eduard Hovy, and Patrick Pantel. 2005.
The omega ontology. In Proceedings of the ONTOLEX
Workshop at IJCNLP 2005.
John C. Platt. 1999. Probabilistic outputs for support
vector machines and comparisons to regularized like-
lihood methods. In ADVANCES IN LARGE MARGIN
CLASSIFIERS, pages 61?74. MIT Press.
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proceedings
of IJCAI 1995.
Rion Snow, Sushant Prakash, Daniel Jurafsky, and An-
drew Y. Ng. 2007. Learning to Merge Word Senses.
In Proceedings of EMNLP-CoNLL, pages 1005?1014,
June.
Benjamin Snyder and Martha Palmer. 2004. The en-
glish all-words task. In Proceedings of ACL/SIGLEX
Senseval-3, pages 41?43.
Carlo Strapparava, Alfio Gliozzo, and Claudiu Giuliano.
2004. Pattern abstraction and term similarity for word
sense disambiguation: Irst at senseval-3. In Proceed-
ings of ACL/SIGLEX Senseval-3.
Zhibiao Wu and Martha Palmer. 1994. Verbs semantics
and lexical selection. In Proceedings of ACL 1994.
Deniz Yuret. 2004. Some experiments with a naive
bayes wsd system. In Proceedings of ACL/SIGLEX
Senseval-3.
19
