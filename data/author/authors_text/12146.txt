Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 49?54,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Multilingual Dependency-based Syntactic and Semantic Parsing
Wanxiang Che, Zhenghua Li, Yongqiang Li, Yuhang Guo, Bing Qin, Ting Liu
Information Retrieval Lab
School of Computer Science and Technology
Harbin Institute of Technology, China, 150001
{car, lzh, yqli, yhguo, qinb, tliu}@ir.hit.edu.cn
Abstract
Our CoNLL 2009 Shared Task system in-
cludes three cascaded components: syntactic
parsing, predicate classification, and semantic
role labeling. A pseudo-projective high-order
graph-based model is used in our syntactic de-
pendency parser. A support vector machine
(SVM) model is used to classify predicate
senses. Semantic role labeling is achieved us-
ing maximum entropy (MaxEnt) model based
semantic role classification and integer linear
programming (ILP) based post inference. Fi-
nally, we win the first place in the joint task,
including both the closed and open challenges.
1 System Architecture
Our CoNLL 2009 Shared Task (Hajic? et al, 2009):
multilingual syntactic and semantic dependencies
system includes three cascaded components: syn-
tactic parsing, predicate classification, and semantic
role labeling.
2 Syntactic Dependency Parsing
We extend our CoNLL 2008 graph-based
model (Che et al, 2008) in four ways:
1. We use bigram features to choose multiple pos-
sible syntactic labels for one arc, and decide the op-
timal label during decoding.
2. We extend the model with sibling features (Mc-
Donald, 2006).
3. We extend the model with grandchildren fea-
tures. Rather than only using the left-most and right-
most grandchildren as Carreras (2007) and Johans-
son and Nugues (2008) did, we use all left and right
grandchildren in our model.
4. We adopt the pseudo-projective approach in-
troduced in (Nivre and Nilsson, 2005) to handle the
non-projective languages including Czech, German
and English.
2.1 Syntactic Label Determining
The model of (Che et al, 2008) decided one la-
bel for each arc before decoding according to uni-
gram features, which caused lower labeled attach-
ment score (LAS). On the other hand, keeping all
possible labels for each arc made the decoding in-
efficient. Therefore, in the system of this year, we
adopt approximate techniques to compromise, as
shown in the following formulas.
f lbluni(h, c, l) = f lbl1 (h, 1, d, l) ? f lbl1 (c, 0, d, l)
L1(h, c) = arg maxK1l?L(w ? f lbluni(h, c, l))
f lblbi (h, c, l) = f lbl2 (h, c, l)
L2(h, c) = arg maxK2l?L1(h,c)(w ? {f lbluni ? f lblbi })
For each arc, we firstly use unigram features to
choose the K1-best labels. The second parameter of
f lbl1 (?) indicates whether the node is the head of the
arc, and the third parameter indicates the direction.
L denotes the whole label set. Then we re-rank the
labels by combining the bigram features, and choose
K2-best labels. During decoding, we only use the
K2 labels chosen for each arc (K2 ? K1 < |L|).
2.2 High-order Model and Algorithm
Following the Eisner (2000) algorithm, we use spans
as the basic unit. A span is defined as a substring
of the input sentence whose sub-tree is already pro-
duced. Only the start or end words of a span can link
with other spans. In this way, the algorithm parses
the left and the right dependence of a word indepen-
dently, and combines them in the later stage.
We follow McDonald (2006)?s implementation of
first-order Eisner parsing algorithm by modifying its
scoring method to incorporate high-order features.
Our extended algorithm is shown in Algorithm 1.
There are four different span-combining opera-
tions. Here we explain two of them that correspond
to right-arc (s < t), as shown in Figure 1 and 2. We
49
Algorithm 1 High-order Eisner Parsing Algorithm
1: C[s][s][c] = 0, 0 ? s ? N , c ? cp, icp # cp: complete; icp: incomplete
2: for j = 1 to N do
3: for s = 0 to N do
4: t = s+ jL
5: if t > N then
6: break
7: end if
# Create incomplete spans
8: C[s][t][icp] = maxs?r<t;l?L2(s,t)(C[s][r][cp] + C[t][r + 1][cp] + Sicp(s, r, t, l))
9: C[t][s][icp] = maxs?r<t;l?L2(t,s)(C[s][r][cp] + C[t][r + 1][cp] + Sicp(t, r, s, l))
# Create complete spans
10: C[s][t][cp] = maxs<r?t;l=C[s][r][icp].label(C[s][r][icp] + C[r][t][cp] + Scp(s, r, t, l))
11: C[t][s][cp] = maxs?r<t;l=C[t][r][icp].label(C[r][s][cp] + C[t][r][icp] + Scp(t, r, s, l))
12: end for
13: end for
follow the way of (McDonald, 2006) and (Carreras,
2007) to represent spans. The other two operations
corresponding to left-arc are similar.
 
Figure 1: Combining two spans into an incomplete span
Figure 1 illustrates line 8 of the algorithm in Al-
gorithm 1, which combines two complete spans into
an incomplete span. A complete span means that
only the head word can link with other words fur-
ther, noted as ??? or ???. An incomplete span
indicates that both the start and end words of the
span will link with other spans in the future, noted as
?99K? or ?L99?. In this operation, we combine two
smaller spans, sps?r and spr+1?t, into sps99Kt with
adding arcs?t. As shown in the following formu-
las, the score of sps99Kt is composed of three parts:
the score of sps?r, the score of spr+1?t, and the
score of adding arcs?t. The score of arcs?t is
determined by four different feature sets: unigram
features, bigram features, sibling features and left
grandchildren features (or inside grandchildren fea-
tures, meaning that the grandchildren lie between s
and t). Note that the sibling features are only related
to the nearest sibling node of t, which is denoted as
sck here. And the inside grandchildren features are
related to all the children of t. This is different from
the models used by Carreras (2007) and Johansson
and Nugues (2008). They only used the left-most
child of t, which is tck? here.
ficp(s, r, t, l) = funi(s, t, l) ? fbi(s, t, l)
? fsib(s, sck, t) ? {?k?i=1 fgrand(s, t, tci, l)}
Sicp(s, r, t, l) = w ? ficp(s, r, t, l)
S(sps99Kt) = S(sps?r) + S(spr+1?t)
+ Sicp(s, r, t, l)
In Figure 2 we combine sps99Kr and spr?t into
sps?t, which explains line 10 in Algorithm 1. The
score of sps?t also includes three parts, as shown
in the following formulas. Although there is no new
arc added in this operation, the third part is neces-
sary because it reflects the right (or called outside)
grandchildren information of arcs?r.
r trc1 rcks r s tr rc1 rck
l l
 
Figure 2: Combining two spans into a complete span
fcp(s, r, t, l) = ?ki=1 fgrand(s, r, rci, l)
Scp(s, r, t, l) = w ? fcp(s, r, t, l)
S(sps?t) = S(sps99Kr)
+ S(spr?t) + Scp(s, r, t, l)
50
2.3 Features
As shown above, features used in our model can be
decomposed into four parts: unigram features, bi-
gram features, sibling features, and grandchildren
features. Each part can be seen as two different sets:
arc-related and label-related features, except sibling
features, because we do not consider labels when us-
ing sibling features. Arc-related features can be un-
derstood as back-off of label-related features. Actu-
ally, label-related features are gained by simply at-
taching the label to the arc-features.
The unigram and bigram features used in our
model are similar to those of (Che et al, 2008), ex-
cept that we use bigram label-related features. The
sibling features we use are similar to those of (Mc-
Donald, 2006), and the grandchildren features are
similar to those of (Carreras, 2007).
3 Predicate Classification
The predicate classification is regarded as a super-
vised word sense disambiguation (WSD) task here.
The task is divided into four steps:
1. Target words selection: predicates with multi-
ple senses appearing in the training data are selected
as target words.
2. Feature extraction: features in the context
around these target words are extracted as shown in
Table 4. The detailed explanation about these fea-
tures can be found from (Che et al, 2008).
3. Classification: for each target word, a Support
Vector Machine (SVM) classifier is used to classify
its sense. As reported by Lee and Ng (2002) and
Guo et al (2007), SVM shows good performance on
the WSD task. Here libsvm (Chang and Lin, 2001)
is used. The linear kernel function is used and the
trade off parameter C is 1.
4. Post processing: for each predicate in the test
data which does not appear in the training data, its
first sense in the frame files is used.
4 Semantic Role Labeling
The semantic role labeling (SRL) can be divided
into two separate stages: semantic role classification
(SRC) and post inference (PI).
During the SRC stage, a Maximum en-
tropy (Berger et al, 1996) classifier is used to
predict the probabilities of a word in the sentence
Language No-duplicated-roles
Catalan arg0-agt, arg0-cau, arg1-pat, arg2-atr, arg2-loc
Chinese A0, A1, A2, A3, A4, A5,
Czech ACT, ADDR, CRIT, LOC, PAT, DIR3, COND
English A0, A1, A2, A3, A4, A5,
German A0, A1, A2, A3, A4, A5,
Japanese DE, GA, TMP, WO
Spanish arg0-agt, arg0-cau, arg1-pat, arg1-tem, arg2-atr,
arg2-loc, arg2-null, arg4-des, argL-null, argM-
cau, argM-ext, argM-fin
Table 1: No-duplicated-roles for different languages
to be each semantic role. We add a virtual role
?NULL? (presenting none of roles is assigned)
to the roles set, so we do not need semantic role
identification stage anymore. For a predicate
of each language, two classifiers (one for noun
predicates, and the other for verb predicates) predict
probabilities of each word in a sentence to be each
semantic role (including virtual role ?NULL?). The
features used in this stage are listed in Table 4.
The probability of each word to be a semantic role
for a predicate is given by the SRC stage. The re-
sults generated by selecting the roles with the largest
probabilities, however, do not satisfy some con-
strains. As we did in the last year?s system (Che et
al., 2008), we use the ILP (Integer Linear Program-
ming) (Punyakanok et al, 2004) to get the global op-
timization, which is satisfied with three constrains:
C1: Each word should be labeled with one and
only one label (including the virtual label ?NULL?).
C2: Roles with a small probability should never
be labeled (except for the virtual role ?NULL?). The
threshold we use in our system is 0.3.
C3: Statistics show that some roles (except for
the virtual role ?NULL?) usually appear once for
a predicate. We impose a no-duplicate-roles con-
straint with a no-duplicate-roles list, which is con-
structed according to the times of semantic roles?
duplication for each single predicate. Table 1 shows
the no-duplicate-roles for different languages.
Our maximum entropy classifier is implemented
with Maximum Entropy Modeling Toolkit1. The
classifier parameters are tuned with the development
data for different languages respectively. lp solve
5.52 is chosen as our ILP problem solver.
1http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html
2http://sourceforge.net/projects/lpsolve
51
5 Experiments
5.1 Experimental Setup
We participate in the CoNLL 2009 shared task
with all 7 languages: Catalan (Taule? et al, 2008),
Chinese (Palmer and Xue, 2009), Czech (Hajic? et
al., 2006), English (Surdeanu et al, 2008), Ger-
man (Burchardt et al, 2006), Japanese (Kawahara
et al, 2002), and Spanish (Taule? et al, 2008). Be-
sides the closed challenge, we also submitted the
open challenge results. Our open challenge strategy
is very simple. We add the SRL development data
of each language into their training data. The pur-
pose is to examine the effect of the additional data,
especially for out-of-domain (ood) data.
Three machines (with 2.5GHz Xeon CPU and
16G memory) were used to train our models. Dur-
ing the peak time, Amazon?s EC2 (Elastic Com-
pute Cloud)3 was used, too. Our system requires
15G memory at most and the longest training time
is about 36 hours.
During training the predicate classification (PC)
and the semantic role labeling (SRL) models, golden
syntactic dependency parsing results are used. Pre-
vious experiments show that the PC and SRL test re-
sults based on golden parse trees are slightly worse
than that based on cross trained parse trees. It is,
however, a pity that we have no enough time and ma-
chines to do cross training for so many languages.
5.2 Results and Discussion
In order to examine the performance of the ILP
based post inference (PI) for different languages, we
adopt a simple PI strategy as baseline, which se-
lects the most likely label (including the virtual la-
bel ?NULL?) except for those duplicate non-virtual
labels with lower probabilities (lower than 0.5). Ta-
ble 2 shows their performance on development data.
We can see that the ILP based post inference can
improve the precision but decrease the recall. Ex-
cept for Czech, almost all languages are improved.
Among them, English benefits most.
The final system results are shown in Table 3.
Comparing with our CoNLL 2008 (Che et al, 2008)
syntactic parsing results on English4, we can see that
our new high-order model improves about 1%.
3http://aws.amazon.com/ec2/
4devel: 85.94%, test: 87.51% and ood: 80.73%
Precision Recall F1
Catalan simple 78.68 77.14 77.90
Catalan ILP 79.42 76.49 77.93
Chinese simple 80.74 74.36 77.42
Chinese ILP 81.97 73.92 77.74
Czech simple 88.54 84.68 86.57
Czech ILP 89.23 84.05 86.56
English simple 83.03 83.55 83.29
English ILP 85.63 83.03 84.31
German simple 78.88 75.87 77.34
German ILP 82.04 74.10 77.87
Japanese simple 88.04 70.68 78.41
Japanese ILP 89.23 70.16 78.56
Spanish simple 76.73 75.92 76.33
Spanish ILP 77.71 75.34 76.51
Table 2: Comparison between different PI strategies
For the open challenge, because we did not mod-
ify the syntactic training data, its results are the same
as the closed ones. We can, therefore, examine the
effect of the additional training data on SRL. We can
see that along with the development data are added
into the training data, the performance on the in-
domain test data is increased. However, it is inter-
esting that the additional data is harmful to the ood
test.
6 Conclusion and Future Work
Our CoNLL 2009 Shared Task system is com-
posed of three cascaded components. The pseudo-
projective high-order syntactic dependency model
outperforms our CoNLL 2008 model (in English).
The additional in-domain (devel) SRL data can help
the in-domain test. However, it is harmful to the ood
test. Our final system achieves promising results. In
the future, we will study how to solve the domain
adaptive problem and how to do joint learning be-
tween syntactic and semantic parsing.
Acknowledgments
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
60803093, 60675034, and the ?863? National High-
Tech Research and Development of China via grant
2008AA01Z144.
52
Syntactic Accuracy (LAS) Semantic Labeled F1 Macro F1 Score
devel test ood devel test ood devel test ood
Catalan closed 86.65 86.56 ?? 77.93 77.10 ?? 82.30 81.84 ??open ?? ?? 77.36 ?? 81.97
Chinese closed 75.73 75.49 ?? 77.74 77.15 ?? 76.79 76.38 ??open ?? ?? 77.23 ?? 76.42
Czech closed 80.07 80.01 76.03 86.56 86.51 85.26 83.33 83.27 80.66open ?? ?? 86.57 85.21 ?? 83.31 80.63
English closed 87.09 88.48 81.57 84.30 85.51 73.82 85.70 87.00 77.71open ?? ?? 85.61 73.66 ?? 87.05 77.63
German closed 85.69 86.19 76.11 77.87 78.61 70.07 81.83 82.44 73.19open ?? ?? 78.61 70.09 ?? 82.44 73.20
Japanese closed 92.55 92.57 ?? 78.56 78.26 ?? 85.86 85.65 ??open ?? ?? 78.35 ?? 85.70
Spanish closed 87.22 87.33 ?? 76.51 76.47 ?? 81.87 81.90 ??open ?? ?? 76.66 ?? 82.00
Average closed ?? 85.23 77.90 ?? 79.94 76.38 ?? 82.64 77.19open 80.06 76.32 82.70 77.15
Table 3: Final system results
References
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In LREC-2006.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In EMNLP/CoNLL-
2007.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a
library for support vector machines.
Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang Li,
Bing Qin, Ting Liu, and Sheng Li. 2008. A cascaded
syntactic and semantic dependency parsing system. In
CoNLL-2008.
Jason Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Advances in Probabilistic
and Other Parsing Technologies.
Yuhang Guo, Wanxiang Che, Yuxuan Hu, Wei Zhang,
and Ting Liu. 2007. HIT-IR-WSD: A wsd system for
english lexical sample task. In SemEval-2007.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k Z?abokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In CoNLL-2009.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based semantic role labeling of Prop-
Bank. In EMNLP-2008.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In LREC-2002.
Yoong Keok Lee and Hwee Tou Ng. 2002. An empir-
ical evaluation of knowledge sources and learning al-
gorithms for word sense disambiguation. In EMNLP-
2002.
Ryan McDonald. 2006. Discriminative Learning and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In ACL-2005.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1).
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav Zi-
mak. 2004. Semantic role labeling via integer linear
programming inference. In Coling-2004.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL-2008.
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In LREC-2008.
53
Catalan Chinese Czech English German Japanese Spanish
ChildrenPOS ? ? ??
ChildrenPOSNoDup ? ? ? ?
ConstituentPOSPattern ? ? ? ? ? ? ? ? ? ? ? ? ? ?
ConstituentPOSPattern+DepRelation ? ? ? ? ? ?
ConstituentPOSPattern+DepwordLemma ? ? ? ? ? ?
ConstituentPOSPattern+HeadwordLemma ? ? ? ? ? ? ? ? ? ?
DepRelation N M ? ? N M ? ? N M ? ? N M ? ? N M ? ? N M ? ?
DepRelation+DepwordLemma ? ? ? ?
DepRelation+Headword N M N M N N M N M N
DepRelation+HeadwordLemma ? ? ? ? ? ? ? ?
DepRelation+HeadwordLemma+DepwordLemma ? ? ? ? ? ? ? ? ? ? ? ?
DepRelation+HeadwordPOS N M N M N M N M N M N
Depword ? ? ? ?
DepwordLemma ? ? ? ? ? ? ? ? ? ? ? ?
DepwordLemma+HeadwordLemma ? ? ? ? ? ?
DepwordLemma+RelationPath ? ? ? ? ? ? ? ? ? ?
DepwordPOS N M N M N M ? ? N M N M ? ? N M
DepwordPOS+HeadwordPOS ? ? ? ?
DownPathLength ? ? ? ?
FirstLemma ? ? ? ? ? ? ? ? ? ? ? ?
FirstPOS ? ? ? ?
FirstPOS+DepwordPOS ? ? ? ? ? ?
FirstWord ? ? ? ?
Headword N M N M N M N M N M ? ? N
HeadwordLemma N M ? ? N M ? ? N M ? ? N M ? ? N M ? ? ? ? N
HeadwordLemma+RelationPath ? ? ? ? ? ? ? ? ? ? ? ?
HeadwordPOS N M N M N M ? ? N M ? ? N M ? ? N M
LastLemma ? ? ? ? ? ? ? ? ? ?
LastPOS ? ? ? ?
LastWord ? ?
Path ? ? ? ? ? ? ? ? ? ? ? ?
Path+RelationPath ? ? ? ? ? ? ? ? ? ?
PathLength ? ? ? ? ? ? ? ? ? ? ? ?
PFEAT N M N M N M
PFEATSplit N M ? ? N M ? ? N M ? ? N M ? ?
PFEATSplitRemoveNULL N M N M N M
PositionWithPredicate ? ? ? ? ? ? ? ? ? ?
Predicate N M ? ? N M N M ? ? N M N M N M ? ?
Predicate+PredicateFamilyship ? ? ? ? ? ? ? ? ? ?
PredicateBagOfPOSNumbered M N M N M N M
PredicateBagOfPOSNumberedWindow5 N M N M N M N M N M
PredicateBagOfPOSOrdered N M N M N M N M N
PredicateBagOfPOSOrderedWindow5 N M N M N M N M N M N M
PredicateBagOfPOSWindow5 N N M N M N M N M N
PredicateBagOfWords M N M N N M N M
PredicateBagOfWordsAndIsDesOfPRED N M N M M N M N M
PredicateBagOfWordsOrdered M N M N M M N M N M
PredicateChildrenPOS N M ? ? N M N M N M N M N M ? ?
PredicateChildrenPOSNoDup N M N M N M N M N M N M
PredicateChildrenREL N M ? ? N M N M N M N M ? ? N M
PredicateChildrenRELNoDup N M ? ? N M N M N M N M ? ? N M
PredicateFamilyship ? ?
PredicateLemma N M ? ? N M ? ? N M ? ? N M ? ? N M ? ? ? ? N M ? ?
PredicateLemma+PredicateFamilyship ? ? ? ? ? ?
PredicateSense ? ? ? ? ? ? ? ? ? ? ? ? ? ?
PredicateSense+DepRelation ? ? ? ?
PredicateSense+DepwordLemma ? ? ? ?
PredicateSense+DepwordPOS ? ? ? ?
PredicateSiblingsPOS N M N M N N M N M N M
PredicateSiblingsPOSNoDup N M ? ? N M N M N M N M N M ? ?
PredicateSiblingsREL N M ? ? N M N M N M N M N M
PredicateSiblingsRELNoDup N M N M ? ? M N M N M ? ? N M ? ?
PredicateVoiceEn N M
PredicateWindow5Bigram N M N M N M N M
PredicateWindow5BigramPOS N M N M N M N M N M N M
RelationPath ? ? ? ? ? ? ? ? ? ? ? ? ? ?
SiblingsPOS ? ? ? ?
SiblingsREL ?
SiblingsRELNoDup ? ? ? ?
UpPath ? ? ? ? ? ? ?
UpPathLength ? ?
UpRelationPath ? ? ? ? ? ?
UpRelationPath+HeadwordLemma ? ? ? ? ? ? ? ?
Table 4: Features that are used in predicate classification (PC) and semantic role labeling (SRL). N: noun predicate
PC, M: verb predicate PC, ?: noun predicate SRL, ?: verb predicate SRL.
54
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 172?182, Dublin, Ireland, August 23-29 2014.
Building Large-Scale Twitter-Specific Sentiment Lexicon :
A Representation Learning Approach
Duyu Tang
\?
, Furu Wei
?
, Bing Qin
\?
, Ming Zhou
?
, Ting Liu
\
\
Research Center for Social Computing and Information Retrieval,
Harbin Institute of Technology, China
?
Microsoft Research, Beijing, China
{dytang, qinb, tliu}@ir.hit.edu.cn
{fuwei, mingzhou}@microsoft.com
Abstract
In this paper, we propose to build large-scale sentiment lexicon from Twitter with a representation
learning approach. We cast sentiment lexicon learning as a phrase-level sentiment classification
task. The challenges are developing effective feature representation of phrases and obtaining
training data with minor manual annotations for building the sentiment classifier. Specifical-
ly, we develop a dedicated neural architecture and integrate the sentiment information of tex-
t (e.g. sentences or tweets) into its hybrid loss function for learning sentiment-specific phrase
embedding (SSPE). The neural network is trained from massive tweets collected with positive
and negative emoticons, without any manual annotation. Furthermore, we introduce the Urban
Dictionary to expand a small number of sentiment seeds to obtain more training data for building
the phrase-level sentiment classifier. We evaluate our sentiment lexicon (TS-Lex) by applying
it in a supervised learning framework for Twitter sentiment classification. Experiment results
on the benchmark dataset of SemEval 2013 show that, TS-Lex yields better performance than
previously introduced sentiment lexicons.
1 Introduction
A sentiment lexicon is a list of words and phrases, such as ?excellent?, ?awful? and ?not bad?, each
of which is assigned with a positive or negative score reflecting its sentiment polarity and strength.
Sentiment lexicon is crucial for sentiment analysis (or opining mining) as it provides rich sentiment in-
formation and forms the foundation of many sentiment analysis systems (Pang and Lee, 2008; Liu, 2012;
Feldman, 2013). Existing sentiment lexicon learning algorithms mostly utilize propagation methods to
estimate the sentiment score of each phrase. These methods typically employ parsing results, syntac-
tic contexts or linguistic information from thesaurus (e.g. WordNet) to calculate the similarity between
phrases. For example, Baccianella et al. (2010) use the glosses information from WordNet; Velikovich et
al. (2010) represent each phrase with its context words from the web documents; Qiu et al. (2011) exploit
the dependency relations between sentiment words and aspect words. However, parsing information and
the linguistic information from WordNet are not suitable for constructing large-scale sentiment lexicon
from Twitter. The reason lies in that WordNet cannot well cover the colloquial expressions in tweets, and
it is hard to have reliable tweet parsers due to the informal language style.
In this paper, we propose to build large-scale sentiment lexicon from Twitter with a representation
learning approach, as illustrated in Figure 1. We cast sentiment lexicon learning as a phrase-level classi-
fication task. Our method contains two part: (1) a representation learning algorithm to effectively learn
the continuous representation of phrases, which are used as features for phrase-level sentiment classifica-
tion, (2) a seed expansion algorithm that enlarge a small list of sentiment seeds to collect training data for
building the phrase-level classifier. Specifically, we learn sentiment-specific phrase embedding (SSPE),
which is a low-dimensional, dense and real-valued vector, by encoding the sentiment information and
?
This work was partly done when the first author was visiting Microsoft Research.
?
Corresponding author.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
172
Sentiment 
Classifier 
Sentiment 
Lexicon 
Phrase Embedding 
NEG: goon looser 
Sentiment Seeds 
Tweets with Emoticons 
Soooo nice~ :) 
It?s horrible :( 
Seed 
Expansion 
Representation 
Learning 
POS: good :) 
NEG: poor :( 
NEU: when he 
Training Data 
POS: wanted fave 
NEU: again place 
[1.31,0.97] good: 
[0.99,1.17] coool: 
[-0.81,-0.7] bad: 
[-0.8,-0.72] mess: 
Learning 
Algorithm 
Figure 1: The representation learning approach for building Twitter-specific sentiment lexicon.
syntactic contexts into the continuous representation of phrases
1
. As a result, the nearest neighbors in the
embedding space of SSPE are favored to have similar semantic usage as well as the same sentiment po-
larity. To this end, we extend the existing phrase embedding learning algorithm (Mikolov et al., 2013b),
and develop a dedicated neural architecture with hybrid loss function to incorporate the supervision from
sentiment polarity of text (e.g. tweets). We learn SSPE from tweets, leveraging massive tweets con-
taining positive and negative emoticons as training set without any manual annotation. To obtain more
training data for building the phrase-level sentiment classifier, we exploit the similar words from Urban
Dictionary
2
, which is a crowd-sourcing resource, to expand a small list of sentiment seeds. Finally, we
utilize the classifier to predict the sentiment score of each phrase in the vocabulary of SSPE, resulting in
the sentiment lexicon.
We evaluate the effectiveness of our sentiment lexicon (TS-Lex) by applying it in a supervised learn-
ing framework (Pang et al., 2002) for Twitter sentiment classification. Experiment results on the bench-
mark dataset of SemEval 2013 show that, TS-Lex yields better performance than previously introduced
lexicons, including two large-scale Twitter-specific sentiment lexicons, and further improves the top-
performed system in SemEval 2013 by feature combination. The quality of SSPE is also evaluated by
regarding SSPE as the feature for sentiment classification of the items in existing sentiment lexicons (Hu
and Liu, 2004; Wilson et al., 2005). Experiment results show that SSPE outperforms existing embedding
learning algorithms. The main contributions of this work are as follows:
? To our best knowledge, this is the first work that leverages the continuous representation of phrases
for building large-scale sentiment lexicon from Twitter;
? We propose a tailored neural architecture for learning the sentiment-specific phrase embedding from
massive tweets selected with positive and negative emoticons;
? We report the results that our lexicon outperforms existing sentiment lexicons by applying them in
a supervised learning framework for Twitter sentiment classification.
2 Related Work
In this section, we give a brief review about building sentiment lexicon and learning continuous repre-
sentation of words and phrases.
2.1 Sentiment Lexicon Learning
Sentiment lexicon is a fundamental component for sentiment analysis, which can be built manually (Das
and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Tur-
ney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods
1
Word/unigram is also regarded as phrase in this paper.
2
http://www.urbandictionary.com/
173
and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by re-
garding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation
algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran,
2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment score
of each item. Under this direction, parsing results, syntactic contexts or linguistic clues in thesaurus are
mostly explored to calculate the similarity between items. Wiebe (2000) utilize the dependency triples
from an existing parser (Lin, 1994). Qiu et al. (2009; 2011) adopt dependency relations between senti-
ment words and aspect words. Esuli and Sebastiani (2005) exploit the glosses information from Wordnet.
Hu and Liu (2004) use the synonym and antonym relations within linguistic resources. Velikovich et al.
(2010) represent words and phrases with their syntactic contexts within a window size from the web
documents. Unlike the dominated propagation based methods, we explore the classification framework
based on representation learning for building large-scale sentiment lexicon from Twitter.
To construct the Twitter-specific sentiment lexicon, Mohammad et al. (2013) use pointwise mutual
information (PMI) between each phrase and hashtag/emoticon seed words, such as #good, #bad, :) and
:(. Chen et al. (2012) utilize the Urban Dictionary and extract the target-dependent sentiment expres-
sions from Twitter. Unlike Mohammad et al. (2013) that only capture the relations between phrases and
sentiment seeds, we exploit the semantic and sentimental connections between phrases through phrase
embedding and propose a representation learning approach to build sentiment lexicon.
2.2 Learning Continuous Representation of Word and Phrase
Continuous representation of words and phrases are proven effective in many NLP tasks (Turian et al.,
2010). Embedding learning algorithms have been extensively studied in recent years (Bengio et al.,
2013), and are dominated by the syntactic context based algorithms (Bengio et al., 2003; Collobert et
al., 2011; Dahl et al., 2012; Huang et al., 2012; Mikolov et al., 2013a; Lebret et al., 2013; Sun et al.,
2014). To integrate the sentiment information of text into the word embedding, Maas et al. (2011) extend
the probabilistic document model (Blei et al., 2003) and predict the sentiment of a sentence with the
embedding of each word. Labutov and Lipson (2013) learn task-specific embedding from an existing
embedding and sentences with gold sentiment polarity. Tang et al. (2014) propose to learn sentiment-
specific word embedding from tweets collected by emoticons for Twitter sentiment classification. Unlike
previous trails, we learn sentiment-specific phrase embedding with a tailored neural network. Unlike
Mikolov et al. (2013b) that only use the syntactic contexts of phrases to learn phrase embedding, we
integrate the sentiment information of text into our method. It is worth noting that we focus on learning
the continuous representation of words and phrases, which is orthogonal with Socher et al. (2011; 2013)
that learn the compositionality of sentences.
3 Methodology
In this section, we describe our method for building large-scale sentiment lexicon from Twitter within a
classification framework, as illustrated in Figure 1. We leverage the continuous representation of phrases
as features, without parsers or hand-crafted rules, and automatically obtain the training data by seed
expansion from Urban Dictionary. After the classifier is built, we employ it to predict the sentiment
distribution of each phrase in the embedding vocabulary, resulting in the sentiment lexicon. To encode
the sentiment information into the continuous representation of phrases, we extend an existing phrase
embedding learning algorithm (Mikolov et al., 2013b) and develop a tailored neural architecture to learn
sentiment-specific phrase embedding (SSPE), as described in subsection 3.1. To automatically obtain
more training data for building the phrase-level sentiment classifier, we use the similar words from Urban
Dictionary to expand a small list of sentiment seeds, as described in subsection 3.2.
3.1 Sentiment-Specific Phrase Embedding
Mikolov et al. (2013b) introduce Skip-Gram to learn phrase embedding based on the context words of
phrases, as illustrated in Figure 2(a).
Given a phrase w
i
, Skip-Gram maps it into its continuous representation e
i
. Then, Skip-Gram utilizes
174
ei 
wi-2 wi-1 wi+1 wi+2 
wi 
ei 
wi-2 wi-1 wi+1 wi+2 
wi 
polj 
(a) Skip-Gram (b) Our Model 
sej 
sj 
Figure 2: The traditional Skip-Gram model and our neural architecture for learning sentiment-specific
phrase embedding (SSPE).
e
i
to predict the context words of w
i
, namely w
i?2
, w
i?1
, w
i+1
, w
i+2
, et al. Hierarchical softmax (Morin
and Bengio, 2005) is leveraged to accelerate the training procedure because the vocabulary size of phrase
table is typically huge. The objective of Skip-Gram is to maximize the average log probability:
f
syntactic
=
1
T
T
?
i=1
?
?c?j?c,j 6=0
log p(w
i+j
|e
i
) (1)
where T is the occurrence of each phrase in the corpus, c is the window size, e
i
is the embedding of the
current phrase w
i
, w
i+j
is the context words of w
i
, p(w
i+j
|e
i
) is calculated with hierarchical softmax.
The basic softmax unit is calculated as softmax
i
= exp(z
i
)/
?
k
exp(z
k
). We leave out the details
of hierarchical softmax (Morin and Bengio, 2005; Mikolov et al., 2013b) due to the page limit. It is
worth noting that, Skip-Gram is capable to learn continuous representation of words and phrases with
the identical model (Mikolov et al., 2013b).
To integrate sentiment information into the continuous representation of phrases, we develop a tailored
neural architecture to learn SSPE, as illustrated in Figure 2(b). Given a triple ?w
i
, s
j
, pol
j
? as input,
where w
i
is a phrase contained in the sentence s
j
whose gold sentiment polarity is pol
j
, our training
objective is to (1) utilize the embedding of w
i
to predict its context words, and (2) use the sentence
representation se
j
to predict the gold sentiment polarity of s
j
, namely pol
j
. We simply average the
embedding of phrases contained in a sentence as its continuous representation (Huang et al., 2012). The
objective of the sentiment part is to maximize the average of log sentiment probability:
f
sentiment
=
1
S
S
?
j=1
log p(pol
j
|se
j
) (2)
where S is the occurrence of each sentence in the corpus,
?
k
pol
jk
= 1. For binary classification
between positive and negative, the distribution of [0,1] is for positive and [1,0] is for negative. Our final
training objective is to maximize the linear combination of the syntactic and sentiment parts:
f = ? ? f
syntactic
+ (1? ?) ? f
sentiment
(3)
where ? weights the two parts. Accordingly, the nearest neighbors in the embedding space of SSPE are
favored to have similar semantic usage as well as the same sentiment polarity.
We train our neural model with stochastic gradient descent and use AdaGrad (Duchi et al., 2011) to
update the parameters. We empirically set embedding length as 50, window size as 3 and the learning
rate of AdaGrad as 0.1. Hyper-parameter ? is tuned on the development set. To obtain large-scale
training corpus, we collect tweets from April, 2013 through TwitterAPI. After filtering the tweets that
are too short (< 5 words) and removing @user and URLs, we collect 10M tweets (5M positive and 5M
negative) with positive and negative emoticons
3
, which is are utilized as the training data to train our
neural model. The vocabulary size is 750,000 after filtering the 1?4 grams through frequency.
3
We use the emoticons selected by Hu et al. (2013), namely :) : ) :-) :D =) as positive and :( : ( :-( as negative ones.
175
3.2 Seed Expansion with Urban Dictionary
Urban Dictionary is a web-based dictionary that contains more than seven million definitions until March,
2013
4
. It was intended as a dictionary of slang, cultural words or phrases not typically found in standard
dictionaries, but it is now used to define any word or phrase. For each item in Urban Dictionary, there is
a list of similar words contributed by volunteers. For example, the similar words of ?cooool? are ?cool?,
?awesome?, ?coooool?, et al
5
and the similar words of ?not bad? are ?good?, ?ok? and ?cool?, et al
6
.
These similar words are typically semantically close to and have the same sentiment polarity with the
target word. We conduct preliminary statistic on the items of Urban Dictionary from ?a? to ?z?, and
find that there are total 799,430 items containing similar words and each of them has about 10.27 similar
words on average.
We utilize Urban Dictionary to expand little sentiment seeds for collecting training data for building
the phrase-level sentiment classifier. We manually label the top frequent 500 words from the vocabulary
of SSPE as positive, negative or neutral. After removing the ambiguous ones, we obtain 125 positive, 109
negative and 140 neutral words, which are regarded as the sentiment seeds
7
. Afterwards, we leverage
the similar words from Urban Dictionary to expand the sentiment seeds. We first build a k-nearest
neighbors (KNN) classifier by regarding the sentiment seeds as gold standard. Then, we employ the KNN
classifier on the items of Urban Dictionary containing similar words, and predict a three-dimensional
discrete vector [knn
pos
, knn
neg
, knn
neu
] for each item, reflecting the hits numbers of sentiment seeds
with different sentiment polarity in its similar words. For example, the vector value of ?not bad? is
[10, 0, 0], which means that there are 10 positive seeds, 0 negative seeds and 0 neutral seeds occur in
its similar words. To ensure the quality of the expanded words, we set threshold for each category to
collect the items with high quality as expanded words. Take the positive category as an example, we
keep an item as positive expanded word if it satisfies knn
pos
> knn
neg
+ threshold
pos
and knn
pos
>
knn
neu
+ threshold
pos
simultaneously. We empirically set the thresholds of positive, negative and
neutral as 6,3,2 respectively by balancing the size of expanded words in three categories. After seed
expansion, we collect 1,512 positive, 1,345 negative and 962 neutral words, which are used as the training
data to build the phrase-level sentiment classifier. We also tried the propagation methods to expand the
sentiment seeds, namely iteratively added the similar words of sentiment seeds from Urban Dictionary
into the expanded word collection. However, the quantity of expanded words is less than the KNN-based
results and the quality is relatively poor.
After obtaining the training data and feature representation of phrases, we build the phrase-level clas-
sifier with softmax, whose length is two for the positive vs negative case:
y(w) = softmax(? ? e
i
+ b) (4)
where ? and b are the parameters of classifier, e
i
is the embedding of the current phrase w
i
, y(w) is the
predicted sentiment distribution of item w
i
. We employ the classifier to predict the sentiment distribution
of each phrase in the vocabulary of SSPE, and save the phrases as well as their sentiment probability in
the positive (negative) lexicon if the positive (negative) probability is larger than 0.5.
4 Experiment
In this section, we conduct experiments to evaluate the effectiveness of our sentiment lexicon (TS-Lex)
by applying it in the supervised learning framework for Twitter sentiment classification, as given in
subsection 4.1. We also directly evaluate the quality of SSPE as it forms the fundamental component for
building sentiment lexicon. We use SSPE as the feature for sentiment classification of items in existing
sentiment lexicons, as described in subsection 4.2.
4
http://en.wikipedia.org/wiki/Urban Dictionary
5
http://www.urbandictionary.com/define.php?term=cooool
6
http://www.urbandictionary.com/define.php?term=not+bad
7
We will publish the sentiment seeds later.
176
4.1 Twitter Sentiment Classification
Experiment Setup and Dataset We conduct experiments on the benchmark Twitter sentiment classi-
fication dataset (message-level) from SemEval 2013 (Nakov et al., 2013). The training and development
sets were completely released to task participants. However, we were unable to download all the training
and development sets because some tweets were deleted or not available due to modified authorization
status. The statistic of the positive and negative tweets in our dataset are given in Table 1(b). We train
positive vs negative classifier with LibLinear (Fan et al., 2008) with default settings on the training set,
tune parameters -c on the dev set and evaluate on the test set. The evaluation metric is Macro-F1.
(a) Sentiment Lexicons
Lexicon Positive Negative Total
HL 2,006 4,780 6,786
MPQA 2,301 4,150 6,451
NRC-Emotion 2,231 3,324 5,555
TS-Lex 178,781 168,845 347,626
HashtagLex 216,791 153,869 370,660
Sentiment140Lex 480,008 260,158 740,166
(b) SemEval 2013 Dataset
Positive Negative Total
Train 2,642 994 3,636
Dev 408 219 627
Test 1,570 601 2,171
Table 1: Statistic of sentiment lexicons and Twitter sentiment classification datasets.
Results and Analysis We compare TS-Lex with HL
8
(Hu and Liu, 2004), MPQA
9
(Wilson et al.,
2005), NRC-Emotion
10
(Mohammad and Turney, 2012), HashtagLex and Sentiment140Lex
11
(Moham-
mad et al., 2013). The statistics of TS-Lex and other sentiment lexicons are illustrated in Table 1(a). HL,
MPQA and NRC-Emotion are traditional sentiment lexicons with a relative small lexicon size. Hashta-
gLex and Sentiment140Lex are Twitter-specific sentiment lexicons. We can find that, TS-Lex is larger
than the traditional sentiment lexicons.
We evaluate the effectiveness of TS-Lex by applying it as the features for Twitter sentiment classifica-
tion in the supervised learning framework (Pang et al., 2002). We conduct experiments in two settings,
namely only utilizing the lexicon features (Unique) and appending lexicon feature to existing feature
sets (Appended). In the first setting, we design the lexicon features as same as the top-performed Twit-
ter sentiment classification system in SemEval2013
12
(Mohammad et al., 2013). For each sentiment
polarity (positive vs negative), the lexicon features are:
? total count of tokens in the tweet with score greater than 0;
? the sum of the scores for all tokens in the tweet;
? the maximal score;
? the non-zero score of the last token in the tweet;
In the second experiment setting, we append the lexicon features to the existing basic feature. We use
the feature sets of Mohammad et al. (2013) excluding the lexicon feature as the basic feature, including
bag-of-words, pos-tagging, emoticons, hashtags, elongated words, etc. Experiment results of the Unique
features and Appended features from different sentiment lexicons on Twitter sentiment classification are
given in Table 2(a).
From Table 2(a), we can find that TS-Lex yields best performance in both Unique and Appended
feature sets among all sentiment lexicons, including two large-scale Twitter-specific sentiment lexicons.
The reason is that the classifier for building TS-Lex utilize (1) the well developed feature representation
of phrases (SSPE), which captures the semantic and sentiment connections between phrases, and (2) the
enlarged sentiment words through web intelligence as training data. HashtagLex and Sentiment140Lex
8
http://www.cs.uic.edu/ liub/FBS/sentiment-analysis.html#lexicon
9
http://mpqa.cs.pitt.edu/lexicons/subj lexicon/
10
http://www.saifmohammad.com/WebPages/ResearchInterests.html
11
We utilize the unigram and bigram lexicons from HashtagLex and Sentiment140Lex.
12
http://www.saifmohammad.com/WebPages/Abstracts/NRC-SentimentAnalysis.htm
177
(a)
Lexicon Unique Appended
HL 60.49 79.40
MPQA 59.15 76.54
NRC-Emotion 54.81 76.79
HashtagLex 65.30 76.67
Sentiment140Lex 72.51 80.68
TS-Lex 78.07 82.36
(b)
Lexicon Unique
Seed 57.92
Expand 60.69
Lexicon(seed) 74.64
TS-Lex 78.07
Table 2: Macro-F1 on Twitter sentiment classification with different lexicon features.
only utilize the relations between phrases and hashtag/emoticon seeds, yet do not well capture the con-
nections between phrases. In the Unique setting, the performances of the traditional lexicons (HL, MPQA
and NRC-Emotion) are lower than large-scale Twitter-specific lexicons (HashtagLex, Sentiment140Lex
and our lexicon). The reason is that, tweets have the informal language style and contain slangs and di-
verse multi-word phrases, which are not well covered by the traditional sentiment lexicons with a small
size. After incorporating the lexicon feature of TS-Lex into the top-performs system (Mohammad et al.,
2013), we further improve the macro-F1 from 84.70% to 85.65%.
Effect of Seed Expansion with Urban Dictionary To verify the effectiveness of seed expansion
through Urban Dictionary, we conduct experiments by applying (1) sentiment seeds (Seed), (2) words
after expansion (Expand), (3) sentiment lexicon generated from the classifier only utilizing sentiment
seeds as training data (Lexicon(seed)), (4) the final lexicon (TS-Lex) exploiting the expanded words as
training data to build sentiment classifier, to produce lexicon features, and only use them for Twitter
sentiment classification (Unique). From Table 2(b), we find that the performance of sentiment seeds and
expanded words are relatively poor due to their low coverage. Under this scenario, seed expansion yields
2.77% improvement (from 57.92% to 60.69%) on macro-F1. By utilizing the expanded words as training
data to build the phrase-level sentiment classifier, TS-Lex obtains 3.43% improvements on Twitter senti-
ment classification (from 74.64% to 78.07%), which verifies the effectiveness of seed expansion through
Urban Dictionary. In addition, we find that only using a small number of sentiment seeds as the training
data, we can obtain superior performance (74.64%) than all baseline lexicons. This indicates that the
representation learning approach effectively capture the semantic and sentimental connections between
phrases through SSPE, and leverage them for building the sentiment lexicon.
Effect of ? in SSPE We tune the hyper-parameter ? of SSPE on the development set of SemEval 2013,
and study its influence on the performance of Twitter sentiment classification by applying the generated
lexicon as features. We utilize the expanded words as training data to train softmax and only utilize the
lexicon features (Unique) for Twitter sentiment classification. Experiment results with different ? are
illustrated in Figure 3(a).
From Figure 3(a), we can see that that SSPE performs better when ? is in the range of [0.1, 0.3], which
is dominated by the sentiment information. The model with ? = 1 stands for Skip-Gram model. The
sharp decline at ? = 1 indicates the importance of sentiment information in learning sentiment-specific
phrase embedding for building sentiment lexicon.
Discussion In the experiment, we do not apply TS-Lex into the unsupervised learning framework for
Twitter sentiment classification. The reason is that the lexicon-based unsupervised method typically
require the sentiment lexicon to have high precision, yet our task is to build large-scale lexicon (TS-Lex)
with broad coverage. We leave this as the future work, although we may set higher threshold (e.g. larger
than 0.5) to increase the precision of TS-Lex and loose the recall.
4.2 Evaluation of Different Representation Learning Methods
Experiment Setup and Dataset We conduct sentiment classification of items in two traditional senti-
ment lexicons, HL (Hu and Liu, 2004) and MPQA (Wilson et al., 2005), to evaluate the effective of the
178
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.735
0.74
0.745
0.75
0.755
0.76
0.765
0.77
0.775
0.78
0.785
?
Ma
cro
?F1
 
 
TS?Lex
(a) SSPE with different ? on the development set for Twitter
sentiment classification.
0.66
0.68
0.7
0.72
0.74
0.76
0.78
0.8
0.82
0.84
0.86
Mac
ro?F
1
 
 
C&W ReEmbed(C&W) W2V ReEmbed(W2V) MVSA SSPE
MPQAHL
(b) Sentiment classification of lexicons with different embed-
ding learning algrithms.
Figure 3: Experiment results with different settings.
sentiment-specific phrase embedding (SSPE). We train the positive vs negative classifier with LibLin-
ear (Fan et al., 2008). The evaluation metric is the macro-F1 of 5-fold cross validation. The statistics of
HL and MPQA are listed in Table 1(a).
Baseline Embedding Learning Algorithms We compare SSPE with the following embedding learn-
ing algorithms:
(1) C&W. C&W is one of the most representative embedding learning algorithms (Collobert et al.,
2011) for learning word embedding, which has been proven effective in many NLP tasks.
(2) W2V. Mikolov et al. (2013a) introduce Word2Vec for learning the continuous vectors for words
and phrases. We utilize Skip-Gram as it performs better than CBOW in the experiments.
(3) MVSA. Maas et al. (2011) learn word vectors for sentiment analysis with a probabilistic model of
documents utilizing the sentiment polarity of documents.
(4) ReEmbed. Lebret et al. (2013) learn task-specific embedding from existing embedding and task-
specific corpus. We utilize the training set of Twitter sentiment classification as the labeled corpus to
re-embed words. ReEmbed(C&W) and ReEmbed(W2V) stand for the use of different embedding results
as the reference word embedding.
The embedding results of the baseline algorithms and SSPE are trained with the same dataset and
parameter sets.
Results and Analysis Experiment results of the baseline embedding learning algorithms and SSPE are
given in Figure 3(b). We can see that SSPE yields best performance on both lexicons. The reason is that
SSPE effectively encode the sentiment information of tweets as well as the syntactic contexts of phrases
from massive data into the continuous representation of phrases. The performances of C&W and W2V
are relatively low because they only utilize the syntactic contexts of items, yet ignore the sentiment in-
formation of text, which is crucial for sentiment analysis. ReEmbed(C&W) and ReEmbed(W2V) achieve
better performance than C&W and W2V because the sentiment information of sentences are incorporated
into the continuous representation of phrases. There is a gap between ReEmbed and SSPE because SSPE
leverages more sentiment supervision from massive tweets collected by positive and negative emoticons.
5 Conclusion
In this paper, we propose building large-scale Twitter-specific sentiment lexicon with a representation
learning approach. Our method contains two parts: (1) a representation learning algorithm to effectively
learn the embedding of phrases, which are used as features for classification, (2) a seed expansion al-
gorithm that enlarge a small list of sentiment seeds to obtain training data for building the phrase-level
sentiment classifier. We introduce a tailored neural architecture and integrate the sentiment information
of tweets into its hybrid loss function for learning sentiment-specific phrase embedding (SSPE). We
learn SSPE from the tweets collected by positive and negative emoticons, without any manual annota-
179
tion. To collect more training data for building the phrase-level classifier, we utilize the similar words
from Urban Dictionary to expand a small list of sentiment seeds. The effectiveness of our sentiment
lexicon (TS-Lex) has been verified through applied in the supervised learning framework for Twitter
sentiment classification. Experiment results on the benchmark dataset of SemEval 2013 show that, TS-
Lex outperforms previously introduced sentiment lexicons and further improves the top-perform system
in SemEval 2013 with feature combination. In future work, we plan to apply TS-Lex into the unsuper-
vised learning framework for Twitter sentiment classification.
Acknowledgements
We thank Nan Yang, Yajuan Duan and Yaming Sun for their great help. This research was partly sup-
ported by National Natural Science Foundation of China (No.61133012, No.61273321, No.61300113).
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. Sentiwordnet 3.0: An enhanced lexical resource
for sentiment analysis and opinion mining. In LREC, volume 10, pages 2200?2204.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language
model. Journal of Machine Learning Research, 3:1137?1155.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation learning: A review and new perspec-
tives. IEEE Trans. Pattern Analysis and Machine Intelligence.
David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. the Journal of machine
Learning research, 3:993?1022.
Lu Chen, Wenbo Wang, Meenakshi Nagarajan, Shaojun Wang, and Amit P Sheth. 2012. Extracting diverse
sentiment expressions with target-dependent polarity from twitter. In ICWSM.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493?2537.
George E Dahl, Ryan P Adams, and Hugo Larochelle. 2012. Training restricted boltzmann machines on word
observations. ICML.
Sanjiv R Das and Mike Y Chen. 2007. Yahoo! for amazon: Sentiment extraction from small talk on the web.
Management Science, 53(9):1375?1388.
John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochas-
tic optimization. The Journal of Machine Learning Research, pages 2121?2159.
Andrea Esuli and Fabrizio Sebastiani. 2005. Determining the semantic orientation of terms through gloss classifi-
cation. In Proceedings of the 14th ACM international conference on Information and knowledge management,
pages 617?624. ACM.
Andrea Esuli and Fabrizio Sebastiani. 2007. Pageranking wordnet synsets: An application to opinion mining. In
ACL, volume 7, pages 442?431.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for
large linear classification. The Journal of Machine Learning Research, 9:1871?1874.
Ronen Feldman. 2013. Techniques and applications for sentiment analysis. Communications of the ACM,
56(4):82?89.
Ming Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM
SIGKDD Conference on Knowledge Discovery and Data Mining, pages 168?177.
Xia Hu, Jiliang Tang, Huiji Gao, and Huan Liu. 2013. Unsupervised sentiment analysis with emotional signals.
In Proceedings of the International World Wide Web Conference, pages 607?618.
Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2012. Improving word representations
via global context and multiple word prototypes. In ACL, pages 873?882. ACL.
180
Soo-Min Kim and Eduard Hovy. 2004. Determining the sentiment of opinions. In Proceedings of the 20th
international conference on Computational Linguistics, page 1367. Association for Computational Linguistics.
Igor Labutov and Hod Lipson. 2013. Re-embedding words. In Annual Meeting of the Association for Computa-
tional Linguistics.
R?emi Lebret, Jo?el Legrand, and Ronan Collobert. 2013. Is deep learning really necessary for word embeddings?
NIPS workshop.
Fangtao Li, Sinno Jialin Pan, Ou Jin, Qiang Yang, and Xiaoyan Zhu. 2012. Cross-domain co-extraction of
sentiment and topic lexicons. In Proceedings of the 50th ACL, pages 410?419. ACL, July.
Dekang Lin. 1994. Principar: an efficient, broad-coverage, principle-based parser. In Proceedings of the 15th
conference on COLING, pages 482?488. Association for Computational Linguistics.
Bing Liu. 2012. Sentiment analysis and opinion mining. Synthesis Lectures on Human Language Technologies,
5(1):1?167.
Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In Proceedings of the Annual Meeting of the Association for
Computational Linguistics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations
in vector space. ICLR.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013b. Distributed representations of
words and phrases and their compositionality. The Conference on Neural Information Processing Systems.
Saif M Mohammad and Peter D Turney. 2012. Crowdsourcing a word?emotion association lexicon. Computa-
tional Intelligence.
Saif M Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. Nrc-canada: Building the state-of-the-art in
sentiment analysis of tweets. Proceedings of the International Workshop on Semantic Evaluation.
Frederic Morin and Yoshua Bengio. 2005. Hierarchical probabilistic neural network language model. In Proceed-
ings of the international workshop on artificial intelligence and statistics, pages 246?252.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva, Veselin Stoyanov, Alan Ritter, and Theresa Wilson. 2013.
Semeval-2013 task 2: Sentiment analysis in twitter. In Proceedings of the International Workshop on Semantic
Evaluation, volume 13.
Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and trends in information
retrieval, 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine
learning techniques. In Proceedings of the Conference on Empirical Methods in Natural Language Processing,
pages 79?86.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009. Expanding domain sentiment lexicon through double
propagation. In IJCAI, volume 9, pages 1199?1204.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2011. Opinion word expansion and target extraction through
double propagation. Computational linguistics, 37(1):9?27.
Delip Rao and Deepak Ravichandran. 2009. Semi-supervised polarity lexicon induction. In Proceedings of the
12th Conference of the European Chapter of the Association for Computational Linguistics, pages 675?682.
Association for Computational Linguistics.
Richard Socher, J. Pennington, E.H. Huang, A.Y. Ng, and C.D. Manning. 2011. Semi-supervised recursive
autoencoders for predicting sentiment distributions. In Conference on Empirical Methods in Natural Language
Processing, pages 151?161.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher
Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Conference
on Empirical Methods in Natural Language Processing, pages 1631?1642.
Yaming Sun, Lei Lin, Duyu Tang, Nan Yang, Zhenzhou Ji, and Xiaolong Wang. 2014. Radical-enhanced chinese
character embedding. arXiv preprint arXiv:1404.4714.
181
Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu, and Bing Qin. 2014. Learning sentiment-specific word
embedding for twitter sentiment classification. In Procedding of the 52th Annual Meeting of Association for
Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for
semi-supervised learning. Annual Meeting of the Association for Computational Linguistics.
Peter D Turney. 2002. Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of
reviews. In Proceedings of Annual Meeting of the Association for Computational Linguistics, pages 417?424.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Hannan, and Ryan McDonald. 2010. The viability of web-
derived polarity lexicons. In Human Language Technologies: The 2010 Annual Conference of the North Amer-
ican Chapter of the Association for Computational Linguistics, pages 777?785. Association for Computational
Linguistics.
Janyce Wiebe. 2000. Learning subjective adjectives from corpora. In AAAI/IAAI, pages 735?740.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level senti-
ment analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages
347?354.
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and Jun Zhao. 2013. Mining opinion words and opinion targets in a
two-stage framework. In Proceedings of the 51st ACL, pages 1764?1773. ACL.
182
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 917?927, Dublin, Ireland, August 23-29 2014.
Triple based Background Knowledge Ranking for Document Enrichment
Muyu Zhang, Bing Qin
?
, Ting Liu, Mao Zheng
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
{myzhang,qinb,tliu,mzheng}@ir.hit.edu.cn
Abstract
Document enrichment is the task of retrieving additional knowledge from external resource over
what is available through source document. This task is essential because of the phenomenon
that text is generally replete with gaps and ellipses since authors assume a certain amount of
background knowledge. The recovery of these gaps is intuitively useful for better understanding
of document. Conventional document enrichment techniques usually rely on Wikipedia which
has great coverage but less accuracy, or Ontology which has great accuracy but less cover-
age. In this study, we propose a document enrichment framework which automatically extracts
?argument
1
, predicate, argument
2
? triple from any text corpus as background knowledge, so
that to ensure the compatibility with any resource (e.g. news text, ontology, and on-line ency-
clopedia) and improve the enriching accuracy. We first incorporate source document and back-
ground knowledge together into a triple based document-level graph and then propose a global
iterative ranking model to propagate relevance score and select the most relevant knowledge
triple. We evaluate our model as a ranking problem and compute the MAP and P&N score to
validate the ranking result. Our final result, a MAP score of 0.676 and P&20 score of 0.417
outperform a strong baseline based on search engine by 0.182 in MAP and 0.04 in P&20.
1 Introduction
Document enrichment is the task to acquire background knowledge from external resources and recover
the omitted information automatically for certain document. This task is essential because authors usu-
ally omit basic but well-known information to make the document more concise. For example, author
omits ?Baghdad is the captain of Iraqi? in the text of Figure 1 (a), which is well-known to readers. Dur-
ing reading process, these gaps will be automatically plugged effortlessly by the background knowledge
in human brain. However, the situation is different for machine because it lacks the ability to acquire
and select the proper background knowledge, which limits the performances of certain NLP applica-
tions. Document enrichment has been proved helpful in these tasks such as web search (Pantel and
Fuxman, 2011), coreference resolution (Bryl et al., 2010), document cluster (Hu et al., 2009) and entity
disambiguation (Bunescu and Pasca, 2006; Sen, 2012).
In the past, there are mainly two kinds of document enrichment researches according to the resource
they relying on. The first line of works make use of WikiPedia, the largest available on-line encyclopedia
as resource and link the entity (e.g. Baghdad) of document to its corresponding Wiki page (e.g. Baghdad
1
in WikiPedia), so that to enrich the document with the context of Wiki page (Bunescu and Pasca, 2006;
Cucerzan, 2007; Han et al., 2011; Kataria et al., 2011; Sen, 2012; He et al., 2013). Despite the great
success of these methods, there remain a great challenge that not all information in the linked Wiki page
is helpful to the understanding of corresponding document. For example, the Wiki page of Baghdad
contains lots of information about city history and culture, which are not quite relevant to the semantic of
context in Figure 1 (a). So treating the whole Wiki page as the enrichment to document may cause noise
?
Corresponding author.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
http://en.wikipedia.org/wiki/Baghdad
917
S1: Coalition may never know if   Iraqi   president   Saddam 
Hussein survived a U.S. air strike yesterday.
S2: A B-1 bomber dropped four 2,000-pound bombs on a building 
in a residential area of   Baghdad  . 
S3: They had got an intelligence reports senior officials were 
meeting there, possibly including Saddam Hussein and  his sons .
BaghdadIraqi hasCapital
Saddam Hussein diedIn
Qusay HusseinSaddam Hussein hasChild
Kadhimiya
k1:
k2:
k3:
Global 
Ranking
(a) Source document (b) Top-3 background knowledge
S1: Coalition may never know if   Iraqi   president   Saddam Hussein 
survived a U.S. air strike yesterday.
S2: A B-1 bomber dropped four 2,000-pound bombs on a building 
in a residential area of   Baghdad  . 
S3: They had got an intelligence reports senior officials were 
meeting there, possibly including  Saddam Hussein  and   his sons .
(a) Source document (b) Two relevant background knowledge
Iraq
Baghdad
Saddam Hussein
Captain
Died In Kadhimiya
Global 
Ranking
k1:
k2:
Figure 1: An example of document enrichment with background knowledge: (a) source document talking
about a U.S. air strike aiming at Saddam in Baghdad (b) two important relevant information, which is
omitted in source document but acquired by our model and enriched as background knowledge .
problem. Another line of works rely on the Ontologies constructed with supervision or even manually
which have great accuracy but less coverage (Motta et al., 2000; Passant, 2007; Fodeh et al., 2011; Kumar
and Salim, 2012). Besides, these methods usually rely on special ontology which is rather difficult to
construct and in turn limits the coverage and application of these methods.
Ideally, we would wish to integrate both coverage and accuracy, where an triple based background
knowledge ranking model may help. Our framework extracts knowledge from any corpus resource in-
cluding WikiPedia to ensure coverage and present knowledge as ?argument
1
, predicate, argument
2
?
triple to reduce noise. This model ranks background knowledge triples according to their relevance to
the source document. The key idea behind the model is that document is constructed by several units of
information, which can be extracted automatically. For every background knowledge b extracted auto-
matically from a relevant corpus, the more units are relevant to b and the more important they are, the
more relevant b becomes to the source document. Thus, we extract both source document information
and background knowledge automatically and present them together in a document-level graph. Then
we propagate the relevance score from the source document information to the background knowledge
during an iterative process. After convergence, we obtain the Top n relevant background knowledge,
rather than retrieving all of them without filtering.
To evaluate our model, we use ACE
2
corpus as source documents and output the ranked list of back-
ground knowledge. Then we train three annotators to check the ranking result and annotating whether
certain knowledge is relevant to corresponding source document separately. We totally annotated more
than 7000 background knowledge by three annotators. We evaluate their annotation consistence by com-
puting the Fleiss
?
Kappa (Fleiss, 1971), a famous criterion in multi-annotator consistence evaluation.
We achieve a Fleiss
?
Kappa of value 0.8066 in best situation and 0.7076 in average, which indicates
the great consistence between three annotators. The ranking result is evaluated with MAP score and
P&N score (Voorhees et al., 2005). We finally achieve aMAP score of 0.676 and P&20 score of 0.417
in Top 20 background knowledge, which are higher by 0.182 and 0.04 than a strong baseline based
on search engine. We also evaluate the effect of the automatically extraction to source document and
background knowledge, which is key to the performance of our method in real application.
2 Triple Graph based Document Representation
We believe that different parts of document are related to each other, rather than isolated. Hence, we
propose a triple graph based document representation to incorporate source document information and
background knowledge. In this presentation, ?argument
1
, predicate, argument
2
? triple serves as node
and the edge between nodes indicates their semantic relevance. In this part, we introduce triple graph
and the way to extract source document information and background knowledge automatically.
2.1 Motivation for triple presentation
Compared to Wiki Page, triple based enrichment helps to reduce noise illustrated in Section 1. Compared
to bag of words, triple based presentation help to reduce ambiguity of single word which is shown in
2
http://catalog.ldc.upenn.edu/LDC2006T06
918
B ar ac k O bam a
H ar v ar d  U niv e r s it y W h it e  H ou s e
(a) Ambiguity (b) Disambiguation
B ar ac k O bam a,  e a r n ,  l a w  d e g r e e
H ar v ar d  U niv e r s it y W h it e  H ou s e
Figure 2: The motivation for the form of triple (a) relevance ambiguity of single word Obama, which
is related to Harvard and White House (b) disambiguation with the help of other triple elements, where
?earn, law degree? help to limit Obama to the graduate of Harvard.
Figure 2. Figre 2 (a) shows that the single word of Obama is related to multiple semantic information
such as Harvard University as a law graduate and White House as the president. After introducing the
information from other elements of the triple, ?earn, law degree? help to disambiguate and limit Obama
to the law graduate of Harvard University only in Figure 2 (b). The form of triple has been used as the
presentation of knowledge in some researches such as knowledge base (Hoffart et al., 2013).
2.2 Nodes in the Graph
There are two kinds of nodes in the triple graph: source document nodes (sd-nodes) and background
knowledge nodes (bk-nodes). Both of them are extracted automatically with Open Information Ex-
traction (Open IE) technology which focuses on extracting assertions from massive corpora without a
pre-specied vocabulary (Banko et al., 2007). Open IE systems are unlexicalized-formed only in terms of
syntactic tokens and closed-word classes, instead of specific nouns and verbs at all costs.
There are existing Open IE systems such as TextRunner (Banko et al., 2007), WOE (Wu and Weld,
2010), and StatSnowball (Zhu et al., 2009). The output of these systems has been used to support many
NLP tasks such as learning selectional preference (Ritter et al., 2010), acquiring sense knowledge (Lin
et al., 2010), and recognizing entailment (Schoenmackers et al., 2010). In this work, we use the famous
Open IE system Reverb (Etzioni et al., 2011), which is generated from TextRunner (Etzioni et al., 2008),
to extract source document information and background knowledge automatically. We use the newest
version of ReVerb (version 1.3) without modification, which is free download on-line
3
.
Source document node (sd-node) Sd-nodes consists of the information extracted from source docu-
ment automatically by open information extraction technology (Banko et al., 2007), especially Reverb,
the famous Open IE system developed by University of Washington (Etzioni et al., 2011). The output
of ReVerb is formed as ?argument
1
, predicate, argument
2
?, which is naturally presented as triple. In
this study, we use ACE corpus as source documents and all sd-nodes are extracted by ReVerb. The setup
of automatic extraction makes our method usable in many real applications. To evaluate the effect of
automatic extraction, we also use the golden annotation within ACE (Doddington et al., 2004) corpus as
source document information and compare the performance that with automatic extraction.
Background knowledge node (bk-node) Bk-nodes consist of the background knowledge extracted
from external corpus resources automatically by Reverb too. We do not rely on certain existed knowl-
edge base and extract background knowledge from external corpus resources for corresponding source
document. This setup makes our methods usable in many real applications. Although we do not rely on
special knowledge base, we do adapt our method for the existed knowledge base such as YAGO (Hoffart
et al., 2013) and compare the performance to evaluate the effect of different knowledge sources.
2.3 Edges in the Graph
The edges between two nodes indicate their semantic relevance, which is evaluated in Section 3.1. There
are two kinds of edges: (1) sd-node to sd-node (2) sd-node to bk-node, both of them are undirected.
Considering all the relevance score originating from sd-nodes, we connect no edge between bk-nodes.
3
http://reverb.cs.washington.edu/
919
Edges between sd-nodes All sd-nodes are extracted from the same document, so they should be related
to each other. We connect each pair of sd-nodes with an edge and set the weight of edge as their semantic
relevance computed in Section 3.1. With this setup, we combine the source document as a whole where
different parts affect each other through the edge.
Edges between sd-node and bk-node The basic idea of our model is to propagate relevance score
from the sd-nodes to bk-nodes. Hence, we connect each pair of sd-node and bk-node with an edge and
set the weight of the edge as their relevance computed in Section 3.1. These edges are all undirected,
which indicates that bk-nodes also affect the relevance score of the sd-nodes during the ranking process.
3 Global Ranking Model
In this study, source document D is presented as the graph of sd-nodes. For every background knowledge
b, the task of evaluating the relevance between b and D is naturally converted into evaluating the relevance
between b and the graph of sd-nodes. So the relevance between b and document D can be computed by
propagating the relevance score from every sd-node of D to b iteratively. After the convergence, the
relevance between b and D can be evaluated by the relevance score of b. Intuitively, three factors affect
their relevance:
? How many sd-nodes is b relevant to ?
? How relevant is b to these sd-nodes?
? How important are these sd-nodes ?
For the first factor, b should be more relevant to source document D if more sd-nodes are relevant
to b. We capture this information by allowing b to receive relevance score from all the sd-nodes. For
the second factor, b should be more relevant to D if more relevant b is to sd-nodes. We consider this
information by evaluating the relevance between b and every sd-node (Section 3.1). For the last factor,
important sd-nodes should have higher impact. We consider this information by evaluating the impor-
tance of sd-nodes and assigning higher initial value to importance ones (Section 3.3). We combine all
factors in the global ranking process to select the top-n relevant background knowledge (Section3.2).
3.1 Relevance Evaluation between Nodes
In this section, we evaluate the semantic relevance between different nodes which is the weight of the
edge between them. We introduce Search Engine as a resource, which has been proven effective in
relevance evaluation (Gligorov et al., 2007). This method is motivated by the phenomenon that the
number of results returned by search engine for query p ? qindicates the relevance between p and q.
However, considering the different popularization of queries, this number alone can not accurately
express their semantic relevance. For example, query car ? automobile gets 294, 300, 000 results,
whereas query car?apple gets 683, 000, 000, which is 2 times higher than the previous one. Obviously,
automobile is more relevant to car rather than Apple. The reason of this phenomenon is that apple
is far more popular than automobile, which increase its possibility of co-occurrence with car. So we
consider the number of results for p?q together with p and q withWebJaccard Coefficient (Bollegala
et al., 2007) to evaluate the relevance between p and q according to Formula 1, where H(p), H(q), and
H(p ? q) indicate the number of results for query p, p, and p ? q.
WebJaccard(p, q) =
{
0 if H(p ? q) ? C
H(p?q)
H(p)+H(q)?H(p?q)
otherwise.
(1)
To convert one ?argument
1
, predicate, argument
2
? triple into query, we use argument
1
?
argument
2
as the query for one triple. We have tried argument
1
? predicate ? argument
2
which
920
is usually very sparse. Besides, the combination of two arguments usually maintain better semantic com-
pleteness of triple compared to other combinations according to our analysis. So this setup aims to bal-
ance completeness and sparseness. Accordingly, two triples are combined as argument
1
?argument
2
?
argument
?
1
? argument
?
2
. Considering the scale and noise in the Web data, it is possible for two words
to appear together accidentally. To reduce the adverse effects attributed to random co-occurrences, we
set 0 to the WebJaccard Coefficient of query p? q, if the number of result is less than a threshold C.
3.2 Iterative Relevance Propagation
Here we propose the relevance propagation based iterative process to evaluate the relevance between cer-
tain background knowledge and source document. Note that standard label propagation mainly focuses
on classification task (Wang and Zhang, 2008). However, we focus on a ranking problem where the best
ranking result is computed during an iterative process in this study. So we make two modifications to
suit the ranking problem better: not reseting the relevance score and introducing the propagation between
source document information during iteration.
Propagation possibility The edge between node
i
and node
j
is weighted by r(i, j) to measure their
relevance. However, r(i, j) cannot completely present the propagation possibility because one node can
be equally relevant to all of its neighbors. Thus, we define p(i, j) based on r(i, j) in formula 2 to indicate
the propagation possibility between node
i
and node
j
.
p(i, j) =
r(i, j)? ?(i, j)
?
k?N
r(k, j)? ?(k, j)
(2)
N is the set of all nodes, ?(i, j) denotes whether an edge exists between node
i
and node
l
in the triple-
graph or not, which indicates whether they may propagate to each other or not. E is the set of edges.
?(i, j) =
{
1 if (i, j) ? E
0 otherwise
(3)
Iterative propagation There are n ? n pairs of nodes, the p(i, j) of them is stored in a matrix P .
we use
~
W = (w
1
, w
2
, ? ? ? , w
n
) to denote the relevance score of all nodes, in which w
i
indicates the
relevance between node
i
and source document D. Here the node
i
can indicate both sd-nodes and bk-
nodes because they are processed during one fellow step. So that we keep updating both sd-nodes and
bk-nodes and do not distinguish them explicitly. The only difference between them is that we initialize
the w
i
of sd-nodes as its importance to D (Section 3.1) while bk-nodes as 0 at the beginning. We use
matrix P together with ?(i, j) to compute the
~
W during a iterative process, where
~
W is updated to
~
W
?
during the end of every iteration. The matrix
~
W
?
is updated according to the following Formula 4:
~
W
?
=
~
W ? P
=
~
W ?
?
?
?
?
p(1, 1) p(1, 2) ? ? ? p(1, n)
p(2, 1) p(2, 2) ? ? ? p(2, n)
? ? ? ? ? ? ? ? ? ? ? ?
p(n, 1) p(n, 2) ? ? ? p(n, n)
?
?
?
?
(4)
each w
i
in
~
W is updated to w
?
i
according to the formula 5, where w
i
is propagated from all the other
w
j
(j 6= i) according to their propagation possibility p(j, i). We also introduce the propagation from
bk-nodes to sd-nodes, where bk-nodes serve as intermediate to help mining latent semantics.
w
?
i
= w
1
? p(1, i) + w
2
? p(2, i) + ? ? ?+ w
n
? p(n, i)
=
?
k?N
w
k
? p(k, i)
=
?
k?N
w
k
?
(
r(i, j)? ?(i, j)
?
k?N
r(k, j)? ?(k, j)
)
(5)
921
3.3 Importance Evaluation for sd-nodes
The main idea of our model is to propagate relevance score from sd-nodes to bk-nodes (Section 3.2).
So the initialization of sd-node is important, which indicates the importance of different source docu-
ment information. This section solves this problem by evaluating the importance of sd-nodes to source
document. We use v
j
to denote the initialization of sd-nodes, which indicates the importance of node
j
(node
j
? set of sd-nodes) to source document. In this section, we propose a modified relevance propa-
gation method to evaluate v
j
for sd-notes. We first construct a triple-graph consisting of sd-nodes only.
Then we initialize the relevance score of sd-nodes according to a simple approach based on text fre-
quency (Kohlsch?utter et al., 2010). We use similar relevance propagation process without resetting the
relevance score at the beginning of every iteration, until a global stable state is achieved. Finally, we
normalize all the relevance scores to get
~
V , which indicates the importance of sd-nodes to the source
document. We return
~
V to the global ranking model (Section 3.2) as part of the input. The initial impor-
tance of bk-nodes is set as 0 at the beginning, which denotes that all bk-nodes are ir-relevant to source
document before the starting of global ranking process.
4 Experiment
We treat our task as a ranking problem, which takes a document as input and output the ranked list of
background knowledge. We evaluate our method as a ranking problem similarly to information retrieval
task and focus on the performances of models with different setups.
4.1 Data Preparation
The experiment data consists of two parts: source document information and corresponding background
knowledge. To select source documents, we use the ACE corpus (Doddington et al., 2004) for 2005 eval-
uation
4
which consists of 599 articles from multiple sources. We use ReVerb to extract these documents
into multi-triples. For background knowledge, we first retrieve relevant web pages with simply term
matching method and then extract these pages with ReVerb into a set of triples serving as background
knowledge. To ensure the quality, we filter them according to the confidence given by ReVerb.
Besides automatic extraction, we also adapt our system to the golden annotation of ACE as source
document information and standard YAGO knowledge base
5
as background knowledge (Hoffart et al.,
2013). We compare its performance with that in fully automatic system and evaluate the effect of auto-
matic extraction. For better comparison with YAGO, we retrieve relevant pages from WikiPedia although
our automatic extraction method is applicable to any corpus resources.
For every outputted list, three trained annotators check the result and decide which background knowl-
edge is relevant to source document. They work separately and check the same list, so that we can e-
valuate their annotation consistence. They totally annotated more than 7000 background knowledge and
achieved a Fleiss
?
Kappa value of 0.8066 in best situation and 0.7076 in average between three anno-
tators, which is a good consistence between multi-annotator (Fleiss, 1971). When collision happened,
we choose the label selected by more annotators.
4.2 Baseline system
Although we treat our task as a ranking problem, it is difficult to apply corresponding methods in tra-
ditional ranking tasks such as information retrieval (IR) (Manning et al., 2008) and entity linking (EL)
(Han et al., 2011; Kataria et al., 2011; Sen, 2012) directly in our task. First, both IR and EL make use of
the link structure between web or Wiki pages. However, our task takes single document as input and no
link exists between documents which makes it difficult to apply IR and EL methods such as page rank
(Page et al., 1999) and collective method (Han et al., 2011; Sen, 2012) in this task directly. Second, EL
usually evaluate the text similarity between certain document and target page in WikiPedia. However,
our task focuses on the ranking of ?argument
1
, predicate, argument
2
? triple, which contains little text
information. Lack of text information also limits the application of corresponding methods in our task.
4
http://catalog.ldc.upenn.edu/LDC2006T06
5
http://www.mpi-inf.mpg.de/yago-naga/yago
922
Setup MAP P&20
Baseline 0.494 0.377
AutoSD + AutoBK + NoInitial 0.504 0.378
AutoSD + AutoBK + WithInitial 0.531 0.406
GoldSD + AutoBK + NoInitial 0.564 0.417
GoldSD + AutoBK + WithInitial 0.553 0.406
GoldSD + YAGO + NoInitial 0.676 0.328
GoldSD + YAGO + WithInitial 0.676 0.328
Table 1: The result of our model in different setups: GoldSD indicates using annotation of ACE corpus as
source document information; YAGO indicates using YAGO knowledge base as background knowledge;
AutoSD and AutoBK means aotomatic extraction to source document and background knowledge; NoIni-
tial and WithInitial means whether using different initial importance to source document information.
For better comparison, we introduce search engine as resource which is proved effective in relevance
evaluation (Gligorov et al., 2007) and propose a search engine based strong baseline. As illustrated
before, the relevance R
i
between background knowledge b
i
and source document D has been converted
into the relevance between b
i
and the triples of D. Hence, we compute R
i
by accumulating all r
ij
, the
relevance scores between b
i
and every sd-node s
j
with the same method in Section 3.1 (R
i
=
?
s
j
?S
r
ij
,
S is the set of sd-nodes). Then we rank all background knowledge according to the value ofR
i
and output
the ranked list as final result. We extract source document and background knowledge automatically in
the baseline system, which makes it applicable in different setups.
4.3 Experiment setup
We evaluate our model in different setups. First, we extract both source document information and
background knowledge automatically. Second, we use golden annotation of ACE as source document
information but extract background knowledge automatically. Third, we use golden annotation of ACE
and introduce standard YAGO as background knowledge. For all of them three, we evaluate the different
performances with and without initial importance of sd-nodes(Section 3.3). We evaluate the performance
with two famous criteria in ranking problem: MAP (Voorhees et al., 2005) requires more accuracy
and focuses on the knowledge in higher position; P&N which require more coverage and pays more
attention to the number of relevant ones in Top N knowledge. Note that we do not evaluate the Recall
performance because there can be millions of background knowledge to be ranked for every document.
It is impossible to check all of them. So we focus on the Top N candidates and evaluate the performance
with MAP and P&N . In this study, we evaluate the Top 20 background knowledge triples which are
most easily to be viewed by users.
4.4 Experiment Result
The performance of our model is shown in Table 1. Our search engine based baseline system achieve
a rather good performance: a MAP value of 0.494 and 0.377 in P&20. At the same time, our model
outperforms the baseline system in almost every setup and evaluation criterion. The best performance of
MAP is achieved by GoldSD+YAGO (0.676), while the best performance of P&20 is achieved by GoldS-
D+AutoBK (0.417). To analyze the result further, we find that the initial importance, automatic extraction
to source document, and to background knowledge have different effect on the final performance.
4.4.1 Effect of automatic extraction to source document
We use ACE corpus as source documents, which contain golden annotation to document information.
So we can evaluate the effect of automatic extraction to source document by comparing the performance
with and without golden annotation. The performance without golden annotation is shown in AutoS-
D+AutoBK of Table 1, while the other one shown in GoldSD+AutoBK. We can find that the performance
of GoldSD+AutoBK is better than that of AutoSD+AutoBK in both MAP and P&20, which indicates that
golden annotation do help to improve the ranking result.
923
We further analyze the result and find an interesting phenomenon: these two systems performs greatly
different with the setup of NoInitial, but equally with the setup of WithInitial, which indicates that the
performance of AutoSD+AutoBK has been improved by evaluating the importance of source document
information (Section 3.3). So we can naturally infer that, with a better importance evaluating method
in AutoSD+AutoBK, we may achieve similar performance compared to that in golden annotation. Note
that, AutoSD+AutoBK is compatible with any corpus which is more useful in real applications.
4.4.2 Effect of automatic extraction to background knowledge
We evaluate the effect of automatic extraction to background knowledge by comparing the performances
between GoldSD+AutoBK and GoldSD+YAGO. In GoldSD+AutoBK, the background knowledge is ex-
tracted automatically with ReVerb, which has greater coverage but less accuracy. In contrast, the GoldS-
D+YAGO make use of YAGO as background knowledge, which is less coverage but better accuracy. This
difference are reflected on the system performance, where GoldSD+YAGO achieves much better result in
MAP, but much worse in P&20. This is partly because that MAP focus on the background knowledge in
higher position which requires more accuracy, while P&20 pays more attention to the number of relevant
background knowledge which require more coverage.
In general, automatic extraction system has better coverage but less accuracy compared to YAGO
based system. However, automatic extraction to background knowledge may help in real applications by
improving coverage greatly. Besides, the loss of accuracy is partly due to the technology of information
extraction which may be improved in the future. In addition, we can also combine these two ways to
acquire background knowledge to balance coverage and accuracy in the future.
4.4.3 Effect of initial importance to source document information
Initial importance to source document information (Section 3.3) is important to the performance of
our models as shown in Table 1. The model AutoSD+AutoBK+WithInitial outperforms the AutoS-
D+AutoBK+NoInitial compared to other setups, which indicates the help of initial importance to the
ranking result. Especially, initial importance to source document information helps most in the set-
up of AutoSD+AutoBK, which is most useful in real applications. So we can naturally infer that, by
proposing better importance evaluating method, we may further improve the performance of AutoS-
D+AutoBK+WithInitial, which will great helpful in the future application of this method.
5 Related Work
Document enrichment focuses on introducing external knowledge into source document. There are main-
ly two kinds of works in this topic according to the resource they relying on. The first line of works make
use of WikiPedia and enrich source document by linking the entity to its corresponding Wiki page (Bunes-
cu and Pasca, 2006; Cucerzan, 2007). In early stage, most researches rely on the similarity between the
context of the mention and the definition of candidate entities by proposing different measuring crite-
ria such as dot product, cosine similarity, KL divergence, Jaccard distance and more complicated ones
(Bunescu and Pasca, 2006; Cucerzan, 2007; Zheng et al., 2010; Hoffart et al., 2011; Zhang et al., 2011).
However, these methods mainly rely on text similarity but neglect the internal structure between mention-
s. So another kind of works explore the structure information with collective disambiguation (Kulkarni
et al., 2009; Kataria et al., 2011; Sen, 2012; He et al., 2013). These methods make use of structure infor-
mation within context and resolve different mentions based on the coherence among decisions. Despite
the success, the entity linking methods rely on WikiPedia which has great coverage but less accuracy.
Another line of works try to improve the accuracy of enrichment by introducing ontologies (Motta
et al., 2000; Passant, 2007; Fodeh et al., 2011; Kumar and Salim, 2012) and structured knowledge
such as WordNet (Nastase et al., 2010) and Mesh (Wang and Lim, 2008). In these studies, resources
usually provides word or phrase semantic information such as synonym (Sun et al., 2011) and antonym
(Sansonnet and Bouchet, 2010). However, these methods rely on special ontologies constructed with
supervision or even manually, which is difficult to expand and in turn limits the application of them.
924
6 Conclusion and Future Work
This study presents a triple based background knowledge ranking model to acquire most relevant back-
ground knowledge to certain source document. We first develop a triple graph based document presen-
tation to combine source document together with the background knowledge. Then we propose a global
iterative ranking model to acquire Top n relevant knowledge, which provide additional information be-
yond the source document. Note that, both source document information and background knowledge
are extracted automatically which is useful in real application. The experiments show that our model
achieves better results over a strong baseline, which indicates the effectiveness of our framework.
Another interesting phenomenon is that YAGO based enrichment model achieved better ranking ac-
curacy, but less coverage compared to automatic extraction model. To combine these two sources of
background knowledge may help to overcome both coverage and accuracy problem. So exploiting prop-
er way to incorporate knowledge base and automatic extraction is an important topic in our future work.
Finally, we believe that this background knowledge based document enriching technology may help in
those semantic based NLP applications such as coherence evaluation, coreference resolution and question
answering. In our future work, we will explore how to make use of these background knowledge in real
applications, hopefully to improve the performance significantly in the future.
Acknowledgements
We thank Muyun Yang and Jianhui Ji for their great help. This work was supported by National Natural
Science Foundation of China(NSFC) via grant 61133012, the National 863 Leading Technology Re-
search Project via grant 2012AA011102 and the National Natural Science Foundation of China Surface
Project via grant 61273321.
References
Michele Banko, Michael J Cafarella, Stephen Soderland, Matthew Broadhead, and Oren Etzioni. 2007. Open
information extraction from the web. In IJCAI, volume 7, pages 2670?2676.
Danushka Bollegala, Yutaka Matsuo, and Mitsuru Ishizuka. 2007. Measuring semantic similarity between words
using web search engines. www, 7:757?766.
Volha Bryl, Claudio Giuliano, Luciano Serafini, and Kateryna Tymoshenko. 2010. Using background knowledge
to support coreference resolution. In ECAI, volume 10, pages 759?764.
Razvan C Bunescu and Marius Pasca. 2006. Using encyclopedic knowledge for named entity disambiguation. In
EACL, volume 6, pages 9?16.
Silviu Cucerzan. 2007. Large-scale named entity disambiguation based on wikipedia data. In EMNLP-CoNLL,
volume 7, pages 708?716.
George R Doddington, Alexis Mitchell, Mark A Przybocki, Lance A Ramshaw, Stephanie Strassel, and Ralph M
Weischedel. 2004. The automatic content extraction (ace) program-tasks, data, and evaluation. In LREC.
Citeseer.
Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S Weld. 2008. Open information extraction from
the web. Communications of the ACM, 51(12):68?74.
Oren Etzioni, Anthony Fader, Janara Christensen, Stephen Soderland, and Mausam Mausam. 2011. Open infor-
mation extraction: The second generation. In Proceedings of the Twenty-Second international joint conference
on Artificial Intelligence-Volume Volume One, pages 3?10. AAAI Press.
Joseph L Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological bulletin, 76(5):378.
Samah Fodeh, Bill Punch, and Pang-Ning Tan. 2011. On ontology-driven document clustering using core semantic
features. Knowledge and information systems, 28(2):395?421.
Risto Gligorov, Warner ten Kate, Zharko Aleksovski, and Frank van Harmelen. 2007. Using google distance to
weight approximate ontology matches. In Proceedings of the 16th international conference on World Wide Web,
pages 767?776. ACM.
925
Xianpei Han, Le Sun, and Jun Zhao. 2011. Collective entity linking in web text: a graph-based method. In
Proceedings of the 34th international ACM SIGIR conference on Research and development in Information
Retrieval, pages 765?774. ACM.
Zhengyan He, Shujie Liu, Mu Li, Ming Zhou, Longkai Zhang, and Houfeng Wang. 2013. Learning entity repre-
sentation for entity disambiguation. Proc. ACL2013.
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen F?urstenau, Manfred Pinkal, Marc Spaniol, Bilyana
Taneva, Stefan Thater, and Gerhard Weikum. 2011. Robust disambiguation of named entities in text. In Pro-
ceedings of the Conference on Empirical Methods in Natural Language Processing, pages 782?792. Association
for Computational Linguistics.
Johannes Hoffart, Fabian M Suchanek, Klaus Berberich, and Gerhard Weikum. 2013. Yago2: a spatially and
temporally enhanced knowledge base from wikipedia. Artificial Intelligence, 194:28?61.
Xiaohua Hu, Xiaodan Zhang, Caimei Lu, Eun K Park, and Xiaohua Zhou. 2009. Exploiting wikipedia as external
knowledge for document clustering. In Proceedings of the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 389?396. ACM.
Saurabh S Kataria, Krishnan S Kumar, Rajeev R Rastogi, Prithviraj Sen, and Srinivasan H Sengamedu. 2011.
Entity disambiguation with hierarchical topic models. In Proceedings of the 17th ACM SIGKDD international
conference on Knowledge discovery and data mining, pages 1037?1045. ACM.
Christian Kohlsch?utter, Peter Fankhauser, and Wolfgang Nejdl. 2010. Boilerplate detection using shallow text
features. In Proceedings of the third ACM international conference on Web search and data mining, pages
441?450. ACM.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and Soumen Chakrabarti. 2009. Collective annotation of
wikipedia entities in web text. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge
discovery and data mining, pages 457?466. ACM.
Yogan Jaya Kumar and Naomie Salim. 2012. Automatic multi document summarization approaches. Journal of
Computer Science, 8(1).
Thomas Lin, Oren Etzioni, et al. 2010. Identifying functional relations in web text. In Proceedings of the
2010 Conference on Empirical Methods in Natural Language Processing, pages 1266?1276. Association for
Computational Linguistics.
Christopher D Manning, Prabhakar Raghavan, and Hinrich Sch?utze. 2008. Introduction to information retrieval,
volume 1. Cambridge University Press Cambridge.
Enrico Motta, Simon Buckingham Shum, and John Domingue. 2000. Ontology-driven document enrichment:
principles, tools and applications. International Journal of Human-Computer Studies, 52(6):1071?1109.
Vivi Nastase, Michael Strube, Benjamin B?orschinger, C?acilia Zirn, and Anas Elghafari. 2010. Wikinet: A very
large scale multi-lingual concept network. In LREC.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The pagerank citation ranking: Bring-
ing order to the web.
Patrick Pantel and Ariel Fuxman. 2011. Jigs and lures: Associating web queries with structured entities. In
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies-Volume 1, pages 83?92. Association for Computational Linguistics.
Alexandre Passant. 2007. Using ontologies to strengthen folksonomies and enrich information retrieval in we-
blogs. In Proceedings of International Conference on Weblogs and Social Media.
Alan Ritter, Oren Etzioni, et al. 2010. A latent dirichlet allocation method for selectional preferences. In Proceed-
ings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 424?434. Association
for Computational Linguistics.
Jean-Paul Sansonnet and Franc?ois Bouchet. 2010. Extraction of agent psychological behaviors from glosses of
wordnet personality adjectives. In Proc. of the 8th European Workshop on Multi-Agent Systems (EUMAS10).
Stefan Schoenmackers, Oren Etzioni, Daniel S Weld, and Jesse Davis. 2010. Learning first-order horn clauses
from web text. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,
pages 1088?1098. Association for Computational Linguistics.
926
Prithviraj Sen. 2012. Collective context-aware topic models for entity disambiguation. In Proceedings of the 21st
international conference on World Wide Web, pages 729?738. ACM.
Koun-Tem Sun, Yueh-Min Huang, and Ming-Chi Liu. 2011. A wordnet-based near-synonyms and similar-looking
word learning system. Educational Technology & Society, 14(1):121?134.
Ellen M Voorhees, Donna K Harman, et al. 2005. TREC: Experiment and evaluation in information retrieval,
volume 63. MIT press Cambridge.
Xudong Wang and Azman O Lim. 2008. Ieee 802.11 s wireless mesh networks: Framework and challenges. Ad
Hoc Networks, 6(6):970?984.
Fei Wang and Changshui Zhang. 2008. Label propagation through linear neighborhoods. Knowledge and Data
Engineering, IEEE Transactions on, 20(1):55?67.
Fei Wu and Daniel S Weld. 2010. Open information extraction using wikipedia. In Proceedings of the 48th An-
nual Meeting of the Association for Computational Linguistics, pages 118?127. Association for Computational
Linguistics.
Wei Zhang, Yan Chuan Sim, Jian Su, and Chew Lim Tan. 2011. Entity linking with effective acronym expansion,
instance selection and topic modeling. In Proceedings of the Twenty-Second international joint conference on
Artificial Intelligence-Volume Volume Three, pages 1909?1914. AAAI Press.
Zhicheng Zheng, Fangtao Li, Minlie Huang, and Xiaoyan Zhu. 2010. Learning to link entities with knowledge
base. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the
Association for Computational Linguistics, pages 483?491. Association for Computational Linguistics.
Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and Ji-Rong Wen. 2009. Statsnowball: a statistical approach to
extracting entity relationships. In Proceedings of the 18th international conference on World wide web, pages
101?110. ACM.
927
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1360?1369, Dublin, Ireland, August 23-29 2014.
Sentence Compression for Target-Polarity Word Collocation Extraction
Yanyan Zhao
1
, Wanxiang Che
2
, Honglei Guo
3
, Bing Qin
2
, Zhong Su
3
and Ting Liu
2?
1: Department of Media Technology and Art, Harbin Institute of Technology
2: Department of Computer Science and Technology, Harbin Institute of Technology
3: IBM Research-China
{yyzhao, bqin, tliu}@ir.hit.edu.cn, {guohl, suzhong}@cn.ibm.com
Abstract
Target-polarity word (T-P) collocation extraction, a basic sentiment analysis task, relies primarily
on syntactic features to identify the relationships between targets and polarity words. A major
problem of current research is that this task focuses on customer reviews, which are natural or
spontaneous, thus posing a challenge to syntactic parsers. We address this problem by proposing
a framework of adding a sentiment sentence compression (Sent Comp) step before performing
T-P collocation extraction. Sent Comp seeks to remove the unnecessary information for senti-
ment analysis, thereby compressing a complicated sentence into one that is shorter and easier to
parse. We apply a discriminative conditional random field model, with some special sentiment-
related features, in order to automatically compress sentiment sentences. Experiments show that
Sent Comp significantly improves the performance of T-P collocation extraction.
1 Introduction
Sentiment analysis deals with the computational treatment of opinion, sentiment and subjectivity in tex-
t (Pang and Lee, 2008), and has received considerable attention in recent years (Liu, 2012). Target-
Polarity word (T-P) collocation extraction, which aims to extract the collocation of a target and its cor-
responding polarity word in a sentiment sentence, is a basic task in sentiment analysis. For example,
in a sentiment sentence ????????????? (The camera has a novel appearance), ????
(appearance) is the target, and ???? (novel) is the polarity word that modifies ???? (appearance).
According, ???, ??? (?appearance, novel?) is the T-P collocation. Generally, T-P collocation is a
basic and complete sentiment unit, thus is very useful for many sentiment analysis applications.
Features derived from syntactic parse trees are particularly useful for T-P collocation extraction (Ab-
basi et al., 2008; Duric and Song, 2012). For example, the syntactic relation ?Adj
ATT
x Noun?, where the
ATT denotes an attributive syntactic relation, can be used as an important evidence to extract the T-P
collocation ???, ??? (?appearance, novel?) in the above sentiment sentence (Bloom et al., 2007;
Qiu et al., 2011; Xu et al., 2013).
However, one major problem of these approaches is the ?naturalness? of sentiment sentences, that is,
such sentences are more natural or spontaneous compared with normal sentences, thus posing a challenge
to syntactic parsers. Accordingly, many wrong syntactic features have been produced and these can
further result in the poor performance of T-P collocation extraction. Taking the sentence in Figure 1(a)
as an example, because the word ???? (fortunately) is so chatty,1 the parsing result is wrong. Thus,
are unable to extract the T-P collocation ???,?? (?keyboard, good?).
To solve the ?naturalness? problem, we can train a parser on sentiment sentences. Unfortunately, an-
notating such data will cost us a lot of time and effort. Instead, in this paper we produce a sentence
compression model, Sent Comp, which is designed especially to compress complicated sentiment sen-
tences into formal and easier to parse ones, further improving T-P collocation extraction.
?
Correspondence author: tliu@ir.hit.edu.cn
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
Note that, in Figure 1, the Chinese word ???? is chatty, although its translated English word ?fortunately? is not. In this
paper, we focus on processing the Chinese data.
1360
?? ?? ?
fortunately keyboard good
SBV
VOB
ROOT
(a) before compression
?? ?
keyboard good
SBV
ROOT
(b) after compression
Figure 1: Parse trees before and after compression.
This idea is motivated by the observation that, current syntactic parsers usually perform accurately
for short, simple and formal sentences, whereas error rates increase for longer, more complex or more
natural and spontaneous sentences (Finkel et al., 2008). Hence, the improvement in syntactic parsing
performance would have a ripple effect over T-P collocation extraction. For example, we can compress
the sentence in Figure 1(a) into a shortened sentence in Figure 1(b) by removing the chatty part ????
(fortunately). We can see that the shortened sentence is now well-formed (in Chinese) and its parse tree
is correct, making it easier to accurately extract T-P collocation.
Traditional sentence compression aims to obtain a shorter grammatical sentence by retaining impor-
tant information (usually important grammar structure) (Jing, 2000). For example, the sentence ?Overall,
this is a great camera.? can be compressed into ?This is a camera.? by removing the adverbial ?overall?
and the modifier ?great?. However, the modifier ?great? is a polarity word and very important for sen-
timent analysis. Therefore, Sent Comp model for sentiment sentences is different from the traditional
compression models, because it needs to retain the important sentiment information, such as the polarity
word. Hence, using Sent Comp, the above sentence can be compressed into ?This is a great camera.?
We regard Sent Comp as a sequence labeling task, which can be solved by a conditional random
fields (CRF) model. Instead of seeking the manual rules on parse trees for compression, as in other
studies (Vickrey and Koller, 2008), this method is an automatic procedure. In this work, we introduce
some sentiment-related features to retain the sentiment information for Sent Comp.
We apply Sent Comp as the first step in the T-P collocation extraction task. First, we compress the
sentiment sentences into easier to parse ones using Sent Comp, after which we employ the state-of-the-
art T-P collocation extraction approach on the compressed sentences. Experimental results on a Chinese
corpus of four product domains show the effectiveness of our approach.
The main contributions of this paper are as follows:
? We present a framework of using sentiment sentence compression preprocessing step to improve T-
P collocation extraction. This framework can better solve the ?over-natural? problem of sentiment
sentences, which poses a challenge to syntactic parsers. More importantly, the idea of this frame-
work can be applied to some other sentiment analysis tasks that rely heavily on syntactic results.
? We develop a simple yet effective compression model Sent Comp for sentiment sentences. To the
best of our knowledge, this is the first sentiment sentence compression model.
2 Background
For our baseline system, we used the state-of-the-art method to extract T-P collocations introduced by
Qiu et al. (2011), who proposed a double propagation method. This idea is based on the observation
that there is a natural syntactic relationship between polarity words and targets owing to the fact that
polarity words are used to modify targets. Furthermore, they also found that polarity words and targets
themselves have relations in some sentiment sentences (Qiu et al., 2011).
Based on this idea, in the double propagation method, we first used an initial seed polarity word lexicon
and the syntactic relations to extract the targets, which can fall into a new target lexicon. Then we used the
target lexicon and the same syntactic relations to extract the polarity words and to subsequently expand
the polarity word lexicon. This is an iterative procedure, because this method can iteratively produce the
new polarity words and targets back and forth using the syntactic relations.
1361
?? ?? ??
function very powerful
SBV
ADV
ROOT
(a) syntactic structure 1
?? ? ??
powerful function
ATT
RAD
ROOT
(b) syntactic structure 2
?? ? ?? ?
function is powerful
SBV VOB RAD
ROOT
(c) syntactic structure 3
?? ? ?? ?? ??
function and service very powerful
COO
LAD ADV
SBV ROOT
(d) syntactic structure 4
?? ?? ?? ? ??
function very powerful and complete
SBV COO
ADV LAD
ROOT
(e) syntactic structure 5
Figure 2: Example of syntactic structure rules for T-P collocation extraction. We showed five examples
from a total of nine syntactic structures. For each kind of syntactic structure (a) to (e), the target is
shown with a red box and the polarity word is shown with a green box. Syntactic structures (a) to (c)
describe the relations between targets and polarity words. Syntactic structure (d), which is extended
from (a), describes the relation between two targets. Syntactic structure (e), which is also extended from
(a), describes the relation between two polarity words. Similarly, we can summarize the other four rules
extended from (b) and (c) to describe the relations between two targets or two polarity words.
We can see that the syntactic relations are important for this method, and Qiu et al. (2011) proposed
eight rules to describe these relations. However, their work only focused on English sentences, whereas
the relations for Chinese sentences are different. Thus, in accordance with Chinese grammar, we pro-
posed nine syntactic structure rules between target t and polarity word p in a Chinese T-P collocation
?t, p?.
2
The three main rules are shown below and some example rules are illustrated in Figure 2.
Rule 1: t
SBV
x p, the ?subject-verb? structure between t and p, such as the example in Figure 2(a).
Rule 2: p
ATT
x t, that p is an attribute for t, such as the example in Figure 2(b).
Rule 3: t
SBV
x ?
VOB
y p, the ?subject-verb-object? structure between t and p, such as the example in
Figure 2(c). The ? denotes any word.
The other six rules can be extended from the three main rules by obtaining the coordination (COO)
relation of t or p, such as t
SBV
x ?
COO
y p in Figure 2(e). Note that the POS for t should be noun and for p
should be adjective.
As described above, the T-P collocation extraction relies heavily on syntactic parsers. Hence, if we
can use the Sent Comp model to improve the performance of parsers, the performance of T-P collocation
extraction can also be improved accordingly.
3 Sentiment Sentence Compression
3.1 Problem Analysis
First, we conducted an error analysis for the results of current T-P collocation extraction, from which we
observed that the ?naturalness? of sentiment sentences is one of the main problems. For examples:
? Chatty form: some sentiment sentences are so chatty, that they bring many difficulties to the parser.
For example, in the sentence ??????? (fortunately the keyboard is good) shown in Figure 1,
the usage of the chatty word ???? (fortunately) affects the accuracy of the syntactic parser.
2
A Chinese natural language processing toolkit, Language Technology Platform (LTP) (Che et al., 2010), was used as our
dependency parser. More information about the syntactic relations can be found in their paper. The state-of-the-art graph-based
dependency parsing model, in the toolkit, was trained on Chinese Dependency Treebank 1.0 (LDC2012T05).
1362
?? ?? ?
besides photo good
ADV
POB
ROOT
comp
?? ?
photo good
SBV
ROOT
(a) parse tree 1 before and after compression
?? ? ? ? ?? ??screen for people feel good
SBV
ATT
POBRAD SBV
ROOT
comp
?? ??screen good
SBV
ROOT
(b) parse tree 2 before and after compression
Figure 3: ?Naturalness? problem of sentiment sentences.
? Conjunction word usage: conjunction words are often used in sentiment sentences to show the dis-
course relations between two sentences. However, there are so many conjunction words in Chinese,
some of which can cause errors among parsers. For example, in Figure 3(a), the parse tree of sen-
tence ???????? (besides the photo is good) is wrong because of the usage of the conjunction
word ???? (besides).
? Feeling words/phrase usage: in sentiment sentences, people often use some feeling words/phrase,
such as ??????? (feel like) in Figure 3(b) or ????? (smell like). Given that the current
syntactic parser cannot handle the feeling words/phrases very well, the T-P collocation ???, ?
?? (?screen, good?) in Figure 3(b) cannot be extracted correctly.
To address the ?naturalness? problem, we compressed the sentiment sentences into one that are shorter
and easier to parse. Similar to the examples in Figure 1 and 3, the compressed sentences can be easily
and correctly parsed. The above analysis can be used as the criteria to guide us in compressing sentiment
sentences when annotating, and can also help us exploit more useful features for automatic sentiment
sentence compression.
3.2 Task Definition
We focus on studying the methods for extractive sentence compression.
3
Formally, extractive sentence
compression aims to shorten a sentence x = x
1
? ? ?x
n
into a substring y = y
1
? ? ? y
m
, where y
i
?
{x
1
, ? ? ? , x
n
}, m ? n.
In this paper, similar to Nomoto (2007), we also treated the sentence compression as a sequence
labeling task which can be solved by a CRF model. We assigned a compression tag t
i
to each word x
i
in
an original sentence x, where t
i
= N if x
i
? y, else t
i
= Y.
A first-order linear-chain CRF is used which defines the following conditional probability:
P (t|x) =
1
Z(x)
?
i
M
i
(t
i
, t
i?1
|x) (1)
where x and t are the input and output sequences respectively, Z(x) is the partition function, and M
i
is
the clique potential for edge clique i. Here, we used the CRFsuite toolkit to train the CRF model.
4
3.3 Features
The features for Sent Comp are listed in Table 1. Aside from the basic word (w), POS tag (t) and
their combination context features (01 ? 04), we introduced some sentiment-related features (05 ? 06)
and latent semantic features (07 ? 08) to better handle sentiment analysis data and generalize word
features. Then we added the syntactic parse features (09), which are commonly used in traditional
sentence compression task.
One sentiment-related feature (feeling(?)) indicates whether a word is a feeling word, which is inspired
by the naturalness problem in Figure 3(b). As discussed above, the current parser often produces wrong
parse trees because of these feeling words. Therefore, the feeling words tend to be removed from a
3
Generally, there are two kinds of sentence compression methods: extractive method and abstractive method. Because
abstractive method needs more resource and is more complicated, in this paper, we only focus on extractive approach.
4
www.chokkan.org/software/crfsuite/
1363
Basic Features
01: w
i+k
,?1 ? k ? 1
02: w
i+k?1
? w
i+k
, 0 ? k ? 1
03: t
i+k
,?2 ? k ? 2
04: t
i+k?1
? t
i+k
,?1 ? k ? 2
Sentiment-related Features
05: feeling(w
i
)
06: polarity(w
i
)
Latent Semantic Features
07: suffix(w
i
) if t(w
i
) == n else prefix(w
i
)
08: cluster(w
i
)
Syntactic Features
09: dependency(w
i
)
Table 1: Features of sentiment sentence compression
sentiment sentence for Sent Comp. We can obtain a feeling word lexicon from HowNet,
5
a popular
Chinese sentiment thesaurus, where a feeling word is defined by DEF={perception|??} tag. Finally,
we collected 38 feeling words, such as?? (realize),?? (find), and?? (think).
The other sentiment-related feature (polarity(?)) indicates whether a word is a polarity word. One
of the main differences between a sentiment sentence and a formal sentence is that the former often
contains polarity words. In contrast to the features of feeling(?), polarity words (e.g., ?great? in the
sentence ?Overall, this is a great camera?) tend to be retained, because they are important and special
to sentiment analysis. In this paper, we treat polarity words as important features, considering that they
are often tagged as modifiers and are easily removed by common sentence compression methods. We
can obtain the polarity feature (polarity(?)) from a polarity lexicon, which can also be obtained from
HowNet.
To generalize the words in sentiment sentences, we proposed two kinds of semantic features. The
first one is a suffix or prefix character feature (prefix(?) or suffix(?)). In contrast to English, the suffix
(for noun) or prefix (for non noun) characters of a Chinese word often carry that word?s core semantic
information. For example, ??? (bicycle), ?? (car), and ?? (train) are all various kinds of ?
(vehicle), which is also the suffix of the three words. Given that all of them may become targets, they
tend to be retained in compressed sentences. The verbs, ?? and ??, can be denoted by their prefix
feel (?), and can be removed from original sentences because they are feeling words.
We used word clustering features (cluster(?)) as the other latent semantic feature to further improve
the generalization over common words. Word clustering features contain some semantic information
and have been successfully used in several natural language processing tasks, including NER (Miller et
al., 2004; Che et al., 2013) and dependency parsing (Koo et al., 2008). For instance, the words ??
and ?? (appearance) belong to the same word cluster, although they have a different suffix or prefix.
Both words are important for T-P collocation extraction and should be retained. We used the Brown
word clustering algorithm (Brown et al., 1992) to obtain the word clusters (Liang, 2005). Raw texts were
obtained from the fifth edition of Chinese Gigaword (LDC2011T13).
Finally, similar to McDonald (2006), we also added the dependency relation between a word and its
parent as the syntactic features. Intuitively, the dependency relations are helpful in carrying out sentence
compression. For example, the ROOT relation typically indicates that the word should not be removed
because it is the main verb of a sentence.
4 Experiments
4.1 Experimental Setup
4.1.1 Corpus
We conducted the experiments on a Chinese corpus of four product domains, which came from the Task3
of the Chinese Opinion Analysis Evaluation (COAE) (Zhao et al., 2008).
6
Table 2 describes the corpus,
5
www.keenage.com
6
www.ir-china.org.cn/coae2008.html
1364
Domain # reviews # sentences # collocations
Camera 138 1,249 1,335
Car 161 1,172 1,312
Notebook 56 623 674
Phone 123 1,350 1,479
All 478 4,394 4,800
Table 2: Corpus statistics for the Chinese corpus of four product domains.
where 4,394 sentiment sentences containing 4,800 T-P collocations are manually found and annotated
from 478 reviews.
We ask annotators to manually compress all the sentiment sentences. Specifically, the annotators
removed some words from a sentiment sentence according to two criteria stated as follows: (1) removing
the word should not change the essential content of the sentence, and (2) removing the word should
not change the sentiment orientation of the sentence. In order to assess the quality of the annotation,
we sampled 500 sentences from this corpus and asked two annotators to perform the annotation. The
resulting word-based Cohen?s kappa (Cohen, 1960) (i.e., a measure of inter-annotator agreement ranging
from zero to one) of 0.7 indicated a good strength of agreement.
4.1.2 Evaluation
Generally, compressions are evaluated using three criteria (McDonald, 2006), namely, grammaticality,
importance, and compression rate. Obviously, the former two are difficult to evaluate objectively. Previ-
ous works used human judgment, which entails a difficult and expensive process. In this paper, similar to
a common sequence labeling task, we simply used the F-score metric of removed words to roughly eval-
uate the performance of sentiment sentence compression. Of course, the final effectiveness of sentence
compression model can be reviewed by the derived T-P collocation extraction task.
For T-P collocation extraction, we applied the traditional P, R and F-score for the final evaluations.
Specially, a fuzzy matching evaluation is adopted for the T-P collocation extraction. That is to say,
given an extracted T-P collocation ?t, p?, whose standard result is ?t
s
, p
s
?, if t is the substring of t
s
, and
meanwhile p is the substring of p
s
, we consider the extracted ?t, p? is a correct T-P collocation.
4.2 Sentiment Sentence Compression Results
Features P(%) R(%) F(%)
Basic (01 ? 04) 76.4 57.4 65.5
+ feeling (05) 75.9 57.6 65.5
+ polarity (06) 76.6 57.6 65.7
+ suffix or prefix (07) 78.4 56.9 66.0
+ cluster (08) 74.9 58.9 65.9
+ dependency (09) 75.3 57.2 65.0
All (01 ? 08) 77.3 59.1 67.0
All - feeling (05) 77.1 58.9 66.8
Table 3: The results of sentiment sentence compression with different features.
Results of Sent Comp with different features are shown in Table 3. All results are reported using five-
fold cross validation. We can see that the performance is improved when we added feeling
7
and polarity
features (05 ? 06) respectively, indicating that the sentiment-related features are useful for sentiment
sentence compression. In addition, the latent semantic features (07 ? 08) are also helpful, especially the
suffix or prefix features, which show better performance than the four other kinds of features.
Nonetheless, the dependency features (09) have a negative on compression performance due to the
specificity of compression for sentiment sentences. That is because the lower dependency parsing per-
formance on sentiment sentences introduces many wrong dependency relations, which counteract the
7
In Table 3, although the performance of adding feeling is comparative to the basic system (Basic (01-04)), the system
without feeling (All - feeling (05), the last line) is worse than the system using all the features (All (01-08)). This can illustrate
the effectiveness of the feeling feature.
1365
Domain Method P(%) R(%) F(%)
no Comp 74.7 58.4 65.6
Camera manual Comp 83.4 62.7 71.6
auto Comp 80.4 62.1 70.1
no Comp 68.2 53.1 59.7
Car manual Comp 76.3 57.7 65.7
auto Comp 72.3 56.1 63.2
no Comp 74.1 56.8 64.3
Notebook manual Comp 82.7 64.5 72.5
auto Comp 79.7 62.8 70.2
no Comp 77.3 60.9 68.1
Phone manual Comp 82.7 65.7 73.2
auto Comp 80.3 63.3 70.8
no Comp 73.7 57.5 64.6
All manual Comp 81.2 62.5 70.6
auto Comp 78.1 60.9 68.4
Table 4: Results on T-P collocation extraction for four product domains.
contribution of the dependency relation features. This is also the reason why we need to compress sen-
timent sentences as the first step for T-P collocation extraction. Finally, when we combine all of useful
features (01 ? 08), the performance achieves the highest score.
It is worth noting that sentiment sentence compression is a new task proposed in this paper. For
simplicity, this paper aims to attempt a simple yet effective sentiment sentence compression model. We
will polish the Sent Comp model in the future work.
4.3 Sent Comp for T-P Collocation Extraction
We designed three comparative systems to demonstrate the effectiveness of Sent Comp for T-P collo-
cation extraction. Note that, Sent Comp is the first step to process the corpus before T-P collocation
extraction. The method for T-P collocation extraction was based on the state-of-the-art method proposed
by Qiu et al. (2011) as described in Section 2.
no Comp - This refers to the system that only uses the T-P collocation extraction method and does not
perform sentence compression as the first step.
manual Comp - This system manually compresses the corpus into a new one as the first step, and then
applies the T-P collocation extraction method on the new compressed corpus.
auto Comp - This system uses Sent Comp as the first step to automatically compress the corpus into a
new one, and then applies the T-P collocation extraction method on the new corpus.
From the descriptions above, we can draw a conclusion that the performance of manual Comp can be
considered as the upper bound for the sentiment sentence compression based T-P collocation extraction
task.
Table 4 shows the experimental results of the three systems on T-P collocation extraction for four prod-
uct domains. Here, manual Comp can significantly (p < 0.01) improved the F-score by approximately
6%,
8
compared with no Comp. This illustrates that the idea of sentiment sentence compression is use-
ful for T-P collocation extraction. Specifically, the proposed method can transform some over-natural
sentences into normal ones, further influencing their final syntactic parsers. Evidently, because the T-P
collocation extraction relies heavily on syntactic features, the more correct syntactic parse trees derived
from the compressed sentences can help to increase the performance of this task.
Compared with no Comp, the auto Comp system also yielded a significantly better results (p < 0.01)
that indicated an improvement of 3.8% in the F-score, despite the fact that the automatic sentence com-
pression model Sent Comp may wrongly compress some sentences. This demonstrates the usefulness
of sentiment sentence compression step in the T-P collocation extraction task and further proves the
effectiveness of our proposed model.
8
We use paired bootstrap resampling significance test (Efron and Tibshirani, 1993).
1366
Moreover, we can observe that the idea of sentence compression and our Sent Comp are useful for
all the four product domains on T-P collocation extraction task, indicating that Sent Comp is domain
adaptive. However, we can find a small gap between auto Comp and manual Comp, which indicates
that the Sent Comp model can still be improved further. In the future, we will explore more effective
sentence compression algorithms to bridge the gap between the two systems.
5 Related Works
5.1 Sentiment Analysis
T-P collocation extraction is a basic task in sentiment analysis. In order to solve this task, most methods
focused on identifying relationships between targets and polarity words. In early studies, researcher-
s recognized the target first, and then chose its polarity word within a window of size k (Hu and Liu,
2004). However, considering that this kind of method is too heuristic, the performance proved to be very
limited. To tackle this problem, many researchers found syntactic patterns that can better describe the
relationships between targets and polarity words. For example, Bloom et al. (2007) constructed a link-
age specification lexicon containing 31 patterns, while Qiu et al. (2011) proposed a double propagation
method that introduced eight heuristic syntactic patterns to extract the collocations. Xu et al. (2013) used
the syntactic patterns to extract the collocation candidates in their two-stage framework.
Based on the above, we can conclude that syntactic features are very important for T-P collocation
extraction. However, the ?naturalness? problem can still seriously affect the performance of syntactic
parser. Once our sentiment sentence compression method can improve the quality of parsing, the perfor-
mance of T-P collocation extraction task can be improved as well. Note that, to date, there is no previous
work using a sentence compression model to improve this task.
5.2 Sentence Compression
Sentence compression is a paraphrasing task aimed at generating sentences shorter than the given ones,
while preserving the essential content (Jing, 2000). There are many applications that can benefit from
a robust compression system, such as summarization systems (Li et al., 2013), semantic role label-
ing (Vickrey and Koller, 2008), relation extraction (Miwa et al., 2010) and so on.
Commonly used to compress sentences, tree-based approaches (Knight and Marcu, 2002; Turner and
Charniak, 2005; Galley and McKeown, 2007; Cohn and Lapata, 2009; Galanis and Androutsopoulos,
2010; Woodsend and Lapata, 2011; Thadani and McKeown, 2013) compress a sentence by editing the
syntactic tree of the original sentence. However, the automatic parsing results may not be correct; thus,
the compressed tree (after removing constituents from a bad parse) may not produce a good compressed
sentence. McDonald (2006), Nomoto (2007), and Clarke and Lapata (2008) tried to solve the problem
by using discriminative models.
Aside from above extractive sentence compression approaches, there is another research line, namely,
abstractive approach, which compresses an original sentence by reordering, substituting, and inserting,
as well as removing (Cohn and Lapata, 2013). This method needs more resource and is more complicat-
ed. Therefore, in this paper, we only focus on extractive approach.
At present, the current sentence compression methods all focus on formal sentences, and few meth-
ods are being proposed to study sentiment sentences. As discussed in the above sections, the current
compression models cannot be directly utilized to T-P collocation extraction owing to the specificity of
sentiment sentences. Therefore, a new compression model for sentiment sentences should be established.
6 Conclusion and Future Work
In this work, we presented a framework that adopted a CRF based sentiment sentence compression mod-
el Sent Comp, as a preprocessing step, to improve the T-P collocation extraction task. Different from
the existing sentence compression models used for formal sentences, Sent Comp incorporated some
sentiment-related features to retain the sentiment information. Experimental results showed that the sys-
tem with the sentence compression step performed better than that without this step, thus demonstrating
the effectiveness of the framework and the compression model Sent Comp.
1367
Generally, the idea of this framework maybe useful for many sentiment analysis tasks that rely heavily
on syntactic results. Thus in the future, we will try to apply the Sent Comp model for these tasks. Besides,
the simplicity and effectiveness of this framework motivates us to pursue the study further. For example,
we will polish the Sent Comp model by exploring more sentiment-related features and exploring other
types of compression models.
Acknowledgments
We thank the anonymous reviewers for their helpful comments. This work was supported by National
Natural Science Foundation of China (NSFC) via grant 61300113, 61133012 and 61273321, the Ministry
of Education Research of Social Sciences Youth funded projects via grant 12YJCZH304, the Fundamen-
tal Research Funds for the Central Universities via grant No.HIT.NSRIF.2013090 and IBM Research-
China Joint Research Project.
References
Ahmed Abbasi, Hsinchun Chen, and Arab Salem. 2008. Sentiment analysis in multiple languages: Feature
selection for opinion classification in web forums. ACM Trans. Inf. Syst., 26(3):12:1?12:34, June.
Kenneth Bloom, Navendu Garg, and Shlomo Argamon. 2007. Extracting appraisal expressions. In HLT-NAACL
2007, pages 308?315.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based
n-gram models of natural language. Comput. Linguist., 18(4):467?479, December.
Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp: A chinese language technology platform. In Coling 2010:
Demonstrations, pages 13?16, Beijing, China, August. Coling 2010 Organizing Committee.
Wanxiang Che, Mengqiu Wang, Christopher D. Manning, and Ting Liu. 2013. Named entity recognition with
bilingual constraints. In Proceedings of the 2013 Conference of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Language Technologies, pages 52?62, Atlanta, Georgia, June.
Association for Computational Linguistics.
James Clarke and Mirella Lapata. 2008. Global inference for sentence compression: An integer linear program-
ming approach. J. Artif. Intell. Res. (JAIR), 31:399?429.
Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement,
20(1):37 ? 46.
Trevor Cohn and Mirella Lapata. 2009. Sentence compression as tree transduction. Journal of Artificial Intelli-
gence Research, 34:637?674.
Trevor Cohn and Mirella Lapata. 2013. An abstractive approach to sentence compression. ACM Transactions on
Intelligent Systems and Technology, 4(3):1?35.
Adnan Duric and Fei Song. 2012. Feature selection for sentiment analysis based on content and syntax models.
Decis. Support Syst., 53(4):704?711, November.
B. Efron and R. J. Tibshirani. 1993. An Introduction to the Bootstrap. Chapman & Hall, New York.
Jenny Rose Finkel, Alex Kleeman, and Christopher D. Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL-08: HLT, pages 959?967, Columbus, Ohio, June. Association for
Computational Linguistics.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An extractive supervised two-stage method for sentence
compression. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter
of the Association for Computational Linguistics, pages 885?893, Los Angeles, California, June. Association
for Computational Linguistics.
Michel Galley and Kathleen McKeown. 2007. Lexicalized Markov grammars for sentence compression. In
Human Language Technologies 2007: The Conference of the North American Chapter of the Association for
Computational Linguistics; Proceedings of the Main Conference, pages 180?187, Rochester, New York, April.
Association for Computational Linguistics.
1368
Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of KDD-2004,
pages 168?177.
Hongyan Jing. 2000. Sentence reduction for automatic text summarization. In IN PROCEEDINGS OF THE 6TH
APPLIED NATURAL LANGUAGE PROCESSING CONFERENCE, pages 310?315.
Kevin Knight and Daniel Marcu. 2002. Summarization beyond sentence extraction: A probabilistic approach to
sentence compression. Artif. Intell., 139(1):91?107, July.
Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL-08: HLT, pages 595?603, Columbus, Ohio, June. Association for Computational Linguistics.
Chen Li, Fei Liu, Fuliang Weng, and Yang Liu. 2013. Document summarization via guided sentence compression.
In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 490?500,
Seattle, Washington, USA, October. Association for Computational Linguistics.
Percy Liang. 2005. Semi-supervised learning for natural language. Master?s thesis, MIT.
Bing Liu. 2012. Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies.
Morgan & Claypool Publishers.
Ryan McDonald. 2006. Discriminative sentence compression with soft syntactic evidence. In In Proc. EACL.
Scott Miller, Jethran Guinness, and Alex Zamanian. 2004. Name tagging with word clusters and discriminative
training. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings,
pages 337?342, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics.
Makoto Miwa, Rune Saetre, Yusuke Miyao, and Jun?ichi Tsujii. 2010. Entity-focused sentence simplification
for relation extraction. In Proceedings of the 23rd International Conference on Computational Linguistics
(COLING 2010), pages 788?796.
Tadashi Nomoto. 2007. Discriminative sentence compression with conditional random fields. Information Pro-
cessing and Management, 43(6):1571?1587, November.
Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1?135,
January.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2011. Opinion word expansion and target extraction through
double propagation. Computational Linguistics, 37(1):9?27.
Kapil Thadani and Kathleen McKeown. 2013. Sentence compression with joint structural inference. In Pro-
ceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 65?74, Sofia,
Bulgaria, August. Association for Computational Linguistics.
Jenine Turner and Eugene Charniak. 2005. Supervised and unsupervised learning for sentence compression. In
Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ?05, pages 290?
297, Stroudsburg, PA, USA. Association for Computational Linguistics.
David Vickrey and Daphne Koller. 2008. Sentence simplification for semantic role labeling. In ACL, pages
344?352. The Association for Computer Linguistics.
Kristian Woodsend and Mirella Lapata. 2011. Learning to simplify sentences with quasi-synchronous grammar
and integer programming. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language
Processing, pages 409?420, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and Jun Zhao. 2013. Mining opinion words and opinion targets
in a two-stage framework. In Proceedings of the 51st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1764?1773, Sofia, Bulgaria, August. Association for Computational
Linguistics.
Jun Zhao, Hongbo Xu, Xuanjing Huang, Songbo Tan, Kang Liu, and Qi Zhang. 2008. Overview of chinese pinion
analysis evaluation 2008. In The First Chinese Opinion Analysis Evaluation (COAE) 2008.
1369
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 160?170, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Collocation Polarity Disambiguation Using Web-based Pseudo Contexts
Yanyan Zhao, Bing Qin and Ting Liu?
Harbin Institute of Technology, Harbin, China
{yyzhao, bqin, tliu}@ir.hit.edu.cn
Abstract
This paper focuses on the task of colloca-
tion polarity disambiguation. The collocation
refers to a binary tuple of a polarity word and
a target (such as ?long, battery life? or ?long,
startup?), in which the sentiment orientation of
the polarity word (?long?) changes along with
different targets (?battery life? or ?startup?).
To disambiguate a collocation?s polarity, pre-
vious work always turned to investigate the
polarities of its surrounding contexts, and then
assigned the majority polarity to the collo-
cation. However, these contexts are limited,
thus the resulting polarity is insufficient to be
reliable. We therefore propose an unsuper-
vised three-component framework to expand
some pseudo contexts from web, to help dis-
ambiguate a collocation?s polarity.Without us-
ing any additional labeled data, experiments
show that our method is effective.
1 Introduction
In recent years, more attention has been paid to
sentiment analysis as it has been widely used in
various natural language processing applications,
such as question answering (Wiebe et al2003;
Yu and Hatzivassiloglou, 2003), information extrac-
tion (Riloff et al2005) and opinion-oriented sum-
marization (Hu and Liu, 2004; Liu et al2005).
Meanwhile, it also brings us lots of interesting and
challenging research topics, such as subjectivity
analysis (Riloff and Wiebe, 2003), sentiment clas-
sification (Pang et al2002; Kim and Hovy, 2005;
?Correspondence author: tliu@ir.hit.edu.cn
Wilson et al2009; He et al2011), opinion re-
trieval (Zhang et al2007; Zhang and Ye, 2008;
Li et al2010) and so on.
One fundamental task for sentiment analysis is
to determine the semantic orientations of words.
For example, the word ?beautiful? is positive, while
?ugly? is negative. Many researchers have devel-
oped several algorithms for this purpose and gener-
ated large static lexicons of words marked with prior
polarities (Hatzivassiloglou and McKeown, 1997;
Turney et al2003; Esuli, 2008; Mohammad et al
2009; Velikovich et al2010). However, there exist
some polarity-ambiguous words, which can dynam-
ically reflect different polarities along with different
contexts. A typical polarity-ambiguous word ???
(?long? in English) is shown with two example sen-
tences as follows.
1. ????[????]t?[?]p?(Positive)
Translated as: The [battery life]t of this camera
is [long]p. (Positive)
2. ????[????]t?[?]p?(Negative)
Translated as: This camera has [long]p
[startup]t. (Negative)
The phrases marked with p superscript are the
polarity-ambiguous words, and the phrases marked
with t superscript are targets modified by the polar-
ity words. In the above two sentences, the sentiment
orientation of the polarity word ??? (?long? in En-
glish) changes along with different targets. When
modifying the target ?????? (?battery life? in
English), its polarity is positive; and when modify-
ing ?????? (?startup? in English), its polarity is
160
negative. In this paper, we especially define the col-
location as a binary tuple of the polarity-ambiguous
word and its modified target, such as ??,?????
(?long, battery life? in English) or ??,?????
(?long, startup? in English). This paper concentrates
on the task of collocation polarity disambiguation.
This is an important task as the problem of
polarity-ambiguity is frequent. We analyze 4,861
common binary tuples of polarity words and their
modified targets from 478 reviews1, and find that
over 20% of them are the collocations defined in this
paper. Therefore, the task of collocation polarity dis-
ambiguation is worthy of study.
For a sentence s containing such a collocation c,
since the in-sentence features are always ambiguous,
it is difficult to disambiguate the polarity of c by us-
ing them. Thus some previous work turned to in-
vestigate its surrounding contexts? polarities (such
as the sentences before or after s), and then assigned
the majority polarity to the collocation c (Hatzivas-
siloglou and McKeown, 1997; Hu and Liu, 2004;
Kanayama and Nasukawa, 2006). However, since
the amount of contexts from the original review is
very limited, the final resulting polarity for the col-
location c is insufficient to be reliable.
Fortunately, most collocations may appear multi-
ple times, in different forms, both within the same
review and within topically-related reviews. Thus
for a collocation, we can collect large amounts of
contexts from other reviews to improve its polarity
disambiguation. These expanded contexts are called
pseudo contexts in this paper. Some previous work
used the similar methods. For example, Ding (Ding
et al2008) expanded some pseudo contexts from
a topically-related review set. But since the review
set is limited, the expanded contexts are still lim-
ited and unreliable. In order to overcome this prob-
lem, we propose an unsupervised three-component
framework to expand more pseudo contexts from
web for the collocation polarity disambiguation.
Without using any labeled data, experiments on
a Chinese data set from four product domains show
that the three-component framework is feasible and
the web-based pseudo contexts are useful for the
collocation polarity disambiguation. Compared to
other previous work, our method achieves an F1
1The dataset will be introduced in Section 4.1 in detail.
score of 72.02%, which is about 15% higher.
The remainder of this paper is organized as fol-
lows. Section 2 introduces the related work. Section
3 shows the proposed approach including three in-
dependent components. Section 4 and 5 presents the
experiments and results. Finally we conclude this
paper in Section 6.
2 Related Work
The key of the collocation polarity disambigua-
tion task is to recognize the polarity word?s sen-
timent orientation of a collocation. There are ba-
sically two types of approaches for word polar-
ity recognition: corpus-based and dictionary-based
approaches. Corpus-based approaches find co-
occurrence patterns of words in the large corpora
to determine the word sentiments, such as the work
in (Hatzivassiloglou and McKeown, 1997; Wiebe,
2000; Riloff and Wiebe, 2003; Turney et al2003;
Kaji and Kitsuregawa, 2007; Velikovich et al
2010). On the other hand, dictionary-based ap-
proaches use synonyms and antonyms in WordNet
to determine word sentiments based on a set of seed
polarity words. Such approaches are studied in (Kim
and Hovy, 2006; Esuli and Sebastiani, 2005; Kamps
et al2004). Overall, most of the above approaches
aim to generate a large static polarity word lexicon
marked with prior polarities.
However, it is not sensible to predict a word?s sen-
timent orientation without considering its context.
In fact, even in the same domain, a word may indi-
cate different polarities depending on what targets it
is applied to, especially for the polarity-ambiguous
words, such as ??? (?long? in English) shown in
Section 1. Based on these, we need to consider both
the polarity words and their modified targets, i.e.,
the collocations mentioned in this paper, rather than
only the polarity words.
To date, the task in this paper is similar with
much previous work. Some researchers exploited
the features of the sentences containing colloca-
tions to help disambiguate the polarity of the
polarity-ambiguous word. For example, Hatzivas-
siloglou (Hatzivassiloglou and McKeown, 1997)
and Kanayama (Kanayama and Nasukawa, 2006)
used conjunction rules to solve this problem from
large domain corpora. Suzuki (Suzuki et al2006)
161
took into account many contextual information of
the word within the sentence, such as exclamation
words, emoticons and so on. However, the experi-
mental results show that these in-sentence features
are not rich enough.
Instead of considering the current sentence alone,
some researchers exploited external information and
evidences in other sentences or other reviews to infer
the collocation?s polarity. For a collocation, Hu (Hu
and Liu, 2004) analyzed its surrounding sentences?
polarities to disambiguate its polarity. Ding (Ding
et al2008) proposed a holistic lexicon-based ap-
proach of using global information to solve this
problem. However, the contexts or evidences from
these two methods are limited and unreliable. Ex-
cept for the above unsupervised methods, some re-
searchers (Wilson et al2005; Wilson et al2009)
proposed supervised methods for this task, which
need large annotated corpora.
In addition, many related works tried to learn
word polarity in a specific domain, but ignored the
problem that even the same word in the same do-
main may indicate different polarities (Jijkoun et al
2010; Bollegala et al2011). And some work (Lu et
al., 2011) combined difference sources of informa-
tion, especially the lexicons and heuristic rules for
this task, but ignored the important role of the con-
text. Besides, there exists some research focusing
on word sense subjectivity disambiguation, which
aims to classify a word sense into subjective or ob-
jective (Wiebe and Mihalcea, 2006; Su and Markert,
2009). Obviously, this task is different from ours.
3 The Proposed Approach
3.1 Overview
The motivation of our approach is to make full use of
web sources to collect more useful pseudo contexts
for a collocation, whose original contexts are lim-
ited or unreliable. The framework of our approach
is illustrated in Figure 1.
In order to disambiguate a collocation?s polarity,
three components are carried out:
1. Query Expansion and Pseudo Context Ac-
quisition: This paper uses the collocation as query.
For a collocation, three heuristic query expansion
strategies are used to generate more flexible queries,
which have the same or completely opposite polar-
A Chinese 
collocation 
in a review
Query 
expansion
Searching
Web 
snippets
Original context 
acquisition
Sentiment 
analysis
Pseudo context 
acquisition
Sentiment 
analysis
Combination Pos/Neg
start
end
Figure 1: The framework of our approach.
ity with this collocation. Searching these queries in
the domain-related websites, lots of snippets can be
acquired. Then we can extract the pseudo contexts
from these snippets.
2. Sentiment Analysis: For both original con-
texts and the expanded pseudo contexts from web, a
simple lexicon-based sentiment computing method
is used to recognize each context?s polarity.
3. Combination: Two strategies are designed to
integrate the polarities of the original and pseudo
contexts, under the assumption that these two kinds
of contexts can be complementary to each other.
It is worth noting that this three-component
framework is flexible and we can try to design dif-
ferent strategies for each component. Next sections
will give a simple example strategy for each compo-
nent to show its feasibility and effectiveness.
3.2 Query Expansion and Pseudo Context
Acquisition
3.2.1 Why Expanding Queries
For a collocation, such as ??,????? (?long,
battery life? in English), the most intuitive query
used for searching is constructed by the form of ?tar-
get + polarity word?, i.e., ????? (battery
life long in English). Even if we search this query
alone, a great many web snippets covering the po-
larity word and target will be retrieved. But why do
we still need to expand the queries?
In fact, for a collocation, though the amount of the
retrieved snippets is large, lots of them cannot pro-
vide accurate pseudo contexts. The reason is that the
162
polarity words in some snippets do not really mod-
ify the targets, such as in the sentence ?The battery
life is short, and finds few buyers for a long time.?
There exist no modifying relation between ?battery
life? and ?long?.
In order to filter these meaningless snippets, we
can simply search with a new query ???????
by surrounding it with quotes (noted as Strategy0).
However, this can drastically decline the amount of
snippets. In addition, as the new query is short, in
many retrieved snippets, there also exist no modify-
ing relations between the polarity words and targets.
As a result, if we just use this query strategy, the ex-
panded pseudo contexts are limited and cannot yield
ideal performance.
Therefore, we need to design some effective
query expansion strategies to ensure that (1) the po-
larity words do modify the targets in the retrieved
web snippets, and (2) the snippets are more enough.
3.2.2 Query Expansion Strategy
We first investigate the modifying relations be-
tween polarity words and the targets, and then con-
struct effective queries.
Observed from previous work (Bloom et al
2007; Kobayashi et al2004; Popescu and Etzioni,
2005), there are two kinds of common relations be-
tween the polarity words and their targets. One is
the ?subject-copula-predicate? relation, such as the
relationship between ?long? and ?battery life? in the
sentence ?The battery life of this camera is long?.
The other is the ?attribute-head? relation, such as
the relationship between them in the sentence ?This
camera has long battery life?.
As a result, three heuristic query expansion strate-
gies are adopted to construct efficient queries for
searching. Take the collocation ??,?????
(?long, battery life? in English) as an example, the
strategies are described as follows.
Strategy1: target + modifier + polarity word:
Such as the query ???????? or ?????
???? (?the battery life is very long? in English).
Different from Strategy0, this strategy adds a mod-
ifier element. It refers to the words that are used to
change the degree of a polarity word, such as ??? or
???? (?very? in English). Due to the usage of the
modifiers, the queries from this strategy can satisfy
the ?subject-copula-predicate? relation.
Strategy2: modifier + polarity word + ?+ tar-
get: Such as the query ????????? or ??
???????? (?very long battery life? in En-
glish). This strategy also uses modifiers to modify
polarity words, and the generated queries can satisfy
the ?attribute-head? relation.
Strategy3: negation word + polarity word +?+
target: Such as the query ????????? or ??
???????? (?not long battery life? in En-
glish). This strategy uses negation words to modify
the polarity words. And the queries from this strat-
egy can satisfy the ?attribute-head? relation. The
only difference is that the polarity of this kind of
queries is opposite to that of the collocation.
Similar to the queries from Strategy0, the queries
generated by Strategy1?3 are all searched with
quotes. In addition, note that the modifier and the
negation word are taken from Modifier Lexicon and
Negation Lexicon introduced in Table 2.
3.2.3 Pseudo Context Acquisition
For each query from Strategy0?3, we search it in
some websites to acquire the related snippets. If we
directly search it using Google without site restric-
tions, it does return all the snippets containing the
query, but lots of them are non-reviews. Further, the
pseudo contexts generated by these non-reviews are
useless or even harmful. To overcome this problem,
the advanced search of Google is used to search the
query within the forum sites of the product domain.
We can flexibly choose several popular forum sites
for each domain. The URLs of the forum sites used
in this paper are listed in Table 1.
Formally, given a collocation c, the expanded
pseudo contexts Conx(c) can be obtained using the
following function:
Conx(c) =
?3
i=0 f(Queryi)
=
?3
i=0
?n
j=1 f(queryij)
(1)
Here, Queryi is the query set generated by the ith
query expansion strategy; queryij is the jth query
generated by the ith strategy. And the parameter n is
the total number of queries from the ith query expan-
sion strategy. From this function, we can collect the
contexts of c by summing up all the pseudo contexts
from every queryij .
163
Domain URL
Camera
http://www.qqdc.com.cn
http://forums.nphoto.net
http://dc.pconline.com.cn
http://photobbs.it168.com
http://club.tech.sina.com.cn/dc
Car
http://bbs.chetx.com
http://bbs.pcauto.com.cn
http://club.autohome.com.cn
http://bbs.cheshi.com
http://www.xcar.com.cn
http://www.autohome.com.cn
Notebook
http://benyouhui.it168.com/index.php
http://nbbbs.zol.com.cn
http://www.ibijiben.com
http://notebook.pconline.com.cn
http://nbbbs.enet.com.cn
Phone
http://bbs.imobile.com.cn
http://sjbbs.zol.com.cn
http://bbs.shouji.com.cn
http://bbs.cnmo.com
http://forum.younet.com
Table 1: The URLs used in context expansion for differ-
ent domains.
In detail, the pseudo context acquisition algorithm
for a collocation c is illustrated in Figure 2. Note
that, the original context acquisition of c can be con-
sidered as a simplified version of the pseudo context
acquisition. That?s because the current review con-
taining c can be considered as only one snippet in
pseudo context acquisition. Thus, we can just carry
out the two steps in (2) of Figure 2 to obtain the orig-
inal contexts.
Analyzing either the pseudo contexts or the orig-
inal contexts, we can find that not all of them are
useful contexts. Thus we will simply filter the noisy
ones by context sentiment computation, and choose
the contexts showing sentiment orientations as the
useful contexts.
3.3 Sentiment Analysis
For both the original and expanded pseudo contexts,
we employ the lexicon-based sentiment computing
method (Hu and Liu, 2004) to compute the polarity
value for each context. This unsupervised approach
is quite straightforward and makes use of the senti-
ment lexicons in Table 2.
The polarity value Polarity(con) for a context con
Algorithm: Pseudo Context Expansion Algorithm
Input: A collocation c and the URL list
Output: The pseudo context set Conx(c)
1. Use Strategy0~3 to expand c and the expanded queries 
are saved as a set Query(c).
2. For any query q Query(c),   acquire its pseudo 
contexts Conx(q) as follows:
(1) search q in the domain-related URL list, the top 100 
retrieved snippets for each URL are collected as Snip(q)
(2) for each snippet sp Snip(q)
find the sentence s containing q
obtain the two sentences before and after s as the 
contexts of q in this sp, noted as Conx(q, sp)
Conx(q) = 
3. Conx(c) =                      =
?
?
U
)(
),(
qSnipsp
spqConx
?
U
)(
)(
cQueryq
qConx
?
U U
)( )(
),(
cQueryq qSnipsp
spqConx
? ?
Figure 2: The algorithm for pseudo context acquisition.
Lexicon Content
Modifier Lexicon
?,??,??,??,?,?,
??,?,??,??,??
(?very? or ?quite? in English)
Negation Lexicon ??,?,??(?no? or ?not? in English)
Positive Lexicon There are 3,730 Chinese wordsare collected from HOWNET1.
Negative Lexicon There are 3,116 Chinese wordsare collected from HOWNET.
1 http://www.keenage.com/html/e index.html.
Table 2: The lexicons used in this paper.
is computed by summing up the polarity values of all
words in con, making use of both the word polarity
defined in the positive and negative lexicons and the
contextual shifters defined in the negation lexicon.
The algorithm is illustrated in Figure 3.
In this algorithm, n is the parameter controlling
the window size within which the negation words
have influence on the polarity words, and here n is
set to 3.
Normally, if the polarity value Polarity(con) is
more than 0, the context con is labeled as positive; if
less than 0, the context is negative. We also consider
the transitional words, such as ???? (?but? in En-
glish). Finally, the contexts with positive/negative
polarities are used as the useful contexts.
164
Domain # of reviews # of c # of single c Sig / All # of multiple c(All) (Sig) (%) / kinds of multiple c
Camera 138 295 183 62.03 112 / 35
Car 161 232 131 56.47 101 / 33
Notebook 56 147 94 63.95 53 / 20
Phone 123 327 192 58.72 135 / 35
Total 478 1001 600 59.94 401 / 123 ? 3.3
Table 3: Statistics for the Chinese collocation corpus.
Algorithm: Sentiment Analysis 
Input: a context con, and three lexicons: Positive_Dic, 
Negative_Dic, Negation_Dic
Output: Polarity value Polarity(con)
1. Segment con into word set W(con)
2. For each word w W(con), compute its polarity value 
Polarity(w) as follows:
(1) if w Positive_Dic, Polarity(w) = 1;
(2) if w Negative_Dic, Polarity(w) = -1;
(3) otherwise, Polarity(w) = 0;
(4) Within the window of n words previous to w, if 
there is a word w? Negation_Dic, 
Polarity(w) = -Polarity(w)
3. Polarity(con)  = 
?
?
?
? )(
)(
conWw
wPolarity
?
?
Figure 3: The algorithm for context polarity computation.
3.4 Combination
After the pseudo context acquisition and polarity
computation, two kinds of effective contexts: orig-
inal contexts and pseudo contexts, and their corre-
sponding polarities can be obtained.
In order to yield a relatively accurate polarity Po-
larity(c) for a collocation c, we exploit the following
combination methods:
1. Majority Voting: Rather than considering the
difference between the two kinds of contexts, this
combination method relies on the polarity tag of
each context. Suppose c has n effective contexts
(including original and pseudo contexts), it can ob-
tain n polarity tags based on the individual sentiment
analysis algorithm. The polarity tag receiving more
votes is chosen as the final polarity of c.
2. Complementation: For a collocation c, we
first employ ?Majority Voting? method just on the
expanded pseudo contexts to obtain the polarity tag.
If the polarity of c cannot be recognized2, the ma-
jority polarity tag voted on the original contexts is
chosen as the final polarity tag.
4 Experimental Setup
4.1 Dataset and Evaluation Metrics
We conduct the experiments on a Chinese colloca-
tion corpus of four product domains, which is from
the Task3 of the Chinese Opinion Analysis Evalua-
tion (COAE)3 (Zhao et al2008). Table 3 describes
the corpus in detail.
From 478 reviews, 1,001 collocations (454 pos-
itive and 547 negative) with polarity-ambiguous
words are found and manually annotated by two an-
notators. Cohen?s kappa (Cohen, 1960), a measure
of inter-annotator agreement ranging from zero to
one, is 0.83, indicating a good strength of agree-
ment 4. In Table 3, Sig of the fourth column denotes
the collocations that appear once in all the domain-
related reviews. And multiple in the last column
denotes the collocations that appear several times.
From Table 3, we can find that among all the re-
views, nearly 60% collocations only appear once.
Even for the multiple collocations, they averagely
appear less than 4 times. Therefore, for a colloca-
tion, if we only consider its original contexts alone
or the expanded pseudo contexts from the domain-
related review set al, the contexts are obviously
limited and unreliable.
Instead of using accuracy, we use precision (P),
recall (R) and F-measure (F1) to measure the perfor-
mance of this task. That?s because two kinds of col-
locations? polarities cannot be disambiguated. One
2The reason will be explained in the last paragraph of Sec-
tion 4.1.
3http://www.ir-china.org.cn/coae2008.html
4A small number of collocations are still difficult to be dis-
ambiguated from contexts.
165
is the sparse collocations, which obtain no effective
contexts. The other is the collocations that acquire
the same amount of positive and negative contexts.
The metrics are defined as follows.
P = correctly disambiguated collocations
disambiguated collocations
(2)
R = correctly disambiguated collocations
all collocations
(3)
F1 = 2PR
P +R
(4)
4.2 System Description
In order to compare our method with previous work,
we build several systems as follows:
NoExp: Following the method proposed by
Hu (Hu and Liu, 2004), without using the expanded
pseudo contexts, we only consider the two original
contexts Senbef and Senaft of a collocation c in the
current review. If Senbef expresses the polarity po-
lar, then Polarity(ac) = polar. Else if Senaft
expresses the polarity polar?, then Polarity(ac) =
polar?. Else, this method cannot disambiguate the
polarity of c. In this method, the transitional words,
such as ???? (?but? in English) are considered.
Expdataset: Following the method proposed by
Ding (Ding et al2008), we solve this task with the
help of the pseudo contexts in the domain-related re-
view dataset. For a collocation c appearing in many
domain-related reviews, this method refers to the po-
larities of the same c in other reviews. The majority
polarity is chosen as final polarity.
Expweb+sig: This method is the same as our
method in this paper, except for (1) not combining
the original contexts, and (2) not using all the three
query expansion strategies, but just using the sin-
gle (abbv. sig) Strategy0. This method expands the
pseudo contexts from the web. The majority polarity
is chosen as the final polarity.
Expweb+exp: This method is the same as our pro-
posed method in this paper, except for not combin-
ing the original contexts. It expands the pseudo con-
texts from the web. And the ?exp? in the subscript
means that this method uses all the query expansion
strategies. The majority polarity of all the pseudo
contexts is chosen as the final polarity.
Expmv/cweb+exp+com: This is the method proposed
in this paper, which combines the original and ex-
panded pseudo contexts. The superscript ?mv/c? is
short for the two combination methods: Majority
Voting and Complementation.
5 Results
5.1 Comparisons among All the Systems
In fact, all the systems shown in Section 4.2 can be
considered as context based methods. The essential
difference among them lies in the contexts they used.
For a collocation, the contexts for NoExp are two
original contexts from the current review. Breaking
down the boundary of the current review,Expdataset
explores the pseudo contexts from other domain-
related reviews. Further, Expweb+sig, Expweb+exp
and Expmv/cweb+exp+com expand the pseudo contexts
from web, which can be considered as a large corpus
and can provide more evidences for the collocation
polarity disambiguation.
System P(%) R(%) F1(%)
NoExp 67.32 41.16 51.08
Expdataset 68.14 47.85 56.22
Expweb+sig 70.00 53.85 60.87
Expweb+exp 74.97 63.14 68.55
Expmvweb+exp+com 75.53 67.83 71.47
Expcweb+exp+com 74.36 69.83 72.02
Table 4: Comparative results for the collocation polarity
disambiguation task.
Table 4 illustrates the comparative results of all
systems for collocation polarity disambiguation. It
can be observed that our system Expmvweb+exp+com
and Expcweb+exp+com outperform all the other sys-
tems. We discuss the experimental results as fol-
lows:
NoExp yields the worst performance, especially
on the recall. The reason is that the original con-
texts used in this system are limited, and some of
them are even noisy. In comparison, Expdataset
adds a post-processing step of expanding pseudo
contexts from the topically-related review dataset,
which achieves a better result with an absolute im-
provement of 5.14% (F1). This suggests that the
contexts expanded from other reviews are helpful in
disambiguating the collocation?s polarity.
166
However, Expdataset is just effective in disam-
biguating the polarity of such a collocation c, which
appears many times in the domain-related reviews.
From Table 3, we can notice that this kind of collo-
cations only accounts for 40% in all the collocations,
and further they appear less than 4 times on average.
Thus, for such a collocation c, the pseudo contexts
expanded from other reviews that contain the same
c are still far from enough, since the review set size
in this system is not very large.
In order to avoid the context limitation problem,
we expand more pseudo contexts from web for each
collocation. We first try to use a simple query
form (Strategy0) for web mining. Table 4 illustrates
that the corresponding system Expweb+sig outper-
forms the system Expdataset. It can demonstrate
that our web mining based pseudo context expan-
sion is useful for disambiguating the collocation?s
polarity, since this system can explore more con-
texts. However, we can find that the performance
is not very ideal. This system can generate some
harmful contexts for the reason of the wrong mod-
ifying relations between polarity words and targets
in the retrieved snippets.
Thus this paper adds three query expansion strate-
gies to generate more and accurate pseudo con-
texts. Table 4 shows that the corresponding sys-
tem Expweb+exp can achieve a better result with F1
= 68.55%, which is significantly (?2 test with p <
0.01) outperforms Expweb+sig. It demonstrates that
the query expansion strategies are useful.
Finally, Table 4 gives the results of our method in
this paper, Expmvweb+exp+com and Expcweb+exp+com,
which combines the original and expanded pseudo
contexts to yield a final polarity. We can ob-
serve that both of these systems outperform the sys-
tem NoExp of just using the original contexts and
the system Expweb+exp of just using the expanded
pseudo contexts. This can illustrate that the two
kinds of contexts are complementary to each other.
In addition, we can also find that the two combi-
nation methods produce similar results. In detail,
Expmvweb+exp+com disambiguates 899 collocations,
679 of them are correct; Expcweb+exp+com disam-
biguates 940 collocations, 699 of them are correct.
We can further find that, although the amount of
original contexts is small, it also plays an important
role in disambiguating the polarities of the collo-
cations that cannot be recognized by the expanded
pseudo contexts.
5.2 The Contributions of the Query Expansion
Strategies
The expanded pseudo contexts from our method can
be partly credited to the query expansion strategies.
Based on this, this section aims to analyze the differ-
ent contributions of the query expansion strategies in
our method.
Strategy P(%) R(%) F1(%) Avg(#)
Strategy0 70.00 53.85 60.87 71
Strategy1 74.14 55.84 63.70 112
Strategy2 61.84 37.56 46.74 26
Strategy3 64.34 33.17 43.77 20
Expweb+exp 74.97 63.14 68.55 229
Table 5: The performance of our method based on each
query expansion strategy for collocation polarity disam-
biguation.
Table 5 provides the performance of our method
based on each query expansion strategy for collo-
cation polarity disambiguation. For each strategy,
?Avg? in Table 5 denotes the average number of
the expanded pseudo contexts for each collocation.
From this table, we can find that the larger the ?Avg?
is, the better (F1) the strategy is. In detail, Strategy1
with the largest ?Avg? has the best performance; and
Strategy3 with the fewest ?Avg? has the worst per-
formance. This can further demonstrate our idea
that more and effective pseudo contexts can improve
the performance of the collocation polarity disam-
biguation task. Expweb+exp integrates all the query
expansion strategies and obtains much more ?Avg?.
Therefore, this can significantly increase the recall
value, and further produce a better result. On the
other hand, the results in Table 5 show that these
heuristic query expansion strategies are effective.
5.3 Deep Experiments in the
Three-Component Framework
In order to do a detailed analysis into our three-
component framework, some deep experiments are
made:
Query Expansion The aim of query expansion
is to retrieve lots of relative snippets, from which
we can extract the useful pseudo contexts. For each
167
Strategy0 Strategy1 Strategy2 Strategy3
(%) (%) (%) (%)
Query Expansion 76.75 94.50 85.50 85.25
Pseudo Context 71.25 73.50 67.50 74.50
Sentiment Analysis 63.00 68.25 59.00 69.75
Table 6: The accuracies of the query expansion, pseudo context and sentiment analysis for each strategy.
snippet, if the polarity word of the collocation does
modify the target, we consider this snippet as a cor-
rect query expansion result.
Pseudo Context For each expanded pseudo con-
text from web, if it shows the same sentiment ori-
entation with the collocation (or opposite with the
collocation?s polarity because of the usage of transi-
tional words), we consider this context as a correct
pseudo context.
Sentiment Analysis For each expanded pseudo
context, if its polarity can be correctly recognized
by the polarity computation method in Figure 3, and
meanwhile it shows the same sentiment orientation
with the collocation, we consider this context as a
correct one.
Table 6 illustrates the accuracy of each experi-
ment for each strategy in detail, where 400 web re-
trieved snippets for Query Expansion and 400 ex-
panded pseudo contexts for Pseudo Context and
Sentiment Analysis are randomly selected and man-
ually evaluated for each strategy.
Seen from Table 6, we can find that:
1. For Query Expansion, all strategies yield good
accuracies except for Strategy0. This can draw a
same conclusion with our analysis in Section 3.2.1.
The queries from Strategy0 are short, thus in many
retrieved snippets, there exist no modifying relations
between the polarity words and targets. Accord-
ingly, the pseudo contexts from these snippets are
incorrect. This can result in the low accuracy of
Strategy0. On the other hand, we can find that the
other three query expansion strategies perform well.
2. Although the final result of our three-
component framework is good, the accuracies of
Pseudo Context and Sentiment Analysis for each
strategy is not very high. This is perhaps caused by
unrefined work on the specific sub-stages. For ex-
ample, we get alhe pseudo contexts using the al-
gorithm in Figure 2. However, in some reviews, the
two sentences before and after the target sentence
have no polarity relation with the target sentence it-
self. This can bring in some noises. On the other
hand, the context polarity computation algorithm in
Figure 3 is just a simple attempt, which is not the
best way to compute the context?s polarity.
In fact, this paper aims to try some simple algo-
rithms for each component to validate the effective-
ness of the three-component framework. We will
polish every component of our framework in future.
6 Conclusion and Future Work
This paper proposes a web-based context expan-
sion framework for collocation polarity disambigua-
tion. The basic assumption of this framework is
that, if a collocation appears in different forms, both
within the same review and within topically-related
reviews, then the large amounts of pseudo contexts
from these reviews can help to disambiguate such
a collocation?s polarity. Based on this assumption,
this framework includes three independent compo-
nents. First, the heuristic query expansion strate-
gies are adopted to expand pseudo contexts from
web; then a simple but effective polarity computa-
tion method is used to recognize the polarities for
both the original contexts and the expanded pseudo
contexts; and finally, we integrate the polarities from
the original and pseudo contexts as the collocation?s
polarity. Without using any additional labeled data,
experiments on a Chinese data set from four product
domains show that the proposed framework outper-
forms other previous work.
This paper can be concluded as follows:
1. A framework including three independent com-
ponents is proposed for collocation polarity
disambiguation. We can try other different al-
gorithms for each component.
2. Web-based pseudo contexts are effective for
disambiguating a collocation?s polarity.
168
3. The query expansion strategies are promising,
which can generate more useful and correct
contexts.
4. The initial contexts from current reviews and
the expanded contexts from web are comple-
mentary to each other.
The immediate extension of our work is to polish
each component of this framework, such as improv-
ing the accuracy of query expansion and pseudo con-
text acquisition, using other effective polarity com-
puting methods for each context and so on. In ad-
dition, we will explore other query expansion strate-
gies to generate more effective contexts.
Acknowledgments
We thank the anonymous reviewers for their helpful
comments. This work was supported by National
Natural Science Foundation of China (NSFC) via
grant 61133012, the National ?863? Leading Tech-
nology Research Project via grant 2012AA011102,
the Ministry of Education Research of Social Sci-
ences Youth funded projects via grant 12YJCZH304
and the Fundamental Research Funds for the Central
Universities via grant No.HIT.NSRIF.2013090.
References
Kenneth Bloom, Navendu Garg, and Shlomo Argamon.
2007. Extracting appraisal expressions. In HLT-
NAACL 2007, pages 308?315.
D. Bollegala, D. Weir, and J. Carroll. 2011. Using mul-
tiple sources to construct a sentiment sensitive the-
saurus for cross-domain sentiment classification. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 132?141. Asso-
ciation for Computational Linguistics.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 20(1):37?46.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A
holistic lexicon-based approach to opinion mining. In
Proceedings of the Conference onWeb Search andWeb
Data Mining (WSDM), pages 231?240.
A. Esuli and F. Sebastiani. 2005. Determining the se-
mantic orientation of terms through gloss analysis. In
Proceedings of the ACM SIGIR Conference on Infor-
mation and Knowledge Management (CIKM), pages
617?624.
A. Esuli. 2008. Automatic generation of lexical re-
sources for opinion mining: models, algorithms and
applications. In ACM SIGIR Forum, volume 42, pages
105?106. ACM.
V. Hatzivassiloglou and K.R. McKeown. 1997. Predict-
ing the semantic orientation of adjectives. In Proceed-
ings of the eighth conference on European chapter of
the Association for Computational Linguistics, pages
174?181. Association for Computational Linguistics.
Yulan He, Chenghua Lin, and Harith Alani. 2011. Auto-
matically extracting polarity-bearing topics for cross-
domain sentiment classification. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 123?131, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
M. Hu and B. Liu. 2004. Mining and summarizing
customer reviews. In Proceedings of the tenth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 168?177. ACM.
V. Jijkoun, M. De Rijke, and W. Weerkamp. 2010. Gen-
erating focused topic-specific sentiment lexicons. In
Proceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 585?594.
Association for Computational Linguistics.
N. Kaji and M. Kitsuregawa. 2007. Building lexicon
for sentiment analysis from massive collection of html
documents. In Proceedings of the Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 1075?1083.
Jaap Kamps, Maarten Marx, R. ort. Mokken, and
Maarten de Rijke. 2004. Using wordnet to measure
semantic orientation of adjectives. In Proceedings of
LREC-2004, pages 1115?1118.
H. Kanayama and T. Nasukawa. 2006. Fully auto-
matic lexicon expansion for domain-oriented senti-
ment analysis. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Process-
ing, pages 355?363. Association for Computational
Linguistics.
Soo-Min Kim and Eduard Hovy. 2005. Automatic detec-
tion of opinion bearing words and sentences. In Pro-
ceedings of IJCNLP-2005, pages 61?66.
S.-M. Kim and E. Hovy. 2006. Identifying and analyz-
ing judgment opinions. In Proceedings of the Joint
Human Language Technology/North American Chap-
ter of the ACL Conference (HLT-NAACL), pages 200?
207.
Nozomi Kobayashi, Kentaro Inui, Yuji Matsumoto, Kenji
Tateishi, and Toshikazu Fukushima. 2004. Collecting
evaluative expressions for opinion extraction. In Pro-
ceedings of the International Joint Conference on Nat-
ural Language Processing (IJCNLP), pages 584?589.
169
Binyang Li, Lanjun Zhou, Shi Feng, and Kam-Fai Wong.
2010. A unified graph model for sentence-based opin-
ion retrieval. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
page 1367?1375.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opinions
on the web. In Proceedings of WWW-2005, pages
342?351.
Y. Lu, M. Castellanos, U. Dayal, and C.X. Zhai. 2011.
Automatic construction of a context-aware sentiment
lexicon: an optimization approach. In Proceedings of
the 20th international conference on World wide web,
pages 347?356. ACM.
S. Mohammad, C. Dunne, and B. Dorr. 2009. Generat-
ing high-coverage semantic orientation lexicons from
overtly marked words and a thesaurus. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing: Volume 2-Volume 2, pages
599?608. Association for Computational Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using ma-
chine learning techniques. In Proceedings of EMNLP-
2002, pages 79?86.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
hltemnlp2005, pages 339?346.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of EMNLP-2003, pages 105?112.
Ellen Riloff, Janyce Wiebe, and William Phillips. 2005.
Exploiting subjectivity classification to improve in-
formation extraction. In Proceedings of AAAI-2005,
pages 1106?1111.
Fangzhong Su and Katja Markert. 2009. Subjectivity
recognition on word senses via semi-supervised min-
cuts. In Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the ACL, pages 1?9.
Yasuhiro Suzuki, Hiroya Takamura, and Manabu Oku-
mura. 2006. Application of semi-supervised learn-
ing to evaluative expression classification. In Com-
putational Linguistics and Intelligent Text Processing,
pages 502?513.
P. Turney, M.L. Littman, et al003. Measuring praise
and criticism: Inference of semantic orientation from
association. ACM Transactions on Information Sys-
tems (TOIS), 21(4):315?346.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and RyanMcDonald. 2010. The viability of web-
derived polarity lexicons. In The 2010 Annual Confer-
ence of the North American Chapter of the Association
for Computational Linguistics, pages 777?785.
Jan Wiebe and Rada Mihalcea. 2006. Word sense and
subjectivity. In Proceedings of the Conference on
Computational Linguistics / Association for Computa-
tional Linguistics (COLING/ACL), pages 1065?1072.
Janyce Wiebe, Eric Breck, and Chris Buckley. 2003.
Recognizing and Organizing Opinions Expressed in
the World Press. In Papers from the AAAI Spring
Symposium on New Directions in Question Answering,
pages 24?26.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of AAAI, pages 735?
740.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of HLT/EMNLP-
2005, pages 347?354.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: an exploration
of features for phrase-level sentiment analysis. Com-
putational Linguistics, 35(3).
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of EMNLP-2003, pages 129?
136.
Min Zhang and Xingyao Ye. 2008. A generation model
to unify topic relevance and lexicon-based sentiment
for opinion retrieval. In Proceedings of the ACM Spe-
cial Interest Group on Information Retrieval (SIGIR),
pages 411?419.
Wei Zhang, Clement Yu, and Weiyi Meng. 2007. Opin-
ion retrieval from blogs. In In proceedings of CIKM,
page 831?840.
Jun Zhao, Hongbo Xu, Xuanjing Huang, Songbo Tan,
Kang Liu, and Qi Zhang. 2008. Overview of chinese
opinion analysis evaluation 2008.
170
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 468?478,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Improving Web Search Ranking by Incorporating Structured 
Annotation of Queries* 
 
 
Xiao Ding1, Zhicheng Dou2, Bing Qin1, Ting Liu1, Ji-Rong Wen3 
 
1Research Center for Social Computing and Information Retrieval 
Harbin Institute of Technology, China 
 
2Microsoft Research Asia, Beijing 100190, China 
 
3Renmin University of China, Beijing, China 
1{xding, qinb, tliu}@ir.hit.edu.cn; 
2zhichdou@microsoft.com; 3jirong.wen@gmail.com 
 
 
 
 
 
 
Abstract? 
Web users are increasingly looking for 
structured data, such as lyrics, job, or recipes, 
using unstructured queries on the web. 
However, retrieving relevant results from such 
data is a challenging problem due to the 
unstructured language of the web queries. In 
this paper, we propose a method to improve 
web search ranking by detecting Structured 
Annotation of queries based on top search 
results. In a structured annotation, the original 
query is split into different units that are 
associated with semantic attributes in the 
corresponding domain. We evaluate our 
techniques using real world queries and achieve 
significant improvement. 
1 Introduction 
Search engines are getting more sophisticated by 
utilizing information from multiple diverse sources. 
One such valuable source of information is 
structured and semi-structured data, which is not 
very difficult to access, owing to information 
extraction (Wong et al, 2009; Etzioni et al, 2008; 
Zhai and Liu 2006) and semantic web efforts. 
                                                          
? *Work was done when the first author was visiting Microsoft 
Research Asia 
Driving the web search evolution are the user 
needs. Users usually have a template in mind when 
formulating queries to search for information. 
Agarwal et al, (2010) surveyed a search log of 15 
million queries from a commercial search engine. 
They found that 90% of queries follow certain 
templates. For example, by issuing the query 
?taylor swift lyrics falling in love?, the users are 
actually seeking for the lyrics of the song ?Mary's 
Song (oh my my my)? by artist Taylor Swift. The 
words ?falling in love? are actually part of the 
lyrics they are searching for. However, some top 
search results are irrelevant to the query, although 
they contain all the query terms. For example, the 
first top search result shown in Figure 1(a) does 
not contain the required lyrics. It just contains the 
lyrics of another song of Taylor Swift, rather than 
the song that users are seeking. 
A possible way to solve the above ranking 
problem is to understand the underlying query 
structure. For example, after recognizing that 
?taylor swift? is an artist name and ?falling in love? 
are part of the lyrics, we can improve the ranking 
by comparing the structured query with the 
corresponding structured data in documents 
(shown in Figure 1(b)). Some previous studies 
investigated how to extract structured information 
from user queries, such as query segmentation 
(Bergsma and Wang, 2007). The task of query 
segmentation is to separate the query words into 
468
disjointed segments so that each segment maps to a 
semantic unit (Li et al, 2011). For example, the 
segmentation of the query ?taylor swift lyrics 
falling in love? can be ?taylor swift | lyrics | falling 
in love?. Since query segmentation cannot tell 
?talylor swift? is an artist name and ?falling in love? 
are part of lyrics, it is still difficult for us to judge 
whether each part of the query segmentations 
matches the right field of the documents or not 
(such as judge whether ?talylor swift? matches the 
artist name in the document). Recently, a lot of 
work (Sarkas et al, 2010; Li et al, 2009) proposed 
the task of structured annotation of queries which 
aims to detect the structure of the query and assign 
a specific label to it. However, to our knowledge, 
the previous methods do not exploit an effective 
approach for improving web search ranking by 
incorporating structured annotation of queries. 
In this paper, we investigate the possibility of 
using structured annotation of queries to improve 
web search ranking. Specifically, we propose a 
greedy algorithm which uses the structured data 
(named annotated tokens in Figure 1(b)) extracted 
from the top search results to annotate the latent 
structured semantics in web queries. We then 
compute matching scores between the annotated 
query and the corresponding structured 
information contained in documents. The top 
search results can be re-ranked according to the 
matching scores. However, it is very difficult to 
extract structured data from all of the search results. 
Hence, we propose a relevance feedback based re-
ranking model. We use these structured documents 
whose matching scores are greater than a threshold 
as feedback documents, to effectively re-rank other 
search results to bring more relevant and novel 
information to the user. 
Experiments on a large web search dataset from 
a major commercial search engine show that the F-
Measure of structured annotation generated by our 
approach is as high as 91%. On this dataset, our re-
ranking model using the structured annotations 
significantly outperforms two baselines. 
The main contributions of our work include: 
1. We propose a novel approach to generate 
structured annotation of queries based on top 
search results. 
2. Although structured annotation of queries has 
been studied previously, to the best of our 
knowledge this is the first paper that attempts 
to improve web search ranking by 
incorporating structured annotation of queries. 
The rest of this paper is organized as follows. 
We briefly introduce related work in Section 2. 
Section 3 presents our method for generating 
structured annotation of queries. We then propose 
two novel re-ranking models based on structured 
annotation in Section 4. Section 5 introduces the 
data used in this paper. We report experimental 
results in Section 6. Finally we conclude the work 
in Section 7. 
 
Figure 1. Search results of query ?taylor swift lyrics falling in love? and processing pipeline 
[Taylor Swift, #artist_name, 0.34]
...
[Mary?s Song (oh my my my), #song_name, 0.16]
[Crazier, #song_name, 0.1]
[Jump Then Fall, #song_name, 0.08]
...
[Growing up and falling in love?, #lyrics, 0.16]
[Feel like I?m falling and ?, #lyrics, 0.1]
[I realize your love is the best ?, #lyrics, 0.08]
d1 [Taylor Swift, #artist_name]
[Crazier, #song_name]
[Feel like I?m falling and ?, #lyrics]
d2 [Taylor Swift, #artist_name]
[Mary?s Song (oh my my my), #song_name]
[Growing up and falling in love?, #lyrics]
d3 [Taylor Swift, #artist_name]
[Jump Then Fall, #song_name]
[I realize your love is the best ?, #lyrics]
d4 [Taylor Swift, #artist_name]
[Mary?s Song (oh my my my), #song_name]
[Growing up and falling in love?, #lyrics]
Search Results (a)
Weighted Annotated Tokens (c)Query Structured Annotation Generation (d)Top Results Re-ranking (e)
Annotated Tokens (b)
1.
2.
3.
4.
Query: taylor swift lyrics falling in love
<[taylor swift, #artist_name] lyrics 
[falling in love, #lyrics]>
 
1. 
  
2. 
  
3. 
  
4. 
1.
2.
3.
4.
469
2 Related Work 
There is a great deal of prior research that 
identifies query structured information. We 
summarize this research according to their 
different approaches. 
2.1 Structured Annotation of Queries 
Recently, a lot of work has been done on 
understanding query structure (Sarkas et al, 2010; 
Li et al, 2009; Bendersky et al, 2010). One 
important method is structured annotation of 
queries which aims to detect the structure of the 
query and assign a specific label to it. Li et al, 
(2009) proposed web query tagging and its goal is 
to assign to each query term a specified category, 
roughly corresponding to a list of attributes. A 
semi-supervised Conditional Random Field (CRF) 
is used to capture dependencies between query 
words and to identify the most likely joint 
assignment of words to ?categories.? Comparing 
with previous work, the advantages of our 
approach are on the following aspects. First, we 
generate structured annotation of queries based on 
top search results, not some global knowledge base 
or query logs. Second, they mainly focus on the 
method of generating structured annotation of 
queries, rather than leverage the generated query 
structures to improve web search rankings. In this 
paper, we not only offer a novel solution for 
generating structured annotation of queries, but 
also propose a re-ranking approach to improve 
Web search based on structured annotation of 
queries. Bendersky et al, (2011) also used top 
search results to generate structured annotation of 
queries. However, the annotations in their 
definition are capitalization, POS tags, and 
segmentation indicators, which are different from 
ours. 
2.2 Query Template Generation 
The concept of query template has been discussed 
in a few recent papers (Agarwal et al, 2010; Pasca 
2011; Liu et al, 2011; Szpektor et al, 2011). A 
query template is a sequence of terms, where each 
term could be a word or an attribute. For example, 
<#artist_name lyrics #lyrics> is a query template, 
?#artist_name? and ?#lyrics? are attributes, and 
?lyrics? is a word. Structured annotation of queries 
is different from query template, as a query 
template can instantiate multiple queries while a 
structured annotation only serves for a specific 
query. Unlike query template, our work is ranking-
oriented. We aim to automatically annotate query 
structure based on top search results, and further 
use these structured annotations to re-rank top 
search results for improving search performance. 
2.3 Query Segmentation 
The task of query segmentation is to separate the 
query words into disjointed segments so that each 
segment maps to a semantic unit (Li et al, 2011). 
Query segmentation techniques have been well 
studied in recent literature (Tan and Peng, 2008; 
Yu and Shi, 2009). However, structured annotation 
of queries cannot only separate the query words 
into disjoint segments but can also assign each 
segment a semantic label which can help the search 
engine to judge whether each part of query 
segmentation matches the right field of the 
documents or not. 
2.4 Entity Search 
The problem of entity search has received a great 
deal of attention in recent years (Guo et al, 2009; 
Bron et al, 2010; Cheng et al, 2007). Its goal is to 
answer information needs that focus on entities. 
The problem of structured annotation of queries is 
related to entity search because for some queries, 
structured annotation items are entities or attributes. 
Some existing entity search approaches also 
exploit knowledge from the structure of webpages 
(Zhao et al, 2005). Annotating query structured 
information differs from entity search in the 
following aspects. First, structured annotation 
based ranking is applicable for all queries, rather 
than just entity related queries. Second, the result 
of an entity search is usually a list of entities, their 
attributes, and associated homepages, whereas our 
work uses the structured information from 
webpages to annotate query structured information 
and further leverage structured annotation of 
queries to re-rank top search results. 
Table 1. Example domain schemas 
Domain Schema Example structured annotations 
lyrics #artist_name 
#song_name 
#lyrics 
<lyrics of [hey jude, #song_name] [beatles, 
#artist_name]> 
job #category 
#location 
<[teacher, #category] job in [America, 
#location]> 
recipe  #directions 
#ingredients 
<[baking, # directions] [bread, # 
ingredients] recipe> 
 
470
3 Structured Annotation of Queries  
3.1 Problem Definition 
We start our discussion by defining some basic 
concepts. A token is defined as a sequence of 
words including space, i.e., one or more words. For 
example, the bigram ?taylor swift? can be a single 
token. As our objective is to find structured 
annotation of queries in a specific domain, we 
begin with a definition of domain schema. 
Definition 1 (Domain Schema): For a given 
domain of interest, the domain schema is the set of 
attributes. We denote the domain schema as ? =
{?1, ?2,? , ??}, where each ??  is the name of an 
attribute of the domain. Sample domain schemas 
are shown in Table 1. In contrast to previous 
methods (Agarwal et al, 2010), our definition of 
domain schema does not need attribute values. For 
the sake of simplicity, this paper assumes that 
attributes in domain schema are available. 
However, it is not difficult to pre-specify attributes 
in a specific domain. 
Definition 2 (Annotated Token): An annotated 
token in a specific domain is a pair [?, ?], where v 
is a token and a is a corresponding attribute for v 
in this domain. [hey jude, #song_name] is an 
example of an annotated token for the ?lyrics? 
domain shown in Table 1. The words ?hey jude? 
comprise a token, and its corresponding attribute 
name is #song_name. If a token does not have any 
corresponding attributes, we denote it as free token. 
Definition 3 (Structured Annotation): A 
structured annotation p is a sequence of terms <
?1,?2,?,?? >, where each ?? could be a free token or 
an annotated token, and at least one of the terms is 
an annotated token, i.e., ?? ? [1, ?] for which ?? is 
an annotated token. 
Given the schema for the domain ?lyrics?, 
<[taylor swift, #artist_name] lyrics [falling in love, 
#lyrics]> is a possible structured annotation for the 
query ?taylor swift lyrics falling in love?. In this 
annotation, [taylor swift, #artist_name] and 
[falling in love, #lyrics] are two annotated tokens. 
The word ?lyrics? is a free token. 
Intuitively, a structured annotation corresponds 
to an interpretation of the query as a request for 
some structured information from documents. The 
set of annotated tokens expresses the information 
need of the documents that have been requested. 
The free tokens may provide more diverse 
information. Annotated tokens and free tokens 
together cover all query terms, reflecting the 
complete user intent of the query. 
3.2 Generating Structured Annotation 
In this paper, given a domain schema A, we 
generate structured annotation for a query q based 
on the top search results of q. We propose using 
top search results, rather than some global 
knowledge base or query logs, because: 
(1) Top search results have been proven to be 
a successful technique for query explanation 
(Bendersky et al, 2010). 
(2) We have observed that in most cases, a 
reasonable percentage of the top search results are 
relevant to the query. By aggregating structured 
information from the top search results, we can get 
more query-dependent annotated tokens than using 
global data sources which may contain more noise 
and outdated. 
(3) Our goal for generating structured 
annotation is to improve the ranking quality of 
queries. Using top search results enables 
simultaneous and consistent detection of structured 
information from documents and queries. 
As mentioned in Section 3.1, we generate 
structured annotation of queries based on annotated 
tokens, which are actually structured data (shown 
in Figure 1(b)) embedded in web documents. In 
this paper, we assume that the annotated tokens are 
Algorithm 1: Query Structured Annotation Generation 
Input: a list of weighted annotated tokens T = {t1, ? , tm} ; 
          a query q = ?w1, ? , wn?  where wi ? W; 
a pre-defined threshold score ?. 
Output: a query structured annotation p = <s1, ? , sk>. 
  1: Set p = q = {s1, ?, sn}, where si = wi 
  2: for u = 1 to T.size do 
  3:       compute ?????(?, ??) 
            = ?????(?, ??. ?)  
            = ??. ? ? ???0??<??????(??? , ??. ?), 
            where pij = si,?,sj, s.t. sl ? W for l ? [i, j]. //pij is just 
in the remaining query words 
  4: end for 
  5: find the maximum matching tu with  
            ???? = ??????1?????????(?, ??) 
  6: if ?????(?, ????) > ? then 
  7:      replace si,?,sj in p with [si,?,sj, tmax.a ] 
  8:      remove tmax from T 
9:      n ? n ? (j - i) 
10:      go to step 2 
11: else  
12:      return p 
13: end if 
 
471
available and we mainly focus on how to use these 
annotated tokens from top search results to 
generate structured annotation of queries. The 
approach is comprised of two parts, one for 
weighting annotated tokens and the other for 
generating structured annotation of queries based 
on the weighted annotated tokens. 
Weighting: As shown in Figure 1, annotated 
tokens extracted from top results may be 
inconsistent, and hence some of the extracted 
annotated tokens are less useful or even useless for 
generating structured annotation. 
We assume that a better annotated token should 
be supported by more top results; while a worse 
annotated token may appear in fewer results. 
Hence we aggregate all the annotated tokens 
extracted from top search results, and evaluate the 
importance of each unique one by a ranking-aware 
voting model as follows. For an annotated token [v, 
a], its weight w is defined as: 
                      ? =
1
?
? ??1????                           (1) 
where wj is a voting from document dj, and 
?? = {
? ? ? + 1
?
,             if [?, ?] ? ??
0,                      else        
 
Here, N is the number of top search results and j 
is the ranking position of document dj. We then 
generate a weighted annotated token [v, a, w] for 
each original unique token [v, a]. 
Generating: The process by which we map a 
query q to Structured Annotation is shown in 
Algorithm 1. The algorithm takes as input a list of 
weighted annotated tokens and the query q, and 
outputs the structured annotation of the query q. 
The algorithm first partitions the query q by 
comparing each sub-sequence of the query with all 
the weighted annotated tokens, and find the 
maximum matching annotated token (line 1 to line 
5). Then, if the degree of match is greater than the 
threshold ? which is a pre-defined threshold score 
for fuzzy string matching, the query substring will 
be assigned the attribute label of the maximum 
matching annotated token (line 6 to line 8). The 
algorithm stops when all the weighted annotated 
tokens have been scanned, and outputs the 
structured annotation of the query.  
Note that in some cases, the query may fail to 
exactly match with the annotated tokens, due to 
spelling errors, acronyms or abbreviations in users? 
queries. For example, in the query ?broken and 
beatuful lyrics?, ?broken and beatuful? is a 
misspelling of ?broken and beautiful.? We adopt a 
fuzzy string matching function for comparing a 
sub-sequence string s with a token v: 
          ???(?, ?) = 1 ?
????????????(?,?)
max (|?|,|?|)
                (2) 
where EditDistance(s, v) measures the edit 
distances of two strings, |s| is the length of string s 
and |v| is the length of string v. 
4 Ranking with Structured Annotation 
Given a domain schema ? = {?1, ?2, ? , ??}, and a 
query q, suppose that ? = < ?1,?2,?,?? >  is the 
structured annotation for query q obtained using 
the method introduced in the above sections. p can 
better reflects the user?s real search intent than the 
original q, as it presents the structured semantic 
information needed instead of a simple word string. 
Therefore, a document di can better satisfy a user?s 
information need if it contains corresponding 
structured semantic information in p. Suppose that 
Ti is the set of annotated tokens extracted from 
document di, we compute a re-ranking score, 
denoted by RScore, for document di as follows: 
RScore(q, di) = ?????(?, ??) 
                      = ?????(?, ??) 
                      = ? ? ?????(?? , ?)????1????  
where 
  ?????(?? , ?)= {
???(?? . ?? , ?. ?),        if ?? . ?? = ?. ?
0,                                else
      (3) 
where ??  is an annotated token in p and t is an 
annotated token in di. We use Equation (2) to 
compute the similarity between values in query 
annotated tokens and values in document annotated 
tokens. We propose two re-ranking models, 
namely the conservative re-ranking model, to re-
rank top results based on RScore and relevance 
feedback based re-ranking model. 
4.1 Conservative Re-ranking Model 
A nature way to re-rank top search results is 
according to their RScore. However, we fail to 
obtain annotated tokens from some retrieved 
documents, and hence the RScore of these 
documents are not available. In the conservative 
re-ranking model, we only re-rank search results 
that have an RScore. For example, suppose there 
are five retrieved documents {d1, d2, d3, d4, d5} for 
query q, we can extract structured information 
from document d3 and d4 and RScore(q, d4) > 
RScore(q, d3). Note that we cannot obtain 
472
structured information from d1, d2, and d5.  In the 
conservative re-ranking method, d1, d2, and d5 
retain their original positions; while d3 and d4 will 
be re-ranked according to their RScore. Therefore, 
the final ranking generated by our conservative re-
ranking model should be {d1, d2, d4, d3, d5}, in 
which the documents are re-ranked among the 
affected positions. 
There is also useful information in the 
documents without structured data, such as 
community question answering websites. However, 
in the conservative re-ranking model they will not 
be re-ranked. This may hurt the performance of our 
re-ranking model. One reasonable solution is 
relevance feedback model. 
4.2 Relevance Feedback based Re-ranking 
Model 
The disadvantage of the conservative re-ranking 
model is that it only can re-rank those top search 
results with structured data. To make up its 
limitation, we propose a relevance feedback based 
re-ranking model. The key idea of this model is 
based on the observation that the search results 
with the corrected annotated tokens could give 
implicit feedback information. Hence, we use these 
structured documents whose RScore are greater 
than a threshold ? (empirically set it as 0.6) as 
feedback documents, to effectively re-rank other 
search results to bring more relevant and novel 
information to the user. 
Formally, given a query Q and a document 
collection C, a retrieval system returns a ranked list 
of documents D. Let di denote the i-th ranked 
document in the ranked list. Our goal is to study 
how to use these feedback documents, J ? {d1,?, 
dk}, to effectively re-rank the other r search results: 
U ? {dk+1,?, dk+r}. A general formula of relevance 
feedback model (Salton et al 1990) R is as follows: 
?(??) = (1 ? ?)??(Q) + ???(J)             (4) 
where ? ? [0, 1] is the feedback coefficient, and ?? 
and ?? are two models that map a query and a set 
of relevant documents, respectively, into some 
comparable representations. For example, they can 
be represented as vectors of weighted terms or 
language models. 
In this paper, we explore the problem in the 
language model framework, particularly the KL-
divergence retrieval model and mixture-model 
feedback method (Zhai and Lafferty, 2001), mainly 
because language models deliver state-of-the-art 
retrieval performance and the mixture-model based 
feedback is one of the most effective feedback 
techniques which outperforms Rocchio feedback. 
4.2.1 The KL-Divergence Retrieval Model 
The KL-divergence retrieval model was introduced 
in Lafferty and Zhai, (2001) as a special case of the 
risk minimization retrieval framework and can 
support feedback more naturally. In this model, 
queries and documents are represented by unigram 
language models. Assuming that these language 
models can be appropriately estimated, KL-  
divergence retrieval model measures the relevance 
value of a document D with respect to a query Q 
by computing the negative Kullback-Leibler 
divergence between the query language model ?? 
and the document language model ?? as follows: 
?(?, ?) = ??(??||??) = ?? ?(?|??)???
?(?|??)
?(?|??)
???       (5) 
where V is the set of words in our vocabulary. 
Intuitively, the retrieval performance of the KL-
divergence relies on the estimation of the 
document model ?? and the query model ??.  
For the set of k relevant documents, the 
document model ??  is estimated as ?(w|??) =
1
?
?
?(?,??)
|??|
?
?=1 , where ?(?, ??) is the count of word 
w in the i-th relevant document, and |??| is the total 
number of words in that document. The document 
model ??  needs to be smoothed and an effective 
method is Dirichlet smoothing (Zhai et al, 2001). 
The query model intuitively captures what the 
user is interested in, and thus would affect retrieval 
performance. With feedback documents, ??  is 
estimated by the mixture-model feedback method. 
4.2.2 The Mixture Model Feedback Method 
As the problem definition in Equation (4), the 
query model can be estimated by the original query 
model ?(?|??) =
?(?,?)
|?|
 (where c(w,Q) is the count 
of word w in the query Q, and |Q| is the total 
number of words in the query) and the feedback 
document model. Zhai and Lafferty, (2001) 
proposed a mixture model feedback method to 
estimate the feedback document model. More 
specifically, the model assumes that the feedback 
documents can be generated by a background 
language model ?(?|?) estimated using the whole 
collection and an unknown topic language model 
473
?? to be estimated. Formally, let F ? C be a set of 
feedback documents. In this paper, F is comprised 
of documents that RScore are greater than?. The 
log-likelihood function of the mixture model is: 
???(?|??) = 
      ? ? ?(?, ?)??? log [(1 ? ?)?(?|??) + ??(?|?)]???     (6) 
where ? ? [0,1)  is a mixture noise parameter 
which controls the weight of the background 
model. Given a fixed ?, a standard EM algorithm 
can then be used to estimate ?(?|??), which is 
then interpolated with the original query model 
?(?|Q) to obtain an improved estimation of the  
query model: 
?(?|??) = (1 ? ?)?(?|?) + ??(?|??)         (7) 
 where ? is the feedback coefficient. 
5 Data 
We used a dataset composed of 12,396 queries 
randomly sampled from query logs of a search 
engine. For each query, we retrieved its top 100 
results from a commercial search engine. The 
documents were judged by human editors. A five-
grade (from 0 to 4 meaning from bad to perfect) 
relevance rating was assigned for each document. 
We used a proprietary query domain classifier to 
identify queries in three domains, namely ?lyrics,?  
?recipe,? and ?job,? from the dataset. The statistics 
about these domains are shown in Table 2. To 
investigate how many queries may potentially have 
structured annotations, we manually created 
structured annotations for these queries. The last 
column of Table 2 shows the percentage of queries 
that have structured annotations created by 
annotators. We found that for each domain, there 
was on average more than 90% of queries 
identified by us that had a certain structured 
annotation. This indicates that a large percentage 
of these queries contain structured information, as 
we expected. 
6 Experimental Results 
In this section, we present the structured annotation 
of queries and further re-rank the top search results 
for the three domains introduced in Section 5. We 
used the ranking returned by a commercial search 
engine as our one of the Baselines. Note that as the 
baseline already uses a large number of ranking 
signals, it is very difficult to improve it any further. 
We evaluate the ranking quality using the widely 
used Normalized Discounted Cumulative Gain 
measure (NDCG) (Javelin and Kekalainen., 2000). 
We use the same configuration for NDCG as 
(Burges et al 2005). More specifically, for a given 
query q, the NDCG@K is computed as: 
                        ?? = 
1
??
? (2?(?)?1)??=1
log (1 + ?)
                            (4) 
Mq is a normalization constant (the ideal NDCG) 
so that a perfect ordering would obtain an NDCG 
of 1; and r(j) is the rating score of the j-th  
document in the ranking list.  
6.1 Overall Results 
6.1.1 Quality of Structured Annotation of 
Queries 
We generated the structured annotation of queries 
based on the top 10 search results and used ? =
0.04  for Algorithm 1. We used several existing 
metrics, P (Precision), R (Recall), and F-Measure 
to evaluate the quality of the structured annotation. 
As a query structured annotation may contain more 
than one annotated token, we concluded that the 
 
Figure 2. Ranking Quality (* indicates significant 
improvement) 
0.54
0.55
0.56
0.57
0.58
0.59
0.6
0.61
0.62
NDCG@1 NDCG@3 NDCG@5
V
a
lu
e
 o
f 
m
e
a
s
u
r
e
m
e
n
t
Measurement
Seg-Ranker Ori-Ranker Con-Ranker FB-Ranker
*
*
*
*
*
*
Table 3. Quality of Structured Annotation. All the 
improvements are significant (p < 0.05) 
Domain Method Precision Recall F-Measure 
lyrics Baseline 
Our 
90.06% 
95.45% 
84.92% 
89.83% 
87.41% 
92.55% 
job Baseline 
Our 
89.62% 
95.31% 
80.14% 
84.93% 
84.62% 
89.82% 
recipe Baseline 
Our 
83.96% 
89.68% 
84.23% 
88.44% 
84.09% 
89.06% 
All Baseline 
Our 
87.88% 
93.61% 
83.10% 
88.45% 
85.42% 
90.96% 
 
Table 2. Domain queries used in our experiment 
Domain Containing 
Keyword 
Queries 
 
Structured  
Annotation% 
lyrics ?lyrics? 196 95% 
job ?job? 124 92% 
recipe ?recipe? 76   93% 
 
474
annotation was correct only if the entire annotation 
was completely the same as the annotation labeled 
by annotators. Otherwise we treated the structured 
annotation as incorrect. Experimental results for 
the three domains are shown in Table 3. We 
compare our approach with Xiao Li, (2010) 
(denoted as baseline), on the dataset described in 
Section 5. They labeled the semantic structure of 
noun phrase queries based on semi-Markov CRFs. 
Our approach achieves better performance than the 
baseline (about 5.5% significant improvement on 
F-Measure). This indicates that the approach of 
generating structured annotation based on the top 
search results is more effective. With the high-
quality structured annotation of queries in hand, it 
may be possible to obtain better ranking results 
using our proposed re-ranking models. 
6.1.2 Re-ranking Result 
We used the models introduced in Section 4 to re-
rank the top 10 search results, based on structured 
annotation of queries and annotated tokens.  
Recall that our goal is to quantify the 
effectiveness of structured annotation of queries 
for real web search. One dimension is to compare 
with the original search results of a commercial 
search engine (denoted as Ori-Ranker). The other 
is to compare with the query segmentation based 
re-ranking model (denoted as Seg-Ranker; Li et 
al., 2011) which tries to improve web search 
ranking by incorporating query segmentation. Li et 
al., (2011) incorporated query segmentation in the 
BM25, unigram language model and bigram 
language model retrieval framework, and bigram 
language model achieved the best performance. In 
this paper, Seg-Ranker integrates bigram language 
model with query segmentation. 
The ranking results of these models are shown 
in Figure 2. This figure shows that all our two 
rankers significantly outperform the Ori-Ranker? 
the original search results of a commercial search 
engine. This means that using high-quality 
structured annotation does help better 
understanding of user intent. By comparing these 
structured annotations and the annotated tokens in 
documents, we can re-rank the more relevant 
results higher and yield better ranking quality. 
Figure 2 also suggests that structured annotation 
based re-ranking models outperform query 
segmentation based re-ranking model. This is 
mainly because structured annotation can not only 
separate the query words into disjoint segments but 
can also assign each segment a semantic label. 
Taking full advantage of the semantic label can 
lead to better ranking performance. 
Furthermore, Figure 2 shows that FB-Ranker 
outperforms Con-Ranker. The main reason is that 
in Con-Ranker, we can only reasonably re-rank the 
search results with structured data. However, in 
FB-Ranker we can not only re-rank the structured 
search results but also can re-rank other documents 
by incorporating implicit information from those 
structured documents.  
On average, FB-Ranker achieves the best 
ranking performance. Table 4 shows more detailed 
Table 4. Detailed ranking results on three domains. 
All the improvements are significant (p < 0.05) 
Domain Ranking Method NDCG@1 NDCG@3 NDCG@5 
lyrics Seg-Ranker 0.572 0.574 0.575 
Ori-Ranker 
FB-Ranker 
0.621 
0.637 
0.628 
0.639 
0.636 
0.647 
recipe Seg-Ranker 0.629 0.631 0.634 
Ori-Ranker 
FB-Ranker 
0.678 
0.707 
0.687 
0.704 
0.696 
0.709 
job Seg-Ranker 0.438 0.413 0.408 
Ori-Ranker 
FB-Ranker 
0.470 
0.504 
0.453 
0.474 
0.442 
0.459 
 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.02 0.04 0.06 0.08 0.1 0.3 0.5 0.7 0.9
V
a
l
u
e
 
o
f
 
m
e
a
s
u
r
e
m
e
n
t
Query structured annotation generation threshold ?
Precision
Recall
F-Measure
0.54
0.55
0.56
0.57
0.58
0.59
0.6
0.61
0 0.02 0. 4 .06 0.08 .1 0.3 0.5 0.7 0.9 perfect
N
D
C
G
@
3
Que tr ctured annotation generation threshold ?
Seg-Ranker Ori-Ranker FB-Ranker
 
                          (a) Quality of re-ranking                                    (b) Quality of query structured annotation 
Figure 3. Quality of re-ranking and quality of query structured annotation with different number of search results 
475
results for the three selected domains. This table 
shows that FB-Ranker consistently outperforms the 
two baseline rankers on these domains. In the 
remaining part of this paper, we will only report 
the results for this ranker, due to space limitations. 
Table 4 also indicates that we can get robust 
ranking improvement in different domains, and we 
will consider applying it to more domains. 
6.2 Experiment with Different Thresholds of 
Query Structured Annotation Algorithm 
As introduced in Algorithm 1, we pre-defined a 
threshold ? for fuzzy string matching. We 
evaluated the quality of re-ranking and query 
structured annotation with different settings for ?. 
The results are shown in Figure 3. We found that: 
(1) When we use ? = 0, which means that the 
structured annotations can be generated no matter 
how small the similarity between the query string 
and a weighted annotated token is, we can get a 
significant NDCG@3 gain of 2.15%. Figure 3(b) 
shows that the precision of the structured 
annotation is lowest when ? = 0 . However, the 
precision is still as high as 0.7375, and the highest 
recall is obtained in this case. This means that the 
quality of the generated structured annotations is 
still reasonable, and hence we can get a ranking 
improvement when ? = 0, as shown in Figure 3(a). 
(2) Figure 3(a) suggests that the quality of re-
ranking increases when the threshold ? increases 
from 0 to 0.05. It then decreases when ? increases 
from 0.06 to 0.5. Comparing these two figures 
shows that the trend of re-ranking performance 
adheres to the quality of the structured annotation. 
The settings for ? dramatically affect the recall and 
precision of the structured annotation; and hence 
the ranking quality is impacted. The larger ? is, the 
lower the recall of the structured annotation is. 
(3) Since the re-ranking performance 
dramatically changes along with the quality of the 
structured annotation, we conducted a re-ranking 
experiment with perfect structured annotations (F-
Measure equal to 1.0). Perfect structured 
annotations mean the annotations created by 
annotators as introduced in Section 5. The results 
are shown in the last bar of Figure 3(a). We did not 
find a large space for ranking improvement. The 
NDCG@3 when using perfect structured 
annotations was 0.606, which is just slightly better 
than our best result (yield when ?=0.05). It 
indicates that our structured annotation generation 
algorithm is already quite effective. 
(4) Figure 3(a) shows that our approach 
outperforms the two baseline approaches with most 
settings for ?. This indicates that our approach is 
relatively stable with different settings for ?. 
6.3 Experiment with Number of Top Search 
Results 
The above experiments are conducted based on the 
top 10 search results. In this section, by adjusting 
the number of top search results, ranging from 2 to 
100, we investigate whether the quality of 
structured annotation of queries and the 
performance of re-ranking are affected by the 
quantity of search results. The results shown in 
Figure 4 indicate that the number of search results 
does affect the quality of structured annotation of 
queries and the performance of re-ranking. 
Structured annotations of queries become better 
when more search results are used from 2 to 20. 
This is because more search results cover more 
websites in our domain list, and hence can generate 
more annotated tokens. More results also provide 
more evidence for voting the importance of 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
2 3 4 5 6 7 8 9 10 20 30 40 50 60 70 80 90 100
V
a
u
l
u
e
 
o
f
 
m
e
a
s
u
r
e
m
e
n
t
Number of search results
Precision
Recall
F-Measure
0.54
0.55
0.56
0.57
0.58
0.59
0.6
0.61
2 3 4 5 6 7 8 9 10 0 30 4 50 60 70 80 90 100
N
D
C
G
@
3
Number of search result
Seg-Ranker Ori-Ranker FB-Ranker
 
                              (a) Quality of re-ranking                                   (b) Quality of query structured annotation 
Figure 4. Quality of re-ranking and quality of query structured annotation with different number of search results 
476
annotated tokens, and hence can improve the 
quality of structured annotation of queries. 
In addition, we also found that structured 
annotation of queries become worse when too 
many lower ranked results are used (e.g, using 
results ranked lower than 20). This is because the 
lower ranked results are less relevant than the 
higher ranked results. They may contain more 
irrelevant or noisy annotated tokens than higher 
ranked documents; and hence using them may 
harm the precision of the structured annotations. 
Figure 4 also indicates that the quality of ranking 
and the accuracy of structured annotations are 
correlated. 
7 Conclusions 
In this paper, we studied the problem of improving 
web search ranking by incorporating structured 
annotation of queries. We proposed a systematic 
solution, first to generate structured annotation of 
queries based on top search results, and then 
launching two structured annotation based re-
ranking models. We performed a large-scale 
evaluation over 12,396 queries from a major search 
engine. The experiment results show that the F-
Measure of query structured annotation generated 
by our approach is as high as 91%. In the same 
dataset, our structured annotation based re-ranking 
model significantly outperforms the original ranker 
? the ranking of a major search engine, with 
improvements 5.2%. 
 
Acknowledgments 
This work was supported by National Natural Science 
Foundation of China (NSFC) via grant 61273321, 
61133012 and the Nation-al 863 Leading Technology 
Research Project via grant 2012AA011102. 
References  
G. Agarwal, G. Kabra, and K. C.-C. Chang. Towards 
rich query interpretation: walking back and forth for 
mining query templates. In Proc. of WWW '10. 
M. Bendersky, W. Bruce Croft and D. A. Smith. Joint 
Annotation of Search Queries, In Proc. of ACL-HLT 
2011. 
M. Bendersky, W. Bruce Croft and D. A. Smith. 
Structural Annotation of Search Queries Using 
Pseudo-Relevance Feedback, In Proc. Of CIKM 2010. 
S. Bergsma and Q. I. Wang. Learning noun phrase 
query segmentation. In Proceedings of EMNLP-
CoNLL'07. 
M. Bron, K. Balog, and M. de Rijke. Ranking related 
entities: components and analyses. In Proc. of 
CIKM ?10. 
C. Buckley. Automatic query expansion using SMART. 
InProc. of TREC-3, pages 69?80, 1995. 
C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, 
N. Hamilton, and G. Hullender. Learning to rank 
usinggradient descent. In Proceedings of ICML '05. 
T. Cheng, X. Yan, and K. C.-C. Chang. Supporting 
entity search: a large-scale prototype search engine. 
In Proc. of SIGMOD ?07. 
O. Etzioni, M. Banko, S. Soderland, and D.S. Weld, 
(2008). Open Information Extraction from the Web, 
Communications of the ACM, 51(12): 68-74. 
J. Guo, G. Xu, X. Cheng, and H. Li. Named entity 
recognition in query. In Proc. Of SIGIR? 2009. 
K. Jarvelin and J. Kekalainen. Ir evaluation methods for 
retrieving highly relevant documents. In SIGIR '00. 
J. Lafferty and C. Zhai, Document language models, 
query models, and risk minimization for information 
retrieval, In Proceedings of SIGIR'01, pages 111-119, 
2001. 
V. Lavrenko and W. B. Croft. Relevance based 
language models. In Proc. of SIGIR, pages 120?127, 
2001. 
Y. Li, BJP. Hsu, CX. Zhai and K. Wang. Unsupervised 
Query Segmentation Using Clickthrough for 
Information Retrieval. In Proc. of SIGIR'11. 
X. Li, Y.-Y. Wang, and A. Acero. Extracting structured 
information from user queries with semi-supervised 
conditional random fields. In Proc. of SIGIR'09. 
Y. Liu, X. Ni, J-T. Sun, Z. Chen. Unsupervised 
Transactional Query Classification Based on 
Webpage Form Understanding. In Proc. of CIKM '11. 
Y. Liu, M. Zhang, L. Ru, and S. Ma. Automatic query 
type identification based on click-through 
information. In LNCS, 2006. 
M. Pasca. Asking What No One Has Asked Before: 
Using Phrase Similarities To Generate Synthetic 
Web Search Queries. In Proc. of CIKM '11. 
G. Salton and C. Buckley. Improving retrieval 
performance by relevance feedback. Journal of the 
American Society for Information Science, 
41(4):288-297, 1990. 
477
N. Sarkas, S. Paparizos, and P. Tsaparas. Structured 
annotations of web queries. In Proc. of SIGMOD'10. 
I. Szpektor, A. Gionis, and Y. Maarek. Improving 
recommendation for long-tail queries via templates. 
In Proc. of WWW '11 
B. Tan and F. Peng. Unsupervised query segmentation 
using generative language models and wikipedia. In 
WWW?08. 
T.-L. Wong, W. Lam, and B. Chen. Mining 
employment market via text block detection and 
adaptive cross-domain information extraction. In 
Proc. SIGIR, pages 283?290, 2009. 
X. Yu and H. Shi. Query segmentation using 
conditional random fields. In Proceedings of KEYS 
'09. 
C. Zhai and J. Lafferty, Model-based feedback in the 
language modeling approach to information 
retrieval , In Proceedings of CIKM'01, pages 403-410, 
2001. 
C. Zhai and J. Lafferty, A study of smoothing methods 
for language models applied to ad hoc information 
retrieval, In Proceedings of SIGIR'01, pages 334-342, 
2001. 
Y. Zhai and B. Liu. Structured data extraction from the 
Web based on partial tree alignment. IEEE Trans. 
Knowl. Data Eng., 18(12):1614?1628, Dec. 2006. 
H. Zhao, W. Meng, Z. Wu, V. Raghavan, and C. Yu. 
Fully automatic wrapper generation for search 
engines. In Proceedings of WWW ?05. 
S. Zheng, R. Song, J.-R. Wen, and D. Wu. Joint 
optimization of wrapper generation and template 
detection. In Proc. of SIGKDD'07. 
478
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 863?868,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Microblog Entity Linking by Leveraging Extra Posts
Yuhang Guo, Bing Qin?, Ting Liu , Sheng Li
Research Center for Social Computing and Information Retrieval
School of Computer Science and Technology
Harbin Institute of Technology, China
{yhguo, bqin?, tliu, sli}@ir.hit.edu.cn
Abstract
Linking name mentions in microblog posts to
a knowledge base, namely microblog entity
linking, is useful for text mining tasks on mi-
croblog. Entity linking in long text has been
well studied in previous works. However few
work has focused on short text such as mi-
croblog post. Microblog posts are short and
noisy. Previous method can extract few fea-
tures from the post context. In this paper we
propose to use extra posts for the microblog
entity linking task. Experimental results show
that our proposed method significantly im-
proves the linking accuracy over traditional
methods by 8.3% and 7.5% respectively.
1 Introduction
Microblogging services (e.g. Twitter) are attracting
millions of users to share and exchange their ideas
and opinions. Millions of new microblog posts are
generated on such open broadcasting platforms ev-
ery day 1. Microblog provides a fruitful and instant
channel of global information publication and acqui-
sition.
A necessary step for the information acquisition
on microblog is to identify which entities a post is
about. Such identification can be challenging be-
cause the entity mention may be ambiguous. Let?s
begin with a real post from Twitter.
(1) No excuse for floods tax, says Abbott
URL
?Corresponding author
1See http://blog.twitter.com/2011/06/ 200-million-tweets-
per-day.html.
This post is about an Australia political lead-
er, Tony Abbot, and his opinion on flood tax
policy. To understand that this post mentions
Tony Abbot is not trivial because the name Ab-
bot can refer to many people and organization-
s. In the Wikipedia page of Abbott, there list-
s more than 20 Abbotts, such as baseball player
Jim Abbott, actor Bud Abbott and company
Abbott Laboratories, etc..
Given a knowledge base (KB) (e.g. Wikipedia),
entity linking is the task to identify the referent KB
entity of a target name mention in plain text. Most
current entity linking techniques are designed for
long text such as news/blog articles (Mihalcea and
Csomai, 2007; Cucerzan, 2007; Milne and Witten,
2008; Han and Sun, 2011; Zhang et al, 2011; Shen
et al, 2012; Kulkarni et al, 2009; Ratinov et al,
2011). Entity linking for microblog posts has not
been well studied.
Comparing with news/blog articles, microblog
posts are:
short each post contains no more than 140 charac-
ters;
fresh the new entity-related content may have not
been included in the knowledge base;
informal acronyms and spoken language writing
style are common.
Due to these properties, few feature can be ex-
tracted from a post. Without enough features, pre-
vious entity linking methods may fail. In order to
overcome the feature sparseness, we turn to another
property of microblog:
863
redundancy For each day, over 340M short mes-
sages are posted in twitter. Similar information
may be posted in different expressions.
For example, we find the following post,
(2) Julia Gillard and Tony Abbott on
the flood levy just after 8.30am on
@612brisbane!
The content of post (2) is highly related to post
(1). In contrast to the confusing post (1), the text
in post (2) explicitly indicates that the Abbott here
refers to the Australian political leader. This inspires
us to bridge the confusing post and the knowledge
base with other posts.
In this paper, we approach the microblog entity
linking by leveraging extra posts. A straightforward
method is to expand the post context with similar
posts, which we call Context-Expansion-based Mi-
croblog Entity Linking (CEMEL). In this method,
we first construct a query with the given post and
then search for it in a collection of posts. From the
search result, we select the most similar posts for the
context expansion. The disambiguation will benefit
from the extra posts if, hopefully, they are related
to the given post in content and include explicit fea-
tures for the disambiguation.
Furthermore, we propose a Graph-based Mi-
croblog Entity Linking (GMEL) method. In contrast
to CEMEL, the extra posts in GMEL are not directly
added into the context. Instead, they are represented
as nodes in a graph, and weighted by their similarity
with the target post. We use an iterative algorithm
in this graph to propagate the entity weights through
the edges between the post nodes.
We conduct experiments on real microblog da-
ta which we harvested from Twitter. Current enti-
ty linking corpus, such as the TAC-KBP data (M-
cNamee and Dang, 2009), mainly focuses on long
text. And few microblog entity linking corpus is
publicly available. In this work, we manually anno-
tated a microblog entity linking corpus. This corpus
inherit the target names from TAC-KBP2009. So it
is comparable with the TAC-KBP2009 corpus.
Experimental results show that the performance
of previous methods drops on microblog posts com-
paring with on long text. Both of CEMEL and
GMEL can significantly improve the performance
over baselines, which means that entity linking sys-
tem on microblog can be improved by leveraging ex-
tra posts. The results also show that GMEL outper-
forms CEMEL significantly.
We summarize our contributions as follows.
? We propose a context-expansion-based and a
graph-based method for microblog entity link-
ing by leveraging extra posts.
? We annotate a microblog entity linking corpus
which is comparable to an existing long text
corpus.
? We show the inefficiency of previous method
on the microblog corpus and our method can
significantly improve the results.
2 Task defination
The microblog entity linking task is that, for a name
mention in a microblog post, the system is to find the
referent entity of the name in a knowledge base, or
return a NIL mark if the entity is absence from the
knowledge base. This definition is close to the en-
tity linking task in the TAC-KBP evaluation (Ji and
Grishman, 2011) except for the context of the target
name is microblog post whereas in TAC-KBP the
context is news article or web log.
Several related tasks have been studied on mi-
croblog posts. In Meij et al (2012)?s work, they
link a post, rather than a name mention in the post,
to relevant Wikipedia concepts. Guo et al (2013a)
and Liu et al (2013) define entity linking as to first
detect all the mentions in a post and then link the
mentions to the knowledge base. In contrast, our
definition (as well as the TAC-KBP definition) fo-
cuses on a concerned name mention across different
posts/documents.
3 Method
A typical entity linking system can be broken down
into two steps:
candidate generation This step narrows down the
candidate entity range from any entity in the
world to a limited set.
candidate ranking This step ranks the candidates
and output the top ranked entity as the result.
864
Figure 1: An example of the GMEL graph. p1 . . . p4 are
post nodes and c1 . . . c3 are candidate entity nodes. Each
post node is connected to the corresponding candidate n-
odes from the knowledge base. The edges between the
nodes are weighted by the similarity between them.
In this paper, we use the candidate generation
method described in Guo et al(2013). For the candi-
date ranking, we use a Vector Space Model (VSM)
and a Learning to Rank (LTR) as baselines. VSM
is an unsupervised method and LTR is a supervised
method. Both of them have achieved the state-of-
the-art performances in the TAC-KBP evaluations.
The major challenge in microblog entity linking
is the lack of context in the post. An ideal solu-
tion is to expand the context with the posts which
contain the same entity. However, automatically
judging whether a name mention in two documents
refers to the same entity, namely cross document co-
reference, is not trivial. Here our solution is to rank
the posts by their possibility of co-reference to the
target one and select the most possible co-referent
posts for the expansion.
CEMEL is based on the assumption that, given a
name and two posts where the name is mentioned,
the higher similarity between the posts the high-
er possibility of their co-reference and that the co-
referent posts may contains useful features for the
disambiguation. However, two literally similar posts
may not be co-referent. If such non co-referent post
is expanded to the context, noises may be included.
Take the following post as an example.
(3) AG Abbott says that bullets have
crossed the border from Mexico to
Texas at least four times. URL
This post is similar to post (1) because they both
contains ?says? and ?URL?. But the Abbott in post
(3) refers to the Texas Attorney General Greg Ab-
bott. In this mean, the expanded context in post (3)
could mislead the disambiguation for post (1). Such
noise can be controlled by setting a strict number of
posts to expand the context or weighting the contri-
bution of this post to the target one.
Our CEMEL method consists of the following
steps: First we construct a query with the terms from
the target post. Second we search for the query in a
microblog post collection using a common informa-
tion retrieval model such as the vector space model.
Note that here we limit the searched posts must con-
tain the target name mention. Then we expand the
target post with top N similar posts and use a typical
entity linking method (such as VSM and LTR) with
the expanded context.
Figure 1 illustrates the graph of GMEL. Each n-
ode of this graph represents an candidate entity (e.g.
c1 . . . c3) or a post of the given target name (e.g.
p1 . . . p4) In this graph, each node represents an en-
tity or a post of the given target name. Between each
pair of post nodes, each pair of entity nodes and each
post node and its candidate entity nodes, there is an
edge. The edge is weighted by the similarity be-
tween the two linked nodes. Entity nodes are labeled
by themselves and candidate nodes are initialized as
unlabeled nodes. For the edges between post node
pairs and entity node pairs, we use cosine similari-
ty. For the edges between a post node and its can-
didate entity nodes, we use the score given by tra-
ditional entity linking methods. We use an iterative
algorithm on this graph to propagate the labels from
the entity nodes to the post nodes. We adapt Label
Propagation (LP) (Zhu and Ghahramani, 2002) and
Modified Adsorption (MAD) (Talukdar and Pereira,
2010) for the iteration over the graph.
4 Experiment
4.1 Data Annotation
Till now, few microblog entity linking data is pub-
licly available. In this work, we manually annotate
a data set on microblog posts2. We collect 15.6 mil-
lion microblog posts in Twitter dated from January
23 to February 8, 2011. In order to compare with ex-
isting entity linking on long text, we select a subset
of target names from TAC-KBP2009 and inherit the
knowledge base in the TAC-KBP evaluation. The
2We published this data so that researchers can reproduce
our results.
865
Figure 2: Percentage of the co-reference posts in the top
N similar posts
Figure 3: Impact of expansion post number in CEMEL
TAC-KBP2009 data set includes 513 target names.
We search for all the target names in the post col-
lection and get 26,643 matches. We randomly sam-
ple 120 posts for each of the top 30 most frequently
matched target names and filter out non-English and
overly short (i.e. less than 3 words) posts. Then
we get 2,258 posts for 25 target names and manual-
ly link the target name mentions in the posts to the
TAC-KBP knowledge base.
In order to evaluate the assumption in CEMEL:
similar posts tend to co-reference, we randomly s-
elect 10 posts for 5 target names respectively and
search for the posts in the post collection. From
the search result of each of the 50 posts, we select
the top 20 posts and manually annotate if they co-
reference with the query post.
4.2 Settings
We generate candidates with the method described
in (Guo et al, 2013b) and use Vector Space Mod-
el (VSM) (Varma et al, 2009) and Learning to Rank
(LTR) (Zheng et al, 2010) as the ranking model. We
Figure 4: Accuracy of GMEL with different rate of extra
post nodes
use Lucene and ListNet with default settings for the
VSM and LTR implementation respectively. We use
bigram feature for VSM and the feature set of (Chen
et al, 2011) for LTR. LTR is evaluated with 10-fold
cross validation. Given a target name, the GMEL
graph includes all the evaluation posts as well as a
set of extra post nodes searched from the post collec-
tion with the query of the target name. We filter out
determiners, interjections, punctuations, emoticon-
s, discourse markers and URLs in the posts with a
twitter part-of-speech tagger (Owoputi et al, 2013).
The similarity between a post and its candidate en-
tities is set with the score given by VSM or LTR
and the similarity between other nodes is set with the
corresponding cosine similarity. We employ junto3
with default settings for the iterative algorithm im-
plementation .
4.3 Results
Figure 2 shows the relationship between similari-
ty and co-reference. From this figure we can see
that the percentage decreases with the growth of N.
When the N is up to 10, about 60% of the similar
posts co-reference with the query post and the de-
crease speed slows down. The Pearson correlation
coefficient between the percentage and the number
of top N is -0.843, which shows a significant corre-
lation between the two variables (with p-value 0.01
under t-test).
Figure 3 shows the impact of the extra post num-
ber for the context expansion in CEMEL. We can see
that the accuracies of VSM and LTR are improved
3See https://github.com/parthatalukdar/junto
866
Figure 5: Label entropy of GMEL with different rate of
extra post nodes
Figure 6: Accuracy of the systems
by CEMEL. The improvements peak with 5-10 ex-
tra posts. Then more extra posts will pull down the
accuracy.
Figure 4 shows the accuracy of GMEL. The x-axis
is the rate of the extra post number over the evalu-
ation post number. We can see that the accuracy of
MAD increases with the number of extra post nodes
at first and then turns to be stable. The accuracy of
LP increases at first and drops when more extra posts
are added into the graph.
Figure 5 shows the information entropy of the la-
bels in LP and MAD. The curves show that the pre-
diction of LP tends to converge into a small number
of labels. This is because LP prefers smoothing la-
belings over the graph (Talukdar and Pereira, 2010).
We also evaluate our baselines on TAC-KBP2009
data set (LTR is trained on TAC-KBP2010 data set).
The accuracy of VSM and LTR are 0.8338 and
0.8372 respectively, which are comparable with the
state-of-the-art result (Hachey et al, 2013).
Figure 6 shows the performances of the systems
on the microblog data. We set the optimal expansion
post number of CEMEL and use MAD algorithm for
GMEL with all searched extra post nodes. From this
figure we can see that the results of VSM and LTR
baselines are comparable and both of them are sig-
nificantly lower than that on TAC-KBP2009 data.
CEMEL improves the VSM and LTR baselines by
4.3% and 2.7% respectively. GMEL improves VSM
and LTR by 8.3% and 7.5% respectively. The results
of GMEL are also significantly better than CEMEL.
All of the improvements are significant under Z-test
with p < 0.05.
5 Conclusion
In this paper we approach microblog entity linking
by leveraging extra posts. We propose a context-
expansion-based and a graph-based method. Exper-
imental results on our data set show that the per-
formance of traditional method drops on the mi-
croblog data. The graph-based method outperform-
s the context-expansion-based method and both of
them significantly improve the accuracy of tradition-
al methods. In the graph-based method the modified
adsorption algorithm performs better than the label
propagation algorithm.
Acknowledgments
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
61273321, 61073126, 61133012 and the National
863 Leading Technology Research Project via grant
2012AA011102. We would like to thank to Wanx-
iang Che, Ruiji Fu, Yanyan Zhao, Wei Song and
several anonymous reviewers for their constructive
comments and suggestions.
References
Zheng Chen, Suzanne Tamang, Adam Lee, and Heng Ji.
2011. A toolkit for knowledge base population. In
SIGIR, pages 1267?1268.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on Wikipedia data. In Proceedings
of the 2007 Joint Conference on Empirical Method-
s in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
708?716, Prague, Czech Republic, June. Association
for Computational Linguistics.
867
Stephen Guo, Ming-Wei Chang, and Emre Kiciman.
2013a. To link or not to link? a study on end-to-
end tweet entity linking. In Proceedings of the 2013
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 1020?1030, Atlanta, Geor-
gia, June. Association for Computational Linguistics.
Yuhang Guo, Bing Qin, Yuqin Li, Ting Liu, and Sheng
Li. 2013b. Improving candidate generation for entity
linking. In Elisabeth Mtais, Farid Meziane, Mohamad
Saraee, Vijayan Sugumaran, and Sunil Vadera, edi-
tors, Natural Language Processing and Information
Systems, volume 7934 of Lecture Notes in Computer
Science, pages 225?236. Springer Berlin Heidelberg.
Ben Hachey, Will Radford, Joel Nothman, Matthew Hon-
nibal, and James R. Curran. 2013. Evaluating en-
tity linking with wikipedia. Artificial Intelligence,
194(0):130 ? 150. ?ce:title?Artificial Intelligence,
Wikipedia and Semi-Structured Resources?/ce:title?.
Xianpei Han and Le Sun. 2011. A generative entity-
mention model for linking entities with knowledge
base. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Techologies, pages 945?954, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Heng Ji and Ralph Grishman. 2011. Knowledge base
population: Successful approaches and challenges. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 1148?1158, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and
Soumen Chakrabarti. 2009. Collective annotation
of wikipedia entities in web text. In Proceedings
of the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining, KDD ?09,
pages 457?466, New York, NY, USA. ACM.
Xiaohua Liu, Yitong Li, Haocheng Wu, Ming Zhou, Furu
Wei, and Yi Lu. 2013. Entity linking for tweets. In
Proceedings of the 51th Annual Meeting of the Asso-
ciation for Computational Linguistics. Association for
Computational Linguistics.
P. McNamee and H.T. Dang. 2009. Overview of
the tac 2009 knowledge base population track. In
Proceedings of the Second Text Analysis Conference
(TAC2009).
Edgar Meij, Wouter Weerkamp, and Maarten de Rijke.
2012. Adding semantics to microblog posts. In Pro-
ceedings of the fifth ACM international conference on
Web search and data mining, WSDM ?12, pages 563?
572, New York, NY, USA. ACM.
Rada Mihalcea and Andras Csomai. 2007. Wikify!: link-
ing documents to encyclopedic knowledge. In CIKM
?07: Proceedings of the sixteenth ACM conference on
Conference on information and knowledge manage-
ment, pages 233?242, New York, NY, USA. ACM.
David Milne and Ian H. Witten. 2008. Learning to link
with wikipedia. In CIKM ?08: Proceeding of the 17th
ACM conference on Information and knowledge man-
agement, pages 509?518, New York, NY, USA. ACM.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conver-
sational text with word clusters. In NAACL2013,
pages 380?390, Atlanta, Georgia, June. Association
for Computational Linguistics.
Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and global algorithms for dis-
ambiguation to wikipedia. In Proceedings of the 49th
Annual Meeting of the Association for Computation-
al Linguistics: Human Language Technologies, pages
1375?1384, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Wei Shen, Jianyong Wang, Ping Luo, and Min Wang.
2012. Linden: linking named entities with knowl-
edge base via semantic knowledge. In Proceedings of
the 21st international conference on World Wide We-
b, WWW ?12, pages 449?458, New York, NY, USA.
ACM.
Partha Pratim Talukdar and Fernando Pereira. 2010.
Experiments in graph-based semi-supervised learning
methods for class-instance acquisition. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 1473?1481, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Vasudeva Varma, Vijay Bharat, Sudheer Kovelamudi,
Praveen Bysani, Santosh GSK, Kiran Kumar N, Kran-
thi Reddy, Karuna Kumar, and Nitin Maganti. 2009.
Iiit hyderabad at tac 2009. In Proceedings of the Sec-
ond Text Analysis Conference (TAC 2009), Gaithers-
burg, Maryland, USA, November.
Wei Zhang, Yan Chuan Sim, Jian Su, and Chew Lim Tan.
2011. Entity linking with effective acronym expan-
sion, instance selection, and topic modeling. In Toby
Walsh, editor, IJCAI 2011, pages 1909?1914.
Zhicheng Zheng, Fangtao Li, Minlie Huang, and Xiaoyan
Zhu. 2010. Learning to link entities with knowledge
base. In NAACL2010, pages 483?491, Los Angeles,
California, June. Association for Computational Lin-
guistics.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learn-
ing from labeled and unlabeled data with label prop-
agation. Technical report, Technical Report CMU-
CALD-02-107, Carnegie Mellon University.
868
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1224?1234,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Exploiting Multiple Sources for Open-domain Hypernym Discovery
Ruiji Fu, Bing Qin, Ting Liu?
Research Center for Social Computing and Information Retrieval
School of Computer Science and Technology
Harbin Institute of Technology, China
{rjfu, bqin, tliu}@ir.hit.edu.cn
Abstract
Hypernym discovery aims to extract such
noun pairs that one noun is a hypernym of
the other. Most previous methods are based
on lexical patterns but perform badly on open-
domain data. Other work extracts hypernym
relations from encyclopedias but has limited
coverage. This paper proposes a simple yet ef-
fective distant supervision framework for Chi-
nese open-domain hypernym discovery. Giv-
en an entity name, we try to discover its hy-
pernyms by leveraging knowledge from mul-
tiple sources, i.e., search engine results, ency-
clopedias, and morphology of the entity name.
First, we extract candidate hypernyms from
the above sources. Then, we apply a statistical
ranking model to select correct hypernyms. A
set of novel features is proposed for the rank-
ing model. We also present a heuristic strate-
gy to build a large-scale noisy training data for
the model without human annotation. Exper-
imental results demonstrate that our approach
outperforms the state-of-the-art methods on a
manually labeled test dataset.
1 Introduction
Hypernym discovery is a task to extract such noun
pairs that one noun is a hypernym of the other (S-
now et al, 2005). A noun H is a hypernym of an-
other noun E if E is an instance or subclass of H. In
other word, H is a semantic class of E. For instance,
?actor? is a hypernym of ?Mel Gibson?; ?dog? is a
hypernym of ?Caucasian sheepdog?; ?medicine? is
a hypernym of ?Aspirin?. Hypernym discovery is
an important subtask of semantic relation extraction
?Email correspondence.
and has many applications in ontology construction
(Suchanek et al, 2008), machine reading (Etzion-
i et al, 2006), question answering (McNamee et al,
2008), and so on.
Some manually constructed thesauri such as
WordNet can also provide some semantic relations
such as hypernyms. However, these thesauri are lim-
ited in its scope and domain, and manual construc-
tion is knowledge-intensive and time-consuming.
Therefore, many researchers try to automatically ex-
tract semantic relations or to construct taxonomies.
Most previous methods on automatic hypernym
discovery are based on lexical patterns and suffer
from the problem that such patterns can only cov-
er a small part of complex linguistic circumstances
(Hearst, 1992; Turney et al, 2003; Zhang et al,
2011). Other work tries to extract hypernym rela-
tions from large-scale encyclopedias like Wikipedia
and achieves high precision (Suchanek et al, 2008;
Hoffart et al, 2012). However, the coverage is limit-
ed since there exist many infrequent and new entities
that are missing in encyclopedias (Lin et al, 2012).
We made similar observation that more than a half
of entities in our data set have no entries in the en-
cyclopedias.
This paper proposes a simple yet effective distan-
t supervision framework for Chinese open-domain
hypernym discovery. Given an entity name, our goal
is to discover its hypernyms by leveraging knowl-
edge from multiple sources. Considering the case
where a person wants to know the meaning of an un-
known entity, he/she may search it in a search engine
and then finds out the answer after going through the
search results. Furthermore, if he/she finds an en-
try about the entity in an authentic web site, such as
Wikipedia, the information will help him/her under-
1224
stand the entity. Also, the morphology of the enti-
ty name can provide supplementary information. In
this paper, we imitate the process. The evidences
from the above sources are integrated in our hyper-
nym discovery model.
Our approach is composed of two major steps:
hypernym candidate extraction and ranking. In the
first step, we collect hypernym candidates from mul-
tiple sources. Given an entity name, we search it in
a search engine and extract high-frequency nouns as
its main candidate hypernyms from the search re-
sults. We also collect the category tags for the entity
from two Chinese encyclopedias and the head word
of the entity as the candidates.
In the second step, we identify correct hypernyms
from the candidates. We view this task as a rank-
ing problem and propose a set of effective features
to build a statistical ranking model. For the param-
eter learning of the model, we also present a heuris-
tic strategy to build a large-scale noisy training data
without human annotation.
Our contributions are as follows:
? We are the first to discover hypernym for Chi-
nese open-domain entities by exploiting mul-
tiple sources. The evidences from different
sources can authenticate and complement each
other to improve both precision and recall.
? We manually annotate a dataset containing
1,879 Chinese entities and their hypernyms,
which will be made publicly available. To the
best of our knowledge, this is the first dataset
for Chinese hypernyms.
? We propose a set of novel and effective fea-
tures for hypernym ranking. Experimental re-
sults show that our method achieves the best
performance.
Furthermore, our approach can be easily ported
from Chinese to English and other languages, except
that a few language dependent features need to be
changed.
The remainder of the paper is organized as fol-
lows: Section 2 discusses the related work. Section
3 introduces our method in detail. Section 4 de-
scribes the experimental setup. Section 5 shows the
experimental results. Conclusion and future work
are presented in Section 6.
2 Related Work
Previous methods for hypernym discovery can be
summarized into two major categories, i.e., pattern-
based methods and encyclopedia-based methods.
Pattern-based methods make use of manually
or automatically constructed patterns to mine hyper-
nym relations from text corpora. The pioneer work
by Hearst (1992) finds that linking two noun phras-
es (NPs) via certain lexical constructions often im-
plies hypernym relations. For example, NP1 is a hy-
pernym of NP2 in the lexical pattern ?such NP1 as
NP2?. Similarly, succeeding researchers follow her
work and use handcrafted patterns to extract hyper-
nym pairs from corpora (Caraballo, 1999; Scott and
Dominic, 2003; Ciaramita and Johnson, 2003; Tur-
ney et al, 2003; Pasca, 2004; Etzioni et al, 2005;
Ritter et al, 2009; Zhang et al, 2011).
Evans (2004) considers the web data as a large
corpus and uses search engines to identify hyper-
nyms based on lexical patterns. Given an arbitrary
document, he takes each capitalized word sequence
as an entity and aims to find its potential hypernyms
through pattern-based web searching. Suppose X is
a capitalized word sequence. Some pattern queries
like ?such as X? are threw into the search engine.
Then, in the retrieved documents, the nouns that im-
mediately precede the pattern are recognized as the
hypernyms of X. This work is most related to ours.
However, the patterns used in his work are too strict
to cover many low-frequency entities, and our ex-
periments show the weakness of the method.
Snow et al (2005) for the first time propose to au-
tomatically extract large numbers of lexico-syntactic
patterns and then detect hypernym relations from
a large newswire corpus. First, they use some
known hypernym-hyponym pairs from WordNet as
seeds and collect many patterns from a syntactical-
ly parsed corpus in a bootstrapping way. Then, they
consider all noun pairs in the same sentence as po-
tential hypernym-hyponym pairs and use a statistical
classifier to recognize the correct ones. All patterns
corresponding to the noun pairs in the corpus are
fed into the classifier as features. Their method re-
lies on accurate syntactic parsers and it is difficult to
guarantee the quality of the automatically extracted
patterns. Our experiments show that their method is
inferior to ours.
1225
Encyclopedia-based methods extract hyper-
nym relations from encyclopedias like Wikipedia
(Suchanek et al, 2008; Hoffart et al, 2012). The
user-labeled information in encyclopedias, such as
category tags in Wikipedia, is often used to derive
hypernym relations.
In the construction of the famous ontology YA-
GO, Suchanek et al (2008) consider the title of each
Wikipedia page as an entity and the corresponding
category tags as its potential hypernyms. They ap-
ply a shallow semantic parser and some rules to dis-
tinguish the correct hypernyms. Heuristically, they
find that if the head of the category tag is a plural
word, the tag is most likely to be a correct hyper-
nym. However, this method cannot be used in Chi-
nese because of the lack of plurality information.
The method of Suchanek et al (2008) cannot han-
dle the case when the entity is absent in Wikipedia.
To solve this problem, Lin et al (2012) connect the
absent entities with the entities present in Wikipedia
sharing common contexts. They utilize the Freebase
semantic types to label the present entities and then
propagate the types to the absent entities. The Free-
base contains most of entities in Wikipedia and as-
signs them semantic types defined in advance. But
there are no such resources in Chinese.
Compared with previous work, our approach tries
to identify hypernyms from multiple sources. The
evidences from different sources can authenticate
and complement each other to improve both preci-
sion and recall. Our experimental results show the
effectiveness of our method.
3 Method
Our method is composed of two steps. First, we col-
lect candidate hypernyms from multiple sources for
a given entity. Then, a statistical model is built for
hypernym ranking based on a set of effective fea-
tures. Besides, we also present a heuristic strategy
to build a large-scale training data.
3.1 Candidate Hypernym Collection from
Multiple Sources
In this work, we collect potential hypernyms from
four sources, i.e., search engine results, two ency-
clopedias, and morphology of the entity name.
We count the co-occurrence frequency between
the target entities and other words in the returned
snippets and titles, and select top N nouns (or noun
phrases) as the main candidates. As the experiments
show, this method can find at least one hypernym
for 86.91% entities when N equals 10 (see Section
5.1). This roughly explains why people often can in-
fer semantic meaning of unknown entities after go-
ing through several search results.
Furthermore, the user-generated encyclopedia
category tags are important clues if the entity exist-
s in a encyclopedia. Thus we add these tags into
the candidates. In this work, we consider two Chi-
nese encyclopedias, Baidubaike and Hudongbaike1,
as hypernym sources.
In addition, the head words of entities are also
their hypernyms sometimes. For example, the head
word of ??2? (Emperor Penguin)? indicates
that it?s a kind of ?? (penguins)?. Thus we put
head words into the hypernym candidates. In Chi-
nese, head words are often laid after their modifiers.
Therefore, we try to segment a given entity. If it can
be segmented and the last word is a noun, we take
the last word as the head word. In our data set, the
head words of 41.35% entities are real hypernyms
(see Section 5.1).
We combine all of these hypernym candidates to-
gether as the input of the second stage. The final
coverage rate reaches 93.24%.
3.2 Hypernym Ranking
After getting the candidate hypernyms, we then
adopt a ranking model to determine the correct hy-
pernym. In this section, we propose several effective
features for the model. The model needs training da-
ta for learning how to rank the data in addition to
parameter setting. Considering that manually anno-
tating a large-scale hypernym dataset is costly and
time-consuming, we present a heuristic strategy to
collect training data. We compare three hypernym
ranking models on this data set, including Support
Vector Machine (SVM) with a linear kernel, SVM
with a radial basis function (RBF) kernel and Logis-
tic Regression (LR).
1Baidubaike (http://baike.baidu.com) and
Hudongbaike (http://www.baike.com) are two largest
Chinese encyclopedias containing more than 6.26 million and
7.87 million entries respectively, while Chinese Wikipedia
contains about 0.72 million entries until September, 2013.
1226
Feature Comment Value Range
Prior the prior probability of a candidate being a potential hypernym [0, 1]
Is Tag
whether a candidate is a category tag in the encyclopedia
page of the entity if it exists
0 or 1
Is Head whether a candidate is the head word of the entity 0 or 1
In Titles
some binary features based on the frequency of occurrence of
a candidate in the document titles in the search results
0 or 1
Synonyms
the ratio of the synonyms of the candidate in the candidate
list of the entity
[0, 1]
Radicals
the ratio of the radicals of characters in a candidate matched
with the last character of the entity
[0, 1]
Source Num the number of sources where the candidate is extracted 1, 2, 3, or 4
Lexicon the hypernym candidate itself and its head word 0 or 1
Table 1: The features for ranking
3.2.1 Features for Ranking
The features for hypernym ranking are shown in
Table 1. We illustrate them in detail in the following.
Hypernym Prior: Intuitively, different words
have different probabilities as hypernyms of some
other words. Some are more probable as hypernyms,
such as animal, plant and fruit. Some other words
such as sun, nature and alias, are not usually used
as hypernyms. Thus we use a prior probability to
express this phenomenon. The assumption is that if
the more frequent that a noun appears as category
tags, the more likely it is a hypernym. We extract
category tags from 2.4 million pages in Baidubaike,
and compute the prior probabilities prior(w) for a
word w being a potential hypernym using Equation
1. countCT (w) denotes the times a word appeared
as a category tag in the encyclopedia pages.
prior(w) =
countCT (w)
?
w? countCT (w
?)
(1)
In Titles: When we enter a query into a search
engine, the engine returns a search result list, which
contains document titles and their snippet text. The
distributions of hypernyms and non-hypernyms in ti-
tles are compared with that in snippets respectively
in our training data. We discover that the average
frequency of occurrence of hypernyms in titles is
15.60 while this number of non-hypernyms is only
5.18, while the difference in snippets is very small
(Table 2). Thus the frequency of candidates in titles
can be used as features. In this work the frequency
Avg. Frequency in
titles snippets
Hypernym 15.60 33.69
Non-Hypernym 5.18 30.61
Table 2: Distributions of candidate hypernyms in titles
and snippets
is divided into three cases: greater than 15.60, less
than 5.18, and between 5.18 and 15.60. Three binary
features are used to represent these cases.
Synonyms: If there exist synonyms of a candi-
date hypernym in the candidate list, the candidate is
probably correct answer. For example, when ???
(medicine)? and ??? (medicine)? both appear in
the candidate list of an entity, the entity is probably
a kind of medicine. We get synonyms of a candidate
from a Chinese semantic thesaurus ? Tongyi Cilin
(Extended) (CilinE for short)2 and compute the s-
core as a feature using Equation 2.
ratiosyn(h, le) =
countsyn(h, le)
len(le)
(2)
Given a hypernym candidate h of an entity e and
the list of all candidates le, we compute the ratio of
the synonyms of h in le. countsyn(h, le) denotes the
count of the synonyms of h in le. len(le) is the total
count of candidates.
2CilinE contains synonym and hypernym relations among
77 thousand words, which is manually organized as a hierarchy
of five levels.
1227
Radicals: Chinese characters are a form of
ideogram. By far, the bulk of Chinese characters
were created by linking together a character with a
related meaning and another character to indicate its
pronunciation. The character with a related meaning
is called radical. Sometimes, it is a important clue to
indicate the semantic class of the whole character.
For example, the radical ??? means insects, so it
hints ?|n (dragonfly)? is a kind of insects. Simi-
larly ??? hints ?nJ (lymphoma)? is a kind of
diseases. Thus we use radicals as a feature the value
of which is computed by using Equation 3.
radical(e, h) =
countRM (e, h)
len(h)
(3)
Here radical(e, h) denotes the ratio of characters
radical-matched with the last character of the entity
e in the hypernym h. countRM (e, h) denotes the
count of the radical-matched characters in h. len(h)
denotes the total count of the characters in h.
3.2.2 Training Data Collection
Now training data is imperative to learn the
weights of the features in Section 3.2.1. Hence, we
propose a heuristic strategy to collect training data
from encyclopedias.
Firstly, we extract a number of open-domain enti-
ties from encyclopedias randomly. Then their hyper-
nym candidates are collected by using the method
proposed in Section 3.1. We select positive training
instances following two principles:
? Principle 1: Among the four sources used for
candidate collection, the more sources from
which the hypernym candidate is extracted, the
more likely it is a correct one.
? Principle 2: The higher the prior of the candi-
date being a hypernym is, the more likely it is a
correct one.
We select the best candidates following Principle
1 and then select the best one in them as a positive
instance following Principle 2. And we select a can-
didate as a negative training instance when it is from
only one source and its prior is the lowest. If there
are synonyms of training instances in the candidates
list, the synonyms are also extended into the training
set.
Domain
# of entities
Dev. Test
Biology 72 351
Health Care 61 291
Food 75 303
Movie 51 204
Industry 56 224
Others 35 136
Total 350 1529
Table 3: The evaluation data
In this way, we collect training data automatically,
which are used to learn the feature weights of the
ranking models.
4 Experimental Setup
In this work, we use Baidu3 search engine, the most
popular search engine for Chinese, and get the top
100 search results for each entity. The Chinese seg-
mentation, POS tagging and dependency parsing is
provided by an open-source Chinese language pro-
cessing platform LTP4 (Che et al, 2010).
4.1 Experimental Data
In our experiments, we prepare open-domain enti-
ties from dictionaries in wide domains, which are
published by a Chinese input method editor soft-
ware Sogou Pinyin5. The domains include biology,
health care, food, movie, industry, and so on. We
sample 1,879 entities from these domain dictionaries
and randomly split them into 1/5 for developmen-
t and 4/5 for test (Table 3). We find that only 865
(46.04%) entities exist in Baidubaike or Hudong-
baike. Then we extract candidate hypernyms for the
entities and ask two annotators to judge each hyper-
nym relation pair true or false manually. A pair (E,
H) is annotated as true if the annotators judge ?E is a
(or a kind of) H? is true. Finally, we get 12.53 candi-
date hypernyms for each entity on average in which
about 2.09 hypernyms are correct. 4,330 hypernym
relation pairs are judged by both the annotators. We
measure the agreement of the judges using the Kap-
pa coefficient (Siegel and Castellan Jr, 1988). The
3http://www.baidu.com
4http://ir.hit.edu.cn/demo/ltp/
5http://pinyin.sogou.com/dict/
1228
0 5 10 15 20
0.2
0.4
0.6
0.8
1.0
Top N
Cove
rage 
Rate
SRNSRN + ET+HW
Figure 1: Effect of candidate hypernym coverage rate
while varying N
Kappa value is 0.79.
Our training data, containing 11,481 positive in-
stances and 18,378 negative ones, is extracted from
Baidubaike and Hudongbaike using the heuristic s-
trategy proposed in Section 3.2.2.
4.2 Experimental Metrics
The evaluation metrics for our task include:
Coverage Rate: We evaluate coverage rate of the
candidate hypernyms. Coverage rate is the number
of entities for which at least one correct hypernym is
found divided by the total number of all entities.
Precision@1: Our method returns a ranked list
of hypernyms for each entity. We evaluate precision
of top-1 hypernyms (the most probable ones) in the
ranked lists, which is the number of correct top-1
hypernyms divided by the number of all entities.
R-precision: It is equivalent to Precision@R
where R is the total number of candidates labeled
as true hypernyms of an entity.
Precision, Recall, and F-score: Besides, we can
convert our ranking models to classification models
by setting thresholds. Varying the thresholds, we can
get different precisions, recalls, and F-scores.
5 Results and Analysis
5.1 The Coverage of Candidate Hypernyms
In this section, we evaluate the coverage rate of the
candidate hypernyms. We check the candidate hy-
pernyms of the whole 1,879 entities in the develop-
ment and test sets and see how many entities we can
collect at least one correct hypernym for.
Source
Coverage
Rate
Avg. #
SR10 0.8691 9.44?
ET 0.3938 3.07
HW 0.4135 0.87?
SR10 + ET 0.8909 12.02
SR10 + HW 0.9117 9.75
ET + HW 0.7073 3.92
SR10 + ET + HW 0.9324 12.53
Table 4: Coverage evaluation of the candidate hypernym
extraction
There are four different sources to collect candi-
dates as described in Section 3.1, which can be di-
vided into three kinds: search results (SR for short),
encyclopedia tags (ET) and head words (HW). For
SR, we select top N frequent nouns (SRN ) in the
search results of an entity as its hypernym candi-
dates. The effect of coverage rate while varying N
is shown in Figure 1. As we can see from the fig-
ure, the coverage rate is improved significantly by
increasing N until N reaches 10. After that, the
improvement becomes slight. When the candidates
from all sources are merged, the coverage rate is fur-
ther improved.
Thus we set N as 10 in the remaining experi-
ments. The detail evaluation is shown in Table 4.
We can see that top 10 frequent nouns in the search
results contain at least one correct hypernym for
86.91% entities in our data set. This coincides with
the intuition that people usually can infer the seman-
tic classes of unknown entities by searching them in
web search engines.
The coverage rate of ET merely reaches 39.38%.
We find the reason is that more than half of the enti-
ties have no encyclopedia pages. The average num-
ber of candidate hypernyms from ET is 3.07. Note
that the number is calculated among all the enti-
ties. We also calculate the average number only for
the present entities in encyclopedias. The number
reaches 6.68. The reason is that for many present en-
tities, the category tags include not only hypernyms
?For some of entities are rare, there may be less than 10
nouns in the search results. So the average count of candidates
is less than 10.
?Not all of the entities can be segmented. We cannot get the
head words of the ones that cannot be segmented.
1229
Method
Present Entities Absent Entities All Entities
P@1 R-Prec P@1 R-Prec P@1 R-Prec
MPattern 0.5542 0.4937 0.4306 0.3638 0.5229 0.4608
MSnow 0.3199 0.2592 0.2827 0.2610 0.3092 0.2597
MPrior 0.7339 0.5483 0.3940 0.3531 0.5494 0.4423
MSVM?linear 0.8569 0.6899 0.6157 0.5837 0.7260 0.6322
MSVM?rbf 0.8484 0.6940 0.6241 0.5901 0.7266 0.6376
MLR 0.8612 0.7052 0.6807 0.6258 0.7632 0.6621
Table 5: Precision@1 and R-Precision results on the test set. Here the present entities mean the entities existing in the
encyclopedias. The absent entities mean the ones not existing in the encyclopedias.
but also related words. For example, ??.|?
% (Bradley Center)? in Baidubaike have 5 tags, i.e.,
?NBA?, ?N? (sports)?, ?N?$? (sports)?, ?;
? (basketball)?, and ?|, (arena)?. Among them,
only ?|, (arena)? is a proper hypernym whereas
the others are some related words indicating mere-
ly thematic vicinity. Comparing the results of SR10
and SR10 + ET, we can see that collecting candidates
from ET can improve coverage, although many in-
correct candidates are added in at the same time.
The HW source provides 0.87 candidates on av-
erage with 41.35% coverage rate. That is to say, for
these entities, people can infer the semantic classes
when they see the surface lexicon.
At last, we combine the candidates from all of the
three sources as the input of the ranking methods.
The coverage rate reaches 93.24%.
We also compare with the manually construct-
ed semantic thesaurus CilinE mentioned in Section
3.2.1. Only 29 entities exist in CilinE (coverage rate
is only 1.54%). That is why we try to automatically
extract hypernym relations.
5.2 Evaluation of the Ranking
5.2.1 Overall Performance Comparison
In this section, we compare our proposed methods
with other methods. Table 5 lists the performance
measured by precision at rank 1 and R-precision of
some key methods. The precision-recall curves of
all the methods are shown in Figure 2. Table 7 lists
the maximum F-scores.
MPattern refers to the pattern-based method of
Hearst (1992). We craft Chinese Hearst-style
patterns (Table 6), in which E represents an entity
and H represents one of its hypernyms. Following
Pattern Translation
E?(??/??) H E is a (a kind of) H
E (!) H E(,) and other H
H (?)(?) E H(,) called E
H (?)(?)X E H(,) such as E
H (?)AO? E H(,) especially E
Table 6: Chinese Hearst-style lexical patterns
Evans (2004), we combine each pattern and each en-
tity and submit them into the Baidu search engine.
For example, for an entity E, we search ?E ??
? (E is a)?, ?E  (E and other)?, and so on. We
select top 100 search results of each query and get
1,285,209 results in all for the entities in the test set.
Then we use the patterns to extract hypernyms from
the search results. The result shows that 508 cor-
rect hypernyms are extracted for 568 entities (1,529
entities in total). Only a small part of the entities
can be extracted hypernyms for. This is mainly be-
cause only a few hypernym relations are expressed
in these fixed patterns in the web, and many ones are
expressed in more flexible manners. The hypernyms
are ranked based on the count of evidences where
the hypernyms are extracted.
MSnow is the method originally proposed by S-
now et al (2005) for English but we adapt it for Chi-
nese. We consider the top 100 search results for each
known hypernym-hyponym pairs as a corpus to ex-
tract lexico-syntactic patterns. Then, an LR classi-
fier is built based on this patterns to recognize hy-
pernym relations. This method considers all noun-
s co-occurred with the focused entity in the same
sentences as candidate hypernyms. So the number
of candidates is huge, which causes inefficiency. In
1230
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
Precision?Recall Curves on the Test Set
Recall
Prec
ision
l l l l llll l llll ll l l l l ll l l ll ll lll ll llllll
l
MSnowMPriorMSVM?l inearMSVM?rbfMLRMPatternMHeurist ic
Figure 2: Precision-Recall curves on the test set
our corpus, there are 652,181 candidates for 1,529
entities (426.54 for each entity on average), most of
which are not hypernyms. One possible reason is
that this method relies on an accurate syntactic pars-
er and it is difficult to guarantee the quality of the
automatically extracted patterns. Even worse, the
low quality of the language in the search results may
make this problem more serious.
MPrior refers to the ranking method based on on-
ly the prior of a candidate being a hypernym. As
Table 5 shows, it outperforms MSnow and achieves
comparable results with MPattern on Precision@1
and R-Precision.
Based on the features proposed in Section 3.2.1,
we train several statistical models based on SVM
and LR on the training data. MSVM?linear and
MSVM?rbf refer to the SVM models based on linear
kernels and RBF kernels respectively. MLR refers
to the LR model. The probabilities6 output by the
models are used to rank the candidate hypernyms.
All of the parameters which need to be set in the
models are selected on the development set. Table
5 shows the best models based on each algorithm.
These supervised models outperform the previous
methods. MLR achieves the best performance.
The precision-recall plot of the methods on the
test set is presented in Figure 2. MHeuristic refers
to the heuristic approach, proposed in Section 3.2.2,
to collect training data. Because this method cannot
6The output of an SVM is the distance from the decision
hyper-plane. Sigmoid functions can be used to convert this un-
calibrated distance into a calibrated posterior probability (Platt,
1999).
Method Max. F-score
MPattern 0.2061
MSnow 0.1514
MHeuristic 0.2803
MPrior 0.5591
MSVM?linear 0.5868
MSVM?rbf 0.6014
MLR 0.5998
Table 7: Summary of maximum F-score on the test set
Feature P@1 R-Prec
Max.
F-score
All 0.7632 0.6621 0.5998
? Prior 0.7534 0.6546 0.5837
? Is Tag 0.6965 0.6039 0.5605
? Is Head 0.7018 0.6036 0.5694
? In Titles 0.7436 0.6513 0.5868
? Synonyms 0.7495 0.6493 0.5831
? Radicals 0.7593 0.6584 0.5890
? Source Num 0.7364 0.6556 0.5984
? Lexicon 0.7377 0.6422 0.5851
? Source Info 0.6128 0.5221 0.5459
Table 8: Performance of LR models with different fea-
tures on the test set
provide ranking information, it is not listed in Ta-
ble 5. For fair comparison of R-precision and recall,
we add the extra correct hypernyms from MPattern
and MSnow to the test data set. The models based
on SVM and LR still perform better than the other
methods. MPattern and MSnow suffer from low re-
call and precision. MHeuristic get a high precision
but a low recall, because it can only deal with a part
of entities appearing in encyclopedias. The preci-
sion of MHeuristic reflects the quality of our training
data. We summarize the maximum F-score of dif-
ferent methods in Table 7.
5.2.2 Feature Effect
Table 8 shows the impact of each feature on the
performance of LR models. When we remove any
one of the features, the performance is degraded
more or less. The most effective features are Is Tag
and Is Head. The last line in Table 8 shows the
performance when we remove all features about
the source information, i.e., Is Tag, Is Head, and
1231
Entity
Top-1
Hypernym
Entity
Top-1
Hypernym
??b??(cefoperazone sodium) ??(drug) ?y(bullet tuna) ~a(fish)
???(finger citron rolls) ?(snack) =?(zirconite) ??(ore)
E????(The Avengers) >K(movie) ?|?d?(Felixstowe) l?(port)
@U=(mastigium) ?O(datum) ?!?(coxal cavity) ??(plant)
?UX?=?s
(Ethanolamine phosphotransferase)
)?
(organism)
?u
(coma)
?
(knowledge)
Table 10: Examples of entity-hypernym pairs extracted by MLR
Domain P@1 R-Prec
Max.
F-score
Biology 0.8165 0.7203 0.6424
Health Care 0.7354 0.5962 0.6061
Food 0.7450 0.6634 0.6938
Movie 0.9310 0.8069 0.7031
Industry 0.6286 0.5841 0.4624
Others 0.6324 0.4936 0.4318
Table 9: Performance of MLR in various domains
Source Num. The performance is degraded sharply.
This indicates the importance of the source informa-
tion for hypernym ranking.
5.2.3 The Performance in Each Domain
In this section, we evaluate the performance of
MLR method in various domains. We can see from
Table 9 that the performance in movie domain is best
while the performance in industry domain is worst.
That is because the information about movies is
abundant on the web. Furthermore, most of movies
have encyclopedia pages. It is easy to get the hy-
pernyms. In contrast, the entities in industry domain
are more uncommon. On the whole, our method is
robust for different domains. In Table 10, some in-
stances in various domains are presented.
5.3 Error Analysis
The uncovered entities7 and the false positives8 are
analyzed after the experiments. Some error exam-
ples are shown in Table 10 (in red font).
7Uncovered entities are entities which we do not collect any
correct hypernyms for in the first step.
8False positives are hypernyms ranked at the first places, but
actually are not correct hypernyms.
Uncovered entities: About 34% of the errors are
caused by uncovered entities. It is found that many
of the uncovered entities are rare entities. Nearly
36% of them are very rare and have only less than
100 search results in all. When we can?t get enough
information of an unknown entity from the search
engine, it?s difficult to know its semantic meaning,
such as ?@U= (mastigium)?, ??!? (coxal cav-
ity)?, ??u (coma)?. The identification of their hy-
pernyms requires more human-crafted knowledge.
The ranking models we used are unable to select
them, as the true synonyms are often below rank 10.
False positives: The remained 66% errors are
false positives. They are mainly owing to the
fact that some other related words in the candi-
date lists are more likely hypernyms. For exam-
ple, ?)? (organism)? is wrongly recognized as
the most probable hypernym of ??UX?=
?s (Ethanolamine phosphotransferase)?, because
the entity often co-occurs with word ?)? (organ-
ism)? and the latter is often used as a hypernym of
some other entities. The correct hypernyms actu-
ally are ?s (enzyme)?, ?z??? (chemical sub-
stance)?, and so on.
6 Conclusion
This paper proposes a novel method for finding
hypernyms of Chinese open-domain entities from
multiple sources. We collect candidate hypernyms
with wide coverage from search results, encyclope-
dia category tags and the head word of the entity.
Then, we propose a set of features to build statisti-
cal models to rank the candidate hypernyms on the
training data collected automatically. In our exper-
iments, we show that our method outperforms the
state-of-the-art methods and achieves the best preci-
1232
sion of 76.32% on a manually labeled test dataset.
All of the features which we propose are effective,
especially the features of source information. More-
over, our method works well in various domains, e-
specially in the movie and biology domains. We al-
so conduct detailed analysis to give more insights
on the error distribution. Except some language de-
pendent features, our approach can be easily trans-
fered from Chinese to other languages. For future
work, we would like to explore knowledge from
more sources to enhance our model, such as seman-
tic thesauri and infoboxes in encyclopedias.
Acknowledgments
This work was supported by National Natu-
ral Science Foundation of China (NSFC) via
grant 61133012, 61073126 and the National 863
Leading Technology Research Project via grant
2012AA011102. Special thanks to Zhenghua Li,
Wanxiang Che, Wei Song, Yanyan Zhao, Yuhang
Guo and the anonymous reviewers for insightful
comments and suggestions. Thanks are also due to
our annotators Ni Han and Zhenghua Li.
References
Sharon A. Caraballo. 1999. Automatic construction of a
hypernym-labeled noun hierarchy from text. In Pro-
ceedings of the 37th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 120?126,
College Park, Maryland, USA, June.
Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp:
A chinese language technology platform. In Coling
2010: Demonstrations, pages 13?16, Beijing, China,
August.
Massimiliano Ciaramita and Mark Johnson. 2003. Su-
persense tagging of unknown nouns in wordnet. In
Proceedings of the 2003 conference on Empirical
methods in natural language processing, pages 168?
175.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S Weld, and Alexander Yates. 2005. Unsuper-
vised named-entity extraction from the web: An exper-
imental study. Artificial Intelligence, 165(1):91?134.
Oren Etzioni, Michele Banko, and Michael J Cafarella.
2006. Machine reading. In AAAI, volume 6, pages
1517?1519.
Richard Evans. 2004. A framework for named enti-
ty recognition in the open domain. Recent Advances
in Natural Language Processing III: Selected Papers
from RANLP 2003, 260:267?274.
Marti A Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of the
14th conference on Computational linguistics-Volume
2, pages 539?545.
Johannes Hoffart, Fabian M Suchanek, Klaus Berberich,
and Gerhard Weikum. 2012. Yago2: a spatially and
temporally enhanced knowledge base from wikipedia.
Artificial Intelligence, pages 1?63.
Thomas Lin, Mausam, and Oren Etzioni. 2012. No noun
phrase left behind: Detecting and typing unlinkable
entities. In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 893?903, Jeju Island, Korea, July.
Paul McNamee, Rion Snow, Patrick Schone, and James
Mayfield. 2008. Learning named entity hyponyms
for question answering. In Proceedings of the Third
International Joint Conference on Natural Language
Processing, pages 799?804.
Marius Pasca. 2004. Acquisition of categorized named
entities for web search. In Proceedings of the thir-
teenth ACM international conference on Information
and knowledge management, pages 137?145.
John Platt. 1999. Probabilistic outputs for support vec-
tor machines and comparisons to regularized likeli-
hood methods. Advances in large margin classifiers,
10(3):61?74.
Alan Ritter, Stephen Soderland, and Oren Etzioni. 2009.
What is this, anyway: Automatic hypernym discovery.
In Proceedings of the 2009 AAAI Spring Symposium
on Learning by Reading and Learning to Read, pages
88?93.
Cederberg Scott and Widdows Dominic. 2003. Using lsa
and noun coordination information to improve the pre-
cision and recall of automatic hyponymy extraction. In
Proceedings of the seventh conference on Natural lan-
guage learning at HLT-NAACL 2003-Volume 4, pages
111?118.
Sidney Siegel and N John Castellan Jr. 1988. Nonpara-
metric statistics for the behavioral sciences. McGraw-
Hill, New York.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Lawrence K. Saul, Yair Weiss, and Le?on
Bottou, editors, Advances in Neural Information Pro-
cessing Systems 17, pages 1297?1304.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. Yago: A large ontology from
wikipedia and wordnet. Web Semantics: Science, Ser-
vices and Agents on the World Wide Web, 6(3):203?
217.
1233
Peter Turney, Michael L Littman, Jeffrey Bigham, and
Victor Shnayder. 2003. Combining independent mod-
ules to solve multiple-choice synonym and analogy
problems. In Proceedings of the International Con-
ference RANLP-2003, pages 482?489.
Fan Zhang, Shuming Shi, Jing Liu, Shuqi Sun, and Chin-
Yew Lin. 2011. Nonlinear evidence fusion and prop-
agation for hyponymy relation mining. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 1159?1168, Portland, Oregon, USA,
June.
1234
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 477?487,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
A Joint Segmentation and Classification Framework
for Sentiment Analysis
Duyu Tang
\?
, Furu Wei
?
, Bing Qin
\
, Li Dong
]?
, Ting Liu
\
, Ming Zhou
?
\
Research Center for Social Computing and Information Retrieval,
Harbin Institute of Technology, China
?
Microsoft Research, Beijing, China
]
Beihang University, Beijing, China
\
{dytang, qinb, tliu}@ir.hit.edu.cn
?
{fuwei, mingzhou}@microsoft.com
]
donglixp@gmail.com
Abstract
In this paper, we propose a joint segmenta-
tion and classification framework for sen-
timent analysis. Existing sentiment clas-
sification algorithms typically split a sen-
tence as a word sequence, which does not
effectively handle the inconsistent senti-
ment polarity between a phrase and the
words it contains, such as ?not bad? and
?a great deal of ?. We address this issue
by developing a joint segmentation and
classification framework (JSC), which si-
multaneously conducts sentence segmen-
tation and sentence-level sentiment classi-
fication. Specifically, we use a log-linear
model to score each segmentation candi-
date, and exploit the phrasal information
of top-ranked segmentations as features to
build the sentiment classifier. A marginal
log-likelihood objective function is de-
vised for the segmentation model, which
is optimized for enhancing the sentiment
classification performance. The joint mod-
el is trained only based on the annotat-
ed sentiment polarity of sentences, with-
out any segmentation annotations. Experi-
ments on a benchmark Twitter sentimen-
t classification dataset in SemEval 2013
show that, our joint model performs com-
parably with the state-of-the-art methods.
1 Introduction
Sentiment classification, which classifies the senti-
ment polarity of a sentence (or document) as posi-
tive or negative, is a major research direction in the
field of sentiment analysis (Pang and Lee, 2008;
Liu, 2012; Feldman, 2013). Majority of existing
approaches follow Pang et al. (2002) and treat sen-
?
This work was partly done when the first and fourth
authors were visiting Microsoft Research.
timent classification as a special case of text cate-
gorization task. Under this perspective, previous
studies typically use pipelined methods with two
steps. They first produce sentence segmentation-
s with separate text analyzers (Choi and Cardie,
2008; Nakagawa et al., 2010; Socher et al., 2013b)
or bag-of-words (Paltoglou and Thelwall, 2010;
Maas et al., 2011). Then, feature learning and sen-
timent classification algorithms take the segmenta-
tion results as inputs to build the sentiment classi-
fier (Socher et al., 2011; Kalchbrenner et al., 2014;
Dong et al., 2014).
The major disadvantage of a pipelined method
is the problem of error propagation, since sen-
tence segmentation errors cannot be corrected by
the sentiment classification model. A typical kind
of error is caused by the polarity inconsistency be-
tween a phrase and the words it contains, such
as ?not bad, bad? and ?a great deal of, great?.
The segmentations based on bag-of-words or syn-
tactic chunkers are not effective enough to han-
dle the polarity inconsistency phenomenons. The
reason lies in that bag-of-words segmentations re-
gard each word as a separate unit, which losses
the word order and does not capture the phrasal
information. The segmentations based on syntac-
tic chunkers typically aim to identify noun group-
s, verb groups or named entities from a sentence.
However, many sentiment indicators are phrases
constituted of adjectives, negations, adverbs or id-
ioms (Liu, 2012; Mohammad et al., 2013a), which
are splitted by syntactic chunkers. Besides, a bet-
ter approach would be to utilize the sentiment in-
formation to improve the segmentor. Accordingly,
the sentiment-specific segmentor will enhance the
performance of sentiment classification in turn.
In this paper, we propose a joint segmentation
and classification framework (JSC) for sentimen-
t analysis, which simultaneous conducts sentence
segmentation and sentence-level sentiment clas-
sification. The framework is illustrated in Fig-
477
Segmentations Input 
that is not bad 
that  is  not  bad 
that  is  not  bad 
that  is  not  bad 
that  is  not  bad 
Polarity: +1 
-1 
-1 
+1 
+1 
<+1,-1>   NO 
Polarity Update 
<+1,-1>   NO 
<+1,+1>  YES 
<+1,+1>  YES 
SC SEG CG 
Update 
SC 
2.3  
1.6 
0.6 
0.4 
0.6 
0.4 
2.3 
1.6 
SEG 
Rank 
Top K  
Figure 1: The joint segmentation and classification framework (JSC) for sentiment classification. CG
represents the candidate generation model, SC means the sentiment classification model and SEG stands
for the segmentation ranking model. Down Arrow means the use of a specified model, and Up Arrow
indicates the update of a model.
ure 1. We develop (1) a candidate generation mod-
el to generate the segmentation candidates of a
sentence, (2) a segmentation ranking model to s-
core each segmentation candidate of a given sen-
tence, and (3) a classification model to predic-
t the sentiment polarity of each segmentation. The
phrasal information of top-ranked candidates from
the segmentation model are utilized as features to
build the sentiment classifier. In turn, the predict-
ed sentiment polarity of segmentation candidates
from classification model are leveraged to update
the segmentor. We score each segmentation can-
didate with a log-linear model, and optimize the
segmentor with a marginal log-likelihood objec-
tive. We train the joint model from sentences an-
notated only with sentiment polarity, without any
segmentation annotations.
We evaluate the effectiveness of our joint mod-
el on a benchmark Twitter sentiment classifica-
tion dataset in SemEval 2013. Results show that
the joint model performs comparably with state-
of-the-art methods, and consistently outperforms
pipeline methods in various experiment settings.
The main contributions of the work presented in
this paper are as follows.
? To our knowledge, this is the first work that
automatically produces sentence segmenta-
tion for sentiment classification within a joint
framework.
? We show that the joint model yields com-
parable performance with the state-of-the-art
methods on the benchmark Twitter sentiment
classification datasets in SemEval 2013.
2 Related Work
Existing approaches for sentiment classification
are dominated by two mainstream directions.
Lexicon-based approaches (Turney, 2002; Ding
et al., 2008; Taboada et al., 2011; Thelwall et
al., 2012) typically utilize a lexicon of sentiment
words, each of which is annotated with the sen-
timent polarity or sentiment strength. Linguis-
tic rules such as intensifications and negations are
usually incorporated to aggregate the sentimen-
t polarity of sentences (or documents). Corpus-
based methods treat sentiment classification as a
special case of text categorization task (Pang et al.,
2002). They mostly build the sentiment classifier
from sentences (or documents) with manually an-
notated sentiment polarity or distantly-supervised
corpora collected by sentiment signals like emoti-
cons (Go et al., 2009; Pak and Paroubek, 2010;
Kouloumpis et al., 2011; Zhao et al., 2012).
Majority of existing approaches follow Pang et
al. (2002) and employ corpus-based method for
sentiment classification. Pang et al. (2002) pi-
oneer to treat the sentiment classification of re-
views as a special case of text categorization prob-
lem and first investigate machine learning meth-
ods. They employ Naive Bayes, Maximum En-
tropy and Support Vector Machines (SVM) with a
diverse set of features. In their experiments, the
best performance is achieved by SVM with bag-
of-words feature. Under this perspective, many s-
tudies focus on designing or learning effective fea-
tures to obtain better classification performance.
On movie or product reviews, Wang and Man-
ning (2012) present NBSVM, which trades-off
478
between Naive Bayes and NB-feature enhanced
SVM. Kim and Zhai (2009) and Paltoglou and
Thelwall (2010) learn the feature weights by in-
vestigating variants weighting functions from In-
formation Retrieval. Nakagawa et al. (2010) uti-
lize dependency trees, polarity-shifting rules and
conditional random fields (Lafferty et al., 2001)
with hidden variables to compute the documen-
t feature. On Twitter, Mohammad et al. (2013b)
develop a state-of-the-art Twitter sentiment classi-
fier in SemEval 2013, using a variety of sentiment
lexicons and hand-crafted features.
With the revival of deep learning (representa-
tion learning (Hinton and Salakhutdinov, 2006;
Bengio et al., 2013; Jones, 2014)), more recen-
t studies focus on learning the low-dimensional,
dense and real-valued vector as text features for
sentiment classification. Glorot et al. (2011) inves-
tigate Stacked Denoising Autoencoders to learn
document vector for domain adaptation in sen-
timent classification. Yessenalina and Cardie
(2011) represent each word as a matrix and
compose words using iterated matrix multipli-
cation. Socher et al. propose Recursive Au-
toencoder (RAE) (2011), Matrix-Vector Recursive
Neural Network (MV-RNN) (2012) and Recur-
sive Neural Tensor Network (RNTN) (2013b) to
learn the composition of variable-length phrases
based on the representation of its children. To
learn the sentence representation, Kalchbrenner et
al. (2014) exploit Dynamic Convolutional Neu-
ral Network and Le and Mikolov (2014) inves-
tigate Paragraph Vector. To learn word vectors
for sentiment analysis, Maas et al. (2011) propose
a probabilistic document model following Blei et
al. (2003), Labutov and Lipson (2013) re-embed
words from existing word embeddings and Tang
et al. (2014b) develop three neural networks to
learn word vectors from tweets containing posi-
tive/negative emoticons.
Unlike most previous corpus-based algorithms
that build sentiment classifier based on splitting a
sentence as a word sequence, we produce sentence
segmentations automatically within a joint frame-
work, and conduct sentiment classification based
on the segmentation results.
3 The Proposed Approach
In this section, we first give the task definition
of two tasks, namely sentiment classification and
sentence segmentation. Then, we present the
overview of the proposed joint segmentation and
classification model (JSC) for sentiment analysis.
The segmentation candidate generation model and
the segmentation ranking model are described in
Section 4. The details of the sentiment classifica-
tion model are presented in Section 5.
3.1 Task Definition
The task of sentiment classification has been well
formalized in previous studies (Pang and Lee,
2008; Liu, 2012). The objective is to identify the
sentiment polarity of a sentence (or document) as
positive or negative
1
.
The task of sentence segmentation aims to s-
plit a sentence into a sequence of exclusive part-
s, each of which is a basic computational unit of
the sentence. An example is illustrated in Table 1.
The original text ?that is not bad? is segmented
as ?[that] [is] [not bad]?. The segmentation re-
sult is composed of three basic computational u-
nits, namely [that], [is] and [not bad].
Type Sample
Sentence that is not bad
Segmentation [that] [is] [not bad]
Basic units [that], [is], [not bad]
Table 1: Example for sentence segmentation.
3.2 Joint Model (JSC)
The overview of the proposed joint segmentation
and classification model (JSC) for sentiment anal-
ysis is illustrated in Figure 1. The intuitions of the
joint model are two-folds:
? The segmentation results have a strong influ-
ence on the sentiment classification perfor-
mance, since they are the inputs of the sen-
timent classification model.
? The usefulness of a segmentation can be
judged by whether the sentiment classifier
can use it to predict the correct sentence po-
larity.
Based on the mutual influence observation, we
formalize the joint model in Algorithm 1. The in-
puts contain two parts, training data and feature
extractors. Each sentence s
i
in the training data
1
In this paper, the sentiment polarity of a sentence is not
relevant to the target (or aspect) it contains (Hu and Liu, 2004;
Jiang et al., 2011; Mitchell et al., 2013).
479
Algorithm 1 The joint segmentation and classifi-
cation framework (JSC) for sentiment analysis
Input:
training data: T = [s
i
, pol
g
i
], 1 ? i ? |T |
segmentation feature extractor: sfe(?)
classification feature extractor: cfe(?)
Output:
sentiment classifier: SC
segmentation ranking model: SEG
1: Generate segmentation candidates ?
i
for each
sentence s
i
in T , 1 ? i ? |T |
2: Initialize sentiment classifier SC
(0)
based on
cfe(?
ij
), randomize j ? [1, |?
i
|], 1 ? i ?
|T |
3: Randomly initialize the segmentation ranking
model SEG
(0)
4: for r ? 1 ... R do
5: Predict the sentiment polarity pol
i
for ?
i
based on SC
(r?1)
and cfe(?
i?
)
6: Update the segmentation model SEG
(r)
with SEG
(r?1)
and [?
i
, sfe(?
i?
),
pol
i?
, pol
g
i
], 1 ? i ? |T |
7: for i? 1 ... |T | do
8: Calculate the segmentation score for ?
i?
based on SEG
(r)
and sfe(?
i?
)
9: Select the top-ranked K segmentation
candidates ?
i?
from ?
i
10: end for
11: Train the sentiment classifier SC
(r)
with
cfe(?
i?
), 1 ? i ? |T |
12: end for
13: SC? SC
(R)
14: SEG? SEG
(R)
T is annotated only with its gold sentiment po-
larity pol
g
i
, without any segmentation annotation-
s. There are two feature extractors for the task
of sentence segmentation (sfe(?)) and sentiment
classification (cfe(?)), respectively. The output-
s of the joint model are the segmentation ranking
model SEG and the sentiment classifier SC.
In Algorithm 1, we first generate segmentation
candidates ?
i
for each sentence s
i
in the training
set (line 1). Each ?
i
contains no less than one
segmentation candidates. We randomly select one
segmentation result from each ?
i
and utilize their
classification features to initialize the sentimen-
t classifier SC
(0)
(line 2). We randomly initialize
the segmentation model SEG
(0)
(line 3). Subse-
quently, we iteratively train the segmentation mod-
el SEG
(r)
and sentiment classifier SC
(r)
in a join-
t manner (line 4-12). At each iteration, we pre-
dict the sentiment polarity of each segmentation
candidate ?
i?
with the current sentiment classifi-
er SC
(r?1)
(line 5), and then leverage them to up-
date the segmentation model SEG
(r)
(line 6). Af-
terwards, we utilize the recently updated segmen-
tation ranking model SEG
(r)
to update the senti-
ment classifier SC
(r)
(line 7-11). We extract the
segmentation features for each segmentation can-
didate ?
i?
, and employ them to calculate the seg-
mentation score (line 8). The top-ranked K seg-
mentation results ?
i?
of each sentence s
i
is select-
ed (line 9), and further used to train the sentimen-
t classifier SC
(r)
(line 11). Finally, after training
R iterations, we dump the segmentation ranking
model SEG
(R)
and sentiment classifier SC
(R)
in
the last iteration as outputs (line 13-14).
At training time, we train the segmentation
model and classification model from sentences
with manually annotated sentiment polarity. At
prediction time, given a test sentence, we gener-
ate its segmentation candidates, and then calculate
segmentation score for each candidate. Afterward-
s, we select the top-ranked K candidates and vote
their predicted sentiment polarity from sentiment
classifier as the final result.
4 Segmentation Model
In this section, we present details of the segmenta-
tion candidate generation model (Section 4.1), the
segmentation ranking model (Section 4.2) and the
feature description for segmentation ranking mod-
el (Section 4.3).
4.1 Segmentation Candidate Generation
In this subsection, we describe the strategy to gen-
erate segmentation candidates for each sentence.
Since the segmentation results have an exponen-
tial search space in the number of words in a
sentence, we approximate the computation using
beam search with constrains on a phrase table,
which is induced from massive corpora.
Many studies have been previously proposed to
recognize phrases in the text. However, it is out
of scope of this work to compare them. We ex-
ploit a data-driven approach given by Mikolov et
al. (2013), which identifies phrases based on the
occurrence frequency of unigrams and bigrams,
freq(w
i
, w
j
) =
freq(w
i
, w
j
)? ?
freq(w
i
)? freq(w
j
)
(1)
480
where ? is a discounting coefficient that prevents
too many phrases consisting of very infrequen-
t words. We run 2-4 times over the corpora to get
longer phrases containing more words. We em-
pirically set ? as 10 in our experiment. We use
the default frequency threshold (value=5) in the
word2vec toolkit
2
to select bi-terms.
Given a sentence, we initialize the beam of each
index with the current word, and sequentially add
phrases into the beam if the new phrase is con-
tained in the phrase table. At each index of a sen-
tence, we rank the segmentation candidates by the
inverted number of items within a segmentation,
and save the top-ranked N segmentation candi-
dates into the beam. An example of the generated
segmentation candidates is given in Table 2.
Type Sample
Sentence that is not bad
Phrase Table [is not], [not bad], [is not bad]
Segmentations
[that] [is not bad]
[that] [is not] [bad]
[that] [is] [not bad]
[that] [is] [not] [bad]
Table 2: Example for segmentation candidate gen-
eration.
4.2 Segmentation Ranking Model
The objective of the segmentation ranking model
is to assign a scalar to each segmentation candi-
date, which indicates the usefulness of the seg-
mentation result for sentiment classification. In
this subsection, we describe a log-linear model to
calculate the segmentation score. To effectively
train the segmentation ranking model, we devise a
marginal log-likelihood as the optimization objec-
tive.
Given a segmentation candidate ?
ij
of the sen-
tence s
i
, we calculate the segmentation score
for ?
ij
with a log-linear model, as given in Equa-
tion 2.
?
ij
= exp(b+
?
k
sfe
ijk
? w
k
) (2)
where ?
ij
is the segmentation score of ?
ij
; sfe
ijk
is the k-th segmentation feature of ?
ij
; w and b are
the parameters of the segmentation ranking model.
During training, given a sentence s
i
and its gold
sentiment polarity pol
g
i
, the optimization objec-
2
Available at https://code.google.com/p/word2vec/
tive of the segmentation ranking model is to max-
imize the segmentation scores of the hit candi-
dates, whose predicted sentiment polarity equal-
s to the gold polarity of sentence pol
p
i
. The loss
function of the segmentation model is given in E-
quation 3.
loss = ?
|T |
?
i=1
log(
?
j?H
i
?
ij
?
j
?
?A
i
?
ij
?
) + ?||w||
2
2
(3)
where T is the training data; A
i
represents all the
segmentation candidates of sentence s
i
; H
i
mean-
s the hit candidates of s
i
; ? is the weight of the
L2-norm regularization factor. We train the seg-
mentation model with L-BFGS (Liu and Nocedal,
1989), running over the complete training data.
4.3 Feature
We design two kinds of features for sentence seg-
mentation, namely the phrase-embedding feature
and the segmentation-specific feature. The final
feature representation of each segmentation is the
concatenation of these two features. It is worth
noting that, the phrase-embedding feature is used
in both sentence segmentation and sentiment clas-
sification.
Segmentation-Specific Feature We empirically
design four segmentation-specific features to re-
flect the information of each segmentation, as list-
ed in Table 3.
Phrase-Embedding Feature We leverage
phrase embedding to generate the features of
segmentation candidates for both sentence seg-
mentation and sentiment classification. The
reason is that, in both tasks, the basic compu-
tational units of each segmentation candidate
might be words or phrases of variable length.
Under this scenario, phrase embedding is highly
suitable as it is capable to represent phrases with
different length into a consistent distributed vector
space (Mikolov et al., 2013). For each phrase,
phrase embedding is a dense, real-valued and
continuous vector. After the phrase embedding is
trained, the nearest neighbors in the embedding
space are favored to have similar grammatical us-
ages and semantic meanings. The effectiveness of
phrase embedding has been verified for building
large-scale sentiment lexicon (Tang et al., 2014a)
and machine translation (Zhang et al., 2014).
We learn phrase embedding with Skip-Gram
model (Mikolov et al., 2013), which is the state-of-
481
Feature Feature Description
#unit the number of basic computation units in the segmentation candidate
#unit / #word the ratio of units? number in a candidate to the length of original sentence
#word ? #unit the difference between sentence length and the number of basic computational units
#unit > 2 the number of basic component units composed of more than two words
Table 3: Segmentation-specific features for segmentation ranking.
Feature Feature Description
All-Caps the number of words with all characters in upper case
Emoticon the presence of positive (or negative) emoticons, whether the last unit is emoticon
Hashtag the number of hashtag
Elongated units the number of basic computational containing elongated words (with one character
repeated more than two times), such as gooood
Sentiment lexicon the number of sentiment words, the score of last sentiment words, the total sentiment
score and the maximal sentiment score for each lexicon
Negation the number of negations as individual units in a segmentation
Bag-of-Units an extension of bag-of-word for a segmentation
Punctuation the number of contiguous sequences of dot, question mark and exclamation mark.
Cluster the presence of units from each of the 1,000 clusters from Twitter NLP tool (Gimpel
et al., 2011)
Table 4: Classification-specific features for sentiment classification.
the-art phrase embedding learning algorithm. We
compose the representation (or feature) of a seg-
mentation candidate from the embedding of the
basic computational units (words or phrases) it
contains. In this paper, we explore min, max and
average convolution functions, which have been
used as simple and effective methods for composi-
tion learning in vector-based semantics (Mitchell
and Lapata, 2010; Collobert et al., 2011; Socher et
al., 2013a; Shen et al., 2014; Tang et al., 2014b),
to calculate the representation of a segmentation
candidate. The final phrase-embedding feature is
the concatenation of vectors derived from different
convolutional functions, as given in Equation 4,
pf(seg) = [pf
max
(seg), pf
min
(seg), pf
avg
(seg)]
(4)
where pf(seg) is the representation of the given
segmentation; pf
x
(seg) is the result of the con-
volutional function x ? {min,max, avg}. Each
convolutional function pf
x
(?) conducts the matrix-
vector operation of x on the sequence represented
by columns in the lookup table of phrase embed-
ding. The output of pf
x
(?) is calculated as
pf
x
(seg) = ?
x
?L
ph
?
seg
(5)
where ?
x
is the convolutional function of pf
x
;
?L
ph
?
seg
is the concatenated column vectors of
the basic computational units in the segmentation;
L
ph
is the lookup table of phrase embedding.
5 Classification Model
For sentiment classification, we follow the su-
pervised learning framework (Pang et al., 2002)
and build the classifier from sentences with man-
ually labelled sentiment polarity. We extend the
state-of-the-art hand-crafted features in SemEval
2013 (Mohammad et al., 2013b), and design the
classification-specific features for each segmenta-
tion. The detailed feature description is given in
Table 4.
6 Experiment
In this section, we conduct experiments to evaluate
the effectiveness of the joint model. We describe
the experiment settings and the result analysis.
6.1 Dataset and Experiment Settings
We conduct sentiment classification of tweets on a
benchmark Twitter sentiment classification dataset
in SemEval 2013. We run 2-class (positive vs neg-
ative) classification as sentence segmentation has a
great influence on the positive/negative polarity of
tweets due to the polarity inconsistency between a
phrase and its constitutes, such as ?not bad, bad?.
482
We leave 3-class classification (positive, negative,
neutral) and fine-grained classification (very neg-
ative, negative, neutral, positive, very positive) in
the future work.
Positive Negative Total
Train 2,642 994 3,636
Dev 408 219 627
Test 1,570 601 2,171
Table 5: Statistics of the SemEval 2013 Twitter
sentiment classification dataset (positive vs nega-
tive).
The statistics of our dataset crawled from Se-
mEval 2013 are given in Table 5. The evalua-
tion metric is the macro-F1 of sentiment classifi-
cation. We train the joint model on the training
set, tune parameters on the dev set and evaluate
on the test set. We train the sentiment classifier
with LibLinear (Fan et al., 2008) and utilize exist-
ing sentiment lexicons
3
to extract classification-
specific features. We randomly crawl 100M tweets
from February 1st, 2013 to April 30th, 2013 with
Twitter API, and use them to learn the phrase em-
bedding with Skip-Gram
4
. The vocabulary size
of the phrase embedding is 926K, from unigram
to 5-gram. The parameter -c in SVM is tuned on
the dev-set in both baseline and our method. We
run the L-BFGS for 50 iterations, and set the reg-
ularization factor ? as 0.003. The beam size N of
the candidate generation model and the top-ranked
segmentation number K are tuned on the dev-set.
6.2 Baseline Methods
We compare the proposed joint model with the fol-
lowing sentiment classification algorithms:
? DistSuper: We collect 10M balanced tweets
selected by positive and negative emoticons
5
as
training data, and build classifier using the Lib-
Linear and ngram features (Go et al., 2009; Zhao
et al., 2012).
? SVM: The n-gram features and Support Vec-
tor Machine are widely-used baseline methods to
build sentiment classifiers (Pang et al., 2002). We
use LibLinear to train the SVM classifier.
3
In this work, we use HL (Hu and Liu, 2004), M-
PQA (Wilson et al., 2005), NRC Emotion Lexicon (Moham-
mad and Turney, 2012), NRC Hashtag Lexicon and Senti-
ment140Lexicon (Mohammad et al., 2013b).
4
https://code.google.com/p/word2vec/
5
We use the emoticons selected by Hu et al. (2013). The
positive emoticons are :) : ) :-) :D =), and the negative emoti-
cons are :( : ( :-( .
? NBSVM: NBSVM (Wang and Manning,
2012) trades-off between Naive Bayes and NB-
features enhanced SVM. We use NBSVM-bi be-
cause it performs best on sentiment classification
of reviews.
? RAE: Recursive Autoencoder (Socher et al.,
2011) has been proven effective for sentiment clas-
sification by learning sentence representation. We
train the RAE using the pre-trained phrase embed-
ding learned from 100M tweets.
? SentiStrength: Thelwall et al. (2012) build a
lexicon-based classifier which uses linguistic rules
to detect the sentiment strength of tweets.
? SSWE
u
: Tang et al. (2014b) propose to learn
sentiment-specific word embedding (SSWE) from
10M tweets collected by emoticons. They apply
SSWE as features for Twitter sentiment classifica-
tion.
? NRC: NRC builds the state-of-the-art system
in SemEval 2013 Twitter Sentiment Classifica-
tion Track, incorporating diverse sentiment lexi-
cons and hand-crafted features (Mohammad et al.,
2013b). We re-implement this system because the
codes are not publicly available. We do not di-
rectly report their results in the evaluation task,
as our training and development sets are smaller
than their dataset. In NRC + PF, We concatenate
the NRC features and the phrase embeddings fea-
ture (PF), and build the sentiment classifier with
LibLinear.
Except for DistSuper, other baseline method-
s are conducted in a supervised manner. We do
not compare with RNTN (Socher et al., 2013b) be-
cause the tweets in our dataset do not have accu-
rately parsed results. Another reason is that, due to
the differences between domains, the performance
of RNTN trained on movie reviews might be de-
creased if directly applied on the tweets (Xiao et
al., 2013).
6.3 Results and Analysis
Table 6 shows the macro-F1 of the baseline sys-
tems as well as our joint model (JSC) on senti-
ment classification of tweets (positive vs negative).
As is shown in Table 6, distant supervision is
relatively weak because the noisy-labeled tweets
are treated as the gold standard, which decreases
the performance of sentiment classifier. The result
of bag-of-unigram feature (74.50%) is not satisfied
as it losses the word order and does not well cap-
483
Method Macro-F1
DistSuper + unigram 61.74
DistSuper + 5-gram 63.92
SVM + unigram 74.50
SVM + 5-gram 74.97
Recursive Autoencoder 75.42
NBSVM 75.28
SentiStrength 73.23
SSWE
u
84.98
NRC (Top System in SemEval 2013) 84.73
NRC + PF 84.75
JSC 85.51
Table 6: Macro-F1 for positive vs negative classi-
fication of tweets.
ture the semantic meaning of phrases. The integra-
tion of high-order n-ngram (up to 5-gram) does not
achieve significant improvement (+0.47%). The
reason is that, if a sentence contains a bigram ?not
bad?, they will use ?bad? and ?not bad? as par-
allel features, which confuses the sentiment clas-
sification model. NBSVM and Recursive Autoen-
coder perform comparatively and have a big gap
in comparison with JSC. In RAE, the representa-
tion of a sentence is composed from the represen-
tation of words it contains. Accordingly, ?great?
in ?a great deal of ? also contributes to the final
sentence representation via composition function.
JSC automatically conducts sentence segmenta-
tion by considering the sentiment polarity of sen-
tence, and utilize the phrasal information from the
segmentations. Ideally, JSC regards phrases like
?not bad? and ?a great deal of ? as basic compu-
tational units, and yields better classification per-
formance. JSC (85.51%) performs slightly better
than the state-of-the-art systems (SSWE
u
, 84.98%;
NRC+PF, 84.75%), which verifies its effective-
ness.
6.4 Comparing Joint and Pipelined Models
We compare the proposed joint model with
pipelined methods on Twitter sentiment classifi-
cation with different feature sets. Figure 2 gives
the experiment results. The tick [A, B] on x-
axis means the use of A as segmentation feature
and the use of B as classification feature. PF
represents the phrase-embedding feature; SF and
CF stand for the segmentation-specific feature and
classification-specific feature, respectively. We
use the bag-of-word segmentation result to build
sentiment classier in Pipeline 1, and use the seg-
mentation candidate with maximum phrase num-
ber in Pipeline 2.
0.7
0.72
0.74
0.76
0.78
0.8
0.82
0.84
0.86
Mac
ro?F
1
 
 
[PF, PF] [PF+SF, PF] [PF, PF+CF] [PF+SF, PF+CF]
Pipeline 1Pipeline 2Joint
Figure 2: Macro-F1 for positive vs negative classi-
fication of tweets with joint and pipelined models.
From Figure 2, we find that the joint model
consistently outperforms pipelined baseline meth-
ods in all feature settings. The reason is that
the pipelined methods suffer from error propaga-
tion, since the errors from linguistic-driven and
bag-of-word segmentations cannot be corrected by
the sentiment classification model. Besides, tra-
ditional segmentors do not update the segmenta-
tion model with the sentiment information of tex-
t. Unlike pipelined methods, the joint model is
capable to address these problems by optimizing
the segmentation model with the classification re-
sults in a joint framework, which yields better
performance on sentiment classification. We also
find that Pipeline 2 always outperforms Pipeline
1, which indicates the usefulness of phrase-based
segmentation for sentiment classification.
6.5 Effect of the beam size N
We investigate the influence of beam size N ,
which is the maximum number of segmentation
candidates of a sentence. In this part, we clamp the
feature set as [PF+SF, PF+CF], and vary the beam
size N in [1,2,4,8,16,32,64]. The experiment re-
sults of macro-F1 on the development set are il-
lustrated in Figure 3 (a). The time cost of each
training iteration is given in Figure 3 (b).
From Figure 3 (a), we can see that when larg-
er beam size is considered, the classification per-
formance is improved. When beam size is 1, the
model stands for the greedy search with the bag-
of-words segmentation. When the beam size is s-
mall, such as 2, beam search losses many phrasal
information of sentences and thus the improve-
ment is not significant. The performance remains
steady when beam size is larger than 16. From
484
1 2 4 8 16 32 640.81
0.82
0.83
0.84
0.85
0.86
Beam Size
Mac
ro?F
1
(a) Macro-F1 score for senti-
ment classification.
1 2 4 8 16 32 640
20
40
60
80
100
120
Beam Size
Run
time
 (Sec
ond)
 
 
(b) Time cost (seconds) of
each training iteration.
Figure 3: Sentiment classification of tweets with
different beam size N .
Figure 3 (b), we can find that the runtime of each
training iteration increases with larger beam size.
It is intuitive as the joint model with larger beam
considers more segmentation results, which in-
creases the training time of the segmentation mod-
el. We set beam size as 16 after parameter learn-
ing.
6.6 Effect of the top-ranked segmentation
number K
We investigate how the top-ranked segmentation
number K affects the performance of sentimen-
t classification. In this part, we set the feature as
[PF+SF, PF+CF], and the beam size as 16. The
results of macro-F1 on the development set are il-
lustrated in Figure 4.
1 3 5 7 9 11 13 150.82
0.83
0.84
0.85
0.86
Top?ranked candidate number
Ma
cro
?F1
Figure 4: Sentiment classification of tweets with
different top-ranked segmentation number K.
From Figure 4, we find that the classification
performance increases with K being larger. The
reason is that when a larger K is used, (1) at train-
ing time, the sentiment classifier is built by using
more phrasal information from multiple segmen-
tations, which benefits from the ensembles; (2) at
test time, the joint model considers several top-
ranked segmentations and get the final sentiment
polarity through voting. The performance remain-
s stable when K is larger than 7, as the phrasal
information has been mostly covered.
7 Conclusion
In this paper, we develop a joint segmentation
and classification framework (JSC) for sentiment
analysis. Unlike existing sentiment classification
algorithms that build sentiment classifier based
on the segmentation results from bag-of-words or
separate segmentors, the proposed joint model si-
multaneously conducts sentence segmentation and
sentiment classification. We introduce a marginal
log-likelihood function to optimize the segmenta-
tion model, and effectively train the joint mod-
el from sentences annotated only with sentiment
polarity, without segmentation annotations of sen-
tences. The effectiveness of the joint model has
been verified by applying it on the benchmark
dataset of Twitter sentiment classification in Se-
mEval 2013. Results show that, the joint model
performs comparably with state-of-the-art meth-
ods, and outperforms pipelined methods in various
settings. In the future, we plan to apply the join-
t model on other domains, such as movie/product
reviews.
Acknowledgements
We thank Nan Yang, Yajuan Duan, Yaming
Sun and Meishan Zhang for their helpful dis-
cussions. We thank the anonymous reviewers
for their insightful comments and feedbacks on
this work. This research was partly supported
by National Natural Science Foundation of Chi-
na (No.61133012, No.61273321, No.61300113).
The contact author of this paper, according to the
meaning given to this role by Harbin Institute of
Technology, is Bing Qin.
References
Yoshua Bengio, Aaron Courville, and Pascal Vincent.
2013. Representation learning: A review and new
perspectives. IEEE Trans. Pattern Analysis and Ma-
chine Intelligence.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993?1022.
Yejin Choi and Claire Cardie. 2008. Learning with
compositional semantics as structural inference for
subsentential sentiment analysis. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, pages 793?801.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
485
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Xiaowen Ding, Bing Liu, and Philip S Yu. 2008. A
holistic lexicon-based approach to opinion mining.
In Proceedings of the International Conference on
Web Search and Data Mining, pages 231?240.
Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming
Zhou, and Ke Xu. 2014. Adaptive recursive neural
network for target-dependent twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 49?54.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
Ronen Feldman. 2013. Techniques and application-
s for sentiment analysis. Communications of the
ACM, 56(4):82?89.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics, pages 42?47.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. Proceed-
ings of International Conference on Machine Learn-
ing.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford, pages 1?12.
G.E. Hinton and R.R. Salakhutdinov. 2006. Reduc-
ing the dimensionality of data with neural networks.
Science, 313(5786):504?507.
Ming Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDDConference on Knowledge Discovery
and Data Mining, pages 168?177.
Xia Hu, Jiliang Tang, Huiji Gao, and Huan Liu.
2013. Unsupervised sentiment analysis with emo-
tional signals. In Proceedings of the International
World Wide Web Conference, pages 607?618.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter sen-
timent classification. The Proceeding of Annual
Meeting of the Association for Computational Lin-
guistics.
Nicola Jones. 2014. Computer science: The learning
machines. Nature, 505(7482):146.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A sentence model based on convolu-
tional neural networks. In Procedding of the 52th
Annual Meeting of Association for Computational
Linguistics.
Hyun Duk Kim and ChengXiang Zhai. 2009. Gener-
ating comparative summaries of contradictory opin-
ions in text. In Proceedings of CIKM 2009. ACM.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the omg! In The International AAAI
Conference on Weblogs and Social Media.
Igor Labutov and Hod Lipson. 2013. Re-embedding
words. In Annual Meeting of the Association for
Computational Linguistics.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of international con-
ference on Machine learning. ACM.
Quoc Le and Tomas Mikolov. 2014. Distributed repre-
sentations of sentences and documents. Proceedings
of International Conference on Machine Learning.
Dong C Liu and Jorge Nocedal. 1989. On the limited
memory bfgs method for large scale optimization.
Mathematical programming, 45(1-3):503?528.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
Andrew L Maas, Raymond E Daly, Peter T Pham, Dan
Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In
Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. Conference on Neural Information Processing
Systems.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Margaret Mitchell, Jacqui Aguilar, Theresa Wilson,
and Benjamin Van Durme. 2013. Open domain tar-
geted sentiment. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1643?1654.
Saif M Mohammad and Peter D Turney. 2012. Crowd-
sourcing a word?emotion association lexicon. Com-
putational Intelligence.
Saif M Mohammad, Bonnie J Dorr, Graeme Hirst, and
Peter D Turney. 2013a. Computing lexical contrast.
Computational Linguistics, 39(3):555?590.
486
Saif M Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013b. Nrc-canada: Building the state-of-
the-art in sentiment analysis of tweets. Proceedings
of the International Workshop on Semantic Evalua-
tion.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classifica-
tion using crfs with hidden variables. In Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 786?794.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In Proceedings of Language Resources and Evalua-
tion Conference, volume 2010.
Georgios Paltoglou and Mike Thelwall. 2010. A s-
tudy of information retrieval weighting schemes for
sentiment analysis. In Proceedings of Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 1386?1395.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 79?86.
Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng,
and Gr?egoire Mesnil. 2014. Learning semantic rep-
resentations using convolutional neural networks for
web search. In Proceedings of the companion publi-
cation of the 23rd international conference on World
wide web companion, pages 373?374.
Richard Socher, J. Pennington, E.H. Huang, A.Y. Ng,
and C.D. Manning. 2011. Semi-supervised recur-
sive autoencoders for predicting sentiment distribu-
tions. In Conference on Empirical Methods in Nat-
ural Language Processing, pages 151?161.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic Com-
positionality Through Recursive Matrix-Vector S-
paces. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing.
Richard Socher, Danqi Chen, Christopher D Manning,
and Andrew Y Ng. 2013a. Reasoning with neu-
ral tensor networks for knowledge base completion.
The Conference on Neural Information Processing
Systems.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013b. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Conference on Empirical Methods in Nat-
ural Language Processing, pages 1631?1642.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional linguistics, 37(2):267?307.
Duyu Tang, Furu Wei, Bing Qin, Ming Zhou, and
Ting Liu. 2014a. Building large-scale twitter-
specific sentiment lexicon : A representation learn-
ing approach. In Proceedings of COLING 2014,
the 25th International Conference on Computation-
al Linguistics, pages 172?182.
Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014b. Learning sentiment-
specific word embedding for twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 1555?1565.
Mike Thelwall, Kevan Buckley, and Georgios Pal-
toglou. 2012. Sentiment strength detection for the
social web. Journal of the American Society for In-
formation Science and Technology, 63(1):163?173.
Peter D Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 417?424.
Sida Wang and Christopher D Manning. 2012. Base-
lines and bigrams: Simple, good sentiment and topic
classification. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 90?94.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 347?354.
Min Xiao, Feipeng Zhao, and Yuhong Guo. 2013.
Learning latent word representations for domain
adaptation using supervised word clustering. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 152?
162, October.
Ainur Yessenalina and Claire Cardie. 2011. Compo-
sitional matrix-space models for sentiment analysis.
In Proceedings of Conference on Empirical Methods
in Natural Language Processing, pages 172?182.
Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, and
Chengqing Zong. 2014. Bilingually-constrained
phrase embeddings for machine translation. In Pro-
ceedings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 111?121.
Jichang Zhao, Li Dong, Junjie Wu, and Ke Xu. 2012.
Moodlens: an emoticon-based sentiment analysis
system for chinese tweets. In Proceedings of the
18th ACM SIGKDD international conference on
Knowledge discovery and data mining.
487
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 377?380,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Generalizing Syntactic Structures for Product Attribute Candidate
Extraction
Yanyan Zhao, Bing Qin, Shen Hu, Ting Liu
Harbin Institute of Technology, Harbin, China
{yyzhao,bqin,shu,tliu}@ir.hit.edu.cn
Abstract
Noun phrases (NP) in a product review are
always considered as the product attribute
candidates in previous work. However, this
method limits the recall of the product at-
tribute extraction. We therefore propose
a novel approach by generalizing syntactic
structures of the product attributes with two
strategies: intuitive heuristics and syntactic
structure similarity. Experiments show that
the proposed approach is effective.
1 Introduction
Product attribute extraction is a fundamental task of
sentiment analysis. It aims to extract the product at-
tributes from a product review, such as ?picture qual-
ity? in the sentence ?The picture quality of Canon is
perfect.? This task is usually performed in two steps:
product attribute candidate extraction and candidate
classification.
Almost all the previous work pays more attention
to the second step, fewer researchers make in-depth
research on the first step. They simply choose the
NPs in a product review as the product attribute can-
didates (Hu and Liu, 2004; Popescu and Etzioni,
2005; Yi et al, 2003). However, this method lim-
its the recall of the product attribute extraction for
two reasons. First, there exist other structures of the
product attributes except NPs. Second, the syntactic
parsing is not perfect, especially for the Non-English
languages, such as Chinese. Experiments on three
Chinese datasets1 show that nearly 15% product at-
tributes are lost, when only using NPs as the can-
didates. Obviously, if using the candidate classifi-
cation techniques on these NP candidates, it would
1It refers to the training data in Section 3.1.
lead to poor performance (especially for recall) for
the final product attribute extraction.
Based on the above discussion, it can be observed
that product attribute candidate extraction is well
worth studying. In this paper, we propose an ap-
proach by generalizing the syntactic structures of the
product attributes to solve this problem. Figure 1
lists some syntactic structure samples from an an-
notated corpus, including the special forms of NPs
in Figure 1(a) and other syntactic structures, such as
VP or IP in Figure 1(b). We can find that the syntac-
tic structures can not only cover more phrase types
besides NP, but also describe the detailed forms of
the product attributes.
NP
NN
??(screen)
NP
NN
??
NP
NN
???(screen    resolution)
NP
QP
CD
?
NP
NN
??(single    track)
NP
ADJP
JJ
?
NP
NN
??(front         seats)
NP
VB
??
NP
NN
??(photographing function)
VP
NP
NN
??
VP
VB
??(screen   display)
IP
(a) syntactic structure samples of NP
(b) syntactic structure samples of other phrases
Figure 1: Syntactic structure samples of the product at-
tributes (acquired by an automatic phrase parser).
In order to exploit more and useful syntactic struc-
tures, two generalization strategies: intuitive heuris-
tics and syntactic structure similarity are used. Ex-
periments on three Chinese domain-specific datasets
show that our approach can significantly improve the
recall of the product attribute candidate extraction,
and furthermore, improve the performance of the fi-
nal product attribute extraction.
377
2 Approach
The standard syntactic structures of the product at-
tributes can be collected from a training set2. Then
a simple method of exact matching can be used to
select the product attribute candidates from the test
set. In particular, for a syntactic structure3 T in
the test set, if T exactly matches with one of the
standard syntactic structures, then its corresponding
string can be treated as a product attribute candidate.
However, this method fails to handle similar syn-
tactic structures, such as the two structures in Fig-
ure 2. Besides, this method treats the syntactic struc-
ture as a whole during exact matching, without con-
sidering any structural information. Therefore, it is
difficult to describe the syntactic structure informa-
tion explicitly. All of these prevent this method from
generalizing unseen data well.
To overcome the above problems, two generaliza-
tion strategies are proposed in this paper. One is to
generalize the syntactic structures with two intuitive
heuristics. The other is to deeply mine the syntactic
structure by decomposing it into several substruc-
tures. Both strategies will be introduced in the fol-
lowing subsections.
2.1 Intuitive Heuristics
Two intuitive heuristics are adopted to generalize the
syntactic structures.
Heu1: For the near-synonymic grammar tags in
syntactic structures, we can generalize them by a
normalized one. Such as the red boxes in Figure 2,
the POSs ?NNS? and ?NN? show the same syntactic
meaning, we can generalize ?NNS? with ?NN?. The
near-synonymic grammar tags are listed in Table 1.
NP
VP NP
VB NNS NP
NN
NP
VP NP
VB NN NN
Heu2
Heu1
Figure 2: Generalizing a syntactic structure with two in-
tuitive heuristics.
Heu2: For the sequence of identical grammar tags
in syntactic structures, we can replace them with
2We use Dan Bikel?s phrase parser for syntactic parsing.
3We simply select the syntactic structures of the strings un-
der three words or four words with ???(?of? in English).
Replaced by Near-synonymic grammar tags
JJ JJR, JJS
NN NNS, NNP, NNPS, CD, NR
RB RBR, RBS
VB VBD, VBG, VBN, VBP, VBZ, VV
S SBAR, SBARQ, SINU, SQ
Table 1: The near-synonymic grammar tags.
one. The reason is that the sequential grammar tags
always describe the same syntactic function as one
grammar tag. Such as the blue circles in Figure 2.
2.2 Syntactic Structure Similarity
The heuristic generalization strategy is too restric-
tive to give a good coverage. Moreover, after this
kind of generalization, the syntactic structure is used
as a whole in exact matching all the same. Thus,
as an alternative to the exact matching, tree kernel
based methods can be used to implicitly explore the
substructures of the syntactic structure in a high-
dimensional space. This kind of methods can di-
rectly calculate the similarity between two substruc-
ture vectors using a kernel function. Tree kernel
based methods are effective in modeling structured
features, which are widely used in many natural
language processing tasks, such as syntactic pars-
ing (Collins and Duffy, 2001) and semantic role la-
beling (Che et al, 2008) and so on.
NP
NN
VP
VB
IP
NP VP
VB
IP
NP
NN
VP
VB NP
NN
VP
VB
IP
NP
NN
VP
IP
NP VP
IP
NP
NN
VP
VB
IP IP
Figure 3: Substructures from a syntactic structure.
In this paper, the syntactic structure for a product
attribute can be decomposed into several substruc-
tures, such as in Figure 3. Correspondingly, the syn-
tactic structure T can be represented by a vector of
integer counts of each substructure type:
?(T ) = (?1(T ), ?2(T ), ..., ?n(T ))
= (# of substructures of type 1,
= # of substructures of type 2,
...,
= # of substructures of type n)
378
After syntactic structure decomposition, we can
count the number of the common substructures as
the similarity between two syntactic structures. The
commonly used convolution tree kernel is applied in
this paper. Its kernel function is defined as follows:
K(T1, T2) = ??(T1),?(T2)?
=
?
i(?i(T1) ? ?i(T2))
Based on these, for a syntactic structure T in the
test set, we can compute the similarity between T
and all the standard syntactic structures by the above
kernel function. A similarity threshold thsim4 is set
to determine whether the string from T is a correct
product attribute candidate.
3 Experiments
3.1 Datasets and Evaluation Metrics
Three domain-specific datasets are used in the ex-
periments, which is from an official Chinese Opin-
ion Analysis Evaluation 2008 (COAE2008) (Zhao et
al., 2008). Table 2 shows the statistics of the three
datasets, each of which is divided into training, de-
velopment and test data in a proportion of 2:1:1.
Domain # of sentences # of standardproduct attributes
Camera 1,780 1,894
Car 2,166 2,504
Phone 2,196 2,293
Table 2: The datasets for three product domains.
Two evaluation metrics, recall and noise ratio, are
designed to evaluate the performance of the prod-
uct attribute candidate extraction. Recall refers to
the proportion of correctly identified attribute candi-
dates in all standard product attributes. Noise ratio
refers to the proportion of incorrectly identified at-
tribute candidates in all candidates.
3.2 Comparative methods
We choose the method, which considers NPs as the
product attribute candidates, as the baseline (shown
as NPs based).
Besides, in order to assess the two generaliza-
tion strategies? effectiveness, four experiments are
designed as follows:
4In the experiments, thsim is set to 0.7, which is tuned on
the development set.
SynStru based: It refers to the syntactic struc-
ture exact matching method, which is implemented
without the two proposed generation strategies.
SynStru h: It refers to the strategy only using the
first generalization.
SynStru kernel: It refers to the strategy only us-
ing the second generalization.
SynStru h+kernel: It refers to the strategy us-
ing both two generalizations, i.e., it refers to our ap-
proach in this paper.
3.3 Results
Table 3 lists the comparative performances on the
test data between our approach and the comparative
methods for product attribute candidate extraction.
Domain Method Recall Noise ratio
Camera
NPs based 81.20% 63.64%
SynStru based 84.80% 67.67%
SynStru h 92.08% 74.74%
SynStru kernel 92.51% 75.92%
SynStru h+kernel 92.72% 76.25%
Car
NPs based 85.25% 69.35%
SynStru based 86.31% 72.66%
SynStru h 93.78% 78.01%
SynStru kernel 94.56% 79.50%
SynStru h+kernel 94.71% 80.44%
Phone
NPs based 84.11% 63.76%
SynStru based 86.26% 67.09%
SynStru h 93.13% 73.62%
SynStru kernel 93.47% 75.11%
SynStru h+kernel 93.63% 75.35%
Table 3: Comparisons between our approach and the
comparative methods for product attribute candidate ex-
traction.
Analyzing the recalls in Table 3, we can find that:
1. The performance of SynStru based method
is better than NPs based method for each domain.
This can illustrate that syntactic structures can cover
more forms of the product attributes. However, the
recall of SynStru based method is not high, either.
2. The two generalization strategies, SynStru h
and SynStru kernel can both significantly improve
the performance for each domain, comparing to the
SynStru based method. This can illustrate that our
two generalization strategies are helpful.
3. Our approach SynStru h+kernel achieves the
best performance. This can illustrate that the two
generalization strategies are complementary to each
379
other. And further, mining and generalizing the syn-
tactic structures is effective for candidate extraction.
However, the noise ratio for each domain is in-
creasing when employing our approach. That?s be-
cause, more kinds of syntactic structures are consid-
ered, more noise is added. However, we can easily
remove the noise in the candidate classification step.
Thus in the next section, we will assess our candi-
date extraction approach by applying it to the prod-
uct attribute extraction task.
4 Application in Product Attribute
Extraction
For the extracted product attribute candidates, we
train a maximum entropy (ME) based binary clas-
sifier to find the correct product attributes. Several
commonly used features are listed in Table 4.
Feature Description
lexical
the words of the product attribute(PA)
the POS for each word of the PA
three words before the PA
three words after the PA
the words? number of the PA
syntactic the syntactic structure of the PA
Is there a stop word in the PA?
binary Is there a polarity word in the PA?
(Y/N) Is there an English word or number in the PA?
Table 4: The feature set for product attribute extraction.
Table 5 shows the product attribute extraction per-
formances on the test data. We can find that the
performance (F1) of our approach is better than
NPs based method for each domain. We discuss the
results as follows:
1. Comparing to the NPs based method, the re-
call of our approach increases a lot for each domain.
This demonstrates that generalized syntactic struc-
tures can cover more forms of product attributes.
2. Comparing to the NPs based method, the pre-
cision of our approach also increases for each do-
main. That?s because syntactic structures are more
specialized than the phrase forms (such as NP, VP)
in the previous work, which can filter some noises
from the phrase(NP) candidates.
5 Conclusion
This paper describes a simple but effective way to
extract the product attribute candidates from product
Domain Method R (%) P (%) F1 (%)
Camera NPs based 59.62 68.38 63.70Our approach 62.96 73.32 67.74
Car NPs based 59.94 64.87 62.31Our approach 67.34 65.90 66.61
Phone NPs based 58.53 71.14 64.22Our approach 67.84 76.13 71.74
Table 5: Comparisons between our approach and the
NPs based method for product attribute extraction.
reviews. The proposed approach is based on deep
analysis into syntactic structures of the product at-
tributes, via intuitive heuristics and syntactic struc-
ture decomposition. Experimental results indicate
that our approach is promising. In future, we will try
more syntactic structure generalization strategies.
Acknowledgments
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
60803093, 60975055, and the ?863?National
High- Tech Research and Development of China via
grant 2008AA01Z144.
References
Wanxiang Che, Min Zhang, AiTi Aw, Chew Lim Tan,
Ting Liu, and Sheng Li. 2008. Using a hybrid con-
volution tree kernel for semantic role labeling. ACM
Trans. Asian Lang. Inf. Process., 7(4).
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In NIPS, pages 625?632.
Minqing Hu and Bing Liu. 2004. Mining opinion fea-
tures in customer reviews. In Proceedings of AAAI-
2004, pages 755?760.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
hltemnlp2005, pages 339?346.
Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu, and
Wayne Niblack. 2003. Sentiment analyzer: Extract-
ing sentiments about a given topic using natural lan-
guage processing techniques. In Proceedings of the
IEEE International Conference on Data Mining.
Jun Zhao, Hongbo Xu, Xuanjing Huang, Songbo Tan,
Kang Liu, and Qi Zhang. 2008. Overview of chinese
opinion analysis evaluation 2008.
380
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1199?1209,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Learning Semantic Hierarchies via Word Embeddings
Ruiji Fu
?
, Jiang Guo
?
, Bing Qin
?
, Wanxiang Che
?
, Haifeng Wang
?
, Ting Liu
??
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
?
Baidu Inc., Beijing, China
{rjfu, jguo, bqin, car, tliu}@ir.hit.edu.cn
wanghaifeng@baidu.com
Abstract
Semantic hierarchy construction aims to
build structures of concepts linked by
hypernym?hyponym (?is-a?) relations. A
major challenge for this task is the
automatic discovery of such relations.
This paper proposes a novel and effec-
tive method for the construction of se-
mantic hierarchies based on word em-
beddings, which can be used to mea-
sure the semantic relationship between
words. We identify whether a candidate
word pair has hypernym?hyponym rela-
tion by using the word-embedding-based
semantic projections between words and
their hypernyms. Our result, an F-score
of 73.74%, outperforms the state-of-the-
art methods on a manually labeled test
dataset. Moreover, combining our method
with a previous manually-built hierarchy
extension method can further improve F-
score to 80.29%.
1 Introduction
Semantic hierarchies are natural ways to orga-
nize knowledge. They are the main components
of ontologies or semantic thesauri (Miller, 1995;
Suchanek et al, 2008). In the WordNet hierar-
chy, senses are organized according to the ?is-a?
relations. For example, ?dog? and ?canine? are
connected by a directed edge. Here, ?canine? is
called a hypernym of ?dog.? Conversely, ?dog?
is a hyponym of ?canine.? As key sources
of knowledge, semantic thesauri and ontologies
can support many natural language processing
applications. However, these semantic resources
are limited in its scope and domain, and their
manual construction is knowledge intensive and
time consuming. Therefore, many researchers
?
Email correspondence.
??organism
??plant
???Ranunculaceae
???Aconitum
??aconite ???medicinal plant
??medicine
??organism
??plant
???Ranunculaceae
???Aconitum
??aconite
???medicinal plant
??medicine
Figure 1: An example of semantic hierarchy con-
struction.
have attempted to automatically extract semantic
relations or to construct taxonomies.
A major challenge for this task is the auto-
matic discovery of hypernym-hyponym relations.
Fu et al (2013) propose a distant supervision
method to extract hypernyms for entities from
multiple sources. The output of their model is
a list of hypernyms for a given enity (left pan-
el, Figure 1). However, there usually also exists
hypernym?hyponym relations among these hy-
pernyms. For instance, ??? (plant)? and
???? (Ranunculaceae)? are both hyper-
nyms of the entity ??? (aconit),? and ??
? (plant)? is also a hypernym of ????
(Ranunculaceae).? Given a list of hypernyms
of an entity, our goal in the present work is to
construct a semantic hierarchy of these hypernyms
(right panel, Figure 1).
1
Some previous works extend and refine
manually-built semantic hierarchies by using other
resources (e.g., Wikipedia) (Suchanek et al,
2008). However, the coverage is limited by the
scope of the resources. Several other works relied
heavily on lexical patterns, which would suffer
from deficiency because such patterns can only
cover a small proportion of complex linguistic cir-
cumstances (Hearst, 1992; Snow et al, 2005).
1
In this study, we focus on Chinese semantic hierarchy
construction. The proposed method can be easily adapted to
other languages.
1199
Besides, distributional similarity methods (Kotler-
man et al, 2010; Lenci and Benotto, 2012) are
based on the assumption that a term can only be
used in contexts where its hypernyms can be used
and that a term might be used in any contexts
where its hyponyms are used. However, it is not
always rational. Our previous method based on
web mining (Fu et al, 2013) works well for hy-
pernym extraction of entity names, but it is unsuit-
able for semantic hierarchy construction which in-
volves many words with broad semantics. More-
over, all of these methods do not use the word
semantics effectively.
This paper proposes a novel approach for se-
mantic hierarchy construction based on word em-
beddings. Word embeddings, also known as dis-
tributed word representations, typically represent
words with dense, low-dimensional and real-
valued vectors. Word embeddings have been
empirically shown to preserve linguistic regular-
ities, such as the semantic relationship between
words (Mikolov et al, 2013b). For example,
v(king) ? v(queen) ? v(man) ? v(woman),
where v(w) is the embedding of the word w. We
observe that a similar property also applies to the
hypernym?hyponym relationship (Section 3.3),
which is the main inspiration of the present study.
However, we further observe that hypernym?
hyponym relations are more complicated than a
single offset can represent. To address this chal-
lenge, we propose a more sophisticated and gen-
eral method ? learning a linear projection which
maps words to their hypernyms (Section 3.3.1).
Furthermore, we propose a piecewise linear pro-
jection method based on relation clustering to
better model hypernym?hyponym relations (Sec-
tion 3.3.2). Subsequently, we identify whether an
unknown word pair is a hypernym?hyponym re-
lation using the projections (Section 3.4). To the
best of our knowledge, we are the first to apply
word embeddings to this task.
For evaluation, we manually annotate a dataset
containing 418 Chinese entities and their hyper-
nym hierarchies, which is the first dataset for this
task as far as we know. The experimental results
show that our method achieves an F-score of
73.74% which significantly outperforms the pre-
vious state-of-the-art methods. Moreover, com-
bining our method with the manually-built hier-
archy extension method proposed by Suchanek et
al. (2008) can further improve F-score to 80.29%.
2 Background
As main components of ontologies, semantic hi-
erarchies have been studied by many researchers.
Some have established concept hierarchies based
on manually-built semantic resources such as
WordNet (Miller, 1995). Such hierarchies have
good structures and high accuracy, but their cov-
erage is limited to fine-grained concepts (e.g.,
?Ranunculaceae? is not included in Word-
Net.). We have made similar obsevation that about
a half of hypernym?hyponym relations are absent
in a Chinese semantic thesaurus. Therefore, a
broader range of resources is needed to supple-
ment the manually built resources. In the construc-
tion of the famous ontology YAGO, Suchanek et
al. (2008) link the categories in Wikipedia onto
WordNet. However, the coverage is still limited
by the scope of Wikipedia.
Several other methods are based on lexical
patterns. They use manually or automatically
constructed lexical patterns to mine hypernym?
hyponym relations from text corpora. A hierarchy
can then be built based on these pairwise relations.
The pioneer work by Hearst (1992) has found
out that linking two noun phrases (NPs) via cer-
tain lexical constructions often implies hypernym
relations. For example, NP
1
is a hypernym of NP
2
in the lexical pattern ?such NP
1
as NP
2
.? Snow et
al. (2005) propose to automatically extract large
numbers of lexico-syntactic patterns and subse-
quently detect hypernym relations from a large
newswire corpus. Their method relies on accurate
syntactic parsers, and the quality of the automat-
ically extracted patterns is difficult to guarantee.
Generally speaking, these pattern-based methods
often suffer from low recall or precision because
of the coverage or the quality of the patterns.
The distributional methods assume that the con-
texts of hypernyms are broader than the ones of
their hyponyms. For distributional similarity com-
puting, each word is represented as a semantic
vector composed of the pointwise mutual infor-
mation (PMI) with its contexts. Kotlerman et al
(2010) design a directional distributional measure
to infer hypernym?hyponym relations based on
the standard IR Average Precision evaluation mea-
sure. Lenci and Benotto (2012) propose anoth-
er measure focusing on the contexts that hyper-
nyms do not share with their hyponyms. However,
broader semantics may not always infer broader
contexts. For example, for terms ?Obama? and
1200
?American people?, it is hard to say whose
contexts are broader.
Our previous work (Fu et al, 2013) applies a
web mining method to discover the hypernyms of
Chinese entities from multiple sources. We as-
sume that the hypernyms of an entity co-occur
with it frequently. It works well for named enti-
ties. But for class names (e.g., singers in Hong
Kong, tropical fruits) with wider range of mean-
ings, this assumption may fail.
In this paper, we aim to identify hypernym?
hyponym relations using word embeddings, which
have been shown to preserve good properties for
capturing semantic relationship between words.
3 Method
In this section, we first define the task formally.
Then we elaborate on our proposed method com-
posed of three major steps, namely, word embed-
ding training, projection learning, and hypernym?
hyponym relation identification.
3.1 Task Definition
Given a list of hypernyms of an entity, our goal is
to construct a semantic hierarchy on it (Figure 1).
We represent the hierarchy as a directed graph
G, in which the nodes denote the words, and the
edges denote the hypernym?hyponym relations.
Hypernym-hyponym relations are asymmetric and
transitive when words are unambiguous:
? ?x, y ? L : x
H
??y ? ?(y
H
??x)
? ?x, y, z ? L : (x
H
??z ? z
H
??y)? x
H
??y
Here, L denotes the list of hypernyms. x, y and
z denote the hypernyms in L. We use
H
?? to
represent a hypernym?hyponym relation in this
paper. Actually, x, y and z are unambiguous as
the hypernyms of a certain entity. Therefore, G
should be a directed acyclic graph (DAG).
3.2 Word Embedding Training
Various models for learning word embeddings
have been proposed, including neural net lan-
guage models (Bengio et al, 2003; Mnih and
Hinton, 2008; Mikolov et al, 2013b) and spec-
tral models (Dhillon et al, 2011). More recent-
ly, Mikolov et al (2013a) propose two log-linear
models, namely the Skip-gram and CBOW model,
to efficiently induce word embeddings. These two
models can be trained very efficiently on a large-
scale corpus because of their low time complexity.
No. Examples
1
v(?)? v(??) ? v(?)? v(??)
v(shrimp)? v(prawn) ? v(fish)? v(gold fish)
2
v(??)? v(??) ? v(??)? v(??)
v(laborer)? v(carpenter) ? v(actor)? v(clown)
3
v(??)? v(??) 6? v(?)? v(??)
v(laborer)? v(carpenter) 6? v(fish)? v(gold fish)
Table 1: Embedding offsets on a sample of
hypernym?hyponym word pairs.
Additionally, their experiment results have shown
that the Skip-gram model performs best in identi-
fying semantic relationship among words. There-
fore, we employ the Skip-gram model for estimat-
ing word embeddings in this study.
The Skip-gram model adopts log-linear classi-
fiers to predict context words given the current
word w(t) as input. First, w(t) is projected to its
embedding. Then, log-linear classifiers are em-
ployed, taking the embedding as input and pre-
dict w(t)?s context words within a certain range,
e.g. k words in the left and k words in the
right. After maximizing the log-likelihood over
the entire dataset using stochastic gradient descent
(SGD), the embeddings are learned.
3.3 Projection Learning
Mikolov et al (2013b) observe that word em-
beddings preserve interesting linguistic regulari-
ties, capturing a considerable amount of syntac-
tic/semantic relations. Looking at the well-known
example: v(king) ? v(queen) ? v(man) ?
v(woman), it indicates that the embedding offsets
indeed represent the shared semantic relation be-
tween the two word pairs.
We observe that the same property also ap-
plies to some hypernym?hyponym relations. As
a preliminary experiment, we compute the em-
bedding offsets between some randomly sampled
hypernym?hyponym word pairs and measure their
similarities. The results are shown in Table 1.
The first two examples imply that a word can
also be mapped to its hypernym by utilizing word
embedding offsets. However, the offset from
?carpenter? to ?laborer? is distant from
the one from ?gold fish? to ?fish,? indicat-
ing that hypernym?hyponym relations should be
more complicated than a single vector offset can
represent. To verify this hypothesis, we com-
pute the embedding offsets over all hypernym?
1201
???-????sportsman - footballer ??-???staff - civil servant??-??laborer - gardener??-???seaman - navigator??-??actor - singer ??-??actor - protagonist??-??actor - clown
??-??position - headmaster
??-???actor - matador
??-???laborer - temporary worker ??-??laborer - carpenter ??-???position ? consul general
??-??staff - airline hostess??-???staff - salesclerk??-???staff - conductor?-??chicken - cock?-????sheep - small-tail Han sheep?-??sheep - ram ?-??equus - zebra ?-??shrimp - prawn
?-??dog - police dog?-???rabbit - wool rabbit
??-???dolphin - white-flag dolphin ?-??fish - shark ?-???fish - tropical fish?-??fish - gold fish
?-??crab - sea crab
?-??donkey - wild ass
Figure 2: Clusters of the vector offsets in training data. The figure shows that the vector offsets distribute
in some clusters. The left cluster shows some hypernym?hyponym relations about animals. The right
one shows some relations about people?s occupations.
hyponym word pairs in our training data and vi-
sualize them.
2
Figure 2 shows that the relations
are adequately distributed in the clusters, which
implies that hypernym?hyponym relations in-
deed can be decomposed into more fine-grained
relations. Moreover, the relations about animals
are spatially close, but separate from the relations
about people?s occupations.
To address this challenge, we propose to learn
the hypernym?hyponym relations using projection
matrices.
3.3.1 A Uniform Linear Projection
Intuitively, we assume that all words can be pro-
jected to their hypernyms based on a uniform tran-
sition matrix. That is, given a word x and its hy-
pernym y, there exists a matrix ? so that y = ?x.
For simplicity, we use the same symbols as the
words to represent the embedding vectors. Ob-
taining a consistent exact ? for the projection of
all hypernym?hyponym pairs is difficult. Instead,
we can learn an approximate ? using Equation 1
on the training data, which minimizes the mean-
squared error:
?
?
= arg min
?
1
N
?
(x,y)
? ?x? y ?
2
(1)
where N is the number of (x, y) word pairs in
the training data. This is a typical linear regres-
sion problem. The only difference is that our pre-
dictions are multi-dimensional vectors instead of
scalar values. We use SGD for optimization.
2
Principal Component Analysis (PCA) is applied for di-
mensionality reduction.
3.3.2 Piecewise Linear Projections
A uniform linear projection may still be under-
representative for fitting all of the hypernym?
hyponym word pairs, because the relations are
rather diverse, as shown in Figure 2. To better
model the various kinds of hypernym?hyponym
relations, we apply the idea of piecewise linear re-
gression (Ritzema, 1994) in this study.
Specifically, the input space is first segmented
into several regions. That is, all word pairs (x, y)
in the training data are first clustered into sever-
al groups, where word pairs in each group are
expected to exhibit similar hypernym?hyponym
relations. Each word pair (x, y) is represented
with their vector offsets: y ? x for clustering.
The reasons are twofold: (1) Mikolov?s work has
shown that the vector offsets imply a certain lev-
el of semantic relationship. (2) The vector off-
sets distribute in clusters well, and the word pairs
which are close indeed represent similar relations,
as shown in Figure 2.
Then we learn a separate projection for each
cluster, respectively (Equation 2).
?
?
k
= arg min
?
k
1
N
k
?
(x,y)?C
k
? ?
k
x? y ?
2
(2)
where N
k
is the amount of word pairs in the k
th
cluster C
k
.
We use the k-means algorithm for clustering,
where k is tuned on a development dataset.
3.3.3 Training Data
To learn the projection matrices, we extract train-
ing data from a Chinese semantic thesaurus,
Tongyi Cilin (Extended) (CilinE for short) which
1202
?
?
?
?
?
Root
L ev el 1
L ev el 2
L ev el 3
L ev el 4
L ev el 5
?  ob j ect
??  animal
??  insect
- -
??  dragonf ly
B
i
18
A
06@
?? : ?? 
( dragonf ly  :  animal)
?? : ?? 
( dragonf ly  :  insect)
C ilinEh y perny m-h y pony m pairs
S ense C ode:  B i1 8 A0 6 @
S ense C ode:  B i1 8 A
S ense C ode:  B i1 8
S ense C ode:  B i
S ense C ode:  B
?? : ?? 
( insect :  animal)
Figure 3: Hierarchy of CilinE and an Example of
Training Data Generation
contains 100,093 words (Che et al, 2010).
3
CilinE
is organized as a hierarchy of five levels, in which
the words are linked by hypernym?hyponym
relations (right panel, Figure 3). Each word in
CilinE has one or more sense codes (some words
are polysemous) that indicate its position in the hi-
erarchy.
The senses of words in the first level, such as
?? (object)? and ??? (time),? are very gen-
eral. The fourth level only has sense codes without
real words. Therefore, we extract words in the sec-
ond, third and fifth levels to constitute hypernym?
hyponym pairs (left panel, Figure 3).
Note that mapping one hyponym to multi-
ple hypernyms with the same projection (?x is
unique) is difficult. Therefore, the pairs with the
same hyponym but different hypernyms are ex-
pected to be clustered into separate groups. Fig-
ure 3 shows that the word ?dragonfly? in the
fifth level has two hypernyms: ?insect? in the
third level and ?animal? in the second level.
Hence the relations dragonfly
H
?? insect and
dragonfly
H
?? animal should fall into differ-
ent clusters.
In our implementation, we apply this constraint
by simply dividing the training data into two cat-
egories, namely, direct and indirect. Hypernym-
hyponym word pair (x, y) is classified into the di-
rect category, only if there doesn?t exist another
word z in the training data, which is a hypernym of
x and a hyponym of y. Otherwise, (x, y) is classi-
fied into the indirect category. Then, data in these
two categories are clustered separately.
3
www.ltp-cloud.com/download/
x
y
?k
? 
x'
?l
Figure 4: In this example, ?
k
x is located in the
circle with center y and radius ?. So y is consid-
ered as a hypernym of x. Conversely, y is not a
hypernym of x
?
.
x
y
z
x
y
(a) (b)
z
x
y
Figure 5: (a) If d(?
j
y, x) > d(?
k
x, y), we re-
move the path from y to x; (b) if d(?
j
y, x) >
d(?
k
x, z) and d(?
j
y, x) > d(?
i
z, y), we reverse
the path from y to x.
3.4 Hypernym-hyponym Relation
Identification
Upon obtaining the clusters of training data and
the corresponding projections, we can identify
whether two words have a hypernym?hyponym re-
lation. Given two words x and y, we find cluster
C
k
whose center is closest to the offset y ? x, and
obtain the corresponding projection ?
k
. For y to
be considered a hypernym of x, one of the two
conditions below must hold.
Condition 1: The projection ?
k
puts ?
k
x close
enough to y (Figure 4). Formally, the euclidean
distance between ?
k
x and y: d(?
k
x, y) must be
less than a threshold ?.
d(?
k
x, y) =? ?
k
x? y ?
2
< ? (3)
Condition 2: There exists another word z sat-
isfying x
H
??z and z
H
??y. In this case, we use the
transitivity of hypernym?hyponym relations.
Besides, the final hierarchy should be a DAG
as discussed in Section 3.1. However, the pro-
jection method cannot guarantee that theoretical-
ly, because the projections are learned from pair-
wise hypernym?hyponym relations without the w-
hole hierarchy structure. All pairwise hypernym?
hyponym relation identification methods would
suffer from this problem actually. It is an inter-
esting problem how to construct a globally opti-
1203
mal semantic hierarchy conforming to the form
of a DAG. But this is not the focus of this paper.
So if some conflicts occur, that is, a relation cir-
cle exists, we remove or reverse the weakest path
heuristically (Figure 5). If a circle has only two
nodes, we remove the weakest path. If a circle has
more than two nodes, we reverse the weakest path
to form an indirect hypernym?hyponym relation.
4 Experimental Setup
4.1 Experimental Data
In this work, we learn word embeddings from a
Chinese encyclopedia corpus named Baidubaike
4
,
which contains about 30 million sentences (about
780 million words). The Chinese segmentation
is provided by the open-source Chinese language
processing platform LTP
5
(Che et al, 2010).
Then, we employ the Skip-gram method (Section
3.2) to train word embeddings. Finally we obtain
the embedding vectors of 0.56 million words.
The training data for projection learning is
collected from CilinE (Section 3.3.3). We ob-
tain 15,247 word pairs of hypernym?hyponym
relations (9,288 for direct relations and 5,959 for
indirect relations).
For evaluation, we collect the hypernyms for
418 entities, which are selected randomly from
Baidubaike, following Fu et al (2013). We then
ask two annotators to manually label the seman-
tic hierarchies of the correct hypernyms. The final
data set contains 655 unique hypernyms and 1,391
hypernym?hyponym relations among them. We
randomly split the labeled data into 1/5 for de-
velopment and 4/5 for testing (Table 2). The hi-
erarchies are represented as relations of pairwise
words. We measure the inter-annotator agreement
using the kappa coefficient (Siegel and Castel-
lan Jr, 1988). The kappa value is 0.96, which indi-
cates a good strength of agreement.
4.2 Evaluation Metrics
We use precision, recall, and F-score as our met-
rics to evaluate the performances of the methods.
Since hypernym?hyponym relations and its re-
verse (hyponym?hypernym) have one-to-one cor-
respondence, their performances are equal. For
4
Baidubaike (baike.baidu.com) is one of the largest
Chinese encyclopedias containing more than 7.05 million en-
tries as of September, 2013.
5
www.ltp-cloud.com/demo/
Relation
# of word pairs
Dev. Test
hypernym?hyponym 312 1,079
hyponym?hypernym
?
312 1,079
unrelated 1,044 3,250
Total 1,668 5,408
Table 2: The evaluation data.
?
Since hypernym?
hyponym relations and hyponym?hypernym
relations have one-to-one correspondence, their
numbers are the same.
1 5 
10 15 
20 
0.45 
0.5 
0.55 
0.6 
0.65 
0.7 
0.75 
0.8 
1 10 20 30 40 50 60 Indirect 
F1-
Sco
re 
Direct 
Figure 6: Performance on development data w.r.t.
cluster size.
simplicity, we only report the performance of the
former in the experiments.
5 Results and Analysis
5.1 Varying the Amount of Clusters
We first evaluate the effect of different number of
clusters based on the development data. We vary
the numbers of the clusters both for the direct and
indirect training word pairs.
As shown in Figure 6, the performance of clus-
tering is better than non-clustering (when the clus-
ter number is 1), thus providing evidences that
learning piecewise projections based on clustering
is reasonable. We finally set the numbers of the
clusters of direct and indirect to 20 and 5, respec-
tively, where the best performances are achieved
on the development data.
5.2 Comparison with Previous Work
In this section, we compare the proposed method
with previous methods, including manually-built
hierarchy extension, pairwise relation extraction
1204
P(%) R(%) F(%)
M
Wiki+CilinE
92.41 60.61 73.20
M
Pattern
97.47 21.41 35.11
M
Snow
60.88 25.67 36.11
M
balApinc
54.96 53.38 54.16
M
invCL
49.63 62.84 55.46
M
Fu
87.40 48.19 62.13
M
Emb
80.54 67.99 73.74
M
Emb+CilinE
80.59 72.42 76.29
M
Emb+Wiki+CilinE
79.78 80.81 80.29
Table 3: Comparison of the proposed method with
existing methods in the test set.
Pattern Translation
w?[??|??] h w is a [a kind of] h
w [?]? h w[,] and other h
h [?]?[?] w h[,] called w
h [?] [?]? w h[,] such as w
h [?]??? w h[,] especially w
Table 4: Chinese Hearst-style lexical patterns. The
contents in square brackets are omissible.
based on patterns, word distributions, and web
mining (Section 2). Results are shown in Table 3.
5.2.1 Overall Comparison
M
Wiki+CilinE
refers to the manually-built hierar-
chy extension method of Suchanek et al (2008).
In our experiment, we use the category taxonomy
of Chinese Wikipedia
6
to extend CilinE. Table 3
shows that this method achieves a high precision
but also a low recall, mainly because of the limit-
ed scope of Wikipedia.
M
Pattern
refers to the pattern-based method of
Hearst (1992). We extract hypernym?hyponym
relations in the Baidubaike corpus, which is al-
so used to train word embeddings (Section 4.1).
We use the Chinese Hearst-style patterns (Table
4) proposed by Fu et al (2013), in which w rep-
resents a word, and h represents one of its hy-
pernyms. The result shows that only a small part
of the hypernyms can be extracted based on these
patterns because only a few hypernym relations
are expressed in these fixed patterns, and many are
expressed in highly flexible manners.
In the same corpus, we apply the method
M
Snow
originally proposed by Snow et al (2005).
The same training data for projections learn-
6
dumps.wikimedia.org/zhwiki/20131205/
ing from CilinE (Section 3.3.3) is used as
seed hypernym?hyponym pairs. Lexico-syntactic
patterns are extracted from the Baidubaike corpus
by using the seeds. We then develop a logistic re-
gression classifier based on the patterns to recog-
nize hypernym?hyponym relations. This method
relies on an accurate syntactic parser, and the qual-
ity of the automatically extracted patterns is diffi-
cult to guarantee.
We re-implement two previous distribution-
al methods M
balApinc
(Kotlerman et al, 2010)
and M
invCL
(Lenci and Benotto, 2012) in the
Baidubaike corpus. Each word is represented as a
feature vector in which each dimension is the PMI
value of the word and its context words. We com-
pute a score for each word pair and apply a thresh-
old to identify whether it is a hypernym?hyponym
relation.
M
Fu
refers to our previous web mining
method (Fu et al, 2013). This method mines hy-
pernyms of a given word w from multiple sources
and returns a ranked list of the hypernyms. We
select the hypernyms with scores over a threshold
of each word in the test set for evaluation. This
method assumes that frequent co-occurrence of a
noun or noun phrase n in multiple sources with w
indicate possibility of n being a hypernym of w.
The results presented in Fu et al (2013) show that
the method works well when w is an entity, but
not when w is a word with a common semantic
concept. The main reason may be that there are
relatively more introductory pages about entities
than about common words in the Web.
M
Emb
is the proposed method based on word
embeddings. Table 3 shows that the proposed
method achieves a better recall and F-score than
all of the previous methods do. It can significantly
(p < 0.01) improve the F-score over the state-of-
the-art method M
Wiki+CilinE
.
M
Emb
and M
CilinE
can also be combined. The
combination strategy is to simply merge all pos-
itive results from the two methods together, and
then to infer new relations based on the transitiv-
ity of hypernym?hyponym relations. The F-score
is further improved from 73.74% to 76.29%. Note
that, the combined method achieves a 4.43% re-
call improvement over M
Emb
, but the precision is
almost unchanged. The reason is that the infer-
ence based on the relations identified automatical-
ly may lead to error propagation. For example, the
relation x
H
??y is incorrectly identified by M
Emb
.
1205
P(%) R(%) F(%)
M
Wiki+CilinE
80.39 19.29 31.12
M
Emb+CilinE
71.16 52.80 60.62
M
Emb+Wiki+CilinE
69.13 61.65 65.17
Table 5: Performance on the out-of-CilinE data in
the test set.
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
Recall
Prec
ision
l l lllllll
l
l
l ll
l MEmb+Wiki+Ci l inEMEmb+Ci l inEMWiki+Ci l inE
Figure 7: Precision-Recall curves on the out-of-
CilinE data in the test set.
When the relation y
H
??z from M
CilinE
is added, it
will cause a new incorrect relation x
H
??z.
Combining M
Emb
with M
Wiki+CilinE
achieves
a 7% F-score improvement over the best baseline
M
Wiki+CilinE
. Therefore, the proposed method
is complementary to the manually-built hierarchy
extension method (Suchanek et al, 2008).
5.2.2 Comparison on the Out-of-CilinE Data
We are greatly interested in the practical perfor-
mance of the proposed method on the hypernym?
hyponym relations outside of CilinE. We say a
word pair is outside of CilinE, as long as there
is one word in the pair not existing in CilinE. In
our test data, about 62% word pairs are outside
of CilinE. Table 5 shows the performances of the
best baseline method and our method on the out-
of-CilinE data. The method exploiting the tax-
onomy in Wikipedia, M
Wiki+CilinE
, achieves the
highest precision but has a low recall. By con-
trast, our method can discover more hypernym?
hyponym relations with some loss of precision,
thereby achieving a more than 29% F-score im-
provement. The combination of these two meth-
ods achieves a further 4.5% F-score improvement
over M
Emb+CilinE
. Generally speaking, the pro-
posed method greatly improves the recall but dam-
ages the precision.
Actually, we can get different precisions and re-
??organism
??plant
???Ranunculaceae
???Aconitum
??aconite
???medicinal plant
??medicine
( a)  C ilinE
??organism
??plant
???Ranunculaceae
???Aconitum
??aconite
???medicinal plant
??medicine
( b )  W ik ipedia+ C ilinE
??organism
??plant
???Ranunculaceae
???Aconitum
??aconite
???medicinal plant
??medicine
( c)  E mb edding
??organism
??plant
???Ranunculaceae
???Aconitum
??aconite
???medicinal plant
??medicine
( d)  E mb edding+ W ik ipedia+ C ilinE
Figure 8: An example for error analysis. The
red paths refer to the relations between the named
entity and its hypernyms extracted using the web
mining method (Fu et al, 2013). The black paths
with hollow arrows denote the relations identified
by the different methods. The boxes with dotted
borders refer to the concepts which are not linked
to correct positions.
calls by adjusting the threshold ? (Equation 3).
Figure 7 shows that M
Emb+CilinE
achieves a high-
er precision than M
Wiki+CilinE
when their recalls
are the same. When they achieve the same preci-
sion, the recall of M
Emb+CilinE
is higher.
5.3 Error Analysis and Discussion
We analyze error cases after experiments. Some
cases are shown in Figure 8. We can see that
there is only one general relation ??? (plant)?
H
?? ??? (organism)? existing in CilinE. Some
fine-grained relations exist in Wikipedia, but the
coverage is limited. Our method based on
word embeddings can discover more hypernym?
hyponym relations than the previous methods can.
When we combine the methods together, we get
the correct hierarchy.
Figure 8 shows that our method loses the
relation ???? (Aconitum)? H?? ????
(Ranunculaceae).? It is because they are
very semantically similar (their cosine similarity
is 0.9038). Their representations are so close to
each other in the embedding space that we have
not find projections suitable for these pairs. The
1206
error statistics show that when the cosine similari-
ties of word pairs are greater than 0.8, the recall is
only 9.5%. This kind of error accounted for about
10.9% among all the errors in our test set. One
possible solution may be adding more data of this
kind to the training set.
6 Related Work
In addition to the works mentioned in Section 2,
we introduce another set of related studies in this
section.
Evans (2004), Ortega-Mendoza et al (2007),
and Sang (2007) consider web data as a large cor-
pus and use search engines to identify hypernyms
based on the lexical patterns of Hearst (1992).
However, the low quality of the sentences in the
search results negatively influence the precision of
hypernym extraction.
Following the method for discovering patterns
automatically (Snow et al, 2005), McNamee et
al. (2008) apply the same method to extract hy-
pernyms of entities in order to improve the perfor-
mance of a question answering system. Ritter et al
(2009) propose a method based on patterns to find
hypernyms on arbitrary noun phrases. They use
a support vector machine classifier to identify the
correct hypernyms from the candidates that match
the patterns. As our experiments show, pattern-
based methods suffer from low recall because of
the low coverage of patterns.
Besides Kotlerman et al (2010) and Lenci and
Benotto (2012), other researchers also propose di-
rectional distributional similarity methods (Weeds
et al, 2004; Geffet and Dagan, 2005; Bhagat et al,
2007; Szpektor et al, 2007; Clarke, 2009). How-
ever, their basic assumption that a hyponym can
only be used in contexts where its hypernyms can
be used and that a hypernym might be used in all
of the contexts where its hyponyms are used may
not always rational.
Snow et al (2006) provides a global optimiza-
tion scheme for extending WordNet, which is d-
ifferent from the above-mentioned pairwise rela-
tionships identification methods.
Word embeddings have been successfully ap-
plied in many applications, such as in sentiment
analysis (Socher et al, 2011b), paraphrase detec-
tion (Socher et al, 2011a), chunking, and named
entity recognition (Turian et al, 2010; Collobert
et al, 2011). These applications mainly utilize
the representing power of word embeddings to al-
leviate the problem of data sparsity. Mikolov et
al. (2013a) and Mikolov et al (2013b) further ob-
serve that the semantic relationship of words can
be induced by performing simple algebraic oper-
ations with word vectors. Their work indicates
that word embeddings preserve some interesting
linguistic regularities, which might provide sup-
port for many applications. In this paper, we
improve on their work by learning multiple lin-
ear projections in the embedding space, to model
hypernym?hyponym relationships within different
clusters.
7 Conclusion and Future Work
This paper proposes a novel method for seman-
tic hierarchy construction based on word em-
beddings, which are trained using a large-scale
corpus. Using the word embeddings, we learn
the hypernym?hyponym relationship by estimat-
ing projection matrices which map words to their
hypernyms. Further improvements are made us-
ing a cluster-based approach in order to model
the more fine-grained relations. Then we propose
a few simple criteria to identity whether a new
word pair is a hypernym?hyponym relation. Based
on the pairwise hypernym?hyponym relations, we
build semantic hierarchies automatically.
In our experiments, the proposed method signif-
icantly outperforms state-of-the-art methods and
achieves the best F1-score of 73.74% on a manual-
ly labeled test dataset. Further experiments show
that our method is complementary to the previous
manually-built hierarchy extension methods.
For future work, we aim to improve word
embedding learning under the guidance of
hypernym?hyponym relations. By including the
hypernym?hyponym relation constraints while
training word embeddings, we expect to improve
the embeddings such that they become more suit-
able for this task.
Acknowledgments
This work was supported by National Natu-
ral Science Foundation of China (NSFC) via
grant 61133012, 61273321 and the National 863
Leading Technology Research Project via grant
2012AA011102. Special thanks to Shiqi Zhao,
Zhenghua Li, Wei Song and the anonymous re-
viewers for insightful comments and suggestions.
We also thank Xinwei Geng and Hongbo Cai for
their help in the experiments.
1207
References
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search, 3:1137?1155.
Rahul Bhagat, Patrick Pantel, Eduard H Hovy, and Ma-
rina Rey. 2007. Ledir: An unsupervised algorith-
m for learning directionality of inference rules. In
EMNLP-CoNLL, pages 161?170.
Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp:
A chinese language technology platform. In Coling
2010: Demonstrations, pages 13?16, Beijing, Chi-
na, August.
Daoud Clarke. 2009. Context-theoretic semantics for
natural language: an overview. In Proceedings of
the Workshop on Geometrical Models of Natural
Language Semantics, pages 112?119. Association
for Computational Linguistics.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Paramveer Dhillon, Dean P Foster, and Lyle H Ungar.
2011. Multi-view learning of word embeddings via
cca. In Advances in Neural Information Processing
Systems, pages 199?207.
Richard Evans. 2004. A framework for named entity
recognition in the open domain. Recent Advances in
Natural Language Processing III: Selected Papers
from RANLP 2003, 260:267?274.
Ruiji Fu, Bing Qin, and Ting Liu. 2013. Exploiting
multiple sources for open-domain hypernym discov-
ery. In EMNLP, pages 1224?1234.
Maayan Geffet and Ido Dagan. 2005. The distribution-
al inclusion hypotheses and lexical entailment. In
Proceedings of the 43rd Annual Meeting on Associa-
tion for Computational Linguistics, pages 107?114.
Association for Computational Linguistics.
Marti A Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th conference on Computational linguistics-
Volume 2, pages 539?545. Association for Compu-
tational Linguistics.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribution-
al similarity for lexical inference. Natural Language
Engineering, 16(4):359?389.
Alessandro Lenci and Giulia Benotto. 2012. Identify-
ing hypernyms in distributional semantic spaces. In
Proceedings of the Sixth International Workshop on
Semantic Evaluation, pages 75?79. Association for
Computational Linguistics.
Paul McNamee, Rion Snow, Patrick Schone, and James
Mayfield. 2008. Learning named entity hyponyms
for question answering. In Proceedings of the
Third International Joint Conference on Natural
Language Processing, pages 799?804.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word rep-
resentations in vector space. arXiv preprint arX-
iv:1301.3781.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of NAACL-
HLT, pages 746?751.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39?41.
Andriy Mnih and Geoffrey E Hinton. 2008. A s-
calable hierarchical distributed language model. In
Advances in neural information processing systems,
pages 1081?1088.
Rosa M Ortega-Mendoza, Luis Villase?nor-Pineda, and
Manuel Montes-y G?omez. 2007. Using lexical
patterns for extracting hyponyms from the web. In
MICAI 2007: Advances in Artificial Intelligence,
pages 904?911. Springer.
Alan Ritter, Stephen Soderland, and Oren Etzioni.
2009. What is this, anyway: Automatic hypernym
discovery. In Proceedings of the 2009 AAAI Spring
Symposium on Learning by Reading and Learning
to Read, pages 88?93.
HP Ritzema. 1994. Drainage principles and
applications.
Erik Tjong Kim Sang. 2007. Extracting hypernym
pairs from the web. In Proceedings of the 45th An-
nual Meeting of the ACL on Interactive Poster and
Demonstration Sessions, pages 165?168. Associa-
tion for Computational Linguistics.
Sidney Siegel and N John Castellan Jr. 1988. Non-
parametric statistics for the behavioral sciences.
McGraw-Hill, New York.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Lawrence K. Saul, Yair Weiss, and
L?eon Bottou, editors, Advances in Neural Informa-
tion Processing Systems 17, pages 1297?1304. MIT
Press, Cambridge, MA.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computation-
al Linguistics, pages 801?808, Sydney, Australia,
July. Association for Computational Linguistics.
1208
Richard Socher, Eric H Huang, Jeffrey Pennin, Christo-
pher D Manning, and Andrew Ng. 2011a. Dynam-
ic pooling and unfolding recursive autoencoders for
paraphrase detection. In Advances in Neural Infor-
mation Processing Systems, pages 801?809.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011b.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151?161. Association for
Computational Linguistics.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. Yago: A large ontology from
wikipedia and wordnet. Web Semantics: Sci-
ence, Services and Agents on the World Wide Web,
6(3):203?217.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acqui-
sition. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
456?463, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394. Association for
Computational Linguistics.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of the 20th internation-
al conference on Computational Linguistics, page
1015. Association for Computational Linguistics.
1209
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1555?1565,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Learning Sentiment-Specific Word Embedding
for Twitter Sentiment Classification
?
Duyu Tang
?
, Furu Wei
?
, Nan Yang
\
, Ming Zhou
?
, Ting Liu
?
, Bing Qin
?
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
?
Microsoft Research, Beijing, China
\
University of Science and Technology of China, Hefei, China
{dytang, tliu, qinb}@ir.hit.edu.cn
{fuwei, v-nayang, mingzhou}@microsoft.com
Abstract
We present a method that learns word em-
bedding for Twitter sentiment classifica-
tion in this paper. Most existing algorithm-
s for learning continuous word represen-
tations typically only model the syntactic
context of words but ignore the sentimen-
t of text. This is problematic for senti-
ment analysis as they usually map word-
s with similar syntactic context but oppo-
site sentiment polarity, such as good and
bad, to neighboring word vectors. We
address this issue by learning sentiment-
specific word embedding (SSWE), which
encodes sentiment information in the con-
tinuous representation of words. Specif-
ically, we develop three neural networks
to effectively incorporate the supervision
from sentiment polarity of text (e.g. sen-
tences or tweets) in their loss function-
s. To obtain large scale training corpora,
we learn the sentiment-specific word em-
bedding from massive distant-supervised
tweets collected by positive and negative
emoticons. Experiments on applying SS-
WE to a benchmark Twitter sentimen-
t classification dataset in SemEval 2013
show that (1) the SSWE feature performs
comparably with hand-crafted features in
the top-performed system; (2) the perfor-
mance is further improved by concatenat-
ing SSWE with existing feature set.
1 Introduction
Twitter sentiment classification has attracted in-
creasing research interest in recent years (Jiang et
al., 2011; Hu et al, 2013). The objective is to clas-
sify the sentiment polarity of a tweet as positive,
?
This work was done when the first and third authors
were visiting Microsoft Research Asia.
negative or neutral. The majority of existing ap-
proaches follow Pang et al (2002) and employ ma-
chine learning algorithms to build classifiers from
tweets with manually annotated sentiment polar-
ity. Under this direction, most studies focus on
designing effective features to obtain better clas-
sification performance. For example, Mohammad
et al (2013) build the top-performed system in the
Twitter sentiment classification track of SemEval
2013 (Nakov et al, 2013), using diverse sentiment
lexicons and a variety of hand-crafted features.
Feature engineering is important but labor-
intensive. It is therefore desirable to discover ex-
planatory factors from the data and make the learn-
ing algorithms less dependent on extensive fea-
ture engineering (Bengio, 2013). For the task of
sentiment classification, an effective feature learn-
ing method is to compose the representation of a
sentence (or document) from the representation-
s of the words or phrases it contains (Socher et
al., 2013b; Yessenalina and Cardie, 2011). Ac-
cordingly, it is a crucial step to learn the word
representation (or word embedding), which is a
dense, low-dimensional and real-valued vector for
a word. Although existing word embedding learn-
ing algorithms (Collobert et al, 2011; Mikolov et
al., 2013) are intuitive choices, they are not effec-
tive enough if directly used for sentiment classi-
fication. The most serious problem is that tradi-
tional methods typically model the syntactic con-
text of words but ignore the sentiment information
of text. As a result, words with opposite polari-
ty, such as good and bad, are mapped into close
vectors. It is meaningful for some tasks such as
pos-tagging (Zheng et al, 2013) as the two words
have similar usages and grammatical roles, but it
becomes a disaster for sentiment analysis as they
have the opposite sentiment polarity.
In this paper, we propose learning sentiment-
specific word embedding (SSWE) for sentiment
analysis. We encode the sentiment information in-
1555
to the continuous representation of words, so that
it is able to separate good and bad to opposite ends
of the spectrum. To this end, we extend the ex-
isting word embedding learning algorithm (Col-
lobert et al, 2011) and develop three neural net-
works to effectively incorporate the supervision
from sentiment polarity of text (e.g. sentences
or tweets) in their loss functions. We learn the
sentiment-specific word embedding from tweet-
s, leveraging massive tweets with emoticons as
distant-supervised corpora without any manual an-
notations. These automatically collected tweet-
s contain noises so they cannot be directly used
as gold training data to build sentiment classifier-
s, but they are effective enough to provide weak-
ly supervised signals for training the sentiment-
specific word embedding.
We apply SSWE as features in a supervised
learning framework for Twitter sentiment classi-
fication, and evaluate it on the benchmark dataset
in SemEval 2013. In the task of predicting posi-
tive/negative polarity of tweets, our method yields
84.89% in macro-F1 by only using SSWE as fea-
ture, which is comparable to the top-performed
system based on hand-crafted features (84.70%).
After concatenating the SSWE feature with ex-
isting feature set, we push the state-of-the-art to
86.58% in macro-F1. The quality of SSWE is al-
so directly evaluated by measuring the word sim-
ilarity in the embedding space for sentiment lexi-
cons. In the accuracy of polarity consistency be-
tween each sentiment word and its top N closest
words, SSWE outperforms existing word embed-
ding learning algorithms.
The major contributions of the work presented
in this paper are as follows.
? We develop three neural networks to learn
sentiment-specific word embedding (SSWE)
from massive distant-supervised tweets with-
out any manual annotations;
? To our knowledge, this is the first work that
exploits word embedding for Twitter senti-
ment classification. We report the results that
the SSWE feature performs comparably with
hand-crafted features in the top-performed
system in SemEval 2013;
? We release the sentiment-specific word em-
bedding learned from 10 million tweets,
which can be adopted off-the-shell in other
sentiment analysis tasks.
2 Related Work
In this section, we present a brief review of the
related work from two perspectives, Twitter senti-
ment classification and learning continuous repre-
sentations for sentiment classification.
2.1 Twitter Sentiment Classification
Twitter sentiment classification, which identifies
the sentiment polarity of short, informal tweets,
has attracted increasing research interest (Jiang et
al., 2011; Hu et al, 2013) in recent years. Gen-
erally, the methods employed in Twitter sentiment
classification follow traditional sentiment classifi-
cation approaches. The lexicon-based approaches
(Turney, 2002; Ding et al, 2008; Taboada et al,
2011; Thelwall et al, 2012) mostly use a dictio-
nary of sentiment words with their associated sen-
timent polarity, and incorporate negation and in-
tensification to compute the sentiment polarity for
each sentence (or document).
The learning based methods for Twitter sen-
timent classification follow Pang et al (2002)?s
work, which treat sentiment classification of texts
as a special case of text categorization issue. Many
studies on Twitter sentiment classification (Pak
and Paroubek, 2010; Davidov et al, 2010; Bar-
bosa and Feng, 2010; Kouloumpis et al, 2011;
Zhao et al, 2012) leverage massive noisy-labeled
tweets selected by positive and negative emoticon-
s as training set and build sentiment classifiers di-
rectly, which is called distant supervision (Go et
al., 2009). Instead of directly using the distant-
supervised data as training set, Liu et al (2012)
adopt the tweets with emoticons to smooth the lan-
guage model and Hu et al (2013) incorporate the
emotional signals into an unsupervised learning
framework for Twitter sentiment classification.
Many existing learning based methods on Twit-
ter sentiment classification focus on feature engi-
neering. The reason is that the performance of sen-
timent classifier being heavily dependent on the
choice of feature representation of tweets. The
most representative system is introduced by Mo-
hammad et al (2013), which is the state-of-the-
art system (the top-performed system in SemEval
2013 Twitter Sentiment Classification Track) by
implementing a number of hand-crafted features.
Unlike the previous studies, we focus on learning
discriminative features automatically from mas-
sive distant-supervised tweets.
1556
2.2 Learning Continuous Representations for
Sentiment Classification
Pang et al (2002) pioneer this field by using bag-
of-word representation, representing each word as
a one-hot vector. It has the same length as the size
of the vocabulary, and only one dimension is 1,
with all others being 0. Under this assumption,
many feature learning algorithms are proposed to
obtain better classification performance (Pang and
Lee, 2008; Liu, 2012; Feldman, 2013). However,
the one-hot word representation cannot sufficient-
ly capture the complex linguistic characteristics of
words.
With the revival of interest in deep learn-
ing (Bengio et al, 2013), incorporating the con-
tinuous representation of a word as features has
been proving effective in a variety of NLP tasks,
such as parsing (Socher et al, 2013a), language
modeling (Bengio et al, 2003; Mnih and Hin-
ton, 2009) and NER (Turian et al, 2010). In the
field of sentiment analysis, Bespalov et al (2011;
2012) initialize the word embedding by Laten-
t Semantic Analysis and further represent each
document as the linear weighted of ngram vec-
tors for sentiment classification. Yessenalina and
Cardie (2011) model each word as a matrix and
combine words using iterated matrix multiplica-
tion. Glorot et al (2011) explore Stacked Denois-
ing Autoencoders for domain adaptation in sen-
timent classification. Socher et al propose Re-
cursive Neural Network (RNN) (2011b), matrix-
vector RNN (2012) and Recursive Neural Tensor
Network (RNTN) (2013b) to learn the composi-
tionality of phrases of any length based on the
representation of each pair of children recursively.
Hermann et al (2013) present Combinatory Cate-
gorial Autoencoders to learn the compositionality
of sentence, which marries the Combinatory Cat-
egorial Grammar with Recursive Autoencoder.
The representation of words heavily relies on
the applications or tasks in which it is used (Lab-
utov and Lipson, 2013). This paper focuses
on learning sentiment-specific word embedding,
which is tailored for sentiment analysis. Un-
like Maas et al (2011) that follow the proba-
bilistic document model (Blei et al, 2003) and
give an sentiment predictor function to each word,
we develop neural networks and map each n-
gram to the sentiment polarity of sentence. Un-
like Socher et al (2011c) that utilize manually
labeled texts to learn the meaning of phrase (or
sentence) through compositionality, we focus on
learning the meaning of word, namely word em-
bedding, from massive distant-supervised tweets.
Unlike Labutov and Lipson (2013) that produce
task-specific embedding from an existing word
embedding, we learn sentiment-specific word em-
bedding from scratch.
3 Sentiment-Specific Word Embedding
for Twitter Sentiment Classification
In this section, we present the details of learn-
ing sentiment-specific word embedding (SSWE)
for Twitter sentiment classification. We pro-
pose incorporating the sentiment information of
sentences to learn continuous representations for
words and phrases. We extend the existing word
embedding learning algorithm (Collobert et al,
2011) and develop three neural networks to learn
SSWE. In the following sections, we introduce the
traditional method before presenting the details of
SSWE learning algorithms. We then describe the
use of SSWE in a supervised learning framework
for Twitter sentiment classification.
3.1 C&W Model
Collobert et al (2011) introduce C&W model to
learn word embedding based on the syntactic con-
texts of words. Given an ngram ?cat chills on a
mat?, C&W replaces the center word with a ran-
dom wordw
r
and derives a corrupted ngram ?cat
chills w
r
a mat?. The training objective is that the
original ngram is expected to obtain a higher lan-
guage model score than the corrupted ngram by a
margin of 1. The ranking objective function can
be optimized by a hinge loss,
loss
cw
(t, t
r
) = max(0, 1? f
cw
(t) + f
cw
(t
r
))
(1)
where t is the original ngram, t
r
is the corrupted
ngram, f
cw
(?) is a one-dimensional scalar repre-
senting the language model score of the input n-
gram. Figure 1(a) illustrates the neural architec-
ture of C&W, which consists of four layers, name-
ly lookup ? linear ? hTanh ? linear (from
bottom to top). The original and corrupted ngram-
s are treated as inputs of the feed-forward neural
network, respectively. The output f
cw
is the lan-
guage model score of the input, which is calculat-
ed as given in Equation 2, where L is the lookup
table of word embedding,w
1
, w
2
, b
1
, b
2
are the pa-
rameters of linear layers.
f
cw
(t) = w
2
(a) + b
2
(2)
1557
so cooool :D 
lookup 
linear 
hTanh 
linear 
softmax 
(a) C&W 
so cooool :D 
(b) SSWEh 
so cooool :D 
(c) SSWEu 
syntactic 
sentiment 
positive 
negative 
Figure 1: The traditional C&W model and our neural networks (SSWE
h
and SSWE
u
) for learning
sentiment-specific word embedding.
a = hTanh(w
1
L
t
+ b
1
) (3)
hTanh(x) =
?
?
?
?
?
?1 if x < ?1
x if ? 1 ? x ? 1
1 if x > 1
(4)
3.2 Sentiment-Specific Word Embedding
Following the traditional C&W model (Collobert
et al, 2011), we incorporate the sentiment infor-
mation into the neural network to learn sentiment-
specific word embedding. We develop three neural
networks with different strategies to integrate the
sentiment information of tweets.
Basic Model 1 (SSWE
h
). As an unsupervised
approach, C&W model does not explicitly capture
the sentiment information of texts. An intuitive
solution to integrate the sentiment information is
predicting the sentiment distribution of text based
on input ngram. We do not utilize the entire sen-
tence as input because the length of different sen-
tences might be variant. We therefore slide the
window of ngram across a sentence, and then pre-
dict the sentiment polarity based on each ngram
with a shared neural network. In the neural net-
work, the distributed representation of higher lay-
er are interpreted as features describing the input.
Thus, we utilize the continuous vector of top layer
to predict the sentiment distribution of text.
Assuming there are K labels, we modify the di-
mension of top layer in C&W model as K and
add a softmax layer upon the top layer. The
neural network (SSWEh) is given in Figure 1(b).
Softmax layer is suitable for this scenario be-
cause its outputs are interpreted as conditional
probabilities. Unlike C&W, SSWE
h
does not gen-
erate any corrupted ngram. Let f
g
(t), where K
denotes the number of sentiment polarity label-
s, be the gold K-dimensional multinomial distri-
bution of input t and
?
k
f
g
k
(t) = 1. For pos-
itive/negative classification, the distribution is of
the form [1,0] for positive and [0,1] for negative.
The cross-entropy error of the softmax layer is :
loss
h
(t) = ?
?
k={0,1}
f
g
k
(t) ? log(f
h
k
(t)) (5)
where f
g
(t) is the gold sentiment distribution and
f
h
(t) is the predicted sentiment distribution.
Basic Model 2 (SSWE
r
). SSWE
h
is trained by
predicting the positive ngram as [1,0] and the neg-
ative ngram as [0,1]. However, the constraint of
SSWE
h
is too strict. The distribution of [0.7,0.3]
can also be interpreted as a positive label because
the positive score is larger than the negative s-
core. Similarly, the distribution of [0.2,0.8] indi-
cates negative polarity. Based on the above obser-
vation, the hard constraints in SSWE
h
should be
relaxed. If the sentiment polarity of a tweet is pos-
itive, the predicted positive score is expected to be
larger than the predicted negative score, and the
exact reverse if the tweet has negative polarity.
We model the relaxed constraint with a rank-
ing objective function and borrow the bottom four
layers from SSWE
h
, namely lookup? linear ?
hTanh ? linear in Figure 1(b), to build the re-
laxed neural network (SSWEr). Compared with
SSWE
h
, the softmax layer is removed because
SSWE
r
does not require probabilistic interpreta-
tion. The hinge loss of SSWE
r
is modeled as de-
1558
scribed below.
loss
r
(t) = max(0, 1? ?
s
(t)f
r
0
(t)
+ ?
s
(t)f
r
1
(t) )
(6)
where f
r
0
is the predicted positive score, f
r
1
is
the predicted negative score, ?
s
(t) is an indicator
function reflecting the sentiment polarity of a sen-
tence,
?
s
(t) =
{
1 if f
g
(t) = [1, 0]
?1 if f
g
(t) = [0, 1]
(7)
Similar with SSWE
h
, SSWE
r
also does not gen-
erate the corrupted ngram.
Unified Model (SSWE
u
). The C&W model
learns word embedding by modeling syntactic
contexts of words but ignoring sentiment infor-
mation. By contrast, SSWE
h
and SSWE
r
learn
sentiment-specific word embedding by integrating
the sentiment polarity of sentences but leaving out
the syntactic contexts of words. We develop a uni-
fied model (SSWEu) in this part, which captures
the sentiment information of sentences as well as
the syntactic contexts of words. SSWE
u
is illus-
trated in Figure 1(c).
Given an original (or corrupted) ngram and
the sentiment polarity of a sentence as the in-
put, SSWE
u
predicts a two-dimensional vector for
each input ngram. The two scalars (f
u
0
, f
u
1
) s-
tand for language model score and sentiment s-
core of the input ngram, respectively. The training
objectives of SSWE
u
are that (1) the original n-
gram should obtain a higher language model score
f
u
0
(t) than the corrupted ngram f
u
0
(t
r
), and (2) the
sentiment score of original ngram f
u
1
(t) should be
more consistent with the gold polarity annotation
of sentence than corrupted ngram f
u
1
(t
r
). The loss
function of SSWE
u
is the linear combination of t-
wo hinge losses,
loss
u
(t, t
r
) = ? ? loss
cw
(t, t
r
)+
(1? ?) ? loss
us
(t, t
r
)
(8)
where loss
cw
(t, t
r
) is the syntactic loss as given
in Equation 1, loss
us
(t, t
r
) is the sentiment loss
as described in Equation 9. The hyper-parameter
? weighs the two parts.
loss
us
(t, t
r
) = max(0, 1? ?
s
(t)f
u
1
(t)
+ ?
s
(t)f
u
1
(t
r
) )
(9)
Model Training. We train sentiment-specific
word embedding from massive distant-supervised
tweets collected with positive and negative emoti-
cons
1
. We crawl tweets from April 1st, 2013 to
April 30th, 2013 with TwitterAPI. We tokenize
each tweet with TwitterNLP (Gimpel et al, 2011),
remove the @user and URLs of each tweet, and fil-
ter the tweets that are too short (< 7 words). Final-
ly, we collect 10M tweets, selected by 5M tweets
with positive emoticons and 5M tweets with nega-
tive emoticons.
We train SSWE
h
, SSWE
r
and SSWE
u
by
taking the derivative of the loss through back-
propagation with respect to the whole set of pa-
rameters (Collobert et al, 2011), and use Ada-
Grad (Duchi et al, 2011) to update the parame-
ters. We empirically set the window size as 3, the
embedding length as 50, the length of hidden lay-
er as 20 and the learning rate of AdaGrad as 0.1
for all baseline and our models. We learn embed-
ding for unigrams, bigrams and trigrams separate-
ly with same neural network and same parameter
setting. The contexts of unigram (bigram/trigram)
are the surrounding unigrams (bigrams/trigrams),
respectively.
3.3 Twitter Sentiment Classification
We apply sentiment-specific word embedding for
Twitter sentiment classification under a supervised
learning framework as in previous work (Pang et
al., 2002). Instead of hand-crafting features, we
incorporate the continuous representation of word-
s and phrases as the feature of a tweet. The senti-
ment classifier is built from tweets with manually
annotated sentiment polarity.
We explore min, average and max convolu-
tional layers (Collobert et al, 2011; Socher et
al., 2011a), which have been used as simple and
effective methods for compositionality learning
in vector-based semantics (Mitchell and Lapata,
2010), to obtain the tweet representation. The re-
sult is the concatenation of vectors derived from
different convolutional layers.
z(tw) = [z
max
(tw), z
min
(tw), z
average
(tw)]
where z(tw) is the representation of tweet tw and
z
x
(tw) is the results of the convolutional layer x ?
{min,max, average}. Each convolutional layer
1
We use the emoticons selected by Hu et al (2013). The
positive emoticons are :) : ) :-) :D =), and the negative emoti-
cons are :( : ( :-( .
1559
zx
employs the embedding of unigrams, bigrams
and trigrams separately and conducts the matrix-
vector operation of x on the sequence represented
by columns in each lookup table. The output of
z
x
is the concatenation of results obtained from
different lookup tables.
z
x
(tw) = [w
x
?L
uni
?
tw
, w
x
?L
bi
?
tw
, w
x
?L
tri
?
tw
]
where w
x
is the convolutional function of z
x
,
?L?
tw
is the concatenated column vectors of the
words in the tweet. L
uni
, L
bi
and L
tri
are the
lookup tables of the unigram, bigram and trigram
embedding, respectively.
4 Experiment
We conduct experiments to evaluate SSWE by in-
corporating it into a supervised learning frame-
work for Twitter sentiment classification. We also
directly evaluate the effectiveness of the SSWE by
measuring the word similarity in the embedding
space for sentiment lexicons.
4.1 Twitter Sentiment Classification
Experiment Setup and Datasets. We conduct
experiments on the latest Twitter sentiment clas-
sification benchmark dataset in SemEval 2013
(Nakov et al, 2013). The training and develop-
ment sets were completely in full to task partici-
pants. However, we were unable to download all
the training and development sets because some
tweets were deleted or not available due to mod-
ified authorization status. The test set is directly
provided to the participants. The distribution of
our dataset is given in Table 1. We train sentiment
classifier with LibLinear (Fan et al, 2008) on the
training set, tune parameter ?c on the dev set and
evaluate on the test set. Evaluation metric is the
Macro-F1 of positive and negative categories
2
.
Positive Negative Neutral Total
Train 2,642 994 3,436 7,072
Dev 408 219 493 1,120
Test 1,570 601 1,639 3,810
Table 1: Statistics of the SemEval 2013 Twitter
sentiment classification dataset.
2
We investigate 2-class Twitter sentiment classifica-
tion (positive/negative) instead of 3-class Twitter sentiment
classification (positive/negative/neutral) in SemEval2013.
Baseline Methods. We compare our method
with the following sentiment classification algo-
rithms:
(1) DistSuper: We use the 10 million tweets se-
lected by positive and negative emoticons as train-
ing data, and build sentiment classifier with Lib-
Linear and ngram features (Go et al, 2009).
(2) SVM: The ngram features and Support Vec-
tor Machine are widely used baseline methods to
build sentiment classifiers (Pang et al, 2002). Li-
bLinear is used to train the SVM classifier.
(3) NBSVM: NBSVM (Wang and Manning,
2012) is a state-of-the-art performer on many sen-
timent classification datasets, which trades-off be-
tween Naive Bayes and NB-enhanced SVM.
(4) RAE: Recursive Autoencoder (Socher et al,
2011c) has been proven effective in many senti-
ment analysis tasks by learning compositionality
automatically. We run RAE with randomly initial-
ized word embedding.
(5) NRC: NRC builds the top-performed system
in SemEval 2013 Twitter sentiment classification
track which incorporates diverse sentiment lexi-
cons and many manually designed features. We
re-implement this system because the codes are
not publicly available
3
. NRC-ngram refers to the
feature set of NRC leaving out ngram features.
Except for DistSuper, other baseline method-
s are conducted in a supervised manner. We do
not compare with RNTN (Socher et al, 2013b) be-
cause we cannot efficiently train the RNTN model.
The reason lies in that the tweets in our dataset do
not have accurately parsed results or fine grained
sentiment labels for phrases. Another reason is
that the RNTN model trained on movie reviews
cannot be directly applied on tweets due to the d-
ifferences between domains (Blitzer et al, 2007).
Results and Analysis. Table 2 shows the macro-
F1 of the baseline systems as well as the SSWE-
based methods on positive/negative sentimen-
t classification of tweets. Distant supervision is
relatively weak because the noisy-labeled tweet-
s are treated as the gold standard, which affects
the performance of classifier. The results of bag-
of-ngram (uni/bi/tri-gram) features are not satis-
fied because the one-hot word representation can-
not capture the latent connections between words.
NBSVM and RAE perform comparably and have
3
For 3-class sentiment classification in SemEval 2013,
our re-implementation of NRC achieved 68.3%, 0.7% low-
er than NRC (69%) due to less training data.
1560
Method Macro-F1
DistSuper + unigram 61.74
DistSuper + uni/bi/tri-gram 63.84
SVM + unigram 74.50
SVM + uni/bi/tri-gram 75.06
NBSVM 75.28
RAE 75.12
NRC (Top System in SemEval) 84.73
NRC - ngram 84.17
SSWE
u
84.98
SSWE
u
+NRC 86.58
SSWE
u
+NRC-ngram 86.48
Table 2: Macro-F1 on positive/negative classifica-
tion of tweets.
a big gap in comparison with the NRC and SSWE-
based methods. The reason is that RAE and NB-
SVM learn the representation of tweets from the
small-scale manually annotated training set, which
cannot well capture the comprehensive linguistic
phenomenons of words.
NRC implements a variety of features and
reaches 84.73% in macro-F1, verifying the impor-
tance of a better feature representation for Twit-
ter sentiment classification. We achieve 84.98%
by using only SSWE
u
as features without borrow-
ing any sentiment lexicons or hand-crafted rules.
The results indicate that SSWE
u
automatically
learns discriminative features from massive tweets
and performs comparable with the state-of-the-art
manually designed features. After concatenating
SSWE
u
with the feature set of NRC, the perfor-
mance is further improved to 86.58%. We also
compare SSWE
u
with the ngram feature by inte-
grating SSWE into NRC-ngram. The concatenated
features SSWE
u
+NRC-ngram (86.48%) outperfor-
m the original feature set of NRC (84.73%).
As a reference, we apply SSWE
u
on subjec-
tive classification of tweets, and obtain 72.17% in
macro-F1 by using only SSWE
u
as feature. Af-
ter combining SSWE
u
with the feature set of NR-
C, we improve NRC from 74.86% to 75.39% for
subjective classification.
Comparision between Different Word Embed-
ding. We compare sentiment-specific word em-
bedding (SSWE
h
, SSWE
r
, SSWE
u
) with base-
line embedding learning algorithms by only us-
ing word embedding as features for Twitter sen-
timent classification. We use the embedding of u-
nigrams, bigrams and trigrams in the experimen-
t. The embeddings of C&W (Collobert et al,
2011), word2vec
4
, WVSA (Maas et al, 2011) and
our models are trained with the same dataset and
same parameter setting. We compare with C&W
and word2vec as they have been proved effective
in many NLP tasks. The trade-off parameter of
ReEmb (Labutov and Lipson, 2013) is tuned on
the development set of SemEval 2013.
Table 3 shows the performance on the pos-
itive/negative classification of tweets
5
. ReEm-
b(C&W) and ReEmb(w2v) stand for the use
of embeddings learned from 10 million distant-
supervised tweets with C&W and word2vec, re-
spectively. Each row of Table 3 represents a word
embedding learning algorithm. Each column s-
tands for a type of embedding used to compose
features of tweets. The column uni+bi denotes the
use of unigram and bigram embedding, and the
column uni+bi+tri indicates the use of unigram,
bigram and trigram embedding.
Embedding unigram uni+bi uni+bi+tri
C&W 74.89 75.24 75.89
Word2vec 73.21 75.07 76.31
ReEmb(C&W) 75.87 ? ?
ReEmb(w2v) 75.21 ? ?
WVSA 77.04 ? ?
SSWE
h
81.33 83.16 83.37
SSWE
r
80.45 81.52 82.60
SSWE
u
83.70 84.70 84.98
Table 3: Macro-F1 on positive/negative classifica-
tion of tweets with different word embeddings.
From the first column of Table 3, we can see that
the performance of C&W and word2vec are obvi-
ously lower than sentiment-specific word embed-
dings by only using unigram embedding as fea-
tures. The reason is that C&W and word2vec do
not explicitly exploit the sentiment information of
the text, resulting in that the words with oppo-
site polarity such as good and bad are mapped
to close word vectors. When such word embed-
dings are fed as features to a Twitter sentimen-
t classifier, the discriminative ability of sentiment
words are weakened thus the classification perfor-
mance is affected. Sentiment-specific word em-
4
Available at https://code.google.com/p/word2vec/. We
utilize the Skip-gram model because it performs better than
CBOW in our experiments.
5
MVSA and ReEmb are not suitable for learning bigram
and trigram embedding because their sentiment predictor
functions only utilize the unigram embedding.
1561
beddings (SSWE
h
, SSWE
r
, SSWE
u
) effectively
distinguish words with opposite sentiment polarity
and perform best in three settings. SSWE outper-
forms MVSA by exploiting more contextual infor-
mation in the sentiment predictor function. SSWE
outperforms ReEmb by leveraging more senti-
ment information from massive distant-supervised
tweets. Among three sentiment-specific word em-
beddings, SSWE
u
captures more context informa-
tion and yields best performance. SSWE
h
and
SSWE
r
obtain comparative results.
From each row of Table 3, we can see that the
bigram and trigram embeddings consistently im-
prove the performance of Twitter sentiment classi-
fication. The underlying reason is that a phrase,
which cannot be accurately represented by uni-
gram embedding, is directly encoded into the n-
gram embedding as an idiomatic unit. A typical
case in sentiment analysis is that the composed
phrase and multiword expression may have a dif-
ferent sentiment polarity than the individual word-
s it contains, such as not [bad] and [great] deal
of (the word in the bracket has different sentiment
polarity with the ngram). A very recent study by
Mikolov et al (2013) also verified the effective-
ness of phrase embedding for analogically reason-
ing phrases.
Effect of ? in SSWE
u
We tune the hyper-
parameter ? of SSWE
u
on the development set by
using unigram embedding as features. As given
in Equation 8, ? is the weighting score of syntac-
tic loss of SSWE
u
and trades-off the syntactic and
sentiment losses. SSWE
u
is trained from 10 mil-
lion distant-supervised tweets.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.77
0.78
0.79
0.8
0.81
0.82
0.83
0.84
?
Ma
cro
?F1
 
 
SSWEu
Figure 2: Macro-F1 of SSWE
u
on the develop-
ment set of SemEval 2013 with different ?.
Figure 2 shows the macro-F1 of SSWE
u
on pos-
itive/negative classification of tweets with differ-
ent ? on our development set. We can see that
SSWE
u
performs better when ? is in the range
of [0.5, 0.6], which balances the syntactic context
and sentiment information. The model with ?=1
stands for C&W model, which only encodes the
syntactic contexts of words. The sharp decline at
?=1 reflects the importance of sentiment informa-
tion in learning word embedding for Twitter senti-
ment classification.
Effect of Distant-supervised Data in SSWE
u
We investigate how the size of the distant-
supervised data affects the performance of SSWE
u
feature for Twitter sentiment classification. We
vary the number of distant-supervised tweets from
1 million to 12 million, increased by 1 million.
We set the ? of SSWE
u
as 0.5, according to the
experiments shown in Figure 2. Results of posi-
tive/negative classification of tweets on our devel-
opment set are given in Figure 3.
1 2 3 4 5 6 7 8 9 10 11 12
x 106
0.77
0.78
0.79
0.8
0.81
0.82
0.83
0.84
# of distant?supervised tweets
Mac
ro?F
1
 
 
SSWEu
Figure 3: Macro-F1 of SSWE
u
with different size
of distant-supervised data on our development set.
We can see that when more distant-supervised
tweets are added, the accuracy of SSWE
u
con-
sistently improves. The underlying reason is that
when more tweets are incorporated, the word em-
bedding is better estimated as the vocabulary size
is larger and the context and sentiment informa-
tion are richer. When we have 10 million distant-
supervised tweets, the SSWE
u
feature increases
the macro-F1 of positive/negative classification of
tweets to 82.94% on our development set. When
we have more than 10 million tweets, the per-
formance remains stable as the contexts of words
have been mostly covered.
4.2 Word Similarity of Sentiment Lexicons
The quality of SSWE has been implicitly evaluat-
ed when applied in Twitter sentiment classification
in the previous subsection. We explicitly evaluate
it in this section through word similarity in the em-
1562
bedding space for sentiment lexicons. The evalua-
tion metric is the accuracy of polarity consistency
between each sentiment word and its topN closest
words in the sentiment lexicon,
Accuracy =
?
#Lex
i=1
?
N
j=1
?(w
i
, c
ij
)
#Lex?N
(10)
where #Lex is the number of words in the senti-
ment lexicon, w
i
is the i-th word in the lexicon, c
ij
is the j-th closest word tow
i
in the lexicon with co-
sine similarity, ?(w
i
, c
ij
) is an indicator function
that is equal to 1 if w
i
and c
ij
have the same sen-
timent polarity and 0 for the opposite case. The
higher accuracy refers to a better polarity consis-
tency of words in the sentiment lexicon. We set N
as 100 in our experiment.
Experiment Setup and Datasets We utilize
the widely-used sentiment lexicons, namely M-
PQA (Wilson et al, 2005) and HL (Hu and Liu,
2004), to evaluate the quality of word embedding.
For each lexicon, we remove the words that do
not appear in the lookup table of word embedding.
We only use unigram embedding in this section
because these sentiment lexicons do not contain
phrases. The distribution of the lexicons used in
this paper is listed in Table 4.
Lexicon Positive Negative Total
HL 1,331 2,647 3,978
MPQA 1,932 2,817 4,749
Joint 1,051 2,024 3,075
Table 4: Statistics of the sentiment lexicons. Join-
t stands for the words that occur in both HL and
MPQA with the same sentiment polarity.
Results. Table 5 shows our results com-
pared to other word embedding learning al-
gorithms. The accuracy of random result is
50% as positive and negative words are ran-
domly occurred in the nearest neighbors of
each word. Sentiment-specific word embed-
dings (SSWE
h
, SSWE
r
, SSWE
u
) outperform ex-
isting neural models (C&W, word2vec) by large
margins. SSWE
u
performs best in three lexicon-
s. SSWE
h
and SSWE
r
have comparable perfor-
mances. Experimental results further demonstrate
that sentiment-specific word embeddings are able
to capture the sentiment information of texts and
distinguish words with opposite sentiment polari-
ty, which are not well solved in traditional neural
Embedding HL MPQA Joint
Random 50.00 50.00 50.00
C&W 63.10 58.13 62.58
Word2vec 66.22 60.72 65.59
ReEmb(C&W) 64.81 59.76 64.09
ReEmb(w2v) 67.16 61.81 66.39
WVSA 68.14 64.07 67.12
SSWE
h
74.17 68.36 74.03
SSWE
r
73.65 68.02 73.14
SSWE
u
77.30 71.74 77.33
Table 5: Accuracy of the polarity consistency of
words in different sentiment lexicons.
models like C&W and word2vec. SSWE outper-
forms MVSA and ReEmb by exploiting more con-
text information of words and sentiment informa-
tion of sentences, respectively.
5 Conclusion
In this paper, we propose learning continuous
word representations as features for Twitter sen-
timent classification under a supervised learning
framework. We show that the word embedding
learned by traditional neural networks are not ef-
fective enough for Twitter sentiment classification.
These methods typically only model the contex-
t information of words so that they cannot dis-
tinguish words with similar context but opposite
sentiment polarity (e.g. good and bad). We learn
sentiment-specific word embedding (SSWE) by
integrating the sentiment information into the loss
functions of three neural networks. We train SS-
WE with massive distant-supervised tweets select-
ed by positive and negative emoticons. The ef-
fectiveness of SSWE has been implicitly evaluat-
ed by using it as features in sentiment classifica-
tion on the benchmark dataset in SemEval 2013,
and explicitly verified by measuring word similar-
ity in the embedding space for sentiment lexicon-
s. Our unified model combining syntactic context
of words and sentiment information of sentences
yields the best performance in both experiments.
Acknowledgments
We thank Yajuan Duan, Shujie Liu, Zhenghua Li,
Li Dong, Hong Sun and Lanjun Zhou for their
great help. This research was partly supported
by National Natural Science Foundation of China
(No.61133012, No.61273321, No.61300113).
1563
References
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy da-
ta. In Proceedings of International Conference on
Computational Linguistics, pages 36?44.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
Yoshua Bengio, Aaron Courville, and Pascal Vincent.
2013. Representation learning: A review and new
perspectives. IEEE Trans. Pattern Analysis and Ma-
chine Intelligence.
Yoshua Bengio. 2013. Deep learning of represen-
tations: Looking forward. arXiv preprint arX-
iv:1305.0445.
Dmitriy Bespalov, Bing Bai, Yanjun Qi, and Ali Shok-
oufandeh. 2011. Sentiment classification based on
supervised latent n-gram analysis. In Proceedings
of the Conference on Information and Knowledge
Management, pages 375?382.
Dmitriy Bespalov, Yanjun Qi, Bing Bai, and Ali Shok-
oufandeh. 2012. Sentiment classification with su-
pervised sequence embedding. In Machine Learn-
ing and Knowledge Discovery in Databases, pages
159?174. Springer.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet alocation. the Journal of ma-
chine Learning research, 3:993?1022.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Annual Meeting of the Association for
Computational Linguistics, volume 7.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of International Con-
ference on Computational Linguistics, pages 241?
249.
Xiaowen Ding, Bing Liu, and Philip S Yu. 2008. A
holistic lexicon-based approach to opinion mining.
In Proceedings of the International Conference on
Web Search and Data Mining, pages 231?240.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, pages 2121?2159.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
Ronen Feldman. 2013. Techniques and application-
s for sentiment analysis. Communications of the
ACM, 56(4):82?89.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics, pages 42?47.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. Proceed-
ings of International Conference on Machine Learn-
ing.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford, pages 1?12.
Karl Moritz Hermann and Phil Blunsom. 2013. The
role of syntax in vector space models of compo-
sitional semantics. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics, pages 894?904.
Ming Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining, pages 168?177.
Xia Hu, Jiliang Tang, Huiji Gao, and Huan Liu.
2013. Unsupervised sentiment analysis with emo-
tional signals. In Proceedings of the International
World Wide Web Conference, pages 607?618.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter sen-
timent classification. The Proceeding of Annual
Meeting of the Association for Computational Lin-
guistics, 1:151?160.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the omg! In The International AAAI
Conference on Weblogs and Social Media.
Igor Labutov and Hod Lipson. 2013. Re-embedding
words. In Annual Meeting of the Association for
Computational Linguistics.
Kun-Lin Liu, Wu-Jun Li, and Minyi Guo. 2012. E-
moticon smoothed language models for twitter sen-
timent analysis. In The Association for the Advance-
ment of Artificial Intelligence.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
1564
Andrew L Maas, Raymond E Daly, Peter T Pham, Dan
Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In
Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corra-
do, and Jeffrey Dean. 2013. Distributed representa-
tions of words and phrases and their compositionali-
ty. The Conference on Neural Information Process-
ing Systems.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Andriy Mnih and Geoffrey E Hinton. 2009. A s-
calable hierarchical distributed language model. In
Advances in neural information processing systems,
pages 1081?1088.
Saif M Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. Nrc-canada: Building the state-of-
the-art in sentiment analysis of tweets. Proceedings
of the International Workshop on Semantic Evalua-
tion.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Proceedings of the International Work-
shop on Semantic Evaluation, volume 13.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In Proceedings of Language Resources and Evalua-
tion Conference, volume 2010.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 79?86.
Richard Socher, Eric H Huang, Jeffrey Pennington,
Andrew Y Ng, and Christopher D Manning. 2011a.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. The Conference
on Neural Information Processing Systems, 24:801?
809.
Richard Socher, Cliff C Lin, Andrew Ng, and Chris
Manning. 2011b. Parsing natural scenes and nat-
ural language with recursive neural networks. In
Proceedings of the International Conference on Ma-
chine Learning, pages 129?136.
Richard Socher, J. Pennington, E.H. Huang, A.Y. Ng,
and C.D. Manning. 2011c. Semi-supervised recur-
sive autoencoders for predicting sentiment distribu-
tions. In Conference on Empirical Methods in Nat-
ural Language Processing, pages 151?161.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic Com-
positionality Through Recursive Matrix-Vector S-
paces. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing.
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013a. Parsing with composi-
tional vector grammars. In Annual Meeting of the
Association for Computational Linguistics.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013b. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Conference on Empirical Methods in Nat-
ural Language Processing, pages 1631?1642.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional linguistics, 37(2):267?307.
Mike Thelwall, Kevan Buckley, and Georgios Pal-
toglou. 2012. Sentiment strength detection for the
social web. Journal of the American Society for In-
formation Science and Technology, 63(1):163?173.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. Annual Meeting of the
Association for Computational Linguistics.
Peter D Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 417?424.
Sida Wang and Christopher D Manning. 2012. Base-
lines and bigrams: Simple, good sentiment and topic
classification. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 90?94.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 347?354.
Ainur Yessenalina and Claire Cardie. 2011. Compo-
sitional matrix-space models for sentiment analysis.
In Proceedings of Conference on Empirical Methods
in Natural Language Processing, pages 172?182.
Jichang Zhao, Li Dong, Junjie Wu, and Ke Xu. 2012.
Moodlens: an emoticon-based sentiment analysis
system for chinese tweets. In Proceedings of the
18th ACM SIGKDD international conference on
Knowledge discovery and data mining.
Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu.
2013. Deep learning for chinese word segmenta-
tion and pos tagging. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 647?657.
1565
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 208?212,
Dublin, Ireland, August 23-24, 2014.
Coooolll: A Deep Learning System for Twitter Sentiment Classification
?
Duyu Tang
?
, Furu Wei
?
, Bing Qin
?
, Ting Liu
?
, Ming Zhou
?
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
?
Microsoft Research, Beijing, China
{dytang, qinb, tliu}@ir.hit.edu.cn
{fuwei, mingzhou}@microsoft.com
Abstract
In this paper, we develop a deep learn-
ing system for message-level Twitter sen-
timent classification. Among the 45 sub-
mitted systems including the SemEval
2013 participants, our system (Coooolll)
is ranked 2nd on the Twitter2014 test set
of SemEval 2014 Task 9. Coooolll is
built in a supervised learning framework
by concatenating the sentiment-specific
word embedding (SSWE) features with
the state-of-the-art hand-crafted features.
We develop a neural network with hybrid
loss function
1
to learn SSWE, which en-
codes the sentiment information of tweets
in the continuous representation of words.
To obtain large-scale training corpora, we
train SSWE from 10M tweets collected by
positive and negative emoticons, without
any manual annotation. Our system can
be easily re-implemented with the publicly
available sentiment-specific word embed-
ding.
1 Introduction
Twitter sentiment classification aims to classify
the sentiment polarity of a tweet as positive, nega-
tive or neutral (Jiang et al., 2011; Hu et al., 2013;
Dong et al., 2014). The majority of existing ap-
proaches follow Pang et al. (2002) and employ ma-
chine learning algorithms to build classifiers from
tweets with manually annotated sentiment polar-
ity. Under this direction, most studies focus on
?
This work was partly done when the first author was
visiting Microsoft Research.
1
This is one of the three sentiment-specific word embed-
ding learning algorithms proposed in Tang et al. (2014).
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
designing effective features to obtain better clas-
sification performance (Pang and Lee, 2008; Liu,
2012; Feldman, 2013). For example, Mohammad
et al. (2013) implement diverse sentiment lexicons
and a variety of hand-crafted features. To leverage
massive tweets containing positive and negative e-
moticons for automatically feature learning, Tang
et al. (2014) propose to learn sentiment-specific
word embedding and Kalchbrenner et al. (2014)
model sentence representation with Dynamic Con-
volutional Neural Network.
In this paper, we develop a deep learning sys-
tem for Twitter sentiment classification. First-
ly, we learn sentiment-specific word embedding
(SSWE) (Tang et al., 2014), which encodes the
sentiment information of text into the continuous
representation of words (Mikolov et al., 2013; Sun
et al., 2014). Afterwards, we concatenate the SS-
WE features with the state-of-the-art hand-crafted
features (Mohammad et al., 2013), and build the
sentiment classifier with the benchmark dataset
from SemEval 2013 (Nakov et al., 2013). To
learn SSWE, we develop a tailored neural net-
work, which incorporates the supervision from
sentiment polarity of tweets in the hybrid loss
function. We learn SSWE from tweets, lever-
aging massive tweets with emoticons as distant-
supervised corpora without any manual annota-
tions.
We evaluate the deep learning system on the
test set of Twitter Sentiment Analysis Track in Se-
mEval 2014
2
. Our system (Coooolll) is ranked
2nd on the Twitter2014 test set, along with the
SemEval 2013 participants owning larger train-
ing data than us. The performance of only us-
ing SSWE as features is comparable to the state-
of-the-art hand-crafted features (detailed in Ta-
ble 3), which verifies the effectiveness of the
sentiment-specific word embedding. We release
the sentiment-specific word embedding learned
2
http://alt.qcri.org/semeval2014/task9/
208
Training 
Data 
Learning 
Algorithm 
Feature 
Representation 
Sentiment 
Classifier 
1 
2 
?. 
 
N 
 
N+1 
N+2 
? 
N+K 
STATE 
Feature 
SSWE 
Feature 
all-cap 
emoticon 
? 
?. 
dimension 1 
dimension 2 
dimension N 
elongated 
Massive 
Tweets 
Embedding 
Learning 
Figure 1: Our deep learning system (Coooolll) for
Twitter sentiment classification.
from 10 million tweets, which can be easily used
to re-implement our system and adopted off-the-
shell in other sentiment analysis tasks.
2 A Deep Learning System
In this section, we present the details of our deep
learning system for Twitter sentiment classifica-
tion. As illustrated in Figure 1, Coooolll is a su-
pervised learning method that builds the sentimen-
t classifier from tweets with manually annotated
sentiment polarity. In our system, the feature rep-
resentation of tweet is composed of two parts, the
sentiment-specific word embedding features (SS-
WE features) and the state-of-the-art hand-crafted
features (STATE features). In the following parts,
we introduce the SSWE features and STATE fea-
tures, respectively.
2.1 SSWE Features
In this part, we first describe the neural network
for learning sentiment-specific word embedding.
Then, we generate the SSWE features of a tweet
from the embedding of words it contains.
Our neural network is an extension of the tra-
ditional C&W model (Collobert et al., 2011), as
illustrated in Figure 2. Unlike C&W model that
learns word embedding by only modeling syntac-
tic contexts of words, we develop SSWEu, which
captures the sentiment information of sentences as
well as the syntactic contexts of words. Given an
original (or corrupted) ngram and the sentiment
polarity of a sentence as the input, SSWE
u
predict-
s a two-dimensional vector for each input ngram.
The two scalars (f
u
0
, f
u
1
) stand for language model
score and sentiment score of the input ngram, re-
so cooool :D 
syntactic 
sentiment 
Figure 2: Our neural network (SSWE
u
) for learn-
ing sentiment-specific word embedding.
spectively. The training objectives of SSWE
u
are
that (1) the original ngram should obtain a high-
er language model score f
u
0
(t) than the corrupted
ngram f
u
0
(t
r
), and (2) the sentiment score of orig-
inal ngram f
u
1
(t) should be more consistent with
the gold polarity annotation of sentence than cor-
rupted ngram f
u
1
(t
r
). The loss function of SSWE
u
is the linear combination of two hinge losses,
loss
u
(t, t
r
) = ? ? loss
cw
(t, t
r
)+
(1? ?) ? loss
us
(t, t
r
)
(1)
where where t is the original ngram, t
r
is the cor-
rupted ngram which is generated from t with mid-
dle word replaced by a randomly selected one,
loss
cw
(t, t
r
) is the syntactic loss as given in E-
quation 2, loss
us
(t, t
r
) is the sentiment loss as
described in Equation 3. The hyper-parameter ?
weighs the two parts.
loss
cw
(t, t
r
) = max(0, 1? f
cw
(t) + f
cw
(t
r
))
(2)
loss
us
(t, t
r
) = max(0, 1? ?
s
(t)f
u
1
(t)
+ ?
s
(t)f
u
1
(t
r
) )
(3)
where ?
s
(t) is an indicator function reflecting the
sentiment polarity of a sentence, whose value is 1
if the sentiment polarity of tweet t is positive and
-1 if t?s polarity is negative. We train sentiment-
specific word embedding from 10M tweets col-
lected with positive and negative emoticons (Hu
et al., 2013). The details of training phase are de-
scribed in Tang et al. (2014).
After finish learning SSWE, we explore min,
average and max convolutional layers (Collobert
et al., 2011; Socher et al., 2011; Mitchell and Lap-
ata, 2010), to obtain the tweet representation. The
result is the concatenation of vectors derived from
different convolutional layers.
209
2.2 STATE Features
We re-implement the state-of-the-art hand-crafted
features (Mohammad et al., 2013) for Twitter sen-
timent classification. The STATE features are de-
scribed below.
? All-Caps. The number of words with all char-
acters in upper case.
? Emoticons. We use the presence of positive
(or negative) emoticons and whether the last
unit of a segmentation is emoticon
3
.
? Elongated Units. The number of elongated
words (with one character repeated more than
two times), such as gooood.
? Sentiment Lexicon. We utilize several senti-
ment lexicons
4
to generate features. We ex-
plore the number of sentiment words, the s-
core of last sentiment words, the total senti-
ment score and the maximal sentiment score
for each lexicon.
? Negation. The number of individual nega-
tions
5
within a tweet.
? Punctuation. The number of contiguous se-
quences of dot, question mark and exclama-
tion mark.
? Cluster. The presence of words from each
of the 1,000 clusters from the Twitter NLP
tool (Gimpel et al., 2011).
? Ngrams. The presence of word ngrams (1-4)
and character ngrams (3-5).
3 Experiments
We evaluate our deep learning system by applying
it for Twitter sentiment classification within a su-
pervised learning framework. We conduct exper-
iments on both positive/negative/neutral and posi-
tive/negative classification of tweets.
3
We use the positive and negative emoticons from Sen-
tiStrength, available at http://sentistrength.wlv.ac.uk/.
4
HL (Hu and Liu, 2004), MPQA (Wilson et al., 2005), N-
RC Emotion (Mohammad and Turney, 2013), NRC Hashtag
and Sentiment140Lexicon (Mohammad et al., 2013).
5
http://sentiment.christopherpotts.net/lingstruc.html
3.1 Dataset and Setting
We train the Twitter sentiment classifier on the
benchmark dataset in SemEval 2013 (Nakov et
al., 2013). The training and development sets were
completely in full to task participants of SemEval
2013. However, we were unable to download al-
l the training and development sets because some
tweets were deleted or not available due to modi-
fied authorization status. The distribution of our
dataset is given in Table 1. We train sentimen-
t classifiers with LibLinear (Fan et al., 2008) on
the training set and dev set, and tune parameter
?c,?wi of SVM on the test set of SemEval 2013.
In both experiment settings, the evaluation met-
ric is the macro-F1 of positive and negative class-
es (Nakov et al., 2013).
Positive Negative Neutral Total
Train 2,642 994 3,436 7,072
Dev 408 219 493 1,120
Test 1,570 601 1,639 3,810
Table 1: Statistics of our SemEval 2013 Twitter
sentiment classification dataset.
The test sets of SemEval 2014 is directly pro-
vided to the participants, which is composed of
five parts. The statistic of test sets in SemEval
2014 is given in Table 2.
Positive Negative Neutral Total
T1 427 304 411 1,142
T2 492 394 1,207 2,093
T3 1,572 601 1,640 3,813
T4 982 202 669 1,939
T5 33 40 13 86
Table 2: Statistics of SemEval 2014 Twitter senti-
ment classification test set. T1 is LiveJournal2014,
T2 is SMS2013, T3 is Twitter2013, T4 is Twit-
ter2014, T5 is Twitter2014Sarcasm.
3.2 Results and Analysis
The experiment results of different methods
on positive/negative/neutral and positive/negative
Twitter sentiment classification are listed in Ta-
ble 3. The meanings of T1?T5 in each column are
described in Table 2. SSWE means the approach
that only utilizes the sentiment-specific word em-
bedding as features for Twitter sentiment classi-
fication. In STATE, we only utilize the existing
features (Mohammad et al., 2013) for building the
210
Method
Positive/Negative/Neutral Positive/Negative
T1 T2 T3 T4 T5 T1 T2 T3 T4 T5
SSWE 70.49 64.29 68.69 66.86 50.00 84.51 85.19 85.06 86.14 62.02
Coooolll 72.90 67.68 70.40 70.14 46.66 86.46 85.32 86.01 87.61 56.55
STATE 71.48 65.43 66.18 67.07 44.89 83.96 82.82 84.39 86.16 58.27
W2V 55.19 52.98 52.33 50.58 49.63 68.87 71.89 74.50 71.52 61.60
Top 74.84 70.28 72.12 70.96 58.16 - - - - - - - - - -
Average 63.52 55.63 59.78 60.41 45.44 - - - - - - - - - -
Table 3: Macro-F1 of positive and negative classes in positive/negative/neutral and positive/negative
Twitter sentiment classification on the test sets (T1-T5, detailed in Table 2) of SemEval 2014. The
performances of Coooolll on the Twitter-relevant test sets are bold.
sentiment classifier. In Coooolll, we use the con-
catenation of SSWE features and STATE features.
In W2V, we only use the word embedding learned
from word2vec
6
as features. Top and Average are
the top and average performance of the 45 team-
s of SemEval 2014, including the SemEval 2013
participants who owns larger training data.
On positive/negative/neutral classification of
tweets as listed in Table 3 (left table), we find
that the learned sentiment-specific word embed-
ding features (SSWE) performs comparable with
the state-of-the-art hand-crafted features (STATE),
especially on the Twitter-relevant test sets (T3
and T4)
7
. After feature combination, Coooolll
yields 4.22% and 3.07% improvement by macro-
F1 on T3 and T4,which verifies the effective-
ness of SSWE by learning discriminate features
from massive data for Twitter sentiment classifi-
cation. From the 45 teams, Coooolll gets the Rank
5/2/3/2 on T1-T4 respectively, along with the Se-
mEval 2013 participants owning larger training
data. We also comparing SSWE with the context-
based word embedding (W2V), which don?t cap-
ture the sentiment supervision of tweets. We find
that W2V is not effective enough for Twitter sen-
timent classification as there is a big gap between
W2V and SSWE on T1-T4. The reason is that W2V
does not capture the sentiment information of text,
which is crucial for sentiment analysis tasks and
effectively leveraged for learning the sentiment-
specific word embedding.
We also conduct experiments on the posi-
6
We utilize the Skip-gram model. The embedding is
trained from the 10M tweets collected by positive and neg-
ative emoticons, as same as the training data of SSWE.
7
The result of STATE on T3 is different from the results
reported in Mohammad et al. (2013) and Tang et al. (2014)
because we have different training data with the former and
different -wi of SVM with the latter.
tive/negative classification of tweets. The reason
is that the sentiment-specific word embedding is
learned from the positive/negative supervision of
tweets through emoticons, which is tailored for
positive/negative classification of tweets. From
Table 3 (right table), we find that the performance
of positive/negative Twitter classification is con-
sistent with the performance of 3-class classifica-
tion. SSWE performs comparable to STATE on T3
and T4, and yields better performance (1.62% and
1.45% improvements on T3 and T4, respectively)
through feature combination. SSWE outperform-
s W2V by large margins (more than 10%) on T3
and T4, which further verifies the effectiveness of
sentiment-specific word embedding.
4 Conclusion
We develop a deep learning system (Coooolll) for
message-level Twitter sentiment classification in
this paper. The feature representation of Cooool-
ll is composed of two parts, a state-of-the-art
hand-crafted features and the sentiment-specific
word embedding (SSWE) features. The SSWE
is learned from 10M tweets collected by posi-
tive and negative emoticons, without any manu-
al annotation. The effectiveness of Coooolll has
been verified in both positive/negative/neutral and
positive/negative classification of tweets. Among
45 systems of SemEval 2014 Task 9 subtask(b),
Coooolll yields Rank 2 on the Twitter2014 test set,
along with the SemEval 2013 participants owning
larger training data.
Acknowledgments
We thank Li Dong for helpful discussions. This
work was partly supported by National Natu-
ral Science Foundation of China (No.61133012,
No.61273321, No.61300113).
211
References
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming
Zhou, and Ke Xu. 2014. Adaptive recursive neural
network for target-dependent twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 49?54.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
Ronen Feldman. 2013. Techniques and application-
s for sentiment analysis. Communications of the
ACM, 56(4):82?89.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics, pages 42?47.
Ming Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDDConference on Knowledge Discovery
and Data Mining, pages 168?177.
Xia Hu, Jiliang Tang, Huiji Gao, and Huan Liu.
2013. Unsupervised sentiment analysis with emo-
tional signals. In Proceedings of the International
World Wide Web Conference, pages 607?618.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter sen-
timent classification. The Proceeding of Annual
Meeting of the Association for Computational Lin-
guistics, 1:151?160.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of the 52nd
Annual Meeting of the Association for Computation-
al Linguistics, pages 655?665.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word rep-
resentations in vector space. arXiv preprint arX-
iv:1301.3781.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Saif M Mohammad and Peter D Turney. 2013. Crowd-
sourcing a word?emotion association lexicon. Com-
putational Intelligence, 29(3):436?465.
Saif M Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. Nrc-canada: Building the state-of-
the-art in sentiment analysis of tweets. Proceedings
of the International Workshop on Semantic Evalua-
tion, pages 321?327.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Proceedings of the International Work-
shop on Semantic Evaluation, volume 13, pages
312?320.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 79?86.
Richard Socher, Eric H Huang, Jeffrey Pennington,
Andrew Y Ng, and Christopher D Manning. 2011.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. The Conference
on Neural Information Processing Systems, 24:801?
809.
Yaming Sun, Lei Lin, Duyu Tang, Nan Yang, Zhenzhou
Ji, and Xiaolong Wang. 2014. Radical-enhanced
chinese character embedding. arXiv preprint arX-
iv:1404.4714.
Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014. Learning sentiment-
specific word embedding for twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 1555?1565.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 347?354.
212
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 238?242
Manchester, August 2008
A Cascaded Syntactic and Semantic Dependency Parsing System
Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang Li, Bing Qin, Ting Liu, Sheng Li
Information Retrieval Lab
School of Computer Science and Technology
Harbin Institute of Technology, China, 150001
{car, lzh, yxhu, yqli, qinb, tliu, ls}@ir.hit.edu.cn
Abstract
We describe our CoNLL 2008 Shared Task
system in this paper. The system includes
two cascaded components: a syntactic and
a semantic dependency parsers. A first-
order projective MSTParser is used as our
syntactic dependency parser. In order to
overcome the shortcoming of the MST-
Parser, that it cannot model more global in-
formation, we add a relabeling stage after
the parsing to distinguish some confusable
labels, such as ADV, TMP, and LOC. Be-
sides adding a predicate identification and
a classification stages, our semantic de-
pendency parsing simplifies the traditional
four stages semantic role labeling into two:
a maximum entropy based argument clas-
sification and an ILP-based post inference.
Finally, we gain the overall labeled macro
F1 = 82.66, which ranked the second posi-
tion in the closed challenge.
1 System Architecture
Our CoNLL 2008 Shared Task (Surdeanu et al,
2008) participating system includes two cascaded
components: a syntactic and a semantic depen-
dency parsers. They are described in Section 2
and 3 respectively. Their experimental results are
shown in Section 4. Section 5 gives our conclusion
and future work.
2 Syntactic Dependency Parsing
MSTParser (McDonald, 2006) is selected as our
basic syntactic dependency parser. It views the
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
syntactic dependency parsing as a problem of
finding maximum spanning trees (MST) in di-
rected graphs. MSTParser provides the state-of-
the-art performance for both projective and non-
projective tree banks.
2.1 Features
The score of each labeled arc is computed through
the Eq. (1) in MSTParser.
score(h, c, l) = w ? f(h, c, l) (1)
where node h represents the head node of the arc,
while node c is the dependent node (or child node).
l denotes the label of the arc.
There are three major differences between our
feature set and McDonald (2006)?s:
1) We use the lemma as a generalization feature
of a word, while McDonald (2006) use the word?s
prefix.
2) We add two new features: ?bet-pos-h-same-
num? and ?bet-pos-c-same-num?. They represent
the number of nodes which locate between node h
and node c and whose POS tags are the same with
h and c respectively.
3) We use more back-off features than McDon-
ald (2006) by completely enumerating all of the
possible combinatorial features.
2.2 Relabeling
By observing the current results of MSTParser on
the development data, we find that the performance
of some labels are far below average, such as ADV,
TMP, LOC. We think the main reason lies in that
MSTParser only uses local features restricted to a
single arc (as shown in Eq. (1)) and fails to use
more global information. Consider two sentences:
?I read books in the room.? and ?I read books in
the afternoon.?. It is hard to correctly label the arc
238
Deprel Total Mislabeled as
NMOD 8,922 NAME [0.4], DEP [0.4], LOC [0.1],
AMOD [0.1]
OBJ 1,728 TMP [0.5], ADV [0.4], OPRD[0.3]
ADV 1,256 TMP [2.9], LOC [2.3], MNR [1.8],
DIR [1.5]
NAME 1,138 NMOD [2.2]
VC 953 PRD [0.9]
DEP 772 NMOD [4.0]
TMP 755 ADV [9.9], LOC [6.5]
LOC 556 ADV [12.6], NMOD [7.9], TMP [5.9]
AMOD 536 ADV [2.2]
PRD 509 VC [4.7]
APPO 444 NMOD [2.5]
OPRD 373 OBJ [4.6]
DIR 119 ADV [18.5]
MNR 109 ADV [28.4]
Table 1: Error Analysis of Each Label
between ?read? and ?in? unless we know the object
of ?in?.
We count the errors of each label, and show the
top ones in Table 1. ?Total? refers to the total num-
ber of the corresponding label in the development
data. The column of ?Mislabeled as? lists the la-
bels that an arc may be mislabeled as. The number
in brackets shows the percentage of mislabeling.
As shown in the table, some labels are often con-
fusable with each other, such as ADV, LOC and
TMP.
2.3 Relabeling using Maximum Entropy
Classifier
We constructed two confusable label set which
have a higher mutual mislabeling proportion:
(NMOD, LOC, ADV, TMP, MNR, DIR) and (OBJ,
OPRD). A maximum entropy classifier is used to
relabel them.
Features are shown in Table 2. The first column
lists local features, which contains information of
the head node h and the dependent node c of an arc.
?+ dir dist? means that conjoining existing features
with arc direction and distance composes new fea-
tures. The second column lists features using the
information of node c?s children. ?word c c? rep-
resents form or lemma of one child of the node
c. ?dir c? and ?dist c? represents the direction and
distance of the arc which links node c to its child.
The back-off technique is also used on these fea-
tures.
Local features (+ dir dist) Global features (+ dir c dist c)
word h word c word h word c word c c
Table 2: Relabeling Feature Set (+ dir dist)
3 Semantic Dependency Parsing
3.1 Architecture
The whole procedure is divided into four separate
stages: Predicate Identification, Predicate Classifi-
cation, Semantic Role Classification, and Post In-
ference.
During the Predicate Identification stage we ex-
amine each word in a sentence to discover target
predicates, including both noun predicates (from
NomBank) and verb predicates (from PropBank).
In the Predicate Classification stage, each predi-
cate is assigned a certain sense number. For each
predicate, the probabilities of a word in the sen-
tence to be each semantic role are predicted in the
Semantic Role Classification stage. Maximum en-
tropy model is selected as our classifiers in these
stages. Finally an ILP (Integer Linear Program-
ming) based method is adopted for post infer-
ence (Punyakanok et al, 2004).
3.2 Predicate Identification
The predicate identification is treated as a binary
classification problem. Each word in a sentence is
predicted to be a predicate or not to be. A set of
features are extracted for each word, and an opti-
mized subset of them are adopted in our final sys-
tem. The following is a full list of the features:
DEPREL (a1): Type of relation to the parent.
WORD (a21), POS (a22), LEMMA (a23),
HEAD (a31), HEAD POS (a32), HEAD LEMMA
(a33): The forms, POS tags and lemmas of a word
and it?s headword (parent) .
FIRST WORD (a41), FIRST POS (a42),
FIRST LEMMA (a43), LAST WORD (a51),
LAST POS (a52), LAST LEMMA (a53): A
corresponding ?constituent? for a word consists
of all descendants of it. The forms, POS tags and
lemmas of both the first and the last words in the
constituent are extracted.
POS PAT (a6): A ?POS pattern? is produced for
the corresponding constituent as follows: a POS
bag is produced with the POS tags of the words
in the constituent except for the first and the last
ones, duplicated tags removed and the original or-
der ignored. Then we have the POS PAT feature
239
by combining the POS tag of the first word, the
bag and the POS tag of the last word.
CHD POS (a71), CHD POS NDUP (a72),
CHD REL (a73), CHD REL NDUP (a74): The
POS tags of the child words are joined to-
gether to form feature CHD POS. With adja-
cently duplicated tags reduced to one, feature
CHD POS NDUP is produced. Similarly we can
get CHD REL and CHD REL NDUP too, with
the relation types substituted for the POS tags.
SIB REL (a81), SIB REL NDUP (a82),
SIB POS (a83), SIB POS NDUP (a84): Sibling
words (including the target word itself) and the
corresponding dependency relations (or POS tags)
are considered as well. Four features are formed
similarly to those of child words.
VERB VOICE (a9): Verbs are examined for
voices: if the headword lemma is either ?be? or
?get?, or else the relation type is ?APPO?, then the
verb is considered passive, otherwise active.
Also we used some ?combined? features which
are combinations of single features. The final op-
timized feature set is (a1, a21, a22, a31, a32, a41,
a42, a51, a52, a6, a72, a73, a74, a81, a82, a83,
a1+a21, a21+a31, a21+a6, a21+a74, a73+a81,
a81+a83).
3.3 Predicate Classification
After predicate identification is done, the resulting
predicates are processed for sense classification. A
classifier is trained for each predicate that has mul-
tiple senses on the training data (There are totally
962 multi-sense predicates on the training corpus,
taking up 14% of all) In additional to those fea-
tures described in the predicate identification sec-
tion, some new ones relating to the predicate word
are introduced:
BAG OF WORD (b11), BAG OF WORD O
(b12): All words in a sentence joined, namely
?Bag of Words?. And an ?ordered? version is in-
troduced where each word is prefixed with a letter
?L?, ?R? or ?T? indicating it?s to the left or right of
the predicate or is the predicate itself.
BAG OF POS O (b21), BAG OF POS N
(b22): The POS tags prefixed with ?L?, ?R? or
?T? indicating the word position joined together,
namely ?Bag of POS (Ordered)?. With the
prefixed letter changed to a number indicating
the distance to the predicate (negative for being
left to the predicate and positive for right), an-
other feature is formed, namely ?Bag of POS
(Numbered)?.
WIND5 BIGRAM (b3): 5 closest words from
both left and right plus the predicate itself, in total
11 words form a ?window?, within which bigrams
are enumerated.
The final optimized feature set for the task of
predicate classification is (a1, a21, a23, a71, a72,
a73, a74, a81, a82, a83, a84, a9, b11, b12, b22, b3,
a71+a9).
3.4 Semantic Role Classification
In our system, the identification and classifica-
tion of semantic roles are achieved in a single
stage (Liu et al, 2005) through one single classi-
fier (actually two, one for noun predicates, and the
other for verb predicates). Each word in a sentence
is given probabilities to be each semantic role (in-
cluding none of the these roles) for a predicate.
Features introduced in addition to those of the pre-
vious subsections are the following:
POS PATH (c11), REL PATH (c12): The ?POS
Path? feature consists of POS tags of the words
along the path from a word to the predicate. Other
than ?Up? and ?Down?, the ?Left? and ?Right? di-
rection of the path is added. Similarly, the ?Re-
lation Path? feature consists of the relation types
along the same path.
UP PATH (c21), UP REL PATH (c22): ?Up-
stream paths? are parts of the above paths that stop
at the common ancestor of a word and the predi-
cate.
PATH LEN (c3): Length of the paths
POSITION (c4): The relative position of a word
to the predicate: Left or Right.
PRED FAMILYSHIP (c5): ?Familyship rela-
tion? between a word and the predicate, being one
of ?self?, ?child?, ?descendant?, ?parent?, ?ances-
tor?, ?sibling?, and ?not-relative?.
PRED SENSE (c6): The lemma plus sense
number of the predicate
As for the task of semantic role classification,
the features of the predicate word in addition to
those of the word under consideration can also
be used; we mark features of the predicate with
an extra ?p?. For example, the head word of
the current word is represented as a31, and the
head word of the predicate is represented as pa31.
So, with no doubt for the representation, our fi-
nal optimized feature set for the task of seman-
tic role classification is (a1, a23, a33, a43, a53,
a6, c11, c12, c21, c3, c4, c6, pa23, pa71, pa73,
240
pa83, a1+a23+a33, a21+c5, a23+c12, a33+c12,
a33+c22, a6+a33, a73+c5, c11+c12, pa71+pa73).
3.5 ILP-based Post Inference
The final semantic role labeling result is gener-
ated through an ILP (Integer Linear Programming)
based post inference method. An ILP problem is
formulated with respect to the probability given by
the above stage. The final labeling is formed at the
same time when the problem is solved.
Let W be the set of words in the sentence, and
R be the set of semantic role labels. A virtual label
?NULL? is also added to R, representing ?none of
the roles is assigned?.
For each word w ? W and semantic role label
r ? R we create a binary variable v
wr
? (0, 1),
whose value indicates whether or not the word w
is labeled as label r. p
wr
denotes the possibil-
ity of word w to be labeled as role r. Obviously,
when objective function f =
?
w,r
log(p
wr
? v
wr
)
is maximized, we can read the optimal labeling for
a predicate from the assignments to the variables
v
wr
. There are three constrains used in our system:
C1: Each relation should be and only be la-
beled with one label (including the virtual label
?NULL?), i.e.:
?
r
v
wr
= 1
C2: Roles with a small probability should never
be labeled (except for the virtual role ?NULL?).
The threshold we use in our system is 0.3, which
is optimized from the development data. i.e.:
v
wr
= 0, if p
wr
< 0.3 and r 6= ?NULL?
C3: Statistics shows that the most roles (ex-
cept for the virtual role ?NULL?) usually appear
only once for a predicate, except for some rare ex-
ception. So we impose a no-duplicated-roles con-
straint with an exception list, which is constructed
according to the times of semantic roles? duplica-
tion for each single predicate (different senses of a
predicate are considered different) and the ratio of
duplication to non-duplication.
?
r
v
wr
? 1,
if < p, r > /? {< p, r > |p ? P, r ? R;
d
pr
c
pr
?d
pr
> 0.3 ? d
pr
> 10}
(2)
where P is the set of predicates; c
pr
denotes the
count of words in the training corpus, which are
Predicate Type Predicate Label
Noun president.01 A3
Verb match.01 A1
Verb tie.01 A1
Verb link.01 A1
Verb rate.01 A0
Verb rate.01 A2
Verb attach.01 A1
Verb connect.01 A1
Verb fit.01 A1
Noun trader.01 SU
Table 3: No-duplicated-roles constraint exception
list (obtained by Eq. (2))
labeled as r ? R for predicate p ? P ; while d
pr
denotes something similar to c
pr
, but what taken
into account are only those words labeled with r,
and there are more than one roles within the sen-
tence for the same predicate. Table 3 lists the com-
plete exception set, which has a size of only 10.
4 Experiments
The original MSTParser
1
is implemented in Java.
We were confronted with memory shortage when
trying to train a model with the entire CoNLL 2008
training data with 4GB memory. Therefore, we
rewrote it with C++ which can manage the mem-
ory more exactly. Since the time was limited, we
only rewrote the projective part without consider-
ing second-order parsing technique.
Our maximum entropy classifier is implemented
with Maximum Entropy Modeling Toolkit
2
. The
classifier parameters: gaussian prior and iterations,
are tuned with the development data for different
stages respectively.
lp solve 5.5
3
is chosen as our ILP problem
solver during the post inference stage.
The training time of the syntactic and the se-
mantic parsers are 22 and 5 hours respectively, on
all training data, with 2.0GHz Xeon CPU and 4G
memory. While the prediction can be done within
10 and 5 minutes on the development data.
4.1 Syntactic Dependency Parsing
The experiments on development data show that
relabeling process is helpful, which improves the
1
http://sourceforge.net/projects/mstparser
2
http://homepages.inf.ed.ac.uk/s0450736/maxent
toolkit.html
3
http://sourceforge.net/projects/lpsolve
241
Precision (%) Recall (%) F1
Pred Identification 91.61 91.36 91.48
Pred Classification 86.61 86.37 86.49
Table 4: The performance of predicate identifica-
tion and classification
Precision (%) Recall (%) F1
Simple 81.02 76.00 78.43
ILP-based 82.53 75.26 78.73
Table 5: Comparison between different post infer-
ence strategies
LAS performance from 85.41% to 85.94%. The fi-
nal syntactic dependency parsing performances on
the WSJ and the Brown test data are 87.51% and
80.73% respectively.
4.2 Semantic Dependency Parsing
The semantic dependency parsing component is
based on the last syntactic dependency parsing
component. All stages of the system are trained
with the closed training corpus, while predicted
against the output of the syntactic parsing.
Performance for predicate identification and
classification is given in Table 4, wherein the clas-
sification is done on top of the identification.
Semantic role classification and the post infer-
ence are done on top of the result of predicate iden-
tification and classification. The final performance
is presented in Table 5. A simple post inference
strategy is given for comparison, where the most
possible label (including the virtual label ?NULL?)
is select except for those duplicated non-virtual la-
bels with lower probabilities (lower than 0.5). Our
ILP-based method produces a gain of 0.30 with re-
spect to the F1 score.
The final semantic dependency parsing perfor-
mance on the development and the test (WSJ and
Brown) data are shown in Table 6.
Precision (%) Recall (%) F1
Development 82.53 75.26 78.73
Test (WSJ) 82.67 77.50 80.00
Test (Brown) 64.38 68.50 66.37
Table 6: Semantic dependency parsing perfor-
mances
4.3 Overall Performance
The overall macro scores of our syntactic and se-
mantic dependency parsing system are 82.38%,
83.78% and 73.57% on the development and two
test (WSJ and Brown) data respectively, which is
ranked the second position in the closed challenge.
5 Conclusion and Future Work
We present our CoNLL 2008 Shared Task system
which is composed of two cascaded components:
a syntactic and a semantic dependency parsers,
which are built with some state-of-the-art methods.
Through a fine tuning features and parameters, the
final system achieves promising results. In order
to improve the performance further, we will study
how to make use of more resources and tools (open
challenge) and how to do joint learning between
syntactic and semantic parsing.
Acknowledgments
The authors would like to thank the reviewers for
their helpful comments. This work was supported
by National Natural Science Foundation of China
(NSFC) via grant 60675034, 60575042, and the
?863? National High-Tech Research and Develop-
ment of China via grant 2006AA01Z145.
References
Liu, Ting, Wanxiang Che, Sheng Li, Yuxuan Hu, and
Huaijun Liu. 2005. Semantic role labeling system
using maximum entropy classifier. In Proceedings
of CoNLL-2005, June.
McDonald, Ryan. 2006. Discriminative Learning and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Punyakanok, Vasin, Dan Roth, Wen-tau Yih, and Dav
Zimak. 2004. Semantic role labeling via integer
linear programming inference. In Proceedings of
Coling-2004, pages 1346?1352.
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008).
242
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 127?130,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
1 
 
 Coreference Resolution System using Maximum Entropy Classifier 
 
 
Weipeng Chen,Muyu Zhang,Bing Qin  
                                                               Center for Information Retrieval 
Harbin Institute of Technology 
                                     {wpchen,myzhang,bing.qin}@ir.hit.edu.cn 
  
  
 
 
Abstract 
In this paper, we present our supervised 
learning approach to coreference resolution 
in ConLL corpus. The system relies on a 
maximum entropy-based classifier for pairs 
of mentions, and adopts a rich linguisitical- 
ly motivated feature set, which mostly has 
been introduced by Soon et al(2001), and 
experiment with alternaive resolution proc- 
ess, preprocessing tools,and classifiers. We 
optimize the system?s performance for M- 
UC (Vilain et al 1995), BCUB (Bagga and 
Baldwin, 1998) and CEAF (Luo, 2005) .  
1. Introduction 
The coreference resolution is the task in which all  
expressions refer to the same entity in a discourse 
will be identified. As the core of natural language 
processing, coreference resolution is significant to 
message understanding, information extraction, 
text summarization, information retrieval, informa-
tion filtration, and machine translation. 
A considerable engineering efforts is needed for 
the full coreference resolution task, and a signifi-
cant part of this effort concerns feature engineering.  
The backbone of our system can be split into two 
subproblems: mention detection and creation of 
entitly. We train a mention detector on the training 
texts. Once the mentions are identified, coreference 
resolution involves partitioning them into subsets 
corresponding to the same entity. This problem is 
cast into the binary classification problem of decid-
ing whether two given mentions are coreferent. 
Our system relies on maximum entropy-based 
classifier for pairs of mentions. Our system relies 
on a rich linguistically motivated feature set. Our 
system architecture makes it possible to define 
other kinds of features: atmoic word and markable 
features. This approach to feature engineering is 
suitable not only for knowledge-rich but also for 
knowledge-poor datasets. Finally, we use the best-
first clustering to create the coreference chains. 
 
2. System Description  
This section briefly describes our system. First the 
mention detection is presented. Next, the features 
which we import are described. Finally, we de-
scribled the learning and encoding methods. 
2.1 Mention Detector  
The first stage of the coreference resolution 
process try to identify the occurrence of mentions 
in document. To detect system mention from a test 
text, we train a mention detector on the training 
data. We formulate the mention problem as a clas-
sification, by assigning to each token in the text a 
label, indicating whether it is a mention or not. 
Hence, to learn the detector, we create one training 
text and derive its class value (one of b, i, o) from 
the annotated data. Each instance represents the  , 
the token under consideration, and consists of 19 
linguistic features, many of which are modeled af-
ter the systems of Bikel et al (1999) and Florian et 
al. (2004) , as describled below. 
(1) Lexical: Tokens in the windows of  three 
words before and after the target word: 
{     ,?,    }. 
(2) Capitalization: Determine whether    is 
IsAllCaP (all the characters of word are ca-
pitalized, such as ?BBN?), IsInitCap (the 
word starts with a capitalized character, 
127
2 
 
such as ?Sally? ), IsCapPeriod (more than 
one characters of word are capitalized but 
not all, and the first character is not capita-
lized too, such ?M.? ), and IsAllLower (all 
the character of word aren?t capitalized, 
such as ?can? ) (see Bikel et al  (1999)). 
(3) Grammatical: The single POS tags of the 
tokens in the window of three words before 
and after the target word{    ,?,    }. 
(4) Semantic: The  named entity (NE) tag and  
the Noun Phrase tag of  .  
We employ maximum entropy-based classifier, for 
training the mention detector. These detected 
mentions are to be used as system mentions in our 
coreference experiment. 
2.2 Features 
To determine which mentions belong to same en-
titly, we need to devise a set of features that is use-
ful in determining whether two mentions corefer or 
not. All the feature value are computed automati-
cally, without any manual intervention. 
     
(1) Distance Feature: A non-negative integer 
feature capture the distance between anap- 
hor and antecedent. If anaphor and antece-
dent are in the same sentence, the value is 
0; If their sentence distance is 1, the value 
is 1, and so on. 
(2) Antecedent-pronoun Feature: A Boolean 
feature capture whether the antecedent is p- 
ronoun or not. True if the antecedent is a p- 
ronoun. Pronouns include reflexive prono-
uns, personal pronouns, and possessive pr- 
onouns.  
(3) Anaphor-pronoun Feature: A Boolean f- 
eature capture whether  the anaphor is pro-
noun or not. True if the anaphor is a pron- 
oun. 
(4) String Match Feature: A non-negative in-
teger feature. If one candidate is a substrin-
g of another, its value is 0, else the value is 
0 plus the edit distance. 
(5) Anaphor Definite Noun Phrase Feature: 
A Boolean feature capture whether the ana- 
phor is a definite noun phrase or not. True 
if the anaphor is a pronoun. In our definiti- 
on, a definite noun phrase is someone that 
start with the word ?the?. 
(6) Anaphor Demonstrative Noun Phrase F-
eature:  A Boolean feature capture wheth- 
er the anaphor is a demonstractive  noun or 
not. True if the anaphor is a demonstractive  
noun. In our definition, a demonstractive  n 
oun is someone that start with the word, su- 
ch as this, that, those, these. 
(7) ProperName Feature: A Boolean feature. 
True if  anphor and antecedent both are pr- 
oper name. 
(8) Gender Feature: Its value are true, false   
or  unknow. If gender of pair of  instance   
matches, its value is true,else if  the value  
is umatches, the value is false; If one of the 
pair instance?s gender is unknown, the val-
ue is uknown.  
(9) Number Feature: A Boolean feature. True 
if the  number of pair of instance is match-
es; 
(10) Alias Feature: A Boolean feature. True if 
two markables refer to the same entity usi- 
ng different notation(acronyms, shorthands, 
etc), its value is true. 
(11) Semantic Feature: Its value are true, fals- 
e, or unknown. If semantic class relateness 
of a pair instance is the same, or one is the 
parent of other, its value is true; Else if the- 
y are unmatch,the value is false; If one of t- 
he the pair instance?s semantic class is unk- 
nown, the value is unknown. 
 
2.3 Learning   
We did not make any effort to optimize the nu- 
mber of training instances for the pair-wise learne- 
r: a positive instance for each adjacent coreferent 
markable pair and negative training instances for a 
markable m and all markables disreferent with m 
that occur before m (Soon et al,2001). For decod-
ing it generates all the possible links inside a win-
dow of 100 markables. 
Our system integrate many machine learning m 
ethods, such as maximum entropy (Tsuruoka,  200- 
6) , Descision Tree,Support Vector Machine  (Joa- 
chims, 2002) . We compare the result using differ- 
ent method in our system, and decide to rely on m-
aximum entropy-based classifier, and it led to the 
best results. 
2.4 Decoding 
In the decoding step, the coreference chains are 
created by the best-first clustering. Each mention is 
128
3 
 
compared with all of its previous mentions with 
probability greater than a fixed threshold, and is 
clustered with the one hightest probability. If none 
has probability greater than the threshold, the men-
tion becomes a new cluster. 
      
3. Setting and data 
3.1 Setting 
Our system has participated in the closed settings 
for English. Which means all the knowledge re-
quired by the mention detector and feature detector   
is obtained from the annotation of the corpus(see 
Pradhan et al  (2007)), with the exception of Wor- 
dNet.  
 
3.2 Data  
We selecte all ConLL training data and develop-
ment data, contain ?gold? files and ?auto? file, to 
train our final system. The "gold" indicates that 
the annotation is that file is hand-annotated and 
adjudicated quality, whereas the second means it 
was produced using a combination of automatic 
tools. The training data distribution is shown in 
Table 1. 
 
Category bc bn mz nw wb 
Quantity 40 1708 142 1666 190 
  Table 1: Final system?s training data distribution 
 
 
In this paper, we report the results from our dev- 
elopment system, which were trained on the traini- 
ng data and tested on the development set. The de- 
tail is shown in Table 2,3. 
 
Category bc bn mz nw wb 
Quantity 32 1526 128 1490 166 
  Table 2: Experiment system?s training data distribution 
  
 
Category bc bn mz nw wb 
Quantity 8 182 14 176 24 
   Table 3: Experiment system?s test set distribution 
 
 
4. Evaluation  
First, we have evaluated our mention detector mo- 
dule, which is train by the ConLL training data. It 
regards all the token as the candidate, and cast it i- 
nto the mention detector, and the detector decides 
it is  mention or not. The mention detector?s result 
is shown in Table4. 
 
 
Metric R P F 
Value 63.6 55.26 59.14 
Table 4: Performance of  mention detector on the de-
velopment set 
 
Second, we have evaluated our system with the 
system mention, and we use the previous mention 
detector to determine the mention boundary. As fo- 
llow, we list the system perfomance  of using MUC, 
B-CUB,CEAF (E) , CEAF (M) , BLANC (Recasens a- 
nd Hovy, in prep)  in Table 5 . 
 
Metric R P F 
MUC 45.53 47.00 46.25 
BCUB 61.29 68.07 64.50 
CEAF(M) 47.47 47.47 47.47 
CEAF(E) 39.23 37.91 38.55 
BLANC 64.00 68.31 65.81 
Table 5 :Result using  system mentions 
 
 
Finally, we  have evaluated our system with the 
gold mentions, which mention?s boundary is corect. 
The system performance is shown in Table 6: 
 
Metric R P F 
MUC 50.15 80.49 61.78 
BCUB 48.87 85.75 62.62 
CEAF(M) 54.50 54.50 54.50 
CEAF(E) 67.38 32.72 44.05 
BLANC 66.03 78.41 70.02 
Table6:Result using  gold mentions 
 
 
Result of system shows a big difference  betwee- 
n using gold mentions and using system mentions. 
In comparison to the system using system mention- 
s, we see that the F-score rises significantly by 
4.21- 15.53 for the system using gold mentions. It  
is worth noting that the F-scorer when using the B- 
CUB metric, the system using system mention rise- 
129
4 
 
s 2.12 for system using gold mention. Although t- 
his is surprising, in my opinion this correlation is 
because the mention detection recall more candid- 
ate mention, and the BCUB metric is benefit for t- 
he mention which is merge into the erroneous 
chain.  
5. Conclusion 
In this paper, we have presented a new modular 
system for coreference in English. We train a men-
tion detector to find the mention?s boundary based 
on maximum entropy classifier to decide pairs of 
mention refer to or not.  
     Due to the flexible architecture, it allows us ex-
tend the system to multi-language. And if it is ne-
cessary, we can obtain other modules to support 
the system. The results obtained confirm the feasi-
bility of our system. 
 
 
References  
Wee Meng Soon,Hwee You Ng,and Daniel Chung 
Yong Lim.2001.A machine learing approach to core-
ference resolution of noun phrases.Computational 
Linguistic(special Issue on Computational Anaphora 
Resolution),27(4):521-544 
Marc Vilain,John Burger,John Aberdeen,Dennis Con-
nolly,and Lynette Hirschman.1995.A modeltheoretic 
coreference scoring scheme.In Proceedings of the 6th 
Message Understanding Conference,pages 45-52. 
Amit Bagga and Breck baldwin.1998.Algorithms for 
scoring coreference chains.In Proceedings of the lin-
guistic Coreference Workshoop at the International 
Conference on Language Resources and Evalua-
tion(LREC-1998),pages 563-566. 
Xiaoqiang Luo.2005.On coreference resoluton perfor-
mance metrics.In Proceeddings of the Annual Meet-
ing of the North American Chapter of the Association 
for Computational Linguistics-Human Language 
Technology Conference(NAACL/HLY-2005),pages 
25-32 
Josef Steinberger,Massimo Poesio,Mijail A.kabadjov- 
b,and Karel jezek.2007.Two uses of anaphora resolu-
tion in summarization.In Information Processing and 
management,Special issue on Summarization,pages 
1663-1680 
Bikel,R.Schwartz,and R.Weischedel.1999.An algorithm 
that learns what's in a name.Machine Learning,34(1-
3):pages211-231 
Florian,H.Hassan,A.Ittycheriah,H.Jing,N.Kambhatla, X. 
Luo,N.Nicolov,and I.Zitouni.2004.A statistical model 
for multilingual entity detection and tracking.In 
Proc.of HLA/NAACL. 
Sameer Pradhan and Lance Ramshaw and Ralph Wei-
schedel and Jessica MacBride and Linnea Micciulla. 
2007.Unrestricted Coreference: Identifying Entities 
and Events in OntoNotes. In Proceedings of the IEEE 
International Conference on Semantic Computing 
(ICSC), Irvine, CA 
Marta Recasens and Eduard Hovy.in prep.BLAN- 
C:Implementing the rand index for coreference eval-
uation. 
Yoshimasa Tsuruoka.2006.A simple c++ library for 
maxium entropy classifiction.Ysujii laboratory,Dep- 
artment of Computer Science,University of Tokyo. 
Throsten Joachims.1999.Making large-scale SVM 
learning practical.In B.Scholkopf,C.Burges,and A.S- 
mola,editors,Advances in Kernel Methods-Support 
Vector Learning.MIT-Press. 
 
 
 
 
 
 
 
 
 
 
 
 
 
130
