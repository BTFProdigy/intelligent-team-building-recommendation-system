Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 621?626,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Correcting Keyboard Layout Errors and Homoglyphs in Queries
Derek Barnes
debarnes@ebay.com
Mahesh Joshi
mahesh.joshi@ebay.com
eBay Inc., 2065 Hamilton Ave, San Jose, CA, 95125, USA
Hassan Sawaf
hsawaf@ebay.com
Abstract
Keyboard layout errors and homoglyphs
in cross-language queries impact our abil-
ity to correctly interpret user informa-
tion needs and offer relevant results.
We present a machine learning approach
to correcting these errors, based largely
on character-level n-gram features. We
demonstrate superior performance over
rule-based methods, as well as a signif-
icant reduction in the number of queries
that yield null search results.
1 Introduction
The success of an eCommerce site depends on
how well users are connected with products and
services of interest. Users typically communi-
cate their desires through search queries; however,
queries are often incomplete and contain errors,
which impact the quantity and quality of search
results.
New challenges arise for search engines in
cross-border eCommerce. In this paper, we fo-
cus on two cross-linguistic phenomena that make
interpreting queries difficult: (i) Homoglyphs:
(Miller, 2013): Tokens such as ?case? (underlined
letters Cyrillic), in which users mix characters
from different character sets that are visually simi-
lar or identical. For instance, English and Russian
alphabets share homoglyphs such as c, a, e, o, y,
k, etc. Although the letters are visually similar or
in some cases identical, the underlying character
codes are different. (ii) Keyboard Layout Errors
(KLEs): (Baytin et al., 2013): When switching
one?s keyboard between language modes, users at
times enter terms in the wrong character set. For
instance, ?????? ????? may appear to be a Rus-
sian query. While ??????? is the Russian word
for ?case?, ?????? is actually the user?s attempt
to enter the characters ?ipad? while leaving their
keyboard in Russian language mode. Queries con-
taining KLEs or homoglyphs are unlikely to pro-
duce any search results, unless the intended ASCII
sequences can be recovered. In a test set sam-
pled from Russian/English queries with null (i.e.
empty) search results (see Section 3.1), we found
approximately 7.8% contained at least one KLE or
homoglyph.
In this paper, we present a machine learning
approach to identifying and correcting query to-
kens containing homoglyphs and KLEs. We show
that the proposed method offers superior accuracy
over rule-based methods, as well as significant im-
provement in search recall. Although we focus our
results on Russian/English queries, the techniques
(particularly for KLEs) can be applied to other lan-
guage pairs that use different character sets, such
as Korean-English and Thai-English.
2 Methodology
In cross-border trade at eBay, multilingual queries
are translated into the inventory?s source language
prior to search. A key application of this, and
the focus of this paper, is the translation of Rus-
sian queries into English, in order to provide Rus-
sian users a more convenient interface to English-
based inventory in North America. The presence
of KLEs and homoglyphs in multilingual queries,
however, leads to poor query translations, which in
turn increases the incidence of null search results.
We have found that null search results correlate
with users exiting our site.
In this work, we seek to correct for KLEs and
homoglyphs, thereby improving query translation,
reducing the incidence of null search results, and
increasing user engagement. Prior to translation
and search, we preprocess multilingual queries
by identifying and transforming KLEs and homo-
glyphs as follows (we use the query ?????? ????
2 new? as a running example):
(a) Tag Tokens: label each query token
621
with one of the following semantically moti-
vated classes, which identify the user?s informa-
tion need: (i) E: a token intended as an English
search term; (ii) R: a Cyrillic token intended as a
Russian search term; (iii) K: A KLE, e.g. ??????
for the term ?ipad?. A token intended as an En-
glish search term, but at least partially entered in
the Russian keyboard layout; (iv) H: A Russian
homoglyph for an English term, e.g. ???w? (un-
derlined letters Cyrillic). Employs visually sim-
ilar letters from the Cyrillic character set when
spelling an intended English term; (v) A: Ambigu-
ous tokens, consisting of numbers and punctuation
characters with equivalent codes that can be en-
tered in both Russian and English keyboard lay-
outs. Given the above classes, our example query
?????? ???? 2 new? should be tagged as ?R K A
E?.
(b) Transform Queries: Apply a deterministic
mapping to transform KLE and homoglyph tokens
from Cyrillic to ASCII characters. For KLEs the
transformation maps between characters that share
the same location in Russian and English keyboard
layouts (e.g. ? ? a, ? ? s). For homoglyphs the
transformation maps between a smaller set of vi-
sually similar characters (e.g. ?? e, ??m). Our
example query would be transformed into ??????
ipad 2 new?.
(c) Translate and Search: Translate the trans-
formed query (into ?case ipad 2 new? for our ex-
ample), and dispatch it to the search engine.
In this paper, we formulate the token-level tag-
ging task as a standard multiclass classification
problem (each token is labeled independently), as
well as a sequence labeling problem (a first order
conditional Markov model). In order to provide
end-to-end results, we preprocess queries by de-
terministically transforming into ASCII the tokens
tagged by our model as KLEs or homoglyphs. We
conclude by presenting an evaluation of the impact
of this transformation on search.
2.1 Features
Our classification and sequence models share a
common set of features grouped into the follow-
ing categories:
2.1.1 Language Model Features
A series of 5-gram, character-level language mod-
els (LMs) capture the structure of different types
of words. Intuitively, valid Russian terms will
have high probability in Russian LMs. In contrast,
KLEs or homoglyph tokens, despite appearing on
the surface to be Russian terms, will generally
have low probability in the LMs trained on valid
Russian words. Once mapped into ASCII (see
Section 2 above), however, these tokens tend to
have higher probability in the English LMs. LMs
are trained on the following corpora:
English and Russian Vocabulary: based on
a collection of open source, parallel En-
glish/Russian corpora (?50M words in all).
English Brands: built from a curated list of 35K
English brand names, which often have distinctive
linguistic properties compared with common En-
glish words (Lowrey et al., 2013).
Russian Transliterations: built from a col-
lection of Russian transliterations of proper
names from Wikipedia (the Russian portion of
guessed-names.ru-en made available as a
part of WMT 2013
1
).
For every input token, each of the above LMs
fires a real-valued feature ? the negated log-
probability of the token in the given language
model. Additionally, for tokens containing Cyril-
lic characters, we consider the token?s KLE and
homoglyph ASCII mappings, where available. For
each mapping, a real-valued feature fires corre-
sponding to the negated log-probability of the
mapped token in the English and Brands LMs.
Lastly, an equivalent set of LM features fires for
the two preceding and following tokens around the
current token, if applicable.
2.1.2 Token Features
We include several features commonly used in
token-level tagging problems, such as case and
shape features, token class (such as letters-only,
digits-only), position of the token within the query,
and token length. In addition, we include fea-
tures indicating the presence of characters from
the ASCII and/or Cyrillic character sets.
2.1.3 Dictionary Features
We incorporate a set of features that indicate
whether a given lowercased query token is a mem-
ber of one of the lexicons described below.
UNIX: The English dictionary shipped with Cen-
tOS, including ?480K entries, used as a lexicon
of common English words.
BRANDS: An expanded version of the curated list
of brand names used for LM features. Includes
1
www.statmt.org/wmt13/
translation-task.html#download
622
?58K brands.
PRODUCT TITLES: A lexicon of over 1.6M en-
tries extracted from a collection of 10M product
titles from eBay?s North American inventory.
QUERY LOGS: A larger, in-domain collection of
approximately 5M entries extracted from ?100M
English search queries on eBay.
Dictionary features fire for Cyrillic tokens when
the KLE and/or homoglyph-mapped version of the
token appears in the above lexicons. Dictionary
features are binary for the Unix and Brands dictio-
naries, and weighted by relative frequency of the
entry for the Product Titles and Query Logs dic-
tionaries.
3 Experiments
3.1 Datasets
The following datasets were used for training and
evaluating the baseline (see Section 3.2 below) and
our proposed systems:
Training Set: A training set of 6472 human-
labeled query examples (17,239 tokens).
In-Domain Query Test Set: A set of 2500 Rus-
sian/English queries (8,357 tokens) randomly se-
lected from queries with null search results. By
focusing on queries with null results, we empha-
size the presence of KLEs and homoglyphs, which
occur in 7.8% of queries in our test set.
Queries were labeled by a team of Russian lan-
guage specialists. The test set was also indepen-
dently reviewed, which resulted in the correction
of labels for 8 out of the 8,357 query tokens.
Although our test set is representative of the
types of problematic queries targeted by our
model, our training data was not sampled using the
same methodology. We expect that the differences
in distributions between training and test sets, if
anything, make the results reported in Section 3.3
somewhat pessimistic
2
.
3.2 Dictionary Baseline
We implemented a rule-based baseline system em-
ploying the dictionaries described in Section 2.1.3.
In this system, each token was assigned a class
k ? {E,R,K,H,A} using a set of rules: a token
among a list of 101 Russian stopwords
3
is tagged
2
As expected, cross-validation experiments on the train-
ing data (for parameter tuning) yielded results slightly higher
than the results reported in Section 3.3, which use a held-out
test set
3
Taken from the Russian Analyzer packaged with Lucene
? see lucene.apache.org.
as R. A token containing only ASCII characters is
labeled as A if all characters are common to En-
glish and Russian keyboards (i.e. numbers and
some punctuation), otherwise E. For tokens con-
taining Cyrillic characters, KLE and homoglyph-
mapped versions are searched in our dictionaries.
If found, K or H are assigned. If both mapped ver-
sions are found in the dictionaries, then either K
or H is assigned probabilistically
4
. In cases where
neither mapped version is found in the dictionary,
the token assigned is either R or A, depending on
whether it consists of purely Cyrillic characters, or
a mix of Cyrillic and ASCII, respectively.
Note that the above tagging rules allow tokens
with classes E and A to be identified with perfect
accuracy. As a result, we omit these classes from
all results reported in this work. We also note
that this simplification applies because we have
restricted our attention to the Russian ? English
direction. In the bidirectional case, ASCII tokens
could represent either English tokens or KLEs (i.e.
a Russian term entered in the English keyboard
layout). We leave the joint treatment of the bidi-
rectional case to future work.
Tag Prec Recall F1
K .528 .924 .672
H .347 .510 .413
R .996 .967 .982
Table 1: Baseline results on the test set, using
UNIX, BRANDS, and the PRODUCT TITLES dic-
tionaries.
We experimented with different combinations
of dictionaries, and found the best combination to
be UNIX, BRANDS, and PRODUCT TITLES dic-
tionaries (see Table 1). We observed a sharp de-
crease in precision when incorporating the QUERY
LOGS dictionary, likely due to noise in the user-
generated content.
Error analysis suggests that shorter words are
the most problematic for the baseline system
5
.
Shorter Cyrillic tokens, when transformed from
Cyrillic to ASCII using KLE or homoglyph map-
pings, have a higher probability of spuriously
mapping to valid English acronyms, model IDs,
or short words. For instance, Russian car brand
????? maps across keyboard layouts to ?dfp?,
4
We experimented with selecting K or H based on a prior
computed from training data; however, results were lower
than those reported, which use random selection.
5
Stopwords are particularly problematic, and hence ex-
cluded from consideration as KLEs or homoglyphs.
623
Tag
Classification Sequence
P R F1 P R F1
LR
K .925 .944 .935 .915 .934 .925
H .708 .667 .687 .686 .686 .686
R .996 .997 .996 .997 .996 .997
RF
K .926 .949 .937 .935 .949 .942
H .732 .588 .652 .750 .588 .659
R .997 .997 .997 .996 .998 .997
Table 2: Classification and sequence tagging re-
sults on the test set
a commonly used acronym in product titles for
?Digital Flat Panel?. Russian words ?????? and
????? similarly map by chance to English words
?verb? and ?her?.
A related problem occurs with product model
IDs, and highlights the limits of treating query to-
kens independently. Consider Cyrillic query ????
e46?. The first token is a Russian transliteration
for the BMW brand. The second token, ?e46?,
has three possible interpretations: i) as a Russian
token; ii) a homoglyph for ASCII ?e46?; or iii)
a KLE for ?t46?. It is difficult to discriminate
between these options without considering token
context, and in this case having some prior knowl-
edge that e46 is a BMW model.
3.3 Machine Learning Models
We trained linear classification models using lo-
gistic regression (LR)
6
, and non-linear models us-
ing random forests (RFs), using implementations
from the Scikit-learn package (Pedregosa et al.,
2011). Sequence models are implemented as first
order conditional Markov models by applying a
beam search (k = 3) on top of the LR and RF
classifiers. The LR and RF models were tuned us-
ing 5-fold cross-validation results, with models se-
lected based on the mean F1 score across R, K, and
H tags.
Table 2 shows the token-level results on our in-
domain test set. As with the baseline, we focus the
model on disambiguating between classes R, K and
H. Each of the reported models performs signifi-
cantly better than the baseline (on each tag), with
statistical significance evaluated usingMcNemar?s
test. The differences between LR and RF mod-
els, as well as sequence and classification variants,
however, are not statistically significant. Each of
the machine learning models achieves a query-
level accuracy score of roughly 98% (the LR se-
6
Although CRFs are state-of-the-art for many tagging
problems, in our experiments they yielded results slightly
lower than LR or RF models.
quence model achieved the lowest with 97.78%,
the RF sequence model the highest with 97.90%).
Our feature ablation experiments show that
the majority of predictive power comes from the
character-level LM features. Dropping LM fea-
tures results in a significant reduction in perfor-
mance (F1 scores .878 and .638 for the RF Se-
quence model on classes K and H). These results
are still significantly above the baseline, suggest-
ing that token and dictionary features are by them-
selves good predictors. However, we do not see
a similar performance reduction when dropping
these feature groups.
We experimented with lexical features, which
are commonly used in token-level tagging prob-
lems. Results, however, were slightly lower than
the results reported in this section. We suspect the
issue is one of overfitting, due to the limited size of
our training data, and general sparsity associated
with lexical features. Continuous word presenta-
tions (Mikolov et al., 2013), noted as future work,
may offer improved generalization.
Error analysis for our machine learning mod-
els suggests patterns similar to those reported in
Section 3.2. Although errors are significantly less
frequent than in our dictionary baseline, shorter
words still present the most difficulty. We note
as future work the use of word-level LM scores
to target errors with shorter words.
3.4 Search Results
Recall that we translate multilingual queries into
English prior to search. KLEs and homoglyphs
in queries result in poor query translations, often
leading to null search results.
To evaluate the impact of KLE and homoglyph
correction, we consider a set of 100k randomly se-
lected Russian/English queries. We consider the
subset of queries that the RF or baseline models
predict as containing a KLE or homoglyph. Next,
we translate into English both the original query,
as well as a transformed version of it, with KLEs
and homoglyphs replaced with their ASCII map-
pings. Lastly, we execute independent searches
using original and transformed query translations.
Table 3 provides details on search results for
original and transformed queries. The baseline
model transforms over 12.6% of the 100k queries.
Of those, 24.3% yield search results where the un-
modified queries had null search results (i.e. Null
? Non-null). In 20.9% of the cases, however, the
624
transformations are destructive (i.e. Non-null ?
Null), and yield null results where the unmodified
query produced results.
Compared with the baseline, the RF model
transforms only 7.4% of the 100k queries; a frac-
tion that is roughly in line with the 7.8% of queries
in our test set that contain KLEs or homoglyphs.
In over 42% of the cases (versus 24.3% for the
baseline), the transformed query generates search
results where the original query yields none. Only
4.81% of the transformations using the RF model
are destructive; a fraction significantly lower than
the baseline.
Note that we distinguish here only between
queries that produce null results, and those that do
not. We do not include queries for which original
and transformed queries both produce (potentially
differing) search results. Evaluating these cases
requires deeper insight into the relevance of search
results, which is left as future work.
Baseline RF model
#Transformed 12,661 7,364
Null? Non-Null 3,078 (24.3%) 3,142 (42.7%)
Non-Null? Null 2,651 (20.9%) 354 (4.81%)
Table 3: Impact of KLE and homoglyph correction
on search results for 100k queries
4 Related Work
Baytin et al. (2013) first refer to keyboard lay-
out errors in their work. However, their focus is
on predicting the performance of spell-correction,
not on fixing KLEs observed in their data. To
our knowledge, our work is the first to introduce
this problem and to propose a machine learning
solution. Since our task is a token-level tagging
problem, it is very similar to the part-of-speech
(POS) tagging task (Ratnaparkhi, 1996), only with
a very small set of candidate tags. We chose
a supervised machine learning approach in order
to achieve maximum precision. However, this
problem can also be approached in an unsuper-
vised setting, similar to the methodWhitelaw et al.
(2009) use for spelling correction. In that setup,
the goal would be to directly choose the correct
transformation for an ill-formed KLE or homo-
glyph, instead of a tagging step followed by a de-
terministic mapping to ASCII.
5 Conclusions and Future Work
We investigate two kinds of errors in search
queries: keyboard layout errors (KLEs) and ho-
moglyphs. Applying machine learning methods,
we are able to accurately identify a user?s intended
query, in spite of the presence of KLEs and ho-
moglyphs. The proposed models are based largely
on compact, character-level language models. The
proposed techniques, when applied to multilingual
queries prior to translation and search, offer signif-
icant gains in search results.
In the future, we plan to focus on additional fea-
tures to improve KLE and homoglyph discrimina-
tion for shorter words and acronyms. Although
lexical features did not prove useful for this work,
presumably due to data sparsity and overfitting
issues, we intend to explore the application of
continuous word representations (Mikolov et al.,
2013). Compared with lexical features, we expect
continuous representations to be less susceptible
to overfitting, and to generalize better to unknown
words. For instance, using continuous word rep-
resentations, Turian et al. (2010) show significant
gains for a named entity recognition task.
We also intend on exploring the use of features
from in-domain, word-level LMs. Word-level fea-
tures are expected to be particularly useful in the
case of spurious mappings (e.g. ????? vs. ?dfp?
from Section 3.2), where context from surround-
ing tokens in a query can often help in resolving
ambiguity. Word-level features may also be useful
in re-ranking translated queries prior to search, in
order to reduce the incidence of erroneous query
transformations generated through our methods.
Finally, our future work will explore KLE and ho-
moglyph correction bidirectionally, as opposed to
the unidirectional approach explored in this work.
Acknowledgments
We would like to thank Jean-David Ruvini, Mike
Dillinger, Sa?sa Hasan, Irina Borisova and the
anonymous reviewers for their valuable feedback.
We also thank our Russian language special-
ists Tanya Badeka, Tatiana Kontsevich and Olga
Pospelova for their support in labeling and review-
ing datasets.
References
Alexey Baytin, Irina Galinskaya, Marina Panina, and
Pavel Serdyukov. 2013. Speller performance pre-
625
diction for query autocorrection. In Proceedings
of the 22nd ACM International Conference on Con-
ference on Information & Knowledge Management,
pages 1821?1824.
Tina M. Lowrey, Larry J. Shrum, and Tony M. Du-
bitsky. 2013. The Relation Between Brand-name
Linguistic Characteristics and Brand-nameMemory.
Journal of Advertising, 32(3):7?17.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.
Tristan Miller. 2013. Russian?English Homoglyphs,
Homographs, and Homographic Translations. Word
Ways: The Journal of Recreational Linguistics,
46(3):165?168.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and Edouard Duchesnay. 2011.
Scikit-learn: Machine Learning in Python. Journal
of Machine Learning Research, 12:2825?2830.
Adwait Ratnaparkhi. 1996. A Maximum Entropy
Model for Part?of?Speech Tagging. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 133?142.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of
ACL, pages 384?394.
Casey Whitelaw, Ben Hutchinson, Grace Y. Chung,
and Ged Ellis. 2009. Using the Web for Lan-
guage Independent Spellchecking and Autocorrec-
tion. In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing,
pages 890?899.
626
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 412?418,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Language Independent Connectivity Strength Features
for Phrase Pivot Statistical Machine Translation
Ahmed El Kholy, Nizar Habash
Center for Computational Learning Systems, Columbia University
{akholy,habash}@ccls.columbia.edu
Gregor Leusch, Evgeny Matusov
Science Applications International Corporation
{gregor.leusch,evgeny.matusov}@saic.com
Hassan Sawaf
eBay Inc.
hsawaf@ebay.com
Abstract
An important challenge to statistical ma-
chine translation (SMT) is the lack of par-
allel data for many language pairs. One
common solution is to pivot through a
third language for which there exist par-
allel corpora with the source and target
languages. Although pivoting is a robust
technique, it introduces some low quality
translations. In this paper, we present two
language-independent features to improve
the quality of phrase-pivot based SMT.
The features, source connectivity strength
and target connectivity strength reflect the
quality of projected alignments between
the source and target phrases in the pivot
phrase table. We show positive results (0.6
BLEU points) on Persian-Arabic SMT as
a case study.
1 Introduction
One of the main issues in statistical machine trans-
lation (SMT) is the scarcity of parallel data for
many language pairs especially when the source
and target languages are morphologically rich. A
common SMT solution to the lack of parallel data
is to pivot the translation through a third language
(called pivot or bridge language) for which there
exist abundant parallel corpora with the source
and target languages. The literature covers many
pivoting techniques. One of the best performing
techniques, phrase pivoting (Utiyama and Isahara,
2007), builds an induced new phrase table between
the source and target. One of the main issues of
this technique is that the size of the newly cre-
ated pivot phrase table is very large (Utiyama and
Isahara, 2007). Moreover, many of the produced
phrase pairs are of low quality which affects the
translation choices during decoding and the over-
all translation quality. In this paper, we introduce
language independent features to determine the
quality of the pivot phrase pairs between source
and target. We show positive results (0.6 BLEU
points) on Persian-Arabic SMT.
Next, we briefly discuss some related work. We
then review two common pivoting strategies and
how we use them in Section 3. This is followed by
our approach to using connectivity strength fea-
tures in Section 4. We present our experimental
results in Section 5.
2 Related Work
Many researchers have investigated the use of piv-
oting (or bridging) approaches to solve the data
scarcity issue (Utiyama and Isahara, 2007; Wu and
Wang, 2009; Khalilov et al, 2008; Bertoldi et al,
2008; Habash and Hu, 2009). The main idea is to
introduce a pivot language, for which there exist
large source-pivot and pivot-target bilingual cor-
pora. Pivoting has been explored for closely re-
lated languages (Hajic? et al, 2000) as well as un-
related languages (Koehn et al, 2009; Habash and
Hu, 2009). Many different pivot strategies have
been presented in the literature. The following
three are perhaps the most common.
The first strategy is the sentence translation
technique in which we first translate the source
sentence to the pivot language, and then translate
the pivot language sentence to the target language
412
(Khalilov et al, 2008).
The second strategy is based on phrase pivot-
ing (Utiyama and Isahara, 2007; Cohn and Lap-
ata, 2007; Wu and Wang, 2009). In phrase pivot-
ing, a new source-target phrase table (translation
model) is induced from source-pivot and pivot-
target phrase tables. Lexical weights and transla-
tion probabilities are computed from the two trans-
lation models.
The third strategy is to create a synthetic source-
target corpus by translating the pivot side of
source-pivot corpus to the target language using an
existing pivot-target model (Bertoldi et al, 2008).
In this paper, we build on the phrase pivoting
approach, which has been shown to be the best
with comparable settings (Utiyama and Isahara,
2007). We extend phrase table scores with two
other features that are language independent.
Since both Persian and Arabic are morphologi-
cally rich, we should mention that there has been
a lot of work on translation to and from morpho-
logically rich languages (Yeniterzi and Oflazer,
2010; Elming and Habash, 2009; El Kholy and
Habash, 2010a; Habash and Sadat, 2006; Kathol
and Zheng, 2008). Most of these efforts are fo-
cused on syntactic and morphological processing
to improve the quality of translation.
To our knowledge, there hasn?t been a lot of
work on Persian and Arabic as a language pair.
The only effort that we are aware of is based
on improving the reordering models for Persian-
Arabic SMT (Matusov and Ko?pru?, 2010).
3 Pivoting Strategies
In this section, we review the two pivoting strate-
gies that are our baselines. We also discuss how
we overcome the large expansion of source-to-
target phrase pairs in the process of creating a
pivot phrase table.
3.1 Sentence Pivoting
In sentence pivoting, English is used as an inter-
face between two separate phrase-based MT sys-
tems; Persian-English direct system and English-
Arabic direct system. Given a Persian sentence,
we first translate the Persian sentence from Per-
sian to English, and then from English to Arabic.
3.2 Phrase Pivoting
In phrase pivoting (sometimes called triangulation
or phrase table multiplication), we train a Persian-
to-Arabic and an English-Arabic translation mod-
els, such as those used in the sentence pivoting
technique. Based on these two models, we induce
a new Persian-Arabic translation model.
Since we build our models on top of Moses
phrase-based SMT (Koehn et al, 2007), we need
to provide the same set of phrase translation prob-
ability distributions.1 We follow Utiyama and Isa-
hara (2007) in computing the probability distribu-
tions. The following are the set of equations used
to compute the lexical probabilities (?) and the
phrase probabilities (pw)
?(f |a) =?
e
?(f |e)?(e|a)
?(a|f) =?
e
?(a|e)?(e|f)
pw(f |a) =
?
e
pw(f |e)pw(e|a)
pw(a|f) =
?
e
pw(a|e)pw(e|f)
where f is the Persian source phrase. e is
the English pivot phrase that is common in both
Persian-English translation model and English-
Arabic translation model. a is the Arabic target
phrase.
We also build a Persian-Arabic reordering table
using the same technique but we compute the re-
ordering weights in a similar manner to Henriquez
et al (2010).
As discussed earlier, the induced Persian-
Arabic phrase and reordering tables are very large.
Table 1 shows the amount of parallel corpora
used to train the Persian-English and the English-
Arabic and the equivalent phrase table sizes com-
pared to the induced Persian-Arabic phrase table.2
We introduce a basic filtering technique dis-
cussed next to address this issue and present some
baseline experiments to test its performance in
Section 5.3.
3.3 Filtering for Phrase Pivoting
The main idea of the filtering process is to select
the top [n] English candidate phrases for each Per-
sian phrase from the Persian-English phrase ta-
ble and similarly select the top [n] Arabic target
phrases for each English phrase from the English-
Arabic phrase table and then perform the pivot-
ing process described earlier to create a pivoted
1Four different phrase translation scores are computed in
Moses? phrase tables: two lexical weighting scores and two
phrase translation probabilities.
2The size of the induced phrase table size is computed but
not created.
413
Training Corpora Phrase Table
Translation Model Size # Phrase Pairs Size
Persian-English ?4M words 96,04,103 1.1GB
English-Arabic ?60M words 111,702,225 14GB
Pivot Persian-Arabic N/A 39,199,269,195 ?2.5TB
Table 1: Translation Models Phrase Table comparison in terms of number of line and sizes.
Persian-Arabic phrase table. To select the top can-
didates, we first rank all the candidates based on
the log linear scores computed from the phrase
translation probabilities and lexical weights mul-
tiplied by the optimized decoding weights then we
pick the top [n] pairs.
We compare the different pivoting strategies
and various filtering thresholds in Section 5.3.
4 Approach
One of the main challenges in phrase pivoting is
the very large size of the induced phrase table.
It becomes even more challenging if either the
source or target language is morphologically rich.
The number of translation candidates (fanout) in-
creases due to ambiguity and richness (discussed
in more details in Section 5.2) which in return
increases the number of combinations between
source and target phrases. Since the only criteria
of matching between the source and target phrase
is through a pivot phrase, many of the induced
phrase pairs are of low quality. These phrase pairs
unnecessarily increase the search space and hurt
the overall quality of translation.
To solve this problem, we introduce two
language-independent features which are added to
the log linear space of features in order to deter-
mine the quality of the pivot phrase pairs. We call
these features connectivity strength features.
Connectivity Strength Features provide two
scores, Source Connectivity Strength (SCS) and
Target Connectivity Strength (TCS). These two
scores are similar to precision and recall metrics.
They depend on the number of alignment links be-
tween words in the source phrase to words of the
target phrase. SCS and TSC are defined in equa-
tions 1 and 2 where S = {i : 1 ? i ? S} is the
set of source words in a given phrase pair in the
pivot phrase table and T = {j : 1 ? j ? T}
is the set of the equivalent target words. The
word alignment between S and T is defined as
A = {(i, j) : i ? S and j ? T }.
SCS = |A||S| (1)
TCS = |A||T | (2)
We get the alignment links by projecting the
alignments of source-pivot to the pivot-target
phrase pairs used in pivoting. If the source-target
phrase pair are connected through more than one
pivot phrase, we take the union of the alignments.
In contrast to the aggregated values represented
in the lexical weights and the phrase probabilities,
connectivity strength features provide additional
information by counting the actual links between
the source and target phrases. They provide an
independent and direct approach to measure how
good or bad a given phrase pair are connected.
Figure 1 and 2 are two examples (one good, one
bad) Persian-Arabic phrase pairs in a pivot phrase
table induced by pivoting through English.3 In the
first example, each Persian word is aligned to an
Arabic word. The meaning is preserved in both
phrases which is reflected in the SCS and TCS
scores. In the second example, only one Persian
word in aligned to one Arabic word in the equiv-
alent phrase and the two phrases conveys two dif-
ferent meanings. The English phrase is not a good
translation for either, which leads to this bad pair-
ing. This is reflected in the SCS and TCS scores.
5 Experiments
In this section, we present a set of baseline ex-
periments including a simple filtering technique to
overcome the huge expansion of the pivot phrase
table. Then we present our results in using connec-
tivity strength features to improve Persian-Arabic
pivot translation quality.
3We use the Habash-Soudi-Buckwalter Arabic transliter-
ation (Habash et al, 2007) in the figures with extensions for
Persian as suggested by Habash (2010).
414
Persian: "A?tmAd"myAn"dw"k?wr " " "? ?"??"()"?"?%$#"?,-. ?"" " " " " " " " " " " " "?trust"between"the"two"countries?"English: "trust"between"the"two"countries"
Arabic:" "Al?q?"byn"Aldwltyn " " " "? /012?52?2$3"34"? ?"" " " " " " " " " " " " "?the"trust"between"the"two"countries?"
Figure 1: An example of strongly connected Persian-Arabic phrase pair through English. All Persian
words are connected to one or more Arabic words. SCS=1.0 and TCS=1.0.
Persian: "AyjAd"cnd"?rkt"m?trk " " " "? 0/.+?",+*(")'&"?$#"? ?"" " " " " " " " " " " " "?Establish"few"joint"companies?"English: "joint"ventures"
Arabic:" "b?D"?rkAt"AlmqAwlAt"fy"Albld" "? 123"?<=>&";:"?89"?6?",+5"? ?"" " " " " " " " " " " " "?Some"construcBon"companies"in"the"country?"
Figure 2: An example of weakly connected Persian-Arabic phrase pairs through English. Only one
Persian word is connected to an Arabic word. SCS=0.25 and TCS=0.2.
5.1 Experimental Setup
In our pivoting experiments, we build two SMT
models. One model to translate from Persian to
English and another model to translate from En-
glish to Arabic. The English-Arabic parallel cor-
pus is about 2.8M sentences (?60M words) avail-
able from LDC4 and GALE5 constrained data. We
use an in-house Persian-English parallel corpus of
about 170K sentences and 4M words.
Word alignment is done using GIZA++ (Och
and Ney, 2003). For Arabic language model-
ing, we use 200M words from the Arabic Giga-
word Corpus (Graff, 2007) together with the Ara-
bic side of our training data. We use 5-grams
for all language models (LMs) implemented us-
ing the SRILM toolkit (Stolcke, 2002). For En-
glish language modeling, we use English Giga-
word Corpus with 5-gram LM using the KenLM
toolkit (Heafield, 2011).
All experiments are conducted using the Moses
phrase-based SMT system (Koehn et al, 2007).
We use MERT (Och, 2003) for decoding weight
4LDC Catalog IDs: LDC2005E83, LDC2006E24,
LDC2006E34, LDC2006E85, LDC2006E92, LDC2006G05,
LDC2007E06, LDC2007E101, LDC2007E103,
LDC2007E46, LDC2007E86, LDC2008E40, LDC2008E56,
LDC2008G05, LDC2009E16, LDC2009G01.
5Global Autonomous Language Exploitation, or GALE,
is a DARPA-funded research project.
optimization. For Persian-English translation
model, weights are optimized using a set 1000 sen-
tences randomly sampled from the parallel cor-
pus while the English-Arabic translation model
weights are optimized using a set of 500 sen-
tences from the 2004 NIST MT evaluation test
set (MT04). The optimized weights are used for
ranking and filtering (discussed in Section 3.3).
We use a maximum phrase length of size 8
across all models. We report results on an in-
house Persian-Arabic evaluation set of 536 sen-
tences with three references. We evaluate using
BLEU-4 (Papineni et al, 2002) and METEOR
(Lavie and Agarwal, 2007).
5.2 Linguistic Preprocessing
In this section we present our motivation and
choice for preprocessing Arabic, Persian, English
data. Both Arabic and Persian are morphologi-
cally complex languages but they belong to two
different language families. They both express
richness and linguistic complexities in different
ways.
One aspect of Arabic?s complexity is its vari-
ous attachable clitics and numerous morphologi-
cal features (Habash, 2010). We follow El
Kholy and Habash (2010a) and use the PATB to-
kenization scheme (Maamouri et al, 2004) in our
415
experiments. We use MADA v3.1 (Habash and
Rambow, 2005; Habash et al, 2009) to tokenize
the Arabic text. We only evaluate on detokenized
and orthographically correct (enriched) output fol-
lowing the work of El Kholy and Habash (2010b).
Persian on the other hand has a relatively sim-
ple nominal system. There is no case system and
words do not inflect with gender except for a few
animate Arabic loanwords. Unlike Arabic, Persian
shows only two values for number, just singular
and plural (no dual), which are usually marked by
either the suffix A?+ +hA and sometimes 	?@+ +An,
or one of the Arabic plural markers. Verbal mor-
phology is very complex in Persian. Each verb
has a past and present root and many verbs have
attached prefix that is regarded part of the root.
A verb in Persian inflects for 14 different tense,
mood, aspect, person, number and voice combina-
tion values (Rasooli et al, 2013). We use Perstem
(Jadidinejad et al, 2010) for segmenting Persian
text.
English, our pivot language, is quite different
from both Arabic and Persian. English is poor
in morphology and barely inflects for number and
tense, and for person in a limited context. English
preprocessing simply includes down-casing, sepa-
rating punctuation and splitting off ??s?.
5.3 Baseline Evaluation
We compare the performance of sentence pivot-
ing against phrase pivoting with different filtering
thresholds. The results are presented in Table 2. In
general, the phrase pivoting outperforms the sen-
tence pivoting even when we use a small filtering
threshold of size 100. Moreover, the higher the
threshold the better the performance but with a di-
minishing gain.
Pivot Scheme BLEU METEOR
Sentence Pivoting 19.2 36.4
Phrase Pivot F100 19.4 37.4
Phrase Pivot F500 20.1 38.1
Phrase Pivot F1K 20.5 38.6
Table 2: Sentence pivoting versus phrase pivoting
with different filtering thresholds (100/500/1000).
We use the best performing setup across the rest
of the experiments.
5.4 Connectivity Strength Features
Evaluation
In this experiment, we test the performance of
adding the connectivity strength features (+Conn)
to the best performing phrase pivoting model
(Phrase Pivot F1K).
Model BLEU METEOR
Sentence Pivoting 19.2 36.4
Phrase Pivot F1K 20.5 38.6
Phrase Pivot F1K+Conn 21.1 38.9
Table 3: Connectivity strength features experi-
ment result.
The results in Table 3 show that we get a
nice improvement of ?0.6/0.5 (BLEU/METEOR)
points by adding the connectivity strength fea-
tures. The differences in BLEU scores between
this setup and all other systems are statistically
significant above the 95% level. Statistical signif-
icance is computed using paired bootstrap resam-
pling (Koehn, 2004).
6 Conclusion and Future Work
We presented an experiment showing the effect of
using two language independent features, source
connectivity score and target connectivity score,
to improve the quality of pivot-based SMT. We
showed that these features help improving the
overall translation quality. In the future, we plan
to explore other features, e.g., the number of the
pivot phases used in connecting the source and tar-
get phrase pair and the similarity between these
pivot phrases. We also plan to explore language
specific features which could be extracted from
some seed parallel data, e.g., syntactic and mor-
phological compatibility of the source and target
phrase pairs.
Acknowledgments
The work presented in this paper was possible
thanks to a generous research grant from Science
Applications International Corporation (SAIC).
The last author (Sawaf) contributed to the effort
while he was at SAIC. We would like to thank M.
Sadegh Rasooli and Jon Dehdari for helpful dis-
cussions and insights into Persian. We also thank
the anonymous reviewers for their insightful com-
ments.
416
References
Nicola Bertoldi, Madalina Barbaiani, Marcello Fed-
erico, and Roldano Cattoni. 2008. Phrase-based
statistical machine translation with pivot languages.
Proceeding of IWSLT, pages 143?149.
Trevor Cohn and Mirella Lapata. 2007. Ma-
chine translation by triangulation: Making ef-
fective use of multi-parallel corpora. In AN-
NUAL MEETING-ASSOCIATION FOR COMPU-
TATIONAL LINGUISTICS, volume 45, page 728.
Ahmed El Kholy and Nizar Habash. 2010a. Ortho-
graphic and Morphological Processing for English-
Arabic Statistical Machine Translation. In Proceed-
ings of Traitement Automatique du Langage Naturel
(TALN-10). Montre?al, Canada.
Ahmed El Kholy and Nizar Habash. 2010b. Tech-
niques for Arabic Morphological Detokenization
and Orthographic Denormalization. In Proceed-
ings of the seventh International Conference on Lan-
guage Resources and Evaluation (LREC), Valletta,
Malta.
Jakob Elming and Nizar Habash. 2009. Syntactic
Reordering for English-Arabic Phrase-Based Ma-
chine Translation. In Proceedings of the EACL 2009
Workshop on Computational Approaches to Semitic
Languages, pages 69?77, Athens, Greece, March.
David Graff. 2007. Arabic Gigaword 3, LDC Cat-
alog No.: LDC2003T40. Linguistic Data Consor-
tium, University of Pennsylvania.
Nizar Habash and Jun Hu. 2009. Improving Arabic-
Chinese Statistical Machine Translation using En-
glish as Pivot Language. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, pages 173?181, Athens, Greece, March.
Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphologi-
cal Disambiguation in One Fell Swoop. In Proceed-
ings of the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL?05), pages 573?
580, Ann Arbor, Michigan.
Nizar Habash and Fatiha Sadat. 2006. Arabic Pre-
processing Schemes for Statistical Machine Transla-
tion. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, Companion Vol-
ume: Short Papers, pages 49?52, New York City,
USA.
Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.
2007. On Arabic Transliteration. In A. van den
Bosch and A. Soudi, editors, Arabic Computa-
tional Morphology: Knowledge-based and Empiri-
cal Methods. Springer.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS
tagging, stemming and lemmatization. In Khalid
Choukri and Bente Maegaard, editors, Proceedings
of the Second International Conference on Arabic
Language Resources and Tools. The MEDAR Con-
sortium, April.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.
Jan Hajic?, Jan Hric, and Vladislav Kubon. 2000. Ma-
chine Translation of Very Close Languages. In Pro-
ceedings of the 6th Applied Natural Language Pro-
cessing Conference (ANLP?2000), pages 7?12, Seat-
tle.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197, Edinburgh, UK.
Carlos Henriquez, Rafael E. Banchs, and Jose? B.
Marin?o. 2010. Learning reordering models for sta-
tistical machine translation with a pivot language.
Amir Hossein Jadidinejad, Fariborz Mahmoudi, and
Jon Dehdari. 2010. Evaluation of PerStem: a sim-
ple and efficient stemming algorithm for Persian. In
Multilingual Information Access Evaluation I. Text
Retrieval Experiments, pages 98?101.
Andreas Kathol and Jing Zheng. 2008. Strategies for
building a Farsi-English smt system from limited re-
sources. In Proceedings of the 9th Annual Confer-
ence of the International Speech Communication As-
sociation (INTERSPEECH2008), pages 2731?2734,
Brisbane, Australia.
M. Khalilov, Marta R. Costa-juss, Jos A. R. Fonollosa,
Rafael E. Banchs, B. Chen, M. Zhang, A. Aw, H. Li,
Jos B. Mario, Adolfo Hernndez, and Carlos A. Hen-
rquez Q. 2008. The talp & i2r smt systems for iwslt
2008. In International Workshop on Spoken Lan-
guage Translation. IWSLT 2008, pg. 116?123.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Christo-
pher Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Christopher Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
Companion Volume Proceedings of the Demo and
Poster Sessions, pages 177?180, Prague, Czech Re-
public.
Philipp Koehn, Alexandra Birch, and Ralf Steinberger.
2009. 462 machine translation systems for europe.
Proceedings of MT Summit XII, pages 65?72.
Philipp Koehn. 2004. Statistical significance tests for-
machine translation evaluation. In Proceedings of
the Empirical Methods in Natural Language Pro-
cessing Conference (EMNLP?04), Barcelona, Spain.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: An
automatic metric for mt evaluation with high levels
of correlation with human judgments. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, pages 228?231, Prague, Czech Repub-
lic.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic Treebank:
Building a Large-Scale Annotated Arabic Corpus.
417
In NEMLAR Conference on Arabic Language Re-
sources and Tools, pages 102?109, Cairo, Egypt.
Evgeny Matusov and Selc?uk Ko?pru?. 2010. Improv-
ing reordering in statistical machine translation from
farsi. In AMTA The Ninth Conference of the Associ-
ation for Machine Translation in the Americas, Den-
ver, Colorado, USA.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?52.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics-Volume 1, pages 160?167. As-
sociation for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311?318,
Philadelphia, PA.
Mohammad Sadegh Rasooli, Manouchehr Kouhestani,
and Amirsaeid Moloodi. 2013. Development of
a Persian syntactic dependency treebank. In The
2013 Conference of the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies (NAACL HLT), At-
lanta, USA.
Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing (ICSLP), volume 2, pages 901?904, Denver,
CO.
Masao Utiyama and Hitoshi Isahara. 2007. A com-
parison of pivot methods for phrase-based statistical
machine translation. In Human Language Technolo-
gies 2007: The Conference of the North American
Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
484?491, Rochester, New York, April. Association
for Computational Linguistics.
Hua Wu and Haifeng Wang. 2009. Revisiting pivot
language approach for machine translation. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 154?162, Suntec, Singapore, August.
Association for Computational Linguistics.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-to-
morphology mapping in factored phrase-based sta-
tistical machine translation from english to turkish.
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 454?
464, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
418
