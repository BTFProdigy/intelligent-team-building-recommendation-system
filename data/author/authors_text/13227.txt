Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 395?404,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Learning from Post-Editing:
Online Model Adaptation for Statistical Machine Translation
Michael Denkowski Chris Dyer Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{mdenkows,cdyer,alavie}@cs.cmu.edu
Abstract
Using machine translation output as a
starting point for human translation has
become an increasingly common applica-
tion of MT. We propose and evaluate three
computationally efficient online methods
for updating statistical MT systems in a
scenario where post-edited MT output is
constantly being returned to the system:
(1) adding new rules to the translation
model from the post-edited content, (2)
updating a Bayesian language model of
the target language that is used by the
MT system, and (3) updating the MT
system?s discriminative parameters with
a MIRA step. Individually, these tech-
niques can substantially improve MT qual-
ity, even over strong baselines. Moreover,
we see super-additive improvements when
all three techniques are used in tandem.
1 Introduction
Using machine translation outputs as a starting
point for human translators is becoming increas-
ingly common and is now arguably one of the most
commercially important applications of MT. Con-
siderable evidence has accumulated showing that
human translators are more productive and accu-
rate when post-editing MT output than when trans-
lating from scratch (Guerberof, 2009; Carl et al.,
2011; Koehn, 2012; Zhechev, 2012, inter alia).
An important (if unsurprising) insight from prior
research in this area is that translators become
more productive as MT quality improves (Tat-
sumi, 2009). While general improvements to MT
continue to lead to further productivity gains, we
explore how MT quality can be improved specifi-
cally in an online post-editing scenario in which
sentence-level MT outputs are constantly being
presented to human experts, edited, and then re-
turned to the system for immediate learning. This
task is challenging in two regards. First, from a
technical perspective, post-edited outputs must be
processed rapidly: a productive post-editor cannot
wait for a standard batch MT training pipeline to
be rerun after each sentence is corrected! Sec-
ond, from a methodological perspective, it is ex-
pensive to run many human subject experiments,
in particular when the human subjects must have
translation expertise. We therefore use a sim-
ulated post-editing paradigm in which either
non-post-edited reference translations or manually
post-edited translations from a similar MT system
are used in lieu of human post-editors (?2). This
paradigm allows us to efficiently develop and eval-
uate systems that can go on to function in real-time
post-editing scenarios without modification.
We present and evaluate three online methods
for improving translation models using feedback
from editors: adding new translations rules to
the translation grammar (?3), updating a Bayesian
language model with observations of the post-
edited output (?4), and using an online discrimi-
native parameter update to minimize model error
(?5). These techniques are computationally effi-
cient and make minimal use of approximation or
heuristics, handling initial and incremental data in
a uniform way. We evaluate these techniques in a
variety of language and data scenarios that mimic
the demands of real-world translation tasks. Com-
pared to a competitive baseline, we show substan-
tial improvement from updating the translation
grammar or language model independently and
super-additive gains from combining these tech-
niques with a MIRA update (?6). We then discuss
how our techniques relate to prior work (?7) and
conclude (?8).
2 Simulated Post-Editing Paradigm
In post-editing scenarios, humans continuously
edit machine translation outputs into production-
quality translations, providing an additional, con-
395
stant stream of data absent in batch translation.
This data consists of highly domain-relevant ref-
erence translations that are minimally different
from MT outputs, making them ideal for learn-
ing. However, true post-editing data is infeasi-
ble to collect during system development and in-
ternal testing as standard MT pipelines require
tens of thousands of sentences to be translated
with low latency. To address this problem, Hardt
and Elming (2010) formulate the task of sim-
ulated post-editing, wherein pre-generated refer-
ence translations are used as a stand-in for actual
post-editing. This approximation is equivalent to
the case where humans edit each translation hy-
pothesis to be identical to the reference rather than
simply correcting the MT output to be grammat-
ical and meaning-equivalent to the source. Our
work uses this approximation for tuning and eval-
uation. We also introduce a more accurate approx-
imation wherein MT output from the target sys-
tem (or a similar system) is post-edited in advance,
creating ?offline? post-edited data that is similar
to expected system outputs and should thus min-
imize unnecessary edits. An experiment in ?6.4
compares the two approximations.
In our simulated post-editing tasks, decoding
(for both the test corpus and each pass over the
development corpus during optimization) begins
with baseline models trained on standard bilin-
gual and monolingual data. After each sentence
is translated, the following take place in order:
First, MIRA uses the new source?reference pair
to update weights for the current models. Second,
the source is aligned to the reference and used to
update the translation grammar. Third, the refer-
ence is added to the Bayesian language model. As
sentences are translated, the models gain valuable
context information, allowing them to zero in on
the target document and translator. Context is re-
set at the start of each development or test corpus.
1
This setup, which allows a uniform approach to
tuning and decoding, is visualized in Figure 1.
3 Translation Grammar Adaptation
Translation models (either phrase tables or syn-
chronous grammars) are typically generated of-
fline from large bilingual text. This is reasonable
in scenarios where available training data is fixed
over long periods of time. However, this approach
1
Initial experiments show this to outperform resetting
models on more fine-grained document boundaries, although
further investigation is warranted.
Hola contestadora ...
Hello voicemail, ...
He llamado a servicio ... I?ve called for tech ...
Ignor?e la advertencia ... I ignored my boss? ...
Ahora anochece, ...
Now it?s evening, and ...
Todav??a sigo en espera ...
I?m still on hold ...
No creo que me hayas ... I don?t think you ...
Ya he presionado cada ... I punched every touch ...
Incremental training data
Source
Target (Reference)
Figure 1: Context when translating an input sen-
tence (bold) with simulated post-editing. Previ-
ous sentences and references (shaded) are added
to the training data. After the current sentence is
translated, it is aligned to the reference (italic) and
added to the context for the next sentence.
does not allow adding new data without repeating
model estimation in its entirety, which may take
hours or days. In this section, we describe a simple
technique for incorporating new bilingual training
data as soon as it is available. Our approach is
an extension of the on-demand grammar extractor
described by Lopez (2008a). We extend the work
initially designed for on-the-fly grammar extrac-
tion from static data (to mitigate the expense of
storing large translation grammars), to specifically
handle incremental data from post-editing.
3.1 Suffix Array Grammar Extraction
Lopez (2008a) introduces an alternative to tradi-
tional model estimation for hierarchical phrase-
based statistical machine translation (Chiang,
2007). Rather than estimating a single grammar
from all training data, the aligned bitext is indexed
using a source-side suffix array (Manber and My-
ers, 1993). When an input sentence is to be trans-
lated, a grammar extraction program samples in-
stances of aligned phrase pairs from the suffix ar-
ray that match the source side of the sentence.
Using statistics from these samples rather than
the entire bitext, a sentence-specific grammar is
rapidly generated. In addition to speed gains from
sampling, indexing the source side of the bitext fa-
cilitates a more powerful feature set. Rules in on-
demand grammars are generated using a sample S
for each source phrase f in the input sentence. The
sample, containing pairs ?f, e?, is used to calculate
the following statistics:
396
Feature Baseline Adaptive
coherent
p(e|f)
C
S
(f, e)
|S|
C
S
(f, e) + C
L
(f, e)
|S|+ |L|
sample size |S| |S|+ |L|
co-occur-
rence ?f, e?
C
S
(f, e) C
S
(f, e)+C
L
(f, e)
singleton f
C
S
(f)
= 1
C
S
(f) + C
L
(f) =
1
singleton
?f, e?
C
S
(f, e)
= 1
C
S
(f, e) + C
L
(f, e)
= 1
post-edit sup-
port ?f, e?
0 C
L
(f, e) > 0
Table 1: Phrase feature definitions for baseline and
adaptive translation models.
? C
S
(f, e): count of instances in S where f
aligns to e (phrase co-occurrence count).
? C
S
(f): count of instances in S where f aligns
to any target phrase.
? |S|: total number of instances in S, equal to
number of occurrences of f in training data,
capped by the sample size limit.
These statistics are used to instantiate translation
rules X??f, e? and calculate scores for the phrase
feature set shown in the ?Baseline? column of Ta-
ble 1. Notably, the coherent phrase translation
probability that conditions on f occurring in the
data (|S|) rather than f being extracted as part of a
phrase pair (C
S
(f)) is shown by Lopez (2008b) to
yield significant improvement over the traditional
translation probability.
3.2 Online Grammar Extraction
When a human translator post-edits MT output, a
new bilingual sentence pair is created. However,
in typical settings, it can be weeks or months be-
fore these training instances are incorporated into
bilingual data and models retrained. Our exten-
sion to on-demand grammar extraction incorpo-
rates these new training instances into the model
immediately. In addition to a static suffix array
that indexes initial data, our system maintains a
dynamic lookup table. Each new sentence pair is
word-aligned with the model estimated from the
initial data (a process often called forced align-
ment). This makes a generally insignificant ap-
proximation with respect to the original alignment
model. Extractable phrase pairs are stored in the
lookup table and phrase occurrences are counted
on the source side. When subsequent grammars
are extracted, the suffix array sample S for each
f is accompanied by an exhaustive lookup L from
the lookup table. Matching statistics are calculated
from L:
? C
L
(f, e): count of instances in L where f
aligns to e.
? C
L
(f): count of instances inLwhere f aligns
to any target phrase.
? |L|: total number of instances of f in post-
editing data (no size limit).
We use combined statistics from S and L to calcu-
late scores for the ?Adaptive? feature set defined in
Table 1. In addition to updating existing features,
we introduce a new indicator feature that identi-
fies rules supported by post-editor feedback. Fur-
ther, our approach allows us to extract rules that
encode translations (phrase mappings and reorder-
ings) only observed in the incremental post-editing
data. This process, which can be seen as influ-
encing the distribution from which grammars are
sampled over time, produces comparable results
to the infeasible process of rebuilding the transla-
tion model after every sentence is translated with
the added benefit of allowing an optimizer to learn
a weight for the post-edited data via the post-edit
support feature. The simple aggregation of statis-
tics allows our model to handle initial and incre-
mental data in a formally consistent way. Further,
any additional features that can be calculated on a
suffix array sample can be matched by an incre-
mental data lookup, making our translation model
a viable platform for further exploration in online
learning for MT.
4 Language Model Adaptation
Adapting language models in an online manner
based on the content they are generating has long
been seen as a promising technique for improving
automatic speech recognition and machine transla-
tion (Kuhn and de Mori, 1990; Zhao et al., 2004;
Sanchis-Trilles, 2012, inter alia). The post-editing
scenario we are considering simplifies this process
somewhat since rather than only having a poste-
rior distribution over machine-generated outputs
(any of which may be ungrammatical), the out-
puts, once edited by human translators, may be
presumed to be grammatical.
We thus take a novel approach to language
model adaptation, building on recent work show-
ing that state-of-the-art language models can be
397
inferred as the posterior predictive distribution
of a Bayesian language model with hierarchi-
cal Pitman-Yor process priors, conditioned on the
training corpus (Teh, 2006). The Bayesian formu-
lation provides a natural way to incorporate pro-
gressively more data: by updating the posterior
distribution given subsequent observations. Fur-
thermore, the nonparametric nature of the model
means that the model is well suited to poten-
tially unbounded growth of vocabulary. Unfortu-
nately, in general, Bayesian techniques are com-
putationally difficult to work with. However, hi-
erarchical Pitman-Yor process language models
(HPYPLMs) are convenient in this regard since
(1) inference can be carried out efficiently in a
convenient collapsed representation (the ?Chinese
restaurant franchise?) and (2) the posterior predic-
tive distribution from a single sample provides a
high quality language model.
We thus use the following procedure. Using
the target side of the bitext as observations, we
run the Gibbs sampling procedure described by
Teh (2006) for 100 iterations in a 3-gram HPY-
PLM. The inferred ?seating configuration? defines
a posterior predictive distribution over words in 2-
gram contexts (as with any 3-gram LM) as well
as a posterior distribution over how the model will
generate subsequent observations. We use the for-
mer as a language model component of a transla-
tion model. And, as post-edited sentences become
available, we add their n-grams to the model us-
ing the later. We do not run any Gibbs sampling.
Just updating the language model in this way, we
obtain the results shown in Table 2 for the experi-
mental conditions described in ?6.
5 Learning Feature Weights
MT system parameter optimization (learning fea-
ture weights for the decoder) is also typically con-
ducted as a batch process. Discriminative learn-
ing techniques such as minimum error rate train-
ing (Och, 2003) are used to find feature weights
that maximize automatic metric score on a small
development corpus. The resulting weight vector
is then used to decode given input sentences. Us-
ing this approach with post-editing tasks presents
two major issues. First, reference translation are
only considered after all sentences are translated,
a mismatch with post-editing where references are
available incrementally. Second, despite the fact
that adaptive feature sets become more powerful
as post-editing data increases, an optimizer must
Spanish?English WMT10 WMT11 TED1 TED2
HPYPLM 25.5 24.8 29.4 26.6
+data 25.8 25.2 29.5 27.0
English?Spanish WMT10 WMT11 TED1 TED2
HPYPLM 25.1 26.8 26.0 24.3
+data 25.4 27.2 26.2 25.0
Arabic?English MT08 MT09 TED1 TED2
HPYPLM 19.3 24.7 9.5 10.0
+data 19.6 24.9 9.8 10.5
Table 2: BLEU scores for systems with trigram
HPYPLM (no large language model), with and
without incremental updates from simulated post-
editing data. Scores are averages over 3 optimizer
runs. Bold scores indicate statistically significant
improvement. Tuning set scores are italicized.
learn a single corpus-level weight for each fea-
ture. This forces an averaging effect that can lead
to decoding individual sentences with suboptimal
weights. We address the first issue by using ref-
erence translations to simulate post-editing (Hardt
and Elming, 2010) at tuning time and the second
by using a version of the margin-infused relaxed
algorithm (Crammer et al., 2006; Eidelman, 2012)
to make online parameter updates during decod-
ing. The result is a consistent approach to tuning
and decoding that brings out the potential of adap-
tive models.
5.1 Parameter Optimization
In order to make our decoding process fully con-
sistent with tuning, we introduce an online dis-
criminative parameter update that allows our adap-
tive translation and language models be weighted
appropriately as more data is available. This re-
quires an optimization algorithm that can func-
tion as an online learner during decoding as well
as a batch optimizer during tuning. Popular opti-
mizers such as MERT (Och, 2003) and pairwise
rank optimization (Hopkins and May, 2011) can-
not be used due to their reliance on corpus-level
optimization. We select the cutting-plane variant
of the margin-infused relaxed algorithm (Chiang,
2012; Crammer et al., 2006) with additional exten-
sions described by Eidelman (2012). MIRA is an
online large-margin learner that makes a param-
eter update after each model prediction with the
objective of choosing the correct output over the
incorrect output by a margin at least as large as the
cost of predicting the incorrect output. Applied
398
to MT system optimization on a development cor-
pus, MIRA proceeds as follows. The MT system
generates a list of the k best translations for a sin-
gle input sentence. From the list, a ?hope? hy-
pothesis is selected as a translation with both high
model score and high automatic metric score. A
?fear? hypothesis is selected as a translation with
high model score but low metric score. Parameters
are updated away from the fear hypothesis, toward
the hope hypothesis, and the system processes the
next input sentence. This process continues for a
set number of passes over the development corpus.
All adaptive systems used in our work are opti-
mized with this variant of MIRA using the param-
eter settings described by Eidelman (2012). For
each pass over the data, translation and language
models have incremental access to reference trans-
lations (simulated post-editing data) as input sen-
tences are translated. Translation and language
models reset to using background data only at the
beginning of each MIRA iteration.
2
5.2 Online Parameter Updates
Our optimization strategy allows us to treat de-
coding as if it were simply the next iteration of
MIRA (or alternatively that MIRA makes a single
pass over an input corpus that consists of the de-
velopment data concatenated n times followed by
unseen input data). After each sentence is trans-
lated, a reference translation (resulting from ac-
tual human post-editing in production or simulated
post-editing for our experiments) is provided to
the models and MIRA makes a parameter update.
In the only departure from our optimization setup,
we decrease the maximum step size for MIRA (de-
scribed in ?6.2), effectively increasing regulariza-
tion strength. This allows us to prefer small ad-
justments to already optimized decoding parame-
ters over the large changes needed during tuning.
It is also important to note that by using MIRA
for updating weights during both tuning and de-
coding, we avoid scaling issues between multiple
optimizers (such as when tuning with MERT and
updating with a passive-aggressive algorithm).
6 Experiments
We evaluate our online extensions to standard
machine translation systems in a series of sim-
2
Resetting translation and language models prevents con-
tamination. If models retained state from previous passes
over the development set, they would include data for input
sentences before they were translated, rather than after as in
post-editing.
Spanish?English WMT10 WMT11 TED1 TED2
Base MERT 29.1 27.9 32.8 29.6
Base MIRA 29.2 28.0 32.7 29.7
G 29.8 28.3 34.2 30.7
L 29.2 28.1 33.0 29.8
M 29.2 28.1 33.1 29.8
G+L+M 30.0 28.8 35.2 31.3
English?Spanish WMT10 WMT11 TED1 TED2
Base MERT 27.8 29.4 26.5 25.7
Base MIRA 27.7 29.6 26.8 26.7
G 28.1 29.8 27.9 27.5
L 27.9 29.7 26.8 26.5
M 27.9 29.7 27.2 26.6
G+L+M 28.4 30.4 28.6 27.9
Arabic?English MT08 MT09 TED1 TED2
Base MERT 21.5 25.0 10.4 10.5
Base MIRA 21.2 25.9 10.6 10.9
G 21.8 26.2 11.0 11.7
L 20.6 25.7 10.6 10.9
M 21.3 25.7 10.8 11.0
G+L+M 21.8 26.5 11.4 11.8
Table 3: BLEU scores for baseline and adap-
tive systems. Scores are averages over three opti-
mizer runs. Highest scores are bold and tuning set
scores are italicized. All fully adaptive systems
(G+L+M) show statistically significant improve-
ment over both MERT and MIRA baselines.
ulated post-editing experiments that cover high-
traffic languages and challenging domains. We
show incremental improvement from our adaptive
models and significantly larger gains when pair-
ing our models with an online parameter update.
We finally validate our adaptive system on actual
post-edited data.
6.1 Data
We conduct a series of simulated post-editing
experiments in three full scale language sce-
narios: Spanish?English, English?Spanish, and
Arabic?English. Spanish?English and English?
Spanish systems are trained on the 2012 NAACL
WMT (Callison-Burch et al., 2012) constrained
resources (2 million bilingual sentences, 300 mil-
lion words of monolingual Spanish, and 1.1 billion
words of monolingual English). Arabic?English
systems are trained on the 2012 NIST OpenMT
(Przybocki, 2012) constrained bilingual resources
plus a selection from the English Gigaword cor-
pus (Parker et al., 2011) (5 million bilingual sen-
tences and 650 million words of monolingual En-
399
glish). We tune and evaluate on standard news
sets: WMT10 and WMT11 for Spanish?English
and English?Spanish, and MT08 and MT09 for
Arabic?English. To simulate real-world post edit-
ing where one translator works on a document at a
time, we use only one of the four available refer-
ence translation sets for MT08 and MT09.
We also evaluate on a blind domain adapta-
tion scenario that mimics the demands placed
on MT systems in real-world translation tasks.
The Web Inventory of Transcribed and Translated
Talks (WIT
3
) corpus (Cettolo et al., 2012) makes
transcriptions of TED talks
3
available in several
languages, including English, Spanish, and Ara-
bic. For each language pair, we select two sets of
10 talk transcripts each (2000-3000 sentences) as
blind evaluation sets. These sets consist of spoken
language covering a broad range of topics. Sys-
tems have no access to any training or develop-
ment data in this domain prior to translation.
6.2 Translation Systems
For each language scenario, we first construct a
competitive baseline system. Bilingual data is
word aligned using the model described by Dyer
et al. (2013) and suffix array-backed transla-
tion grammars are extracted using the method
described by Lopez (2008a). We add the stan-
dard lexical and derivation features
4
from Lopez
(2008b) and Dyer et al. (2010). An unpruned,
modified Kneser-Ney-smoothed 4-gram language
model is estimated using the KenLM toolkit
(Heafield et al., 2013). Feature weights are op-
timized using the lattice-based variant of MERT
(Macherey et al., 2008; Och, 2003) on either
WMT10 or MT08. Evaluation sets are translated
using the cdec decoder (Dyer et al., 2010) and
evaluated with the BLEU metric (Papineni et al.,
2002). These results are listed as ?Base MERT?
in Table 3. To establish a baseline for our adap-
tive systems, we tune the same baseline system
using cutting-plane MIRA with 500-best lists, the
pseudo-document approximation described by Ei-
delman (2012), and a maximum update size of
0.01. We begin with uniform weights and make
20 passes over the development corpus. Results
for this system are listed as ?Base MIRA?.
To evaluate the impact of each online model
adaptation technique, we report the results for the
3
http://www.ted.com/talks
4
Derivation features consist of word count, discretized
rule-level non-terminal count (0, 1, or 2), glue rule count,
and out-of-vocabulary pass-through count.
News TED Talks
New Supp New Supp
Spanish?English 15% 19% 14% 18%
English?Spanish 12% 16% 9% 13%
Arabic?English 9% 12% 23% 28%
Table 5: Percentages of new rules (only seen
in incremental data) and post-edit supported rules
(Rules from all data for which the ?post-edit sup-
port ?f, e?? feature fires) in grammars by domain.
following systems in Table 3:
? G: Baseline MIRA system with online gram-
mar extraction, including incrementally up-
dating existing phrase features plus an addi-
tional indicator feature for post-edit support.
? L: Baseline MIRA with a trigram hierarchi-
cal Pitman-Yor process language model that
is incrementally updated, including a sepa-
rate out-of-vocabulary feature.
? M: Baseline MIRA with online feature
weight updates from cutting-plane MIRA.
Finally, we report results for a fully adaptive
system that includes online grammar, language
model, and feature weight updates. This system
is reported as ?G+L+M?. To account for optimizer
instability, all systems are tuned (consisting of
running either MERT or MIRA) and evaluated 3
times. We report average scores over optimizer
runs and conduct statistical significance tests us-
ing the methods described by Clark et al. (2011).
6.3 Results
Our simulated translation post-editing experi-
ments are summarized in Table 3. Simply mov-
ing from MERT to cutting-plane MIRA for pa-
rameter optimization yields improvement in most
cases, corroborating existing work (Eidelman,
2012). Using incremental post-editing data to up-
date translation grammars (G) yields further im-
provement in all cases evaluated. Gains are signif-
icantly larger for TED talks where translator feed-
back can bridge the gap between domains. Table 5
shows the aggregate percentages of rules in online
grammars that are entirely new (extracted from
post-editing instances only) or post-edit supported
(superset of new rules). While percentages vary
by data set, the overall trend is a combination of
learning new vocabulary and reordering and dis-
ambiguating existing translation choices.
The introduction of a trigram Bayesian lan-
guage model (L) yields mixed results: in some
400
Base MERT and changing the definition of what the Zona Cero is .
G+L+M and the changing definition of what the Ground Zero is .
Reference and the changing definition of what Ground Zero is .
Base MERT was that when we side by side comparisons with coal , timber
G+L+M was that when we did side-by-side comparisons with wood charcoal ,
Reference was when we did side-by-side comparisons with wood charcoal ,
Base MERT There was a way ? there was one ?
G+L+M There was a way ? there had to be a way ?
Reference There was a way ? there had to be a way ?
Table 4: Translation examples from baseline and fully adaptive systems of Spanish TED talks into En-
glish. Examples illustrate (from top to bottom) learning translations for new vocabulary items, selecting
correct translation candidates for the domain, and learning domain-appropriate phrasing.
cases it leads to slight improvement and in oth-
ers, degradation. It appears that a static but large
4-gram language model often outperforms an in-
crementally updated but smaller trigram model.
Further, learning a single weight for the Bayesian
model can lead to a harmful mismatch. As a tun-
ing pass over the development corpus proceeds,
the model incorporates additional data and MIRA
learns a weight corresponding to its predictive
ability at the end of the corpus. During decod-
ing, all sentences are translated with this language
model weight, even before the model can ade-
quately adapt itself to the target domain. This
problem is alleviated in our fully adaptive system.
Using cutting-plane MIRA to incrementally up-
date weights during decoding (M) also leads to
mixed results, frequently resulting in both small
increases and decreases in score. This could be
due to the noise incurred when making small ad-
justments to static features after each sentence:
depending on the similarity between the previous
and current sentence and the limit of the step size
(regularization strength), a parameter update may
slightly improve or degrade translation.
Finally, we see significantly larger gains for
our fully adaptive system (G+L+M) that com-
bines adaptive translation grammars and language
models with online parameter updates. In many
cases, the difference between the baseline sys-
tems and our adaptive system is greater than the
sum of the differences from our individual tech-
niques, demonstrating the effectiveness of com-
bining online learning methods. Our final sys-
tem has two key advantages over any individual
extension. First, incremental updates from MIRA
can rescale weights for features that change over
time, keeping the model consistent. Second, the
Bayesian language model?s out-of-vocabulary fea-
ture can discriminate between true OOV items
and vocabulary items in the post-editing data not
present in the monolingual data. By contrast, the
only OOVs in the baseline system are untranslated
items, as the target side of the bitext is included in
the language model training data. This interplay
between the adaptive components in our transla-
tion system leads to significant gains over MERT
and MIRA baselines. Table 4 contains examples
from our system?s output that exemplify key im-
provements in translation quality. With respect to
performance, our fully adaptive system translates
an average of 1.5 sentences per second per CPU
core. The additional cost incurred updating trans-
lation grammars and language models is less than
one second per sentence (though the baseline cost
of on-demand grammar extraction can be up to a
few seconds). In total, the system is well within
the acceptable speed range needed to function in
real-time human translation scenarios.
6.4 Evaluation Using Post-Edited References
The 2012 ACL Workshop on Machine Translation
(Callison-Burch et al., 2012) makes available a set
of 1832 English?Spanish parallel news source sen-
tences, independent references, initial MT outputs,
and post-edited MT outputs. The employed MT
system is trained on largely the same resources as
our own English?Spanish system, granting the op-
portunity for a much closer approximation to an
actual post-editing task; our system configurations
score between 54 and 56 BLEU against the sam-
ple MT, indicating that humans post-edited trans-
lations similar but not identical to our own. We
split the data into development and test sets, each
916 sentences, and run 3 iterations of optimizing
on the development set and evaluating on the test
set with both the MERT baseline and our G+L+M
401
system on both types of references. Using inde-
pendent references for tuning and evaluation (as
before), our system yields an improvement of 0.6
BLEU (23.3 to 23.9). With post-edited references,
our system yields an improvement of 1.3 BLEU
(43.0 to 44.3). This provides strong evidence that
our adaptive systems would provide better trans-
lations (both in terms of absolute quality and im-
provement over a standard baseline) for real-world
post-editing scenarios.
7 Related Work
Prior work has led to the extension of standard
phrase-based translation systems to make use of
incrementally available data.
5
Approaches gen-
erally fall into categories of adding new data to
translation models and of using incremental data
to adjust model parameters (feature weights). In
the first case, Nepveu et al. (2004) use cache-based
translation and language models to incorporate
data from the current document into a computer-
aided translation scenario. Ortiz-Mart??nez et al.
(2010) augment a standard translation model by
storing sufficient statistics in addition to feature
scores for phrase pairs, allowing feature values to
be incrementally updated as new sentence pairs
are available for phrase extraction. Hardt and Elm-
ing (2010) demonstrate the benefit of maintain-
ing a distinction between background and post-
editing data in an adaptive model with simulated
post-editing. Though not targeted at post-editing
applications, the most similar work to our online
grammar adaptation is the stream-based transla-
tion model described by Levenberg et al. (2010).
The authors introduce a dynamic suffix array that
can incorporate new training text as it becomes
available. Sanchis-Trilles (2012) proposes a strat-
egy for online language model adaptation wherein
several smaller domain-specific models are built
and their scores interpolated for each sentence
translated based on the target domain.
Focusing on incrementally updating model pa-
rameters with post-editing data, Mart??nez-G?omez
et al. (2012) and L?opez-Salcedo et al. (2012)
show improvement under some conditions when
using techniques including passive-aggressive al-
gorithms, perceptron, and discriminative ridge re-
gression to adapt feature weights for systems ini-
tially tuned using MERT. This work also uses ref-
erence translations to simulate post-editing. Saluja
5
Prior to phrase-based systems, NISHIDA et al. (1988)
use post-editing data to correct errors in transfer-based MT.
et al. (2012) introduce a support vector machine-
based algorithm capable of learning from binary-
labeled examples. This learning algorithm is used
to incrementally adjust feature weights given user
feedback on whether a translation is ?good? or
?bad?. As with our work, this strategy can be used
during both optimization and decoding.
Finally, Simard and Foster (2013) apply a
pipeline solution to the post-editing task wherein
a second stage automatic post-editor (APE) sys-
tem learns to replicate the corrections made to ini-
tial MT output by human translators. As incre-
mental data accumulates, the APE (itself a statisti-
cal phrase-based system) attempts to ?correct? the
MT output before it is shown to humans.
8 Conclusion
Casting machine translation for post-editing as
an online learning task, we have presented three
methods for incremental model adaptation: adding
data to the indexed bitext from which gram-
mars are extracted, updating a Bayesian language
model with incremental data, and using an on-
line discriminative parameter update during de-
coding. These methods, which allow the sys-
tem to handle all data in a uniform way, are ap-
plied to a strong baseline system optimized using
MIRA in conjunction with simulated post-editing.
In addition to showing gains for individual meth-
ods under various circumstances, we report super-
additive improvement from combining our tech-
niques to produce a fully adaptive system. Im-
provements generalize over language and data sce-
narios, with the greatest gains realized in blind
out-of-domain tasks where the system must rely
heavily on post-editor feedback to improve qual-
ity. Gains are also more significant when using of-
fline post-edited references, showing promise for
applying our techniques to real-world post-editing
tasks. All software used for our online model
adaptation experiments is freely available under an
open source license as part of the cdec toolkit.
6
Acknowledgements
This work is supported in part by the National Sci-
ence Foundation under grant IIS-0915327, by the
Qatar National Research Fund (a member of the
Qatar Foundation) under grant NPRP 09-1140-1-
177, and by the NSF-sponsored XSEDE program
under grant TG-CCR110017.
6
http://www.cs.cmu.edu/
?
mdenkows/
cdec-realtime.html
402
References
[Callison-Burch et al.2012] Chris Callison-Burch,
Philipp Koehn, Christof Monz, Matt Post, Radu
Soricut, and Lucia Specia. 2012. Findings of the
2012 workshop on statistical machine translation.
In Proceedings of the Seventh Workshop on Statis-
tical Machine Translation, pages 10?51, Montr?eal,
Canada, June. Association for Computational
Linguistics.
[Carl et al.2011] Michael Carl, Barbara Dragsted,
Jakob Elming, Daniel Hardt, and Arnt Lykke
Jakobsen. 2011. The process of post-editing: A
pilot study. Copenhagen Studies in Language,
41:131?142.
[Cettolo et al.2012] Mauro Cettolo, Christian Girardi,
and Marcello Federico. 2012. Wit
3
: Web inventory
of transcribed and translated talks. In Proceedings
of the Sixteenth Annual Conference of the European
Association for Machine Translation.
[Chiang2007] David Chiang. 2007. Hierarchical
phrase-based translation. Computational Linguis-
tics, 33.
[Chiang2012] David Chiang. 2012. Hope and fear for
discriminative training of statistical translation mod-
els. Journal of Machine Learning Research, pages
1159?1187, April.
[Clark et al.2011] Jonathan H. Clark, Chris Dyer, Alon
Lavie, and Noah A. Smith. 2011. Better hypothe-
sis testing for statistical machine translation: Con-
trolling for optimizer instability. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 176?181, Portland, Oregon, USA, June.
Association for Computational Linguistics.
[Crammer et al.2006] Koby Crammer, Ofer Dekel,
Joseph Keshet, Shai Shalev-Shwartz, and Yoram
Singer. 2006. Online passive-aggressive algo-
rithms. Journal of Machine Learning Research,
pages 551?558, March.
[Dyer et al.2010] Chris Dyer, Adam Lopez, Juri Gan-
itkevitch, Jonathan Weese, Ferhan Ture, Phil Blun-
som, Hendra Setiawan, Vladimir Eidelman, and
Philip Resnik. 2010. cdec: A decoder, alignment,
and learning framework for finite-state and context-
free translation models. In Proceedings of the ACL
2010 System Demonstrations, pages 7?12, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
[Dyer et al.2013] Chris Dyer, Victor Chahuneau, and
Noah A. Smith. 2013. A simple, fast, and effective
reparameterization of IBM model 2. In The 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies.
[Eidelman2012] Vladimir Eidelman. 2012. Optimiza-
tion strategies for online large-margin learning in
machine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
480?489, Montr?eal, Canada, June. Association for
Computational Linguistics.
[Guerberof2009] Ana Guerberof. 2009. Productivity
and quality in mt post-editing. In Proceedings of MT
Summit XII - Workshop: Beyond Translation Memo-
ries: New Tools for Translators MT.
[Hardt and Elming2010] Daniel Hardt and Jakob Elm-
ing. 2010. Incremental re-training for post-editing
smt. In Proceedings of the Ninth Conference of the
Association for Machine Translation in the Ameri-
cas.
[Heafield et al.2013] Kenneth Heafield, Ivan
Pouzyrevsky, Jonathan H. Clark, and Philipp
Koehn. 2013. Scalable modified Kneser-Ney
language model estimation. In Proceedings of
the 51st Annual Meeting of the Association for
Computational Linguistics, Sofia, Bulgaria, August.
[Hopkins and May2011] Mark Hopkins and Jonathan
May. 2011. Tuning as ranking. In Proceedings of
the 2011 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1352?1362, Edin-
burgh, Scotland, UK., July. Association for Compu-
tational Linguistics.
[Koehn2012] Philipp Koehn. 2012. Computer-aided
translation. Machine Translation Marathon.
[Kuhn and de Mori1990] Roland Kuhn and Renato
de Mori. 1990. A cache-based natural language
model for speech recognition. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
12(6).
[Levenberg et al.2010] Abby Levenberg, Chris
Callison-Burch, and Miles Osborne. 2010.
Stream-based translation models for statistical
machine translation. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 394?402, Los Angeles,
California, June. Association for Computational
Linguistics.
[Lopez2008a] Adam Lopez. 2008a. Machine transla-
tion by pattern matching. In Dissertation, Univer-
sity of Maryland, March.
[Lopez2008b] Adam Lopez. 2008b. Tera-scale transla-
tion models via pattern matching. In Proceedings
of the 22nd International Conference on Compu-
tational Linguistics (Coling 2008), pages 505?512,
Manchester, UK, August. Coling 2008 Organizing
Committee.
[L?opez-Salcedo et al.2012] Francisco-Javier L?opez-
Salcedo, Germ?an Sanchis-Trilles, and Francisco
Casacuberta. 2012. Online learning of log-linear
weights in interactive machine translation. Ad-
vances in Speech and Language Technologies for
Iberian Languages, pages 277?286.
403
[Macherey et al.2008] Wolfgang Macherey, Franz Och,
Ignacio Thayer, and Jakob Uszkoreit. 2008. Lattice-
based minimum error rate training for statistical ma-
chine translation. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 725?734, Honolulu, Hawaii, Oc-
tober. Association for Computational Linguistics.
[Manber and Myers1993] Udi Manber and Gene My-
ers. 1993. Suffix arrays: A new method for on-
line string searches. SIAM Journal of Computing,
22:935?948.
[Mart??nez-G?omez et al.2012] Pascual Mart??nez-
G?omez, Germ?an Sanchis-Trilles, and Francisco
Casacuberta. 2012. Online adaptation strategies
for statistical machine translation in post-editing
scenarios. Pattern Recognition, 45:3193?3203.
[Nepveu et al.2004] Laurent Nepveu, Guy Lapalme,
Philippe Langlais, and George Foster. 2004. Adap-
tive language and translation models for interactive
machine translation. In Dekang Lin and Dekai Wu,
editors, Proceedings of EMNLP 2004, pages 190?
197, Barcelona, Spain, July. Association for Com-
putational Linguistics.
[NISHIDA et al.1988] Fujio NISHIDA, Shinobu
TAKAMATSU, Tadaaki TANI, and Tsunehisa
DOI. 1988. Feedback of correcting information
in postediting to a machine translation system. In
Proc. of COLING.
[Och2003] Franz Josef Och. 2003. Minimum error rate
training in statistical machine translation. In Pro-
ceedings of the 41st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 160?167,
Sapporo, Japan, July. Association for Computational
Linguistics.
[Ortiz-Mart??nez et al.2010] Daniel Ortiz-Mart??nez, Is-
mael Garc??a-Varea, and Francisco Casacuberta.
2010. Online learning for interactive statistical ma-
chine translation. In Human Language Technolo-
gies: The 2010 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 546?554, Los Ange-
les, California, June. Association for Computational
Linguistics.
[Papineni et al.2002] Kishore Papineni, Salim Roukos,
Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a
method for automatic evaluation of machine trans-
lation. In Proceedings of 40th Annual Meeting
of the Association for Computational Linguistics,
pages 311?318, Philadelphia, Pennsylvania, USA,
July. Association for Computational Linguistics.
[Parker et al.2011] Robert Parker, David Graff, Junbo
Kong, Ke Chen, and Kazuaki Maeda. 2011. En-
glish Gigaword Fifth Edition, June. Linguistic Data
Consortium, LDC2011T07.
[Przybocki2012] Mark Przybocki. 2012. Nist open
machine translation 2012 evaluation (openmt12).
http://www.nist.gov/itl/iad/mig/openmt12.cfm.
[Saluja et al.2012] Avneesh Saluja, Ian Lane, and Ying
Zhang. 2012. Machine translation with binary feed-
back: a large-margin approach. In Proceedings of
the Tenth Biennial Conference of the Association for
Machine Translation in the Americas.
[Sanchis-Trilles2012] Germ?an Sanchis-Trilles. 2012.
Building task-oriented machine translation systems.
In Ph.D. Thesis, Universitat Politcnica de Valncia.
[Simard and Foster2013] Michel Simard and George
Foster. 2013. PEPr: Post-edit propagation using
phrase-based statistical machine translation. In Pro-
ceedings of the XIV Machine Translation Summit,
pages 191?198,, September.
[Tatsumi2009] Midori Tatsumi. 2009. Correlation
between automatic evaluation metric scores, post-
editing speed, and some other factors. In Proceed-
ings of the Twelfth Machine Translation Summit.
[Teh2006] Yee Whye Teh. 2006. A hierarchical
Bayesian language model based on Pitman-Yor pro-
cesses. In Proc. of ACL.
[Zhao et al.2004] Bing Zhao, Matthias Eck, and
Stephan Vogel. 2004. Language model adaptation
for statistical machine translation with structured
query models. In Proc. of COLING.
[Zhechev2012] Ventsislav Zhechev. 2012. Machine
Translation Infrastructure and Post-editing Perfor-
mance at Autodesk. In AMTA 2012 Workshop
on Post-Editing Technology and Practice (WPTP
2012), pages 87?96, San Diego, USA, October. As-
sociation for Machine Translation in the Americas
(AMTA).
404
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 250?253,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Extending the METEOR Machine Translation Evaluation Metric to the
Phrase Level
Michael Denkowski and Alon Lavie
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15232, USA
{mdenkows,alavie}@cs.cmu.edu
Abstract
This paper presents METEOR-NEXT, an ex-
tended version of the METEOR metric de-
signed to have high correlation with post-
editing measures of machine translation qual-
ity. We describe changes made to the met-
ric?s sentence aligner and scoring scheme as
well as a method for tuning the metric?s pa-
rameters to optimize correlation with human-
targeted Translation Edit Rate (HTER). We
then show that METEOR-NEXT improves cor-
relation with HTER over baseline metrics, in-
cluding earlier versions of METEOR, and ap-
proaches the correlation level of a state-of-the-
art metric, TER-plus (TERp).
1 Introduction
Recent focus on the need for accurate automatic
metrics for evaluating the quality of machine trans-
lation output has spurred much development in the
field of MT. Workshops such as WMT09 (Callison-
Burch et al, 2009) and the MetricsMATR08 chal-
lenge (Przybocki et al, 2008) encourage the devel-
opment of new MT metrics and reliable human judg-
ment tasks.
This paper describes our work extending the ME-
TEOR metric to improve correlation with human-
targeted Translation Edit Rate (HTER) (Snover et
al., 2006), a semi-automatic post-editing based met-
ric which measures the distance between MT out-
put and a targeted reference. We identify several
limitations of the original METEOR metric and de-
scribe our modifications to improve performance on
this task. Our extended metric, METEOR-NEXT, is
then tuned to maximize segment-level correlation
with HTER scores and tested against several base-
line metrics. We show that METEOR-NEXT outper-
forms earlier versions of METEOR when tuned to the
same HTER data and approaches the performance of
a state-of-the-art TER-based metric, TER-plus.
2 The METEOR-NEXT Metric
2.1 Traditional METEOR Scoring
Given a machine translation hypothesis and a refer-
ence translation, the traditional METEOR metric cal-
culates a lexical similarity score based on a word-
to-word alignment between the two strings (Baner-
jee and Lavie, 2005). When multiple references are
available, the hypothesis is scored against each and
the reference producing the highest score is used.
Alignments are built incrementally in a series of
stages using the following METEOR matchers:
Exact: Words are matched if and only if their sur-
face forms are identical.
Stem: Words are stemmed using a language-
appropriate Snowball Stemmer (Porter, 2001) and
matched if the stems are identical.
Synonym: Words are matched if they are both
members of a synonym set according to the Word-
Net (Miller and Fellbaum, 2007) database. This
matcher is limited to translations into English.
At each stage, one of the above matchers iden-
tifies all possible word matches between the two
translations using words not aligned in previous
stages. An alignment is then identified as the largest
subset of these matches in which every word in each
sentence aligns to zero or one words in the other sen-
250
tence. If multiple such alignments exist, the align-
ment is chosen that best preserves word order by
having the fewest crossing alignment links. At the
end of each stage, matched words are fixed so that
they are not considered in future stages. The final
alignment is defined as the union of all stage align-
ments.
Once an alignment has been constructed, the to-
tal number of unigram matches (m), the number of
words in the hypothesis (t), and the number of words
in the reference (r) are used to calculate precision
(P = m/t) and recall (R = m/r). The parame-
terized harmonic mean of P and R (van Rijsbergen,
1979) is then calculated:
Fmean =
P ?R
? ? P + (1 ? ?) ?R
To account for differences in word order, the min-
imum number of ?chunks? (ch) is calculated where a
chunk is defined as a series of matched unigrams that
is contiguous and identically ordered in both sen-
tences. The fragmentation (frag = ch/m) is then
used to calculate a fragmentation penalty:
Pen = ? ? frag?
The final METEOR score is then calculated:
Score = (1 ? Pen) ? Fmean
The free parameters ?, ?, and ? can be tuned to
maximize correlation with various types of human
judgments (Lavie and Agarwal, 2007).
2.2 Extending the METEOR Aligner
Traditional METEOR is limited to unigram matches,
making it strictly a word-level metric. By focus-
ing on only one match type per stage, the aligner
misses a significant part of the possible alignment
space. Further, selecting partial alignments based
only on the fewest number of per-stage crossing
alignment links can in practice lead to missing full
alignments with the same number of matches in
fewer chunks. Our extended aligner addresses these
limitations by introducing support for multiple-word
phrase matches and considering all possible matches
in a single alignment stage.
We introduce an additional paraphrase matcher
which matches phrases (one or more successive
words) if one phrase is considered a paraphrase of
the other by a paraphrase database. For English, we
use the paraphrase database developed by Snover et
al. (2009), using techniques presented by Bannard
and Callison-Burch (2005).
The extended aligner first constructs a search
space by applying all matchers in sequence to iden-
tify all possible matches between the hypothesis and
reference. To reduce redundant matches, stem and
synonym matches between pairs of words which
have already been identified as exact matches are not
considered. Matches have start positions and lengths
in both sentences; a word occurring less than length
positions after a match start is said to be covered by
the match. As exact, stem, and synonym matches
will always have length one in both sentences, they
can be considered phrase matches of length one.
Since other matches can cover phrases of different
lengths in the two sentences, matches are now said
to be one-to-one at the phrase level rather than the
word level.
Once all possible matches have been identified,
the aligner identifies the final alignment as the
largest subset of these matches meeting the follow-
ing criteria in order of importance:
1. Each word in each sentence is covered by zero
or one matches
2. Largest number of covered words across both
sentences
3. Smallest number of chunks, where a chunk is
now defined as a series of matched phrases that
is contiguous and identically ordered in both
sentences
4. Smallest sum of absolute distances between
match start positions in the two sentences (pre-
fer to align words and phrases that occur at sim-
ilar positions in both sentences)
The resulting alignment is selected from the full
space of possible alignments and directly optimizes
the statistics on which the the final score will be cal-
culated.
2.3 Extended METEOR Scoring
Once an alignment has been chosen, the METEOR-
NEXT score is calculated using extended versions of
251
the traditional METEOR statistics. We also introduce
a tunable weight vector used to dictate the relative
contribution of each match type. The extended ME-
TEOR score is calculated as follows.
The number of words in the hypothesis (t) and
reference (r) are counted. For each of the match-
ers (mi), count the number of words covered by
matches of this type in the hypothesis (mi(t)) and
reference (mi(r)) and apply the appropriate module
weight (wi). The weighted Precision and Recall are
then calculated:
P =
?
iwi ?mi(t)
t
R =
?
iwi ?mi(r)
r
The minimum number of chunks (ch) is then cal-
culated using the new chunk definition. Once P , R,
and ch are calculated, the remaining statistics and
final score can be calculated as in Section 2.1.
3 Tuning for Post-Editing Measures of
Quality
Human-targeted Translation Edit Rate (HTER)
(Snover et al, 2006), is a semi-automatic assessment
of machine translation quality based on the number
of edits required to correct translation hypotheses. A
human annotator edits each MT hypothesis so that it
is meaning-equivalent with a reference translation,
with an emphasis on making the minimum possible
number of edits. The Translation Edit Rate (TER)
is then calculated using the human-edited transla-
tion as a targeted reference for the MT hypothe-
sis. The resulting scores are shown to correlate well
with other types of human judgments (Snover et al,
2006).
3.1 Tuning Toward HTER
The GALE (Olive, 2005) Phase 2 unsequestered
data includes HTER scores for multiple Arabic-to-
English and Chinese-to-English MT systems. We
used HTER scores for 10838 segments from 1045
documents from this data set to tune both the orig-
inal METEOR and METEOR-NEXT. Both were ex-
haustively tuned to maximize the length-weighted
segment-level Pearson?s correlation with the HTER
scores. This produced globally optimal ?, ?, and ?
values for METEOR and optimal ?, ?, ? values plus
stem, synonym, and paraphrase match weights for
Task ? ? ?
Adequacy & Fluency 0.81 0.83 0.28
Ranking 0.95 0.50 0.50
HTER 0.70 1.95 0.50
HTER (extended) 0.65 1.95 0.45
Stem Syn Par
0 0.4 0.9
Table 1: Parameter values for various METEOR tasks for
translations into English.
METEOR-NEXT (with the weight of exact matches
fixed at 1). Table 1 compares the new HTER pa-
rameters to those tuned for other tasks including ad-
equacy and fluency (Lavie and Agarwal, 2007) and
ranking (Agarwal and Lavie, 2008).
As observed by Snover et al (2009), HTER
prefers metrics which are more balanced between
precision and recall: this results in the lowest values
of ? for any task. Additionally, non-exact matches
receive lower weights, with stem matches receiving
zero weight. This reflects a weakness in HTER scor-
ing where words with matching stems are treated as
completely dissimilar, requiring full word substitu-
tions (Snover et al, 2006).
4 Experiments
The GALE (Olive, 2005) Phase 3 unsequestered
data includes HTER scores for Arabic-to-English
MT output. We created a test set from HTER scores
of 2245 segments from 195 documents in this data
set. Our evaluation metric (METEOR-NEXT-hter)
was tested against the following established metrics:
BLEU (Papineni et al, 2002) with a maximum N -
gram length of 4, TER (Snover et al, 2006), versions
of METEOR based on release 0.7 tuned for adequacy
and fluency (METEOR-0.7-af) (Lavie and Agarwal,
2007), ranking (METEOR-0.7-rank) (Agarwal and
Lavie, 2008), and HTER (METEOR-0.7-hter). Also
included is the HTER-tuned version of TER-plus
(TERp-hter), a metric with state-of-the-art perfor-
mance in recent evaluations (Snover et al, 2009).
Length-weighted Pearson?s and Spearman?s correla-
tion are shown for all metrics at both the segment
(Table 2) and document level (Table 3). System level
correlations are not shown as the Phase 3 data only
contained the output of 2 systems.
252
Metric Pearson?s r Spearman?s ?
BLEU-4 -0.496 -0.510
TER 0.539 0.510
METEOR-0.7-af -0.573 -0.561
METEOR-0.7-rank -0.561 -0.556
METEOR-0.7-hter -0.574 -0.562
METEOR-NEXT-hter -0.600 -0.581
TERp-hter 0.627 0.610
Table 2: Segment level correlation with HTER.
Metric Pearson?s r Spearman?s ?
BLEU-4 -0.689 -0.686
TER 0.675 0.679
METEOR-0.7-af -0.696 -0.699
METEOR-0.7-rank -0.691 -0.693
METEOR-0.7-hter -0.704 -0.705
METEOR-NEXT-hter -0.719 -0.713
TERp-hter 0.738 0.747
Table 3: Document level correlation with HTER.
METEOR-NEXT-hter outperforms all baseline
metrics at both the segment and document level.
Bootstrap sampling indicates that the segment-level
correlation improvements of 0.026 in Pearson?s r
and 0.019 in Spearman?s ? over METEOR-0.7-hter
are statistically significant at the 95% level. TERp?s
correlation with HTER is still significantly higher
across all categories. Our metric does run signifi-
cantly faster than TERp, scoring approximately 120
segments per second to TERp?s 3.8.
5 Conclusions
We have presented an extended METEOR metric
which shows higher correlation with HTER than
baseline metrics, including traditional METEOR
tuned on the same data. Our extensions are not
specific to HTER tasks; improved alignments and
additional features should improve performance on
any task having sufficient tuning data. Although our
metric does not outperform TERp, it should be noted
that HTER incorporates TER alignments, providing
TER-based metrics a natural advantage. Our metric
also scores segments relatively quickly, making it a
viable choice for tuning MT systems.
Acknowledgements
This work was funded in part by NSF grants IIS-
0534932 and IIS-0915327.
References
Abhaya Agarwal and Alon Lavie. 2008. Meteor, m-bleu
and m-ter: Evaluation Metrics for High-Correlation
with Human Rankings of Machine Translation Output.
In Proc. of WMT08, pages 115?118.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Proc.
of the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization, pages 65?72.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proc. of
ACL05, pages 597?604.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In Proc.
of WMT09, pages 1?28.
Alon Lavie and Abhaya Agarwal. 2007. METEOR: An
Automatic Metric for MT Evaluation with High Lev-
els of Correlation with Human Judgments. In Proc. of
WMT07, pages 228?231.
George Miller and Christiane Fellbaum. 2007. WordNet.
http://wordnet.princeton.edu/.
Joseph Olive. 2005. Global Autonomous Language Ex-
ploitation (GALE). DARPA/IPTO Proposer Informa-
tion Pamphlet.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proc. of ACL02,
pages 311?318.
Martin Porter. 2001. Snowball: A language for stem-
ming algorithms. http://snowball.tartarus.org/texts/.
M. Przybocki, K. Peterson, and S Bronsart. 2008.
Official results of the NIST 2008 "Metrics for
MAchine TRanslation" Challenge (MetricsMATR08).
http://nist.gov/speech/tests/metricsmatr/2008/results/.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proc. of AMTA-2006, pages 223?231.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, Adequacy, or
HTER? Exploring Different Human Judgments with a
Tunable MT Metric. In Proc. of WMT09, pages 259?
268.
C. van Rijsbergen, 1979. Information Retrieval, chap-
ter 7. 2nd edition.
253
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 57?61,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Exploring Normalization Techniques for Human Judgments of Machine
Translation Adequacy Collected Using Amazon Mechanical Turk
Michael Denkowski and Alon Lavie
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15232, USA
{mdenkows,alavie}@cs.cmu.edu
Abstract
This paper discusses a machine translation
evaluation task conducted using Amazon Me-
chanical Turk. We present a translation ade-
quacy assessment task for untrained Arabic-
speaking annotators and discuss several tech-
niques for normalizing the resulting data. We
present a novel 2-stage normalization tech-
nique shown to have the best performance on
this task and further discuss the results of all
techniques and the usability of the resulting
adequacy scores.
1 Introduction
Human judgments of translation quality play a vital
role in the development of effective machine trans-
lation (MT) systems. Such judgments can be used
to measure system quality in evaluations (Callison-
Burch et al, 2009) and to tune automatic metrics
such as METEOR (Banerjee and Lavie, 2005) which
act as stand-ins for human evaluators. However, col-
lecting reliable human judgments often requires sig-
nificant time commitments from expert annotators,
leading to a general scarcity of judgments and a sig-
nificant time lag when seeking judgments for new
tasks or languages.
Amazon?s Mechanical Turk (MTurk) service fa-
cilitates inexpensive collection of large amounts of
data from users around the world. However, Turk-
ers are not trained to provide reliable annotations for
natural language processing (NLP) tasks, and some
Turkers attempt to game the system by submitting
random answers. For these reasons, NLP tasks must
be designed to be accessible to untrained users and
data normalization techniques must be employed to
ensure that the data collected is usable.
This paper describes a MT evaluation task for
translations of English into Arabic conducted us-
ing MTurk and compares several data normaliza-
tion techniques. A novel 2-stage normalization tech-
nique is demonstrated to produce the highest agree-
ment between Turkers and experts while retaining
enough judgments to provide a robust tuning set for
automatic evaluation metrics.
2 Data Set
Our data set consists of human adequacy judgments
for automatic translations of 1314 English sentences
into Arabic. The English source sentences and Ara-
bic reference translations are taken from the Arabic-
English sections of the NIST Open Machine Trans-
lation Evaluation (Garofolo, 2001) data sets for 2002
through 2005. Selected sentences are between 10
and 20 words in length on the Arabic side. Arabic
machine translation (MT) hypotheses are obtained
by passing the English sentences through Google?s
free online translation service.
2.1 Data Collection
Human judgments of translation adequacy are col-
lected for each of the 1314 Arabic MT output hy-
potheses. Given a translation hypothesis and the
corresponding reference translation, annotators are
asked to assign an adequacy score according to the
following scale:
4 ? Hypothesis is completely meaning equivalent
with the reference translation.
57
3 ? Hypothesis captures more than half of meaning
of the reference translation.
2 ? Hypothesis captures less than half of meaning
of the reference translation.
1 ? Hypothesis captures no meaning of the refer-
ence translation.
Adequacy judgments are collected from untrained
Arabic-speaking annotators using Amazon?s Me-
chanical Turk (MTurk) service. We create a human
intelligence task (HIT) type that presents Turkers
with a MT hypothesis/reference pair and asks for
an adequacy judgment. To make this task accessi-
ble to non-experts, the traditional definitions of ad-
equacy scores are replaced with the following: (4)
excellent, (3) good, (2) bad, (1) very bad. Each rat-
ing is accompanied by an example from the data set
which fits the corresponding criteria from the tradi-
tional scale. To make this task accessible to the Ara-
bic speakers we would like to complete the HITs,
the instructions are provided in Arabic as well as En-
glish.
To allow experimentation with various data nor-
malization techniques, we collect judgments from
10 unique Turkers for each of the translations. We
also ask an expert to provide ?gold standard? judg-
ments for 101 translations drawn uniformly from the
data. These 101 translations are recombined with the
data and repeated such that every 6th translation has
a gold standard judgment, resulting in a total of 1455
HITs. We pay Turkers $0.01 per HIT and Ama-
zon fees of $0.005 per HIT, leading to a total cost
of $218.25 for data collection and an effective cost
of $0.015 per judgment. Despite requiring Arabic
speakers, our HITs are completed at a rate of 1000-
3000 per day. It should be noted that the vast ma-
jority of Turkers working on our HITs are located in
India, with fewer in Arabic-speaking countries such
as Egypt and Syria.
3 Normalization Techniques
We apply multiple normalization techniques to the
data set and evaluate their relative performance.
Several techniques use the following measures:
? ?: For judgments (J = j1...jn) and gold stan-
dard (G = g1...gn), we define average distance:
?(J,G) =
?n
i=1 |gi ? ji|
n
? K: For two annotators, Cohen?s kappa coeffi-
cient (Smeeton, 1985) is defined:
K =
P (A)? P (E)
1? P (E)
where P (A) is the proportion of times that an-
notators agree and P (E) is the proportion of
times that agreement is expected by chance.
3.1 Straight Average
The baseline approach consists of keeping all judg-
ments and taking the straight average on a per-
translation basis without additional normalization.
3.2 Removing Low-Agreement Judges
Following Callison-Burch et al (2009), we calcu-
late pairwise inter-annotator agreement (P (A)) of
each annotator with all others and remove judgments
from annotators with P (A) below some threshold.
We set this threshold such that the highest overall
agreement can be achieved while retaining at least
one judgment for each translation.
3.3 Removing Outlying Judgments
For a given translation and human judgments
(j1...jn), we calculate the distance (?) of each judg-
ment from the mean (j?):
?(ji) = |ji ? j?|
We then remove outlying judgments with ?(ji) ex-
ceeding some threshold. This threshold is also set
such that the highest agreement is achieved while
retaining at least one judgment per translation.
3.4 Weighted Voting
Following Callison-Burch (2009), we treat evalua-
tion as a weighted voting problem where each anno-
tator?s contribution is weighted by agreement with
either a gold standard or with other annotators. For
this evaluation, we weigh contribution by P (A) with
the 101 gold standard judgments.
58
3.5 Scaling Judgments
To account for the notion that some annotators judge
translations more harshly than others, we apply per-
annotator scaling to the adequacy judgments based
on annotators? signed distance from gold standard
judgments. For judgments (J = j1...jn) and gold
standard (G = g1...gn), an additive scaling factor is
calculated:
?+(J,G) =
?n
i=1 gi ? ji
n
Adding this scaling factor to each judgment has the
effect of shifting the judgments? center of mass to
match that of the gold standard.
3.6 2-Stage Technique
We combine judgment scaling with weighted vot-
ing to produce a 2-stage normalization technique
addressing two types of divergence in Turker judg-
ments from the gold standard. Divergence can be
either consistent, where Turkers regularly assign
higher or lower scores than experts, or random,
where Turkers guess blindly or do not understand
the task.
Stage 1: Given a gold standard (G = g1...gn),
consistent divergences are corrected by calculat-
ing ?+(J,G) for each annotator?s judgments (J =
ji...jn) and applying ?+(J,G) to each ji to produce
adjusted judgment set J ?. If ?(J ?, G) < ?(J,G),
where ?(J,G) is defined in Section 3, the annotator
is considered consistently divergent and J ? is used
in place of J . Inconsistently divergent annotators?
judgments are unaffected by this stage.
Stage 2: All annotators are considered in a
weighted voting scenario. In this case, annotator
contribution is determined by a distance measure
similar to the kappa coefficient. For judgments (J =
j1...jn) and gold standard (G = g1...gn), we define:
K?(J,G) =
(max ???(J,G))? E(?)
max ?? E(?)
where max ? is the average maximum distance be-
tween judgments and E(?) is the expected distance
between judgments. Perfect agreement with the gold
standard produces K? = 1 while chance agreement
produces K? = 0. Annotators with K? ? 0 are re-
moved from the voting pool and final scores are cal-
culated as the weighted averages of judgments from
all remaining annotators.
Type ? K?
Uniform-a 1.02 0.184
Uniform-b 1.317 -0.053
Gaussian-2 1.069 0.145
Gaussian-2.5 0.96 0.232
Gaussian-3 1.228 0.018
Table 2: Weights assigned to random data
4 Results
Table 1 outlines the performance of all normaliza-
tion techniques. To calculate P (A) and K with the
gold standard, final adequacy scores are rounded to
the nearest whole number. As shown in the table, re-
moving low-agreement annotators or outlying judg-
ments greatly improves Turker agreement and, in
the case of removing judgments, decreases distance
from the gold standard. However, these approaches
remove a large portion of the judgments, leaving a
skewed data set. When removing judgments, 1172
of the 1314 translations receive a score of 3, making
tasks such as tuning automatic metrics infeasible.
Weighing votes by agreement with the gold stan-
dard retains most judgments, though neither Turker
agreement nor agreement with the gold standard im-
proves. The scaling approach retains all judgments
and slightly improves correlation and ?, though K
decreases. As scaled judgments are not whole num-
bers, Turker P (A) and K are not applicable.
The 2-stage approach outperforms all other tech-
niques when compared against the gold standard,
being the only technique to significantly raise cor-
relation. Over 90% of the judgments are used, as
shown in Figure 1. Further, the distribution of fi-
nal adequacy scores (shown in Figure 2) resembles
a normal distribution, allowing this data to be used
for tuning automatic evaluation metrics.
4.1 Resistance to Randomness
To verify that our 2-stage technique handles prob-
lematic data properly, we simulate user data from
5 unreliable Turkers. Turkers ?Uniform-a? and
?Uniform-b? draw answers randomly from a uni-
form distribution. ?Gaussian? Turkers draw answers
randomly from Gaussian distributions with ? = 1
and ? according to name. Each ?Turker? contributes
one judgment for each translation. As shown in Ta-
59
Gold Standard Turker
Technique Retained Correlation ? P (A) K P (A) K
Straight Average 14550 0.078 0.988 0.356 0.142 0.484 0.312
Remove Judges 6627 -0.152 1.002 0.347 0.129 0.664 0.552
Remove Judgments 9250 0 0.891 0.356 0.142 0.944 0.925
Weighted Voting 14021 0.152 0.968 0.356 0.142 0.484 0.312
Scale Judgments 14550 0.24 0.89 0.317 0.089 N/A N/A
2-Stage Technique 13621 0.487 0.836 0.366 0.155 N/A N/A
Table 1: Performance of normalization techniques
0 0.25 0.5 0.75 1
0
500
1000
1500
2000
2500
3000
3500
4000
4500
5000
Vote Weight
Nu
mb
er 
of J
ud
gm
en
ts
Figure 1: Distribution of weights for judgments
ble 2, only Gaussian-2.5 receives substantial weight
while the others receive low or zero weight. This fol-
lows from the fact that the actual data follows a sim-
ilar distribution, and thus the random Turkers have
negligible impact on the final distribution of scores.
5 Conclusions and Future Work
We have presented an Arabic MT evaluation task
conducted using Amazon MTurk and discussed
several possibilities for normalizing the collected
data. Our 2-stage normalization technique has been
shown to provide the highest agreement between
Turkers and experts while retaining enough judg-
ments to avoid problems of data sparsity and appro-
priately down-weighting random data. As we cur-
rently have a single set of expert judgments, our fu-
ture work involves collecting additional judgments
from multiple experts against which to further test
our techniques. We then plan to use normalized
0 . 2 5
7
17
077
017
.77
.17
34Vote Wig hNV
utm
bV
Nih
rigV
fm
VJ
dn
Figure 2: Distribution of adequacy scores after 2-stage
normalization
Turker adequacy judgments to tune an Arabic ver-
sion of the METEOR (Banerjee and Lavie, 2005) MT
evaluation metric.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Proc.
ACL WIEEMMTS.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of WMT09. In
Proc. WMT09.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In Proc. EMNLP09.
John Garofolo. 2001. NIST Open Machine Translation
Evaluation. http://www.itl.nist.gov/iad/mig/tests/mt/.
N. C. Smeeton. 1985. Early History of the Kappa Statis-
tic. In Biometrics, volume 41.
60
Fi
gu
re
3:
E
xa
m
pl
e
H
IT
as
se
en
by
T
ur
ke
rs
61
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 66?70,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Turker-Assisted Paraphrasing for English-Arabic Machine Translation
Michael Denkowski and Hassan Al-Haj and Alon Lavie
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15232, USA
{mdenkows,hhaj,alavie}@cs.cmu.edu
Abstract
This paper describes a semi-automatic para-
phrasing task for English-Arabic machine
translation conducted using Amazon Me-
chanical Turk. The method for automatically
extracting paraphrases is described, as are
several human judgment tasks completed by
Turkers. An ideal task type, revised specif-
ically to address feedback from Turkers, is
shown to be sophisticated enough to identify
and filter problem Turkers while remaining
simple enough for non-experts to complete.
The results of this task are discussed along
with the viability of using this data to combat
data sparsity in MT.
1 Introduction
Many language pairs have large amounts of paral-
lel text that can be used to build statistical machine
translation (MT) systems. For such language pairs,
resources for system tuning and evaluation tend to
be disproportionately abundant in the language typ-
ically used as the target. For example, the NIST
Open Machine Translation Evaluation (OpenMT)
2009 (Garofolo, 2009) constrained Arabic-English
development and evaluation data includes four En-
glish translations for each Arabic source sentence,
as English is the usual target language. However,
when considering this data to tune and evaluate
an English-to-Arabic system, each English sentence
has a single Arabic translation and such translations
are often identical. With at most one reference trans-
lation for each source sentence, standard minimum
error rate training (Och, 2003) to the BLEU met-
ric (Papineni et al, 2002) becomes problematic, as
BLEU relies on the availability of multiple refer-
ences.
We describe a semi-automatic paraphrasing
technique that addresses this problem by identifying
paraphrases that can be used to create new reference
translations based on valid phrase substitutions on
existing references. Paraphrases are automatically
extracted from a large parallel corpus and filtered by
quality judgments collected from human annotators
using Amazon Mechanical Turk. As Turkers are
not trained to complete natural language processing
(NLP) tasks and can dishonestly submit random
judgments, we develop a task type that is able to
catch problem Turkers while remaining simple
enough for untrained annotators to understand.
2 Data Set
The parallel corpus used for paraphrasing con-
sists of all Arabic-English sentence pairs in the
NIST OpenMT Evaluation 2009 (Garofolo, 2009)
constrained training data. The target corpus to be
paraphrased consists of the 728 Arabic sentences
from the OpenMT 2002 (Garofolo, 2002) develop-
ment data.
2.1 Paraphrase Extraction
We conduct word alignment and phrase extraction
on the parallel data to produce a phrase table con-
taining Arabic-English phrase pairs (a, e) with trans-
lation probabilities P (a|e) and P (e|a). Follow-
66
ing Bannard and Callison-Burch (2005), we iden-
tify Arabic phrases (a1) in the target corpus that are
translated by at least one English phrase (e). We
identify paraphrase candidates as alternate Arabic
phrases (a2) that translate e. The probability of a2
being a paraphrase of a1 given foreign phrases e is
defined:
P (a2|a1) =
?
e
P (e|a1)P (a2|e)
A language model trained on the Arabic side of the
parallel corpus is used to further score the possi-
ble paraphrases. As each original phrase (a1) oc-
curs in some sentence (s1) in the target corpus, a
paraphrased sentence (s2) can be created by replac-
ing a1 with one of its paraphrases (a2). The final
paraphrase score considers context, scaling the para-
phrase probability proportionally to the change in
log-probability of the sentence:
F (a2, s2|a1, s1) = P (a2|a1)
logP (s1)
logP (s2)
These scores can be combined for each pair (a1, a2)
to obtain overall paraphrase scores, however we
use the F scores directly as our task considers the
sentences in which paraphrases occur.
3 Turker Paraphrase Assessment
To determine which paraphrases to use to trans-
form the development set references, we elicit bi-
nary judgments of quality from human annotators.
While collecting this data from experts would be ex-
pensive and time consuming, Amazon?s Mechani-
cal Turk (MTurk) service facilitates the rapid collec-
tion of large amounts of inexpensive data from users
around the world. As these users are not trained
to work on natural language processing tasks, any
work posted on MTurk must be designed such that
it can be understood and completed successfully by
untrained annotators. Further, some Turkers attempt
to dishonestly profit from entering random answers,
creating a need for tasks to have built-in measures
for identifying and filtering out problem Turkers.
Our original evaluation task consists of eliciting
two yes/no judgments for each paraphrase and cor-
responding sentence. Shown the original phrase
(a1) and the paraphrase (a2), annotators are asked
whether or not these two phrases could have the
same meaning in some possible context. Annotators
are then shown the original sentence (s1) and the
paraphrased sentence (s2) and asked whether these
two sentences have the same meaning. This task has
the attractive property that if s1 and s2 have the same
meaning, a1 and a2 can have the same meaning. An-
notators assigning ?yes? to the sentence pair should
always assign ?yes? to the phrase pair.
To collect these judgments from MTurk, we de-
sign a human intelligence task (HIT) that presents
Turkers with two instances of the above task along
with a text area for optional feedback. The task
description asks skilled Arabic speakers to evalu-
ate paraphrases of Arabic text. For each HIT, we
pay Turkers $0.01 and Amazon fees of $0.005 for
a total label cost of $0.015. For our initial test,
we ask Turkers to evaluate the 400 highest-scoring
paraphrases, collecting 3 unique judgments for each
paraphrase in and out of context. These HITs were
completed at a rate of 200 per day.
Examining the results, we notice that most
Turkers assign ?yes? to the sentence pairs more
often than to the phrase pairs, which should not be
possible. To determine whether quality of Turkers
might be an issue, we run another test for the same
400 paraphrases, this time paying Turkers $0.02 per
HIT and requiring a worker approval rate of 98% to
work on this task. These HITs, completed by high
quality Turkers at a rate of 100 per day, resulted
in similarly impossible data. However, we also
received valuable feedback from one of the Turkers.
3.1 Turker Feedback
We received a comment from one Turker that
our evaluation task was causing confusion. The
Turker would select ?no? for some paraphrase in
isolation due to missing information. However, the
Turker would then select ?yes? for the paraphrased
sentence, as the context surrounding the phrase
rendered the missing information unnecessary.
This illustrates the point that untrained annotators
understand the idea of ?possible context? differently
from experts and allows us to restructure our HITs
to be ideal for untrained Turkers.
67
3.2 Revised Main Task
We simplify our task to eliminate as many sources
of ambiguity as possible. Our revised task simply
presents annotators with the original sentence la-
beled ?sentence 1? and the paraphrased sentence la-
beled ?sentence 2?, and asks whether or not the two
sentences have the same meaning. Each HIT, titled
?Evaluate Arabic Sentences?, presents Turkers with
2 such tasks, pays $0.02, and costs $0.005 in Ama-
zon fees.
Without additional consideration, this task re-
mains highly susceptible to random answers from
dishonest or unreliable Turkers. To ensure that such
Turkers are identified and removed, we intersperse
absolute positive and negative examples with the
sentence pairs from our data set. Absolute posi-
tives consist of the same original sentence s1 re-
peated twice and should always receive a ?yes? judg-
ment. Absolute negatives consist of some origi-
nal s1 and a different, randomly selected original
sentence s?1 with several words dropped to obscure
meaning. Absolute negatives should always receive
a ?no? judgment. Positive and negative control cases
can be inserted with a frequency based either on de-
sired confidence that enough cases are encountered
for normalization or on the availability of funds.
Inserting either a positive or negative control
case every 5th task increases the per-label cost to
$0.0156. We use this task type to collect 3 unique
judgments for each of the 1280 highest-scoring
paraphrases at a total cost of $60.00 for 2400 HITs.
These HITs were completed substantially faster at a
rate of 500-1000 per day. The results of this task are
discussed in section 4.
3.3 Editing Task
We conduct an additional experiment to see if Turk-
ers will fix paraphrases judged to be incorrect. The
task extends the sentence evaluation task described
in the previous section by asking Turkers who select
?no? to edit the paraphrase text in the second sen-
tence such that the sentences have the same mean-
ing. While the binary judgment task is used for fil-
tering only, this editing task ensures a usable data
point for every HIT completed. As such, fewer total
HITs are required and high quality Turkers can be
0 0.25 0.5 0.75 1
0
2
4
6
8
10
12
14
16
18
20
Accuracy of judgments of control cases
Nu
mb
er 
of T
urk
ers
Figure 1: Turker accuracy classifying control cases
paid more for each HIT. We run 3 sequential tests
for this task, offering $0.02, $0.04, and $0.10 per
paraphrase approved or edited.
Examining the results, we found that regardless
of price, very few paraphrases were actually edited,
even when Turkers selected ?no? for sentence
equality. While this allows us to easily identify and
remove problem Turkers, it does not solve the issue
that honest Turkers either cannot or will not provide
usable paraphrase edits for this price range. A brief
examination by an expert indicates that the $0.02
per HIT edits are actually better than the $0.10 per
HIT edits.
4 Results
Our main task of 2400 HITs was completed through
the combined effort of 47 unique Turkers. As shown
Figure 1, these Turkers have varying degrees of ac-
curacy classifying the control cases. The two most
common classes of Turkers include (1) those spend-
ing 15 or more seconds per judgment and scoring
above 0.9 accuracy on the control cases and (2) those
spending 5-10 seconds per judgment and scoring be-
tween 0.4 and 0.6 accuracy as would be expected by
chance. As such, we accept but do not consider the
judgments of Turkers scoring between 0.7 and 0.9
accuracy on the control set, and reject all HITs for
Turkers scoring below 0.7, republishing them to be
completed by other workers.
68
Decision Confirm Reject Undec.
Paraphrases 726 423 131
Table 1: Turker judgments of top 1280 paraphrases
Figure 2: Paraphrases confirmed by Turkers
After removing judgments from below-threshold
annotators, all remaining judgments are used to
confirm or reject the covered paraphrases. If a
paraphrase has at least 2 remaining judgments, it is
confirmed if at least 2 annotators judge it positively
and rejected otherwise. Paraphrases with fewer than
2 remaining judgments are considered undecidable.
Table 1 shows the distribution of results for the 1280
top-scoring paraphrases. As shown in the table,
726 paraphrases are confirmed as legitimate phrase
substitutions on reference translations, providing
an average of almost one paraphrase per reference.
Figures 2 and 3 show example Arabic paraphrases
filtered by Turkers.
5 Conclusions
We have presented a semi-automatic paraphrasing
technique for creating additional reference transla-
tions. The paraphrase extraction technique provides
a ranked list of paraphrases and their contexts which
can be incrementally filtered by human judgments.
Our judgment task is designed to address specific
Turker feedback, remaining simple enough for
non-experts while successfully catching problem
users. The $60.00 worth of judgments collected
produces enough paraphrases to apply an average
Figure 3: Paraphrases rejected by Turkers
of one phrase substitution to each reference. Our
future work includes collecting sufficient data to
substitute multiple paraphrases into each Arabic
reference in our development set, producing a full
additional set of reference translations for use tuning
our English-to-Arabic MT system. The resulting
individual paraphrases can also be used for other
tasks in MT and NLP.
Acknowledgements
This work was supported by a $100 credit from
Amazon.com, Inc. as part of a shared task for the
NAACL 2010 workshop ?Creating Speech and Lan-
guage Data With Amazon?s Mechanical Turk?.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Proc. of
ACL.
John Garofolo. 2002. NIST OpenMT Eval. 2002.
http://www.itl.nist.gov/iad/mig/tests/mt/2002/.
John Garofolo. 2009. NIST OpenMT Eval. 2009.
http://www.itl.nist.gov/iad/mig/tests/mt/2009/.
Franz Josef Och. 2003. Minimum Error Rate Training
for Statistical Machine Translation. In Proc. of ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proc. of ACL.
69
Fi
gu
re
4:
E
xa
m
pl
e
H
IT
as
se
en
by
T
ur
ke
rs
70
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 339?342,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
METEOR-NEXT and the METEOR Paraphrase Tables: Improved
Evaluation Support for Five Target Languages
Michael Denkowski and Alon Lavie
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15232, USA
{mdenkows,alavie}@cs.cmu.edu
Abstract
This paper describes our submission to
the WMT10 Shared Evaluation Task and
MetricsMATR10. We present a version
of the METEOR-NEXT metric with para-
phrase tables for five target languages. We
describe the creation of these paraphrase
tables and conduct a tuning experiment
that demonstrates consistent improvement
across all languages over baseline ver-
sions of the metric without paraphrase re-
sources.
1 Introduction
Workshops such as WMT (Callison-Burch et al,
2009) and MetricsMATR (Przybocki et al, 2008)
focus on the need for accurate automatic met-
rics for evaluating the quality of machine transla-
tion (MT) output. While these workshops evalu-
ate metric performance on many target languages,
most metrics are limited to English due to the rel-
ative lack of lexical resources for other languages.
This paper describes a language-independent
method for adding paraphrase support to the
METEOR-NEXT metric for all WMT10 target lan-
guages. Taking advantage of the large parallel cor-
pora released for the translation tasks often accom-
panying evaluation tasks, we automatically con-
struct paraphrase tables using the pivot method
(Bannard and Callison-Burch, 2005). We use the
WMT09 human evaluation data to tune versions
of METEOR-NEXT with and without paraphrases
and report significantly better performance for ver-
sions with paraphrase support.
2 The METEOR-NEXT Metric
The METEOR-NEXT metric (Denkowski and
Lavie, 2010) evaluates a machine translation hy-
pothesis against a reference translation by calcu-
lating a similarity score based on an alignment be-
tween the two strings. When multiple references
are provided, the hypothesis is scored against each
and the reference producing the highest score is
used. Alignments are formed in two stages: search
space construction and alignment selection.
For a single hypothesis-reference pair, the space
of possible alignments is constructed by identify-
ing all possible word and phrase matches between
the strings according to the following matchers:
Exact: Words are matched if and only if their sur-
face forms are identical.
Stem: Words are stemmed using a language-
appropriate Snowball Stemmer (Porter, 2001) and
matched if the stems are identical.
Synonym: Words are matched if they are both
members of a synonym set according to the Word-
Net (Miller and Fellbaum, 2007) database.
Paraphrase: Phrases are matched if they are
listed as paraphrases in a paraphrase table. The
tables used are described in Section 3.
Previously, full support has been limited to En-
glish, with French, German, and Spanish having
exact and stem match support only, and Czech
having exact match support only.
Although the exact, stem, and synonym match-
ers identify word matches while the paraphrase
matcher identifies phrase matches, all matches can
be generalized to phrase matches with a start po-
sition and phrase length in each string. A word
occurring less than length positions after a match
start is considered covered by the match. Ex-
act, stem, and synonym matches always cover one
word in each string.
Once the search space is constructed, the final
alignment is identified as the largest possible sub-
set of all matches meeting the following criteria in
order of importance:
1. Each word in each sentence is covered by
zero or one matches
2. Largest number of covered words across both
339
sentences
3. Smallest number of chunks, where a chunk
is defined as a series of matched phrases that
is contiguous and identically ordered in both
sentences
4. Smallest sum of absolute distances between
match start positions in the two sentences
(prefer to align words and phrases that occur
at similar positions in both sentences)
Once an alignment is selected, the METEOR-
NEXT score is calculated as follows. The num-
ber of words in the translation hypothesis (t) and
reference (r) are counted. For each of the match-
ers (mi), count the number of words covered by
matches of this type in the hypothesis (mi(t)) and
reference (mi(r)) and apply matcher weight (wi).
The weighted Precision and Recall are then calcu-
lated:
P =
?
iwi ?mi(t)
|t|
R =
?
iwi ?mi(r)
|r|
The parameterized harmonic mean of P and R
(van Rijsbergen, 1979) is then calculated:
Fmean =
P ?R
? ? P + (1? ?) ?R
To account for gaps and differences in word or-
der, a fragmentation penalty (Lavie and Agar-
wal, 2007) is calculated using the total number of
matched words (m) and number of chunks (ch):
Pen = ? ?
(
ch
m
)?
The final METEOR-NEXT score is then calculated:
Score = (1? Pen) ? Fmean
The parameters ?, ?, ?, and wi...wn can be
tuned to maximize correlation with various types
of human judgments.
3 The METEOR Paraphrase Tables
To extend support for WMT10 target languages,
we use released parallel corpora to construct para-
phrase tables for English, Czech, German, Span-
ish, and French. These tables are used by the
METEOR-NEXT paraphrase matcher to identify
additional phrase matches in each language.
3.1 Paraphrasing with Parallel Corpora
Following Bannard and Callison-Burch (2005),
we extract paraphrases automatically from bilin-
gual corpora using a pivot phrase method. For a
given language pair, word alignment, phrase ex-
traction, and phrase scoring are conducted on par-
allel corpora to build a single bilingual phrase ta-
ble for the language pair. For each native phrase
(n1) in the table, we identify each foreign phrase
(f ) that translates n1. Each alternate native phrase
(n2 6= n1) that translates f is considered a para-
phrase of n1 with probability P (f |n1) ? P (n2|f).
The total probability of n2 paraphrasing n1 is
given as the sum over all f :
P (n2|n1) =
?
f
P (f |n1) ? P (n2|f)
The same method can be used to identify foreign
paraphrases (f1, f2) given native pivot phrases
n. To merge same-language paraphrases ex-
tracted from different parallel corpora, we take the
mean of the corpus-specific paraphrase probabili-
ties (PC) weighted by the size of the corpora (C)
used for paraphrase extraction:
P (n2|n1) =
?
C |C| ? PC(n2|n1)?
C |C|
To improve paraphrase accuracy, we apply mul-
tiple filtering techniques during paraphrase extrac-
tion. The following are applied to each paraphrase
instance (n1, f, n2):
1. Discard paraphrases with very low probabil-
ity (P (f |n1) ? P (n2|f) < 0.001)
2. Discard paraphrases for which n1, f , or n2
contain any punctuation characters.
3. Discard paraphrases for which n1, f , or
n2 contain only common words. Common
words are defined as having relative fre-
quency of 0.001 or greater in the parallel cor-
pus.
Remaining phrase instances are summed to con-
struct corpus-specific paraphrase tables. Same-
language paraphrase tables are selectively merged
as part of the tuning process described in Sec-
tion 4.2. Final paraphrase tables are further fil-
tered to include only paraphrases with probabili-
ties above a final threshold (0.01).
340
Language Pair Corpus Phrase Table
Target Source Sentences Phrase Pairs
English Czech 7,321,950 128,326,269
English German 1,630,132 84,035,599
English Spanish 7,965,250 363,714,779
English French 8,993,161 404,883,736
German Spanish 1,305,650 70,992,157
Table 1: Sizes of training corpora and phrase ta-
bles used for paraphrase extraction
Language Pivot Languages Phrase Pairs
English German, Spanish, 6,236,236
French
Czech English 756,113
German English, Spanish 3,521,052
Spanish English, German 6,352,690
French English 3,382,847
Table 2: Sizes of final paraphrase tables
3.2 Available Data
We conduct paraphrase extraction using parallel
corpora released for the WMT10 Shared Trans-
lation Task. This includes Europarl corpora
(French-English, Spanish-English, and German-
English), news commentary (French-English,
Spanish-English, German-English, and Czech-
English), United Nations corpora (French-English
and Spanish-English), and the CzEng (Bojar and
Z?abokrtsky?, 2009) corpus sections 0-8 (Czech-
English). In addition, we use the German-Spanish
Europarl corpus released for WMT08 (Callison-
Burch et al, 2008).
3.3 Paraphrase Table Construction
Using all available data for each language pair,
we create bilingual phrase tables for the follow-
ing: French-English, Spanish-English, German-
English, Czech-English, and German-Spanish.
The full training corpora and resulting phrase ta-
bles are described in Table 1. For each phrase ta-
ble, both foreign and native paraphrases are ex-
tracted. Same-language paraphrases are selec-
tively merged as described in Section 4.2 to pro-
duce the final paraphrase tables described in Ta-
ble 2. To keep table size reasonable, we only ex-
tract paraphrases for phrases occurring in target
corpora consisting of the pooled development data
from the WMT08, WMT09, and WMT10 trans-
lation tasks (10,158 sentences for Czech, 20,258
sentences for all other languages).
Target Systems Usable Judgments
English 45 20,357
Czech 5 11,242
German 11 6,563
Spanish 9 3,249
French 12 2,967
Table 3: Human ranking judgment data from
WMT09
4 Tuning METEOR-NEXT
4.1 Development Data
As part of the WMT10 Shared Evaluation Task,
data from WMT09 (Callison-Burch et al, 2009),
including system output, reference translations,
and human judgments, is available for metric de-
velopment. As metrics are evaluated primarily
on their ability to rank system output on the seg-
ment level, we select the human ranking judg-
ments from WMT09 as our development set (de-
scribed in Table 3).
4.2 Tuning Procedure
Tuning a version of METEOR-NEXT consists of
selecting parameters (?, ?, ?, wi...wn) that opti-
mize an objective function for a given language.
If multiple paraphrase tables exist for a language,
tuning also requires selecting the optimal set of ta-
bles to merge.
For WMT10, we tune to rank consistency on the
WMT09 data. Following Callison-Burch et. al
(2009), we discard judgments where system out-
puts are deemed equivalent and calculate the pro-
portion of remaining judgments preserved when
system outputs are ranked by automatic metric
scores. For each target language, tuning is con-
ducted as an exhaustive grid search over metric pa-
rameters and possible paraphrase tables, resulting
in global optima for both.
5 Experiments
To evaluate the impact of our paraphrase ta-
bles on metric performance, we tune versions of
METEOR-NEXT with and without the paraphrase
matchers for each language. For further compar-
ison, we tune a version of METEOR-NEXT using
the TERp English paraphrase table (Snover et al,
2009) used by previous versions of the metric.
As shown in Table 4, the addition of paraphrases
leads to a better tuning point for every target lan-
guage. The best scoring subset of paraphrase ta-
341
Language Paraphrases Rank Consistency ? ? ? wexact wstem wsyn wpar
English none 0.619 0.85 2.35 0.45 1.00 0.80 0.60 ?
TERp 0.625 0.70 1.40 0.25 1.00 0.80 0.80 0.60
de+es+fr 0.629 0.75 0.60 0.35 1.00 0.80 0.80 0.60
Czech none 0.564 0.95 0.20 0.70 1.00 ? ? ?
en 0.574 0.95 2.15 0.35 1.00 ? ? 0.40
German none 0.550 0.20 0.75 0.25 1.00 0.80 ? ?
en+es 0.576 0.75 0.80 0.90 1.00 0.20 ? 0.80
Spanish none 0.586 0.95 0.55 0.90 1.00 0.80 ? ?
en+de 0.608 0.15 0.25 0.75 1.00 0.80 ? 0.40
French none 0.696 0.95 0.80 0.35 1.00 0.60 ? ?
en 0.707 0.90 0.85 0.45 1.00 0.00 ? 0.60
Table 4: Optimal METEOR-NEXT parameters with and without paraphrases for WMT10 target languages
bles for English also outperforms the TERp para-
phrase table.
Analysis of the phrase matches contributed by
the paraphrase matchers reveals an interesting
point about the task of paraphrasing for MT eval-
uation. Despite filtering techniques, the final para-
phrase tables include some unusual, inaccurate,
or highly context-dependent paraphrases. How-
ever, the vast majority of matches identified be-
tween actual system output and reference trans-
lations correspond to valid paraphrases. In many
cases, the evaluation task itself acts as a final filter;
to produce a phrase that can match a spurious para-
phrase, not only must a MT system produce incor-
rect output, but it must produce output that over-
laps exactly with an obscure paraphrase of some
phrase in the reference translation. As systems
are far more likely to produce phrases with similar
words to those in reference translations, far more
valid paraphrases exist in typical system output.
6 Conclusions
We have presented versions of METEOR-NEXT
and paraphrase tables for five target languages.
Tuning experiments indicate consistent improve-
ments across all languages over baseline versions
of the metric. Created for MT evaluation, the ME-
TEOR paraphrase tables can also be used for other
tasks in MT and natural language processing. Fur-
ther, the techniques used to build the paraphrase
tables are language-independent and can be used
to improve evaluation support for other target lan-
guages. METEOR-NEXT, the METEOR paraphrase
tables, and the software used to generate para-
phrases are released under an open source license
and made available via the METEOR website.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proc. of
ACL05.
Ondr?ej Bojar and Zdene?k Z?abokrtsky?. 2009. CzEng
0.9: Large Parallel Treebank with Rich Annotation.
Prague Bulletin of Mathematical Linguistics.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proc. of WMT08.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of WMT09. In
Proc. of WMT09.
Michael Denkowski and Alon Lavie. 2010. Extend-
ing the METEOR Machine Translation Metric to
the Phrase Level for Improved Correlation with Hu-
man Post-Editing Judgments. In Proc. NAACL/HLT
2010.
Alon Lavie and Abhaya Agarwal. 2007. METEOR:
An Automatic Metric for MT Evaluation with High
Levels of Correlation with Human Judgments. In
Proc. of WMT07.
George Miller and Christiane Fellbaum. 2007. Word-
Net. http://wordnet.princeton.edu/.
Martin Porter. 2001. Snowball: A language for stem-
ming algorithms. http://snowball.tartarus.org/texts/.
M. Przybocki, K. Peterson, and S Bronsart. 2008. Offi-
cial results of the NIST 2008 ?Metrics for MAchine
TRanslation? Challenge (MetricsMATR08).
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, Adequacy, or
HTER? Exploring Different Human Judgments with
a Tunable MT Metric. In Proc. of WMT09.
C. van Rijsbergen, 1979. Information Retrieval, chap-
ter 7. 2nd edition.
342
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 85?91,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of
Machine Translation Systems
Michael Denkowski and Alon Lavie
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15232, USA
{mdenkows,alavie}@cs.cmu.edu
Abstract
This paper describes Meteor 1.3, our submis-
sion to the 2011 EMNLP Workshop on Sta-
tistical Machine Translation automatic evalua-
tion metric tasks. New metric features include
improved text normalization, higher-precision
paraphrase matching, and discrimination be-
tween content and function words. We include
Ranking and Adequacy versions of the metric
shown to have high correlation with human
judgments of translation quality as well as a
more balanced Tuning version shown to out-
perform BLEU in minimum error rate training
for a phrase-based Urdu-English system.
1 Introduction
The Meteor1 metric (Banerjee and Lavie, 2005;
Denkowski and Lavie, 2010b) has been shown to
have high correlation with human judgments in eval-
uations such as the 2010 ACL Workshop on Statisti-
cal Machine Translation and NIST Metrics MATR
(Callison-Burch et al, 2010). However, previous
versions of the metric are still limited by lack of
punctuation handling, noise in paraphrase matching,
and lack of discrimination between word types. We
introduce new resources for all WMT languages in-
cluding text normalizers, filtered paraphrase tables,
and function word lists. We show that the addition of
these resources to Meteor allows tuning versions of
the metric that show higher correlation with human
translation rankings and adequacy scores on unseen
1The metric name has previously been stylized as ?ME-
TEOR? or ?METEOR?. As of version 1.3, the official stylization
is simply ?Meteor?.
test data. The evaluation resources are modular, us-
able with any other evaluation metric or MT soft-
ware.
We also conduct a MT system tuning experiment
on Urdu-English data to compare the effectiveness
of using multiple versions of Meteor in minimum
error rate training. While versions tuned to various
types of human judgments do not perform as well
as the widely used BLEU metric (Papineni et al,
2002), a balanced Tuning version of Meteor consis-
tently outperforms BLEU over multiple end-to-end
tune-test runs on this data set.
The versions of Meteor corresponding to the
translation evaluation task submissions, (Ranking
and Adequacy), are described in Sections 3 through
5 while the submission to the tunable metrics task,
(Tuning), is described in Section 6.
2 New Metric Resources
2.1 Meteor Normalizer
Whereas previous versions of Meteor simply strip
punctuation characters prior to scoring, version 1.3
includes a new text normalizer intended specifi-
cally for translation evaluation. The normalizer first
replicates the behavior of the tokenizer distributed
with the Moses toolkit (Hoang et al, 2007), includ-
ing handling of non-breaking prefixes. After tok-
enization, we add several rules for normalization,
intended to reduce meaning-equivalent punctuation
styles to common forms. The following two rules
are particularly helpful:
? Remove dashes between hyphenated words.
(Example: far-off ? far off)
85
? Remove full stops in acronyms/initials. (Exam-
ple: U.N. ? UN)
Consider the behavior of the Moses tokenizer
and Meteor normalizers given a reference trans-
lation containing the phrase ?U.S.-based
organization?:
Moses: U.S.-based organization
Meteor ?1.2: U S based organization
Meteor 1.3: US based organization
Of these, only the Meteor 1.3 normalization
allows metrics to match all of the following
stylizations:
U.S.-based organization
US-based organization
U.S. based organization
US based organization
While intended for Meteor evaluation, use of this
normalizer is a suitable preprocessing step for other
metrics to improve accuracy when reference sen-
tences are stylistically different from hypotheses.
2.2 Filtered Paraphrase Tables
The original Meteor paraphrase tables (Denkowski
and Lavie, 2010b) are constructed using the phrase
table ?pivoting? technique described by Bannard
and Callison-Burch (2005). Many paraphrases suf-
fer from word accumulation, the appending of un-
aligned words to one or both sides of a phrase rather
than finding a true rewording from elsewhere in par-
allel data. To improve the precision of the para-
phrase tables, we filter out all cases of word accumu-
lation by removing paraphrases where one phrase is
a substring of the other. Table 1 lists the number of
phrase pairs found in each paraphrase table before
and after filtering. In addition to improving accu-
racy, the reduction of phrase table sizes also reduces
the load time and memory usage of the Meteor para-
phrase matcher. The tables are a modular resource
suitable for other MT or NLP software.
2.3 Function Word Lists
Commonly used metrics such as BLEU and ear-
lier versions of Meteor make no distinction between
content and function words. This can be problem-
atic for ranking-based evaluations where two system
Language Phrase Pairs After Filtering
English 6.24M 5.27M
Czech 756K 684K
German 3.52M 3.00M
Spanish 6.35M 5.30M
French 3.38M 2.84M
Table 1: Sizes of paraphrase tables before and after filter-
ing
Language Corpus Size (sents) FW Learned
English 836M 93
Czech 230M 68
French 374M 85
German 309M 92
Spanish 168M 66
Table 2: Monolingual corpus size (words) and number of
function words learned for each language
outputs can differ by a single word, such as mistrans-
lating either a main verb or a determiner. To improve
Meteor?s discriminative power in such cases, we in-
troduce a function word list for each WMT language
and a new ? parameter to adjust the relative weight
given to content words (any word not on the list) ver-
sus function words (see Section 3). Function word
lists are estimated according to relative frequency in
large monolingual corpora. For each language, we
pool freely available WMT 2011 data consisting of
Europarl (Koehn, 2005), news (sentence-uniqued),
and news commentary data. Any word with relative
frequency of 10?3 or greater is added to the func-
tion word list. Table 2 lists corpus size and number
of function words learned for each language. In ad-
dition to common words, punctuation symbols con-
sistently rise to the tops of function word lists.
3 Meteor Scoring
Meteor evaluates translation hypotheses by align-
ing them to reference translations and calculating
sentence-level similarity scores. This section de-
scribes our extended version of the metric.
For a hypothesis-reference pair, the search space
of possible alignments is constructed by identifying
all possible matches between the two sentences ac-
cording to the following matchers:
Exact: Match words if their surface forms are iden-
86
tical.
Stem: Stem words using a language-appropriate
Snowball Stemmer (Porter, 2001) and match if the
stems are identical.
Synonym: Match words if they share member-
ship in any synonym set according to the Word-
Net (Miller and Fellbaum, 2007) database.
Paraphrase: Match phrases if they are listed as
paraphrases in the paraphrase tables described in
Section 2.2.
All matches are generalized to phrase matches
with a start position and phrase length in each sen-
tence. Any word occurring less than length posi-
tions after a match start is considered covered by
the match. The exact and paraphrase matchers sup-
port all five WMT languages while the stem matcher
is limited to English, French, German, and Spanish
and the synonym matcher is limited to English.
Once matches are identified, the final alignment is
resolved as the largest subset of all matches meeting
the following criteria in order of importance:
1. Require each word in each sentence to be cov-
ered by zero or one matches.
2. Maximize the number of covered words across
both sentences.
3. Minimize the number of chunks, where a chunk
is defined as a series of matches that is contigu-
ous and identically ordered in both sentences.
4. Minimize the sum of absolute distances be-
tween match start positions in the two sen-
tences. (Break ties by preferring to align words
and phrases that occur at similar positions in
both sentences.)
Given an alignment, the metric score is calculated
as follows. Content and function words are iden-
tified in the hypothesis (hc, hf ) and reference (rc,
rf ) according to the function word lists described in
Section 2.3. For each of the matchers (mi), count
the number of content and function words covered
by matches of this type in the hypothesis (mi(hc),
mi(hf )) and reference (mi(rc), mi(rf )). Calculate
weighted precision and recall using matcher weights
(wi...wn) and content-function word weight (?):
P =
?
iwi ? (? ?mi(hc) + (1? ?) ?mi(hf ))
? ? |hc|+ (1? ?) ? |hf |
Target WMT09 WMT10 Combined
English 20,357 24,915 45,272
Czech 11,242 9,613 20,855
French 2,967 5,904 7,062
German 6,563 10,892 17,455
Spanish 3,249 3,813 7,062
Table 3: Human ranking judgment data from 2009 and
2010 WMT evaluations
R =
?
iwi ? (? ?mi(rc) + (1? ?) ?mi(rf ))
? ? |rc|+ (1? ?) ? |rf |
The parameterized harmonic mean of P and R (van
Rijsbergen, 1979) is then calculated:
Fmean =
P ?R
? ? P + (1? ?) ?R
To account for gaps and differences in word order,
a fragmentation penalty is calculated using the total
number of matched words (m, average over hypoth-
esis and reference) and number of chunks (ch):
Pen = ? ?
(
ch
m
)?
The Meteor score is then calculated:
Score = (1? Pen) ? Fmean
The parameters ?, ?, ?, ?, and wi...wn are tuned
to maximize correlation with human judgments.
4 Parameter Optimization
4.1 Development Data
The 2009 and 2010 WMT shared evaluation data
sets are made available as development data for
WMT 2011. Data sets include MT system outputs,
reference translations, and human rankings of trans-
lation quality. Table 3 lists the number of judgments
for each evaluation and combined totals.
4.2 Tuning Procedure
To evaluate a metric?s performance on a data set, we
count the number of pairwise translation rankings
preserved when translations are re-ranked by met-
ric score. We then compute Kendall?s ? correlation
coefficient as follows:
? =
concordant pairs?discordant pairs
total pairs
87
Tune ? (WMT09) Test ? (WMT10)
Lang Met1.2 Met1.3 Met1.2 Met1.3
English 0.258 0.276 0.320 0.343
Czech 0.148 0.162 0.220 0.215
French 0.414 0.437 0.370 0.384
German 0.152 0.180 0.170 0.155
Spanish 0.216 0.240 0.310 0.326
Table 5: Meteor 1.2 and 1.3 correlation with ranking
judgments on tune and test data
For each WMT language, we learn Meteor pa-
rameters that maximize ? over the combined 2009
and 2010 data sets using an exhaustive parametric
sweep. The resulting parameters, listed in Table 4,
are used in the default Ranking version of Meteor
1.3.
For each language, the ? parameter is above 0.5,
indicating a preference for content words over func-
tion words. In addition, the fragmentation penalties
are generally less severe across languages. The ad-
ditional features in Meteor 1.3 allow for more bal-
anced parameters that distribute responsibility for
penalizing various types of erroneous translations.
5 Evaluation Experiments
To compare Meteor 1.3 against previous versions of
the metric on the task of evaluating MT system out-
puts, we tune a version for each language on 2009
WMT data and evaluate on 2010 data. This repli-
cates the 2010 WMT shared evaluation task, allow-
ing comparison to Meteor 1.2. Table 5 lists correla-
tion of each metric version with ranking judgments
on tune and test data. Meteor 1.3 shows significantly
higher correlation on both tune and test data for En-
glish, French, and Spanish while Czech and German
demonstrate overfitting with higher correlation on
tune data but lower on test data. This overfitting ef-
fect is likely due to the limited number of systems
providing translations into these languages and the
difficulty of these target languages leading to sig-
nificantly noisier translations skewing the space of
metric scores. We believe that tuning to combined
2009 and 2010 data will counter these issues for the
official Ranking version.
Meteor-1.2 r Meteor-1.3 r
Tune / Test MT08 MT09 MT08 MT09
MT08 0.620 0.625 0.650 0.636
MT09 0.612 0.630 0.642 0.648
Tune / Test P2 P3 P2 P3
P2 -0.640 -0.596 -0.642 -0.594
P3 -0.638 -0.600 -0.625 -0.612
Table 6: Meteor 1.2 and 1.3 correlation with adequacy
and H-TER scores on tune and test data
5.1 Generalization to Other Tasks
To evaluate the impact of new features on other
evaluation tasks, we follow Denkowski and Lavie
(2010a), tuning versions of Meteor to maximize
length-weighted sentence-level Pearson?s r correla-
tion coefficient with adequacy and H-TER (Snover
et al, 2006) scores of translations. Data sets in-
clude 2008 and 2009 NIST Open Machine Trans-
lation Evaluation adequacy data (Przybocki, 2009)
and GALE P2 and P3 H-TER data (Olive, 2005).
For each type of judgment, metric versions are tuned
and tested on each year and scores are compared.
We compare Meteor 1.3 results with those from ver-
sion 1.2 with results shown in Table 6. For both
adequacy data sets, Meteor 1.3 significantly outper-
forms version 1.2 on both tune and test data. The
version tuned on MT09 data is selected as the official
Adequacy version of Meteor 1.3. H-TER versions
either show no improvement or degradation due to
overfitting. Examination of the optimal H-TER pa-
rameter sets reveals a mismatch between evalua-
tion metric and human judgment type. As H-TER
evaluation is ultimately limited by the TER aligner,
there is no distinction between content and function
words, and words sharing stems are considered non-
matches. As such, these features do not help Meteor
improve correlation, but rather act as a source of ad-
ditional possibility for overfitting.
6 MT System Tuning Experiments
The 2011 WMT Tunable Metrics task consists of
using Z-MERT (Zaidan, 2009) to tune a pre-built
Urdu-English Joshua (Li et al, 2009) system to a
new evaluation metric on a tuning set with 4 refer-
ence translations and decoding a test set using the re-
sulting parameter set. As this task does not provide a
88
Language ? ? ? ? wexact wstem wsyn wpar
English 0.85 0.20 0.60 0.75 1.00 0.60 0.80 0.60
Czech 0.95 0.20 0.60 0.80 1.00 ? ? 0.40
French 0.90 1.40 0.60 0.65 1.00 0.20 ? 0.40
German 0.95 1.00 0.55 0.55 1.00 0.80 ? 0.20
Spanish 0.65 1.30 0.50 0.80 1.00 0.80 ? 0.60
Table 4: Optimal Meteor parameters for WMT target languages on 2009 and 2010 data (Meteor 1.3 Ranking)
devtest set, we select a version of Meteor by explor-
ing the effectiveness of using multiple versions of
the metric to tune phrase-based translation systems
for the same language pair.
We use the 2009 NIST Open Machine Transla-
tion Evaluation Urdu-English parallel data (Przy-
bocki, 2009) plus 900M words of monolingual data
from the English Gigaword corpus (Parker et al,
2009) to build a standard Moses system (Hoang et
al., 2007) as follows. Parallel data is word aligned
using the MGIZA++ toolkit (Gao and Vogel, 2008)
and alignments are symmetrized using the ?grow-
diag-final-and? heuristic. Phrases are extracted us-
ing standard phrase-based heuristics (Koehn et al,
2003) and used to build a translation table and lex-
icalized reordering model. A standard SRI 5-gram
language model (Stolke, 2002) is estimated from
monolingual data. Using Z-MERT, we tune this sys-
tem to baseline metrics as well as the versions of
Meteor discussed in previous sections. We also tune
to a balanced Tuning version of Meteor designed to
minimize bias. This data set provides a single set
of reference translations for MERT. To account for
the variance of MERT, we run end-to-end tuning 3
times for each metric and report the average results
on two unseen test sets: newswire and weblog. Test
set translations are evaluated using BLEU, TER, and
Meteor 1.2. The parameters for each Meteor version
are listed in Table 7 while the results are listed in
Table 8.
The results are fairly consistent across both test
sets: the Tuning version of Meteor outperforms
BLEU across all metrics while versions of Meteor
that perform well on other tasks perform poorly in
tuning. This illustrates the differences between eval-
uation and tuning tasks. In evaluation tasks, metrics
are engineered to score 1-best translations from sys-
tems most often tuned to BLEU. As listed in Table 7,
Newswire
Tuning Metric BLEU TER Met1.2
BLEU 23.67 72.48 50.45
TER 25.35 59.72 48.60
TER-BLEU/2 26.25 61.66 49.69
Meteor-tune 24.89 69.54 51.29
Meteor-rank 19.28 94.64 49.78
Meteor-adq 22.86 77.27 51.40
Meteor-hter 25.23 66.71 50.90
Weblog
Tuning Metric BLEU TER Met1.2
BLEU 17.10 76.28 41.86
TER 17.07 64.32 39.75
TER-BLEU/2 18.14 65.77 40.68
Meteor-tune 18.07 73.83 42.78
Meteor-rank 14.34 98.86 42.75
Meteor-adq 16.76 81.63 43.43
Meteor-hter 18.12 70.47 42.28
Table 8: Average metric scores for Urdu-English systems
tuned to baseline metrics and versions of Meteor
these parameters are often skewed to emphasize the
differences between system outputs. In the tuning
scenario, MERT optimizes translation quality with
respect to the tuning metric. If a metric is biased (for
example, assigning more weight to recall than preci-
sion), it will guide the MERT search toward patho-
logical translations that receive lower scores across
other metrics. Balanced between precision and re-
call, content and function words, and word choice
versus fragmentation, the Tuning version of Meteor
is significantly less susceptible to gaming. Chosen
as the official submission for WMT 2011, we be-
lieve that this Tuning version of Meteor will further
generalize to other tuning scenarios.
89
Task ? ? ? ? wexact wstem wsyn wpar
Ranking 0.85 0.20 0.60 0.75 1.00 0.60 0.80 0.60
Adequacy 0.75 1.40 0.45 0.70 1.00 1.00 0.60 0.80
H-TER 0.40 1.50 0.35 0.55 1.00 0.20 0.60 0.80
Tuning 0.50 1.00 0.50 0.50 1.00 0.50 0.50 0.50
Table 7: Parameters for Meteor 1.3 tasks
7 Conclusions
We have presented Ranking, Adequacy, and Tun-
ing versions of Meteor 1.3. The Ranking and Ad-
equacy versions are shown to have high correlation
with human judgments except in cases of overfitting
due to skewed tuning data. We believe that these
overfitting issues are lessened when tuning to com-
bined 2009 and 2010 data due to increased variety
in translation characteristics. The Tuning version of
Meteor is shown to outperform BLEU in minimum
error rate training of a phrase-based system on small
Urdu-English data and we believe that it will gener-
alize well to other tuning scenarios. The source code
and all resources for Meteor 1.3 and the version of
Z-MERT with Meteor integration will be available
for download from the Meteor website.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Proc.
of ACL WIEEMMTS 2005.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proc. of
ACL2005.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 Joint Workshop on Sta-
tistical Machine Translation and Metrics for Machine
Translation. In Proc. of ACL WMT/MetricsMATR
2010.
Michael Denkowski and Alon Lavie. 2010a. Choosing
the Right Evaluation for Machine Translation: an Ex-
amination of Annotator and Automatic Metric Perfor-
mance on Human Judgment Tasks. In Proc. of AMTA
2010.
Michael Denkowski and Alon Lavie. 2010b. METEOR-
NEXT and the METEOR Paraphrase Tables: Improve
Evaluation Support for Five Target Languages. In
Proc. of ACL WMT/MetricsMATR 2010.
Qin Gao and Stephan Vogel. 2008. Parallel Implemen-
tations of Word Alignment Tool. In Proc. of ACL
WSETQANLP 2008.
Hieu Hoang, Alexandra Birch, Chris Callison-burch,
Richard Zens, Rwth Aachen, Alexandra Constantin,
Marcello Federico, Nicola Bertoldi, Chris Dyer,
Brooke Cowan, Wade Shen, Christine Moran, and On-
dej Bojar. 2007. Moses: Open Source Toolkit for Sta-
tistical Machine Translation. In Proc. of ACL 2007.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Proc.
of NAACL/HLT 2003.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proc. of MT Sum-
mit 2005.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An Open Source Toolkit for Parsing-based
Machine Translation. In Proc. of WMT 2009.
George Miller and Christiane Fellbaum. 2007. WordNet.
http://wordnet.princeton.edu/.
Joseph Olive. 2005. Global Autonomous Language Ex-
ploitation (GALE). DARPA/IPTO Proposer Informa-
tion Pamphlet.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proc. of ACL 2002.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English Gigaword Fourth Edi-
tion. Linguistic Data Consortium, LDC2009T13.
Martin Porter. 2001. Snowball: A language for stem-
ming algorithms. http://snowball.tartarus.org/texts/.
Mark Przybocki. 2009. NIST Open
Machine Translation 2009 Evaluation.
http://www.itl.nist.gov/iad/mig/tests/mt/2009/.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proc. of AMTA 2006.
Andreas Stolke. 2002. SRILM - an Extensible Language
Modeling Toolkit. In Proc. of ICSLP 2002.
C. van Rijsbergen, 1979. Information Retrieval, chap-
ter 7. 2nd edition.
90
Omar F. Zaidan. 2009. Z-MERT: A Fully Configurable
Open Source Tool for Minimum Error Rate Training
of Machine Translation Systems. The Prague Bulletin
of Mathematical Linguistics.
91
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 261?266,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
The CMU-Avenue French-English Translation System
Michael Denkowski Greg Hanneman Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{mdenkows,ghannema,alavie}@cs.cmu.edu
Abstract
This paper describes the French-English trans-
lation system developed by the Avenue re-
search group at Carnegie Mellon University
for the Seventh Workshop on Statistical Ma-
chine Translation (NAACL WMT12). We
present a method for training data selection,
a description of our hierarchical phrase-based
translation system, and a discussion of the im-
pact of data size on best practice for system
building.
1 Introduction
We describe the French-English translation sys-
tem constructed by the Avenue research group at
Carnegie Mellon University for the shared trans-
lation task in the Seventh Workshop on Statistical
Machine Translation. The core translation system
uses the hierarchical phrase-based model described
by Chiang (2007) with sentence-level grammars ex-
tracted and scored using the methods described by
Lopez (2008). Improved techniques for data selec-
tion and monolingual text processing significantly
improve the performance of the baseline system.
Over half of all parallel data for the French-
English track is provided by the Giga-FrEn cor-
pus (Callison-Burch et al, 2009). Assembled from
crawls of bilingual websites, this corpus is known to
be noisy, containing sentences that are either not par-
allel or not natural language. Rather than simply in-
cluding or excluding the resource in its entirety, we
use a relatively simple technique inspired by work in
machine translation quality estimation to select the
best portions of the corpus for inclusion in our train-
ing data. Including around 60% of the Giga-FrEn
chosen by this technique yields an improvement of
0.7 BLEU.
Prior to model estimation, we process all parallel
and monolingual data using in-house tokenization
and normalization scripts that detect word bound-
aries better than the provided WMT12 scripts. After
translation, we apply a monolingual rule-based post-
processing step to correct obvious errors and make
sentences more acceptable to human judges. The
post-processing step alone yields an improvement of
0.3 BLEU to the final system.
We conclude with a discussion of the impact of
data size on important decisions for system building.
Experimental results show that ?best practice? deci-
sions for smaller data sizes do not necessarily carry
over to systems built with ?WMT-scale? data, and
provide some explanation for why this is the case.
2 Training Data
Training data provided for the French-English trans-
lation task includes parallel corpora taken from Eu-
ropean Parliamentary proceedings (Koehn, 2005),
news commentary, and United Nations documents.
Together, these sets total approximately 13 million
sentences. In addition, a large, web-crawled parallel
corpus termed the ?Giga-FrEn? (Callison-Burch et
al., 2009) is made available. While this corpus con-
tains over 22 million parallel sentences, it is inher-
ently noisy. Many parallel sentences crawled from
the web are neither parallel nor sentences. To make
use of this large data source, we employ data se-
lection techniques discussed in the next subsection.
261
Corpus Sentences
Europarl 1,857,436
News commentary 130,193
UN doc 11,684,454
Giga-FrEn 1stdev 7,535,699
Giga-FrEn 2stdev 5,801,759
Total 27,009,541
Table 1: Parallel training data
Parallel data used to build our final system totals 27
million sentences. Precise figures for the number of
sentences in each data set, including selections from
the Giga-FrEn, are found in Table 1.
2.1 Data Selection as Quality Estimation
Drawing inspiration from the workshop?s featured
task, we cast the problem of data selection as one
of quality estimation. Specia et al (2009) report
several estimators of translation quality, the most ef-
fective of which detect difficult-to-translate source
sentences, ungrammatical translations, and transla-
tions that align poorly to their source sentences. We
can easily adapt several of these predictive features
to select good sentence pairs from noisy parallel cor-
pora such as the Giga-FrEn.
We first pre-process the Giga-FrEn by removing
lines with invalid Unicode characters, control char-
acters, and insufficient concentrations of Latin char-
acters. We then score each sentence pair in the re-
maining set (roughly 90% of the original corpus)
with the following features:
Source language model: a 4-gram modified
Kneser-Ney smoothed language model trained on
French Europarl, news commentary, UN doc, and
news crawl corpora. This model assigns high scores
to grammatical source sentences and lower scores to
ungrammatical sentences and non-sentences such as
site maps, large lists of names, and blog comments.
Scores are normalized by number of n-grams scored
per sentence (length + 1). The model is built using
the SRILM toolkit (Stolke, 2002).
Target language model: a 4-gram modified
Kneser-Ney smoothed language model trained on
English Europarl, news commentary, UN doc, and
news crawl corpora. This model scores grammati-
cality on the target side.
Word alignment scores: source-target and
target-source MGIZA++ (Gao and Vogel, 2008)
force-alignment scores using IBM Model 4 (Och
and Ney, 2003). Model parameters are estimated
on 2 million words of French-English Europarl and
news commentary text. Scores are normalized by
the number of alignment links. These features mea-
sure the extent to which translations are parallel with
their source sentences.
Fraction of aligned words: source-target and
target-source ratios of aligned words to total words.
These features balance the link-normalized align-
ment scores.
To determine selection criteria, we use this feature
set to score the news test sets from 2008 through
2011 (10K parallel sentences) and calculate the
mean and standard deviation of each feature score
distribution. We then select two subsets of the Giga-
FrEn, ?1stdev? and ?2stdev?. The 1stdev set in-
cludes sentence pairs for which the score for each
feature is above a threshold defined as the develop-
ment set mean minus one standard deviation. The
2stdev set includes sentence pairs not included in
1stdev that meet the per-feature threshold of mean
minus two standard deviations. Hard, per-feature
thresholding is motivated by the notion that a sen-
tence pair must meet al the criteria discussed above
to constitute good translation. For example, high
source and target language model scores are irrel-
evant if the sentences are not parallel.
As primarily news data is used for determining
thresholds and building language models, this ap-
proach has the added advantage of preferring par-
allel data in the domain we are interested in translat-
ing. Our final translation system uses data from both
1stdev and 2stdev, corresponding to roughly 60% of
the Giga-FrEn corpus.
2.2 Monolingual Data
Monolingual English data includes European Parlia-
mentary proceedings (Koehn, 2005), news commen-
tary, United Nations documents, news crawl, the En-
glish side of the Giga-FrEn, and the English Giga-
word Fourth Edition (Parker et al, 2009). We use all
available data subject to the following selection de-
cisions. We apply the initial filter to the Giga-FrEn
to remove non-text sections, leaving approximately
90% of the corpus. We exclude the known prob-
262
Corpus Words
Europarl 59,659,916
News commentary 5,081,368
UN doc 286,300,902
News crawl 1,109,346,008
Giga-FrEn 481,929,410
Gigaword 4th edition 1,960,921,287
Total 3,903,238,891
Table 2: Monolingual language modeling data (uniqued)
lematic New York Times section of the Gigaword.
As many data sets include repeated boilerplate text
such as copyright information or browser compat-
ibility notifications, we unique sentences from the
UN doc, news crawl, Giga-FrEn, and Gigaword sets
by source. Final monolingual data totals 4.7 billion
words before uniqueing and 3.9 billion after. Word
counts for all data sources are shown in Table 2.
2.3 Text Processing
All monolingual and parallel system data is run
through a series of pre-processing steps before
construction of the language model or translation
model. We first run an in-house normalization script
over all text in order to convert certain variably en-
coded characters to a canonical form. For example,
thin spaces and non-breaking spaces are normalized
to standard ASCII space characters, various types of
?curly? and ?straight? quotation marks are standard-
ized as ASCII straight quotes, and common French
and English ligatures characters (e.g. ?, fi) are re-
placed with standard equivalents.
English text is tokenized with the Penn Treebank-
style tokenizer attached to the Stanford parser (Klein
and Manning, 2003), using most of the default op-
tions. We set the tokenizer to Americanize vari-
ant spellings such as color vs. colour or behavior
vs. behaviour. Currency-symbol normalization is
avoided.
For French text, we use an in-house tokenization
script. Aside from the standard tokenization based
on punctuation marks, this step includes French-
specific rules for handling apostrophes (French eli-
sion), hyphens in subject-verb inversions (includ-
ing the French t euphonique), and European-style
numbers. When compared to the default WMT12-
provided tokenization script, our custom French
rules more accurately identify word boundaries, par-
ticularly in the case of hyphens. Figure 1 highlights
the differences in sample phrases. Subject-verb in-
versions are broken apart, while other hyphenated
words are unaffected; French aujourd?hui (?today?)
is retained as a single token to match English.
Parallel data is run through a further filtering step
to remove sentence pairs that, by their length char-
acteristics alone, are very unlikely to be true parallel
data. Sentence pairs that contain more than 95 to-
kens on either side are globally discarded, as are sen-
tence pairs where either side contains a token longer
than 25 characters. Remaining pairs are checked for
length ratio between French and English, and sen-
tences are discarded if their English translations are
either too long or too short given the French length.
Allowable ratios are determined from the tokenized
training data and are set such that approximately the
middle 95% of the data, in terms of length ratio, is
kept for each French length.
3 Translation System
Our translation system uses cdec (Dyer et al,
2010), an implementation of the hierarchical phrase-
based translation model (Chiang, 2007) that uses the
KenLM library (Heafield, 2011) for language model
inference. The system translates from cased French
to cased English; at no point do we lowercase data.
The Parallel data is aligned in both directions us-
ing the MGIZA++ (Gao and Vogel, 2008) imple-
mentation of IBM Model 4 and symmetrized with
the grow-diag-final heuristic (Och and Ney,
2003). The aligned corpus is then encoded as a
suffix array to facilitate sentence-level grammar ex-
traction and scoring (Lopez, 2008). Grammars are
extracted using the heuristics described by Chiang
(Chiang, 2007) and feature scores are calculated ac-
cording to Lopez (2008).
Modified Knesser-Ney smoothed (Chen and
Goodman, 1996) n-gram language models are built
from the monolingual English data using the SRI
language modeling toolkit (Stolke, 2002). We ex-
periment with both 4-gram and 5-gram models.
System parameters are optimized using minimum
error rate training (Och, 2003) to maximize the
corpus-level cased BLEU score (Papineni et al,
263
Base: Y a-t-il un colle`gue pour prendre la parole
Custom: Y a -t-il un colle`gue pour prendre la parole
Base: Peut-e?tre , a` ce sujet , puis-je dire a` M. Ribeiro i Castro
Custom: Peut-e?tre , a` ce sujet , puis -je dire a` M. Ribeiro i Castro
Base: le proce`s-verbal de la se?ance d? aujourd? hui
Custom: le proce`s-verbal de la se?ance d? aujourd?hui
Base: s? e?tablit environ a` 1,2 % du PIB
Custom: s? e?tablit environ a` 1.2 % du PIB
Figure 1: Customized French tokenization rules better identify word boundaries.
pre?-e?l?ectoral ? pre-electoral
mosa??que ? mosaique
de?ragulation ? deragulation
Figure 2: Examples of cognate translation
2002) on news-test 2008 (2051 sentences). This de-
velopment set is chosen for its known stability and
reliability.
Our baseline translation system uses Viterbi de-
coding while our final system uses segment-level
Minimum Bayes-Risk decoding (Kumar and Byrne,
2004) over 500-best lists using 1 - BLEU as the loss
function.
3.1 Post-Processing
Our final system includes a monolingual rule-based
post-processing step that corrects obvious transla-
tion errors. Examples of correctable errors include
capitalization, mismatched punctuation, malformed
numbers, and incorrectly split compound words. We
finally employ a coarse cognate translation system
to handle out-of-vocabulary words. We assume that
uncapitalized French source words passed through
to the English output are cognates of English words
and translate them by removing accents. This fre-
quently leads to (in order of desirability) fully cor-
rect translations, correct translations with foreign
spellings, or correct translations with misspellings.
All of the above are generally preferable to untrans-
lated foreign words. Examples of cognate transla-
tions for OOV words in newstest 2011 are shown in
Figure 2.1
1Some OOVs are caused by misspellings in the dev-test
source sentences. In these cases we can salvage misspelled En-
glish words in place of misspelled French words
BLEU (cased) Meteor TER
base 5-gram 28.4 27.4 33.7 53.2
base 4-gram 29.1 28.1 34.0 52.5
+1stdev GFE 29.3 28.3 34.2 52.1
+2stdev GFE 29.8 28.9 34.5 51.7
+5g/1K/MBR 29.9 29.0 34.5 51.5
+post-process 30.2 29.2 34.7 51.3
Table 3: Newstest 2011 (dev-test) translation results
4 Experiments
Beginning with a baseline translation system, we in-
crementally evaluate the contribution of additional
data and components. System performance is eval-
uated on newstest 2011 using BLEU (uncased and
cased) (Papineni et al, 2002), Meteor (Denkowski
and Lavie, 2011), and TER (Snover et al, 2006).
For full consistency with WMT11, we use the NIST
scoring script, TER-0.7.25, and Meteor-1.3 to eval-
uate cased, detokenized translations. Results are
shown in Table 3, where each evaluation point is the
result of a full tune/test run that includes MERT for
parameter optimization.
The baseline translation system is built from 14
million parallel sentences (Europarl, news commen-
tary, and UN doc) and all monolingual data. Gram-
mars are extracted using the ?tight? heuristic that
requires phrase pairs to be bounded by word align-
ments. Both 4-gram and 5-gram language models
are evaluated. Viterbi decoding is conducted with a
cube pruning pop limit (Chiang, 2007) of 200. For
this data size, the 4-gram model is shown to signifi-
cantly outperform the 5-gram.
Adding the 1stdev and 2stdev sets from the Giga-
FrEn increases the parallel data size to 27 million
264
BLEU (cased) Meteor TER
587M tight 29.1 28.1 34.0 52.5
587M loose 29.3 28.3 34.0 52.5
745M tight 29.8 28.9 34.5 51.7
745M loose 29.6 28.6 34.3 52.0
Table 4: Results for extraction heuristics (dev-test)
sentences and further improves performance. These
runs require new grammars to be extracted, but
use the same 4-gram language model and decoding
method as the baseline system. With large training
data, moving to a 5-gram language model, increas-
ing the cube pruning pop limit to 1000, and using
Minimum Bayes-Risk decoding (Kumar and Byrne,
2004) over 500-best lists collectively show a slight
improvement. Monolingual post-processing yields
further improvement. This decoding/processing
scheme corresponds to our final translation system.
4.1 Impact of Data Size
The WMT French-English track provides an oppor-
tunity to experiment in a space of data size that is
generally not well explored. We examine the impact
of data sizes of hundreds of millions of words on
two significant system building decisions: grammar
extraction and language model estimation. Compar-
ative results are reported on the newstest 2011 set.
In the first case, we compare the ?tight? extrac-
tion heuristic that requires phrases to be bounded
by word alignments to the ?loose? heuristic that al-
lows unaligned words at phrase edges. Lopez (2008)
shows that for a parallel corpus of 107 million
words, using the loose heuristic produces much
larger grammars and improves performance by a full
BLEU point. However, even our baseline system
is trained on substantially more data (587 million
words on the English side) and the addition of the
Giga-FrEn sets increases data size to 745 million
words, seven times that used in the cited work. For
each data size, we decode with grammars extracted
using each heuristic and a 4-gram language model.
As shown in Table 4, the differences are much
smaller and the tight heuristic actually produces the
best result for the full data scenario. We believe
this to be directly linked to word alignment quality:
smaller training data results in sparser, noisier word
BLEU (cased) Meteor TER
587M 4-gram 29.1 28.1 34.0 52.5
587M 5-gram 28.4 27.4 33.7 53.2
745M 4-gram 29.8 28.9 34.5 51.7
745M 5-gram 29.8 28.9 34.4 51.7
Table 5: Results for language model order (dev-test)
alignments while larger data results in denser, more
accurate alignments. In the first case, accumulating
unaligned words can make up for shortcomings in
alignment quality. In the second, better rules are ex-
tracted by trusting the stronger alignment model.
We also compare 4-gram and 5-gram language
model performance with systems using tight gram-
mars extracted from 587 million and 745 million
sentences. As shown in Table 5, the 4-gram sig-
nificantly outperforms the 5-gram with smaller data
while the two are indistinguishable with larger data2.
With modified Kneser-Ney smoothing, a lower or-
der model will outperform a higher order model if
the higher order model constantly backs off to lower
orders. With stronger grammars learned from larger
parallel data, the system is able to produce output
that matches longer n-grams in the language model.
5 Summary
We have presented the French-English translation
system built for the NAACL WMT12 shared transla-
tion task, including descriptions of our data selection
and text processing techniques. Experimental re-
sults have shown incremental improvement for each
addition to our baseline system. We have finally
discussed the impact of the availability of WMT-
scale data on system building decisions and pro-
vided comparative experimental results.
References
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In Proc.
of ACL WMT 2009.
2We find that for the full data system, also increasing the
cube pruning pop limit and using MBR decoding yields a very
slight improvement with the 5-gram model over the same de-
coding scheme with the 4-gram.
265
Stanley F. Chen and Joshua Goodman. 1996. An Em-
pirical Study of Smoothing Techniques for Language
Modeling. In Proc. of ACL 1996.
David Chiang. 2007. Hierarchical Phrase-Based Trans-
lation.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3:
Automatic Metric for Reliable Optimization and Eval-
uation of Machine Translation Systems. In Proc. of
the EMNLP WMT 2011.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec:
A Decoder, Alignment, and Learning Framework for
Finite-State and Context-Free Translation Models. In
Proc. of ACL 2010.
Qin Gao and Stephan Vogel. 2008. Parallel Implemen-
tations of Word Alignment Tool. In Proc. of ACL
WSETQANLP 2008.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proc. of EMNLP WMT
2011.
Dan Klein and Christopher D. Manning. 2003. Accurate
Unlexicalized Parsing. In Proc. of ACL 2003.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proc. of MT Sum-
mit 2005.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-Risk Decoding for Statistical Machine Transla-
tion. In Proc. of NAACL/HLT 2004.
Adam Lopez. 2008. Tera-Scale Translation Models via
Pattern Matching. In Proc. of COLING 2008.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29.
Franz Josef Och. 2003. Minimum Error Rate Training
for Statistical Machine Translation. In Proc. of ACL
2003.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proc. of ACL 2002.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English Gigaword Fourth Edi-
tion. Linguistic Data Consortium, LDC2009T13.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proc. of AMTA 2006.
Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran
Wang, and John Shawe-Taylor. 2009. Improving the
Confidence of Machine Translation Quality Estimates.
In Proc. of MT Summit XII.
Andreas Stolke. 2002. SRILM - an Extensible Language
Modeling Toolkit. In Proc. of ICSLP 2002.
266
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 70?77,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The CMU Machine Translation Systems at WMT 2013:
Syntax, Synthetic Translation Options, and Pseudo-References
Waleed Ammar Victor Chahuneau Michael Denkowski Greg Hanneman
Wang Ling Austin Matthews Kenton Murray Nicola Segall Yulia Tsvetkov
Alon Lavie Chris Dyer?
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
?Corresponding author: cdyer@cs.cmu.edu
Abstract
We describe the CMU systems submit-
ted to the 2013 WMT shared task in ma-
chine translation. We participated in three
language pairs, French?English, Russian?
English, and English?Russian. Our
particular innovations include: a label-
coarsening scheme for syntactic tree-to-
tree translation and the use of specialized
modules to create ?synthetic translation
options? that can both generalize beyond
what is directly observed in the parallel
training data and use rich source language
context to decide how a phrase should
translate in context.
1 Introduction
The MT research group at Carnegie Mellon Uni-
versity?s Language Technologies Institute par-
ticipated in three language pairs for the 2013
Workshop on Machine Translation shared trans-
lation task: French?English, Russian?English,
and English?Russian. Our French?English sys-
tem (?3) showcased our group?s syntactic sys-
tem with coarsened nonterminal types (Hanne-
man and Lavie, 2011). Our Russian?English and
English?Russian system demonstrate a new multi-
phase approach to translation that our group is us-
ing, in which synthetic translation options (?4)
to supplement the default translation rule inven-
tory that is extracted from word-aligned training
data. In the Russian-English system (?5), we used
a CRF-based transliterator (Ammar et al, 2012)
to propose transliteration candidates for out-of-
vocabulary words, and used a language model
to insert or remove common function words in
phrases according to an n-gram English language
model probability. In the English?Russian system
(?6), we used a conditional logit model to predict
the most likely inflectional morphology of Rus-
sian lemmas, conditioning on rich source syntac-
tic features (?6.1). In addition to being able to
generate inflected forms that were otherwise unob-
served in the parallel training data, the translations
options generated in this matter had features re-
flecting their appropriateness given much broader
source language context than usually would have
been incorporated in current statistical MT sys-
tems.
For our Russian?English system, we addition-
ally used a secondary ?pseudo-reference? transla-
tion when tuning the parameters of our Russian?
English system. This was created by automatically
translating the Spanish translation of the provided
development data into English. While the output
of an MT system is not always perfectly gram-
matical, previous work has shown that secondary
machine-generated references improve translation
quality when only a single human reference is
available when BLEU is used as an optimization
criterion (Madnani, 2010; Dyer et al, 2011).
2 Common System Components
The decoder infrastructure we used was cdec
(Dyer et al, 2010). Only the constrained data
resources provided for the shared task were used
for training both the translation and language
models. Word alignments were generated us-
ing the Model 2 variant described in Dyer et al
(2013). Language models used modified Kneser-
Ney smoothing estimated using KenLM (Heafield,
2011). Translation model parameters were dis-
criminatively set to optimize BLEU on a held-out
development set using an online passive aggres-
sive algorithm (Eidelman, 2012) or, in the case of
70
the French?English system, using the hypergraph
MERT algorithm and optimizing towards BLEU
(Kumar et al, 2009). The remainder of the paper
will focus on our primary innovations in the vari-
ous system pairs.
3 French-English Syntax System
Our submission for French?English is a tree-to-
tree translation system that demonstrates several
innovations from group?s research on SCFG-based
translation.
3.1 Data Selection
We divided the French?English training data into
two categories: clean data (Europarl, News Com-
mentary, UN Documents) totaling 14.8 million
sentence pairs, and web data (Common Crawl,
Giga-FrEn) totaling 25.2 million sentence pairs.
To reduce the volume of data used, we filtered
non-parallel and other unhelpful segments accord-
ing to the technique described by Denkowski et al
(2012). This procedure uses a lexical translation
model learned from just the clean data, as well as
source and target n-gram language models to com-
pute the following feature scores:
? French and English 4-gram log likelihood (nor-
malized by length);
? French?English and English?French lexical
translation log likelihood (normalized by
length); and,
? Fractions of aligned words under the French?
English and English?French models.
We pooled previous years? WMT news test sets
to form a reference data set. We computed the
same features. To filter the web data, we retained
only sentence for which each feature score was
no lower than two standard deviations below the
mean on the reference data. This reduced the web
data from 25.2 million to 16.6 million sentence
pairs. Parallel segments from all parts of the data
that were blank on either side, were longer than 99
tokens, contained a token of more than 30 charac-
ters, or had particularly unbalanced length ratios
were also removed. After filtering, 30.9 million
sentence pairs remained for rule extraction: 14.4
million from the clean data, and 16.5 million from
the web data.
3.2 Preprocessing and Grammar Extraction
Our French?English system uses parse trees in
both the source and target languages, so tokeniza-
tion in this language pair was carried out to match
the tokenizations expected by the parsers we used
(English data was tokenized with the Stanford to-
kenizer for English and an in-house tokenizer for
French that targets the tokenization used by the
Berkeley French parser). Both sides of the par-
allel training data were parsed using the Berkeley
latent variable parser.
Synchronous context-free grammar rules were
extracted from the corpus following the method of
Hanneman et al (2011). This decomposes each
tree pair into a collection of SCFG rules by ex-
haustively identifying aligned subtrees to serve as
rule left-hand sides and smaller aligned subtrees
to be abstracted as right-hand-side nonterminals.
Basic subtree alignment heuristics are similar to
those by Galley et al (2006), and composed rules
are allowed. The computational complexity is held
in check by a limit on the number of RHS elements
(nodes and terminals), rather than a GHKM-style
maximum composition depth or Hiero-style max-
imum rule span. Our rule extractor also allows
?virtual nodes,? or the insertion of new nodes in
the parse tree to subdivide regions of flat struc-
ture. Virtual nodes are similar to the A+B ex-
tended categories of SAMT (Zollmann and Venu-
gopal, 2006), but with the added constraint that
they may not conflict with the surrounding tree
structure.
Because the SCFG rules are labeled with non-
terminals composed from both the source and tar-
get trees, the nonterminal inventory is quite large,
leading to estimation difficulties. To deal with
this, we automatically coarsening the nonterminal
labels (Hanneman and Lavie, 2011). Labels are
agglomeratively clustered based on a histogram-
based similarity function that looks at what tar-
get labels correspond to a particular source label
and vice versa. The number of clusters used is de-
termined based on spikes in the distance between
successive clustering iterations, or by the number
of source, target, or joint labels remaining. Start-
ing from a default grammar of 877 French, 2580
English, and 131,331 joint labels, we collapsed
the label space for our WMT system down to 50
French, 54 English, and 1814 joint categories.1
1Selecting the stopping point still requires a measure of
intuition. The label set size of 1814 chosen here roughly cor-
responds to the number of joint labels that would exist in the
grammar if virtual nodes were not included. This equivalence
has worked well in practice in both internal and published ex-
periments on other data sets (Hanneman and Lavie, 2013).
71
Extracted rules each have 10 features associated
with them. For an SCFG rule with source left-
hand side `s, target left-hand side `t, source right-
hand side rs, and target right-hand side rt, they
are:
? phrasal translation log relative frequencies
log f(rs | rt) and log f(rt | rs);
? labeling relative frequency log f(`s, `t | rs, rt)
and generation relative frequency
log f(rs, rt | `s, `t);
? lexical translation log probabilities log plex(rs |
rt) and log plex(rt | rs), defined similarly to
Moses?s definition;
? a rarity score exp( 1c )?1exp(1)?1 for a rule with frequency
c (this score is monotonically decreasing in the
rule frequency); and,
? three binary indicator features that mark
whether a rule is fully lexicalized, fully abstract,
or a glue rule.
Grammar filtering. Even after collapsing la-
bels, the extracted SCFGs contain an enormous
number of rules ? 660 million rule types from just
under 4 billion extracted instances. To reduce the
size of the grammar, we employ a combination of
lossless filtering and lossy pruning. We first prune
all rules to select no more than the 60 most fre-
quent target-side alternatives for any source RHS,
then do further filtering to produce grammars for
each test sentence:
? Lexical rules are filtered to the sentence level.
Only phrase pairs whose source sides match the
test sentence are retained.
? Abstract rules (whose RHS are all nontermi-
nals) are globally pruned. Only the 4000 most
frequently observed rules are retained.
? Mixed rules (whose RHS are a mix of terminals
and nonterminals) must match the test sentence,
and there is an additional frequency cutoff.
After this filtering, the number of completely lex-
ical rules that match a given sentence is typically
low, up to a few thousand rules. Each fully ab-
stract rule can potentially apply to every sentence;
the strict pruning cutoff in use for these rules is
meant to focus the grammar to the most important
general syntactic divergences between French and
English. Most of the latitude in grammar pruning
comes from adjusting the frequency cutoff on the
mixed rules since this category of rule is by far the
most common type. We conducted experiments
with three different frequency cutoffs: 100, 200,
and 500, with each increase decreasing the gram-
mar size by 70?80 percent.
3.3 French?English Experiments
We tuned our system to the newstest2008 set of
2051 segments. Aside from the official new-
stest2013 test set (3000 segments), we also col-
lected test-set scores from last year?s newstest2012
set (3003 segments). Automatic metric scores
are computed according to BLEU (Papineni et al,
2002), METEOR (Denkowski and Lavie, 2011),
and TER (Snover et al, 2006), all computed ac-
cording to MultEval v. 0.5 (Clark et al, 2011).
Each system variant is run with two independent
MERT steps in order to control for optimizer in-
stability.
Table 1 presents the results, with the metric
scores averaged over both MERT runs. Quite in-
terestingly, we find only minor differences in both
tune and test scores despite the large differences in
filtered/pruned grammar size as the cutoff for par-
tially abstract rules increases. No system is fully
statistically separable (at p < 0.05) from the oth-
ers according to MultEval?s approximate random-
ization algorithm. The closest is the variant with
cutoff 200, which is generally judged to be slightly
worse than the other two. METEOR claims full
distinction on the 2013 test set, ranking the sys-
tem with the strictest grammar cutoff (500) best.
This is the version that we ultimately submitted to
the shared translation task.
4 Synthetic Translation Options
Before discussing our Russian?English and
English?Russian systems, we introduce the
concept of synthetic translation options, which
we use in these systems. We provide a brief
overview here; for more detail, we refer the reader
to Tsvetkov et al (2013).
In language pairs that are typologically similar,
words and phrases map relatively directly from
source to target languages, and the standard ap-
proach to learning phrase pairs by extraction from
parallel data can be very effective. However, in
language pairs in which individual source lan-
guage words have many different possible transla-
tions (e.g., when the target language word could
have many different inflections or could be sur-
rounded by different function words that have no
72
Dev (2008) Test (2012) Test (2013)
System BLEU METR TER BLEU METR TER BLEU METR TER
Cutoff 100 22.52 31.44 59.22 27.73 33.30 53.25 28.34 * 33.19 53.07
Cutoff 200 22.34 31.40 59.21 * 27.33 33.26 53.23 * 28.05 * 33.07 53.16
Cutoff 500 22.80 31.64 59.10 27.88 * 33.58 53.09 28.27 * 33.31 53.13
Table 1: French?English automatic metric scores for three grammar pruning cutoffs, averaged over two
MERT runs each. Scores that are statistically separable (p < 0.05) from both others in the same column
are marked with an asterisk (*).
direct correspondence in the source language), we
can expect the standard phrasal inventory to be
incomplete, except when very large quantities of
parallel data are available or for very frequent
words. There simply will not be enough exam-
ples from which to learn the ideal set of transla-
tion options. Therefore, since phrase based trans-
lation can only generate input/output word pairs
that were directly observed in the training corpus,
the decoder?s only hope for producing a good out-
put is to find a fluent, meaning-preserving transla-
tion using incomplete translation lexicons. Syn-
thetic translation option generation seeks to fill
these gaps using secondary generation processes
that produce possible phrase translation alterna-
tives that are not directly extractable from the
training data. By filling in gaps in the transla-
tion options used to construct the sentential trans-
lation search space, global discriminative transla-
tion models and language models can be more ef-
fective than they would otherwise be.
From a practical perspective, synthetic transla-
tion options are attractive relative to trying to build
more powerful models of translation since they
enable focus on more targeted translation prob-
lems (for example, transliteration, or generating
proper inflectional morphology for a single word
or phrase). Since they are translation options and
not complete translations, many of them may be
generated.
In the following system pairs, we use syn-
thetic translation options to augment hiero gram-
mar rules learned in the usual way. The synthetic
phrases we include augment draw from several
sources:
? transliterations of OOV Russian words (?5.3);
? English target sides with varied function words
(for example, given a phrase that translates into
cat we procedure variants like the cat, a cat and
of the cat); and,
? when translating into Russian, we generate
phrases by first predicting the most likely Rus-
sian lemma for a source word or phrase, and
then, conditioned on the English source context
(including syntactic and lexical features), we
predict the most likely inflection of the lemma
(?6.1).
5 Russian?English System
5.1 Data
We used the same parallel data for both the
Russian?English and English Russian systems.
Except for filtering to remove sentence pairs
whose log length ratios were statistical outliers,
we only filtered the Common Crawl corpus to re-
move sentence pairs with less than 50% concentra-
tion of Cyrillic characters on the Russian side. The
remaining data was tokenized and lower-cased.
For language models, we trained 4-gram Markov
models using the target side of the bitext and any
available monolingual data (including Gigaword
for English). Additionally, we trained 7-gram lan-
guage models using 600-class Brown clusters with
Witten-Bell smoothing.2
5.2 Baseline System
Our baseline Russian?English system is a hierar-
chical phrase-based translation model as imple-
mented in cdec (Chiang, 2007; Dyer et al, 2010).
SCFG translation rules that plausibly match each
sentence in the development and deftest sets were
extracted from the aligned parallel data using the
suffix array indexing technique of Lopez (2008).
A Russian morphological analyzer was used to
lemmatize the training, development, and test
data, and the ?noisier channel? translation ap-
proach of Dyer (2007) was used in the Russian?
English system to let unusually inflected surface
forms back off to per-lemma translations.
2http://www.ark.cs.cmu.edu/cdyer/ru-600/.
73
5.3 Synthetic Translations: Transliteration
Analysis revealed that about one third of the un-
seen Russian tokens in the development set con-
sisted of named entities which should be translit-
erated. We used individual Russian-English word
pairs in Wikipedia parallel headlines 3 to train a
linear-chained CRF tagger which labels each char-
acter in the Russian token with a sequence of zero
or more English characters (Ammar et al, 2012).
Since Russian names in the training set were in
nominative case, we used a simple rule-based mor-
phological generator to produce possible inflec-
tions and filtered out the ones not present in the
Russian monolingual corpus. At decoding, un-
seen Russian tokens are fed to the transliterator
which produces the most probable 20 translitera-
tions. We add a synthetic translation option for
each of the transliterations with four features: an
indicator feature for transliterations, the CRF un-
normalized score, the trigram character-LM log-
probability, and the divergence from the average
length-ratio between an English name and its Rus-
sian transliteration.
5.4 Synthetic Translations: Function Words
Slavic languages like Russian have a large number
of different inflected forms for each lemma, repre-
senting different cases, tenses, and aspects. Since
our training data is rather limited relative to the
number of inflected forms that are possible, we use
an English language model to generate a variety
of common function word contexts for each con-
tent word phrase. These are added to the phrase
table with a feature indicating that they were not
actually observed in the training data, but rather
hallucinated using SRILM?s disambig tool.
5.5 Summary
Table 5.5 summarizes our Russian-English trans-
lation results. In the submitted system, we addi-
tionally used MBR reranking to combine the 500-
best outputs of our system, with the 500-best out-
puts of a syntactic system constructed similarly to
the French?English system.
6 English?Russian System
The bilingual training data was identical to the
filtered data used in the previous section. Word
alignments was performed after lemmatizing the
3We contributed the data set to the shared task participants
at http://www.statmt.org/wmt13/wiki-titles.ru-en.tar.gz
Table 2: Russian-English summary.
Condition BLEU
Baseline 30.8
Function words 30.9
Transliterations 31.1
Russian side of the training corpus. An unpruned,
modified Kneser-Ney smoothed 4-gram language
model (Chen and Goodman, 1996) was estimated
from all available Russian text (410 million words)
using the KenLM toolkit (Heafield et al, 2013).
A standard hierarchical phrase-based system
was trained with rule shape indicator features, ob-
tained by replacing terminals in translation rules
by a generic symbol. MIRA training was per-
formed to learn feature weights.
Additionally, word clusters (Brown et al, 1992)
were obtained for the complete monolingual Rus-
sian data. Then, an unsmoothed 7-gram language
model was trained on these clusters and added as
a feature to the translation system. Indicator fea-
tures were also added for each cluster and bigram
cluster occurence. These changes resulted in an
improvement of more than a BLEU point on our
held-out development set.
6.1 Predicting Target Morphology
We train a classifier to predict the inflection of
each Russian word independently given the cor-
responding English sentence and its word align-
ment. To do this, we first process the Russian
side of the parallel training data using a statisti-
cal morphological tagger (Sharoff et al, 2008) to
obtain lemmas and inflection tags for each word
in context. Then, we obtain part-of-speech tags
and dependency parses of the English side of the
parallel data (Martins et al, 2010), as well as
Brown clusters (Brown et al, 1992). We extract
features capturing lexical and syntactical relation-
ships in the source sentence and train structured
linear logistic regression models to predict the tag
of each English word independently given its part-
of-speech.4 In practice, due to the large size of
the corpora and of the feature space dimension,
we were only able to use about 10% of the avail-
able bilingual data, sampled randomly from the
Common Crawl corpus. We also restricted the
4We restrict ourselves to verbs, nouns, adjectives, adverbs
and cardinals since these open-class words carry most inflec-
tion in Russian.
74
??? ???????? ???????? ???? ?? ?? ?????????
she had attempted to cross the road on her bike
PRP   VBD         VBN          TO    VB       DT     NN    IN  PRP$   NN
nsubj
aux
xcomp
aux
????????_V*+*mis/sfm/e
C50   C473        C28          C8    C275   C37   C43  C82 C94   C331
Figure 1: The classifier is trained to predict the verbal inflection mis-sfm-e based on the linear and
syntactic context of the words aligned to the Russian word; given the stem ???????? (pytat?sya), this
inflection paradigm produces the observed surface form ???????? (pytalas?).
set of possible inflections for each word to the set
of tags that were observed with its lemma in the
full monolingual training data. This was neces-
sary because of our choice to use a tagger, which
is not able to synthesize surface forms for a given
lemma-tag pair.
We then augment the standard hierarchical
phrase-base grammars extracted for the baseline
systems with new rules containing inflections not
necessarily observed in the parallel training data.
We start by training a non-gappy phrase transla-
tion model on the bilingual data where the Russian
has been lemmatized.5 Then, before translating an
English sentence, we extract translation phrases
corresponding to this specific sentence and re-
inflect each word in the target side of these phrases
using the classifier with features extracted from
the source sentence words and annotations. We
keep the original phrase-based translation features
and add the inflection score predicted by the clas-
sifier as well as indicator features for the part-of-
speech categories of the re-inflected words.
On a held-out development set, these synthetic
phrases produce a 0.3 BLEU point improvement.
Interestingly, the feature weight learned for using
these phrases is positive, indicating that useful in-
flections might be produced by this process.
7 Conclusion
The CMU systems draws on a large number of
different research directions. Techniques such as
MBR reranking and synthetic phrases allow dif-
ferent contributors to focus on different transla-
5We keep intact words belonging to non-predicted cate-
gories.
tion problems that are ultimately recombined into
a single system. Our performance, in particular,
on English?Russian machine translation was quite
satisfying, we attribute our biggest gains in this
language pair to the following:
? Our inflection model that predicted how an En-
glish word ought best be translated, given its
context. This enabled us to generate forms that
were not observed in the parallel data or would
have been rare independent of context with pre-
cision.
? Brown cluster language models seem to be quite
effective at modeling long-range morphological
agreement patterns quite reliably.
Acknowledgments
We sincerely thank the organizers of the work-
shop for their hard work, year after year, and the
reviewers for their careful reading of the submit-
ted draft of this paper. This research work was
supported in part by the U. S. Army Research
Laboratory and the U. S. Army Research Office
under contract/grant number W911NF-10-1-0533,
by the National Science Foundation under grant
IIS-0915327, by a NPRP grant (NPRP 09-1140-
1-177) from the Qatar National Research Fund (a
member of the Qatar Foundation), and by com-
puting resources provided by the NSF-sponsored
XSEDE program under grant TG-CCR110017.
The statements made herein are solely the respon-
sibility of the authors.
75
References
Waleed Ammar, Chris Dyer, and Noah A. Smith. 2012.
Transliteration by sequence labeling with lattice en-
codings and reranking. In NEWS workshop at ACL.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computional Linguistics, 18(4):467?479.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 310?318, Santa Cruz, California, USA,
June. Association for Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing
for statistical machine translation: Crontrolling for
optimizer instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Short Papers, pages 176?181, Portland,
Oregon, USA, June.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 85?91, Edinburgh, Scot-
land, UK, July.
Michael Denkowski, Greg Hanneman, and Alon Lavie.
2012. The cmu-avenue french-english translation
system. In Proceedings of the NAACL 2012 Work-
shop on Statistical Machine Translation.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proc. of ACL.
Chris Dyer, Kevin Gimpel, Jonathan H. Clark, and
Noah A. Smith. 2011. The CMU-ARK German-
English translation system. In Proceedings of the
Sixth Workshop on Statistical Machine Translation.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM Model 2. In Proc. of NAACL.
Chris Dyer. 2007. The ?noiser channel?: Translation
from morphologically complex languages. In Pro-
ceedings of WMT.
Vladimir Eidelman. 2012. Optimization strategies for
online large-margin learning in machine translation.
In Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the ACL, pages 961?968, Sydney, Australia,
July.
Greg Hanneman and Alon Lavie. 2011. Automatic
category label coarsening for syntax-based machine
translation. In Proceedings of SSST-5: Fifth Work-
shop on Syntax, Semantics, and Structure in Statis-
tical Translation, pages 98?106, Portland, Oregon,
USA, June.
Greg Hanneman and Alon Lavie. 2013. Improving
syntax-augmented machine translation by coarsen-
ing the label set. In Proceedings of NAACL-HLT
2013, pages 288?297, Atlanta, Georgia, USA, June.
Greg Hanneman, Michelle Burroughs, and Alon Lavie.
2011. A general-purpose rule extractor for SCFG-
based machine translation. In Proceedings of SSST-
5: Fifth Workshop on Syntax, Semantics, and Struc-
ture in Statistical Translation, pages 135?144, Port-
land, Oregon, USA, June.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, Sofia, Bulgaria,
August.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, Scotland, UK, July.
Shankar Kumar, Wolfgang Macherey, Chris Dyer,
and Franz Och. 2009. Efficient minimum error
rate training and minimum Bayes-risk decoding for
translation hypergraphs and lattices. In Proc. of
ACL-IJCNLP.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proc. of COLING.
Nitin Madnani. 2010. The Circle of Meaning: From
Translation to Paraphrasing and Back. Ph.D. the-
sis, Department of Computer Science, University of
Maryland College Park.
Andre? F. T. Martins, Noah A. Smith, Eric P. Xing, Pe-
dro M. Q. Aguiar, and Ma?rio A. T. Figueiredo. 2010.
Turbo parsers: Dependency parsing by approximate
variational inference. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July.
76
Serge Sharoff, Mikhail Kopotev, Tomaz Erjavec, Anna
Feldman, and Dagmar Divjak. 2008. Designing and
evaluating a russian tagset. In Proc. of LREC.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the Seventh Conference of the As-
sociation for Machine Translation in the Americas,
pages 223?231, Cambridge, Massachusetts, USA,
August.
Yulia Tsvetkov, Chris Dyer, Lori Levin, and Archna
Batia. 2013. Generating English determiners in
phrase-based translation with synthetic translation
options. In Proceedings of the Eighth Workshop on
Statistical Machine Translation.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings of the Workshop on Statistical Ma-
chine Translation, pages 138?141, New York, New
York, USA, June.
77
Workshop on Humans and Computer-assisted Translation, pages 72?77,
Gothenburg, Sweden, 26 April 2014.
c?2014 Association for Computational Linguistics
Real Time Adaptive Machine Translation for Post-Editing with
cdec and TransCenter
Michael Denkowski Alon Lavie Isabel Lacruz
?
Chris Dyer
Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213 USA
?
Institute for Applied Linguistics, Kent State University, Kent, OH 44242 USA
{mdenkows,alavie,cdyer}@cs.cmu.edu ilacruz@kent.edu
Abstract
Using machine translation output as a
starting point for human translation has
recently gained traction in the transla-
tion community. This paper describes
cdec Realtime, a framework for build-
ing adaptive MT systems that learn from
post-editor feedback, and TransCenter, a
web-based translation interface that con-
nects users to Realtime systems and logs
post-editing activity. This combination
allows the straightforward deployment of
MT systems specifically for post-editing
and analysis of human translator produc-
tivity when working with these systems.
All tools, as well as actual post-editing
data collected as part of a validation exper-
iment, are freely available under an open
source license.
1 Introduction
This paper describes the end-to-end machine
translation post-editing setup provided by cdec
Realtime and TransCenter. As the quality of MT
systems continues to improve, the idea of using
automatic translation as a primary technology in
assisting human translators has become increas-
ingly attractive. Recent work has explored the
possibilities of integrating MT into human transla-
tion workflows by providing MT-generated trans-
lations as a starting point for translators to cor-
rect, as opposed to translating source sentences
from scratch. The motivation for this process is
to dramatically reduce human translation effort
while improving translator productivity and con-
sistency. This computer-aided approach is directly
applicable to the wealth of scenarios that still re-
quire precise human-quality translation that MT
is currently unable to deliver, including an ever-
increasing number of government, commercial,
and community-driven projects.
The software described in the following sec-
tions enables users to translate documents with
the assistance of an adaptive MT system using
a web-based interface. The system learns from
user feedback, improving translation quality as
users work. All user interaction is logged, al-
lowing post-editing sessions to be replayed and
analyzed. All software is freely available under
an open source license, allowing anyone to eas-
ily build, deploy, and evaluate MT systems specif-
ically for post-editing. We first describe the under-
lying adaptive MT paradigm (?2) and the Realtime
implementation (?3). We then describe Trans-
Center (?4) and the results of an end-to-end post-
editing experiment with human translators (?5).
All data collected as part of this validation experi-
ment is also publicly available.
2 Adaptive Machine Translation
Traditional machine translation systems operate in
batch mode: statistical translation models are es-
timated from large volumes of sentence-parallel
bilingual text and then used to translate new text.
Incorporating new data requires a full system re-
build, an expensive operation taking up to days of
time. As such, MT systems in production scenar-
ios typically remain static for large periods of time
(months or even indefinitely). Recently, an adap-
tive MT paradigm has been introduced specifi-
cally for post-editing (Denkowski et al., 2014).
Three major MT system components are extended
to support online updates, allowing human post-
editor feedback to be immediately incorporated:
? An online translation model is updated to in-
clude new translations extracted from post-
editing data.
? A dynamic language model is updated to in-
clude post-edited target language text.
? An online update is made to the system?s
feature weights after each sentence is post-
edited.
72
These extensions allow the MT system to gener-
ate improved translations that require significantly
less effort to correct for later sentences in the doc-
ument. This paradigm is now implemented in
the freely available cdec (Dyer et al., 2010) ma-
chine translation toolkit as Realtime, part of the
pycdec (Chahuneau et al., 2012) Python API.
Standard MT systems use aggregate statistics
from all training text to learn a single large
translation grammar (in the case of cdec?s hi-
erarchical phrase-based model (Chiang, 2007), a
synchronous context-free grammar) consisting of
rules annotated with feature scores. As an alter-
native, the bitext can be indexed using a suffix ar-
ray (Lopez, 2008), a data structure allowing fast
source-side lookups. When a new sentence is to be
translated, training sentences that share spans of
text with the input sentence are sampled from the
suffix array. Statistics from the sample are used to
learn a small, sentence-specific grammar on-the-
fly. The adaptive paradigm extends this approach
to support online updates by also indexing the
new bilingual sentences generated as a post-editor
works. When a new sentence is translated, match-
ing sentences are sampled from the post-editing
data as well as the suffix array. All feature scores
that can be computed on a suffix array sample can
be identically computed on the combined sample,
allowing uniform handling of all data. An addi-
tional ?post-edit support? feature is included that
indicates whether a grammar rule was extracted
from the post-editing data. This allows an opti-
mizer to learn to prefer translations that originate
from human feedback. This adaptation approach
also serves as a platform for exploring expanded
post-editing-aware feature sets; any feature that
can be computed from standard text can be added
to the model and will automatically include post-
editing data. Implementationally, feature scoring
is broken out into a single Python source file con-
taining a single function for each feature score.
New feature functions can be added easily.
The adaptive paradigm uses two language mod-
els. A standard (static) n-gram language model es-
timated on large monolingual text allows the sys-
tem to prefer translations more similar to human-
generated text in the target language. A (dy-
namic) Bayesian n-gram language model (Teh,
2006) can be updated with observations of the
post-edited output in a straightforward way. This
smaller model exactly covers the training bitext
and all post-editing data, letting the system up-
weight translations with newly learned vocabu-
lary and phrasing absent in the large monolingual
text. Finally, the margin-infused relaxed algorithm
(MIRA) (Crammer et al., 2006; Eidelman, 2012)
is used to make an online parameter update after
each sentence is post-edited, minimizing model er-
ror. This allows the system to continuously rescale
weights for translation and language model fea-
tures that adapt over time.
Since true post-editing data is infeasible to col-
lect during system development and internal test-
ing, as standard MT pipelines require tens of thou-
sands of sentences to be translated with low la-
tency, a simulated post-editing paradigm (Hardt
and Elming, 2010) can be used, wherein pre-
generated reference translations act as a stand-in
for actual post-editing. This approximation is ef-
fective for tuning and internal evaluation when
real post-editing data is unavailable. In simulated
post-editing tasks, decoding (for both the test cor-
pus and each pass over the development corpus
during optimization) begins with baseline mod-
els trained on standard bilingual and monolingual
text. After each sentence is translated, the fol-
lowing take place in order: First, MIRA uses the
new source?reference pair to update weights for
the current models. Second, the source is aligned
to the reference using word-alignment models
learned from the initial data and used to update the
translation grammar. Third, the reference is added
to the Bayesian language model. As sentences are
translated, the models gain valuable context infor-
mation, allowing them to adapt to the specific tar-
get document and translator. Context is reset at the
start of each development or test corpus. Systems
optimized with simulated post-editing can then be
deployed to serve real human translators without
further modification.
3 cdec Realtime
Now included as part of the free, open source
cdec machine translation toolkit (Dyer et al.,
2010), Realtime
1
provides an efficient implemen-
tation of the adaptive MT paradigm that can serve
an arbitrary number of unique post-editors concur-
rently. A full Realtime tutorial, including step-
by-step instructions for installing required soft-
ware and building full adaptive systems, is avail-
1
https://github.com/redpony/cdec/tree/
master/realtime
73
import rt
# Start new Realtime translator using a Spanish--English
# system and automatic, language-independent text normalization
# (pre-tokenization and post-detokenization)
translator = rt.RealtimeTranslator(?es-en.d?, tmpdir=?/tmp?, cache_size=5,
norm=True)
# Translate a sentence for user1
translation = translator.translate(?Muchas gracias Chris.?, ctx_name=?user1?)
# Learn from user1?s post-edited transaltion
translator.learn(?Muchas gracias Chris.?, ?Thank you so much, Chris.?,
ctx_name=?user1?)
# Save, free, and reload state for user1
translator.save_state(file_or_stringio=?user1.state?, ctx_name=?user1?)
translator.drop_ctx(ctx_name=?user1?)
translator.load_state(file_or_stringio=?user1.state?, ctx_name=?user1?)
Figure 1: Sample code using the Realtime Python API to translate and learn from post-editing.
able online.
2
Building an adaptive system begins
with the usual MT pipeline steps: word alignment,
bitext indexing (for suffix array grammar extrac-
tion), and standard n-gram language model esti-
mation. Additionally, the cpyp
3
package, also
freely available, is used to estimate a Bayesian
n-gram language model on the target side of the
bitext. The cdec grammar extractor and dy-
namic language model implementations both in-
clude support for efficient inclusion of incremental
data, allowing optimization with simulated post-
editing to be parallelized. The resulting system,
optimized for post-editing, is then ready for de-
ployment with Realtime.
At runtime, a Realtime system operates as fol-
lows. A single instance of the indexed bitext is
loaded into memory for grammar extraction. Sin-
gle instances of the directional word alignment
models are loaded into memory for force-aligning
post-edited data. When a new user requests a
translation, a new context is started. The follow-
ing are loaded into memory: a table of all post-
edited data from the user, a user-specific dynamic
language model, and a user-specific decoder (in
this case an instance of MIRA that has a user-
specific decoder and set of weights). Each user
also requires an instance of the large static lan-
guage model, though all users effectively share a
single instance through the memory mapped im-
plementation of KenLM (Heafield, 2011). When a
2
http://www.cs.cmu.edu/
?
mdenkows/
cdec-realtime.html
3
https://github.com/redpony/cpyp
new sentence is to be translated, the grammar ex-
tractor samples from the shared background data
plus the user-specific post-editing data to generate
a sentence-specific grammar incorporating data
from all prior sentences translated by the same
user. The sentence is then decoded using the user
and time-specific grammar, current weights, and
current dynamic language model. When a post-
edited sentence is available as feedback, the fol-
lowing happen in order: (1) the source-reference
pair is used to update feature weights with MIRA,
(2) the source-reference pair is force-aligned and
added to the indexed post-editing data, and (3) the
dynamic language model is updated with the ref-
erence. User state (current weights and indexed
post-edited data for grammars and the language
model) can be saved and loaded, allowing mod-
els to be loaded and freed from memory as trans-
lators start and stop their work. Figure 1 shows
a minimal example of the above using the Real-
time package. While this paper describes integra-
tion with TransCenter, a tool primarily targeting
data collection and analysis, the Realtime Python
API allows straightforward integration with other
computer-assisted translation tools such as full-
featured translation workbench environments.
4 TransCenter: Web-Based Translation
Research Suite
The TransCenter software (Denkowski and Lavie,
2012) dramatically lowers barriers in post-editing
data collection and increases the accuracy and de-
scriptiveness of the collected data. TransCenter
74
Figure 2: Example of editing and rating machine translations with the TransCenter web interface.
Figure 3: Example TransCenter summary report for a single user on a document.
provides a web-based translation editing interface
that remotely monitors and records user activity.
The ?live? version
4
now uses cdec Realtime to
provide on-demand MT that automatically learns
from post-editor feedback. Translators use a web
browser to access a familiar two-column editing
environment (shown in Figure 2) from any com-
puter with an Internet connection. The left column
displays the source sentences, while the right col-
umn, initially empty, is incrementally populated
with translations from the Realtime system as the
user works. For each sentence, the translator ed-
its the MT output to be grammatically correct and
convey the same information as the source sen-
tence. During editing, all user actions (key presses
and mouse clicks) are logged so that the full edit-
ing process can be replayed and analyzed. After
editing, the final translation is reported to the Re-
altime system for learning and the next transla-
tion is generated. The user is additionally asked
to rate the amount of work required to post-edit
each sentence immediately after completing it,
yielding maximally accurate feedback. The rating
scale ranges from 5 (no post-editing required) to
1 (requires total re-translation). TransCenter also
records the number of seconds each sentence is
focused, allowing for exact timing measurements.
A pause button is available if the translator needs
to take breaks. TransCenter can generate reports
4
https://github.com/mjdenkowski/
transcenter-live
of translator effort as measured by (1) keystroke,
(2) exact timing, and (3) actual translator post-
assessment. Final translations are also available
for calculating edit distance. Millisecond-level
timing of all user actions further facilitates time
sequence analysis of user actions and pauses. Fig-
ure 3 shows an example summary report gener-
ated by TransCenter showing a user?s activity on
each sentence in a document. This information
is also output in a simple comma-separated value
format for maximum interoperability with other
standards-compliant tools.
TransCenter automatically handles resource
management with Realtime. When a TransCenter
server is started, it loads a Realtime system with
zero contexts into memory. As users log in to work
on documents, new contexts are created to deliver
on-demand translations. As users finish work-
ing or take extended breaks, contexts automati-
cally time out and resources are freed. Translator
and document-specific state is automatically saved
when contexts time out and reloaded when transla-
tors resume work with built-in safeguards against
missing or duplicating any post-editing data due
to timeouts or Internet connectivity issues. This
allows any number of translators to work on trans-
lation tasks at their convenience.
5 Experiments
In a preliminary experiment to evaluate the impact
of adaptive MT in real-world post-editing scenar-
75
HTER Rating
Baseline 19.26 4.19
Adaptive 17.01 4.31
Table 1: Aggregate HTER scores and average
translator self-ratings (5 point scale) of post-
editing effort for translations of TED talks from
Spanish into English.
ios, we compare a static Spanish?English MT sys-
tem to a comparable adaptive system on a blind
out-of-domain test. Competitive with the current
state-of-the-art, both systems are trained on the
2012 NAACL WMT (Callison-Burch et al., 2012)
constrained resources (2 million bilingual sen-
tences) using the cdec toolkit (Dyer et al., 2010).
Blind post-editing evaluation sets are drawn from
the Web Inventory of Transcribed and Translated
Talks (WIT
3
) corpus (Cettolo et al., 2012) that
makes transcriptions of TED talks
5
available in
several languages, including English and Spanish.
We select 4 excerpts from Spanish talk transcripts
(totaling 100 sentences) to be translated into En-
glish. Five students training to be professional
translators post-edit machine translations of these
excerpts using TransCenter. Translations are pro-
vided by either the static or fully adaptive system.
Tasks are divided such that each user translates
2 excerpts with the static system and 2 with the
adaptive system and each excerpt is post-edited ei-
ther 2 or 3 times with each system. Users do not
know which system is providing the translations.
Using the data collected by TransCenter, we
evaluate post-editing effort with the established
human-targeted translation edit rate (HTER) met-
ric (Snover et al., 2006). HTER computes an
edit distance score between initial MT outputs and
the ?targeted? references created by human post-
editing, with lower scores being better. Results
for the two systems are aggregated over all users
and documents. Shown in Table 1, introducing
an adaptive MT system results in a significant re-
duction in editing effort. We additionally aver-
age the user post-ratings for each translation by
system to evaluate user perception of the adap-
tive system compared to the static baseline. Also
shown in Table 1, we see a slight preference for
the adaptive system. This data, as well as precise
keystroke, mouse click, and timing information is
5
http://www.ted.com/talks
made freely available for further analysis.
6
Trans-
Center records all data necessary for more sophis-
ticated editing time analysis (Koehn, 2012) as well
as analysis of translator behavior, including pauses
(used as an indicator of cognitive effort) (Lacruz et
al., 2012).
6 Related Work
There has been a recent push for new computer-
aided translation (CAT) tools that leverage adap-
tive machine translation. The CASMACAT
7
project (Alabau et al., 2013) focuses on building
state-of-the-art tools for computer-aided transla-
tion. This includes translation predictions backed
by machine translation systems that incrementally
update model parameters as users edit translations
(Mart??nez-G?omez et al., 2012; L?opez-Salcedo et
al., 2012). The MateCat
8
project (Cattelan, 2013)
specifically aims to integrate machine translation
(including online model adaptation and translation
quality estimation) into a web-based CAT tool.
Bertoldi et al. (2013) show improvements in trans-
lator productivity when using the MateCat tool
with an adaptive MT system that uses cache-based
translation and language models.
7 Conclusion
This paper describes the free, open source MT
post-editing setup provided by cdec Realtime
and TransCenter. All software and the data col-
lected for a preliminary post-editing experiment
are all freely available online. A live demon-
stration of adaptive MT post-editing powered by
Realtime and TransCenter is scheduled for the
2014 EACL Workshop on Humans and Computer-
assisted Translation (HaCaT 2014).
Acknowledgements
This work is supported in part by the National Sci-
ence Foundation under grant IIS-0915327, by the
Qatar National Research Fund (a member of the
Qatar Foundation) under grant NPRP 09-1140-1-
177, and by the NSF-sponsored XSEDE program
under grant TG-CCR110017.
6
www.cs.cmu.edu/
?
mdenkows/
transcenter-round1.tar.gz
7
http://casmacat.eu/
8
http://www.matecat.com/
76
References
Vicent Alabau, Ragnar Bonk, Christian Buck, Michael
Carl, Francisco Casacuberta, Mercedes Garc??a-
Mart??nez, Jes?us Gonz?alez-Rubio, Philipp Koehn,
Luis A. Leiva, Bartolom?e Mesa-Lao, Daniel Ortiz-
Mart??nez, Herv?e Saint-Amand, Germ?an Sanchis-
Trilles, and Chara Tsoukala. 2013. Casmacat:
An open source workbench for advanced computer
aided translation. In The Prague Bulletin of Mathe-
matical Linguistics, pages 101?112.
Nicola Bertoldi, Mauro Cettolo, and Marcello Fed-
erico. 2013. Cache-based online adaptation for ma-
chine translation enhanced computer assisted trans-
lation. In Proceedings of the XIV Machine Transla-
tion Summit, pages 35?42.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montr?eal, Canada, June. Association for
Computational Linguistics.
Alessandro Cattelan. 2013. Second version of Mate-
Cat tool. Deliverable 4.2.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. Wit
3
: Web inventory of transcribed
and translated talks. In Proceedings of the Sixteenth
Annual Conference of the European Association for
Machine Translation.
Victor Chahuneau, Noah A. Smith, and Chris Dyer.
2012. pycdec: A python interface to cdec. The
Prague Bulletin of Mathematical Linguistics, 98:51?
61.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, pages 551?558, March.
Michael Denkowski and Alon Lavie. 2012. Trans-
Center: Web-based translation research suite. In
AMTA 2012 Workshop on Post-Editing Technology
and Practice Demo Session.
Michael Denkowski, Chris Dyer, and Alon Lavie.
2014. Learning from post-editing: Online model
adaptation for statistical machine translation. In
Proceedings of the 14th Conference of the European
Chapter of the Association for Computational Lin-
guistics.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of the ACL 2010 System Demonstra-
tions, pages 7?12, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Vladimir Eidelman. 2012. Optimization strategies for
online large-margin learning in machine translation.
In Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 480?489, Montr?eal,
Canada, June. Association for Computational Lin-
guistics.
Daniel Hardt and Jakob Elming. 2010. Incremental
re-training for post-editing smt. In Proceedings of
the Ninth Conference of the Association for Machine
Translation in the Americas.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197, Edinburgh, Scotland, United Kingdom,
July.
Philipp Koehn. 2012. Computer-aided translation.
Machine Translation Marathon.
Isabel Lacruz, Gregory M. Shreve, and Erik Angelone.
2012. Average Pause Ratio as an Indicator of Cogni-
tive Effort in Post-Editing: A Case Study. In AMTA
2012 Workshop on Post-Editing Technology and
Practice (WPTP 2012), pages 21?30, San Diego,
USA, October. Association for Machine Translation
in the Americas (AMTA).
Adam Lopez. 2008. Machine translation by pattern
matching. In Dissertation, University of Maryland,
March.
Francisco-Javier L?opez-Salcedo, Germ?an Sanchis-
Trilles, and Francisco Casacuberta. 2012. On-
line learning of log-linear weights in interactive ma-
chine translation. Advances in Speech and Lan-
guage Technologies for Iberian Languages, pages
277?286.
Pascual Mart??nez-G?omez, Germ?an Sanchis-Trilles, and
Francisco Casacuberta. 2012. Online adaptation
strategies for statistical machine translation in post-
editing scenarios. Pattern Recognition, 45:3193?
3203.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In Proceedings of the 7th Conference of the.
Association for Machine Translation of the Ameri-
cas, pages 223?231.
Yee Whye Teh. 2006. A hierarchical Bayesian lan-
guage model based on Pitman-Yor processes. In
Proc. of ACL.
77
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 376?380,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Meteor Universal: Language Specific Translation Evaluation
for Any Target Language
Michael Denkowski Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{mdenkows,alavie}@cs.cmu.edu
Abstract
This paper describes Meteor Universal, re-
leased for the 2014 ACL Workshop on
Statistical Machine Translation. Meteor
Universal brings language specific evalu-
ation to previously unsupported target lan-
guages by (1) automatically extracting lin-
guistic resources (paraphrase tables and
function word lists) from the bitext used to
train MT systems and (2) using a univer-
sal parameter set learned from pooling hu-
man judgments of translation quality from
several language directions. Meteor Uni-
versal is shown to significantly outperform
baseline BLEU on two new languages,
Russian (WMT13) and Hindi (WMT14).
1 Introduction
Recent WMT evaluations have seen a variety of
metrics employ language specific resources to
replicate human translation rankings far better
than simple baselines (Callison-Burch et al., 2011;
Callison-Burch et al., 2012; Mach?a?cek and Bojar,
2013; Snover et al., 2009; Denkowski and Lavie,
2011; Dahlmeier et al., 2011; Chen et al., 2012;
Wang and Manning, 2012, inter alia). While the
wealth of linguistic resources for the WMT lan-
guages allows the development of sophisticated
metrics, most of the world?s 7,000+ languages lack
the prerequisites for building advanced metrics.
Researchers working on low resource languages
are usually limited to baseline BLEU (Papineni et
al., 2002) for evaluating translation quality.
Meteor Universal brings language specific eval-
uation to any target language by combining lin-
guistic resources automatically learned from MT
system training data with a universal metric pa-
rameter set that generalizes across languages.
Given only the bitext used to build a standard
phrase-based translation system, Meteor Universal
learns a paraphrase table and function word list,
two of the most consistently beneficial language
specific resources employed in versions of Me-
teor. Whereas previous versions of Meteor require
human ranking judgments in the target language
to learn parameters, Meteor Universal uses a sin-
gle parameter set learned from pooling judgments
from several languages. This universal parameter
set captures general preferences shown by human
evaluators across languages. We show this ap-
proach to significantly outperform baseline BLEU
for two new languages, Russian and Hindi. The
following sections review Meteor?s scoring func-
tion (?2), describe the automatic extraction of lan-
guage specific resources (?3), discuss training of
the universal parameter set (?4), report experimen-
tal results (?5), describe released software (?6),
and conclude (?7).
2 Meteor Scoring
Meteor evaluates translation hypotheses by align-
ing them to reference translations and calculating
sentence-level similarity scores. For a hypothesis-
reference pair, the space of possible alignments is
constructed by exhaustively identifying all possi-
ble matches between the sentences according to
the following matchers:
Exact: Match words if their surface forms are
identical.
Stem: Stem words using a language appropriate
Snowball Stemmer (Porter, 2001) and match if the
stems are identical.
Synonym: Match words if they share member-
ship in any synonym set according to the WordNet
database (Miller and Fellbaum, 2007).
Paraphrase: Match phrases if they are listed as
376
paraphrases in a language appropriate paraphrase
table (described in ?3.2).
All matches are generalized to phrase matches
with a span in each sentence. Any word occur-
ring within the span is considered covered by the
match. The final alignment is then resolved as the
largest subset of all matches meeting the following
criteria in order of importance:
1. Require each word in each sentence to be
covered by zero or one matches.
2. Maximize the number of covered words
across both sentences.
3. Minimize the number of chunks, where a
chunk is defined as a series of matches that
is contiguous and identically ordered in both
sentences.
4. Minimize the sum of absolute distances be-
tween match start indices in the two sen-
tences. (Break ties by preferring to align
phrases that occur at similar positions in both
sentences.)
Alignment resolution is conducted as a beam
search using a heuristic based on the above cri-
teria.
The Meteor score for an aligned sentence pair is
calculated as follows. Content and function words
are identified in the hypothesis (h
c
, h
f
) and ref-
erence (r
c
, r
f
) according to a function word list
(described in ?3.1). For each of the matchers
(m
i
), count the number of content and function
words covered by matches of this type in the hy-
pothesis (m
i
(h
c
), m
i
(h
f
)) and reference (m
i
(r
c
),
m
i
(r
f
)). Calculate weighted precision and re-
call using matcher weights (w
i
...w
n
) and content-
function word weight (?):
P =
?
i
w
i
? (? ?m
i
(h
c
) + (1? ?) ?m
i
(h
f
))
? ? |h
c
|+ (1? ?) ? |h
f
|
R =
?
i
w
i
? (? ?m
i
(r
c
) + (1? ?) ?m
i
(r
f
))
? ? |r
c
|+ (1? ?) ? |r
f
|
The parameterized harmonic mean of P and R
(van Rijsbergen, 1979) is then calculated:
F
mean
=
P ?R
? ? P + (1? ?) ?R
To account for gaps and differences in word order,
a fragmentation penalty is calculated using the to-
tal number of matched words (m, averaged over
hypothesis and reference) and number of chunks
(ch):
Pen = ? ?
(
ch
m
)
?
The Meteor score is then calculated:
Score = (1? Pen) ? F
mean
The parameters?, ?, ?, ?, andw
i
...w
n
are tuned
to maximize correlation with human judgments.
3 Language Specific Resources
Meteor uses language specific resources to dra-
matically improve evaluation accuracy. While
some resources such as WordNet and the Snowball
stemmers are limited to one or a few languages,
other resources can be learned from data for any
language. Meteor Universal uses the same bitext
used to build statistical translation systems to learn
function words and paraphrases. Used in con-
junction with the universal parameter set, these re-
sources bring language specific evaluation to new
target languages.
3.1 Function Word Lists
The function word list is used to discriminate be-
tween content and function words in the target lan-
guage. Meteor Universal counts words in the tar-
get side of the training bitext and considers any
word with relative frequency above 10
?3
to be a
function word. This list is used only during the
scoring stage of evaluation, where the tunable ?
parameter controls the relative weight of content
versus function words. When tuned to match hu-
man judgments, this parameter usually reflects a
greater importance for content words.
3.2 Paraphrase Tables
Paraphrase tables allow many-to-many matches
that can encapsulate any local language phenom-
ena, including morphology, synonymy, and true
paraphrasing. Identifying these matches allows
far more sophisticated evaluation than is possible
with simple surface form matches. In Meteor Uni-
versal, paraphrases act as the catch-all for non-
exact matches. Paraphrases are automatically ex-
tracted from the training bitext using the transla-
tion pivot approach (Bannard and Callison-Burch,
2005). First, a standard phrase table is learned
from the bitext (Koehn et al., 2003). Paraphrase
extraction then proceeds as follows. For each tar-
get language phrase (e
1
) in the table, find each
377
source phrase f that e
1
translates. Each alternate
phrase (e
2
6= e
1
) that translates f is considered
a paraphrase with probability P (f |e
1
) ? P (e
2
|f).
The total probability of e
2
being a paraphrase of
e
1
is the sum over all possible pivot phrases f :
P (e
2
|e
1
) =
?
f
P (f |e
1
) ? P (e
2
|f)
To improve paraphrase precision, we apply
several language independent pruning techniques.
The following are applied to each paraphrase in-
stance (e
1
, f , e
2
):
? Discard instances with very low probability
(P (f |e
1
) ? P (e
2
|f) < 0.001).
? Discard instances where e
1
, f , or e
2
contain
punctuation characters.
? Discard instances where e
1
, f , or e
2
con-
tain only function words (relative frequency
above 10
?3
in the bitext).
The following are applied to each final paraphrase
(e
1
, e
2
) after summing over all instances:
? Discard paraphrases with very low probabil-
ity (P (e
2
|e
1
) < 0.01).
? Discard paraphrases where e
2
is a sub-phrase
of e
1
.
This constitutes the full Meteor paraphrasing
pipeline that has been used to build tables for
fully supported languages (Denkowski and Lavie,
2011). Paraphrases for new languages have the
added advantage of being extracted from the same
bitext that MT systems use for phrase extraction,
resulting in ideal paraphrase coverage for evalu-
ated systems.
4 Universal Parameter Set
Traditionally, building a version of Meteor for a
new target language has required a set of human-
scored machine translations, most frequently in
the form of WMT rankings. The general lack of
availability of these judgments has severely lim-
ited the number of languages for which Meteor
versions could be trained. Meteor Universal ad-
dresses this problem with the introduction of a
?universal? parameter set that captures general hu-
man preferences that apply to all languages for
Direction Judgments
cs-en 11,021
de-en 11,934
es-en 9,796
fr-en 11,594
en-cs 18,805
en-de 14,553
en-es 11,834
en-fr 11,562
Total 101,099
Table 1: Binary ranking judgments per language
direction used to learn parameters for Meteor Uni-
versal
which judgment data does exist. We learn this pa-
rameter set by pooling over 100,000 binary rank-
ing judgments from WMT12 (Callison-Burch et
al., 2012) that cover 8 language directions (de-
tails in Table 1). Data for each language is scored
using the same resources (function word list and
paraphrase table only) and scoring parameters are
tuned to maximize agreement (Kendall?s ? ) over
all judgments from all languages, leading to a sin-
gle parameter set. The universal parameter set en-
codes the following general human preferences:
? Prefer recall over precision.
? Prefer word choice over word order.
? Prefer correct translations of content words
over function words.
? Prefer exact matches over paraphrase
matches, while still giving significant credit
to paraphrases.
Table 2 compares the universal parameters to those
learned for specific languages in previous versions
of Meteor. Notably, the universal parameter set is
more balanced, showing a normalizing effect from
generalizing across several language directions.
5 Experiments
We evaluate the Universal version of Meteor
against full language dedicated versions of Meteor
and baseline BLEU on the WMT13 rankings. Re-
sults for English, Czech, German, Spanish, and
French are biased in favor of Meteor Universal
since rankings for these target languages are in-
cluded in the training data while Russian consti-
tutes a true held out test. We also report the re-
sults of the WMT14 Hindi evaluation task. Shown
378
Language ? ? ? ? w
exact
w
stem
w
syn
w
par
English 0.85 0.20 0.60 0.75 1.00 0.60 0.80 0.60
Czech 0.95 0.20 0.60 0.80 1.00 ? ? 0.40
German 0.95 1.00 0.55 0.55 1.00 0.80 ? 0.20
Spanish 0.65 1.30 0.50 0.80 1.00 0.80 ? 0.60
French 0.90 1.40 0.60 0.65 1.00 0.20 ? 0.40
Universal 0.70 1.40 0.30 0.70 1.00 ? ? 0.60
Table 2: Comparison of parameters for language specific and universal versions of Meteor.
WMT13 ? M-Full M-Universal BLEU
English 0.214 0.206 0.124
Czech 0.092 0.085 0.044
German 0.163 0.157 0.097
Spanish 0.106 0.101 0.068
French 0.150 0.137 0.099
Russian ? 0.128 0.068
WMT14 ? M-Full M-Universal BLEU
Hindi ? 0.264 0.227
Table 3: Sentence-level correlation with human
rankings (Kendall?s ? ) for Meteor (language spe-
cific versions), Meteor Universal, and BLEU
in Table 3, Meteor Universal significantly out-
performs baseline BLEU in all cases while suf-
fering only slight degradation compared to ver-
sions of Meteor tuned for individual languages.
For Russian, correlation is nearly double that of
BLEU. This provides substantial evidence that
Meteor Universal will further generalize, bringing
improved evaluation accuracy to new target lan-
guages currently limited to baseline language in-
dependent metrics.
For the WMT14 evaluation, we use the tradi-
tional language specific versions of Meteor for all
language directions except Hindi. This includes
Russian, for which additional language specific re-
sources (a Snowball word stemmer) help signifi-
cantly. For Hindi, we use the release version of
Meteor Universal to extract linguistic resources
from the constrained training bitext provided for
the shared translation task. These resources are
used with the universal parameter set to score all
system outputs for the English?Hindi direction.
6 Software
Meteor Universal is included in Meteor version
1.5 which is publicly released for WMT14.
Meteor 1.5 can be downloaded from the official
webpage
1
and a full tutorial for Meteor Universal
is available online.
2
Building a version of Meteor
for a new language requires a training bitext
(corpus.f, corpus.e) and a standard Moses format
phrase table (phrase-table.gz) (Koehn et al.,
2007). To extract linguistic resources for Meteor,
run the new language script:
$ python scripts/new_language.py out \
corpus.f corpus.e phrase-table.gz
To use the resulting files to score translations with
Meteor, use the new language option:
$ java -jar meteor-
*
.jar test ref -new \
out/meteor-files
Meteor 1.5, including Meteor Universal, is free
software released under the terms of the GNU
Lesser General Public License.
7 Conclusion
This paper describes Meteor Universal, a version
of the Meteor metric that brings language specific
evaluation to any target language using the same
resources used to build statistical translation sys-
tems. Held out tests show Meteor Universal to sig-
nificantly outperform baseline BLEU on WMT13
Russian and WMT14 Hindi. Meteor version 1.5 is
freely available open source software.
Acknowledgements
This work is supported in part by the National Sci-
ence Foundation under grant IIS-0915327, by the
Qatar National Research Fund (a member of the
Qatar Foundation) under grant NPRP 09-1140-1-
177, and by the NSF-sponsored XSEDE program
under grant TG-CCR110017.
1
http://www.cs.cmu.edu/~alavie/METEOR/
2
http://www.cs.cmu.edu/~mdenkows/meteor-
universal.html
379
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?05), pages
597?604, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 22?64, Edinburgh, Scot-
land, July. Association for Computational Linguis-
tics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montr?eal, Canada, June. Association for
Computational Linguistics.
Boxing Chen, Roland Kuhn, and George Foster. 2012.
Improving amber, an mt evaluation metric. In Pro-
ceedings of the Seventh Workshop on Statistical Ma-
chine Translation, pages 59?63, Montr?eal, Canada,
June. Association for Computational Linguistics.
Daniel Dahlmeier, Chang Liu, and Hwee Tou Ng.
2011. Tesla at wmt 2011: Translation evaluation and
tunable metric. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 78?
84, Edinburgh, Scotland, July. Association for Com-
putational Linguistics.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 85?91, Edinburgh, Scot-
land, July. Association for Computational Linguis-
tics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of NAACL/HLT 2003.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic,
June. Association for Computational Linguistics.
Matou?s Mach?a?cek and Ond?rej Bojar. 2013. Results of
the WMT13 metrics shared task. In Proceedings of
the Eighth Workshop on Statistical Machine Trans-
lation, pages 45?51, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
George Miller and Christiane Fellbaum. 2007. Word-
Net. http://wordnet.princeton.edu/.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July.
Martin Porter. 2001. Snowball: A language for stem-
ming algorithms. http://snowball.tartarus.org/texts/.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER? Exploring different human judgments with
a tunable MT metric. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
259?268, Athens, Greece, March. Association for
Computational Linguistics.
C. J. van Rijsbergen, 1979. Information Retrieval,
chapter 7. Butterworths, London, UK, 2nd edition.
Mengqiu Wang and Christopher Manning. 2012.
Spede: Probabilistic edit distance metrics for mt
evaluation. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 76?
83, Montr?eal, Canada, June. Association for Com-
putational Linguistics.
380
